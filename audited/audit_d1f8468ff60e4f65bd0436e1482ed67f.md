# Audit Report

## Title
Non-Atomic Multi-Shard Commit in State Restore Causes Permanent Merkle Tree Corruption

## Summary
The `StateMerkleDb::commit_no_progress()` function writes Jellyfish Merkle tree nodes to 17 separate RocksDB instances (16 shards + metadata DB) sequentially without cross-database atomicity. A process crash during state snapshot restore can leave some shards committed while others remain uncommitted, permanently corrupting the Merkle tree structure and violating state consistency invariants.

## Finding Description

The vulnerability exists in the interaction between three components:

**1. State Snapshot Restore Async Commit Path:** [1](#0-0) 

During state snapshot restore, frozen Merkle tree nodes are written asynchronously. The `wait_for_async_commit()` function correctly blocks until the async write completes. [2](#0-1) 

**2. Multi-Shard Write Implementation:** [3](#0-2) 

The `write_node_batch()` method distributes nodes across shards based on the first nibble of their path (0-15 for shards, root node to metadata DB). [4](#0-3) 

**3. Sequential Non-Atomic Commit:** [5](#0-4) 

The `commit_no_progress()` function writes to each shard database sequentially. Each individual `write_schemas()` call is atomic and durable (sync=true), but there is NO atomicity across the 17 separate databases. [6](#0-5) [7](#0-6) 

**The Critical Flaw:**

When a process crash, power failure, OOM kill, or SIGKILL occurs during the sequential write loop, some shards will have committed their batches (with sync=true ensuring durability) while others will not. This creates an inconsistent Merkle tree where:

- Nodes in shards 0-N are permanently committed to disk
- Nodes in shards N+1 through 15 are missing
- Parent internal nodes may reference non-existent children
- The tree structure violates the invariant that all descendants of a committed node must also be committed

**Recovery Failure:** [8](#0-7) 

The `recover_partial_nodes()` function attempts crash recovery by scanning existing nodes from storage. However, it cannot distinguish between nodes that legitimately don't exist yet versus nodes missing due to partial commit. It will reconstruct a partial state treating the corrupted tree as valid, leading to incorrect root hash computation.

## Impact Explanation

**Severity: CRITICAL** (up to $1,000,000 per Aptos Bug Bounty)

This vulnerability breaks the following critical invariants:

1. **State Consistency Invariant (#4)**: "State transitions must be atomic and verifiable via Merkle proofs"
   - Partial commits create non-atomic state transitions
   - Merkle proofs become unverifiable when child nodes are missing

2. **Deterministic Execution Invariant (#1)**: "All validators must produce identical state roots for identical blocks"
   - If different nodes crash at different points during restore, they end up with different corrupted states
   - Nodes cannot reach consensus on state root hash

**Concrete Impact:**

- **Permanent State Corruption**: Merkle tree structure is permanently inconsistent on disk
- **Consensus Safety Violation**: Nodes with corrupted state may disagree on state root hashes
- **Network Partition Risk**: If enough nodes have different corrupted states, the network cannot reach consensus
- **Requires Manual Intervention**: Recovery may require wiping state and resyncing from scratch, or even a coordinated hardfork if multiple validators are affected
- **Silent Corruption**: The corruption may not be detected immediately but manifest later as verification failures

This qualifies as:
- "Consensus/Safety violations" (Critical tier)
- "Non-recoverable network partition" (Critical tier)
- "State inconsistencies requiring intervention" (Medium tier minimum)

## Likelihood Explanation

**Likelihood: HIGH**

State snapshot restore is a long-running operation that can take hours or days for large state. During this window, numerous crash scenarios are realistic:

1. **Process Crashes**: Out-of-memory conditions, panic bugs, resource exhaustion
2. **System Failures**: Power outages, hardware failures, kernel panics
3. **Operational Events**: Process kill signals (SIGKILL), container restarts, deployment updates
4. **Storage Issues**: Disk full errors, I/O failures (though individual writes would error)

The vulnerability requires no attacker action - it's a reliability bug that naturally occurs under normal operational stress. Any node performing state sync from snapshot is vulnerable.

**Exploitation Requirements:**
- No privileged access required
- No malicious input needed
- Occurs naturally during operational disruptions
- Affects all nodes performing state snapshot restore

## Recommendation

Implement atomic multi-shard commit using one of the following approaches:

**Option 1: Two-Phase Commit with Write-Ahead Log**
```rust
pub(crate) fn commit_no_progress_atomic(
    &self,
    top_level_batch: SchemaBatch,
    batches_for_shards: Vec<SchemaBatch>,
) -> Result<()> {
    // Phase 1: Write intent log
    let commit_id = generate_unique_id();
    self.write_commit_intent(commit_id, &top_level_batch, &batches_for_shards)?;
    
    // Phase 2: Write to all shards
    for shard_id in 0..NUM_STATE_SHARDS {
        self.state_merkle_db_shards[shard_id]
            .write_schemas(batches_for_shards[shard_id].clone())?;
    }
    self.state_merkle_metadata_db.write_schemas(top_level_batch)?;
    
    // Phase 3: Mark commit as complete
    self.mark_commit_complete(commit_id)?;
    
    Ok(())
}
```

**Option 2: Pre-Commit Validation with Rollback**
```rust
pub(crate) fn commit_no_progress_validated(
    &self,
    top_level_batch: SchemaBatch,
    batches_for_shards: Vec<SchemaBatch>,
) -> Result<()> {
    // Create checkpoint before commit
    let checkpoint_id = self.create_checkpoint()?;
    
    // Attempt commit
    let result = self.commit_all_shards(top_level_batch, batches_for_shards);
    
    match result {
        Ok(()) => {
            self.remove_checkpoint(checkpoint_id)?;
            Ok(())
        },
        Err(e) => {
            // Rollback to checkpoint on any failure
            self.rollback_to_checkpoint(checkpoint_id)?;
            Err(e)
        }
    }
}
```

**Option 3: Single-Database Commit (Preferred)**
```rust
// Store all nodes in a single transaction batch
pub(crate) fn commit_no_progress_single_transaction(
    &self,
    top_level_batch: SchemaBatch,
    batches_for_shards: Vec<SchemaBatch>,
) -> Result<()> {
    // Merge all batches into a single atomic write
    let mut unified_batch = SchemaBatch::new();
    
    for (shard_id, shard_batch) in batches_for_shards.into_iter().enumerate() {
        unified_batch.merge(shard_batch)?;
    }
    unified_batch.merge(top_level_batch)?;
    
    // Single atomic write
    self.unified_db.write_schemas(unified_batch)
}
```

## Proof of Concept

```rust
/// Reproduction scenario for partial commit vulnerability
/// 
/// Setup:
/// 1. Create StateMerkleDb with sharding enabled
/// 2. Prepare a node batch spanning multiple shards (e.g., nodes with first nibbles 0-15)
/// 3. Simulate crash during write_node_batch by killing process after N shards written
///
/// Expected Result:
/// - Shards 0 to N-1 contain the committed nodes (durable due to sync=true)
/// - Shards N to 15 are missing the nodes
/// - Merkle tree is in inconsistent state
/// - Recovery via recover_partial_nodes produces incorrect tree
///
/// Verification:
/// 1. Restart the process and attempt to continue restore
/// 2. Observe that recover_partial_nodes reconstructs state with missing nodes
/// 3. Compute root hash - it will differ from expected_root_hash
/// 4. Attempt to verify proofs - they will fail due to missing children

#[cfg(test)]
mod vulnerability_poc {
    use super::*;
    use std::sync::atomic::{AtomicUsize, Ordering};
    use std::sync::Arc;
    
    #[test]
    fn test_partial_commit_corruption() {
        // Setup: Create test database with sharding
        let db = setup_test_state_merkle_db_with_sharding();
        
        // Create a batch of nodes distributed across all 16 shards
        let node_batch = create_test_nodes_across_shards();
        
        // Simulate crash after writing only 8 shards
        let crash_after_shard = Arc::new(AtomicUsize::new(8));
        
        // Inject crash behavior into write path
        let result = std::panic::catch_unwind(|| {
            db.write_node_batch_with_crash_injection(
                &node_batch,
                crash_after_shard.clone()
            )
        });
        
        assert!(result.is_err(), "Expected simulated crash");
        
        // Verify partial state: shards 0-7 have nodes, shards 8-15 don't
        for shard_id in 0..8 {
            assert!(shard_has_nodes(&db, shard_id), 
                "Shard {} should have committed nodes", shard_id);
        }
        for shard_id in 8..16 {
            assert!(!shard_has_nodes(&db, shard_id),
                "Shard {} should NOT have nodes due to crash", shard_id);
        }
        
        // Attempt recovery
        let restore = JellyfishMerkleRestore::new(
            Arc::new(db),
            version,
            expected_root_hash,
            false
        ).expect("Recovery should succeed");
        
        // Compute root hash with corrupted tree
        let computed_root = restore.finish_and_get_root_hash().unwrap();
        
        // This will fail: computed root differs from expected due to missing nodes
        assert_ne!(computed_root, expected_root_hash,
            "VULNERABILITY: Corrupted tree produces wrong root hash");
    }
}
```

**Notes:**

The vulnerability lies in the architectural decision to use 17 separate RocksDB instances without cross-database transaction support. While individual writes are atomic and durable, the sequence of writes across databases is not atomic. This is a fundamental limitation of the current sharding approach that cannot be fixed with the `?` operator alone - process crashes bypass error handling entirely.

### Citations

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L265-334)
```rust
    /// Recovers partial nodes from storage. We do this by looking at all the ancestors of the
    /// rightmost leaf. The ones do not exist in storage are the partial nodes.
    fn recover_partial_nodes(
        store: &dyn TreeReader<K>,
        version: Version,
        rightmost_leaf_node_key: NodeKey,
    ) -> Result<Vec<InternalInfo<K>>> {
        ensure!(
            !rightmost_leaf_node_key.nibble_path().is_empty(),
            "Root node would not be written until entire restoration process has completed \
             successfully.",
        );

        // Start from the parent of the rightmost leaf. If this internal node exists in storage, it
        // is not a partial node. Go to the parent node and repeat until we see a node that does
        // not exist. This node and all its ancestors will be the partial nodes.
        let mut node_key = rightmost_leaf_node_key.gen_parent_node_key();
        while store.get_node_option(&node_key, "restore")?.is_some() {
            node_key = node_key.gen_parent_node_key();
        }

        // Next we reconstruct all the partial nodes up to the root node, starting from the bottom.
        // For all of them, we scan all its possible child positions and see if there is one at
        // each position. If the node is not the bottom one, there is additionally a partial node
        // child at the position `previous_child_index`.
        let mut partial_nodes = vec![];
        // Initialize `previous_child_index` to `None` for the first iteration of the loop so the
        // code below treats it differently.
        let mut previous_child_index = None;

        loop {
            let mut internal_info = InternalInfo::new_empty(node_key.clone());

            for i in 0..previous_child_index.unwrap_or(16) {
                let child_node_key = node_key.gen_child_node_key(version, (i as u8).into());
                if let Some(node) = store.get_node_option(&child_node_key, "restore")? {
                    let child_info = match node {
                        Node::Internal(internal_node) => ChildInfo::Internal {
                            hash: Some(internal_node.hash()),
                            leaf_count: Some(internal_node.leaf_count()),
                        },
                        Node::Leaf(leaf_node) => ChildInfo::Leaf(leaf_node),
                        Node::Null => unreachable!("Child cannot be Null"),
                    };
                    internal_info.set_child(i, child_info);
                }
            }

            // If this is not the lowest partial node, it will have a partial node child at
            // `previous_child_index`. Set the hash of this child to `None` because it is a
            // partial node and we do not know its hash yet. For the lowest partial node, we just
            // find all its known children from storage in the loop above.
            if let Some(index) = previous_child_index {
                internal_info.set_child(index, ChildInfo::Internal {
                    hash: None,
                    leaf_count: None,
                });
            }

            partial_nodes.push(internal_info);
            if node_key.nibble_path().is_empty() {
                break;
            }
            previous_child_index = node_key.nibble_path().last().map(|x| u8::from(x) as usize);
            node_key = node_key.gen_parent_node_key();
        }

        partial_nodes.reverse();
        Ok(partial_nodes)
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L393-410)
```rust
        // Write the frozen nodes to storage.
        if self.async_commit {
            self.wait_for_async_commit()?;
            let (tx, rx) = channel();
            self.async_commit_result = Some(rx);

            let mut frozen_nodes = HashMap::new();
            std::mem::swap(&mut frozen_nodes, &mut self.frozen_nodes);
            let store = self.store.clone();

            IO_POOL.spawn(move || {
                let res = store.write_node_batch(&frozen_nodes);
                tx.send(res).unwrap();
            });
        } else {
            self.store.write_node_batch(&self.frozen_nodes)?;
            self.frozen_nodes.clear();
        }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L741-746)
```rust
    pub fn wait_for_async_commit(&mut self) -> Result<()> {
        if let Some(rx) = self.async_commit_result.take() {
            rx.recv()??;
        }
        Ok(())
    }
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L173-190)
```rust
    /// Only used by fast sync / restore.
    pub(crate) fn commit_no_progress(
        &self,
        top_level_batch: SchemaBatch,
        batches_for_shards: Vec<SchemaBatch>,
    ) -> Result<()> {
        ensure!(
            batches_for_shards.len() == NUM_STATE_SHARDS,
            "Shard count mismatch."
        );
        let mut batches = batches_for_shards.into_iter();
        for shard_id in 0..NUM_STATE_SHARDS {
            let state_merkle_batch = batches.next().unwrap();
            self.state_merkle_db_shards[shard_id].write_schemas(state_merkle_batch)?;
        }

        self.state_merkle_metadata_db.write_schemas(top_level_batch)
    }
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L920-932)
```rust
        // Get the top level batch and sharded batch from raw NodeBatch
        let mut top_level_batch = SchemaBatch::new();
        let mut jmt_shard_batches: Vec<SchemaBatch> = Vec::with_capacity(NUM_STATE_SHARDS);
        jmt_shard_batches.resize_with(NUM_STATE_SHARDS, SchemaBatch::new);
        node_batch.iter().try_for_each(|(node_key, node)| {
            if let Some(shard_id) = node_key.get_shard_id() {
                jmt_shard_batches[shard_id].put::<JellyfishMerkleNodeSchema>(node_key, node)
            } else {
                top_level_batch.put::<JellyfishMerkleNodeSchema>(node_key, node)
            }
        })?;
        self.commit_no_progress(top_level_batch, jmt_shard_batches)
    }
```

**File:** types/src/nibble/nibble_path/mod.rs (L222-229)
```rust
    // Returns the shard_id of the NibblePath, or None if it is root.
    pub fn get_shard_id(&self) -> Option<usize> {
        if self.num_nibbles() > 0 {
            Some(usize::from(self.get_nibble(0)))
        } else {
            None
        }
    }
```

**File:** storage/schemadb/src/lib.rs (L289-309)
```rust
    fn write_schemas_inner(&self, batch: impl IntoRawBatch, option: &WriteOptions) -> DbResult<()> {
        let labels = [self.name.as_str()];
        let _timer = APTOS_SCHEMADB_BATCH_COMMIT_LATENCY_SECONDS.timer_with(&labels);

        let raw_batch = batch.into_raw_batch(self)?;

        let serialized_size = raw_batch.inner.size_in_bytes();
        self.inner
            .write_opt(raw_batch.inner, option)
            .into_db_res()?;

        raw_batch.stats.commit();
        APTOS_SCHEMADB_BATCH_COMMIT_BYTES.observe_with(&[&self.name], serialized_size as f64);

        Ok(())
    }

    /// Writes a group of records wrapped in a [`SchemaBatch`].
    pub fn write_schemas(&self, batch: impl IntoRawBatch) -> DbResult<()> {
        self.write_schemas_inner(batch, &sync_write_option())
    }
```

**File:** storage/schemadb/src/lib.rs (L371-378)
```rust
/// For now we always use synchronous writes. This makes sure that once the operation returns
/// `Ok(())` the data is persisted even if the machine crashes. In the future we might consider
/// selectively turning this off for some non-critical writes to improve performance.
fn sync_write_option() -> rocksdb::WriteOptions {
    let mut opts = rocksdb::WriteOptions::default();
    opts.set_sync(true);
    opts
}
```
