# Audit Report

## Title
Epoch Rollback Vulnerability in Table Info Backup Metadata Due to TOCTOU Race Condition and Missing Epoch Validation

## Summary
The `BackupRestoreMetadata` struct and associated backup functions in the indexer-grpc-table-info component fail to prevent epoch rollback attacks. The `update_metadata` function lacks epoch validation, and `backup_the_snapshot_and_cleanup` contains a Time-of-Check Time-of-Use (TOCTOU) race condition. This allows concurrent backup operations from multiple fullnodes to overwrite newer epoch metadata with older values, causing state inconsistencies in the table info indexer database. [1](#0-0) 

## Finding Description

The vulnerability exists in two locations:

**1. Missing Epoch Validation in `update_metadata`:**

The `update_metadata` function unconditionally overwrites the GCS metadata file without verifying that the new epoch is greater than or equal to the current epoch. [2](#0-1) 

**2. TOCTOU Race Condition in `backup_the_snapshot_and_cleanup`:**

The backup flow performs a non-atomic check-then-update sequence:
- Reads metadata at line 544
- Checks if `metadata.epoch >= epoch` at line 576
- Much later, calls `backup_db_snapshot_and_update_metadata` at line 601, which updates metadata at line 240 [3](#0-2) [4](#0-3) 

**Attack Scenario:**

When multiple fullnodes share the same GCS bucket for backups (a documented configuration), the following race occurs:

1. Initial state: `metadata.epoch = 50`
2. Node A (slow, at epoch 60): Reads metadata, sees epoch 50, check passes (50 < 60)
3. Node B (fast, at epoch 100): Reads metadata, sees epoch 50, check passes (50 < 100)
4. Node B completes upload and calls `update_metadata(100)` → `metadata.epoch = 100`
5. Node A completes upload and calls `update_metadata(60)` → **`metadata.epoch = 60`** (rollback!)
6. Final state: Metadata points to epoch 60, but epoch 100 snapshot exists in GCS

**State Consistency Violation:**

The restore function uses the metadata epoch to determine which snapshot to download: [5](#0-4) 

After the rollback, nodes attempting to restore will download epoch 60 instead of epoch 100, missing 40 epochs of table info data and causing API/indexer state inconsistencies.

## Impact Explanation

This vulnerability qualifies as **HIGH severity** under the Aptos bug bounty program criteria:

- **"Significant protocol violations"**: The backup/restore protocol's fundamental assumption that metadata always points to the latest epoch is violated
- **"State inconsistencies requiring intervention"**: Incorrect metadata causes nodes to restore to outdated states, requiring manual intervention to identify and fix

**Affected Components:**
- Table info indexer database integrity
- API correctness (stale table metadata served to clients)
- Node recovery procedures (incorrect restore points)

**Note**: While this affects the indexer layer rather than consensus, the table info database is critical for API functionality and understanding Move table structures. Incorrect table type information could cause transaction interpretation errors in downstream systems.

## Likelihood Explanation

**Likelihood: MEDIUM to HIGH**

**Triggering Conditions:**
1. Multiple fullnodes configured to backup to the same GCS bucket (documented in README)
2. Nodes processing epochs at different speeds (common due to varying hardware/network)
3. Concurrent backup operations with overlapping time windows

**TOCTOU Window:**
- The race window spans from metadata read (line 544) to metadata write (line 601)
- This window includes snapshot compression and GCS upload, which can take seconds to minutes
- Large databases increase the window, making the race more likely

**Realistic Scenarios:**
- Organizations running multiple backup nodes for redundancy
- Node restarts causing one node to be significantly behind
- Network partitions or hardware issues causing processing delays

## Recommendation

Implement atomic epoch validation using GCS conditional write preconditions:

```rust
pub async fn update_metadata(&self, chain_id: u64, epoch: u64) -> anyhow::Result<()> {
    // First, get current metadata to obtain generation number
    let current_metadata = self.get_metadata().await;
    
    // Validate epoch is not rolling back
    if let Some(current) = current_metadata {
        if epoch < current.epoch {
            anyhow::bail!(
                "Epoch rollback prevented: attempting to update to epoch {} when current is {}",
                epoch,
                current.epoch
            );
        }
    }
    
    let metadata = BackupRestoreMetadata::new(chain_id, epoch);
    
    // Use if_generation_match precondition for atomic compare-and-swap
    // This ensures no concurrent update has modified the metadata
    loop {
        match self.gcs_client.upload_object(
            &UploadObjectRequest {
                bucket: self.bucket_name.clone(),
                // Add: if_generation_match or if_metageneration_match
                // to ensure atomic update based on last known version
                ..Default::default()
            },
            serde_json::to_vec(&metadata).unwrap(),
            &UploadType::Simple(Media {
                name: Borrowed(METADATA_FILE_NAME),
                content_type: Borrowed(JSON_FILE_TYPE),
                content_length: None,
            }),
        ).await {
            Ok(_) => {
                aptos_logger::info!(
                    "[Table Info] Successfully updated metadata to epoch {}",
                    epoch
                );
                return Ok(());
            },
            Err(Error::Response(err)) if err.code == 412 => {
                // Precondition failed - metadata was updated concurrently
                // Re-check epoch and retry if still valid
                let latest = self.get_metadata().await;
                if let Some(latest_meta) = latest {
                    if epoch <= latest_meta.epoch {
                        aptos_logger::info!(
                            "Skipping metadata update: epoch {} already at {}",
                            epoch, latest_meta.epoch
                        );
                        return Ok(());
                    }
                }
                continue;
            },
            Err(Error::Response(err)) if (err.is_retriable() && err.code == 429) => {
                tokio::time::sleep(Duration::from_millis(500)).await;
                continue;
            },
            Err(err) => {
                anyhow::bail!("Failed to update metadata: {}", err);
            },
        }
    }
}
```

**Alternative**: Add distributed locking using GCS object creation as a lock primitive, or use a separate coordination service (etcd, ZooKeeper) to serialize backup operations.

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::Arc;
    use tokio::sync::Barrier;

    #[tokio::test]
    async fn test_epoch_rollback_race_condition() {
        // Setup: Create GCS operator with test bucket
        let bucket_name = "test-epoch-rollback-bucket".to_string();
        let operator = Arc::new(GcsBackupRestoreOperator::new(bucket_name).await);
        
        // Initialize metadata at epoch 50
        operator.update_metadata(1, 50).await.unwrap();
        
        // Simulate two nodes backing up concurrently
        let operator_a = Arc::clone(&operator);
        let operator_b = Arc::clone(&operator);
        let barrier = Arc::new(Barrier::new(2));
        
        let handle_a = tokio::spawn({
            let barrier = Arc::clone(&barrier);
            async move {
                // Node A: slow node at epoch 60
                let metadata = operator_a.get_metadata().await.unwrap();
                assert_eq!(metadata.epoch, 50);
                
                // Wait for both to read metadata
                barrier.wait().await;
                
                // Simulate slow upload (Node B will finish first)
                tokio::time::sleep(Duration::from_secs(2)).await;
                
                // Update to epoch 60
                operator_a.update_metadata(1, 60).await.unwrap();
            }
        });
        
        let handle_b = tokio::spawn({
            let barrier = Arc::clone(&barrier);
            async move {
                // Node B: fast node at epoch 100
                let metadata = operator_b.get_metadata().await.unwrap();
                assert_eq!(metadata.epoch, 50);
                
                // Wait for both to read metadata
                barrier.wait().await;
                
                // Fast upload completes first
                tokio::time::sleep(Duration::from_millis(100)).await;
                
                // Update to epoch 100
                operator_b.update_metadata(1, 100).await.unwrap();
            }
        });
        
        // Wait for both operations
        handle_a.await.unwrap();
        handle_b.await.unwrap();
        
        // Verify the vulnerability: metadata should show epoch 60 (rollback!)
        // even though epoch 100 was written first
        let final_metadata = operator.get_metadata().await.unwrap();
        
        // This assertion demonstrates the vulnerability:
        // Expected: epoch 100 (latest)
        // Actual: epoch 60 (rolled back)
        assert_eq!(final_metadata.epoch, 60, 
            "Vulnerability demonstrated: epoch rolled back from 100 to 60");
        
        println!("VULNERABILITY CONFIRMED: Metadata rolled back to epoch {} despite epoch 100 being uploaded", 
            final_metadata.epoch);
    }
}
```

## Notes

This vulnerability is specific to the table info indexer backup system, not the main Aptos blockchain consensus or state database. However, table info is critical for API functionality and correct interpretation of Move table structures. An epoch rollback could cause downstream systems to misinterpret table data or serve stale information to clients.

The issue is exacerbated by the fact that the restore functionality (`restore_db_snapshot`) relies entirely on the metadata epoch value to determine which snapshot to restore, with no validation against available snapshots or blockchain state.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/mod.rs (L22-32)
```rust
#[derive(Serialize, Deserialize, Copy, Clone, Debug)]
pub struct BackupRestoreMetadata {
    pub chain_id: u64,
    pub epoch: u64,
}

impl BackupRestoreMetadata {
    pub fn new(chain_id: u64, epoch: u64) -> Self {
        Self { chain_id, epoch }
    }
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L124-162)
```rust
    pub async fn update_metadata(&self, chain_id: u64, epoch: u64) -> anyhow::Result<()> {
        let metadata = BackupRestoreMetadata::new(chain_id, epoch);
        loop {
            match self
                .gcs_client
                .upload_object(
                    &UploadObjectRequest {
                        bucket: self.bucket_name.clone(),
                        ..Default::default()
                    },
                    serde_json::to_vec(&metadata).unwrap(),
                    &UploadType::Simple(Media {
                        name: Borrowed(METADATA_FILE_NAME),
                        content_type: Borrowed(JSON_FILE_TYPE),
                        content_length: None,
                    }),
                )
                .await
            {
                Ok(_) => {
                    aptos_logger::info!(
                        "[Table Info] Successfully updated metadata to GCS bucket: {}",
                        METADATA_FILE_NAME
                    );
                    return Ok(());
                },
                // https://cloud.google.com/storage/quotas
                // add retry logic due to: "Maximum rate of writes to the same object name: One write per second"
                Err(Error::Response(err)) if (err.is_retriable() && err.code == 429) => {
                    info!("Retried with rateLimitExceeded on gcs single object at epoch {} when updating the metadata", epoch);
                    tokio::time::sleep(Duration::from_millis(500)).await;
                    continue;
                },
                Err(err) => {
                    anyhow::bail!("Failed to update metadata: {}", err);
                },
            }
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L166-260)
```rust
    pub async fn backup_db_snapshot_and_update_metadata(
        &self,
        chain_id: u64,
        epoch: u64,
        snapshot_path: PathBuf,
    ) -> anyhow::Result<()> {
        // chain id + epoch is the unique identifier for the snapshot.
        let snapshot_tar_file_name = format!("chain_id_{}_epoch_{}", chain_id, epoch);
        let snapshot_path_closure = snapshot_path.clone();
        aptos_logger::info!(
            snapshot_tar_file_name = snapshot_tar_file_name.as_str(),
            "[Table Info] Starting to compress the folder.",
        );
        // If target path does not exist, wait and log.
        if !snapshot_path.exists() {
            aptos_logger::warn!(
                snapshot_path = snapshot_path.to_str(),
                snapshot_tar_file_name = snapshot_tar_file_name.as_str(),
                epoch = epoch,
                "[Table Info] Directory does not exist. Waiting for the directory to be created."
            );
            tokio::time::sleep(std::time::Duration::from_secs(30)).await;
            return Ok(());
        }
        let tar_file = task::spawn_blocking(move || {
            aptos_logger::info!(
                snapshot_tar_file_name = snapshot_tar_file_name.as_str(),
                "[Table Info] Compressing the folder."
            );
            let result = create_tar_gz(snapshot_path_closure.clone(), &snapshot_tar_file_name);
            aptos_logger::info!(
                snapshot_tar_file_name = snapshot_tar_file_name.as_str(),
                result = result.is_ok(),
                "[Table Info] Compressed the folder."
            );
            result
        })
        .await
        .context("Failed to spawn task to create snapshot backup file.")?
        .context("Failed to create tar.gz file in blocking task")?;
        aptos_logger::info!(
            "[Table Info] Created snapshot tar file: {:?}",
            tar_file.file_name().unwrap()
        );

        // Open the file in async mode to stream it
        let file = File::open(&tar_file)
            .await
            .context("Failed to open gzipped tar file for reading")?;
        let file_stream = tokio_util::io::ReaderStream::new(file);

        let filename = generate_blob_name(chain_id, epoch);

        aptos_logger::info!(
            "[Table Info] Uploading snapshot to GCS bucket: {}",
            filename
        );
        match self
            .gcs_client
            .upload_streamed_object(
                &UploadObjectRequest {
                    bucket: self.bucket_name.clone(),
                    ..Default::default()
                },
                file_stream,
                &UploadType::Simple(Media {
                    name: filename.clone().into(),
                    content_type: Borrowed(TAR_FILE_TYPE),
                    content_length: None,
                }),
            )
            .await
        {
            Ok(_) => {
                self.update_metadata(chain_id, epoch).await?;
                let snapshot_path_clone = snapshot_path.clone();
                fs::remove_file(&tar_file)
                    .and_then(|_| fs::remove_dir_all(snapshot_path_clone))
                    .await
                    .expect("Failed to clean up after db snapshot upload");
                aptos_logger::info!(
                    "[Table Info] Successfully uploaded snapshot to GCS bucket: {}",
                    filename
                );
            },
            Err(err) => {
                error!("Failed to upload snapshot: {}", err);
                // TODO: better error handling, i.e., permanent failure vs transient failure.
                // For example, permission issue vs rate limit issue.
                anyhow::bail!("Failed to upload snapshot: {}", err);
            },
        };

        Ok(())
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L264-274)
```rust
    pub async fn restore_db_snapshot(
        &self,
        chain_id: u64,
        metadata: BackupRestoreMetadata,
        db_path: PathBuf,
        base_path: PathBuf,
    ) -> anyhow::Result<()> {
        assert!(metadata.chain_id == chain_id, "Chain ID mismatch.");

        let epoch = metadata.epoch;
        let epoch_based_filename = generate_blob_name(chain_id, epoch);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L544-603)
```rust
    let backup_metadata = backup_restore_operator.get_metadata().await;
    if let Some(metadata) = backup_metadata {
        if metadata.chain_id != (ledger_chain_id as u64) {
            panic!(
                "Table Info backup chain id does not match with current network. Expected: {}, found in backup: {}",
                context.chain_id().id(),
                metadata.chain_id
            );
        }
    } else {
        aptos_logger::warn!(
            epoch = epoch,
            snapshot_folder_name = snapshot_folder_name,
            "[Table Info] No backup metadata found. Skipping the backup."
        );
    }

    let start_time = std::time::Instant::now();
    // temporary path to store the snapshot
    let snapshot_dir = context
        .node_config
        .get_data_dir()
        .join(snapshot_folder_name.clone());
    // If the backup is for old epoch, clean up and return.
    if let Some(metadata) = backup_metadata {
        aptos_logger::info!(
            epoch = epoch,
            metadata_epoch = metadata.epoch,
            snapshot_folder_name = snapshot_folder_name,
            snapshot_dir = snapshot_dir.to_str(),
            "[Table Info] Checking the metadata before backup."
        );
        if metadata.epoch >= epoch {
            aptos_logger::info!(
                epoch = epoch,
                snapshot_folder_name = snapshot_folder_name,
                "[Table Info] Snapshot already backed up. Skipping the backup."
            );
            // Remove the snapshot directory.
            std::fs::remove_dir_all(snapshot_dir).unwrap();
            return;
        }
    } else {
        aptos_logger::warn!(
            epoch = epoch,
            snapshot_folder_name = snapshot_folder_name,
            "[Table Info] No backup metadata found."
        );
    }
    aptos_logger::info!(
        epoch = epoch,
        snapshot_folder_name = snapshot_folder_name,
        snapshot_dir = snapshot_dir.to_str(),
        "[Table Info] Backing up the snapshot."
    );
    // TODO: add checks to handle concurrent backup jobs.
    backup_restore_operator
        .backup_db_snapshot_and_update_metadata(ledger_chain_id as u64, epoch, snapshot_dir.clone())
        .await
        .expect("Failed to upload snapshot in table info service");
```
