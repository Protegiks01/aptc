# Audit Report

## Title
Mempool Async Runtime Thread Exhaustion via Blocking IO_POOL Operations Under High Transaction Load

## Summary
The mempool's `IO_POOL` thread pool is invoked via blocking `install()` calls from async contexts without proper `spawn_blocking()` protection, leading to tokio runtime thread starvation under high transaction load. This causes validator node slowdowns and degraded consensus participation.

## Finding Description

The mempool implements transaction processing using a combination of tokio async runtime and rayon thread pools. The `IO_POOL` is defined as a static rayon ThreadPool with default configuration: [1](#0-0) 

This pool is used in `process_incoming_transactions()` to parallelize database reads for fetching account sequence numbers: [2](#0-1) 

The critical issue is that `process_incoming_transactions()` is called from async functions that run on the tokio runtime: [3](#0-2) [4](#0-3) 

These async functions are spawned via `BoundedExecutor`, which uses `tokio::spawn()` (NOT `spawn_blocking()`): [5](#0-4) [6](#0-5) 

The `BoundedExecutor` implementation confirms it uses regular `tokio::spawn()`: [7](#0-6) 

**Attack Vector:**
1. Attacker floods the mempool by broadcasting large batches of transactions (e.g., 1000+ TPS)
2. Each batch triggers an async task via `BoundedExecutor` (limited to 4-16 concurrent tasks)
3. Each task calls `process_incoming_transactions()` which blocks the tokio thread via `IO_POOL.install()`
4. If all BoundedExecutor slots are filled with tasks blocking on IO_POOL work, and IO_POOL threads are saturated with previous batches, the tokio runtime becomes starved
5. The mempool runtime has no explicit thread count configuration (defaults to `num_cpus`): [8](#0-7) 

The default concurrent inbound sync limit is only 4 workers: [9](#0-8) 

This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits" - the blocking operation exhausts tokio runtime threads without proper resource management.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos Bug Bounty program criteria:

**"Validator node slowdowns"** - Under sustained high transaction load, validators experience:
- Tokio runtime thread starvation blocking network event processing
- Delayed consensus message handling
- Reduced block production capacity
- Potential missed consensus rounds leading to validator performance penalties

The impact affects all validator nodes during high-load scenarios such as:
- Network-wide transaction floods
- Malicious peer broadcasting attack
- Legitimate traffic spikes during high-demand periods

## Likelihood Explanation

**Likelihood: Medium-High**

**Attacker Requirements:**
- Network peer access (any node can broadcast transactions)
- Ability to generate transaction batches (no special privileges required)
- Sustained load generation (achievable with basic scripting)

**Triggering Conditions:**
- Mempool receives 500+ TPS (easily achievable for a motivated attacker)
- BoundedExecutor slots fill with blocking tasks (4-16 concurrent workers)
- IO_POOL threads saturated with parallel DB reads
- No spawn_blocking protection exists

**Feasibility:**
The attack requires no validator compromise, no consensus manipulation, and minimal resources. Any network participant can trigger this by broadcasting high volumes of valid-looking transactions. The lack of stress tests for this scenario indicates it has not been validated under production load conditions.

## Recommendation

**Immediate Fix:**
Wrap the `IO_POOL.install()` call in `tokio::task::spawn_blocking()` to prevent blocking the async runtime:

```rust
// In process_incoming_transactions()
let account_seq_numbers = tokio::task::spawn_blocking(move || {
    IO_POOL.install(|| {
        transactions
            .par_iter()
            .map(|(t, _, _)| match t.replay_protector() {
                ReplayProtector::Nonce(_) => Ok(None),
                ReplayProtector::SequenceNumber(_) => {
                    get_account_sequence_number(&state_view, t.sender())
                        .map(Some)
                        .inspect_err(|e| {
                            error!(LogSchema::new(LogEntry::DBError).error(e));
                            counters::DB_ERROR.inc();
                        })
                },
            })
            .collect::<Vec<_>>()
    })
})
.await
.expect("IO_POOL task panicked");
```

**Long-term Improvements:**
1. Add explicit thread count configuration for IO_POOL (currently uses default)
2. Implement backpressure mechanisms for transaction ingestion under load
3. Add comprehensive stress tests verifying behavior at 5000+ TPS
4. Monitor tokio runtime thread saturation metrics
5. Consider separate dedicated runtime for CPU-bound mempool operations

## Proof of Concept

```rust
#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_io_pool_saturation_causes_runtime_starvation() {
    use std::sync::atomic::{AtomicU64, Ordering};
    use std::sync::Arc;
    use std::time::{Duration, Instant};
    
    // Setup mempool with minimal concurrent workers
    let mut config = MempoolConfig::default();
    config.shared_mempool_max_concurrent_inbound_syncs = 4;
    
    let smp = MockSharedMempool::new_with_config(config);
    let runtime_blocked = Arc::new(AtomicU64::new(0));
    let blocked_clone = runtime_blocked.clone();
    
    // Spawn background task to detect runtime starvation
    tokio::spawn(async move {
        let mut interval = tokio::time::interval(Duration::from_millis(100));
        loop {
            let start = Instant::now();
            interval.tick().await;
            let elapsed = start.elapsed();
            // If tokio is blocked, interval ticks will be delayed
            if elapsed > Duration::from_millis(500) {
                blocked_clone.fetch_add(1, Ordering::Relaxed);
            }
        }
    });
    
    // Flood mempool with 1000 transaction batches
    for batch in 0..1000 {
        let mut txns = Vec::new();
        for i in 0..100 {
            txns.push(TestTransaction::new(
                batch * 100 + i,
                ReplayProtector::SequenceNumber(0),
                1
            ).make_signed_transaction());
        }
        
        // Submit batch (will trigger IO_POOL.install() blocking)
        for txn in txns {
            let _ = smp.add_txn(txn).await;
        }
    }
    
    tokio::time::sleep(Duration::from_secs(2)).await;
    
    // If runtime was starved, background task detected delays
    let blocked_count = runtime_blocked.load(Ordering::Relaxed);
    assert!(
        blocked_count > 5,
        "Expected runtime starvation under load, but detected {} delays",
        blocked_count
    );
}
```

## Notes

The vulnerability is exacerbated by the default `shared_mempool_max_concurrent_inbound_syncs = 4` configuration, which creates only 4 concurrent worker slots. While VFNs increase this to 16, both configurations are vulnerable to thread pool exhaustion under sustained load. The absence of stress tests for IO_POOL saturation scenarios allowed this blocking-in-async anti-pattern to remain undetected in production code.

### Citations

**File:** mempool/src/thread_pool.rs (L8-13)
```rust
pub(crate) static IO_POOL: Lazy<rayon::ThreadPool> = Lazy::new(|| {
    rayon::ThreadPoolBuilder::new()
        .thread_name(|index| format!("mempool_io_{}", index))
        .build()
        .unwrap()
});
```

**File:** mempool/src/shared_mempool/tasks.rs (L129-153)
```rust
pub(crate) async fn process_client_transaction_submission<NetworkClient, TransactionValidator>(
    smp: SharedMempool<NetworkClient, TransactionValidator>,
    transaction: SignedTransaction,
    callback: oneshot::Sender<Result<SubmissionStatus>>,
    timer: HistogramTimer,
) where
    NetworkClient: NetworkClientInterface<MempoolSyncMsg>,
    TransactionValidator: TransactionValidation + 'static,
{
    timer.stop_and_record();
    let _timer = counters::process_txn_submit_latency_timer_client();
    let ineligible_for_broadcast =
        smp.network_interface.is_validator() && !smp.broadcast_within_validator_network();
    let timeline_state = if ineligible_for_broadcast {
        TimelineState::NonQualified
    } else {
        TimelineState::NotReady
    };
    let statuses: Vec<(SignedTransaction, (MempoolStatus, Option<StatusCode>))> =
        process_incoming_transactions(
            &smp,
            vec![(transaction, None, Some(BroadcastPeerPriority::Primary))],
            timeline_state,
            true,
        );
```

**File:** mempool/src/shared_mempool/tasks.rs (L210-230)
```rust
pub(crate) async fn process_transaction_broadcast<NetworkClient, TransactionValidator>(
    smp: SharedMempool<NetworkClient, TransactionValidator>,
    // The sender of the transactions can send the time at which the transactions were inserted
    // in the sender's mempool. The sender can also send the priority of this node for the sender
    // of the transactions.
    transactions: Vec<(
        SignedTransaction,
        Option<u64>,
        Option<BroadcastPeerPriority>,
    )>,
    message_id: MempoolMessageId,
    timeline_state: TimelineState,
    peer: PeerNetworkId,
    timer: HistogramTimer,
) where
    NetworkClient: NetworkClientInterface<MempoolSyncMsg>,
    TransactionValidator: TransactionValidation,
{
    timer.stop_and_record();
    let _timer = counters::process_txn_submit_latency_timer(peer.network_id());
    let results = process_incoming_transactions(&smp, transactions, timeline_state, false);
```

**File:** mempool/src/shared_mempool/tasks.rs (L335-350)
```rust
    let account_seq_numbers = IO_POOL.install(|| {
        transactions
            .par_iter()
            .map(|(t, _, _)| match t.replay_protector() {
                ReplayProtector::Nonce(_) => Ok(None),
                ReplayProtector::SequenceNumber(_) => {
                    get_account_sequence_number(&state_view, t.sender())
                        .map(Some)
                        .inspect_err(|e| {
                            error!(LogSchema::new(LogEntry::DBError).error(e));
                            counters::DB_ERROR.inc();
                        })
                },
            })
            .collect::<Vec<_>>()
    });
```

**File:** mempool/src/shared_mempool/coordinator.rs (L189-196)
```rust
            bounded_executor
                .spawn(tasks::process_client_transaction_submission(
                    smp.clone(),
                    txn,
                    callback,
                    task_start_timer,
                ))
                .await;
```

**File:** mempool/src/shared_mempool/coordinator.rs (L332-341)
```rust
    bounded_executor
        .spawn(tasks::process_transaction_broadcast(
            smp_clone,
            transactions,
            message_id,
            timeline_state,
            peer,
            task_start_timer,
        ))
        .await;
```

**File:** crates/bounded-executor/src/executor.rs (L45-52)
```rust
    pub async fn spawn<F>(&self, future: F) -> JoinHandle<F::Output>
    where
        F: Future + Send + 'static,
        F::Output: Send + 'static,
    {
        let permit = self.acquire_permit().await;
        self.executor.spawn(future_with_permit(future, permit))
    }
```

**File:** mempool/src/shared_mempool/runtime.rs (L102-102)
```rust
    let runtime = aptos_runtimes::spawn_named_runtime("shared-mem".into(), None);
```

**File:** config/src/config/mempool_config.rs (L116-116)
```rust
            shared_mempool_max_concurrent_inbound_syncs: 4,
```
