# Audit Report

## Title
Consensus Observer Pending Block Loss Due to Incorrect Split-Based Removal Logic

## Summary
The `remove_ready_block()` function in the consensus observer's pending block store uses a BTreeMap split operation based on received payload epoch/round parameters. When payloads arrive out-of-order or with rounds that don't correspond to any pending block's first round, valid pending blocks can be incorrectly dropped as "out-of-date", causing permanent loss of blocks and consensus observer desynchronization. [1](#0-0) 

## Finding Description

The consensus observer maintains pending blocks (ordered blocks waiting for payloads) in a BTreeMap keyed by `(epoch, first_block_round)`. Each pending block can contain multiple pipelined blocks at different rounds. [2](#0-1) 

When a block payload arrives, `remove_ready_pending_block()` is called with the payload's epoch and round to check if any pending blocks are now ready for processing. [3](#0-2) 

The critical vulnerability occurs in the split-based removal logic:

1. **Split Operation**: The function splits the BTreeMap at `(received_payload_epoch, received_payload_round + 1)`, creating two sets: blocks with keys < split point (lower half) and blocks with keys >= split point (higher half). [4](#0-3) 

2. **Last Block Check**: It pops the last (highest key) block from the lower half and checks if it's ready. [5](#0-4) 

3. **Mass Deletion**: All remaining blocks in the lower half are unconditionally dropped as "out-of-date". [6](#0-5) 

4. **Store Rebuild**: The store is cleared and rebuilt with only the higher round blocks. [7](#0-6) 

**Attack Scenario:**

Assume pending blocks:
- Block A: key `(epoch: 10, round: 100)`, contains pipelined blocks at rounds `[100, 101, 102]`
- Block B: key `(epoch: 10, round: 110)`, contains pipelined blocks at rounds `[110, 111, 112]`

Malicious/out-of-order flow:
1. A payload for round `105` arrives first (could be from a legitimate but delayed network message, or from a different ordered block not yet received as pending)
2. `order_ready_pending_block(10, 105)` is called [8](#0-7) 

3. `remove_ready_block(10, 105)` splits at `(10, 106)`
4. Block A (key `10, 100`) remains in lower half; Block B (key `10, 110`) moves to higher half
5. `pop_last()` retrieves Block A
6. Block A's payloads don't all exist yet (only round 105 arrived, not 100, 101, 102)
7. Block A's last round (`102`) is NOT > `105`, so it's NOT re-inserted into higher rounds
8. No other blocks remain in lower half
9. Store is cleared and rebuilt with only Block B
10. **Block A is permanently lost**

When payloads for rounds 100, 101, 102 arrive later, Block A is no longer in the pending store, and these blocks are never processed. The consensus observer becomes desynchronized.

The function incorrectly assumes that if a payload for round R arrives, all pending blocks with first_round < R are outdated. This assumption breaks when:
- Payloads arrive out of order
- A payload arrives for a round that doesn't match any pending block's first round
- Network delays cause certain payloads to arrive before the corresponding ordered block

## Impact Explanation

**Severity: HIGH**

This vulnerability causes:

1. **Permanent Block Loss**: Valid pending blocks can be irreversibly removed from the store with no recovery mechanism.

2. **Consensus Observer Desynchronization**: The observer will miss blocks and fail to maintain correct chain state, breaking the **Deterministic Execution** invariant for observer nodes.

3. **Observer Liveness Failure**: Once blocks are lost, the observer cannot progress and requires manual intervention or restart to resync.

4. **Network-Wide Impact**: This affects all consensus observer nodes in the network, as any node can receive out-of-order payloads from malicious or delayed peers.

Per Aptos bug bounty criteria, this qualifies as **High Severity** because it causes:
- Significant protocol violations (observer nodes failing to maintain consensus state)
- Node slowdowns and potential failures requiring intervention
- State inconsistencies between full validators and observer nodes

While not directly causing fund loss, this undermines the reliability of the consensus observer system, which is critical for network monitoring and light client functionality.

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability is highly likely to occur because:

1. **Network Conditions**: P2P networks inherently have message reordering. Payloads can arrive out-of-order due to:
   - Network latency variations
   - Peer selection and connection quality differences
   - Packet loss and retransmission

2. **Malicious Peers**: Any malicious network peer can trigger this by:
   - Sending legitimate block payloads in crafted order
   - No validator access required
   - No cryptographic forgery needed (payloads are legitimate, just delivered at wrong time) [9](#0-8) 

3. **No Validation**: The payload processing path validates signatures and digests but does NOT validate that the payload's round corresponds to any pending block's first round. [10](#0-9) 

4. **Multiple Pipelined Blocks**: Since ordered blocks contain multiple pipelined blocks at different rounds, there are many opportunities for the split point to fall between a pending block's first and last rounds.

## Recommendation

**Fix 1: Validate Payload Belongs to Pending Block**

Before calling `remove_ready_pending_block`, verify that the payload round corresponds to a block within an existing pending block's range:

```rust
pub fn remove_ready_pending_block(
    &mut self,
    received_payload_epoch: u64,
    received_payload_round: Round,
) -> Option<Arc<PendingBlockWithMetadata>> {
    // Find if this payload belongs to any pending block
    let mut target_pending_block_key = None;
    for ((epoch, first_round), pending_block) in self.pending_block_store.blocks_without_payloads.iter() {
        if *epoch == received_payload_epoch {
            let first_round = pending_block.ordered_block().first_block().round();
            let last_round = pending_block.ordered_block().last_block().round();
            if received_payload_round >= first_round && received_payload_round <= last_round {
                target_pending_block_key = Some((*epoch, *first_round));
                break;
            }
        }
    }
    
    // Only proceed if payload belongs to a pending block
    if let Some(key) = target_pending_block_key {
        self.pending_block_store.remove_ready_block(
            key.0, 
            key.1,  // Use first_round, not payload round
            &mut self.block_payload_store,
        )
    } else {
        None  // Payload doesn't belong to any pending block yet
    }
}
```

**Fix 2: Use First Block Round for Split**

Modify `remove_ready_block` to accept the first block's round instead of individual payload rounds, and only remove blocks whose entire range is below the ready block:

```rust
pub fn remove_ready_block(
    &mut self,
    pending_block_epoch: u64,
    pending_block_first_round: Round,
    block_payload_store: &mut BlockPayloadStore,
) -> Option<Arc<PendingBlockWithMetadata>> {
    // Get the specific pending block
    let pending_block = self.blocks_without_payloads.remove(&(pending_block_epoch, pending_block_first_round))?;
    
    // Check if all payloads exist for this block
    if block_payload_store.all_payloads_exist(pending_block.ordered_block().blocks()) {
        // Remove from hash store as well
        let first_block = pending_block.ordered_block().first_block();
        self.blocks_without_payloads_by_hash.remove(&first_block.id());
        
        Some(pending_block)
    } else {
        // Not ready yet, re-insert it
        self.blocks_without_payloads.insert((pending_block_epoch, pending_block_first_round), pending_block.clone());
        self.blocks_without_payloads_by_hash.insert(pending_block.ordered_block().first_block().id(), pending_block);
        
        None
    }
}
```

This approach:
- Only attempts to remove the specific pending block that the payload belongs to
- Never drops unrelated pending blocks as "out-of-date"
- Maintains all pending blocks until their payloads arrive
- Preserves the invariant that pending blocks are only removed when ready or explicitly outdated by consensus state

## Proof of Concept

```rust
#[test]
fn test_pending_block_loss_with_wrong_round() {
    use crate::consensus_observer::observer::pending_blocks::PendingBlockStore;
    use crate::consensus_observer::observer::payload_store::BlockPayloadStore;
    use aptos_config::config::ConsensusObserverConfig;
    
    // Create stores
    let config = ConsensusObserverConfig::default();
    let mut pending_store = PendingBlockStore::new(config.clone());
    let mut payload_store = BlockPayloadStore::new(config);
    
    // Create pending block A with rounds [100, 101, 102], keyed by 100
    let block_a = create_ordered_block(10, 100, 3); // epoch 10, starting round 100, 3 blocks
    let pending_a = create_pending_block_metadata(block_a.clone());
    pending_store.insert_pending_block(pending_a);
    
    // Create pending block B with rounds [110, 111, 112], keyed by 110
    let block_b = create_ordered_block(10, 110, 3);
    let pending_b = create_pending_block_metadata(block_b.clone());
    pending_store.insert_pending_block(pending_b);
    
    // Verify both blocks are in store
    assert_eq!(pending_store.blocks_without_payloads.len(), 2);
    assert!(pending_store.blocks_without_payloads.contains_key(&(10, 100)));
    assert!(pending_store.blocks_without_payloads.contains_key(&(10, 110)));
    
    // Malicious scenario: payload for round 105 arrives (doesn't match any first_round)
    let payload_105 = create_block_payload(10, 105);
    payload_store.insert_block_payload(payload_105, true);
    
    // Call remove_ready_block with wrong round 105
    let ready_block = pending_store.remove_ready_block(10, 105, &mut payload_store);
    
    // BUG: Block A is lost!
    assert!(ready_block.is_none()); // No block was ready
    assert_eq!(pending_store.blocks_without_payloads.len(), 1); // Only 1 block remains
    assert!(!pending_store.blocks_without_payloads.contains_key(&(10, 100))); // Block A lost!
    assert!(pending_store.blocks_without_payloads.contains_key(&(10, 110))); // Block B remains
    
    // Block A is permanently lost, even if its payloads arrive later
    let payload_100 = create_block_payload(10, 100);
    payload_store.insert_block_payload(payload_100, true);
    // No way to recover Block A now - it's gone from pending store
}
```

This PoC demonstrates that calling `remove_ready_block` with a round that doesn't match any pending block's first round causes valid pending blocks to be permanently lost.

---

**Notes**

The vulnerability exists in the core removal logic of the pending block store and affects all consensus observer nodes. The issue stems from an incorrect assumption that payload arrival order implies block ordering, which doesn't hold in asynchronous P2P networks. The recommended fix requires validating that payloads belong to pending blocks before attempting removal, or redesigning the removal logic to target specific blocks rather than using BTreeMap splits based on arbitrary rounds.

### Citations

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L112-132)
```rust
    pub fn insert_pending_block(&mut self, pending_block: Arc<PendingBlockWithMetadata>) {
        // Get the first block in the ordered blocks
        let first_block = pending_block.ordered_block().first_block();

        // Insert the block into the store using the epoch round of the first block
        let first_block_epoch_round = (first_block.epoch(), first_block.round());
        match self.blocks_without_payloads.entry(first_block_epoch_round) {
            Entry::Occupied(_) => {
                // The block is already in the store
                warn!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "A pending block was already found for the given epoch and round: {:?}",
                        first_block_epoch_round
                    ))
                );
            },
            Entry::Vacant(entry) => {
                // Insert the block into the store
                entry.insert(pending_block.clone());
            },
        }
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L200-256)
```rust
    pub fn remove_ready_block(
        &mut self,
        received_payload_epoch: u64,
        received_payload_round: Round,
        block_payload_store: &mut BlockPayloadStore,
    ) -> Option<Arc<PendingBlockWithMetadata>> {
        // Calculate the round at which to split the blocks
        let split_round = received_payload_round.saturating_add(1);

        // Split the blocks at the epoch and round
        let mut blocks_at_higher_rounds = self
            .blocks_without_payloads
            .split_off(&(received_payload_epoch, split_round));

        // Check if the last block is ready (this should be the only ready block).
        // Any earlier blocks are considered out-of-date and will be dropped.
        let mut ready_block = None;
        if let Some((epoch_and_round, pending_block)) = self.blocks_without_payloads.pop_last() {
            // If all payloads exist for the block, then the block is ready
            if block_payload_store.all_payloads_exist(pending_block.ordered_block().blocks()) {
                ready_block = Some(pending_block);
            } else {
                // Otherwise, check if we're still waiting for higher payloads for the block
                let last_pending_block_round = pending_block.ordered_block().last_block().round();
                if last_pending_block_round > received_payload_round {
                    blocks_at_higher_rounds.insert(epoch_and_round, pending_block);
                }
            }
        }

        // Check if any out-of-date blocks are going to be dropped
        if !self.blocks_without_payloads.is_empty() {
            info!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Dropped {:?} out-of-date pending blocks before epoch and round: {:?}",
                    self.blocks_without_payloads.len(),
                    (received_payload_epoch, received_payload_round)
                ))
            );
        }

        // TODO: optimize this flow!

        // Clear all blocks from the pending block stores
        self.clear_missing_blocks();

        // Update the pending block stores to only include the blocks at higher rounds
        self.blocks_without_payloads = blocks_at_higher_rounds;
        for pending_block in self.blocks_without_payloads.values() {
            let first_block = pending_block.ordered_block().first_block();
            self.blocks_without_payloads_by_hash
                .insert(first_block.id(), pending_block.clone());
        }

        // Return the ready block (if one exists)
        ready_block
    }
```

**File:** consensus/src/consensus_observer/observer/block_data.rs (L244-254)
```rust
    pub fn remove_ready_pending_block(
        &mut self,
        received_payload_epoch: u64,
        received_payload_round: Round,
    ) -> Option<Arc<PendingBlockWithMetadata>> {
        self.pending_block_store.remove_ready_block(
            received_payload_epoch,
            received_payload_round,
            &mut self.block_payload_store,
        )
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L355-439)
```rust
    /// Processes the block payload message
    async fn process_block_payload_message(
        &mut self,
        peer_network_id: PeerNetworkId,
        message_received_time: Instant,
        block_payload: BlockPayload,
    ) {
        // Get the epoch and round for the block
        let block_epoch = block_payload.epoch();
        let block_round = block_payload.round();

        // Determine if the payload is behind the last ordered block, or if it already exists
        let last_ordered_block = self.observer_block_data.lock().get_last_ordered_block();
        let payload_out_of_date =
            (block_epoch, block_round) <= (last_ordered_block.epoch(), last_ordered_block.round());
        let payload_exists = self
            .observer_block_data
            .lock()
            .existing_payload_entry(&block_payload);

        // If the payload is out of date or already exists, ignore it
        if payload_out_of_date || payload_exists {
            // Update the metrics for the dropped block payload
            update_metrics_for_dropped_block_payload_message(peer_network_id, &block_payload);
            return;
        }

        // Update the metrics for the received block payload
        update_metrics_for_block_payload_message(peer_network_id, &block_payload);

        // Verify the block payload digests
        if let Err(error) = block_payload.verify_payload_digests() {
            // Log the error and update the invalid message counter
            error!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Failed to verify block payload digests! Ignoring block: {:?}, from peer: {:?}. Error: {:?}",
                    block_payload.block(), peer_network_id,
                    error
                ))
            );
            increment_invalid_message_counter(&peer_network_id, metrics::BLOCK_PAYLOAD_LABEL);
            return;
        }

        // If the payload is for the current epoch, verify the proof signatures
        let epoch_state = self.get_epoch_state();
        let verified_payload = if block_epoch == epoch_state.epoch {
            // Verify the block proof signatures
            if let Err(error) = block_payload.verify_payload_signatures(&epoch_state) {
                // Log the error and update the invalid message counter
                error!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "Failed to verify block payload signatures! Ignoring block: {:?}, from peer: {:?}. Error: {:?}",
                        block_payload.block(), peer_network_id, error
                    ))
                );
                increment_invalid_message_counter(&peer_network_id, metrics::BLOCK_PAYLOAD_LABEL);
                return;
            }

            true // We have successfully verified the signatures
        } else {
            false // We can't verify the signatures yet
        };

        // Update the latency metrics for block payload processing
        update_message_processing_latency_metrics(
            message_received_time,
            &peer_network_id,
            metrics::BLOCK_PAYLOAD_LABEL,
        );

        // Update the payload store with the payload
        self.observer_block_data
            .lock()
            .insert_block_payload(block_payload, verified_payload);

        // Check if there are blocks that were missing payloads but are
        // now ready because of the new payload. Note: this should only
        // be done if the payload has been verified correctly.
        if verified_payload {
            self.order_ready_pending_block(block_epoch, block_round)
                .await;
        }
    }
```
