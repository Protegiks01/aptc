# Audit Report

## Title
Unbounded Transaction Vector in TransactionsInStorage Causes Memory Exhaustion in Indexer Service

## Summary
The `TransactionsInStorage` protobuf message contains an unbounded repeated field `transactions` that lacks size validation during deserialization. An attacker with write access to the storage backend can craft malicious files containing millions of transaction entries, causing memory exhaustion and denial of service when the indexer service reads and deserializes these files.

## Finding Description

The `TransactionsInStorage` struct defines a repeated field for transactions with no size constraints: [1](#0-0) 

When the indexer service reads transaction data from file storage (GCS or local), it performs deserialization without any bounds checking: [2](#0-1) 

The file reading path downloads entire files into memory with no size validation: [3](#0-2) 

The critical vulnerability occurs in the deserialization path: [4](#0-3) 

While upload code enforces a limit of 1000 transactions per file: [5](#0-4) [6](#0-5) 

**This validation exists only on the upload side, not during deserialization**, creating a defense-in-depth vulnerability.

**Attack Path:**
1. Attacker gains write access to GCS bucket (via compromised credentials, bucket misconfiguration, or insider threat)
2. Attacker crafts malicious protobuf file with millions of `Transaction` entries
3. File can be compressed (LZ4) to appear small in storage
4. When indexer service reads the file via `get_raw_file()`, it downloads the entire file
5. Decompression expands the file significantly
6. `TransactionsInStorage::decode()` deserializes all transactions without size checks
7. Memory allocation for millions of transactions causes OOM crash
8. Indexer service becomes unavailable

This breaks **Invariant #9: Resource Limits** - all operations must respect memory and computational limits.

## Impact Explanation

This vulnerability qualifies as **HIGH severity** per the Aptos bug bounty criteria:
- **API crashes**: The indexer service crashes due to memory exhaustion, causing API unavailability
- **Significant protocol violations**: Violates resource limit invariants and defense-in-depth principles

The impacted services include:
- Indexer-grpc data services that provide blockchain data to applications
- Historical and live data APIs
- Any service depending on the indexer infrastructure

While this does not directly affect consensus validators, it impacts critical infrastructure that applications rely on for blockchain data access.

## Likelihood Explanation

**Likelihood: Medium to High** depending on storage security posture

**Required conditions:**
- Attacker needs write access to storage backend (GCS bucket or local file system)
- This could occur through:
  - Compromised service account credentials
  - Misconfigured storage bucket (e.g., public write access)
  - Insider threat
  - Supply chain attack on storage infrastructure

**Exploitation complexity: Low**
- Once storage access is obtained, creating malicious protobuf is straightforward
- No special timing or race conditions required
- Attack is repeatable and deterministic

The lack of bounds checking means even legitimate bugs or data corruption could trigger the issue accidentally.

## Recommendation

**Immediate Fix:** Add size validation during deserialization:

```rust
pub fn into_transactions_in_storage(self) -> Result<TransactionsInStorage> {
    match self {
        FileEntry::Lz4CompressionProto(bytes) => {
            let mut decompressor = Decoder::new(&bytes[..])
                .context("Lz4 decompression failed")?;
            let mut decompressed = Vec::new();
            decompressor
                .read_to_end(&mut decompressed)
                .context("Lz4 decompression failed")?;
            
            let transactions_in_storage = TransactionsInStorage::decode(decompressed.as_slice())
                .context("proto deserialization failed")?;
            
            // Validate transaction count
            if transactions_in_storage.transactions.len() > FILE_ENTRY_TRANSACTION_COUNT as usize {
                anyhow::bail!(
                    "Transaction count {} exceeds maximum allowed {}",
                    transactions_in_storage.transactions.len(),
                    FILE_ENTRY_TRANSACTION_COUNT
                );
            }
            
            Ok(transactions_in_storage)
        },
        // ... handle other cases with similar validation
    }
}
```

**Additional hardening:**
1. Add maximum decompressed size checks before full decompression
2. Implement streaming deserialization for large messages
3. Add memory usage monitoring and circuit breakers
4. Validate file integrity with checksums before deserialization

## Proof of Concept

```rust
use aptos_protos::{
    indexer::v1::TransactionsInStorage,
    transaction::v1::Transaction,
};
use prost::Message;
use lz4::EncoderBuilder;
use std::io::Write;

#[test]
fn test_unbounded_transactions_memory_exhaustion() {
    // Create malicious TransactionsInStorage with excessive transactions
    let malicious_txn_count = 10_000_000; // 10 million transactions
    
    let mut malicious_transactions = Vec::new();
    for i in 0..malicious_txn_count {
        malicious_transactions.push(Transaction {
            version: i,
            epoch: 1,
            ..Transaction::default()
        });
    }
    
    let malicious_storage = TransactionsInStorage {
        starting_version: Some(0),
        transactions: malicious_transactions,
    };
    
    // Serialize to protobuf
    let mut serialized = Vec::new();
    malicious_storage.encode(&mut serialized).unwrap();
    
    // Compress with LZ4 (file will be small due to repetitive data)
    let mut compressed = EncoderBuilder::new()
        .level(4)
        .build(Vec::new())
        .unwrap();
    compressed.write_all(&serialized).unwrap();
    let compressed_data = compressed.finish().0;
    
    println!("Compressed size: {} bytes", compressed_data.len());
    println!("Uncompressed size: {} bytes", serialized.len());
    println!("Transaction count: {}", malicious_txn_count);
    
    // Simulating FileEntry::into_transactions_in_storage() deserialization
    // This will attempt to allocate memory for 10 million transactions
    // causing memory exhaustion in production environment
    
    // Note: In actual attack, attacker would upload compressed_data to GCS
    // and wait for indexer to read and deserialize it, causing crash
}
```

**Notes:**
- This vulnerability is specific to the indexer-grpc infrastructure, not the core blockchain validators
- The attack requires storage access, limiting the attacker pool but not eliminating the risk
- Defense-in-depth principle dictates that deserialization should validate untrusted data, even from "trusted" storage
- Similar validation exists on upload but is missing on read, creating an asymmetric trust model
- The gRPC message size limit of 256MB applies only to network messages, not file storage reads

### Citations

**File:** protos/rust/src/pb/aptos.indexer.v1.rs (L108-118)
```rust
/// This is for storage only.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TransactionsInStorage {
    /// Required; transactions data.
    #[prost(message, repeated, tag="1")]
    pub transactions: ::prost::alloc::vec::Vec<super::super::transaction::v1::Transaction>,
    /// Required; chain id.
    #[prost(uint64, optional, tag="2")]
    pub starting_version: ::core::option::Option<u64>,
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/compression_util.rs (L12-12)
```rust
pub const FILE_ENTRY_TRANSACTION_COUNT: u64 = 1000;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/compression_util.rs (L262-292)
```rust
    pub fn into_transactions_in_storage(self) -> TransactionsInStorage {
        match self {
            FileEntry::Lz4CompressionProto(bytes) => {
                let mut decompressor = Decoder::new(&bytes[..]).expect("Lz4 decompression failed.");
                let mut decompressed = Vec::new();
                decompressor
                    .read_to_end(&mut decompressed)
                    .expect("Lz4 decompression failed.");
                TransactionsInStorage::decode(decompressed.as_slice())
                    .expect("proto deserialization failed.")
            },
            FileEntry::JsonBase64UncompressedProto(bytes) => {
                let file: TransactionsLegacyFile =
                    serde_json::from_slice(bytes.as_slice()).expect("json deserialization failed.");
                let transactions = file
                    .transactions_in_base64
                    .into_iter()
                    .map(|base64| {
                        let bytes: Vec<u8> =
                            base64::decode(base64).expect("base64 decoding failed.");
                        Transaction::decode(bytes.as_slice())
                            .expect("proto deserialization failed.")
                    })
                    .collect::<Vec<Transaction>>();
                TransactionsInStorage {
                    starting_version: Some(file.starting_version),
                    transactions,
                }
            },
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/gcs.rs (L103-124)
```rust
    async fn get_raw_file(&self, version: u64) -> anyhow::Result<Vec<u8>> {
        let file_entry_key_path = self.get_file_entry_key_path(version);
        match Object::download(&self.bucket_name, file_entry_key_path.as_str()).await {
            Ok(file) => Ok(file),
            Err(cloud_storage::Error::Other(err)) => {
                if err.contains("No such object: ") {
                    anyhow::bail!("[Indexer File] Transactions file not found. Gap might happen between cache and file store. {}", err)
                } else {
                    anyhow::bail!(
                        "[Indexer File] Error happens when downloading transaction file. {}",
                        err
                    );
                }
            },
            Err(err) => {
                anyhow::bail!(
                    "[Indexer File] Error happens when transaction file. {}",
                    err
                );
            },
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/gcs.rs (L215-222)
```rust
        anyhow::ensure!(
            start_version % FILE_ENTRY_TRANSACTION_COUNT == 0,
            "Starting version has to be a multiple of BLOB_STORAGE_SIZE."
        );
        anyhow::ensure!(
            batch_size == FILE_ENTRY_TRANSACTION_COUNT as usize,
            "The number of transactions to upload has to be multiplier of BLOB_STORAGE_SIZE."
        );
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_reader.rs (L234-239)
```rust
        let transactions_in_storage = tokio::task::spawn_blocking(move || {
            FileEntry::new(bytes, StorageFormat::Lz4CompressedProto).into_transactions_in_storage()
        })
        .await?;

        Ok(transactions_in_storage.transactions)
```
