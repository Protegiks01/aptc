# Audit Report

## Title
Silent Event Sequence Gap Detection Failure in PostgreSQL Indexer Leads to Data Loss

## Summary
The Aptos PostgreSQL-based external indexer (`crates/indexer/`) does not validate that event sequence numbers are consecutive for a given GUID, allowing missing events to be silently dropped from the indexed database. This creates incomplete event streams that applications consume without any error indication, leading to incorrect application state.

## Finding Description

The Aptos blockchain emits events with monotonically increasing sequence numbers for each unique GUID (account_address + creation_number). The protocol guarantees that events for a given GUID will have consecutive sequence numbers starting from 0, incremented by 1 each time.

However, the PostgreSQL-based external indexer has no mechanism to detect when events are missing from the stream. The vulnerability exists in the event processing pipeline:

1. **Event Extraction Without Validation**: In the `Event` model, events are extracted from transactions without any sequence number validation. [1](#0-0) 

2. **Blind Insertion**: The `insert_events` function simply inserts events into PostgreSQL using conflict resolution, but never checks if sequence numbers are consecutive. [2](#0-1) 

3. **No Gap Detection in Transaction Processing**: The `from_transactions` method extracts events and filters them, but performs no sequence validation. [3](#0-2) 

**Attack/Failure Scenario:**
- The indexer fetches transactions from a node API endpoint
- Due to a bug in the fetcher, network issue, database corruption, or node restart, transactions at versions 100-105 are skipped
- Events from those missing transactions (e.g., critical NFT transfers or token deposits) are never indexed
- The indexer continues processing from version 106 onwards
- Result: For affected GUIDs, the sequence numbers jump (e.g., 0,1,2,3,10,11,12 instead of 0,1,2,3,4,5,6,7,8,9,10,11,12)
- Applications querying the events table receive incomplete streams
- **No error is thrown** - the data loss is completely silent

**Contrast with Internal Indexer**: The internal storage indexer at `storage/indexer/` implements explicit gap detection through sequence number validation, confirming this is a known concern that was addressed in one system but omitted from the PostgreSQL indexer. [4](#0-3) 

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos bug bounty criteria:

**"State inconsistencies requiring intervention"** - Applications consuming the indexed events will have incorrect state due to missing event data. This affects:
- NFT transfer history tracking (missing ownership changes)
- Token activity monitoring (missing deposit/withdrawal events)
- DeFi protocols tracking events for critical state updates
- Analytics platforms providing incomplete data to users

**"Limited funds loss or manipulation"** - While the indexer itself doesn't hold funds, applications relying on complete event streams for business logic (e.g., automated market makers, NFT marketplaces) could make incorrect decisions leading to financial losses for users.

The issue doesn't reach High/Critical severity because:
- It doesn't directly affect consensus or the core blockchain
- Funds aren't directly stolen from the blockchain
- The node's internal indexer is unaffected
- Recovery is possible by reindexing from scratch

## Likelihood Explanation

**Moderate to High Likelihood:**

1. **Historical Precedent**: Blockchain indexers commonly experience failures due to:
   - Node restarts during version tracking inconsistencies
   - Network partitions during transaction fetching
   - Race conditions in distributed systems
   - Database transaction rollbacks

2. **Code Evidence**: The current implementation has no defensive checks against gaps, as confirmed by the absence of any sequence validation logic in the entire `crates/indexer/` codebase.

3. **Real-World Triggers**:
   - Indexer crash and restart with incorrect version checkpoint
   - Bug in `Fetcher::run()` causing version jumps
   - Node serving corrupted or incomplete transaction data
   - PostgreSQL transaction failures during insertion leading to partial data loss

4. **Detection Difficulty**: The silent nature makes this hard to detect until users report incorrect data, by which time the damage is done.

## Recommendation

Implement sequence number gap detection in the event processing pipeline. Add validation before inserting events:

```rust
// In crates/indexer/src/processors/default_processor.rs
fn validate_event_sequences(events: &[EventModel]) -> Result<(), String> {
    use std::collections::HashMap;
    
    // Group events by GUID (account_address, creation_number)
    let mut guid_sequences: HashMap<(String, i64), Vec<i64>> = HashMap::new();
    
    for event in events {
        let guid = (event.account_address.clone(), event.creation_number);
        guid_sequences.entry(guid).or_default().push(event.sequence_number);
    }
    
    // For each GUID, verify sequences are consecutive when sorted
    for ((addr, creation_num), mut sequences) in guid_sequences {
        sequences.sort_unstable();
        
        // Check for gaps (allowing for batches from different starting points)
        for window in sequences.windows(2) {
            if window[1] != window[0] + 1 && window[0] != window[1] {
                return Err(format!(
                    "Event sequence gap detected for GUID ({}, {}): {} -> {}",
                    addr, creation_num, window[0], window[1]
                ));
            }
        }
    }
    
    Ok(())
}

// Call before insert_events in insert_to_db_impl:
fn insert_to_db_impl(...) -> Result<(), diesel::result::Error> {
    // Add validation
    validate_event_sequences(events)
        .map_err(|e| diesel::result::Error::DatabaseError(
            diesel::result::DatabaseErrorKind::Unknown,
            Box::new(e)
        ))?;
    
    // ... existing insertion code
}
```

Additionally, implement a background checker that periodically validates event sequence continuity in the database:

```sql
-- Query to detect gaps
WITH event_sequences AS (
  SELECT 
    account_address,
    creation_number,
    sequence_number,
    LAG(sequence_number) OVER (
      PARTITION BY account_address, creation_number 
      ORDER BY sequence_number
    ) as prev_seq
  FROM events
)
SELECT 
  account_address,
  creation_number,
  prev_seq,
  sequence_number,
  (sequence_number - prev_seq - 1) as gap_size
FROM event_sequences
WHERE prev_seq IS NOT NULL 
  AND sequence_number != prev_seq + 1
ORDER BY gap_size DESC;
```

## Proof of Concept

```rust
// Test file: crates/indexer/src/models/events_test.rs
#[cfg(test)]
mod tests {
    use super::*;
    use aptos_api_types::{Event as APIEvent, EventGuid};
    
    #[test]
    fn test_missing_event_sequence_undetected() {
        // Create events with a gap in sequence numbers
        let events = vec![
            create_test_event("0xabc", 1, 0),
            create_test_event("0xabc", 1, 1),
            // Sequence 2 is missing here!
            create_test_event("0xabc", 1, 3),
            create_test_event("0xabc", 1, 4),
        ];
        
        // Current implementation accepts this without error
        let event_models = EventModel::from_events(&events, 100, 50);
        
        // This should fail but doesn't - demonstrating the vulnerability
        assert_eq!(event_models.len(), 4);
        
        // Verify the gap exists
        let sequences: Vec<i64> = event_models.iter()
            .map(|e| e.sequence_number)
            .collect();
        assert_eq!(sequences, vec![0, 1, 3, 4]); // Gap at 2!
        
        // In production, this would be silently inserted into PostgreSQL
        // Applications querying for all events would miss sequence #2
    }
    
    fn create_test_event(address: &str, creation_num: u64, seq_num: u64) -> APIEvent {
        // Helper to create test events
        // Implementation details omitted for brevity
    }
}
```

**Exploitation Steps:**
1. Set up an indexer pointing to a test node
2. Insert transactions with events for a specific GUID (sequences 0-10)
3. Manually skip indexing transaction containing sequence number 5
4. Continue indexing from sequence 6 onwards
5. Query the events table for that GUID
6. Observe that sequence 5 is missing, but no error was logged
7. Applications consuming this data now have an incomplete event stream

This demonstrates that the vulnerability allows silent data loss that would go unnoticed until users report incorrect application state.

### Citations

**File:** crates/indexer/src/models/events.rs (L42-79)
```rust
impl Event {
    pub fn from_event(
        event: &APIEvent,
        transaction_version: i64,
        transaction_block_height: i64,
        event_index: i64,
    ) -> Self {
        Event {
            account_address: standardize_address(&event.guid.account_address.to_string()),
            creation_number: event.guid.creation_number.0 as i64,
            sequence_number: event.sequence_number.0 as i64,
            transaction_version,
            transaction_block_height,
            type_: event.typ.to_string(),
            data: event.data.clone(),
            event_index: Some(event_index),
        }
    }

    pub fn from_events(
        events: &[APIEvent],
        transaction_version: i64,
        transaction_block_height: i64,
    ) -> Vec<Self> {
        events
            .iter()
            .enumerate()
            .map(|(index, event)| {
                Self::from_event(
                    event,
                    transaction_version,
                    transaction_block_height,
                    index as i64,
                )
            })
            .collect::<Vec<EventModel>>()
    }
}
```

**File:** crates/indexer/src/processors/default_processor.rs (L276-297)
```rust
fn insert_events(
    conn: &mut PgConnection,
    items_to_insert: &[EventModel],
) -> Result<(), diesel::result::Error> {
    use schema::events::dsl::*;
    let chunks = get_chunks(items_to_insert.len(), EventModel::field_count());
    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::events::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict((account_address, creation_number, sequence_number))
                .do_update()
                .set((
                    inserted_at.eq(excluded(inserted_at)),
                    event_index.eq(excluded(event_index)),
                )),
            None,
        )?;
    }
    Ok(())
}
```

**File:** crates/indexer/src/models/transactions.rs (L256-291)
```rust
    pub fn from_transactions(
        transactions: &[APITransaction],
    ) -> (
        Vec<Self>,
        Vec<TransactionDetail>,
        Vec<EventModel>,
        Vec<WriteSetChangeModel>,
        Vec<WriteSetChangeDetail>,
    ) {
        let mut txns = vec![];
        let mut txn_details = vec![];
        let mut events = vec![];
        let mut wscs = vec![];
        let mut wsc_details = vec![];

        for txn in transactions {
            let (txn, txn_detail, event_list, mut wsc_list, mut wsc_detail_list) =
                Self::from_transaction(txn);
            let mut event_v1_list = event_list
                .into_iter()
                .filter(|e| {
                    !(e.sequence_number == 0
                        && e.creation_number == 0
                        && e.account_address == DEFAULT_ACCOUNT_ADDRESS)
                })
                .collect::<Vec<_>>();
            txns.push(txn);
            if let Some(a) = txn_detail {
                txn_details.push(a);
            }
            events.append(&mut event_v1_list);
            wscs.append(&mut wsc_list);
            wsc_details.append(&mut wsc_detail_list);
        }
        (txns, txn_details, events, wscs, wsc_details)
    }
```

**File:** storage/indexer/src/db_indexer.rs (L217-256)
```rust
            u64,     // sequence number
            Version, // transaction version it belongs to
            u64,     // index among events for the same transaction
        )>,
    > {
        let mut iter = self.db.iter::<EventByKeySchema>()?;
        iter.seek(&(*event_key, start_seq_num))?;

        let mut result = Vec::new();
        let mut cur_seq = start_seq_num;
        for res in iter.take(limit as usize) {
            let ((path, seq), (ver, idx)) = res?;
            if path != *event_key || ver > ledger_version {
                break;
            }
            if seq != cur_seq {
                let msg = if cur_seq == start_seq_num {
                    "First requested event is probably pruned."
                } else {
                    "DB corruption: Sequence number not continuous."
                };
                bail!("{} expected: {}, actual: {}", msg, cur_seq, seq);
            }
            result.push((seq, ver, idx));
            cur_seq += 1;
        }

        Ok(result)
    }

    #[cfg(any(test, feature = "fuzzing"))]
    pub fn get_restore_version_and_progress(
        &self,
    ) -> Result<Option<(Version, StateSnapshotProgress)>> {
        let mut iter = self.db.iter::<InternalIndexerMetadataSchema>()?;
        iter.seek_to_first();
        let mut last_version = None;
        let mut last_progress = None;
        for res in iter {
            let (key, value) = res?;
```
