# Audit Report

## Title
Thread Pool Creation Panic Vulnerability Preventing Validator Consensus Participation Under Resource Constraints

## Summary
The sharded block executor initialization contains unchecked `unwrap()` calls on `rayon::ThreadPoolBuilder::build()` that will panic if OS thread creation fails due to resource limits. This causes validator crashes during block execution, preventing consensus participation and creating availability vulnerabilities under resource-constrained conditions.

## Finding Description

The codebase contains critical panic vulnerabilities in thread pool initialization paths that are invoked during validator block execution. Specifically:

**Primary Issue Location:** [1](#0-0) 

When validators execute blocks using the sharded executor, the `SHARDED_BLOCK_EXECUTOR` static is lazily initialized, which creates multiple `ShardedExecutorService` instances. Each service attempts to build a rayon thread pool, and any failure results in an immediate panic.

**Secondary Issue Location:** [2](#0-1) 

The remote executor service (a separate process for distributed sharded execution) has an identical vulnerability during startup initialization.

**Execution Flow to Vulnerability:**

1. During block execution, the executor workflow invokes sharded execution: [3](#0-2) 

2. On first access, `SHARDED_BLOCK_EXECUTOR` is lazily initialized: [4](#0-3) 

3. This calls `setup_local_executor_shards` which creates executor services for each shard: [5](#0-4) 

4. Each shard's `ShardedExecutorService::new()` builds a thread pool with `.unwrap()`, causing panic if thread creation fails due to:
   - OS thread limits (ulimit -u on Linux)
   - Insufficient memory for thread stacks
   - System resource exhaustion
   - Container/VM resource constraints

**Invariant Violations:**

This breaks the critical invariant: **"Resource Limits: All operations must respect gas, storage, and computational limits"**. Validators must handle resource constraints gracefully rather than panicking, which violates availability requirements for consensus participation.

## Impact Explanation

**Severity: HIGH** (per Aptos Bug Bounty criteria: "Validator node slowdowns, API crashes, Significant protocol violations")

**Impact Details:**

1. **Validator Crash**: When thread pool creation fails, the validator process panics and terminates
2. **Consensus Participation Failure**: Crashed validators cannot vote on blocks, propose blocks, or execute transactions
3. **Persistent Availability Issues**: If OS resources remain constrained (common in containerized deployments with resource limits), the validator cannot successfully restart
4. **Network Liveness Risk**: If multiple validators are affected simultaneously during high-load periods or in resource-constrained environments, network liveness could be impacted (approaching the 1/3 Byzantine threshold)

This does not reach CRITICAL severity because it requires specific resource constraint conditions and doesn't directly cause fund loss or consensus safety violations. However, it significantly impacts validator availability and network resilience.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH** in production environments

**Factors Increasing Likelihood:**

1. **Containerized Deployments**: Validators running in Kubernetes/Docker with strict resource limits are susceptible
2. **Cloud VM Constraints**: VMs with limited thread/memory allocations can hit OS limits
3. **High Load Periods**: During network congestion, validators may have many concurrent threads
4. **Restart Scenarios**: After crashes or maintenance, validators restarting under load are vulnerable
5. **Multi-Shard Configuration**: More shards = more threads needed, increasing probability of hitting limits

**Realistic Scenario:**
- Validator deployed in container with ulimit -u 4096
- Under normal load: ~3000 threads active across all processes
- Spike in transaction volume + state sync operations
- Sharded executor tries to create additional threads (num_threads + 2 per shard)
- Thread limit exceeded → panic → validator offline
- Cannot rejoin until load decreases or limits increased

## Recommendation

Replace all `unwrap()` calls on `ThreadPoolBuilder::build()` with proper error handling that logs the error and either:
1. Falls back to a smaller thread pool configuration
2. Returns a Result that allows graceful degradation
3. Uses a global thread pool with controlled concurrency

**Recommended Fix for ShardedExecutorService:**

```rust
pub fn new(
    shard_id: ShardId,
    num_shards: usize,
    num_threads: usize,
    coordinator_client: Arc<dyn CoordinatorClient<S>>,
    cross_shard_client: Arc<dyn CrossShardClient>,
) -> Result<Self, ThreadPoolBuildError> {
    let executor_thread_pool = Arc::new(
        rayon::ThreadPoolBuilder::new()
            .thread_name(move |i| format!("sharded-executor-shard-{}-{}", shard_id, i))
            .num_threads(num_threads + 2)
            .build()
            .map_err(|e| {
                error!(
                    "Failed to create thread pool for shard {}: {}. Falling back to smaller pool.",
                    shard_id, e
                );
                // Attempt fallback with minimal threads
                rayon::ThreadPoolBuilder::new()
                    .thread_name(move |i| format!("sharded-executor-shard-{}-{}", shard_id, i))
                    .num_threads(2) // Minimal: 1 for execution + 1 for cross-shard
                    .build()
            })?,
    );
    Ok(Self {
        shard_id,
        num_shards,
        executor_thread_pool,
        coordinator_client,
        cross_shard_client,
    })
}
```

**Similar fix needed for RemoteStateViewClient:** [6](#0-5) 

## Proof of Concept

```rust
// Rust test to reproduce the panic
#[test]
#[should_panic(expected = "thread pool")]
fn test_thread_pool_creation_panic_on_resource_limit() {
    use std::sync::atomic::{AtomicUsize, Ordering};
    use std::sync::Arc;
    
    // Simulate resource exhaustion by creating threads until we approach OS limit
    let counter = Arc::new(AtomicUsize::new(0));
    let mut threads = vec![];
    
    // Create many threads to consume OS thread quota
    // (actual limit depends on ulimit -u)
    for i in 0..4000 {
        let counter_clone = counter.clone();
        match std::thread::Builder::new()
            .name(format!("exhaust-{}", i))
            .spawn(move || {
                counter_clone.fetch_add(1, Ordering::SeqCst);
                std::thread::sleep(std::time::Duration::from_secs(60));
            }) {
            Ok(t) => threads.push(t),
            Err(_) => break, // Approaching limit
        }
    }
    
    // Now try to create ShardedExecutorService - this will panic
    // because ThreadPoolBuilder::build() will fail
    rayon::ThreadPoolBuilder::new()
        .num_threads(100)
        .build()
        .unwrap(); // PANIC HERE when OS cannot create more threads
}
```

**Reproduction Steps:**

1. Set conservative OS thread limit: `ulimit -u 2048`
2. Start Aptos validator node with sharded execution enabled
3. Generate high transaction volume to trigger sharded block execution
4. Observe validator panic with message related to thread pool creation failure
5. Validator remains offline until OS resources are freed or limits increased

## Notes

This vulnerability affects both local (in-process) and remote (distributed) sharded execution configurations. The local executor lazy initialization means the panic may occur during normal operation rather than process startup, making it particularly insidious as validators can appear to start successfully but crash on their first sharded block execution under resource constraints.

The issue is exacerbated by the fact that the number of threads scales with the number of shards, and the codebase uses `num_cpus::get()` which may request more threads than the OS can provide in constrained environments.

### Citations

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L58-66)
```rust
        let executor_thread_pool = Arc::new(
            rayon::ThreadPoolBuilder::new()
                // We need two extra threads for the cross-shard commit receiver and the thread
                // that is blocked on waiting for execute block to finish.
                .thread_name(move |i| format!("sharded-executor-shard-{}-{}", shard_id, i))
                .num_threads(num_threads + 2)
                .build()
                .unwrap(),
        );
```

**File:** execution/executor-service/src/remote_state_view.rs (L79-90)
```rust
    pub fn new(
        shard_id: ShardId,
        controller: &mut NetworkController,
        coordinator_address: SocketAddr,
    ) -> Self {
        let thread_pool = Arc::new(
            rayon::ThreadPoolBuilder::new()
                .thread_name(move |index| format!("remote-state-view-shard-{}-{}", shard_id, index))
                .num_threads(num_cpus::get())
                .build()
                .unwrap(),
        );
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L256-276)
```rust
    fn execute_block_sharded<V: VMBlockExecutor>(
        partitioned_txns: PartitionedTransactions,
        state_view: Arc<CachedStateView>,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<Vec<TransactionOutput>> {
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        } else {
            Ok(V::execute_block_sharded(
                &SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        }
    }
```

**File:** execution/executor-service/src/local_executor_helper.rs (L14-21)
```rust
pub static SHARDED_BLOCK_EXECUTOR: Lazy<
    Arc<Mutex<ShardedBlockExecutor<CachedStateView, LocalExecutorClient<CachedStateView>>>>,
> = Lazy::new(|| {
    info!("LOCAL_SHARDED_BLOCK_EXECUTOR created");
    Arc::new(Mutex::new(
        LocalExecutorClient::create_local_sharded_block_executor(AptosVM::get_num_shards(), None),
    ))
});
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L105-125)
```rust
        let executor_shards = command_rxs
            .into_iter()
            .zip(result_txs)
            .zip(cross_shard_msg_rxs)
            .enumerate()
            .map(|(shard_id, ((command_rx, result_tx), cross_shard_rxs))| {
                let cross_shard_client = LocalCrossShardClient::new(
                    global_cross_shard_tx.clone(),
                    cross_shard_msg_txs.clone(),
                    cross_shard_rxs,
                );
                Self::new(
                    shard_id as ShardId,
                    num_shards,
                    num_threads,
                    command_rx,
                    result_tx,
                    cross_shard_client,
                )
            })
            .collect();
```
