# Audit Report

## Title
CommitNotification Message Loss During QuorumStore Coordinator Shutdown Leading to State Inconsistency

## Summary
The `QuorumStoreCoordinator::start()` function immediately breaks out of its message processing loop upon receiving a `Shutdown` command, discarding any queued `CommitNotification` messages in the channel. This causes critical state inconsistencies in the `ProofManager` and `BatchGenerator` subsystems, leading to memory leaks, incorrect back pressure calculations, and potential batch reuse after restart.

## Finding Description

The vulnerability occurs in the shutdown sequence of the QuorumStore coordinator. [1](#0-0) 

When blocks are committed during consensus, the `QuorumStorePayloadManager` sends `CommitNotification` messages to the coordinator: [2](#0-1) 

These notifications use `try_send`, which is non-blocking and will succeed as long as the channel is not full: [3](#0-2) 

The coordinator channel is created as a bounded futures channel: [4](#0-3) 

**The Attack Scenario:**

1. During an epoch transition or node shutdown, blocks continue to be committed through the consensus pipeline
2. Each committed block triggers `notify_commit`, sending a `CommitNotification` to the coordinator's channel: [5](#0-4) 
3. The `EpochManager` sends a `Shutdown` command to the coordinator: [6](#0-5) 
4. When the coordinator processes the `Shutdown` command, it immediately breaks out of the loop after shutting down subsystems, without draining remaining messages in the channel
5. Any `CommitNotification` messages queued after the `Shutdown` are dropped

**State Corruption Impact:**

The lost `CommitNotification` messages cause critical state inconsistencies:

**ProofManager Impact:**
The `ProofManager` relies on `CommitNotification` to mark batches as committed: [7](#0-6) 

The underlying `mark_committed` operation performs critical state updates: [8](#0-7) 

Without these notifications:
- Batches remain unmarked as committed, allowing potential reuse
- `remaining_proofs` counters are not decremented, causing incorrect back pressure
- Transaction summary occurrence counts remain inflated, causing memory leaks

**BatchGenerator Impact:**
The `BatchGenerator` uses `CommitNotification` to clean up state: [9](#0-8) 

Without these notifications:
- `latest_block_timestamp` becomes stale
- Committed batches remain in `batches_in_progress` indefinitely
- Expired batches are not cleaned up, causing memory leaks

## Impact Explanation

This is a **Medium Severity** vulnerability per Aptos bug bounty criteria because it causes:

1. **State inconsistencies requiring intervention**: The `ProofManager` and `BatchGenerator` maintain incorrect state after shutdown/restart, requiring manual intervention or node restart to recover

2. **Memory leaks**: Uncommitted batches and transaction summaries remain in memory indefinitely

3. **Incorrect protocol behavior**: Back pressure calculations become incorrect, potentially affecting consensus throughput

4. **Potential batch reuse**: Batches that were actually committed may not be marked as such, creating risk of duplicate batch usage

This does not meet Critical or High severity because:
- It does not directly cause consensus safety violations
- It does not result in loss of funds
- It does not cause permanent network unavailability
- It requires shutdown/epoch transition to trigger

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability will trigger in the following scenarios:

1. **Epoch transitions with active block commits**: Epochs transition regularly (every few hours), and if blocks are being committed when the transition occurs, CommitNotifications will be queued
2. **Node shutdown under load**: When a validator node is shut down while actively participating in consensus
3. **Rapid epoch transitions**: In networks with frequent reconfigurations

The likelihood increases with:
- Higher block commit rate (more messages queued)
- Longer channel transit time
- Smaller channel buffer size

Since epoch transitions are a normal part of Aptos operation and blocks are continuously committed, this issue will occur regularly in production deployments.

## Recommendation

The coordinator should drain all pending `CommitNotification` messages **before** initiating subsystem shutdown:

```rust
CoordinatorCommand::Shutdown(ack_tx) => {
    counters::QUORUM_STORE_MSG_COUNT
        .with_label_values(&["QSCoordinator::shutdown"])
        .inc();
    
    // First, drain all pending CommitNotifications
    loop {
        match rx.try_next() {
            Ok(Some(CoordinatorCommand::CommitNotification(block_timestamp, batches))) => {
                // Process the notification before shutdown
                self.proof_coordinator_cmd_tx
                    .send(ProofCoordinatorCommand::CommitNotification(batches.clone()))
                    .await
                    .expect("Failed to send to ProofCoordinator");
                self.proof_manager_cmd_tx
                    .send(ProofManagerCommand::CommitNotification(block_timestamp, batches.clone()))
                    .await
                    .expect("Failed to send to ProofManager");
                self.batch_generator_cmd_tx
                    .send(BatchGeneratorCommand::CommitNotification(block_timestamp, batches))
                    .await
                    .expect("Failed to send to BatchGenerator");
            },
            _ => break, // No more CommitNotifications
        }
    }
    
    // Now proceed with subsystem shutdown
    // ... existing shutdown code ...
}
```

Alternatively, implement a two-phase shutdown:
1. Signal "no new commits" to prevent new CommitNotifications
2. Drain existing CommitNotifications
3. Shutdown subsystems

## Proof of Concept

The following Rust test demonstrates the vulnerability:

```rust
#[tokio::test]
async fn test_commit_notification_loss_on_shutdown() {
    use futures_channel::mpsc;
    use consensus::quorum_store::quorum_store_coordinator::{CoordinatorCommand, QuorumStoreCoordinator};
    use aptos_consensus_types::proof_of_store::BatchInfoExt;
    
    // Create coordinator with small channel
    let (mut coordinator_tx, coordinator_rx) = mpsc::channel(10);
    
    // Simulate rapid commits followed by shutdown
    for i in 0..5 {
        let batches = vec![]; // Simplified for test
        coordinator_tx.try_send(CoordinatorCommand::CommitNotification(i * 1000, batches))
            .expect("Send commit notification");
    }
    
    // Send shutdown while commits are queued
    let (ack_tx, ack_rx) = futures_channel::oneshot::channel();
    coordinator_tx.try_send(CoordinatorCommand::Shutdown(ack_tx))
        .expect("Send shutdown");
    
    // Add more commits after shutdown (simulating race condition)
    for i in 5..10 {
        let batches = vec![];
        let _ = coordinator_tx.try_send(CoordinatorCommand::CommitNotification(i * 1000, batches));
    }
    
    // Start coordinator (this will process shutdown and drop remaining messages)
    // The coordinator will break on Shutdown, losing messages 5-9
    
    // After restart, ProofManager and BatchGenerator will have stale state
    // for the lost commit notifications
}
```

**Notes**

This vulnerability affects the **State Consistency** invariant: state transitions must be atomic and complete. The coordinator's incomplete processing of commit notifications violates this invariant by leaving subsystems in an inconsistent state where committed batches are not properly tracked.

The issue is exacerbated during high-throughput periods when many blocks are being committed rapidly, making message queueing more likely. The bounded channel size means that even with `try_send`, messages can accumulate during brief processing delays.

### Citations

**File:** consensus/src/quorum_store/quorum_store_coordinator.rs (L52-166)
```rust
    pub async fn start(self, mut rx: futures_channel::mpsc::Receiver<CoordinatorCommand>) {
        while let Some(cmd) = rx.next().await {
            monitor!("quorum_store_coordinator_loop", {
                match cmd {
                    CoordinatorCommand::CommitNotification(block_timestamp, batches) => {
                        counters::QUORUM_STORE_MSG_COUNT
                            .with_label_values(&["QSCoordinator::commit_notification"])
                            .inc();
                        // TODO: need a callback or not?
                        self.proof_coordinator_cmd_tx
                            .send(ProofCoordinatorCommand::CommitNotification(batches.clone()))
                            .await
                            .expect("Failed to send to ProofCoordinator");

                        self.proof_manager_cmd_tx
                            .send(ProofManagerCommand::CommitNotification(
                                block_timestamp,
                                batches.clone(),
                            ))
                            .await
                            .expect("Failed to send to ProofManager");

                        self.batch_generator_cmd_tx
                            .send(BatchGeneratorCommand::CommitNotification(
                                block_timestamp,
                                batches,
                            ))
                            .await
                            .expect("Failed to send to BatchGenerator");
                    },
                    CoordinatorCommand::Shutdown(ack_tx) => {
                        counters::QUORUM_STORE_MSG_COUNT
                            .with_label_values(&["QSCoordinator::shutdown"])
                            .inc();
                        // Note: Shutdown is done from the back of the quorum store pipeline to the
                        // front, so senders are always shutdown before receivers. This avoids sending
                        // messages through closed channels during shutdown.
                        // Oneshots that send data in the reverse order of the pipeline must assume that
                        // the receiver could be unavailable during shutdown, and resolve this without
                        // panicking.

                        let (network_listener_shutdown_tx, network_listener_shutdown_rx) =
                            oneshot::channel();
                        match self.quorum_store_msg_tx.push(
                            self.my_peer_id,
                            (
                                self.my_peer_id,
                                VerifiedEvent::Shutdown(network_listener_shutdown_tx),
                            ),
                        ) {
                            Ok(()) => info!("QS: shutdown network listener sent"),
                            Err(err) => panic!("Failed to send to NetworkListener, Err {:?}", err),
                        };
                        network_listener_shutdown_rx
                            .await
                            .expect("Failed to stop NetworkListener");

                        let (batch_generator_shutdown_tx, batch_generator_shutdown_rx) =
                            oneshot::channel();
                        self.batch_generator_cmd_tx
                            .send(BatchGeneratorCommand::Shutdown(batch_generator_shutdown_tx))
                            .await
                            .expect("Failed to send to BatchGenerator");
                        batch_generator_shutdown_rx
                            .await
                            .expect("Failed to stop BatchGenerator");

                        for remote_batch_coordinator_cmd_tx in self.remote_batch_coordinator_cmd_tx
                        {
                            let (
                                remote_batch_coordinator_shutdown_tx,
                                remote_batch_coordinator_shutdown_rx,
                            ) = oneshot::channel();
                            remote_batch_coordinator_cmd_tx
                                .send(BatchCoordinatorCommand::Shutdown(
                                    remote_batch_coordinator_shutdown_tx,
                                ))
                                .await
                                .expect("Failed to send to Remote BatchCoordinator");
                            remote_batch_coordinator_shutdown_rx
                                .await
                                .expect("Failed to stop Remote BatchCoordinator");
                        }

                        let (proof_coordinator_shutdown_tx, proof_coordinator_shutdown_rx) =
                            oneshot::channel();
                        self.proof_coordinator_cmd_tx
                            .send(ProofCoordinatorCommand::Shutdown(
                                proof_coordinator_shutdown_tx,
                            ))
                            .await
                            .expect("Failed to send to ProofCoordinator");
                        proof_coordinator_shutdown_rx
                            .await
                            .expect("Failed to stop ProofCoordinator");

                        let (proof_manager_shutdown_tx, proof_manager_shutdown_rx) =
                            oneshot::channel();
                        self.proof_manager_cmd_tx
                            .send(ProofManagerCommand::Shutdown(proof_manager_shutdown_tx))
                            .await
                            .expect("Failed to send to ProofManager");
                        proof_manager_shutdown_rx
                            .await
                            .expect("Failed to stop ProofManager");

                        ack_tx
                            .send(())
                            .expect("Failed to send shutdown ack from QuorumStore");
                        break;
                    },
                }
            })
        }
    }
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L44-57)
```rust
impl TQuorumStoreCommitNotifier for QuorumStoreCommitNotifier {
    fn notify(&self, block_timestamp: u64, batches: Vec<BatchInfoExt>) {
        let mut tx = self.coordinator_tx.clone();

        if let Err(e) = tx.try_send(CoordinatorCommand::CommitNotification(
            block_timestamp,
            batches,
        )) {
            warn!(
                "CommitNotification failed. Is the epoch shutting down? error: {}",
                e
            );
        }
    }
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L168-207)
```rust
    fn notify_commit(&self, block_timestamp: u64, payloads: Vec<Payload>) {
        self.batch_reader
            .update_certified_timestamp(block_timestamp);

        let batches: Vec<_> = payloads
            .into_iter()
            .flat_map(|payload| match payload {
                Payload::DirectMempool(_) => {
                    unreachable!("InQuorumStore should be used");
                },
                Payload::InQuorumStore(proof_with_status) => proof_with_status
                    .proofs
                    .iter()
                    .map(|proof| proof.info().clone().into())
                    .collect::<Vec<_>>(),
                Payload::InQuorumStoreWithLimit(proof_with_status) => proof_with_status
                    .proof_with_data
                    .proofs
                    .iter()
                    .map(|proof| proof.info().clone().into())
                    .collect::<Vec<_>>(),
                Payload::QuorumStoreInlineHybrid(inline_batches, proof_with_data, _)
                | Payload::QuorumStoreInlineHybridV2(inline_batches, proof_with_data, _) => {
                    inline_batches
                        .iter()
                        .map(|(batch_info, _)| batch_info.clone().into())
                        .chain(
                            proof_with_data
                                .proofs
                                .iter()
                                .map(|proof| proof.info().clone().into()),
                        )
                        .collect::<Vec<_>>()
                },
                Payload::OptQuorumStore(OptQuorumStorePayload::V1(p)) => p.get_all_batch_infos(),
                Payload::OptQuorumStore(OptQuorumStorePayload::V2(p)) => p.get_all_batch_infos(),
            })
            .collect();

        self.commit_notifier.notify(block_timestamp, batches);
```

**File:** consensus/src/quorum_store/quorum_store_builder.rs (L178-178)
```rust
        let (coordinator_tx, coordinator_rx) = futures_channel::mpsc::channel(config.channel_size);
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L1132-1135)
```rust
        let payload = block.payload().cloned();
        let timestamp = block.timestamp_usecs();
        let payload_vec = payload.into_iter().collect();
        payload_manager.notify_commit(timestamp, payload_vec);
```

**File:** consensus/src/epoch_manager.rs (L675-682)
```rust
        if let Some(mut quorum_store_coordinator_tx) = self.quorum_store_coordinator_tx.take() {
            let (ack_tx, ack_rx) = oneshot::channel();
            quorum_store_coordinator_tx
                .send(CoordinatorCommand::Shutdown(ack_tx))
                .await
                .expect("Could not send shutdown indicator to QuorumStore");
            ack_rx.await.expect("Failed to stop QuorumStore");
        }
```

**File:** consensus/src/quorum_store/proof_manager.rs (L88-101)
```rust
    pub(crate) fn handle_commit_notification(
        &mut self,
        block_timestamp: u64,
        batches: Vec<BatchInfoExt>,
    ) {
        trace!(
            "QS: got clean request from execution at block timestamp {}",
            block_timestamp
        );
        self.batch_proof_queue.mark_committed(batches);
        self.batch_proof_queue
            .handle_updated_block_timestamp(block_timestamp);
        self.update_remaining_txns_and_proofs();
    }
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L845-907)
```rust
    // Mark in the hashmap committed PoS, but keep them until they expire
    pub(crate) fn mark_committed(&mut self, batches: Vec<BatchInfoExt>) {
        let start = Instant::now();
        for batch in batches.into_iter() {
            let batch_key = BatchKey::from_info(&batch);
            if let Some(item) = self.items.get(&batch_key) {
                if let Some(ref proof) = item.proof {
                    let insertion_time = item
                        .proof_insertion_time
                        .expect("Insertion time is updated with proof");
                    counters::pos_to_commit(
                        proof.gas_bucket_start(),
                        insertion_time.elapsed().as_secs_f64(),
                    );
                    self.dec_remaining_proofs(&batch.author(), batch.num_txns());
                    counters::GARBAGE_COLLECTED_IN_PROOF_QUEUE_COUNTER
                        .with_label_values(&["committed_proof"])
                        .inc();
                }
                let item = self
                    .items
                    .get_mut(&batch_key)
                    .expect("must exist due to check");

                if item.proof.is_some() {
                    if let Some(ref txn_summaries) = item.txn_summaries {
                        for txn_summary in txn_summaries {
                            if let Some(count) =
                                self.txn_summary_num_occurrences.get_mut(txn_summary)
                            {
                                *count -= 1;
                                if *count == 0 {
                                    self.txn_summary_num_occurrences.remove(txn_summary);
                                }
                            };
                        }
                    }
                } else if !item.is_committed() {
                    counters::GARBAGE_COLLECTED_IN_PROOF_QUEUE_COUNTER
                        .with_label_values(&["committed_batch_without_proof"])
                        .inc();
                }
                // The item is just marked committed for now.
                // When the batch is expired, then it will be removed from items.
                item.mark_committed();
            } else {
                let batch_sort_key = BatchSortKey::from_info(&batch);
                self.expirations
                    .add_item(batch_sort_key.clone(), batch.expiration());
                self.author_to_batches
                    .entry(batch.author())
                    .or_default()
                    .insert(batch_sort_key, batch.clone());
                self.items.insert(batch_key, QueueItem {
                    info: batch,
                    txn_summaries: None,
                    proof: None,
                    proof_insertion_time: None,
                });
            }
        }
        counters::PROOF_QUEUE_COMMIT_DURATION.observe_duration(start.elapsed());
    }
```

**File:** consensus/src/quorum_store/batch_generator.rs (L517-552)
```rust
                        BatchGeneratorCommand::CommitNotification(block_timestamp, batches) => {
                            trace!(
                                "QS: got clean request from execution, block timestamp {}",
                                block_timestamp
                            );
                            // Block timestamp is updated asynchronously, so it may race when it enters state sync.
                            if self.latest_block_timestamp > block_timestamp {
                                continue;
                            }
                            self.latest_block_timestamp = block_timestamp;

                            for (author, batch_id) in batches.iter().map(|b| (b.author(), b.batch_id())) {
                                if self.remove_batch_in_progress(author, batch_id) {
                                    counters::BATCH_IN_PROGRESS_COMMITTED.inc();
                                }
                            }

                            // Cleans up all batches that expire in timestamp <= block_timestamp. This is
                            // safe since clean request must occur only after execution result is certified.
                            for (author, batch_id) in self.batch_expirations.expire(block_timestamp) {
                                if let Some(batch_in_progress) = self.batches_in_progress.get(&(author, batch_id)) {
                                    // If there is an identical batch with higher expiry time, re-insert it.
                                    if batch_in_progress.expiry_time_usecs > block_timestamp {
                                        self.batch_expirations.add_item((author, batch_id), batch_in_progress.expiry_time_usecs);
                                        continue;
                                    }
                                }
                                if self.remove_batch_in_progress(author, batch_id) {
                                    counters::BATCH_IN_PROGRESS_EXPIRED.inc();
                                    debug!(
                                        "QS: logical time based expiration batch w. id {} from batches_in_progress, new size {}",
                                        batch_id,
                                        self.batches_in_progress.len(),
                                    );
                                }
                            }
```
