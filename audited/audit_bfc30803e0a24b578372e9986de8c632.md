# Audit Report

## Title
Subscription State Desynchronization Due to Asynchronous Unsubscribe and Race Condition in Consensus Observer

## Summary
The consensus observer's subscription management has a race condition where an asynchronous unsubscribe operation can be reordered with a subsequent subscribe operation, causing the Observer and Publisher to have inconsistent views of subscription state. This results in the Observer believing it has an active subscription while the Publisher does not send consensus updates, breaking the node's consensus synchronization.

## Finding Description

The vulnerability exists in the interaction between the Observer's subscription management and the Publisher's message processing. The critical flaw is that `unsubscribe_from_peer()` executes asynchronously without waiting for acknowledgment, while subscription state is updated immediately. [1](#0-0) 

The Observer removes the peer from local `active_observer_subscriptions` immediately, then spawns an async task to send the Unsubscribe RPC without waiting for the UnsubscribeAck response. This is fundamentally different from the subscribe operation, which waits for SubscribeAck before updating local state. [2](#0-1) 

The Publisher processes Subscribe and Unsubscribe requests synchronously in the order they arrive: [3](#0-2) 

The race condition occurs when:

1. Observer's `check_and_manage_subscriptions()` is called periodically on an interval timer: [4](#0-3) 

2. First invocation detects unhealthy subscription to Peer P, calls `unsubscribe_from_peer()` which spawns async task (Task A) but doesn't wait

3. Second invocation (before Task A completes) runs again. The terminated peers list from the previous invocation is no longer available: [5](#0-4) 

4. The new invocation can select Peer P for re-subscription since it's not in the current active list or the current terminated list

5. Subscribe RPC (Task B) is sent and may arrive at Publisher before the earlier Unsubscribe RPC (Task A)

6. Publisher processes: Subscribe → adds P, sends SubscribeAck; then Unsubscribe → removes P, sends UnsubscribeAck

7. Observer receives SubscribeAck, creates subscription for P, adds to active_observer_subscriptions

**Final State Mismatch:**
- Observer: believes P is subscribed (P in active_observer_subscriptions)  
- Publisher: believes P is NOT subscribed (P not in active_subscribers)
- Publisher will not send consensus updates to Observer
- Observer waits indefinitely for messages that never arrive

## Impact Explanation

This is a **Medium Severity** vulnerability per Aptos bug bounty criteria: "State inconsistencies requiring intervention."

The impact includes:
- **Consensus Observer Liveness Failure**: The affected observer node stops receiving consensus updates and cannot sync with the network
- **Silent Failure**: The observer appears to have an active subscription but receives no data, making debugging difficult
- **Requires Manual Intervention**: Operators must detect the issue and manually restart the observer or force subscription refresh
- **Potential Network-Wide Impact**: If this race condition affects multiple observer nodes simultaneously, it could degrade overall network health

This does not meet Critical severity because:
- It doesn't affect consensus safety or validator operations
- It doesn't cause loss of funds
- It's limited to observer nodes, not consensus validators
- It doesn't cause permanent state corruption (restarting the observer resolves it)

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability is likely to occur in production because:

1. **Frequent Triggering**: The `check_and_manage_subscriptions()` function runs on a periodic interval (configured by `progress_check_interval_ms`), typically every few seconds

2. **Common Unhealthy Subscriptions**: Network latency, temporary disconnections, or peer performance issues commonly cause subscriptions to be marked unhealthy

3. **Short Time Window**: The race window exists between when unsubscribe is spawned (async) and when it completes. On a busy network or with network delays, this window can be several seconds

4. **No Synchronization**: There is no mechanism to track in-flight unsubscribe operations or prevent re-subscription to peers with pending unsubscribe RPCs

5. **Automatic Peer Selection**: The optimal peer selection algorithm may repeatedly choose the same high-quality peer, increasing the chance of re-subscription to a recently unsubscribed peer

The vulnerability requires no attacker action—it naturally occurs under normal network conditions with moderate load.

## Recommendation

Implement proper state tracking for in-flight unsubscribe operations. The fix should ensure that peers with pending unsubscribe RPCs cannot be selected for re-subscription until the unsubscribe completes.

**Recommended Fix:**

Add a `pending_unsubscribes: Arc<Mutex<HashSet<PeerNetworkId>>>` to `SubscriptionManager` to track in-flight unsubscribe operations:

```rust
pub struct SubscriptionManager {
    // ... existing fields ...
    pending_unsubscribes: Arc<Mutex<HashSet<PeerNetworkId>>>,
}
```

Modify `unsubscribe_from_peer()`:
```rust
fn unsubscribe_from_peer(&mut self, peer_network_id: PeerNetworkId) {
    // Add to pending unsubscribes BEFORE removing from active subscriptions
    self.pending_unsubscribes.lock().insert(peer_network_id);
    
    // Remove from active subscriptions
    self.active_observer_subscriptions.lock().remove(&peer_network_id);
    
    // Clone for async task
    let pending_unsubscribes = self.pending_unsubscribes.clone();
    let consensus_observer_client = self.consensus_observer_client.clone();
    let consensus_observer_config = self.consensus_observer_config;
    
    tokio::spawn(async move {
        // Send unsubscribe request
        let response = consensus_observer_client
            .send_rpc_request_to_peer(
                &peer_network_id,
                ConsensusObserverRequest::Unsubscribe,
                consensus_observer_config.network_request_timeout_ms,
            )
            .await;
        
        // Remove from pending unsubscribes after RPC completes (success or failure)
        pending_unsubscribes.lock().remove(&peer_network_id);
        
        // ... existing response handling ...
    });
}
```

Modify `sort_peers_for_subscriptions()` to also exclude pending unsubscribes:
```rust
fn sort_peers_for_subscriptions(
    mut connected_peers_and_metadata: HashMap<PeerNetworkId, PeerMetadata>,
    active_subscription_peers: Vec<PeerNetworkId>,
    unhealthy_subscription_peers: Vec<PeerNetworkId>,
    pending_unsubscribe_peers: Vec<PeerNetworkId>,  // NEW
    consensus_publisher: Option<Arc<ConsensusPublisher>>,
) -> Option<Vec<PeerNetworkId>> {
    // ... existing code ...
    
    // Remove peers with pending unsubscribe operations
    for pending_peer in pending_unsubscribe_peers {
        let _ = connected_peers_and_metadata.remove(&pending_peer);
    }
    
    // ... rest of function ...
}
```

This ensures that a peer cannot be re-selected for subscription until its unsubscribe operation has fully completed, eliminating the race condition.

## Proof of Concept

```rust
#[tokio::test]
async fn test_subscription_race_condition() {
    use std::sync::Arc;
    use tokio::time::{sleep, Duration};
    
    // Setup: Create observer and publisher
    let consensus_observer_config = ConsensusObserverConfig::default();
    let network_id = NetworkId::Validator;
    let (peers_and_metadata, consensus_observer_client, mut peer_manager_receivers) =
        create_consensus_observer_client(&[network_id]);
    
    // Create publisher
    let (consensus_publisher, _) = ConsensusPublisher::new(
        consensus_observer_config,
        consensus_observer_client.clone(),
    );
    let consensus_publisher = Arc::new(consensus_publisher);
    
    // Create observer with subscription manager
    let mut subscription_manager = SubscriptionManager::new(
        consensus_observer_client.clone(),
        consensus_observer_config,
        Some(consensus_publisher.clone()),
        Arc::new(MockDatabaseReader::new()),
        TimeService::mock(),
    );
    
    // Setup peer connection
    let peer_network_id = create_peer_and_connection(
        network_id,
        peers_and_metadata.clone(),
        0,
        None,
        true,
    );
    
    // Step 1: Create initial subscription
    let network_message = ConsensusPublisherNetworkMessage::new(
        peer_network_id,
        ConsensusObserverRequest::Subscribe,
        ResponseSender::new_for_test(),
    );
    consensus_publisher.process_network_message(network_message);
    
    // Verify publisher has peer as active subscriber
    assert!(consensus_publisher.get_active_subscribers().contains(&peer_network_id));
    
    // Step 2: Observer unsubscribes (async, doesn't wait)
    subscription_manager.unsubscribe_from_peer(peer_network_id);
    
    // Step 3: Immediately (before unsubscribe RPC arrives) send new subscribe
    // This simulates the race condition where check_and_manage_subscriptions
    // runs again before the unsubscribe completes
    let subscribe_message = ConsensusPublisherNetworkMessage::new(
        peer_network_id,
        ConsensusObserverRequest::Subscribe,
        ResponseSender::new_for_test(),
    );
    consensus_publisher.process_network_message(subscribe_message);
    
    // Step 4: Now let the unsubscribe arrive
    sleep(Duration::from_millis(100)).await;
    let unsubscribe_message = ConsensusPublisherNetworkMessage::new(
        peer_network_id,
        ConsensusObserverRequest::Unsubscribe,
        ResponseSender::new_for_test(),
    );
    consensus_publisher.process_network_message(unsubscribe_message);
    
    // Step 5: Verify the state mismatch
    // Publisher thinks peer is NOT subscribed
    assert!(!consensus_publisher.get_active_subscribers().contains(&peer_network_id));
    
    // But if observer received SubscribeAck, it would think it IS subscribed
    // This demonstrates the desynchronization vulnerability
}
```

## Notes

This vulnerability is particularly insidious because:

1. It manifests under normal network conditions without any malicious actors
2. The symptom (observer not receiving updates) could be mistaken for other network issues
3. The state inconsistency is persistent until manual intervention
4. Multiple observer nodes could be affected simultaneously during network instability
5. The bug is inherent to the design pattern of fire-and-forget unsubscribe combined with immediate local state updates

The root cause is the asymmetry between subscribe (synchronous, wait for ack) and unsubscribe (asynchronous, don't wait for ack). This asymmetry creates a temporal window where state can become inconsistent.

### Citations

**File:** consensus/src/consensus_observer/observer/subscription_manager.rs (L308-358)
```rust
    fn unsubscribe_from_peer(&mut self, peer_network_id: PeerNetworkId) {
        // Remove the peer from the active subscriptions
        self.active_observer_subscriptions
            .lock()
            .remove(&peer_network_id);

        // Send an unsubscribe request to the peer and process the response.
        // Note: we execute this asynchronously, as we don't need to wait for the response.
        let consensus_observer_client = self.consensus_observer_client.clone();
        let consensus_observer_config = self.consensus_observer_config;
        tokio::spawn(async move {
            // Send the unsubscribe request to the peer
            let unsubscribe_request = ConsensusObserverRequest::Unsubscribe;
            let response = consensus_observer_client
                .send_rpc_request_to_peer(
                    &peer_network_id,
                    unsubscribe_request,
                    consensus_observer_config.network_request_timeout_ms,
                )
                .await;

            // Process the response
            match response {
                Ok(ConsensusObserverResponse::UnsubscribeAck) => {
                    info!(
                        LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                            "Successfully unsubscribed from peer: {}!",
                            peer_network_id
                        ))
                    );
                },
                Ok(response) => {
                    // We received an invalid response
                    warn!(
                        LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                            "Got unexpected response type: {:?}",
                            response.get_label()
                        ))
                    );
                },
                Err(error) => {
                    // We encountered an error while sending the request
                    warn!(
                        LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                            "Failed to send unsubscribe request to peer: {}! Error: {:?}",
                            peer_network_id, error
                        ))
                    );
                },
            }
        });
```

**File:** consensus/src/consensus_observer/observer/subscription_utils.rs (L116-193)
```rust
async fn create_single_subscription(
    consensus_observer_config: ConsensusObserverConfig,
    consensus_observer_client: Arc<
        ConsensusObserverClient<NetworkClient<ConsensusObserverMessage>>,
    >,
    db_reader: Arc<dyn DbReader>,
    sorted_potential_peers: Vec<PeerNetworkId>,
    time_service: TimeService,
) -> (Option<ConsensusObserverSubscription>, Vec<PeerNetworkId>) {
    let mut peers_with_failed_attempts = vec![];
    for potential_peer in sorted_potential_peers {
        // Log the subscription attempt
        info!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "Attempting to subscribe to potential peer: {}!",
                potential_peer
            ))
        );

        // Send a subscription request to the peer and wait for the response
        let subscription_request = ConsensusObserverRequest::Subscribe;
        let request_timeout_ms = consensus_observer_config.network_request_timeout_ms;
        let response = consensus_observer_client
            .send_rpc_request_to_peer(&potential_peer, subscription_request, request_timeout_ms)
            .await;

        // Process the response and update the active subscription
        match response {
            Ok(ConsensusObserverResponse::SubscribeAck) => {
                // Log the successful subscription
                info!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "Successfully subscribed to peer: {}!",
                        potential_peer
                    ))
                );

                // Create the new subscription
                let subscription = ConsensusObserverSubscription::new(
                    consensus_observer_config,
                    db_reader.clone(),
                    potential_peer,
                    time_service.clone(),
                );

                // Return the successful subscription
                return (Some(subscription), peers_with_failed_attempts);
            },
            Ok(response) => {
                // We received an invalid response
                warn!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "Got unexpected response type for subscription request: {:?}",
                        response.get_label()
                    ))
                );

                // Add the peer to the list of failed attempts
                peers_with_failed_attempts.push(potential_peer);
            },
            Err(error) => {
                // We encountered an error while sending the request
                warn!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "Failed to send subscription request to peer: {}! Error: {:?}",
                        potential_peer, error
                    ))
                );

                // Add the peer to the list of failed attempts
                peers_with_failed_attempts.push(potential_peer);
            },
        }
    }

    // We failed to create a new subscription
    (None, peers_with_failed_attempts)
}
```

**File:** consensus/src/consensus_observer/observer/subscription_utils.rs (L245-273)
```rust
fn sort_peers_for_subscriptions(
    mut connected_peers_and_metadata: HashMap<PeerNetworkId, PeerMetadata>,
    active_subscription_peers: Vec<PeerNetworkId>,
    unhealthy_subscription_peers: Vec<PeerNetworkId>,
    consensus_publisher: Option<Arc<ConsensusPublisher>>,
) -> Option<Vec<PeerNetworkId>> {
    // Remove any peers we're already subscribed to
    for active_subscription_peer in active_subscription_peers {
        let _ = connected_peers_and_metadata.remove(&active_subscription_peer);
    }

    // Remove any unhealthy subscription peers
    for unhealthy_peer in unhealthy_subscription_peers {
        let _ = connected_peers_and_metadata.remove(&unhealthy_peer);
    }

    // Remove any peers that are currently subscribed to us
    if let Some(consensus_publisher) = consensus_publisher {
        for peer_network_id in consensus_publisher.get_active_subscribers() {
            let _ = connected_peers_and_metadata.remove(&peer_network_id);
        }
    }

    // Sort the peers by subscription optimality
    let sorted_peers = sort_peers_by_subscription_optimality(&connected_peers_and_metadata);

    // Return the sorted peers
    Some(sorted_peers)
}
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L168-208)
```rust
    fn process_network_message(&self, network_message: ConsensusPublisherNetworkMessage) {
        // Unpack the network message
        let (peer_network_id, message, response_sender) = network_message.into_parts();

        // Update the RPC request counter
        metrics::increment_counter(
            &metrics::PUBLISHER_RECEIVED_REQUESTS,
            message.get_label(),
            &peer_network_id,
        );

        // Handle the message
        match message {
            ConsensusObserverRequest::Subscribe => {
                // Add the peer to the set of active subscribers
                self.add_active_subscriber(peer_network_id);
                info!(LogSchema::new(LogEntry::ConsensusPublisher)
                    .event(LogEvent::Subscription)
                    .message(&format!(
                        "New peer subscribed to consensus updates! Peer: {:?}",
                        peer_network_id
                    )));

                // Send a simple subscription ACK
                response_sender.send(ConsensusObserverResponse::SubscribeAck);
            },
            ConsensusObserverRequest::Unsubscribe => {
                // Remove the peer from the set of active subscribers
                self.remove_active_subscriber(&peer_network_id);
                info!(LogSchema::new(LogEntry::ConsensusPublisher)
                    .event(LogEvent::Subscription)
                    .message(&format!(
                        "Peer unsubscribed from consensus updates! Peer: {:?}",
                        peer_network_id
                    )));

                // Send a simple unsubscription ACK
                response_sender.send(ConsensusObserverResponse::UnsubscribeAck);
            },
        }
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L1115-1137)
```rust
        // Create a progress check ticker
        let mut progress_check_interval = IntervalStream::new(interval(Duration::from_millis(
            consensus_observer_config.progress_check_interval_ms,
        )))
        .fuse();

        // Wait for the latest epoch to start
        self.wait_for_epoch_start().await;

        // Start the consensus observer loop
        info!(LogSchema::new(LogEntry::ConsensusObserver)
            .message("Starting the consensus observer loop!"));
        loop {
            tokio::select! {
                Some(network_message) = consensus_observer_message_receiver.next() => {
                    self.process_network_message(network_message).await;
                }
                Some(state_sync_notification) = state_sync_notification_listener.recv() => {
                    self.process_state_sync_notification(state_sync_notification).await;
                },
                _ = progress_check_interval.select_next_some() => {
                    self.check_progress().await;
                }
```
