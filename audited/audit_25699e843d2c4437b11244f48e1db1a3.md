# Audit Report

## Title
Storage Leak: Orphaned Stale State Value Index Entries After AIP-97 Sharding Migration

## Summary
After migrating from non-sharded to sharded storage mode (AIP-97), stale state value index entries from the legacy `StaleStateValueIndexSchema` remain unpruned indefinitely. The pruner exclusively processes the new `StaleStateValueIndexByKeyHashSchema` when sharding is enabled, leaving old metadata database entries and their associated state values to accumulate, wasting storage space. [1](#0-0) 

## Finding Description
The Aptos storage layer maintains two separate column families for tracking stale state values:

1. **`StaleStateValueIndexSchema`**: Used when sharding is disabled, stores `(stale_since_version, version, state_key)` in the metadata database
2. **`StaleStateValueIndexByKeyHashSchema`**: Used when sharding is enabled, stores `(stale_since_version, version, state_key_hash)` in shard databases [2](#0-1) [3](#0-2) 

The write path uses an **exclusive** conditional: when writing stale index entries, the system writes to ONLY ONE schema based on the `enable_sharding` flag: [4](#0-3) 

The pruner follows the same exclusive pattern. When sharding is enabled, the metadata pruner iterates through `StaleStateValueIndexByKeyHashSchema` (but delegates actual deletion to shard pruners), completely ignoring any entries in the legacy `StaleStateValueIndexSchema`: [5](#0-4) 

When sharding is disabled, the pruner deletes from `StaleStateValueIndexSchema` at lines 62-63. However, when sharding is enabled (lines 35-50), it only iterates through the new schema without any cleanup of the old schema.

**The Vulnerability:** During the AIP-97 migration from non-sharded to sharded mode:
1. Nodes accumulate stale index entries in `StaleStateValueIndexSchema` (metadata DB)
2. After enabling sharding per mainnet/testnet requirements, all new entries go to `StaleStateValueIndexByKeyHashSchema`
3. The pruner only processes `StaleStateValueIndexByKeyHashSchema` when sharding is enabled
4. Old entries in `StaleStateValueIndexSchema` remain forever, along with their associated `StateValueSchema` entries
5. Storage usage grows indefinitely with orphaned data [6](#0-5) 

## Impact Explanation
**Severity: Medium** (Storage Waste / State Inconsistency Requiring Intervention)

This issue causes unbounded storage growth on mainnet and testnet nodes that migrated from non-sharded to sharded storage:

- **Affected Nodes**: All mainnet/testnet nodes that performed the AIP-97 migration
- **Storage Impact**: Orphaned stale index entries and state values accumulate indefinitely in the metadata database
- **Operational Impact**: Increased disk usage, slower database operations on the metadata DB, potential performance degradation
- **No Consensus Impact**: Does not affect state correctness, consensus, or transaction execution
- **Recovery**: Requires manual intervention or database migration tooling to cleanup orphaned entries

While this does not directly threaten consensus or cause fund loss, it represents a state management inefficiency that violates the storage pruning invariant and requires operational intervention to resolve.

## Likelihood Explanation
**Likelihood: High** for nodes that migrated from non-sharded to sharded mode.

- **Occurrence**: Automatic and guaranteed for any node that performed the AIP-97 migration
- **Affected Population**: All mainnet/testnet nodes (sharding is mandatory per config validation)
- **No Attacker Required**: This is an operational issue, not an exploit
- **Detection**: Operators can observe growing metadata database size despite pruning being enabled
- **Workaround**: No automatic cleanup mechanism exists; requires manual database maintenance

## Recommendation
Implement a **migration cleanup phase** that purges legacy `StaleStateValueIndexSchema` entries when sharding is first enabled:

```rust
// In StateKvMetadataPruner::prune() or during initialization:

if self.state_kv_db.enabled_sharding() {
    // Check if legacy schema needs cleanup (one-time migration)
    if self.needs_legacy_cleanup() {
        self.cleanup_legacy_stale_indices(target_version)?;
    }
    
    // ... existing sharded pruning logic ...
} else {
    // ... existing non-sharded pruning logic ...
}

fn cleanup_legacy_stale_indices(&self, target_version: Version) -> Result<()> {
    let mut batch = SchemaBatch::new();
    let mut iter = self.state_kv_db.metadata_db()
        .iter::<StaleStateValueIndexSchema>()?;
    iter.seek_to_first();
    
    for item in iter {
        let (index, _) = item?;
        if index.stale_since_version <= target_version {
            batch.delete::<StaleStateValueIndexSchema>(&index)?;
            batch.delete::<StateValueSchema>(&(index.state_key, index.version))?;
        }
    }
    
    // Mark cleanup as complete
    batch.put::<DbMetadataSchema>(
        &DbMetadataKey::LegacyStaleIndexCleanupComplete,
        &DbMetadataValue::Bool(true)
    )?;
    
    self.state_kv_db.metadata_db().write_schemas(batch)
}
```

Alternatively, provide a database migration tool that operators run manually during the AIP-97 migration process.

## Proof of Concept

**Rust Reproduction Steps:**

1. Initialize a node with `enable_storage_sharding = false`
2. Execute transactions to generate state updates and stale entries
3. Let the pruner run to ensure `StaleStateValueIndexSchema` gets populated
4. Check metadata DB size: `du -sh metadata_db/`
5. Stop the node and enable sharding: `enable_storage_sharding = true`
6. Restart and execute more transactions
7. Let the pruner run for the pruning window duration
8. Query both schemas:
   ```rust
   // Old schema (non-sharded) - should be empty but isn't
   let old_count = metadata_db.iter::<StaleStateValueIndexSchema>()?.count();
   
   // New schema (sharded) - gets pruned correctly  
   let new_count = db_shard(0).iter::<StaleStateValueIndexByKeyHashSchema>()?.count();
   
   assert_eq!(old_count, 0, "Legacy stale indices should be cleaned up");
   ```
9. Observe that `old_count > 0`, proving orphaned entries remain
10. Verify metadata DB size does not decrease despite pruning being active

This demonstrates that the two index schemas diverge after migration, with the legacy schema accumulating unpruned entries indefinitely.

### Citations

**File:** storage/aptosdb/src/schema/mod.rs (L52-54)
```rust
pub const STALE_STATE_VALUE_INDEX_CF_NAME: ColumnFamilyName = "stale_state_value_index";
pub const STALE_STATE_VALUE_INDEX_BY_KEY_HASH_CF_NAME: ColumnFamilyName =
    "stale_state_value_index_by_key_hash";
```

**File:** storage/aptosdb/src/schema/stale_state_value_index/mod.rs (L1-39)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

//! This module defines the physical storage schema for information related to outdated state
//! values, which are ready to be pruned after being old enough.
//!
//! An index entry in this data set has 3 pieces of information:
//!     1. The version since which a state value (in another data set) becomes stale, meaning,
//! replaced by an updated value.
//!     2. The version this state value was updated identified by the state key.
//!     3. The state_key to identify the stale state value.
//!
//! ```text
//! |<-------------------key------------------->|
//! | stale_since_version | version | state_key |
//! ```
//!
//! `stale_since_version` is serialized in big endian so that records in RocksDB will be in order of
//! its numeric value.

use crate::schema::{ensure_slice_len_eq, ensure_slice_len_gt, STALE_STATE_VALUE_INDEX_CF_NAME};
use anyhow::Result;
use aptos_schemadb::{
    define_schema,
    schema::{KeyCodec, SeekKeyCodec, ValueCodec},
};
use aptos_types::{
    state_store::{state_key::StateKey, state_value::StaleStateValueIndex},
    transaction::Version,
};
use byteorder::{BigEndian, ReadBytesExt, WriteBytesExt};
use std::{io::Write, mem::size_of};

define_schema!(
    StaleStateValueIndexSchema,
    StaleStateValueIndex,
    (),
    STALE_STATE_VALUE_INDEX_CF_NAME
);
```

**File:** storage/aptosdb/src/schema/stale_state_value_index_by_key_hash/mod.rs (L1-37)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

//! This module defines the physical storage schema for information related to outdated state
//! values, which are ready to be pruned after being old enough.
//!
//! An index entry in this data set has 3 pieces of information:
//!     1. The version since which a state value (in another data set) becomes stale, meaning,
//! replaced by an updated value.
//!     2. The version this state value was updated identified by the state key.
//!     3. The state_key to identify the stale state value.
//!
//! ```text
//! |<-------------------key------------------------>|
//! | stale_since_version | version | state_key_hash |
//! ```
//!
//! `stale_since_version` is serialized in big endian so that records in RocksDB will be in order of
//! its numeric value.

use crate::schema::{ensure_slice_len_eq, STALE_STATE_VALUE_INDEX_BY_KEY_HASH_CF_NAME};
use anyhow::Result;
use aptos_crypto::HashValue;
use aptos_schemadb::{
    define_schema,
    schema::{KeyCodec, SeekKeyCodec, ValueCodec},
};
use aptos_types::{state_store::state_value::StaleStateValueByKeyHashIndex, transaction::Version};
use byteorder::{BigEndian, ReadBytesExt, WriteBytesExt};
use std::{io::Write, mem::size_of};

define_schema!(
    StaleStateValueIndexByKeyHashSchema,
    StaleStateValueByKeyHashIndex,
    (),
    STALE_STATE_VALUE_INDEX_BY_KEY_HASH_CF_NAME
);
```

**File:** storage/aptosdb/src/state_store/mod.rs (L985-1015)
```rust
    fn put_state_kv_index(
        batch: &mut NativeBatch,
        enable_sharding: bool,
        stale_since_version: Version,
        version: Version,
        key: &StateKey,
    ) {
        if enable_sharding {
            batch
                .put::<StaleStateValueIndexByKeyHashSchema>(
                    &StaleStateValueByKeyHashIndex {
                        stale_since_version,
                        version,
                        state_key_hash: key.hash(),
                    },
                    &(),
                )
                .unwrap();
        } else {
            batch
                .put::<StaleStateValueIndexSchema>(
                    &StaleStateValueIndex {
                        stale_since_version,
                        version,
                        state_key: (*key).clone(),
                    },
                    &(),
                )
                .unwrap();
        }
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs (L28-73)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();

        if self.state_kv_db.enabled_sharding() {
            let num_shards = self.state_kv_db.num_shards();
            // NOTE: This can be done in parallel if it becomes the bottleneck.
            for shard_id in 0..num_shards {
                let mut iter = self
                    .state_kv_db
                    .db_shard(shard_id)
                    .iter::<StaleStateValueIndexByKeyHashSchema>()?;
                iter.seek(&current_progress)?;
                for item in iter {
                    let (index, _) = item?;
                    if index.stale_since_version > target_version {
                        break;
                    }
                }
            }
        } else {
            let mut iter = self
                .state_kv_db
                .metadata_db()
                .iter::<StaleStateValueIndexSchema>()?;
            iter.seek(&current_progress)?;
            for item in iter {
                let (index, _) = item?;
                if index.stale_since_version > target_version {
                    break;
                }
                batch.delete::<StaleStateValueIndexSchema>(&index)?;
                batch.delete::<StateValueSchema>(&(index.state_key, index.version))?;
            }
        }

        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        self.state_kv_db.metadata_db().write_schemas(batch)
    }
```

**File:** config/src/config/storage_config.rs (L664-668)
```rust
            if (chain_id.is_testnet() || chain_id.is_mainnet())
                && config_yaml["rocksdb_configs"]["enable_storage_sharding"].as_bool() != Some(true)
            {
                panic!("Storage sharding (AIP-97) is not enabled in node config. Please follow the guide to migration your node, and set storage.rocksdb_configs.enable_storage_sharding to true explicitly in your node config. https://aptoslabs.notion.site/DB-Sharding-Migration-Public-Full-Nodes-1978b846eb7280b29f17ceee7d480730");
            }
```
