# Audit Report

## Title
Panic-Based Process Termination via Closed Channel in Secure Network Controller Causes Validator/Executor Node Crash

## Summary
The secure network controller's message handling implementation uses `.unwrap()` on channel send operations without handling channel closure scenarios. When a message receiver is dropped (during shutdown, errors, or component restarts), subsequent message deliveries trigger a panic that terminates the entire process via the global panic handler, causing complete node unavailability.

## Finding Description

The vulnerability exists in two critical locations where messages are forwarded to registered handlers:

**Location 1: Inbound Handler (Direct Send)** [1](#0-0) 

**Location 2: gRPC Service Handler (Network Messages)** [2](#0-1) 

Both implementations call `handler.send(message).unwrap()` or `handler.send(msg).unwrap()` on crossbeam channels. The handlers are stored as `Sender<Message>` instances in a HashMap. [3](#0-2) 

These channels are created as unbounded: [4](#0-3) 

**Attack Propagation Path:**

1. **Channel Closure Scenario**: When a component (e.g., `RemoteCoordinatorClient`) is dropped or encounters an error, its receiver is dropped, closing the channel. [5](#0-4) 

2. **Message Arrival**: A message arrives for the closed channel, either via:
   - External gRPC call to `simple_msg_exchange()`
   - Internal self-send from outbound handler [6](#0-5) 

3. **Panic Trigger**: The `send()` operation on a closed channel returns `Err`, causing `.unwrap()` to panic.

4. **Process Termination**: The global panic handler is invoked, which calls `process::exit(12)`: [7](#0-6) 

5. **Complete Node Failure**: The entire executor shard or validator process terminates. [8](#0-7) 

**Broken Invariants:**
- **Network Resilience**: The system should gracefully handle component failures without cascading crashes
- **Error Handling**: Network errors should return proper error codes, not panic
- **Consensus Safety**: Validators must maintain liveness; process termination breaks this guarantee

## Impact Explanation

**Critical Severity** - This meets multiple critical impact categories per the Aptos bug bounty:

1. **Total Loss of Liveness/Network Availability**: When an executor shard process terminates, the validator cannot execute blocks or participate in consensus. The validator becomes non-functional until manual restart.

2. **Consensus Disruption**: If multiple validators are affected simultaneously (e.g., during a coordinated attack or widespread component restart), consensus can stall, requiring intervention.

3. **Non-Recoverable Without Restart**: The process exit is permanent; automatic recovery is impossible without external process management.

The vulnerability is particularly severe for remote executor deployments where executor shards run as separate processes coordinating with validator nodes. A single malicious message to a shard experiencing a component error can crash the entire shard process, crippling block execution.

## Likelihood Explanation

**High Likelihood** - Multiple realistic scenarios trigger this vulnerability:

1. **Shutdown Race Conditions**: During normal shutdown sequences, components may drop receivers before the network controller stops accepting messages. The shutdown implementation does not wait for in-flight messages: [9](#0-8) 

2. **Component Errors**: If an executor component encounters an error and drops its receiver, all subsequent messages for that component crash the process.

3. **Normal Operation**: The outbound handler processes messages in a loop. If it calls `send_incoming_message_to_handler()` for a self-send after the receiver is dropped, the panic terminates the entire outbound processing loop, affecting all message types.

4. **No Recovery Mechanism**: There is no error handling, retry logic, or graceful degradation. The first failed send terminates the process.

**Attacker Requirements:**
- Send a network message to a target node
- Timing: Message arrives after receiver is dropped (e.g., during component restart/error)
- No special privileges required
- Can be triggered unintentionally during normal operations

## Recommendation

Replace `.unwrap()` with proper error handling that logs failures and returns errors instead of panicking:

**Fix for inbound_handler.rs:**
```rust
pub fn send_incoming_message_to_handler(&self, message_type: &MessageType, message: Message) {
    if let Some(handler) = self.inbound_handlers.lock().unwrap().get(message_type) {
        if let Err(e) = handler.send(message) {
            warn!(
                "Failed to send message to handler for type {:?}: {}. Handler may be closed.",
                message_type, e
            );
        }
    } else {
        warn!("No handler registered for message type: {:?}", message_type);
    }
}
```

**Fix for grpc_network_service/mod.rs:**
```rust
async fn simple_msg_exchange(
    &self,
    request: Request<NetworkMessage>,
) -> Result<Response<Empty>, Status> {
    // ... existing code ...
    
    if let Some(handler) = self.inbound_handlers.lock().unwrap().get(&message_type) {
        if let Err(e) = handler.send(msg) {
            error!(
                "Failed to send message from {:?} for type {:?}: {}",
                remote_addr, message_type, e
            );
            return Err(Status::internal("Handler channel closed"));
        }
    } else {
        error!(
            "No handler registered for sender: {:?} and msg type {:?}",
            remote_addr, message_type
        );
        return Err(Status::not_found("No handler registered"));
    }
    Ok(Response::new(Empty {}))
}
```

Additionally, implement graceful shutdown coordination to ensure receivers remain alive until all in-flight messages are processed.

## Proof of Concept

```rust
// Create a test that demonstrates the panic
#[test]
#[should_panic(expected = "called `Result::unwrap()` on an `Err` value")]
fn test_closed_channel_panic() {
    use aptos_secure_net::network_controller::{Message, MessageType, NetworkController};
    use std::{
        net::{IpAddr, Ipv4Addr, SocketAddr},
        thread,
        time::Duration,
    };
    
    let server_port = aptos_config::utils::get_available_port();
    let server_addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), server_port);
    
    let mut network_controller = 
        NetworkController::new("test".to_string(), server_addr, 1000);
    
    // Create inbound channel and immediately drop the receiver
    let message_type = "test_type".to_string();
    {
        let _receiver = network_controller.create_inbound_channel(message_type.clone());
        // receiver is dropped here, closing the channel
    }
    
    network_controller.start();
    thread::sleep(Duration::from_millis(100));
    
    // Attempt to send a message - this will panic and crash the process
    network_controller
        .inbound_handler
        .lock()
        .unwrap()
        .send_incoming_message_to_handler(
            &MessageType::new(message_type),
            Message::new(vec![1, 2, 3]),
        );
    
    // With the bug, execution never reaches here - process exits
}
```

**Real-world reproduction:**
1. Deploy an Aptos validator with remote executor shards
2. Start an executor shard process
3. Simulate a component error that drops a receiver (e.g., kill the executor thread)
4. Send a gRPC message to the shard for the closed handler type
5. Observe process termination via panic/exit

### Citations

**File:** secure/net/src/network_controller/inbound_handler.rs (L17-22)
```rust
pub struct InboundHandler {
    service: String,
    listen_addr: SocketAddr,
    rpc_timeout_ms: u64,
    inbound_handlers: Arc<Mutex<HashMap<MessageType, Sender<Message>>>>,
}
```

**File:** secure/net/src/network_controller/inbound_handler.rs (L66-74)
```rust
    pub fn send_incoming_message_to_handler(&self, message_type: &MessageType, message: Message) {
        // Check if there is a registered handler for the sender
        if let Some(handler) = self.inbound_handlers.lock().unwrap().get(message_type) {
            // Send the message to the registered handler
            handler.send(message).unwrap();
        } else {
            warn!("No handler registered for message type: {:?}", message_type);
        }
    }
```

**File:** secure/net/src/grpc_network_service/mod.rs (L105-113)
```rust
        if let Some(handler) = self.inbound_handlers.lock().unwrap().get(&message_type) {
            // Send the message to the registered handler
            handler.send(msg).unwrap();
        } else {
            error!(
                "No handler registered for sender: {:?} and msg type {:?}",
                remote_addr, message_type
            );
        }
```

**File:** secure/net/src/network_controller/mod.rs (L128-137)
```rust
    pub fn create_inbound_channel(&mut self, message_type: String) -> Receiver<Message> {
        let (inbound_sender, inbound_receiver) = unbounded();

        self.inbound_handler
            .lock()
            .unwrap()
            .register_handler(message_type, inbound_sender);

        inbound_receiver
    }
```

**File:** secure/net/src/network_controller/mod.rs (L155-166)
```rust
    pub fn shutdown(&mut self) {
        info!("Shutting down network controller at {}", self.listen_addr);
        if let Some(shutdown_signal) = self.inbound_server_shutdown_tx.take() {
            shutdown_signal.send(()).unwrap();
        }

        if let Some(shutdown_signal) = self.outbound_task_shutdown_tx.take() {
            shutdown_signal.send(Message::new(vec![])).unwrap_or_else(|_| {
                warn!("Failed to send shutdown signal to outbound task; probably already shutdown");
            })
        }
    }
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L19-47)
```rust
pub struct RemoteCoordinatorClient {
    state_view_client: Arc<RemoteStateViewClient>,
    command_rx: Receiver<Message>,
    result_tx: Sender<Message>,
    shard_id: ShardId,
}

impl RemoteCoordinatorClient {
    pub fn new(
        shard_id: ShardId,
        controller: &mut NetworkController,
        coordinator_address: SocketAddr,
    ) -> Self {
        let execute_command_type = format!("execute_command_{}", shard_id);
        let execute_result_type = format!("execute_result_{}", shard_id);
        let command_rx = controller.create_inbound_channel(execute_command_type);
        let result_tx =
            controller.create_outbound_channel(coordinator_address, execute_result_type);

        let state_view_client =
            RemoteStateViewClient::new(shard_id, controller, coordinator_address);

        Self {
            state_view_client: Arc::new(state_view_client),
            command_rx,
            result_tx,
            shard_id,
        }
    }
```

**File:** secure/net/src/network_controller/outbound_handler.rs (L147-153)
```rust
            if remote_addr == socket_addr {
                // If the remote address is the same as the local address, then we are sending a message to ourselves
                // so we should just pass it to the inbound handler
                inbound_handler
                    .lock()
                    .unwrap()
                    .send_incoming_message_to_handler(message_type, msg);
```

**File:** crates/crash-handler/src/lib.rs (L48-57)
```rust
    // Do not kill the process if the panics happened at move-bytecode-verifier.
    // This is safe because the `state::get_state()` uses a thread_local for storing states. Thus the state can only be mutated to VERIFIER by the thread that's running the bytecode verifier.
    //
    // TODO: once `can_unwind` is stable, we should assert it. See https://github.com/rust-lang/rust/issues/92988.
    if state::get_state() == VMState::VERIFIER || state::get_state() == VMState::DESERIALIZER {
        return;
    }

    // Kill the process
    process::exit(12);
```

**File:** execution/executor-service/src/process_executor_service.rs (L16-45)
```rust
impl ProcessExecutorService {
    pub fn new(
        shard_id: ShardId,
        num_shards: usize,
        num_threads: usize,
        coordinator_address: SocketAddr,
        remote_shard_addresses: Vec<SocketAddr>,
    ) -> Self {
        let self_address = remote_shard_addresses[shard_id];
        info!(
            "Starting process remote executor service on {}; coordinator address: {}, other shard addresses: {:?}; num threads: {}",
            self_address, coordinator_address, remote_shard_addresses, num_threads
        );
        aptos_node_resource_metrics::register_node_metrics_collector(None);
        let _mp = MetricsPusher::start_for_local_run(
            &("remote-executor-service-".to_owned() + &shard_id.to_string()),
        );

        AptosVM::set_concurrency_level_once(num_threads);
        let mut executor_service = ExecutorService::new(
            shard_id,
            num_shards,
            num_threads,
            self_address,
            coordinator_address,
            remote_shard_addresses,
        );
        executor_service.start();
        Self { executor_service }
    }
```
