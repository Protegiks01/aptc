# Audit Report

## Title
Indexer-Ledger Pruning Atomicity Violation Causes Query Failures with "DB Corruption" Errors

## Summary
The `TransactionPruner::prune()` function writes to the indexer database and ledger database in two separate, non-atomic operations. If the indexer write succeeds but the ledger write fails, the indexer will have pruned account-to-transaction mappings while the ledger still contains the transaction data. This creates gaps in the indexer's sequence number mapping, causing subsequent account transaction queries to fail with spurious "DB corruption" errors due to non-contiguous sequence numbers. [1](#0-0) 

## Finding Description
The vulnerability exists in the pruning logic where two separate databases are updated without transactional guarantees:

1. **Indexer Write (Line 67)**: The indexer database is written first, deleting entries from `OrderedTransactionByAccountSchema` and updating the indexer's progress marker.

2. **Ledger Write (Line 73)**: The ledger database is written second, deleting actual transaction data and updating the ledger's progress marker. [2](#0-1) 

**The Failure Scenario:**

When the indexer write succeeds but the ledger write fails (due to disk I/O errors, disk space exhaustion, or database corruption), the system enters an inconsistent state:
- Indexer DB: Transaction-by-account mappings for versions [current_progress, target_version) are DELETED
- Ledger DB: Transaction data for versions [current_progress, target_version) still EXISTS
- Future pruning attempts will try to prune the same ledger data again

**Query Failure Path:**

The `AccountOrderedTransactionsIter` enforces strict sequence number continuity. If an account has transactions at sequence numbers [0...100] and the indexer has pruned entries [50...60):

1. Query requests account transactions starting from seq_num 0
2. Iterator returns (0,v0), (1,v1), ..., (49,v49) successfully
3. Iterator expects next seq_num = 50 (sets `expected_next_seq_num = 50`)
4. Next indexer entry is (60,v60) because [50,59] were pruned
5. Iterator detects gap: actual=60, expected=50
6. Iterator returns error: **"DB corruption: account transactions sequence numbers are not contiguous"** [3](#0-2) 

The query fails even though:
- The ledger still contains all transaction data (nothing was actually lost)
- There is no real database corruption
- The data is perfectly accessible if queried directly from the ledger

**How Data Becomes Accessible:**

When account transaction queries are made through the indexer, they follow this two-step process:
1. Query indexer for (account, seq_num) â†’ transaction_version mappings
2. Fetch actual transactions from ledger using those versions [4](#0-3) 

The indexer mappings act as an index over the ledger data. When these mappings have gaps due to partial pruning, the query path breaks.

## Impact Explanation
This vulnerability qualifies as **HIGH severity** under Aptos Bug Bounty criteria:

**API Crashes**: Account transaction queries will fail with "DB corruption" errors, causing API endpoint failures. This affects:
- `/accounts/{address}/transactions` API endpoint
- Any service querying historical transaction data by account
- Explorer interfaces showing account transaction history
- Wallet applications fetching user transaction history

**Data Availability Impact**: Users cannot access their transaction history through the standard query interface, even though the data exists in the ledger. This breaks the fundamental guarantee that committed transactions remain queryable.

**Operational Impact**: 
- The spurious "DB corruption" error message will trigger false alarms
- Operators may attempt unnecessary database recovery procedures
- The issue persists until manual intervention (database restoration or index rebuilding)

This breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable." The pruning operation is not atomic across the two databases.

## Likelihood Explanation
This vulnerability has **MEDIUM-to-HIGH likelihood** of occurrence:

**Triggering Conditions** (any can cause write_schemas to fail):
- Disk space exhaustion during pruning operations
- I/O errors or storage hardware failures
- Database corruption detected during write
- Process crashes/kills during the narrow window between writes
- File system errors or permission issues [5](#0-4) 

**Realistic Scenarios:**
1. **Disk Space**: Nodes running low on disk space may fail the second write after the first succeeds
2. **Gradual Fill**: As pruning runs continuously, disk space decreases until a write fails
3. **Crash Recovery**: System crashes between the two writes leave inconsistent state
4. **Hardware Degradation**: Failing disks may succeed some writes and fail others

**No Attacker Required**: This is a reliability issue that occurs naturally under operational stress, not requiring malicious action. This makes it MORE likely than vulnerabilities requiring attack scenarios.

**Production Evidence**: Storage failures are common in distributed systems running at scale, especially during intensive operations like pruning large version ranges.

## Recommendation

**Solution**: Ensure atomic consistency between indexer and ledger database updates by either:

### Option 1: Single Transaction (Preferred)
Write both indexer and ledger changes in a single `SchemaBatch` that gets committed atomically:

```rust
fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
    let mut batch = SchemaBatch::new();
    let candidate_transactions =
        self.get_pruning_candidate_transactions(current_progress, target_version)?;
    
    // Prepare ledger pruning operations
    self.ledger_db
        .transaction_db()
        .prune_transaction_by_hash_indices(
            candidate_transactions.iter().map(|(_, txn)| txn.hash()),
            &mut batch,
        )?;
    self.ledger_db.transaction_db().prune_transactions(
        current_progress,
        target_version,
        &mut batch,
    )?;
    self.transaction_store
        .prune_transaction_summaries_by_account(&candidate_transactions, &mut batch)?;
    
    // Add indexer pruning to the SAME batch
    if let Some(indexer_db) = self.internal_indexer_db.as_ref() {
        if indexer_db.transaction_enabled() {
            self.transaction_store
                .prune_transaction_by_account(&candidate_transactions, &mut batch)?;
        }
    }
    
    // Update progress markers in the same batch
    batch.put::<DbMetadataSchema>(
        &DbMetadataKey::TransactionPrunerProgress,
        &DbMetadataValue::Version(target_version),
    )?;
    
    // Single atomic write for both databases
    self.ledger_db.transaction_db().write_schemas(batch)
}
```

### Option 2: Write-Ahead Progress Tracking
If separate databases are required, reverse the write order and use conservative progress tracking:

1. Write ledger DB first (with progress marker)
2. Write indexer DB second
3. On failure, ledger is already pruned so retry is safe
4. Use ledger progress as source of truth for retry logic

### Option 3: Crash Recovery
Add indexer-ledger consistency verification and repair on startup:
- Check for progress marker mismatches
- Rebuild indexer indices from ledger data when gaps detected
- Log warnings for operators

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use aptos_temppath::TempPath;
    use aptos_types::transaction::{Transaction, Version};
    
    #[test]
    fn test_partial_pruning_causes_query_failure() {
        // Setup: Create ledger with account transactions at seq_nums 0-100
        let tmpdir = TempPath::new();
        let (ledger_db, indexer_db) = setup_test_dbs(&tmpdir);
        let account = AccountAddress::random();
        
        // Insert 100 transactions for the account
        for i in 0..100 {
            insert_test_transaction(&ledger_db, &indexer_db, account, i, i as Version);
        }
        
        // Simulate partial pruning: indexer succeeds, ledger fails
        let pruner = TransactionPruner::new(
            Arc::new(TransactionStore::new(ledger_db.clone())),
            ledger_db.clone(),
            0,
            Some(indexer_db.clone()),
        ).unwrap();
        
        // Manually prune indexer entries [50, 60) only
        let mut index_batch = SchemaBatch::new();
        for seq_num in 50..60 {
            index_batch.delete::<OrderedTransactionByAccountSchema>(
                &(account, seq_num)
            ).unwrap();
        }
        indexer_db.get_inner_db_ref().write_schemas(index_batch).unwrap();
        
        // DO NOT prune ledger - simulating write failure
        
        // Attempt to query account transactions
        let db_indexer = DBIndexer::new(indexer_db, Arc::new(ledger_db));
        let result = db_indexer.get_account_ordered_transactions(
            account,
            0,  // start from beginning
            100, // request all transactions
            false,
            99,  // ledger_version
        );
        
        // Expected: Query fails with "DB corruption" error
        match result {
            Err(e) => {
                assert!(e.to_string().contains("not contiguous"),
                    "Expected sequence number continuity error, got: {}", e);
            },
            Ok(_) => panic!("Query should have failed due to sequence gap"),
        }
        
        // Verify: Ledger still has all the data
        for i in 50..60 {
            let txn = ledger_db.transaction_db_raw()
                .get::<TransactionSchema>(&(i as Version))
                .unwrap();
            assert!(txn.is_some(), "Transaction {} should still exist", i);
        }
    }
}
```

## Notes

**Affected Versions**: This vulnerability affects any Aptos node with the internal indexer enabled (`enable_transaction = true` in `InternalIndexerDBConfig`). Nodes without the internal indexer are not affected as they use the fallback path that writes to a single batch. [6](#0-5) 

**Detection**: Monitor for:
- "DB corruption: account transactions sequence numbers are not contiguous" errors in query logs
- Indexer progress marker (`IndexerMetadataKey::TransactionPrunerProgress`) ahead of ledger progress marker (`DbMetadataKey::TransactionPrunerProgress`)
- Write failure errors during pruning operations followed by query failures

**Workaround**: If this issue is encountered in production:
1. Stop the pruner
2. Rebuild the indexer indices from ledger data
3. Implement Option 1 fix before resuming pruning

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L37-74)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        let candidate_transactions =
            self.get_pruning_candidate_transactions(current_progress, target_version)?;
        self.ledger_db
            .transaction_db()
            .prune_transaction_by_hash_indices(
                candidate_transactions.iter().map(|(_, txn)| txn.hash()),
                &mut batch,
            )?;
        self.ledger_db.transaction_db().prune_transactions(
            current_progress,
            target_version,
            &mut batch,
        )?;
        self.transaction_store
            .prune_transaction_summaries_by_account(&candidate_transactions, &mut batch)?;
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::TransactionPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
        if let Some(indexer_db) = self.internal_indexer_db.as_ref() {
            if indexer_db.transaction_enabled() {
                let mut index_batch = SchemaBatch::new();
                self.transaction_store
                    .prune_transaction_by_account(&candidate_transactions, &mut index_batch)?;
                index_batch.put::<InternalIndexerMetadataSchema>(
                    &IndexerMetadataKey::TransactionPrunerProgress,
                    &IndexerMetadataValue::Version(target_version),
                )?;
                indexer_db.get_inner_db_ref().write_schemas(index_batch)?;
            } else {
                self.transaction_store
                    .prune_transaction_by_account(&candidate_transactions, &mut batch)?;
            }
        }
        self.ledger_db.transaction_db().write_schemas(batch)
    }
```

**File:** storage/indexer_schemas/src/utils.rs (L84-93)
```rust
                // Ensure seq_num_{i+1} == seq_num_{i} + 1
                if let Some(expected_seq_num) = self.expected_next_seq_num {
                    ensure!(
                        seq_num == expected_seq_num,
                        "DB corruption: account transactions sequence numbers are not contiguous: \
                     actual: {}, expected: {}",
                        seq_num,
                        expected_seq_num,
                    );
                };
```

**File:** storage/indexer/src/db_indexer.rs (L586-612)
```rust
    pub fn get_account_ordered_transactions(
        &self,
        address: AccountAddress,
        start_seq_num: u64,
        limit: u64,
        include_events: bool,
        ledger_version: Version,
    ) -> Result<AccountOrderedTransactionsWithProof> {
        self.indexer_db
            .ensure_cover_ledger_version(ledger_version)?;
        error_if_too_many_requested(limit, MAX_REQUEST_LIMIT)?;

        let txns_with_proofs = self
            .indexer_db
            .get_account_ordered_transactions_iter(address, start_seq_num, limit, ledger_version)?
            .map(|result| {
                let (_seq_num, txn_version) = result?;
                self.main_db_reader.get_transaction_by_version(
                    txn_version,
                    ledger_version,
                    include_events,
                )
            })
            .collect::<Result<Vec<_>>>()?;

        Ok(AccountOrderedTransactionsWithProof::new(txns_with_proofs))
    }
```

**File:** storage/schemadb/src/lib.rs (L306-309)
```rust
    /// Writes a group of records wrapped in a [`SchemaBatch`].
    pub fn write_schemas(&self, batch: impl IntoRawBatch) -> DbResult<()> {
        self.write_schemas_inner(batch, &sync_write_option())
    }
```
