# Audit Report

## Title
Timestamp Field Not Updated in V1 Token Indexer Tables Leading to Temporal Anomalies

## Summary
The indexer's V1 token tables (`current_collection_datas`, `current_token_datas`, `current_token_ownerships`) fail to update the `last_transaction_timestamp` field during database upserts, causing the timestamp to become stale and desynchronized from `last_transaction_version`. This violates timestamp monotonicity guarantees and creates temporal anomalies in indexed data.

## Finding Description

When the indexer processes token-related transactions, it creates entries in both historical and "current" tables. The "current" tables maintain the latest state for each token/collection/ownership record.

The bug exists in three database upsert functions: [1](#0-0) 

The `insert_current_collection_datas()` function's UPDATE SET clause includes `last_transaction_version` but **omits** `last_transaction_timestamp`. Similarly, `insert_current_token_datas()` and `insert_current_token_ownerships()` have the same issue: [2](#0-1) [3](#0-2) 

In contrast, the V2 token tables correctly include the timestamp field in their UPDATE clauses: [4](#0-3) 

**How the Bug Manifests:**

1. Collection X is created at version 100 with timestamp 1000
2. Initial INSERT succeeds → DB has: `{version: 100, timestamp: 1000}`
3. Collection X is updated at version 200 with timestamp 2000
4. INSERT conflict on `collection_data_id_hash`
5. UPDATE executes with WHERE clause `100 <= 200` → TRUE
6. SET clause updates `last_transaction_version = 200` but **not** `last_transaction_timestamp`
7. Final state: `{version: 200, timestamp: 1000}` ← **Timestamp is stale!**

The schema shows both fields exist: [5](#0-4) 

While on-chain timestamps are guaranteed monotonic by the Move framework: [6](#0-5) 

The indexer fails to preserve this invariant in its database representation.

## Impact Explanation

**Medium Severity** - State inconsistencies requiring intervention:

1. **Data Integrity**: The indexer database contains incorrect temporal data where `last_transaction_timestamp` doesn't correspond to `last_transaction_version`
2. **Query Inconsistency**: API clients querying collection/token metadata receive stale timestamps that don't match the actual on-chain update time
3. **Analytics Corruption**: Historical analysis and reporting based on indexer data will show incorrect temporal patterns
4. **Downstream Impact**: NFT marketplaces, wallets, and dApps relying on indexer timestamps for chronological ordering or time-based logic may malfunction

While this doesn't affect on-chain consensus or fund security, it corrupts the indexer's data model - a critical infrastructure component that many applications depend on for accurate blockchain state queries.

## Likelihood Explanation

**Very High** - This occurs automatically during normal indexer operation:

- Every time a collection, token, or ownership is updated in a subsequent transaction, the timestamp becomes stale
- No attack required - it's a systematic bug affecting all V1 token updates
- The indexer processes transactions continuously, so the bug manifests frequently
- V2 tables are correct, proving this is an oversight in V1 table maintenance

## Recommendation

Add `last_transaction_timestamp` to the UPDATE SET clause in all three V1 token upsert functions:

**For `insert_current_collection_datas()`:** Add line after `last_transaction_version`:
```rust
last_transaction_timestamp.eq(excluded(last_transaction_timestamp)),
```

**For `insert_current_token_datas()`:** Add line after `last_transaction_version`:
```rust
last_transaction_timestamp.eq(excluded(last_transaction_timestamp)),
```

**For `insert_current_token_ownerships()`:** Add line after `last_transaction_version`:
```rust
last_transaction_timestamp.eq(excluded(last_transaction_timestamp)),
```

This matches the pattern already used correctly in V2 tables and ensures temporal consistency between version and timestamp fields.

## Proof of Concept

```rust
// Rust test demonstrating the bug
#[test]
fn test_timestamp_not_updated_on_collection_update() {
    let mut conn = establish_test_db_connection();
    
    // Step 1: Initial insert at version 100, timestamp 1000
    let initial_collection = CurrentCollectionData {
        collection_data_id_hash: "test_hash".to_string(),
        creator_address: "0x1".to_string(),
        collection_name: "Test".to_string(),
        last_transaction_version: 100,
        last_transaction_timestamp: NaiveDateTime::from_timestamp(1000, 0),
        // ... other fields
    };
    
    insert_current_collection_datas(&mut conn, &[initial_collection]).unwrap();
    
    // Verify initial state
    let result = current_collection_datas::table
        .filter(collection_data_id_hash.eq("test_hash"))
        .first::<CurrentCollectionDataQuery>(&mut conn)
        .unwrap();
    assert_eq!(result.last_transaction_version, 100);
    assert_eq!(result.last_transaction_timestamp.timestamp(), 1000);
    
    // Step 2: Update at version 200, timestamp 2000
    let updated_collection = CurrentCollectionData {
        collection_data_id_hash: "test_hash".to_string(),
        creator_address: "0x1".to_string(),
        collection_name: "Test Updated".to_string(),
        last_transaction_version: 200,
        last_transaction_timestamp: NaiveDateTime::from_timestamp(2000, 0),
        // ... other fields
    };
    
    insert_current_collection_datas(&mut conn, &[updated_collection]).unwrap();
    
    // Step 3: Verify the bug - version updated but timestamp STALE
    let result = current_collection_datas::table
        .filter(collection_data_id_hash.eq("test_hash"))
        .first::<CurrentCollectionDataQuery>(&mut conn)
        .unwrap();
    
    assert_eq!(result.last_transaction_version, 200); // ✓ Updated correctly
    assert_eq!(result.last_transaction_timestamp.timestamp(), 1000); // ✗ BUG: Still 1000, should be 2000!
    assert_eq!(result.collection_name, "Test Updated"); // ✓ Other fields updated
    
    // This proves temporal anomaly: version=200 but timestamp=1000
}
```

### Citations

**File:** crates/indexer/src/processors/token_processor.rs (L380-410)
```rust
fn insert_current_token_ownerships(
    conn: &mut PgConnection,
    items_to_insert: &[CurrentTokenOwnership],
) -> Result<(), diesel::result::Error> {
    use schema::current_token_ownerships::dsl::*;

    let chunks = get_chunks(items_to_insert.len(), CurrentTokenOwnership::field_count());

    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::current_token_ownerships::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict((token_data_id_hash, property_version, owner_address))
                .do_update()
                .set((
                    creator_address.eq(excluded(creator_address)),
                    collection_name.eq(excluded(collection_name)),
                    name.eq(excluded(name)),
                    amount.eq(excluded(amount)),
                    token_properties.eq(excluded(token_properties)),
                    last_transaction_version.eq(excluded(last_transaction_version)),
                    collection_data_id_hash.eq(excluded(collection_data_id_hash)),
                    table_type.eq(excluded(table_type)),
                    inserted_at.eq(excluded(inserted_at)),
                )),
            Some(" WHERE current_token_ownerships.last_transaction_version <= excluded.last_transaction_version "),
        )?;
    }
    Ok(())
}
```

**File:** crates/indexer/src/processors/token_processor.rs (L412-453)
```rust
fn insert_current_token_datas(
    conn: &mut PgConnection,
    items_to_insert: &[CurrentTokenData],
) -> Result<(), diesel::result::Error> {
    use schema::current_token_datas::dsl::*;

    let chunks = get_chunks(items_to_insert.len(), CurrentTokenData::field_count());

    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::current_token_datas::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict(token_data_id_hash)
                .do_update()
                .set((
                    creator_address.eq(excluded(creator_address)),
                    collection_name.eq(excluded(collection_name)),
                    name.eq(excluded(name)),
                    maximum.eq(excluded(maximum)),
                    supply.eq(excluded(supply)),
                    largest_property_version.eq(excluded(largest_property_version)),
                    metadata_uri.eq(excluded(metadata_uri)),
                    payee_address.eq(excluded(payee_address)),
                    royalty_points_numerator.eq(excluded(royalty_points_numerator)),
                    royalty_points_denominator.eq(excluded(royalty_points_denominator)),
                    maximum_mutable.eq(excluded(maximum_mutable)),
                    uri_mutable.eq(excluded(uri_mutable)),
                    description_mutable.eq(excluded(description_mutable)),
                    properties_mutable.eq(excluded(properties_mutable)),
                    royalty_mutable.eq(excluded(royalty_mutable)),
                    default_properties.eq(excluded(default_properties)),
                    last_transaction_version.eq(excluded(last_transaction_version)),
                    collection_data_id_hash.eq(excluded(collection_data_id_hash)),
                    description.eq(excluded(description)),
                    inserted_at.eq(excluded(inserted_at)),
                )),
            Some(" WHERE current_token_datas.last_transaction_version <= excluded.last_transaction_version "),
        )?;
    }
    Ok(())
}
```

**File:** crates/indexer/src/processors/token_processor.rs (L455-488)
```rust
fn insert_current_collection_datas(
    conn: &mut PgConnection,
    items_to_insert: &[CurrentCollectionData],
) -> Result<(), diesel::result::Error> {
    use schema::current_collection_datas::dsl::*;

    let chunks = get_chunks(items_to_insert.len(), CurrentCollectionData::field_count());

    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::current_collection_datas::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict(collection_data_id_hash)
                .do_update()
                .set((
                    creator_address.eq(excluded(creator_address)),
                    collection_name.eq(excluded(collection_name)),
                    description.eq(excluded(description)),
                    metadata_uri.eq(excluded(metadata_uri)),
                    supply.eq(excluded(supply)),
                    maximum.eq(excluded(maximum)),
                    maximum_mutable.eq(excluded(maximum_mutable)),
                    uri_mutable.eq(excluded(uri_mutable)),
                    description_mutable.eq(excluded(description_mutable)),
                    last_transaction_version.eq(excluded(last_transaction_version)),
                    table_handle.eq(excluded(table_handle)),
                    inserted_at.eq(excluded(inserted_at)),
                )),
            Some(" WHERE current_collection_datas.last_transaction_version <= excluded.last_transaction_version "),
        )?;
    }
    Ok(())
}
```

**File:** crates/indexer/src/processors/token_processor.rs (L690-723)
```rust
    items_to_insert: &[CurrentCollectionV2],
) -> Result<(), diesel::result::Error> {
    use schema::current_collections_v2::dsl::*;

    let chunks = get_chunks(items_to_insert.len(), CurrentCollectionV2::field_count());

    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::current_collections_v2::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict(collection_id)
                .do_update()
                .set((
                    creator_address.eq(excluded(creator_address)),
                    collection_name.eq(excluded(collection_name)),
                    description.eq(excluded(description)),
                    uri.eq(excluded(uri)),
                    current_supply.eq(excluded(current_supply)),
                    max_supply.eq(excluded(max_supply)),
                    total_minted_v2.eq(excluded(total_minted_v2)),
                    mutable_description.eq(excluded(mutable_description)),
                    mutable_uri.eq(excluded(mutable_uri)),
                    table_handle_v1.eq(excluded(table_handle_v1)),
                    token_standard.eq(excluded(token_standard)),
                    last_transaction_version.eq(excluded(last_transaction_version)),
                    last_transaction_timestamp.eq(excluded(last_transaction_timestamp)),
                    inserted_at.eq(excluded(inserted_at)),
                )),
            Some(" WHERE current_collections_v2.last_transaction_version <= excluded.last_transaction_version "),
        )?;
    }
    Ok(())
}
```

**File:** crates/indexer/src/schema.rs (L190-211)
```rust
diesel::table! {
    current_collection_datas (collection_data_id_hash) {
        #[max_length = 64]
        collection_data_id_hash -> Varchar,
        #[max_length = 66]
        creator_address -> Varchar,
        #[max_length = 128]
        collection_name -> Varchar,
        description -> Text,
        #[max_length = 512]
        metadata_uri -> Varchar,
        supply -> Numeric,
        maximum -> Numeric,
        maximum_mutable -> Bool,
        uri_mutable -> Bool,
        description_mutable -> Bool,
        last_transaction_version -> Int8,
        inserted_at -> Timestamp,
        #[max_length = 66]
        table_handle -> Varchar,
        last_transaction_timestamp -> Timestamp,
    }
```

**File:** aptos-move/framework/aptos-framework/sources/timestamp.move (L32-50)
```text
    public fun update_global_time(
        account: &signer,
        proposer: address,
        timestamp: u64
    ) acquires CurrentTimeMicroseconds {
        // Can only be invoked by AptosVM signer.
        system_addresses::assert_vm(account);

        let global_timer = borrow_global_mut<CurrentTimeMicroseconds>(@aptos_framework);
        let now = global_timer.microseconds;
        if (proposer == @vm_reserved) {
            // NIL block with null address as proposer. Timestamp must be equal.
            assert!(now == timestamp, error::invalid_argument(EINVALID_TIMESTAMP));
        } else {
            // Normal block. Time must advance
            assert!(now < timestamp, error::invalid_argument(EINVALID_TIMESTAMP));
            global_timer.microseconds = timestamp;
        };
    }
```
