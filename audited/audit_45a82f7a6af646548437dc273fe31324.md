# Audit Report

## Title
Node Restart Failure Due to Inconsistent Pruner State After First Iteration Error

## Summary
If a pruner's `prune()` method fails on the first iteration after the metadata pruner succeeds but before sub-pruners complete, the node enters an inconsistent state where metadata progress is ahead of sub-pruner progress. On restart, sub-pruners attempt to catch up during initialization, and if they fail, the node panics and becomes permanently unable to restart, causing total loss of liveness.

## Finding Description

The vulnerability exists in the pruning subsystem's error handling during the first pruning iteration. Both `LedgerPruner` and `StateKvPruner` follow a two-phase pruning pattern:

1. **Phase 1**: Metadata pruner updates its progress in the database
2. **Phase 2**: Sub-pruners execute in parallel and update their individual progress

The critical flaw occurs at [1](#0-0)  where the pruning sequence is:

- First, `ledger_metadata_pruner.prune()` executes and commits `LedgerPrunerProgress` to the database
- Then sub-pruners execute in parallel
- Only if ALL operations succeed is `self.record_progress()` called to update the in-memory progress counter

If any sub-pruner fails after the metadata pruner succeeds, the system enters an inconsistent state:
- Database metadata indicates pruning reached version X
- Some sub-pruner progress counters show version X (those that succeeded)
- Other sub-pruner progress counters show the old version (those that failed)
- The main pruner's in-memory `self.progress` remains at the old version

The vulnerability manifests on node restart. During initialization at [2](#0-1) , each sub-pruner's constructor reads its progress from the database and attempts to catch up to the metadata progress by calling `prune()`.

For example, in `EventStorePruner::new()` at [3](#0-2) , if the catch-up `prune()` call fails, the constructor returns an error.

The fatal blow occurs at [4](#0-3)  where `LedgerPruner::new()` failure causes a panic via `.expect()`, permanently preventing node restart.

The same vulnerability exists in `StateKvPruner` at [5](#0-4) .

**Attack Scenario:**
1. Node is running normally with pruner enabled
2. A pruning operation begins; target version is set ahead of current progress
3. First iteration of `prune()` executes
4. Metadata pruner succeeds, writing progress to database
5. A sub-pruner (e.g., `TransactionPruner`) encounters an error (corrupted data, disk I/O failure, or bug in pruning logic for a specific transaction type)
6. The pruning function returns error; in-memory progress is NOT updated
7. The pruner worker continues retrying, but if the error is persistent (e.g., corruption at a specific version), it fails repeatedly
8. Operator restarts the node to investigate
9. During initialization, the sub-pruner attempts to catch up from its old progress to the metadata progress
10. The catch-up operation hits the same persistent error
11. Initialization fails with panic
12. **Node cannot restart**

## Impact Explanation

This vulnerability qualifies as **Critical Severity** per the Aptos bug bounty program criteria because it causes:

**Total loss of liveness/network availability**: Once triggered, the affected node becomes permanently unable to restart without manual database intervention or code patching. The `.expect()` panic at initialization is unrecoverable through normal restart procedures.

The impact extends beyond a single node:
- Validator nodes affected by this bug cannot participate in consensus
- If multiple validators hit the same pruning error (e.g., due to a consensus bug affecting all nodes identically), the network could lose sufficient validators to halt
- Even for non-validator nodes, loss of RPC endpoints and indexers degrades network accessibility

The vulnerability violates the **State Consistency** invariant (Invariant #4): the pruning subsystem should maintain atomic progress across all sub-components. It also violates basic fault-tolerance expectations that transient errors should not cause permanent node failure.

## Likelihood Explanation

**Likelihood: MEDIUM**

The vulnerability requires a persistent pruning error, which can occur through several realistic scenarios:

**High Likelihood Triggers:**
- Database corruption at a specific version (disk failure, power loss during write)
- Edge case transaction that causes pruning logic to fail (e.g., malformed historical data, unexpected transaction variant)
- Resource exhaustion during pruning (out of disk space when writing pruning batch)

**Lower Likelihood Triggers:**
- Bug in pruning logic for specific transaction types
- Race condition in parallel pruning operations

Importantly, this is **NOT** directly exploitable by external attackers without validator access. However:
- An attacker could potentially craft transactions that trigger bugs in pruning logic
- Database corruption can occur naturally in production environments
- The error becomes **permanent** once the state inconsistency is created, making recovery difficult

The severity is amplified because:
1. The first iteration is when the bug is most likely to manifest (no prior successful pruning to establish patterns)
2. Once triggered, the node cannot recover without manual intervention
3. The `.expect()` panic provides no graceful degradation or recovery path

## Recommendation

**Immediate Fix: Implement Atomic Two-Phase Commit for Pruning**

The pruning operation should either succeed completely or fail completely. The metadata progress should only be updated after ALL sub-pruners succeed.

**Code Fix for LedgerPruner:** [1](#0-0) 

Modify the `prune()` method to:
1. Execute all pruning operations WITHOUT committing progress to database
2. Only commit metadata progress after all sub-pruners confirm success
3. Use a single atomic database transaction for all progress updates

**Alternative Fix: Graceful Degradation on Initialization Failure** [4](#0-3) 

Replace `.expect()` with graceful error handling:
- Log the initialization failure
- Disable pruning but allow the node to start
- Provide operator tools to inspect and repair pruner state
- Allow manual reset of pruner progress to a consistent state

**Recommended Approach: Combine Both Fixes**
1. Implement atomic two-phase commit to prevent state inconsistency
2. Add graceful degradation for initialization failures as defense-in-depth
3. Add monitoring and alerts for pruning errors
4. Implement automated recovery procedures (e.g., reset all sub-pruner progress to metadata progress if inconsistency detected)

**Additional Recommendations:**
- Add validation on startup to detect and repair pruner state inconsistencies
- Implement idempotent pruning operations that can safely retry
- Add circuit breaker pattern to disable pruning after repeated failures
- Improve error messages to guide operators in recovery

## Proof of Concept

**Rust Reproduction Steps:**

```rust
// This PoC demonstrates the vulnerability by simulating a pruner failure scenario
// File: storage/aptosdb/src/pruner/test_pruner_failure.rs

#[cfg(test)]
mod tests {
    use super::*;
    
    // Step 1: Create a mock sub-pruner that fails on a specific version range
    struct FailingSubPruner {
        fail_at_version: Version,
    }
    
    impl DBSubPruner for FailingSubPruner {
        fn name(&self) -> &str { "FailingSubPruner" }
        
        fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
            if current_progress <= self.fail_at_version && target_version > self.fail_at_version {
                return Err(anyhow!("Simulated pruning failure at version {}", self.fail_at_version));
            }
            Ok(())
        }
    }
    
    // Step 2: Simulate the first pruning iteration with partial failure
    #[test]
    fn test_pruner_state_inconsistency_on_first_iteration() {
        // Setup: Create pruner with initial progress = 100
        let ledger_db = create_test_ledger_db();
        let pruner = LedgerPruner::new(ledger_db, None).unwrap();
        
        // Inject a failing sub-pruner into the pruner's sub_pruners list
        // (In real code, this would be one of the standard sub-pruners hitting an error)
        
        // Step 3: Set target version and trigger first pruning iteration
        pruner.set_target_version(150);
        
        // Step 4: Call prune() - metadata pruner succeeds, sub-pruner fails
        let result = pruner.prune(50); // batch_size = 50
        assert!(result.is_err()); // Should fail due to FailingSubPruner
        
        // Step 5: Verify inconsistent state
        // metadata_progress should be 150 in database
        let metadata_progress = ledger_db.metadata_db().get_pruner_progress().unwrap();
        assert_eq!(metadata_progress, 150);
        
        // But pruner's in-memory progress should still be 100
        assert_eq!(pruner.progress(), 100);
        
        // Step 6: Simulate node restart - this should panic
        // Drop the pruner to simulate shutdown
        drop(pruner);
        
        // Try to create a new pruner - this will panic in init_pruner()
        // because sub-pruner initialization tries to catch up and fails
        let result = std::panic::catch_unwind(|| {
            LedgerPruner::new(ledger_db, None)
        });
        
        assert!(result.is_err()); // Panicked due to .expect()
        // Node is now unable to restart!
    }
}
```

**Key PoC Elements:**
1. Mock a sub-pruner that consistently fails at a specific version
2. Trigger first pruning iteration with target version past the failure point
3. Verify metadata progress updates but in-memory progress doesn't
4. Attempt to restart (recreate pruner) - observe initialization panic
5. Confirm node cannot restart without manual intervention

## Notes

This vulnerability affects both `LedgerPruner` and `StateKvPruner` identically. The root cause is the non-atomic progress update pattern combined with panic-on-failure initialization.

The specific sub-pruner implementations at [6](#0-5)  all follow the same catch-up pattern during initialization, making any of them potential failure points.

The worker thread implementation at [7](#0-6)  handles errors by logging and retrying, which is appropriate for runtime errors but doesn't prevent the state inconsistency from being created in the first place.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L62-92)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning ledger data."
            );
            self.ledger_metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning ledger data is done.");
        }

        Ok(target_version)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L138-170)
```rust
        let event_store_pruner = Box::new(EventStorePruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
            internal_indexer_db.clone(),
        )?);
        let persisted_auxiliary_info_pruner = Box::new(PersistedAuxiliaryInfoPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);
        let transaction_accumulator_pruner = Box::new(TransactionAccumulatorPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);

        let transaction_auxiliary_data_pruner = Box::new(TransactionAuxiliaryDataPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);

        let transaction_info_pruner = Box::new(TransactionInfoPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);
        let transaction_pruner = Box::new(TransactionPruner::new(
            Arc::clone(&transaction_store),
            Arc::clone(&ledger_db),
            metadata_progress,
            internal_indexer_db,
        )?);
        let write_set_pruner = Box::new(WriteSetPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L85-109)
```rust
    pub(in crate::pruner) fn new(
        ledger_db: Arc<LedgerDb>,
        metadata_progress: Version,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.event_db_raw(),
            &DbMetadataKey::EventPrunerProgress,
            metadata_progress,
        )?;

        let myself = EventStorePruner {
            ledger_db,
            internal_indexer_db,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up EventStorePruner."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L146-149)
```rust
        let pruner = Arc::new(
            LedgerPruner::new(ledger_db, internal_indexer_db)
                .expect("Failed to create ledger pruner."),
        );
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_pruner_manager.rs (L114-115)
```rust
        let pruner =
            Arc::new(StateKvPruner::new(state_kv_db).expect("Failed to create state kv pruner."));
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L78-104)
```rust
    pub(in crate::pruner) fn new(
        transaction_store: Arc<TransactionStore>,
        ledger_db: Arc<LedgerDb>,
        metadata_progress: Version,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.transaction_db_raw(),
            &DbMetadataKey::TransactionPrunerProgress,
            metadata_progress,
        )?;

        let myself = TransactionPruner {
            transaction_store,
            ledger_db,
            internal_indexer_db,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up TransactionPruner."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L52-69)
```rust
    // Loop that does the real pruning job.
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
    }
```
