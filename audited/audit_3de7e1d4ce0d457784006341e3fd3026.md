# Audit Report

## Title
Critical State Snapshot Restoration Vulnerability: Missing Chunk Continuity Validation Allows Incomplete State Recovery Leading to Consensus Violations

## Summary
The state snapshot restoration process lacks validation that chunks are contiguous and complete. An attacker with access to backup storage can modify the manifest JSON to create gaps in chunk sequences, resulting in nodes restoring incomplete state that appears valid but is missing critical data. This can lead to consensus violations, permanent network partitions, and state inconsistencies across the network.

## Finding Description

The vulnerability exists in the state snapshot restoration flow where chunks from backup manifests are processed without validating completeness or continuity. The system breaks the following critical invariants:

1. **State Consistency Invariant**: State transitions must be atomic and verifiable via Merkle proofs
2. **Deterministic Execution Invariant**: All validators must produce identical state roots for identical blocks

**Attack Flow:**

The manifest is a plain JSON file loaded without integrity verification: [1](#0-0) 

During restoration, chunks from the manifest are processed sequentially without gap validation: [2](#0-1) 

The only validation is that keys within and across chunks are in increasing order, which does NOT detect gaps: [3](#0-2) 

The `SparseMerkleRangeProof` verification only proves that the given range is correct relative to the tree structure, not that all state is present: [4](#0-3) 

**Exploitation Scenario:**

1. Attacker obtains a legitimate state snapshot backup (manifest + chunks)
2. Modifies the manifest JSON to remove chunks (e.g., removes chunks covering indices 100,000-199,999)
3. Uploads the modified manifest to backup storage
4. When a node performs restoration:
   - Chunk 0-99,999 restores successfully (proof validates)
   - Chunk 200,000-299,999 restores successfully (keys are > previous, proof validates)
   - Missing range 100,000-199,999 becomes sparse tree placeholders
5. The restored state appears valid but is incomplete
6. If the missing chunks contain validator stake data, governance accounts, or critical smart contracts, the node will have incorrect state
7. Multiple nodes restoring from the same tampered backup will have identical incomplete state, but different from honest nodes, causing consensus violations

The restoration code never validates:
- That chunks are contiguous (chunk[i].last_idx + 1 == chunk[i+1].first_idx)
- That the first chunk starts at index 0
- That all expected state indices are covered

## Impact Explanation

This vulnerability qualifies as **Critical Severity** under the Aptos Bug Bounty program for the following reasons:

1. **Consensus/Safety Violations**: Nodes restoring from tampered backups will have different state roots than honest nodes. This breaks the fundamental consensus safety property - validators will disagree on the state, potentially causing chain splits or consensus failure.

2. **Non-recoverable Network Partition**: If multiple validators restore from the same tampered backup, they form a partition with incomplete state. Since the state root hash may differ from honest nodes, they cannot achieve consensus. This requires manual intervention or a hard fork to resolve.

3. **Loss of Funds**: If the missing chunks contain account balances, staking data, or governance token distributions, affected nodes will have incorrect views of user funds, potentially leading to loss or freezing of funds.

4. **State Consistency Violation**: Different nodes will have fundamentally inconsistent views of the blockchain state, breaking the deterministic execution invariant that is core to blockchain security.

The attack is particularly severe because:
- Nodes use backup restoration for bootstrapping and disaster recovery
- The restored node appears to function normally but has corrupted state
- The vulnerability affects the core state management layer that all components depend on
- Detection is difficult as each chunk's proof validates independently [5](#0-4) 

## Likelihood Explanation

**Likelihood: Medium to High**

**Attack Requirements:**
1. Access to backup storage (S3, GCS, or local filesystem) - Often shared or accessible in cloud environments
2. Ability to modify the manifest JSON file - No cryptographic protection exists
3. Basic understanding of the chunk structure - Documented in code and manifest schema

**Factors Increasing Likelihood:**
- Backup storage is often less secured than live production systems
- The manifest is plain JSON with no signatures or integrity checks
- Nodes routinely restore from backups for disaster recovery and bootstrapping
- Multiple nodes may restore from the same compromised backup simultaneously
- No runtime detection exists for incomplete state restoration

**Factors Decreasing Likelihood:**
- Requires the attacker to have access to backup storage infrastructure
- Sophisticated attack requiring understanding of Aptos state structure
- May be detected if state root mismatches occur during sync attempts

However, the severity is so high that even a medium likelihood combined with critical impact makes this an unacceptable risk.

## Recommendation

Implement comprehensive chunk continuity and completeness validation during state snapshot restoration:

**1. Add Chunk Continuity Validation** in `StateSnapshotRestoreController::run_impl()`:

```rust
// After line 173 in restore.rs, add validation:
ensure!(
    chunks.first().map_or(true, |c| c.first_idx == 0),
    "First chunk must start at index 0, got {}",
    chunks.first().unwrap().first_idx
);

for i in 0..chunks.len() - 1 {
    ensure!(
        chunks[i].last_idx + 1 == chunks[i + 1].first_idx,
        "Gap detected between chunks: chunk {} ends at {}, chunk {} starts at {}",
        i,
        chunks[i].last_idx,
        i + 1,
        chunks[i + 1].first_idx
    );
}
```

**2. Add Total Item Count Validation**:

Store the total expected item count in the manifest and validate it:

```rust
// In manifest.rs, add field to StateSnapshotBackup:
pub total_items: usize,

// In restore.rs, validate:
let expected_total = manifest.total_items;
let actual_total = manifest.chunks.last().map_or(0, |c| c.last_idx + 1);
ensure!(
    actual_total == expected_total,
    "Incomplete chunk set: expected {} items, manifest covers {} items",
    expected_total,
    actual_total
);
```

**3. Add Manifest Integrity Protection**:

Sign the manifest with a trusted key and verify the signature during restore:

```rust
// Add signature field to StateSnapshotBackup
pub manifest_signature: Option<Signature>,

// Verify before using
if let Some(sig) = &manifest.manifest_signature {
    verify_manifest_signature(&manifest, sig, trusted_public_key)?;
}
```

**4. Add Runtime State Completeness Check**:

After restoration completes, verify the tree contains the expected number of leaves:

```rust
// In finish() implementation
let actual_leaf_count = self.num_keys_received;
ensure!(
    actual_leaf_count == expected_total_items,
    "Incomplete state: expected {} items, restored {} items",
    expected_total_items,
    actual_leaf_count
);
```

## Proof of Concept

**Reproduction Steps:**

1. **Create a legitimate backup**:
```bash
aptos-db-tool backup state-snapshot \
  --state-snapshot-epoch 100 \
  --backup-service-address http://localhost:6186 \
  --storage-path /backup/path
```

2. **Modify the manifest JSON** (`state_epoch_100_ver_12345/state.manifest`):
```json
{
  "version": 12345,
  "epoch": 100,
  "root_hash": "0x...",
  "chunks": [
    {
      "first_idx": 0,
      "last_idx": 99999,
      "first_key": "0x...",
      "last_key": "0x...",
      "blobs": "0-.chunk",
      "proof": "0-99999.proof"
    },
    // DELETE THIS CHUNK - creates gap
    // {
    //   "first_idx": 100000,
    //   "last_idx": 199999,
    //   ...
    // },
    {
      "first_idx": 200000,
      "last_idx": 299999,
      "first_key": "0x...",
      "last_key": "0x...",
      "blobs": "200000-.chunk",
      "proof": "200000-299999.proof"
    }
  ],
  "proof": "state.proof"
}
```

3. **Restore from the tampered backup**:
```bash
aptos-db-tool restore bootstrap-db \
  --target-db-dir /restored/db \
  --storage-path /backup/path \
  --state-manifest state_epoch_100_ver_12345/state.manifest
```

4. **Observe the result**:
   - Restoration completes without error
   - Database is missing state keys in range 100,000-199,999
   - State root may differ from honest nodes
   - Node appears operational but has corrupted state

**Expected Behavior**: Restoration should fail with an error indicating gaps in chunk sequence.

**Actual Behavior**: Restoration succeeds, creating an incomplete database with missing state data represented by sparse tree placeholders.

## Notes

This vulnerability is particularly concerning because:

1. **Silent Failure**: The restoration appears successful with no errors or warnings
2. **Distributed Impact**: Multiple nodes can simultaneously restore incomplete state from the same compromised backup
3. **Core Infrastructure**: Affects the fundamental state management layer that all blockchain operations depend on
4. **Bootstrap Risk**: New validators joining the network via backup restoration are vulnerable
5. **Disaster Recovery**: Organizations relying on backups for disaster recovery could restore corrupted state across their entire infrastructure

The fix requires adding multiple layers of validation at different stages of the restoration process to ensure completeness and integrity of the restored state.

### Citations

**File:** storage/backup/backup-cli/src/utils/storage_ext.rs (L35-36)
```rust
    async fn load_json_file<T: DeserializeOwned>(&self, file_handle: &FileHandleRef) -> Result<T> {
        Ok(serde_json::from_slice(&self.read_all(file_handle).await?)?)
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L123-136)
```rust
        let manifest: StateSnapshotBackup =
            self.storage.load_json_file(&self.manifest_handle).await?;
        let (txn_info_with_proof, li): (TransactionInfoWithProof, LedgerInfoWithSignatures) =
            self.storage.load_bcs_file(&manifest.proof).await?;
        txn_info_with_proof.verify(li.ledger_info(), manifest.version)?;
        let state_root_hash = txn_info_with_proof
            .transaction_info()
            .ensure_state_checkpoint_hash()?;
        ensure!(
            state_root_hash == manifest.root_hash,
            "Root hash mismatch with that in proof. root hash: {}, expected: {}",
            manifest.root_hash,
            state_root_hash,
        );
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L187-226)
```rust
        let futs_iter = chunks.into_iter().enumerate().map(|(chunk_idx, chunk)| {
            let storage = storage.clone();
            async move {
                tokio::spawn(async move {
                    let blobs = Self::read_state_value(&storage, chunk.blobs.clone()).await?;
                    let proof = storage.load_bcs_file(&chunk.proof).await?;
                    Result::<_>::Ok((chunk_idx, chunk, blobs, proof))
                })
                .await?
            }
        });
        let con = self.concurrent_downloads;
        let mut futs_stream = stream::iter(futs_iter).buffered_x(con * 2, con);
        let mut start = None;
        while let Some((chunk_idx, chunk, mut blobs, proof)) = futs_stream.try_next().await? {
            start = start.or_else(|| Some(Instant::now()));
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["add_state_chunk"]);
            let receiver = receiver.clone();
            if self.validate_modules {
                blobs = tokio::task::spawn_blocking(move || {
                    Self::validate_modules(&blobs);
                    blobs
                })
                .await?;
            }
            tokio::task::spawn_blocking(move || {
                receiver.lock().as_mut().unwrap().add_chunk(blobs, proof)
            })
            .await??;
            leaf_idx.set(chunk.last_idx as i64);
            info!(
                chunk = chunk_idx,
                chunks_to_add = chunks_to_add,
                last_idx = chunk.last_idx,
                values_per_second = ((chunk.last_idx + 1 - start_idx) as f64
                    / start.as_ref().unwrap().elapsed().as_secs_f64())
                    as u64,
                "State chunk added.",
            );
        }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L373-388)
```rust
        for (key, value_hash) in chunk {
            let hashed_key = key.hash();
            if let Some(ref prev_leaf) = self.previous_leaf {
                ensure!(
                    &hashed_key > prev_leaf.account_key(),
                    "State keys must come in increasing order.",
                )
            }
            self.previous_leaf.replace(LeafNode::new(
                hashed_key,
                value_hash,
                (key.clone(), self.version),
            ));
            self.add_one(key, value_hash);
            self.num_keys_received += 1;
        }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L390-391)
```rust
        // Verify what we have added so far is all correct.
        self.verify(proof)?;
```
