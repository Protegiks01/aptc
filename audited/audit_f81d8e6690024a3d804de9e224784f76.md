# Audit Report

## Title
Race Condition in Multi-Shard Commit Allows Pruner to Process Incomplete Stale Node Indices

## Summary
A critical race condition exists between pruner initialization and database recovery during node restart. When a multi-shard commit fails partway through, the pruner can read and act on partially committed stale node indices before the `StateStore` recovery mechanism truncates the incomplete data, leading to incorrect pruning operations and state inconsistencies across nodes.

## Finding Description

The Aptos storage layer uses sharded databases to improve write parallelism. When committing state merkle tree updates at version N, the system writes to 16 separate shard databases in parallel, followed by a top-levels batch to the metadata database. [1](#0-0) 

Each shard batch contains Jellyfish Merkle nodes, stale node indices, and a shard-specific commit progress marker. The stale node indices track which nodes became obsolete and can be pruned. [2](#0-1) 

**The vulnerability occurs due to the initialization order:** [3](#0-2) 

The system creates pruner managers (line 60-67) **before** creating the StateStore (line 70). During pruner initialization, each shard pruner runs a catch-up operation that reads stale indices from the database: [4](#0-3) 

The catch-up calls `self.prune()` which reads stale node indices and deletes corresponding nodes: [5](#0-4) 

**The race condition manifests as follows:**

1. During commit, if the system crashes after some shards commit but before the top-levels batch commits, partial stale indices remain in shards 0-7 while shards 8-15 have none for that version.

2. On restart, `StateMerkleDb::open()` reads the overall commit progress and only truncates if it exists: [6](#0-5) 

3. If the top-levels batch never committed, there's no overall progress for version N, so **no truncation occurs** at the StateMerkleDb level.

4. The pruners initialize and immediately run catch-up, **reading the partial stale indices from shards 0-7**.

5. Only **after** pruner initialization does `StateStore::new()` call `sync_commit_progress()`: [7](#0-6) 

This recovery reads the `OverallCommitProgress` from the ledger database and truncates state merkle data, but by this time the pruner has already processed incomplete metadata.

**Breaking Invariant #4: State Consistency** - The system fails to maintain atomic state transitions. Different nodes that crash at different points will prune different subsets of nodes, leading to divergent storage states despite having the same logical blockchain state.

## Impact Explanation

This vulnerability has **High Severity** impact according to Aptos bug bounty criteria:

1. **Significant Protocol Violation**: The state consistency invariant is broken. Nodes can have different historical storage states even when their current state is identical.

2. **State Inconsistencies Requiring Intervention**: Nodes that processed partial stale indices will have deleted some merkle nodes while other nodes retain them. This creates permanent divergence in historical state availability.

3. **Validator Node Issues**: Nodes may fail to serve historical state proofs or sync operations due to missing nodes that were incorrectly pruned based on incomplete metadata.

4. **Storage Corruption**: The pruner deletes nodes based on partial information, potentially removing nodes that should be retained or failing to remove nodes that should be deleted, leading to storage bloat and inconsistency.

The impact is not **Critical** because:
- Current state (latest version) remains correct
- Consensus on new blocks is not directly affected
- No fund loss occurs
- The network continues operating

However, it qualifies as **High** severity due to significant protocol violations and potential for requiring manual intervention to restore consistency.

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability triggers when:
1. A multi-shard commit is in progress (happens on every state update)
2. The system crashes/fails after some (but not all) shard batches commit
3. The node restarts normally

**Factors increasing likelihood:**
- Aptos processes high transaction throughput, meaning commits happen frequently
- The parallel shard commit window (16 shards in parallel) creates a significant time window for crashes
- Hardware failures, out-of-memory conditions, or power losses during commit operations are realistic scenarios
- The vulnerability is deterministic once the partial commit occurs

**Factors affecting impact:**
- Different nodes crashing at different times will have different subsets of partial indices
- No attacker action required - this is a race condition in normal operation
- Affects any node that experiences a crash during multi-shard commit

## Recommendation

**Fix: Ensure database recovery completes before pruner initialization**

The fix requires reordering initialization to ensure `sync_commit_progress()` runs **before** pruner catch-up operations:

```rust
// In aptosdb_internal.rs new_with_dbs()

// Option 1: Run sync_commit_progress before creating pruners
if !readonly {
    StateStore::sync_commit_progress_static(
        &ledger_db,
        &state_kv_db, 
        &state_merkle_db,
        /*crash_if_difference_is_too_large=*/ true,
    )?;
}

let state_merkle_pruner = StateMerklePrunerManager::new(
    Arc::clone(&state_merkle_db),
    pruner_config.state_merkle_pruner_config,
);
// ... rest of initialization
```

Alternatively:

```rust
// Option 2: Defer pruner catch-up until after StateStore is fully initialized
// Modify StateMerkleShardPruner::new() to skip immediate catch-up
// and add explicit catch-up call after StateStore::new()

impl StateMerkleShardPruner {
    pub fn new_deferred(shard_id: usize, db_shard: Arc<DB>, metadata_progress: Version) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &S::progress_metadata_key(Some(shard_id)),
            metadata_progress,
        )?;
        Ok(Self {
            shard_id,
            db_shard,
            _phantom: PhantomData,
        })
        // Skip catch-up here
    }
    
    pub fn catch_up(&self, progress: Version, target: Version) -> Result<()> {
        self.prune(progress, target, usize::MAX)
    }
}
```

## Proof of Concept

```rust
// Test demonstrating the vulnerability
// File: storage/aptosdb/src/pruner/state_merkle_pruner/test_race_condition.rs

#[test]
fn test_partial_commit_pruner_race() {
    use crate::AptosDB;
    use aptos_temppath::TempPath;
    use aptos_types::transaction::Version;
    
    let tmpdir = TempPath::new();
    
    // 1. Create DB and perform initial commit
    let db = AptosDB::new_for_test(&tmpdir);
    // ... commit version 0 successfully ...
    
    // 2. Simulate partial commit for version 1
    // Manually write stale indices to shards 0-7 only
    for shard_id in 0..8 {
        let shard_db = db.state_merkle_db().db_shard(shard_id);
        let mut batch = SchemaBatch::new();
        
        // Write a stale node index
        let stale_index = StaleNodeIndex {
            stale_since_version: 1,
            node_key: NodeKey::new_empty_path(0),
        };
        batch.put::<StaleNodeIndexSchema>(&stale_index, &())?;
        
        // Write shard commit progress
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateMerkleShardCommitProgress(shard_id),
            &DbMetadataValue::Version(1),
        )?;
        
        shard_db.write_schemas(batch)?;
    }
    // Shards 8-15 have no data for version 1
    // No top-levels batch committed (no StateMerkleCommitProgress for version 1)
    
    drop(db);
    
    // 3. Reopen DB - this triggers the race condition
    let db = AptosDB::new_for_test(&tmpdir);
    
    // 4. Verify pruner read partial indices
    // Check that shards 0-7 had nodes pruned while 8-15 did not
    for shard_id in 0..8 {
        let shard_db = db.state_merkle_db().db_shard(shard_id);
        let mut iter = shard_db.iter::<StaleNodeIndexSchema>()?;
        iter.seek(&1)?;
        // Should have processed (deleted) the stale index during catch-up
        assert!(iter.next().transpose()?.is_none() || 
                iter.next().transpose()?.unwrap().0.stale_since_version > 1);
    }
    
    for shard_id in 8..16 {
        let shard_db = db.state_merkle_db().db_shard(shard_id);
        // Should have no stale indices for version 1
        let mut iter = shard_db.iter::<StaleNodeIndexSchema>()?;
        iter.seek(&1)?;
        assert!(iter.next().transpose()?.is_none() || 
                iter.next().transpose()?.unwrap().0.stale_since_version != 1);
    }
    
    // This demonstrates the vulnerability: pruner processed incomplete metadata
    // Nodes in shards 0-7 were pruned based on partial indices
    // Nodes in shards 8-15 were not marked for pruning at all
}
```

## Notes

This vulnerability affects the **state management** subsystem and violates the **State Consistency** invariant (Invariant #4). While individual RocksDB batch operations are atomic, the multi-phase commit across sharded databases creates a window for partial commits. The race between pruner initialization and recovery truncation allows the pruner to observe and act on this inconsistent state, leading to permanent storage divergence across nodes.

The fix requires careful reordering of initialization to ensure all recovery operations complete before any component reads potentially inconsistent data.

### Citations

**File:** storage/aptosdb/src/state_merkle_db.rs (L147-171)
```rust
    pub(crate) fn commit(
        &self,
        version: Version,
        top_levels_batch: impl IntoRawBatch,
        batches_for_shards: Vec<impl IntoRawBatch + Send>,
    ) -> Result<()> {
        ensure!(
            batches_for_shards.len() == NUM_STATE_SHARDS,
            "Shard count mismatch."
        );
        THREAD_MANAGER.get_io_pool().install(|| {
            batches_for_shards
                .into_par_iter()
                .enumerate()
                .for_each(|(shard_id, batch)| {
                    self.db_shard(shard_id)
                        .write_schemas(batch)
                        .unwrap_or_else(|err| {
                            panic!("Failed to commit state merkle shard {shard_id}: {err}")
                        });
                })
        });

        self.commit_top_levels(version, top_levels_batch)
    }
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L350-391)
```rust
    pub(crate) fn create_jmt_commit_batch_for_shard(
        &self,
        version: Version,
        shard_id: Option<usize>,
        tree_update_batch: &TreeUpdateBatch<StateKey>,
        previous_epoch_ending_version: Option<Version>,
    ) -> Result<RawBatch> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["create_jmt_commit_batch_for_shard"]);

        let mut batch = self.db(shard_id).new_native_batch();

        let node_batch = tree_update_batch
            .node_batch
            .iter()
            .flatten()
            .collect::<Vec<_>>();
        node_batch.iter().try_for_each(|(node_key, node)| {
            ensure!(node_key.get_shard_id() == shard_id, "shard_id mismatch");
            batch.put::<JellyfishMerkleNodeSchema>(node_key, node)
        })?;

        let stale_node_index_batch = tree_update_batch
            .stale_node_index_batch
            .iter()
            .flatten()
            .collect::<Vec<_>>();
        stale_node_index_batch.iter().try_for_each(|row| {
            ensure!(row.node_key.get_shard_id() == shard_id, "shard_id mismatch");
            if previous_epoch_ending_version.is_some()
                && row.node_key.version() <= previous_epoch_ending_version.unwrap()
            {
                batch.put::<StaleNodeIndexCrossEpochSchema>(row, &())
            } else {
                // These are processed by the state merkle pruner.
                batch.put::<StaleNodeIndexSchema>(row, &())
            }
        })?;

        Self::put_progress(Some(version), shard_id, &mut batch)?;

        batch.into_raw_batch(self.db(shard_id))
    }
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L668-677)
```rust
        if !readonly {
            if let Some(overall_state_merkle_commit_progress) =
                get_state_merkle_commit_progress(&state_merkle_db)?
            {
                truncate_state_merkle_db_shards(
                    &state_merkle_db,
                    overall_state_merkle_commit_progress,
                )?;
            }
        }
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L43-110)
```rust
    fn new_with_dbs(
        ledger_db: LedgerDb,
        hot_state_merkle_db: Option<StateMerkleDb>,
        state_merkle_db: StateMerkleDb,
        state_kv_db: StateKvDb,
        pruner_config: PrunerConfig,
        buffered_state_target_items: usize,
        hack_for_tests: bool,
        empty_buffered_state_for_restore: bool,
        skip_index_and_usage: bool,
        internal_indexer_db: Option<InternalIndexerDB>,
        hot_state_config: HotStateConfig,
    ) -> Self {
        let ledger_db = Arc::new(ledger_db);
        let hot_state_merkle_db = hot_state_merkle_db.map(Arc::new);
        let state_merkle_db = Arc::new(state_merkle_db);
        let state_kv_db = Arc::new(state_kv_db);
        let state_merkle_pruner = StateMerklePrunerManager::new(
            Arc::clone(&state_merkle_db),
            pruner_config.state_merkle_pruner_config,
        );
        let epoch_snapshot_pruner = StateMerklePrunerManager::new(
            Arc::clone(&state_merkle_db),
            pruner_config.epoch_snapshot_pruner_config.into(),
        );
        let state_kv_pruner =
            StateKvPrunerManager::new(Arc::clone(&state_kv_db), pruner_config.ledger_pruner_config);
        let state_store = Arc::new(StateStore::new(
            Arc::clone(&ledger_db),
            hot_state_merkle_db,
            Arc::clone(&state_merkle_db),
            Arc::clone(&state_kv_db),
            state_merkle_pruner,
            epoch_snapshot_pruner,
            state_kv_pruner,
            buffered_state_target_items,
            hack_for_tests,
            empty_buffered_state_for_restore,
            skip_index_and_usage,
            internal_indexer_db.clone(),
            hot_state_config,
        ));

        let ledger_pruner = LedgerPrunerManager::new(
            Arc::clone(&ledger_db),
            pruner_config.ledger_pruner_config,
            internal_indexer_db,
        );

        AptosDB {
            ledger_db: Arc::clone(&ledger_db),
            state_kv_db: Arc::clone(&state_kv_db),
            event_store: Arc::new(EventStore::new(ledger_db.event_db().db_arc())),
            state_store,
            transaction_store: Arc::new(TransactionStore::new(Arc::clone(&ledger_db))),
            ledger_pruner,
            _rocksdb_property_reporter: RocksdbPropertyReporter::new(
                ledger_db,
                state_merkle_db,
                state_kv_db,
            ),
            pre_commit_lock: std::sync::Mutex::new(()),
            commit_lock: std::sync::Mutex::new(()),
            indexer: None,
            skip_index_and_usage,
            update_subscriber: None,
        }
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L31-56)
```rust
    pub(in crate::pruner) fn new(
        shard_id: usize,
        db_shard: Arc<DB>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &S::progress_metadata_key(Some(shard_id)),
            metadata_progress,
        )?;
        let myself = Self {
            shard_id,
            db_shard,
            _phantom: PhantomData,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up {} shard {shard_id}.",
            S::name(),
        );
        myself.prune(progress, metadata_progress, usize::MAX)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L58-100)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
        max_nodes_to_prune: usize,
    ) -> Result<()> {
        loop {
            let mut batch = SchemaBatch::new();
            let (indices, next_version) = StateMerklePruner::get_stale_node_indices(
                &self.db_shard,
                current_progress,
                target_version,
                max_nodes_to_prune,
            )?;

            indices.into_iter().try_for_each(|index| {
                batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
                batch.delete::<S>(&index)
            })?;

            let mut done = true;
            if let Some(next_version) = next_version {
                if next_version <= target_version {
                    done = false;
                }
            }

            if done {
                batch.put::<DbMetadataSchema>(
                    &S::progress_metadata_key(Some(self.shard_id)),
                    &DbMetadataValue::Version(target_version),
                )?;
            }

            self.db_shard.write_schemas(batch)?;

            if done {
                break;
            }
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L490-498)
```rust
            if state_merkle_target_version < state_merkle_max_version {
                info!(
                    state_merkle_max_version = state_merkle_max_version,
                    target_version = state_merkle_target_version,
                    "Start state merkle truncation..."
                );
                truncate_state_merkle_db(&state_merkle_db, state_merkle_target_version)
                    .expect("Failed to truncate state merkle db.");
            }
```
