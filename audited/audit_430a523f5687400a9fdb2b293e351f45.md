# Audit Report

## Title
Buffer Data Loss in Indexer-Grpc-v2-File-Store-Backfiller Due to Missing Final Flush

## Summary
The `backfill()` function in the indexer-grpc-v2-file-store-backfiller fails to flush buffered transactions when the gRPC stream ends prematurely. This causes silent data loss when transactions are buffered but never sent through the channel to `do_upload`, resulting in incomplete backfill data in the file store.

## Finding Description

The vulnerability exists in the channel lifecycle and buffer management between the sending and receiving tasks in the `backfill()` function.

**Architecture:** [1](#0-0) 

A channel is created with capacity 10. Two tasks are spawned:

1. **Receiving task** (waits for buffered transaction batches): [2](#0-1) 

2. **Sending task** (processes gRPC stream and buffers transactions): [3](#0-2) 

**The Critical Issue:**

The `FileStoreOperatorV2` only flushes buffered transactions when either: [4](#0-3) 

This means transactions are dumped only when buffer size â‰¥ 50MB OR when `(transaction.version + 1) % num_txns_per_folder == 0`.

**There is no explicit flush mechanism** in `FileStoreOperatorV2`: [5](#0-4) 

**Data Loss Scenario:**

1. Backfiller requests transactions [1000, 2000) where `num_txns_per_folder = 1000`
2. gRPC stream ends prematurely (network interruption, fullnode sync delay, etc.) after providing only transactions [1000, 1500]
3. For transactions 1000-1500:
   - None satisfy `(version + 1) % 1000 == 0` (only version 1999 would)
   - If total size < 50MB, no dump is triggered
4. Stream ends (line 173 while loop exits), sending task completes
5. Buffered transactions [1000, 1500] remain in `file_store_operator.buffer`
6. Task exits without flushing, `tx` is dropped
7. **Transactions [1000, 1500] are never uploaded to file store**

The gRPC stream from the fullnode waits for transactions to become available: [6](#0-5) 

However, the stream can return early if the abort handle is triggered or if there are errors. Even without errors, if the fullnode doesn't have all historical data yet during a backfill operation, the stream may provide partial data.

## Impact Explanation

This is a **HIGH severity** data integrity vulnerability affecting the indexer infrastructure:

1. **Silent Data Loss**: Transactions are lost without error notification. The progress file is only updated after the scope exits, so a partial backfill appears incomplete but doesn't indicate which specific transactions were lost.

2. **File Store Gaps**: The file store will have missing transaction ranges, breaking data completeness guarantees for downstream consumers.

3. **Detection Difficulty**: Since the backfiller logs "Backfilling versions [X, Y) is finished" even when the buffer wasn't flushed, operators may not realize data was lost.

4. **Manual Intervention Required**: Detecting and fixing these gaps requires manual audit and re-running backfills.

Per Aptos bug bounty criteria, this qualifies as **High Severity**: "Significant protocol violations" - the indexer-grpc file store protocol assumes complete transaction data, and this violates that guarantee.

## Likelihood Explanation

**Likelihood: Medium to High**

This can occur whenever:
- Network interruptions during backfill operations
- Fullnode is still syncing and doesn't have all requested historical data
- gRPC connection timeouts or server restarts during backfill
- Any condition causing the stream to end before providing all `num_transactions_per_folder` transactions

Given that backfills often run against fullnodes that may be syncing or experiencing network issues, this scenario is realistic and likely to occur in production deployments.

## Recommendation

Add an explicit final flush before the sending task completes. After the gRPC stream ends, check if the buffer has any remaining transactions and flush them:

**Fix in processor.rs (after line 199):**

```rust
// After the stream processing loop, flush any remaining buffered transactions
if !file_store_operator.buffer.is_empty() {
    file_store_operator
        .dump_transactions_to_file(false, tx.clone())
        .await
        .unwrap();
}
```

**Alternative: Add a public flush method to FileStoreOperatorV2:**

```rust
// In file_store_operator.rs
pub async fn flush_buffer(
    &mut self,
    tx: Sender<(Vec<Transaction>, BatchMetadata, bool)>,
) -> Result<()> {
    if !self.buffer.is_empty() {
        self.dump_transactions_to_file(false, tx).await?;
    }
    Ok(())
}
```

Then call it in processor.rs after the stream ends:
```rust
file_store_operator.flush_buffer(tx.clone()).await.unwrap();
```

## Proof of Concept

```rust
// Rust test demonstrating the vulnerability
#[tokio::test]
async fn test_buffer_loss_on_premature_stream_end() {
    use tokio::sync::mpsc;
    use aptos_indexer_grpc_utils::file_store_operator_v2::file_store_operator::FileStoreOperatorV2;
    use aptos_indexer_grpc_utils::file_store_operator_v2::common::BatchMetadata;
    use aptos_protos::transaction::v1::Transaction;
    
    const NUM_TXNS_PER_FOLDER: u64 = 1000;
    const MAX_SIZE_PER_FILE: usize = 50 * (1 << 20); // 50MB
    
    let (tx, mut rx) = mpsc::channel(10);
    let mut file_store_operator = FileStoreOperatorV2::new(
        MAX_SIZE_PER_FILE,
        NUM_TXNS_PER_FOLDER,
        1000, // starting version
        BatchMetadata::default(),
    );
    
    // Simulate receiving only 500 transactions (not reaching batch boundary)
    // None will trigger end_batch since (1000..1500).all(|v| (v+1) % 1000 != 0)
    for version in 1000..1500 {
        let mut txn = Transaction::default();
        txn.version = version;
        
        // This should buffer but not dump (size < 50MB, not at batch boundary)
        file_store_operator
            .buffer_and_maybe_dump_transactions_to_file(txn, tx.clone())
            .await
            .unwrap();
    }
    
    // Simulate stream ending prematurely (drop tx without flushing)
    drop(tx);
    
    // Check what was actually sent to the channel
    let mut received_count = 0;
    while let Some(_) = rx.recv().await {
        received_count += 1;
    }
    
    // Bug: No batches were sent because buffer wasn't flushed
    // 500 transactions are lost!
    assert_eq!(received_count, 0, "Expected 0 batches sent, but got {}", received_count);
    
    // The file_store_operator still has 500 buffered transactions that were never uploaded
}
```

**Notes:**
- This vulnerability is specific to the backfiller component, not the main indexer-grpc streaming service (which runs in an infinite loop)
- The file_store_uploader.rs has a similar pattern but runs continuously, so doesn't have this issue
- Impact is limited to data integrity in the indexer subsystem, not core blockchain consensus

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-v2-file-store-backfiller/src/processor.rs (L149-149)
```rust
                    let (tx, mut rx) = tokio::sync::mpsc::channel(10);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-v2-file-store-backfiller/src/processor.rs (L151-158)
```rust
                    s.spawn(async move {
                        while let Some((transactions, batch_metadata, end_batch)) = rx.recv().await
                        {
                            self.do_upload(transactions, batch_metadata, end_batch)
                                .await
                                .unwrap();
                        }
                    });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-v2-file-store-backfiller/src/processor.rs (L160-205)
```rust
                    s.spawn(async move {
                        // Create a grpc client to the fullnode.
                        let mut grpc_client = create_grpc_client(fullnode_grpc_address).await;
                        let request = tonic::Request::new(GetTransactionsFromNodeRequest {
                            starting_version: Some(task_version),
                            transactions_count: Some(num_transactions_per_folder),
                        });
                        let mut stream = grpc_client
                            .get_transactions_from_node(request)
                            .await
                            .unwrap()
                            .into_inner();

                        while let Some(response_item) = stream.next().await {
                            match response_item {
                                Ok(r) => {
                                    assert!(r.chain_id == chain_id);
                                    match r.response.unwrap() {
                                        Response::Data(data) => {
                                            let transactions = data.transactions;
                                            for transaction in transactions {
                                                file_store_operator
                                                    .buffer_and_maybe_dump_transactions_to_file(
                                                        transaction,
                                                        tx.clone(),
                                                    )
                                                    .await
                                                    .unwrap();
                                            }
                                        },
                                        Response::Status(_) => {
                                            continue;
                                        },
                                    }
                                },
                                Err(e) => {
                                    panic!("Error when getting transactions from fullnode: {e}.")
                                },
                            }
                        }

                        info!(
                            "Backfilling versions [{task_version}, {}) is finished.",
                            task_version + num_transactions_per_folder
                        );
                    });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_operator.rs (L10-90)
```rust
pub struct FileStoreOperatorV2 {
    max_size_per_file: usize,
    num_txns_per_folder: u64,

    buffer: Vec<Transaction>,
    buffer_size_in_bytes: usize,
    buffer_batch_metadata: BatchMetadata,
    version: u64,
}

impl FileStoreOperatorV2 {
    pub fn new(
        max_size_per_file: usize,
        num_txns_per_folder: u64,
        version: u64,
        batch_metadata: BatchMetadata,
    ) -> Self {
        Self {
            max_size_per_file,
            num_txns_per_folder,
            buffer: vec![],
            buffer_size_in_bytes: 0,
            buffer_batch_metadata: batch_metadata,
            version,
        }
    }

    pub fn version(&self) -> u64 {
        self.version
    }

    /// Buffers a transaction, if the size of the buffer exceeds the threshold, or the transaction
    /// is the last one in the batch, dump the buffer to the file store.
    pub async fn buffer_and_maybe_dump_transactions_to_file(
        &mut self,
        transaction: Transaction,
        tx: Sender<(Vec<Transaction>, BatchMetadata, bool)>,
    ) -> Result<()> {
        let end_batch = (transaction.version + 1) % self.num_txns_per_folder == 0;
        let size_bytes = transaction.encoded_len();
        ensure!(
            self.version == transaction.version,
            "Gap is found when buffering transaction, expected: {}, actual: {}",
            self.version,
            transaction.version,
        );
        self.buffer.push(transaction);
        self.buffer_size_in_bytes += size_bytes;
        self.version += 1;
        if self.buffer_size_in_bytes >= self.max_size_per_file || end_batch {
            self.dump_transactions_to_file(end_batch, tx).await?;
        }

        Ok(())
    }

    async fn dump_transactions_to_file(
        &mut self,
        end_batch: bool,
        tx: Sender<(Vec<Transaction>, BatchMetadata, bool)>,
    ) -> Result<()> {
        let transactions = std::mem::take(&mut self.buffer);
        let first_version = transactions.first().unwrap().version;
        self.buffer_batch_metadata.files.push(FileMetadata {
            first_version,
            last_version: first_version + transactions.len() as u64,
            size_bytes: self.buffer_size_in_bytes,
        });
        self.buffer_size_in_bytes = 0;

        tx.send((transactions, self.buffer_batch_metadata.clone(), end_batch))
            .await
            .map_err(anyhow::Error::msg)?;

        if end_batch {
            self.buffer_batch_metadata = BatchMetadata::default();
        }

        Ok(())
    }
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L550-578)
```rust
    async fn ensure_highest_known_version(&mut self) -> bool {
        let mut empty_loops = 0;
        while self.highest_known_version == 0 || self.current_version > self.highest_known_version {
            if let Some(abort_handle) = self.abort_handle.as_ref() {
                if abort_handle.load(Ordering::SeqCst) {
                    return false;
                }
            }
            if empty_loops > 0 {
                tokio::time::sleep(Duration::from_millis(RETRY_TIME_MILLIS)).await;
            }
            empty_loops += 1;
            if let Err(err) = self.set_highest_known_version() {
                error!(
                    error = format!("{:?}", err),
                    "[Indexer Fullnode] Failed to set highest known version"
                );
                continue;
            } else {
                sample!(
                    SampleRate::Frequency(10),
                    info!(
                        highest_known_version = self.highest_known_version,
                        "[Indexer Fullnode] Found new highest known version",
                    )
                );
            }
        }
        true
```
