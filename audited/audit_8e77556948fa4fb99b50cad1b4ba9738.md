# Audit Report

## Title
TOCTOU Race Condition in Data Stream Initialization Bypasses Availability Validation

## Summary
A Time-of-Check-Time-of-Use (TOCTOU) race condition exists in `process_new_stream_request()` where `advertised_data` can change between the availability check at line 287 and actual stream initialization, allowing streams to be created with stale availability information that may no longer be valid when requests are sent. [1](#0-0) 

## Finding Description

The vulnerability occurs in the following sequence:

1. **Availability Check with Snapshot (Line 273, 287):** The code captures a snapshot of `advertised_data` from the global data summary and validates stream availability against it: [2](#0-1) 

2. **Race Window:** Between line 287 and when the stream is actually initialized, a background task periodically refreshes the global data summary: [3](#0-2) 

The `ArcSwap` allows lock-free updates, meaning the cached global data summary can change at any time: [4](#0-3) 

3. **Use with Different Data (Line 345, 369):** When the stream is later initialized, it fetches a FRESH global data summary that may have different advertised ranges: [5](#0-4) 

4. **No Re-validation:** The `initialize_data_requests()` method does NOT re-validate availability - it blindly creates requests based on the stream's internal state: [6](#0-5) 

5. **Request Creation Without Validation:** Stream engines create requests without checking if data is still available. For example, `TransactionStreamEngine` uses the stream's `next_request_version` without re-checking availability: [7](#0-6) 

The availability check only happens at stream creation: [8](#0-7) 

6. **Failure Handling:** When requests fail due to unavailable data, the failure count increments. After exceeding `max_request_retry` (default 5), the stream terminates: [9](#0-8) [10](#0-9) 

**Attack Scenario:**
- At T0: Network advertises transactions [100, 1000]
- At T1: Client requests stream for transactions [500, 600]
- At T1: `ensure_data_is_available()` validates [500, 600] ⊆ [100, 1000] ✓
- At T2: Network conditions change (peers offline, data pruned), new advertised range [700, 1000]
- At T3: `initialize_data_requests()` creates requests for [500, 600] using NEW advertised data
- At T4: Requests fail because [500, 600] ⊄ [700, 1000]
- After 5 failures, stream terminates unexpectedly

## Impact Explanation

This vulnerability has **High Severity** impact per the Aptos bug bounty criteria:

- **Validator Node Slowdowns:** State sync streams failing unexpectedly can cause validators to fall behind, requiring stream re-creation and recovery, resulting in synchronization delays
- **State Sync Disruption:** Critical sync streams terminating forces the state sync driver to restart synchronization, causing delays in catching up to the network
- **Availability Impact:** If multiple streams are affected simultaneously during periods of high network churn, validators may experience significant sync delays affecting their ability to participate in consensus

While this doesn't directly cause consensus safety violations, it affects the **State Consistency** invariant by allowing state sync operations to fail in unexpected ways that shouldn't occur if validation were atomic.

## Likelihood Explanation

**High Likelihood** - This race condition can occur naturally without malicious intent:

- The background refresher runs periodically (configurable, but typically every few seconds)
- Network churn regularly causes peers to join/leave, changing advertised data ranges
- Data pruning and garbage collection on network nodes shrinks available ranges
- The race window spans from stream creation through first initialization (potentially hundreds of milliseconds)
- No synchronization prevents `advertised_data` from changing between check and use

The vulnerability is particularly likely during:
- Network instability or high peer churn
- Validator set changes or epoch transitions
- Storage pruning operations across the network
- Bootstrap scenarios where data availability fluctuates

## Recommendation

**Fix: Store advertised_data snapshot with the stream and use it consistently**

Modify `process_new_stream_request()` to pass the validated advertised_data to the stream for later use:

```rust
fn process_new_stream_request(
    &mut self,
    request_message: &StreamRequestMessage,
    stream_update_notifier: aptos_channel::Sender<(), StreamUpdateNotification>,
) -> Result<DataStreamListener, Error> {
    // ... existing code ...
    
    // Refresh and capture advertised_data snapshot
    refresh_global_data_summary(
        self.aptos_data_client.clone(),
        self.global_data_summary.clone(),
    );
    let global_data_summary = self.get_global_data_summary();
    let advertised_data = global_data_summary.advertised_data.clone();
    
    // Create stream with snapshot
    let (data_stream, stream_listener) = DataStream::new(
        self.data_client_config,
        self.streaming_service_config,
        stream_id,
        &request_message.stream_request,
        stream_update_notifier,
        self.aptos_data_client.clone(),
        self.notification_id_generator.clone(),
        &advertised_data,
        self.time_service.clone(),
    )?;
    
    // Verify with same snapshot
    data_stream.ensure_data_is_available(&advertised_data)?;
    
    // Store the validated snapshot with the stream
    data_stream.set_validated_advertised_data(advertised_data);
    
    // Store stream
    self.data_streams.insert(stream_id, data_stream);
    Ok(stream_listener)
}
```

Then in `update_progress_of_data_stream()`, use the stored snapshot for initialization:

```rust
async fn update_progress_of_data_stream(
    &mut self,
    data_stream_id: &DataStreamId,
) -> Result<(), Error> {
    let data_stream = self.get_data_stream(data_stream_id)?;
    
    if !data_stream.data_requests_initialized() {
        // Use the validated snapshot for initialization
        let validated_summary = data_stream.get_validated_global_data_summary();
        data_stream.initialize_data_requests(validated_summary)?;
    } else {
        // Use fresh summary for ongoing requests
        let global_data_summary = self.get_global_data_summary();
        data_stream.process_data_responses(global_data_summary).await?;
    }
    Ok(())
}
```

This ensures the validated `advertised_data` snapshot is used atomically for both the check and the initial use, eliminating the TOCTOU race condition.

## Proof of Concept

```rust
#[tokio::test]
async fn test_advertised_data_race_condition() {
    use aptos_data_client::global_summary::{AdvertisedData, GlobalDataSummary};
    use aptos_storage_service_types::responses::CompleteDataRange;
    
    // Create initial advertised data with transactions [100, 1000]
    let initial_advertised_data = AdvertisedData {
        transactions: vec![CompleteDataRange::new(100, 1000).unwrap()],
        transaction_outputs: vec![CompleteDataRange::new(100, 1000).unwrap()],
        ..AdvertisedData::empty()
    };
    
    // Create streaming service with initial data
    let (streaming_client, mut streaming_service) = 
        create_streaming_client_and_server_with_data(initial_advertised_data.clone());
    
    // Request stream for transactions [500, 600] - should pass validation
    let stream_request = StreamRequest::GetAllTransactions(
        GetAllTransactionsRequest {
            start_version: 500,
            end_version: 600,
            proof_version: 600,
            include_events: false,
        }
    );
    
    // Validate stream creation succeeds
    let (request_msg, response_rx) = create_stream_request_message(stream_request);
    streaming_service.handle_stream_request_message(
        request_msg,
        streaming_service.stream_update_notifier.clone()
    );
    let stream_listener = response_rx.await.unwrap().unwrap();
    
    // SIMULATE RACE: Update global_data_summary to have transactions [700, 1000]
    // (simulating network churn, data pruning)
    let new_advertised_data = AdvertisedData {
        transactions: vec![CompleteDataRange::new(700, 1000).unwrap()],
        transaction_outputs: vec![CompleteDataRange::new(700, 1000).unwrap()],
        ..AdvertisedData::empty()
    };
    streaming_service.global_data_summary.store(Arc::new(GlobalDataSummary {
        advertised_data: new_advertised_data,
        optimal_chunk_sizes: OptimalChunkSizes { /* ... */ },
    }));
    
    // Try to initialize stream - should fail because [500, 600] not in [700, 1000]
    streaming_service.check_progress_of_all_data_streams().await;
    
    // Verify stream fails and terminates after max_request_retry
    // The stream was validated against [100, 1000] but uses [700, 1000]
    // demonstrating the TOCTOU race condition
}
```

## Notes

This vulnerability demonstrates a classic TOCTOU race condition where the validation (`ensure_data_is_available`) and use (`initialize_data_requests`) operate on different snapshots of shared state (`advertised_data`). The `ArcSwap` mechanism enables lock-free concurrent updates, but the code doesn't account for the advertised data changing between validation and use. This is particularly problematic because the background refresher task runs independently and continuously updates the global data summary based on network conditions, creating a persistent race window that can be triggered by normal network operations without requiring any malicious behavior.

### Citations

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L254-306)
```rust
    fn process_new_stream_request(
        &mut self,
        request_message: &StreamRequestMessage,
        stream_update_notifier: aptos_channel::Sender<(), StreamUpdateNotification>,
    ) -> Result<DataStreamListener, Error> {
        // Increment the stream creation counter
        metrics::increment_counter(
            &metrics::CREATE_DATA_STREAM,
            request_message.stream_request.get_label(),
        );

        // Refresh the cached global data summary
        refresh_global_data_summary(
            self.aptos_data_client.clone(),
            self.global_data_summary.clone(),
        );

        // Create a new data stream
        let stream_id = self.stream_id_generator.next();
        let advertised_data = self.get_global_data_summary().advertised_data.clone();
        let (data_stream, stream_listener) = DataStream::new(
            self.data_client_config,
            self.streaming_service_config,
            stream_id,
            &request_message.stream_request,
            stream_update_notifier,
            self.aptos_data_client.clone(),
            self.notification_id_generator.clone(),
            &advertised_data,
            self.time_service.clone(),
        )?;

        // Verify the data stream can be fulfilled using the currently advertised data
        data_stream.ensure_data_is_available(&advertised_data)?;

        // Store the data stream internally
        if self.data_streams.insert(stream_id, data_stream).is_some() {
            return Err(Error::UnexpectedErrorEncountered(format!(
                "Duplicate data stream found! This should not occur! ID: {:?}",
                stream_id,
            )));
        }
        info!(LogSchema::new(LogEntry::HandleStreamRequest)
            .stream_id(stream_id)
            .event(LogEvent::Success)
            .message(&format!(
                "Stream created for request: {:?}",
                request_message
            )));

        // Return the listener
        Ok(stream_listener)
    }
```

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L341-384)
```rust
    async fn update_progress_of_data_stream(
        &mut self,
        data_stream_id: &DataStreamId,
    ) -> Result<(), Error> {
        let global_data_summary = self.get_global_data_summary();

        // If there was a send failure, terminate the stream
        let data_stream = self.get_data_stream(data_stream_id)?;
        if data_stream.send_failure() {
            info!(
                (LogSchema::new(LogEntry::TerminateStream)
                    .stream_id(*data_stream_id)
                    .event(LogEvent::Success)
                    .message("There was a send failure, terminating the stream."))
            );
            metrics::DATA_STREAM_SEND_FAILURE.inc();
            if self.data_streams.remove(data_stream_id).is_none() {
                return Err(Error::UnexpectedErrorEncountered(format!(
                    "Failed to terminate stream id {:?} for send failure! Stream not found.",
                    data_stream_id
                )));
            }
            return Ok(());
        }

        // Drive data stream progress
        if !data_stream.data_requests_initialized() {
            // Initialize the request batch by sending out data client requests
            data_stream.initialize_data_requests(global_data_summary)?;
            info!(
                (LogSchema::new(LogEntry::InitializeStream)
                    .stream_id(*data_stream_id)
                    .event(LogEvent::Success)
                    .message("Data stream initialized."))
            );
        } else {
            // Process any data client requests that have received responses
            data_stream
                .process_data_responses(global_data_summary)
                .await?;
        }

        Ok(())
    }
```

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L408-428)
```rust
/// Spawns a task that periodically refreshes the global data summary
fn spawn_global_data_summary_refresher<T: AptosDataClientInterface + Send + Clone + 'static>(
    data_streaming_service_config: DataStreamingServiceConfig,
    aptos_data_client: T,
    cached_global_data_summary: Arc<ArcSwap<GlobalDataSummary>>,
) {
    tokio::spawn(async move {
        loop {
            // Refresh the cached global data summary
            refresh_global_data_summary(
                aptos_data_client.clone(),
                cached_global_data_summary.clone(),
            );

            // Sleep for a while before refreshing the cache again
            let sleep_duration_ms =
                data_streaming_service_config.global_summary_refresh_interval_ms;
            tokio::time::sleep(Duration::from_millis(sleep_duration_ms)).await;
        }
    });
}
```

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L430-452)
```rust
/// Refreshes the global data summary and updates the cache
fn refresh_global_data_summary<T: AptosDataClientInterface + Send + Clone + 'static>(
    aptos_data_client: T,
    cached_global_data_summary: Arc<ArcSwap<GlobalDataSummary>>,
) {
    // Fetch the global data summary and update the cache
    match fetch_global_data_summary(aptos_data_client) {
        Ok(global_data_summary) => {
            // Update the cached global data summary
            cached_global_data_summary.store(Arc::new(global_data_summary));
        },
        Err(error) => {
            // Otherwise, log an error and increment the error counter
            sample!(
                SampleRate::Duration(Duration::from_secs(GLOBAL_DATA_REFRESH_LOG_FREQ_SECS)),
                warn!(LogSchema::new(LogEntry::RefreshGlobalData)
                    .event(LogEvent::Error)
                    .error(&error))
            );
            metrics::increment_counter(&metrics::GLOBAL_DATA_SUMMARY_ERROR, error.get_label());
        },
    }
}
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L209-219)
```rust
    /// Initializes the data client requests by sending out the first batch
    pub fn initialize_data_requests(
        &mut self,
        global_data_summary: GlobalDataSummary,
    ) -> Result<(), Error> {
        // Initialize the data client requests queue
        self.sent_data_requests = Some(VecDeque::new());

        // Create and send the data client requests to the network
        self.create_and_send_client_requests(&global_data_summary)
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L440-454)
```rust
    /// Processes any data client responses that have been received. Note: the
    /// responses must be processed in FIFO order.
    pub async fn process_data_responses(
        &mut self,
        global_data_summary: GlobalDataSummary,
    ) -> Result<(), Error> {
        if self.stream_engine.is_stream_complete()
            || self.request_failure_count >= self.streaming_service_config.max_request_retry
            || self.send_failure
        {
            if !self.send_failure && self.stream_end_notification_id.is_none() {
                self.send_end_of_stream_notification().await?;
            }
            return Ok(()); // There's nothing left to do
        }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L729-744)
```rust
    fn resend_data_client_request(
        &mut self,
        data_client_request: &DataClientRequest,
    ) -> Result<(), Error> {
        // Increment the number of client failures for this request
        self.request_failure_count += 1;

        // Resend the client request
        let pending_client_response = self.send_client_request(true, data_client_request.clone());

        // Push the pending response to the head of the sent requests queue
        self.get_sent_data_requests()?
            .push_front(pending_client_response);

        Ok(())
    }
```

**File:** state-sync/data-streaming-service/src/stream_engine.rs (L1797-1847)
```rust
    fn create_data_client_requests(
        &mut self,
        max_number_of_requests: u64,
        max_in_flight_requests: u64,
        num_in_flight_requests: u64,
        global_data_summary: &GlobalDataSummary,
        _unique_id_generator: Arc<U64IdGenerator>,
    ) -> Result<Vec<DataClientRequest>, Error> {
        // Calculate the request end version and optimal chunk sizes
        let (request_end_version, optimal_chunk_sizes) = match &self.request {
            StreamRequest::GetAllTransactions(request) => (
                request.end_version,
                global_data_summary
                    .optimal_chunk_sizes
                    .transaction_chunk_size,
            ),
            StreamRequest::GetAllTransactionOutputs(request) => (
                request.end_version,
                global_data_summary
                    .optimal_chunk_sizes
                    .transaction_output_chunk_size,
            ),
            StreamRequest::GetAllTransactionsOrOutputs(request) => (
                request.end_version,
                global_data_summary
                    .optimal_chunk_sizes
                    .transaction_output_chunk_size,
            ),
            request => invalid_stream_request!(request),
        };

        // Calculate the number of requests to send
        let num_requests_to_send = calculate_num_requests_to_send(
            max_number_of_requests,
            max_in_flight_requests,
            num_in_flight_requests,
        );

        // Create the client requests
        let client_requests = create_data_client_request_batch(
            self.next_request_version,
            request_end_version,
            num_requests_to_send,
            optimal_chunk_sizes,
            self.clone().into(),
        )?;

        // Return the requests
        self.update_request_tracking(&client_requests)?;
        Ok(client_requests)
    }
```

**File:** state-sync/data-streaming-service/src/stream_engine.rs (L1849-1867)
```rust
    fn is_remaining_data_available(&self, advertised_data: &AdvertisedData) -> Result<bool, Error> {
        let (request_end_version, advertised_ranges) = match &self.request {
            StreamRequest::GetAllTransactions(request) => {
                (request.end_version, &advertised_data.transactions)
            },
            StreamRequest::GetAllTransactionOutputs(request) => {
                (request.end_version, &advertised_data.transaction_outputs)
            },
            StreamRequest::GetAllTransactionsOrOutputs(request) => {
                (request.end_version, &advertised_data.transaction_outputs)
            },
            request => invalid_stream_request!(request),
        };
        Ok(AdvertisedData::contains_range(
            self.next_stream_version,
            request_end_version,
            advertised_ranges,
        ))
    }
```
