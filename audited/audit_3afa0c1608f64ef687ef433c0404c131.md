# Audit Report

## Title
Missing Fork Detection on Storage NotFound Errors Enables Undetected Consensus Divergence

## Summary
When a validator receives certified state from other validators (via QuorumCert or LedgerInfo) but encounters NotFound errors in local storage, the error handling does not trigger fork detection protocols. Instead, the system treats it as InvalidPayloadData and silently retries, potentially leaving validators on divergent chains indefinitely without detection.

## Finding Description

The Aptos state synchronization system fails to detect forks when storage NotFound errors indicate state divergence between validators. The vulnerability exists in the error handling path from storage errors through the state sync driver.

**The vulnerable flow:**

1. **Error Propagation**: Storage NotFound errors propagate as generic errors: [1](#0-0) [2](#0-1) 

2. **Generic Error Handling**: Storage synchronizer wraps all errors generically: [3](#0-2) 

3. **No Fork Detection**: Driver treats all storage synchronizer errors as InvalidPayloadData: [4](#0-3) 

4. **Silent Retry**: The system simply resets the stream and retries without comparing state with other validators.

**The Critical Gap:**

When a validator has divergent state (due to corruption, incomplete sync, or an actual fork), and receives a QuorumCert from other validators claiming certain state exists:

- The validator attempts to read or verify local state
- Gets `NotFound` because it has different state at that version
- Error is treated as a generic "data problem" 
- Stream resets and retries with different peers
- **No comparison of root hashes with other validators occurs**
- **No consensus safety alert is raised**
- **Fork remains undetected**

**Contrast with Test Fork Detection:**

Fork detection exists only in test code, which manually compares accumulator root hashes: [5](#0-4) [6](#0-5) 

This verification is **never** triggered during normal operation when storage errors occur.

**Attack Scenario:**

1. Validator experiences state corruption or is on a minority fork
2. Receives certified QuorumCert from honest majority
3. Attempts to sync, but local state reads return NotFound
4. Verification cannot complete because dependent state is missing
5. Error propagates as generic UnexpectedError
6. System retries indefinitely without detecting it's on wrong chain
7. Validator may continue on forked chain if it can source data from Byzantine peers

## Impact Explanation

This represents a **Critical** severity vulnerability under the Aptos bug bounty criteria:

- **Consensus Safety Violation**: Breaks the fundamental invariant that validators must detect when they diverge from the canonical chain. The first critical invariant states "All validators must produce identical state roots for identical blocks" - this vulnerability allows violations to go undetected.

- **Non-Recoverable Network Partition**: If multiple validators end up on different forks without detection, this can create a network partition requiring manual intervention or hardfork to resolve. The system lacks automatic fork detection to trigger recovery.

- **Total Loss of Liveness**: Validators stuck on wrong chains cannot make progress with the honest majority, leading to liveness failures for affected nodes.

The vulnerability directly violates:
- **Invariant #1** (Deterministic Execution): Validators may have different states without detection
- **Invariant #2** (Consensus Safety): Fork prevention fails when detection mechanisms are absent
- **Invariant #4** (State Consistency): State divergence cannot be verified or corrected

## Likelihood Explanation

**High Likelihood** in the following scenarios:

1. **Byzantine Failures**: If > 1/3 validators are Byzantine and create forks, honest validators receiving conflicting state would encounter NotFound errors without detecting the fork

2. **State Corruption**: Database corruption or incomplete state sync naturally triggers NotFound errors, but the system cannot distinguish between "data not arrived yet" vs "on wrong chain"

3. **Execution Divergence**: Any bug causing execution divergence would manifest as NotFound errors during sync, with no automatic fork detection

4. **Recovery Scenarios**: Validators recovering from crashes or rejoining the network are particularly vulnerable

The attack requires no special privileges - it exploits a gap in the error handling logic that affects normal operations.

## Recommendation

Implement automatic fork detection when NotFound errors occur for state claimed by certified QuorumCerts:

```rust
// In state-sync/state-sync-driver/src/driver.rs
async fn handle_error_notification(&mut self, error_notification: ErrorNotification) {
    warn!(LogSchema::new(LogEntry::SynchronizerNotification)
        .error_notification(error_notification.clone())
        .message("Received an error notification from the storage synchronizer!"));

    // NEW: Check for potential fork before treating as generic error
    if self.should_trigger_fork_detection(&error_notification).await {
        // Compare our state root with other validators at the problematic version
        if let Some(version) = self.extract_version_from_error(&error_notification) {
            match self.check_fork_at_version(version).await {
                Ok(true) => {
                    // Fork detected - trigger safety alert
                    error!("FORK DETECTED: State divergence at version {}", version);
                    self.trigger_consensus_safety_violation(version).await;
                    return;
                }
                Ok(false) => {
                    // No fork - proceed with normal error handling
                }
                Err(e) => {
                    warn!("Failed to check for fork: {:?}", e);
                }
            }
        }
    }

    // Existing error handling...
    let notification_id = error_notification.notification_id;
    let notification_feedback = NotificationFeedback::InvalidPayloadData;
    // ... rest of existing code
}

async fn check_fork_at_version(&self, version: Version) -> Result<bool> {
    // Get our local root hash at this version (if it exists)
    let local_root = match self.storage.get_transaction_info_by_version(version) {
        Ok(txn_info) => txn_info.accumulator_root_hash(),
        Err(_) => return Ok(false), // Can't compare if we don't have it
    };
    
    // Query multiple peers for their root hash at this version
    let peer_roots = self.query_peers_for_root_hash(version).await?;
    
    // If majority of peers have different root hash, we're on a fork
    let different_roots = peer_roots.iter()
        .filter(|&root| *root != local_root)
        .count();
    
    Ok(different_roots > peer_roots.len() / 2)
}
```

Additionally, enhance error types to distinguish NotFound scenarios:

```rust
// In storage/storage-interface/src/errors.rs
#[derive(Clone, Debug, Error)]
pub enum AptosDbError {
    #[error("{0} not found.")]
    NotFound(String),
    
    // NEW: Distinguish different NotFound scenarios
    #[error("State not found at version {0}, but certified by quorum cert")]
    PotentialForkDetected(Version),
    
    // ... rest of enum
}
```

## Proof of Concept

```rust
// Integration test demonstrating the vulnerability
#[tokio::test]
async fn test_missing_fork_detection_on_storage_error() {
    // Setup: Create a validator with corrupted state
    let mut validator = setup_test_validator().await;
    
    // Simulate state divergence by corrupting local storage at version 100
    corrupt_storage_at_version(&validator, 100).await;
    
    // Create a certified QuorumCert from other validators at version 150
    // This QC references state at version 100 that we don't have (or have wrong)
    let qc = create_certified_qc_at_version(150, &validator.validator_set).await;
    
    // Attempt to sync to this QC
    let sync_result = validator.sync_to_quorum_cert(qc.clone()).await;
    
    // VULNERABILITY: sync_result will be an error, but:
    // 1. No fork detection was triggered
    // 2. No comparison with other validators occurred
    // 3. No safety alert was raised
    // 4. System will just retry with InvalidPayloadData
    
    assert!(sync_result.is_err());
    
    // Check that fork detection was NOT triggered
    assert!(!validator.was_fork_detection_triggered());
    
    // Check that the validator is still trying to sync
    // (rather than entering a safety mode)
    assert!(validator.is_actively_syncing());
    
    // Verify no consensus safety violation was recorded
    assert!(validator.get_safety_violations().is_empty());
    
    // The validator will keep retrying indefinitely without detecting
    // that it's on a different chain than the quorum
}
```

## Notes

This vulnerability is particularly dangerous because:

1. **Silent Failures**: Validators on forks continue operating without knowing they've diverged
2. **No Manual Detection**: Operators have no alerts that fork detection should be performed
3. **Persistent State**: Once on a fork, a validator may remain there indefinitely
4. **Network Partition Risk**: Multiple validators on different forks fragment the network

The fix requires implementing runtime fork detection similar to what exists in test code, but triggered automatically when storage errors suggest state divergence from certified quorum certificates.

### Citations

**File:** storage/storage-interface/src/errors.rs (L13-14)
```rust
    #[error("{0} not found.")]
    NotFound(String),
```

**File:** execution/executor-types/src/error.rs (L53-59)
```rust
impl From<AptosDbError> for ExecutorError {
    fn from(error: AptosDbError) -> Self {
        Self::InternalError {
            error: format!("{}", error),
        }
    }
}
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L1321-1347)
```rust
async fn send_storage_synchronizer_error(
    mut error_notification_sender: mpsc::UnboundedSender<ErrorNotification>,
    notification_id: NotificationId,
    error_message: String,
) {
    // Log the storage synchronizer error
    let error_message = format!("Storage synchronizer error: {:?}", error_message);
    error!(LogSchema::new(LogEntry::StorageSynchronizer).message(&error_message));

    // Update the storage synchronizer error metrics
    let error = Error::UnexpectedError(error_message);
    metrics::increment_counter(&metrics::STORAGE_SYNCHRONIZER_ERRORS, error.get_label());

    // Send an error notification to the driver
    let error_notification = ErrorNotification {
        error: error.clone(),
        notification_id,
    };
    if let Err(error) = error_notification_sender.send(error_notification).await {
        error!(
            LogSchema::new(LogEntry::StorageSynchronizer).message(&format!(
                "Failed to send error notification! Error: {:?}",
                error
            ))
        );
    }
}
```

**File:** state-sync/state-sync-driver/src/driver.rs (L494-533)
```rust
    /// Handles an error notification sent by the storage synchronizer
    async fn handle_error_notification(&mut self, error_notification: ErrorNotification) {
        warn!(LogSchema::new(LogEntry::SynchronizerNotification)
            .error_notification(error_notification.clone())
            .message("Received an error notification from the storage synchronizer!"));

        // Terminate the currently active streams
        let notification_id = error_notification.notification_id;
        let notification_feedback = NotificationFeedback::InvalidPayloadData;
        if self.bootstrapper.is_bootstrapped() {
            if let Err(error) = self
                .continuous_syncer
                .handle_storage_synchronizer_error(NotificationAndFeedback::new(
                    notification_id,
                    notification_feedback,
                ))
                .await
            {
                error!(LogSchema::new(LogEntry::SynchronizerNotification)
                    .message(&format!(
                        "Failed to terminate the active stream for the continuous syncer! Error: {:?}",
                        error
                    )));
            }
        } else if let Err(error) = self
            .bootstrapper
            .handle_storage_synchronizer_error(NotificationAndFeedback::new(
                notification_id,
                notification_feedback,
            ))
            .await
        {
            error!(
                LogSchema::new(LogEntry::SynchronizerNotification).message(&format!(
                    "Failed to terminate the active stream for the bootstrapper! Error: {:?}",
                    error
                ))
            );
        };
    }
```

**File:** testsuite/forge/src/interface/swarm.rs (L171-192)
```rust
    async fn are_root_hashes_equal_at_version(
        clients: &[RestClient],
        version: u64,
    ) -> Result<bool> {
        let root_hashes = try_join_all(
            clients
                .iter()
                .map(|node| node.get_transaction_by_version(version))
                .collect::<Vec<_>>(),
        )
        .await?
        .into_iter()
        .map(|r| {
            r.into_inner()
                .transaction_info()
                .unwrap()
                .accumulator_root_hash
        })
        .collect::<Vec<_>>();

        Ok(root_hashes.windows(2).all(|w| w[0] == w[1]))
    }
```

**File:** testsuite/forge/src/interface/swarm.rs (L195-240)
```rust
    async fn fork_check(&self, epoch_duration: Duration) -> Result<()> {
        // Lots of errors can actually occur after an epoch change so guarantee that we change epochs here
        // This can wait for 2x epoch to at least force the caller to be explicit about the epoch duration
        self.wait_for_all_nodes_to_change_epoch(epoch_duration * 2)
            .await?;

        let clients = self
            .validators()
            .map(|node| node.rest_client())
            .chain(self.full_nodes().map(|node| node.rest_client()))
            .collect::<Vec<_>>();

        let versions = try_join_all(
            clients
                .iter()
                .map(|node| node.get_ledger_information())
                .collect::<Vec<_>>(),
        )
        .await?
        .into_iter()
        .map(|resp| resp.into_inner().version)
        .collect::<Vec<u64>>();
        let min_version = versions
            .iter()
            .min()
            .copied()
            .ok_or_else(|| anyhow!("Unable to query nodes for their latest version"))?;
        let max_version = versions
            .iter()
            .max()
            .copied()
            .ok_or_else(|| anyhow!("Unable to query nodes for their latest version"))?;

        if !Self::are_root_hashes_equal_at_version(&clients, min_version).await? {
            return Err(anyhow!("Fork check failed"));
        }

        self.wait_for_all_nodes_to_catchup_to_version(max_version, Duration::from_secs(10))
            .await?;

        if !Self::are_root_hashes_equal_at_version(&clients, max_version).await? {
            return Err(anyhow!("Fork check failed"));
        }

        Ok(())
    }
```
