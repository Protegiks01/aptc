# Audit Report

## Title
Quorum Store V2 Batch Storage Leak: Incorrect Column Family Deletion Causes Unbounded Disk Growth

## Summary
The quorum store database garbage collection logic incorrectly deletes V2 batches from the V1 column family (`BATCH_CF_NAME`) instead of the V2 column family (`BATCH_V2_CF_NAME`). This causes V2 batches to persist indefinitely on disk, leading to unbounded storage growth that can eventually cause validator node failures and network liveness issues.

## Finding Description

The quorum store maintains three isolated RocksDB column families for batch storage:
- `BATCH_CF_NAME` ("batch") - stores V1 batches
- `BATCH_V2_CF_NAME` ("batch_v2") - stores V2 batches  
- `BATCH_ID_CF_NAME` ("batch_ID") - stores batch ID counters

While the schema layer properly isolates these column families through type-safe access patterns, the garbage collection logic violates this isolation by attempting to delete V2 batches using V1 deletion methods. [1](#0-0) 

**Bug Location 1: Epoch Transition Garbage Collection**

In `gc_previous_epoch_batches_from_db_v2`, the function reads V2 batches from the correct column family but deletes them from the wrong one: [2](#0-1) 

Line 214 correctly calls `db.get_all_batches_v2()` which reads from `BATCH_V2_CF_NAME`, but line 241 incorrectly calls `db.delete_batches(expired_keys)` which deletes from `BATCH_CF_NAME` instead of calling `db.delete_batches_v2(expired_keys)`.

**Bug Location 2: Expiration-Based Garbage Collection**

In `update_certified_timestamp`, expired batches from the unified in-memory cache (which contains both V1 and V2 batches) are only deleted from the V1 column family: [3](#0-2) 

The `clear_expired_payload` method removes entries from the in-memory cache that contains `PersistedValue<BatchInfoExt>` (which can be either V1 or V2), but only returns digest hashes without version information: [4](#0-3) 

At line 458, the full `PersistedValue<BatchInfoExt>` is available (which has `batch_info().is_v2()` to determine version), but only the digest is returned. Line 536 then calls `db.delete_batches(expired_keys)` which only deletes from `BATCH_CF_NAME`.

**Root Cause**

The schema layer correctly enforces column family isolation through type-safe operations: [5](#0-4) 

Each schema operation uses `S::COLUMN_FAMILY_NAME` to access the correct column family. However, the batch store logic fails to maintain this isolation during deletion by:
1. Using the wrong deletion method (`delete_batches` instead of `delete_batches_v2`)
2. Discarding version information when returning expired keys

**Configuration Context**

V2 batches are enabled via the `enable_batch_v2` configuration flag: [6](#0-5) 

When enabled, validators create and persist V2 batches that are never properly cleaned up from disk.

## Impact Explanation

**Severity: HIGH** (qualifies for up to $50,000 per Aptos bug bounty)

This vulnerability causes:

1. **Validator Node Slowdowns**: As the `BATCH_V2_CF_NAME` column family grows unbounded, RocksDB operations become slower due to increased compaction overhead and larger index structures. This directly matches the HIGH severity criteria of "Validator node slowdowns".

2. **Storage Exhaustion**: Over time, the accumulation of undeleted V2 batches will fill the validator's disk. When disk space is exhausted:
   - The validator cannot write new batches or commit new blocks
   - The validator may crash or become unresponsive
   - If multiple validators are affected, network liveness degrades

3. **State Inconsistency**: The in-memory cache and persistent storage become inconsistent - the cache believes batches are deleted, but they remain on disk. This violates the "State Consistency" invariant.

4. **Resource Limit Violation**: The bug allows unbounded storage growth, violating the "Resource Limits" invariant that all operations must respect storage constraints.

While this doesn't directly cause consensus safety violations or fund loss, it represents a significant protocol violation that degrades network availability over time.

## Likelihood Explanation

**Likelihood: HIGH**

This bug will occur with 100% certainty on any validator network where:
1. The `enable_batch_v2` configuration is set to `true`
2. The network runs long enough for multiple epoch transitions or batch expirations to occur

No attacker action is required - this is a passive bug in normal operation. The impact accumulates over time:
- **Short term (hours-days)**: Minimal impact as storage growth is gradual
- **Medium term (weeks)**: Noticeable disk usage increase and potential performance degradation
- **Long term (months)**: Risk of disk exhaustion and validator failures

The bug is guaranteed to trigger because:
- Epoch transitions call `gc_previous_epoch_batches_from_db_v2` automatically
- Batch expirations call `update_certified_timestamp` continuously during operation
- Both code paths contain the deletion bug

## Recommendation

**Fix 1: Correct Epoch Transition Garbage Collection** [7](#0-6) 

Change line 241 from:
```rust
db.delete_batches(expired_keys)
```

To:
```rust
db.delete_batches_v2(expired_keys)
```

**Fix 2: Implement Version-Aware Expiration Deletion**

Modify `clear_expired_payload` to return version information: [8](#0-7) 

Change the return type to `Vec<(HashValue, bool)>` where the bool indicates `is_v2`, and update line 468 to:
```rust
ret.push((h, value.batch_info().is_v2()));
```

Then modify `update_certified_timestamp`: [9](#0-8) 

Split the expired keys by version and call the appropriate deletion method for each:
```rust
let expired_keys = self.clear_expired_payload(certified_time);
let (v1_keys, v2_keys): (Vec<_>, Vec<_>) = expired_keys
    .into_iter()
    .partition(|(_, is_v2)| !is_v2);

if let Err(e) = self.db.delete_batches(v1_keys.into_iter().map(|(k, _)| k).collect()) {
    debug!("Error deleting V1 batches: {:?}", e)
}
if let Err(e) = self.db.delete_batches_v2(v2_keys.into_iter().map(|(k, _)| k).collect()) {
    debug!("Error deleting V2 batches: {:?}", e)
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod column_family_isolation_test {
    use super::*;
    use aptos_consensus_types::proof_of_store::{BatchInfo, BatchInfoExt};
    use aptos_crypto::HashValue;
    use aptos_types::quorum_store::BatchId;
    use tempfile::TempDir;

    #[test]
    fn test_v2_batch_deletion_bug() {
        // Setup: Create a QuorumStoreDB
        let temp_dir = TempDir::new().unwrap();
        let db = QuorumStoreDB::new(temp_dir.path());
        
        // Create and save a V2 batch
        let batch_info_v2 = BatchInfoExt::new_v2(
            PeerId::random(),
            BatchId::new_for_test(1),
            1, // epoch
            1000000, // expiration
            HashValue::random(),
            10, // num_txns
            1000, // num_bytes
            0, // gas_bucket_start
            BatchKind::Normal,
        );
        let persisted_v2 = PersistedValue::new(batch_info_v2.clone(), None);
        db.save_batch_v2(persisted_v2.clone()).unwrap();
        
        // Verify V2 batch exists in BATCH_V2_CF_NAME
        assert!(db.get_batch_v2(batch_info_v2.digest()).unwrap().is_some());
        
        // Simulate the bug: call delete_batches (V1 deletion) on V2 digest
        db.delete_batches(vec![*batch_info_v2.digest()]).unwrap();
        
        // BUG DEMONSTRATION: V2 batch still exists after "deletion"
        // because delete_batches only deleted from BATCH_CF_NAME, not BATCH_V2_CF_NAME
        let still_exists = db.get_batch_v2(batch_info_v2.digest()).unwrap();
        assert!(still_exists.is_some(), 
            "V2 batch should still exist - demonstrating the storage leak bug");
        
        // Correct deletion should use delete_batches_v2
        db.delete_batches_v2(vec![*batch_info_v2.digest()]).unwrap();
        assert!(db.get_batch_v2(batch_info_v2.digest()).unwrap().is_none(),
            "V2 batch should now be deleted");
    }
}
```

## Notes

This vulnerability demonstrates a critical failure in maintaining data isolation guarantees at the application layer despite correct infrastructure-level column family separation. The RocksDB column families are properly isolated at the storage engine level, but incorrect API usage in the garbage collection logic violates this isolation.

The issue is particularly insidious because:
1. It fails silently - no errors are raised when deleting from the wrong column family
2. The in-memory cache appears correct while disk state diverges
3. The impact is gradual but eventually catastrophic

Networks that have not yet enabled `enable_batch_v2=true` are not affected, but any future activation of V2 batches will trigger this bug immediately.

### Citations

**File:** consensus/src/quorum_store/schema.rs (L14-16)
```rust
pub(crate) const BATCH_CF_NAME: ColumnFamilyName = "batch";
pub(crate) const BATCH_ID_CF_NAME: ColumnFamilyName = "batch_ID";
pub(crate) const BATCH_V2_CF_NAME: ColumnFamilyName = "batch_v2";
```

**File:** consensus/src/quorum_store/batch_store.rs (L212-243)
```rust
    fn gc_previous_epoch_batches_from_db_v2(db: Arc<dyn QuorumStoreStorage>, current_epoch: u64) {
        let db_content = db
            .get_all_batches_v2()
            .expect("failed to read data from db");
        info!(
            epoch = current_epoch,
            "QS: Read batches from storage. Len: {}",
            db_content.len(),
        );

        let mut expired_keys = Vec::new();
        for (digest, value) in db_content {
            let epoch = value.epoch();

            trace!(
                "QS: Batchreader recovery content epoch {:?}, digest {}",
                epoch,
                digest
            );

            if epoch < current_epoch {
                expired_keys.push(digest);
            }
        }

        info!(
            "QS: Batch store bootstrap expired keys len {}",
            expired_keys.len()
        );
        db.delete_batches(expired_keys)
            .expect("Deletion of expired keys should not fail");
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L442-472)
```rust
    #[allow(clippy::unwrap_used)]
    pub(crate) fn clear_expired_payload(&self, certified_time: u64) -> Vec<HashValue> {
        // To help slow nodes catch up via execution without going to state sync we keep the blocks for 60 extra seconds
        // after the expiration time. This will help remote peers fetch batches that just expired but are within their
        // execution window.
        let expiration_time = certified_time.saturating_sub(self.expiration_buffer_usecs);
        let expired_digests = self.expirations.lock().expire(expiration_time);
        let mut ret = Vec::new();
        for h in expired_digests {
            let removed_value = match self.db_cache.entry(h) {
                Occupied(entry) => {
                    // We need to check up-to-date expiration again because receiving the same
                    // digest with a higher expiration would update the persisted value and
                    // effectively extend the expiration.
                    if entry.get().expiration() <= expiration_time {
                        self.persist_subscribers.remove(entry.get().digest());
                        Some(entry.remove())
                    } else {
                        None
                    }
                },
                Vacant(_) => unreachable!("Expired entry not in cache"),
            };
            // No longer holding the lock on db_cache entry.
            if let Some(value) = removed_value {
                self.free_quota(value);
                ret.push(h);
            }
        }
        ret
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L530-539)
```rust
    pub fn update_certified_timestamp(&self, certified_time: u64) {
        trace!("QS: batch reader updating time {:?}", certified_time);
        self.last_certified_time
            .fetch_max(certified_time, Ordering::SeqCst);

        let expired_keys = self.clear_expired_payload(certified_time);
        if let Err(e) = self.db.delete_batches(expired_keys) {
            debug!("Error deleting batches: {:?}", e)
        }
    }
```

**File:** storage/schemadb/src/lib.rs (L215-232)
```rust
    /// Reads single record by key.
    pub fn get<S: Schema>(&self, schema_key: &S::Key) -> DbResult<Option<S::Value>> {
        let _timer = APTOS_SCHEMADB_GET_LATENCY_SECONDS.timer_with(&[S::COLUMN_FAMILY_NAME]);

        let k = <S::Key as KeyCodec<S>>::encode_key(schema_key)?;
        let cf_handle = self.get_cf_handle(S::COLUMN_FAMILY_NAME)?;

        let result = self.inner.get_cf(cf_handle, k).into_db_res()?;
        APTOS_SCHEMADB_GET_BYTES.observe_with(
            &[S::COLUMN_FAMILY_NAME],
            result.as_ref().map_or(0.0, |v| v.len() as f64),
        );

        result
            .map(|raw_value| <S::Value as ValueCodec<S>>::decode_value(&raw_value))
            .transpose()
            .map_err(Into::into)
    }
```

**File:** config/src/config/quorum_store_config.rs (L102-102)
```rust
    pub enable_batch_v2: bool,
```
