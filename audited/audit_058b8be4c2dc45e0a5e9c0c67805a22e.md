# Audit Report

## Title
Unencrypted HTTP Metrics Scraping Enables Man-in-the-Middle Tampering of Validator Health Data

## Summary
The Node Health Checker (NHC) system scrapes Prometheus metrics from validator fullnodes using unencrypted HTTP connections with no integrity protection. The URL scheme is hardcoded to HTTP, enabling network attackers to perform Man-in-the-Middle (MITM) attacks and tamper with critical consensus metrics (such as `aptos_consensus_timeout_count`) during transmission. This allows malicious validators to appear healthy when they are actually compromised or failing, undermining the integrity of validator health assessments used for AIT (Aptos Incentivized Testnet) participation.

## Finding Description

The vulnerability exists in the validator fullnode (VFN) health checking flow used for AIT3 validator registration. When the `fn-check-client` tool queries validator health:

1. **Network addresses are extracted from on-chain validator set** [1](#0-0) 

2. **IP addresses are hardcoded to HTTP URLs** - The critical vulnerability occurs here where the scheme is forced to `http://`: [2](#0-1) 

3. **These HTTP URLs are passed to NHC** which queries the node with these unencrypted endpoints: [3](#0-2) 

4. **NHC scrapes Prometheus metrics over HTTP** - The MetricsProvider performs plain HTTP GET requests with no authentication: [4](#0-3) 

5. **No TLS configuration in HTTP client** - The reqwest client is built without any TLS certificate validation settings: [5](#0-4) 

6. **Consensus metrics are extracted without integrity verification**: [6](#0-5) 

**Attack Scenario:**
A malicious validator operator participating in AIT3:
1. Registers their VFN with an on-chain network address
2. Positions a MITM proxy on the network path (or controls network infrastructure)
3. When NHC scrapes metrics via HTTP, the attacker intercepts the request
4. Attacker modifies the Prometheus response to show healthy metrics:
   - Sets `aptos_consensus_timeout_count` to 0 (hiding timeout issues)
   - Manipulates peer counts, round numbers, or other health indicators
5. NHC receives tampered data and reports the node as healthy
6. Malicious/failing node passes validation and participates in AIT3

The security invariant broken is **Data Integrity**: "Prometheus scrape results must be protected against tampering during transmission." No cryptographic integrity protection exists—no HTTPS, no signed metrics, no HMAC verification, no authentication tokens.

## Impact Explanation

**Medium Severity** per Aptos Bug Bounty criteria:
- **State inconsistencies requiring intervention**: Tampered health metrics lead to incorrect validator health assessments, creating inconsistencies in the monitoring state that may require manual intervention to correct
- **Limited manipulation**: While the attack doesn't directly compromise consensus or cause fund loss, it manipulates the validator selection/assessment process for AIT3

The impact is constrained because:
- Node-checker is an ecosystem monitoring tool, not part of core consensus
- Attack requires network-level MITM capability
- Does not directly cause consensus safety violations or fund theft
- Primarily affects validator health reporting and AIT participation decisions

However, the vulnerability is significant because:
- AIT3 validator selection may rely on these health assessments
- Consensus timeout metrics (`aptos_consensus_timeout_count`) are critical indicators
- Allows unhealthy/malicious validators to masquerade as healthy
- Undermines trust in the entire validator monitoring infrastructure

## Likelihood Explanation

**Moderate to High Likelihood:**

**Favorable conditions for exploitation:**
- HTTP is hardcoded (line 21 of helpers.rs), making MITM attacks trivial
- Validator operators control their network infrastructure and could position proxies
- No authentication makes spoofing trivial—any HTTP server can respond with fake metrics
- The attack surface is large (every VFN metrics endpoint checked)
- Cloud/datacenter environments may have compromised network paths

**Attack complexity:**
- Low technical barrier: Simple HTTP MITM proxy (e.g., mitmproxy, Burp Suite)
- No cryptographic bypasses needed
- Attacker only needs to modify plaintext HTTP responses
- Can be fully automated

**Real-world feasibility:**
- Validators often run in cloud environments where network MITM is possible
- BGP hijacking, ARP spoofing, or compromised routers enable interception
- DNS hijacking could redirect traffic to attacker-controlled metrics servers
- The use case (remote validator health checking over the internet) makes MITM realistic

## Recommendation

Implement multiple layers of security:

**1. Enforce HTTPS with certificate validation:**

Modify `helpers.rs` to use HTTPS:
```rust
// ecosystem/node-checker/fn-check-client/src/helpers.rs
pub fn extract_network_address(network_address: &NetworkAddress) -> Result<(Url, u16)> {
    // ... existing address parsing ...
    match socket_addr {
        SocketAddr::V4(addr) => Ok((
            Url::parse(&format!("https://{}", addr.ip()))  // Use HTTPS instead of HTTP
                .context("Failed to parse address as URL")?,
            addr.port(),
        )),
        // ...
    }
}
```

**2. Configure TLS validation in reqwest client:**

Update `node_address.rs`:
```rust
// ecosystem/node-checker/src/configuration/node_address.rs
pub fn get_metrics_client(&self, timeout: Duration) -> Result<reqwest::Client> {
    match self.metrics_port {
        Some(_) => Ok(reqwest::ClientBuilder::new()
            .timeout(timeout)
            .tls_built_in_root_certs(true)  // Add explicit TLS validation
            .cookie_provider(self.cookie_store.clone())
            .build()
            .unwrap()),
        None => Err(anyhow!("Cannot build metrics client without a metrics port")),
    }
}
```

**3. Add authentication (recommended additional layer):**

Implement bearer token authentication for metrics endpoints:
```rust
// ecosystem/node-checker/src/provider/metrics.rs
pub async fn get_scrape(&self) -> Result<Scrape, ProviderError> {
    let mut request = self.client.get(self.metrics_url.clone());
    
    // Add bearer token if configured
    if let Some(token) = &self.config.auth_token {
        request = request.bearer_auth(token);
    }
    
    let response = request.send().await
        .with_context(|| format!("Failed to get data from {}", self.metrics_url))
        .map_err(|e| ProviderError::RetryableEndpointError("/metrics", e))?;
    // ... rest of implementation
}
```

**4. Validate URL scheme before use:**

Add validation in `check.rs` to reject non-HTTPS URLs in production:
```rust
// Reject HTTP URLs in production environments
if node_url.scheme() != "https" && !cfg!(test) {
    return SingleCheckResult::NodeCheckFailure(NodeCheckFailure::new(
        "Metrics endpoint must use HTTPS for security".to_string(),
        NodeCheckFailureCode::InsecureScheme,
    ));
}
```

## Proof of Concept

**Setup MITM Attack:**

```bash
# Terminal 1: Start a malicious HTTP server that returns fake healthy metrics
cat > fake_metrics.txt << 'EOF'
# HELP aptos_consensus_timeout_count Consensus timeout count
# TYPE aptos_consensus_timeout_count counter
aptos_consensus_timeout_count 0
EOF

python3 -m http.server 9101 --bind 0.0.0.0 &

# Terminal 2: Configure network to intercept traffic (example using iptables)
# Redirect actual validator metrics to attacker's fake server
sudo iptables -t nat -A PREROUTING -p tcp --dport 9101 \
    -j DNAT --to-destination 127.0.0.1:9101
```

**Run fn-check-client:**
```bash
# The tool will scrape from the MITM server
cargo run -p aptos-fn-check-client -- \
    --nhc-address http://127.0.0.1:20121 \
    --nhc-baseline-config-name ait3_vfn \
    check-validator-full-nodes \
    --node-address http://fullnode.devnet.aptoslabs.com

# Expected result: Node appears healthy despite actual metrics showing failures
# The ConsensusTimeoutsChecker will report 0 timeouts (fake data)
# instead of the real timeout count from the validator
```

**Demonstrate tampering detection absence:**
```rust
// This test demonstrates that tampered metrics are accepted without validation
#[tokio::test]
async fn test_tampered_metrics_accepted() {
    // Start mock HTTP server with manipulated metrics
    let mock_server = MockServer::start().await;
    
    Mock::given(method("GET"))
        .and(path("/metrics"))
        .respond_with(ResponseTemplate::new(200).set_body_string(
            "aptos_consensus_timeout_count 0\n" // Fake healthy value
        ))
        .mount(&mock_server)
        .await;
    
    // Create metrics provider pointing to malicious server
    let url = Url::parse(&mock_server.uri()).unwrap();
    let client = Arc::new(reqwest::Client::new());
    let provider = MetricsProvider::new(
        MetricsProviderConfig::default(),
        client,
        url,
        9101
    );
    
    // Scrape succeeds with tampered data - no integrity check fails
    let scrape = provider.provide().await.unwrap();
    
    // Tampered metric is accepted as valid
    assert_eq!(
        get_metric_value(&scrape, "aptos_consensus_timeout_count", None),
        Some(0) // Attacker's fake value is trusted
    );
}
```

**Notes**

The vulnerability is rooted in a fundamental design choice: hardcoding HTTP for metrics scraping in a system designed to check remote validators over the public internet. While Prometheus metrics are often served over HTTP in trusted internal networks, the AIT3 use case requires checking validators operated by external parties across untrusted networks.

The comparison with other code in the repository is telling—the governance metadata fetcher explicitly uses `.tls_built_in_root_certs(true)`: [7](#0-6) 

This demonstrates that the team understands the need for TLS validation when fetching data from external sources, but this principle was not applied to the node-checker's metrics scraping functionality.

Additional context from the codebase shows this is used for critical infrastructure: [8](#0-7) 

The node-checker is explicitly used to "confirm operators are running quality, operational VFNs as part of AIT3", making the integrity of these health checks security-relevant for validator selection and network quality assurance.

### Citations

**File:** ecosystem/node-checker/fn-check-client/src/get_vfns.rs (L85-101)
```rust
            let vfn_addresses = match validator_info.config().fullnode_network_addresses() {
                Ok(vfn_addresses) => vfn_addresses,
                Err(e) => {
                    invalid_node_address_results
                        .entry(*account_address)
                        .or_insert_with(Vec::new)
                        .push(SingleCheck::new(
                            SingleCheckResult::CouldNotDeserializeNetworkAddress(
                                CouldNotDeserializeNetworkAddress {
                                    message: format!("{:#}", e),
                                },
                            ),
                            None,
                        ));
                    continue;
                },
            };
```

**File:** ecosystem/node-checker/fn-check-client/src/helpers.rs (L20-24)
```rust
        SocketAddr::V4(addr) => Ok((
            Url::parse(&format!("http://{}", addr.ip()))
                .context("Failed to parse address as URL")?,
            addr.port(),
        )),
```

**File:** ecosystem/node-checker/fn-check-client/src/check.rs (L198-209)
```rust
        // Build up query params.
        let mut params = HashMap::new();
        params.insert("node_url", node_url.to_string());
        params.insert("api_port", api_port.to_string());
        params.insert("noise_port", noise_port.to_string());
        params.insert(
            "baseline_configuration_name",
            self.nhc_baseline_config_name.clone(),
        );
        if let Some(public_key) = public_key {
            params.insert("public_key", public_key.to_encoded_string().unwrap());
        }
```

**File:** ecosystem/node-checker/src/provider/metrics.rs (L59-85)
```rust
    pub async fn get_scrape(&self) -> Result<Scrape, ProviderError> {
        let response = self
            .client
            .get(self.metrics_url.clone())
            .send()
            .await
            .with_context(|| format!("Failed to get data from {}", self.metrics_url))
            .map_err(|e| ProviderError::RetryableEndpointError("/metrics", e))?;
        let body = response
            .text()
            .await
            .with_context(|| {
                format!(
                    "Failed to process response body from {} as text",
                    self.metrics_url
                )
            })
            .map_err(|e| ProviderError::ParseError(anyhow!(e)))?;
        Scrape::parse(body.lines().map(|l| Ok(l.to_string())))
            .with_context(|| {
                format!(
                    "Failed to parse response text from {} as a Prometheus scrape",
                    self.metrics_url
                )
            })
            .map_err(|e| ProviderError::ParseError(anyhow!(e)))
    }
```

**File:** ecosystem/node-checker/src/configuration/node_address.rs (L94-105)
```rust
    pub fn get_metrics_client(&self, timeout: Duration) -> Result<reqwest::Client> {
        match self.metrics_port {
            Some(_) => Ok(reqwest::ClientBuilder::new()
                .timeout(timeout)
                .cookie_provider(self.cookie_store.clone())
                .build()
                .unwrap()),
            None => Err(anyhow!(
                "Cannot build metrics client without a metrics port"
            )),
        }
    }
```

**File:** ecosystem/node-checker/src/checker/consensus_timeouts.rs (L95-152)
```rust
    async fn check(
        &self,
        providers: &ProviderCollection,
    ) -> Result<Vec<CheckResult>, CheckerError> {
        let target_metrics_provider = get_provider!(
            providers.target_metrics_provider,
            self.config.common.required,
            MetricsProvider
        );

        let first_scrape = match target_metrics_provider.provide().await {
            Ok(scrape) => scrape,
            Err(e) => {
                return Ok(vec![Self::build_result(
                    "Failed to check consensus timeouts".to_string(),
                    0,
                    format!(
                        "Failed to scrape metrics from your node (1st time): {:#}",
                        e
                    ),
                )])
            },
        };

        tokio::time::sleep(target_metrics_provider.config.common.check_delay()).await;

        let second_scrape = match target_metrics_provider.provide().await {
            Ok(scrape) => scrape,
            Err(e) => {
                return Ok(vec![Self::build_result(
                    "Failed to check consensus timeouts".to_string(),
                    0,
                    format!(
                        "Failed to scrape metrics from your node (2nd time): {:#}",
                        e
                    ),
                )])
            },
        };

        let mut check_results = vec![];

        let previous_round = self
            .get_consensus_timeouts(&first_scrape, "first")
            .unwrap(&mut check_results);

        let latest_round = self
            .get_consensus_timeouts(&second_scrape, "second")
            .unwrap(&mut check_results);

        if !check_results.is_empty() {
            return Ok(check_results);
        }

        Ok(vec![self.build_check_result(
            previous_round.unwrap(),
            latest_round.unwrap(),
        )])
```

**File:** crates/aptos/src/governance/mod.rs (L422-425)
```rust
    let client = reqwest::ClientBuilder::default()
        .tls_built_in_root_certs(true)
        .build()
        .map_err(|err| CliError::UnexpectedError(format!("Failed to build HTTP client {}", err)))?;
```

**File:** ecosystem/node-checker/fn-check-client/README.md (L1-10)
```markdown
# Validator FullNode (VFN) NHC periodic checker

## Description
This tool is a client to NHC that does the following:
1. Get the validator set from any node participating in a network we want to test.
2. Process that to create a map from operator account address to VFN network addresses.
3. Query NHC for each VFN.
4. Push the results to BigQuery.

The original intent behind this tool is to confirm operators are running quality, operational VFNs as part of AIT3. This tool can be easily adapted for other use cases down the line.
```
