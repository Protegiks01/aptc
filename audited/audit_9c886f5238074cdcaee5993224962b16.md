# Audit Report

## Title
Unauthenticated Access to Backup Service Exposing Complete Blockchain State in Kubernetes Deployments

## Summary
The backup service HTTP endpoints lack authentication and are configured to listen on all network interfaces (`0.0.0.0:6186`) in Kubernetes deployments, exposing the complete blockchain state, transaction history, and sensitive ledger data to any workload within the cluster without authorization.

## Finding Description

The `reply_with_bytes_sender()` function in the backup service handlers is used to stream sensitive blockchain data through HTTP endpoints without any authentication or authorization checks. [1](#0-0) 

These handlers are exposed via unauthenticated HTTP GET endpoints: [2](#0-1) 

The endpoints expose critical blockchain data through the `BackupHandler`: [3](#0-2) [4](#0-3) 

While the default configuration binds to localhost only (`127.0.0.1:6186`): [5](#0-4) 

The production Kubernetes deployments override this to listen on all interfaces (`0.0.0.0:6186`): [6](#0-5) [7](#0-6) 

The service is then exposed within the Kubernetes cluster: [8](#0-7) 

**Attack Path:**
1. Attacker deploys a malicious pod or compromises an existing pod in the same Kubernetes cluster
2. From within the cluster, the attacker makes HTTP GET requests to `http://<fullnode-service>:6186/`
3. Attacker can query endpoints like:
   - `/state_snapshot/<version>` - Extract complete blockchain state
   - `/transactions/<start>/<count>` - Extract transaction history with events and write sets
   - `/epoch_ending_ledger_infos/<start>/<end>` - Extract consensus metadata
4. No authentication is required, allowing unlimited data extraction

This breaks **Invariant #8: Access Control** - sensitive system data should be protected from unauthorized access.

## Impact Explanation

**Severity: Medium (potentially High in multi-tenant environments)**

This vulnerability enables complete information disclosure of:
- All account states, balances, and resources at any version
- Complete transaction history including events and state changes
- Smart contract bytecode and execution details
- Consensus-related ledger information

**Specific Impacts:**
- **Privacy Violations**: All user transaction histories and account states are exposed
- **Reconnaissance**: Attackers can identify high-value accounts for targeted attacks
- **Business Intelligence Leakage**: Competitors could analyze transaction patterns
- **Regulatory Concerns**: Unauthorized access to financial data

This qualifies as **Medium Severity** per the bug bounty program criteria as it constitutes significant information disclosure that could aid in further attacks, though it doesn't directly result in fund loss or consensus violations. In multi-tenant cluster environments, this could escalate to **High Severity** due to cross-tenant data exposure.

## Likelihood Explanation

**Likelihood: High**

This vulnerability is highly likely to be exploited in:
- **Multi-tenant Kubernetes clusters** where multiple organizations share infrastructure
- **Development/staging environments** with relaxed security controls
- **Clusters running third-party applications** that could be malicious
- **Post-compromise scenarios** where an attacker has gained initial pod-level access

The attack requires only:
- Network access to the Kubernetes cluster (e.g., from any pod)
- Basic HTTP client capability (curl, wget, etc.)
- No special privileges or authentication

The service type is `ClusterIP` by default, meaning it's accessible cluster-wide but not externally exposed. However, any compromised or malicious workload in the cluster can freely access all blockchain data.

## Recommendation

**Immediate Actions:**

1. **Implement Authentication**: Add authentication middleware to the backup service endpoints, such as:
   - Mutual TLS (mTLS) with client certificates
   - Bearer token authentication
   - API key validation

2. **Revert to Localhost Binding**: Change production deployments back to `127.0.0.1:6186` and use Kubernetes sidecars or init containers for backup operations

3. **Add Network Policies**: Implement Kubernetes NetworkPolicies to restrict which pods can access the backup service port

**Code Fix Example:**

For authentication middleware in the backup service:

```rust
// In storage/backup/backup-service/src/lib.rs
use warp::Filter;

fn with_auth() -> impl Filter<Extract = (), Error = warp::Rejection> + Clone {
    warp::header::optional::<String>("authorization")
        .and_then(|auth_header: Option<String>| async move {
            match auth_header {
                Some(token) if validate_token(&token) => Ok(()),
                _ => Err(warp::reject::custom(Unauthorized)),
            }
        })
}

pub fn start_backup_service(address: SocketAddr, db: Arc<AptosDB>) -> Runtime {
    let backup_handler = db.get_backup_handler();
    let routes = get_routes(backup_handler).with(with_auth()); // Add auth
    // ... rest of implementation
}
```

For deployment configuration, revert to localhost:

```yaml
# In terraform/helm/fullnode/files/fullnode-base.yaml
storage:
  backup_service_address: "127.0.0.1:6186"  # Changed from 0.0.0.0:6186
```

## Proof of Concept

**Prerequisites:**
- Access to a Kubernetes cluster with a deployed Aptos fullnode
- kubectl access to deploy pods

**Exploitation Steps:**

```bash
# 1. Deploy an attacker pod in the same cluster
kubectl run attacker --image=curlimages/curl --command -- sleep infinity

# 2. Get the backup service endpoint
BACKUP_SERVICE=$(kubectl get svc -l app.kubernetes.io/name=fullnode -o jsonpath='{.items[0].metadata.name}')

# 3. Extract blockchain state (from within attacker pod)
kubectl exec attacker -- curl -s "http://${BACKUP_SERVICE}:6186/db_state"

# 4. Extract transactions
kubectl exec attacker -- curl -s "http://${BACKUP_SERVICE}:6186/transactions/0/1000" > transactions.bin

# 5. Extract complete state snapshot
kubectl exec attacker -- curl -s "http://${BACKUP_SERVICE}:6186/state_snapshot/1000000" > state.bin

# No authentication is required for any of these requests
# All blockchain data is now accessible to the attacker
```

**Verification:**
The attacker can successfully retrieve:
- Database state information (epoch, version)
- Raw transaction data including events and write sets
- Complete state snapshots at any version
- Transaction range proofs and merkle tree data

All without any form of authentication or authorization.

## Notes

This vulnerability is particularly concerning in:
- **Cloud-hosted Kubernetes clusters** where compute resources may be shared
- **Clusters running untrusted workloads** or third-party applications
- **Development environments** that may have weaker security postures

While the default localhost-only binding provides security, the production Helm charts override this configuration, creating a significant attack surface. The lack of NetworkPolicies for fullnodes (unlike validators) further exacerbates this issue.

The exposure is limited to cluster-internal access by default, but this still represents a significant security boundary violation, especially as Kubernetes best practices recommend defense-in-depth rather than relying solely on network segmentation.

### Citations

**File:** storage/backup/backup-service/src/handlers/utils.rs (L46-65)
```rust
pub(super) fn reply_with_bytes_sender<F>(
    backup_handler: &BackupHandler,
    endpoint: &'static str,
    f: F,
) -> Box<dyn Reply>
where
    F: FnOnce(BackupHandler, &mut bytes_sender::BytesSender) -> DbResult<()> + Send + 'static,
{
    let (sender, stream) = bytes_sender::BytesSender::new(endpoint);

    // spawn and forget, error propagates through the `stream: TryStream<_>`
    let bh = backup_handler.clone();
    let _join_handle = tokio::task::spawn_blocking(move || {
        let _timer =
            BACKUP_TIMER.timer_with(&[&format!("backup_service_bytes_sender_{}", endpoint)]);
        abort_on_error(f)(bh, sender)
    });

    Box::new(Response::new(Body::wrap_stream(stream)))
}
```

**File:** storage/backup/backup-service/src/handlers/mod.rs (L27-147)
```rust
pub(crate) fn get_routes(backup_handler: BackupHandler) -> BoxedFilter<(impl Reply,)> {
    // GET db_state
    let bh = backup_handler.clone();
    let db_state = warp::path::end()
        .map(move || reply_with_bcs_bytes(DB_STATE, &bh.get_db_state()?))
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_range_proof/<version>/<end_key>
    let bh = backup_handler.clone();
    let state_range_proof = warp::path!(Version / HashValue)
        .map(move |version, end_key| {
            reply_with_bcs_bytes(
                STATE_RANGE_PROOF,
                &bh.get_account_state_range_proof(end_key, version)?,
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_snapshot/<version>
    let bh = backup_handler.clone();
    let state_snapshot = warp::path!(Version)
        .map(move |version| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT, move |bh, sender| {
                bh.get_state_item_iter(version, 0, usize::MAX)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET state_item_count/<version>
    let bh = backup_handler.clone();
    let state_item_count = warp::path!(Version)
        .map(move |version| {
            reply_with_bcs_bytes(
                STATE_ITEM_COUNT,
                &(bh.get_state_item_count(version)? as u64),
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_snapshot_chunk/<version>/<start_idx>/<limit>
    let bh = backup_handler.clone();
    let state_snapshot_chunk = warp::path!(Version / usize / usize)
        .map(move |version, start_idx, limit| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT_CHUNK, move |bh, sender| {
                bh.get_state_item_iter(version, start_idx, limit)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET state_root_proof/<version>
    let bh = backup_handler.clone();
    let state_root_proof = warp::path!(Version)
        .map(move |version| {
            reply_with_bcs_bytes(STATE_ROOT_PROOF, &bh.get_state_root_proof(version)?)
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET epoch_ending_ledger_infos/<start_epoch>/<end_epoch>/
    let bh = backup_handler.clone();
    let epoch_ending_ledger_infos = warp::path!(u64 / u64)
        .map(move |start_epoch, end_epoch| {
            reply_with_bytes_sender(&bh, EPOCH_ENDING_LEDGER_INFOS, move |bh, sender| {
                bh.get_epoch_ending_ledger_info_iter(start_epoch, end_epoch)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET transactions/<start_version>/<num_transactions>
    let bh = backup_handler.clone();
    let transactions = warp::path!(Version / usize)
        .map(move |start_version, num_transactions| {
            reply_with_bytes_sender(&bh, TRANSACTIONS, move |bh, sender| {
                bh.get_transaction_iter(start_version, num_transactions)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET transaction_range_proof/<first_version>/<last_version>
    let bh = backup_handler;
    let transaction_range_proof = warp::path!(Version / Version)
        .map(move |first_version, last_version| {
            reply_with_bcs_bytes(
                TRANSACTION_RANGE_PROOF,
                &bh.get_transaction_range_proof(first_version, last_version)?,
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // Route by endpoint name.
    let routes = warp::any()
        .and(warp::path(DB_STATE).and(db_state))
        .or(warp::path(STATE_RANGE_PROOF).and(state_range_proof))
        .or(warp::path(STATE_SNAPSHOT).and(state_snapshot))
        .or(warp::path(STATE_ITEM_COUNT).and(state_item_count))
        .or(warp::path(STATE_SNAPSHOT_CHUNK).and(state_snapshot_chunk))
        .or(warp::path(STATE_ROOT_PROOF).and(state_root_proof))
        .or(warp::path(EPOCH_ENDING_LEDGER_INFOS).and(epoch_ending_ledger_infos))
        .or(warp::path(TRANSACTIONS).and(transactions))
        .or(warp::path(TRANSACTION_RANGE_PROOF).and(transaction_range_proof));

    // Serve all routes for GET only.
    warp::get()
        .and(routes)
        .with(warp::log::custom(|info| {
            let endpoint = info.path().split('/').nth(1).unwrap_or("-");
            LATENCY_HISTOGRAM.observe_with(
                &[endpoint, info.status().as_str()],
                info.elapsed().as_secs_f64(),
            )
        }))
        .boxed()
}
```

**File:** storage/aptosdb/src/backup/backup_handler.rs (L41-109)
```rust
    pub fn get_transaction_iter(
        &self,
        start_version: Version,
        num_transactions: usize,
    ) -> Result<
        impl Iterator<
                Item = Result<(
                    Transaction,
                    PersistedAuxiliaryInfo,
                    TransactionInfo,
                    Vec<ContractEvent>,
                    WriteSet,
                )>,
            > + '_,
    > {
        let txn_iter = self
            .ledger_db
            .transaction_db()
            .get_transaction_iter(start_version, num_transactions)?;
        let mut txn_info_iter = self
            .ledger_db
            .transaction_info_db()
            .get_transaction_info_iter(start_version, num_transactions)?;
        let mut event_vec_iter = self
            .ledger_db
            .event_db()
            .get_events_by_version_iter(start_version, num_transactions)?;
        let mut write_set_iter = self
            .ledger_db
            .write_set_db()
            .get_write_set_iter(start_version, num_transactions)?;
        let mut persisted_aux_info_iter = self
            .ledger_db
            .persisted_auxiliary_info_db()
            .get_persisted_auxiliary_info_iter(start_version, num_transactions)?;

        let zipped = txn_iter.enumerate().map(move |(idx, txn_res)| {
            let version = start_version + idx as u64; // overflow is impossible since it's check upon txn_iter construction.

            let txn = txn_res?;
            let txn_info = txn_info_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "TransactionInfo not found when Transaction exists, version {}",
                    version
                ))
            })??;
            let event_vec = event_vec_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "Events not found when Transaction exists., version {}",
                    version
                ))
            })??;
            let write_set = write_set_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "WriteSet not found when Transaction exists, version {}",
                    version
                ))
            })??;
            let persisted_aux_info = persisted_aux_info_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "PersistedAuxiliaryInfo not found when Transaction exists, version {}",
                    version
                ))
            })??;
            BACKUP_TXN_VERSION.set(version as i64);
            Ok((txn, persisted_aux_info, txn_info, event_vec, write_set))
        });
        Ok(zipped)
    }
```

**File:** storage/aptosdb/src/backup/backup_handler.rs (L145-162)
```rust
    pub fn get_state_item_iter(
        &self,
        version: Version,
        start_idx: usize,
        limit: usize,
    ) -> Result<impl Iterator<Item = Result<(StateKey, StateValue)>> + Send + use<>> {
        let iterator = self
            .state_store
            .get_state_key_and_value_iter(version, start_idx)?
            .take(limit)
            .enumerate()
            .map(move |(idx, res)| {
                BACKUP_STATE_SNAPSHOT_VERSION.set(version as i64);
                BACKUP_STATE_SNAPSHOT_LEAF_IDX.set((start_idx + idx) as i64);
                res
            });
        Ok(Box::new(iterator))
    }
```

**File:** config/src/config/storage_config.rs (L433-436)
```rust
impl Default for StorageConfig {
    fn default() -> StorageConfig {
        StorageConfig {
            backup_service_address: SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 6186),
```

**File:** terraform/helm/fullnode/files/fullnode-base.yaml (L67-68)
```yaml
storage:
  backup_service_address: "0.0.0.0:6186"
```

**File:** terraform/helm/aptos-node/files/configs/fullnode-base.yaml (L13-16)
```yaml
storage:
  rocksdb_configs:
    enable_storage_sharding: true
  backup_service_address: "0.0.0.0:6186"
```

**File:** terraform/helm/fullnode/templates/service.yaml (L42-56)
```yaml
apiVersion: v1
kind: Service
metadata:
  name: {{ include "aptos-fullnode.fullname" . }}
  labels:
    {{- include "aptos-fullnode.labels" . | nindent 4 }}
spec:
  selector:
    {{- include "aptos-fullnode.selectorLabels" . | nindent 4 }}
    app.kubernetes.io/name: fullnode
  ports:
  - name: backup
    port: 6186
  - name: metrics
    port: 9101
```
