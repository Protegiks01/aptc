# Audit Report

## Title
Silent Thread Panic in Remote Executor Service Leads to Non-Functional Shard Without Error Propagation

## Summary
The `ExecutorService::start()` method spawns a worker thread to run `ShardedExecutorService::start()` but does not store the join handle or implement any panic recovery mechanism. When the worker thread encounters fatal errors (BCS deserialization failures, channel disconnections), it panics silently, leaving the shard in a non-functional state while appearing operational to the caller. This breaks the critical invariant that execution errors must be visible and handleable.

## Finding Description

The vulnerability exists in the error propagation chain between `ExecutorService::start()` and `ShardedExecutorService::start()` in the remote execution service architecture.

**Vulnerable Code Path:**

In [1](#0-0) , the `start()` method spawns a thread without storing the join handle:

The spawned thread calls `ShardedExecutorService::start()` which runs an infinite loop at [2](#0-1) . This loop receives commands and sends results via the `CoordinatorClient` trait.

The `RemoteCoordinatorClient` implementation contains three critical unwrap() calls that can panic:

1. **BCS Deserialization Panic**: [3](#0-2) 

2. **BCS Serialization Panic**: [4](#0-3) 

3. **Channel Send Panic**: [5](#0-4) 

**Attack Scenario:**

1. A coordinator sends a malformed `ExecuteBlockCommand` with corrupted BCS encoding
2. The remote executor's worker thread receives the message
3. BCS deserialization fails at the unwrap() call
4. The worker thread panics and terminates
5. The `ExecutorService::start()` has already returned successfully
6. The shard appears operational but is actually dead
7. Future work sent to this shard is silently dropped
8. The coordinator waits indefinitely for results, causing timeouts

**Contrast with Local Implementation:**

The local executor service properly handles this by storing join handles and joining them on cleanup: [6](#0-5)  and [7](#0-6) 

The remote implementation lacks any equivalent panic detection or recovery mechanism.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos Bug Bounty criteria:

**Validator Node Slowdowns**: When a shard silently fails, work assigned to that shard is never completed. The coordinator must timeout waiting for results, significantly degrading validator performance during block execution.

**Partial Execution Failures**: If multiple shards fail due to network issues or malformed messages, the entire sharded block execution system becomes unreliable, potentially causing validator nodes to fall behind the network or miss block proposals.

**Non-Deterministic Failures**: The silent nature of the failure makes debugging extremely difficult. Operators may not realize shards have died until execution timeouts accumulate.

**Resource Leaks**: The spawned thread holds references to the executor service and network channels. When it panics without cleanup, these resources may leak.

The impact is amplified because:
- No error logs are emitted when the thread panics silently
- No monitoring or health checks can detect the dead shard
- The coordinator has no backpressure mechanism to handle missing shards
- Recovery requires restarting the entire executor service

## Likelihood Explanation

This vulnerability has **High Likelihood** of occurring in production:

**Network Instability**: In distributed environments, network connections can close unexpectedly. When the coordinator disconnects, the channel send operation at [5](#0-4)  will fail and panic.

**Protocol Version Mismatches**: If the coordinator and executor are running different versions with incompatible message formats, BCS deserialization will consistently fail.

**Malformed Messages**: While validators are trusted, bugs in the coordinator's serialization logic or memory corruption could produce malformed messages that crash the executor.

**No Defensive Programming**: The code makes no attempt to validate messages or handle errors gracefully - every error path leads to a panic via unwrap().

**Already Observed Pattern**: The local implementation explicitly handles this scenario with join handles, indicating the developers were aware of the threading failure mode but failed to apply the same pattern to the remote implementation.

## Recommendation

**Immediate Fix**: Store the join handle and either join it during shutdown or periodically check if it has panicked:

```rust
pub struct ExecutorService {
    shard_id: ShardId,
    controller: NetworkController,
    executor_service: Arc<ShardedExecutorService<RemoteStateViewClient>>,
    join_handle: Option<thread::JoinHandle<()>>,
}

impl ExecutorService {
    pub fn start(&mut self) {
        self.controller.start();
        let thread_name = format!("ExecutorService-{}", self.shard_id);
        let builder = thread::Builder::new().name(thread_name);
        let executor_service_clone = self.executor_service.clone();
        let join_handle = builder
            .spawn(move || {
                executor_service_clone.start();
            })
            .expect("Failed to spawn thread");
        self.join_handle = Some(join_handle);
    }
    
    pub fn shutdown(&mut self) {
        self.controller.shutdown();
        if let Some(handle) = self.join_handle.take() {
            let _ = handle.join();
        }
    }
    
    pub fn is_alive(&self) -> bool {
        self.join_handle.as_ref()
            .map(|h| !h.is_finished())
            .unwrap_or(false)
    }
}
```

**Proper Fix**: Replace unwrap() calls with proper error handling in RemoteCoordinatorClient:

```rust
impl CoordinatorClient<RemoteStateViewClient> for RemoteCoordinatorClient {
    fn receive_execute_command(&self) -> ExecutorShardCommand<RemoteStateViewClient> {
        match self.command_rx.recv() {
            Ok(message) => {
                match bcs::from_bytes::<RemoteExecutionRequest>(&message.data) {
                    Ok(request) => {
                        match request {
                            RemoteExecutionRequest::ExecuteBlock(command) => {
                                let state_keys = Self::extract_state_keys(&command);
                                self.state_view_client.init_for_block(state_keys);
                                let (sub_blocks, concurrency, onchain_config) = command.into();
                                ExecutorShardCommand::ExecuteSubBlocks(
                                    self.state_view_client.clone(),
                                    sub_blocks,
                                    concurrency,
                                    onchain_config,
                                )
                            }
                        }
                    }
                    Err(e) => {
                        error!("Failed to deserialize command: {}", e);
                        ExecutorShardCommand::Stop
                    }
                }
            },
            Err(_) => ExecutorShardCommand::Stop,
        }
    }

    fn send_execution_result(&self, result: Result<Vec<Vec<TransactionOutput>>, VMStatus>) {
        let remote_execution_result = RemoteExecutionResult::new(result);
        match bcs::to_bytes(&remote_execution_result) {
            Ok(output_message) => {
                if let Err(e) = self.result_tx.send(Message::new(output_message)) {
                    error!("Failed to send execution result: {}", e);
                }
            }
            Err(e) => {
                error!("Failed to serialize execution result: {}", e);
            }
        }
    }
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::time::Duration;
    use std::thread;

    #[test]
    fn test_silent_thread_panic() {
        // Setup: Create a remote executor service
        let mut executor_service = ExecutorService::new(
            0, // shard_id
            4, // num_shards
            8, // num_threads
            "127.0.0.1:8080".parse().unwrap(),
            "127.0.0.1:8081".parse().unwrap(),
            vec![], // remote_shard_addresses
        );
        
        // Call start() - this spawns the worker thread
        executor_service.start();
        
        // The start() method returns immediately
        // At this point, we have no way to know if the worker thread is alive
        
        // Simulate a malformed message being sent that causes BCS deserialization to fail
        // (In practice, this would be sent over the network by the coordinator)
        
        // Wait a bit to let the thread panic
        thread::sleep(Duration::from_millis(100));
        
        // Problem: No way to detect that the worker thread has died!
        // The ExecutorService has no join_handle field
        // The following would fail if we had proper error propagation:
        
        // assert!(executor_service.is_alive()); // This method doesn't exist!
        
        // Expected: The thread should either:
        // 1. Not panic and handle errors gracefully, OR
        // 2. Propagate the panic to the parent via join handle
        
        // Actual: The thread panics silently and the shard becomes non-functional
    }
}
```

**Notes**

This vulnerability demonstrates a critical gap between the local and remote executor implementations. The local version properly manages thread lifecycle with join handles [8](#0-7) , while the remote version lacks this protection [9](#0-8) .

The root cause is the use of `.unwrap()` in network communication paths that can fail in production environments. Combined with the lack of panic handling in the spawned thread, this creates a silent failure mode that violates the principle of explicit error handling in distributed systems.

### Citations

**File:** execution/executor-service/src/remote_executor_service.rs (L15-19)
```rust
pub struct ExecutorService {
    shard_id: ShardId,
    controller: NetworkController,
    executor_service: Arc<ShardedExecutorService<RemoteStateViewClient>>,
}
```

**File:** execution/executor-service/src/remote_executor_service.rs (L57-67)
```rust
    pub fn start(&mut self) {
        self.controller.start();
        let thread_name = format!("ExecutorService-{}", self.shard_id);
        let builder = thread::Builder::new().name(thread_name);
        let executor_service_clone = self.executor_service.clone();
        builder
            .spawn(move || {
                executor_service_clone.start();
            })
            .expect("Failed to spawn thread");
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L215-272)
```rust
    pub fn start(&self) {
        trace!(
            "Shard starting, shard_id={}, num_shards={}.",
            self.shard_id,
            self.num_shards
        );
        let mut num_txns = 0;
        loop {
            let command = self.coordinator_client.receive_execute_command();
            match command {
                ExecutorShardCommand::ExecuteSubBlocks(
                    state_view,
                    transactions,
                    concurrency_level_per_shard,
                    onchain_config,
                ) => {
                    num_txns += transactions.num_txns();
                    trace!(
                        "Shard {} received ExecuteBlock command of block size {} ",
                        self.shard_id,
                        num_txns
                    );
                    let exe_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "execute_block"]);
                    let ret = self.execute_block(
                        transactions,
                        state_view.as_ref(),
                        BlockExecutorConfig {
                            local: BlockExecutorLocalConfig::default_with_concurrency_level(
                                concurrency_level_per_shard,
                            ),
                            onchain: onchain_config,
                        },
                    );
                    drop(state_view);
                    drop(exe_timer);

                    let _result_tx_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "result_tx"]);
                    self.coordinator_client.send_execution_result(ret);
                },
                ExecutorShardCommand::Stop => {
                    break;
                },
            }
        }
        let exe_time = SHARDED_EXECUTOR_SERVICE_SECONDS
            .get_metric_with_label_values(&[&self.shard_id.to_string(), "execute_block"])
            .unwrap()
            .get_sample_sum();
        info!(
            "Shard {} is shutting down; On shard execution tps {} txns/s ({} txns / {} s)",
            self.shard_id,
            (num_txns as f64 / exe_time),
            num_txns,
            exe_time
        );
    }
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L89-89)
```rust
                let request: RemoteExecutionRequest = bcs::from_bytes(&message.data).unwrap();
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L117-117)
```rust
        let output_message = bcs::to_bytes(&remote_execution_result).unwrap();
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L118-118)
```rust
        self.result_tx.send(Message::new(output_message)).unwrap();
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L34-36)
```rust
    join_handle: Option<thread::JoinHandle<()>>,
    phantom: std::marker::PhantomData<S>,
}
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L55-62)
```rust
        let join_handle = thread::Builder::new()
            .name(format!("executor-shard-{}", shard_id))
            .spawn(move || executor_service.start())
            .unwrap();
        Self {
            join_handle: Some(join_handle),
            phantom: std::marker::PhantomData,
        }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L228-238)
```rust
impl<S: StateView + Sync + Send + 'static> Drop for LocalExecutorClient<S> {
    fn drop(&mut self) {
        for command_tx in self.command_txs.iter() {
            let _ = command_tx.send(ExecutorShardCommand::Stop);
        }

        // wait for join handles to finish
        for executor_service in self.executor_services.iter_mut() {
            let _ = executor_service.join_handle.take().unwrap().join();
        }
    }
```
