# Audit Report

## Title
Unhandled Panics in spawn_blocking Calls Cause Restore Process Crash and Potential Database Inconsistency

## Summary
Multiple `spawn_blocking()` calls in the transaction restore process use `.expect("spawn_blocking failed")` for error handling, which causes the entire restore process to crash if the spawned blocking task panics. This occurs at lines 675, 701, and 714, while other spawn_blocking calls at lines 507, 591, and 614 properly propagate errors. The inconsistent error handling can leave the database in an inconsistent state and prevent disaster recovery.

## Finding Description
The vulnerability exists in the transaction restore pipeline where critical ChunkExecutor operations are executed in blocking tasks. Three specific spawn_blocking calls handle JoinErrors using `.expect()`, which panics on any error: [1](#0-0) [2](#0-1) [3](#0-2) 

These blocking tasks call ChunkExecutor methods that can panic in several scenarios:

**Scenario 1: Uninitialized ChunkExecutor**
The `enqueue_chunks` and `commit` methods contain `.expect("not reset")` calls: [4](#0-3) [5](#0-4) 

**Scenario 2: Error with Pending Pre-Commit**
The `update_ledger` method uses `with_inner`, which deliberately panics if an error occurs when `has_pending_pre_commit` is true: [6](#0-5) [7](#0-6) 

When a panic occurs inside spawn_blocking, tokio catches it and returns a JoinError. The `.expect("spawn_blocking failed")` then panics, crashing the async stream and aborting the entire restore process.

In contrast, other spawn_blocking calls in the same file properly handle errors: [8](#0-7) [9](#0-8) 

This inconsistency indicates that the `.expect()` usage is likely unintentional.

## Impact Explanation
This vulnerability meets **Medium Severity** criteria per the Aptos bug bounty program:

1. **State Inconsistencies Requiring Intervention**: When the restore process crashes mid-operation, particularly during `update_ledger` or `commit` phases, the database can be left in an inconsistent state. The `has_pending_pre_commit` flag indicates transactions are pre-committed but not synced, and a panic at this point prevents proper cleanup.

2. **Availability Impact**: The restore process is critical for disaster recovery. If it crashes due to a panic, node operators cannot restore their nodes from backup, requiring manual intervention and potentially complete database reset.

3. **Affects Node Availability**: While not directly affecting consensus or causing fund loss, this prevents nodes from recovering during disaster scenarios, impacting overall network resilience.

The issue does not reach Critical or High severity because:
- It occurs in the backup/restore path, not in live transaction processing
- It doesn't directly affect consensus safety or cause fund loss
- It requires specific triggering conditions rather than being easily exploitable

## Likelihood Explanation
**Likelihood: Medium**

The vulnerability can be triggered under realistic conditions:

1. **During Restore Operations**: Any restore operation where the ChunkExecutor is not properly initialized will trigger the panic. This could occur due to:
   - Race conditions during initialization
   - Incomplete cleanup from previous restore attempts
   - Database corruption or inconsistent state

2. **Error with Pending Pre-Commit**: If the database has pre-committed but not synced transactions (detected at initialization): [10](#0-9) 

Any subsequent error in `update_ledger` will trigger the deliberate panic, crashing the restore process.

3. **Corrupted Backup Data**: Malformed backup data could trigger errors in the blocking operations, causing panics if those operations use `.expect()` or `.unwrap()` internally.

While not easily exploitable by external attackers, the conditions can occur during legitimate restore operations, especially in recovery scenarios where the database state may be uncertain.

## Recommendation
Replace `.expect("spawn_blocking failed")` with proper error propagation using `.await?` or similar error handling, consistent with other spawn_blocking calls in the same file:

```rust
// Line 675-686: Change from
tokio::task::spawn_blocking(move || {
    chunk_replayer.enqueue_chunks(
        txns,
        persisted_aux_info,
        txn_infos,
        write_sets,
        events,
        &verify_execution_mode,
    )
})
.await
.expect("spawn_blocking failed")

// To:
tokio::task::spawn_blocking(move || {
    chunk_replayer.enqueue_chunks(
        txns,
        persisted_aux_info,
        txn_infos,
        write_sets,
        events,
        &verify_execution_mode,
    )
})
.await
.map_err(|e| anyhow::anyhow!("Enqueue chunks task failed: {}", e))?
```

Apply the same pattern to lines 701-703 and 714-729. This ensures that JoinErrors (including panics in spawned tasks) are converted to proper Result errors that propagate through the stream and are handled by the restore coordinator.

Additionally, review the ChunkExecutor initialization and the deliberate panic at line 99-102 to ensure proper error handling instead of panicking, especially in restore scenarios.

## Proof of Concept
The following scenario demonstrates the vulnerability:

```rust
// Reproduction steps:
// 1. Start a restore operation with TransactionRestoreBatchController
// 2. Ensure ChunkExecutor.inner is None (not reset) or has pending pre-commit
// 3. Trigger replay_transactions which will call spawn_blocking at line 675

// When enqueue_chunks is called inside spawn_blocking:
// - It accesses self.inner.read().as_ref().expect("not reset")
// - If inner is None, this panics with "not reset"
// - tokio catches the panic and returns JoinError
// - The .expect("spawn_blocking failed") panics
// - The entire restore process crashes

// Test scenario:
#[tokio::test]
async fn test_restore_panic_on_uninitialized_chunk_executor() {
    // Setup: Create DB and ChunkExecutor without proper initialization
    let db = create_test_db();
    let chunk_executor = ChunkExecutor::<AptosVMBlockExecutor>::new(db);
    // Don't call reset() - leave inner as None
    
    // Create restore controller with transactions to replay
    let restore_handler = setup_restore_handler();
    let txns_stream = create_transaction_stream();
    
    // Attempt to replay transactions
    // This will panic at line 686 when spawn_blocking fails due to
    // enqueue_chunks panicking with "not reset"
    let result = replay_transactions(&restore_handler, txns_stream).await;
    
    // Expected: Result should be Err, but actual behavior is panic
    // which crashes the entire restore process
    assert!(result.is_err()); // This won't be reached due to panic
}
```

The vulnerability can be triggered by:
1. Starting a restore operation
2. Ensuring ChunkExecutor is not properly initialized (inner is None)
3. The spawn_blocking call at line 675 will panic when enqueue_chunks accesses the uninitialized inner
4. This crashes the entire restore process instead of returning an error

## Notes
The vulnerability is exacerbated by the deliberate panic design in `with_inner` when errors occur with pending pre-committed state. While this panic may be intentional for data integrity (as indicated by the comment "Hit error with pending pre-committed ledger, panicking"), it should be handled gracefully in the restore context to allow proper cleanup and error reporting rather than crashing the entire process.

The inconsistent error handling pattern (some spawn_blocking calls use `.await??` while others use `.expect()`) suggests this was an oversight rather than intentional design, further supporting the validity of this finding.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L507-517)
```rust
                        tokio::task::spawn_blocking(move || {
                            restore_handler.save_transactions(
                                first_version,
                                &txns_to_save,
                                &persisted_aux_info_to_save,
                                &txn_infos_to_save,
                                &event_vecs_to_save,
                                write_sets_to_save,
                            )
                        })
                        .await??;
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L591-606)
```rust
                    tokio::task::spawn_blocking(move || {
                        // we directly save transaction and kvs to DB without involving chunk executor
                        handler.save_transactions_and_replay_kv(
                            base_version,
                            &txns,
                            &persisted_aux_info,
                            &txn_infos,
                            &events,
                            write_sets,
                        )?;
                        // return the last version after the replaying
                        Ok(base_version + offset - 1)
                    })
                    .err_into::<anyhow::Error>()
                    .await
                }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L675-687)
```rust
                    tokio::task::spawn_blocking(move || {
                        chunk_replayer.enqueue_chunks(
                            txns,
                            persisted_aux_info,
                            txn_infos,
                            write_sets,
                            events,
                            &verify_execution_mode,
                        )
                    })
                    .await
                    .expect("spawn_blocking failed")
                }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L701-704)
```rust
                    tokio::task::spawn_blocking(move || chunk_replayer.update_ledger())
                        .await
                        .expect("spawn_blocking failed")
                }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L714-730)
```rust
                    tokio::task::spawn_blocking(move || {
                        let v = chunk_replayer.commit()?;

                        let total_replayed = v - first_version + 1;
                        TRANSACTION_REPLAY_VERSION.set(v as i64);
                        info!(
                            version = v,
                            accumulative_tps = (total_replayed as f64
                                / replay_start.elapsed().as_secs_f64())
                                as u64,
                            "Transactions replayed."
                        );
                        Ok(total_replayed)
                    })
                    .await
                    .expect("spawn_blocking failed")
                }
```

**File:** execution/executor/src/chunk_executor/mod.rs (L89-106)
```rust
    fn with_inner<F, T>(&self, f: F) -> Result<T>
    where
        F: FnOnce(&ChunkExecutorInner<V>) -> Result<T>,
    {
        let locked = self.inner.read();
        let inner = locked.as_ref().expect("not reset");

        let has_pending_pre_commit = inner.has_pending_pre_commit.load(Ordering::Acquire);
        f(inner).map_err(|error| {
            if has_pending_pre_commit {
                panic!(
                    "Hit error with pending pre-committed ledger, panicking. {:?}",
                    error,
                );
            }
            error
        })
    }
```

**File:** execution/executor/src/chunk_executor/mod.rs (L202-206)
```rust
    fn update_ledger(&self) -> Result<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["chunk", "update_ledger"]);

        self.with_inner(|inner| inner.update_ledger())
    }
```

**File:** execution/executor/src/chunk_executor/mod.rs (L236-249)
```rust
    pub fn new(db: DbReaderWriter) -> Result<Self> {
        let commit_queue = ChunkCommitQueue::new_from_db(&db.reader)?;

        let next_pre_committed_version = commit_queue.expecting_version();
        let next_synced_version = db.reader.get_synced_version()?.map_or(0, |v| v + 1);
        assert!(next_synced_version <= next_pre_committed_version);
        let has_pending_pre_commit = next_synced_version < next_pre_committed_version;

        Ok(Self {
            db,
            commit_queue: Mutex::new(commit_queue),
            has_pending_pre_commit: AtomicBool::new(has_pending_pre_commit),
            _phantom: PhantomData,
        })
```

**File:** execution/executor/src/chunk_executor/mod.rs (L425-436)
```rust
        self.inner
            .read()
            .as_ref()
            .expect("not reset")
            .enqueue_chunks(
                transactions,
                persisted_aux_info,
                transaction_infos,
                write_sets,
                event_vecs,
                verify_execution_mode,
            )
```

**File:** execution/executor/src/chunk_executor/mod.rs (L439-443)
```rust
    fn commit(&self) -> Result<Version> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["replayer", "commit"]);

        self.inner.read().as_ref().expect("not reset").commit()
    }
```
