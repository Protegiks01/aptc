# Audit Report

## Title
ProofOfStore Replay Attack Enables Validator DoS Through Cache Eviction

## Summary
The ProofOfStore verification cache has a hardcoded 20-second TTL, allowing attackers to replay valid ProofOfStore messages after cache eviction. Each replay triggers expensive cryptographic multi-signature verification, enabling CPU exhaustion attacks against validator nodes that can degrade consensus participation.

## Finding Description

The ProofCache is configured with a 20-second time-to-live in the EpochManager initialization: [1](#0-0) 

The ProofOfStore::verify() function checks the cache before performing cryptographic verification: [2](#0-1) 

When a ProofOfStore message is received, verification occurs in the UnverifiedEvent::verify() flow before any deduplication: [3](#0-2) 

The expensive verify_multi_signatures operation aggregates public keys and verifies BLS signatures: [4](#0-3) 

While the BatchProofQueue provides deduplication to prevent state inconsistency: [5](#0-4) 

**The critical flaw**: This deduplication occurs AFTER cryptographic verification completes. The attack proceeds as follows:

1. Attacker captures valid ProofOfStore messages from network traffic
2. Waits 21+ seconds for automatic cache eviction (TTL expires)
3. Replays captured messages to the validator node
4. Each replay triggers full multi-signature verification (lines 642-647 in proof_of_store.rs)
5. BatchProofQueue deduplication rejects insertion, but CPU work is already done
6. Attacker floods validator with replays, exhausting CPU resources
7. Validator becomes slow or unresponsive, missing proposals and votes

This breaks the Resource Limits invariant: "All operations must respect gas, storage, and computational limits." An attacker can force unbounded cryptographic operations without rate limiting.

## Impact Explanation

This vulnerability qualifies as **HIGH severity** per Aptos bug bounty criteria: "Validator node slowdowns."

**Consensus Impact**: A validator under CPU exhaustion attack may:
- Miss block proposals during assigned rounds
- Delay or fail to submit votes on proposals
- Reduce network consensus throughput
- Risk being marked as underperforming

**Scalability**: The attack scales with:
- Number of validators in quorum (more signatures to aggregate/verify)
- Number of batches in each ProofOfStoreMsg (up to max_num_batches)
- Attacker's bandwidth to replay messages

**Network-Wide Risk**: If multiple validators are simultaneously attacked, consensus liveness could be significantly degraded, approaching the safety threshold if enough validators are affected.

## Likelihood Explanation

**Likelihood: HIGH**

The attack is trivial to execute:
1. **No special access required**: Any network peer can observe ProofOfStore messages
2. **Simple timing**: Wait 20 seconds for guaranteed cache eviction
3. **No cryptographic work**: Attacker just replays captured traffic
4. **No defense mechanism**: No rate limiting exists before cache lookup
5. **Deterministic**: 20-second TTL is hardcoded and predictable

The 20-second cache TTL is particularly problematic because:
- Batches may remain valid for much longer (minutes to hours based on expiration)
- Network latency and congestion mean messages naturally arrive after 20 seconds
- Legitimate retransmissions become indistinguishable from attacks

## Recommendation

Implement defense-in-depth by combining multiple mitigations:

**1. Move deduplication before verification:**
```rust
pub fn verify(&self, validator: &ValidatorVerifier, cache: &ProofCache) -> anyhow::Result<()> {
    let batch_info_ext: BatchInfoExt = self.info.clone().into();
    
    // Check cache first (existing)
    if let Some(signature) = cache.get(&batch_info_ext) {
        if signature == self.multi_signature {
            return Ok(());
        }
    }
    
    // NEW: Check if this batch_info was recently verified (even if expired from main cache)
    // Use a longer-TTL lightweight cache storing only digests
    static VERIFIED_DIGESTS: LazyLock<Cache<HashValue, ()>> = LazyLock::new(|| {
        Cache::builder()
            .max_capacity(100_000)
            .time_to_live(Duration::from_secs(600)) // 10 minutes
            .build()
    });
    
    let digest = batch_info_ext.digest();
    if VERIFIED_DIGESTS.get(digest).is_some() {
        return Ok(()); // Already verified recently
    }
    
    // Perform expensive verification
    let result = validator
        .verify_multi_signatures(&self.info, &self.multi_signature)
        .context(format!("Failed to verify ProofOfStore for batch: {:?}", self.info));
    
    if result.is_ok() {
        cache.insert(batch_info_ext, self.multi_signature.clone());
        VERIFIED_DIGESTS.insert(*digest, ());
    }
    result
}
```

**2. Increase cache TTL and capacity:**
```rust
proof_cache: Cache::builder()
    .max_capacity(node_config.consensus.proof_cache_capacity.max(10_000))
    .initial_capacity(1_000)
    .time_to_live(Duration::from_secs(300)) // Increase from 20s to 5min
    .build(),
```

**3. Add per-peer rate limiting** in the network layer to limit ProofOfStoreMsg frequency per sender.

**4. Track recent verification work** and reject messages that require re-verification too frequently.

## Proof of Concept

```rust
// PoC demonstrating the vulnerability
// This would be added to consensus/src/quorum_store/tests/

#[tokio::test]
async fn test_proof_replay_dos() {
    use aptos_consensus_types::proof_of_store::{ProofOfStore, ProofOfStoreMsg, ProofCache};
    use mini_moka::sync::Cache;
    use std::time::Duration;
    
    // Setup validator verifier and create valid ProofOfStore
    let validator_signer = ValidatorSigner::random(None);
    let validator_verifier = ValidatorVerifier::new_single(
        validator_signer.author(),
        validator_signer.public_key(),
    );
    
    // Create ProofCache with 1-second TTL for fast testing
    let proof_cache: ProofCache = Cache::builder()
        .max_capacity(1000)
        .time_to_live(Duration::from_secs(1))
        .build();
    
    // Create valid ProofOfStore
    let batch_info = create_test_batch_info();
    let signature = create_test_aggregate_signature(&batch_info, &validator_signer);
    let proof = ProofOfStore::new(batch_info, signature);
    
    // First verification - should do full crypto work and cache
    let start = std::time::Instant::now();
    proof.verify(&validator_verifier, &proof_cache).unwrap();
    let first_verify_time = start.elapsed();
    
    // Second verification - should hit cache (fast)
    let start = std::time::Instant::now();
    proof.verify(&validator_verifier, &proof_cache).unwrap();
    let cached_verify_time = start.elapsed();
    assert!(cached_verify_time < first_verify_time / 10); // Should be 10x+ faster
    
    // Wait for cache eviction (1+ seconds)
    tokio::time::sleep(Duration::from_secs(2)).await;
    
    // Third verification - cache evicted, full crypto work again (VULNERABILITY)
    let start = std::time::Instant::now();
    proof.verify(&validator_verifier, &proof_cache).unwrap();
    let replay_verify_time = start.elapsed();
    
    // Demonstrates replay triggers full verification after cache eviction
    assert!(replay_verify_time > first_verify_time / 2); // Similar to first time
    
    // Simulate DoS: Replay many times
    let dos_start = std::time::Instant::now();
    for _ in 0..100 {
        tokio::time::sleep(Duration::from_secs(2)).await; // Wait for cache eviction
        proof.verify(&validator_verifier, &proof_cache).unwrap();
    }
    let total_dos_time = dos_start.elapsed();
    
    println!("DoS Impact: 100 replays took {:?}", total_dos_time);
    println!("Average per verification: {:?}", total_dos_time / 100);
    
    // This demonstrates unbounded CPU work from replays
}
```

## Notes

The vulnerability is exacerbated by:
1. **Legitimate use cases**: Network retransmissions and late-arriving messages naturally trigger cache misses, making detection difficult
2. **No attribution**: Replayed messages appear valid and properly signed
3. **Amplification**: A single captured ProofOfStoreMsg can contain multiple ProofOfStore objects (up to max_num_batches), multiplying the verification cost
4. **Quorum scaling**: Larger validator sets mean more expensive multi-signature operations

The recommended fix maintains correctness while preventing DoS by using a secondary, longer-lived digest cache that survives main cache eviction.

### Citations

**File:** consensus/src/epoch_manager.rs (L250-254)
```rust
            proof_cache: Cache::builder()
                .max_capacity(node_config.consensus.proof_cache_capacity)
                .initial_capacity(1_000)
                .time_to_live(Duration::from_secs(20))
                .build(),
```

**File:** consensus/consensus-types/src/proof_of_store.rs (L635-652)
```rust
    pub fn verify(&self, validator: &ValidatorVerifier, cache: &ProofCache) -> anyhow::Result<()> {
        let batch_info_ext: BatchInfoExt = self.info.clone().into();
        if let Some(signature) = cache.get(&batch_info_ext) {
            if signature == self.multi_signature {
                return Ok(());
            }
        }
        let result = validator
            .verify_multi_signatures(&self.info, &self.multi_signature)
            .context(format!(
                "Failed to verify ProofOfStore for batch: {:?}",
                self.info
            ));
        if result.is_ok() {
            cache.insert(batch_info_ext, self.multi_signature.clone());
        }
        result
    }
```

**File:** consensus/src/round_manager.rs (L212-228)
```rust
            UnverifiedEvent::ProofOfStoreMsg(p) => {
                if !self_message {
                    p.verify(max_num_batches, validator, proof_cache)?;
                    counters::VERIFY_MSG
                        .with_label_values(&["proof_of_store"])
                        .observe(start_time.elapsed().as_secs_f64());
                }
                VerifiedEvent::ProofOfStoreMsg(Box::new((*p).into()))
            },
            UnverifiedEvent::ProofOfStoreMsgV2(p) => {
                if !self_message {
                    p.verify(max_num_batches, validator, proof_cache)?;
                    counters::VERIFY_MSG
                        .with_label_values(&["proof_of_store_v2"])
                        .observe(start_time.elapsed().as_secs_f64());
                }
                VerifiedEvent::ProofOfStoreMsg(p)
```

**File:** types/src/validator_verifier.rs (L345-386)
```rust
    pub fn verify_multi_signatures<T: CryptoHash + Serialize>(
        &self,
        message: &T,
        multi_signature: &AggregateSignature,
    ) -> std::result::Result<(), VerifyError> {
        // Verify the number of signature is not greater than expected.
        Self::check_num_of_voters(self.len() as u16, multi_signature.get_signers_bitvec())?;
        let mut pub_keys = vec![];
        let mut authors = vec![];
        for index in multi_signature.get_signers_bitvec().iter_ones() {
            let validator = self
                .validator_infos
                .get(index)
                .ok_or(VerifyError::UnknownAuthor)?;
            authors.push(validator.address);
            pub_keys.push(validator.public_key());
        }
        // Verify the quorum voting power of the authors
        self.check_voting_power(authors.iter(), true)?;
        #[cfg(any(test, feature = "fuzzing"))]
        {
            if self.quorum_voting_power == 0 {
                // This should happen only in case of tests.
                // TODO(skedia): Clean up the test behaviors to not rely on empty signature
                // verification
                return Ok(());
            }
        }
        // Verify empty multi signature
        let multi_sig = multi_signature
            .sig()
            .as_ref()
            .ok_or(VerifyError::EmptySignature)?;
        // Verify the optimistically aggregated signature.
        let aggregated_key =
            PublicKey::aggregate(pub_keys).map_err(|_| VerifyError::FailedToAggregatePubKey)?;

        multi_sig
            .verify(message, &aggregated_key)
            .map_err(|_| VerifyError::InvalidMultiSignature)?;
        Ok(())
    }
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L175-188)
```rust
    pub(crate) fn insert_proof(&mut self, proof: ProofOfStore<BatchInfoExt>) {
        if proof.expiration() <= self.latest_block_timestamp {
            counters::inc_rejected_pos_count(counters::POS_EXPIRED_LABEL);
            return;
        }
        let batch_key = BatchKey::from_info(proof.info());
        if self
            .items
            .get(&batch_key)
            .is_some_and(|item| item.proof.is_some() || item.is_committed())
        {
            counters::inc_rejected_pos_count(counters::POS_DUPLICATE_LABEL);
            return;
        }
```
