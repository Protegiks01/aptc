# Audit Report

## Title
ConsensusDB WAL Non-Durability Enables Double-Voting and Consensus Safety Violations

## Summary
The Aptos consensus system uses non-durable storage writes for critical safety state in both ConsensusDB and SafetyRules storage. When validators use OnDiskStorage backend (allowed by default configurations), machine crashes can cause permanent loss of vote records, enabling double-voting in the same round for different blocks. This violates AptosBFT consensus safety guarantees and can cause chain splits.

## Finding Description

The vulnerability exists in two critical storage layers that must maintain consensus safety state:

**1. ConsensusDB Non-Durability:**

When a validator votes, ConsensusDB persists the vote using `save_vote()` which calls `commit()`: [1](#0-0) 

The `commit()` function uses `write_schemas_relaxed()`: [2](#0-1) 

This method explicitly does NOT sync to disk and warns about data loss on machine crashes: [3](#0-2) 

**2. SafetyRules OnDiskStorage Non-Durability:**

SafetyRules stores safety-critical state (including `last_voted_round` and `last_vote`) via PersistentSafetyStorage, which can use OnDiskStorage backend. The OnDiskStorage implementation writes without sync: [4](#0-3) 

OnDiskStorage is explicitly documented as unsuitable for production: [5](#0-4) 

**3. Configuration Vulnerability:**

Despite documentation warnings, the configuration sanitizer ONLY blocks InMemoryStorage for mainnet validators, but permits OnDiskStorage: [6](#0-5) 

Additionally, default validator configurations use OnDiskStorage: [7](#0-6) [8](#0-7) 

**4. Double-Voting Prevention Mechanism:**

The SafetyRules implementation prevents double-voting by checking that new vote rounds exceed the last voted round: [9](#0-8) 

This check is also enforced during vote construction: [10](#0-9) 

**Attack Scenario:**

1. Validator votes at round R for block B1
2. SafetyRules updates `safety_data.last_voted_round = R` via OnDiskStorage (no sync)
3. ConsensusDB saves vote via `write_schemas_relaxed()` (no sync)
4. Machine crashes before OS page cache flush (~30 second window)
5. On recovery, both storage systems load stale data with lost vote information
6. Recovery logic filters out votes that don't match current epoch: [11](#0-10) 
7. Validator's `last_voted_round` reverts to pre-vote state
8. New proposal arrives for round R with different block B2
9. Double-voting check passes because `R > last_voted_round` (stale value)
10. Validator votes for B2 at round R
11. **Result: Validator has double-voted in round R for conflicting blocks**

## Impact Explanation

This is **CRITICAL severity** per Aptos Bug Bounty criteria under "Consensus/Safety Violations":

1. **Direct Consensus Safety Violation**: Enables a single validator to vote twice in the same round for different blocks after crash recovery, violating the fundamental no-double-voting invariant of AptosBFT.

2. **Chain Split Risk**: If multiple validators experience correlated crashes (regional power outage, infrastructure failure), multiple validators can simultaneously double-vote, potentially creating competing quorum certificates and causing permanent chain divergence.

3. **Requires Hardfork**: A consensus safety violation with diverged state would require manual intervention or hardfork to resolve, as the blockchain cannot automatically recover from conflicting committed blocks.

4. **Affects Production Deployments**: Despite OnDiskStorage being documented as "not for production," the sanitizer permits it for mainnet, and default configurations demonstrate its usage, meaning production validators may deploy with vulnerable configurations.

## Likelihood Explanation

**Likelihood: Medium**

**Triggering Conditions:**
- Requires machine crash (power failure, kernel panic, hardware failure) during the ~30 second page cache flush window after voting
- Multiple validators must crash for significant impact (chain split)
- More likely during regional infrastructure failures or correlated events

**Mitigating Factors:**
- Production validators SHOULD use VaultStorage per documentation
- Requires configuration error (ignoring documentation warnings)
- Process crashes (without machine reboot) don't trigger the vulnerability

**Amplifying Factors:**
- Default configs show OnDiskStorage usage, may lead operators to deploy incorrectly
- Sanitizer doesn't enforce VaultStorage requirement for mainnet
- High vote frequency increases exposure to vulnerability window
- Cloud infrastructure can have correlated failures

## Recommendation

**Immediate Fixes:**

1. **Enhance Configuration Sanitizer**: Reject OnDiskStorage for mainnet validators:
```rust
// In safety_rules_config.rs sanitize() function
if chain_id.is_mainnet() 
    && node_type.is_validator() 
    && (safety_rules_config.backend.is_in_memory() 
        || matches!(safety_rules_config.backend, SecureBackend::OnDiskStorage(_)))
{
    return Err(Error::ConfigSanitizerFailed(
        sanitizer_name,
        "Mainnet validators must use VaultStorage backend for safety-critical data".to_string(),
    ));
}
```

2. **Update Default Configs**: Change validator YAML templates to use VaultStorage or include prominent warnings.

3. **Add Durability to OnDiskStorage**: If OnDiskStorage must be supported, add `sync_all()` after writes:
```rust
fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
    let contents = serde_json::to_vec(data)?;
    let mut file = File::create(self.temp_path.path())?;
    file.write_all(&contents)?;
    file.sync_all()?;  // Add sync before rename
    fs::rename(&self.temp_path, &self.file_path)?;
    Ok(())
}
```

4. **Consider ConsensusDB Durability**: Evaluate using `write_schemas()` with sync for safety-critical writes like votes and timeout certificates.

## Proof of Concept

A complete PoC would require orchestrating machine crashes in a test environment, which is infrastructure-dependent. However, the vulnerability can be demonstrated through code inspection:

1. The non-durable write paths are confirmed in the codebase
2. The sanitizer gap is verified in configuration validation code
3. The double-voting prevention mechanism's reliance on persisted state is documented
4. Recovery logic that discards lost votes is implemented as shown

The vulnerability is triggered by operational conditions (machine crashes) rather than malicious input, making it a configuration and implementation security issue rather than requiring a traditional exploit PoC.

### Citations

**File:** consensus/src/consensusdb/mod.rs (L115-119)
```rust
    pub fn save_vote(&self, last_vote: Vec<u8>) -> Result<(), DbError> {
        let mut batch = SchemaBatch::new();
        batch.put::<SingleEntrySchema>(&SingleEntryKey::LastVote, &last_vote)?;
        self.commit(batch)
    }
```

**File:** consensus/src/consensusdb/mod.rs (L156-159)
```rust
    fn commit(&self, batch: SchemaBatch) -> Result<(), DbError> {
        self.db.write_schemas_relaxed(batch)?;
        Ok(())
    }
```

**File:** storage/schemadb/src/lib.rs (L311-318)
```rust
    /// Writes without sync flag in write option.
    /// If this flag is false, and the machine crashes, some recent
    /// writes may be lost.  Note that if it is just the process that
    /// crashes (i.e., the machine does not reboot), no writes will be
    /// lost even if sync==false.
    pub fn write_schemas_relaxed(&self, batch: impl IntoRawBatch) -> DbResult<()> {
        self.write_schemas_inner(batch, &WriteOptions::default())
    }
```

**File:** secure/storage/src/on_disk.rs (L16-22)
```rust
/// OnDiskStorage represents a key value store that is persisted to the local filesystem and is
/// intended for single threads (or must be wrapped by a Arc<RwLock<>>). This provides no permission
/// checks and simply offers a proof of concept to unblock building of applications without more
/// complex data stores. Internally, it reads and writes all data to a file, which means that it
/// must make copies of all key material which violates the code base. It violates it because
/// the anticipation is that data stores would securely handle key material. This should not be used
/// in production.
```

**File:** secure/storage/src/on_disk.rs (L64-69)
```rust
    fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
        let contents = serde_json::to_vec(data)?;
        let mut file = File::create(self.temp_path.path())?;
        file.write_all(&contents)?;
        fs::rename(&self.temp_path, &self.file_path)?;
        Ok(())
```

**File:** config/src/config/safety_rules_config.rs (L85-96)
```rust
        if let Some(chain_id) = chain_id {
            // Verify that the secure backend is appropriate for mainnet validators
            if chain_id.is_mainnet()
                && node_type.is_validator()
                && safety_rules_config.backend.is_in_memory()
            {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "The secure backend should not be set to in memory storage in mainnet!"
                        .to_string(),
                ));
            }
```

**File:** docker/compose/aptos-node/validator.yaml (L11-13)
```yaml
    backend:
      type: "on_disk_storage"
      path: secure-data.json
```

**File:** terraform/helm/aptos-node/files/configs/validator-base.yaml (L14-16)
```yaml
    backend:
      type: "on_disk_storage"
      path: secure-data.json
```

**File:** consensus/safety-rules/src/safety_rules.rs (L213-232)
```rust
    pub(crate) fn verify_and_update_last_vote_round(
        &self,
        round: Round,
        safety_data: &mut SafetyData,
    ) -> Result<(), Error> {
        if round <= safety_data.last_voted_round {
            return Err(Error::IncorrectLastVotedRound(
                round,
                safety_data.last_voted_round,
            ));
        }

        safety_data.last_voted_round = round;
        trace!(
            SafetyLogSchema::new(LogEntry::LastVotedRound, LogEvent::Update)
                .last_voted_round(safety_data.last_voted_round)
        );

        Ok(())
    }
```

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L77-80)
```rust
        self.verify_and_update_last_vote_round(
            proposed_block.block_data().round(),
            &mut safety_data,
        )?;
```

**File:** consensus/src/persistent_liveness_storage.rs (L404-408)
```rust
        Ok(RecoveryData {
            last_vote: match last_vote {
                Some(v) if v.epoch() == epoch => Some(v),
                _ => None,
            },
```
