# Audit Report

## Title
Race Condition in Block Pruning Creates Orphaned Blocks Causing Node Crash on Restart

## Summary
The `delete_blocks_and_quorum_certificates()` function does not verify that deleted blocks have no dependent child blocks in storage. Combined with a critical race condition in block insertion, this enables creation of orphaned blocks where a child block exists in storage but its parent has been deleted. This causes validator nodes to panic on restart, breaking consensus liveness.

## Finding Description

The vulnerability stems from two interconnected issues:

**Issue 1: No Dependency Verification in Deletion**

The `delete_blocks_and_quorum_certificates()` function deletes blocks and their QCs by ID without verifying whether other blocks in storage depend on them. [1](#0-0) 

This function deletes both a block and its QC, but does not check if there are other blocks in the database whose `parent_id` references the deleted block.

**Issue 2: Race Condition Between Storage Write and In-Memory Insertion**

Block insertion happens in two non-atomic steps: [2](#0-1) 

Between line 513 (storage write) and line 515 (in-memory insertion with lock), there is NO lock held on the BlockTree. During this window, the pruning logic can execute.

**Issue 3: Pruning Based on In-Memory State Only**

The pruning logic in `commit_callback` determines what to prune based on the in-memory tree structure: [3](#0-2) 

The `find_blocks_to_prune` method traverses only the in-memory tree structure using the `children` field: [4](#0-3) 

If a block exists in storage but not yet in the in-memory tree, it won't be considered when making pruning decisions.

**Attack Scenario:**

1. Block A (round 10) exists in both storage and in-memory tree
2. Consensus progresses to round 110 (window_size = 100, so window_start_round = 11)
3. A validator creates Block B (round 109) with `parent_id = A.id()` (this is valid per Block validation rules)
4. Thread 1: Starts inserting Block B, writes B to storage (line 513)
5. Thread 2: Acquires write lock, runs `commit_callback` for round 110 commit
6. Thread 2: `find_blocks_to_prune(new_window_root)` traverses in-memory tree, doesn't see B yet
7. Thread 2: Identifies Block A (round 10) should be pruned (below window)
8. Thread 2: Calls `storage.prune_tree([A])` â†’ `delete_blocks_and_quorum_certificates([A])`
9. Thread 2: Block A is deleted from storage
10. Thread 2: Releases write lock
11. Thread 1: Acquires write lock, inserts B into in-memory tree (line 515)
12. **Result**: Block B exists in both storage and memory, but its parent A is deleted from storage

**On Restart:**

When the node restarts, the recovery process loads blocks from storage and attempts to rebuild the BlockTree: [5](#0-4) 

When attempting to insert Block B, the BlockTree requires the parent to exist: [6](#0-5) 

At line 321, it bails with "Parent block not found". This propagates to line 294-296 in block_store.rs, causing a panic with message "[BlockStore] failed to insert block during build".

## Impact Explanation

**Critical Severity** - This vulnerability meets multiple critical impact criteria:

1. **Non-recoverable Network Partition**: Affected validator nodes cannot restart without manual database cleanup or rollback, potentially requiring emergency maintenance or even a hardfork if widespread.

2. **Consensus Safety Violation**: Different nodes may end up in inconsistent states depending on timing. Some nodes may have the orphaned block, others may not, leading to divergent views of the block tree.

3. **Total Loss of Liveness**: Affected nodes panic on restart and cannot participate in consensus, directly reducing the active validator set. If enough validators are affected simultaneously (e.g., during a coordinated crash or network partition followed by restart), the network could fail to maintain quorum.

4. **Permanent Node Failure**: The only recovery path is manual intervention to clean up the consensus database, which is operationally complex and error-prone.

The vulnerability is particularly severe because:
- It can be triggered by Byzantine validators within the < 1/3 Byzantine threshold
- It affects node availability permanently (until manual intervention)
- The race condition window is non-trivial in production environments with concurrent block processing

## Likelihood Explanation

**High Likelihood** - This vulnerability is likely to occur in production for several reasons:

1. **Natural Race Condition**: The race window between storage write (line 513) and in-memory insertion (line 515) occurs on every block insertion. With high transaction throughput and concurrent block processing, this window is frequently exposed.

2. **Valid Block Construction**: Blocks are only required to have `parent.round() < self.round()` per the validation rules: [7](#0-6) 

There is no constraint preventing blocks from having parents many rounds behind, making "delayed fork" blocks valid.

3. **Byzantine Exploitation**: A malicious validator (within the < 1/3 Byzantine assumption) can deliberately create blocks with old parents timed to coincide with pruning operations, increasing the probability of triggering the race condition.

4. **Network Delays**: In distributed environments, network delays can cause blocks to arrive out of order, naturally creating scenarios where recent blocks reference older parents that are being pruned.

5. **No Defensive Checks**: There are no defensive mechanisms to detect or prevent this condition - no validation that parent blocks won't be pruned, no verification before deletion, and no recovery logic.

## Recommendation

**Fix 1: Atomic Storage Write and In-Memory Insertion**

Acquire the BlockTree write lock BEFORE writing to storage:

```rust
pub async fn insert_block_inner(
    &self,
    pipelined_block: PipelinedBlock,
) -> anyhow::Result<Arc<PipelinedBlock>> {
    // ... existing validation code ...
    
    // Acquire lock BEFORE storage write to prevent races with pruning
    let mut tree = self.inner.write();
    
    // Write to storage while holding lock
    self.storage
        .save_tree(vec![pipelined_block.block().clone()], vec![])
        .context("Insert block failed when saving block")?;
    
    // Insert into in-memory tree (already holding lock)
    tree.insert_block(pipelined_block)
}
```

**Fix 2: Verify No Dependent Blocks Before Deletion**

Add dependency checking to `delete_blocks_and_quorum_certificates`:

```rust
pub fn delete_blocks_and_quorum_certificates(
    &self,
    block_ids: Vec<HashValue>,
) -> Result<(), DbError> {
    if block_ids.is_empty() {
        return Err(anyhow::anyhow!("Consensus block ids is empty!").into());
    }
    
    // Verify no blocks in storage reference the blocks being deleted
    let all_blocks = self.get_all::<BlockSchema>()?;
    let block_ids_set: HashSet<_> = block_ids.iter().collect();
    
    for (_, block) in all_blocks {
        if block_ids_set.contains(&block.parent_id()) 
            && !block_ids_set.contains(&block.id()) {
            return Err(anyhow::anyhow!(
                "Cannot delete block {} because child block {} depends on it",
                block.parent_id(),
                block.id()
            ).into());
        }
    }
    
    let mut batch = SchemaBatch::new();
    block_ids.iter().try_for_each(|hash| {
        batch.delete::<BlockSchema>(hash)?;
        batch.delete::<QCSchema>(hash)
    })?;
    self.commit(batch)
}
```

**Fix 3: Recovery Resilience**

Make recovery more resilient to orphaned blocks by filtering them out instead of panicking:

```rust
// In RecoveryData::find_blocks_to_prune, ensure orphaned blocks are pruned
// In BlockStore::build, catch parent-not-found errors and log warnings instead of panicking
```

## Proof of Concept

```rust
// Rust test demonstrating the race condition
#[tokio::test]
async fn test_orphaned_block_race_condition() {
    // Setup: Create BlockStore with initial blocks
    let mut block_store = create_test_block_store();
    
    // Block A at round 10
    let block_a = create_test_block(10, genesis_id);
    block_store.insert_block(block_a.clone()).await.unwrap();
    
    // Advance to round 110 (window_size = 100)
    for round in 11..=110 {
        let block = create_test_block(round, prev_id);
        block_store.insert_block(block).await.unwrap();
        prev_id = block.id();
    }
    
    // Simulate race condition
    let block_store_clone = block_store.clone();
    let block_a_id = block_a.id();
    
    // Thread 1: Insert Block B (child of A) at round 109
    let handle1 = tokio::spawn(async move {
        let block_b = create_test_block_with_parent(109, block_a_id);
        // This writes to storage first (line 513)
        // Race window HERE before line 515
        block_store_clone.insert_block(block_b).await
    });
    
    // Thread 2: Trigger pruning that deletes Block A
    let handle2 = tokio::spawn(async move {
        // Commit at round 110 triggers pruning
        let commit_block = create_test_block(110, prev_id);
        // This will prune Block A (round 10 < window_start 11)
        block_store.commit_and_prune(commit_block).await
    });
    
    // Wait for both operations
    let _ = tokio::join!(handle1, handle2);
    
    // Restart node - this will panic!
    // Recovery loads Block B from storage
    // Block B's parent A is missing
    // BlockTree::insert_block bails: "Parent block not found"
    // Panic: "[BlockStore] failed to insert block during build"
    let result = recover_from_storage();
    assert!(result.is_err()); // Should panic with parent not found
}
```

## Notes

This vulnerability represents a fundamental flaw in the atomicity guarantees of the consensus storage layer. The separation of storage writes from in-memory updates creates a critical race window where pruning decisions based on incomplete information can corrupt the persistent state. This affects the core consensus safety invariant that all validators must maintain consistent block tree state for recovery and continued operation.

### Citations

**File:** consensus/src/consensusdb/mod.rs (L139-152)
```rust
    pub fn delete_blocks_and_quorum_certificates(
        &self,
        block_ids: Vec<HashValue>,
    ) -> Result<(), DbError> {
        if block_ids.is_empty() {
            return Err(anyhow::anyhow!("Consensus block ids is empty!").into());
        }
        let mut batch = SchemaBatch::new();
        block_ids.iter().try_for_each(|hash| {
            batch.delete::<BlockSchema>(hash)?;
            batch.delete::<QCSchema>(hash)
        })?;
        self.commit(batch)
    }
```

**File:** consensus/src/block_storage/block_store.rs (L282-297)
```rust
        for block in blocks {
            if block.round() <= root_block_round {
                block_store
                    .insert_committed_block(block)
                    .await
                    .unwrap_or_else(|e| {
                        panic!(
                            "[BlockStore] failed to insert committed block during build {:?}",
                            e
                        )
                    });
            } else {
                block_store.insert_block(block).await.unwrap_or_else(|e| {
                    panic!("[BlockStore] failed to insert block during build {:?}", e)
                });
            }
```

**File:** consensus/src/block_storage/block_store.rs (L512-515)
```rust
        self.storage
            .save_tree(vec![pipelined_block.block().clone()], vec![])
            .context("Insert block failed when saving block")?;
        self.inner.write().insert_block(pipelined_block)
```

**File:** consensus/src/block_storage/block_tree.rs (L307-339)
```rust
    pub(super) fn insert_block(
        &mut self,
        block: PipelinedBlock,
    ) -> anyhow::Result<Arc<PipelinedBlock>> {
        let block_id = block.id();
        if let Some(existing_block) = self.get_block(&block_id) {
            debug!("Already had block {:?} for id {:?} when trying to add another block {:?} for the same id",
                       existing_block,
                       block_id,
                       block);
            Ok(existing_block)
        } else {
            match self.get_linkable_block_mut(&block.parent_id()) {
                Some(parent_block) => parent_block.add_child(block_id),
                None => bail!("Parent block {} not found", block.parent_id()),
            };
            let linkable_block = LinkableBlock::new(block);
            let arc_block = Arc::clone(linkable_block.executed_block());
            assert!(self.id_to_block.insert(block_id, linkable_block).is_none());
            // Note: the assumption is that we have/enforce unequivocal proposer election.
            if let Some(old_block_id) = self.round_to_ids.get(&arc_block.round()) {
                warn!(
                    "Multiple blocks received for round {}. Previous block id: {}",
                    arc_block.round(),
                    old_block_id
                );
            } else {
                self.round_to_ids.insert(arc_block.round(), block_id);
            }
            counters::NUM_BLOCKS_IN_TREE.inc();
            Ok(arc_block)
        }
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L405-434)
```rust
    pub(super) fn find_blocks_to_prune(
        &self,
        next_window_root_id: HashValue,
    ) -> VecDeque<HashValue> {
        // Nothing to do if this is the window root
        if next_window_root_id == self.window_root_id {
            return VecDeque::new();
        }

        let mut blocks_pruned = VecDeque::new();
        let mut blocks_to_be_pruned = vec![self.linkable_window_root()];

        while let Some(block_to_remove) = blocks_to_be_pruned.pop() {
            block_to_remove.executed_block().abort_pipeline();
            // Add the children to the blocks to be pruned (if any), but stop when it reaches the
            // new root
            for child_id in block_to_remove.children() {
                if next_window_root_id == *child_id {
                    continue;
                }
                blocks_to_be_pruned.push(
                    self.get_linkable_block(child_id)
                        .expect("Child must exist in the tree"),
                );
            }
            // Track all the block ids removed
            blocks_pruned.push_back(block_to_remove.id());
        }
        blocks_pruned
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L567-600)
```rust
    pub fn commit_callback(
        &mut self,
        storage: Arc<dyn PersistentLivenessStorage>,
        block_id: HashValue,
        block_round: Round,
        finality_proof: WrappedLedgerInfo,
        commit_decision: LedgerInfoWithSignatures,
        window_size: Option<u64>,
    ) {
        let current_round = self.commit_root().round();
        let committed_round = block_round;
        let commit_proof = finality_proof
            .create_merged_with_executed_state(commit_decision)
            .expect("Inconsistent commit proof and evaluation decision, cannot commit block");

        debug!(
            LogSchema::new(LogEvent::CommitViaBlock).round(current_round),
            committed_round = committed_round,
            block_id = block_id,
        );

        let window_root_id = self.find_window_root(block_id, window_size);
        let ids_to_remove = self.find_blocks_to_prune(window_root_id);

        if let Err(e) = storage.prune_tree(ids_to_remove.clone().into_iter().collect()) {
            // it's fine to fail here, as long as the commit succeeds, the next restart will clean
            // up dangling blocks, and we need to prune the tree to keep the root consistent with
            // executor.
            warn!(error = ?e, "fail to delete block");
        }
        self.process_pruned_blocks(ids_to_remove);
        self.update_window_root(window_root_id);
        self.update_highest_commit_cert(commit_proof);
    }
```

**File:** consensus/consensus-types/src/block.rs (L474-478)
```rust
        let parent = self.quorum_cert().certified_block();
        ensure!(
            parent.round() < self.round(),
            "Block must have a greater round than parent's block"
        );
```
