# Audit Report

## Title
State Synchronization Incomplete Due to Index Manipulation and Missing Final Root Hash Verification

## Summary
A malicious peer can manipulate the `first_index` and `last_index` fields in `StateValueChunkWithProof` to cause a victim node to accept an incomplete state during synchronization. By sending duplicate keys with a crafted proof indicating the chunk is final, the attacker bypasses proof verification (since duplicate keys are filtered before verification) and causes the system to mark sync as complete despite missing critical state data.

## Finding Description

The vulnerability exists in the state synchronization process where index tracking is decoupled from actual key writes. The attack exploits three key weaknesses:

**Weakness 1: Early Return Without Verification**

When all keys in a chunk are filtered as duplicates, the system returns success without verifying the proof: [1](#0-0) 

**Weakness 2: Index Tracking Decoupled from Actual Writes**

The system tracks progress using the attacker-controlled `last_index` field rather than counting actual keys written: [2](#0-1) 

**Weakness 3: Missing Final Root Hash Verification**

The `finish_impl()` method finalizes the state tree without verifying that the computed root hash matches the expected root hash: [3](#0-2) 

**Attack Execution:**

1. Victim node expects to sync 1000 state keys (K0-K999) at version V with expected root hash H_expected
2. Attacker controls the peer the victim is syncing from
3. **Chunk 1 (Legitimate)**: Attacker sends indices 0-499 with keys K0-K499 and valid proof P1
   - Victim processes and writes K0-K499
   - Updates `next_state_index_to_process = 500`
4. **Chunk 2 (Malicious)**: Attacker sends indices 500-999 with DUPLICATE keys K0-K499 and crafted proof P_last
   - Proof P_last has all-placeholder right siblings (indicating "last chunk")
   - Bootstrapper validation passes: [4](#0-3) 
   - Storage layer receives chunk and filters all keys as duplicates
   - Returns `Ok()` without calling `verify()` on the proof
   - `is_last_chunk()` examines the attacker-provided proof P_last: [5](#0-4) 
   - Returns `true`, marking sync as complete
5. System calls `finish()` without verifying the final root hash
6. **Result**: Only 500/1000 keys written, state incomplete, but system believes sync succeeded

The storage layer completely ignores the `first_index` and `last_index` fields when writing keys: [6](#0-5) 

This breaks the **State Consistency** invariant: the node's state tree will have a different root hash than honest nodes, violating deterministic execution requirements.

## Impact Explanation

**Critical Severity** - This vulnerability qualifies for the highest severity category because:

1. **Consensus Safety Violation**: The victim node will have incomplete state with an incorrect root hash. When attempting to execute new transactions or participate in consensus, the node will produce different state roots than honest validators, breaking the deterministic execution invariant.

2. **State Inconsistency**: The victim node cannot correctly verify or execute transactions that touch the missing state keys (K500-K999), causing divergence from the canonical chain.

3. **Potential Network Partition**: Multiple nodes syncing from malicious peers could end up with different incomplete states, fragmenting the network.

4. **Byzantine Fault Tolerance Compromise**: This attack works with a single malicious peer (no stake required), violating assumptions about state sync security.

## Likelihood Explanation

**High Likelihood** because:

1. **No Privileged Access Required**: Any peer participating in the P2P network can execute this attack against nodes syncing from them
2. **Simple to Execute**: The attacker only needs to capture legitimate state chunks from honest nodes and rearrange them with manipulated indices
3. **Detection Difficulty**: The attack succeeds silently during initial state sync, and the victim only discovers the problem later when accessing missing keys
4. **Common Scenario**: New nodes and nodes recovering from downtime regularly perform full state synchronization from peers

The attack becomes **certain** if the attacker can influence which peer the victim syncs from (e.g., eclipse attacks, targeted peer selection).

## Recommendation

Implement a final root hash verification in `finish_impl()`:

```rust
pub fn finish_impl(mut self) -> Result<()> {
    self.wait_for_async_commit()?;
    
    // ... existing finalization logic ...
    
    self.freeze(0);
    self.store.write_node_batch(&self.frozen_nodes)?;
    
    // NEW: Verify the final root hash matches expected
    let root_key = NodeKey::new_empty_path(self.version);
    let actual_root = self.store
        .get_node_option(&root_key, "finish_verification")?
        .ok_or_else(|| anyhow!("Root node not found after finalization"))?;
    
    ensure!(
        actual_root.hash() == self.expected_root_hash,
        "State sync verification failed: final root hash {} does not match expected {}",
        actual_root.hash(),
        self.expected_root_hash
    );
    
    Ok(())
}
```

Additionally, add a verification step before skipping entire chunks:

```rust
pub fn add_chunk_impl(
    &mut self,
    mut chunk: Vec<(&K, HashValue)>,
    proof: SparseMerkleRangeProof,
) -> Result<()> {
    // ... existing overlap filtering ...
    
    if chunk.is_empty() {
        // NEW: Verify proof even when all keys are filtered
        if let Some(prev_leaf) = &self.previous_leaf {
            proof.verify(
                self.expected_root_hash,
                SparseMerkleLeafNode::new(*prev_leaf.account_key(), prev_leaf.value_hash()),
                vec![], // No left siblings for duplicate chunk
            )?;
        }
        return Ok(());
    }
    
    // ... rest of implementation ...
}
```

## Proof of Concept

```rust
// This demonstrates the attack scenario
// Place in: state-sync/state-sync-driver/src/tests/

#[tokio::test]
async fn test_index_manipulation_attack() {
    // Setup: Create a state with 1000 keys
    let mut storage = MockDatabaseWriter::new();
    let mut bootstrapper = create_test_bootstrapper();
    
    // Expected: sync all 1000 keys (K0-K999)
    let expected_keys: Vec<StateKey> = (0..1000).map(|i| create_test_key(i)).collect();
    let expected_root_hash = compute_root_hash(&expected_keys);
    
    // Attack Phase 1: Send legitimate chunk with first 500 keys
    let chunk1 = StateValueChunkWithProof {
        first_index: 0,
        last_index: 499,
        first_key: expected_keys[0].hash(),
        last_key: expected_keys[499].hash(),
        raw_values: expected_keys[0..500].iter()
            .map(|k| (k.clone(), create_test_value()))
            .collect(),
        proof: create_valid_proof(&expected_keys[0..500]),
        root_hash: expected_root_hash,
    };
    
    // Victim processes chunk 1
    bootstrapper.process_state_values_payload(1, chunk1).await.unwrap();
    assert_eq!(bootstrapper.state_value_syncer.next_state_index_to_process, 500);
    
    // Attack Phase 2: Send malicious chunk with DUPLICATE keys but claiming indices 500-999
    let chunk2_malicious = StateValueChunkWithProof {
        first_index: 500,
        last_index: 999,
        first_key: expected_keys[0].hash(), // DUPLICATE key
        last_key: expected_keys[499].hash(), // DUPLICATE key
        raw_values: expected_keys[0..500].iter() // DUPLICATE keys!
            .map(|k| (k.clone(), create_test_value()))
            .collect(),
        proof: create_last_chunk_proof(&expected_keys[0..500]), // Proof with placeholder siblings
        root_hash: expected_root_hash,
    };
    
    // Victim processes malicious chunk - SHOULD FAIL but succeeds!
    let result = bootstrapper.process_state_values_payload(2, chunk2_malicious).await;
    assert!(result.is_ok()); // Attack succeeds
    
    // Verification: State is incomplete
    let actual_root = storage.get_root_hash(version).unwrap();
    assert_ne!(actual_root, expected_root_hash); // Root hash mismatch!
    
    // Only 500 keys written, not 1000
    for i in 500..1000 {
        let key = create_test_key(i);
        assert!(storage.get_state_value(&key).is_none()); // Missing keys!
    }
    
    // But system believes sync is complete
    assert_eq!(bootstrapper.state_value_syncer.next_state_index_to_process, 1000);
}
```

This vulnerability allows an unprivileged attacker to cause victim nodes to accept incomplete state, breaking consensus safety and creating divergent chain views across the network.

### Citations

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L349-371)
```rust
        if let Some(prev_leaf) = &self.previous_leaf {
            let skip_until = chunk
                .iter()
                .find_position(|(key, _hash)| key.hash() > *prev_leaf.account_key());
            chunk = match skip_until {
                None => {
                    info!("Skipping entire chunk.");
                    return Ok(());
                },
                Some((0, _)) => chunk,
                Some((num_to_skip, next_leaf)) => {
                    info!(
                        num_to_skip = num_to_skip,
                        next_leaf = next_leaf,
                        "Skipping leaves."
                    );
                    chunk.split_off(num_to_skip)
                },
            }
        };
        if chunk.is_empty() {
            return Ok(());
        }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L750-789)
```rust
    pub fn finish_impl(mut self) -> Result<()> {
        self.wait_for_async_commit()?;
        // Deal with the special case when the entire tree has a single leaf or null node.
        if self.partial_nodes.len() == 1 {
            let mut num_children = 0;
            let mut leaf = None;
            for i in 0..16 {
                if let Some(ref child_info) = self.partial_nodes[0].children[i] {
                    num_children += 1;
                    if let ChildInfo::Leaf(node) = child_info {
                        leaf = Some(node.clone());
                    }
                }
            }

            match num_children {
                0 => {
                    let node_key = NodeKey::new_empty_path(self.version);
                    assert!(self.frozen_nodes.is_empty());
                    self.frozen_nodes.insert(node_key, Node::Null);
                    self.store.write_node_batch(&self.frozen_nodes)?;
                    return Ok(());
                },
                1 => {
                    if let Some(node) = leaf {
                        let node_key = NodeKey::new_empty_path(self.version);
                        assert!(self.frozen_nodes.is_empty());
                        self.frozen_nodes.insert(node_key, node.into());
                        self.store.write_node_batch(&self.frozen_nodes)?;
                        return Ok(());
                    }
                },
                _ => (),
            }
        }

        self.freeze(0);
        self.store.write_node_batch(&self.frozen_nodes)?;
        Ok(())
    }
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L872-893)
```rust
                StorageDataChunk::States(notification_id, states_with_proof) => {
                    // Commit the state value chunk
                    let all_states_synced = states_with_proof.is_last_chunk();
                    let last_committed_state_index = states_with_proof.last_index;
                    let num_state_values = states_with_proof.raw_values.len();

                    let result = state_snapshot_receiver.add_chunk(
                        states_with_proof.raw_values,
                        states_with_proof.proof.clone(),
                    );

                    // Handle the commit result
                    match result {
                        Ok(()) => {
                            // Update the logs and metrics
                            info!(
                                LogSchema::new(LogEntry::StorageSynchronizer).message(&format!(
                                    "Committed a new state value chunk! Chunk size: {:?}, last persisted index: {:?}",
                                    num_state_values,
                                    last_committed_state_index
                                ))
                            );
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L923-956)
```rust
        // Verify the payload start index is valid
        let expected_start_index = self.state_value_syncer.next_state_index_to_process;
        if expected_start_index != state_value_chunk_with_proof.first_index {
            self.reset_active_stream(Some(NotificationAndFeedback::new(
                notification_id,
                NotificationFeedback::InvalidPayloadData,
            )))
            .await?;
            return Err(Error::VerificationError(format!(
                "The start index of the state values was invalid! Expected: {:?}, received: {:?}",
                expected_start_index, state_value_chunk_with_proof.first_index
            )));
        }

        // Verify the end index and number of state values is valid
        let expected_num_state_values = state_value_chunk_with_proof
            .last_index
            .checked_sub(state_value_chunk_with_proof.first_index)
            .and_then(|version| version.checked_add(1)) // expected_num_state_values = last_index - first_index + 1
            .ok_or_else(|| {
                Error::IntegerOverflow("The expected number of state values has overflown!".into())
            })?;
        let num_state_values = state_value_chunk_with_proof.raw_values.len() as u64;
        if expected_num_state_values != num_state_values {
            self.reset_active_stream(Some(NotificationAndFeedback::new(
                notification_id,
                NotificationFeedback::InvalidPayloadData,
            )))
            .await?;
            return Err(Error::VerificationError(format!(
                "The expected number of state values was invalid! Expected: {:?}, received: {:?}",
                expected_num_state_values, num_state_values,
            )));
        }
```

**File:** types/src/state_store/state_value.rs (L355-364)
```rust
impl StateValueChunkWithProof {
    /// Returns true iff this chunk is the last chunk (i.e., there are no
    /// more state values to write to storage after this chunk).
    pub fn is_last_chunk(&self) -> bool {
        let right_siblings = self.proof.right_siblings();
        right_siblings
            .iter()
            .all(|sibling| *sibling == *SPARSE_MERKLE_PLACEHOLDER_HASH)
    }
}
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L228-258)
```rust
    fn add_chunk(&mut self, chunk: Vec<(K, V)>, proof: SparseMerkleRangeProof) -> Result<()> {
        let kv_fn = || {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_value_add_chunk"]);
            self.kv_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk(chunk.clone())
        };

        let tree_fn = || {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["jmt_add_chunk"]);
            self.tree_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk_impl(chunk.iter().map(|(k, v)| (k, v.hash())).collect(), proof)
        };
        match self.restore_mode {
            StateSnapshotRestoreMode::KvOnly => kv_fn()?,
            StateSnapshotRestoreMode::TreeOnly => tree_fn()?,
            StateSnapshotRestoreMode::Default => {
                // We run kv_fn with TreeOnly to restore the usage of DB
                let (r1, r2) = IO_POOL.join(kv_fn, tree_fn);
                r1?;
                r2?;
            },
        }

        Ok(())
    }
```
