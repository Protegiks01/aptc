# Audit Report

## Title
Resource Exhaustion via Premature committed_hash() Computation on Oversized Transactions

## Summary
The `filter_transactions()` function calls `committed_hash()` on transactions before validating transaction size limits, enabling DoS attacks through submission of specially crafted large transactions that trigger expensive clone and serialization operations.

## Finding Description

The vulnerability exists in the transaction filtering pipeline where `committed_hash()` is invoked before transaction size validation occurs. This breaks the Resource Limits invariant by allowing computational-intensive operations on oversized transactions.

**Attack Flow:**

1. **Transaction Entry**: Transactions enter the mempool via API submission (up to 8 MB limit) or P2P broadcast (up to 64 MiB network message limit). [1](#0-0) [2](#0-1) 

2. **Filter Processing Before Validation**: In `process_incoming_transactions()`, transactions reach `filter_transactions()` before any VM validation occurs. [3](#0-2) 

3. **Premature Hash Computation - Path A**: When the filter uses `TransactionId` matchers, `committed_hash()` is called on ALL transactions during matching. [4](#0-3) 

4. **Premature Hash Computation - Path B**: When transactions are rejected by the filter, `committed_hash()` is called for logging purposes. [5](#0-4) 

5. **Expensive Operation**: The `committed_hash()` implementation clones the entire `SignedTransaction` and serializes it via BCS before hashing. [6](#0-5) 

6. **Late Size Validation**: Transaction size validation (64 KB limit) only occurs later during VM validation via `check_gas()`. [7](#0-6) [8](#0-7) 

The critical issue is that BCS serialization processes the entire transaction payload. This creates a gap where transactions between 64 KB and 8 MB (API) or 64 MiB (network) can reach the filtering logic and trigger expensive hash computations before being rejected by size validation.

## Impact Explanation

This qualifies as **High Severity** under the Aptos Bug Bounty program's "Validator node slowdowns" category.

An attacker can:
- Submit oversized transactions (e.g., `Script` with multi-MB bytecode up to network limits)
- If nodes have `TransactionId` filters configured, trigger hash computation on ALL incoming transactions
- Even without such filters, cause hash computation on rejected transactions (e.g., from blacklisted senders) during logging
- Force validators to perform CPU-intensive clone + serialization + hash operations before size validation rejects the transactions

The computational cost scales linearly with transaction size, allowing attackers to amplify their attack by submitting transactions approaching the 8 MB API limit or 64 MiB network limit while the legitimate size limit is only 64 KB.

## Likelihood Explanation

**Likelihood: Medium to High**

- No special privileges required - any user can submit transactions via API or broadcast via P2P
- Attack requires minimal resources - attacker only needs to send oversized transactions
- The vulnerability is present in the code path whenever filter configurations are active
- Path A (TransactionId filters): If deployed to block known malicious transactions, affects ALL incoming transactions
- Path B (Rejection filters): Common configuration for blocking spam senders would trigger hash computation on all rejected transactions
- Exploitation is deterministic - sending large transactions will trigger the expensive operation when filters are configured

The exploitability depends on filter configuration, but rejection filters for spam prevention are a realistic deployment scenario.

## Recommendation

Move transaction size validation before the filtering logic to reject oversized transactions early:

1. Add size validation in `process_incoming_transactions()` before calling `filter_transactions()`
2. Check transaction size against `max_transaction_size_in_bytes` parameter
3. Reject oversized transactions immediately with appropriate status code
4. Alternatively, avoid calling `committed_hash()` for rejected transactions in logging (use transaction sender/sequence instead)
5. Consider caching hash computation more aggressively or deferring it until after all validation passes

## Proof of Concept

```rust
// Conceptual PoC - demonstrates the attack flow
// 1. Create an oversized transaction (e.g., 1 MB payload)
let large_payload = vec![0u8; 1024 * 1024]; // 1 MB
let script = Script::new(large_payload, vec![], vec![]);
let txn = create_signed_transaction(script);

// 2. Submit to mempool via API or P2P
// 3. Transaction reaches filter_transactions() 
// 4. If TransactionId filters exist OR transaction is rejected:
//    - committed_hash() is called (line 187 or 447)
//    - This clones the 1 MB transaction (line 1338)
//    - Serializes via BCS and computes hash
// 5. Only afterwards, VM validation rejects it for size (line 109)

// Attacker can repeat this to cause sustained CPU load on validators
```

## Notes

- The vulnerability requires either TransactionId filters (Path A) or rejection filters with INFO logging (Path B) to be triggered
- Impact is configuration-dependent but realistic for nodes with spam prevention measures
- The 64 MiB network limit provides significant amplification potential compared to the 64 KB size limit
- This is a logic flaw (improper operation ordering) rather than a pure network DoS attack, qualifying it for "Validator node slowdowns" category

### Citations

**File:** config/src/config/api_config.rs (L97-97)
```rust
const DEFAULT_REQUEST_CONTENT_LENGTH_LIMIT: u64 = 8 * 1024 * 1024; // 8 MB
```

**File:** config/src/config/network_config.rs (L50-50)
```rust
pub const MAX_MESSAGE_SIZE: usize = 64 * 1024 * 1024; /* 64 MiB */
```

**File:** mempool/src/shared_mempool/tasks.rs (L318-321)
```rust
    // Filter out any disallowed transactions
    let mut statuses = vec![];
    let transactions =
        filter_transactions(&smp.transaction_filter_config, transactions, &mut statuses);
```

**File:** mempool/src/shared_mempool/tasks.rs (L441-448)
```rust
                info!(LogSchema::event_log(
                    LogEntry::TransactionFilter,
                    LogEvent::TransactionRejected
                )
                .message(&format!(
                    "Transaction {} rejected by filter",
                    transaction.committed_hash()
                )));
```

**File:** crates/aptos-transaction-filters/src/transaction_filter.rs (L187-187)
```rust
            TransactionMatcher::TransactionId(id) => signed_transaction.committed_hash() == *id,
```

**File:** types/src/transaction/mod.rs (L1335-1339)
```rust
    pub fn committed_hash(&self) -> HashValue {
        *self
            .committed_hash
            .get_or_init(|| Transaction::UserTransaction(self.clone()).hash())
    }
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L73-76)
```rust
            max_transaction_size_in_bytes: NumBytes,
            "max_transaction_size_in_bytes",
            64 * 1024
        ],
```

**File:** aptos-move/aptos-vm/src/gas.rs (L109-121)
```rust
    } else if txn_metadata.transaction_size > txn_gas_params.max_transaction_size_in_bytes {
        speculative_warn!(
            log_context,
            format!(
                "[VM] Transaction size too big {} (max {})",
                txn_metadata.transaction_size, txn_gas_params.max_transaction_size_in_bytes
            ),
        );
        return Err(VMStatus::error(
            StatusCode::EXCEEDED_MAX_TRANSACTION_SIZE,
            None,
        ));
    }
```
