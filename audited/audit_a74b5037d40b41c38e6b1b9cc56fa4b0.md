# Audit Report

## Title
Cache-Database Inconsistency via Delete-Insert Race in Batch Store

## Summary
A race condition exists in `BatchStore` where `db.delete_batches()` can interleave with `db.save_batch()` for the same digest, causing newly persisted batches to be deleted from the database while remaining absent from the in-memory cache. This creates a cache-DB inconsistency that makes batches permanently inaccessible to the node, leading to block execution failures and consensus liveness issues.

## Finding Description

The vulnerability manifests through a Time-Of-Check-Time-Of-Use (TOCTOU) race between two concurrent operations:

**Persist Path** (`persist_inner()` → `save()` → `db.save_batch()`): [1](#0-0) 

The `save()` method checks if `value.expiration() > last_certified_time` before inserting to cache. However, this check is non-atomic with the subsequent database write that occurs in `persist_inner()`: [2](#0-1) 

**Garbage Collection Path** (`update_certified_timestamp()` → `clear_expired_payload()` → `db.delete_batches()`): [3](#0-2) 

**The Race Window:**

1. Thread A reads `last_certified_time` (T0) and checks expiration (T1 > T0) ✓
2. Thread A inserts batch X into `db_cache` via `insert_to_cache()`
3. Thread B updates `last_certified_time` to T2 where T2 > T1 (batch X now expired)
4. Thread B calls `clear_expired_payload()` which removes X from `db_cache`
5. Thread B calls `db.delete_batches([X])` which deletes X from database
6. Thread A calls `db.save_batch()` which writes X back to database

**Result:** Batch X exists in the RocksDB database but NOT in the `db_cache` DashMap.

**Root Cause:** The RocksDB operations use `write_schemas_relaxed()` which provides atomicity only within a single `WriteBatch`, not across separate operations: [4](#0-3) 

The `delete_batches()` and `save_batch()` operations are separate WriteBatch executions with no mutual exclusion: [5](#0-4) [6](#0-5) 

**Impact on Retrieval:** When execution attempts to retrieve the affected batch, `get_batch_from_local()` checks only the cache: [7](#0-6) 

Since the batch is absent from cache, it returns `ExecutorError::CouldNotGetData`, causing block execution to fail even though the batch exists in the database.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos Bug Bounty program criteria for "Significant protocol violations" because:

1. **State Consistency Violation**: Breaks the critical invariant that cache and database must remain synchronized. The system assumes `db_cache` is the source of truth for batch availability.

2. **Consensus Liveness Impact**: When blocks reference the orphaned batch, nodes cannot execute them, returning: [8](#0-7) 

This blocks consensus progression until the batch is re-fetched from peers or the block times out.

3. **Cascading Failures**: Multiple nodes experiencing this race simultaneously for the same batch can cause network-wide execution stalls, as all nodes believe the batch is unavailable locally.

4. **Non-Deterministic**: The race is timing-dependent and can occur unpredictably in production under concurrent load, making it difficult to diagnose and recover from.

## Likelihood Explanation

**Likelihood: MEDIUM to HIGH**

The race occurs when these conditions align (all common in production):

1. **Batch Persistence**: New batches are continuously being persisted via `persist()` calls during normal quorum store operations
2. **Timestamp Updates**: `update_certified_timestamp()` is called regularly as consensus progresses and blocks are certified
3. **Expiration Timing**: The batch's expiration time falls within the narrow window between cache insertion and DB write

**Triggering Factors:**
- High transaction throughput increases concurrent `persist()` operations
- Short batch expiration times (close to `last_certified_time`) widen the race window
- Node synchronization delays can cause batches to be near-expired when persisted
- Epoch transitions trigger garbage collection, increasing `update_certified_timestamp()` calls

The vulnerability requires no attacker action—it's a natural consequence of concurrent operations under normal load.

## Recommendation

Implement atomic cache-database consistency by ensuring batch deletion checks the database atomically before removing from cache, or by holding a lock during the persist operation:

**Option 1: Atomic Persist-Delete Check**
```rust
pub(crate) fn save(&self, value: &PersistedValue<BatchInfoExt>) -> anyhow::Result<bool> {
    let last_certified_time = self.last_certified_time();
    if value.expiration() > last_certified_time {
        // Atomically check timestamp again while holding cache lock
        let result = self.insert_to_cache(value)?;
        
        // Recheck expiration after cache insertion to detect concurrent GC
        if value.expiration() <= self.last_certified_time() {
            // Expired during insertion - remove from cache
            self.db_cache.remove(value.digest());
            bail!("Batch expired during persistence");
        }
        
        return Ok(result);
    }
    // ... existing error path
}
```

**Option 2: Defer DB Write Until After Timestamp Check**
```rust
fn persist_inner(&self, batch_info: BatchInfoExt, persist_request: PersistedValue<BatchInfoExt>) 
    -> Option<SignedBatchInfo<BatchInfoExt>> {
    
    match self.save(&persist_request) {
        Ok(needs_db) => {
            if needs_db {
                // Recheck expiration immediately before DB write
                if persist_request.expiration() <= self.last_certified_time() {
                    // Batch expired - remove from cache
                    self.db_cache.remove(persist_request.digest());
                    return None;
                }
                
                // Safe to persist to DB now
                if !batch_info.is_v2() {
                    self.db.save_batch(persist_request.try_into().unwrap()).unwrap();
                } else {
                    self.db.save_batch_v2(persist_request).unwrap();
                }
            }
            // ... rest of function
        }
    }
}
```

**Option 3: Lock-Based Atomicity (Most Robust)**
Add a per-digest lock in `BatchStore` to serialize persist and delete operations for the same digest, ensuring no race between cache/DB updates.

## Proof of Concept

```rust
// Multi-threaded race condition test for BatchStore
// Place in consensus/src/quorum_store/tests/batch_store_test.rs

#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_delete_insert_race_condition() {
    use std::sync::Arc;
    use std::time::Duration;
    
    let db = Arc::new(MockQuorumStoreDB::new());
    let signer = ValidatorSigner::random([0u8; 32]);
    let batch_store = Arc::new(BatchStore::new(
        1, true, 1000, db, 10000, 10000, 100, signer, 60_000_000
    ));
    
    // Create a batch with expiration at T=5000
    let batch_info = create_test_batch_info(5000);
    let persist_request = PersistedValue::new(batch_info.clone(), Some(vec![]));
    let digest = *persist_request.digest();
    
    let store_clone = batch_store.clone();
    
    // Thread 1: Persist batch (will take time to write to DB)
    let persist_handle = tokio::spawn(async move {
        // Simulate slow DB write
        let result = store_clone.save(&persist_request);
        tokio::time::sleep(Duration::from_millis(50)).await;
        if result.is_ok() {
            store_clone.db.save_batch_v2(persist_request).ok();
        }
    });
    
    // Thread 2: Update timestamp to expire the batch
    let store_clone = batch_store.clone();
    let gc_handle = tokio::spawn(async move {
        tokio::time::sleep(Duration::from_millis(10)).await;
        // Update to time 6000, making batch expired
        store_clone.update_certified_timestamp(6000);
    });
    
    persist_handle.await.unwrap();
    gc_handle.await.unwrap();
    
    // Verify race condition: batch in DB but not in cache
    let in_cache = batch_store.db_cache.get(&digest).is_some();
    let in_db = batch_store.db.get_batch_v2(&digest).unwrap().is_some();
    
    // Bug: batch exists in DB but not cache!
    assert!(!in_cache, "Batch should not be in cache after GC");
    assert!(in_db, "Race condition: batch persisted to DB after deletion");
    
    // This will fail to retrieve the batch
    let result = batch_store.get_batch_from_local(&digest);
    assert!(matches!(result, Err(ExecutorError::CouldNotGetData)));
}
```

**Notes**

This race condition represents a fundamental atomicity violation in the batch store's two-phase persistence protocol (cache-then-DB). The vulnerability is inherent to the current design where expiration checks and database operations are not performed under mutual exclusion. Production systems under moderate to high load will encounter this race, causing intermittent but severe consensus disruptions when blocks reference affected batches.

### Citations

**File:** consensus/src/quorum_store/batch_store.rs (L419-439)
```rust
    pub(crate) fn save(&self, value: &PersistedValue<BatchInfoExt>) -> anyhow::Result<bool> {
        let last_certified_time = self.last_certified_time();
        if value.expiration() > last_certified_time {
            fail_point!("quorum_store::save", |_| {
                // Skip caching and storing value to the db
                Ok(false)
            });
            counters::GAP_BETWEEN_BATCH_EXPIRATION_AND_CURRENT_TIME_WHEN_SAVE.observe(
                Duration::from_micros(value.expiration() - last_certified_time).as_secs_f64(),
            );

            return self.insert_to_cache(value);
        }
        counters::NUM_BATCH_EXPIRED_WHEN_SAVE.inc();
        bail!(
            "Incorrect expiration {} in epoch {}, last committed timestamp {}",
            value.expiration(),
            self.epoch(),
            last_certified_time,
        );
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L497-514)
```rust
        match self.save(&persist_request) {
            Ok(needs_db) => {
                trace!("QS: sign digest {}", persist_request.digest());
                if needs_db {
                    if !batch_info.is_v2() {
                        let persist_request =
                            persist_request.try_into().expect("Must be a V1 batch");
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch(persist_request)
                            .expect("Could not write to DB");
                    } else {
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch_v2(persist_request)
                            .expect("Could not write to DB")
                    }
                }
```

**File:** consensus/src/quorum_store/batch_store.rs (L530-539)
```rust
    pub fn update_certified_timestamp(&self, certified_time: u64) {
        trace!("QS: batch reader updating time {:?}", certified_time);
        self.last_certified_time
            .fetch_max(certified_time, Ordering::SeqCst);

        let expired_keys = self.clear_expired_payload(certified_time);
        if let Err(e) = self.db.delete_batches(expired_keys) {
            debug!("Error deleting batches: {:?}", e)
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L571-585)
```rust
    pub(crate) fn get_batch_from_local(
        &self,
        digest: &HashValue,
    ) -> ExecutorResult<PersistedValue<BatchInfoExt>> {
        if let Some(value) = self.db_cache.get(digest) {
            if value.payload_storage_mode() == StorageMode::PersistedOnly {
                self.get_batch_from_db(digest, value.batch_info().is_v2())
            } else {
                // Available in memory.
                Ok(value.clone())
            }
        } else {
            Err(ExecutorError::CouldNotGetData)
        }
    }
```

**File:** storage/schemadb/src/lib.rs (L316-318)
```rust
    pub fn write_schemas_relaxed(&self, batch: impl IntoRawBatch) -> DbResult<()> {
        self.write_schemas_inner(batch, &WriteOptions::default())
    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L93-101)
```rust
    fn delete_batches(&self, digests: Vec<HashValue>) -> Result<(), DbError> {
        let mut batch = SchemaBatch::new();
        for digest in digests.iter() {
            trace!("QS: db delete digest {}", digest);
            batch.delete::<BatchSchema>(digest)?;
        }
        self.db.write_schemas_relaxed(batch)?;
        Ok(())
    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L110-117)
```rust
    fn save_batch(&self, batch: PersistedValue<BatchInfo>) -> Result<(), DbError> {
        trace!(
            "QS: db persists digest {} expiration {:?}",
            batch.digest(),
            batch.expiration()
        );
        self.put::<BatchSchema>(batch.digest(), &batch)
    }
```

**File:** execution/executor-types/src/error.rs (L41-42)
```rust
    #[error("request timeout")]
    CouldNotGetData,
```
