# Audit Report

## Title
Channel Pollution in Sharded Block Executor Causes Non-Deterministic State Roots and Consensus Failure

## Summary
The `get_output_from_shards()` function in the sharded block executor exhibits a critical vulnerability where unread shard results accumulate in unbounded channels across block executions. When one shard returns a `VMStatus` error, subsequent shards' results are never consumed from their channels, causing stale data to be read during future block executions. This breaks deterministic execution and can cause consensus failures across the network.

## Finding Description

The sharded block executor maintains a singleton instance with persistent unbounded channels for collecting shard execution results. [1](#0-0) 

These result channels are created once during initialization using unbounded crossbeam channels: [2](#0-1) 

The critical vulnerability lies in the `get_output_from_shards()` function, which iterates through shard result channels sequentially and returns immediately upon encountering an error: [3](#0-2) 

When a shard returns `Err(VMStatus)` at line 171, the `?` operator causes an immediate return. All shards execute in parallel, so shards after the failed one may have already completed and sent their results to their channels. These results are never read and remain in the unbounded channels.

Block-level VMStatus errors occur when critical transactions (BlockMetadataTransaction or GenesisTransaction) fail during execution, triggering `ExecutionStatus::Abort`: [4](#0-3) 

This error propagates up and gets converted to `BlockExecutionError::FatalVMError`, which is then returned as the block execution error: [5](#0-4) 

**Attack Scenario:**

**Block N execution:**
1. All 4 shards execute transactions in parallel
2. Shard 0 completes successfully → sends result A0 to channel
3. Shard 1 completes successfully → sends result A1 to channel  
4. Shard 2 encounters FatalVMError → sends error E2 to channel
5. Shard 3 completes successfully → sends result A3 to channel
6. Coordinator reads A0 (success), A1 (success), E2 (error), returns immediately
7. **Result A3 is never read and remains in shard 3's channel**

**Block N+1 execution:**
1. All 4 shards execute new transactions successfully
2. Shard 0 sends result B0 to channel
3. Shard 1 sends result B1 to channel
4. Shard 2 sends result B2 to channel
5. Shard 3 sends result B3 to channel (B3 is now queued AFTER the stale A3)
6. Coordinator reads B0, B1, B2, **A3** (stale result from Block N!)
7. **Incorrect state transitions applied, wrong state root computed**

The executor instance is locked during each block execution but the same channels persist: [6](#0-5) 

No channel cleanup occurs between block executions. The Drop implementation only sends stop commands when the executor is destroyed, which never happens for the singleton: [7](#0-6) 

## Impact Explanation

**Severity: CRITICAL** (up to $1,000,000 per Aptos Bug Bounty)

This vulnerability violates **Invariant #1: Deterministic Execution** - the most fundamental consensus requirement that all validators must produce identical state roots for identical blocks.

The impact cascades as follows:

1. **Consensus Safety Violation**: Different validators may experience different error patterns across blocks due to timing variations, implementation differences, or transient conditions. When validator A has an error in shard 2 on block N but validator B has an error in shard 1, their channel states diverge permanently.

2. **Non-Deterministic State Roots**: On subsequent blocks, validators read different stale results from their polluted channels, causing them to compute different state roots for the same block. This directly violates consensus safety.

3. **Network Partition**: When validators produce different state roots, they cannot reach consensus on subsequent blocks. This creates a permanent fork requiring manual intervention or a hard fork to resolve.

4. **Unpredictable Behavior**: The bug's effects accumulate over time as more errors occur, making the system increasingly unstable and unpredictable.

This meets the **"Consensus/Safety violations"** and **"Non-recoverable network partition (requires hardfork)"** criteria for Critical severity.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

While block-level VMStatus errors (FatalVMError) are designed to be rare, they can occur under several realistic conditions:

1. **Framework Bugs**: Bugs in the Aptos Framework code executed by BlockMetadataTransaction can trigger fatal errors
2. **Invariant Violations**: Code invariant errors during parallel execution trigger FatalVMError paths
3. **State Corruption**: Corrupted state or unexpected conditions can cause system transactions to fail
4. **Upgrade Issues**: Framework upgrades or on-chain parameter changes may introduce conditions causing failures

Once a single FatalVMError occurs in any shard, the channel pollution begins. The bug has a compounding effect - each error accumulates more stale results, increasing the probability of reading stale data on future blocks.

The vulnerability requires:
- ✅ No attacker privileges (occurs naturally from system bugs)
- ✅ No special network access
- ✅ No coordination between nodes
- ✅ Exploitable through normal blockchain operation

The singleton executor pattern ensures channels persist across all block executions in production, making this vulnerability active on all Aptos nodes running the sharded executor.

## Recommendation

**Immediate Fix**: Drain all unread results from shard channels before returning from `get_output_from_shards()` when an error occurs.

```rust
fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
    let _timer = WAIT_FOR_SHARDED_OUTPUT_SECONDS.start_timer();
    trace!("LocalExecutorClient Waiting for results");
    let mut results = vec![];
    let mut first_error: Option<VMStatus> = None;
    
    for (i, rx) in self.result_rxs.iter().enumerate() {
        match rx.recv()
            .unwrap_or_else(|_| panic!("Did not receive output from shard {}", i)) {
            Ok(result) => {
                if first_error.is_none() {
                    results.push(result);
                }
                // If we already have an error, discard this success result
            },
            Err(err) => {
                if first_error.is_none() {
                    first_error = Some(err);
                }
                // Continue reading from remaining shards to drain channels
            },
        }
    }
    
    // Return error only after reading all shard results
    match first_error {
        Some(err) => Err(err),
        None => Ok(results),
    }
}
```

**Alternative Fix**: Use bounded channels with try_recv() and explicit cleanup, or recreate channels for each block execution.

**Long-term Fix**: Redesign the error handling to ensure atomic success/failure across all shards - either all shards succeed or the entire block fails before any results are sent to channels.

## Proof of Concept

```rust
#[test]
fn test_channel_pollution_vulnerability() {
    use crossbeam_channel::unbounded;
    use std::thread;
    
    // Simulate 4 shards with result channels (similar to local_executor_shard.rs:88-91)
    let (txs, rxs): (Vec<_>, Vec<_>) = (0..4)
        .map(|_| unbounded::<Result<String, String>>())
        .unzip();
    
    println!("=== Block N Execution ===");
    // Block N: Shards execute in parallel, shard 2 fails
    for (i, tx) in txs.iter().enumerate() {
        let tx = tx.clone();
        thread::spawn(move || {
            if i == 2 {
                tx.send(Err(format!("Error from shard {}", i))).unwrap();
            } else {
                tx.send(Ok(format!("Block_N_Result_{}", i))).unwrap();
            }
        });
    }
    
    // Coordinator reads results (similar to get_output_from_shards:164-175)
    let mut block_n_results = vec![];
    for (i, rx) in rxs.iter().enumerate() {
        match rx.recv().unwrap() {
            Ok(result) => {
                println!("Shard {}: {}", i, result);
                block_n_results.push(result);
            },
            Err(err) => {
                println!("Shard {}: ERROR - {}", i, err);
                println!("Returning error immediately - subsequent results NOT read!");
                break; // Simulates the `?` operator at line 171
            }
        }
    }
    
    println!("\n=== Block N+1 Execution ===");
    // Block N+1: All shards succeed
    for (i, tx) in txs.iter().enumerate() {
        let tx = tx.clone();
        thread::spawn(move || {
            tx.send(Ok(format!("Block_N+1_Result_{}", i))).unwrap();
        });
    }
    
    // Coordinator reads results for Block N+1
    let mut block_n1_results = vec![];
    for (i, rx) in rxs.iter().enumerate() {
        match rx.recv().unwrap() {
            Ok(result) => {
                println!("Shard {}: {}", i, result);
                block_n1_results.push(result);
            },
            Err(err) => {
                println!("Shard {}: ERROR - {}", i, err);
                break;
            }
        }
    }
    
    println!("\n=== VULNERABILITY DEMONSTRATION ===");
    println!("Expected shard 3 result: Block_N+1_Result_3");
    println!("Actual shard 3 result:   {}", block_n1_results[3]);
    
    // VULNERABILITY: Shard 3 returned stale data from Block N instead of Block N+1!
    assert_eq!(block_n1_results[3], "Block_N_Result_3", 
        "CRITICAL BUG: Stale result from previous block returned!");
}
```

**Expected Output:**
```
=== Block N Execution ===
Shard 0: Block_N_Result_0
Shard 1: Block_N_Result_1
Shard 2: ERROR - Error from shard 2
Returning error immediately - subsequent results NOT read!

=== Block N+1 Execution ===
Shard 0: Block_N+1_Result_0
Shard 1: Block_N+1_Result_1
Shard 2: Block_N+1_Result_2
Shard 3: Block_N_Result_3

=== VULNERABILITY DEMONSTRATION ===
Expected shard 3 result: Block_N+1_Result_3
Actual shard 3 result:   Block_N_Result_3
```

This demonstrates that shard 3's result from Block N was incorrectly consumed during Block N+1 execution, proving the channel pollution vulnerability.

### Citations

**File:** execution/executor-service/src/local_executor_helper.rs (L14-21)
```rust
pub static SHARDED_BLOCK_EXECUTOR: Lazy<
    Arc<Mutex<ShardedBlockExecutor<CachedStateView, LocalExecutorClient<CachedStateView>>>>,
> = Lazy::new(|| {
    info!("LOCAL_SHARDED_BLOCK_EXECUTOR created");
    Arc::new(Mutex::new(
        LocalExecutorClient::create_local_sharded_block_executor(AptosVM::get_num_shards(), None),
    ))
});
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L88-91)
```rust
        let (result_txs, result_rxs): (
            Vec<Sender<Result<Vec<Vec<TransactionOutput>>, VMStatus>>>,
            Vec<Receiver<Result<Vec<Vec<TransactionOutput>>, VMStatus>>>,
        ) = (0..num_shards).map(|_| unbounded()).unzip();
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L164-175)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        let _timer = WAIT_FOR_SHARDED_OUTPUT_SECONDS.start_timer();
        trace!("LocalExecutorClient Waiting for results");
        let mut results = vec![];
        for (i, rx) in self.result_rxs.iter().enumerate() {
            results.push(
                rx.recv()
                    .unwrap_or_else(|_| panic!("Did not receive output from shard {}", i))?,
            );
        }
        Ok(results)
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L228-239)
```rust
impl<S: StateView + Sync + Send + 'static> Drop for LocalExecutorClient<S> {
    fn drop(&mut self) {
        for command_tx in self.command_txs.iter() {
            let _ = command_tx.send(ExecutorShardCommand::Stop);
        }

        // wait for join handles to finish
        for executor_service in self.executor_services.iter_mut() {
            let _ = executor_service.join_handle.take().unwrap().join();
        }
    }
}
```

**File:** aptos-move/aptos-vm/src/block_executor/vm_wrapper.rs (L99-115)
```rust
            // execute_single_transaction only returns an error when transactions that should never fail
            // (BlockMetadataTransaction and GenesisTransaction) return an error themselves.
            Err(err) => {
                if err.status_code() == StatusCode::SPECULATIVE_EXECUTION_ABORT_ERROR {
                    ExecutionStatus::SpeculativeExecutionAbortError(
                        err.message().cloned().unwrap_or_default(),
                    )
                } else if err.status_code()
                    == StatusCode::DELAYED_FIELD_OR_BLOCKSTM_CODE_INVARIANT_ERROR
                {
                    ExecutionStatus::DelayedFieldsCodeInvariantError(
                        err.message().cloned().unwrap_or_default(),
                    )
                } else {
                    ExecutionStatus::Abort(err)
                }
            },
```

**File:** aptos-move/block-executor/src/executor.rs (L2240-2248)
```rust
                    }
                    error!(
                        "Sequential execution FatalVMError by transaction {}",
                        idx as TxnIndex
                    );
                    // Record the status indicating the unrecoverable VM failure.
                    return Err(SequentialBlockExecutionError::ErrorToReturn(
                        BlockExecutionError::FatalVMError(err),
                    ));
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L269-275)
```rust
            Ok(V::execute_block_sharded(
                &SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        }
```
