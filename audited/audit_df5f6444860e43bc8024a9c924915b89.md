# Audit Report

## Title
Indexer Task Silent Failure Creates Zombie State with No Recovery Mechanism

## Summary
In `bootstrap_internal_indexer_db()` and `bootstrap()`, spawned async tasks can panic without propagating errors to the main runtime, leaving indexer services in a non-functional zombie state with no monitoring or automatic recovery.

## Finding Description

The vulnerability exists in two locations where indexer tasks are spawned without proper error handling:

**Location 1:** [1](#0-0) 

**Location 2:** [2](#0-1) 

When these tasks are spawned, the `JoinHandle` is immediately dropped. In Tokio's runtime model, when a spawned task panics:
1. The panic is caught by the runtime
2. Only that specific task terminates
3. The runtime and other tasks continue running
4. Without the `JoinHandle`, no one can detect the panic

**Panic Sources in InternalIndexerDBService:**
- [3](#0-2)  - Explicit panic on update receiver failure
- [4](#0-3)  - Panic on version mismatch (state keys)
- [5](#0-4)  - Panic on version mismatch (transactions)
- [6](#0-5)  - Panic on version mismatch (events)

**Panic Sources in TableInfoService:**
- [7](#0-6)  - Unwrap on fetch_batches
- [8](#0-7)  - Expect on snapshot creation
- [9](#0-8)  - Unwrap on version update
- [10](#0-9)  - Explicit panic on processing error

**Zombie State Manifestation:**
The runtime is kept alive in the `AptosHandle` struct [11](#0-10) , but the actual indexer task is dead. This creates a zombie state where:
- The node appears healthy
- API endpoints expecting indexer data return errors
- No alerts or logs indicate the failure
- Manual node restart required for recovery

## Impact Explanation

This qualifies as **High Severity** per Aptos bug bounty criteria:
- **API crashes**: API queries relying on indexer data fail after task termination
- **Validator node degradation**: While not a complete failure, indexer APIs become non-functional

The vulnerability breaks the **Resource Limits** invariant: the system should gracefully handle errors and provide monitoring capabilities for all critical services.

## Likelihood Explanation

**Likelihood: Medium-High**

Panic conditions can occur during:
- Node restarts with inconsistent indexer database state (version mismatches)
- Database corruption or filesystem issues
- Internal channel failures during high load
- Epoch transitions with backup enabled (snapshot failures)

These are operational conditions that can realistically occur in production without malicious intent. Once triggered, the system remains in the degraded state until manual intervention.

## Recommendation

Implement proper error handling and monitoring for spawned tasks:

```rust
// For bootstrap_internal_indexer_db()
let join_handle = runtime.spawn(async move {
    if let Err(e) = indexer_service.run(&config_clone).await {
        error!("Internal indexer DB service failed: {:?}", e);
        // Optionally: implement retry logic or graceful shutdown
    }
});

// Store join_handle for monitoring
// Add health check endpoint that verifies task is still running

// For bootstrap()
let join_handle = runtime.spawn(async move {
    // ... context setup ...
    parser.run().await;
    // Consider wrapping in error handling or adding panic hook
});
```

Additionally:
1. Add health monitoring for indexer tasks
2. Implement graceful error recovery instead of panics
3. Add metrics/alerts when indexer tasks terminate
4. Consider supervisor pattern for automatic restart

## Proof of Concept

```rust
// Simulation of the vulnerability
use tokio::runtime::Runtime;

#[tokio::test]
async fn test_indexer_zombie_state() {
    // Create runtime (simulating bootstrap_internal_indexer_db)
    let runtime = Runtime::new().unwrap();
    
    // Spawn task that will panic (simulating .unwrap() on error)
    runtime.spawn(async {
        panic!("Simulating indexer panic from version mismatch");
    });
    
    // Runtime continues running
    std::thread::sleep(std::time::Duration::from_millis(100));
    
    // No error is propagated - runtime is healthy but task is dead
    // This simulates the zombie state where:
    // - Runtime is alive (stored in AptosHandle)
    // - Indexer task is dead
    // - No monitoring detects the failure
    // - API queries to indexer will fail
}
```

To reproduce in actual codebase:
1. Start a node with internal indexer enabled
2. Manually corrupt the indexer database to create version mismatches
3. Restart the node
4. The indexer task will panic during `get_start_version()`
5. Node continues running but indexer queries fail
6. No alerts or logs clearly indicate the indexer is non-functional

## Notes

This vulnerability represents a critical gap in operational resilience. While the indexer is not consensus-critical, its failure degrades node functionality significantly. The silent failure mode makes debugging and incident response extremely difficult, as operators may not immediately realize the indexer has failed until users report API errors.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/runtime.rs (L42-44)
```rust
    runtime.spawn(async move {
        indexer_service.run(&config_clone).await.unwrap();
    });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/runtime.rs (L83-110)
```rust
    runtime.spawn(async move {
        let context = Arc::new(Context::new(
            chain_id,
            db_rw.reader.clone(),
            mp_sender,
            node_config.clone(),
            None,
        ));

        // DB backup is optional
        let backup_restore_operator = match node_config.indexer_table_info.table_info_service_mode {
            TableInfoServiceMode::Backup(gcs_bucket_name) => Some(Arc::new(
                GcsBackupRestoreOperator::new(gcs_bucket_name).await,
            )),
            _ => None,
        };

        let parser = TableInfoService::new(
            context,
            indexer_async_v2_clone.next_version(),
            node_config.indexer_table_info.parser_task_count,
            node_config.indexer_table_info.parser_batch_size,
            backup_restore_operator,
            indexer_async_v2_clone,
        );

        parser.run().await;
    });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/internal_indexer_db_service.rs (L114-116)
```rust
            if start_version != state_start_version {
                panic!("Cannot start state indexer because the progress doesn't match.");
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/internal_indexer_db_service.rs (L125-127)
```rust
            if start_version != transaction_start_version {
                panic!("Cannot start transaction indexer because the progress doesn't match.");
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/internal_indexer_db_service.rs (L136-138)
```rust
            if start_version != event_start_version {
                panic!("Cannot start event indexer because the progress doesn't match.");
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/internal_indexer_db_service.rs (L179-179)
```rust
                        panic!("Failed to get update from update_receiver: {}", e);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L114-114)
```rust
            let transactions = self.fetch_batches(batches, ledger_version).await.unwrap();
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L142-142)
```rust
                    .expect("Failed to snapshot indexer async v2");
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L304-304)
```rust
                    .unwrap();
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L308-311)
```rust
            Err(err) => panic!(
                "[Table Info] Error processing table info batches: {:?}",
                err
            ),
```

**File:** aptos-node/src/lib.rs (L870-870)
```rust
        _indexer_db_runtime: internal_indexer_db_runtime,
```
