Based on my comprehensive analysis of the Aptos Core codebase, I have validated this security claim and found it to be a **genuine Critical severity vulnerability**.

# Audit Report

## Title
QuorumStoreInlineHybrid Payload Availability Check Bypass Enables Chain Halt Attack

## Summary
The `check_payload_availability()` function unconditionally returns `Ok()` for `QuorumStoreInlineHybrid` payloads without verifying that batches referenced by ProofOfStore entries are actually available locally. This allows a malicious proposer to include valid but unavailable batch proofs in blocks, causing all validators to enter an infinite retry loop during execution, halting the entire blockchain.

## Finding Description
The vulnerability exists in the payload availability checking logic for `QuorumStoreInlineHybrid` payloads. The `check_payload_availability()` function always returns `Ok()` for this payload type based on the incorrect assumption that ProofOfStore entries guarantee batch data availability. [1](#0-0) 

While ProofOfStore entries contain valid aggregate signatures from a quorum of validators, they only prove that validators signed the batch metadata in the pastâ€”not that the actual batch data is currently available. The verification logic only checks cryptographic signatures: [2](#0-1) 

In contrast, `OptQuorumStore` payloads properly check batch availability by verifying that batches exist locally and returning an error with missing authors if unavailable. [3](#0-2) 

**Attack Flow:**

1. A malicious proposer creates a block with `QuorumStoreInlineHybrid` payload containing ProofOfStore entries for non-expired batches that are no longer available (e.g., due to network partitions, validator crashes before persistence, or partial propagation).

2. The block passes all verification checks because ProofOfStore signatures are cryptographically valid.

3. The block passes `check_payload_availability()` because it unconditionally returns `Ok()` for this payload type.

4. Validators proceed to vote without waiting for batch data: [4](#0-3) 

5. The block receives enough votes and gets committed via a QuorumCert.

6. During execution, the pipeline's materialize phase calls `get_transactions()`, which attempts to fetch unavailable batches through the batch requester. [5](#0-4) 

7. The batch requester retries up to the configured limit (default 10 retries), then returns `ExecutorError::CouldNotGetData`: [6](#0-5) 

8. This error propagates to `materialize_block()`, which is caught by an infinite retry loop with no timeout: [7](#0-6) 

9. All honest validators get stuck in this retry loop (sleeping 100ms between attempts), completely halting the blockchain.

**Key Attack Scenarios:**
- **Network Partition**: Include batches only available on isolated validators
- **Validator Crash Recovery**: Include batches from validators that crashed before disk persistence completed
- **Partial Batch Propagation**: Include batches that were not fully distributed to the network

Note that expired batches are skipped during transaction fetching, so the attack requires non-expired but unavailable batches. [8](#0-7) 

## Impact Explanation
This is a **Critical severity** vulnerability causing total loss of network availability:

- **Complete Chain Halt**: All validators attempting to execute the block hang indefinitely in the infinite retry loop, completely stopping block production and transaction processing.
- **Non-Recoverable Without Manual Intervention**: The retry loop has no timeout mechanism (comment states "the loop can only be abort by the caller"). The chain cannot self-recover and requires emergency intervention such as coordinated validator restarts or emergency patches.
- **Network-Wide Impact**: Affects all honest validators simultaneously once the malicious block is committed.
- **No Defense Mechanism**: The `check_payload_availability()` bypass means there is no protection against this attack.

Per Aptos bug bounty criteria, "Total loss of liveness/network availability" qualifies as **Critical severity** ($1,000,000), as the vulnerability causes complete network halt requiring emergency intervention.

## Likelihood Explanation
**Likelihood: High**

- **Low Attacker Requirements**: Any validator selected as block proposer can trigger this attack. No Byzantine collusion or special privileges required beyond normal validator participation.
- **Simple Execution**: The attacker only needs to include ProofOfStore entries for non-expired but unavailable batches in their proposal. Such batches can naturally arise from network conditions.
- **Natural Occurrence Possible**: This vulnerability can be triggered unintentionally during network partitions, validator failures, or timing edge cases where batches are not fully propagated before being included in proposals.
- **No Defense Mechanism**: The unconditional `Ok()` return for `QuorumStoreInlineHybrid` payloads means there is no protection against this attack vector.

## Recommendation
Implement proper batch availability checking for `QuorumStoreInlineHybrid` payloads, similar to the logic used for `OptQuorumStore` payloads:

1. Before returning `Ok()` in `check_payload_availability()`, verify that all batches referenced by ProofOfStore entries actually exist in the local batch reader.

2. If any batches are missing, return `Err(missing_authors)` to trigger the wait mechanism in the round manager.

3. Add a timeout mechanism to the materialize retry loop to prevent indefinite hangs, with appropriate error handling and recovery procedures.

4. Consider adding batch expiration checks earlier in the validation pipeline to reject blocks containing expired batch references before they can be voted on.

## Proof of Concept
To demonstrate this vulnerability, a malicious proposer would:

1. Collect valid ProofOfStore entries from the quorum store protocol
2. Wait for some validators to garbage collect or lose those batches (due to network partition, crash, or expiration on their local storage)
3. Create a block proposal including these ProofOfStore entries in a `QuorumStoreInlineHybrid` payload
4. Submit the proposal when selected as block leader
5. Observe that validators vote for the block (because `check_payload_availability()` returns `Ok()`)
6. Once committed, observe all validators hang in the materialize retry loop attempting to fetch unavailable batches

The vulnerability can be triggered in a test environment by:
- Creating a `QuorumStoreInlineHybrid` block with ProofOfStore for batches not in the local batch store
- Verifying the block passes `check_payload_availability()`
- Attempting execution and observing the infinite retry in the materialize phase

## Notes
This vulnerability represents a fundamental flaw in the assumption that ProofOfStore signatures guarantee current batch data availability. The cryptographic signatures only prove historical validation, not present-moment accessibility. The lack of actual availability verification, combined with the infinite retry loop without timeout, creates a critical attack vector for blockchain halting.

### Citations

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L102-107)
```rust
            if block_timestamp <= batch_info.expiration() {
                futures.push(batch_reader.get_batch(batch_info, responders.clone()));
            } else {
                debug!("QSE: skipped expired batch {}", batch_info.digest());
            }
        }
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L126-163)
```rust
    async fn get_transactions_quorum_store_inline_hybrid(
        &self,
        block: &Block,
        inline_batches: &[(BatchInfo, Vec<SignedTransaction>)],
        proof_with_data: &ProofWithData,
        max_txns_to_execute: &Option<u64>,
        block_gas_limit_override: &Option<u64>,
    ) -> ExecutorResult<BlockTransactionPayload> {
        let all_transactions = {
            let mut all_txns = process_qs_payload(
                proof_with_data,
                self.batch_reader.clone(),
                block,
                &self.ordered_authors,
            )
            .await?;
            all_txns.append(
                &mut inline_batches
                    .iter()
                    // TODO: Can clone be avoided here?
                    .flat_map(|(_batch_info, txns)| txns.clone())
                    .collect(),
            );
            all_txns
        };
        let inline_batches = inline_batches
            .iter()
            .map(|(batch_info, _)| batch_info.clone())
            .collect();
        Ok(BlockTransactionPayload::new_quorum_store_inline_hybrid(
            all_transactions,
            proof_with_data.proofs.clone(),
            *max_txns_to_execute,
            *block_gas_limit_override,
            inline_batches,
            self.enable_payload_v2,
        ))
    }
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L360-408)
```rust
            Payload::QuorumStoreInlineHybrid(inline_batches, proofs, _)
            | Payload::QuorumStoreInlineHybridV2(inline_batches, proofs, _) => {
                fn update_availability_metrics<'a>(
                    batch_reader: &Arc<dyn BatchReader>,
                    is_proof_label: &str,
                    batch_infos: impl Iterator<Item = &'a BatchInfo>,
                ) {
                    for (author, chunk) in &batch_infos.chunk_by(|info| info.author()) {
                        let (available_count, missing_count) = chunk
                            .map(|info| batch_reader.exists(info.digest()))
                            .fold((0, 0), |(available_count, missing_count), item| {
                                if item.is_some() {
                                    (available_count + 1, missing_count)
                                } else {
                                    (available_count, missing_count + 1)
                                }
                            });
                        counters::CONSENSUS_PROPOSAL_PAYLOAD_BATCH_AVAILABILITY_IN_QS
                            .with_label_values(&[
                                &author.to_hex_literal(),
                                is_proof_label,
                                "available",
                            ])
                            .inc_by(available_count as u64);
                        counters::CONSENSUS_PROPOSAL_PAYLOAD_BATCH_AVAILABILITY_IN_QS
                            .with_label_values(&[
                                &author.to_hex_literal(),
                                is_proof_label,
                                "missing",
                            ])
                            .inc_by(missing_count as u64);
                    }
                }

                update_availability_metrics(
                    &self.batch_reader,
                    "false",
                    inline_batches.iter().map(|(batch_info, _)| batch_info),
                );
                update_availability_metrics(
                    &self.batch_reader,
                    "true",
                    proofs.proofs.iter().map(|proof| proof.info()),
                );

                // The payload is considered available because it contains only proofs that guarantee network availabiliy
                // or inlined transactions.
                Ok(())
            },
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L409-442)
```rust
            Payload::OptQuorumStore(OptQuorumStorePayload::V1(p)) => {
                let mut missing_authors = BitVec::with_num_bits(self.ordered_authors.len() as u16);
                for batch in p.opt_batches().deref() {
                    if self.batch_reader.exists(batch.digest()).is_none() {
                        let index = *self
                            .address_to_validator_index
                            .get(&batch.author())
                            .expect("Payload author should have been verified");
                        missing_authors.set(index as u16);
                    }
                }
                if missing_authors.all_zeros() {
                    Ok(())
                } else {
                    Err(missing_authors)
                }
            },
            Payload::OptQuorumStore(OptQuorumStorePayload::V2(p)) => {
                let mut missing_authors = BitVec::with_num_bits(self.ordered_authors.len() as u16);
                for batch in p.opt_batches().deref() {
                    if self.batch_reader.exists(batch.digest()).is_none() {
                        let index = *self
                            .address_to_validator_index
                            .get(&batch.author())
                            .expect("Payload author should have been verified");
                        missing_authors.set(index as u16);
                    }
                }
                if missing_authors.all_zeros() {
                    Ok(())
                } else {
                    Err(missing_authors)
                }
            },
```

**File:** consensus/consensus-types/src/proof_of_store.rs (L635-650)
```rust
    pub fn verify(&self, validator: &ValidatorVerifier, cache: &ProofCache) -> anyhow::Result<()> {
        let batch_info_ext: BatchInfoExt = self.info.clone().into();
        if let Some(signature) = cache.get(&batch_info_ext) {
            if signature == self.multi_signature {
                return Ok(());
            }
        }
        let result = validator
            .verify_multi_signatures(&self.info, &self.multi_signature)
            .context(format!(
                "Failed to verify ProofOfStore for batch: {:?}",
                self.info
            ));
        if result.is_ok() {
            cache.insert(batch_info_ext, self.multi_signature.clone());
        }
```

**File:** consensus/src/round_manager.rs (L1262-1285)
```rust
        if block_store.check_payload(&proposal).is_err() {
            debug!("Payload not available locally for block: {}", proposal.id());
            counters::CONSENSUS_PROPOSAL_PAYLOAD_AVAILABILITY
                .with_label_values(&["missing"])
                .inc();
            let start_time = Instant::now();
            let deadline = self.round_state.current_round_deadline();
            let future = async move {
                (
                    block_store.wait_for_payload(&proposal, deadline).await,
                    proposal,
                    start_time,
                )
            }
            .boxed();
            self.futures.push(future);
            return Ok(());
        }

        counters::CONSENSUS_PROPOSAL_PAYLOAD_AVAILABILITY
            .with_label_values(&["available"])
            .inc();

        self.check_backpressure_and_process_proposal(proposal).await
```

**File:** consensus/src/quorum_store/batch_requester.rs (L101-180)
```rust
    pub(crate) async fn request_batch(
        &self,
        digest: HashValue,
        expiration: u64,
        responders: Arc<Mutex<BTreeSet<PeerId>>>,
        mut subscriber_rx: oneshot::Receiver<PersistedValue<BatchInfoExt>>,
    ) -> ExecutorResult<Vec<SignedTransaction>> {
        let validator_verifier = self.validator_verifier.clone();
        let mut request_state = BatchRequesterState::new(responders, self.retry_limit);
        let network_sender = self.network_sender.clone();
        let request_num_peers = self.request_num_peers;
        let my_peer_id = self.my_peer_id;
        let epoch = self.epoch;
        let retry_interval = Duration::from_millis(self.retry_interval_ms as u64);
        let rpc_timeout = Duration::from_millis(self.rpc_timeout_ms as u64);

        monitor!("batch_request", {
            let mut interval = time::interval(retry_interval);
            let mut futures = FuturesUnordered::new();
            let request = BatchRequest::new(my_peer_id, epoch, digest);
            loop {
                tokio::select! {
                    _ = interval.tick() => {
                        // send batch request to a set of peers of size request_num_peers
                        if let Some(request_peers) = request_state.next_request_peers(request_num_peers) {
                            for peer in request_peers {
                                futures.push(network_sender.request_batch(request.clone(), peer, rpc_timeout));
                            }
                        } else if futures.is_empty() {
                            // end the loop when the futures are drained
                            break;
                        }
                    },
                    Some(response) = futures.next() => {
                        match response {
                            Ok(BatchResponse::Batch(batch)) => {
                                counters::RECEIVED_BATCH_RESPONSE_COUNT.inc();
                                let payload = batch.into_transactions();
                                return Ok(payload);
                            }
                            // Short-circuit if the chain has moved beyond expiration
                            Ok(BatchResponse::NotFound(ledger_info)) => {
                                counters::RECEIVED_BATCH_NOT_FOUND_COUNT.inc();
                                if ledger_info.commit_info().epoch() == epoch
                                    && ledger_info.commit_info().timestamp_usecs() > expiration
                                    && ledger_info.verify_signatures(&validator_verifier).is_ok()
                                {
                                    counters::RECEIVED_BATCH_EXPIRED_COUNT.inc();
                                    debug!("QS: batch request expired, digest:{}", digest);
                                    return Err(ExecutorError::CouldNotGetData);
                                }
                            }
                            Ok(BatchResponse::BatchV2(_)) => {
                                error!("Batch V2 response is not supported");
                            }
                            Err(e) => {
                                counters::RECEIVED_BATCH_RESPONSE_ERROR_COUNT.inc();
                                debug!("QS: batch request error, digest:{}, error:{:?}", digest, e);
                            }
                        }
                    },
                    result = &mut subscriber_rx => {
                        match result {
                            Ok(persisted_value) => {
                                counters::RECEIVED_BATCH_FROM_SUBSCRIPTION_COUNT.inc();
                                let (_, maybe_payload) = persisted_value.unpack();
                                return Ok(maybe_payload.expect("persisted value must exist"));
                            }
                            Err(err) => {
                                debug!("channel closed: {}", err);
                            }
                        };
                    },
                }
            }
            counters::RECEIVED_BATCH_REQUEST_TIMEOUT_COUNT.inc();
            debug!("QS: batch request timed out, digest:{}", digest);
            Err(ExecutorError::CouldNotGetData)
        })
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L633-646)
```rust
        // the loop can only be abort by the caller
        let result = loop {
            match preparer.materialize_block(&block, qc_rx.clone()).await {
                Ok(input_txns) => break input_txns,
                Err(e) => {
                    warn!(
                        "[BlockPreparer] failed to prepare block {}, retrying: {}",
                        block.id(),
                        e
                    );
                    tokio::time::sleep(Duration::from_millis(100)).await;
                },
            }
        };
```
