# Audit Report

## Title
Non-Atomic Metadata Updates in IndexerAsyncV2 Allow Inconsistent Database State After Node Crashes

## Summary
The `IndexerAsyncV2` implementation in `storage/indexer/src/db_v2.rs` splits table info data writes and metadata updates into two separate database transactions. A node crash between these transactions leaves the database in an inconsistent state where indexed data exists but the metadata pointer (`LatestVersion`) doesn't reflect the indexed versions, violating the atomic state transition guarantee.

## Finding Description

The vulnerability exists in how `IndexerAsyncV2` handles metadata updates compared to table info data writes: [1](#0-0) 

The `index_with_annotator` method writes `TableInfoSchema` entries to the database and commits them atomically via `write_schemas` at line 113. However, the corresponding metadata update happens in a completely separate transaction: [2](#0-1) 

The `update_next_version` method writes `IndexerMetadataSchema::LatestVersion` using a separate `DB::put` call, which internally creates its own atomic transaction: [3](#0-2) 

In the table info service, these are called sequentially but not atomically: [4](#0-3) 

**Attack Scenario:**
1. Node processes versions 100-199 and calls `index_table_info`
2. `index_with_annotator` successfully writes all `TableInfoSchema` entries and commits
3. Node crashes before `update_next_version` is called (line 302-303)
4. After restart, `IndexerMetadataSchema::LatestVersion` still reports version 99
5. Database contains table info for versions 100-199 but metadata doesn't acknowledge this
6. Indexer attempts to re-process versions 100-199, causing operational confusion

**Contrast with Correct Implementation:**

The older `Indexer` implementation correctly includes metadata in the same atomic batch: [5](#0-4) 

Lines 140-143 add the metadata update to the same `SchemaBatch` before the atomic `write_schemas` call at line 144, ensuring atomicity.

Similarly, `InternalIndexerMetadataSchema` updates are correctly handled atomically: [6](#0-5) 

All metadata versions are added to the same batch as the data (lines 525-545) before being sent for atomic commit.

**Invariant Violation:**
This breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable." The database enters a state where data and metadata are inconsistent, requiring manual intervention to resolve.

## Impact Explanation

This qualifies as **Medium Severity** per Aptos bug bounty criteria: "State inconsistencies requiring intervention."

**Specific Impacts:**
- **Operational Disruption**: After crashes, operators must manually verify and potentially correct metadata to match actual indexed data
- **Re-indexing Overhead**: The indexer will wastefully re-process transactions that were already indexed, consuming resources
- **Monitoring Confusion**: Metrics and monitoring based on metadata will incorrectly report progress
- **Potential Race Conditions**: If multiple indexer instances run concurrently and crash at different points, they may have conflicting views of what's indexed

This does NOT reach Critical or High severity because:
- No loss of funds or consensus violations occur
- No permanent network partition or total liveness failure
- The indexer is a secondary index system, not the critical consensus path
- Re-processing is idempotent (table info writes check for existing data)

However, intervention IS required to restore consistent state, qualifying this as Medium severity.

## Likelihood Explanation

**Likelihood: Medium to High**

Node crashes are common operational occurrences:
- Hardware failures
- Process crashes due to bugs
- OOM conditions
- Operator-initiated restarts during the narrow window between transactions
- Network partitions causing process termination

The vulnerability window is small (microseconds to milliseconds between the two writes) but non-zero. Given that indexers process continuously and crashes are inevitable in production systems, this inconsistent state will occur with reasonable probability over time.

The parallel processing nature of `IndexerAsyncV2` (processing batches in multiple threads) increases the likelihood that table info writes complete while the final metadata update is pending.

## Recommendation

**Fix: Include metadata update in the same atomic batch as table info writes**

Modify `index_with_annotator` to include the metadata update in the same `SchemaBatch`:

```rust
pub fn index_with_annotator<R: StateView>(
    &self,
    annotator: &AptosValueAnnotator<R>,
    first_version: Version,
    write_sets: &[&WriteSet],
) -> Result<()> {
    let end_version = first_version + write_sets.len() as Version;
    let mut table_info_parser = TableInfoParser::new(self, annotator, &self.pending_on);
    for write_set in write_sets {
        for (state_key, write_op) in write_set.write_op_iter() {
            table_info_parser.collect_table_info_from_write_op(state_key, write_op)?;
        }
    }
    let mut batch = SchemaBatch::new();
    match self.finish_table_info_parsing(&mut batch, &table_info_parser.result) {
        Ok(_) => {},
        Err(err) => {
            aptos_logger::error!(
                first_version = first_version,
                end_version = end_version,
                error = ?&err,
                "[DB] Failed to parse table info"
            );
            bail!("{}", err);
        },
    };
    
    // ADD METADATA UPDATE TO THE SAME BATCH
    batch.put::<IndexerMetadataSchema>(
        &MetadataKey::LatestVersion,
        &MetadataValue::Version(end_version - 1),
    )?;
    
    self.db.write_schemas(batch)?;
    
    // Update in-memory version only after successful commit
    self.next_version.store(end_version, Ordering::Relaxed);
    
    Ok(())
}
```

Then remove the separate `update_next_version` call from the table info service, or make it a no-op that only updates the in-memory counter (which would be redundant since `index_with_annotator` now does it).

This matches the pattern used in the older `Indexer` implementation and ensures atomicity.

## Proof of Concept

**Scenario to trigger the vulnerability:**

1. Start an Aptos node with `IndexerAsyncV2` enabled for table info indexing
2. Allow it to process transactions normally
3. During active indexing, send SIGKILL to the process between the two transaction commits:
   - Monitor for successful `write_schemas` calls in `index_with_annotator`
   - Kill process before `update_next_version` executes
4. Restart the node
5. Query `IndexerMetadataSchema::LatestVersion` and compare with actual table info entries in the database

**Expected result:**
- Metadata reports version N
- Database contains table info for versions N+1 through N+M
- Inconsistent state requiring manual intervention

**Verification steps:**
```rust
// After crash and restart
let metadata_version = indexer_async_v2.next_version(); // Returns stale version
let actual_table_handles = /* query all TableInfoSchema entries */;
// Verify table handles exist for versions > metadata_version
// This proves the inconsistent state
```

The vulnerability can be reliably reproduced by introducing a controlled crash point between lines 113 and the eventual `update_next_version` call in the processing flow.

---

**Notes:**
- This vulnerability only affects `IndexerAsyncV2` (used in table info indexing), not the older `Indexer` or the `InternalIndexerMetadataSchema` used for event/transaction/state indexing
- The parallel processing model of `IndexerAsyncV2` makes the race window larger as multiple batches complete before the final metadata update
- While re-processing is designed to be idempotent, the inconsistent state violates the system's atomicity guarantees and requires operational intervention

### Citations

**File:** storage/indexer/src/db_v2.rs (L87-114)
```rust
    pub fn index_with_annotator<R: StateView>(
        &self,
        annotator: &AptosValueAnnotator<R>,
        first_version: Version,
        write_sets: &[&WriteSet],
    ) -> Result<()> {
        let end_version = first_version + write_sets.len() as Version;
        let mut table_info_parser = TableInfoParser::new(self, annotator, &self.pending_on);
        for write_set in write_sets {
            for (state_key, write_op) in write_set.write_op_iter() {
                table_info_parser.collect_table_info_from_write_op(state_key, write_op)?;
            }
        }
        let mut batch = SchemaBatch::new();
        match self.finish_table_info_parsing(&mut batch, &table_info_parser.result) {
            Ok(_) => {},
            Err(err) => {
                aptos_logger::error!(
                    first_version = first_version,
                    end_version = end_version,
                    error = ?&err,
                    "[DB] Failed to parse table info"
                );
                bail!("{}", err);
            },
        };
        self.db.write_schemas(batch)?;
        Ok(())
```

**File:** storage/indexer/src/db_v2.rs (L117-123)
```rust
    pub fn update_next_version(&self, end_version: u64) -> Result<()> {
        self.db.put::<IndexerMetadataSchema>(
            &MetadataKey::LatestVersion,
            &MetadataValue::Version(end_version - 1),
        )?;
        self.next_version.store(end_version, Ordering::Relaxed);
        Ok(())
```

**File:** storage/schemadb/src/lib.rs (L239-243)
```rust
    pub fn put<S: Schema>(&self, key: &S::Key, value: &S::Value) -> DbResult<()> {
        // Not necessary to use a batch, but we'd like a central place to bump counters.
        let mut batch = self.new_native_batch();
        batch.put::<S>(key, value)?;
        self.write_schemas(batch)
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L301-304)
```rust
                // Update rocksdb's to be processed next version after verifying all txns are successfully parsed
                self.indexer_async_v2
                    .update_next_version(end_version + 1)
                    .unwrap();
```

**File:** storage/indexer/src/lib.rs (L126-144)
```rust
        let mut batch = SchemaBatch::new();
        match table_info_parser.finish(&mut batch) {
            Ok(_) => {},
            Err(err) => {
                aptos_logger::error!(first_version = first_version, end_version = end_version, error = ?&err);
                write_sets
                    .iter()
                    .enumerate()
                    .for_each(|(i, write_set)| {
                        aptos_logger::error!(version = first_version as usize + i, write_set = ?write_set);
                    });
                db_other_bail!("Failed to parse table info: {:?}", err);
            },
        };
        batch.put::<IndexerMetadataSchema>(
            &MetadataKey::LatestVersion,
            &MetadataValue::Version(end_version - 1),
        )?;
        self.db.write_schemas(batch)?;
```

**File:** storage/indexer/src/db_indexer.rs (L524-548)
```rust
        if self.indexer_db.transaction_enabled() {
            batch.put::<InternalIndexerMetadataSchema>(
                &MetadataKey::TransactionVersion,
                &MetadataValue::Version(version - 1),
            )?;
        }
        if self.indexer_db.event_enabled() {
            batch.put::<InternalIndexerMetadataSchema>(
                &MetadataKey::EventVersion,
                &MetadataValue::Version(version - 1),
            )?;
        }
        if self.indexer_db.statekeys_enabled() {
            batch.put::<InternalIndexerMetadataSchema>(
                &MetadataKey::StateVersion,
                &MetadataValue::Version(version - 1),
            )?;
        }
        batch.put::<InternalIndexerMetadataSchema>(
            &MetadataKey::LatestVersion,
            &MetadataValue::Version(version - 1),
        )?;
        self.sender
            .send(Some(batch))
            .map_err(|e| AptosDbError::Other(e.to_string()))?;
```
