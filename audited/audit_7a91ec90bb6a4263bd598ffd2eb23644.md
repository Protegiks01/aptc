# Audit Report

## Title
Unbounded Memory Allocation in Backup Restore Deserialization Leading to Denial of Service

## Summary
The `LoadedChunk::load()` function in the transaction backup restore code uses unbounded BCS deserialization on attacker-controlled backup file data. This allows an attacker who can control backup files to craft malicious BCS-encoded records that cause memory exhaustion, leading to crashes and denial of service during node recovery operations.

## Finding Description

The vulnerability exists in the transaction restore deserialization flow where backup data is processed without size limits: [1](#0-0) 

The flow has two critical weaknesses:

**1. Unbounded Record Reading:** The `read_record_bytes()` method allocates memory based on a u32 size prefix from the backup file without validation: [2](#0-1) 

This allows allocation of up to 4GB (u32::MAX bytes) per record based solely on attacker-controlled data.

**2. Unbounded BCS Deserialization:** The deserialized types (Transaction, WriteSet, Vec<ContractEvent>, etc.) can contain arbitrarily large nested structures. While other security-critical paths use bounded deserialization, the backup restore code does not: [3](#0-2) 

The API enforces a `MAX_SIGNED_TRANSACTION_DEPTH` limit of 16 when deserializing user-submitted transactions, but the backup restore uses unbounded `bcs::from_bytes()`.

**Attack Scenario:**

1. Attacker gains control over backup storage (via compromise, misconfiguration, or social engineering)
2. Attacker crafts malicious backup files with:
   - Large u32 size prefixes (e.g., 1GB-4GB)
   - BCS-encoded structures with millions of Vec elements
   - Each element containing large Bytes fields
3. Node operator initiates restore from compromised backup
4. `read_record_bytes()` allocates gigabytes of memory per record
5. `bcs::from_bytes()` deserializes unbounded nested structures
6. Node experiences OOM crash or memory exhaustion

The runtime size limits for WriteSets (10MB) and ContractEvents (10MB) defined in the gas schedule only apply during VM execution, not during backup restore: [4](#0-3) 

This creates a bypass where backup restore can process data that would be rejected during normal transaction execution.

## Impact Explanation

**Severity: High**

This vulnerability enables:

1. **Validator Node Crashes**: Memory exhaustion causing OOM kills during critical recovery operations
2. **Recovery Denial of Service**: Prevents successful restoration from backups, blocking disaster recovery
3. **Operational Disruption**: Forces manual intervention and delays network recovery

Per the Aptos bug bounty criteria, this qualifies as **High Severity** due to:
- Validator node slowdowns/crashes (explicit High severity category)
- Significant protocol violations (bypassing resource limits invariant)

While not reaching Critical severity (no consensus break or fund loss), this represents a serious availability risk during the critical backup restore operation that could prevent network recovery.

## Likelihood Explanation

**Likelihood: Medium**

The attack requires:
- Control over backup storage location OR ability to social engineer operators
- Operator initiating restore from malicious source
- No additional authentication beyond storage access

This is realistic because:
1. Backup storage credentials may be less protected than validator keys
2. Operators might restore from untrusted backups during emergency recovery
3. Cloud storage misconfigurations are common (public S3 buckets, weak IAM policies)
4. No integrity checks validate data size before deserialization

The impact is amplified during disaster recovery scenarios when operators are under pressure and may skip verification steps.

## Recommendation

Implement size limits for backup deserialization at multiple layers:

**1. Add per-record size limit in read_record_bytes():**

```rust
// In read_record_bytes() at line 54
const MAX_RECORD_SIZE: usize = 100 << 20; // 100MB reasonable limit
let record_size = u32::from_be_bytes(size_buf.as_ref().try_into()?) as usize;
ensure!(
    record_size <= MAX_RECORD_SIZE,
    "Record size {} exceeds maximum allowed size {}",
    record_size,
    MAX_RECORD_SIZE
);
```

**2. Use bounded BCS deserialization in LoadedChunk::load():**

```rust
// At lines 121 and 130, replace bcs::from_bytes with:
const MAX_DESERIALIZATION_DEPTH: usize = 32; // Match network protocol limits

let (txn, txn_info, events, write_set) = 
    bcs::from_bytes_with_limit(&record_bytes, MAX_DESERIALIZATION_DEPTH)?;
```

**3. Validate cumulative allocations:**

Track total memory allocated per chunk and enforce limits to prevent death by a thousand cuts:

```rust
let mut total_allocated: usize = 0;
const MAX_CHUNK_MEMORY: usize = 1 << 30; // 1GB per chunk

while let Some(record_bytes) = file.read_record_bytes().await? {
    total_allocated = total_allocated.saturating_add(record_bytes.len());
    ensure!(
        total_allocated <= MAX_CHUNK_MEMORY,
        "Chunk total allocation {} exceeds limit",
        total_allocated
    );
    // ... deserialization
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_malicious_backup_oom() {
    use bytes::BufMut;
    use std::io::Cursor;
    
    // Create malicious backup record with 2GB allocation claim
    let malicious_size: u32 = 2_000_000_000; // 2GB
    let mut malicious_record = vec![];
    malicious_record.put_u32(malicious_size);
    // Add minimal data to trigger allocation attempt
    malicious_record.extend_from_slice(&[0u8; 1024]);
    
    let mut reader = Cursor::new(malicious_record);
    
    // This will attempt to allocate 2GB and likely crash/hang
    let result = reader.read_record_bytes().await;
    
    // Without size limits, this either:
    // 1. Allocates 2GB (memory exhaustion)
    // 2. Fails with allocation error (still DoS)
    // 3. Hangs waiting for 2GB of data that never arrives
    
    // With proper size limits, this should fail fast with error
    assert!(result.is_err(), "Should reject oversized records");
}

#[tokio::test]  
async fn test_malicious_bcs_large_vec() {
    use aptos_types::contract_event::ContractEvent;
    
    // Create BCS-encoded Vec with millions of elements
    let mut malicious_events: Vec<ContractEvent> = Vec::new();
    for i in 0..10_000_000 {
        // Each event with large data payload
        let event_data = vec![0u8; 1024]; // 1KB each
        malicious_events.push(ContractEvent::new_v2(
            TypeTag::Bool, 
            event_data
        ).unwrap());
    }
    
    // Serialize to BCS
    let encoded = bcs::to_bytes(&malicious_events).unwrap();
    
    // Without limits, deserializing this allocates ~10GB
    // (10M events * 1KB each)
    let result: Result<Vec<ContractEvent>> = bcs::from_bytes(&encoded);
    
    // This should either OOM or take excessive time
    // With proper limits, should fail fast
}
```

## Notes

This vulnerability specifically affects the **backup restore code path**, which is separate from the normal transaction execution path. While VM execution has proper resource limits (10MB WriteSet limit, gas metering, etc.), these protections are bypassed during backup restore operations.

The vulnerability is exploitable by anyone who can influence the backup source location, making it a realistic attack vector during disaster recovery scenarios. The fix should align backup restore limits with the existing runtime limits to maintain defense-in-depth.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L112-131)
```rust
        while let Some(record_bytes) = file.read_record_bytes().await? {
            let (txn, aux_info, txn_info, events, write_set): (
                _,
                PersistedAuxiliaryInfo,
                _,
                _,
                WriteSet,
            ) = match manifest.format {
                TransactionChunkFormat::V0 => {
                    let (txn, txn_info, events, write_set) = bcs::from_bytes(&record_bytes)?;
                    (
                        txn,
                        PersistedAuxiliaryInfo::None,
                        txn_info,
                        events,
                        write_set,
                    )
                },
                TransactionChunkFormat::V1 => bcs::from_bytes(&record_bytes)?,
            };
```

**File:** storage/backup/backup-cli/src/utils/read_record_bytes.rs (L44-67)
```rust
    async fn read_record_bytes(&mut self) -> Result<Option<Bytes>> {
        let _timer = BACKUP_TIMER.timer_with(&["read_record_bytes"]);
        // read record size
        let mut size_buf = BytesMut::with_capacity(4);
        self.read_full_buf_or_none(&mut size_buf).await?;
        if size_buf.is_empty() {
            return Ok(None);
        }

        // empty record
        let record_size = u32::from_be_bytes(size_buf.as_ref().try_into()?) as usize;
        if record_size == 0 {
            return Ok(Some(Bytes::new()));
        }

        // read record
        let mut record_buf = BytesMut::with_capacity(record_size);
        self.read_full_buf_or_none(&mut record_buf).await?;
        if record_buf.is_empty() {
            bail!("Hit EOF when reading record.")
        }

        Ok(Some(record_buf.freeze()))
    }
```

**File:** api/src/transactions.rs (L851-851)
```rust
    const MAX_SIGNED_TRANSACTION_DEPTH: usize = 16;
```

**File:** aptos-move/aptos-vm-types/src/storage/change_set_configs.rs (L1-1)
```rust
// Copyright (c) Aptos Foundation
```
