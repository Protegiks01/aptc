# Audit Report

## Title
Memory Exhaustion via Oversized OptProposalMsg Deserialization Before Consensus Size Validation

## Summary
An attacker can send `OptProposalMsg` messages with validator_txns and payload vectors up to the network limit (~62 MiB), causing memory allocation during BCS deserialization before consensus-level size limits (6 MiB max_receiving_block_bytes) are enforced, enabling memory exhaustion attacks on validator nodes.

## Finding Description

The vulnerability exists due to a critical ordering mismatch between network-layer deserialization and consensus-layer validation:

**1. No Size Checks in OptBlockData::new()**

The `OptBlockData::new()` constructor accepts validator_txns and payload without any size validation: [1](#0-0) 

**2. Network Deserialization Occurs Before Validation**

When an `OptProposalMsg` arrives over the network, it is deserialized using BCS with only a recursion depth limit (not a size limit): [2](#0-1) 

The recursion limit (RECURSION_LIMIT = 64) only prevents deeply nested structures, NOT large vectors: [3](#0-2) 

**3. Network Allows Messages Up To 64 MiB**

The network layer permits messages up to MAX_MESSAGE_SIZE: [4](#0-3) 

**4. Consensus Expects Much Smaller Messages**

However, consensus is configured to reject blocks exceeding much smaller limits: [5](#0-4) 

**5. Size Validation Happens AFTER Deserialization**

The size limits are only enforced in `process_proposal()`, which occurs after the full message has been deserialized: [6](#0-5) 

**6. verify_well_formed() Does NOT Check Sizes**

The `verify_well_formed()` function only validates round relationships, timestamps, and epochsâ€”not vector sizes: [7](#0-6) 

**Attack Flow:**

1. Attacker crafts `OptProposalMsg` with ~50,000 transactions (fits within 64 MiB network limit but exceeds 10,000 transaction consensus limit)
2. Message is sent to validator nodes
3. Network layer deserializes entire message, allocating memory for all 50,000 transactions
4. `OptProposalMsg.verify()` validates cryptographic signatures but not sizes
5. Only when `process_proposal()` is called does the node discover it exceeds limits and reject it
6. Memory has already been allocated and must be freed

By sending multiple such messages concurrently (exploiting parallel deserialization), an attacker can cause significant memory pressure or out-of-memory conditions. [8](#0-7) 

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos bug bounty program:

- **Resource Exhaustion Leading to Node Instability**: Repeated oversized messages can cause memory pressure, garbage collection pauses, and potential validator slowdowns
- **DoS Vector Against Consensus Participants**: All validators receiving these messages are affected simultaneously
- **Breaks Resource Limits Invariant**: Violates "All operations must respect gas, storage, and computational limits"

While a single 62 MiB allocation is manageable, concurrent processing of multiple such messages (with `max_parallel_deserialization_tasks` allowing parallel processing) can accumulate to gigabytes of unnecessary memory allocation. This is particularly concerning during network stress or when combined with other resource-intensive operations.

The 10x mismatch between network limits (64 MiB) and consensus limits (6 MiB) represents a significant architectural gap that enables this attack.

## Likelihood Explanation

**High Likelihood:**
- Requires no validator access or special privileges
- Attacker only needs network connectivity to any validator
- Message construction is straightforward using standard Aptos tools
- No sophisticated timing or race condition exploitation needed
- All validators receiving optimistic proposals are vulnerable

The attack is limited only by:
- Network rate limiting (if configured)
- Connection limits per peer
- The attacker's bandwidth

## Recommendation

**Implement Size Validation During Deserialization:**

Add a custom deserializer for `OptProposalMsg` that enforces size limits before allocating memory:

```rust
// In consensus/consensus-types/src/opt_proposal_msg.rs

impl OptProposalMsg {
    pub fn from_bytes_with_size_check(bytes: &[u8], max_bytes: usize) -> anyhow::Result<Self> {
        ensure!(
            bytes.len() <= max_bytes,
            "OptProposalMsg size {} exceeds limit {}",
            bytes.len(),
            max_bytes
        );
        bcs::from_bytes(bytes).map_err(|e| anyhow!("{:?}", e))
    }
}
```

**Alternative: Enforce Consensus Limits at Network Layer:**

Configure the network layer to reject consensus messages exceeding `max_receiving_block_bytes` before deserialization:

```rust
// In consensus/src/network.rs or network configuration
const MAX_CONSENSUS_MESSAGE_SIZE: usize = 10 * 1024 * 1024; // 10 MiB buffer above 6 MiB limit

// Check message size before deserialization
if message_bytes.len() > MAX_CONSENSUS_MESSAGE_SIZE {
    return Err(anyhow!("Consensus message too large"));
}
```

**Best Practice: Streaming Deserialization with Early Rejection:**

Implement streaming deserialization that checks cumulative size as vectors are being deserialized, allowing early rejection before full memory allocation.

## Proof of Concept

```rust
// Proof of Concept: Create oversized OptProposalMsg

use aptos_consensus_types::{
    opt_block_data::OptBlockData,
    opt_proposal_msg::OptProposalMsg,
    common::Payload,
};
use aptos_types::transaction::SignedTransaction;

fn create_oversized_opt_proposal() -> OptProposalMsg {
    // Create a large vector of transactions exceeding consensus limits
    let mut large_payload = Vec::new();
    
    // Add 50,000 dummy transactions (~1KB each = ~50MB total)
    // This fits within 64 MiB network limit but exceeds 10,000 tx consensus limit
    for i in 0..50000 {
        let dummy_txn = create_dummy_signed_transaction(i);
        large_payload.push(dummy_txn);
    }
    
    let opt_block_data = OptBlockData::new(
        vec![], // validator_txns
        Payload::DirectMempool(large_payload), // oversized payload
        author,
        epoch,
        round,
        timestamp,
        parent,
        grandparent_qc,
    );
    
    let sync_info = create_sync_info();
    
    OptProposalMsg::new(opt_block_data, sync_info)
}

// When this message is serialized and sent over the network:
// 1. Network layer accepts it (< 64 MiB)
// 2. BCS deserialization allocates memory for all 50,000 transactions
// 3. verify_well_formed() passes (doesn't check sizes)
// 4. Only in process_proposal() is it rejected for exceeding 10,000 tx limit
// 5. Memory has already been allocated unnecessarily
```

## Notes

This vulnerability specifically affects `OptProposalMsg` messages used in optimistic consensus proposals. Regular `ProposalMsg` messages have the same underlying issue but may have different impact characteristics. The vulnerability is exacerbated by the parallel deserialization mechanism which allows multiple oversized messages to be processed concurrently, multiplying the memory impact.

### Citations

**File:** consensus/consensus-types/src/opt_block_data.rs (L31-53)
```rust
    pub fn new(
        validator_txns: Vec<ValidatorTransaction>,
        payload: Payload,
        author: Author,
        epoch: u64,
        round: Round,
        timestamp_usecs: u64,
        parent: BlockInfo,
        grandparent_qc: QuorumCert,
    ) -> Self {
        Self {
            epoch,
            round,
            timestamp_usecs,
            parent,
            block_body: OptBlockBody::V0 {
                validator_txns,
                payload,
                author,
                grandparent_qc,
            },
        }
    }
```

**File:** consensus/consensus-types/src/opt_block_data.rs (L75-116)
```rust
    pub fn verify_well_formed(&self) -> anyhow::Result<()> {
        let parent = self.parent();
        let grandparent_qc = self.grandparent_qc().certified_block();
        ensure!(
            grandparent_qc.round() + 1 == parent.round(),
            "Block's parent's round {} must be one more than grandparent's round {}",
            parent.round(),
            grandparent_qc.round(),
        );
        ensure!(
            parent.round() + 1 == self.round(),
            "Block's round {} must be one more than parent's round {}",
            self.round(),
            parent.round(),
        );
        ensure!(
            grandparent_qc.epoch() == self.epoch() && parent.epoch() == self.epoch(),
            "Block's parent and grantparent should be in the same epoch"
        );
        ensure!(
            !grandparent_qc.has_reconfiguration(),
            "Optimistic proposals are disallowed after the reconfiguration block"
        );

        self.payload().verify_epoch(self.epoch())?;

        ensure!(
            self.timestamp_usecs() > parent.timestamp_usecs()
                && parent.timestamp_usecs() > grandparent_qc.timestamp_usecs(),
            "Blocks must have strictly increasing timestamps"
        );

        let current_ts = duration_since_epoch();

        // we can say that too far is 5 minutes in the future
        const TIMEBOUND: u64 = 300_000_000;
        ensure!(
            self.timestamp_usecs() <= (current_ts.as_micros() as u64).saturating_add(TIMEBOUND),
            "Blocks must not be too far in the future"
        );
        Ok(())
    }
```

**File:** network/framework/src/protocols/wire/handshake/v1/mod.rs (L38-39)
```rust
pub const USER_INPUT_RECURSION_LIMIT: usize = 32;
pub const RECURSION_LIMIT: usize = 64;
```

**File:** network/framework/src/protocols/wire/handshake/v1/mod.rs (L260-262)
```rust
    fn bcs_decode<T: DeserializeOwned>(&self, bytes: &[u8], limit: usize) -> anyhow::Result<T> {
        bcs::from_bytes_with_limit(bytes, limit).map_err(|e| anyhow!("{:?}", e))
    }
```

**File:** config/src/config/network_config.rs (L50-50)
```rust
pub const MAX_MESSAGE_SIZE: usize = 64 * 1024 * 1024; /* 64 MiB */
```

**File:** config/src/config/consensus_config.rs (L227-231)
```rust
            max_sending_block_bytes: 3 * 1024 * 1024, // 3MB
            max_receiving_block_txns: *MAX_RECEIVING_BLOCK_TXNS,
            max_sending_inline_txns: 100,
            max_sending_inline_bytes: 200 * 1024,       // 200 KB
            max_receiving_block_bytes: 6 * 1024 * 1024, // 6MB
```

**File:** consensus/src/round_manager.rs (L1178-1193)
```rust
        let payload_len = proposal.payload().map_or(0, |payload| payload.len());
        let payload_size = proposal.payload().map_or(0, |payload| payload.size());
        ensure!(
            num_validator_txns + payload_len as u64 <= self.local_config.max_receiving_block_txns,
            "Payload len {} exceeds the limit {}",
            payload_len,
            self.local_config.max_receiving_block_txns,
        );

        ensure!(
            validator_txns_total_bytes + payload_size as u64
                <= self.local_config.max_receiving_block_bytes,
            "Payload size {} exceeds the limit {}",
            payload_size,
            self.local_config.max_receiving_block_bytes,
        );
```

**File:** network/framework/src/protocols/network/mod.rs (L217-219)
```rust
        let data_event_stream = peer_mgr_notifs_rx.map(|notification| {
            tokio::task::spawn_blocking(move || received_message_to_event(notification))
        });
```
