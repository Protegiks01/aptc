# Audit Report

## Title
Resource Exhaustion via Indefinite Secret Share Requester Tasks Due to DropGuard Lifecycle Bug

## Summary
The `spawn_share_requester_task()` function in `SecretShareManager` spawns async tasks that can run indefinitely when secret sharing threshold is not met. DropGuards holding abort handles are only dropped when blocks complete secret sharing or during epoch reset. If blocks never complete secret sharing (due to insufficient validator responses), the tasks accumulate over time, consuming resources and potentially causing node degradation or denial of service.

## Finding Description

The vulnerability exists in the lifecycle management of spawned secret share requester tasks. [1](#0-0) 

When processing incoming blocks, each block spawns a task that multicasts share requests to validators. [2](#0-1) 

The spawned task uses `ReliableBroadcast::multicast()` which retries failed RPCs indefinitely with exponential backoff until aggregation completes. [3](#0-2) 

Aggregation only completes when sufficient validator weight meets the threshold. [4](#0-3) 

The DropGuards are stored in QueueItems and only dropped when blocks become "fully secret shared" and are dequeued. [5](#0-4) 

**Attack Scenario:**
1. Network partition or validator downtime causes < threshold validators to respond to share requests
2. Secret sharing aggregation never completes for affected blocks
3. QueueItems remain in the BlockQueue indefinitely (until epoch reset)
4. DropGuards are never dropped, so spawned tasks continue running
5. Tasks retry share requests periodically with exponential backoff (max 3 seconds)
6. Over hundreds of rounds within an epoch, dozens to hundreds of tasks accumulate
7. Each task consumes: tokio task slot, memory for future state, CPU for retries, network bandwidth
8. Resource exhaustion degrades node performance or causes crash
9. Additionally, blocks without completed secret sharing block downstream processing

This violates the **Resource Limits** invariant - operations must respect computational and resource limits.

## Impact Explanation

**Medium Severity** per Aptos bug bounty criteria:
- **State inconsistencies requiring intervention**: Blocks accumulate in queue without being processed downstream, creating pipeline blockage
- **Validator node slowdowns**: Resource exhaustion from accumulated tasks degrades node performance
- Does not directly cause fund loss or consensus safety violations
- Impact is gradual (accumulates over time within epoch)
- Gets cleaned up on epoch transition [6](#0-5) 

The issue can realistically occur during network issues or with < 1/3 Byzantine validators withholding shares, making it exploitable in production environments.

## Likelihood Explanation

**Moderate to High Likelihood:**
- Network partitions and validator downtime are realistic operational scenarios
- Byzantine validators (< 1/3) can deliberately withhold shares to trigger accumulation
- Aptos consensus produces blocks at ~1/second, enabling rapid task accumulation
- No timeout mechanism exists in the multicast operation - it retries indefinitely
- The backoff configuration allows unlimited retries [7](#0-6) 

The attack does not require validator access for natural occurrence (network issues), though Byzantine validators can deliberately exploit it.

## Recommendation

**Implement task lifecycle bounds with timeout mechanisms:**

1. **Add overall timeout to multicast operations:**
```rust
fn spawn_share_requester_task(&self, metadata: SecretShareMetadata) -> DropGuard {
    let rb = self.reliable_broadcast.clone();
    let aggregate_state = Arc::new(SecretShareAggregateState::new(
        self.secret_share_store.clone(),
        metadata.clone(),
        self.config.clone(),
    ));
    let epoch_state = self.epoch_state.clone();
    let secret_share_store = self.secret_share_store.clone();
    
    // Add configurable timeout (e.g., 30-60 seconds)
    let task_timeout = Duration::from_secs(60);
    
    let task = async move {
        tokio::time::sleep(Duration::from_millis(300)).await;
        let maybe_existing_shares = secret_share_store.lock().get_all_shares_authors(&metadata);
        if let Some(existing_shares) = maybe_existing_shares {
            let request = RequestSecretShare::new(metadata.clone());
            let targets = epoch_state
                .verifier
                .get_ordered_account_addresses_iter()
                .filter(|author| !existing_shares.contains(author))
                .collect::<Vec<_>>();
            
            // Wrap multicast with timeout
            if let Err(e) = tokio::time::timeout(
                task_timeout,
                rb.multicast(request, aggregate_state, targets)
            ).await {
                warn!(
                    round = metadata.round,
                    "Share requester task timed out after {:?}", task_timeout
                );
            }
        }
    };
    
    let (abort_handle, abort_registration) = AbortHandle::new_pair();
    tokio::spawn(Abortable::new(task, abort_registration));
    DropGuard::new(abort_handle)
}
```

2. **Add periodic cleanup of stale QueueItems:**
Monitor queue items that remain unfinished beyond a threshold duration and explicitly drop their DropGuards.

3. **Add max queue size enforcement:**
Limit the maximum number of pending QueueItems to prevent unbounded growth.

## Proof of Concept

```rust
// Reproduction steps (integration test):

#[tokio::test]
async fn test_secret_share_task_accumulation() {
    // Setup: Create SecretShareManager with test configuration
    let (manager, mut block_rx) = create_test_manager();
    
    // Simulate validator network where < threshold respond
    let mock_network = MockNetwork::with_responding_validators(2, 10); // 2/10 < threshold
    
    // Generate 100 consecutive blocks
    for round in 1..=100 {
        let block = create_test_block(round);
        manager.process_incoming_blocks(vec![block]).await;
    }
    
    // Verify: Check task accumulation
    tokio::time::sleep(Duration::from_secs(10)).await;
    
    // Expected: 100 tasks still running (never completed aggregation)
    let active_tasks = count_active_share_requester_tasks(&manager);
    assert!(active_tasks >= 90, "Tasks should accumulate when aggregation fails");
    
    // Expected: Queue contains all 100 blocks (none became ready)
    let queue_size = manager.block_queue.queue().len();
    assert_eq!(queue_size, 100, "Blocks should accumulate in queue");
    
    // Expected: Downstream receives no blocks (all blocked)
    assert!(block_rx.try_recv().is_err(), "No blocks should be forwarded");
    
    // Verify resource consumption
    let memory_usage = measure_memory_usage(&manager);
    assert!(memory_usage > BASELINE * 10, "Memory should grow significantly");
}
```

## Notes

The vulnerability is exacerbated by:
- No task-level timeout in the multicast operation
- Exponential backoff has a maximum delay (3 seconds) but no maximum total duration
- QueueItems only removed when fully secret shared or during epoch reset
- High block production rate (~1/second) enables rapid accumulation

The issue is partially mitigated by epoch transitions that trigger `process_reset()` and replace the BlockQueue, dropping all pending DropGuards. However, within a single epoch (which can last hours), significant resource exhaustion can occur.

### Citations

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L112-130)
```rust
    async fn process_incoming_blocks(&mut self, blocks: OrderedBlocks) {
        let rounds: Vec<u64> = blocks.ordered_blocks.iter().map(|b| b.round()).collect();
        info!(rounds = rounds, "Processing incoming blocks.");

        let mut share_requester_handles = Vec::new();
        let mut pending_secret_key_rounds = HashSet::new();
        for block in blocks.ordered_blocks.iter() {
            let handle = self.process_incoming_block(block).await;
            share_requester_handles.push(handle);
            pending_secret_key_rounds.insert(block.round());
        }

        let queue_item = QueueItem::new(
            blocks,
            Some(share_requester_handles),
            pending_secret_key_rounds,
        );
        self.block_queue.push_back(queue_item);
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L172-184)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.secret_share_store
            .lock()
            .update_highest_known_round(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L237-277)
```rust
    fn spawn_share_requester_task(&self, metadata: SecretShareMetadata) -> DropGuard {
        let rb = self.reliable_broadcast.clone();
        let aggregate_state = Arc::new(SecretShareAggregateState::new(
            self.secret_share_store.clone(),
            metadata.clone(),
            self.config.clone(),
        ));
        let epoch_state = self.epoch_state.clone();
        let secret_share_store = self.secret_share_store.clone();
        let task = async move {
            // TODO(ibalajiarun): Make this configurable
            tokio::time::sleep(Duration::from_millis(300)).await;
            let maybe_existing_shares = secret_share_store.lock().get_all_shares_authors(&metadata);
            if let Some(existing_shares) = maybe_existing_shares {
                let epoch = epoch_state.epoch;
                let request = RequestSecretShare::new(metadata.clone());
                let targets = epoch_state
                    .verifier
                    .get_ordered_account_addresses_iter()
                    .filter(|author| !existing_shares.contains(author))
                    .collect::<Vec<_>>();
                info!(
                    epoch = epoch,
                    round = metadata.round,
                    "[SecretShareManager] Start broadcasting share request for {}",
                    targets.len(),
                );
                rb.multicast(request, aggregate_state, targets)
                    .await
                    .expect("Broadcast cannot fail");
                info!(
                    epoch = epoch,
                    round = metadata.round,
                    "[SecretShareManager] Finish broadcasting share request",
                );
            }
        };
        let (abort_handle, abort_registration) = AbortHandle::new_pair();
        tokio::spawn(Abortable::new(task, abort_registration));
        DropGuard::new(abort_handle)
    }
```

**File:** crates/reliable-broadcast/src/lib.rs (L167-204)
```rust
            loop {
                tokio::select! {
                    Some((receiver, result)) = rpc_futures.next() => {
                        let aggregating = aggregating.clone();
                        let future = executor.spawn(async move {
                            (
                                    receiver,
                                    result
                                        .and_then(|msg| {
                                            msg.try_into().map_err(|e| anyhow::anyhow!("{:?}", e))
                                        })
                                        .and_then(|ack| aggregating.add(receiver, ack)),
                            )
                        }).await;
                        aggregate_futures.push(future);
                    },
                    Some(result) = aggregate_futures.next() => {
                        let (receiver, result) = result.expect("spawned task must succeed");
                        match result {
                            Ok(may_be_aggragated) => {
                                if let Some(aggregated) = may_be_aggragated {
                                    return Ok(aggregated);
                                }
                            },
                            Err(e) => {
                                log_rpc_failure(e, receiver);

                                let backoff_strategy = backoff_policies
                                    .get_mut(&receiver)
                                    .expect("should be present");
                                let duration = backoff_strategy.next().expect("should produce value");
                                rpc_futures
                                    .push(send_message(receiver, Some(duration)));
                            },
                        }
                    },
                    else => unreachable!("Should aggregate with all responses")
                }
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L38-46)
```rust
    pub fn try_aggregate(
        self,
        secret_share_config: &SecretShareConfig,
        metadata: SecretShareMetadata,
        decision_tx: Sender<SecretSharedKey>,
    ) -> Either<Self, SecretShare> {
        if self.total_weight < secret_share_config.threshold() {
            return Either::Left(self);
        }
```

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L112-127)
```rust
    pub fn dequeue_ready_prefix(&mut self) -> Vec<OrderedBlocks> {
        let mut ready_prefix = vec![];
        while let Some((_starting_round, item)) = self.queue.first_key_value() {
            if item.is_fully_secret_shared() {
                let (_, item) = self.queue.pop_first().expect("First key must exist");
                for block in item.blocks() {
                    observe_block(block.timestamp_usecs(), BlockStage::SECRET_SHARING_READY);
                }
                let QueueItem { ordered_blocks, .. } = item;
                ready_prefix.push(ordered_blocks);
            } else {
                break;
            }
        }
        ready_prefix
    }
```

**File:** config/src/config/dag_consensus_config.rs (L104-123)
```rust
pub struct ReliableBroadcastConfig {
    pub backoff_policy_base_ms: u64,
    pub backoff_policy_factor: u64,
    pub backoff_policy_max_delay_ms: u64,

    pub rpc_timeout_ms: u64,
}

impl Default for ReliableBroadcastConfig {
    fn default() -> Self {
        Self {
            // A backoff policy that starts at 100ms and doubles each iteration up to 3secs.
            backoff_policy_base_ms: 2,
            backoff_policy_factor: 50,
            backoff_policy_max_delay_ms: 3000,

            rpc_timeout_ms: 1000,
        }
    }
}
```
