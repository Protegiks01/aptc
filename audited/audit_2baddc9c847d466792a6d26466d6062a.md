# Audit Report

## Title
Batch Expiration Bypass Enables Resource Exhaustion DOS via Unbounded Memory Growth in Consensus Layer

## Summary
The `insert_batches()` function in `batch_proof_queue.rs` accepts batches with arbitrary expiration timestamps without validation, allowing malicious validators to inject batches with expiration set to `u64::MAX` that will never be garbage collected, causing unbounded memory growth and consensus node resource exhaustion.

## Finding Description

The vulnerability exists in the batch validation and insertion flow where `BatchMsg` messages bypass expiration validation, unlike `SignedBatchInfo` messages which properly validate expiration bounds.

**Root Cause:**

When batches arrive via `BatchMsg`, the verification process at [1](#0-0)  calls `BatchMsg::verify()` which validates message structure, author identity, and payload integrity at [2](#0-1)  but **does not validate the expiration timestamp**.

In contrast, `SignedBatchInfo` messages properly validate expiration at [3](#0-2)  by checking that expiration does not exceed `current_time + max_batch_expiry_gap_usecs`, preventing far-future or malicious expiration values.

**Attack Path:**

1. A Byzantine validator constructs a `Batch` with `expiration = u64::MAX`
2. Sends this batch via `BatchMsg` over the consensus network
3. The receiving node validates the message at [4](#0-3)  but expiration is not checked
4. Batch flows through network listener at [5](#0-4)  to batch coordinator
5. Batch coordinator processes at [6](#0-5)  with no expiration validation
6. Proof manager calls `insert_batches()` at [7](#0-6) 
7. Batch is inserted into expiration tracking at [8](#0-7)  with expiration `u64::MAX`
8. The cleanup mechanism at [9](#0-8)  calls `expirations.expire(block_timestamp)` but since `block_timestamp` can never exceed `u64::MAX` (max value of `u64`), these batches are **never cleaned up**

**Broken Invariant:**

This violates the "**Resource Limits**" invariant: "All operations must respect gas, storage, and computational limits." The system allows unbounded memory accumulation in the batch proof queue's data structures: `items` HashMap, `author_to_batches` BTreeMap, `expirations` TimeExpirations, and `txn_summary_num_occurrences` HashMap.

## Impact Explanation

**High Severity** - This vulnerability enables a resource exhaustion DOS attack:

1. **Memory Exhaustion**: Malicious batches accumulate indefinitely in multiple data structures, causing unbounded memory growth until out-of-memory (OOM) conditions
2. **Performance Degradation**: The `pull_internal()` function at [10](#0-9)  iterates over all batches, becoming progressively slower as malicious batches accumulate
3. **Back Pressure**: Resource counters trigger back pressure at [11](#0-10)  blocking new legitimate transactions
4. **Node Crashes**: OOM conditions crash consensus nodes, affecting network liveness

This qualifies as **High Severity** under Aptos Bug Bounty criteria: "Validator node slowdowns" and potential "API crashes."

## Likelihood Explanation

**Likelihood: High**

- **Attack Complexity: Low** - A malicious validator simply sets `expiration: u64::MAX` when constructing batches
- **Attacker Requirements: Validator Status** - Requires being part of the validator set, but Byzantine fault tolerance assumes up to 1/3 validators may be malicious
- **Detection Difficulty: High** - The attack is silent and gradual; malicious batches appear valid and only manifest as slow memory growth
- **Attack Cost: Negligible** - No economic cost beyond validator stake (which malicious validators already possess)

The system is designed to tolerate Byzantine validators, yet this vulnerability allows a single malicious validator to cause resource exhaustion DOS, breaking the Byzantine fault tolerance guarantees.

## Recommendation

Add expiration validation to `BatchMsg::verify()` method similar to `SignedBatchInfo::verify()`:

```rust
// In consensus/src/quorum_store/types.rs, modify BatchMsg::verify()
pub fn verify(
    &self,
    peer_id: PeerId,
    max_num_batches: usize,
    max_batch_expiry_gap_usecs: u64,  // Add this parameter
    verifier: &ValidatorVerifier,
) -> anyhow::Result<()> {
    ensure!(!self.batches.is_empty(), "Empty message");
    ensure!(
        self.batches.len() <= max_num_batches,
        "Too many batches: {} > {}",
        self.batches.len(),
        max_num_batches
    );
    let epoch_authors = verifier.address_to_validator_index();
    for batch in self.batches.iter() {
        ensure!(
            epoch_authors.contains_key(&batch.author()),
            "Invalid author {} for batch {} in current epoch",
            batch.author(),
            batch.digest()
        );
        ensure!(
            batch.author() == peer_id,
            "Batch author doesn't match sender"
        );
        
        // ADD THIS VALIDATION
        let current_time = aptos_infallible::duration_since_epoch().as_micros() as u64;
        ensure!(
            batch.expiration() <= current_time + max_batch_expiry_gap_usecs,
            "Batch expiration too far in future: {} > {}",
            batch.expiration(),
            current_time + max_batch_expiry_gap_usecs
        );
        
        batch.verify()?
    }
    Ok(())
}
```

Update all call sites in [12](#0-11)  to pass `max_batch_expiry_gap_usecs` parameter.

## Proof of Concept

```rust
#[test]
fn test_batch_expiration_bypass() {
    use aptos_types::PeerId;
    use aptos_consensus_types::proof_of_store::BatchInfoExt;
    use crate::quorum_store::batch_proof_queue::BatchProofQueue;
    use crate::quorum_store::batch_store::BatchStore;
    use std::sync::Arc;
    
    // Setup
    let my_peer_id = PeerId::random();
    let batch_store = Arc::new(BatchStore::new(/* ... */));
    let mut queue = BatchProofQueue::new(
        my_peer_id,
        batch_store,
        1_000_000, // batch_expiry_gap_when_init_usecs
    );
    
    // Create malicious batch with u64::MAX expiration
    let malicious_batch_info = BatchInfoExt::new_v1(
        my_peer_id,
        BatchId::new(1),
        1, // epoch
        u64::MAX, // MALICIOUS EXPIRATION
        HashValue::random(),
        10, // num_txns
        1000, // num_bytes
        0, // gas_bucket_start
    );
    
    let txn_summaries = vec![/* ... */];
    
    // Insert malicious batch
    queue.insert_batches(vec![(malicious_batch_info.clone(), txn_summaries)]);
    
    // Simulate time passing - set block timestamp to near u64::MAX
    let near_max_timestamp = u64::MAX - 1_000_000;
    queue.handle_updated_block_timestamp(near_max_timestamp);
    
    // VULNERABILITY: Malicious batch should be expired but remains in queue
    assert!(
        queue.items.contains_key(&BatchKey::from_info(&malicious_batch_info)),
        "Malicious batch with u64::MAX expiration was not cleaned up!"
    );
    
    // Memory continues to grow as more malicious batches are added
    // Eventually causing OOM and node crash
}
```

**Notes:**
- This vulnerability is particularly dangerous because it's exploitable by any validator in the Byzantine threat model (up to 1/3 of validators may be malicious)
- The asymmetry between `BatchMsg` and `SignedBatchInfo` validation creates this security gap
- The fix requires passing `max_batch_expiry_gap_usecs` through the verification chain and validating expiration bounds for all batch types

### Citations

**File:** consensus/src/round_manager.rs (L166-177)
```rust
            UnverifiedEvent::BatchMsg(b) => {
                if !self_message {
                    b.verify(peer_id, max_num_batches, validator)?;
                    counters::VERIFY_MSG
                        .with_label_values(&["batch"])
                        .observe(start_time.elapsed().as_secs_f64());
                }
                VerifiedEvent::BatchMsg(Box::new((*b).into()))
            },
            UnverifiedEvent::BatchMsgV2(b) => {
                if !self_message {
                    b.verify(peer_id, max_num_batches, validator)?;
```

**File:** consensus/src/quorum_store/types.rs (L433-461)
```rust
    pub fn verify(
        &self,
        peer_id: PeerId,
        max_num_batches: usize,
        verifier: &ValidatorVerifier,
    ) -> anyhow::Result<()> {
        ensure!(!self.batches.is_empty(), "Empty message");
        ensure!(
            self.batches.len() <= max_num_batches,
            "Too many batches: {} > {}",
            self.batches.len(),
            max_num_batches
        );
        let epoch_authors = verifier.address_to_validator_index();
        for batch in self.batches.iter() {
            ensure!(
                epoch_authors.contains_key(&batch.author()),
                "Invalid author {} for batch {} in current epoch",
                batch.author(),
                batch.digest()
            );
            ensure!(
                batch.author() == peer_id,
                "Batch author doesn't match sender"
            );
            batch.verify()?
        }
        Ok(())
    }
```

**File:** consensus/consensus-types/src/proof_of_store.rs (L469-479)
```rust
        if self.expiration()
            > aptos_infallible::duration_since_epoch().as_micros() as u64
                + max_batch_expiry_gap_usecs
        {
            bail!(
                "Batch expiration too far in future: {} > {}",
                self.expiration(),
                aptos_infallible::duration_since_epoch().as_micros() as u64
                    + max_batch_expiry_gap_usecs
            );
        }
```

**File:** consensus/src/quorum_store/network_listener.rs (L90-91)
```rust
                        self.remote_batch_coordinator_tx[idx]
                            .send(BatchCoordinatorCommand::NewBatches(author, batches))
```

**File:** consensus/src/quorum_store/batch_coordinator.rs (L173-244)
```rust
    pub(crate) async fn handle_batches_msg(
        &mut self,
        author: PeerId,
        batches: Vec<Batch<BatchInfoExt>>,
    ) {
        if let Err(e) = self.ensure_max_limits(&batches) {
            error!("Batch from {}: {}", author, e);
            counters::RECEIVED_BATCH_MAX_LIMIT_FAILED.inc();
            return;
        }

        let Some(batch) = batches.first() else {
            error!("Empty batch received from {}", author.short_str().as_str());
            return;
        };

        // Filter the transactions in the batches. If any transaction is rejected,
        // the message will be dropped, and all batches will be rejected.
        if self.transaction_filter_config.is_enabled() {
            let transaction_filter = &self.transaction_filter_config.batch_transaction_filter();
            for batch in batches.iter() {
                for transaction in batch.txns() {
                    if !transaction_filter.allows_transaction(
                        batch.batch_info().batch_id(),
                        batch.author(),
                        batch.digest(),
                        transaction,
                    ) {
                        error!(
                            "Transaction {}, in batch {}, from {}, was rejected by the filter. Dropping {} batches!",
                            transaction.committed_hash(),
                            batch.batch_info().batch_id(),
                            author.short_str().as_str(),
                            batches.len()
                        );
                        counters::RECEIVED_BATCH_REJECTED_BY_FILTER.inc();
                        return;
                    }
                }
            }
        }

        let approx_created_ts_usecs = batch
            .info()
            .expiration()
            .saturating_sub(self.batch_expiry_gap_when_init_usecs);

        if approx_created_ts_usecs > 0 {
            observe_batch(
                approx_created_ts_usecs,
                batch.author(),
                BatchStage::RECEIVED,
            );
        }

        let mut persist_requests = vec![];
        for batch in batches.into_iter() {
            // TODO: maybe don't message batch generator if the persist is unsuccessful?
            if let Err(e) = self
                .sender_to_batch_generator
                .send(BatchGeneratorCommand::RemoteBatch(batch.clone()))
                .await
            {
                warn!("Failed to send batch to batch generator: {}", e);
            }
            persist_requests.push(batch.into());
        }
        counters::RECEIVED_BATCH_COUNT.inc_by(persist_requests.len() as u64);
        if author != self.my_peer_id {
            counters::RECEIVED_REMOTE_BATCH_COUNT.inc_by(persist_requests.len() as u64);
        }
        self.persist_and_send_digests(persist_requests, approx_created_ts_usecs);
```

**File:** consensus/src/quorum_store/proof_manager.rs (L84-84)
```rust
        self.batch_proof_queue.insert_batches(batch_summaries);
```

**File:** consensus/src/quorum_store/proof_manager.rs (L245-265)
```rust
    pub(crate) fn qs_back_pressure(&self) -> BackPressure {
        if self.remaining_total_txn_num > self.back_pressure_total_txn_limit
            || self.remaining_total_proof_num > self.back_pressure_total_proof_limit
        {
            sample!(
                SampleRate::Duration(Duration::from_millis(200)),
                info!(
                    "Quorum store is back pressured with {} txns, limit: {}, proofs: {}, limit: {}",
                    self.remaining_total_txn_num,
                    self.back_pressure_total_txn_limit,
                    self.remaining_total_proof_num,
                    self.back_pressure_total_proof_limit
                );
            );
        }

        BackPressure {
            txn_count: self.remaining_total_txn_num > self.back_pressure_total_txn_limit,
            proof_count: self.remaining_total_proof_num > self.back_pressure_total_proof_limit,
        }
    }
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L282-283)
```rust
            self.expirations
                .add_item(batch_sort_key, batch_info.expiration());
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L561-714)
```rust
    fn pull_internal(
        &mut self,
        batches_without_proofs: bool,
        excluded_batches: &HashSet<BatchInfoExt>,
        exclude_authors: &HashSet<Author>,
        max_txns: PayloadTxnsSize,
        max_txns_after_filtering: u64,
        soft_max_txns_after_filtering: u64,
        return_non_full: bool,
        block_timestamp: Duration,
        min_batch_age_usecs: Option<u64>,
    ) -> (Vec<&QueueItem>, PayloadTxnsSize, u64, bool) {
        let mut result = Vec::new();
        let mut cur_unique_txns = 0;
        let mut cur_all_txns = PayloadTxnsSize::zero();
        let mut excluded_txns = 0;
        let mut full = false;
        // Set of all the excluded transactions and all the transactions included in the result
        let mut filtered_txns = HashSet::new();
        for batch_info in excluded_batches {
            let batch_key = BatchKey::from_info(batch_info);
            if let Some(txn_summaries) = self
                .items
                .get(&batch_key)
                .and_then(|item| item.txn_summaries.as_ref())
            {
                for txn_summary in txn_summaries {
                    filtered_txns.insert(*txn_summary);
                }
            }
        }

        let max_batch_creation_ts_usecs = min_batch_age_usecs
            .map(|min_age| aptos_infallible::duration_since_epoch().as_micros() as u64 - min_age);
        let mut iters = vec![];
        for (_, batches) in self
            .author_to_batches
            .iter()
            .filter(|(author, _)| !exclude_authors.contains(author))
        {
            let batch_iter = batches.iter().rev().filter_map(|(sort_key, info)| {
                if let Some(item) = self.items.get(&sort_key.batch_key) {
                    let batch_create_ts_usecs =
                        item.info.expiration() - self.batch_expiry_gap_when_init_usecs;

                    // Ensure that the batch was created at least `min_batch_age_usecs` ago to
                    // reduce the chance of inline fetches.
                    if max_batch_creation_ts_usecs
                        .is_some_and(|max_create_ts| batch_create_ts_usecs > max_create_ts)
                    {
                        return None;
                    }

                    if item.is_committed() {
                        return None;
                    }
                    if !(batches_without_proofs ^ item.proof.is_none()) {
                        return Some((info, item));
                    }
                }
                None
            });
            iters.push(batch_iter);
        }

        while !iters.is_empty() {
            iters.shuffle(&mut thread_rng());
            iters.retain_mut(|iter| {
                if full {
                    return false;
                }

                if let Some((batch, item)) = iter.next() {
                    if excluded_batches.contains(batch) {
                        excluded_txns += batch.num_txns();
                    } else {
                        // Calculate the number of unique transactions if this batch is included in the result
                        let unique_txns = if let Some(ref txn_summaries) = item.txn_summaries {
                            cur_unique_txns
                                + txn_summaries
                                    .iter()
                                    .filter(|txn_summary| {
                                        !filtered_txns.contains(txn_summary)
                                            && block_timestamp.as_secs()
                                                < txn_summary.expiration_timestamp_secs
                                    })
                                    .count() as u64
                        } else {
                            cur_unique_txns + batch.num_txns()
                        };
                        if cur_all_txns + batch.size() > max_txns
                            || unique_txns > max_txns_after_filtering
                        {
                            // Exceeded the limit for requested bytes or number of transactions.
                            full = true;
                            return false;
                        }
                        cur_all_txns += batch.size();
                        // Add this batch to filtered_txns and calculate the number of
                        // unique transactions added in the result so far.
                        cur_unique_txns +=
                            item.txn_summaries
                                .as_ref()
                                .map_or(batch.num_txns(), |summaries| {
                                    summaries
                                        .iter()
                                        .filter(|summary| {
                                            filtered_txns.insert(**summary)
                                                && block_timestamp.as_secs()
                                                    < summary.expiration_timestamp_secs
                                        })
                                        .count() as u64
                                });
                        assert!(item.proof.is_none() == batches_without_proofs);
                        result.push(item);
                        if cur_all_txns == max_txns
                            || cur_unique_txns == max_txns_after_filtering
                            || cur_unique_txns >= soft_max_txns_after_filtering
                        {
                            full = true;
                            return false;
                        }
                    }
                    true
                } else {
                    false
                }
            })
        }
        info!(
            // before non full check
            block_total_txns = cur_all_txns,
            block_unique_txns = cur_unique_txns,
            max_txns = max_txns,
            max_txns_after_filtering = max_txns_after_filtering,
            soft_max_txns_after_filtering = soft_max_txns_after_filtering,
            max_bytes = max_txns.size_in_bytes(),
            result_is_proof = !batches_without_proofs,
            result_count = result.len(),
            full = full,
            return_non_full = return_non_full,
            "Pull payloads from QuorumStore: internal"
        );

        counters::EXCLUDED_TXNS_WHEN_PULL.observe(excluded_txns as f64);

        if full || return_non_full {
            // Stable sort, so the order of proofs within an author will not change.
            result.sort_by_key(|item| Reverse(item.info.gas_bucket_start()));
            (result, cur_all_txns, cur_unique_txns, full)
        } else {
            (Vec::new(), PayloadTxnsSize::zero(), 0, full)
        }
    }
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L729-729)
```rust
        let expired = self.expirations.expire(block_timestamp);
```
