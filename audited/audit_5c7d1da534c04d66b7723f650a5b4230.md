# Audit Report

## Title
Out-of-Memory Vulnerability in Consensus Database Loading During Node Bootstrap and Admin Service Endpoints

## Summary
Multiple functions in the consensus layer load entire datasets into memory without streaming or pagination, causing potential OOM kills on validator nodes during restart or when admin endpoints are accessed. The vulnerable functions `get_all_batches()`, `get_all_batches_v2()`, and `get_data()` collect complete database iterators into memory structures without size limits.

## Finding Description

The vulnerability exists in multiple code paths that load unbounded datasets into memory:

**1. QuorumStore Batch Loading**

The `get_all_batches()` method collects the entire database iterator into a HashMap without size limits or streaming: [1](#0-0) 

Similarly, `get_all_batches_v2()` exhibits the same pattern: [2](#0-1) 

**2. Node Bootstrap Memory Exhaustion**

During node restart when `is_new_epoch` is false, `populate_cache_and_gc_expired_batches_v1()` loads all batches into memory before filtering: [3](#0-2) 

The v2 variant repeats this pattern: [4](#0-3) 

Both functions are invoked during non-epoch-transition restarts: [5](#0-4) 

**3. Consensus Database Loading**

The `get_data()` method loads all blocks and quorum certificates into vectors using `collect()` without size limits: [6](#0-5) 

This is called during consensus recovery: [7](#0-6) 

**4. Admin Service Endpoints**

The `/debug/consensus/quorumstoredb` endpoint loads all batches: [8](#0-7) 

The `/debug/consensus/block` endpoint loads both all batches and all consensus data: [9](#0-8) 

And again in the BCS variant: [10](#0-9) 

**Resource Limits**

The per-peer `db_quota` is configured to 300 MB by default: [11](#0-10) 

The QuotaManager enforces per-peer limits but not total database size: [12](#0-11) 

**Attack Scenario:**

In a network with 100 validators, each peer can contribute up to 300 MB (db_quota). The total database size can theoretically reach 30 GB. When a validator node with 8 GB RAM restarts, the `populate_cache_and_gc_expired_batches` functions attempt to load the entire database into memory, causing an OOM kill that prevents the node from restarting successfully.

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos Bug Bounty criteria:

1. **Validator Node Slowdowns (High Severity)**: Memory pressure during database loading causes severe performance degradation. Nodes experience extended startup times and potential memory thrashing that affects consensus participation.

2. **API Crashes (High Severity)**: Admin service endpoints crash when attempting to load large datasets, making debugging tools unusable during incident response.

3. **Node Restart Failures**: Validators with limited memory configurations cannot successfully restart after database accumulation, leading to extended downtime, validator penalties, and reduced network participation.

The impact directly affects network liveness and validator operations. While not causing fund loss or consensus safety violations, it creates operational barriers and centralization risks where only high-memory nodes can operate reliably.

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability manifests under normal network conditions:

1. **Natural Accumulation**: Batches accumulate during regular network operation as validators receive and store batches from peers, respecting per-peer quotas but without total database size limits.

2. **Common Configurations**: Many validators run on cost-optimized VMs with 4-8 GB RAM, making them vulnerable when database size approaches or exceeds available memory.

3. **Frequent Triggers**: Node restarts occur regularly for upgrades, configuration changes, or recovery from crashes. Each restart attempts to load the entire database.

4. **Admin Tool Usage**: Operators legitimately use debug endpoints during troubleshooting, potentially triggering OOM at critical moments.

The vulnerability requires no malicious behavior—it occurs naturally as databases grow during normal operation.

## Recommendation

Implement streaming and pagination for database loading operations:

1. **For get_all_batches()**: Modify to return an iterator instead of collecting into HashMap, allowing callers to process batches incrementally.

2. **For populate_cache_and_gc_expired_batches()**: Process batches in batches (paginated reads), filtering expired entries before loading into memory.

3. **For get_data()**: Implement chunked loading with configurable maximum sizes for blocks and QCs.

4. **For admin endpoints**: Add pagination parameters and size limits to prevent loading entire databases.

5. **Add total database size limits**: Implement global quotas across all peers, not just per-peer quotas.

6. **Memory safety checks**: Validate available system memory before attempting large loads and fail gracefully with clear error messages.

## Proof of Concept

```rust
// Demonstrating the vulnerability requires a populated database
// This pseudocode shows the issue:

// 1. Start validator node, accumulate batches from 100 peers
// 2. Each peer contributes 300 MB → total 30 GB database
// 3. Restart node with 8 GB RAM
// 4. populate_cache_and_gc_expired_batches_v1() called
// 5. get_all_batches() collects entire 30 GB into HashMap
// 6. OOM kill occurs, node fails to restart

// To reproduce in a test environment:
// - Configure multiple validator nodes
// - Generate high transaction volume to accumulate batches
// - Restart a validator with limited memory (docker memory limit)
// - Observe OOM during populate_cache_and_gc_expired_batches
// - Or call /debug/consensus/quorumstoredb endpoint
```

A full reproduction requires a multi-validator testnet with sufficient transaction volume to accumulate large databases, then restarting nodes with memory limits to trigger OOM conditions.

### Citations

**File:** consensus/src/quorum_store/quorum_store_db.rs (L103-108)
```rust
    fn get_all_batches(&self) -> Result<HashMap<HashValue, PersistedValue<BatchInfo>>> {
        let mut iter = self.db.iter::<BatchSchema>()?;
        iter.seek_to_first();
        iter.map(|res| res.map_err(Into::into))
            .collect::<Result<HashMap<HashValue, PersistedValue<BatchInfo>>>>()
    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L133-138)
```rust
    fn get_all_batches_v2(&self) -> Result<HashMap<HashValue, PersistedValue<BatchInfoExt>>> {
        let mut iter = self.db.iter::<BatchV2Schema>()?;
        iter.seek_to_first();
        iter.map(|res| res.map_err(Into::into))
            .collect::<Result<HashMap<HashValue, PersistedValue<BatchInfoExt>>>>()
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L41-109)
```rust
pub(crate) struct QuotaManager {
    memory_balance: usize,
    db_balance: usize,
    batch_balance: usize,
    // Recording the provided quotas for asserts.
    memory_quota: usize,
    db_quota: usize,
    batch_quota: usize,
}

impl QuotaManager {
    pub(crate) fn new(db_quota: usize, memory_quota: usize, batch_quota: usize) -> Self {
        assert!(db_quota >= memory_quota);
        Self {
            memory_balance: memory_quota,
            db_balance: db_quota,
            batch_balance: batch_quota,
            memory_quota,
            db_quota,
            batch_quota,
        }
    }

    pub(crate) fn update_quota(&mut self, num_bytes: usize) -> anyhow::Result<StorageMode> {
        if self.batch_balance == 0 {
            counters::EXCEEDED_BATCH_QUOTA_COUNT.inc();
            bail!("Batch quota exceeded ");
        }

        if self.db_balance >= num_bytes {
            self.batch_balance -= 1;
            self.db_balance -= num_bytes;

            if self.memory_balance >= num_bytes {
                self.memory_balance -= num_bytes;
                Ok(StorageMode::MemoryAndPersisted)
            } else {
                Ok(StorageMode::PersistedOnly)
            }
        } else {
            counters::EXCEEDED_STORAGE_QUOTA_COUNT.inc();
            bail!("Storage quota exceeded ");
        }
    }

    fn assert_quota(balance: usize, to_free: usize, quota: usize, kind: &str) {
        assert!(
            balance + to_free <= quota,
            "Balance {} + to_free {} more than {} quota {}",
            balance,
            to_free,
            kind,
            quota,
        );
    }

    pub(crate) fn free_quota(&mut self, num_bytes: usize, storage_mode: StorageMode) {
        Self::assert_quota(self.batch_balance, 1, self.batch_quota, "Batch");
        self.batch_balance += 1;

        Self::assert_quota(self.db_balance, num_bytes, self.db_quota, "DB");
        self.db_balance += num_bytes;

        if matches!(storage_mode, StorageMode::MemoryAndPersisted) {
            Self::assert_quota(self.memory_balance, num_bytes, self.memory_quota, "Memory");
            self.memory_balance += num_bytes;
        }
    }
}
```

**File:** consensus/src/quorum_store/batch_store.rs (L161-176)
```rust
        } else {
            Self::populate_cache_and_gc_expired_batches_v1(
                db_clone.clone(),
                epoch,
                last_certified_time,
                expiration_buffer_usecs,
                &batch_store,
            );
            Self::populate_cache_and_gc_expired_batches_v2(
                db_clone,
                epoch,
                last_certified_time,
                expiration_buffer_usecs,
                &batch_store,
            );
        }
```

**File:** consensus/src/quorum_store/batch_store.rs (L252-254)
```rust
        let db_content = db
            .get_all_batches()
            .expect("failed to read v1 data from db");
```

**File:** consensus/src/quorum_store/batch_store.rs (L299-301)
```rust
        let db_content = db
            .get_all_batches_v2()
            .expect("failed to read v1 data from db");
```

**File:** consensus/src/consensusdb/mod.rs (L90-99)
```rust
        let consensus_blocks = self
            .get_all::<BlockSchema>()?
            .into_iter()
            .map(|(_, block)| block)
            .collect();
        let consensus_qcs = self
            .get_all::<QCSchema>()?
            .into_iter()
            .map(|(_, qc)| qc)
            .collect();
```

**File:** consensus/src/persistent_liveness_storage.rs (L521-524)
```rust
        let raw_data = self
            .db
            .get_data()
            .expect("unable to recover consensus data");
```

**File:** crates/aptos-admin-service/src/server/consensus/mod.rs (L171-171)
```rust
        for (digest, _batch) in quorum_store_db.get_all_batches()? {
```

**File:** crates/aptos-admin-service/src/server/consensus/mod.rs (L186-188)
```rust
    let all_batches = quorum_store_db.get_all_batches()?;

    let (_, _, blocks, _) = consensus_db.consensus_db().get_data()?;
```

**File:** crates/aptos-admin-service/src/server/consensus/mod.rs (L222-224)
```rust
    let all_batches = quorum_store_db.get_all_batches()?;

    let (_, _, blocks, _) = consensus_db.consensus_db().get_data()?;
```

**File:** config/src/config/quorum_store_config.rs (L134-134)
```rust
            db_quota: 300_000_000,
```
