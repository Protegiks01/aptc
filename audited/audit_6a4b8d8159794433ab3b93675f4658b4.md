# Audit Report

## Title
Silent Failure in Background Pruner Worker Allows Undetected Node Degradation Leading to Availability Loss

## Summary
Background pruning operations in AptosDB fail silently without notifying the main consensus thread. When pruning errors occur continuously (due to I/O errors, disk corruption, or permission issues), the pruner worker only logs errors and retries indefinitely. This allows old data to accumulate undetected, eventually exhausting disk space and halting the validator node. Unlike snapshot generation which has proper error propagation via `error_notification_sender`, the pruning subsystem lacks any mechanism to alert the main thread of persistent failures.

## Finding Description

The AptosDB storage system employs three background pruner workers (LedgerPruner, StateMerklePruner, StateKvPruner) that run in separate threads to delete old data beyond the configured prune window. The critical flaw exists in the error handling logic: [1](#0-0) 

When `self.pruner.prune(self.batch_size)` returns an error, the code only logs it and continues the loop. There is no mechanism to:
1. Notify the main consensus/commit thread of the failure
2. Track consecutive failure counts
3. Escalate persistent failures to an alert system
4. Prevent new commits when pruning is consistently failing

The main commit path sets pruner targets but never checks pruner health: [2](#0-1) 

The pruner managers provide an `is_pruning_pending()` method that could be used for health checks, but it's only used in tests: [3](#0-2) 

**Attack Scenario:**

1. Node operates normally with pruning enabled
2. Underlying storage encounters issues (disk corruption, I/O errors, permission changes, or filesystem bugs)
3. Pruning operations begin failing with RocksDB write errors when calling: [4](#0-3) 

4. Errors are caught and logged in the pruner worker, but execution continues
5. Old data accumulates as pruning makes no progress: transactions, events, state values, merkle nodes
6. Disk space gradually fills up over hours/days
7. Eventually, main consensus writes fail when disk reaches capacity
8. Node halts, unable to commit new blocks
9. If this affects multiple validators due to a common environmental issue (e.g., systemwide storage driver bug), network liveness is impacted

**Contrast with Snapshot Generation:**

State sync's snapshot generation properly propagates errors: [5](#0-4) 

This demonstrates that proper error propagation patterns exist in the codebase but were not applied to the pruner subsystem.

## Impact Explanation

This qualifies as **High Severity** under the Aptos bug bounty program criteria:

- **Validator node slowdowns**: As disk fills, write performance degrades
- **Total loss of liveness**: Node completely halts when disk is full, unable to participate in consensus
- **Network availability**: If multiple validators encounter similar storage issues, network liveness is impacted

The vulnerability breaks two critical invariants:

1. **State Consistency**: Nodes should maintain manageable storage footprints through pruning
2. **Availability**: Nodes should detect and report critical subsystem failures before they cause total failure

While external monitoring (Prometheus alerts for disk space) exists, node-internal error detection should be the first line of defense. Silent failures violate the principle of fail-fast and allow cascading failures. [6](#0-5) 

## Likelihood Explanation

**Likelihood: Medium-High**

Conditions that trigger this vulnerability:
- Disk hardware failures (bad sectors, controller issues)
- Filesystem corruption or bugs
- Permission changes on data directories
- Storage driver bugs
- Resource exhaustion (inodes, file descriptors)
- Cloud storage transient failures that become persistent

These are realistic operational scenarios that occur in production environments. The likelihood increases with:
- Large validator sets (more nodes = higher probability of environmental issues)
- Diverse infrastructure (different cloud providers, hardware configurations)
- Long-running nodes (more exposure to storage degradation)

The vulnerability is not directly exploitable by an external attacker, but represents a critical operational hazard that can lead to availability issues meeting High severity criteria.

## Recommendation

Implement error propagation and health monitoring for background pruner operations:

**1. Add pruner health tracking:**
```rust
// In PrunerWorkerInner
struct PrunerWorkerInner {
    pruning_time_interval_in_ms: u64,
    pruner: Arc<dyn DBPruner>,
    batch_size: usize,
    quit_worker: AtomicBool,
    consecutive_failures: AtomicUsize,  // NEW
    last_success_time: Arc<Mutex<Instant>>,  // NEW
}
```

**2. Track failure patterns and escalate:**
```rust
fn work(&self) {
    const MAX_CONSECUTIVE_FAILURES: usize = 100;
    const MAX_FAILURE_DURATION: Duration = Duration::from_secs(3600);
    
    while !self.quit_worker.load(Ordering::SeqCst) {
        let pruner_result = self.pruner.prune(self.batch_size);
        
        if pruner_result.is_err() {
            let failures = self.consecutive_failures.fetch_add(1, Ordering::SeqCst) + 1;
            let failure_duration = self.last_success_time.lock().elapsed();
            
            sample!(
                SampleRate::Duration(Duration::from_secs(1)),
                error!(
                    error = ?pruner_result.err().unwrap(),
                    consecutive_failures = failures,
                    "Pruner has error."
                )
            );
            
            // CRITICAL: Panic if pruning is persistently broken
            if failures >= MAX_CONSECUTIVE_FAILURES || failure_duration > MAX_FAILURE_DURATION {
                panic!(
                    "Pruner {} failed {} consecutive times over {:?}. Node must restart.",
                    self.pruner.name(), failures, failure_duration
                );
            }
            
            sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            continue;
        }
        
        // Reset on success
        self.consecutive_failures.store(0, Ordering::SeqCst);
        *self.last_success_time.lock() = Instant::now();
        
        if !self.pruner.is_pruning_pending() {
            sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
        }
    }
}
```

**3. Add pre-commit health checks:**
```rust
fn pre_commit_ledger(&self, chunk: ChunkToCommit, sync_commit: bool) -> Result<()> {
    // Check pruner health before allowing commits
    if self.ledger_pruner.is_pruner_enabled() {
        self.ledger_pruner.check_health()?;
    }
    // ... existing code
}
```

The fail-fast approach (panic on persistent failures) ensures operators are immediately alerted and can take corrective action before disk exhaustion causes total failure.

## Proof of Concept

**Reproduction Steps:**

1. Set up a validator node with pruning enabled and a small prune window (e.g., 10,000 versions)

2. Simulate pruning failures by introducing storage errors:
   - Modify file permissions on the RocksDB data directory to cause write failures
   - Or use a full disk scenario by limiting available space
   - Or inject RocksDB errors via fault injection

3. Observe the pruner worker behavior:
   - Monitor logs for repeated "Pruner has error" messages
   - Check metrics: `pruner_versions{label="target"}` continues increasing
   - Check metrics: `pruner_versions{label="progress"}` stops advancing
   - Note: No alerts or panics occur despite continuous failures

4. Continue committing transactions:
   - The main consensus path continues normally
   - Disk usage grows steadily as old data is not pruned
   - No errors are surfaced to consensus or operators

5. Eventually disk fills completely:
   - Main database writes begin failing with "No space left on device"
   - Node halts, unable to commit blocks
   - Consensus participation stops

**Expected vs Actual Behavior:**

- **Expected**: Pruner failures should panic the node or at minimum emit critical alerts after persistent failures, allowing operators to intervene before disk exhaustion
- **Actual**: Pruner failures are silently logged and retried indefinitely, allowing disk to fill without node-level awareness

This demonstrates the delegation safety failure: errors in delegated background operations (pruning) do not propagate to the main thread, violating the fail-fast principle and enabling cascading availability failures.

**Notes**

The vulnerability specifically affects **background pruning** operations. Snapshot generation in state-sync correctly implements error propagation through the `error_notification_sender` channel mechanism. The pruner subsystem should adopt a similar pattern or implement health checks in the commit path to ensure delegation safety.

### Citations

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L53-69)
```rust
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L625-632)
```rust
            // Activate the ledger pruner and state kv pruner.
            // Note the state merkle pruner is activated when state snapshots are persisted
            // in their async thread.
            self.ledger_pruner
                .maybe_set_pruner_target_db_version(version);
            self.state_store
                .state_kv_pruner
                .maybe_set_pruner_target_db_version(version);
```

**File:** storage/aptosdb/src/pruner/pruner_manager.rs (L37-38)
```rust
    #[allow(unused)]
    fn is_pruning_pending(&self) -> bool;
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L92-92)
```rust
            self.db_shard.write_schemas(batch)?;
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L956-965)
```rust
                        Err(error) => {
                            let error =
                                format!("Failed to commit state value chunk! Error: {:?}", error);
                            send_storage_synchronizer_error(
                                error_notification_sender.clone(),
                                notification_id,
                                error,
                            )
                            .await;
                        },
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L60-90)
```rust
        let state_merkle_pruner = StateMerklePrunerManager::new(
            Arc::clone(&state_merkle_db),
            pruner_config.state_merkle_pruner_config,
        );
        let epoch_snapshot_pruner = StateMerklePrunerManager::new(
            Arc::clone(&state_merkle_db),
            pruner_config.epoch_snapshot_pruner_config.into(),
        );
        let state_kv_pruner =
            StateKvPrunerManager::new(Arc::clone(&state_kv_db), pruner_config.ledger_pruner_config);
        let state_store = Arc::new(StateStore::new(
            Arc::clone(&ledger_db),
            hot_state_merkle_db,
            Arc::clone(&state_merkle_db),
            Arc::clone(&state_kv_db),
            state_merkle_pruner,
            epoch_snapshot_pruner,
            state_kv_pruner,
            buffered_state_target_items,
            hack_for_tests,
            empty_buffered_state_for_restore,
            skip_index_and_usage,
            internal_indexer_db.clone(),
            hot_state_config,
        ));

        let ledger_pruner = LedgerPrunerManager::new(
            Arc::clone(&ledger_db),
            pruner_config.ledger_pruner_config,
            internal_indexer_db,
        );
```
