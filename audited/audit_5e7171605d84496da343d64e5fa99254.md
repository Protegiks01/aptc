# Audit Report

## Title
Non-Atomic File Writes Leave Partial Data Causing Persistent Indexer DoS

## Summary
The local file store operator uses non-atomic `tokio::fs::write()` operations that can leave partial transaction data on disk when the filesystem becomes full. Subsequent reads of corrupted files trigger panic-based error handling in deserialization code, causing the indexer to crash repeatedly until manual intervention removes the corrupted files.

## Finding Description

The vulnerability exists in the write-read-deserialize pipeline of the indexer-grpc local file store operator:

**Write Path:** The `upload_transaction_batch()` function writes transaction batches to disk using `tokio::fs::write()`. [1](#0-0) 

This operation is **not atomic**. When the filesystem becomes full during the write operation, the system call can fail after writing partial data, leaving an incomplete file on disk. While the error is caught and returned [2](#0-1) , the partial file is never cleaned up or rolled back.

**Read Path:** When the indexer subsequently reads transaction data via `get_raw_file()`, it successfully reads whatever data exists in the file, including partial/corrupted data. [3](#0-2) 

There is no validation of file integrity, checksums, or size verification before the data is passed to deserialization.

**Deserialization Path:** The `into_transactions_in_storage()` function attempts to deserialize the corrupted data using `.expect()` for error handling. [4](#0-3) 

For LZ4-compressed data, the decompression and protobuf deserialization all use `.expect()` which triggers a **panic** on any error. [5](#0-4) 

Similarly, for JSON-based legacy format, JSON deserialization, base64 decoding, and protobuf deserialization all use `.expect()`. [6](#0-5) 

**Attack Scenario:**
1. Filesystem fills up during transaction batch upload (naturally or induced through storage exhaustion)
2. `tokio::fs::write()` partially completes before failing, leaving corrupted file
3. Error is returned but corrupted file remains on disk
4. Indexer restarts or attempts to serve this transaction range
5. `get_raw_file()` successfully reads the partial data
6. Deserialization encounters corrupt LZ4/JSON/protobuf data
7. `.expect()` causes thread panic, crashing the indexer
8. Crash loop continues on every restart until manual file deletion

## Impact Explanation

This qualifies as **Medium Severity** under the Aptos bug bounty program criteria: "State inconsistencies requiring intervention."

The vulnerability causes:
- **Persistent Denial of Service**: The indexer-grpc service becomes unavailable and cannot recover automatically
- **State Inconsistency**: Corrupted transaction files persist on disk, creating inconsistent indexer state
- **Manual Intervention Required**: Operations team must identify and manually delete corrupted files to restore service
- **Data Availability Impact**: Downstream consumers relying on the indexer gRPC service lose access to transaction data

While this does not directly affect blockchain consensus or validator operations, it impacts critical infrastructure that many applications depend on for transaction data access.

## Likelihood Explanation

**Likelihood: Medium-High**

Filesystem exhaustion is a realistic scenario in production environments:
- Rapid transaction volume growth can fill storage faster than anticipated
- Log file accumulation can consume available space
- Misconfigured disk quotas or monitoring gaps
- Storage hardware failures reducing available capacity

Once a partial file is written, the crash is deterministic and will occur on every indexer startup or read attempt for that transaction range. The vulnerability requires no attacker sophistication - it's triggered by environmental conditions.

## Recommendation

Implement atomic file write operations and graceful error handling:

1. **Use Atomic Writes**: Write to a temporary file, then atomically rename:
```rust
let temp_path = txns_path.with_extension(".tmp");
tokio::fs::write(&temp_path, file_entry.into_inner()).await?;
tokio::fs::rename(&temp_path, &txns_path).await?;
```

2. **Replace Panics with Result Types**: Change `into_transactions_in_storage()` to return `Result<TransactionsInStorage, Error>` instead of panicking.

3. **Add File Validation**: Implement checksums or size validation before deserialization to detect corrupted files early.

4. **Implement Recovery Logic**: Add corrupted file detection and automatic cleanup:
```rust
async fn get_raw_file(&self, version: u64) -> anyhow::Result<Vec<u8>> {
    let file_path = self.path.join(file_entry_key);
    match tokio::fs::read(&file_path).await {
        Ok(file) => {
            // Validate file integrity before returning
            if validate_file_integrity(&file, self.storage_format) {
                Ok(file)
            } else {
                // Corrupted file detected, clean up and return error
                tokio::fs::remove_file(&file_path).await.ok();
                anyhow::bail!("Corrupted file detected and removed")
            }
        }
        Err(err) => // existing error handling
    }
}
```

## Proof of Concept

```rust
// Reproduction steps:
// 1. Set up local file store operator with limited disk space
// 2. Fill filesystem to near capacity
// 3. Trigger transaction batch upload

use std::fs;
use std::io::Write;

#[tokio::test]
async fn test_partial_write_vulnerability() {
    // Create a small filesystem (in real scenario, this would be actual disk)
    let temp_dir = tempfile::tempdir().unwrap();
    let file_path = temp_dir.path().join("test_transaction.bin");
    
    // Simulate large data write
    let large_data = vec![0u8; 1024 * 1024]; // 1MB
    
    // Simulate filesystem full by writing to actual file
    // In production, this happens when tokio::fs::write() encounters ENOSPC
    let mut file = fs::File::create(&file_path).unwrap();
    
    // Write partial data then stop (simulates ENOSPC error)
    file.write_all(&large_data[..512]).unwrap();
    drop(file);
    
    // Now try to read and deserialize this partial file
    let corrupted_data = tokio::fs::read(&file_path).await.unwrap();
    
    // This will panic when trying to deserialize
    let file_entry = FileEntry::new(corrupted_data, StorageFormat::Lz4CompressedProto);
    
    // This call will panic with "Lz4 decompression failed" or "proto deserialization failed"
    // In production, this crash loop continues until manual intervention
    let result = std::panic::catch_unwind(|| {
        file_entry.into_transactions_in_storage()
    });
    
    assert!(result.is_err(), "Should panic on corrupted data");
}
```

## Notes

This vulnerability is specific to the `LocalFileStoreOperator` implementation. The GCS-based implementation may have different behavior depending on Google Cloud Storage's write consistency guarantees. However, the lack of integrity validation and panic-based error handling affects both implementations when reading corrupted data from any source.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/local.rs (L58-73)
```rust
    async fn get_raw_file(&self, version: u64) -> anyhow::Result<Vec<u8>> {
        let file_entry_key = FileEntry::build_key(version, self.storage_format).to_string();
        let file_path = self.path.join(file_entry_key);
        match tokio::fs::read(file_path).await {
            Ok(file) => Ok(file),
            Err(err) => {
                if err.kind() == std::io::ErrorKind::NotFound {
                    anyhow::bail!("[Indexer File] Transactions file not found. Gap might happen between cache and file store. {}", err)
                } else {
                    anyhow::bail!(
                        "[Indexer File] Error happens when transaction file. {}",
                        err
                    );
                }
            },
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/local.rs (L185-188)
```rust
                match tokio::fs::write(txns_path, file_entry.into_inner()).await {
                    Ok(_) => Ok(()),
                    Err(err) => Err(anyhow::Error::from(err)),
                }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/compression_util.rs (L262-292)
```rust
    pub fn into_transactions_in_storage(self) -> TransactionsInStorage {
        match self {
            FileEntry::Lz4CompressionProto(bytes) => {
                let mut decompressor = Decoder::new(&bytes[..]).expect("Lz4 decompression failed.");
                let mut decompressed = Vec::new();
                decompressor
                    .read_to_end(&mut decompressed)
                    .expect("Lz4 decompression failed.");
                TransactionsInStorage::decode(decompressed.as_slice())
                    .expect("proto deserialization failed.")
            },
            FileEntry::JsonBase64UncompressedProto(bytes) => {
                let file: TransactionsLegacyFile =
                    serde_json::from_slice(bytes.as_slice()).expect("json deserialization failed.");
                let transactions = file
                    .transactions_in_base64
                    .into_iter()
                    .map(|base64| {
                        let bytes: Vec<u8> =
                            base64::decode(base64).expect("base64 decoding failed.");
                        Transaction::decode(bytes.as_slice())
                            .expect("proto deserialization failed.")
                    })
                    .collect::<Vec<Transaction>>();
                TransactionsInStorage {
                    starting_version: Some(file.starting_version),
                    transactions,
                }
            },
        }
    }
```
