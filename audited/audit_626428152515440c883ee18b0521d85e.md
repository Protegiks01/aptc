# Audit Report

## Title
DKG Transcript Deserialization Memory Exhaustion DoS via Unbounded Nested Vector Length Fields

## Summary
The Distributed Key Generation (DKG) transcript deserialization process in Aptos Core does not enforce bounds checks on nested vector length fields. As a result, an attacker can construct malicious DKG transcript messages with manipulated size claims that can cause excessive memory allocation and potential out-of-memory (OOM) crashes on validator nodes, without any authentication or cryptographic verification occurring first.

## Finding Description
The vulnerability is present in the deserialization code path for `Subtranscript<E>` (containing deeply nested `Vec` structures) which is used in DKG cryptographic transcript aggregation and in Move VM on-chain validation. The deserialization is performed with `bcs::from_bytes()` with no container depth or element count limit, directly into structures like `Vec<Vec<Vec<E::G1>>>` for the `Cs` field. An attacker can send a minimal-sized DKGTranscript that encodes extremely large claimed vector lengths. Since the nested length fields are trusted by the deserializer, this leads to potential pre-allocation or iteration over huge numbers of elliptic curve points, far exceeding operational memory limitsâ€”even when the total message size is small and within currently permitted network message size limits. Critically, this occurs before any cryptographic or protocol-level verification steps.

Both the DKG peer transcript aggregation path and the Move VM DKG validator path are affected, as both call `bcs::from_bytes()` without length or recursion limit when decoding the transcript bytes field.

## Impact Explanation
This bug meets the criteria for **High Severity** as defined by the Aptos bug bounty, since it allows unauthenticated network DoS on validator nodes during DKG session epochs. The issue enables:
- Validator node resource exhaustion (memory and/or CPU)
- Node crashes from OOM
- Potential loss of network liveness if exploited against all validators
- Exploitability pre-authentication and pre-verification, by any network peer

The issue falls short of Critical since there is no fund loss, persistent state corruption, or consensus split, but the availability impact is severe and recovery is not automatic.

## Likelihood Explanation
**Likelihood: High**  
- Any network peer can construct and send an exploit message.  
- Exploit can be triggered by simply sending a BCS-encoded message with deeply nested, large length-prefixed vectors.  
- The current limit of 64 MiB on incoming messages is sufficient to encode length fields for vectors claiming billions of elements.  
- Exploit succeeds before authentication or cryptographic verification (all such steps occur after the deserialization).  
- Possible during every epoch DKG session (though not continuously).

## Recommendation
Mitigate by enforcing tight container length and nesting depth limits during transcript deserialization. Specifically:
- Replace all uses of `bcs::from_bytes()` for DKG transcript deserialization with `bcs::from_bytes_with_limit()` using a safe maximum (e.g., the same recursion/nesting depth as consensus and mempool, typically `RECURSION_LIMIT = 64`).
- Where possible, add element count or byte-size caps before allocating nested vectors, or pre-allocate only up to safe thresholds.

## Proof of Concept
A proof-of-concept Rust test:  
- Compose a BCS-encoded byte vector representing a `Subtranscript` with the outer, middle, and inner `Vec` length fields set to a very high value (e.g., 1000 each), and minimal valid curve element data.
- Submit this as a DKG transcript to a validator RPC endpoint (or to the VM via a DKG transaction).
- Observe that the victim node attempts to allocate or iterate over vast numbers of elements, causing resource exhaustion or crash prior to entering any cryptographic checks.

---

### Citations [1](#0-0) [2](#0-1) [3](#0-2) 

---

## Notes

- No evidence of a container or recursion limit on transcript deserialization for the critical `bcs::from_bytes()` calls, unlike the rest of network deserialization which uses `from_bytes_with_limit`.
- The network message size limit does not prevent the attack, because length fields in BCS allow a small message (many length prefixes, little data) to claim a massive nested structure.
- Both DKG peer transcript aggregation (off-chain) and on-chain Move VM validation are impacted pre-authentication and pre-verification.
- The vulnerability is strictly about **validator node availability and resource exhaustion** from unauthenticated attacks during DKG epochs. Fund safety, consensus safety, and state correctness are not compromised, so the highest valid severity is High.

If further validation or an explicit PoC in the actual deployed environment is required, it is recommended to add a test with an oversized transcript to confirm this behavior in production.

### Citations

**File:** crates/aptos-dkg/src/pvss/chunky/weighted_transcriptv2.rs (L73-86)
```rust
pub struct Subtranscript<E: Pairing> {
    // The dealt public key
    #[serde(deserialize_with = "ark_de")]
    pub V0: E::G2,
    // The dealt public key shares
    #[serde(deserialize_with = "ark_de")]
    pub Vs: Vec<Vec<E::G2>>,
    /// First chunked ElGamal component: C[i][j] = s_{i,j} * G + r_j * ek_i. Here s_i = \sum_j s_{i,j} * B^j // TODO: change notation because B is not a group element?
    #[serde(deserialize_with = "ark_de")]
    pub Cs: Vec<Vec<Vec<E::G1>>>, // TODO: maybe make this and the other fields affine? The verifier will have to do it anyway... and we are trying to speed that up
    /// Second chunked ElGamal component: R[j] = r_j * H
    #[serde(deserialize_with = "ark_de")]
    pub Rs: Vec<Vec<E::G1>>,
}
```

**File:** dkg/src/transcript_aggregation/mod.rs (L70-110)
```rust
        let DKGTranscript {
            metadata,
            transcript_bytes,
        } = dkg_transcript;
        ensure!(
            metadata.epoch == self.epoch_state.epoch,
            "[DKG] adding peer transcript failed with invalid node epoch",
        );

        let peer_power = self.epoch_state.verifier.get_voting_power(&sender);
        ensure!(
            peer_power.is_some(),
            "[DKG] adding peer transcript failed with illegal dealer"
        );
        ensure!(
            metadata.author == sender,
            "[DKG] adding peer transcript failed with node author mismatch"
        );
        let transcript = bcs::from_bytes(transcript_bytes.as_slice()).map_err(|e| {
            anyhow!("[DKG] adding peer transcript failed with trx deserialization error: {e}")
        })?;
        let mut trx_aggregator = self.trx_aggregator.lock();
        if trx_aggregator.contributors.contains(&metadata.author) {
            return Ok(None);
        }

        S::verify_transcript_extra(&transcript, &self.epoch_state.verifier, false, Some(sender))
            .context("extra verification failed")?;

        S::verify_transcript(&self.dkg_pub_params, &transcript).map_err(|e| {
            anyhow!("[DKG] adding peer transcript failed with trx verification failure: {e}")
        })?;

        // All checks passed. Aggregating.
        let is_self = self.my_addr == sender;
        if !is_self && !self.valid_peer_transcript_seen {
            let secs_since_dkg_start =
                duration_since_epoch().as_secs_f64() - self.start_time.as_secs_f64();
            DKG_STAGE_SECONDS
                .with_label_values(&[
                    self.my_addr.to_hex().as_str(),
```

**File:** aptos-move/aptos-vm/src/validator_txns/dkg.rs (L104-113)
```rust
        // Deserialize transcript and verify it.
        let pub_params = DefaultDKG::new_public_params(&in_progress_session_state.metadata);
        let transcript = bcs::from_bytes::<<DefaultDKG as DKGTrait>::Transcript>(
            dkg_node.transcript_bytes.as_slice(),
        )
        .map_err(|_| Expected(TranscriptDeserializationFailed))?;

        DefaultDKG::verify_transcript(&pub_params, &transcript)
            .map_err(|_| Expected(TranscriptVerificationFailed))?;

```
