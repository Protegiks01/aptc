# Audit Report

## Title
Unbounded Database Growth Due to Missing V2 Batch Deletion in QuorumStore

## Summary
The QuorumStore's batch expiration mechanism fails to delete expired V2 batches from RocksDB during normal operation, causing unbounded database growth that can lead to disk exhaustion and validator node failures. While V1 batches are correctly deleted when they expire, V2 batches accumulate indefinitely until node restart.

## Finding Description

The QuorumStore consensus component maintains transaction batches in RocksDB using two separate column families: "batch" for V1 batches and "batch_v2" for V2 batches. [1](#0-0) 

When batches are persisted to the database, the code correctly distinguishes between V1 and V2 batches and saves them to the appropriate schema: [2](#0-1) 

However, during normal operation when batches expire via timestamp updates, the `update_certified_timestamp` method only deletes V1 batches from the database: [3](#0-2) 

The critical bug is on line 536, which only calls `self.db.delete_batches(expired_keys)` - the V1 deletion method. There is no corresponding call to `self.db.delete_batches_v2(expired_keys)` for V2 batches.

The `clear_expired_payload` method removes expired batches from the in-memory cache and returns a list of digest hashes without tracking whether each batch is V1 or V2: [4](#0-3) 

Additionally, there is a second bug in the epoch transition garbage collection. The `gc_previous_epoch_batches_from_db_v2` function reads V2 batches but incorrectly attempts to delete them using the V1 deletion method: [5](#0-4) 

Line 241 calls `db.delete_batches(expired_keys)` when it should call `db.delete_batches_v2(expired_keys)`.

V2 batches are only correctly deleted during node restarts in `populate_cache_and_gc_expired_batches_v2`: [6](#0-5) 

Line 333 correctly calls `db.delete_batches_v2(expired_keys)`.

**Feature Status:** V2 batches are currently disabled by default but exist as a configuration option: [7](#0-6) 

When enabled, this bug will cause unbounded disk growth as expired V2 batches accumulate without being deleted during normal operation or epoch transitions.

## Impact Explanation

**Severity: Medium** (up to $10,000 per Aptos bug bounty)

This vulnerability causes:
1. **Unbounded disk space consumption**: V2 batches accumulate continuously during normal operation, potentially filling disk space within hours to days depending on batch creation rate
2. **Validator node failures**: When disk space is exhausted, nodes crash or become unresponsive
3. **Network availability impact**: If multiple validators are affected simultaneously, network liveness degrades
4. **Operational disruption**: Requires manual intervention (node restart) to trigger cleanup, as epoch transitions have the same bug

This qualifies as Medium severity under "State inconsistencies requiring intervention" per the Aptos bug bounty program. While it doesn't directly cause loss of funds or consensus violations, it creates operational instability requiring manual remediation and can impact network availability.

## Likelihood Explanation

**Likelihood: High** (when V2 batches are enabled)

This vulnerability:
- Occurs automatically during normal consensus operation without any attacker action
- Affects all validator nodes running with V2 batch support enabled
- Manifests continuously as batches expire (typically within hours)
- Has no mitigation during normal operation or epoch transitions
- Is deterministic and reproducible in any environment with V2 batches enabled

The bug will definitely manifest in production if V2 batches are enabled via configuration. Currently, V2 batches are disabled by default, but the feature exists in production code with full infrastructure support, making this a valid vulnerability in an optional feature.

## Recommendation

Fix both bugs by ensuring V2 batches are deleted alongside V1 batches:

**Fix 1: Update `update_certified_timestamp` to delete both V1 and V2 batches:**
```rust
pub fn update_certified_timestamp(&self, certified_time: u64) {
    trace!("QS: batch reader updating time {:?}", certified_time);
    self.last_certified_time
        .fetch_max(certified_time, Ordering::SeqCst);

    let expired_keys = self.clear_expired_payload(certified_time);
    if let Err(e) = self.db.delete_batches(expired_keys.clone()) {
        debug!("Error deleting V1 batches: {:?}", e)
    }
    if let Err(e) = self.db.delete_batches_v2(expired_keys) {
        debug!("Error deleting V2 batches: {:?}", e)
    }
}
```

**Fix 2: Update `gc_previous_epoch_batches_from_db_v2` to use correct deletion method:**
```rust
fn gc_previous_epoch_batches_from_db_v2(db: Arc<dyn QuorumStoreStorage>, current_epoch: u64) {
    // ... existing code ...
    db.delete_batches_v2(expired_keys)  // Changed from delete_batches
        .expect("Deletion of expired keys should not fail");
}
```

## Proof of Concept

The vulnerability can be demonstrated by:
1. Enabling `enable_batch_v2: true` in QuorumStoreConfig
2. Running a validator node and observing V2 batch creation
3. Monitoring the RocksDB `batch_v2` column family size growth over time
4. Observing that expired V2 batches are never deleted during normal operation
5. Verifying cleanup only occurs after node restart

A complete PoC would require setting up a test validator with V2 batches enabled and monitoring disk usage over an extended period, which is beyond the scope of this static analysis but can be readily reproduced in a test environment.

### Citations

**File:** consensus/src/quorum_store/schema.rs (L14-16)
```rust
pub(crate) const BATCH_CF_NAME: ColumnFamilyName = "batch";
pub(crate) const BATCH_ID_CF_NAME: ColumnFamilyName = "batch_ID";
pub(crate) const BATCH_V2_CF_NAME: ColumnFamilyName = "batch_v2";
```

**File:** consensus/src/quorum_store/batch_store.rs (L212-243)
```rust
    fn gc_previous_epoch_batches_from_db_v2(db: Arc<dyn QuorumStoreStorage>, current_epoch: u64) {
        let db_content = db
            .get_all_batches_v2()
            .expect("failed to read data from db");
        info!(
            epoch = current_epoch,
            "QS: Read batches from storage. Len: {}",
            db_content.len(),
        );

        let mut expired_keys = Vec::new();
        for (digest, value) in db_content {
            let epoch = value.epoch();

            trace!(
                "QS: Batchreader recovery content epoch {:?}, digest {}",
                epoch,
                digest
            );

            if epoch < current_epoch {
                expired_keys.push(digest);
            }
        }

        info!(
            "QS: Batch store bootstrap expired keys len {}",
            expired_keys.len()
        );
        db.delete_batches(expired_keys)
            .expect("Deletion of expired keys should not fail");
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L292-336)
```rust
    fn populate_cache_and_gc_expired_batches_v2(
        db: Arc<dyn QuorumStoreStorage>,
        current_epoch: u64,
        last_certified_time: u64,
        expiration_buffer_usecs: u64,
        batch_store: &BatchStore,
    ) {
        let db_content = db
            .get_all_batches_v2()
            .expect("failed to read v1 data from db");
        info!(
            epoch = current_epoch,
            "QS: Read v1 batches from storage. Len: {}, Last Cerified Time: {}",
            db_content.len(),
            last_certified_time
        );

        let mut expired_keys = Vec::new();
        let gc_timestamp = last_certified_time.saturating_sub(expiration_buffer_usecs);
        for (digest, value) in db_content {
            let expiration = value.expiration();
            trace!(
                "QS: Batchreader recovery content exp {:?}, digest {}",
                expiration,
                digest
            );

            if expiration < gc_timestamp {
                expired_keys.push(digest);
            } else {
                batch_store
                    .insert_to_cache(&value)
                    .expect("Storage limit exceeded upon BatchReader construction");
            }
        }

        info!(
            "QS: Batch store bootstrap expired keys len {}",
            expired_keys.len()
        );
        tokio::task::spawn_blocking(move || {
            db.delete_batches_v2(expired_keys)
                .expect("Deletion of expired keys should not fail");
        });
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L443-472)
```rust
    pub(crate) fn clear_expired_payload(&self, certified_time: u64) -> Vec<HashValue> {
        // To help slow nodes catch up via execution without going to state sync we keep the blocks for 60 extra seconds
        // after the expiration time. This will help remote peers fetch batches that just expired but are within their
        // execution window.
        let expiration_time = certified_time.saturating_sub(self.expiration_buffer_usecs);
        let expired_digests = self.expirations.lock().expire(expiration_time);
        let mut ret = Vec::new();
        for h in expired_digests {
            let removed_value = match self.db_cache.entry(h) {
                Occupied(entry) => {
                    // We need to check up-to-date expiration again because receiving the same
                    // digest with a higher expiration would update the persisted value and
                    // effectively extend the expiration.
                    if entry.get().expiration() <= expiration_time {
                        self.persist_subscribers.remove(entry.get().digest());
                        Some(entry.remove())
                    } else {
                        None
                    }
                },
                Vacant(_) => unreachable!("Expired entry not in cache"),
            };
            // No longer holding the lock on db_cache entry.
            if let Some(value) = removed_value {
                self.free_quota(value);
                ret.push(h);
            }
        }
        ret
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L501-513)
```rust
                    if !batch_info.is_v2() {
                        let persist_request =
                            persist_request.try_into().expect("Must be a V1 batch");
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch(persist_request)
                            .expect("Could not write to DB");
                    } else {
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch_v2(persist_request)
                            .expect("Could not write to DB")
                    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L530-539)
```rust
    pub fn update_certified_timestamp(&self, certified_time: u64) {
        trace!("QS: batch reader updating time {:?}", certified_time);
        self.last_certified_time
            .fetch_max(certified_time, Ordering::SeqCst);

        let expired_keys = self.clear_expired_payload(certified_time);
        if let Err(e) = self.db.delete_batches(expired_keys) {
            debug!("Error deleting batches: {:?}", e)
        }
    }
```

**File:** config/src/config/quorum_store_config.rs (L102-144)
```rust
    pub enable_batch_v2: bool,
}

impl Default for QuorumStoreConfig {
    fn default() -> QuorumStoreConfig {
        QuorumStoreConfig {
            channel_size: 1000,
            proof_timeout_ms: 10000,
            batch_generation_poll_interval_ms: 25,
            batch_generation_min_non_empty_interval_ms: 50,
            batch_generation_max_interval_ms: 250,
            sender_max_batch_txns: DEFEAULT_MAX_BATCH_TXNS,
            // TODO: on next release, remove BATCH_PADDING_BYTES
            sender_max_batch_bytes: 1024 * 1024 - BATCH_PADDING_BYTES,
            sender_max_num_batches: DEFAULT_MAX_NUM_BATCHES,
            sender_max_total_txns: 1500,
            // TODO: on next release, remove DEFAULT_MAX_NUM_BATCHES * BATCH_PADDING_BYTES
            sender_max_total_bytes: 4 * 1024 * 1024 - DEFAULT_MAX_NUM_BATCHES * BATCH_PADDING_BYTES,
            receiver_max_batch_txns: 100,
            receiver_max_batch_bytes: 1024 * 1024 + BATCH_PADDING_BYTES,
            receiver_max_num_batches: 20,
            receiver_max_total_txns: 2000,
            receiver_max_total_bytes: 4 * 1024 * 1024
                + DEFAULT_MAX_NUM_BATCHES
                + BATCH_PADDING_BYTES,
            batch_request_num_peers: 5,
            batch_request_retry_limit: 10,
            batch_request_retry_interval_ms: 500,
            batch_request_rpc_timeout_ms: 5000,
            batch_expiry_gap_when_init_usecs: Duration::from_secs(60).as_micros() as u64,
            remote_batch_expiry_gap_when_init_usecs: Duration::from_millis(500).as_micros() as u64,
            memory_quota: 120_000_000,
            db_quota: 300_000_000,
            batch_quota: 300_000,
            back_pressure: QuorumStoreBackPressureConfig::default(),
            // number of batch coordinators to handle QS batch messages, should be >= 1
            num_workers_for_remote_batches: 10,
            batch_buckets: DEFAULT_BUCKETS.to_vec(),
            allow_batches_without_pos_in_proposal: true,
            enable_opt_quorum_store: true,
            opt_qs_minimum_batch_age_usecs: Duration::from_millis(50).as_micros() as u64,
            enable_payload_v2: false,
            enable_batch_v2: false,
```
