# Audit Report

## Title
Node Initialization Failure Due to Inconsistent Pruner Progress Metadata After Backup Restore

## Summary
The `TransactionPruner::new()` function fails catastrophically when `metadata_progress` (from `LedgerPrunerProgress`) is less than the stored `TransactionPrunerProgress`, causing complete node initialization failure and loss of liveness. This can occur after partial backup restores or database manipulations where pruner progress metadata becomes inconsistent.

## Finding Description

The vulnerability exists in the initialization logic of `TransactionPruner` and all similar sub-pruners in the ledger pruning system. [1](#0-0) 

During initialization, the code retrieves two version values:
1. `metadata_progress` - from `LedgerPrunerProgress` (the authoritative ledger metadata pruner progress)
2. `progress` - from `TransactionPrunerProgress` (this sub-pruner's stored progress) [2](#0-1) 

If the stored `TransactionPrunerProgress` exists and is **greater** than `metadata_progress`, the code attempts to call `prune(progress, metadata_progress)` where `progress > metadata_progress`. This violates the invariant that start ≤ end in the pruning range. [3](#0-2) 

The `get_pruning_candidate_transactions()` function explicitly validates this invariant and **fails hard** if violated, preventing node initialization from completing.

**How This Can Occur:**

1. **Partial Backup Restore**: If a database is restored from backups where different metadata keys were captured at different points in time, `LedgerPrunerProgress` might reflect an earlier version while `TransactionPrunerProgress` reflects a later version.

2. **Manual Database Manipulation**: If an operator manually modifies `LedgerPrunerProgress` to an earlier version (e.g., to revert pruning state) without adjusting all sub-pruner progress keys.

3. **Restore Without Proper Metadata Sync**: If transaction data is restored but pruner progress metadata is not properly synchronized through `save_min_readable_version()`. [4](#0-3) 

The `LedgerPruner` initializes the `LedgerMetadataPruner` first, retrieves its progress, then passes this as `metadata_progress` to all sub-pruners. If any sub-pruner has a stored progress greater than this, initialization fails.

## Impact Explanation

**Severity: Medium** (per Aptos bug bounty criteria: "State inconsistencies requiring intervention")

**Impact:**
- **Complete node initialization failure** - The node cannot start up and join the network
- **Liveness violation** - The affected node is completely unavailable
- **Requires manual intervention** - Operators must manually fix the database metadata or restore from a consistent backup
- **Potential multi-node impact** - If multiple nodes restore from the same inconsistent backup, multiple nodes fail simultaneously

This breaks the **liveness invariant** and can cause **partial network unavailability** if multiple validators are affected. While it doesn't directly cause consensus safety violations or fund loss, it represents a critical operational failure that requires manual database intervention to recover.

## Likelihood Explanation

**Likelihood: Medium**

This issue can realistically occur in several scenarios:

1. **Disaster Recovery**: During emergency restores from backup after data center failures, operators may inadvertently use backups from slightly different time points for different database components.

2. **Database Migration**: When migrating nodes to new infrastructure, partial restores or selective data copying can create inconsistent metadata states.

3. **Manual Operations**: Operators troubleshooting pruning issues might manually modify metadata keys without understanding the synchronization requirements.

4. **Backup Tool Bugs**: Bugs in backup/restore tooling that don't properly capture or restore all metadata keys atomically.

The restore code path attempts to prevent this via `save_min_readable_version()` calls: [5](#0-4) 

However, this only occurs in specific code paths like `finalize_state_snapshot()`, and not all restore scenarios may properly invoke this synchronization.

## Recommendation

**Fix: Add defensive validation in sub-pruner initialization to handle backward progress gracefully**

The code should detect when `metadata_progress < stored_progress` and handle it as a recoverable condition rather than a fatal error:

```rust
pub(in crate::pruner) fn new(
    transaction_store: Arc<TransactionStore>,
    ledger_db: Arc<LedgerDb>,
    metadata_progress: Version,
    internal_indexer_db: Option<InternalIndexerDB>,
) -> Result<Self> {
    let progress = get_or_initialize_subpruner_progress(
        ledger_db.transaction_db_raw(),
        &DbMetadataKey::TransactionPrunerProgress,
        metadata_progress,
    )?;

    let myself = TransactionPruner {
        transaction_store,
        ledger_db,
        internal_indexer_db,
    };

    // Defensive check: if stored progress is ahead of metadata progress,
    // reset it to metadata progress instead of attempting invalid backward pruning
    if progress > metadata_progress {
        warn!(
            progress = progress,
            metadata_progress = metadata_progress,
            "TransactionPruner progress ahead of metadata progress. Resetting to metadata progress."
        );
        ledger_db.transaction_db().write_pruner_progress(metadata_progress)?;
        // No catch-up pruning needed
    } else if progress < metadata_progress {
        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up TransactionPruner."
        );
        myself.prune(progress, metadata_progress)?;
    }

    Ok(myself)
}
```

**Additional Hardening:**
1. Add validation in `get_or_initialize_subpruner_progress()` to ensure the stored progress never exceeds `metadata_progress`
2. Enhance backup/restore tooling to atomically capture and restore all pruner progress metadata
3. Add pre-initialization validation that checks all sub-pruner progress values are ≤ `LedgerPrunerProgress`

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use aptos_temppath::TempPath;
    
    #[test]
    fn test_backward_progress_initialization_failure() {
        // Setup: Create a database with TransactionPrunerProgress ahead of LedgerPrunerProgress
        let tmpdir = TempPath::new();
        let db = DB::open(
            tmpdir.path(),
            "test_db",
            &[
                DbMetadataSchema::column_family_name(),
                TransactionSchema::column_family_name(),
            ],
        )
        .unwrap();
        
        // Simulate inconsistent state after backup restore:
        // LedgerPrunerProgress = 1000 (from older backup)
        db.put::<DbMetadataSchema>(
            &DbMetadataKey::LedgerPrunerProgress,
            &DbMetadataValue::Version(1000),
        ).unwrap();
        
        // TransactionPrunerProgress = 1500 (from newer backup or not reset)
        db.put::<DbMetadataSchema>(
            &DbMetadataKey::TransactionPrunerProgress,
            &DbMetadataValue::Version(1500),
        ).unwrap();
        
        // Attempt to initialize TransactionPruner with metadata_progress=1000
        let ledger_db = Arc::new(LedgerDb::new_for_test(db));
        let transaction_store = Arc::new(TransactionStore::new(ledger_db.clone()));
        
        // This should fail with an error from get_pruning_candidate_transactions
        // because it tries to call prune(1500, 1000) where start > end
        let result = TransactionPruner::new(
            transaction_store,
            ledger_db,
            1000, // metadata_progress
            None,
        );
        
        // Node initialization fails here
        assert!(result.is_err());
        assert!(result.unwrap_err().to_string().contains("must be >="));
    }
}
```

**Notes**

This vulnerability affects all sub-pruners in the ledger pruning system that follow the same initialization pattern:
- `EventStorePruner` [6](#0-5) 
- `TransactionInfoPruner` [7](#0-6) 
- `TransactionAccumulatorPruner`, `WriteSetPruner`, `PersistedAuxiliaryInfoPruner`, and others

The issue is systemic across the entire pruner architecture. While normal operations maintain consistency through synchronized pruning, edge cases in backup/restore workflows can create the inconsistent state that triggers this failure.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L78-104)
```rust
    pub(in crate::pruner) fn new(
        transaction_store: Arc<TransactionStore>,
        ledger_db: Arc<LedgerDb>,
        metadata_progress: Version,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.transaction_db_raw(),
            &DbMetadataKey::TransactionPrunerProgress,
            metadata_progress,
        )?;

        let myself = TransactionPruner {
            transaction_store,
            ledger_db,
            internal_indexer_db,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up TransactionPruner."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L106-131)
```rust
    fn get_pruning_candidate_transactions(
        &self,
        start: Version,
        end: Version,
    ) -> Result<Vec<(Version, Transaction)>> {
        ensure!(end >= start, "{} must be >= {}", end, start);

        let mut iter = self
            .ledger_db
            .transaction_db_raw()
            .iter::<TransactionSchema>()?;
        iter.seek(&start)?;

        // The capacity is capped by the max number of txns we prune in a single batch. It's a
        // relatively small number set in the config, so it won't cause high memory usage here.
        let mut txns = Vec::with_capacity((end - start) as usize);
        for item in iter {
            let (version, txn) = item?;
            if version >= end {
                break;
            }
            txns.push((version, txn));
        }

        Ok(txns)
    }
```

**File:** storage/aptosdb/src/pruner/pruner_utils.rs (L44-60)
```rust
pub(crate) fn get_or_initialize_subpruner_progress(
    sub_db: &DB,
    progress_key: &DbMetadataKey,
    metadata_progress: Version,
) -> Result<Version> {
    Ok(
        if let Some(v) = sub_db.get::<DbMetadataSchema>(progress_key)? {
            v.expect_version()
        } else {
            sub_db.put::<DbMetadataSchema>(
                progress_key,
                &DbMetadataValue::Version(metadata_progress),
            )?;
            metadata_progress
        },
    )
}
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L118-194)
```rust
    pub fn new(
        ledger_db: Arc<LedgerDb>,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Result<Self> {
        info!(name = LEDGER_PRUNER_NAME, "Initializing...");

        let ledger_metadata_pruner = Box::new(
            LedgerMetadataPruner::new(ledger_db.metadata_db_arc())
                .expect("Failed to initialize ledger_metadata_pruner."),
        );

        let metadata_progress = ledger_metadata_pruner.progress()?;

        info!(
            metadata_progress = metadata_progress,
            "Created ledger metadata pruner, start catching up all sub pruners."
        );

        let transaction_store = Arc::new(TransactionStore::new(Arc::clone(&ledger_db)));

        let event_store_pruner = Box::new(EventStorePruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
            internal_indexer_db.clone(),
        )?);
        let persisted_auxiliary_info_pruner = Box::new(PersistedAuxiliaryInfoPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);
        let transaction_accumulator_pruner = Box::new(TransactionAccumulatorPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);

        let transaction_auxiliary_data_pruner = Box::new(TransactionAuxiliaryDataPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);

        let transaction_info_pruner = Box::new(TransactionInfoPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);
        let transaction_pruner = Box::new(TransactionPruner::new(
            Arc::clone(&transaction_store),
            Arc::clone(&ledger_db),
            metadata_progress,
            internal_indexer_db,
        )?);
        let write_set_pruner = Box::new(WriteSetPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);

        let pruner = LedgerPruner {
            target_version: AtomicVersion::new(metadata_progress),
            progress: AtomicVersion::new(metadata_progress),
            ledger_metadata_pruner,
            sub_pruners: vec![
                event_store_pruner,
                persisted_auxiliary_info_pruner,
                transaction_accumulator_pruner,
                transaction_auxiliary_data_pruner,
                transaction_info_pruner,
                transaction_pruner,
                write_set_pruner,
            ],
        };

        info!(
            name = pruner.name(),
            progress = metadata_progress,
            "Initialized."
        );

        Ok(pruner)
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L225-234)
```rust
            self.ledger_pruner.save_min_readable_version(version)?;
            self.state_store
                .state_merkle_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .epoch_snapshot_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .state_kv_pruner
                .save_min_readable_version(version)?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L85-109)
```rust
    pub(in crate::pruner) fn new(
        ledger_db: Arc<LedgerDb>,
        metadata_progress: Version,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.event_db_raw(),
            &DbMetadataKey::EventPrunerProgress,
            metadata_progress,
        )?;

        let myself = EventStorePruner {
            ledger_db,
            internal_indexer_db,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up EventStorePruner."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_info_pruner.rs (L36-57)
```rust
impl TransactionInfoPruner {
    pub(in crate::pruner) fn new(
        ledger_db: Arc<LedgerDb>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.transaction_info_db_raw(),
            &DbMetadataKey::TransactionInfoPrunerProgress,
            metadata_progress,
        )?;

        let myself = TransactionInfoPruner { ledger_db };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up TransactionInfoPruner."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```
