# Audit Report

## Title
Stale Epoch Pending Node Causes Validator Liveness Failure During Epoch Transitions

## Summary
The `NodeSchema` stores a single pending node using an empty tuple `()` as the database key without epoch information. When a validator crashes and restarts in a new epoch, if the stale pending node's round coincidentally matches the expected round in the new epoch, the validator will broadcast a wrong-epoch node that gets rejected by all peers, causing temporary loss of validator participation until automatic recovery occurs.

## Finding Description

The DAG consensus implementation uses `NodeSchema` to persist a single "pending node" for crash recovery purposes. [1](#0-0) 

When a validator creates a node for broadcast, it saves it as pending before broadcasting: [2](#0-1) 

The pending node is retrieved during validator initialization: [3](#0-2) 

**The Critical Flaw**: The recovery logic only checks if the pending node's round matches the expected round, without validating the epoch: [4](#0-3) 

The `ConsensusDB` persists across epochs: [5](#0-4) 

Furthermore, `delete_pending_node()` is never called anywhere in the codebase (verified via grep), meaning stale pending nodes are never cleaned up.

**Attack Scenario:**
1. Validator crashes in epoch N with pending_node (epoch=N, round=1000)
2. Network progresses to epoch N+1
3. Epoch N+1 happens to start at round 999 (rounds are continuous across epochs, not reset)
4. Validator restarts in epoch N+1
5. Recovery condition passes: `1000 == 999 + 1` âœ“
6. Validator broadcasts stale node with epoch=N in epoch=N+1
7. All peers reject it due to epoch mismatch validation: [6](#0-5) 
8. Validator cannot collect votes and remains stuck until other validators' progress triggers round advancement

## Impact Explanation

This qualifies as **Medium Severity** per Aptos bug bounty criteria:
- **State inconsistencies requiring intervention**: The affected validator cannot participate in consensus until automatic recovery occurs through receiving certified nodes from other validators
- **Not Critical** because: No funds at risk, no permanent damage, no safety violation, and automatic recovery exists
- **Not High** because: Only affects individual validators temporarily, not the entire network

If multiple validators crash simultaneously during epoch transitions (e.g., coordinated network disruption), the compounding effect could temporarily degrade network performance, but Byzantine fault tolerance (2f+1 honest validators) ensures consensus continues.

## Likelihood Explanation

**Likelihood: Medium**

Required conditions:
1. Validator crash with pending node stored (Common during upgrades/restarts)
2. Epoch transition while validator is offline (Happens regularly every ~2 hours)
3. Stale round exactly matches `new_epoch_highest_round + 1` (Low probability but possible)

The third condition has low individual probability, but over many epoch transitions and validators, the vulnerability will manifest periodically. The impact is amplified during network-wide events (coordinated upgrades, infrastructure failures) where multiple validators restart simultaneously.

## Recommendation

Add epoch validation in the pending node recovery logic:

```rust
// In DagDriver::new(), modify lines 117-128:
if let Some(node) = pending_node.filter(|node| 
    node.epoch() == epoch_state.epoch &&  // ADD THIS CHECK
    node.round() == highest_strong_links_round + 1
) {
    // ... existing resume logic
} else {
    // Clean up stale pending node from different epoch
    if let Some(node) = pending_node {
        if node.epoch() != epoch_state.epoch {
            let _ = storage.delete_pending_node();
        }
    }
    // ... existing new round logic
}
```

Additionally, implement proper cleanup by calling `delete_pending_node()` when:
1. A node becomes certified
2. Epoch changes
3. A new pending node is saved (before overwriting)

## Proof of Concept

```rust
#[test]
fn test_stale_epoch_pending_node_liveness_failure() {
    // Setup: Create validator in epoch 1
    let epoch1_state = create_epoch_state(1, vec![validator_a, validator_b, validator_c, validator_d]);
    let storage = Arc::new(MockStorage::new());
    let dag = Arc::new(DagStore::new(epoch1_state.clone(), storage.clone(), ...));
    
    // Validator A creates pending node for epoch 1, round 1000
    let pending_node = Node::new(
        1, // epoch
        1000, // round
        validator_a,
        timestamp,
        vec![],
        Payload::empty(...),
        strong_links_999,
        Extensions::empty(),
    );
    storage.save_pending_node(&pending_node).unwrap();
    
    // Simulate crash - validator A goes offline
    drop(dag);
    
    // Network progresses to epoch 2, starting at round 999
    let epoch2_state = create_epoch_state(2, vec![validator_a, validator_b, validator_c, validator_d]);
    let dag2 = Arc::new(DagStore::new(epoch2_state.clone(), storage.clone(), ...));
    dag2.write().add_certified_nodes_at_round(999, create_certified_nodes(...));
    
    // Validator A restarts in epoch 2
    let driver = DagDriver::new(
        validator_a,
        epoch2_state.clone(),
        dag2.clone(),
        ...,
        storage.clone(),
        ...
    );
    
    // BUG: Driver resumes broadcasting the epoch 1 node in epoch 2
    // Verify that:
    // 1. The stale node gets broadcast
    // 2. All peers reject it due to epoch mismatch
    // 3. Validator A cannot collect votes and make progress
    // 4. Eventually recovers when receiving certified nodes from others
    
    // Expected: Driver should detect epoch mismatch and enter new round instead
}
```

**Notes**

The vulnerability stems from incomplete crash recovery logic that fails to validate epoch consistency. While the design of using a single pending node slot is intentional and correct (each validator only proposes one node at a time), the lack of epoch validation during recovery creates a window for validators to broadcast stale nodes after epoch transitions. The issue is mitigated by automatic recovery when the validator receives certified nodes from active participants, but causes unnecessary consensus participation delays and potential network performance degradation during critical periods like coordinated upgrades.

### Citations

**File:** consensus/src/consensusdb/schema/dag/mod.rs (L22-22)
```rust
define_schema!(NodeSchema, (), Node, NODE_CF_NAME);
```

**File:** consensus/src/dag/dag_driver.rs (L90-92)
```rust
        let pending_node = storage
            .get_pending_node()
            .expect("should be able to read dag storage");
```

**File:** consensus/src/dag/dag_driver.rs (L117-128)
```rust
        if let Some(node) =
            pending_node.filter(|node| node.round() == highest_strong_links_round + 1)
        {
            debug!(
                LogSchema::new(LogEvent::NewRound).round(node.round()),
                "Resume round"
            );
            driver
                .round_state
                .set_current_round(node.round())
                .expect("must succeed");
            driver.broadcast_node(node);
```

**File:** consensus/src/dag/dag_driver.rs (L314-316)
```rust
        self.storage
            .save_pending_node(&new_node)
            .expect("node must be saved");
```

**File:** consensus/src/epoch_manager.rs (L1479-1484)
```rust
        let dag_storage = Arc::new(StorageAdapter::new(
            epoch,
            epoch_to_validators,
            self.storage.consensus_db(),
            self.storage.aptos_db(),
        ));
```

**File:** consensus/src/dag/rb_handler.rs (L113-118)
```rust
        ensure!(
            node.epoch() == self.epoch_state.epoch,
            "different epoch {}, current {}",
            node.epoch(),
            self.epoch_state.epoch
        );
```
