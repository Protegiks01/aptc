# Audit Report

## Title
Transaction Backup Range Proof Verification Bypass Allows Silent Backup Corruption

## Summary
The `write_chunk()` function in `storage/backup/backup-cli/src/backup_types/transaction/backup.rs` fetches transaction range proofs from the backup service but stores them without verifying they correspond to the correct version range. A compromised or malicious backup service can provide valid proofs for incorrect version ranges, creating corrupted backups that appear successful but fail during disaster recovery.

## Finding Description

The transaction backup process fails to validate that range proofs match the requested version range before persisting them to storage. This breaks the fundamental security guarantee that backups should be verifiable at creation time, not just at restore time.

**Vulnerable Code Flow:**

In the `write_chunk()` function [1](#0-0) , the code:

1. Requests a range proof from the backup service client for a specific version range
2. Copies the proof directly to storage without any verification  
3. Creates a `TransactionChunk` manifest claiming these are the correct versions

The proof fetching occurs at [2](#0-1)  with no validation.

**Attack Scenario:**

1. Attacker compromises or MITMs the backup service endpoint
2. Backup client requests transactions for versions 1000-2000 and their proof
3. Malicious service returns:
   - Transaction data for versions 5000-6000 (wrong range, but correct count of 1001 transactions)
   - A valid `TransactionAccumulatorRangeProof` for versions 5000-6000
   - A properly signed `LedgerInfoWithSignatures` for that range
4. Backup client counts the transactions [3](#0-2)  - sees 1001 transactions âœ“
5. Client stores data with manifest claiming versions 1000-2000 (based on its own tracking, not the actual data)
6. Backup completes successfully with **no indication of corruption**

**Why Current Validations Don't Catch This:**

The backup process only verifies transaction count matches the request, not that the actual version numbers or proofs are correct. During restore, the proof verification at [4](#0-3)  will fail because:

- The manifest claims versions 1000-2000  
- The proof verification tries to verify starting from position 1000
- But the proof siblings are for position 5000-6000
- The Merkle root computation fails, aborting the restore

The `AccumulatorRangeProof::verify()` implementation at [5](#0-4)  computes the Merkle root from a specific leaf index position, so using the wrong position causes verification failure.

## Impact Explanation

**Severity: HIGH**

This vulnerability enables **silent backup corruption** with the following impacts:

1. **Disaster Recovery Failure**: Operators believe they have valid backups, but all backups may be corrupted and unusable
2. **Availability Impact**: In a disaster scenario (hardware failure, data corruption, attack), node recovery becomes impossible 
3. **Delayed Detection**: The corruption is only discovered during restore attempts, when it's too late
4. **No Warning**: Backup operations complete successfully, providing false confidence

This qualifies as **High Severity** under the Aptos bug bounty program as a "Significant protocol violation" affecting backup/restore infrastructure integrity. While it doesn't directly compromise consensus or funds (the corrupted data is rejected during restore), it can render entire backup sets useless, creating a critical availability risk.

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability can be triggered when:

1. **Compromised Backup Service**: Malware or unauthorized access to the backup service node
2. **MITM Attack**: If backup service communication isn't properly authenticated/encrypted
3. **Misconfigured Backup Infrastructure**: Using untrusted or improperly secured backup endpoints
4. **Cloud/External Backup Services**: If operators outsource backup services to third parties

The likelihood increases because:
- Backup services may be exposed to networks for remote backup capabilities
- Long-running backup processes provide extended attack windows
- Operators may not immediately notice corrupted backups until attempting recovery
- No incremental verification occurs during multi-chunk backups

## Recommendation

**Immediate Fix: Verify Range Proofs During Backup**

Add proof verification in the `write_chunk()` function before persisting to storage:

```rust
async fn write_chunk(
    &self,
    backup_handle: &BackupHandleRef,
    chunk_bytes: &[u8],
    first_version: u64,
    last_version: u64,
) -> Result<TransactionChunk> {
    // Fetch the proof
    let mut proof_bytes = Vec::new();
    tokio::io::copy(
        &mut self
            .client
            .get_transaction_range_proof(first_version, last_version)
            .await?,
        &mut tokio::io::BufWriter::new(&mut proof_bytes),
    )
    .await?;
    
    // ADDED: Deserialize and verify the proof
    let (range_proof, ledger_info): (TransactionAccumulatorRangeProof, LedgerInfoWithSignatures) = 
        bcs::from_bytes(&proof_bytes)?;
    
    // ADDED: Parse transactions to get their infos for verification
    let mut txn_infos = Vec::new();
    let mut reader = std::io::Cursor::new(chunk_bytes);
    while reader.position() < chunk_bytes.len() as u64 {
        let len = reader.read_u32::<BigEndian>()?;
        let mut record_bytes = vec![0u8; len as usize];
        reader.read_exact(&mut record_bytes)?;
        let (_, _, txn_info, _, _): (Transaction, PersistedAuxiliaryInfo, TransactionInfo, Vec<ContractEvent>, WriteSet) = 
            bcs::from_bytes(&record_bytes)?;
        txn_infos.push(txn_info);
    }
    
    // ADDED: Verify the proof covers the correct range
    let txn_list_with_proof = TransactionListWithProof::new(
        vec![], // transactions not needed for proof verification
        None,   // events not needed
        Some(first_version),
        TransactionInfoListWithProof::new(range_proof.clone(), txn_infos),
    );
    txn_list_with_proof.verify(ledger_info.ledger_info(), Some(first_version))?;
    
    // Write verified proof to storage
    let (proof_handle, mut proof_file) = self
        .storage
        .create_for_write(
            backup_handle,
            &Self::chunk_proof_name(first_version, last_version),
        )
        .await?;
    proof_file.write_all(&proof_bytes).await?;
    proof_file.shutdown().await?;
    
    // ... rest of the function
}
```

**Additional Recommendations:**

1. Add cryptographic authentication between backup client and service (mutual TLS)
2. Implement periodic backup verification as part of operational procedures
3. Log proof verification results for audit trails
4. Consider adding checksums/signatures to manifest files

## Proof of Concept

**Setup: Mock Malicious Backup Service**

```rust
// Create a malicious backup service handler that returns wrong proofs
#[tokio::test]
async fn test_backup_proof_verification_bypass() {
    // 1. Setup: Create mock backup service that returns wrong version range
    let malicious_service = MockBackupService::new()
        .with_transaction_handler(|start, count| {
            // Return DIFFERENT versions than requested
            let wrong_start = start + 4000;
            get_transactions_from_db(wrong_start, count)
        })
        .with_proof_handler(|first, last| {
            // Return proof for DIFFERENT range matching the wrong transactions
            let wrong_first = first + 4000;
            let wrong_last = last + 4000;
            get_valid_proof_from_db(wrong_first, wrong_last)
        });
    
    // 2. Execute backup with malicious service
    let backup_controller = TransactionBackupController::new(
        TransactionBackupOpt {
            start_version: 1000,
            num_transactions: 1001,
        },
        global_opt,
        Arc::new(malicious_service.client()),
        storage,
    );
    
    // 3. Backup completes successfully (BUG!)
    let manifest_handle = backup_controller.run().await.unwrap();
    println!("Backup succeeded - but contains wrong data!");
    
    // 4. Attempt restore - this is where the corruption is detected
    let restore_controller = TransactionRestoreController::new(
        TransactionRestoreOpt {
            manifest_handle,
            replay_from_version: None,
            kv_only_replay: None,
        },
        restore_global_opt,
        storage,
        None,
        VerifyExecutionMode::NoVerify,
    );
    
    // 5. Restore FAILS because proof verification detects the mismatch
    let result = restore_controller.run().await;
    assert!(result.is_err());
    assert!(result.unwrap_err().to_string().contains("Root hashes do not match"));
    
    println!("Restore failed - backup was corrupted!");
    println!("Impact: Disaster recovery is IMPOSSIBLE with this backup");
}
```

**Notes**

The vulnerability exists because the backup system assumes the backup service is trusted and returns correct data. However, defense-in-depth principles dictate that external data sources should always be validated before persistence, especially for critical infrastructure like disaster recovery backups.

The current implementation defers all validation to restore time, creating a window where operators have false confidence in their backup integrity. This is particularly dangerous because backup verification is typically only performed during disaster scenarios when it's too late to create new backups.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/transaction/backup.rs (L107-114)
```rust
        assert!(!chunk_bytes.is_empty());
        let expected_next_version = self.start_version + self.num_transactions as u64;
        ensure!(
            current_ver == expected_next_version,
            "Server did not return all transactions requested. Expecting last version {}, got {}",
            expected_next_version,
            current_ver,
        );
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/backup.rs (L149-187)
```rust
    async fn write_chunk(
        &self,
        backup_handle: &BackupHandleRef,
        chunk_bytes: &[u8],
        first_version: u64,
        last_version: u64,
    ) -> Result<TransactionChunk> {
        let (proof_handle, mut proof_file) = self
            .storage
            .create_for_write(
                backup_handle,
                &Self::chunk_proof_name(first_version, last_version),
            )
            .await?;
        tokio::io::copy(
            &mut self
                .client
                .get_transaction_range_proof(first_version, last_version)
                .await?,
            &mut proof_file,
        )
        .await?;
        proof_file.shutdown().await?;

        let (chunk_handle, mut chunk_file) = self
            .storage
            .create_for_write(backup_handle, &Self::chunk_name(first_version))
            .await?;
        chunk_file.write_all(chunk_bytes).await?;
        chunk_file.shutdown().await?;

        Ok(TransactionChunk {
            first_version,
            last_version,
            transactions: chunk_handle,
            proof: proof_handle,
            format: TransactionChunkFormat::V1,
        })
    }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L167-167)
```rust
        txn_list_with_proof.verify(ledger_info.ledger_info(), Some(manifest.first_version))?;
```

**File:** types/src/proof/definition.rs (L616-714)
```rust
    /// Verifies the proof is correct. The verifier needs to have `expected_root_hash`, the index
    /// of the first leaf and all of the leaves in possession.
    pub fn verify(
        &self,
        expected_root_hash: HashValue,
        first_leaf_index: Option<u64>,
        leaf_hashes: &[HashValue],
    ) -> Result<()> {
        if first_leaf_index.is_none() {
            ensure!(
                leaf_hashes.is_empty(),
                "first_leaf_index indicated empty list while leaf_hashes is not empty.",
            );
            ensure!(
                self.left_siblings.is_empty() && self.right_siblings.is_empty(),
                "No siblings are needed.",
            );
            return Ok(());
        }

        ensure!(
            self.left_siblings.len() <= MAX_ACCUMULATOR_PROOF_DEPTH,
            "Proof has more than {} ({}) left siblings.",
            MAX_ACCUMULATOR_PROOF_DEPTH,
            self.left_siblings.len(),
        );
        ensure!(
            self.right_siblings.len() <= MAX_ACCUMULATOR_PROOF_DEPTH,
            "Proof has more than {} ({}) right siblings.",
            MAX_ACCUMULATOR_PROOF_DEPTH,
            self.right_siblings.len(),
        );
        ensure!(
            !leaf_hashes.is_empty(),
            "leaf_hashes is empty while first_leaf_index indicated non-empty list.",
        );

        let mut left_sibling_iter = self.left_siblings.iter().peekable();
        let mut right_sibling_iter = self.right_siblings.iter().peekable();

        let mut first_pos = Position::from_leaf_index(
            first_leaf_index.expect("first_leaf_index should not be None."),
        );
        let mut current_hashes = leaf_hashes.to_vec();
        let mut parent_hashes = vec![];

        // Keep reducing the list of hashes by combining all the children pairs, until there is
        // only one hash left.
        while current_hashes.len() > 1
            || left_sibling_iter.peek().is_some()
            || right_sibling_iter.peek().is_some()
        {
            let mut children_iter = current_hashes.iter();

            // If the first position on the current level is a right child, it needs to be combined
            // with a sibling on the left.
            if first_pos.is_right_child() {
                let left_hash = *left_sibling_iter.next().ok_or_else(|| {
                    format_err!("First child is a right child, but missing sibling on the left.")
                })?;
                let right_hash = *children_iter.next().expect("The first leaf must exist.");
                parent_hashes.push(MerkleTreeInternalNode::<H>::new(left_hash, right_hash).hash());
            }

            // Next we take two children at a time and compute their parents.
            let mut children_iter = children_iter.as_slice().chunks_exact(2);
            for chunk in children_iter.by_ref() {
                let left_hash = chunk[0];
                let right_hash = chunk[1];
                parent_hashes.push(MerkleTreeInternalNode::<H>::new(left_hash, right_hash).hash());
            }

            // Similarly, if the last position is a left child, it needs to be combined with a
            // sibling on the right.
            let remainder = children_iter.remainder();
            assert!(remainder.len() <= 1);
            if !remainder.is_empty() {
                let left_hash = remainder[0];
                let right_hash = *right_sibling_iter.next().ok_or_else(|| {
                    format_err!("Last child is a left child, but missing sibling on the right.")
                })?;
                parent_hashes.push(MerkleTreeInternalNode::<H>::new(left_hash, right_hash).hash());
            }

            first_pos = first_pos.parent();
            current_hashes.clear();
            std::mem::swap(&mut current_hashes, &mut parent_hashes);
        }

        ensure!(
            current_hashes[0] == expected_root_hash,
            "{}: Root hashes do not match. Actual root hash: {:x}. Expected root hash: {:x}.",
            type_name::<Self>(),
            current_hashes[0],
            expected_root_hash,
        );

        Ok(())
    }
```
