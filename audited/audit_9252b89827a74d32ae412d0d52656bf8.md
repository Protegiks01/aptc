# Audit Report

## Title
Silent Batch Persistence Failure Causes Consensus Liveness Degradation and Resource Exhaustion

## Summary
The `persist()` function in `batch_store.rs` returns an empty `Vec<SignedBatchInfo>` when all `persist_inner()` calls fail, but callers treat this identically to the case where no batches were provided. This ambiguous return value causes the system to proceed as if persistence succeeded, leading to unpersisted batches being broadcasted to the network and tracked in the proof manager, resulting in transaction delays, false back pressure, and potential liveness degradation.

## Finding Description

The vulnerability exists in the `persist()` function implementation and its callers: [1](#0-0) 

The function iterates through persist requests and only adds successfully persisted batches to the result vector. When all `persist_inner()` calls fail (returning `None`), an empty vector is returned with no error indication. [2](#0-1) 

The `persist_inner()` function returns `None` when `save()` fails (quota exceeded, expiration issues) or when signature generation fails, but these failures are only logged as debug messages.

**Critical Caller #1 - BatchGenerator:** [3](#0-2) 

The batch generator completely ignores the return value of `persist()` and broadcasts batches regardless of whether persistence succeeded. If all persist calls fail:
- Batches are still broadcasted to the network
- Transactions remain marked as "in progress" locally
- Other validators receive and persist these batches
- The original node cannot form a proof of store (batch not in local cache)
- Transactions are stuck until proof timeout

**Critical Caller #2 - BatchCoordinator:** [4](#0-3) 

The batch coordinator creates the `batches` variable from `persist_requests` **before** the persist call, then unconditionally sends it to the proof manager regardless of whether persistence succeeded. When persist fails:
- Batches with transaction summaries are added to proof queue
- These count toward remaining transaction metrics
- Affects back pressure calculations
- When building proposals, fetching these batches fails silently [5](#0-4) 

When the proof manager attempts to pull unpersisted batches for inline inclusion, `get_batch_from_local()` fails and the batch is skipped with only a warning, resulting in incomplete proposals.

**Root Cause:**
The empty vector return value is ambiguous - it cannot distinguish between:
1. All persist calls failed (error condition requiring corrective action)
2. No batches were provided as input (normal condition)

The system assumes case #2 and proceeds normally, violating the invariant that batches must be persisted before being used in consensus.

## Impact Explanation

**Severity: HIGH** (up to $50,000 per bug bounty criteria)

This vulnerability causes **significant protocol violations** and **validator node performance degradation**:

1. **Liveness Degradation**: Transactions get stuck in "in progress" state until timeout (proof_timeout_ms), delaying inclusion in blocks by potentially several seconds.

2. **Resource Exhaustion**: Network bandwidth is wasted broadcasting batches that cannot form proofs of store. Unpersisted batches accumulate in the proof manager, consuming memory.

3. **False Back Pressure**: The proof manager counts unpersisted batch transaction summaries in its remaining transaction metrics, potentially triggering back pressure that artificially throttles batch creation even though the "stuck" batches cannot be used. [6](#0-5) 

4. **State Inconsistency**: Nodes have different views of which batches are available - some nodes believe batches exist (in proof queue) while they're not actually persisted in batch store.

5. **Proposal Building Failures**: When creating inline blocks, attempts to fetch unpersisted batches fail silently, resulting in proposals with fewer transactions than expected.

This breaks the **State Consistency** invariant (all state transitions must be atomic and verifiable) and degrades the **Consensus Liveness** guarantee.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

This vulnerability triggers under realistic operational conditions:

1. **Quota Exhaustion**: The `QuotaManager` enforces memory, database, and batch quotas per peer. Under high load or during DoS attempts, these quotas can be exhausted, causing `save()` to fail. [7](#0-6) 

2. **Expiration Race Conditions**: Batches can expire between creation and persistence if `last_certified_time` advances rapidly. [8](#0-7) 

3. **Signature Generation Failures**: Although less likely, fail points or cryptographic issues could cause `generate_signed_batch_info()` to fail.

The vulnerability requires no attacker action - it occurs naturally under load or resource pressure.

## Recommendation

**Fix 1: Make persist() return a Result type**

```rust
pub trait BatchWriter: Send + Sync {
    fn persist(
        &self,
        persist_requests: Vec<PersistedValue<BatchInfoExt>>,
    ) -> Result<Vec<SignedBatchInfo<BatchInfoExt>>, BatchPersistError>;
}
```

Define an error type that distinguishes between partial and complete failures:

```rust
pub enum BatchPersistError {
    PartialFailure { 
        succeeded: Vec<SignedBatchInfo<BatchInfoExt>>,
        failed_count: usize 
    },
    CompleteFailure { reason: String },
}
```

**Fix 2: Update callers to handle errors**

In `batch_generator.rs`:
```rust
match self.batch_writer.persist(persist_requests) {
    Ok(signed_infos) if !signed_infos.is_empty() => {
        // Broadcast batches only if persist succeeded
        if self.config.enable_batch_v2 {
            network_sender.broadcast_batch_msg_v2(batches).await;
        }
    },
    Err(e) => {
        error!("Failed to persist batches: {:?}", e);
        // Release transactions back to mempool
        for batch in batches {
            self.remove_batch_in_progress(self.my_peer_id, batch.batch_id());
        }
    },
    _ => {}
}
```

In `batch_coordinator.rs`:
```rust
match batch_store.persist(persist_requests) {
    Ok(signed_infos) if !signed_infos.is_empty() => {
        network_sender.send_signed_batch_info_msg_v2(signed_infos, vec![peer_id]).await;
        // Only send to proof manager if persist succeeded
        let _ = sender_to_proof_manager
            .send(ProofManagerCommand::ReceiveBatches(batches))
            .await;
    },
    Err(e) => {
        error!("Failed to persist batches from {}: {:?}", peer_id, e);
        counters::BATCH_PERSIST_FAILED_COUNT.inc();
    },
    _ => {}
}
```

**Fix 3: Add monitoring**
Add metrics to track persist failures:
```rust
counters::BATCH_PERSIST_FAILED_COUNT.inc();
counters::BATCH_PERSIST_PARTIAL_FAILURE_COUNT.inc();
```

## Proof of Concept

```rust
// Reproduction scenario in Rust integration test
#[tokio::test]
async fn test_persist_failure_causes_liveness_degradation() {
    // Setup: Create batch store with minimal quotas to trigger failures
    let db = Arc::new(MockQuorumStoreDB::new());
    let batch_store = BatchStore::new(
        1, // epoch
        true, // is_new_epoch
        0, // last_certified_time
        db,
        100, // memory_quota (very small)
        200, // db_quota
        10, // batch_quota
        validator_signer,
        1000000,
    );
    
    // Step 1: Fill quotas by persisting batches until quota exhausted
    let mut batches = vec![];
    for i in 0..20 {
        let batch = create_test_batch(i, 50); // 50 txns each
        batches.push(batch);
    }
    
    // This should succeed for first few batches, then fail
    let signed_infos = batch_store.persist(
        batches.iter().map(|b| b.clone().into()).collect()
    );
    
    // Step 2: Verify that an empty result was returned
    // (Current implementation returns empty vec on failure)
    assert!(signed_infos.len() < batches.len(), 
            "Expected some batches to fail persistence");
    
    // Step 3: In batch_generator, batches would still be broadcasted
    // because the code ignores the return value
    
    // Step 4: Verify transactions are stuck in "in progress"
    // Cannot form proofs because batches not in local store
    
    // Step 5: Verify proof manager received unpersisted batches
    // and they count toward metrics but cannot be fetched
    
    // Expected: Transactions delayed until timeout
    // Expected: False back pressure triggered
    // Expected: Proposal building skips these batches with warnings
}
```

## Notes

This vulnerability demonstrates a classic anti-pattern where error conditions are silently converted to empty results, making it impossible for callers to distinguish success from failure. The fix requires both the callee to properly signal errors and callers to handle them appropriately. The impact on consensus liveness and resource utilization makes this a significant security concern requiring prompt remediation.

### Citations

**File:** consensus/src/quorum_store/batch_store.rs (L64-84)
```rust
    pub(crate) fn update_quota(&mut self, num_bytes: usize) -> anyhow::Result<StorageMode> {
        if self.batch_balance == 0 {
            counters::EXCEEDED_BATCH_QUOTA_COUNT.inc();
            bail!("Batch quota exceeded ");
        }

        if self.db_balance >= num_bytes {
            self.batch_balance -= 1;
            self.db_balance -= num_bytes;

            if self.memory_balance >= num_bytes {
                self.memory_balance -= num_bytes;
                Ok(StorageMode::MemoryAndPersisted)
            } else {
                Ok(StorageMode::PersistedOnly)
            }
        } else {
            counters::EXCEEDED_STORAGE_QUOTA_COUNT.inc();
            bail!("Storage quota exceeded ");
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L419-439)
```rust
    pub(crate) fn save(&self, value: &PersistedValue<BatchInfoExt>) -> anyhow::Result<bool> {
        let last_certified_time = self.last_certified_time();
        if value.expiration() > last_certified_time {
            fail_point!("quorum_store::save", |_| {
                // Skip caching and storing value to the db
                Ok(false)
            });
            counters::GAP_BETWEEN_BATCH_EXPIRATION_AND_CURRENT_TIME_WHEN_SAVE.observe(
                Duration::from_micros(value.expiration() - last_certified_time).as_secs_f64(),
            );

            return self.insert_to_cache(value);
        }
        counters::NUM_BATCH_EXPIRED_WHEN_SAVE.inc();
        bail!(
            "Incorrect expiration {} in epoch {}, last committed timestamp {}",
            value.expiration(),
            self.epoch(),
            last_certified_time,
        );
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L488-528)
```rust
    fn persist_inner(
        &self,
        batch_info: BatchInfoExt,
        persist_request: PersistedValue<BatchInfoExt>,
    ) -> Option<SignedBatchInfo<BatchInfoExt>> {
        assert!(
            &batch_info == persist_request.batch_info(),
            "Provided batch info doesn't match persist request batch info"
        );
        match self.save(&persist_request) {
            Ok(needs_db) => {
                trace!("QS: sign digest {}", persist_request.digest());
                if needs_db {
                    if !batch_info.is_v2() {
                        let persist_request =
                            persist_request.try_into().expect("Must be a V1 batch");
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch(persist_request)
                            .expect("Could not write to DB");
                    } else {
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch_v2(persist_request)
                            .expect("Could not write to DB")
                    }
                }
                if !batch_info.is_v2() {
                    self.generate_signed_batch_info(batch_info.info().clone())
                        .ok()
                        .map(|inner| inner.into())
                } else {
                    self.generate_signed_batch_info(batch_info).ok()
                }
            },
            Err(e) => {
                debug!("QS: failed to store to cache {:?}", e);
                None
            },
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L614-628)
```rust
    fn persist(
        &self,
        persist_requests: Vec<PersistedValue<BatchInfoExt>>,
    ) -> Vec<SignedBatchInfo<BatchInfoExt>> {
        let mut signed_infos = vec![];
        for persist_request in persist_requests.into_iter() {
            let batch_info = persist_request.batch_info().clone();
            if let Some(signed_info) = self.persist_inner(batch_info, persist_request.clone()) {
                self.notify_subscribers(persist_request);
                signed_infos.push(signed_info);
            }
        }
        signed_infos
    }
}
```

**File:** consensus/src/quorum_store/batch_generator.rs (L486-501)
```rust
                            let persist_start = Instant::now();
                            let mut persist_requests = vec![];
                            for batch in batches.clone().into_iter() {
                                persist_requests.push(batch.into());
                            }
                            self.batch_writer.persist(persist_requests);
                            counters::BATCH_CREATION_PERSIST_LATENCY.observe_duration(persist_start.elapsed());

                            if self.config.enable_batch_v2 {
                                network_sender.broadcast_batch_msg_v2(batches).await;
                            } else {
                                let batches = batches.into_iter().map(|batch| {
                                    batch.try_into().expect("Cannot send V2 batch with flag disabled")
                                }).collect();
                                network_sender.broadcast_batch_msg(batches).await;
                            }
```

**File:** consensus/src/quorum_store/batch_coordinator.rs (L90-134)
```rust
        tokio::spawn(async move {
            let peer_id = persist_requests[0].author();
            let batches = persist_requests
                .iter()
                .map(|persisted_value| {
                    (
                        persisted_value.batch_info().clone(),
                        persisted_value.summary(),
                    )
                })
                .collect();

            if persist_requests[0].batch_info().is_v2() {
                let signed_batch_infos = batch_store.persist(persist_requests);
                if !signed_batch_infos.is_empty() {
                    if approx_created_ts_usecs > 0 {
                        observe_batch(approx_created_ts_usecs, peer_id, BatchStage::SIGNED);
                    }
                    network_sender
                        .send_signed_batch_info_msg_v2(signed_batch_infos, vec![peer_id])
                        .await;
                }
            } else {
                let signed_batch_infos = batch_store.persist(persist_requests);
                if !signed_batch_infos.is_empty() {
                    assert!(!signed_batch_infos
                        .first()
                        .expect("must not be empty")
                        .is_v2());
                    if approx_created_ts_usecs > 0 {
                        observe_batch(approx_created_ts_usecs, peer_id, BatchStage::SIGNED);
                    }
                    let signed_batch_infos = signed_batch_infos
                        .into_iter()
                        .map(|sbi| sbi.try_into().expect("Batch must be V1 batch"))
                        .collect();
                    network_sender
                        .send_signed_batch_info_msg(signed_batch_infos, vec![peer_id])
                        .await;
                }
            }
            let _ = sender_to_proof_manager
                .send(ProofManagerCommand::ReceiveBatches(batches))
                .await;
        });
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L540-549)
```rust
            if let Ok(mut persisted_value) = self.batch_store.get_batch_from_local(batch.digest()) {
                if let Some(txns) = persisted_value.take_payload() {
                    result.push((batch, txns));
                }
            } else {
                warn!(
                    "Couldn't find a batch in local storage while creating inline block: {:?}",
                    batch.digest()
                );
            }
```

**File:** consensus/src/quorum_store/proof_manager.rs (L245-265)
```rust
    pub(crate) fn qs_back_pressure(&self) -> BackPressure {
        if self.remaining_total_txn_num > self.back_pressure_total_txn_limit
            || self.remaining_total_proof_num > self.back_pressure_total_proof_limit
        {
            sample!(
                SampleRate::Duration(Duration::from_millis(200)),
                info!(
                    "Quorum store is back pressured with {} txns, limit: {}, proofs: {}, limit: {}",
                    self.remaining_total_txn_num,
                    self.back_pressure_total_txn_limit,
                    self.remaining_total_proof_num,
                    self.back_pressure_total_proof_limit
                );
            );
        }

        BackPressure {
            txn_count: self.remaining_total_txn_num > self.back_pressure_total_txn_limit,
            proof_count: self.remaining_total_proof_num > self.back_pressure_total_proof_limit,
        }
    }
```
