# Audit Report

## Title
Cascading Validator Crash Due to Unhandled Network Failures in Cross-Shard Message Transmission

## Summary
The `RemoteCrossShardClient::send_cross_shard_msg()` function uses `unwrap()` on channel send operations without any retry logic. When combined with panic-on-failure behavior in the underlying gRPC client, transient network failures cause a cascading crash that terminates the entire validator process, resulting in complete node unavailability.

## Finding Description

The vulnerability exists in a two-stage failure cascade:

**Stage 1: gRPC Network Failure Panic** [1](#0-0) 

When `GRPCNetworkMessageServiceClientWrapper::send_message()` encounters any network error (connection refused, timeout, peer unavailable), it panics immediately. The code includes a TODO comment acknowledging the need for retry logic with exponential backoff, but this is not implemented.

**Stage 2: Channel Send Failure Panic** [2](#0-1) 

The `send_cross_shard_msg()` function calls `unwrap()` on the channel send operation. For unbounded channels created via `NetworkController::create_outbound_channel()`, `send()` only fails when the receiver has been dropped. [3](#0-2) 

**The Cascading Failure:**

1. During block execution, cross-shard messages are sent to communicate transaction state updates between shards [4](#0-3) 

2. When a transient network failure occurs (remote peer down, network partition, timeout), the gRPC `send_message()` call panics
3. This panic unwinds the outbound handler task in `OutboundHandler::process_one_outgoing_message()` [5](#0-4) 

4. When the async task exits, all `Receiver<Message>` instances in the `outbound_handlers` vector are dropped
5. All subsequent `send()` calls fail with `SendError` because receivers are disconnected
6. The next `send_cross_shard_msg()` call hits the `unwrap()` and panics
7. The global panic handler catches this and **terminates the entire validator process** with `process::exit(12)` [6](#0-5) 

**Attack Vector:**
An attacker can trigger this by:
- Temporarily disrupting network connectivity to target validator's remote shards
- DDoS attacks on remote shard endpoints to cause timeouts
- Exploiting network partitions during high-load scenarios

The validator executing a sharded block will crash when any cross-shard message transmission encounters a network error, leaving the validator completely unavailable until manual restart.

## Impact Explanation

This is a **High Severity** issue under the Aptos Bug Bounty criteria:

**Validator Node Crashes**: The vulnerability causes the entire validator process to exit via `process::exit(12)`, resulting in total node unavailability. This directly maps to "Validator node slowdowns" but is more severe as it causes complete crashes rather than just slowdowns.

**Impact Scope:**
- Complete loss of validator availability (not just degradation)
- No automatic recovery - requires manual node restart
- Affects block execution during sharded execution mode
- Can be triggered by natural network transients or deliberate attacks
- No validator collusion required

**Broken Invariants:**
1. **Liveness**: The validator cannot process blocks while crashed
2. **Availability**: The node is completely unavailable, affecting consensus participation
3. **Resilience**: The system should gracefully handle transient network failures, not crash

While this doesn't directly cause consensus safety violations or fund loss, it severely impacts network liveness and validator participation, which are critical for blockchain operation.

## Likelihood Explanation

**HIGH LIKELIHOOD** - This will occur naturally in distributed systems:

1. **Transient Network Failures are Common**: Network timeouts, packet loss, temporary peer unavailability, and connection resets happen regularly in production distributed systems

2. **No Recovery Mechanism**: There is zero error handling or retry logic - the first network error causes immediate failure

3. **Acknowledged Known Issue**: The TODO comment in the code explicitly states "Retry with exponential backoff on failures", indicating the developers are aware this error handling is missing

4. **Broad Attack Surface**: Any of the remote shard endpoints experiencing issues triggers the crash

5. **Weaponization Potential**: An attacker can deliberately trigger this through:
   - DDoS on specific validator endpoints
   - Network disruption during critical execution phases
   - Exploiting validators during network congestion

The combination of common triggering conditions, zero error resilience, and acknowledged missing functionality makes this highly likely to occur in production environments.

## Recommendation

Implement comprehensive error handling at both levels:

**Level 1: gRPC Client Retry Logic**

In `grpc_network_service/mod.rs`, replace the panic with exponential backoff retry:

```rust
pub async fn send_message(
    &mut self,
    sender_addr: SocketAddr,
    message: Message,
    mt: &MessageType,
) -> Result<(), tonic::Status> {
    let request = tonic::Request::new(NetworkMessage {
        message: message.data,
        message_type: mt.get_type(),
    });
    
    // Implement exponential backoff retry
    let max_retries = 3;
    let mut retry_delay = Duration::from_millis(100);
    
    for attempt in 0..=max_retries {
        match self.remote_channel.simple_msg_exchange(request.clone()).await {
            Ok(_) => return Ok(()),
            Err(e) if attempt < max_retries => {
                warn!(
                    "Attempt {}/{} failed sending message to {}: {}. Retrying in {:?}",
                    attempt + 1, max_retries, self.remote_addr, e, retry_delay
                );
                tokio::time::sleep(retry_delay).await;
                retry_delay *= 2; // Exponential backoff
            }
            Err(e) => {
                error!(
                    "Failed sending message to {} after {} attempts: {}",
                    self.remote_addr, max_retries + 1, e
                );
                return Err(e);
            }
        }
    }
    unreachable!()
}
```

**Level 2: Channel Send Error Handling**

In `remote_cross_shard_client.rs`, handle send failures gracefully:

```rust
fn send_cross_shard_msg(&self, shard_id: ShardId, round: RoundId, msg: CrossShardMsg) {
    let input_message = bcs::to_bytes(&msg).unwrap();
    let tx = self.message_txs[shard_id][round].lock().unwrap();
    
    if let Err(e) = tx.send(Message::new(input_message)) {
        error!(
            "Failed to send cross-shard message to shard {} round {}: {:?}. \
             Channel may be disconnected due to network handler failure.",
            shard_id, round, e
        );
        // Consider adding fallback mechanism or error propagation
    }
}
```

**Level 3: Outbound Handler Resilience**

Update `OutboundHandler::process_one_outgoing_message()` to catch panics in the gRPC send and continue processing:

```rust
if remote_addr == socket_addr {
    inbound_handler
        .lock()
        .unwrap()
        .send_incoming_message_to_handler(message_type, msg);
} else {
    // Catch panics from send_message
    let result = std::panic::AssertUnwindSafe(async {
        grpc_clients
            .get_mut(remote_addr)
            .unwrap()
            .send_message(*socket_addr, msg, message_type)
            .await
    });
    
    if let Err(e) = result.await {
        error!(
            "Panic caught while sending message to {}: {:?}",
            remote_addr, e
        );
        // Continue processing other messages
    }
}
```

## Proof of Concept

Create a test that demonstrates the failure:

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::net::{IpAddr, Ipv4Addr, SocketAddr};
    use std::thread;
    use std::time::Duration;

    #[test]
    #[should_panic(expected = "Error")]
    fn test_network_failure_causes_crash() {
        // Create a NetworkController
        let port = 9000;
        let addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), port);
        let mut controller = NetworkController::new(
            "test".to_string(),
            addr,
            1000
        );
        
        // Create a RemoteCrossShardClient with an unreachable remote address
        let unreachable_addr = SocketAddr::new(
            IpAddr::V4(Ipv4Addr::new(192, 0, 2, 1)), // TEST-NET-1
            9999
        );
        let client = RemoteCrossShardClient::new(
            &mut controller,
            vec![unreachable_addr]
        );
        
        controller.start();
        thread::sleep(Duration::from_millis(100));
        
        // Attempt to send a cross-shard message
        // This should panic due to network error
        let msg = CrossShardMsg::StopMsg;
        client.send_cross_shard_msg(0, 0, msg);
        
        // If we reach here, the test fails
        controller.shutdown();
    }
}
```

**Notes:**
- The vulnerability requires the remote execution mode to be active (when `RemoteCrossShardClient` is used)
- Local execution using `LocalCrossShardClient` is not affected as it uses in-memory channels
- The issue is exacerbated during high-throughput scenarios when many cross-shard messages are being sent
- Current production deployments may not fully exercise this code path if sharded execution is not widely deployed yet, but it remains a critical vulnerability for when this feature is enabled

### Citations

**File:** secure/net/src/grpc_network_service/mod.rs (L150-159)
```rust
        // TODO: Retry with exponential backoff on failures
        match self.remote_channel.simple_msg_exchange(request).await {
            Ok(_) => {},
            Err(e) => {
                panic!(
                    "Error '{}' sending message to {} on node {:?}",
                    e, self.remote_addr, sender_addr
                );
            },
        }
```

**File:** execution/executor-service/src/remote_cross_shard_client.rs (L55-59)
```rust
    fn send_cross_shard_msg(&self, shard_id: ShardId, round: RoundId, msg: CrossShardMsg) {
        let input_message = bcs::to_bytes(&msg).unwrap();
        let tx = self.message_txs[shard_id][round].lock().unwrap();
        tx.send(Message::new(input_message)).unwrap();
    }
```

**File:** secure/net/src/network_controller/mod.rs (L115-126)
```rust
    pub fn create_outbound_channel(
        &mut self,
        remote_peer_addr: SocketAddr,
        message_type: String,
    ) -> Sender<Message> {
        let (outbound_sender, outbound_receiver) = unbounded();

        self.outbound_handler
            .register_handler(message_type, remote_peer_addr, outbound_receiver);

        outbound_sender
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L103-134)
```rust
    fn send_remote_update_for_success(
        &self,
        txn_idx: TxnIndex,
        txn_output: &OnceCell<TransactionOutput>,
    ) {
        let edges = self.dependent_edges.get(&txn_idx).unwrap();
        let write_set = txn_output
            .get()
            .expect("Committed output must be set")
            .write_set();

        for (state_key, write_op) in write_set.expect_write_op_iter() {
            if let Some(dependent_shard_ids) = edges.get(state_key) {
                for (dependent_shard_id, round_id) in dependent_shard_ids.iter() {
                    trace!("Sending remote update for success for shard id {:?} and txn_idx: {:?}, state_key: {:?}, dependent shard id: {:?}", self.shard_id, txn_idx, state_key, dependent_shard_id);
                    let message = RemoteTxnWriteMsg(RemoteTxnWrite::new(
                        state_key.clone(),
                        Some(write_op.clone()),
                    ));
                    if *round_id == GLOBAL_ROUND_ID {
                        self.cross_shard_client.send_global_msg(message);
                    } else {
                        self.cross_shard_client.send_cross_shard_msg(
                            *dependent_shard_id,
                            *round_id,
                            message,
                        );
                    }
                }
            }
        }
    }
```

**File:** secure/net/src/network_controller/outbound_handler.rs (L103-162)
```rust
    async fn process_one_outgoing_message(
        outbound_handlers: Vec<(Receiver<Message>, SocketAddr, MessageType)>,
        socket_addr: &SocketAddr,
        inbound_handler: Arc<Mutex<InboundHandler>>,
        grpc_clients: &mut HashMap<SocketAddr, GRPCNetworkMessageServiceClientWrapper>,
    ) {
        loop {
            let mut select = Select::new();
            for (receiver, _, _) in outbound_handlers.iter() {
                select.recv(receiver);
            }

            let index;
            let msg;
            let _timer;
            {
                let oper = select.select();
                _timer = NETWORK_HANDLER_TIMER
                    .with_label_values(&[&socket_addr.to_string(), "outbound_msgs"])
                    .start_timer();
                index = oper.index();
                match oper.recv(&outbound_handlers[index].0) {
                    Ok(m) => {
                        msg = m;
                    },
                    Err(e) => {
                        warn!(
                            "{:?} for outbound handler on {:?}. This can happen in shutdown,\
                             but should not happen otherwise",
                            e.to_string(),
                            socket_addr
                        );
                        return;
                    },
                }
            }

            let remote_addr = &outbound_handlers[index].1;
            let message_type = &outbound_handlers[index].2;

            if message_type.get_type() == "stop_task" {
                return;
            }

            if remote_addr == socket_addr {
                // If the remote address is the same as the local address, then we are sending a message to ourselves
                // so we should just pass it to the inbound handler
                inbound_handler
                    .lock()
                    .unwrap()
                    .send_incoming_message_to_handler(message_type, msg);
            } else {
                grpc_clients
                    .get_mut(remote_addr)
                    .unwrap()
                    .send_message(*socket_addr, msg, message_type)
                    .await;
            }
        }
    }
```

**File:** crates/crash-handler/src/lib.rs (L26-57)
```rust
pub fn setup_panic_handler() {
    panic::set_hook(Box::new(move |pi: &PanicHookInfo<'_>| {
        handle_panic(pi);
    }));
}

// Formats and logs panic information
fn handle_panic(panic_info: &PanicHookInfo<'_>) {
    // The Display formatter for a PanicHookInfo contains the message, payload and location.
    let details = format!("{}", panic_info);
    let backtrace = format!("{:#?}", Backtrace::new());

    let info = CrashInfo { details, backtrace };
    let crash_info = toml::to_string_pretty(&info).unwrap();
    error!("{}", crash_info);
    // TODO / HACK ALARM: Write crash info synchronously via eprintln! to ensure it is written before the process exits which error! doesn't guarantee.
    // This is a workaround until https://github.com/aptos-labs/aptos-core/issues/2038 is resolved.
    eprintln!("{}", crash_info);

    // Wait till the logs have been flushed
    aptos_logger::flush();

    // Do not kill the process if the panics happened at move-bytecode-verifier.
    // This is safe because the `state::get_state()` uses a thread_local for storing states. Thus the state can only be mutated to VERIFIER by the thread that's running the bytecode verifier.
    //
    // TODO: once `can_unwind` is stable, we should assert it. See https://github.com/rust-lang/rust/issues/92988.
    if state::get_state() == VMState::VERIFIER || state::get_state() == VMState::DESERIALIZER {
        return;
    }

    // Kill the process
    process::exit(12);
```
