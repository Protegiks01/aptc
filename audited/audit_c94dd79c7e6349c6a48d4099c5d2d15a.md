# Audit Report

## Title
Transaction Loss During Quorum Store Emergency Rollback Due to Immutable Timeline State

## Summary
The `process_config_update()` function in mempool does not migrate existing transactions when `quorum_store_enabled` transitions from enabled to disabled. Transactions marked as `NonQualified` during quorum store operation are never added to the `timeline_index`, preventing their broadcast to other validators. During an emergency rollback, these transactions become stranded—unable to be broadcast via mempool and no longer pulled by the now-disabled quorum store, resulting in permanent transaction loss.

## Finding Description

When quorum store is enabled, client-submitted transactions are marked with `TimelineState::NonQualified` to prevent redundant mempool broadcasting, as quorum store handles transaction propagation. [1](#0-0) 

The config update handler modifies the `broadcast_within_validator_network` flag based on quorum store state: [2](#0-1) 

The critical flaw is in transaction insertion logic. Transactions with `NonQualified` state are **never** added to the `timeline_index`, which is used for broadcasting to peer validators: [3](#0-2) 

The `TimelineState` is set once at transaction insertion and never updated: [4](#0-3) 

Transaction broadcasting to peers exclusively reads from `timeline_index`: [5](#0-4) 

The codebase contains an explicit acknowledgment of this issue: [6](#0-5) 

**Attack Scenario:**

1. **Initial State (QS Enabled):** User submits transaction T to Validator A → T marked `NonQualified` → added to `priority_index` only, NOT `timeline_index` → awaiting quorum store propagation
2. **Emergency Rollback:** Network-wide config change disables quorum store → `broadcast_within_validator_network` changes to `true` → new transactions now broadcast-eligible
3. **Transaction Stranded:** Transaction T remains `NonQualified` in Validator A's mempool → NOT added to `timeline_index` → will NEVER be broadcast to Validators B, C, D
4. **Loss Scenarios:**
   - If Validator A crashes before consensus pulls T → T permanently lost (no other validator has it)
   - If Validator A is Byzantine/faulty → T may never reach consensus
   - Network-wide transaction availability compromised

## Impact Explanation

This is **HIGH severity** per Aptos bug bounty criteria:

1. **Significant Protocol Violations:** Violates transaction propagation guarantees fundamental to mempool operation
2. **Transaction Loss:** User transactions can be permanently lost during emergency rollbacks
3. **Consensus Liveness Issues:** If leader validator has stranded transactions, other validators cannot validate proposals, causing potential liveness failures
4. **Network Availability Impact:** During critical rollback scenarios, a subset of transactions become unavailable network-wide

The comment explicitly states "some transactions may not be propagated, they will neither go through a mempool broadcast or quorum store batch," confirming the developers are aware but consider it acceptable for "unexpected" emergency scenarios. However, this represents a serious availability and reliability issue that impacts end users.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

- **Trigger Condition:** Requires emergency rollback of quorum store (explicitly mentioned as possible in comments)
- **Affected Transactions:** All transactions submitted during quorum store era that haven't been processed by consensus before rollback
- **No Mitigation:** Zero automatic recovery mechanism; no code path updates `TimelineState` post-insertion
- **Real-World Relevance:** Emergency rollbacks are rare but critical scenarios where this bug has maximum impact
- **User Impact:** Affects regular users submitting transactions, not just validators

While emergency rollbacks are infrequent, when they occur (e.g., critical quorum store bug discovery), this vulnerability guarantees transaction loss, making it a high-probability issue conditional on the rollback event.

## Recommendation

Implement automatic transaction state migration during config updates:

```rust
pub(crate) async fn process_config_update<V, P>(
    config_update: OnChainConfigPayload<P>,
    validator: Arc<RwLock<V>>,
    broadcast_within_validator_network: Arc<RwLock<bool>>,
    mempool: Arc<Mutex<CoreMempool>>, // NEW: Add mempool reference
) where
    V: TransactionValidation,
    P: OnChainConfigProvider,
{
    // ... existing validator restart code ...

    let consensus_config: anyhow::Result<OnChainConsensusConfig> = config_update.get();
    match consensus_config {
        Ok(consensus_config) => {
            let new_broadcast_mode = 
                !consensus_config.quorum_store_enabled() && !consensus_config.is_dag_enabled();
            let old_broadcast_mode = *broadcast_within_validator_network.read();
            
            *broadcast_within_validator_network.write() = new_broadcast_mode;
            
            // NEW: Migrate transactions on false->true transition
            if !old_broadcast_mode && new_broadcast_mode {
                migrate_nonqualified_transactions(&mempool);
            }
        },
        // ... error handling ...
    }
}

fn migrate_nonqualified_transactions(mempool: &Arc<Mutex<CoreMempool>>) {
    let mut pool = mempool.lock();
    // Iterate all NonQualified transactions and promote them to broadcast-eligible
    pool.transactions.promote_nonqualified_to_ready();
}
```

Additionally, implement `promote_nonqualified_to_ready()` in `TransactionStore` to:
1. Iterate all transactions with `TimelineState::NonQualified`
2. Update their state to `TimelineState::NotReady`
3. Call `process_ready_transaction()` to add them to `timeline_index`

This ensures zero transaction loss during emergency rollbacks while maintaining backward compatibility.

## Proof of Concept

```rust
// Reproduction steps (pseudo-code for integration test)

#[tokio::test]
async fn test_transaction_loss_during_qs_rollback() {
    // 1. Initialize network with quorum store ENABLED
    let mut swarm = SwarmBuilder::new_local(4)
        .with_quorum_store(true)
        .build()
        .await;
    
    // 2. Submit transaction to Validator 0
    let sender = swarm.validators()[0].clone();
    let txn = create_test_transaction();
    sender.submit_transaction(txn.clone()).await.unwrap();
    
    // 3. Verify transaction is in Validator 0's mempool as NonQualified
    let mempool_state = sender.get_mempool_state().await;
    assert_eq!(mempool_state.get_timeline_state(&txn.hash()), 
               TimelineState::NonQualified);
    assert!(!mempool_state.in_timeline_index(&txn.hash()));
    
    // 4. Trigger emergency rollback: disable quorum store
    swarm.update_onchain_config(|config| {
        config.consensus_config.quorum_store_enabled = false;
    }).await;
    
    // 5. Wait for config propagation
    tokio::time::sleep(Duration::from_secs(2)).await;
    
    // 6. Verify transaction is STILL NonQualified (not migrated)
    let mempool_state = sender.get_mempool_state().await;
    assert_eq!(mempool_state.get_timeline_state(&txn.hash()), 
               TimelineState::NonQualified);
    
    // 7. Check other validators - transaction NOT broadcast
    for i in 1..4 {
        let validator = swarm.validators()[i].clone();
        let mempool_state = validator.get_mempool_state().await;
        assert!(!mempool_state.contains_transaction(&txn.hash()),
                "Transaction should NOT be in Validator {}'s mempool", i);
    }
    
    // 8. Simulate Validator 0 crash
    swarm.validators()[0].stop().await;
    
    // 9. Attempt to commit transaction - FAILS
    // No validator has the transaction; it's permanently lost
    let result = swarm.wait_for_transaction(&txn.hash(), Duration::from_secs(10)).await;
    assert!(result.is_err(), "Transaction should be lost");
}
```

**Expected Result:** Transaction is permanently lost after Validator 0 crashes, demonstrating the vulnerability.

**Actual Behavior:** The test would confirm that `NonQualified` transactions are never migrated to `timeline_index` during config transitions, causing transaction loss when the submitting validator becomes unavailable.

## Notes

This vulnerability is explicitly acknowledged in the codebase comments but dismissed as acceptable for "unexpected" emergency rollback scenarios. However, the lack of any mitigation or recovery mechanism makes this a genuine HIGH severity issue that violates fundamental transaction availability guarantees. The fix is straightforward and should be implemented to ensure zero transaction loss during critical operational transitions.

### Citations

**File:** mempool/src/shared_mempool/tasks.rs (L140-146)
```rust
    let ineligible_for_broadcast =
        smp.network_interface.is_validator() && !smp.broadcast_within_validator_network();
    let timeline_state = if ineligible_for_broadcast {
        TimelineState::NonQualified
    } else {
        TimelineState::NotReady
    };
```

**File:** mempool/src/shared_mempool/tasks.rs (L780-794)
```rust
    let consensus_config: anyhow::Result<OnChainConsensusConfig> = config_update.get();
    match consensus_config {
        Ok(consensus_config) => {
            *broadcast_within_validator_network.write() =
                !consensus_config.quorum_store_enabled() && !consensus_config.is_dag_enabled()
        },
        Err(e) => {
            error!(
                "Failed to read on-chain consensus config, keeping value broadcast_within_validator_network={}: {}",
                *broadcast_within_validator_network.read(),
                e
            );
        },
    }
}
```

**File:** mempool/src/core_mempool/transaction_store.rs (L559-567)
```rust
                // If timeline_state is `NonQualified`, then the transaction is never added to the timeline_index,
                // and never broadcasted to the shared mempool.
                let ready_for_mempool_broadcast = txn.timeline_state == TimelineState::NotReady;
                if ready_for_mempool_broadcast {
                    self.timeline_index
                        .get_mut(&sender_bucket)
                        .unwrap()
                        .insert(txn);
                }
```

**File:** mempool/src/core_mempool/transaction_store.rs (L774-838)
```rust
    pub(crate) fn read_timeline(
        &self,
        sender_bucket: MempoolSenderBucket,
        timeline_id: &MultiBucketTimelineIndexIds,
        count: usize,
        before: Option<Instant>,
        // The priority of the receipient of the transactions
        priority_of_receiver: BroadcastPeerPriority,
    ) -> (Vec<(SignedTransaction, u64)>, MultiBucketTimelineIndexIds) {
        let mut batch = vec![];
        let mut batch_total_bytes: u64 = 0;
        let mut last_timeline_id = timeline_id.id_per_bucket.clone();

        // Add as many transactions to the batch as possible
        for (i, bucket) in self
            .timeline_index
            .get(&sender_bucket)
            .unwrap_or_else(|| {
                panic!(
                    "Unable to get the timeline index for the sender bucket {}",
                    sender_bucket
                )
            })
            .read_timeline(timeline_id, count, before)
            .iter()
            .enumerate()
            .rev()
        {
            for (address, replay_protector) in bucket {
                if let Some(txn) = self.get_mempool_txn(address, *replay_protector) {
                    let transaction_bytes = txn.txn.raw_txn_bytes_len() as u64;
                    if batch_total_bytes.saturating_add(transaction_bytes) > self.max_batch_bytes {
                        break; // The batch is full
                    } else {
                        batch.push((
                            txn.txn.clone(),
                            aptos_infallible::duration_since_epoch_at(
                                &txn.insertion_info.ready_time,
                            )
                            .as_millis() as u64,
                        ));
                        batch_total_bytes = batch_total_bytes.saturating_add(transaction_bytes);
                        if let TimelineState::Ready(timeline_id) = txn.timeline_state {
                            last_timeline_id[i] = timeline_id;
                        }
                        let bucket = self.get_bucket(txn.ranking_score, &txn.get_sender());
                        Mempool::log_txn_latency(
                            &txn.insertion_info,
                            bucket.as_str(),
                            BROADCAST_BATCHED_LABEL,
                            priority_of_receiver.to_string().as_str(),
                        );
                        counters::core_mempool_txn_ranking_score(
                            BROADCAST_BATCHED_LABEL,
                            BROADCAST_BATCHED_LABEL,
                            bucket.as_str(),
                            txn.ranking_score,
                        );
                    }
                }
            }
        }

        (batch, last_timeline_id.into())
    }
```

**File:** mempool/src/core_mempool/transaction.rs (L75-85)
```rust
#[derive(Clone, Copy, PartialEq, Eq, Debug, Deserialize, Hash, Serialize)]
pub enum TimelineState {
    // The transaction is ready for broadcast.
    // Associated integer represents it's position in the log of such transactions.
    Ready(u64),
    // Transaction is not yet ready for broadcast, but it might change in a future.
    NotReady,
    // Transaction will never be qualified for broadcasting.
    // Currently we don't broadcast transactions originated on other peers.
    NonQualified,
}
```

**File:** mempool/src/shared_mempool/types.rs (L95-103)
```rust
    pub fn broadcast_within_validator_network(&self) -> bool {
        // This value will be changed true -> false via onchain config when quorum store is enabled.
        // On the transition from true -> false, all transactions in mempool will be eligible for
        // at least one of mempool broadcast or quorum store batch.
        // A transition from false -> true is unexpected -- it would only be triggered if quorum
        // store needs an emergency rollback. In this case, some transactions may not be propagated,
        // they will neither go through a mempool broadcast or quorum store batch.
        *self.broadcast_within_validator_network.read()
    }
```
