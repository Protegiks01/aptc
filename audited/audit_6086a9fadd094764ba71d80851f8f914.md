# Audit Report

## Title
TOCTOU Race Condition in Leader Reputation Causes Non-Deterministic Leader Selection

## Summary
A Time-of-Check-Time-of-Use (TOCTOU) race condition exists in `get_block_metadata()` where validators can read different database versions between checking `latest_db_version` and calling `refresh_db_result()`, causing different validators to compute different leaders for the same consensus round, resulting in liveness failures.

## Finding Description

The vulnerability exists in the leader reputation system used by ProposerAndVoterV2 (the default consensus configuration). [1](#0-0) 

The TOCTOU race occurs in `AptosDBBackend::get_block_metadata()`:

1. **Time of Check (Line 176)**: The function reads `latest_db_version` from the database
2. **Race Window**: Between lines 176 and 197, the database can receive new committed blocks from consensus
3. **Time of Use (Line 197-198)**: The function checks if `version < latest_db_version` and calls `refresh_db_result()`

Different validators calling this function at slightly different times will read different values of `latest_db_version`. When `refresh_db_result()` is called, it fetches the latest block events from the database's current state [2](#0-1) , which may include blocks committed after the initial version check. This causes different validators to cache different sets of block events.

When computing the leader for a consensus round, validators use these cached events to compute a root hash [3](#0-2) , which is then used as part of the seed for weighted random leader selection [4](#0-3) . The leader selection uses `choose_index()` which performs SHA-3-256 hashing on the seed [5](#0-4) . Different root hashes produce different SHA-3-256 outputs, leading to different leader selections.

The default consensus configuration uses ProposerAndVoterV2 [6](#0-5) , which enables `use_root_hash` for seed generation [7](#0-6) , making this vulnerability active by default.

**Attack Scenario:**

1. Network operates with ProposerAndVoterV2 (default configuration)
2. Validators A and B both advance to round 100
3. Validator A calls `get_block_metadata()` at time T1, reads `latest_db_version = 1000`
4. A new block is committed to some validators' databases, advancing them to version 1001
5. Validator B calls `get_block_metadata()` at time T2, reads `latest_db_version = 1001`
6. Validator A's cached data is at version 1000, check `1000 < 1000 = false`, uses stale cache
7. Validator B's cached data is at version 1000, check `1000 < 1001 = true`, refreshes and gets events up to version 1001
8. Validator A computes `root_hash_A` from version 1000's accumulator
9. Validator B computes `root_hash_B` from version 1001's accumulator (different!)
10. Both compute leader with `state = [root_hash, epoch, round]`
11. Different root hashes → different SHA-3-256 outputs → different leaders selected
12. The validator who computed themselves as leader proposes a block
13. Validators with different leader computations reject the proposal [8](#0-7) 
14. If <2/3 validators agree on the leader, no quorum certificate forms
15. Round times out, causing liveness degradation

While `CachedProposerElection` caches results per validator [9](#0-8) , it doesn't prevent different validators from caching different values.

## Impact Explanation

This vulnerability causes **High Severity** consensus liveness issues that qualify under the Aptos bug bounty program's "Significant protocol violations" and "Validator node slowdowns" categories.

The impact manifests as:
- **Intermittent Round Failures**: When validators disagree on the leader, proposals are rejected by a subset of validators, preventing quorum formation and causing round timeouts
- **Reduced Network Throughput**: Failed rounds reduce the effective transaction processing rate
- **Protocol Invariant Violation**: Breaks the consensus assumption that all validators deterministically agree on the leader for each round
- **Consensus Liveness Degradation**: While not causing total liveness loss (validators eventually progress), it creates unnecessary delays

This does not directly cause safety violations (chain splits or double-spending) because AptosBFT's safety mechanisms remain intact. However, persistent liveness issues severely impact network utility.

## Likelihood Explanation

**Likelihood: Medium-High**

This vulnerability triggers naturally during normal network operation without requiring malicious actors:

**Triggering Conditions:**
1. Validators must be using ProposerAndVoterV2 configuration (default setting)
2. Asynchronous block commits cause validators' databases to be at slightly different versions
3. Multiple validators compute the same round's leader within the race window

**Frequency Factors:**
- Database commits are asynchronous relative to consensus voting
- Network delays cause validators to commit blocks at different times
- The race window spans from reading `latest_db_version` to using it (~microseconds to milliseconds)
- Every round's leader computation is susceptible when database states diverge

In high-throughput networks with geographic distribution, timing differences are common. The vulnerability becomes more likely as:
- Network latency increases between validators
- Block production rate increases
- Database commit times vary due to system load

## Recommendation

Implement atomic snapshot isolation for leader reputation queries to ensure all validators use consistent database states:

**Option 1: Snapshot at Single Version (Recommended)**
Read the database version once and use it consistently throughout the function. Modify `get_block_metadata()` to pass the version to `refresh_db_result()` and ensure `get_latest_block_events()` uses that specific version instead of the current latest.

**Option 2: Lock-Based Consistency**
Acquire a read lock on the database version at the start of `get_block_metadata()` and release it after computing the root hash. This ensures no database updates occur during leader computation.

**Option 3: Deterministic Metadata Agreement**
Include block metadata commitments in the quorum certificates themselves, forcing validators to agree on the metadata used for leader selection as part of consensus.

**Code Fix (Option 1 - Recommended):**

Modify `refresh_db_result()` to accept and use a specific version instead of reading the latest:

```rust
fn refresh_db_result(
    &self,
    locked: &mut MutexGuard<'_, Option<(Vec<VersionedNewBlockEvent>, u64, bool)>>,
    target_db_version: u64,
) -> Result<(Vec<VersionedNewBlockEvent>, u64, bool)> {
    let limit = self.window_size + self.seek_len;
    
    // Read events up to target_db_version, not latest
    let events = self.aptos_db.get_block_events_up_to_version(limit, target_db_version)?;
    
    // ... rest of the function
}
```

And modify `get_block_metadata()` to read version once at the start and use it consistently.

## Proof of Concept

**Conceptual PoC:**

```rust
// Simulated test demonstrating the race condition
#[tokio::test]
async fn test_toctou_race_different_leaders() {
    // Setup: Two validators with separate database instances
    let db_a = create_test_db_with_blocks(1..=100); // DB at version 1000
    let db_b = create_test_db_with_blocks(1..=100); // DB at version 1000
    
    let backend_a = AptosDBBackend::new(100, 100, Arc::new(db_a));
    let backend_b = AptosDBBackend::new(100, 100, Arc::new(db_b));
    
    // Validator A computes leader
    let (events_a, root_hash_a) = backend_a.get_block_metadata(epoch, round);
    
    // Simulate new block committed to validator B's database
    db_b.commit_block(block_101); // Now at version 1001
    
    // Validator B computes leader
    let (events_b, root_hash_b) = backend_b.get_block_metadata(epoch, round);
    
    // Assert: Different root hashes due to race
    assert_ne!(root_hash_a, root_hash_b);
    
    // Compute leaders using LeaderReputation
    let leader_a = compute_leader_with_root_hash(root_hash_a, epoch, round, validators);
    let leader_b = compute_leader_with_root_hash(root_hash_b, epoch, round, validators);
    
    // Assert: Different leaders computed
    assert_ne!(leader_a, leader_b);
    
    // This causes consensus round failure when leader_a proposes but
    // validators with computation matching leader_b reject the proposal
}
```

**Reproduction Steps:**
1. Deploy testnet with ProposerAndVoterV2 configuration
2. Monitor validator logs during high transaction load
3. Inject artificial delay in database commits on some validators
4. Observe "InvalidConsensusProposal" warnings in logs where validators reject valid proposals
5. Measure increased round timeout frequency during test period

## Notes

The vulnerability is exacerbated by the `std::cmp::max(latest_db_version, max_returned_version)` pattern at line 96, which attempts to handle version inconsistencies but actually masks the underlying race condition. The caching mechanism in `CachedProposerElection` prevents re-computation within a validator but doesn't address cross-validator inconsistency.

This issue affects deterministic leader selection, a critical consensus property. While AptosBFT maintains safety through voting rules, the liveness impact of non-deterministic leader selection can be significant in production networks with geographic distribution and network latency.

### Citations

**File:** consensus/src/liveness/leader_reputation.rs (L70-101)
```rust
    fn refresh_db_result(
        &self,
        locked: &mut MutexGuard<'_, Option<(Vec<VersionedNewBlockEvent>, u64, bool)>>,
        latest_db_version: u64,
    ) -> Result<(Vec<VersionedNewBlockEvent>, u64, bool)> {
        // assumes target round is not too far from latest commit
        let limit = self.window_size + self.seek_len;

        let events = self.aptos_db.get_latest_block_events(limit)?;

        let max_returned_version = events.first().map_or(0, |first| first.transaction_version);

        let new_block_events = events
            .into_iter()
            .map(|event| {
                Ok(VersionedNewBlockEvent {
                    event: bcs::from_bytes::<NewBlockEvent>(event.event.event_data())?,
                    version: event.transaction_version,
                })
            })
            .collect::<Result<Vec<VersionedNewBlockEvent>, bcs::Error>>()?;

        let hit_end = new_block_events.len() < limit;

        let result = (
            new_block_events,
            std::cmp::max(latest_db_version, max_returned_version),
            hit_end,
        );
        **locked = Some(result.clone());
        Ok(result)
    }
```

**File:** consensus/src/liveness/leader_reputation.rs (L153-162)
```rust
            let root_hash = self
                .aptos_db
                .get_accumulator_root_hash(max_version)
                .unwrap_or_else(|_| {
                    error!(
                        "We couldn't fetch accumulator hash for the {} version, for {} epoch, {} round",
                        max_version, target_epoch, target_round,
                    );
                    HashValue::zero()
                });
```

**File:** consensus/src/liveness/leader_reputation.rs (L170-214)
```rust
    fn get_block_metadata(
        &self,
        target_epoch: u64,
        target_round: Round,
    ) -> (Vec<NewBlockEvent>, HashValue) {
        let mut locked = self.db_result.lock();
        let latest_db_version = self.aptos_db.get_latest_ledger_info_version().unwrap_or(0);
        // lazy init db_result
        if locked.is_none() {
            if let Err(e) = self.refresh_db_result(&mut locked, latest_db_version) {
                warn!(
                    error = ?e, "[leader reputation] Fail to initialize db result",
                );
                return (vec![], HashValue::zero());
            }
        }
        let (events, version, hit_end) = {
            // locked is somenthing
            #[allow(clippy::unwrap_used)]
            let result = locked.as_ref().unwrap();
            (&result.0, result.1, result.2)
        };

        let has_larger = events
            .first()
            .is_some_and(|e| (e.event.epoch(), e.event.round()) >= (target_epoch, target_round));
        // check if fresher data has potential to give us different result
        if !has_larger && version < latest_db_version {
            let fresh_db_result = self.refresh_db_result(&mut locked, latest_db_version);
            match fresh_db_result {
                Ok((events, _version, hit_end)) => {
                    self.get_from_db_result(target_epoch, target_round, &events, hit_end)
                },
                Err(e) => {
                    // fails if requested events were pruned / or we never backfil them.
                    warn!(
                        error = ?e, "[leader reputation] Fail to refresh window",
                    );
                    (vec![], HashValue::zero())
                },
            }
        } else {
            self.get_from_db_result(target_epoch, target_round, events, hit_end)
        }
    }
```

**File:** consensus/src/liveness/leader_reputation.rs (L717-732)
```rust
        let state = if self.use_root_hash {
            [
                root_hash.to_vec(),
                self.epoch.to_le_bytes().to_vec(),
                round.to_le_bytes().to_vec(),
            ]
            .concat()
        } else {
            [
                self.epoch.to_le_bytes().to_vec(),
                round.to_le_bytes().to_vec(),
            ]
            .concat()
        };

        let chosen_index = choose_index(stake_weights, state);
```

**File:** consensus/src/liveness/proposer_election.rs (L39-69)
```rust
fn next_in_range(state: Vec<u8>, max: u128) -> u128 {
    // hash = SHA-3-256(state)
    let hash = aptos_crypto::HashValue::sha3_256_of(&state).to_vec();
    let mut temp = [0u8; 16];
    copy_slice_to_vec(&hash[..16], &mut temp).expect("next failed");
    // return hash[0..16]
    u128::from_le_bytes(temp) % max
}

// chose index randomly, with given weight distribution
pub(crate) fn choose_index(mut weights: Vec<u128>, state: Vec<u8>) -> usize {
    let mut total_weight = 0;
    // Create cumulative weights vector
    // Since we own the vector, we can safely modify it in place
    for w in &mut weights {
        total_weight = total_weight
            .checked_add(w)
            .expect("Total stake shouldn't exceed u128::MAX");
        *w = total_weight;
    }
    let chosen_weight = next_in_range(state, total_weight);
    weights
        .binary_search_by(|w| {
            if *w <= chosen_weight {
                Ordering::Less
            } else {
                Ordering::Greater
            }
        })
        .expect_err("Comparison never returns equals, so it's always guaranteed to be error")
}
```

**File:** types/src/on_chain_config/consensus_config.rs (L481-505)
```rust
impl Default for ConsensusConfigV1 {
    fn default() -> Self {
        Self {
            decoupled_execution: true,
            back_pressure_limit: 10,
            exclude_round: 40,
            max_failed_authors_to_store: 10,
            proposer_election_type: ProposerElectionType::LeaderReputation(
                LeaderReputationType::ProposerAndVoterV2(ProposerAndVoterConfig {
                    active_weight: 1000,
                    inactive_weight: 10,
                    failed_weight: 1,
                    failure_threshold_percent: 10, // = 10%
                    // In each round we get stastics for the single proposer
                    // and large number of validators. So the window for
                    // the proposers needs to be significantly larger
                    // to have enough useful statistics.
                    proposer_window_num_validators_multiplier: 10,
                    voter_window_num_validators_multiplier: 1,
                    weight_by_voting_power: true,
                    use_history_from_previous_epoch_max_count: 5,
                }),
            ),
        }
    }
```

**File:** types/src/on_chain_config/consensus_config.rs (L541-544)
```rust
    pub fn use_root_hash_for_seed(&self) -> bool {
        // all versions after V1 should use root hash
        !matches!(self, Self::ProposerAndVoter(_))
    }
```

**File:** consensus/src/liveness/unequivocal_proposer_election.rs (L46-60)
```rust
    pub fn is_valid_proposal(&self, block: &Block) -> bool {
        block.author().is_some_and(|author| {
            let valid_author = self.is_valid_proposer(author, block.round());
            if !valid_author {
                warn!(
                    SecurityEvent::InvalidConsensusProposal,
                    "Proposal is not from valid author {}, expected {} for round {} and id {}",
                    author,
                    self.get_valid_proposer(block.round()),
                    block.round(),
                    block.id()
                );

                return false;
            }
```

**File:** consensus/src/liveness/cached_proposer_election.rs (L40-58)
```rust
    pub fn get_or_compute_entry(&self, round: Round) -> (Author, f64) {
        let mut recent_elections = self.recent_elections.lock();

        if round > self.window as u64 {
            *recent_elections = recent_elections.split_off(&(round - self.window as u64));
        }

        *recent_elections.entry(round).or_insert_with(|| {
            let _timer = PROPOSER_ELECTION_DURATION.start_timer();
            let result = self
                .proposer_election
                .get_valid_proposer_and_voting_power_participation_ratio(round);
            info!(
                "ProposerElection for epoch {} and round {}: {:?}",
                self.epoch, round, result
            );
            result
        })
    }
```
