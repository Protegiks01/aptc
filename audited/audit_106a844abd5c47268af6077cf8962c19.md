# Audit Report

## Title
Silent Commit Proof Delivery Failure Causes Validators to Miss Committed Blocks

## Summary
The `send_commit_proof()` function in `consensus/src/network.rs` discards RPC errors with `let _ =`, allowing failures to silently prevent validators from committing blocks that the network has already committed. When the RPC queue is full or timeouts occur, commit proofs never reach the buffer manager, causing validators to indefinitely lag behind the network's commit state.

## Finding Description

The vulnerability occurs in the consensus synchronization flow when validators fast-forward their commit state. [1](#0-0) 

When a validator receives a SyncInfo message with a higher commit certificate, it calls `sync_to_highest_commit_cert()` to fast-forward commits. [2](#0-1) 

This spawns a task that sends a commit proof to itself via RPC, but critically, the RPC result is discarded. The RPC has a 500ms timeout and sends through a self-message channel that gets queued in the `rpc_tx` bounded channel. [3](#0-2) 

The `rpc_tx` channel has only 10 slots capacity and uses FIFO queueing. [4](#0-3) 

When the queue is full, the NetworkTask's push operation silently drops the newest message (FIFO policy) but returns Ok(()), logging no error. [5](#0-4) 

Since the RPC response callback is dropped with the message, the RPC caller times out after 500ms. The error is ignored by `send_commit_proof()`, so the commit proof never reaches the buffer manager. [6](#0-5) 

When the block finishes execution, no pending commit proof exists to apply, so the block never advances to aggregated state and is never committed, breaking the **State Consistency** invariant (validators must maintain consistent commit state with the network).

## Impact Explanation

This qualifies as **High Severity** under "Validator node slowdowns" and "Significant protocol violations":

1. **Consensus State Inconsistency**: Validators have ordered and executed blocks but fail to commit them, creating divergent commit states across the network
2. **Client Impact**: Clients querying affected validators see stale committed state, breaking data consistency guarantees
3. **Validator Liveness**: Affected validators may fail health checks or trigger unnecessary state synchronization
4. **Cascading Failures**: If the condition persists (sustained high load), validators can fall arbitrarily far behind, potentially triggering emergency state sync

The impact is limited to availability and consistency rather than safety (no double-spending or forks), but it violates the critical invariant that all honest validators must converge on the same committed state.

## Likelihood Explanation

**HIGH likelihood** during normal operations:

1. **Small Queue Capacity**: The `rpc_tx` queue has only 10 slots, which fills quickly during:
   - Epoch transitions when validators exchange SyncInfo messages
   - Network partitions recovering with catch-up sync
   - High transaction throughput periods with many block retrievals

2. **Silent Failure**: No errors are logged when messages are dropped (FIFO queue returns Ok()), making the issue invisible to operators

3. **No Retry Mechanism**: Once the commit proof is lost, there's no automatic retry - the validator relies on receiving another SyncInfo to trigger the same code path, which may fail again

4. **500ms Timeout**: Under load, the buffer manager may take >500ms to process commit messages, causing legitimate timeouts even when messages are delivered

## Recommendation

**Fix 1: Check and log RPC errors**
```rust
pub async fn send_commit_proof(&self, ledger_info: LedgerInfoWithSignatures) {
    fail_point!("consensus::send::commit_decision", |_| ());
    let msg = ConsensusMsg::CommitMessage(Box::new(CommitMessage::Decision(
        CommitDecision::new(ledger_info.clone()),
    )));
    if let Err(e) = self
        .send_rpc(self.author, msg, Duration::from_millis(500))
        .await 
    {
        error!(
            error = ?e,
            round = ledger_info.commit_info().round(),
            block_id = ledger_info.commit_info().id(),
            "Failed to send commit proof to self, block may not be committed"
        );
        // Consider retrying or using a direct channel instead of RPC
    }
}
```

**Fix 2: Increase RPC queue capacity or use priority queuing for commit proofs**

**Fix 3: Use direct channel delivery instead of RPC for self-messages** to avoid queue congestion and timeouts

## Proof of Concept

**Scenario**: Validator under high load during sync

```rust
// Simulated test scenario (conceptual - requires integration test setup)
#[tokio::test]
async fn test_commit_proof_loss_under_load() {
    // 1. Create validator with full rpc_rx queue (10 pending requests)
    // 2. Trigger sync with higher commit certificate
    // 3. Observe send_commit_proof() called
    // 4. Verify RPC times out due to queue full
    // 5. Verify no error logged
    // 6. Verify block remains uncommitted despite being executed
    // 7. Verify validator commit_root lags behind network
}
```

**Reproduction Steps**:
1. Start validator node
2. Generate 10 concurrent block retrieval requests to fill `rpc_tx` queue
3. Trigger `sync_to_highest_commit_cert()` by sending SyncInfo with higher commit certificate
4. Monitor logs - no error logged for dropped commit proof
5. Query validator's commit state - observe lag behind network despite having ordered/executed the block
6. Block remains in executed state indefinitely until another sync attempt (which may also fail)

**Notes**

This issue represents a **reliability vulnerability** rather than a direct security exploit. While not exploitable by external attackers to steal funds or break consensus safety, it causes validators to fail at their core responsibility: committing blocks in sync with the network. The silent failure mode (no error logging) makes diagnosis difficult, and the lack of retry mechanisms means validators can remain perpetually behind until manual intervention or fortunate timing resolves the queue congestion.

The vulnerability is particularly concerning because:
- It violates the State Consistency invariant silently
- It affects validator availability during critical sync operations
- The bounded queue (10 slots) is insufficient for burst traffic scenarios
- Error suppression prevents monitoring and alerting systems from detecting the issue

### Citations

**File:** consensus/src/network.rs (L316-332)
```rust
    pub async fn send_rpc_to_self(
        &self,
        msg: ConsensusMsg,
        timeout_duration: Duration,
    ) -> anyhow::Result<ConsensusMsg> {
        let (tx, rx) = oneshot::channel();
        let protocol = RPC[0];
        let self_msg = Event::RpcRequest(self.author, msg.clone(), RPC[0], tx);
        self.self_sender.clone().send(self_msg).await?;
        if let Ok(Ok(Ok(bytes))) = timeout(timeout_duration, rx).await {
            let response_msg =
                tokio::task::spawn_blocking(move || protocol.from_bytes(&bytes)).await??;
            Ok(response_msg)
        } else {
            bail!("self rpc failed");
        }
    }
```

**File:** consensus/src/network.rs (L540-548)
```rust
    pub async fn send_commit_proof(&self, ledger_info: LedgerInfoWithSignatures) {
        fail_point!("consensus::send::commit_decision", |_| ());
        let msg = ConsensusMsg::CommitMessage(Box::new(CommitMessage::Decision(
            CommitDecision::new(ledger_info),
        )));
        let _ = self
            .send_rpc(self.author, msg, Duration::from_millis(500))
            .await;
    }
```

**File:** consensus/src/network.rs (L768-769)
```rust
        let (rpc_tx, rpc_rx) =
            aptos_channel::new(QueueStyle::FIFO, 10, Some(&counters::RPC_CHANNEL_MSGS));
```

**File:** consensus/src/network.rs (L1020-1025)
```rust
                    if let Err(e) = self
                        .rpc_tx
                        .push((peer_id, discriminant(&req)), (peer_id, req))
                    {
                        warn!(error = ?e, "aptos channel closed");
                    };
```

**File:** consensus/src/block_storage/sync_manager.rs (L528-541)
```rust
    async fn sync_to_highest_commit_cert(
        &self,
        ledger_info: &LedgerInfoWithSignatures,
        network: Arc<NetworkSender>,
    ) {
        // if the block exists between commit root and ordered root
        if self.commit_root().round() < ledger_info.commit_info().round()
            && self.block_exists(ledger_info.commit_info().id())
            && self.ordered_root().round() >= ledger_info.commit_info().round()
        {
            let proof = ledger_info.clone();
            tokio::spawn(async move { network.send_commit_proof(proof).await });
        }
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L667-673)
```rust
        if let Some(commit_proof) = self.drain_pending_commit_proof_till(round) {
            if !new_item.is_aggregated()
                && commit_proof.ledger_info().commit_info().id() == block_id
            {
                new_item = new_item.try_advance_to_aggregated_with_ledger_info(commit_proof)
            }
        }
```
