# Audit Report

## Title
Transaction Lifecycle Inconsistency: Failed MempoolError Notifications Cause Repeated Re-execution of Rejected Transactions

## Summary
The `MempoolNotifier` in consensus can fail to notify mempool about rejected transactions due to channel saturation or timeout, causing these transactions to remain in mempool and be repeatedly re-executed by consensus, leading to resource exhaustion and state inconsistencies.

## Finding Description

The vulnerability exists in the coordination mechanism between consensus and mempool for handling rejected transactions. When consensus executes a block containing transactions that fail execution, it attempts to notify mempool via `MempoolNotifier.notify_failed_txn()` to remove these transactions. However, this notification can fail in multiple ways:

**Failure Point 1: Channel Send Failure**

The consensus-to-mempool channel uses `try_send` which fails immediately if the buffer is full: [1](#0-0) 

The channel has a buffer size of only 1: [2](#0-1) [3](#0-2) 

When mempool is processing another request (e.g., `GetBatchRequest`), the `try_send` fails immediately with a channel full error.

**Failure Point 2: Timeout**

The notification waits for an acknowledgment with a configurable timeout: [4](#0-3) 

With a default timeout of 1000ms: [5](#0-4) 

If mempool is under load or slow to respond, the callback times out and returns a `MempoolError`.

**Critical Flaw: Silent Error Handling**

When the notification fails, the error is merely logged and the pipeline continues: [6](#0-5) 

The transactions are never removed from mempool.

**Transaction Re-execution Cycle**

When a block commits (regardless of transaction execution results), all batches are removed from the exclude list via `remove_batch_in_progress`: [7](#0-6) 

Since mempool never received the rejection notification, it still contains these failed transactions. The `reject_transaction` method would remove them if notified: [8](#0-7) 

But since the notification failed, transactions remain in mempool. On the next batch pull, consensus excludes only transactions currently in progress. The previously-failed transactions are no longer excluded, so they get pulled again, fail execution again, and the cycle repeats until the transaction's TTL expires.

**Attack Scenario:**
1. Attacker submits transactions that pass validation but fail execution (e.g., `SEQUENCE_NUMBER_TOO_OLD`, `INSUFFICIENT_BALANCE`)
2. Consensus pulls these transactions and executes them
3. Transactions fail; consensus attempts to notify mempool via `RejectNotification`
4. If mempool is busy, the notification fails (channel full or timeout)
5. Error is logged but consensus continues; block commits
6. Batch cleanup removes transactions from exclude list
7. On next pull, mempool returns the same failed transactions
8. Cycle repeats until transaction TTL expires

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos bug bounty program:

1. **State Inconsistencies**: The transaction lifecycle is broken - consensus believes transactions are rejected while mempool believes they're still valid. This violates the state consistency invariant between components.

2. **Resource Exhaustion**: Failed transactions are re-executed repeatedly, wasting:
   - **Computational Resources**: VM execution of the same failing transactions
   - **Network Bandwidth**: Re-pulling transactions between mempool and consensus
   - **Storage**: Failed transactions remain in mempool until TTL expiration

3. **Denial-of-Service Potential**: An attacker can craft many transactions that pass validation but fail execution, causing validators to waste resources on repeated re-execution.

4. **Limited by TTL**: The impact is bounded by transaction TTL, providing eventual cleanup via garbage collection: [9](#0-8) 

This does not constitute Critical severity (no fund loss or consensus break) or High severity (no permanent node slowdown), but clearly fits Medium severity criteria for "state inconsistencies requiring manual intervention" and resource waste.

## Likelihood Explanation

This vulnerability has **HIGH likelihood** of occurring in production environments:

1. **Small Channel Buffer**: The channel buffer size of 1 means any concurrent processing causes immediate send failures
2. **Reasonable Timeout**: 1000ms timeout can be exceeded under normal load conditions
3. **Natural Conditions**: Does not require malicious intent - can occur naturally when:
   - Mempool is processing large batch requests
   - System is under high transaction load
   - Legitimate transactions fail execution due to state changes between validation and execution

4. **No Retry Mechanism**: Confirmed by code analysis - no retry logic or queuing exists for failed notifications
5. **Silent Failure**: The error is only logged, making it difficult to detect in production

## Recommendation

Implement one or more of the following mitigations:

1. **Increase Channel Buffer Size**: Change `INTRA_NODE_CHANNEL_BUFFER_SIZE` from 1 to a larger value (e.g., 100) to reduce immediate failures

2. **Add Retry Mechanism**: Implement retry logic with exponential backoff for failed notifications:
```rust
// In post_ledger_update, replace the single notification attempt with:
const MAX_RETRIES: usize = 3;
for attempt in 0..MAX_RETRIES {
    match mempool_notifier.notify_failed_txn(&txns, user_txn_status).await {
        Ok(_) => break,
        Err(e) if attempt < MAX_RETRIES - 1 => {
            warn!("Retry {} for mempool notification: {:?}", attempt + 1, e);
            tokio::time::sleep(Duration::from_millis(100 * 2_u64.pow(attempt as u32))).await;
        },
        Err(e) => error!("Failed to notify mempool after {} retries: {:?}", MAX_RETRIES, e),
    }
}
```

3. **Use Bounded Queue**: Replace `try_send` with a bounded queue that blocks or uses `send().await` to ensure delivery

4. **Track Failed Notifications**: Maintain a separate queue of failed notifications that are retried asynchronously

## Proof of Concept

The vulnerability can be demonstrated by:

1. Submitting a transaction with `SEQUENCE_NUMBER_TOO_OLD` to mempool
2. Simulating mempool being busy processing a `GetBatchRequest` (holding the mempool lock)
3. Having consensus execute the transaction (which fails)
4. Observing that `notify_failed_txn` fails with channel full error
5. Verifying the transaction remains in mempool after batch cleanup
6. Pulling from mempool again and observing the same transaction is returned
7. Re-executing the transaction and repeating the cycle

A complete Rust test would require mocking the channel behavior and mempool lock contention, demonstrating that failed transactions are re-pulled when not in the exclude list and notification has failed.

## Notes

This vulnerability represents a genuine state consistency issue in the transaction lifecycle between consensus and mempool components. While the impact is bounded by TTL-based cleanup, the resource waste and state inconsistency make it a valid Medium severity finding. The small channel buffer size (1) and lack of retry mechanism make this highly likely to occur in production under normal load conditions.

### Citations

**File:** consensus/src/txn_notifier.rs (L82-85)
```rust
        self.consensus_to_mempool_sender
            .clone()
            .try_send(req)
            .map_err(anyhow::Error::from)?;
```

**File:** consensus/src/txn_notifier.rs (L87-95)
```rust
        if let Err(e) = monitor!(
            "notify_mempool",
            timeout(
                Duration::from_millis(self.mempool_executed_txn_timeout_ms),
                callback_rcv
            )
            .await
        ) {
            Err(format_err!("[consensus] txn notifier did not receive ACK for commit notification sent to mempool on time: {:?}", e).into())
```

**File:** aptos-node/src/services.rs (L47-47)
```rust
const INTRA_NODE_CHANNEL_BUFFER_SIZE: usize = 1;
```

**File:** aptos-node/src/services.rs (L185-186)
```rust
    let (consensus_to_mempool_sender, consensus_to_mempool_receiver) =
        mpsc::channel(INTRA_NODE_CHANNEL_BUFFER_SIZE);
```

**File:** config/src/config/consensus_config.rs (L233-233)
```rust
            mempool_executed_txn_timeout_ms: 1000,
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L964-971)
```rust
            if let Err(e) = mempool_notifier
                .notify_failed_txn(&txns, user_txn_status)
                .await
            {
                error!(
                    error = ?e, "Failed to notify mempool of rejected txns",
                );
            }
```

**File:** consensus/src/quorum_store/batch_generator.rs (L517-532)
```rust
                        BatchGeneratorCommand::CommitNotification(block_timestamp, batches) => {
                            trace!(
                                "QS: got clean request from execution, block timestamp {}",
                                block_timestamp
                            );
                            // Block timestamp is updated asynchronously, so it may race when it enters state sync.
                            if self.latest_block_timestamp > block_timestamp {
                                continue;
                            }
                            self.latest_block_timestamp = block_timestamp;

                            for (author, batch_id) in batches.iter().map(|b| (b.author(), b.batch_id())) {
                                if self.remove_batch_in_progress(author, batch_id) {
                                    counters::BATCH_IN_PROGRESS_COMMITTED.inc();
                                }
                            }
```

**File:** mempool/src/core_mempool/mempool.rs (L112-133)
```rust
    pub(crate) fn reject_transaction(
        &mut self,
        sender: &AccountAddress,
        replay_protector: ReplayProtector,
        hash: &HashValue,
        reason: &DiscardedVMStatus,
    ) {
        if *reason == DiscardedVMStatus::SEQUENCE_NUMBER_TOO_NEW {
            self.log_reject_transaction(sender, replay_protector, counters::COMMIT_IGNORED_LABEL);
            // Do not remove the transaction from mempool
            return;
        }

        let label = if *reason == DiscardedVMStatus::SEQUENCE_NUMBER_TOO_OLD {
            counters::COMMIT_REJECTED_DUPLICATE_LABEL
        } else {
            counters::COMMIT_REJECTED_LABEL
        };
        self.log_reject_transaction(sender, replay_protector, label);
        self.transactions
            .reject_transaction(sender, replay_protector, hash);
    }
```

**File:** mempool/src/shared_mempool/tasks.rs (L664-665)
```rust
                    let curr_time = aptos_infallible::duration_since_epoch();
                    mempool.gc_by_expiration_time(curr_time);
```
