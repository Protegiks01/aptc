# Audit Report

## Title
DKG Transcript Verification Cost Asymmetry Enabling Validator CPU Exhaustion

## Summary
A malicious validator proposer can include cryptographically invalid but structurally valid DKG transcripts in block proposals, forcing all other validators to perform expensive cryptographic operations (FFT, multi-exponentiation, pairing checks) during verification before detecting invalidity, causing CPU exhaustion.

## Finding Description

The DKG transcript verification process involves multiple expensive cryptographic operations that are performed **after** basic structural checks but **before** detecting cryptographic invalidity. When a block proposal is received, each validator transaction is verified in the consensus layer. [1](#0-0) 

For `DKGResult` transactions, the verification chain is:

1. **Entry Point**: `ValidatorTransaction::verify()` deserializes the transcript bytes and calls `RealDKG::verify_transcript_extra()` [2](#0-1) 

2. **Transcript Verification**: Performs basic checks (dealer indices, addresses) then calls the expensive PVSS `verify()` method [3](#0-2) 

3. **PVSS Verification**: Executes three computationally expensive operations:
   - **Batch BLS signature verification** (elliptic curve operations)
   - **Low-degree test** using FFT (O(n log n) complexity)
   - **Multi-pairing check** (most expensive - multiple bilinear pairings) [4](#0-3) 

The low-degree test performs FFT on the polynomial coefficients: [5](#0-4) 

**Attack Scenario**: A malicious validator can craft DKG transcripts with valid BCS serialization and valid elliptic curve points (passing deserialization) but with cryptographically incorrect relationships. When included in their block proposal, all validators must execute expensive verification operations before detecting the cryptographic invalidity.

**Cost Asymmetry**:
- **Attacker cost**: Serialize random valid elliptic curve points (cheap)
- **Defender cost**: FFT + multi-exponentiation + multiple pairings (very expensive)

## Impact Explanation

This is a **Medium severity** DoS vulnerability under the Aptos bug bounty criteria ("Validator node slowdowns"). While it causes computational resource exhaustion on all validators during proposal processing, the impact is constrained by:

1. **Per-block limits**: Default configuration allows only 2 validator transactions per block [6](#0-5) 

2. **Proposal slot limitation**: Attacker can only include invalid transcripts when they are the designated proposer
3. **Block rejection**: Invalid proposals are rejected and don't affect consensus safety
4. **Detection potential**: Repeated invalid proposals from the same validator are observable

The vulnerability breaks the **Resource Limits** invariant (all operations must respect computational limits) by allowing forced execution of expensive cryptographic operations on invalid inputs.

## Likelihood Explanation

**Moderate likelihood** for the following reasons:

- Requires the attacker to be a validator (high barrier to entry - requires stake)
- Can only be exploited during the attacker's proposal slots (limited frequency based on validator set size)
- Each attack opportunity is limited to 2 invalid transcripts (default configuration)
- Repeated abuse would damage the validator's reputation and could lead to stake slashing

However, even a single malicious validator in a large validator set can periodically cause CPU spikes on all other validators, potentially degrading network performance during critical periods.

## Recommendation

Implement **early cryptographic validation** before expensive operations or add **proof-of-work** requirements for validator transaction submission:

**Option 1: Size-based early rejection**
Add checks on transcript structure sizes before expensive verification to reject obviously malformed transcripts.

**Option 2: Incremental verification with caching**
Cache verification results by transcript hash to avoid re-verifying identical invalid transcripts across proposals.

**Option 3: Reputation-based rate limiting**
Track validators who submit invalid validator transactions and apply stricter limits or temporary exclusion from proposal rights.

**Option 4: Lightweight pre-verification**
Add a fast cryptographic check (e.g., hash-based commitment verification) before full PVSS verification to detect most invalid transcripts early.

## Proof of Concept

```rust
// Reproduction steps:

// 1. Create a malicious validator node that modifies proposal generation
// 2. Generate an invalid DKG transcript with valid structure but invalid crypto:

use aptos_types::dkg::{DKGTranscript, DKGTranscriptMetadata};
use aptos_types::validator_txn::ValidatorTransaction;

// Create structurally valid but cryptographically invalid transcript
let invalid_transcript = {
    let metadata = DKGTranscriptMetadata {
        epoch: current_epoch,
        author: malicious_validator_address,
    };
    
    // Serialize random valid elliptic curve points (parseable but cryptographically incorrect)
    let fake_transcripts = Transcripts {
        main: generate_random_weighted_transcript(), // Random but valid curve points
        fast: Some(generate_random_weighted_transcript()),
    };
    
    DKGTranscript {
        metadata,
        transcript_bytes: bcs::to_bytes(&fake_transcripts).unwrap(),
    }
};

// 3. When proposing, include in block via modified vtxn pool
vtxn_pool.put(Topic::DKG, Arc::new(ValidatorTransaction::DKGResult(invalid_transcript)), None);

// 4. All other validators will:
//    - Receive proposal
//    - Call process_proposal() 
//    - Execute expensive verify() on invalid transcript
//    - Waste CPU on FFT, multi-exp, pairings
//    - Eventually detect invalidity and reject block

// 5. Measure CPU time spent on verification using profiling tools
// Expected: >100ms per invalid transcript on commodity hardware
// With 2 invalid transcripts per block, ~200ms CPU waste per malicious proposal
```

**Note**: This vulnerability requires validator access, which limits its exploitability per the Aptos threat model. However, Byzantine validators (up to 1/3) are within BFT consensus assumptions.

### Citations

**File:** consensus/src/round_manager.rs (L1126-1136)
```rust
        if let Some(vtxns) = proposal.validator_txns() {
            for vtxn in vtxns {
                let vtxn_type_name = vtxn.type_name();
                ensure!(
                    is_vtxn_expected(&self.randomness_config, &self.jwk_consensus_config, vtxn),
                    "unexpected validator txn: {:?}",
                    vtxn_type_name
                );
                vtxn.verify(self.epoch_state.verifier.as_ref())
                    .context(format!("{} verify failed", vtxn_type_name))?;
            }
```

**File:** types/src/dkg/mod.rs (L83-87)
```rust
    pub(crate) fn verify(&self, verifier: &ValidatorVerifier) -> Result<()> {
        let transcripts: Transcripts = bcs::from_bytes(&self.transcript_bytes)
            .context("Transcripts deserialization failed")?;
        RealDKG::verify_transcript_extra(&transcripts, verifier, true, None)
    }
```

**File:** types/src/dkg/real_dkg/mod.rs (L332-374)
```rust
    fn verify_transcript(
        params: &Self::PublicParams,
        trx: &Self::Transcript,
    ) -> anyhow::Result<()> {
        // Verify dealer indices are valid.
        let dealers = trx
            .main
            .get_dealers()
            .iter()
            .map(|player| player.id)
            .collect::<Vec<usize>>();
        let num_validators = params.session_metadata.dealer_validator_set.len();
        ensure!(
            dealers.iter().all(|id| *id < num_validators),
            "real_dkg::verify_transcript failed with invalid dealer index."
        );

        let all_eks = params.pvss_config.eks.clone();

        let addresses = params.verifier.get_ordered_account_addresses();
        let dealers_addresses = dealers
            .iter()
            .filter_map(|&pos| addresses.get(pos))
            .cloned()
            .collect::<Vec<_>>();

        let spks = dealers_addresses
            .iter()
            .filter_map(|author| params.verifier.get_public_key(author))
            .collect::<Vec<_>>();

        let aux = dealers_addresses
            .iter()
            .map(|address| (params.pvss_config.epoch, address))
            .collect::<Vec<_>>();

        trx.main.verify(
            &params.pvss_config.wconfig,
            &params.pvss_config.pp,
            &spks,
            &all_eks,
            &aux,
        )?;
```

**File:** crates/aptos-dkg/src/pvss/das/weighted_protocol.rs (L280-377)
```rust
    fn verify<A: Serialize + Clone>(
        &self,
        sc: &<Self as traits::Transcript>::SecretSharingConfig,
        pp: &Self::PublicParameters,
        spks: &[Self::SigningPubKey],
        eks: &[Self::EncryptPubKey],
        auxs: &[A],
    ) -> anyhow::Result<()> {
        self.check_sizes(sc)?;
        let n = sc.get_total_num_players();
        if eks.len() != n {
            bail!("Expected {} encryption keys, but got {}", n, eks.len());
        }
        let W = sc.get_total_weight();

        // Deriving challenges by flipping coins: less complex to implement & less likely to get wrong. Creates bad RNG risks but we deem that acceptable.
        let mut rng = rand::thread_rng();
        let extra = random_scalars(2 + W * 3, &mut rng);

        let sok_vrfy_challenge = &extra[W * 3 + 1];
        let g_2 = pp.get_commitment_base();
        let g_1 = pp.get_encryption_public_params().pubkey_base();
        batch_verify_soks::<G1Projective, A>(
            self.soks.as_slice(),
            g_1,
            &self.V[W],
            spks,
            auxs,
            sok_vrfy_challenge,
        )?;

        let ldt = LowDegreeTest::random(
            &mut rng,
            sc.get_threshold_weight(),
            W + 1,
            true,
            sc.get_batch_evaluation_domain(),
        );
        ldt.low_degree_test_on_g1(&self.V)?;

        //
        // Correctness of encryptions check
        //

        let alphas_betas_and_gammas = &extra[0..W * 3 + 1];
        let (alphas_and_betas, gammas) = alphas_betas_and_gammas.split_at(2 * W + 1);
        let (alphas, betas) = alphas_and_betas.split_at(W + 1);
        assert_eq!(alphas.len(), W + 1);
        assert_eq!(betas.len(), W);
        assert_eq!(gammas.len(), W);

        let lc_VR_hat = G2Projective::multi_exp_iter(
            self.V_hat.iter().chain(self.R_hat.iter()),
            alphas_and_betas.iter(),
        );
        let lc_VRC = G1Projective::multi_exp_iter(
            self.V.iter().chain(self.R.iter()).chain(self.C.iter()),
            alphas_betas_and_gammas.iter(),
        );
        let lc_V_hat = G2Projective::multi_exp_iter(self.V_hat.iter().take(W), gammas.iter());
        let mut lc_R_hat = Vec::with_capacity(n);

        for i in 0..n {
            let p = sc.get_player(i);
            let weight = sc.get_player_weight(&p);
            let s_i = sc.get_player_starting_index(&p);

            lc_R_hat.push(g2_multi_exp(
                &self.R_hat[s_i..s_i + weight],
                &gammas[s_i..s_i + weight],
            ));
        }

        let h = pp.get_encryption_public_params().message_base();
        let g_2_neg = g_2.neg();
        let eks = eks
            .iter()
            .map(Into::<G1Projective>::into)
            .collect::<Vec<G1Projective>>();
        // The vector of left-hand-side ($\mathbb{G}_2$) inputs to each pairing in the multi-pairing.
        let lhs = [g_1, &lc_VRC, h].into_iter().chain(&eks);
        // The vector of right-hand-side ($\mathbb{G}_2$) inputs to each pairing in the multi-pairing.
        let rhs = [&lc_VR_hat, &g_2_neg, &lc_V_hat]
            .into_iter()
            .chain(&lc_R_hat);

        let res = multi_pairing(lhs, rhs);
        if res != Gt::identity() {
            bail!(
                "Expected zero during multi-pairing check for {} {}, but got {}",
                sc,
                <Self as traits::Transcript>::scheme_name(),
                res
            );
        }

        return Ok(());
    }
```

**File:** crates/aptos-dkg/src/pvss/low_degree_test.rs (L195-236)
```rust
    pub fn dual_code_word(self) -> Vec<Scalar> {
        // println!("dual_code_word   > t: {t}, n: {n}, includes_zero: {includes_zero}");

        // Accounts for the size of `f` being the `n` evaluations of f(X) at the roots-of-unity and f(0)
        // when `include_zero` is true.
        let fft_size = if self.includes_zero {
            self.n - 1
        } else {
            self.n
        };
        let f_0 = self.f[0];

        // Compute $f(\omega^i)$ for all $i \in [0, n)$
        let dom = self.batch_dom.get_subdomain(fft_size);
        let mut f_evals = self.f;
        fft_assign(&mut f_evals, &dom);
        f_evals.truncate(fft_size);

        let v = all_lagrange_denominators(&self.batch_dom, fft_size, self.includes_zero);

        // Append f(0), if `include_zero` is true
        let mut extra = Vec::with_capacity(1);
        if self.includes_zero {
            extra.push(f_0);
        }

        // println!(
        //     "|v| = {}, |f_evals| = {}, |extra| = {}",
        //     v.len(),
        //     f_evals.len(),
        //     extra.len()
        // );

        // Compute $v_i f(\omega^i), \forall i \in [0, n)$, and $v_n f(0)$ if `include_zero` is true.
        debug_assert_eq!(f_evals.len() + extra.len(), v.len());
        f_evals
            .iter()
            .chain(extra.iter())
            .zip(v.iter())
            .map(|(v, f)| v.mul(f))
            .collect::<Vec<Scalar>>()
    }
```

**File:** types/src/on_chain_config/consensus_config.rs (L125-144)
```rust
const VTXN_CONFIG_PER_BLOCK_LIMIT_TXN_COUNT_DEFAULT: u64 = 2;
const VTXN_CONFIG_PER_BLOCK_LIMIT_TOTAL_BYTES_DEFAULT: u64 = 2097152; //2MB

#[derive(Clone, Debug, Deserialize, PartialEq, Eq, Serialize)]
pub enum ValidatorTxnConfig {
    /// Disabled. In Jolteon, it also means to not use `BlockType::ProposalExt`.
    V0,
    /// Enabled. Per-block vtxn count and their total bytes are limited.
    V1 {
        per_block_limit_txn_count: u64,
        per_block_limit_total_bytes: u64,
    },
}

impl ValidatorTxnConfig {
    pub fn default_for_genesis() -> Self {
        Self::V1 {
            per_block_limit_txn_count: VTXN_CONFIG_PER_BLOCK_LIMIT_TXN_COUNT_DEFAULT,
            per_block_limit_total_bytes: VTXN_CONFIG_PER_BLOCK_LIMIT_TOTAL_BYTES_DEFAULT,
        }
```
