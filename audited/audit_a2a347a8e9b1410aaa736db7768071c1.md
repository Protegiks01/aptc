# Audit Report

## Title
Blocking Thread Pool Exhaustion in Backup Service via Dropped JoinHandle

## Summary
The backup service's `reply_with_bytes_sender()` function spawns blocking tasks using `tokio::task::spawn_blocking()` but immediately drops the returned `JoinHandle`, creating an unbounded queue of long-running database operations. An attacker with network access to the backup service can exhaust the limited 64-thread blocking pool, causing denial of service for legitimate backup operations. [1](#0-0) 

## Finding Description
The backup service exposes four HTTP endpoints that use `reply_with_bytes_sender()`: state snapshots, state snapshot chunks, epoch ending ledger infos, and transactions. Each invocation spawns a blocking task to perform potentially long-running database operations (iterating millions of records, disk I/O, serialization). [2](#0-1) 

The critical flaw is at line 58 where the JoinHandle is prefixed with `_` and immediately dropped. While tokio's `spawn_blocking` returns immediately without blocking the caller, dropped tasks continue executing and occupy blocking thread slots until completion. [3](#0-2) 

The backup service runtime has a hardcoded limit of 64 blocking threads, explicitly set to prevent REST API overload: [4](#0-3) 

The backup service is initialized with this limited runtime: [5](#0-4) 

**Attack Vector:**
1. Attacker sends 100+ rapid requests to `/state_snapshot/<version>` or `/transactions/<start>/<large_count>`
2. Each request spawns a blocking task that performs expensive operations via BackupHandler iterators
3. With only 64 threads available, the 65th request and beyond queue indefinitely
4. Long-running database iterations (potentially millions of records) keep threads occupied for seconds/minutes
5. Even if clients disconnect, tasks continue processing records and serializing data until the first buffer flush (10KB) [6](#0-5) [7](#0-6) 

The backup service defaults to localhost binding but can be configured to expose externally: [8](#0-7) [9](#0-8) 

## Impact Explanation
This vulnerability qualifies as **High Severity** under Aptos bug bounty criteria: "API crashes" and "Validator node slowdowns."

**Direct Impact:**
- Backup service becomes unresponsive when blocking thread pool is saturated
- Legitimate backup operations (critical for disaster recovery) are delayed or fail
- Memory consumption from unbounded task queue growth

**Operational Impact:**
- Backup coordinator cannot continuously backup blockchain data
- Node operators lose disaster recovery capability
- Cannot bootstrap new nodes from backups during attacks

While the backup service uses a separate runtime, prolonged attacks could impact overall node performance through CPU/memory contention, especially since backup operations involve heavy disk I/O and serialization.

## Likelihood Explanation
**Likelihood: Medium-High** depending on configuration.

**Attack Requirements:**
- Network access to backup service endpoint (default: localhost-only, but configurable)
- Ability to send HTTP GET requests
- No authentication required

**Default Configuration:** Low likelihood - service binds to `127.0.0.1:6186`, limiting exposure to local processes only.

**Production Configuration:** High likelihood - operators may expose backup service on internal networks or to specific backup coordinators, making it accessible to network attackers or compromised services. The documentation shows backup coordinators routinely access this service.

**Exploitation Complexity:** Trivial - simple HTTP requests with no special crafting required.

## Recommendation

**Immediate Fix:** Implement bounded concurrency control using a semaphore before spawning blocking tasks. Replace direct `tokio::task::spawn_blocking` with a `BoundedExecutor` pattern already used elsewhere in the codebase: [10](#0-9) 

**Recommended Solution:**
```rust
// In handlers/utils.rs
pub(super) fn reply_with_bytes_sender<F>(
    backup_handler: &BackupHandler,
    endpoint: &'static str,
    f: F,
    bounded_executor: &BoundedExecutor, // Add this parameter
) -> Box<dyn Reply>
where
    F: FnOnce(BackupHandler, &mut bytes_sender::BytesSender) -> DbResult<()> + Send + 'static,
{
    let (sender, stream) = bytes_sender::BytesSender::new(endpoint);
    
    let bh = backup_handler.clone();
    let endpoint_clone = endpoint;
    
    // Use bounded executor instead of direct spawn_blocking
    let _join_handle = bounded_executor.spawn_blocking(move || {
        let _timer = BACKUP_TIMER.timer_with(&[&format!("backup_service_bytes_sender_{}", endpoint_clone)]);
        abort_on_error(f)(bh, sender)
    });
    
    Box::new(Response::new(Body::wrap_stream(stream)))
}
```

**Additional Mitigations:**
1. Add authentication to backup service endpoints
2. Implement per-client rate limiting
3. Add request timeout mechanism
4. Monitor blocking thread pool utilization via metrics
5. Document security implications of external backup service exposure

## Proof of Concept

```rust
// Test demonstrating thread pool exhaustion
#[tokio::test]
async fn test_backup_service_thread_pool_exhaustion() {
    use reqwest::Client;
    use std::time::Duration;
    
    // Setup: Start backup service on test node
    let tmpdir = TempPath::new();
    let db = Arc::new(AptosDB::new_for_test(&tmpdir));
    let port = get_available_port();
    let addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), port);
    let _rt = start_backup_service(addr, db);
    
    let client = Client::new();
    let base_url = format!("http://127.0.0.1:{}", port);
    
    // Attack: Send 100 concurrent requests to state_snapshot endpoint
    // This will spawn 100 blocking tasks, but only 64 can execute
    let mut handles = vec![];
    for i in 0..100 {
        let url = format!("{}/state_snapshot/{}", base_url, i);
        let client_clone = client.clone();
        let handle = tokio::spawn(async move {
            let start = std::time::Instant::now();
            let result = client_clone.get(&url).send().await;
            (start.elapsed(), result.is_ok())
        });
        handles.push(handle);
    }
    
    // Wait for all requests
    let results: Vec<_> = futures::future::join_all(handles).await;
    
    // Verify: Later requests should be significantly delayed
    let timings: Vec<_> = results.iter().map(|r| r.as_ref().unwrap().0).collect();
    let first_64_avg = timings[..64].iter().sum::<Duration>() / 64;
    let remaining_avg = timings[64..].iter().sum::<Duration>() / (timings.len() - 64) as u32;
    
    // Expect significant delay for queued requests
    assert!(remaining_avg > first_64_avg * 2, 
        "Queued requests should show significant delay due to thread pool saturation");
    
    // Now try a legitimate backup request - should be delayed
    let legitimate_start = std::time::Instant::now();
    let _ = client.get(format!("{}/db_state", base_url)).send().await;
    assert!(legitimate_start.elapsed() > Duration::from_secs(1),
        "Legitimate requests are delayed due to saturated thread pool");
}
```

## Notes

**Default Configuration Safety:** The default `127.0.0.1:6186` binding provides significant protection, limiting this vulnerability to local attackers or SSRF scenarios. However, the documentation explicitly shows this service is meant to be accessed by backup coordinators, suggesting production deployments may expose it more broadly.

**Architectural Context:** While the backup service uses a separate runtime, it runs in the same process as the validator node. Sustained attacks could degrade overall node performance through resource contention (CPU, memory, disk I/O), though the impact is less severe than if it shared the main validator runtime's thread pool.

**Existing Patterns:** The codebase already has `BoundedExecutor` infrastructure for exactly this type of concurrency control. Other services like peer-monitoring-service use it successfully, making this a straightforward fix with existing primitives.

### Citations

**File:** storage/backup/backup-service/src/handlers/utils.rs (L46-65)
```rust
pub(super) fn reply_with_bytes_sender<F>(
    backup_handler: &BackupHandler,
    endpoint: &'static str,
    f: F,
) -> Box<dyn Reply>
where
    F: FnOnce(BackupHandler, &mut bytes_sender::BytesSender) -> DbResult<()> + Send + 'static,
{
    let (sender, stream) = bytes_sender::BytesSender::new(endpoint);

    // spawn and forget, error propagates through the `stream: TryStream<_>`
    let bh = backup_handler.clone();
    let _join_handle = tokio::task::spawn_blocking(move || {
        let _timer =
            BACKUP_TIMER.timer_with(&[&format!("backup_service_bytes_sender_{}", endpoint)]);
        abort_on_error(f)(bh, sender)
    });

    Box::new(Response::new(Body::wrap_stream(stream)))
}
```

**File:** storage/backup/backup-service/src/handlers/mod.rs (L47-110)
```rust
    // GET state_snapshot/<version>
    let bh = backup_handler.clone();
    let state_snapshot = warp::path!(Version)
        .map(move |version| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT, move |bh, sender| {
                bh.get_state_item_iter(version, 0, usize::MAX)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET state_item_count/<version>
    let bh = backup_handler.clone();
    let state_item_count = warp::path!(Version)
        .map(move |version| {
            reply_with_bcs_bytes(
                STATE_ITEM_COUNT,
                &(bh.get_state_item_count(version)? as u64),
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_snapshot_chunk/<version>/<start_idx>/<limit>
    let bh = backup_handler.clone();
    let state_snapshot_chunk = warp::path!(Version / usize / usize)
        .map(move |version, start_idx, limit| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT_CHUNK, move |bh, sender| {
                bh.get_state_item_iter(version, start_idx, limit)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET state_root_proof/<version>
    let bh = backup_handler.clone();
    let state_root_proof = warp::path!(Version)
        .map(move |version| {
            reply_with_bcs_bytes(STATE_ROOT_PROOF, &bh.get_state_root_proof(version)?)
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET epoch_ending_ledger_infos/<start_epoch>/<end_epoch>/
    let bh = backup_handler.clone();
    let epoch_ending_ledger_infos = warp::path!(u64 / u64)
        .map(move |start_epoch, end_epoch| {
            reply_with_bytes_sender(&bh, EPOCH_ENDING_LEDGER_INFOS, move |bh, sender| {
                bh.get_epoch_ending_ledger_info_iter(start_epoch, end_epoch)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET transactions/<start_version>/<num_transactions>
    let bh = backup_handler.clone();
    let transactions = warp::path!(Version / usize)
        .map(move |start_version, num_transactions| {
            reply_with_bytes_sender(&bh, TRANSACTIONS, move |bh, sender| {
                bh.get_transaction_iter(start_version, num_transactions)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-50)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;

    // Verify the given name has an appropriate length
    if thread_name.len() > MAX_THREAD_NAME_LENGTH {
        panic!(
            "The given runtime thread name is too long! Max length: {}, given name: {}",
            MAX_THREAD_NAME_LENGTH, thread_name
        );
    }

    // Create the runtime builder
    let atomic_id = AtomicUsize::new(0);
    let thread_name_clone = thread_name.clone();
    let mut builder = Builder::new_multi_thread();
    builder
        .thread_name_fn(move || {
            let id = atomic_id.fetch_add(1, Ordering::SeqCst);
            format!("{}-{}", thread_name_clone, id)
        })
        .on_thread_start(on_thread_start)
        .disable_lifo_slot()
        // Limit concurrent blocking tasks from spawn_blocking(), in case, for example, too many
        // Rest API calls overwhelm the node.
        .max_blocking_threads(MAX_BLOCKING_THREADS)
```

**File:** storage/backup/backup-service/src/lib.rs (L12-30)
```rust
pub fn start_backup_service(address: SocketAddr, db: Arc<AptosDB>) -> Runtime {
    let backup_handler = db.get_backup_handler();
    let routes = get_routes(backup_handler);

    let runtime = aptos_runtimes::spawn_named_runtime("backup".into(), None);

    // Ensure that we actually bind to the socket first before spawning the
    // server tasks. This helps in tests to prevent races where a client attempts
    // to make a request before the server task is actually listening on the
    // socket.
    //
    // Note: we need to enter the runtime context first to actually bind, since
    //       tokio TcpListener can only be bound inside a tokio context.
    let _guard = runtime.enter();
    let server = warp::serve(routes).bind(address);
    runtime.handle().spawn(server);
    info!("Backup service spawned.");
    runtime
}
```

**File:** storage/backup/backup-service/src/handlers/bytes_sender.rs (L21-26)
```rust
impl BytesSender {
    const MAX_BATCHES: usize = 100;
    #[cfg(not(test))]
    const TARGET_BATCH_SIZE: usize = 10 * 1024;
    #[cfg(test)]
    const TARGET_BATCH_SIZE: usize = 10;
```

**File:** storage/aptosdb/src/backup/backup_handler.rs (L41-109)
```rust
    pub fn get_transaction_iter(
        &self,
        start_version: Version,
        num_transactions: usize,
    ) -> Result<
        impl Iterator<
                Item = Result<(
                    Transaction,
                    PersistedAuxiliaryInfo,
                    TransactionInfo,
                    Vec<ContractEvent>,
                    WriteSet,
                )>,
            > + '_,
    > {
        let txn_iter = self
            .ledger_db
            .transaction_db()
            .get_transaction_iter(start_version, num_transactions)?;
        let mut txn_info_iter = self
            .ledger_db
            .transaction_info_db()
            .get_transaction_info_iter(start_version, num_transactions)?;
        let mut event_vec_iter = self
            .ledger_db
            .event_db()
            .get_events_by_version_iter(start_version, num_transactions)?;
        let mut write_set_iter = self
            .ledger_db
            .write_set_db()
            .get_write_set_iter(start_version, num_transactions)?;
        let mut persisted_aux_info_iter = self
            .ledger_db
            .persisted_auxiliary_info_db()
            .get_persisted_auxiliary_info_iter(start_version, num_transactions)?;

        let zipped = txn_iter.enumerate().map(move |(idx, txn_res)| {
            let version = start_version + idx as u64; // overflow is impossible since it's check upon txn_iter construction.

            let txn = txn_res?;
            let txn_info = txn_info_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "TransactionInfo not found when Transaction exists, version {}",
                    version
                ))
            })??;
            let event_vec = event_vec_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "Events not found when Transaction exists., version {}",
                    version
                ))
            })??;
            let write_set = write_set_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "WriteSet not found when Transaction exists, version {}",
                    version
                ))
            })??;
            let persisted_aux_info = persisted_aux_info_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "PersistedAuxiliaryInfo not found when Transaction exists, version {}",
                    version
                ))
            })??;
            BACKUP_TXN_VERSION.set(version as i64);
            Ok((txn, persisted_aux_info, txn_info, event_vec, write_set))
        });
        Ok(zipped)
    }
```

**File:** config/src/config/storage_config.rs (L433-436)
```rust
impl Default for StorageConfig {
    fn default() -> StorageConfig {
        StorageConfig {
            backup_service_address: SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 6186),
```

**File:** storage/README.md (L64-66)
```markdown
  # Address the backup service listens on. By default the port is open to only
  # the localhost, so the backup cli tool can only access data in the same host.
  backup_service_address: "127.0.0.1:6186"
```

**File:** crates/bounded-executor/src/executor.rs (L1-80)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

//! A bounded tokio [`Handle`]. Only a bounded number of tasks can run
//! concurrently when spawned through this executor, defined by the initial
//! `capacity`.

use futures::future::{Future, FutureExt};
use std::sync::Arc;
use tokio::{
    runtime::Handle,
    sync::{OwnedSemaphorePermit, Semaphore},
    task::JoinHandle,
};

#[derive(Clone, Debug)]
pub struct BoundedExecutor {
    semaphore: Arc<Semaphore>,
    executor: Handle,
}

impl BoundedExecutor {
    /// Create a new `BoundedExecutor` from an existing tokio [`Handle`]
    /// with a maximum concurrent task capacity of `capacity`.
    pub fn new(capacity: usize, executor: Handle) -> Self {
        let semaphore = Arc::new(Semaphore::new(capacity));
        Self {
            semaphore,
            executor,
        }
    }

    async fn acquire_permit(&self) -> OwnedSemaphorePermit {
        self.semaphore.clone().acquire_owned().await.unwrap()
    }

    fn try_acquire_permit(&self) -> Option<OwnedSemaphorePermit> {
        self.semaphore.clone().try_acquire_owned().ok()
    }

    /// Spawn a [`Future`] on the `BoundedExecutor`. This function is async and
    /// will block if the executor is at capacity until one of the other spawned
    /// futures completes. This function returns a [`JoinHandle`] that the caller
    /// can `.await` on for the results of the [`Future`].
    pub async fn spawn<F>(&self, future: F) -> JoinHandle<F::Output>
    where
        F: Future + Send + 'static,
        F::Output: Send + 'static,
    {
        let permit = self.acquire_permit().await;
        self.executor.spawn(future_with_permit(future, permit))
    }

    /// Try to spawn a [`Future`] on the `BoundedExecutor`. If the `BoundedExecutor`
    /// is at capacity, this will return an `Err(F)`, passing back the future the
    /// caller attempted to spawn. Otherwise, this will spawn the future on the
    /// executor and send back a [`JoinHandle`] that the caller can `.await` on
    /// for the results of the [`Future`].
    pub fn try_spawn<F>(&self, future: F) -> Result<JoinHandle<F::Output>, F>
    where
        F: Future + Send + 'static,
        F::Output: Send + 'static,
    {
        match self.try_acquire_permit() {
            Some(permit) => Ok(self.executor.spawn(future_with_permit(future, permit))),
            None => Err(future),
        }
    }

    /// Like [`BoundedExecutor::spawn`] but spawns the given closure onto a
    /// blocking task (see [`tokio::task::spawn_blocking`] for details).
    pub async fn spawn_blocking<F, R>(&self, func: F) -> JoinHandle<R>
    where
        F: FnOnce() -> R + Send + 'static,
        R: Send + 'static,
    {
        let permit = self.acquire_permit().await;
        self.executor
            .spawn_blocking(function_with_permit(func, permit))
    }
```
