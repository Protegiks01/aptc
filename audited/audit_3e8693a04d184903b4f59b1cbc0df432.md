# Audit Report

## Title
Missing Bounds Validation in DKG Secret Share Decryption Causes Validator Node Panics

## Summary
The `try_get_rand_config_for_new_epoch()` function in `consensus/src/epoch_manager.rs` fails to validate that the validator index (`my_index`) is within bounds of the DKG transcript's weighted configuration before using it for secret share decryption and public key share retrieval. This can cause validator nodes to panic during epoch transitions if there's a mismatch between the new epoch's validator set and the DKG session's target validator set.

## Finding Description
During epoch transitions with randomness enabled, validators must decrypt their secret shares from the completed DKG transcript. The function obtains `my_index` from the new epoch state's validator verifier, but never validates this index against the bounds of the DKG transcript's configuration. [1](#0-0) 

This index is then used in two critical operations without bounds checking:

1. **Public Key Share Retrieval** - The code iterates over all validators in the new epoch state and retrieves public key shares: [2](#0-1) 

2. **Secret Share Decryption** - The index is passed to decrypt the validator's own secret share: [3](#0-2) 

Both operations eventually call functions in the weighted configuration that access arrays using the player ID without bounds checking: [4](#0-3) [5](#0-4) 

If `player.id` (the validator index) is >= `self.weights.len()` (the number of validators in the DKG session), these accesses will **panic** with an out-of-bounds error.

**Root Cause:** There is no explicit validation that:
- `new_epoch_state.verifier.len() == dkg_pub_params.pvss_config.wconfig.get_total_num_players()`
- All validator addresses in the new epoch state exist in the DKG session's target validator set
- The validator ordering matches between the two sets

While validator set changes are blocked during reconfiguration [6](#0-5) , the code provides no defensive validation against potential bugs, race conditions, or edge cases that could cause a mismatch.

## Impact Explanation
**High Severity** - This issue can cause validator node crashes during epoch transitions:

1. **Validator Node Unavailability**: Affected validators will panic and crash when trying to initialize randomness for the new epoch, causing temporary loss of voting power
2. **Consensus Liveness Impact**: If enough validators crash simultaneously, it could delay or prevent epoch transitions
3. **Critical System Operation**: The failure occurs during epoch initialization, a critical phase where consensus must be maintained

The issue does not constitute Critical severity because:
- It does not directly cause consensus safety violations (chain splits)
- It does not result in fund loss or theft
- Recovery is possible by restarting affected nodes (though randomness would remain unavailable)

However, it qualifies as **High severity** per bug bounty criteria as it causes "Validator node slowdowns" and "Significant protocol violations" through unexpected crashes during critical operations.

## Likelihood Explanation
**Medium Likelihood** - While the normal code path should ensure validator sets match, several factors increase the probability:

1. **No Defensive Validation**: The complete absence of bounds checking means any future bug or edge case will cause crashes
2. **Complex Epoch Transition Logic**: The validator set construction involves multiple stages and could have subtle bugs
3. **Historical Changes**: Code changes to validator set management or DKG initialization could introduce mismatches
4. **Non-Determinism Risk**: Any source of non-determinism in validator ordering could trigger the issue

The issue is not "High Likelihood" because validator set changes during reconfiguration are explicitly blocked, making mismatches unlikely under normal operation.

## Recommendation
Add explicit validation that the new epoch's validator set matches the DKG session's target validator set:

```rust
fn try_get_rand_config_for_new_epoch(
    &self,
    consensus_key: Arc<PrivateKey>,
    new_epoch_state: &EpochState,
    onchain_randomness_config: &OnChainRandomnessConfig,
    maybe_dkg_state: anyhow::Result<DKGState>,
    consensus_config: &OnChainConsensusConfig,
) -> Result<(RandConfig, Option<RandConfig>), NoRandomnessReason> {
    // ... existing checks ...
    
    let dkg_pub_params = DefaultDKG::new_public_params(&dkg_session.metadata);
    
    // ADDED: Validate validator set size matches
    let dkg_num_validators = dkg_pub_params.pvss_config.wconfig.get_total_num_players();
    if new_epoch_state.verifier.len() != dkg_num_validators {
        return Err(NoRandomnessReason::ValidatorSetMismatch(format!(
            "New epoch has {} validators but DKG session was for {} validators",
            new_epoch_state.verifier.len(),
            dkg_num_validators
        )));
    }
    
    let my_index = new_epoch_state
        .verifier
        .address_to_validator_index()
        .get(&self.author)
        .copied()
        .ok_or_else(|| NoRandomnessReason::NotInValidatorSet)?;
    
    // ADDED: Validate my_index is within bounds
    if my_index >= dkg_num_validators {
        return Err(NoRandomnessReason::IndexOutOfBounds(format!(
            "Validator index {} is out of bounds for DKG config with {} validators",
            my_index,
            dkg_num_validators
        )));
    }
    
    // ... rest of function ...
}
```

Additionally, add a new error variant to `NoRandomnessReason`:
```rust
pub enum NoRandomnessReason {
    // ... existing variants ...
    ValidatorSetMismatch(String),
    IndexOutOfBounds(String),
}
```

## Proof of Concept
The following demonstrates the panic scenario:

```rust
// Scenario: DKG session created for 10 validators, but epoch state has 15 validators
// (e.g., due to a bug in validator set construction)

// In try_get_rand_config_for_new_epoch():
// 1. dkg_pub_params created from DKG session with target_validator_set of 10 validators
// 2. new_epoch_state.verifier has 15 validators
// 3. Code iterates from 0..15 to get pk_shares

let pk_shares = (0..new_epoch_state.verifier.len())  // 0..15
    .map(|id| {
        transcript
            .main
            .get_public_key_share(&dkg_pub_params.pvss_config.wconfig, &Player { id })
            // When id >= 10, this calls:
            // sc.get_player_weight(player) where player.id = 10, 11, 12, 13, or 14
            // Which accesses: self.weights[10..14]
            // But weights.len() == 10
            // PANIC: index out of bounds
    })
    .collect::<Vec<_>>();

// Thread will panic with: "index out of bounds: the len is 10 but the index is 10"
// Validator node crashes during epoch initialization
```

To reproduce, a test would need to:
1. Create a DKG session with N validators
2. Modify the epoch state to have M > N validators (requires injecting a buggy validator set)
3. Call `try_get_rand_config_for_new_epoch()`
4. Observe the panic at the array access

**Notes:**
- This vulnerability requires no attacker privileges - it's triggered by internal code logic
- The lack of defensive validation means any future bug introducing a validator set mismatch will cause crashes
- The issue is exacerbated by the silent assumption that validator sets will always match without enforcement

### Citations

**File:** consensus/src/epoch_manager.rs (L1047-1052)
```rust
        let my_index = new_epoch_state
            .verifier
            .address_to_validator_index()
            .get(&self.author)
            .copied()
            .ok_or_else(|| NoRandomnessReason::NotInValidatorSet)?;
```

**File:** consensus/src/epoch_manager.rs (L1066-1072)
```rust
        let (sk, pk) = DefaultDKG::decrypt_secret_share_from_transcript(
            &dkg_pub_params,
            &transcript,
            my_index as u64,
            &dkg_decrypt_key,
        )
        .map_err(NoRandomnessReason::SecretShareDecryptionFailed)?;
```

**File:** consensus/src/epoch_manager.rs (L1080-1086)
```rust
        let pk_shares = (0..new_epoch_state.verifier.len())
            .map(|id| {
                transcript
                    .main
                    .get_public_key_share(&dkg_pub_params.pvss_config.wconfig, &Player { id })
            })
            .collect::<Vec<_>>();
```

**File:** crates/aptos-crypto/src/weighted_config.rs (L162-165)
```rust
    /// Returns the weight of a specific player.
    pub fn get_player_weight(&self, player: &Player) -> usize {
        self.weights[player.id]
    }
```

**File:** crates/aptos-dkg/src/pvss/das/weighted_protocol.rs (L202-206)
```rust
        let weight = sc.get_player_weight(player);
        let mut pk_shares = Vec::with_capacity(weight);

        for j in 0..weight {
            let k = sc.get_share_index(player.id, j).unwrap();
```

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L1910-1912)
```text
    fun assert_reconfig_not_in_progress() {
        assert!(!reconfiguration_state::is_in_progress(), error::invalid_state(ERECONFIGURATION_IN_PROGRESS));
    }
```
