# Audit Report

## Title
Peer Reputation System Fails to Distinguish Between Local Network Failures and Peer Misbehavior

## Summary
The `RequestTracker` in the peer monitoring service and the peer scoring system in state-sync data client fail to distinguish between failures attributable to the peer versus failures caused by local network issues, leading to unfair peer reputation penalties and potential network degradation.

## Finding Description

The Aptos peer reputation systems conflate two fundamentally different failure modes when tracking peer reliability:

1. **Request failed to send** - The request couldn't leave the local node due to connection issues (`RpcError::NotConnected`), resource constraints (`RpcError::TooManyPending`), or other local problems
2. **Request sent but peer failed to respond** - The request successfully reached the peer, but the peer timed out or responded incorrectly (`RpcError::TimedOut`, invalid responses)

### Peer Monitoring Service Issue [1](#0-0) 

All errors from the network request are handled uniformly: [2](#0-1) 

Which calls: [3](#0-2) 

The `record_response_failure()` method increments the consecutive failure counter for ALL error types: [4](#0-3) 

The RpcError enum includes errors that are clearly not the peer's fault: [5](#0-4) 

### State-Sync Data Client Issue [6](#0-5) 

Line 865 applies `ErrorType::NotUseful` to ALL RPC errors, including `NotConnected` which is not the peer's fault. This multiplies the peer's score by 0.95: [7](#0-6) 

Peers below the ignore threshold cannot serve requests: [8](#0-7) 

## Impact Explanation

**Medium Severity** - This issue causes state inconsistencies and availability degradation:

1. **Unfair Peer Penalization**: Legitimate peers are blamed for local connection issues, causing their reputation scores to decrease unfairly
2. **State-Sync Degradation**: In the data client, after ~47 consecutive `NotConnected` errors, a peer drops below `IGNORE_PEER_THRESHOLD` (25.0) and becomes ignored for data requests, slowing state synchronization
3. **Future Disconnect Risk**: The peer monitoring service has a TODO indicating disconnect functionality will be implemented, which would make this issue more severe
4. **Network Fragmentation Potential**: Under sustained local network issues, a node could unfairly mark all its peers as unreliable, leading to self-isolation

While this doesn't directly cause consensus violations or fund loss, it creates state inconsistencies requiring intervention (Medium severity per bounty criteria) and could contribute to network liveness issues.

## Likelihood Explanation

**Medium Likelihood**: This occurs naturally in production environments where:
- Nodes experience temporary network partitions
- Firewall rules temporarily block connections
- Network infrastructure issues cause connection drops
- Connection pool exhaustion triggers `TooManyPending` errors

The issue doesn't require malicious intent - it happens organically during normal network disruptions. However, an attacker who can disrupt local network connectivity could amplify this to cause faster peer score degradation.

## Recommendation

Distinguish between failure types and only penalize peers for failures attributable to them:

**For Peer Monitoring Service:**
```rust
pub fn record_response_failure(&mut self, is_peer_fault: bool) {
    // Only increment if it's the peer's fault
    if is_peer_fault {
        self.num_consecutive_request_failures += 1;
    }
}
```

Modify error handling to categorize errors:
```rust
Err(error) => {
    let is_peer_fault = match &error {
        Error::RpcError(rpc_err) => matches!(
            rpc_err,
            RpcError::TimedOut | 
            RpcError::InvalidRpcResponse | 
            RpcError::ApplicationError(_)
        ),
        _ => true, // Default to peer fault for unknown errors
    };
    peer_state_value
        .write()
        .handle_monitoring_service_response_error(&peer_network_id, error, is_peer_fault);
    return;
}
```

**For State-Sync Data Client:**
```rust
let error_type = match error {
    aptos_storage_service_client::Error::RpcError(rpc_error) => match rpc_error {
        RpcError::NotConnected(_) | 
        RpcError::TooManyPending(_) => {
            // Don't penalize peer for local issues
            return Err(Error::DataIsUnavailable(rpc_error.to_string()));
        },
        RpcError::TimedOut => ErrorType::NotUseful,
        RpcError::InvalidRpcResponse => ErrorType::Malicious,
        _ => ErrorType::NotUseful,
    },
    ...
};
self.notify_bad_response(id, peer, &request, error_type);
```

## Proof of Concept

```rust
// Test demonstrating unfair peer penalization
#[tokio::test]
async fn test_peer_penalized_for_local_network_issue() {
    // Setup peer monitoring with a peer
    let config = PeerMonitoringServiceConfig::default();
    let time_service = TimeService::mock();
    let mut request_tracker = RequestTracker::new(
        config.latency_monitoring.latency_ping_interval_ms,
        time_service,
    );
    
    // Simulate 3 consecutive NotConnected errors (not peer's fault)
    for _ in 0..3 {
        request_tracker.request_started();
        request_tracker.request_completed();
        // In real code, RpcError::NotConnected would trigger this
        request_tracker.record_response_failure();
    }
    
    // Assert: Peer is unfairly marked with 3 consecutive failures
    assert_eq!(request_tracker.get_num_consecutive_failures(), 3);
    // This would trigger disconnect warning despite peer not being at fault
}

#[tokio::test]
async fn test_state_sync_peer_ignored_for_connection_issues() {
    let config = Arc::new(AptosDataClientConfig::default());
    let peer_states = PeerStates::new(config.clone());
    let peer = PeerNetworkId::random();
    
    // Simulate 47 NotConnected errors (not peer's fault)
    for _ in 0..47 {
        peer_states.update_score_error(peer, ErrorType::NotUseful);
    }
    
    let peer_state = peer_states.get_peer_state(&peer).unwrap();
    // Assert: Legitimate peer is now ignored due to local connection issues
    assert!(peer_state.get_score() < IGNORE_PEER_THRESHOLD);
}
```

## Notes

This vulnerability represents a semantic gap in failure attribution that becomes exploitable under network disruption scenarios. While the peer monitoring service currently only logs warnings, the state-sync impact is real and measurable. The issue should be addressed before the disconnect functionality (noted in TODO comment) is implemented to prevent unfair peer disconnections.

### Citations

**File:** peer-monitoring-service/client/src/peer_states/peer_state.rs (L123-131)
```rust
            // Process any response errors
            let monitoring_service_response = match monitoring_service_response {
                Ok(monitoring_service_response) => monitoring_service_response,
                Err(error) => {
                    peer_state_value
                        .write()
                        .handle_monitoring_service_response_error(&peer_network_id, error);
                    return;
                },
```

**File:** peer-monitoring-service/client/src/peer_states/latency_info.rs (L60-72)
```rust
    fn handle_request_failure(&self, peer_network_id: &PeerNetworkId) {
        // Update the number of ping failures for the request tracker
        self.request_tracker.write().record_response_failure();

        // TODO: If the number of ping failures is too high, disconnect from the node
        let num_consecutive_failures = self.request_tracker.read().get_num_consecutive_failures();
        if num_consecutive_failures >= self.latency_monitoring_config.max_latency_ping_failures {
            warn!(LogSchema::new(LogEntry::LatencyPing)
                .event(LogEvent::TooManyPingFailures)
                .peer(peer_network_id)
                .message("Too many ping failures occurred for the peer!"));
        }
    }
```

**File:** peer-monitoring-service/client/src/peer_states/latency_info.rs (L197-211)
```rust
    fn handle_monitoring_service_response_error(
        &mut self,
        peer_network_id: &PeerNetworkId,
        error: Error,
    ) {
        // Handle the failure
        self.handle_request_failure(peer_network_id);

        // Log the error
        warn!(LogSchema::new(LogEntry::LatencyPing)
            .event(LogEvent::ResponseError)
            .message("Error encountered when pinging peer!")
            .peer(peer_network_id)
            .error(&error));
    }
```

**File:** peer-monitoring-service/client/src/peer_states/request_tracker.rs (L101-104)
```rust
    /// Records a failure for the request
    pub fn record_response_failure(&mut self) {
        self.num_consecutive_request_failures += 1;
    }
```

**File:** network/framework/src/protocols/rpc/error.rs (L14-44)
```rust
pub enum RpcError {
    #[error("Error: {0:?}")]
    Error(#[from] anyhow::Error),

    #[error("IO error: {0}")]
    IoError(#[from] io::Error),

    #[error("Bcs error: {0:?}")]
    BcsError(#[from] bcs::Error),

    #[error("Not connected with peer: {0}")]
    NotConnected(PeerId),

    #[error("Received invalid rpc response message")]
    InvalidRpcResponse,

    #[error("Application layer unexpectedly dropped response channel")]
    UnexpectedResponseChannelCancel,

    #[error("Error in application layer handling rpc request: {0:?}")]
    ApplicationError(anyhow::Error),

    #[error("Error sending on mpsc channel, connection likely shutting down: {0:?}")]
    MpscSendError(#[from] mpsc::SendError),

    #[error("Too many pending RPCs: {0}")]
    TooManyPending(u32),

    #[error("Rpc timed out")]
    TimedOut,
}
```

**File:** state-sync/aptos-data-client/src/client.rs (L830-868)
```rust
            Err(error) => {
                // Convert network error and storage service error types into
                // data client errors. Also categorize the error type for scoring
                // purposes.
                let client_error = match error {
                    aptos_storage_service_client::Error::RpcError(rpc_error) => match rpc_error {
                        RpcError::NotConnected(_) => {
                            Error::DataIsUnavailable(rpc_error.to_string())
                        },
                        RpcError::TimedOut => {
                            Error::TimeoutWaitingForResponse(rpc_error.to_string())
                        },
                        _ => Error::UnexpectedErrorEncountered(rpc_error.to_string()),
                    },
                    aptos_storage_service_client::Error::StorageServiceError(err) => {
                        Error::UnexpectedErrorEncountered(err.to_string())
                    },
                    _ => Error::UnexpectedErrorEncountered(error.to_string()),
                };

                warn!(
                    (LogSchema::new(LogEntry::StorageServiceResponse)
                        .event(LogEvent::ResponseError)
                        .request_type(&request.get_label())
                        .request_id(id)
                        .peer(&peer)
                        .error(&client_error))
                );

                increment_request_counter(
                    &metrics::ERROR_RESPONSES,
                    client_error.get_label(),
                    peer,
                );

                self.notify_bad_response(id, peer, &request, ErrorType::NotUseful);
                Err(client_error)
            },
        }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L151-160)
```rust
    /// Returns true iff the peer is currently ignored
    fn is_ignored(&self) -> bool {
        // Only ignore peers if the config allows it
        if !self.data_client_config.ignore_low_score_peers {
            return false;
        }

        // Otherwise, ignore peers with a low score
        self.score <= IGNORE_PEER_THRESHOLD
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L167-174)
```rust
    /// Updates the score of the peer according to an error
    fn update_score_error(&mut self, error: ErrorType) {
        let multiplier = match error {
            ErrorType::NotUseful => NOT_USEFUL_MULTIPLIER,
            ErrorType::Malicious => MALICIOUS_MULTIPLIER,
        };
        self.score = f64::max(self.score * multiplier, MIN_SCORE);
    }
```
