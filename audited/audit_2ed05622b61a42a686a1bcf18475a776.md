# Audit Report

## Title
Unbounded Concurrent Batch Fetches Enable Resource Exhaustion DoS

## Summary
A malicious block proposer can include many small batches (with unique digests) in a block proposal, causing receiving validators to spawn thousands of concurrent tokio tasks that exhaust file descriptors, network bandwidth, and system resources, leading to validator node degradation or crash.

## Finding Description

The quorum store payload manager lacks limits on the number of concurrent batch fetch operations. When a block proposal is received, the `prefetch_payload_data` function initiates batch fetches for all batches in the payload without any concurrency control.

**Attack Path:**

1. **Malicious Proposal Creation**: A proposer creates a block with payload containing many small batches (e.g., 2,000 batches × 5 transactions = 10,000 transactions total). [1](#0-0) 

2. **Validation Bypass**: The proposal passes validation because only `max_receiving_block_txns` (10,000) and `max_receiving_block_bytes` limits are checked, not batch count. [2](#0-1) 

3. **Prefetch Triggered**: The `prefetch_payload_data` function calls `request_transactions` for all batches. [3](#0-2) 

4. **Unbounded Task Spawning**: For each unique batch digest, `get_or_fetch_batch` spawns an independent tokio task via `tokio::spawn(fut.clone())`. [4](#0-3) 

5. **Resource Exhaustion**: Each spawned task makes network RPC calls to fetch batch data, creating thousands of concurrent connections. [5](#0-4) 

**Key Vulnerability**: The deduplication mechanism in `inflight_fetch_requests` only prevents duplicate fetches of the *same* digest. It provides no protection against many *different* digests, allowing unbounded concurrent task spawning.

**Invariant Violated**: Resource Limits invariant - "All operations must respect gas, storage, and computational limits." The system fails to limit concurrent batch fetch operations.

## Impact Explanation

**Severity: Medium to High**

This vulnerability enables:
- **File descriptor exhaustion**: Each network connection consumes a file descriptor, potentially exhausting the system's fd limit (typically 1024-65536)
- **Network bandwidth saturation**: Thousands of concurrent requests can saturate network links
- **CPU/Memory pressure**: Managing thousands of tokio tasks and concurrent operations
- **Validator degradation**: Affected validators experience slowdowns, timeouts, or crashes

According to Aptos bug bounty criteria:
- **High Severity**: "Validator node slowdowns" - matches this impact
- **Medium Severity**: "State inconsistencies requiring intervention" - if validators crash/restart

The attack can target specific validators during their proposal processing, potentially causing consensus delays or liveness issues if enough validators are affected.

## Likelihood Explanation

**Likelihood: Medium**

**Attacker Requirements:**
- Must be a validator in the active validator set (requires staking)
- Must be elected as proposer for a round
- Can execute attack when it's their turn to propose

**Ease of Exploitation:**
- Attack is straightforward once proposer role is obtained
- No complex timing or race conditions required
- Can be repeated in any epoch where attacker is a validator

**Detection/Prevention:**
- Current implementation has no rate limiting or detection
- Validators would see resource exhaustion symptoms but may not identify the root cause immediately
- No existing monitoring specifically for this attack pattern

## Recommendation

Implement a semaphore-based concurrency limit for batch fetches:

```rust
// In BatchReaderImpl struct, add:
batch_fetch_semaphore: Arc<tokio::sync::Semaphore>,

// In new():
const MAX_CONCURRENT_BATCH_FETCHES: usize = 100;
Self {
    batch_store,
    batch_requester: Arc::new(batch_requester),
    inflight_fetch_requests: Arc::new(Mutex::new(HashMap::new())),
    batch_fetch_semaphore: Arc::new(tokio::sync::Semaphore::new(MAX_CONCURRENT_BATCH_FETCHES)),
}

// In get_or_fetch_batch, acquire permit before spawning:
let semaphore = self.batch_fetch_semaphore.clone();
let fut = async move {
    let _permit = semaphore.acquire().await.expect("Semaphore closed");
    // ... rest of the fetch logic
}
```

Additionally, add a limit on the maximum number of batches/proofs in a block proposal during validation:

```rust
// In ProposalMsg::verify or Payload::verify:
const MAX_PROOFS_PER_BLOCK: usize = 200;
ensure!(
    payload.num_proofs() <= MAX_PROOFS_PER_BLOCK,
    "Payload contains too many proofs: {} > {}",
    payload.num_proofs(),
    MAX_PROOFS_PER_BLOCK
);
```

## Proof of Concept

**Note**: This vulnerability requires validator privileges to exploit, as only validators can create block proposals. The PoC demonstrates the technical mechanism but requires a validator node setup to execute.

```rust
// Conceptual PoC demonstrating the attack
#[test]
fn test_concurrent_batch_fetch_dos() {
    // 1. Create a payload with many small batches
    let num_batches = 2000;
    let txns_per_batch = 5;
    let mut proofs = Vec::new();
    
    for i in 0..num_batches {
        let batch = create_small_batch(i, txns_per_batch);
        let proof = create_proof_of_store(batch);
        proofs.push(proof);
    }
    
    let payload = Payload::InQuorumStore(ProofWithData::new(proofs));
    let block = create_block_with_payload(payload);
    
    // 2. When this block is received by other validators,
    //    prefetch_payload_data will spawn 2000 concurrent tasks
    
    // 3. Each task attempts to fetch batch data, consuming:
    //    - 2000 file descriptors (TCP connections)
    //    - Network bandwidth for 2000 concurrent requests
    //    - Memory for 2000 tokio tasks
    
    // Expected: System resources exhausted, validator degraded/crashed
    // Actual: No rate limiting prevents this scenario
}
```

**Notes:**
- The attack stays within `max_receiving_block_txns` limit (2000 × 5 = 10,000 txns)
- Each batch has a unique digest, bypassing deduplication
- The semaphore-based fix would limit concurrent fetches to a reasonable number

### Citations

**File:** config/src/config/consensus_config.rs (L23-24)
```rust
pub(crate) static MAX_RECEIVING_BLOCK_TXNS: Lazy<u64> =
    Lazy::new(|| 10000.max(2 * MAX_SENDING_BLOCK_TXNS));
```

**File:** consensus/src/round_manager.rs (L1180-1193)
```rust
        ensure!(
            num_validator_txns + payload_len as u64 <= self.local_config.max_receiving_block_txns,
            "Payload len {} exceeds the limit {}",
            payload_len,
            self.local_config.max_receiving_block_txns,
        );

        ensure!(
            validator_txns_total_bytes + payload_size as u64
                <= self.local_config.max_receiving_block_bytes,
            "Payload size {} exceeds the limit {}",
            payload_size,
            self.local_config.max_receiving_block_bytes,
        );
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L210-228)
```rust
    fn prefetch_payload_data(&self, payload: &Payload, author: Author, timestamp: u64) {
        // This is deprecated.
        // TODO(ibalajiarun): Remove this after migrating to OptQuorumStore type
        let request_txns_and_update_status =
            move |proof_with_status: &ProofWithData, batch_reader: Arc<dyn BatchReader>| {
                Self::request_transactions(
                    proof_with_status
                        .proofs
                        .iter()
                        .map(|proof| {
                            (
                                proof.info().clone(),
                                proof.shuffled_signers(&self.ordered_authors),
                            )
                        })
                        .collect(),
                    timestamp,
                    batch_reader,
                );
```

**File:** consensus/src/quorum_store/batch_store.rs (L663-723)
```rust
    fn get_or_fetch_batch(
        &self,
        batch_info: BatchInfo,
        responders: Vec<PeerId>,
    ) -> Shared<Pin<Box<dyn Future<Output = ExecutorResult<Vec<SignedTransaction>>> + Send>>> {
        let mut responders = responders.into_iter().collect();

        self.inflight_fetch_requests
            .lock()
            .entry(*batch_info.digest())
            .and_modify(|fetch_unit| {
                fetch_unit.responders.lock().append(&mut responders);
            })
            .or_insert_with(|| {
                let responders = Arc::new(Mutex::new(responders));
                let responders_clone = responders.clone();

                let inflight_requests_clone = self.inflight_fetch_requests.clone();
                let batch_store = self.batch_store.clone();
                let requester = self.batch_requester.clone();

                let fut = async move {
                    let batch_digest = *batch_info.digest();
                    defer!({
                        inflight_requests_clone.lock().remove(&batch_digest);
                    });
                    // TODO(ibalajiarun): Support V2 batch
                    if let Ok(mut value) = batch_store.get_batch_from_local(&batch_digest) {
                        Ok(value.take_payload().expect("Must have payload"))
                    } else {
                        // Quorum store metrics
                        counters::MISSED_BATCHES_COUNT.inc();
                        let subscriber_rx = batch_store.subscribe(*batch_info.digest());
                        let payload = requester
                            .request_batch(
                                batch_digest,
                                batch_info.expiration(),
                                responders,
                                subscriber_rx,
                            )
                            .await?;
                        batch_store.persist(vec![PersistedValue::new(
                            batch_info.into(),
                            Some(payload.clone()),
                        )]);
                        Ok(payload)
                    }
                }
                .boxed()
                .shared();

                tokio::spawn(fut.clone());

                BatchFetchUnit {
                    responders: responders_clone,
                    fut,
                }
            })
            .fut
            .clone()
    }
```

**File:** consensus/src/quorum_store/batch_requester.rs (L101-180)
```rust
    pub(crate) async fn request_batch(
        &self,
        digest: HashValue,
        expiration: u64,
        responders: Arc<Mutex<BTreeSet<PeerId>>>,
        mut subscriber_rx: oneshot::Receiver<PersistedValue<BatchInfoExt>>,
    ) -> ExecutorResult<Vec<SignedTransaction>> {
        let validator_verifier = self.validator_verifier.clone();
        let mut request_state = BatchRequesterState::new(responders, self.retry_limit);
        let network_sender = self.network_sender.clone();
        let request_num_peers = self.request_num_peers;
        let my_peer_id = self.my_peer_id;
        let epoch = self.epoch;
        let retry_interval = Duration::from_millis(self.retry_interval_ms as u64);
        let rpc_timeout = Duration::from_millis(self.rpc_timeout_ms as u64);

        monitor!("batch_request", {
            let mut interval = time::interval(retry_interval);
            let mut futures = FuturesUnordered::new();
            let request = BatchRequest::new(my_peer_id, epoch, digest);
            loop {
                tokio::select! {
                    _ = interval.tick() => {
                        // send batch request to a set of peers of size request_num_peers
                        if let Some(request_peers) = request_state.next_request_peers(request_num_peers) {
                            for peer in request_peers {
                                futures.push(network_sender.request_batch(request.clone(), peer, rpc_timeout));
                            }
                        } else if futures.is_empty() {
                            // end the loop when the futures are drained
                            break;
                        }
                    },
                    Some(response) = futures.next() => {
                        match response {
                            Ok(BatchResponse::Batch(batch)) => {
                                counters::RECEIVED_BATCH_RESPONSE_COUNT.inc();
                                let payload = batch.into_transactions();
                                return Ok(payload);
                            }
                            // Short-circuit if the chain has moved beyond expiration
                            Ok(BatchResponse::NotFound(ledger_info)) => {
                                counters::RECEIVED_BATCH_NOT_FOUND_COUNT.inc();
                                if ledger_info.commit_info().epoch() == epoch
                                    && ledger_info.commit_info().timestamp_usecs() > expiration
                                    && ledger_info.verify_signatures(&validator_verifier).is_ok()
                                {
                                    counters::RECEIVED_BATCH_EXPIRED_COUNT.inc();
                                    debug!("QS: batch request expired, digest:{}", digest);
                                    return Err(ExecutorError::CouldNotGetData);
                                }
                            }
                            Ok(BatchResponse::BatchV2(_)) => {
                                error!("Batch V2 response is not supported");
                            }
                            Err(e) => {
                                counters::RECEIVED_BATCH_RESPONSE_ERROR_COUNT.inc();
                                debug!("QS: batch request error, digest:{}, error:{:?}", digest, e);
                            }
                        }
                    },
                    result = &mut subscriber_rx => {
                        match result {
                            Ok(persisted_value) => {
                                counters::RECEIVED_BATCH_FROM_SUBSCRIPTION_COUNT.inc();
                                let (_, maybe_payload) = persisted_value.unpack();
                                return Ok(maybe_payload.expect("persisted value must exist"));
                            }
                            Err(err) => {
                                debug!("channel closed: {}", err);
                            }
                        };
                    },
                }
            }
            counters::RECEIVED_BATCH_REQUEST_TIMEOUT_COUNT.inc();
            debug!("QS: batch request timed out, digest:{}", digest);
            Err(ExecutorError::CouldNotGetData)
        })
    }
```
