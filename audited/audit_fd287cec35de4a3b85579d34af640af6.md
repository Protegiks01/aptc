# Audit Report

## Title
CurrentTokenOwnership Reflects Incorrect State After Indexer Crash Due to Transaction Version WHERE Clause

## Summary
The Aptos indexer's crash recovery mechanism allows `CurrentTokenOwnership` table to contain data from transactions that are marked as "not yet processed" in `processor_status`, causing incorrect ownership states to be served to users and applications querying the indexer API.

## Finding Description

The indexer uses a two-phase commit pattern where it:
1. First commits token ownership data to database tables (including `current_token_ownerships`)
2. Then updates `processor_status` to mark those transactions as successfully processed [1](#0-0) 

The `insert_current_token_ownerships` function includes a WHERE clause to prevent old data from overwriting new data: [2](#0-1) 

The critical issue is that `CurrentTokenOwnership` uses a composite primary key of `(token_data_id_hash, property_version, owner_address)`: [3](#0-2) 

**Attack Scenario:**

1. **Version 100-110 processed**: Transaction at version 105 transfers token X from Alice to Bob
   - Database commits: `CurrentTokenOwnership(token_X, v1, Alice)` with `amount=0`, `last_transaction_version=105`
   - Database commits: `CurrentTokenOwnership(token_X, v1, Bob)` with `amount=1`, `last_transaction_version=105`
   
2. **Indexer crashes** before calling `update_last_processed_version(110)`

3. **Indexer restarts**: Reads `processor_status` which still shows `last_success_version=90`, resumes from version 91 [4](#0-3) 

4. **Version 91-100 reprocessed**: Transaction at version 95 transfers token X from Charlie to Alice
   - Attempts to update: `CurrentTokenOwnership(token_X, v1, Alice)` with `amount=1`, `last_transaction_version=95`
   - WHERE clause checks: `105 <= 95` evaluates to FALSE
   - **UPDATE REJECTED** - Alice's row remains at `amount=0` (INCORRECT!)
   - Alice should have `amount=1` according to successfully processed transactions (version â‰¤ 100)

5. **Ownership state is now incorrect**: The database shows Alice with 0 tokens and Bob with 1 token, even though according to `processor_status`, only transactions up to version 100 have been processed, at which point Alice should own the token.

The vulnerability exists because the WHERE clause assumes monotonically increasing versions during normal operation, but doesn't account for the crash/restart scenario where we need to FORCE reprocessing of already-committed-but-not-marked-successful data.

## Impact Explanation

**HIGH Severity** - This meets the "State inconsistencies requiring intervention" criteria with significant real-world impact:

1. **Incorrect Ownership Data**: Users querying `current_token_ownerships` receive wrong ownership information
2. **API/Application Failures**: All applications relying on indexer APIs (wallets, marketplaces, explorers) display incorrect token ownership
3. **Temporal Window**: The inconsistency persists until future transactions (101-110) are reprocessed, which could be minutes to hours depending on blockchain activity
4. **Widespread Impact**: Affects all token transfers processed in the gap between database commit and `processor_status` update
5. **No Self-Healing**: The system doesn't automatically detect or fix this inconsistency

While this doesn't affect on-chain state (which remains correct), the indexer is critical infrastructure for user-facing applications, making this a significant availability and correctness issue.

## Likelihood Explanation

**MEDIUM to HIGH likelihood**:

1. **Common Trigger**: Indexer crashes occur during normal operations (OOM errors, deployment updates, infrastructure failures)
2. **Race Window**: The vulnerability window exists between lines 203-227 (database transaction commit) and line 252 (`update_last_processed_version` call): [5](#0-4) 

3. **High Transaction Volume**: Aptos processes thousands of transactions per second, increasing the probability of catching transactions in this window during a crash
4. **No Detection Mechanism**: The system doesn't detect this inconsistency - it silently serves incorrect data

## Recommendation

Implement one of the following fixes:

**Option 1: Database Cleanup on Restart (Recommended)**
On indexer startup, delete all rows from `current_*` tables where `last_transaction_version > last_success_version` from `processor_status`:

```rust
// In runtime.rs, after getting start_version and before starting the fetcher
pub fn cleanup_uncommitted_data(
    conn_pool: &PgDbPool,
    processor_name: &str,
    last_success_version: i64,
) -> Result<()> {
    let mut conn = conn_pool.get()?;
    
    // Delete uncommitted current_token_ownerships
    diesel::delete(
        current_token_ownerships::table
            .filter(current_token_ownerships::last_transaction_version.gt(last_success_version))
    ).execute(&mut conn)?;
    
    // Repeat for other current_* tables
    // current_token_datas, current_collections, etc.
    
    Ok(())
}
```

**Option 2: Atomic Update with processor_status**
Move `update_last_processed_version` inside the same database transaction as the data inserts. This ensures atomicity but may impact performance.

**Option 3: Conditional WHERE Clause**
Modify the WHERE clause to allow updates when the new version is greater than `last_success_version`:

```rust
let last_success_version = get_processor_status(&processor_name, conn)?;
Some(format!(
    " WHERE current_token_ownerships.last_transaction_version <= EXCLUDED.last_transaction_version \
     OR EXCLUDED.last_transaction_version > {}",
    last_success_version
))
```

## Proof of Concept

```rust
// Rust test demonstrating the vulnerability
#[test]
fn test_crash_restart_ownership_inconsistency() {
    let pool = create_test_db_pool();
    let mut conn = pool.get().unwrap();
    
    // Step 1: Simulate processing version 105 (Alice -> Bob transfer)
    let ownership_v105_alice = CurrentTokenOwnership {
        token_data_id_hash: "token_x".to_string(),
        property_version: BigDecimal::from(1),
        owner_address: "alice".to_string(),
        amount: BigDecimal::from(0), // Alice has 0 after transfer
        last_transaction_version: 105,
        // ... other fields
    };
    let ownership_v105_bob = CurrentTokenOwnership {
        token_data_id_hash: "token_x".to_string(),
        property_version: BigDecimal::from(1),
        owner_address: "bob".to_string(),
        amount: BigDecimal::from(1), // Bob has 1 after transfer
        last_transaction_version: 105,
        // ... other fields
    };
    
    insert_current_token_ownerships(&mut conn, &[ownership_v105_alice, ownership_v105_bob]).unwrap();
    
    // CRASH HAPPENS HERE - processor_status NOT updated (still at version 90)
    
    // Step 2: Restart - reprocess version 95 (Charlie -> Alice transfer)
    let ownership_v95_alice = CurrentTokenOwnership {
        token_data_id_hash: "token_x".to_string(),
        property_version: BigDecimal::from(1),
        owner_address: "alice".to_string(),
        amount: BigDecimal::from(1), // Alice should have 1 at version 95
        last_transaction_version: 95,
        // ... other fields
    };
    
    // This insert will be REJECTED by WHERE clause
    insert_current_token_ownerships(&mut conn, &[ownership_v95_alice]).unwrap();
    
    // Step 3: Verify inconsistency
    let alice_ownership = current_token_ownerships::table
        .filter(current_token_ownerships::owner_address.eq("alice"))
        .first::<CurrentTokenOwnership>(&mut conn)
        .unwrap();
    
    // BUG: Alice shows amount=0 (from v105) when she should have amount=1 (from v95)
    assert_eq!(alice_ownership.amount, BigDecimal::from(0)); // INCORRECT!
    assert_eq!(alice_ownership.last_transaction_version, 105); // Future version!
    
    // Meanwhile processor_status shows last_success_version <= 100
    // This is an inconsistent state!
}
```

## Notes

This vulnerability is specific to the indexer component and does not affect on-chain state or consensus. However, it represents a critical consistency violation for off-chain infrastructure that many applications depend on. The issue stems from the lack of transactional atomicity between data updates and progress tracking, combined with an overly restrictive WHERE clause that doesn't account for crash recovery scenarios.

### Citations

**File:** crates/indexer/src/runtime.rs (L251-261)
```rust
        tailer
            .update_last_processed_version(&processor_name, batch_end_version)
            .unwrap_or_else(|e| {
                error!(
                    processor_name = processor_name,
                    end_version = batch_end_version,
                    error = format!("{:?}", e),
                    "Failed to update last processed version!"
                );
                panic!("Failed to update last processed version: {:?}", e);
            });
```

**File:** crates/indexer/src/processors/token_processor.rs (L200-227)
```rust
    match conn
        .build_transaction()
        .read_write()
        .run::<_, Error, _>(|pg_conn| {
            insert_to_db_impl(
                pg_conn,
                (&tokens, &token_ownerships, &token_datas, &collection_datas),
                (
                    &current_token_ownerships,
                    &current_token_datas,
                    &current_collection_datas,
                ),
                &token_activities,
                &current_token_claims,
                &current_ans_lookups,
                &nft_points,
                (
                    &collections_v2,
                    &token_datas_v2,
                    &token_ownerships_v2,
                    &current_collections_v2,
                    &current_token_datas_v2,
                    &current_token_ownerships_v2,
                    &token_activities_v2,
                    &current_token_v2_metadata,
                ),
            )
        }) {
```

**File:** crates/indexer/src/processors/token_processor.rs (L380-410)
```rust
fn insert_current_token_ownerships(
    conn: &mut PgConnection,
    items_to_insert: &[CurrentTokenOwnership],
) -> Result<(), diesel::result::Error> {
    use schema::current_token_ownerships::dsl::*;

    let chunks = get_chunks(items_to_insert.len(), CurrentTokenOwnership::field_count());

    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::current_token_ownerships::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict((token_data_id_hash, property_version, owner_address))
                .do_update()
                .set((
                    creator_address.eq(excluded(creator_address)),
                    collection_name.eq(excluded(collection_name)),
                    name.eq(excluded(name)),
                    amount.eq(excluded(amount)),
                    token_properties.eq(excluded(token_properties)),
                    last_transaction_version.eq(excluded(last_transaction_version)),
                    collection_data_id_hash.eq(excluded(collection_data_id_hash)),
                    table_type.eq(excluded(table_type)),
                    inserted_at.eq(excluded(inserted_at)),
                )),
            Some(" WHERE current_token_ownerships.last_transaction_version <= excluded.last_transaction_version "),
        )?;
    }
    Ok(())
}
```

**File:** crates/indexer/src/models/token_models/token_ownerships.rs (L44-60)
```rust
#[derive(Debug, Deserialize, FieldCount, Identifiable, Insertable, Serialize)]
#[diesel(primary_key(token_data_id_hash, property_version, owner_address))]
#[diesel(table_name = current_token_ownerships)]
pub struct CurrentTokenOwnership {
    pub token_data_id_hash: String,
    pub property_version: BigDecimal,
    pub owner_address: String,
    pub creator_address: String,
    pub collection_name: String,
    pub name: String,
    pub amount: BigDecimal,
    pub token_properties: serde_json::Value,
    pub last_transaction_version: i64,
    pub collection_data_id_hash: String,
    pub table_type: String,
    pub last_transaction_timestamp: chrono::NaiveDateTime,
}
```

**File:** crates/indexer/src/indexer/tailer.rs (L193-200)
```rust
    /// Get last version processed successfully from databse
    pub fn get_start_version(&self, processor_name: &String) -> Result<Option<i64>> {
        let mut conn = self.connection_pool.get()?;

        match ProcessorStatusV2Query::get_by_processor(processor_name, &mut conn)? {
            Some(status) => Ok(Some(status.last_success_version + 1)),
            None => Ok(None),
        }
```
