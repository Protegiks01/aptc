# Audit Report

## Title
Race Condition in sync_for_duration() Allows BlockExecutor Reset During Active Pipeline Execution

## Summary
The `sync_for_duration()` path in `ExecutionClient` calls `executor.reset()` before aborting in-flight pipeline tasks, creating a race condition where `BlockExecutorInner` is initialized with a stale database snapshot that doesn't include blocks currently being executed by the consensus pipeline.

## Finding Description
The vulnerability stems from improper ordering of operations in the state synchronization flow. When `sync_for_duration()` is invoked, it performs the following sequence: [1](#0-0) 

The critical issue is that `execution_proxy.sync_for_duration()` (which calls `executor.reset()`) executes BEFORE `ExecutionClient.reset()` (which aborts pipeline tasks). This ordering is reversed compared to `sync_to_target()`: [2](#0-1) 

The race condition occurs when:

1. Pipeline task executes `execute_and_update_state()`, adding a block to the in-memory `BlockTree`
2. The task releases locks and returns (but `ledger_update()` hasn't been called yet)
3. `sync_for_duration()` is triggered, acquiring `write_mutex`
4. `executor.finish()` sets `inner` to `None`
5. `executor.reset()` creates a new `BlockExecutorInner` from the database [3](#0-2) 

6. The new `BlockExecutorInner` reads from the database via `BlockTree::new()` [4](#0-3) [5](#0-4) 

7. The database doesn't contain the block that was just executed (not yet committed) [6](#0-5) 

8. Pipeline task tries to call `ledger_update()` with the block ID, but the new `BlockExecutorInner` doesn't have it [7](#0-6) 

The synchronization locks don't prevent this race because:
- `execution_lock` only protects `execute_and_update_state()` calls, not the gap between execution and ledger_update
- `write_mutex` in `ExecutionProxy` doesn't coordinate with pipeline execution
- Pipeline tasks spawn asynchronously without acquiring `write_mutex` [8](#0-7) 

## Impact Explanation
**Severity: Medium**

This vulnerability causes state inconsistencies requiring intervention. When the race condition is triggered:

1. `ledger_update()` fails with `BlockNotFound` error, causing pipeline task failures
2. The validator may enter an inconsistent state where executed blocks are lost
3. Breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable"
4. Breaks the **Deterministic Execution** invariant: Different timing could lead to different execution outcomes

While this doesn't directly lead to consensus violations or fund loss, it can cause validator nodes to fail or require manual intervention to recover, meeting the Medium severity criteria of "State inconsistencies requiring intervention."

## Likelihood Explanation
**Likelihood: Medium**

This race condition can occur during normal operations:
- Validators regularly fall behind and trigger state sync
- High transaction throughput increases the window for the race
- No attacker control required - it's a natural timing-dependent bug

However, it requires precise timing where:
- A block has completed `execute_and_update_state()` but not yet called `ledger_update()`
- State sync is triggered in this narrow window

The window is small but non-zero, especially under load when multiple blocks are in the pipeline simultaneously.

## Recommendation
Reorder operations in `sync_for_duration()` to match `sync_to_target()`:

```rust
async fn sync_for_duration(
    &self,
    duration: Duration,
) -> Result<LedgerInfoWithSignatures, StateSyncError> {
    // Reset the rand and buffer managers FIRST to abort pipeline tasks
    let result = self.execution_proxy.sync_for_duration(duration).await;
    if let Ok(latest_synced_ledger_info) = &result {
        self.reset(latest_synced_ledger_info).await?;
    }
    
    // THEN perform state sync and executor reset
    let result = self.execution_proxy.sync_for_duration(duration).await;
    result
}
```

Wait, this won't work as-is. The correct fix is:

```rust
async fn sync_for_duration(
    &self,
    duration: Duration,
) -> Result<LedgerInfoWithSignatures, StateSyncError> {
    // Abort pipeline tasks BEFORE calling executor.reset()
    // Get a dummy ledger info to trigger reset, or track current state
    self.abort_all_pipeline_tasks().await?;
    
    // Now safe to call executor reset
    let result = self.execution_proxy.sync_for_duration(duration).await;
    
    if let Ok(latest_synced_ledger_info) = &result {
        self.reset(latest_synced_ledger_info).await?;
    }
    
    result
}
```

Alternatively, make `ExecutionProxy.sync_for_duration()` abort pipeline tasks before calling `executor.reset()`.

## Proof of Concept
This requires an integration test with careful timing control:

```rust
// Pseudo-code for reproduction
#[tokio::test]
async fn test_sync_during_execution_race() {
    let executor = create_block_executor();
    let execution_client = create_execution_client(executor);
    
    // Start executing a block in pipeline
    let block_future = tokio::spawn(async move {
        execution_client.execute_and_update_state(block, parent_id, config).await
    });
    
    // Wait for execute to complete but before ledger_update
    tokio::time::sleep(Duration::from_millis(10)).await;
    
    // Trigger sync_for_duration (this will call executor.reset())
    let sync_future = tokio::spawn(async move {
        execution_client.sync_for_duration(Duration::from_secs(1)).await
    });
    
    // Wait for both
    let (exec_result, sync_result) = tokio::join!(block_future, sync_future);
    
    // Now try ledger_update - should fail with BlockNotFound
    let ledger_result = execution_client.ledger_update(block_id, parent_id).await;
    assert!(matches!(ledger_result, Err(ExecutorError::BlockNotFound(_))));
}
```

## Notes
This vulnerability specifically affects the `sync_for_duration()` path. The `sync_to_target()` path correctly aborts pipeline tasks before calling `executor.reset()`. The inconsistency between these two code paths indicates this was likely an oversight rather than an intentional design decision.

### Citations

**File:** consensus/src/pipeline/execution_client.rs (L642-659)
```rust
    async fn sync_for_duration(
        &self,
        duration: Duration,
    ) -> Result<LedgerInfoWithSignatures, StateSyncError> {
        fail_point!("consensus::sync_for_duration", |_| {
            Err(anyhow::anyhow!("Injected error in sync_for_duration").into())
        });

        // Sync for the specified duration
        let result = self.execution_proxy.sync_for_duration(duration).await;

        // Reset the rand and buffer managers to the new synced round
        if let Ok(latest_synced_ledger_info) = &result {
            self.reset(latest_synced_ledger_info).await?;
        }

        result
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L661-672)
```rust
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });

        // Reset the rand and buffer managers to the target round
        self.reset(&target).await?;

        // TODO: handle the state sync error (e.g., re-push the ordered
        // blocks to the buffer manager when it's reset but sync fails).
        self.execution_proxy.sync_to_target(target).await
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L90-95)
```rust
    fn reset(&self) -> Result<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "reset"]);

        *self.inner.write() = Some(BlockExecutorInner::new(self.db.clone())?);
        Ok(())
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L115-129)
```rust
    fn ledger_update(
        &self,
        block_id: HashValue,
        parent_block_id: HashValue,
    ) -> ExecutorResult<StateComputeResult> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "ledger_update"]);

        self.inner
            .read()
            .as_ref()
            .ok_or_else(|| ExecutorError::InternalError {
                error: "BlockExecutor is not reset".into(),
            })?
            .ledger_update(block_id, parent_block_id)
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L173-180)
```rust
    pub fn new(db: DbReaderWriter) -> Result<Self> {
        let block_tree = BlockTree::new(&db.reader)?;
        Ok(Self {
            db,
            block_tree,
            block_executor: V::new(),
        })
    }
```

**File:** execution/executor/src/block_executor/block_tree/mod.rs (L179-184)
```rust
    pub fn new(db: &Arc<dyn DbReader>) -> Result<Self> {
        let block_lookup = Arc::new(BlockLookup::new());
        let root = Mutex::new(Self::root_from_db(&block_lookup, db)?);

        Ok(Self { root, block_lookup })
    }
```

**File:** execution/executor/src/block_executor/block_tree/mod.rs (L207-228)
```rust
    fn root_from_db(block_lookup: &Arc<BlockLookup>, db: &Arc<dyn DbReader>) -> Result<Arc<Block>> {
        let ledger_info_with_sigs = db.get_latest_ledger_info()?;
        let ledger_info = ledger_info_with_sigs.ledger_info();
        let ledger_summary = db.get_pre_committed_ledger_summary()?;

        ensure!(
            ledger_summary.version() == Some(ledger_info.version()),
            "Missing ledger info at the end of the ledger. latest version {:?}, LI version {}",
            ledger_summary.version(),
            ledger_info.version(),
        );

        let id = if ledger_info.ends_epoch() {
            epoch_genesis_block_id(ledger_info)
        } else {
            ledger_info.consensus_block_id()
        };

        let output = PartialStateComputeResult::new_empty(ledger_summary);

        block_lookup.fetch_or_add_block(id, output, None)
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L857-867)
```rust
        tokio::task::spawn_blocking(move || {
            executor
                .execute_and_update_state(
                    (block.id(), txns, auxiliary_info).into(),
                    block.parent_id(),
                    onchain_execution_config,
                )
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
```
