# Audit Report

## Title
Partial Batch Commit Creates Temporary Ledger Inconsistency in Transaction Database

## Summary
The `commit_transactions()` function commits transaction batches sequentially without atomic rollback. When `write_schemas()` fails mid-process, previously written batches remain committed while subsequent batches are never written, creating gaps in the transaction database. This is compounded by parallel commits to other databases, resulting in cross-database inconsistencies.

## Finding Description

The vulnerability exists in the sequential batch commit pattern within `commit_transactions()`. [1](#0-0) 

When transactions are committed, they're split into multiple batches and committed sequentially. If `write_schemas()` fails on batch N, the `?` operator immediately returns the error, leaving batches 0..N-1 permanently committed to disk while batches N+1..M are never attempted.

**Example scenario:**
- 200 transactions (versions 1000-1199) split into 4 batches
- Batch 0 (1000-1049): SUCCESS â†’ written to disk
- Batch 1 (1050-1099): FAILURE (disk full, I/O error, corruption)
- Batches 2-3: NEVER ATTEMPTED
- Result: transaction_db contains only versions 1000-1049, missing 1050-1199

This is worsened by parallel execution in `calculate_and_commit_ledger_and_state_kv()` [2](#0-1)  where multiple databases commit simultaneously. When the transaction_db thread panics due to `.unwrap()` on the failed write [3](#0-2) , other threads continue executing and may complete successfully, creating cross-database inconsistency where:

- `transaction_db`: has partial data (1000-1049 only)
- `event_db`: has complete data (1000-1199)
- `write_set_db`: has complete data (1000-1199)
- `transaction_info_db`: has complete data (1000-1199)

This violates the fundamental invariant that all ledger databases should maintain consistency with each other.

## Impact Explanation

**Severity: MEDIUM** (State inconsistencies requiring intervention)

While this breaks the "State Consistency" invariant, the impact is mitigated by:

1. **Process Exit**: The panic handler crashes the node immediately [4](#0-3) , preventing the inconsistent state from being served for more than milliseconds
2. **Automatic Recovery**: On restart, `sync_commit_progress()` [5](#0-4)  truncates all databases back to `OverallCommitProgress`, cleaning up partial commits
3. **Query Protection**: External APIs validate requests against `ledger_version`, preventing access to uncommitted data

However, the issue represents a significant design weakness that could escalate if the TODO at line 275 is implemented [6](#0-5)  to propagate errors instead of panicking.

## Likelihood Explanation

**Likelihood: LOW-MEDIUM**

Triggering requires:
- Disk space exhaustion during commits
- I/O errors from hardware failures  
- RocksDB write failures due to corruption

These are operational issues that can occur in production but are not directly controllable by external attackers. The brief window (milliseconds) between failure and process exit further reduces exploitability.

## Recommendation

Implement atomic cross-database transactions or two-phase commit protocol:

```rust
// Pseudo-code for atomic commit across databases
pub fn commit_transactions_atomic(&self, ...) -> Result<()> {
    // Phase 1: Prepare all batches without committing
    let prepared_batches = self.prepare_all_batches(transactions)?;
    
    // Phase 2: Commit atomically or rollback all
    match self.commit_all_or_none(prepared_batches) {
        Ok(_) => Ok(()),
        Err(e) => {
            self.rollback_partial_commits()?;
            Err(e)
        }
    }
}
```

Alternatively, write all batches to a single atomic RocksDB WriteBatch instead of committing incrementally, or implement proper transaction log with rollback capability.

**Critical**: Do NOT implement the TODO to propagate errors without first implementing proper atomic commit guarantees, as this would eliminate the current crash-based mitigation.

## Proof of Concept

```rust
#[test]
fn test_partial_batch_commit_creates_gap() {
    let tmpdir = TempPath::new();
    let db = AptosDB::new_for_test(&tmpdir);
    
    // Create 200 transactions
    let txns: Vec<Transaction> = (0..200)
        .map(|_| create_test_transaction())
        .collect();
    
    // Simulate disk full after first batch by mocking write_schemas
    // to fail after first 50 transactions
    let result = db.ledger_db()
        .transaction_db()
        .commit_transactions(1000, &txns, false);
    
    // Should fail on second batch
    assert!(result.is_err());
    
    // Check that first batch (1000-1049) exists
    for version in 1000..1050 {
        assert!(db.get_transaction(version).is_ok());
    }
    
    // Check that second batch (1050-1099) is missing (gap created)
    for version in 1050..1100 {
        assert!(db.get_transaction(version).is_err());
    }
    
    // This demonstrates the gap within transaction_db
}
```

**Notes:**
- The vulnerability exists in the codebase but is currently mitigated by panic-based process termination and automatic recovery
- The design lacks atomic cross-database transaction guarantees, relying instead on crash recovery
- External queries are protected by version checks, limiting the exploitability window
- This represents a robustness concern rather than a directly exploitable security vulnerability under current implementation
- Future changes to error handling (per TODO comment) could significantly increase severity if implemented without proper atomic commit mechanisms

### Citations

**File:** storage/aptosdb/src/ledger_db/transaction_db.rs (L121-123)
```rust
            for batch in batches {
                self.db().write_schemas(batch)?
            }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L271-319)
```rust
        THREAD_MANAGER.get_non_exe_cpu_pool().scope(|s| {
            // TODO(grao): Write progress for each of the following databases, and handle the
            // inconsistency at the startup time.
            //
            // TODO(grao): Consider propagating the error instead of panic, if necessary.
            s.spawn(|_| {
                self.commit_events(
                    chunk.first_version,
                    chunk.transaction_outputs,
                    skip_index_and_usage,
                )
                .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .write_set_db()
                    .commit_write_sets(chunk.first_version, chunk.transaction_outputs)
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .transaction_db()
                    .commit_transactions(
                        chunk.first_version,
                        chunk.transactions,
                        skip_index_and_usage,
                    )
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .persisted_auxiliary_info_db()
                    .commit_auxiliary_info(chunk.first_version, chunk.persisted_auxiliary_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_state_kv_and_ledger_metadata(chunk, skip_index_and_usage)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_transaction_infos(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                new_root_hash = self
                    .commit_transaction_accumulator(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
        });
```

**File:** crates/crash-handler/src/lib.rs (L56-57)
```rust
    // Kill the process
    process::exit(12);
```

**File:** storage/aptosdb/src/state_store/mod.rs (L410-502)
```rust
    pub fn sync_commit_progress(
        ledger_db: Arc<LedgerDb>,
        state_kv_db: Arc<StateKvDb>,
        state_merkle_db: Arc<StateMerkleDb>,
        crash_if_difference_is_too_large: bool,
    ) {
        let ledger_metadata_db = ledger_db.metadata_db();
        if let Some(overall_commit_progress) = ledger_metadata_db
            .get_synced_version()
            .expect("DB read failed.")
        {
            info!(
                overall_commit_progress = overall_commit_progress,
                "Start syncing databases..."
            );
            let ledger_commit_progress = ledger_metadata_db
                .get_ledger_commit_progress()
                .expect("Failed to read ledger commit progress.");
            assert_ge!(ledger_commit_progress, overall_commit_progress);

            let state_kv_commit_progress = state_kv_db
                .metadata_db()
                .get::<DbMetadataSchema>(&DbMetadataKey::StateKvCommitProgress)
                .expect("Failed to read state K/V commit progress.")
                .expect("State K/V commit progress cannot be None.")
                .expect_version();
            assert_ge!(state_kv_commit_progress, overall_commit_progress);

            // LedgerCommitProgress was not guaranteed to commit after all ledger changes finish,
            // have to attempt truncating every column family.
            info!(
                ledger_commit_progress = ledger_commit_progress,
                "Attempt ledger truncation...",
            );
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");

            // State K/V commit progress isn't (can't be) written atomically with the data,
            // because there are shards, so we have to attempt truncation anyway.
            info!(
                state_kv_commit_progress = state_kv_commit_progress,
                "Start state KV truncation..."
            );
            let difference = state_kv_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_state_kv_db(
                &state_kv_db,
                state_kv_commit_progress,
                overall_commit_progress,
                std::cmp::max(difference as usize, 1), /* batch_size */
            )
            .expect("Failed to truncate state K/V db.");

            let state_merkle_max_version = get_max_version_in_state_merkle_db(&state_merkle_db)
                .expect("Failed to get state merkle max version.")
                .expect("State merkle max version cannot be None.");
            if state_merkle_max_version > overall_commit_progress {
                let difference = state_merkle_max_version - overall_commit_progress;
                if crash_if_difference_is_too_large {
                    assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
                }
            }
            let state_merkle_target_version = find_tree_root_at_or_before(
                ledger_metadata_db,
                &state_merkle_db,
                overall_commit_progress,
            )
            .expect("DB read failed.")
            .unwrap_or_else(|| {
                panic!(
                    "Could not find a valid root before or at version {}, maybe it was pruned?",
                    overall_commit_progress
                )
            });
            if state_merkle_target_version < state_merkle_max_version {
                info!(
                    state_merkle_max_version = state_merkle_max_version,
                    target_version = state_merkle_target_version,
                    "Start state merkle truncation..."
                );
                truncate_state_merkle_db(&state_merkle_db, state_merkle_target_version)
                    .expect("Failed to truncate state merkle db.");
            }
        } else {
            info!("No overall commit progress was found!");
        }
    }
```
