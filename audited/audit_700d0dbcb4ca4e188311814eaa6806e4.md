# Audit Report

## Title
Race Condition in Hot State Cache Causing Validator Panic on Non-Value-Write Updates

## Summary
A race condition exists between the hot state cache population during transaction execution and the asynchronous hot state committer thread. When a key is read and cached as hot, then evicted by the committer, and finally accessed during a MakeHot operation, the assertion `assert!(slot.is_cold())` fails, causing validator node crashes. [1](#0-0) 

## Finding Description
The vulnerability stems from a Time-Of-Check-Time-Of-Use (TOCTOU) race condition in the hot state management system. The attack flow is:

1. **Cache Population Phase**: During block execution, `CachedStateView` is created with a hot state view (Arc to `HotStateBase`) containing DashMap shards. [2](#0-1) 

When a state key is read, if it exists in hot state, it's cached as a hot slot: [3](#0-2) 

2. **Asynchronous Eviction Phase**: The `Committer` thread runs asynchronously and can evict keys from hot state by removing them from the DashMap: [4](#0-3) 

3. **State Update Phase**: When `update_with_memorized_reads` processes non-value-write operations (MakeHot), it calls `apply_one_update`: [5](#0-4) 

The LRU's `get_slot` checks the hot state view, which now returns `None` because the key was removed by the Committer. However, `expect_old_slot` finds the hot slot in the cache (from step 1): [6](#0-5) 

4. **Assertion Failure**: The assertion expects a cold slot but receives a hot slot, causing panic.

The hot state view's mutable nature enables this race: [7](#0-6) [8](#0-7) 

## Impact Explanation
**Severity: High** - This meets the "Validator node slowdowns" and "API crashes" criteria from the bug bounty program.

**Deterministic Execution Violation**: When this race occurs, different validators may experience the crash at different times depending on thread scheduling, breaking the deterministic execution invariant. Some validators crash while processing a block, others succeed, leading to temporary network disruption.

**Availability Impact**: Validator nodes crash with a panic, requiring restart. During high transaction load with frequent hot state evictions, this can affect multiple validators, impacting network liveness and block production rate.

**Consensus Degradation**: While not a total consensus failure, repeated crashes reduce the effective validator set size, bringing the network closer to the 2/3 threshold needed for liveness.

## Likelihood Explanation
**Likelihood: Medium to High** depending on network conditions.

**Favorable Conditions for Exploitation**:
1. High transaction throughput causing frequent hot state evictions
2. MakeHot operations being issued (read-heavy workloads trigger these)
3. Small hot state capacity leading to aggressive LRU eviction

**Natural Occurrence**: This is not a targeted attack but a natural race condition that occurs during normal operation when:
- Hot state is near capacity and evictions are frequent
- Execution and state commitment happen concurrently
- The race window between cache population and update processing is hit

**Attack Amplification**: An attacker can increase likelihood by:
- Submitting read-heavy transactions to populate hot state to capacity
- Triggering frequent evictions through access pattern manipulation
- Timing transactions to maximize the race window

The vulnerability doesn't require any special privileges - it occurs naturally during high network load.

## Recommendation
**Fix: Snapshot Hot State View at Cache Creation Time**

The root cause is sharing a mutable hot state view between the cache and the LRU. The fix is to capture an immutable snapshot when creating `CachedStateView`:

```rust
// In HotStateBase, add a snapshot method:
pub fn snapshot(&self) -> HashMap<StateKey, StateSlot> {
    let mut snapshot = HashMap::new();
    for shard_id in 0..NUM_STATE_SHARDS {
        for entry in self.shards[shard_id].iter() {
            snapshot.insert(entry.key().clone(), entry.value().clone());
        }
    }
    snapshot
}

// Create an ImmutableHotStateView wrapper:
struct ImmutableHotStateView {
    snapshot: HashMap<StateKey, StateSlot>,
}

impl HotStateView for ImmutableHotStateView {
    fn get_state_slot(&self, state_key: &StateKey) -> Option<StateSlot> {
        self.snapshot.get(state_key).cloned()
    }
}

// In CachedStateView::new_impl, use the immutable view:
let immutable_hot_state = Arc::new(ImmutableHotStateView {
    snapshot: hot_state_base.snapshot(),
});
```

**Alternative Fix**: Add validation in `expect_old_slot` to handle hot slots gracefully:
```rust
fn expect_old_slot(...) -> StateSlot {
    // existing code
    let mut slot = if let Some(slot) = overlay.get(key) {
        slot
    } else {
        cache.get(key).unwrap_or_else(...).value().clone()
    };
    
    // Convert hot slots from cache to cold to prevent assertion failure
    if slot.is_hot() {
        slot = slot.to_cold();
    }
    
    slot
}
```

## Proof of Concept

```rust
// Rust reproduction test (add to storage/storage-interface/src/state_store/tests/)
#[tokio::test]
async fn test_hot_state_race_condition() {
    use std::sync::Arc;
    use std::thread;
    use std::time::Duration;
    
    // Setup: Create hot state with key K
    let hot_state = Arc::new(create_hot_state_with_key("K"));
    
    // Thread 1: Simulate cache population
    let hot_state_clone1 = Arc::clone(&hot_state);
    let cache_thread = thread::spawn(move || {
        let cache = ShardedStateCache::new_empty(Some(100));
        // Read key K from hot state (it exists)
        if let Some(slot) = hot_state_clone1.get_state_slot(&StateKey::raw(b"K")) {
            cache.try_insert(&StateKey::raw(b"K"), &slot);
            assert!(slot.is_hot()); // Cached as hot
        }
        cache
    });
    
    // Thread 2: Simulate committer evicting K
    let hot_state_clone2 = Arc::clone(&hot_state);
    let evict_thread = thread::spawn(move || {
        thread::sleep(Duration::from_millis(10)); // Race window
        // Committer evicts key K
        hot_state_clone2.remove_key(&StateKey::raw(b"K"));
    });
    
    let cache = cache_thread.join().unwrap();
    evict_thread.join().unwrap();
    
    // Thread 3: Simulate apply_one_update for MakeHot(K)
    // LRU checks hot state: returns None (evicted)
    assert!(hot_state.get_state_slot(&StateKey::raw(b"K")).is_none());
    
    // expect_old_slot checks cache: returns hot slot
    let cached_slot = cache.get_cloned(&StateKey::raw(b"K")).unwrap();
    assert!(cached_slot.is_hot());
    
    // This would panic: assert!(cached_slot.is_cold());
    println!("Race condition reproduced: hot slot in cache but not in hot state view");
}
```

**Notes**
- The line number discrepancy (question mentions line 263, actual is line 320) suggests code may have changed, but the vulnerability location is confirmed at the cold slot assertion in `apply_one_update`.
- This is a concurrency bug that violates the assumption that hot state views are immutable snapshots during execution.
- The vulnerability affects all validators running under high load with active hot state management.
- Priority should be given to the immutable snapshot fix as it prevents the race condition entirely rather than mitigating symptoms.

### Citations

**File:** storage/storage-interface/src/state_store/state.rs (L286-326)
```rust
    fn apply_one_update(
        lru: &mut HotStateLRU,
        overlay: &LayeredMap<StateKey, StateSlot>,
        read_cache: &StateCacheShard,
        key: &StateKey,
        update: &StateUpdateRef,
        refresh_interval: Version,
    ) -> Option<HotStateValue> {
        if let Some(state_value_opt) = update.state_op.as_state_value_opt() {
            lru.insert((*key).clone(), update.to_result_slot().unwrap());
            return Some(HotStateValue::new(state_value_opt.cloned(), update.version));
        }

        if let Some(mut slot) = lru.get_slot(key) {
            let mut refreshed = true;
            let slot_to_insert = if slot.is_hot() {
                if slot.expect_hot_since_version() + refresh_interval <= update.version {
                    slot.refresh(update.version);
                } else {
                    refreshed = false;
                }
                slot
            } else {
                slot.to_hot(update.version)
            };
            if refreshed {
                let ret = HotStateValue::clone_from_slot(&slot_to_insert);
                lru.insert((*key).clone(), slot_to_insert);
                Some(ret)
            } else {
                None
            }
        } else {
            let slot = Self::expect_old_slot(overlay, read_cache, key);
            assert!(slot.is_cold());
            let slot = slot.to_hot(update.version);
            let ret = HotStateValue::clone_from_slot(&slot);
            lru.insert((*key).clone(), slot);
            Some(ret)
        }
    }
```

**File:** storage/storage-interface/src/state_store/state.rs (L370-385)
```rust
    fn expect_old_slot(
        overlay: &LayeredMap<StateKey, StateSlot>,
        cache: &StateCacheShard,
        key: &StateKey,
    ) -> StateSlot {
        if let Some(slot) = overlay.get(key) {
            return slot;
        }

        // TODO(aldenhu): avoid cloning the state value (by not using DashMap)
        cache
            .get(key)
            .unwrap_or_else(|| panic!("Key {:?} must exist in the cache.", key))
            .value()
            .clone()
    }
```

**File:** storage/storage-interface/src/state_store/state_view/cached_state_view.rs (L233-253)
```rust
    fn get_unmemorized(&self, state_key: &StateKey) -> Result<StateSlot> {
        COUNTER.inc_with(&["sv_unmemorized"]);

        let ret = if let Some(slot) = self.speculative.get_state_slot(state_key) {
            COUNTER.inc_with(&["sv_hit_speculative"]);
            slot
        } else if let Some(slot) = self.hot.get_state_slot(state_key) {
            COUNTER.inc_with(&["sv_hit_hot"]);
            slot
        } else if let Some(base_version) = self.base_version() {
            COUNTER.inc_with(&["sv_cold"]);
            StateSlot::from_db_get(
                self.cold
                    .get_state_value_with_version_by_version(state_key, base_version)?,
            )
        } else {
            StateSlot::ColdVacant
        };

        Ok(ret)
    }
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L72-89)
```rust
#[derive(Debug)]
pub struct HotStateBase<K = StateKey, V = StateSlot>
where
    K: Eq + std::hash::Hash,
{
    shards: [Shard<K, V>; NUM_STATE_SHARDS],
}

impl<K, V> HotStateBase<K, V>
where
    K: Clone + Eq + std::hash::Hash,
    V: Clone,
{
    fn new_empty(max_items_per_shard: usize) -> Self {
        Self {
            shards: arr![Shard::new(max_items_per_shard); 16],
        }
    }
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L100-105)
```rust
impl HotStateView for HotStateBase<StateKey, StateSlot> {
    fn get_state_slot(&self, state_key: &StateKey) -> Option<StateSlot> {
        let shard_id = state_key.get_shard_id();
        self.get_from_shard(shard_id, state_key).map(|v| v.clone())
    }
}
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L235-260)
```rust
    fn commit(&mut self, to_commit: &State) {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["hot_state_commit"]);

        let mut n_insert = 0;
        let mut n_update = 0;
        let mut n_evict = 0;

        let delta = to_commit.make_delta(&self.committed.lock());
        for shard_id in 0..NUM_STATE_SHARDS {
            for (key, slot) in delta.shards[shard_id].iter() {
                if slot.is_hot() {
                    let key_size = key.size();
                    self.total_key_bytes += key_size;
                    self.total_value_bytes += slot.size();
                    if let Some(old_slot) = self.base.shards[shard_id].insert(key, slot) {
                        self.total_key_bytes -= key_size;
                        self.total_value_bytes -= old_slot.size();
                        n_update += 1;
                    } else {
                        n_insert += 1;
                    }
                } else if let Some((key, old_slot)) = self.base.shards[shard_id].remove(&key) {
                    self.total_key_bytes -= key.size();
                    self.total_value_bytes -= old_slot.size();
                    n_evict += 1;
                }
```
