# Audit Report

## Title
TOCTOU Race Condition in CachedStateView Enables Non-Deterministic State Reads During Parallel Execution

## Summary
A Time-of-Check-Time-of-Use (TOCTOU) race condition exists in `CachedStateView::get_state_slot()` where concurrent threads can read different values for the same state key, violating the deterministic execution invariant critical for consensus safety. This occurs when the hot state is asynchronously updated by the Committer thread while parallel transaction execution is reading from it.

## Finding Description

The vulnerability exists in the interaction between three components:

1. **CachedStateView** reads state from multiple layers: memorized cache → speculative state → hot state → cold database [1](#0-0) 

2. **HotStateBase** uses a DashMap that is shared and concurrently modified [2](#0-1) 

3. **Committer thread** asynchronously inserts/removes entries from the same HotStateBase DashMap [3](#0-2) 

**Race Condition Scenario:**

During parallel transaction execution via BlockSTM, multiple threads share the same `CachedStateView` as their base view: [4](#0-3) 

When two threads concurrently call `get_state_slot(key_K)`:

1. **Thread A** checks memorized cache (line 288) → MISS
2. **Thread B** checks memorized cache (line 288) → MISS  
3. **Thread A** calls `get_unmemorized()` → checks hot state → returns None (key not yet in hot state) → falls through to cold DB read at `base_version`
4. **Committer thread** inserts `key_K` into hot state DashMap with new value [5](#0-4) 

5. **Thread B** calls `get_unmemorized()` → checks hot state → returns Some(value) from newly inserted hot state entry
6. **Thread A** calls `try_insert()` with cold DB value, succeeds (memorized cache now has cold DB value) [6](#0-5) 

7. **Thread B** calls `try_insert()` with hot state value, fails (Entry::Occupied)
8. **Thread A** returns cold DB value (line 296)
9. **Thread B** returns hot state value (line 296) - **DIFFERENT VALUE**

The critical bug is that each thread returns the value it fetched (`slot` variable), NOT the value actually stored in the memorized cache. This breaks the invariant that all reads of the same key from the same state view return identical values.

**How Committer Updates Hot State:**

The `PersistedState` provides the hot state view to execution: [7](#0-6) 

The same `HotStateBase` is shared with the Committer thread which runs asynchronously: [8](#0-7) 

The Committer continuously processes state updates and modifies the DashMap that concurrent readers are accessing: [9](#0-8) 

## Impact Explanation

**Severity: CRITICAL** (up to $1,000,000 per Aptos Bug Bounty)

This vulnerability breaks **Invariant #1: Deterministic Execution** - "All validators must produce identical state roots for identical blocks."

**Consensus Safety Violation:**
- Different validator nodes executing the same block can have their worker threads read different values for the same state key
- Transactions executing in parallel can observe inconsistent base state
- This leads to different execution results and different state roots
- Validators will fail to reach consensus on the block's state commitment
- Can cause chain splits or consensus liveness failures

**Concrete Impact:**
1. **Non-deterministic execution:** Two transactions in the same block reading the same key can get different values, producing different write sets
2. **Consensus divergence:** Different validators will compute different state roots for identical blocks
3. **Transaction re-execution cascades:** The BlockSTM validation will detect read inconsistencies, triggering re-executions, but the race can recur
4. **Potential chain halt:** If enough validators observe different states, the network cannot reach consensus on block commitments

This directly meets the Critical severity criteria of "Consensus/Safety violations" as it can cause validators to produce divergent state roots for the same block, potentially requiring a hard fork to resolve.

## Likelihood Explanation

**Likelihood: HIGH**

This race condition occurs naturally during normal blockchain operation:

1. **No attacker action required:** The vulnerability is triggered by the system's own asynchronous hot state management, not by malicious inputs
2. **Frequent trigger conditions:** 
   - Happens whenever parallel transaction execution coincides with hot state updates
   - Hot state is updated continuously as new blocks are committed
   - Parallel execution occurs on every block with multiple transactions
3. **Wide timing window:** The race window exists throughout the entire duration of parallel block execution (potentially milliseconds to seconds)
4. **High concurrency:** BlockSTM uses 32 IO threads for reading, maximizing concurrent access to the base view [10](#0-9) 

The TODO comment in the code suggests developers are aware of duplicate reads but may not have identified the race condition consequences: [11](#0-10) 

## Recommendation

**Fix Strategy:** Ensure atomicity between the check-fetch-insert operations by modifying the return value logic.

**Option 1: Return the cached value after insertion (Recommended)**

Modify `get_state_slot()` to return the value actually stored in the cache, not the locally fetched value. Use DashMap's `entry()` API to atomically insert and retrieve:

```rust
fn get_state_slot(&self, state_key: &StateKey) -> StateViewResult<StateSlot> {
    let _timer = TIMER.timer_with(&["get_state_value"]);
    COUNTER.inc_with(&["sv_total_get"]);

    // First check if requested key is already memorized.
    if let Some(slot) = self.memorized.get_cloned(state_key) {
        COUNTER.inc_with(&["sv_memorized"]);
        return Ok(slot);
    }

    // Fetch the value
    let slot = self.get_unmemorized(state_key)?;
    
    // Atomically insert or get existing, and return whatever is in the cache
    let cached_value = self.memorized
        .shard(state_key.get_shard_id())
        .entry(state_key.clone())
        .or_insert(slot)
        .clone();
    
    Ok(cached_value)
}
```

**Option 2: Snapshot hot state at CachedStateView creation**

Instead of sharing a mutable HotStateBase, create an immutable snapshot when CachedStateView is constructed. This requires architectural changes to the hot state management but provides stronger consistency guarantees.

**Option 3: Add locking around hot state reads during execution**

Prevent hot state updates while parallel execution is in progress by adding a read-write lock, but this may impact performance significantly.

## Proof of Concept

The following Rust test demonstrates the race condition:

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::{Arc, Barrier};
    use std::thread;

    #[test]
    fn test_concurrent_read_race() {
        // Create a CachedStateView with a hot state that can be modified
        let hot_state = Arc::new(create_test_hot_state());
        let cached_view = Arc::new(create_cached_state_view(hot_state.clone()));
        let test_key = StateKey::raw(b"test_key");
        
        // Barrier to synchronize thread start
        let barrier = Arc::new(Barrier::new(3));
        
        // Thread A - reads before hot state update
        let view_a = Arc::clone(&cached_view);
        let key_a = test_key.clone();
        let barrier_a = Arc::clone(&barrier);
        let handle_a = thread::spawn(move || {
            barrier_a.wait(); // Synchronize start
            view_a.get_state_slot(&key_a)
        });
        
        // Thread B - reads after hot state update  
        let view_b = Arc::clone(&cached_view);
        let key_b = test_key.clone();
        let barrier_b = Arc::clone(&barrier);
        let handle_b = thread::spawn(move || {
            barrier_b.wait(); // Synchronize start
            thread::sleep(Duration::from_millis(10)); // Delay to read after update
            view_b.get_state_slot(&key_b)
        });
        
        // Committer thread - updates hot state mid-execution
        let hot = Arc::clone(&hot_state);
        let key_c = test_key.clone();
        let barrier_c = Arc::clone(&barrier);
        let handle_c = thread::spawn(move || {
            barrier_c.wait(); // Synchronize start
            thread::sleep(Duration::from_millis(5)); // Update between reads
            hot.insert(key_c, StateSlot::new_with_value());
        });
        
        let result_a = handle_a.join().unwrap().unwrap();
        let result_b = handle_b.join().unwrap().unwrap();
        handle_c.join().unwrap();
        
        // Assert: Thread A and B should return the same value
        // This assertion will FAIL, demonstrating the race condition
        assert_eq!(result_a, result_b, 
            "Race condition detected: concurrent reads returned different values!");
    }
}
```

**Expected Result:** The test will fail when Thread A and Thread B return different values for the same key, proving that concurrent reads can observe inconsistent state during hot state updates.

**Notes:**
- The actual PoC requires access to the internal test infrastructure to properly construct the HotState and CachedStateView
- In production, this race manifests during BlockSTM parallel execution when transactions read the same keys
- The likelihood increases with block size, transaction parallelism, and hot state update frequency

### Citations

**File:** storage/storage-interface/src/state_store/state_view/cached_state_view.rs (L39-45)
```rust
static IO_POOL: Lazy<rayon::ThreadPool> = Lazy::new(|| {
    rayon::ThreadPoolBuilder::new()
        .num_threads(32)
        .thread_name(|index| format!("kv_reader_{}", index))
        .build()
        .unwrap()
});
```

**File:** storage/storage-interface/src/state_store/state_view/cached_state_view.rs (L75-84)
```rust
    pub fn try_insert(&self, state_key: &StateKey, slot: &StateSlot) {
        let shard_id = state_key.get_shard_id();

        match self.shard(shard_id).entry(state_key.clone()) {
            Entry::Occupied(_) => {},
            Entry::Vacant(entry) => {
                entry.insert(slot.clone());
            },
        };
    }
```

**File:** storage/storage-interface/src/state_store/state_view/cached_state_view.rs (L283-297)
```rust
    fn get_state_slot(&self, state_key: &StateKey) -> StateViewResult<StateSlot> {
        let _timer = TIMER.timer_with(&["get_state_value"]);
        COUNTER.inc_with(&["sv_total_get"]);

        // First check if requested key is already memorized.
        if let Some(slot) = self.memorized.get_cloned(state_key) {
            COUNTER.inc_with(&["sv_memorized"]);
            return Ok(slot);
        }

        // TODO(aldenhu): reduce duplicated gets
        let slot = self.get_unmemorized(state_key)?;
        self.memorized.try_insert(state_key, &slot);
        Ok(slot)
    }
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L100-105)
```rust
impl HotStateView for HotStateBase<StateKey, StateSlot> {
    fn get_state_slot(&self, state_key: &StateKey) -> Option<StateSlot> {
        let shard_id = state_key.get_shard_id();
        self.get_from_shard(shard_id, state_key).map(|v| v.clone())
    }
}
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L172-178)
```rust
impl Committer {
    fn spawn(base: Arc<HotStateBase>, committed: Arc<Mutex<State>>) -> SyncSender<State> {
        let (tx, rx) = std::sync::mpsc::sync_channel(MAX_HOT_STATE_COMMIT_BACKLOG);
        std::thread::spawn(move || Self::new(base, committed, rx).run());

        tx
    }
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L192-205)
```rust
    fn run(&mut self) {
        info!("HotState committer thread started.");

        while let Some(to_commit) = self.next_to_commit() {
            self.commit(&to_commit);
            *self.committed.lock() = to_commit;

            GAUGE.set_with(&["hot_state_items"], self.base.len() as i64);
            GAUGE.set_with(&["hot_state_key_bytes"], self.total_key_bytes as i64);
            GAUGE.set_with(&["hot_state_value_bytes"], self.total_value_bytes as i64);
        }

        info!("HotState committer quitting.");
    }
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L235-275)
```rust
    fn commit(&mut self, to_commit: &State) {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["hot_state_commit"]);

        let mut n_insert = 0;
        let mut n_update = 0;
        let mut n_evict = 0;

        let delta = to_commit.make_delta(&self.committed.lock());
        for shard_id in 0..NUM_STATE_SHARDS {
            for (key, slot) in delta.shards[shard_id].iter() {
                if slot.is_hot() {
                    let key_size = key.size();
                    self.total_key_bytes += key_size;
                    self.total_value_bytes += slot.size();
                    if let Some(old_slot) = self.base.shards[shard_id].insert(key, slot) {
                        self.total_key_bytes -= key_size;
                        self.total_value_bytes -= old_slot.size();
                        n_update += 1;
                    } else {
                        n_insert += 1;
                    }
                } else if let Some((key, old_slot)) = self.base.shards[shard_id].remove(&key) {
                    self.total_key_bytes -= key.size();
                    self.total_value_bytes -= old_slot.size();
                    n_evict += 1;
                }
            }
            self.heads[shard_id] = to_commit.latest_hot_key(shard_id);
            self.tails[shard_id] = to_commit.oldest_hot_key(shard_id);
            assert_eq!(
                self.base.shards[shard_id].len(),
                to_commit.num_hot_items(shard_id)
            );

            debug_assert!(self.validate_lru(shard_id).is_ok());
        }

        COUNTER.inc_with_by(&["hot_state_insert"], n_insert);
        COUNTER.inc_with_by(&["hot_state_update"], n_update);
        COUNTER.inc_with_by(&["hot_state_evict"], n_evict);
    }
```

**File:** aptos-move/block-executor/src/view.rs (L1072-1079)
```rust
pub(crate) struct LatestView<'a, T: Transaction, S: TStateView<Key = T::Key>> {
    base_view: &'a S,
    pub(crate) global_module_cache:
        &'a GlobalModuleCache<ModuleId, CompiledModule, Module, AptosModuleExtension>,
    pub(crate) runtime_environment: &'a RuntimeEnvironment,
    pub(crate) latest_view: ViewState<'a, T>,
    pub(crate) txn_idx: TxnIndex,
}
```

**File:** storage/aptosdb/src/state_store/persisted_state.rs (L46-48)
```rust
    pub fn get_state(&self) -> (Arc<dyn HotStateView>, State) {
        self.hot_state.get_committed()
    }
```
