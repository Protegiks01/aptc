# Audit Report

## Title
Synchronous BCS Deserialization of Large Network Frames Enables CPU Exhaustion Attack

## Summary
The network layer performs synchronous BCS deserialization of incoming network frames (up to 4 MiB each) directly in the async event loop without offloading to blocking tasks. An attacker can open multiple connections and send batches of maximum-size frames to exhaust CPU resources through repeated expensive deserialization operations, causing validator node slowdowns.

## Finding Description

The Aptos network protocol implements a two-level deserialization architecture:

1. **Wire-level deserialization**: Incoming frames are BCS-deserialized into `MultiplexMessage` structs
2. **Application-level deserialization**: Messages are further deserialized into application-specific types

The vulnerability exists at the wire level. When a network frame arrives, the `MultiplexMessageStream::poll_next` method synchronously deserializes it using BCS: [1](#0-0) 

This deserialization happens directly in the async context, blocking the event loop until complete. While application-level deserialization is protected by offloading to `spawn_blocking` tasks with parallelism limits: [2](#0-1) 

The wire-level deserialization has no such protection.

**Attack Path:**

1. Attacker opens multiple connections (up to `MAX_INBOUND_CONNECTIONS`, default 100): [3](#0-2) 

2. Each connection is handled by a separate Peer task: [4](#0-3) 

3. Attacker sends maximum-size frames (4 MiB each) on each connection: [5](#0-4) 

4. Each frame triggers synchronous BCS deserialization in the Peer's event loop
5. With 100 connections all deserializing large frames, CPU exhaustion occurs

The vulnerability violates the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." No rate limiting exists on the number of messages (only byte-based rate limiting on the socket).

## Impact Explanation

This vulnerability meets **High Severity** criteria per the Aptos bug bounty program:
- **Validator node slowdowns**: Sustained CPU exhaustion degrades consensus participation
- **Protocol availability impact**: Slow message processing affects network responsiveness

The attack causes:
- CPU exhaustion across executor threads handling peer connections
- Delayed processing of consensus messages and transactions
- Potential consensus timeout failures if critical messages are delayed
- Degraded validator performance and missed block proposals

While byte-based rate limiting may exist at the socket level, it does not prevent an attacker from:
- Sending many large messages within the byte budget
- Opening maximum allowed connections (100) to amplify the attack
- Forcing repeated expensive deserialization operations

## Likelihood Explanation

**Likelihood: High**

The attack is straightforward to execute:
- Requires only network access to validator nodes (typical for P2P networks)
- No privileged access or authentication bypass needed
- Can be launched from multiple source IPs to bypass IP-based rate limits
- Frames only need to be valid BCS-encoded data (no complex crafting required)

The impact becomes significant when:
- Multiple attackers coordinate (or single attacker uses multiple IPs)
- Target validator is under load from normal operations
- Attack sustained over time to prevent recovery

Existing mitigations are insufficient:
- Byte-based rate limiting doesn't prevent the attack (large messages fit within limits)
- Connection limits (100) are high enough for effective attack
- TCP backpressure only limits socket reading, not deserialization cost

## Recommendation

**Solution: Offload wire-level BCS deserialization to blocking tasks**

Modify `MultiplexMessageStream::poll_next` to perform BCS deserialization in `tokio::task::spawn_blocking`:

```rust
// In MultiplexMessageStream implementation
impl<TReadSocket: AsyncRead + Unpin> Stream for MultiplexMessageStream<TReadSocket> {
    type Item = Result<MultiplexMessage, ReadError>;

    fn poll_next(self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Option<Self::Item>> {
        // Instead of synchronous deserialization, spawn blocking task
        // and track the pending future
    }
}
```

**Alternative mitigations:**

1. **Add per-message rate limiting**: Implement token bucket rate limiter for message count (not just bytes)
2. **Reduce MAX_INBOUND_CONNECTIONS**: Lower the connection limit for unknown peers
3. **Add deserialization timeout**: Kill tasks that take too long deserializing
4. **Implement priority queuing**: Process consensus messages before untrusted peer messages

The recommended fix maintains wire protocol compatibility while protecting against resource exhaustion.

## Proof of Concept

```rust
// Attacker code (pseudo-Rust)
use tokio::net::TcpStream;

async fn attack_validator(target: &str) {
    // Open maximum connections
    let mut connections = vec![];
    for _ in 0..100 {
        if let Ok(stream) = TcpStream::connect(target).await {
            connections.push(stream);
        }
    }
    
    // Create maximum-size frame with valid BCS data
    let max_frame_size = 4 * 1024 * 1024; // 4 MiB
    let attack_message = create_large_valid_bcs_message(max_frame_size);
    
    // Send frames on all connections simultaneously
    loop {
        for conn in &mut connections {
            // Send length-delimited frame
            let _ = send_frame(conn, &attack_message).await;
        }
    }
}

fn create_large_valid_bcs_message(size: usize) -> Vec<u8> {
    // Create a valid DirectSendMsg with large payload
    let msg = NetworkMessage::DirectSendMsg(DirectSendMsg {
        protocol_id: ProtocolId::ConsensusDirectSend,
        priority: 0,
        raw_msg: vec![0u8; size - 100], // Fill to max size
    });
    bcs::to_bytes(&msg).unwrap()
}
```

**To reproduce:**
1. Deploy test validator node
2. Run attacker code against validator endpoint
3. Monitor validator CPU usage (should spike to 100%)
4. Observe consensus message processing delays via metrics
5. Confirm slowdown persists while attack continues

## Notes

This vulnerability exists because wire-level deserialization was not designed with resource exhaustion attacks in mind. The application-level deserialization correctly uses `spawn_blocking` with parallelism limits, but this protection doesn't extend to the wire protocol layer where the attack surface exists.

The fix requires careful implementation to avoid introducing new issues (e.g., task spawn overhead, backpressure management). The change should be tested under load to ensure it doesn't degrade normal performance while successfully mitigating the attack.

### Citations

**File:** network/framework/src/protocols/wire/messaging/v1/mod.rs (L225-241)
```rust
    fn poll_next(self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Option<Self::Item>> {
        match self.project().framed_read.poll_next(cx) {
            Poll::Ready(Some(Ok(frame))) => {
                let frame = frame.freeze();

                match bcs::from_bytes(&frame) {
                    Ok(message) => Poll::Ready(Some(Ok(message))),
                    // Failed to deserialize the NetworkMessage
                    Err(err) => {
                        let mut frame = frame;
                        let frame_len = frame.len();
                        // Keep a few bytes from the frame for debugging
                        frame.truncate(8);
                        let err = ReadError::DeserializeError(err, frame_len, frame);
                        Poll::Ready(Some(Err(err)))
                    },
                }
```

**File:** network/framework/src/protocols/network/mod.rs (L217-219)
```rust
        let data_event_stream = peer_mgr_notifs_rx.map(|notification| {
            tokio::task::spawn_blocking(move || received_message_to_event(notification))
        });
```

**File:** config/src/config/network_config.rs (L44-44)
```rust
pub const MAX_INBOUND_CONNECTIONS: usize = 100;
```

**File:** config/src/config/network_config.rs (L49-49)
```rust
pub const MAX_FRAME_SIZE: usize = 4 * 1024 * 1024; /* 4 MiB large messages will be chunked into multiple frames and streamed */
```

**File:** network/framework/src/peer_manager/mod.rs (L679-679)
```rust
        self.executor.spawn(peer.start());
```
