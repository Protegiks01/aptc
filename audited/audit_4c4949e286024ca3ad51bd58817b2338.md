# Audit Report

## Title
Peer Monitoring Service: Stale State Mutation Through Arc References After Peer Disconnect/Reconnect Cycle

## Summary
The `get_peer_state()` function clones `PeerState` objects containing shared `Arc` references, allowing mutations to persist even after the peer is removed from the HashMap during garbage collection. When the peer reconnects, concurrent operations can occur on both old (detached) and new state objects, causing duplicate monitoring requests and resource waste.

## Finding Description

The vulnerability exists in the peer monitoring service's state management design: [1](#0-0) 

The `PeerState` structure contains an `Arc<RwLock<HashMap>>` that is shallow-copied during cloning: [2](#0-1) 

When `get_peer_state()` is called, it clones the `PeerState`, incrementing the `Arc` reference count but pointing to the same underlying data. The cloned state is then used to spawn async monitoring tasks: [3](#0-2) 

The async task holds Arc references that can outlive the peer's presence in the HashMap: [4](#0-3) 

**Attack Scenario:**
1. Peer X connects, `PeerState` created in HashMap
2. Main loop clones `PeerState`, spawns async task A
3. Peer X disconnects, garbage collection removes from HashMap: [5](#0-4) 
4. Task A still holds cloned `PeerState` with Arc references
5. Peer X reconnects, new `PeerState` created: [6](#0-5) 
6. Main loop clones new `PeerState`, spawns async task B
7. Both tasks run concurrently for the same peer, sending duplicate requests

This violates the request tracking invariant enforced by `RequestTracker`: [7](#0-6) 

The check at line 78-79 is designed to prevent concurrent requests, but fails when two separate `PeerState` objects exist for the same peer.

## Impact Explanation

This issue qualifies as **Medium Severity** under the Aptos bug bounty program:
- **State inconsistencies requiring intervention**: The peer monitoring state becomes inconsistent with duplicate tracking states for the same peer
- Potential for resource exhaustion through duplicate requests if exploited at scale
- Incorrect metrics affecting consensus observer peer selection: [8](#0-7) 

While the question labels this as "High", it does not clearly meet the High severity criteria:
- No demonstrated validator node slowdown at realistic scale
- No API crashes identified
- Peer monitoring is not consensus-critical

The peer monitoring metadata is used for peer selection but does not affect consensus safety, fund security, or critical protocol invariants.

## Likelihood Explanation

**Medium Likelihood**: 
- Requires attacker to control peers capable of rapid connect/disconnect cycles
- Network layer may implement rate limiting or connection controls
- Race window requires specific timing (peer disconnect between clone and task spawn)
- Production networks may have protections against rapid connection churn

The vulnerability is real but exploitation requires sustained peer churn at scale to cause measurable impact.

## Recommendation

Implement one of the following fixes:

**Option 1: Peer ID validation in async tasks**
```rust
// In refresh_peer_state_key, add peer ID tracking
let peer_state_generation = peer_state.get_generation_id();
let request_task = async move {
    // Before sending request, verify peer still exists with same generation
    if !is_peer_state_current(peer_network_id, peer_state_generation) {
        return; // Abort stale task
    }
    // ... rest of task
};
```

**Option 2: Cancel tokens for garbage collected peers**
Implement cancellation tokens that terminate async tasks when peers are garbage collected.

**Option 3: Direct HashMap access with lock held**
Remove cloning and hold the HashMap read lock during operations (trades off performance for consistency).

## Proof of Concept

```rust
// Rust test demonstrating the race condition
#[tokio::test]
async fn test_stale_state_mutation_race() {
    let node_config = NodeConfig::default();
    let time_service = TimeService::mock();
    let peer_monitor_state = PeerMonitorState::new();
    let peer_id = PeerNetworkId::random();
    
    // Create initial state
    peer_monitor_state.peer_states.write().insert(
        peer_id,
        PeerState::new(node_config.clone(), time_service.clone())
    );
    
    // Clone state (simulating get_peer_state)
    let cloned_state_1 = peer_monitor_state.peer_states.read().get(&peer_id).cloned().unwrap();
    
    // Simulate peer disconnect and garbage collection
    peer_monitor_state.peer_states.write().remove(&peer_id);
    
    // Simulate peer reconnect with new state
    peer_monitor_state.peer_states.write().insert(
        peer_id,
        PeerState::new(node_config.clone(), time_service.clone())
    );
    
    // Clone new state
    let cloned_state_2 = peer_monitor_state.peer_states.read().get(&peer_id).cloned().unwrap();
    
    // Verify both states can mark requests as in-flight concurrently
    let tracker_1 = cloned_state_1.get_request_tracker(&PeerStateKey::LatencyInfo).unwrap();
    let tracker_2 = cloned_state_2.get_request_tracker(&PeerStateKey::LatencyInfo).unwrap();
    
    assert!(!tracker_1.read().in_flight_request());
    assert!(!tracker_2.read().in_flight_request());
    
    tracker_1.write().request_started();
    tracker_2.write().request_started();
    
    // Both show in-flight, demonstrating duplicate request possibility
    assert!(tracker_1.read().in_flight_request());
    assert!(tracker_2.read().in_flight_request());
}
```

## Notes

While the cloned state mutation through Arc references is technically exploitable, the practical security impact is limited to resource waste and monitoring inconsistencies rather than consensus safety violations. The issue represents a design tradeoff between performance (avoiding long-held locks) and strict state isolation. The severity should be considered **Medium** rather than High, as it does not clearly meet the Aptos bug bounty criteria for High severity (validator node slowdowns, API crashes, or significant protocol violations).

### Citations

**File:** peer-monitoring-service/client/src/peer_states/mod.rs (L91-106)
```rust
fn get_peer_state(
    peer_monitor_state: &PeerMonitorState,
    peer_network_id: &PeerNetworkId,
) -> Result<PeerState, Error> {
    let peer_state = peer_monitor_state
        .peer_states
        .read()
        .get(peer_network_id)
        .cloned();
    peer_state.ok_or_else(|| {
        Error::UnexpectedError(format!(
            "Failed to find the peer state. This shouldn't happen! Peer: {:?}",
            peer_network_id
        ))
    })
}
```

**File:** peer-monitoring-service/client/src/peer_states/peer_state.rs (L37-40)
```rust
#[derive(Clone, Debug)]
pub struct PeerState {
    state_entries: Arc<RwLock<HashMap<PeerStateKey, Arc<RwLock<PeerStateValue>>>>>, // The state entries for the peer
}
```

**File:** peer-monitoring-service/client/src/peer_states/peer_state.rs (L80-84)
```rust
        // Mark the request as having started. We do this here to prevent
        // the monitor loop from selecting the same peer state key concurrently.
        let request_tracker = self.get_request_tracker(peer_state_key)?;
        request_tracker.write().request_started();

```

**File:** peer-monitoring-service/client/src/peer_states/peer_state.rs (L98-122)
```rust
        let request_task = async move {
            // Add some amount of jitter before sending the request.
            // This helps to prevent requests from becoming too bursty.
            sleep(Duration::from_millis(request_jitter_ms)).await;

            // Start the request timer
            let start_time = time_service.now();

            // Send the request to the peer and wait for a response
            let request_id = request_id_generator.next();
            let monitoring_service_response = network::send_request_to_peer(
                peer_monitoring_client,
                &peer_network_id,
                request_id,
                monitoring_service_request.clone(),
                request_timeout_ms,
            )
            .await;

            // Stop the timer and calculate the duration
            let request_duration_secs = start_time.elapsed().as_secs_f64();

            // Mark the in-flight request as now complete
            request_tracker.write().request_completed();

```

**File:** peer-monitoring-service/client/src/lib.rs (L160-178)
```rust
fn create_states_for_new_peers(
    node_config: &NodeConfig,
    peer_monitor_state: &PeerMonitorState,
    time_service: &TimeService,
    connected_peers_and_metadata: &HashMap<PeerNetworkId, PeerMetadata>,
) {
    for peer_network_id in connected_peers_and_metadata.keys() {
        let state_exists = peer_monitor_state
            .peer_states
            .read()
            .contains_key(peer_network_id);
        if !state_exists {
            peer_monitor_state.peer_states.write().insert(
                *peer_network_id,
                PeerState::new(node_config.clone(), time_service.clone()),
            );
        }
    }
}
```

**File:** peer-monitoring-service/client/src/lib.rs (L181-202)
```rust
fn garbage_collect_peer_states(
    peer_monitor_state: &PeerMonitorState,
    connected_peers_and_metadata: &HashMap<PeerNetworkId, PeerMetadata>,
) {
    // Get the set of peers with existing states
    let peers_with_existing_states: Vec<PeerNetworkId> = peer_monitor_state
        .peer_states
        .read()
        .keys()
        .cloned()
        .collect();

    // Remove the states for disconnected peers
    for peer_network_id in peers_with_existing_states {
        if !connected_peers_and_metadata.contains_key(&peer_network_id) {
            peer_monitor_state
                .peer_states
                .write()
                .remove(&peer_network_id);
        }
    }
}
```

**File:** peer-monitoring-service/client/src/peer_states/request_tracker.rs (L76-90)
```rust
    pub fn new_request_required(&self) -> bool {
        // There's already an in-flight request. A new one should not be sent.
        if self.in_flight_request() {
            return false;
        }

        // Otherwise, check the last request time for freshness
        match self.last_request_time {
            Some(last_request_time) => {
                self.time_service.now()
                    > last_request_time.add(Duration::from_micros(self.request_interval_usec))
            },
            None => true, // A request should be sent immediately
        }
    }
```

**File:** consensus/src/consensus_observer/observer/subscription_utils.rs (L283-312)
```rust
pub fn sort_peers_by_subscription_optimality(
    peers_and_metadata: &HashMap<PeerNetworkId, PeerMetadata>,
) -> Vec<PeerNetworkId> {
    // Group peers and latencies by validator distance, i.e., distance -> [(peer, latency)]
    let mut unsupported_peers = Vec::new();
    let mut peers_and_latencies_by_distance = BTreeMap::new();
    for (peer_network_id, peer_metadata) in peers_and_metadata {
        // Verify that the peer supports consensus observer
        if !supports_consensus_observer(peer_metadata) {
            unsupported_peers.push(*peer_network_id);
            continue; // Skip the peer
        }

        // Get the distance and latency for the peer
        let distance = get_distance_for_peer(peer_network_id, peer_metadata);
        let latency = get_latency_for_peer(peer_network_id, peer_metadata);

        // If the distance is not found, use the maximum distance
        let distance =
            distance.unwrap_or(aptos_peer_monitoring_service_types::MAX_DISTANCE_FROM_VALIDATORS);

        // If the latency is not found, use a large latency
        let latency = latency.unwrap_or(MAX_PING_LATENCY_SECS);

        // Add the peer and latency to the distance group
        peers_and_latencies_by_distance
            .entry(distance)
            .or_insert_with(Vec::new)
            .push((*peer_network_id, OrderedFloat(latency)));
    }
```
