# Audit Report

## Title
Memory Limit Bypass in State Sync via First-Item Exception and Compression Mismatch

## Summary
The `max_response_bytes` limit set by clients in state sync requests can be bypassed by up to 21x, potentially causing memory exhaustion. The server's "always include first item" policy allows responses exceeding the client's limit, while compression/decompression validation uses `MAX_APPLICATION_MESSAGE_SIZE` instead of the client-requested limit.

## Finding Description

The vulnerability exists in the state sync protocol's size enforcement mechanism across three components:

**1. Client-side limit setting:** [1](#0-0) 

**2. Server-side "first item always included" policy:** [2](#0-1) 

When `always_allow_first_item` is true (which occurs for transaction and transaction output requests), the first data item is included even if it exceeds `max_response_size`. This is seen in the transaction output path: [3](#0-2) 

And in transaction requests: [4](#0-3) 

**3. Compression using MAX_APPLICATION_MESSAGE_SIZE instead of max_response_bytes:** [5](#0-4) 

**4. Decompression validation also using MAX_APPLICATION_MESSAGE_SIZE:** [6](#0-5) 

**Attack Flow:**

1. Client sets `max_response_bytes = 1 MB` to limit memory usage
2. Client requests transaction outputs from a peer
3. Server calculates effective limit but allows first item regardless of size
4. First transaction output has uncompressed size of ~21 MB (maximum possible: 10 MB write sets + 10 MB events + 1 MB transaction): [7](#0-6) 

5. Server serializes 21 MB response and compresses it (e.g., to 12 MB)
6. Compression check passes because 12 MB < 61.875 MiB (`MAX_APPLICATION_MESSAGE_SIZE`)
7. Client receives compressed response and decompresses
8. Decompression reads size prefix (21 MB), validates against `MAX_APPLICATION_MESSAGE_SIZE` (not original 1 MB limit), allocates 21 MB
9. Client expected 1 MB but allocated 21 MB (21x amplification)

## Impact Explanation

This vulnerability meets **HIGH severity** criteria per the Aptos bug bounty program:

- **Validator node slowdowns**: Nodes performing state sync with multiple concurrent streams can experience memory exhaustion, leading to performance degradation
- **API crashes**: On resource-constrained nodes, unexpected memory allocations across many concurrent requests can trigger OOM conditions, causing crashes

If a node has 100 concurrent state sync streams:
- Expected memory allocation: 100 × 1 MB = 100 MB  
- Actual memory allocation: 100 × 21 MB = 2.1 GB

This breaks **Critical Invariant #9**: "Resource Limits: All operations must respect gas, storage, and computational limits." The `max_response_bytes` parameter is explicitly designed to limit resource consumption, but can be bypassed.

## Likelihood Explanation

**Likelihood: MEDIUM to HIGH**

This issue occurs whenever:
1. Clients set low `max_response_bytes` values for memory management
2. Large transactions (with substantial outputs/events) exist on-chain
3. State sync requests fetch these large transactions

The "always include first item" design is intentional for forward progress, making this a systematic issue rather than an edge case. Any syncing node can be affected, and the presence of governance transactions (up to 1 MB) or transactions with maximum outputs (10 MB + 10 MB) makes exploitation likely.

## Recommendation

**Fix 1: Enforce client limit during decompression**

Modify the decompression validation to respect the original `max_response_bytes` from the request: [6](#0-5) 

Pass the client's `max_response_bytes` to `get_data_response()` and validate the decompressed size against it, not just `MAX_APPLICATION_MESSAGE_SIZE`.

**Fix 2: Add post-compression size validation**

After compression, verify that the compressed response size doesn't exceed `max_response_bytes` (with some overhead allowance for compression metadata): [5](#0-4) 

**Fix 3: Limit "first item exception" magnitude**

Modify the "always include first item" policy to have a bounded exception (e.g., max 2x the limit): [2](#0-1) 

Add a check: `if always_allow_first_item && self.num_items_fetched == 0 && serialized_data_size <= (self.max_response_size * 2)`

## Proof of Concept

```rust
// Rust test demonstrating the vulnerability
#[tokio::test]
async fn test_max_response_bytes_bypass() {
    // Setup: Create a storage service with a large transaction output
    // containing maximum allowed write sets (10 MB) and events (10 MB)
    
    // Client requests with max_response_bytes = 1 MB
    let client_config = AptosDataClientConfig {
        max_response_bytes: 1_000_000, // 1 MB limit
        ..Default::default()
    };
    
    // Request transaction outputs
    let request = StorageServiceRequest::new(
        DataRequest::GetTransactionOutputsWithProof(...),
        true, // use_compression
    );
    
    // Server builds response
    // First transaction output is 21 MB (due to max write sets + events)
    // Server allows it due to "always include first item" policy
    
    // Measure memory allocated during decompression
    let response = client.send_request_to_peer_and_decode::<...>(peer, request, timeout).await?;
    
    // Assert: Memory allocated (21 MB) >> requested limit (1 MB)
    // This demonstrates the 21x amplification vulnerability
}
```

**Notes:**

The root cause is a **design flaw** in the interaction between three components:
1. Server's "always include first item" policy (intentional for forward progress)
2. Compression/decompression using `MAX_APPLICATION_MESSAGE_SIZE` instead of client limits
3. Transaction output size limits allowing up to ~21 MB per transaction

This creates a bounded but significant memory limit bypass that can cause availability issues on syncing nodes.

### Citations

**File:** state-sync/aptos-data-client/src/client.rs (L153-156)
```rust
    /// Returns the maximum number of bytes that can be returned in a single response
    fn get_max_response_bytes(&self) -> u64 {
        self.data_client_config.max_response_bytes
    }
```

**File:** state-sync/storage-service/server/src/storage.rs (L438-440)
```rust
                    if response_progress_tracker
                        .data_items_fits_in_response(true, total_serialized_bytes)
                    {
```

**File:** state-sync/storage-service/server/src/storage.rs (L1111-1118)
```rust
        self.get_transaction_outputs_with_proof_by_size(
            proof_version,
            start_version,
            end_version,
            self.config.max_network_chunk_bytes,
            false,
            self.config.enable_size_and_time_aware_chunking,
        )
```

**File:** state-sync/storage-service/server/src/storage.rs (L1394-1412)
```rust
    /// Returns true iff the given data item fits in the response
    /// (i.e., it does not overflow the maximum response size).
    ///
    /// Note: If `always_allow_first_item` is true, the first item is
    /// always allowed (even if it overflows the maximum response size).
    pub fn data_items_fits_in_response(
        &self,
        always_allow_first_item: bool,
        serialized_data_size: u64,
    ) -> bool {
        if always_allow_first_item && self.num_items_fetched == 0 {
            true // We always include at least one item
        } else {
            let new_serialized_data_size = self
                .serialized_data_size
                .saturating_add(serialized_data_size);
            new_serialized_data_size < self.max_response_size
        }
    }
```

**File:** state-sync/storage-service/types/src/responses.rs (L74-94)
```rust
    pub fn new(data_response: DataResponse, perform_compression: bool) -> Result<Self, Error> {
        if perform_compression {
            // Serialize and compress the raw data
            let raw_data = bcs::to_bytes(&data_response)
                .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
            let compressed_data = aptos_compression::compress(
                raw_data,
                CompressionClient::StateSync,
                MAX_APPLICATION_MESSAGE_SIZE,
            )?;

            // Create the compressed response
            let label = data_response.get_label().to_string() + COMPRESSION_SUFFIX_LABEL;
            Ok(StorageServiceResponse::CompressedResponse(
                label,
                compressed_data,
            ))
        } else {
            Ok(StorageServiceResponse::RawResponse(data_response))
        }
    }
```

**File:** state-sync/storage-service/types/src/responses.rs (L97-111)
```rust
    pub fn get_data_response(&self) -> Result<DataResponse, Error> {
        match self {
            StorageServiceResponse::CompressedResponse(_, compressed_data) => {
                let raw_data = aptos_compression::decompress(
                    compressed_data,
                    CompressionClient::StateSync,
                    MAX_APPLICATION_MESSAGE_SIZE,
                )?;
                let data_response = bcs::from_bytes::<DataResponse>(&raw_data)
                    .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                Ok(data_response)
            },
            StorageServiceResponse::RawResponse(data_response) => Ok(data_response.clone()),
        }
    }
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L159-172)
```rust
            max_bytes_all_write_ops_per_transaction: NumBytes,
            { 5.. => "max_bytes_all_write_ops_per_transaction" },
            10 << 20, // all write ops from a single transaction are 10MB max
        ],
        [
            max_bytes_per_event: NumBytes,
            { 5.. => "max_bytes_per_event" },
            1 << 20, // a single event is 1MB max
        ],
        [
            max_bytes_all_events_per_transaction: NumBytes,
            { 5.. => "max_bytes_all_events_per_transaction"},
            10 << 20, // all events from a single transaction are 10MB max
        ],
```
