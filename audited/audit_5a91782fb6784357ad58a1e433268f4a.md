# Audit Report

## Title
TOCTTOU Race Condition in ScheduledBroadcast::poll() Causing Lost Wakeups and Broadcast Stalls

## Summary
A Time-of-Check to Time-of-Use (TOCTTOU) race condition exists in the `ScheduledBroadcast::poll()` implementation between checking if the deadline has passed and storing the waker. This race can cause the spawned timer task to fire and consume the waker while `poll()` is still executing, resulting in a newly stored waker that will never be woken. This causes mempool broadcasts to stall indefinitely, degrading transaction propagation across the network.

## Finding Description

The `ScheduledBroadcast` struct implements a custom Future for scheduling mempool broadcasts at specific deadlines. [1](#0-0) 

The implementation spawns a tokio task that sleeps until the deadline and then wakes the stored waker: [2](#0-1) 

The `poll()` method checks if the current time is before the deadline, and if so, stores the waker and returns Pending: [3](#0-2) 

**The Race Condition:**

The vulnerability occurs when there is no atomicity between checking `Instant::now() < self.deadline` and storing the waker. The following sequence can occur:

1. Thread 1 (polling): `poll()` is called at time T (microseconds before deadline)
2. Thread 1: Checks `Instant::now() < self.deadline` â†’ returns TRUE
3. **Context switch or scheduling delay**
4. **Deadline passes**
5. Thread 2 (spawned task): Timer expires, wakes up from `sleep_until()`
6. Thread 2: Acquires lock on `self.waker.lock()`
7. Thread 2: Executes `waker.take()` and calls `waker.wake()`
8. Thread 2: Releases lock (waker is now None)
9. Thread 1: Resumes execution, acquires lock on `self.waker.lock()`
10. Thread 1: Stores NEW waker: `*waker = Some(waker_clone)`
11. Thread 1: Returns `Poll::Pending`

**Result:** The spawned task has already fired and will never fire again. The newly stored waker will NEVER be woken, causing the broadcast to stall indefinitely until some external event triggers another poll.

The broadcasts are scheduled in `execute_broadcast`: [4](#0-3) 

With default configuration, broadcasts occur every 10ms in normal mode or 30 seconds in backoff mode: [5](#0-4) 

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos Bug Bounty criteria:

1. **Validator Node Slowdowns**: When broadcasts stall, validators cannot efficiently propagate transactions to peers, causing slowdowns in transaction dissemination.

2. **Transaction Propagation Degradation**: Stuck broadcasts prevent timely delivery of transactions across the network. This affects the mempool's ability to synchronize transaction state between nodes.

3. **Indirect Consensus Impact**: While not a direct consensus violation, delayed transaction propagation can impact consensus performance. Validators may not have access to transactions needed for block proposals in a timely manner.

4. **Network-Wide Effect**: The coordinator polls multiple scheduled broadcasts simultaneously: [6](#0-5) 

If multiple broadcasts become stuck due to this race condition, transaction propagation across the entire network can be severely degraded.

## Likelihood Explanation

**Likelihood: Medium-High**

While the race window is extremely small (microseconds), several factors make this likely to occur:

1. **Frequent Execution**: Mempool broadcasts occur every 10ms by default, resulting in thousands of broadcast scheduling events per second across multiple peers.

2. **Probabilistic Occurrence**: Over time, with millions of broadcasts, the probability of hitting this race condition becomes significant, especially under high load or CPU contention.

3. **Scheduling Variance**: Modern operating systems can introduce scheduling delays at any point. Under high system load, context switches can easily occur within the microsecond window between the deadline check and waker storage.

4. **No External Trigger Required**: This is not an attack that requires malicious behavior - it's a timing bug that can occur naturally during normal operation.

5. **Difficult to Diagnose**: When broadcasts stall, there is no error message or indication of what went wrong, making the issue difficult to detect and diagnose in production.

## Recommendation

Implement atomic check-and-store logic by moving the deadline check inside the lock, or redesign the waker management to eliminate the race condition:

**Option 1: Check deadline after acquiring lock**
```rust
fn poll(self: Pin<&mut Self>, context: &mut Context) -> Poll<Self::Output> {
    let mut waker = self.waker.lock();
    
    if Instant::now() < self.deadline {
        let waker_clone = context.waker().clone();
        *waker = Some(waker_clone);
        Poll::Pending
    } else {
        Poll::Ready((self.peer, self.backoff))
    }
}
```

**Option 2: Use atomic flag for synchronization**
```rust
pub(crate) struct ScheduledBroadcast {
    deadline: Instant,
    peer: PeerNetworkId,
    backoff: bool,
    waker: Arc<Mutex<Option<Waker>>>,
    fired: Arc<AtomicBool>,
}

// In new():
let fired = Arc::new(AtomicBool::new(false));
let fired_clone = fired.clone();

executor.spawn(async move {
    tokio::time::sleep_until(tokio_instant).await;
    fired_clone.store(true, Ordering::SeqCst);
    let mut waker = waker_clone.lock();
    if let Some(waker) = waker.take() {
        waker.wake()
    }
});

// In poll():
fn poll(self: Pin<&mut Self>, context: &mut Context) -> Poll<Self::Output> {
    if self.fired.load(Ordering::SeqCst) || Instant::now() >= self.deadline {
        Poll::Ready((self.peer, self.backoff))
    } else {
        let waker_clone = context.waker().clone();
        let mut waker = self.waker.lock();
        *waker = Some(waker_clone);
        Poll::Pending
    }
}
```

**Option 3: Use tokio::time::Sleep directly (recommended)**
```rust
pub(crate) struct ScheduledBroadcast {
    deadline: Pin<Box<tokio::time::Sleep>>,
    peer: PeerNetworkId,
    backoff: bool,
}

impl ScheduledBroadcast {
    pub fn new(deadline: Instant, peer: PeerNetworkId, backoff: bool, _executor: Handle) -> Self {
        let tokio_instant = tokio::time::Instant::from_std(deadline);
        Self {
            deadline: Box::pin(tokio::time::sleep_until(tokio_instant)),
            peer,
            backoff,
        }
    }
}

impl Future for ScheduledBroadcast {
    type Output = (PeerNetworkId, bool);

    fn poll(mut self: Pin<&mut Self>, context: &mut Context) -> Poll<Self::Output> {
        match self.deadline.as_mut().poll(context) {
            Poll::Ready(_) => Poll::Ready((self.peer, self.backoff)),
            Poll::Pending => Poll::Pending,
        }
    }
}
```

## Proof of Concept

```rust
// This test demonstrates the race condition by adding artificial delays
// to increase the probability of hitting the race window

use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::runtime::Handle;
use futures::task::{Context, Poll};
use futures::Future;
use std::pin::Pin;

#[tokio::test]
async fn test_scheduled_broadcast_race_condition() {
    use mempool::shared_mempool::types::ScheduledBroadcast;
    use aptos_config::network_id::{NetworkId, PeerNetworkId};
    use aptos_types::PeerId;
    
    let runtime = Handle::current();
    let peer = PeerNetworkId::new(NetworkId::Validator, PeerId::random());
    
    // Schedule a broadcast with a very short deadline (1ms)
    let deadline = Instant::now() + Duration::from_millis(1);
    let mut broadcast = Box::pin(ScheduledBroadcast::new(
        deadline,
        peer,
        false,
        runtime.clone(),
    ));
    
    // Create a waker
    let waker = futures::task::noop_waker();
    let mut context = Context::from_waker(&waker);
    
    // Poll the future multiple times in rapid succession
    // This increases the chance of hitting the race condition
    for _ in 0..100 {
        match broadcast.as_mut().poll(&mut context) {
            Poll::Ready(_) => {
                println!("Broadcast completed normally");
                return;
            }
            Poll::Pending => {
                // Add small delay to simulate scheduling variance
                tokio::time::sleep(Duration::from_micros(500)).await;
            }
        }
    }
    
    // If we get here, the broadcast is stuck (race condition occurred)
    panic!("Broadcast stuck - race condition detected!");
}
```

**Notes:**
- The actual race is difficult to reproduce reliably in a test due to its microsecond timing window
- In production, this manifests as occasional broadcast stalls that self-resolve when new broadcasts are scheduled
- Monitoring for stuck broadcasts would require tracking whether scheduled broadcasts complete within expected timeframes

### Citations

**File:** mempool/src/shared_mempool/types.rs (L124-130)
```rust
pub(crate) struct ScheduledBroadcast {
    /// Time of scheduled broadcast
    deadline: Instant,
    peer: PeerNetworkId,
    backoff: bool,
    waker: Arc<Mutex<Option<Waker>>>,
}
```

**File:** mempool/src/shared_mempool/types.rs (L137-146)
```rust
        if deadline > Instant::now() {
            let tokio_instant = tokio::time::Instant::from_std(deadline);
            executor.spawn(async move {
                tokio::time::sleep_until(tokio_instant).await;
                let mut waker = waker_clone.lock();
                if let Some(waker) = waker.take() {
                    waker.wake()
                }
            });
        }
```

**File:** mempool/src/shared_mempool/types.rs (L162-172)
```rust
    fn poll(self: Pin<&mut Self>, context: &mut Context) -> Poll<Self::Output> {
        if Instant::now() < self.deadline {
            let waker_clone = context.waker().clone();
            let mut waker = self.waker.lock();
            *waker = Some(waker_clone);

            Poll::Pending
        } else {
            Poll::Ready((self.peer, self.backoff))
        }
    }
```

**File:** mempool/src/shared_mempool/tasks.rs (L116-121)
```rust
    scheduled_broadcasts.push(ScheduledBroadcast::new(
        Instant::now() + Duration::from_millis(interval_ms),
        peer,
        schedule_backoff,
        executor,
    ))
```

**File:** config/src/config/mempool_config.rs (L111-112)
```rust
            shared_mempool_tick_interval_ms: 10,
            shared_mempool_backoff_interval_ms: 30_000,
```

**File:** mempool/src/shared_mempool/coordinator.rs (L83-120)
```rust
    let mut scheduled_broadcasts = FuturesUnordered::new();
    let mut update_peers_interval =
        tokio::time::interval(Duration::from_millis(peer_update_interval_ms));

    // Spawn a dedicated task to handle commit notifications from state sync
    spawn_commit_notification_handler(&smp, mempool_listener);

    // Use a BoundedExecutor to restrict only `workers_available` concurrent
    // worker tasks that can process incoming transactions.
    let workers_available = smp.config.shared_mempool_max_concurrent_inbound_syncs;
    let bounded_executor = BoundedExecutor::new(workers_available, executor.clone());

    let initial_reconfig = mempool_reconfig_events
        .next()
        .await
        .expect("Reconfig sender dropped, unable to start mempool");
    handle_mempool_reconfig_event(
        &mut smp,
        &bounded_executor,
        initial_reconfig.on_chain_configs,
    )
    .await;

    loop {
        let _timer = counters::MAIN_LOOP.start_timer();
        ::futures::select! {
            msg = client_events.select_next_some() => {
                handle_client_request(&mut smp, &bounded_executor, msg).await;
            },
            msg = quorum_store_requests.select_next_some() => {
                tasks::process_quorum_store_request(&smp, msg);
            },
            reconfig_notification = mempool_reconfig_events.select_next_some() => {
                handle_mempool_reconfig_event(&mut smp, &bounded_executor, reconfig_notification.on_chain_configs).await;
            },
            (peer, backoff) = scheduled_broadcasts.select_next_some() => {
                tasks::execute_broadcast(peer, backoff, &mut smp, &mut scheduled_broadcasts, executor.clone()).await;
            },
```
