# Audit Report

## Title
Certified Timestamp Divergence Causes Premature Batch Expiration and Block Execution Failures

## Summary
Network partitions and sync lag cause validators to have divergent `certified_timestamp` values. When a validator with a lower certified_timestamp requests a batch, validators with higher certified_timestamps incorrectly signal that the batch has globally expired, causing the requester to short-circuit and fail block execution even when the batch is valid for the block's timestamp.

## Finding Description

The vulnerability exists in the batch request-response flow between validators in the Quorum Store system.

**Core Issue:**

When a validator requests a batch that it doesn't have locally, responders return `BatchResponse::NotFound(ledger_info)` containing their current ledger info. [1](#0-0) 

The requester then checks if the responder's ledger_info timestamp exceeds the batch expiration, and if so, **immediately short-circuits** the entire request: [2](#0-1) 

**Why This Is Wrong:**

The batch expiration should be evaluated relative to the **block's timestamp** being processed, not the responder's current certified_timestamp. A batch can be valid for processing a block with timestamp T₁ even if some validators have progressed to timestamp T₂ > batch_expiration.

**Attack Scenario:**

1. Network partition or sync lag occurs
2. Validator A: `certified_time = 150` (ahead)
3. Validator B: `certified_time = 50` (behind)  
4. Batch exists with `expiration = 100`
5. Validator A deletes the batch via `clear_expired_payload` because `150 > 100` [3](#0-2) 
6. Validator B receives block proposal with `timestamp = 90` referencing this batch
7. Validator B checks expiration: `90 ≤ 100` (valid) and attempts to fetch the batch [4](#0-3) 
8. Validator B requests from Validator A
9. Validator A responds: `NotFound(ledger_info)` where `ledger_info.timestamp = 150`
10. Validator B checks: `150 > 100` (TRUE) → **immediately returns error**
11. Validator B **fails to execute the block** despite the batch being valid

The certified_timestamp is updated atomically when blocks are committed: [5](#0-4) 

And is checked when saving batches to reject expired ones: [6](#0-5) 

## Impact Explanation

**High Severity** per Aptos bug bounty criteria:

1. **Validator node slowdowns**: Validators with lower certified_timestamps cannot execute blocks, getting stuck in the consensus process

2. **Significant protocol violations**: Breaks the invariant that all honest validators should be able to execute the same valid blocks deterministically

3. **Liveness Impact**: If enough validators are behind during network partition or high latency, consensus cannot proceed as a quorum cannot execute the proposed blocks

4. **Asymmetric Execution**: Different validators succeed/fail at executing identical blocks based on their certified_timestamp, causing state divergence risks

This violates **Critical Invariant #1: Deterministic Execution** - all validators must be able to produce identical state roots for identical blocks.

## Likelihood Explanation

**High Likelihood:**

- Network partitions occur naturally in distributed systems
- Validators routinely operate at different sync states during normal operation
- No malicious behavior required - happens through network conditions
- The vulnerability triggers whenever:
  - A block references a batch that some validators have deleted
  - The requesting validator queries a validator with higher certified_timestamp
  - This is a common occurrence during network latency spikes or partition recovery

## Recommendation

**Fix the short-circuit logic** to compare batch expiration against the **block's timestamp** being processed, not the responder's certified_timestamp.

Modify `batch_requester.rs` to accept and use the block timestamp:

```rust
pub(crate) async fn request_batch(
    &self,
    digest: HashValue,
    expiration: u64,
    block_timestamp: u64,  // ADD THIS PARAMETER
    responders: Arc<Mutex<BTreeSet<PeerId>>>,
    mut subscriber_rx: oneshot::Receiver<PersistedValue<BatchInfoExt>>,
) -> ExecutorResult<Vec<SignedTransaction>> {
    // ... existing code ...
    
    Ok(BatchResponse::NotFound(ledger_info)) => {
        counters::RECEIVED_BATCH_NOT_FOUND_COUNT.inc();
        // CHANGE: Compare against block_timestamp, not ledger_info timestamp
        if block_timestamp > expiration {
            counters::RECEIVED_BATCH_EXPIRED_COUNT.inc();
            debug!("QS: batch request expired for block timestamp, digest:{}", digest);
            return Err(ExecutorError::CouldNotGetData);
        }
        // REMOVE the check against ledger_info.timestamp
        // Keep retrying with other responders
    }
}
```

The call sites should pass the block timestamp: [7](#0-6) 

## Proof of Concept

**Reproduction Steps:**

1. Set up a test network with 4 validators (A, B, C, D)
2. Create a batch with `expiration = 100 seconds`
3. Commit blocks on validators A, C, D advancing their `certified_time` to 150
4. Induce network partition isolating validator B at `certified_time = 50`
5. Broadcast a block proposal with `timestamp = 90` referencing the batch
6. Validators A, C, D delete the batch from their stores (expired per their certified_time)
7. Validator B attempts to execute the block
8. Validator B requests the batch from A, C, or D
9. Observe: Request immediately fails with `CouldNotGetData` error
10. Observe: Validator B cannot execute the block despite it being valid

**Expected vs Actual:**
- **Expected**: Validator B should keep trying different responders or use batch subscription mechanism
- **Actual**: Validator B short-circuits on first `NotFound` response with high timestamp, failing block execution

## Notes

This vulnerability demonstrates how local state (certified_timestamp) that should not affect consensus decisions can inadvertently cause execution divergence. The fix requires distinguishing between "batch expired for this specific block" (compare against block timestamp) vs "batch expired according to responder's view" (which should not cause immediate failure).

### Citations

**File:** consensus/src/quorum_store/quorum_store_builder.rs (L417-418)
```rust
                    match aptos_db_clone.get_latest_ledger_info() {
                        Ok(ledger_info) => BatchResponse::NotFound(ledger_info),
```

**File:** consensus/src/quorum_store/batch_requester.rs (L144-150)
```rust
                                if ledger_info.commit_info().epoch() == epoch
                                    && ledger_info.commit_info().timestamp_usecs() > expiration
                                    && ledger_info.verify_signatures(&validator_verifier).is_ok()
                                {
                                    counters::RECEIVED_BATCH_EXPIRED_COUNT.inc();
                                    debug!("QS: batch request expired, digest:{}", digest);
                                    return Err(ExecutorError::CouldNotGetData);
```

**File:** consensus/src/quorum_store/batch_store.rs (L420-421)
```rust
        let last_certified_time = self.last_certified_time();
        if value.expiration() > last_certified_time {
```

**File:** consensus/src/quorum_store/batch_store.rs (L443-448)
```rust
    pub(crate) fn clear_expired_payload(&self, certified_time: u64) -> Vec<HashValue> {
        // To help slow nodes catch up via execution without going to state sync we keep the blocks for 60 extra seconds
        // after the expiration time. This will help remote peers fetch batches that just expired but are within their
        // execution window.
        let expiration_time = certified_time.saturating_sub(self.expiration_buffer_usecs);
        let expired_digests = self.expirations.lock().expire(expiration_time);
```

**File:** consensus/src/quorum_store/batch_store.rs (L530-533)
```rust
    pub fn update_certified_timestamp(&self, certified_time: u64) {
        trace!("QS: batch reader updating time {:?}", certified_time);
        self.last_certified_time
            .fetch_max(certified_time, Ordering::SeqCst);
```

**File:** consensus/src/quorum_store/batch_store.rs (L696-703)
```rust
                        let payload = requester
                            .request_batch(
                                batch_digest,
                                batch_info.expiration(),
                                responders,
                                subscriber_rx,
                            )
                            .await?;
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L102-106)
```rust
            if block_timestamp <= batch_info.expiration() {
                futures.push(batch_reader.get_batch(batch_info, responders.clone()));
            } else {
                debug!("QSE: skipped expired batch {}", batch_info.digest());
            }
```
