# Audit Report

## Title
Health Checker State Desynchronization Leading to Memory Leak and Resource Exhaustion

## Summary
The health checker's `connected_peers()` function can return a stale view of connected peers that diverges from the actual network layer state. This occurs because connection events can be dropped when the subscriber channel is full, causing the health checker's internal `health_check_data` map to accumulate stale peer entries that are never cleaned up. This leads to memory leaks, wasted CPU cycles, and potential denial-of-service against validator nodes.

## Finding Description

The health checker maintains its own view of connected peers in the `health_check_data` HashMap [1](#0-0) , which is updated via connection events from the network layer. The `connected_peers()` function returns peers directly from this map [2](#0-1) .

**The Synchronization Vulnerability:**

The health checker subscribes to connection events from `PeersAndMetadata` [3](#0-2) . When peers connect or disconnect, the health checker updates its internal state [4](#0-3) .

However, the event broadcast mechanism drops events when the subscriber channel is full (capacity: 1000) [5](#0-4) . When a `LostPeer` event is dropped, the health checker never removes the peer from its `health_check_data` map.

**The Cleanup Failure:**

When the health checker detects ping failures, it attempts to disconnect the peer [6](#0-5) . However, the `disconnect_peer()` function only removes entries from `health_check_data` if the disconnect succeeds [7](#0-6) .

When a peer is already disconnected at the network layer, the disconnect attempt returns `PeerManagerError::NotConnected` [8](#0-7) , causing the cleanup to fail and leaving the stale entry in the map permanently.

**Attack Scenario:**

1. Attacker rapidly connects and disconnects from the validator node
2. If the health checker processes events slowly, the channel buffer fills (1000 events)
3. `LostPeer` events for disconnected peers are dropped silently
4. Stale peer entries remain in `health_check_data` indefinitely
5. Every ping interval (default: 10 seconds) [9](#0-8) , the health checker attempts to ping these ghost peers
6. Each ping fails, incrementing failure counters
7. After 3 failures (default) [10](#0-9) , disconnect is attempted but fails (peer already gone)
8. The cycle repeats infinitely, accumulating more stale entries over time

## Impact Explanation

**High Severity** per Aptos bug bounty criteria - **Validator Node Slowdowns**:

1. **Memory Leak**: Unbounded growth of `health_check_data` map as stale entries accumulate. Each entry consumes memory, and with no cleanup mechanism, this grows indefinitely.

2. **CPU Exhaustion**: Every 10 seconds, the health checker iterates through all entries in `health_check_data` (including stale ones) and attempts to ping them. Each failed ping wastes CPU cycles and network resources.

3. **Log Spam**: Each ping failure generates warning logs [11](#0-10) , and disconnect failures generate additional warnings [12](#0-11) , flooding the logging system.

4. **Availability Impact**: Resource exhaustion from memory leaks and CPU waste can slow down validator nodes, affecting their ability to participate in consensus, process transactions, and maintain network health.

This violates the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." The unbounded memory growth and continuous failed operations violate resource constraints.

## Likelihood Explanation

**High Likelihood**:

- **No Authentication Required**: Any network peer can initiate connections without special privileges
- **Exploitable by Design**: The event dropping behavior is intentional (to prevent blocking) but creates the vulnerability
- **No Rate Limiting**: No built-in protection against rapid connect/disconnect cycles
- **No Size Limits**: `health_check_data` has no maximum size cap
- **No Garbage Collection**: No timeout-based cleanup of old entries
- **Production Default Values**: The 10-second ping interval and 1000-event buffer make this easily triggerable in practice

An attacker can trigger this by simply opening and closing TCP connections repeatedly, which is trivial to automate.

## Recommendation

**Fix 1**: Unconditional cleanup in `disconnect_peer()`:

```rust
pub async fn disconnect_peer(
    &mut self,
    peer_network_id: PeerNetworkId,
    disconnect_reason: DisconnectReason,
) -> Result<(), Error> {
    let _ = self.update_connection_state(peer_network_id, ConnectionState::Disconnecting);
    let result = self
        .network_client
        .disconnect_from_peer(peer_network_id, disconnect_reason)
        .await;
    let peer_id = peer_network_id.peer_id();
    
    // Remove from health_check_data regardless of disconnect result
    // If peer is already disconnected, we still want to clean up our state
    self.health_check_data.write().remove(&peer_id);
    
    result
}
```

**Fix 2**: Add periodic garbage collection to detect and remove stale entries that don't match the actual network state.

**Fix 3**: Use a more reliable synchronization mechanism instead of dropping events - consider using the actual `PeersAndMetadata` state directly rather than maintaining a separate map.

**Fix 4**: Add metrics counter for dropped events and maximum size limits for `health_check_data`.

## Proof of Concept

```rust
#[tokio::test]
async fn test_health_checker_memory_leak() {
    use aptos_config::network_id::NetworkId;
    use aptos_types::PeerId;
    use std::collections::HashMap;
    
    // Setup: Create health checker with mock network interface
    let network_id = NetworkId::Validator;
    let (connection_tx, connection_rx) = tokio::sync::mpsc::channel(1000);
    
    // Simulate rapid connect/disconnect cycles
    let mut peer_ids = Vec::new();
    for i in 0..2000 {
        let peer_id = PeerId::random();
        peer_ids.push(peer_id);
        
        // Send NewPeer event
        let metadata = create_mock_connection_metadata(peer_id);
        connection_tx.send(ConnectionNotification::NewPeer(metadata.clone(), network_id))
            .await
            .ok();
        
        // Immediately send LostPeer event
        // After 1000 events, the channel fills and events start dropping
        connection_tx.send(ConnectionNotification::LostPeer(metadata, network_id))
            .await
            .ok();
    }
    
    // Give time for processing
    tokio::time::sleep(tokio::time::Duration::from_secs(1)).await;
    
    // Verify: Check that health_check_data contains stale entries
    // The first ~500 pairs should be processed correctly
    // The remaining ~1500 LostPeer events are dropped
    // Result: ~1000+ stale entries remain in health_check_data
    
    // These stale entries will be pinged every 10 seconds forever
    // causing memory leak, CPU waste, and log spam
}
```

## Notes

This vulnerability demonstrates a critical flaw in the event-driven synchronization model where the health checker's view of network state can permanently diverge from reality. The issue is exacerbated by:

1. The conditional cleanup logic in `disconnect_peer()` that assumes disconnect success means the peer should be removed
2. The lack of any fallback mechanism to reconcile state with the actual network layer
3. The silent dropping of events when channels are full, making the problem hard to detect

The fix requires either unconditional cleanup (treating the health checker's local state as authoritative for cleanup) or implementing a more robust synchronization mechanism that doesn't rely on potentially-dropped events.

### Citations

**File:** network/framework/src/protocols/health_checker/interface.rs (L40-40)
```rust
    health_check_data: RwLock<HashMap<PeerId, HealthCheckData>>,
```

**File:** network/framework/src/protocols/health_checker/interface.rs (L59-61)
```rust
    pub fn connected_peers(&self) -> Vec<PeerId> {
        self.health_check_data.read().keys().cloned().collect()
    }
```

**File:** network/framework/src/protocols/health_checker/interface.rs (L77-79)
```rust
        if result.is_ok() {
            self.health_check_data.write().remove(&peer_id);
        }
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L160-165)
```rust
        let connection_events = self
            .connection_events_injection
            .take()
            .unwrap_or_else(|| self.network_interface.get_peers_and_metadata().subscribe());
        let mut connection_events =
            tokio_stream::wrappers::ReceiverStream::new(connection_events).fuse();
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L210-227)
```rust
                    match conn_event {
                        ConnectionNotification::NewPeer(metadata, network_id) => {
                            // PeersAndMetadata is a global singleton across all networks; filter connect/disconnect events to the NetworkId that this HealthChecker instance is watching
                            if network_id == self_network_id {
                                self.network_interface.create_peer_and_health_data(
                                    metadata.remote_peer_id, self.round
                                );
                            }
                        }
                        ConnectionNotification::LostPeer(metadata, network_id) => {
                            // PeersAndMetadata is a global singleton across all networks; filter connect/disconnect events to the NetworkId that this HealthChecker instance is watching
                            if network_id == self_network_id {
                                self.network_interface.remove_peer_and_health_data(
                                    &metadata.remote_peer_id
                                );
                            }
                        }
                    }
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L344-352)
```rust
                warn!(
                    NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                    round = round,
                    "{} Ping failed for peer: {} round: {} with error: {:#}",
                    self.network_context,
                    peer_id.short_str(),
                    round,
                    err
                );
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L364-391)
```rust
                if failures > self.ping_failures_tolerated {
                    info!(
                        NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                        "{} Disconnecting from peer: {}",
                        self.network_context,
                        peer_id.short_str()
                    );
                    let peer_network_id =
                        PeerNetworkId::new(self.network_context.network_id(), peer_id);
                    if let Err(err) = timeout(
                        Duration::from_millis(50),
                        self.network_interface.disconnect_peer(
                            peer_network_id,
                            DisconnectReason::NetworkHealthCheckFailure,
                        ),
                    )
                    .await
                    {
                        warn!(
                            NetworkSchema::new(&self.network_context)
                                .remote_peer(&peer_id),
                            error = ?err,
                            "{} Failed to disconnect from peer: {} with error: {:?}",
                            self.network_context,
                            peer_id.short_str(),
                            err
                        );
                    }
```

**File:** network/framework/src/application/storage.rs (L376-384)
```rust
            if let Err(err) = dest.try_send(event.clone()) {
                match err {
                    TrySendError::Full(_) => {
                        // Tried to send to an app, but the app isn't handling its messages fast enough.
                        // Drop message. Maybe increment a metrics counter?
                        sample!(
                            SampleRate::Duration(Duration::from_secs(1)),
                            warn!("PeersAndMetadata.broadcast() failed, some app is slow"),
                        );
```

**File:** network/framework/src/peer_manager/mod.rs (L494-494)
```rust
                    if let Err(err) = resp_tx.send(Err(PeerManagerError::NotConnected(peer_id))) {
```

**File:** config/src/config/network_config.rs (L38-38)
```rust
pub const PING_INTERVAL_MS: u64 = 10_000;
```

**File:** config/src/config/network_config.rs (L40-40)
```rust
pub const PING_FAILURES_TOLERATED: u64 = 3;
```
