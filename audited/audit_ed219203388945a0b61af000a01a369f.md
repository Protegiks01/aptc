# Audit Report

## Title
Resource Exhaustion in Indexer gRPC Service Due to Unverified Client Handshake

## Summary
The `get_transactions_from_node()` function spawns a processing task that continues to consume resources (database I/O, CPU, memory) even when clients fail to receive the initial handshake message, leading to resource exhaustion and potential denial of service against the indexer fullnode service.

## Finding Description

The vulnerability exists in the handshake and task lifecycle management of the gRPC streaming service. When a client connects to `get_transactions_from_node()`, the function spawns a tokio task that immediately sends an `init_status` message to confirm the connection parameters. [1](#0-0) 

The critical issue is that the task only verifies that the message was successfully sent to the **local channel buffer**, not that the client actually received it over the network. If `tx.send()` returns `Ok`, the task assumes success and proceeds to the main processing loop. [2](#0-1) 

The spawned task then repeatedly calls `process_next_batch()`, which performs expensive operations **before** checking if the client is consuming messages: [3](#0-2) 

The expensive processing includes:
1. Database I/O to fetch transactions from storage (lines 104-108)
2. CPU-intensive conversion to API types (lines 145-208) 
3. CPU-intensive conversion to protobuf (lines 145-208)
4. Only then attempting to send to the channel (lines 221-226) [4](#0-3) 

**Attack Scenario:**
1. Attacker opens multiple gRPC connections to the indexer service
2. Accepts TCP connections but never reads from the streams (simulating network issues)
3. Each spawned task successfully sends init_status to its channel (channel has default capacity of 35 messages)
4. Each task continues processing, fetching from database and converting transactions
5. Tasks consume resources until channels fill up, then block waiting to send
6. Server resources (DB connections, CPU, memory, tokio task slots) become exhausted [5](#0-4) 

The channel has a small default buffer size of 35 messages, but the damage is already done during the processing phase before messages are sent.

**Broken Invariant:**
This violates the documented invariant: "Resource Limits: All operations must respect gas, storage, and computational limits." There are no per-connection resource limits, timeouts, or validation that clients are actively consuming data.

## Impact Explanation

This qualifies as **Medium Severity** per the Aptos bug bounty criteria for the following reasons:

1. **Resource Exhaustion:** An attacker can exhaust server resources (database connections, CPU, memory) by opening multiple connections without consuming the streams, causing service degradation or unavailability.

2. **No Rate Limiting:** The server configuration shows HTTP/2 keepalive but no explicit connection limits or concurrent stream limits: [6](#0-5) 

3. **Validator Node Impact:** While the indexer-grpc service is optional, if run on validator nodes, this could cause "Validator node slowdowns" which qualifies as High severity. However, as this primarily affects indexer infrastructure, Medium severity is more appropriate.

4. **Amplification Factor:** Each stalled connection spawns a task that continues doing expensive work (DB queries, conversions) for multiple batches before detecting the issue.

## Likelihood Explanation

This vulnerability has **HIGH likelihood** of occurrence:

1. **Natural Occurrence:** Network issues causing slow or stalled clients are common in production environments
2. **Easy to Trigger:** Any client can connect to the publicly exposed gRPC endpoint
3. **No Authentication Required:** The service accepts connections without verifying client capability to consume data
4. **Amplification Effect:** The default configuration spawns 20 parallel processor tasks per connection, multiplying the resource consumption [7](#0-6) 

The attack requires only basic network access and can be automated to open multiple connections simultaneously.

## Recommendation

Implement the following mitigations:

1. **Add Connection Timeout:** Implement a timeout for the initial handshake to detect non-responsive clients early:

```rust
// After sending init_status
let init_timeout = tokio::time::timeout(
    Duration::from_secs(5),
    async {
        // Wait for first data request/acknowledgment from client
        // or monitor that ReceiverStream is being consumed
    }
).await;

if init_timeout.is_err() {
    info!("[Indexer Fullnode] Client failed to acknowledge init, closing stream");
    return;
}
```

2. **Add Early Detection of Slow Consumers:** Check channel capacity before doing expensive work:

```rust
// In process_next_batch(), before fetching from storage
if self.transactions_sender.capacity() == 0 {
    // Channel is full, client not consuming
    tokio::time::sleep(Duration::from_millis(100)).await;
    if self.transactions_sender.capacity() == 0 {
        // Still full, client is stalled
        return vec![];
    }
}
```

3. **Implement Connection Limits:** Add maximum concurrent connections at the tonic Server level:

```rust
let tonic_server = Server::builder()
    .http2_keepalive_interval(Some(std::time::Duration::from_secs(60)))
    .http2_keepalive_timeout(Some(std::time::Duration::from_secs(5)))
    .concurrency_limit_per_connection(256)  // Add this
    .max_concurrent_streams(Some(100))      // Add this
```

4. **Add Per-Connection Resource Tracking:** Track and limit resources (DB queries, CPU time) consumed per connection.

## Proof of Concept

```rust
// Rust client that demonstrates the vulnerability
use aptos_protos::internal::fullnode::v1::{
    fullnode_data_client::FullnodeDataClient, GetTransactionsFromNodeRequest,
};
use tonic::Request;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Connect to indexer fullnode
    let mut client = FullnodeDataClient::connect("http://localhost:50051").await?;
    
    // Create multiple connections
    for i in 0..50 {
        let mut client_clone = client.clone();
        tokio::spawn(async move {
            let request = Request::new(GetTransactionsFromNodeRequest {
                starting_version: Some(1000 * i),
                transactions_count: Some(100000),
            });
            
            // Get the stream but never read from it
            match client_clone.get_transactions_from_node(request).await {
                Ok(response) => {
                    let _stream = response.into_inner();
                    // Never read from stream, just keep connection open
                    println!("Connection {} established, not reading...", i);
                    tokio::time::sleep(tokio::time::Duration::from_secs(3600)).await;
                },
                Err(e) => eprintln!("Connection {} failed: {}", i, e),
            }
        });
    }
    
    // Keep main thread alive
    tokio::time::sleep(tokio::time::Duration::from_secs(3600)).await;
    Ok(())
}
```

**Expected Behavior:** The server spawns 50 tasks, each continuously fetching from database and processing transactions, consuming CPU and memory even though no client is reading the data. Server resources become exhausted, impacting legitimate clients.

## Notes

- This vulnerability specifically affects the indexer-grpc fullnode data service, not core consensus components
- The issue is exacerbated by the default configuration spawning 20 parallel processor tasks per connection
- Similar patterns may exist in the `LocalnetDataService` implementation which shares the same `IndexerStreamCoordinator` logic
- The HTTP/2 keepalive mechanism (60s interval, 5s timeout) is insufficient to detect this condition as the TCP connection remains active

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/fullnode_data_service.rs (L101-133)
```rust
        tokio::spawn(async move {
            // Initialize the coordinator that tracks starting version and processes transactions
            let mut coordinator = IndexerStreamCoordinator::new(
                context,
                starting_version,
                ending_version,
                processor_task_count,
                processor_batch_size,
                output_batch_size,
                tx.clone(),
                // For now the request for this interface doesn't include a txn filter
                // because it is only used for the txn stream filestore worker, which
                // needs every transaction. Later we may add support for txn filtering
                // to this interface too.
                None,
                Some(abort_handle.clone()),
            );
            // Sends init message (one time per request) to the client in the with chain id and starting version. Basically a handshake
            let init_status = get_status(StatusType::Init, starting_version, None, ledger_chain_id);
            match tx.send(Result::<_, Status>::Ok(init_status)).await {
                Ok(_) => {
                    // TODO: Add request details later
                    info!(
                        start_version = starting_version,
                        chain_id = ledger_chain_id,
                        service_type = SERVICE_TYPE,
                        "[Indexer Fullnode] Init connection"
                    );
                },
                Err(_) => {
                    panic!("[Indexer Fullnode] Unable to initialize stream");
                },
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/fullnode_data_service.rs (L135-150)
```rust
            while coordinator.current_version < coordinator.end_version {
                let start_time = std::time::Instant::now();
                // Processes and sends batch of transactions to client
                let results = coordinator.process_next_batch().await;
                if abort_handle.load(Ordering::SeqCst) {
                    info!("FullnodeDataService is aborted.");
                    break;
                }
                if results.is_empty() {
                    info!(
                        start_version = starting_version,
                        chain_id = ledger_chain_id,
                        "[Indexer Fullnode] Client disconnected."
                    );
                    break;
                }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L101-108)
```rust
    pub async fn process_next_batch(&mut self) -> Vec<Result<EndVersion, Status>> {
        let fetching_start_time = std::time::Instant::now();
        // Stage 1: fetch transactions from storage.
        let sorted_transactions_from_storage_with_size =
            self.fetch_transactions_from_storage().await;
        if sorted_transactions_from_storage_with_size.is_empty() {
            return vec![];
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L221-226)
```rust
        for response in responses {
            if self.transactions_sender.send(Ok(response)).await.is_err() {
                // Error from closed channel. This means the client has disconnected.
                return vec![];
            }
        }
```

**File:** config/src/config/indexer_grpc_config.rs (L19-19)
```rust
const DEFAULT_TRANSACTION_CHANNEL_SIZE: usize = 35;
```

**File:** config/src/config/indexer_grpc_config.rs (L23-28)
```rust
pub fn get_default_processor_task_count(use_data_service_interface: bool) -> u16 {
    if use_data_service_interface {
        1
    } else {
        20
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/runtime.rs (L101-104)
```rust
        let tonic_server = Server::builder()
            .http2_keepalive_interval(Some(std::time::Duration::from_secs(60)))
            .http2_keepalive_timeout(Some(std::time::Duration::from_secs(5)))
            .add_service(reflection_service_clone);
```
