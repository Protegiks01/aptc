# Audit Report

## Title
Unbounded Memory Growth in Connection Notifications Channel Enables Sybil-Based DoS Attack

## Summary
The `conn_notifs_channel` in `peer_manager` uses a `PerKeyQueue` with an unbounded `HashMap` to store connection notifications keyed by `PeerId`. An attacker can generate unlimited unique `PeerIds` (via x25519 key pair generation) and rapidly connect/disconnect to fill the channel's internal HashMap, causing memory exhaustion and preventing legitimate peer notifications from being processed, leading to validator node slowdown or crash.

## Finding Description

The connection notifications channel is vulnerable to a Sybil attack through unbounded HashMap growth: [1](#0-0) 

The channel is configured with `max_queue_size_per_key = 1`, limiting messages per PeerId, but the underlying `PerKeyQueue` implementation maintains an unbounded `HashMap<K, VecDeque<T>>` for tracking distinct keys: [2](#0-1) 

When new connections are established, the `PeerManager` sends notifications to all connection event handlers: [3](#0-2) [4](#0-3) 

Each unique `PeerId` creates a new entry in the HashMap when pushed: [5](#0-4) 

While garbage collection exists to remove empty queues, it only triggers every 50 dequeue operations and only removes queues that are already empty: [6](#0-5) 

**Attack Path:**
1. Attacker generates many x25519 key pairs, each yielding a unique cryptographically-valid `PeerId`
2. Attacker rapidly connects with different `PeerIds` (passing Noise handshake authentication)
3. Each connection passes the `inbound_connection_limit` check (which only limits *simultaneous* connections): [7](#0-6) 

4. Each new `PeerId` triggers a `NewPeer` notification pushed to `conn_notifs_channel`
5. The channel's HashMap grows to accommodate all distinct `PeerIds`
6. If connections occur faster than the `ConnectivityManager` processes notifications, queues remain non-empty and memory grows unbounded
7. Eventually causes OOM, crashing the validator node or severely degrading performance

The `inbound_connection_limit` only restricts concurrent connections from unknown peers, not the rate of distinct `PeerIds` over time. There is no rate limiting on connection establishment frequency.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program criteria:

- **Validator node slowdowns**: Memory exhaustion causes progressive performance degradation as the system struggles with allocation and garbage collection pressure
- **API crashes**: OOM condition can crash the node, requiring manual restart
- **Significant protocol violations**: Breaks the Resource Limits invariant that "all operations must respect gas, storage, and computational limits"

The attack affects validator availability and network liveness, potentially preventing consensus participation if enough validators are simultaneously attacked. While it doesn't directly steal funds or violate consensus safety, it can cause network-wide liveness failures if coordinated against multiple validators.

## Likelihood Explanation

**High likelihood:**

- **Trivial attacker requirements**: Attacker only needs to generate x25519 key pairs and establish TCP connections
- **No special privileges needed**: Any network peer can execute the attack
- **No rate limiting**: No connection rate limits beyond the simultaneous connection cap
- **Fast attack execution**: Modern hardware can generate thousands of key pairs per second
- **Low resource cost**: Each HashMap entry is relatively small (~96 bytes for VecDeque + key overhead), but multiplied by millions of entries becomes gigabytes
- **Detection difficulty**: Looks like legitimate connection churn from many peers

The attack is trivial to execute, requires minimal resources, and is difficult to distinguish from normal network behavior until memory exhaustion occurs.

## Recommendation

Implement a bounded limit on the total number of distinct keys (PeerIds) tracked in the connection notifications channel:

1. **Add max_keys parameter to PerKeyQueue**: Introduce a limit on HashMap size, rejecting new keys once the limit is reached (keeping only most recent or prioritized peers)

2. **Implement connection rate limiting**: Add per-IP connection rate limits to prevent rapid connection cycling from the same source

3. **Aggressive GC for transient peers**: For unknown/untrusted peers, implement more aggressive garbage collection that removes entries after a timeout period, not just when queues are empty

4. **Priority-based eviction**: When the key limit is reached, evict Unknown role peers before trusted validators

Example fix for PerKeyQueue:

```rust
pub(crate) struct PerKeyQueue<K: Eq + Hash + Clone, T> {
    queue_style: QueueStyle,
    per_key_queue: HashMap<K, VecDeque<T>>,
    round_robin_queue: VecDeque<K>,
    max_queue_size: NonZeroUsize,
    max_keys: Option<NonZeroUsize>, // NEW: limit total keys
    num_popped_since_gc: u32,
    counters: Option<&'static IntCounterVec>,
}

pub(crate) fn push(&mut self, key: K, message: T) -> Option<T> {
    // NEW: Check if we've hit the max keys limit
    if let Some(max_keys) = self.max_keys {
        if !self.per_key_queue.contains_key(&key) 
            && self.per_key_queue.len() >= max_keys.get() {
            // Reject new keys or implement eviction policy
            return Some(message);
        }
    }
    // ... existing push logic
}
```

## Proof of Concept

```rust
// Rust reproduction test for network/framework/src/peer_manager/tests.rs

#[tokio::test]
async fn test_sybil_attack_memory_exhaustion() {
    use aptos_types::PeerId;
    use crate::peer_manager::conn_notifs_channel;
    use crate::transport::ConnectionMetadata;
    use aptos_config::network_id::NetworkId;
    use crate::peer_manager::ConnectionNotification;
    
    let (mut sender, mut receiver) = conn_notifs_channel::new();
    
    // Simulate attacker creating many unique PeerIds
    let num_attack_peers = 100_000;
    
    for i in 0..num_attack_peers {
        let peer_id = PeerId::random(); // Each random PeerId represents a new key
        let conn_meta = ConnectionMetadata::mock(peer_id);
        let notif = ConnectionNotification::NewPeer(
            conn_meta,
            NetworkId::Validator
        );
        
        // Push notification for each unique PeerId
        sender.push(peer_id, notif).expect("Push should succeed");
        
        // Receiver processes slowly (simulating real-world scenario)
        if i % 1000 == 0 {
            // Only consume 1 out of every 1000 messages
            let _ = receiver.select_next_some().await;
        }
    }
    
    // At this point, the internal HashMap has grown to contain
    // ~99,000 distinct PeerId entries, each with a VecDeque
    // This would consume significant memory (potentially GBs)
    // and cause performance degradation or OOM
    
    println!("Attack successful: {} unique PeerIds queued", 
             num_attack_peers - (num_attack_peers / 1000));
}
```

This PoC demonstrates how an attacker can create many unique `PeerIds` and overwhelm the channel's internal HashMap, with the receiver unable to keep up with the flood of notifications.

## Notes

The codebase shows awareness of transient peer issues through comments about memory allocation and GC implementation for "lots of transient peers", but the fix is incomplete: [8](#0-7) 

While the GC helps with truly transient peers that disconnect and have empty queues, it doesn't protect against an active Sybil attack where notifications remain queued faster than they're consumed. The fundamental issue is the unbounded HashMap growth based on attacker-controlled distinct keys.

### Citations

**File:** network/framework/src/peer_manager/conn_notifs_channel.rs (L18-20)
```rust
pub fn new() -> (Sender, Receiver) {
    aptos_channel::new(QueueStyle::LIFO, 1, None)
}
```

**File:** crates/channel/src/message_queues.rs (L45-63)
```rust
pub(crate) struct PerKeyQueue<K: Eq + Hash + Clone, T> {
    /// QueueStyle for the messages stored per key
    queue_style: QueueStyle,
    /// per_key_queue maintains a map from a Key to a queue
    /// of all the messages from that Key. A Key is usually
    /// represented by AccountAddress
    per_key_queue: HashMap<K, VecDeque<T>>,
    /// This is a (round-robin)queue of Keys which have pending messages
    /// This queue will be used for performing round robin among
    /// Keys for choosing the next message
    round_robin_queue: VecDeque<K>,
    /// Maximum number of messages to store per key
    max_queue_size: NonZeroUsize,
    /// Number of messages dequeued since last GC
    num_popped_since_gc: u32,
    /// Optional counters for recording # enqueued, # dequeued, and # dropped
    /// messages
    counters: Option<&'static IntCounterVec>,
}
```

**File:** crates/channel/src/message_queues.rs (L112-127)
```rust
    pub(crate) fn push(&mut self, key: K, message: T) -> Option<T> {
        if let Some(c) = self.counters.as_ref() {
            c.with_label_values(&["enqueued"]).inc();
        }

        let key_message_queue = self
            .per_key_queue
            .entry(key.clone())
            // Only allocate a small initial queue for a new key. Previously, we
            // allocated a queue with all `max_queue_size_per_key` entries;
            // however, this breaks down when we have lots of transient peers.
            // For example, many of our queues have a max capacity of 1024. To
            // handle a single rpc from a transient peer, we would end up
            // allocating ~ 96 b * 1024 ~ 64 Kib per queue.
            .or_insert_with(|| VecDeque::with_capacity(1));

```

**File:** crates/channel/src/message_queues.rs (L174-206)
```rust
            // Remove empty per-key-queues every `POPS_PER_GC` successful dequeue
            // operations.
            //
            // aptos-channel never removes keys from its PerKeyQueue (without
            // this logic). This works fine for the validator network, where we
            // have a bounded set of peers that almost never changes; however,
            // this does not work for servicing public clients, where we can have
            // large and frequent connection churn.
            //
            // Periodically removing these empty queues prevents us from causing
            // an effective memory leak when we have lots of transient peers in
            // e.g. the public-facing vfn use-case.
            //
            // This GC strategy could probably be more sophisticated, though it
            // seems to work well in some basic stress tests / micro benches.
            //
            // See: common/channel/src/bin/many_keys_stress_test.rs
            //
            // For more context, see: https://github.com/aptos-labs/aptos-core/issues/5543
            self.num_popped_since_gc += 1;
            if self.num_popped_since_gc >= POPS_PER_GC {
                self.num_popped_since_gc = 0;
                self.remove_empty_queues();
            }
        }

        message
    }

    /// Garbage collect any empty per-key-queues.
    fn remove_empty_queues(&mut self) {
        self.per_key_queue.retain(|_key, queue| !queue.is_empty());
    }
```

**File:** network/framework/src/peer_manager/mod.rs (L351-390)
```rust
        // Verify that we have not reached the max connection limit for unknown inbound peers
        if conn.metadata.origin == ConnectionOrigin::Inbound {
            // Everything below here is meant for unknown peers only. The role comes from
            // the Noise handshake and if it's not `Unknown` then it is trusted.
            if conn.metadata.role == PeerRole::Unknown {
                // TODO: Keep track of somewhere else to not take this hit in case of DDoS
                // Count unknown inbound connections
                let unknown_inbound_conns = self
                    .active_peers
                    .iter()
                    .filter(|(peer_id, (metadata, _))| {
                        metadata.origin == ConnectionOrigin::Inbound
                            && trusted_peers
                                .get(peer_id)
                                .is_none_or(|peer| peer.role == PeerRole::Unknown)
                    })
                    .count();

                // Reject excessive inbound connections made by unknown peers
                // We control outbound connections with Connectivity manager before we even send them
                // and we must allow connections that already exist to pass through tie breaking.
                if !self
                    .active_peers
                    .contains_key(&conn.metadata.remote_peer_id)
                    && unknown_inbound_conns + 1 > self.inbound_connection_limit
                {
                    info!(
                        NetworkSchema::new(&self.network_context)
                            .connection_metadata_with_address(&conn.metadata),
                        "{} Connection rejected due to connection limit: {}",
                        self.network_context,
                        conn.metadata
                    );
                    counters::connections_rejected(&self.network_context, conn.metadata.origin)
                        .inc();
                    self.disconnect(conn);
                    return;
                }
            }
        }
```

**File:** network/framework/src/peer_manager/mod.rs (L688-693)
```rust
        // Send NewPeer notification to connection event handlers.
        if send_new_peer_notification {
            let notif =
                ConnectionNotification::NewPeer(conn_meta, self.network_context.network_id());
            self.send_conn_notification(peer_id, notif);
        }
```

**File:** network/framework/src/peer_manager/mod.rs (L698-714)
```rust
    /// Sends a `ConnectionNotification` to all event handlers, warns on failures
    fn send_conn_notification(&mut self, peer_id: PeerId, notification: ConnectionNotification) {
        for handler in self.connection_event_handlers.iter_mut() {
            if let Err(e) = handler.push(peer_id, notification.clone()) {
                warn!(
                    NetworkSchema::new(&self.network_context)
                        .remote_peer(&peer_id),
                    error = ?e,
                    connection_notification = notification,
                    "{} Failed to send notification {} to handler for peer: {}. Error: {:?}",
                    self.network_context,
                    notification,
                    peer_id.short_str(),
                    e
                );
            }
        }
```
