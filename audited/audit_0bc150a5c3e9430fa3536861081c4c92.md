# Audit Report

## Title
Synchronous Database Calls in EpochRetrievalRequest Processing Block Event Loop Causing Consensus Message Drops

## Summary
The `EpochManager` processes `EpochRetrievalRequest` messages synchronously with blocking database calls in its async event loop, creating a processing bottleneck. When multiple peers flood these requests, the prolonged event loop blocking prevents timely message dequeuing, causing legitimate consensus messages (votes, proposals) to accumulate in bounded channels and be dropped, resulting in potential consensus liveness failures.

## Finding Description

The Aptos consensus system uses a bounded channel architecture where each `(peer, message_type)` pair has a queue with capacity 10 using FIFO eviction policy. [1](#0-0) 

When the queue reaches capacity, FIFO drops the newest incoming message. [2](#0-1) 

The vulnerability arises from how `EpochRetrievalRequest` messages are processed. Unlike other consensus messages (proposals, votes) which are spawned to a bounded executor for async verification, `EpochRetrievalRequest` messages are processed directly in the main event loop's `check_epoch()` method. [3](#0-2) 

The `process_epoch_retrieval()` method performs a **synchronous database query** to fetch epoch change proofs, which can retrieve up to 100 epoch ending ledger infos. [4](#0-3) 

The database call `get_epoch_ending_ledger_infos()` is synchronous and blocks the async runtime thread. [5](#0-4) 

This creates a critical bottleneck because the main event loop processes messages sequentially. [6](#0-5) 

**Attack Path:**
1. Multiple malicious peers each send 10 `EpochRetrievalRequest` messages (filling their per-key queues)
2. The event loop dequeues these messages in round-robin fashion
3. Each request triggers a synchronous database query that blocks the event loop
4. While blocked, new consensus messages (votes, proposals) from legitimate validators continue arriving from the network
5. These messages are pushed to their respective channel queues, but cannot be dequeued due to the blocked loop
6. If a validator sends more than 10 messages of the same type during this period, the channel overflows using FIFO eviction
7. Critical consensus messages are silently dropped, breaking consensus liveness

The channel push failure is only logged as a warning with no retry mechanism. [7](#0-6) 

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos Bug Bounty program criteria for "Validator node slowdowns" and "Significant protocol violations."

**Impact:**
- **Consensus Liveness Failure**: Dropped votes prevent quorum formation, stalling block production
- **Proposal Loss**: Dropped proposals prevent round progression, causing timeouts
- **Validator Performance Degradation**: Affected validators miss consensus participation
- **Network-Wide Slowdown**: If multiple validators are targeted, overall network throughput degrades

The vulnerability does not directly threaten safety (no double-signing or chain splits), but severely impacts liveness, which is critical for blockchain operation. A sustained attack could halt the network until manual intervention.

## Likelihood Explanation

**Likelihood: High**

The attack is practical because:

1. **No Authentication Required**: Any network peer can send `EpochRetrievalRequest` messages without being a validator
2. **Low Resource Cost**: Attackers only need to maintain network connections and send lightweight requests
3. **Amplification Factor**: Each request triggers expensive database operations, creating asymmetric resource consumption
4. **No Rate Limiting**: The code has no per-peer rate limiting for `EpochRetrievalRequest` messages beyond the channel capacity
5. **Coordinated Attack**: Multiple Sybil peers can amplify the effect (N peers Ã— 10 messages = 10N queued database queries)

The only validation is that `request.end_epoch <= self.epoch()`, which doesn't prevent flooding. [8](#0-7) 

## Recommendation

**Immediate Fix:**
1. Process `EpochRetrievalRequest` asynchronously using `spawn_blocking()` to avoid blocking the event loop
2. Add per-peer rate limiting for epoch retrieval requests
3. Implement request deduplication to prevent redundant database queries

**Code Fix:**

In `consensus/src/epoch_manager.rs`, modify `check_epoch()` to spawn epoch retrieval processing:

```rust
ConsensusMsg::EpochRetrievalRequest(request) => {
    ensure!(
        request.end_epoch <= self.epoch(),
        "[EpochManager] Received EpochRetrievalRequest beyond what we have locally"
    );
    
    // Spawn blocking task to avoid blocking event loop
    let storage = self.storage.clone();
    let network_sender = self.network_sender.clone();
    let request = *request;
    
    tokio::task::spawn_blocking(move || {
        match storage
            .aptos_db()
            .get_epoch_ending_ledger_infos(request.start_epoch, request.end_epoch)
        {
            Ok(proof) => {
                let msg = ConsensusMsg::EpochChangeProof(Box::new(proof));
                let _ = network_sender.send_to(peer_id, msg);
            }
            Err(e) => {
                warn!("Failed to get epoch proof: {:?}", e);
            }
        }
    });
},
```

**Additional Hardening:**
- Add per-peer request counters with time-based reset
- Implement request coalescing for identical epoch ranges
- Use a separate channel for epoch retrieval with lower priority

## Proof of Concept

```rust
// PoC demonstrating channel overflow under epoch retrieval flood
// This would be added as an integration test

#[tokio::test]
async fn test_epoch_retrieval_flood_drops_consensus_messages() {
    // Setup: Create test network with one honest validator and multiple malicious peers
    let (mut test_harness, validator, malicious_peers) = setup_test_network().await;
    
    // Step 1: Malicious peers flood EpochRetrievalRequests
    for peer in malicious_peers {
        for _ in 0..10 {
            let request = EpochRetrievalRequest {
                start_epoch: 0,
                end_epoch: 100, // Request large range to maximize DB time
            };
            peer.send(ConsensusMsg::EpochRetrievalRequest(Box::new(request))).await;
        }
    }
    
    // Step 2: Honest validator sends 15 VoteMsg (exceeding channel capacity of 10)
    let mut vote_acks = vec![];
    for i in 0..15 {
        let vote = create_test_vote(i);
        let ack = validator.send_with_ack(ConsensusMsg::VoteMsg(Box::new(vote))).await;
        vote_acks.push(ack);
    }
    
    // Step 3: Wait for processing
    tokio::time::sleep(Duration::from_secs(2)).await;
    
    // Verification: At least 5 votes should be dropped due to channel overflow
    let delivered = vote_acks.iter().filter(|ack| ack.was_delivered()).count();
    assert!(
        delivered < 15,
        "Expected some votes to be dropped, but all {} were delivered",
        delivered
    );
    assert!(
        delivered <= 10,
        "Channel capacity is 10, but {} votes were delivered",
        delivered
    );
    
    // Impact: Consensus cannot form quorum without all votes
    let quorum_formed = test_harness.check_quorum_status().await;
    assert!(!quorum_formed, "Consensus should fail without sufficient votes");
}
```

**Notes:**
- This vulnerability breaks the consensus liveness invariant
- The synchronous database operation in an async context is the root cause
- The bounded channel architecture amplifies the impact through message drops
- Real-world database latency under load makes this attack highly effective

### Citations

**File:** consensus/src/network.rs (L757-761)
```rust
        let (consensus_messages_tx, consensus_messages) = aptos_channel::new(
            QueueStyle::FIFO,
            10,
            Some(&counters::CONSENSUS_CHANNEL_MSGS),
        );
```

**File:** consensus/src/network.rs (L799-813)
```rust
    fn push_msg(
        peer_id: AccountAddress,
        msg: ConsensusMsg,
        tx: &aptos_channel::Sender<
            (AccountAddress, Discriminant<ConsensusMsg>),
            (AccountAddress, ConsensusMsg),
        >,
    ) {
        if let Err(e) = tx.push((peer_id, discriminant(&msg)), (peer_id, msg)) {
            warn!(
                remote_peer = peer_id,
                error = ?e, "Error pushing consensus msg",
            );
        }
    }
```

**File:** crates/channel/src/message_queues.rs (L134-147)
```rust
        if key_message_queue.len() >= self.max_queue_size.get() {
            if let Some(c) = self.counters.as_ref() {
                c.with_label_values(&["dropped"]).inc();
            }
            match self.queue_style {
                // Drop the newest message for FIFO
                QueueStyle::FIFO => Some(message),
                // Drop the oldest message for LIFO
                QueueStyle::LIFO | QueueStyle::KLAST => {
                    let oldest = key_message_queue.pop_front();
                    key_message_queue.push_back(message);
                    oldest
                },
            }
```

**File:** consensus/src/epoch_manager.rs (L451-476)
```rust
    fn process_epoch_retrieval(
        &mut self,
        request: EpochRetrievalRequest,
        peer_id: AccountAddress,
    ) -> anyhow::Result<()> {
        debug!(
            LogSchema::new(LogEvent::ReceiveEpochRetrieval)
                .remote_peer(peer_id)
                .epoch(self.epoch()),
            "[EpochManager] receive {}", request,
        );
        let proof = self
            .storage
            .aptos_db()
            .get_epoch_ending_ledger_infos(request.start_epoch, request.end_epoch)
            .map_err(DbError::from)
            .context("[EpochManager] Failed to get epoch proof")?;
        let msg = ConsensusMsg::EpochChangeProof(Box::new(proof));
        if let Err(err) = self.network_sender.send_to(peer_id, msg) {
            warn!(
                "[EpochManager] Failed to send epoch proof to {}, with error: {:?}",
                peer_id, err,
            );
        }
        Ok(())
    }
```

**File:** consensus/src/epoch_manager.rs (L1677-1686)
```rust
            ConsensusMsg::EpochRetrievalRequest(request) => {
                ensure!(
                    request.end_epoch <= self.epoch(),
                    "[EpochManager] Received EpochRetrievalRequest beyond what we have locally"
                );
                monitor!(
                    "process_epoch_retrieval",
                    self.process_epoch_retrieval(*request, peer_id)
                )?;
            },
```

**File:** consensus/src/epoch_manager.rs (L1930-1936)
```rust
            tokio::select! {
                (peer, msg) = network_receivers.consensus_messages.select_next_some() => {
                    monitor!("epoch_manager_process_consensus_messages",
                    if let Err(e) = self.process_message(peer, msg).await {
                        error!(epoch = self.epoch(), error = ?e, kind = error_kind(&e));
                    });
                },
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L995-1005)
```rust
    fn get_epoch_ending_ledger_infos(
        &self,
        start_epoch: u64,
        end_epoch: u64,
    ) -> Result<(Vec<LedgerInfoWithSignatures>, bool)> {
        self.get_epoch_ending_ledger_infos_impl(
            start_epoch,
            end_epoch,
            MAX_NUM_EPOCH_ENDING_LEDGER_INFO,
        )
    }
```
