# Audit Report

## Title
Memory Exhaustion Vulnerability in Transaction Restore Due to Unvalidated BATCH_SIZE

## Summary
The transaction restore functionality uses a hardcoded `BATCH_SIZE=10000` constant without validating the actual memory consumption of batched transactions. When restoring backups containing large governance transactions (up to 1MB payload + 10MB write sets + 10MB events per transaction), batching 10,000 such transactions can consume hundreds of gigabytes of memory, causing restore operations to fail due to memory exhaustion. [1](#0-0) 

## Finding Description
The restore process batches transactions for processing using a fixed `BATCH_SIZE=10000` constant. This constant is applied indiscriminately to all transactions regardless of their actual size. [2](#0-1) [3](#0-2) 

The Aptos gas parameters enforce maximum sizes per transaction:
- Regular transactions: 64KB payload
- Governance transactions: 1MB payload  
- Write sets: 10MB per transaction
- Events: 10MB per transaction [4](#0-3) [5](#0-4) 

These limits are enforced during transaction execution, meaning legitimate backups can contain maximum-sized transactions. [6](#0-5) 

During restore, the `try_chunks(BATCH_SIZE)` operation collects 10,000 transaction tuples into memory simultaneously: [7](#0-6) [8](#0-7) 

**Memory consumption calculation:**
- Maximum per transaction: 1MB (payload) + 10MB (write set) + 10MB (events) ≈ 21MB serialized
- In-memory overhead (Vec allocations, BTreeMap nodes in WriteSet structures): ~1.4x multiplier
- Effective memory per max-sized transaction: ~30MB
- Batch of 10,000: 10,000 × 30MB = **300GB**

Even with smaller but still large transactions (e.g., 100KB payload + 1MB write sets + 1MB events), a batch would consume: 10,000 × ~2.5MB = **25GB**, which can still exhaust memory on typical nodes.

The restore handlers have no memory bounds checking: [9](#0-8) 

## Impact Explanation
This qualifies as **Medium Severity** under "State inconsistencies requiring intervention" per the Aptos bug bounty criteria.

**Operational Impact:**
1. **Restore Failure**: Node operators cannot restore from backups when they contain sequences of large governance transactions
2. **Disaster Recovery Prevention**: In a catastrophic failure scenario requiring network-wide restore, nodes with insufficient memory cannot rejoin
3. **Cascade Effect**: Failed restores delay network recovery, potentially extending downtime

**Affected Scenarios:**
- Restoring from backups containing large governance proposals with substantial write sets
- State synchronization after network upgrades involving major framework changes
- Disaster recovery operations where multiple nodes need to restore simultaneously

The issue is exacerbated by:
- No configuration option to adjust BATCH_SIZE based on available memory
- No runtime memory monitoring to adapt batch sizes
- No documentation warning operators about memory requirements for large transaction restores

## Likelihood Explanation
**Likelihood: Medium**

This vulnerability triggers when:
1. Blockchain contains governance transactions approaching maximum sizes (legitimately submitted)
2. Backup files are created containing these transactions
3. Restore operations are performed

**Factors increasing likelihood:**
- Governance transactions regularly exceed 100KB with substantial state changes
- Framework upgrades can generate multi-megabyte write sets
- Large-scale airdrops or token migrations create significant event data
- No operator awareness of this limitation (undocumented)

**Factors decreasing likelihood:**
- Most regular transactions are small (<10KB)
- Backup chunk size limit (128MB) naturally distributes transactions across chunks
- Concurrent downloads parameter limits simultaneous chunk processing

However, the `try_chunks` operation batches transactions **across** chunks, so multiple small chunks containing large transactions can still trigger the issue.

## Recommendation

**Immediate Fix:** Implement dynamic batch sizing based on actual transaction data size:

```rust
// Replace fixed BATCH_SIZE with adaptive batching
const MAX_BATCH_MEMORY_MB: usize = 512; // Configurable limit
const MIN_BATCH_SIZE: usize = 100;
const MAX_BATCH_SIZE: usize = 10000;

fn calculate_transaction_memory_size(
    txn: &Transaction,
    write_set: &WriteSet, 
    events: &Vec<ContractEvent>
) -> usize {
    // Estimate in-memory size with overhead
    let txn_size = txn.raw_bytes_len();
    let write_set_size: usize = write_set.write_op_iter()
        .map(|(k, v)| k.size() + v.bytes().map(|b| b.len()).unwrap_or(0))
        .sum();
    let events_size: usize = events.iter()
        .map(|e| e.event_data().len())
        .sum();
    
    // Apply 1.5x overhead multiplier for in-memory structures
    ((txn_size + write_set_size + events_size) * 3) / 2
}

// Use adaptive chunking instead of fixed try_chunks(BATCH_SIZE)
let mut batch = Vec::new();
let mut batch_memory_bytes = 0;
let max_batch_memory_bytes = MAX_BATCH_MEMORY_MB * 1024 * 1024;

for item in txns_to_execute_stream {
    let (txn, aux_info, txn_info, write_set, events) = item?;
    let item_size = calculate_transaction_memory_size(&txn, &write_set, &events);
    
    if !batch.is_empty() && 
       (batch_memory_bytes + item_size > max_batch_memory_bytes ||
        batch.len() >= MAX_BATCH_SIZE) {
        // Process current batch
        process_batch(batch, batch_memory_bytes)?;
        batch = Vec::new();
        batch_memory_bytes = 0;
    }
    
    batch.push((txn, aux_info, txn_info, write_set, events));
    batch_memory_bytes += item_size;
    
    if batch.len() >= MIN_BATCH_SIZE && batch_memory_bytes >= max_batch_memory_bytes / 2 {
        // Opportunistically process when half-full to maintain throughput
        process_batch(batch, batch_memory_bytes)?;
        batch = Vec::new();
        batch_memory_bytes = 0;
    }
}
```

**Additional Mitigations:**
1. Add `--max-batch-memory-mb` CLI parameter to GlobalRestoreOpt
2. Log warnings when processing transactions exceeding 1MB
3. Document memory requirements in restore operation guides
4. Add monitoring metrics for restore batch memory consumption

## Proof of Concept

```rust
// Integration test demonstrating memory exhaustion risk
#[test]
fn test_large_transaction_restore_memory_consumption() {
    use aptos_types::transaction::{Transaction, WriteSet, TransactionInfo};
    use aptos_types::contract_event::ContractEvent;
    
    // Simulate 10,000 large governance transactions
    let mut transactions = Vec::new();
    let mut write_sets = Vec::new();
    let mut events = Vec::new();
    
    for i in 0..10000 {
        // Create 1MB governance transaction
        let mut txn_bytes = vec![0u8; 1024 * 1024];
        txn_bytes[0..8].copy_from_slice(&i.to_le_bytes());
        
        // Create 10MB write set (simulated as 10 x 1MB write ops)
        let mut ws_ops = Vec::new();
        for j in 0..10 {
            let key = StateKey::raw(format!("key_{}_{}", i, j).as_bytes());
            let value_bytes = vec![0u8; 1024 * 1024]; // 1MB value
            ws_ops.push((key, WriteOp::legacy_modification(value_bytes.into())));
        }
        let write_set = WriteSet::new(ws_ops).unwrap();
        
        // Create 10MB events (simulated as 10 x 1MB events)  
        let mut event_vec = Vec::new();
        for j in 0..10 {
            let event_data = vec![0u8; 1024 * 1024]; // 1MB event
            // Create ContractEvent with large data
            event_vec.push(create_large_event(event_data));
        }
        
        transactions.push(txn_bytes);
        write_sets.push(write_set);
        events.push(event_vec);
    }
    
    // Measure memory before and after batching
    let memory_before = get_process_memory_mb();
    
    // Simulate try_chunks(BATCH_SIZE) behavior - collect all into vectors
    let batch: Vec<_> = (0..10000)
        .map(|i| (
            transactions[i].clone(),
            write_sets[i].clone(), 
            events[i].clone()
        ))
        .collect();
    
    let memory_after = get_process_memory_mb();
    let memory_consumed = memory_after - memory_before;
    
    // Assert that memory consumption exceeds reasonable bounds
    assert!(
        memory_consumed > 20_000, // 20GB threshold
        "Large transaction batch consumed {} MB, exceeding safe limits",
        memory_consumed
    );
    
    println!("Memory consumed by 10,000 max-sized transactions: {} MB", memory_consumed);
    println!("This would cause OOM on nodes with <64GB RAM");
}
```

**Notes:**
- This is a legitimate operational issue, not an active attack vector since backup files are cryptographically verified
- The vulnerability manifests when restoring backups containing legitimate large governance transactions
- Impact is primarily on disaster recovery capability rather than active exploitation
- The fix requires balancing memory safety with restore throughput performance

### Citations

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L62-62)
```rust
const BATCH_SIZE: usize = if cfg!(test) { 2 } else { 10000 };
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L575-576)
```rust
        let db_commit_stream = txns_to_execute_stream
            .try_chunks(BATCH_SIZE)
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L578-585)
```rust
            .map_ok(|chunk| {
                let (txns, persisted_aux_info, txn_infos, write_sets, events): (
                    Vec<_>,
                    Vec<_>,
                    Vec<_>,
                    Vec<_>,
                    Vec<_>,
                ) = chunk.into_iter().multiunzip();
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L658-659)
```rust
        let ledger_update_stream = txns_to_execute_stream
            .try_chunks(BATCH_SIZE)
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L661-668)
```rust
            .map_ok(|chunk| {
                let (txns, persisted_aux_info, txn_infos, write_sets, events): (
                    Vec<_>,
                    Vec<_>,
                    Vec<_>,
                    Vec<_>,
                    Vec<_>,
                ) = chunk.into_iter().multiunzip();
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L73-81)
```rust
            max_transaction_size_in_bytes: NumBytes,
            "max_transaction_size_in_bytes",
            64 * 1024
        ],
        [
            max_transaction_size_in_bytes_gov: NumBytes,
            { RELEASE_V1_13.. => "max_transaction_size_in_bytes.gov" },
            1024 * 1024
        ],
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L154-172)
```rust
            max_bytes_per_write_op: NumBytes,
            { 5.. => "max_bytes_per_write_op" },
            1 << 20, // a single state item is 1MB max
        ],
        [
            max_bytes_all_write_ops_per_transaction: NumBytes,
            { 5.. => "max_bytes_all_write_ops_per_transaction" },
            10 << 20, // all write ops from a single transaction are 10MB max
        ],
        [
            max_bytes_per_event: NumBytes,
            { 5.. => "max_bytes_per_event" },
            1 << 20, // a single event is 1MB max
        ],
        [
            max_bytes_all_events_per_transaction: NumBytes,
            { 5.. => "max_bytes_all_events_per_transaction"},
            10 << 20, // all events from a single transaction are 10MB max
        ],
```

**File:** aptos-move/aptos-vm-types/src/storage/change_set_configs.rs (L86-128)
```rust
    pub fn check_change_set(&self, change_set: &impl ChangeSetInterface) -> Result<(), VMStatus> {
        let storage_write_limit_reached = |maybe_message: Option<&str>| {
            let mut err = PartialVMError::new(StatusCode::STORAGE_WRITE_LIMIT_REACHED);
            if let Some(message) = maybe_message {
                err = err.with_message(message.to_string())
            }
            Err(err.finish(Location::Undefined).into_vm_status())
        };

        if self.max_write_ops_per_transaction != 0
            && change_set.num_write_ops() as u64 > self.max_write_ops_per_transaction
        {
            return storage_write_limit_reached(Some("Too many write ops."));
        }

        let mut write_set_size = 0;
        for (key, op_size) in change_set.write_set_size_iter() {
            if let Some(len) = op_size.write_len() {
                let write_op_size = len + (key.size() as u64);
                if write_op_size > self.max_bytes_per_write_op {
                    return storage_write_limit_reached(None);
                }
                write_set_size += write_op_size;
            }
            if write_set_size > self.max_bytes_all_write_ops_per_transaction {
                return storage_write_limit_reached(None);
            }
        }

        let mut total_event_size = 0;
        for event in change_set.events_iter() {
            let size = event.event_data().len() as u64;
            if size > self.max_bytes_per_event {
                return storage_write_limit_reached(None);
            }
            total_event_size += size;
            if total_event_size > self.max_bytes_all_events_per_transaction {
                return storage_write_limit_reached(None);
            }
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/backup/restore_utils.rs (L115-176)
```rust
pub(crate) fn save_transactions(
    state_store: Arc<StateStore>,
    ledger_db: Arc<LedgerDb>,
    first_version: Version,
    txns: &[Transaction],
    persisted_aux_info: &[PersistedAuxiliaryInfo],
    txn_infos: &[TransactionInfo],
    events: &[Vec<ContractEvent>],
    write_sets: Vec<WriteSet>,
    existing_batch: Option<(
        &mut LedgerDbSchemaBatches,
        &mut ShardedStateKvSchemaBatch,
        &mut SchemaBatch,
    )>,
    kv_replay: bool,
) -> Result<()> {
    if let Some((ledger_db_batch, state_kv_batches, _state_kv_metadata_batch)) = existing_batch {
        save_transactions_impl(
            state_store,
            ledger_db,
            first_version,
            txns,
            persisted_aux_info,
            txn_infos,
            events,
            write_sets.as_ref(),
            ledger_db_batch,
            state_kv_batches,
            kv_replay,
        )?;
    } else {
        let mut ledger_db_batch = LedgerDbSchemaBatches::new();
        let mut sharded_kv_schema_batch = state_store
            .state_db
            .state_kv_db
            .new_sharded_native_batches();
        save_transactions_impl(
            Arc::clone(&state_store),
            Arc::clone(&ledger_db),
            first_version,
            txns,
            persisted_aux_info,
            txn_infos,
            events,
            write_sets.as_ref(),
            &mut ledger_db_batch,
            &mut sharded_kv_schema_batch,
            kv_replay,
        )?;
        // get the last version and commit to the state kv db
        // commit the state kv before ledger in case of failure happens
        let last_version = first_version + txns.len() as u64 - 1;
        state_store
            .state_db
            .state_kv_db
            .commit(last_version, None, sharded_kv_schema_batch)?;

        ledger_db.write_schemas(ledger_db_batch)?;
    }

    Ok(())
}
```
