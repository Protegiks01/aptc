# Audit Report

## Title
Unique Transaction Limit Bypass via Untracked OptBatch Transactions in Proposal Generation

## Summary
The `handle_proposal_request()` function in `proof_manager.rs` fails to track unique transactions from `opt_batches` when calculating limits for `inline_batches`, allowing proposals to exceed the `max_txns_after_filtering` limit designed to prevent resource exhaustion and ensure deterministic execution across validators.

## Finding Description

The vulnerability exists in a three-stage payload pulling process where unique transaction counts are not properly accumulated:

**Stage 1 - Pull Proofs (Lines 114-122):** [1](#0-0) 

This returns `cur_unique_txns` representing unique transactions from proof_block after filtering duplicates with `excluded_batches`.

**Stage 2 - Pull OptBatches (Lines 129-152):** [2](#0-1) 

The `pull_batches` method returns three values: `(batches, payload_size, unique_txns)`, but **line 134 discards the unique transaction count** using `_`. The TODO comment at line 130 acknowledges this limitation but doesn't address the security impact.

**Stage 3 - Pull Inline Batches (Lines 161-166):** [3](#0-2) 

The limit calculation **only subtracts `cur_unique_txns`** from proof_block, completely ignoring unique transactions added by opt_batches. This allows inline batches to add transactions that push the total unique count beyond the intended limit.

**Root Cause in pull_internal:** [4](#0-3) 

The check uses `max_txns_after_filtering` as an **absolute** limit, but when called for inline batches, it receives the original `request.max_txns_after_filtering` without adjustment for transactions already pulled by proof_block and opt_batches.

**Exploitation Scenario:**
1. `request.max_txns_after_filtering = 100` (unique transaction limit)
2. `pull_proofs` adds 40 unique transactions → `cur_unique_txns = 40`
3. `pull_batches` (opt) adds 50 unique transactions (within limit of 60) → **discarded**
4. Line 165 calculates inline limit as `100 - 40 = 60` transactions
5. `pull_batches_with_transactions` can add 60 NEW unique transactions (relative to filtered_txns containing 90 txns)
6. **Total unique transactions: 40 + 50 + 60 = 150 > 100** ✗

The vulnerability violates **Resource Limits Invariant #9** which states "All operations must respect gas, storage, and computational limits."

## Impact Explanation

**Severity: High** (up to $50,000 per Aptos Bug Bounty)

This vulnerability causes **significant protocol violations** through:

1. **Consensus Safety Risk**: Different validators may have different views of available batches, causing some to accept proposals with 150 unique transactions while others reject them for exceeding the 100-transaction limit, potentially causing consensus disagreement.

2. **Resource Exhaustion**: The `max_txns_after_filtering` limit exists to prevent blocks from becoming too large for timely execution. Bypassing this limit could cause:
   - Validator node slowdowns during block execution
   - Increased memory consumption during transaction processing  
   - Extended block execution times affecting network liveness

3. **Deterministic Execution Violation**: If validators process proposals differently based on their local state of available batches, this breaks **Invariant #1** (Deterministic Execution).

## Likelihood Explanation

**Likelihood: Medium-High**

The vulnerability triggers automatically when:
- OptQuorumStore payload parameters are enabled (`request.maybe_optqs_payload_pull_params.is_some()`)
- Sufficient batches exist across all three categories (proofs, opt batches, inline batches)
- Each category contributes unique transactions

This is a **normal operational scenario** in production, not an edge case. The exploitation doesn't require:
- Malicious validator behavior
- Coordinated attacks
- Special transaction crafting

The only requirement is sufficient transaction volume across validator nodes, which occurs regularly under normal network load.

## Recommendation

Track and accumulate unique transaction counts across all three stages:

```rust
let (opt_batches, opt_batch_txns_size, opt_unique_txns) =
    if let Some(ref params) = request.maybe_optqs_payload_pull_params {
        let max_opt_batch_txns_size = request.max_txns - txns_with_proof_size;
        let max_opt_batch_txns_after_filtering = request.max_txns_after_filtering - cur_unique_txns;
        let (opt_batches, opt_payload_size, opt_unique) =
            self.batch_proof_queue.pull_batches(
                &excluded_batches
                    .iter()
                    .cloned()
                    .chain(proof_block.iter().map(|proof| proof.info().clone()))
                    .collect(),
                &params.exclude_authors,
                max_opt_batch_txns_size,
                max_opt_batch_txns_after_filtering,
                request.soft_max_txns_after_filtering,
                request.return_non_full,
                request.block_timestamp,
                Some(params.minimum_batch_age_usecs),
            );
        (opt_batches, opt_payload_size, opt_unique)  // Don't discard unique count
    } else {
        (Vec::new(), PayloadTxnsSize::zero(), 0)
    };

// Accumulate total unique count
let total_unique_txns = cur_unique_txns + opt_unique_txns;

// ... later when pulling inline batches ...
max_inline_txns_to_pull.set_count(min(
    max_inline_txns_to_pull.count(),
    request
        .max_txns_after_filtering
        .saturating_sub(total_unique_txns),  // Use accumulated count
));
```

Additionally, pass the adjusted limit to `pull_batches_with_transactions`:

```rust
let max_txns_after_filtering_for_inline = 
    request.max_txns_after_filtering.saturating_sub(total_unique_txns);
    
let (inline_batches, inline_payload_size, _) =
    self.batch_proof_queue.pull_batches_with_transactions(
        &excluded_batches
            .iter()
            .cloned()
            .chain(proof_block.iter().map(|proof| proof.info().clone()))
            .chain(opt_batches.clone())
            .collect(),
        max_inline_txns_to_pull,
        max_txns_after_filtering_for_inline,  // Use adjusted limit
        request.soft_max_txns_after_filtering,
        request.return_non_full,
        request.block_timestamp,
    );
```

## Proof of Concept

Since this is a consensus-layer vulnerability requiring multiple validators and batch states, a full PoC would require:

```rust
// Rust unit test in consensus/src/quorum_store/tests/proof_manager_test.rs

#[test]
fn test_unique_txn_limit_bypass_via_opt_batches() {
    // Setup: Create ProofManager with max_txns_after_filtering = 100
    let max_unique_txns = 100;
    
    // Create batches with unique transactions:
    // - proof_block: 40 unique txns
    // - opt_batches: 50 unique txns (non-overlapping with proof_block)
    // - Available inline batches: 60 unique txns (non-overlapping with above)
    
    // When: handle_proposal_request is called
    // Expected: Total unique txns should be capped at 100
    // Actual: Total unique txns = 40 + 50 + 60 = 150
    
    // Assertion: The returned payload should have at most 100 unique transactions
    // But it will contain 150, violating the limit
}
```

The test would demonstrate that when opt_batches contribute unique transactions, the inline batch pulling logic doesn't account for them, allowing the total unique count to exceed `max_txns_after_filtering`.

**Notes**

The TODO comment at line 130 explicitly states "Support unique txn calculation", confirming this is incomplete functionality rather than intentional design. However, the security impact was not addressed, allowing this to become an exploitable vulnerability affecting consensus safety and resource limits.

### Citations

**File:** consensus/src/quorum_store/proof_manager.rs (L114-122)
```rust
        let (proof_block, txns_with_proof_size, cur_unique_txns, proof_queue_fully_utilized) =
            self.batch_proof_queue.pull_proofs(
                &excluded_batches,
                request.max_txns,
                request.max_txns_after_filtering,
                request.soft_max_txns_after_filtering,
                request.return_non_full,
                request.block_timestamp,
            );
```

**File:** consensus/src/quorum_store/proof_manager.rs (L129-152)
```rust
        let (opt_batches, opt_batch_txns_size) =
            // TODO(ibalajiarun): Support unique txn calculation
            if let Some(ref params) = request.maybe_optqs_payload_pull_params {
                let max_opt_batch_txns_size = request.max_txns - txns_with_proof_size;
                let max_opt_batch_txns_after_filtering = request.max_txns_after_filtering - cur_unique_txns;
                let (opt_batches, opt_payload_size, _) =
                    self.batch_proof_queue.pull_batches(
                        &excluded_batches
                            .iter()
                            .cloned()
                            .chain(proof_block.iter().map(|proof| proof.info().clone()))
                            .collect(),
                        &params.exclude_authors,
                        max_opt_batch_txns_size,
                        max_opt_batch_txns_after_filtering,
                        request.soft_max_txns_after_filtering,
                        request.return_non_full,
                        request.block_timestamp,
                        Some(params.minimum_batch_age_usecs),
                    );
                (opt_batches, opt_payload_size)
            } else {
                (Vec::new(), PayloadTxnsSize::zero())
            };
```

**File:** consensus/src/quorum_store/proof_manager.rs (L161-166)
```rust
                max_inline_txns_to_pull.set_count(min(
                    max_inline_txns_to_pull.count(),
                    request
                        .max_txns_after_filtering
                        .saturating_sub(cur_unique_txns),
                ));
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L651-657)
```rust
                        if cur_all_txns + batch.size() > max_txns
                            || unique_txns > max_txns_after_filtering
                        {
                            // Exceeded the limit for requested bytes or number of transactions.
                            full = true;
                            return false;
                        }
```
