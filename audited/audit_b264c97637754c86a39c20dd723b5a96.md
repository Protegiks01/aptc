# Audit Report

## Title
Unbounded Memory Growth in BatchGenerator Due to Missing Cleanup of Remote Batches During State Sync and Consensus Liveness Failures

## Summary
The `BatchGenerator` maintains two unbounded in-memory data structures (`batches_in_progress` and `txns_in_progress_sorted`) that track batches and transactions from all validators. Remote batches are only cleaned up via `CommitNotification` messages, which may not be delivered during state synchronization or consensus liveness failures. This causes unbounded memory accumulation that can lead to validator node crashes and network-wide availability issues.

## Finding Description

The `BatchGenerator` struct maintains two critical in-memory structures without size limits: [1](#0-0) 

The `insert_batch()` function adds remote batches to these structures without any bounds checking: [2](#0-1) 

Remote batches are inserted with a 500ms expiry time: [3](#0-2) [4](#0-3) 

The **only** cleanup mechanism for remote batches is through `CommitNotification` commands, which expire batches based on block timestamp: [5](#0-4) 

Critically, the `ProofExpiration` cleanup path **only applies to locally-created batches**, not remote batches: [6](#0-5) 

`CommitNotification` is sent when blocks are committed and executed: [7](#0-6) 

**Attack Scenarios:**

1. **State Sync Memory Exhaustion**: When a validator node is catching up via state synchronization, it may not be actively committing blocks and thus not receiving regular `CommitNotification` messages. However, the node continues to receive batch messages from other validators. Without cleanup, these remote batches accumulate unbounded in memory.

2. **Consensus Liveness Failure**: During network partitions or consensus issues where no blocks are being committed, `CommitNotification` messages stop flowing. Validators continue generating and broadcasting batches to make progress, but these batches accumulate without cleanup on receiving nodes.

3. **Delayed Block Timestamps**: If block timestamps don't advance properly or CommitNotifications are delayed, batches with 500ms expiry times will accumulate beyond their intended lifetime.

This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits."

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program criteria: "Validator node slowdowns" and API crashes.

**Specific Impacts:**
- **Memory Exhaustion**: Unbounded growth of `batches_in_progress` and `txns_in_progress_sorted` can consume all available memory on validator nodes, leading to out-of-memory (OOM) crashes.
- **Validator Availability**: Crashed validator nodes reduce network participation and can impact consensus if enough validators are affected simultaneously.
- **Network Liveness**: If multiple validators experience memory exhaustion during a consensus liveness issue, the problem becomes self-reinforcing and can lead to extended network downtime.
- **State Sync Disruption**: Nodes attempting to sync state may crash before completing synchronization, making network recovery more difficult.

Per the receiver limits, each batch message can contain up to 2,000 transactions across 20 batches. A malicious or compromised validator could maximize these limits, and even honest validators during normal operation contribute to accumulation during the vulnerable periods.

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability has multiple realistic trigger conditions:

1. **State Sync (High Likelihood)**: New validators joining the network or validators catching up after downtime routinely perform state synchronization. During extended sync periods (which can last hours for historical data), the node continuously receives batch messages without cleanup.

2. **Consensus Liveness Issues (Medium Likelihood)**: Network partitions, validator outages, or protocol bugs that temporarily halt block production have occurred in blockchain networks. During these periods, the vulnerability activates automatically.

3. **No Attacker Privileges Required**: The vulnerability triggers through normal validator operations. While only validators can send batch messages (verified through signature checks), no malicious intent is required - honest validators sending legitimate batches during vulnerable periods trigger the issue.

4. **No Rate Limiting on Accumulation**: While individual batch messages are rate-limited, there's no limit on the total number of batches accumulated over time when cleanup doesn't occur.

The vulnerability is particularly concerning because it can affect multiple validators simultaneously during network-wide events (consensus issues), creating cascading failures.

## Recommendation

Implement bounded data structures with eviction policies and add periodic cleanup mechanisms that don't depend solely on `CommitNotification`:

**Recommended Fixes:**

1. **Add Size Limits**: Implement maximum capacity limits on `batches_in_progress` and `txns_in_progress_sorted` with LRU or FIFO eviction policies.

2. **Wall-Clock Time Cleanup**: Add periodic cleanup based on wall-clock time in addition to blockchain logical time. Batches with expired `expiry_time_usecs` should be removed even without `CommitNotification`.

3. **Per-Peer Limits**: Enforce per-validator limits on the number of batches in progress to prevent any single validator from consuming excessive memory.

4. **Defensive Cleanup**: Implement a periodic cleanup timer in the main event loop that removes batches based on wall-clock time expiry:

```rust
// In the main event loop, add a periodic cleanup timer
let mut cleanup_interval = time::interval(Duration::from_secs(1));

loop {
    tokio::select! {
        // ... existing branches ...
        
        _ = cleanup_interval.tick() => {
            let current_time = aptos_infallible::duration_since_epoch().as_micros() as u64;
            let expired_batches: Vec<_> = self.batch_expirations
                .expire(current_time)
                .collect();
            
            for (author, batch_id) in expired_batches {
                if self.remove_batch_in_progress(author, batch_id) {
                    counters::BATCH_IN_PROGRESS_WALL_CLOCK_EXPIRED.inc();
                }
            }
        }
    }
}
```

5. **Memory Monitoring**: Add metrics and alerts for the size of `batches_in_progress` and `txns_in_progress_sorted` to detect accumulation early.

## Proof of Concept

**Reproduction Steps:**

1. **Setup**: Deploy a validator node and monitor its memory usage.

2. **Trigger State Sync**: 
   - Stop the node for an extended period
   - Let other validators continue producing blocks and batches
   - Restart the node to begin state sync

3. **Observe Memory Growth**:
   - During state sync, the node processes incoming batch messages
   - `CommitNotification` isn't sent during catch-up phase
   - Monitor `batches_in_progress.len()` via debug logs or metrics
   - Observe memory consumption growing unbounded

4. **Alternative - Consensus Liveness Test**:
   - Simulate a network partition affecting consensus
   - Configure validators to continue sending batches
   - Observe memory accumulation on all validators without cleanup

**Expected Outcome**: Memory grows unbounded, eventually causing OOM conditions and node crashes.

**Rust Test Sketch**:
```rust
#[tokio::test]
async fn test_remote_batch_accumulation_without_commit() {
    // Create BatchGenerator
    let mut generator = create_test_batch_generator();
    
    // Simulate receiving many remote batches
    for i in 0..10000 {
        let batch = create_test_batch(peer_id, batch_id + i, 100);
        generator.handle_remote_batch(
            peer_id,
            BatchId::new(i),
            batch.into_transactions()
        );
    }
    
    // Verify accumulation without cleanup
    assert!(generator.batches_in_progress.len() > 5000);
    assert!(generator.txns_in_progress_sorted.len() > 100000);
    
    // Simulate time passing beyond expiry (500ms)
    tokio::time::sleep(Duration::from_secs(2)).await;
    
    // WITHOUT CommitNotification, batches remain
    assert!(generator.batches_in_progress.len() > 5000);
}
```

## Notes

This vulnerability is particularly severe because:

1. **No Mitigation Currently Exists**: The code has no bounds checking or fallback cleanup mechanisms
2. **Network-Wide Impact**: Multiple validators can be affected simultaneously during consensus issues
3. **Self-Reinforcing**: Memory exhaustion can worsen consensus problems, creating a positive feedback loop
4. **Silent Accumulation**: Memory grows gradually without obvious symptoms until OOM crash

The vulnerability exists in the core consensus path and affects validator stability, making it a High Severity issue per the Aptos bug bounty criteria.

### Citations

**File:** consensus/src/quorum_store/batch_generator.rs (L68-69)
```rust
    batches_in_progress: HashMap<(PeerId, BatchId), BatchInProgress>,
    txns_in_progress_sorted: BTreeMap<TransactionSummary, TransactionInProgress>,
```

**File:** consensus/src/quorum_store/batch_generator.rs (L123-171)
```rust
    fn insert_batch(
        &mut self,
        author: PeerId,
        batch_id: BatchId,
        txns: Vec<SignedTransaction>,
        expiry_time_usecs: u64,
    ) {
        if self.batches_in_progress.contains_key(&(author, batch_id)) {
            return;
        }

        let txns_in_progress: Vec<_> = txns
            .par_iter()
            .with_min_len(optimal_min_len(txns.len(), 32))
            .map(|txn| {
                (
                    TransactionSummary::new(
                        txn.sender(),
                        txn.replay_protector(),
                        txn.committed_hash(),
                    ),
                    TransactionInProgress::new(txn.gas_unit_price()),
                )
            })
            .collect();

        let mut txns = vec![];
        for (summary, info) in txns_in_progress {
            let txn_info = self
                .txns_in_progress_sorted
                .entry(summary)
                .or_insert_with(|| TransactionInProgress::new(info.gas_unit_price));
            txn_info.increment();
            txn_info.gas_unit_price = info.gas_unit_price.max(txn_info.gas_unit_price);
            txns.push(summary);
        }
        let updated_expiry_time_usecs = self
            .batches_in_progress
            .get(&(author, batch_id))
            .map_or(expiry_time_usecs, |batch_in_progress| {
                expiry_time_usecs.max(batch_in_progress.expiry_time_usecs)
            });
        self.batches_in_progress.insert(
            (author, batch_id),
            BatchInProgress::new(txns, updated_expiry_time_usecs),
        );
        self.batch_expirations
            .add_item((author, batch_id), updated_expiry_time_usecs);
    }
```

**File:** consensus/src/quorum_store/batch_generator.rs (L392-401)
```rust
    pub(crate) fn handle_remote_batch(
        &mut self,
        author: PeerId,
        batch_id: BatchId,
        txns: Vec<SignedTransaction>,
    ) {
        let expiry_time_usecs = aptos_infallible::duration_since_epoch().as_micros() as u64
            + self.config.remote_batch_expiry_gap_when_init_usecs;
        self.insert_batch(author, batch_id, txns, expiry_time_usecs);
    }
```

**File:** consensus/src/quorum_store/batch_generator.rs (L517-552)
```rust
                        BatchGeneratorCommand::CommitNotification(block_timestamp, batches) => {
                            trace!(
                                "QS: got clean request from execution, block timestamp {}",
                                block_timestamp
                            );
                            // Block timestamp is updated asynchronously, so it may race when it enters state sync.
                            if self.latest_block_timestamp > block_timestamp {
                                continue;
                            }
                            self.latest_block_timestamp = block_timestamp;

                            for (author, batch_id) in batches.iter().map(|b| (b.author(), b.batch_id())) {
                                if self.remove_batch_in_progress(author, batch_id) {
                                    counters::BATCH_IN_PROGRESS_COMMITTED.inc();
                                }
                            }

                            // Cleans up all batches that expire in timestamp <= block_timestamp. This is
                            // safe since clean request must occur only after execution result is certified.
                            for (author, batch_id) in self.batch_expirations.expire(block_timestamp) {
                                if let Some(batch_in_progress) = self.batches_in_progress.get(&(author, batch_id)) {
                                    // If there is an identical batch with higher expiry time, re-insert it.
                                    if batch_in_progress.expiry_time_usecs > block_timestamp {
                                        self.batch_expirations.add_item((author, batch_id), batch_in_progress.expiry_time_usecs);
                                        continue;
                                    }
                                }
                                if self.remove_batch_in_progress(author, batch_id) {
                                    counters::BATCH_IN_PROGRESS_EXPIRED.inc();
                                    debug!(
                                        "QS: logical time based expiration batch w. id {} from batches_in_progress, new size {}",
                                        batch_id,
                                        self.batches_in_progress.len(),
                                    );
                                }
                            }
```

**File:** consensus/src/quorum_store/batch_generator.rs (L554-563)
```rust
                        BatchGeneratorCommand::ProofExpiration(batch_ids) => {
                            for batch_id in batch_ids {
                                counters::BATCH_IN_PROGRESS_TIMEOUT.inc();
                                debug!(
                                    "QS: received timeout for proof of store, batch id = {}",
                                    batch_id
                                );
                                // Not able to gather the proof, allow transactions to be polled again.
                                self.remove_batch_in_progress(self.my_peer_id, batch_id);
                            }
```

**File:** config/src/config/quorum_store_config.rs (L132-132)
```rust
            remote_batch_expiry_gap_when_init_usecs: Duration::from_millis(500).as_micros() as u64,
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L168-208)
```rust
    fn notify_commit(&self, block_timestamp: u64, payloads: Vec<Payload>) {
        self.batch_reader
            .update_certified_timestamp(block_timestamp);

        let batches: Vec<_> = payloads
            .into_iter()
            .flat_map(|payload| match payload {
                Payload::DirectMempool(_) => {
                    unreachable!("InQuorumStore should be used");
                },
                Payload::InQuorumStore(proof_with_status) => proof_with_status
                    .proofs
                    .iter()
                    .map(|proof| proof.info().clone().into())
                    .collect::<Vec<_>>(),
                Payload::InQuorumStoreWithLimit(proof_with_status) => proof_with_status
                    .proof_with_data
                    .proofs
                    .iter()
                    .map(|proof| proof.info().clone().into())
                    .collect::<Vec<_>>(),
                Payload::QuorumStoreInlineHybrid(inline_batches, proof_with_data, _)
                | Payload::QuorumStoreInlineHybridV2(inline_batches, proof_with_data, _) => {
                    inline_batches
                        .iter()
                        .map(|(batch_info, _)| batch_info.clone().into())
                        .chain(
                            proof_with_data
                                .proofs
                                .iter()
                                .map(|proof| proof.info().clone().into()),
                        )
                        .collect::<Vec<_>>()
                },
                Payload::OptQuorumStore(OptQuorumStorePayload::V1(p)) => p.get_all_batch_infos(),
                Payload::OptQuorumStore(OptQuorumStorePayload::V2(p)) => p.get_all_batch_infos(),
            })
            .collect();

        self.commit_notifier.notify(block_timestamp, batches);
    }
```
