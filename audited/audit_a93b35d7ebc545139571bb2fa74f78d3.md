# Audit Report

## Title
Race Condition Causing Silent Backup Overwrite in Multi-Node GCS Table Info Backup Configuration

## Summary
Multiple nodes configured to backup table info to the same GCS bucket with identical `chain_id` will overwrite each other's backups due to non-atomic snapshot naming and upload operations, resulting in silent data loss without collision detection or prevention mechanisms.

## Finding Description

The table info backup system generates snapshot blob names based solely on `chain_id` and `epoch`, without any node-specific identifier. [1](#0-0) 

When multiple fullnodes are configured to backup to the same GCS bucket (a realistic deployment scenario for organizational redundancy), they generate identical blob names for the same epoch. [2](#0-1) 

The upload operation uses default GCS settings without preconditions, allowing unconditional overwrites. [3](#0-2) 

While there is a metadata check to prevent redundant backups [4](#0-3) , this check is vulnerable to race conditions when multiple nodes reach epoch boundaries simultaneously:

1. All nodes create local snapshots at epoch boundary
2. All nodes check metadata, see previous epoch is backed up
3. All nodes proceed to upload simultaneously
4. Last node to complete wins, previous uploads are silently lost
5. Metadata reflects the epoch is backed up, but not which node's data

This breaks the **State Consistency** invariant - backups cannot be reliably used for disaster recovery if they may be silently overwritten by parallel operations.

## Impact Explanation

This qualifies as **High Severity** under "Significant protocol violations" and **Medium Severity** under "State inconsistencies requiring intervention":

1. **Silent Data Loss**: Healthy backups can be overwritten by corrupted ones with no warning
2. **Backup Integrity Violation**: Cannot guarantee disaster recovery capability
3. **Operational Risk**: Multi-node deployments (common for redundancy) face unreliable backups
4. **No Detection Mechanism**: System provides no alerts when overwrites occur
5. **Metadata Inconsistency**: Single metadata file cannot track which node's backup is actually stored

The metadata update operation also lacks atomicity. [5](#0-4) 

## Likelihood Explanation

**High Likelihood** in production environments:

1. **Common Configuration**: Organizations often deploy multiple fullnodes for redundancy sharing infrastructure
2. **Simultaneous Triggering**: Nodes syncing from the same network reach epoch boundaries near-simultaneously
3. **No Documentation**: The README provides no warnings about single-bucket-per-deployment requirement [6](#0-5) 
4. **Default Behavior**: No configuration flags prevent or warn about this scenario

## Recommendation

Implement node-specific backup paths and atomic upload operations:

```rust
// In backup_restore/mod.rs
pub fn generate_blob_name(chain_id: u64, epoch: u64, node_id: &str) -> String {
    format!(
        "{}/chain_id_{}_node_{}_epoch_{}.tar.gz",
        FILE_FOLDER_NAME, chain_id, node_id, epoch
    )
}
```

Add GCS precondition checks to prevent overwrites:

```rust
// In gcs.rs backup_db_snapshot_and_update_metadata
match self.gcs_client.upload_streamed_object(
    &UploadObjectRequest {
        bucket: self.bucket_name.clone(),
        // Add precondition: only upload if object doesn't exist
        if_generation_match: Some(0),
        ..Default::default()
    },
    // ... rest of upload
```

Add configuration validation to detect and warn about shared buckets across multiple nodes.

## Proof of Concept

**Configuration Setup:**
```yaml
# Node A - fullnode_a.yaml
indexer_table_info:
  parser_task_count: 10
  parser_batch_size: 100
  table_info_service_mode:
    Backup: "shared-backup-bucket"

# Node B - fullnode_b.yaml  
indexer_table_info:
  parser_task_count: 10
  parser_batch_size: 100
  table_info_service_mode:
    Backup: "shared-backup-bucket"
```

**Reproduction Steps:**
1. Deploy two fullnodes with identical chain_id configured to backup to same GCS bucket
2. Allow both nodes to sync and reach epoch boundary (e.g., epoch 100)
3. Both nodes will create local snapshots with name `snapshot_chain_1_epoch_100`
4. Both nodes upload to GCS as `files/chain_id_1_epoch_100.tar.gz`
5. Check GCS bucket - only one backup exists (last upload wins)
6. Check metadata.json - shows epoch 100 backed up, but no indication which node's data
7. If Node A had healthy state and Node B had corruption, healthy backup is lost

**Verification:**
Check GCS object metadata to see modification timestamps and verify multiple overwrites occurred for same epoch without any error logging.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/mod.rs (L15-20)
```rust
pub fn generate_blob_name(chain_id: u64, epoch: u64) -> String {
    format!(
        "{}/chain_id_{}_epoch_{}.tar.gz",
        FILE_FOLDER_NAME, chain_id, epoch
    )
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L429-429)
```rust
            .join(snapshot_folder_name(chain_id as u64, epoch));
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L576-585)
```rust
        if metadata.epoch >= epoch {
            aptos_logger::info!(
                epoch = epoch,
                snapshot_folder_name = snapshot_folder_name,
                "[Table Info] Snapshot already backed up. Skipping the backup."
            );
            // Remove the snapshot directory.
            std::fs::remove_dir_all(snapshot_dir).unwrap();
            return;
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L124-162)
```rust
    pub async fn update_metadata(&self, chain_id: u64, epoch: u64) -> anyhow::Result<()> {
        let metadata = BackupRestoreMetadata::new(chain_id, epoch);
        loop {
            match self
                .gcs_client
                .upload_object(
                    &UploadObjectRequest {
                        bucket: self.bucket_name.clone(),
                        ..Default::default()
                    },
                    serde_json::to_vec(&metadata).unwrap(),
                    &UploadType::Simple(Media {
                        name: Borrowed(METADATA_FILE_NAME),
                        content_type: Borrowed(JSON_FILE_TYPE),
                        content_length: None,
                    }),
                )
                .await
            {
                Ok(_) => {
                    aptos_logger::info!(
                        "[Table Info] Successfully updated metadata to GCS bucket: {}",
                        METADATA_FILE_NAME
                    );
                    return Ok(());
                },
                // https://cloud.google.com/storage/quotas
                // add retry logic due to: "Maximum rate of writes to the same object name: One write per second"
                Err(Error::Response(err)) if (err.is_retriable() && err.code == 429) => {
                    info!("Retried with rateLimitExceeded on gcs single object at epoch {} when updating the metadata", epoch);
                    tokio::time::sleep(Duration::from_millis(500)).await;
                    continue;
                },
                Err(err) => {
                    anyhow::bail!("Failed to update metadata: {}", err);
                },
            }
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L223-237)
```rust
        match self
            .gcs_client
            .upload_streamed_object(
                &UploadObjectRequest {
                    bucket: self.bucket_name.clone(),
                    ..Default::default()
                },
                file_stream,
                &UploadType::Simple(Media {
                    name: filename.clone().into(),
                    content_type: Borrowed(TAR_FILE_TYPE),
                    content_length: None,
                }),
            )
            .await
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/README.md (L12-20)
```markdown
* Add following to fullnode.yaml to backup your table info db.
```
  indexer_table_info:
    parser_task_count: 10
    parser_batch_size: 100
    table_info_service_mode:
        Backup:
            your-bucket-name
  ```
```
