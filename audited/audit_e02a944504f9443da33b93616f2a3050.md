# Audit Report

## Title
Unbounded Memory and CPU Consumption in EventStorePruner Initialization During Large Range Catch-up

## Summary
The EventStorePruner's initialization performs unbounded pruning operations when catching up a large backlog, potentially causing memory exhaustion (1-2+ GB) and CPU starvation (several minutes) during node startup. This can prevent validators from successfully restarting and participating in consensus.

## Finding Description
During EventStorePruner initialization, the catch-up mechanism attempts to prune the entire gap between saved progress and current metadata progress without batching. [1](#0-0) 

The vulnerability occurs when there's a significant gap (millions of versions) between `EventPrunerProgress` and the `LedgerPrunerProgress` (metadata_progress). This can happen through:
1. Database corruption causing EventPrunerProgress metadata to be lost or reset
2. Persistent I/O errors preventing EventStorePruner from updating progress while other pruners succeed
3. Schema migration bugs causing progress inconsistencies
4. Manual database operations by node operators

The pruning operation allocates unbounded memory in three places:

1. **Event count vector**: `prune_event_indices` creates a `Vec<usize>` storing event counts for each version [2](#0-1) 

2. **Delete operations batch**: Both `prune_event_indices` and `prune_events` accumulate delete operations into a SchemaBatch, which has no memory limits [3](#0-2) 

3. **Accumulator deletions**: `prune_event_accumulator` adds additional delete operations for Merkle accumulator nodes [4](#0-3) 

For 1 million versions with 10 events per version average:
- Vec<usize>: ~8 MB
- Index delete operations: ~880 MB (2 ops per event × 10M events × ~44 bytes)
- Event delete operations: ~160 MB (10M events × ~16 bytes)
- Accumulator delete operations: ~96 MB (4M nodes × ~24 bytes)
- **Total: ~1.14 GB minimum**, plus HashMap overhead

The normal pruning operation uses batching at 5,000 versions per batch [5](#0-4) , but this batching is bypassed during initialization [6](#0-5) 

## Impact Explanation
This is a **High Severity** vulnerability per the Aptos bug bounty criteria due to:

1. **Validator node slowdowns**: Initialization can take many minutes, preventing timely participation in consensus
2. **Node crashes**: Memory exhaustion can trigger OOM kills, causing validator downtime
3. **Availability impact**: Failed restarts affect network liveness and validator rewards

The impact is limited to node availability rather than consensus safety or fund loss, placing it in the High Severity category (validator node slowdowns/crashes) rather than Critical.

## Likelihood Explanation
**Likelihood: Medium**

While not a normal operation scenario, this can realistically occur through:
- **Database corruption** from hardware failures or power loss
- **Persistent I/O errors** causing EventStorePruner to fall behind
- **Schema migration bugs** during software upgrades
- **Operator errors** during manual database maintenance

The vulnerability requires a database inconsistency between pruner progress states, which is not common but plausible in production environments, especially during incident recovery or hardware issues.

## Recommendation
Implement batched catch-up during EventStorePruner initialization:

```rust
pub(in crate::pruner) fn new(
    ledger_db: Arc<LedgerDb>,
    metadata_progress: Version,
    internal_indexer_db: Option<InternalIndexerDB>,
) -> Result<Self> {
    let progress = get_or_initialize_subpruner_progress(
        ledger_db.event_db_raw(),
        &DbMetadataKey::EventPrunerProgress,
        metadata_progress,
    )?;

    let myself = EventStorePruner {
        ledger_db,
        internal_indexer_db,
    };

    info!(
        progress = progress,
        metadata_progress = metadata_progress,
        "Catching up EventStorePruner."
    );
    
    // FIXED: Use batched catch-up instead of unbounded prune
    const CATCHUP_BATCH_SIZE: u64 = 5_000; // Match normal batch_size
    let mut current_progress = progress;
    while current_progress < metadata_progress {
        let batch_target = std::cmp::min(
            current_progress + CATCHUP_BATCH_SIZE,
            metadata_progress
        );
        myself.prune(current_progress, batch_target)?;
        current_progress = batch_target;
    }

    Ok(myself)
}
```

Apply the same fix to all other sub-pruners that have similar catch-up logic.

## Proof of Concept
```rust
// Add to storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner_test.rs

#[test]
fn test_large_range_catchup_memory_usage() {
    // Setup: Create database with 100K versions of events
    let tmp_dir = TempPath::new();
    let db = AptosDB::new_for_test(&tmp_dir);
    
    // Insert 100K versions with 10 events each (1M total events)
    for version in 0..100_000 {
        let events: Vec<ContractEvent> = (0..10)
            .map(|i| create_test_event(version, i))
            .collect();
        db.save_transactions(..., &events, ...).unwrap();
    }
    
    // Simulate inconsistency: Set LedgerPrunerProgress ahead but reset EventPrunerProgress
    db.ledger_db.metadata_db().put::<DbMetadataSchema>(
        &DbMetadataKey::LedgerPrunerProgress,
        &DbMetadataValue::Version(100_000)
    ).unwrap();
    
    db.ledger_db.event_db_raw().delete::<DbMetadataSchema>(
        &DbMetadataKey::EventPrunerProgress
    ).unwrap();
    
    // Measure memory before
    let mem_before = get_process_memory();
    
    // This should trigger the vulnerability: unbounded catch-up from 0 to 100K
    let start = std::time::Instant::now();
    let result = EventStorePruner::new(
        Arc::clone(db.ledger_db()),
        100_000, // metadata_progress
        None
    );
    let duration = start.elapsed();
    
    let mem_after = get_process_memory();
    let mem_used = mem_after - mem_before;
    
    // Assert memory usage is excessive (should be >500 MB for 100K versions)
    assert!(mem_used > 500 * 1024 * 1024, 
        "Expected >500MB memory usage, got {} MB", 
        mem_used / (1024 * 1024)
    );
    
    // Assert initialization time is excessive (should be >30 seconds)
    assert!(duration.as_secs() > 30,
        "Expected >30s initialization, got {}s",
        duration.as_secs()
    );
}
```

The test demonstrates that initialization with a large catch-up range consumes excessive memory and time, confirming the vulnerability.

## Notes
All other ledger sub-pruners (TransactionPruner, TransactionInfoPruner, TransactionAccumulatorPruner, etc.) follow the same pattern and have identical vulnerabilities in their initialization catch-up logic. A comprehensive fix should address all sub-pruners systematically.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L84-109)
```rust
impl EventStorePruner {
    pub(in crate::pruner) fn new(
        ledger_db: Arc<LedgerDb>,
        metadata_progress: Version,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.event_db_raw(),
            &DbMetadataKey::EventPrunerProgress,
            metadata_progress,
        )?;

        let myself = EventStorePruner {
            ledger_db,
            internal_indexer_db,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up EventStorePruner."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/ledger_db/event_db.rs (L192-222)
```rust
    pub(crate) fn prune_event_indices(
        &self,
        start: Version,
        end: Version,
        mut indices_batch: Option<&mut SchemaBatch>,
    ) -> Result<Vec<usize>> {
        let mut ret = Vec::new();

        let mut current_version = start;

        for events in self.get_events_by_version_iter(start, (end - start) as usize)? {
            let events = events?;
            ret.push(events.len());

            if let Some(ref mut batch) = indices_batch {
                for event in events {
                    if let ContractEvent::V1(v1) = event {
                        batch.delete::<EventByKeySchema>(&(*v1.key(), v1.sequence_number()))?;
                        batch.delete::<EventByVersionSchema>(&(
                            *v1.key(),
                            current_version,
                            v1.sequence_number(),
                        ))?;
                    }
                }
            }
            current_version += 1;
        }

        Ok(ret)
    }
```

**File:** storage/schemadb/src/batch.rs (L127-149)
```rust
/// `SchemaBatch` holds a collection of updates that can be applied to a DB atomically. The updates
/// will be applied in the order in which they are added to the `SchemaBatch`.
#[derive(Debug, Default)]
pub struct SchemaBatch {
    rows: DropHelper<HashMap<ColumnFamilyName, Vec<WriteOp>>>,
    stats: SampledBatchStats,
}

impl SchemaBatch {
    /// Creates an empty batch.
    pub fn new() -> Self {
        Self::default()
    }

    /// keep these on the struct itself so that we don't need to update each call site.
    pub fn put<S: Schema>(&mut self, key: &S::Key, value: &S::Value) -> DbResult<()> {
        <Self as WriteBatch>::put::<S>(self, key, value)
    }

    pub fn delete<S: Schema>(&mut self, key: &S::Key) -> DbResult<()> {
        <Self as WriteBatch>::delete::<S>(self, key)
    }
}
```

**File:** storage/aptosdb/src/event_store/mod.rs (L319-336)
```rust
    /// Prunes events by accumulator store for a range of version in [begin, end)
    pub(crate) fn prune_event_accumulator(
        &self,
        begin: Version,
        end: Version,
        db_batch: &mut SchemaBatch,
    ) -> anyhow::Result<()> {
        let mut iter = self.event_db.iter::<EventAccumulatorSchema>()?;
        iter.seek(&(begin, Position::from_inorder_index(0)))?;
        while let Some(((version, position), _)) = iter.next().transpose()? {
            if version >= end {
                return Ok(());
            }
            db_batch.delete::<EventAccumulatorSchema>(&(version, position))?;
        }
        Ok(())
    }
}
```

**File:** config/src/config/storage_config.rs (L387-396)
```rust
impl Default for LedgerPrunerConfig {
    fn default() -> Self {
        LedgerPrunerConfig {
            enable: true,
            prune_window: 90_000_000,
            batch_size: 5_000,
            user_pruning_window_offset: 200_000,
        }
    }
}
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L62-92)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning ledger data."
            );
            self.ledger_metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning ledger data is done.");
        }

        Ok(target_version)
    }
```
