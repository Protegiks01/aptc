# Audit Report

## Title
Zero Request Interval Allows Peer Monitoring Service Request Spamming

## Summary

The `RequestTracker::new()` function accepts `request_interval_ms = 0` without validation, creating a tracker that enables continuous request spamming to connected peers. When the peer monitoring loop executes every second, it will send new requests as soon as previous requests complete, causing resource exhaustion on peer nodes. This violates the Resource Limits invariant and can degrade network monitoring performance. [1](#0-0) 

## Finding Description

The vulnerability exists in the request interval validation logic. The `new()` function multiplies the input by 1000 to convert milliseconds to microseconds but performs no validation to ensure the interval is non-zero: [2](#0-1) 

The configuration values are loaded from node config files without validation in the config sanitizer: [3](#0-2) 

When `request_interval_usec = 0`, the `new_request_required()` logic becomes problematic. The comparison `self.time_service.now() > last_request_time.add(Duration::from_micros(0))` simplifies to `now() > last_request_time`, which evaluates to true immediately after any time advancement: [4](#0-3) 

The peer monitoring loop calls `refresh_peer_states()` every `peer_monitor_interval_usec` (default 1 second), which checks each tracker's `new_request_required()`: [5](#0-4) [6](#0-5) 

**Attack Path:**
1. Node operator (accidentally or maliciously) sets `latency_ping_interval_ms: 0`, `network_info_request_interval_ms: 0`, or `node_info_request_interval_ms: 0` in config
2. `RequestTracker::new()` accepts zero value without validation
3. Peer monitor loop executes every 1 second
4. `new_request_required()` returns true immediately after previous request completes
5. Node continuously sends monitoring requests to all connected peers
6. Peer nodes experience resource exhaustion (CPU, network bandwidth, thread pool)

While the in-flight request check prevents concurrent requests, it does not prevent sequential spamming. The server-side `BoundedExecutor` limits concurrent request processing but not sequential spam: [7](#0-6) 

## Impact Explanation

This qualifies as **Medium Severity** under the Aptos bug bounty criteria for the following reasons:

1. **Resource Exhaustion**: Affected peer nodes experience continuous processing of monitoring requests, consuming CPU cycles, network bandwidth, and bounded executor threads
2. **Service Degradation**: Legitimate peer monitoring becomes degraded as resources are consumed by spam requests
3. **Amplification Potential**: Multiple misconfigured nodes can coordinate (intentionally or accidentally) to overwhelm peer nodes
4. **Monitoring Service Failure**: Critical peer monitoring metrics may become unreliable or unavailable

This does not reach High or Critical severity because:
- It does not directly affect consensus safety or liveness
- It does not cause fund loss or theft
- It does not cause permanent network partition
- It requires node operator configuration (though this could be accidental)

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability is likely to occur because:

1. **No Validation**: There is no config validation preventing zero values in `PeerMonitoringServiceConfig`
2. **Accidental Misconfiguration**: A typo (typing `0` instead of `10000`) could trigger this
3. **Unclear Semantics**: Zero intervals are semantically invalid but syntactically accepted
4. **Multiple Attack Vectors**: Three separate config fields can trigger this (latency, network info, node info intervals)
5. **Default Values Not Enforced**: Config deserializer accepts any u64 value

The config sanitizer validates many other settings but not peer monitoring intervals: [8](#0-7) 

## Recommendation

Add input validation to reject zero or unreasonably small intervals:

**Option 1: Constructor Validation**
```rust
pub fn new(request_interval_ms: u64, time_service: TimeService) -> Result<Self, Error> {
    if request_interval_ms == 0 {
        return Err(Error::InvalidConfig(
            "request_interval_ms must be greater than 0".to_string()
        ));
    }
    let request_interval_usec = request_interval_ms * 1000;
    Ok(RequestTracker::new_with_microseconds(request_interval_usec, time_service))
}
```

**Option 2: Config Sanitizer Validation**
```rust
impl ConfigSanitizer for PeerMonitoringServiceConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let config = &node_config.peer_monitoring_service;
        
        // Validate latency monitoring intervals
        if config.latency_monitoring.latency_ping_interval_ms == 0 {
            return Err(Error::ConfigSanitizerFailed(
                Self::get_sanitizer_name(),
                "latency_ping_interval_ms must be greater than 0".into(),
            ));
        }
        
        // Validate network monitoring intervals
        if config.network_monitoring.network_info_request_interval_ms == 0 {
            return Err(Error::ConfigSanitizerFailed(
                Self::get_sanitizer_name(),
                "network_info_request_interval_ms must be greater than 0".into(),
            ));
        }
        
        // Validate node monitoring intervals
        if config.node_monitoring.node_info_request_interval_ms == 0 {
            return Err(Error::ConfigSanitizerFailed(
                Self::get_sanitizer_name(),
                "node_info_request_interval_ms must be greater than 0".into(),
            ));
        }
        
        Ok(())
    }
}
```

Recommended minimum intervals:
- `latency_ping_interval_ms`: minimum 1000ms (1 second)
- `network_info_request_interval_ms`: minimum 5000ms (5 seconds)  
- `node_info_request_interval_ms`: minimum 5000ms (5 seconds)

## Proof of Concept

```rust
#[cfg(test)]
mod zero_interval_vulnerability_test {
    use super::*;
    use aptos_time_service::TimeService;
    use std::time::Duration;

    #[test]
    fn test_zero_interval_allows_continuous_requests() {
        // Create a request tracker with zero interval (vulnerability)
        let time_service = TimeService::mock();
        let mut request_tracker = RequestTracker::new(0, time_service.clone());
        
        // Initially, a request should be required
        assert!(request_tracker.new_request_required());
        
        // Mark a request as started
        request_tracker.request_started();
        assert!(!request_tracker.new_request_required()); // In-flight check works
        
        // Complete the request
        request_tracker.request_completed();
        
        // With zero interval, a new request is immediately required
        // This demonstrates the vulnerability
        let mock_time = time_service.into_mock();
        mock_time.advance(Duration::from_nanos(1)); // Even 1 nanosecond is enough
        
        assert!(request_tracker.new_request_required()); // VULNERABILITY: Always true!
        
        // Simulate rapid request spam
        for _ in 0..1000 {
            assert!(request_tracker.new_request_required());
            request_tracker.request_started();
            request_tracker.request_completed();
            mock_time.advance(Duration::from_millis(1));
            // Request is immediately required again - continuous spam!
        }
    }
    
    #[test]
    fn test_proper_interval_prevents_spam() {
        // Create a request tracker with proper interval (30 seconds)
        let time_service = TimeService::mock();
        let mut request_tracker = RequestTracker::new(30_000, time_service.clone());
        
        // Initially, a request should be required
        assert!(request_tracker.new_request_required());
        
        // Mark a request as started
        request_tracker.request_started();
        request_tracker.request_completed();
        
        // With proper interval, request is NOT immediately required
        let mock_time = time_service.into_mock();
        mock_time.advance(Duration::from_secs(1));
        assert!(!request_tracker.new_request_required()); // Correctly waits
        
        // Only after 30 seconds should new request be allowed
        mock_time.advance(Duration::from_secs(30));
        assert!(request_tracker.new_request_required()); // Now ready
    }
}
```

**Notes**

This vulnerability demonstrates a critical principle: **all external inputs, including configuration values, must be validated**. The lack of validation in `RequestTracker::new()` and the config sanitizer creates an attack vector that can be exploited either accidentally (configuration typo) or intentionally (malicious node operator).

While peer monitoring is not a consensus-critical component, resource exhaustion attacks can degrade network health and monitoring reliability, which are important for network operations and security observability.

### Citations

**File:** peer-monitoring-service/client/src/peer_states/request_tracker.rs (L23-38)
```rust
    pub fn new(request_interval_ms: u64, time_service: TimeService) -> Self {
        let request_interval_usec = request_interval_ms * 1000;
        RequestTracker::new_with_microseconds(request_interval_usec, time_service)
    }

    /// Creates a new request tracker with the given request interval in usec
    pub fn new_with_microseconds(request_interval_usec: u64, time_service: TimeService) -> Self {
        Self {
            in_flight_request: false,
            last_request_time: None,
            last_response_time: None,
            num_consecutive_request_failures: 0,
            request_interval_usec,
            time_service,
        }
    }
```

**File:** peer-monitoring-service/client/src/peer_states/request_tracker.rs (L76-90)
```rust
    pub fn new_request_required(&self) -> bool {
        // There's already an in-flight request. A new one should not be sent.
        if self.in_flight_request() {
            return false;
        }

        // Otherwise, check the last request time for freshness
        match self.last_request_time {
            Some(last_request_time) => {
                self.time_service.now()
                    > last_request_time.add(Duration::from_micros(self.request_interval_usec))
            },
            None => true, // A request should be sent immediately
        }
    }
```

**File:** config/src/config/peer_monitoring_config.rs (L40-56)
```rust
pub struct LatencyMonitoringConfig {
    pub latency_ping_interval_ms: u64, // The interval (ms) between latency pings for each peer
    pub latency_ping_timeout_ms: u64,  // The timeout (ms) for each latency ping
    pub max_latency_ping_failures: u64, // Max ping failures before the peer connection fails
    pub max_num_latency_pings_to_retain: usize, // The max latency pings to retain per peer
}

impl Default for LatencyMonitoringConfig {
    fn default() -> Self {
        Self {
            latency_ping_interval_ms: 30_000, // 30 seconds
            latency_ping_timeout_ms: 20_000,  // 20 seconds
            max_latency_ping_failures: 3,
            max_num_latency_pings_to_retain: 10,
        }
    }
}
```

**File:** peer-monitoring-service/client/src/lib.rs (L104-122)
```rust
    let monitoring_service_config = node_config.peer_monitoring_service;
    let peer_monitor_duration =
        Duration::from_micros(monitoring_service_config.peer_monitor_interval_usec);
    let peer_monitor_ticker = time_service.interval(peer_monitor_duration);
    futures::pin_mut!(peer_monitor_ticker);

    // Start the peer monitoring loop
    info!(LogSchema::new(LogEntry::PeerMonitorLoop)
        .event(LogEvent::StartedPeerMonitorLoop)
        .message("Starting the peer monitor!"));
    loop {
        // Wait for the next round before pinging peers
        peer_monitor_ticker.next().await;

        // Get all connected peers
        let connected_peers_and_metadata =
            match peers_and_metadata.get_connected_peers_and_metadata() {
                Ok(connected_peers_and_metadata) => connected_peers_and_metadata,
                Err(error) => {
```

**File:** peer-monitoring-service/client/src/peer_states/mod.rs (L55-68)
```rust
            // Update the state if it needs to be refreshed
            let should_refresh_peer_state_key = request_tracker.read().new_request_required();
            if should_refresh_peer_state_key {
                peer_state.refresh_peer_state_key(
                    monitoring_service_config,
                    &peer_state_key,
                    peer_monitoring_client.clone(),
                    *peer_network_id,
                    peer_metadata.clone(),
                    peer_monitor_state.request_id_generator.clone(),
                    time_service.clone(),
                    runtime.clone(),
                )?;
            }
```

**File:** peer-monitoring-service/server/src/lib.rs (L66-81)
```rust
        let bounded_executor = BoundedExecutor::new(
            node_config.peer_monitoring_service.max_concurrent_requests as usize,
            executor,
        );
        let start_time = time_service.now();

        Self {
            base_config,
            bounded_executor,
            network_requests,
            peers_and_metadata,
            start_time,
            storage,
            time_service,
        }
    }
```

**File:** config/src/config/config_sanitizer.rs (L40-70)
```rust
    fn sanitize(
        node_config: &NodeConfig,
        node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        // If config sanitization is disabled, don't do anything!
        if node_config.node_startup.skip_config_sanitizer {
            return Ok(());
        }

        // Sanitize all of the sub-configs
        AdminServiceConfig::sanitize(node_config, node_type, chain_id)?;
        ApiConfig::sanitize(node_config, node_type, chain_id)?;
        BaseConfig::sanitize(node_config, node_type, chain_id)?;
        ConsensusConfig::sanitize(node_config, node_type, chain_id)?;
        DagConsensusConfig::sanitize(node_config, node_type, chain_id)?;
        ExecutionConfig::sanitize(node_config, node_type, chain_id)?;
        sanitize_failpoints_config(node_config, node_type, chain_id)?;
        sanitize_fullnode_network_configs(node_config, node_type, chain_id)?;
        IndexerGrpcConfig::sanitize(node_config, node_type, chain_id)?;
        InspectionServiceConfig::sanitize(node_config, node_type, chain_id)?;
        LoggerConfig::sanitize(node_config, node_type, chain_id)?;
        MempoolConfig::sanitize(node_config, node_type, chain_id)?;
        NetbenchConfig::sanitize(node_config, node_type, chain_id)?;
        StateSyncConfig::sanitize(node_config, node_type, chain_id)?;
        StorageConfig::sanitize(node_config, node_type, chain_id)?;
        InternalIndexerDBConfig::sanitize(node_config, node_type, chain_id)?;
        sanitize_validator_network_config(node_config, node_type, chain_id)?;

        Ok(()) // All configs passed validation
    }
```
