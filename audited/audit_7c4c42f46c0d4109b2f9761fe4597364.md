# Audit Report

## Title
Indexer-GRPC DataClient Infinite Retry Loop Without Timeout Causes Complete Service Unavailability

## Summary
The `DataClient` in the indexer-grpc-data-service-v2 contains an infinite retry loop without timeout, backoff, or error handling. When all GrpcManager connections fail, the service hangs indefinitely, making the entire indexer unavailable to all downstream applications (wallets, explorers, dApps) that depend on blockchain data access.

## Finding Description

The vulnerability exists in the `fetch_transactions` method which is the single point of data ingestion for the entire indexer service. [1](#0-0) 

The method implements an infinite `loop` that continuously retries gRPC requests on failure without any timeout, exponential backoff, circuit breaker, or maximum retry limit. The TODO comment explicitly acknowledges missing error handling.

The `FetchManager` creates a single `DataClient` instance with no redundancy: [2](#0-1) 

When the `DataClient` hangs, the `FetchManager` cannot fetch new data: [3](#0-2) 

The `InMemoryCache` waits indefinitely for fetch tasks to complete: [4](#0-3) 

All client requests through `LiveDataService` block waiting for cache data: [5](#0-4) 

**Attack Scenarios:**
1. **Network Partition**: Indexer loses connectivity to all configured GrpcManager nodes
2. **Configuration Error**: Invalid GrpcManager addresses in config
3. **Upstream Failures**: All fullnode GrpcManager services crash or become unreachable
4. **Malicious Network**: Attacker causes network disruption between indexer and GrpcManagers

While `ConnectionManager` supports multiple addresses, the random selection provides no benefit when ALL connections fail simultaneously: [6](#0-5) 

## Impact Explanation

This qualifies as **High Severity** under Aptos bug bounty criteria for "API crashes" and "Significant protocol violations":

1. **Complete Service Unavailability**: The entire indexer-grpc service becomes unresponsive, preventing all downstream applications from accessing blockchain data

2. **Cascading Downstream Failures**: Wallets, blockchain explorers, dApps, analytics platforms, and any service depending on the indexer API lose real-time blockchain data access

3. **No Graceful Degradation**: Unlike transient errors, this creates a permanent hang requiring manual service restart

4. **Single Point of Failure**: Zero redundancy in the data fetching pipeline - one component failure cascades to total service failure

5. **Manual Intervention Required**: Automated health checks cannot detect the hang since the service remains technically "alive" but non-functional

## Likelihood Explanation

**High Likelihood** due to:

1. **Multiple Failure Modes**: Network issues, misconfiguration, upstream service failures, or resource exhaustion can all trigger this condition

2. **No Defensive Programming**: Absence of timeouts, circuit breakers, or retry limits means any transient issue becomes permanent

3. **Production Reality**: Network partitions, DNS failures, firewall changes, and service outages occur regularly in distributed systems

4. **Acknowledged Technical Debt**: The TODO comment indicates this is a known limitation, increasing the probability it will manifest in production

## Recommendation

Implement comprehensive error handling with timeouts, exponential backoff, circuit breaker pattern, and maximum retry limits:

```rust
pub(super) async fn fetch_transactions(&self, starting_version: u64) -> Result<Vec<Transaction>, DataClientError> {
    const MAX_RETRIES: u32 = 5;
    const INITIAL_BACKOFF_MS: u64 = 100;
    const MAX_BACKOFF_MS: u64 = 10_000;
    const REQUEST_TIMEOUT: Duration = Duration::from_secs(30);
    
    let request = GetTransactionsRequest {
        starting_version: Some(starting_version),
        transactions_count: None,
        batch_size: None,
        transaction_filter: None,
    };
    
    for attempt in 0..MAX_RETRIES {
        let mut client = self.connection_manager.get_grpc_manager_client_for_request();
        
        match tokio::time::timeout(REQUEST_TIMEOUT, client.get_transactions(request.clone())).await {
            Ok(Ok(response)) => {
                let transactions = response.into_inner().transactions;
                if transactions.is_empty() {
                    return Ok(vec![]);
                }
                if transactions.first().unwrap().version == starting_version {
                    return Ok(transactions);
                }
                // Version mismatch, retry
            }
            Ok(Err(e)) => {
                warn!("gRPC error on attempt {}: {:?}", attempt + 1, e);
            }
            Err(_) => {
                warn!("Request timeout on attempt {}", attempt + 1);
            }
        }
        
        // Exponential backoff
        let backoff = std::cmp::min(
            INITIAL_BACKOFF_MS * 2_u64.pow(attempt),
            MAX_BACKOFF_MS
        );
        tokio::time::sleep(Duration::from_millis(backoff)).await;
    }
    
    Err(DataClientError::MaxRetriesExceeded)
}
```

Additionally, implement circuit breaker pattern and multiple DataClient instances for redundancy.

## Proof of Concept

```rust
// Test reproduction steps:
// 1. Configure indexer-grpc-data-service-v2 with invalid GrpcManager addresses
// 2. Start the service - it will initialize successfully
// 3. Observe that continuously_fetch_latest_data() hangs indefinitely
// 4. All client requests to get_transactions will timeout waiting for data
// 5. Service remains "alive" but completely non-functional

#[cfg(test)]
mod tests {
    use super::*;
    use std::time::Duration;
    
    #[tokio::test]
    async fn test_dataclient_hangs_on_all_connection_failures() {
        // Configure ConnectionManager with unreachable addresses
        let invalid_addresses = vec![
            "http://127.0.0.1:1".to_string(),  // Nothing listening here
            "http://127.0.0.1:2".to_string(),
        ];
        
        let connection_manager = Arc::new(
            ConnectionManager::new(
                1, // chain_id
                invalid_addresses,
                "http://127.0.0.1:8080".to_string(),
                true,
            ).await
        );
        
        let data_client = DataClient::new(connection_manager);
        
        // This call will hang indefinitely due to infinite retry loop
        let fetch_task = tokio::spawn(async move {
            data_client.fetch_transactions(0).await
        });
        
        // Timeout after 5 seconds to prove it hangs
        let result = tokio::time::timeout(
            Duration::from_secs(5),
            fetch_task
        ).await;
        
        assert!(result.is_err(), "Expected timeout but fetch_transactions returned");
    }
}
```

---

## Notes

This vulnerability specifically affects the **indexer-grpc-data-service-v2**, which is a critical infrastructure component for blockchain data access but is separate from the core consensus and execution layers. While this does not compromise blockchain state integrity or consensus safety, it creates a complete denial-of-service condition for all applications depending on indexed blockchain data. The issue is explicitly documented as technical debt via the TODO comment, and the lack of basic resilience patterns (timeouts, circuit breakers, backoff) makes this a highly probable production failure mode.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/data_client.rs (L18-43)
```rust
    pub(super) async fn fetch_transactions(&self, starting_version: u64) -> Vec<Transaction> {
        trace!("Fetching transactions from GrpcManager, start_version: {starting_version}.");

        let request = GetTransactionsRequest {
            starting_version: Some(starting_version),
            transactions_count: None,
            batch_size: None,
            transaction_filter: None,
        };
        loop {
            let mut client = self
                .connection_manager
                .get_grpc_manager_client_for_request();
            let response = client.get_transactions(request.clone()).await;
            if let Ok(response) = response {
                let transactions = response.into_inner().transactions;
                if transactions.is_empty() {
                    return vec![];
                }
                if transactions.first().unwrap().version == starting_version {
                    return transactions;
                }
            }
            // TODO(grao): Error handling.
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/fetch_manager.rs (L23-32)
```rust
    pub(super) fn new(
        data_manager: Arc<RwLock<DataManager>>,
        connection_manager: Arc<ConnectionManager>,
    ) -> Self {
        Self {
            data_manager,
            data_client: Arc::new(DataClient::new(connection_manager)),
            fetching_latest_data_task: RwLock::new(None),
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/fetch_manager.rs (L34-38)
```rust
    pub(super) async fn fetch_past_data(&self, version: u64) -> usize {
        let _timer = TIMER.with_label_values(&["fetch_past_data"]).start_timer();
        Self::fetch_and_update_cache(self.data_client.clone(), self.data_manager.clone(), version)
            .await
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/in_memory_cache.rs (L50-63)
```rust
        while starting_version >= self.data_manager.read().await.end_version {
            trace!("Reached head, wait...");
            let num_transactions = self
                .fetch_manager
                .fetching_latest_data_task
                .read()
                .await
                .as_ref()
                .unwrap()
                .clone()
                .await;

            trace!("Done waiting, got {num_transactions} transactions at head.");
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/mod.rs (L185-194)
```rust
            if let Some((transactions, batch_size_bytes, last_processed_version)) = self
                .in_memory_cache
                .get_data(
                    next_version,
                    ending_version,
                    max_num_transactions_per_batch,
                    max_bytes_per_batch,
                    &filter,
                )
                .await
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/connection_manager.rs (L172-179)
```rust
    pub(crate) fn get_grpc_manager_client_for_request(&self) -> GrpcManagerClient<Channel> {
        let mut rng = thread_rng();
        self.grpc_manager_connections
            .iter()
            .choose(&mut rng)
            .map(|kv| kv.value().clone())
            .unwrap()
    }
```
