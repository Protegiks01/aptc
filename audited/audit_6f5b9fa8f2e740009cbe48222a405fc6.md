# Audit Report

## Title
Atomicity Violation in QuorumStoreCoordinator Causes Partial State Updates and Network Inconsistency

## Summary
The `QuorumStoreCoordinator` module lacks atomicity guarantees when distributing `CommitNotification` messages to its three sub-components. If any component fails to receive the notification due to a channel send failure, the coordinator panics, but components that already received the message will have processed it, creating an inconsistent state across the quorum store subsystem that can lead to transaction liveness failures and contribute to network partitions.

## Finding Description

The `QuorumStoreCoordinator::start()` method processes `CommitNotification` commands by sequentially sending them to three components: `ProofCoordinator`, `ProofManager`, and `BatchGenerator`. [1](#0-0) 

Each send operation uses `.await.expect()`, which means:
1. If the send succeeds, the message is queued in the receiver's channel
2. If the send fails (receiver dropped), the coordinator panics immediately
3. No rollback or compensation mechanism exists

**The Atomicity Violation:**

When a `CommitNotification(block_timestamp, batches)` is processed:
1. `ProofCoordinator` receives and processes the notification successfully
2. `ProofManager`'s receiver is dropped (due to crash/panic)
3. The coordinator's send to `ProofManager` returns `Err(SendError)`
4. The `.expect()` causes coordinator to panic
5. `BatchGenerator` never receives the notification

**Result:** The three components now have divergent state:
- **ProofCoordinator**: Has marked batches from `block_timestamp` as completed
- **ProofManager**: Crashed or has stale state, missing commit notification
- **BatchGenerator**: Still has old `latest_block_timestamp`, expired batches remain in `batches_in_progress`

**Impact on BatchGenerator:**

The `BatchGenerator` uses `CommitNotification` to update critical state: [2](#0-1) 

If `BatchGenerator` misses a `CommitNotification`:
- `latest_block_timestamp` remains stale
- Committed batches are not removed from `batches_in_progress`
- These batches' transactions remain in `txns_in_progress_sorted`
- Future mempool pulls exclude these transactions indefinitely
- Transactions become stuck and cannot be re-proposed

**Impact on ProofManager:**

The `ProofManager` processes commit notifications to clean up state: [3](#0-2) 

Missing notifications cause:
- Stale `latest_block_timestamp` in `BatchProofQueue`
- Expired proofs not garbage collected
- Back pressure metrics calculated incorrectly
- Memory accumulation

**No Recovery Mechanism:**

There is no reconciliation protocol to detect or correct state divergence between components. Each component maintains independent state with no global coordination mechanism.

## Impact Explanation

This vulnerability qualifies as **High Severity** per Aptos bug bounty criteria:

1. **State Inconsistencies Requiring Intervention**: The divergent state across components cannot self-heal and requires node restart or manual intervention to resolve.

2. **Transaction Liveness Failure**: Transactions stuck in `BatchGenerator`'s `batches_in_progress` cannot be re-pulled from mempool, causing permanent liveness issues for affected transactions until the node is restarted.

3. **Validator Node Slowdowns**: Memory leaks from accumulated expired batches in `ProofManager` and stuck transactions in `BatchGenerator` cause progressive performance degradation.

4. **Potential Network Partition**: If different validators experience partial failures at different times, they may have divergent views of which batches are committed, potentially contributing to consensus disagreements and network instability.

5. **Violates Critical Invariants**:
   - **State Consistency**: State transitions are not atomic
   - **Deterministic Execution**: Different validators may process blocks differently due to inconsistent quorum store state

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability can be triggered by:

1. **Component Crashes**: Any panic or error in `ProofManager`, `ProofCoordinator`, or `BatchGenerator` that causes the task to terminate will drop its receiver, causing subsequent sends to fail.

2. **Resource Exhaustion**: If a component's message processing loop becomes blocked or extremely slow, its channel could fill up (though less likely with 1000 buffer size).

3. **Epoch Transition Race Conditions**: During epoch transitions, components may be shutting down while commit notifications are still being sent.

4. **No Special Privileges Required**: This is a logic bug that occurs during normal operation, not requiring any malicious input or validator collusion.

The code comment at line 60 suggests the developers were aware of potential issues ("TODO: need a callback or not?"), but no proper transaction semantics were implemented. [4](#0-3) 

## Recommendation

Implement atomic broadcast semantics for `CommitNotification` messages using one of these approaches:

**Option 1: Two-Phase Commit Protocol**
```rust
CoordinatorCommand::CommitNotification(block_timestamp, batches) => {
    // Phase 1: Send to all components
    let send_results = futures::join!(
        self.proof_coordinator_cmd_tx.send(ProofCoordinatorCommand::CommitNotification(batches.clone())),
        self.proof_manager_cmd_tx.send(ProofManagerCommand::CommitNotification(block_timestamp, batches.clone())),
        self.batch_generator_cmd_tx.send(BatchGeneratorCommand::CommitNotification(block_timestamp, batches.clone()))
    );
    
    // Phase 2: Verify all sends succeeded
    if send_results.0.is_err() || send_results.1.is_err() || send_results.2.is_err() {
        error!("Failed to atomically deliver CommitNotification - initiating recovery");
        // Either retry or trigger component restart to ensure consistency
        // DO NOT proceed with partial delivery
        return;
    }
}
```

**Option 2: Acknowledgment-Based Approach**
Add acknowledgment channels so the coordinator can verify all components processed the notification before considering it complete.

**Option 3: Shared State with Version Numbers**
Use a shared atomic state structure that all components read from, eliminating the need for message passing for state synchronization.

## Proof of Concept

```rust
// Rust test demonstrating the atomicity violation
#[tokio::test]
async fn test_partial_commit_notification_atomicity_violation() {
    use tokio::sync::mpsc;
    use consensus::quorum_store::quorum_store_coordinator::{
        QuorumStoreCoordinator, CoordinatorCommand
    };
    
    // Setup channels
    let (proof_coord_tx, mut proof_coord_rx) = mpsc::channel(10);
    let (proof_mgr_tx, mut proof_mgr_rx) = mpsc::channel(10);
    let (batch_gen_tx, mut batch_gen_rx) = mpsc::channel(10);
    
    // Simulate ProofManager crash by dropping its receiver
    drop(proof_mgr_rx);
    
    // Create coordinator
    let coordinator = QuorumStoreCoordinator::new(
        test_peer_id(),
        batch_gen_tx,
        vec![],
        proof_coord_tx,
        proof_mgr_tx,
        test_quorum_store_msg_tx(),
    );
    
    let (coord_tx, coord_rx) = futures_channel::mpsc::channel(10);
    
    // Spawn coordinator
    tokio::spawn(async move {
        coordinator.start(coord_rx).await;
    });
    
    // Send CommitNotification
    coord_tx.send(CoordinatorCommand::CommitNotification(
        1000,
        vec![test_batch_info()],
    )).await.unwrap();
    
    // Verify partial delivery
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    // ProofCoordinator received message
    assert!(proof_coord_rx.try_recv().is_ok(), "ProofCoordinator should receive message");
    
    // BatchGenerator did NOT receive message (coordinator panicked first)
    assert!(batch_gen_rx.try_recv().is_err(), "BatchGenerator should NOT receive message");
    
    // This demonstrates the atomicity violation:
    // - ProofCoordinator processed the commit
    // - BatchGenerator never received it
    // - State is now inconsistent
}
```

## Notes

The vulnerability is exacerbated by the fact that `QuorumStoreCommitNotifier::notify()` uses `try_send()` which silently drops notifications if the coordinator's channel is full: [5](#0-4) 

This creates an additional failure mode where commit notifications can be lost entirely without any component being aware, further increasing the likelihood and severity of state inconsistencies.

The ordered shutdown sequence in the `Shutdown` command demonstrates that developers understood the importance of coordination, but this same rigor was not applied to `CommitNotification` handling: [6](#0-5)

### Citations

**File:** consensus/src/quorum_store/quorum_store_coordinator.rs (L56-81)
```rust
                    CoordinatorCommand::CommitNotification(block_timestamp, batches) => {
                        counters::QUORUM_STORE_MSG_COUNT
                            .with_label_values(&["QSCoordinator::commit_notification"])
                            .inc();
                        // TODO: need a callback or not?
                        self.proof_coordinator_cmd_tx
                            .send(ProofCoordinatorCommand::CommitNotification(batches.clone()))
                            .await
                            .expect("Failed to send to ProofCoordinator");

                        self.proof_manager_cmd_tx
                            .send(ProofManagerCommand::CommitNotification(
                                block_timestamp,
                                batches.clone(),
                            ))
                            .await
                            .expect("Failed to send to ProofManager");

                        self.batch_generator_cmd_tx
                            .send(BatchGeneratorCommand::CommitNotification(
                                block_timestamp,
                                batches,
                            ))
                            .await
                            .expect("Failed to send to BatchGenerator");
                    },
```

**File:** consensus/src/quorum_store/quorum_store_coordinator.rs (L82-162)
```rust
                    CoordinatorCommand::Shutdown(ack_tx) => {
                        counters::QUORUM_STORE_MSG_COUNT
                            .with_label_values(&["QSCoordinator::shutdown"])
                            .inc();
                        // Note: Shutdown is done from the back of the quorum store pipeline to the
                        // front, so senders are always shutdown before receivers. This avoids sending
                        // messages through closed channels during shutdown.
                        // Oneshots that send data in the reverse order of the pipeline must assume that
                        // the receiver could be unavailable during shutdown, and resolve this without
                        // panicking.

                        let (network_listener_shutdown_tx, network_listener_shutdown_rx) =
                            oneshot::channel();
                        match self.quorum_store_msg_tx.push(
                            self.my_peer_id,
                            (
                                self.my_peer_id,
                                VerifiedEvent::Shutdown(network_listener_shutdown_tx),
                            ),
                        ) {
                            Ok(()) => info!("QS: shutdown network listener sent"),
                            Err(err) => panic!("Failed to send to NetworkListener, Err {:?}", err),
                        };
                        network_listener_shutdown_rx
                            .await
                            .expect("Failed to stop NetworkListener");

                        let (batch_generator_shutdown_tx, batch_generator_shutdown_rx) =
                            oneshot::channel();
                        self.batch_generator_cmd_tx
                            .send(BatchGeneratorCommand::Shutdown(batch_generator_shutdown_tx))
                            .await
                            .expect("Failed to send to BatchGenerator");
                        batch_generator_shutdown_rx
                            .await
                            .expect("Failed to stop BatchGenerator");

                        for remote_batch_coordinator_cmd_tx in self.remote_batch_coordinator_cmd_tx
                        {
                            let (
                                remote_batch_coordinator_shutdown_tx,
                                remote_batch_coordinator_shutdown_rx,
                            ) = oneshot::channel();
                            remote_batch_coordinator_cmd_tx
                                .send(BatchCoordinatorCommand::Shutdown(
                                    remote_batch_coordinator_shutdown_tx,
                                ))
                                .await
                                .expect("Failed to send to Remote BatchCoordinator");
                            remote_batch_coordinator_shutdown_rx
                                .await
                                .expect("Failed to stop Remote BatchCoordinator");
                        }

                        let (proof_coordinator_shutdown_tx, proof_coordinator_shutdown_rx) =
                            oneshot::channel();
                        self.proof_coordinator_cmd_tx
                            .send(ProofCoordinatorCommand::Shutdown(
                                proof_coordinator_shutdown_tx,
                            ))
                            .await
                            .expect("Failed to send to ProofCoordinator");
                        proof_coordinator_shutdown_rx
                            .await
                            .expect("Failed to stop ProofCoordinator");

                        let (proof_manager_shutdown_tx, proof_manager_shutdown_rx) =
                            oneshot::channel();
                        self.proof_manager_cmd_tx
                            .send(ProofManagerCommand::Shutdown(proof_manager_shutdown_tx))
                            .await
                            .expect("Failed to send to ProofManager");
                        proof_manager_shutdown_rx
                            .await
                            .expect("Failed to stop ProofManager");

                        ack_tx
                            .send(())
                            .expect("Failed to send shutdown ack from QuorumStore");
                        break;
                    },
```

**File:** consensus/src/quorum_store/batch_generator.rs (L517-553)
```rust
                        BatchGeneratorCommand::CommitNotification(block_timestamp, batches) => {
                            trace!(
                                "QS: got clean request from execution, block timestamp {}",
                                block_timestamp
                            );
                            // Block timestamp is updated asynchronously, so it may race when it enters state sync.
                            if self.latest_block_timestamp > block_timestamp {
                                continue;
                            }
                            self.latest_block_timestamp = block_timestamp;

                            for (author, batch_id) in batches.iter().map(|b| (b.author(), b.batch_id())) {
                                if self.remove_batch_in_progress(author, batch_id) {
                                    counters::BATCH_IN_PROGRESS_COMMITTED.inc();
                                }
                            }

                            // Cleans up all batches that expire in timestamp <= block_timestamp. This is
                            // safe since clean request must occur only after execution result is certified.
                            for (author, batch_id) in self.batch_expirations.expire(block_timestamp) {
                                if let Some(batch_in_progress) = self.batches_in_progress.get(&(author, batch_id)) {
                                    // If there is an identical batch with higher expiry time, re-insert it.
                                    if batch_in_progress.expiry_time_usecs > block_timestamp {
                                        self.batch_expirations.add_item((author, batch_id), batch_in_progress.expiry_time_usecs);
                                        continue;
                                    }
                                }
                                if self.remove_batch_in_progress(author, batch_id) {
                                    counters::BATCH_IN_PROGRESS_EXPIRED.inc();
                                    debug!(
                                        "QS: logical time based expiration batch w. id {} from batches_in_progress, new size {}",
                                        batch_id,
                                        self.batches_in_progress.len(),
                                    );
                                }
                            }
                        },
```

**File:** consensus/src/quorum_store/proof_manager.rs (L88-101)
```rust
    pub(crate) fn handle_commit_notification(
        &mut self,
        block_timestamp: u64,
        batches: Vec<BatchInfoExt>,
    ) {
        trace!(
            "QS: got clean request from execution at block timestamp {}",
            block_timestamp
        );
        self.batch_proof_queue.mark_committed(batches);
        self.batch_proof_queue
            .handle_updated_block_timestamp(block_timestamp);
        self.update_remaining_txns_and_proofs();
    }
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L44-58)
```rust
impl TQuorumStoreCommitNotifier for QuorumStoreCommitNotifier {
    fn notify(&self, block_timestamp: u64, batches: Vec<BatchInfoExt>) {
        let mut tx = self.coordinator_tx.clone();

        if let Err(e) = tx.try_send(CoordinatorCommand::CommitNotification(
            block_timestamp,
            batches,
        )) {
            warn!(
                "CommitNotification failed. Is the epoch shutting down? error: {}",
                e
            );
        }
    }
}
```
