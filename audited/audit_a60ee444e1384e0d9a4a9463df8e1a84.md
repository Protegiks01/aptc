# Audit Report

## Title
Premature OptQS Re-enablement via Non-PayloadUnavailable Timeout Bypass in Exponential Window Failure Tracker

## Summary
The `compute_failure_window()` function in `consensus/src/liveness/proposal_status_tracker.rs` treats only `PayloadUnavailable` timeouts as failures that trigger exponential backoff window doubling. Other timeout types (`ProposalNotReceived`, `NoQC`, `Unknown`) are incorrectly counted as "successes," allowing OptQS (Optimistic Quorum Store) to be re-enabled prematurely after a sequence of non-PayloadUnavailable timeouts, even when the network is experiencing consistent failures.

## Finding Description

The exponential window backoff mechanism is designed to disable OptQS after payload availability failures and require a window of consecutive successes before re-enabling it. However, the implementation contains a critical logic flaw in how it classifies round outcomes. [1](#0-0) 

The matcher function counts ANY status that is NOT `PayloadUnavailable` as a "success": [2](#0-1) 

This means that consensus timeout failures like `RoundTimeoutReason::ProposalNotReceived`, `RoundTimeoutReason::NoQC`, and `RoundTimeoutReason::Unknown` are treated identically to genuine successes (`NewRoundReason::QCReady`). [3](#0-2) 

**Attack Scenario:**

1. **Initial State**: OptQS is enabled, window = 2
2. **Round N**: A `PayloadUnavailable` timeout occurs
   - Window doubles: window = 4
   - Success count resets: last_consecutive_success_count = 0
   - OptQS is disabled (requires 4 consecutive "successes" to re-enable)
3. **Rounds N+1 to N+4**: Network experiences other timeout types (e.g., `ProposalNotReceived` from Byzantine proposers, or `NoQC` from quorum failures)
   - Each is incorrectly counted as a "success"
   - last_consecutive_success_count increases: 1, 2, 3, 4
4. **OptQS Re-enables**: When the proposal generator calls `get_params()`, it checks if `last_consecutive_success_count >= window`: [4](#0-3) 

Since count (4) >= window (4), OptQS returns parameters and operates, even though the network just experienced 4 consecutive timeout failures.

The timeout reasons are determined in `round_manager.rs`: [5](#0-4) 

Byzantine validators can manipulate timeout types by:
- Not sending proposals when they're the leader → `ProposalNotReceived`
- Not voting to prevent quorum → `NoQC`
- Network adversaries can cause similar outcomes naturally

## Impact Explanation

**Severity: High** - This meets the "Validator node slowdowns" and "Significant protocol violations" criteria from the Aptos bug bounty program.

The vulnerability causes:
1. **Resource Waste**: Validators continue attempting OptQS payload pulls when the network is unstable, wasting CPU, memory, and bandwidth
2. **Performance Degradation**: OptQS failures during network stress compound existing performance issues
3. **Liveness Risk**: Under sustained attack or network stress, the continued use of OptQS when it's likely to fail can contribute to slower block production and temporary liveness degradation
4. **Backoff Mechanism Defeat**: The exponential backoff protection is bypassed, preventing the system from adapting to adverse conditions

While this doesn't directly cause fund loss or consensus safety violations (Critical severity), it significantly impacts validator performance and network health during stress conditions.

## Likelihood Explanation

**Likelihood: High**

This vulnerability can manifest in two ways:

1. **Natural Occurrence**: During network stress, partitions, or high load, validators naturally experience mixed timeout types. A `PayloadUnavailable` timeout followed by other timeout types is a realistic failure pattern.

2. **Malicious Exploitation**: Byzantine validators (up to < 1/3 by BFT assumption) can deliberately:
   - Cause initial `PayloadUnavailable` timeouts to trigger backoff
   - Then cause `ProposalNotReceived` timeouts by not sending proposals when elected as leader
   - Or abstain from voting to cause `NoQC` timeouts
   - This accumulates "successes" and prematurely re-enables OptQS

The aggregation logic in `pending_votes.rs` shows timeout reasons are determined by voting power majority: [6](#0-5) 

Even honest validators experiencing different timeout types will have their reasons aggregated, potentially creating the bypass condition without any malicious intent.

## Recommendation

Modify the logic to treat ALL timeout types as failures or at minimum as neutral (neither success nor failure). Only genuine `QCReady` round transitions should increment the success count.

**Recommended Fix:**

```rust
fn compute_failure_window(&mut self) {
    self.last_consecutive_success_count = self.last_consecutive_statuses_matching(|reason| {
        // Only QCReady is a true success; all timeouts are failures or neutral
        matches!(reason, NewRoundReason::QCReady)
    });
    
    // Check if most recent status is ANY timeout (failure)
    let is_recent_failure = self.past_round_statuses
        .back()
        .map(|reason| matches!(reason, NewRoundReason::Timeout(_)))
        .unwrap_or(false);
    
    if is_recent_failure {
        self.window *= 2;
        self.window = self.window.min(self.max_window);
    } else if self.last_consecutive_success_count == self.past_round_statuses.len() {
        self.window = 2;
    }
}
```

This ensures:
- Only actual `QCReady` rounds count as successes
- ANY timeout type triggers window doubling
- OptQS only re-enables after genuine consensus successes

## Proof of Concept

Add this test to `consensus/src/liveness/proposal_status_tracker.rs`:

```rust
#[test]
fn test_non_payload_timeout_bypass() {
    let (_signers, verifier) = random_validator_verifier(4, None, false);
    let mut tracker =
        ExponentialWindowFailureTracker::new(100, verifier.get_ordered_account_addresses());
    
    // Start with successful rounds
    tracker.push(NewRoundReason::QCReady);
    tracker.push(NewRoundReason::QCReady);
    assert_eq!(tracker.window, 2);
    assert_eq!(tracker.last_consecutive_success_count, 2);
    
    // PayloadUnavailable triggers backoff
    tracker.push(NewRoundReason::Timeout(
        RoundTimeoutReason::PayloadUnavailable {
            missing_authors: BitVec::with_num_bits(4),
        },
    ));
    assert_eq!(tracker.window, 4); // Window doubled
    assert_eq!(tracker.last_consecutive_success_count, 0);
    
    // Demonstrate the bypass: 4 non-PayloadUnavailable timeouts
    tracker.push(NewRoundReason::Timeout(RoundTimeoutReason::ProposalNotReceived));
    assert_eq!(tracker.last_consecutive_success_count, 1); // Incorrectly counted as success!
    
    tracker.push(NewRoundReason::Timeout(RoundTimeoutReason::NoQC));
    assert_eq!(tracker.last_consecutive_success_count, 2); // Still counting as success!
    
    tracker.push(NewRoundReason::Timeout(RoundTimeoutReason::Unknown));
    assert_eq!(tracker.last_consecutive_success_count, 3);
    
    tracker.push(NewRoundReason::Timeout(RoundTimeoutReason::ProposalNotReceived));
    assert_eq!(tracker.last_consecutive_success_count, 4);
    
    // VULNERABILITY: OptQS would now re-enable because count >= window
    // even though we just had 4 consecutive timeout failures!
    assert!(tracker.last_consecutive_success_count >= tracker.window,
        "OptQS re-enables after non-PayloadUnavailable timeout sequence");
}
```

This test demonstrates that after window doubling from `PayloadUnavailable`, four consecutive timeout failures of other types are sufficient to satisfy the window requirement and re-enable OptQS, defeating the purpose of the exponential backoff mechanism.

### Citations

**File:** consensus/src/liveness/proposal_status_tracker.rs (L65-78)
```rust
    fn compute_failure_window(&mut self) {
        self.last_consecutive_success_count = self.last_consecutive_statuses_matching(|reason| {
            !matches!(
                reason,
                NewRoundReason::Timeout(RoundTimeoutReason::PayloadUnavailable { .. })
            )
        });
        if self.last_consecutive_success_count == 0 {
            self.window *= 2;
            self.window = self.window.min(self.max_window);
        } else if self.last_consecutive_success_count == self.past_round_statuses.len() {
            self.window = 2;
        }
    }
```

**File:** consensus/src/liveness/proposal_status_tracker.rs (L137-143)
```rust
        if tracker.last_consecutive_success_count < tracker.window {
            warn!(
                "Skipping OptQS: (last_consecutive_successes) {} < {} (window)",
                tracker.last_consecutive_success_count, tracker.window
            );
            return None;
        }
```

**File:** consensus/consensus-types/src/round_timeout.rs (L16-22)
```rust
#[derive(Deserialize, Serialize, Clone, PartialEq, Eq, Hash, Debug)]
pub enum RoundTimeoutReason {
    Unknown,
    ProposalNotReceived,
    PayloadUnavailable { missing_authors: BitVec },
    NoQC,
}
```

**File:** consensus/src/round_manager.rs (L968-983)
```rust
    fn compute_timeout_reason(&self, round: Round) -> RoundTimeoutReason {
        if self.round_state().vote_sent().is_some() {
            return RoundTimeoutReason::NoQC;
        }

        match self.block_store.get_block_for_round(round) {
            None => RoundTimeoutReason::ProposalNotReceived,
            Some(block) => {
                if let Err(missing_authors) = self.block_store.check_payload(block.block()) {
                    RoundTimeoutReason::PayloadUnavailable { missing_authors }
                } else {
                    RoundTimeoutReason::Unknown
                }
            },
        }
    }
```

**File:** consensus/src/pending_votes.rs (L93-153)
```rust
    fn aggregated_timeout_reason(&self, verifier: &ValidatorVerifier) -> RoundTimeoutReason {
        let mut reason_voting_power: HashMap<RoundTimeoutReason, u128> = HashMap::new();
        let mut missing_batch_authors: HashMap<usize, u128> = HashMap::new();
        // let ordered_authors = verifier.get_ordered_account_addresses();
        for (author, reason) in &self.timeout_reason {
            // To aggregate the reason, we only care about the variant type itself and
            // exclude any data within the variants.
            let reason_key = match reason {
                reason @ RoundTimeoutReason::Unknown
                | reason @ RoundTimeoutReason::ProposalNotReceived
                | reason @ RoundTimeoutReason::NoQC => reason.clone(),
                RoundTimeoutReason::PayloadUnavailable { missing_authors } => {
                    for missing_idx in missing_authors.iter_ones() {
                        *missing_batch_authors.entry(missing_idx).or_default() +=
                            verifier.get_voting_power(author).unwrap_or_default() as u128;
                    }
                    RoundTimeoutReason::PayloadUnavailable {
                        // Since we care only about the variant type, we replace the bitvec
                        // with a placeholder.
                        missing_authors: BitVec::with_num_bits(verifier.len() as u16),
                    }
                },
            };
            *reason_voting_power.entry(reason_key).or_default() +=
                verifier.get_voting_power(author).unwrap_or_default() as u128;
        }
        // The aggregated timeout reason is the reason with the most voting power received from
        // at least f+1 peers by voting power. If such voting power does not exist, then the
        // reason is unknown.

        reason_voting_power
            .into_iter()
            .max_by_key(|(_, voting_power)| *voting_power)
            .filter(|(_, voting_power)| {
                verifier
                    .check_aggregated_voting_power(*voting_power, false)
                    .is_ok()
            })
            .map(|(reason, _)| {
                // If the aggregated reason is due to unavailable payload, we will compute the
                // aggregated missing authors bitvec counting batch authors that have been reported
                // missing by minority peers.
                if matches!(reason, RoundTimeoutReason::PayloadUnavailable { .. }) {
                    let mut aggregated_bitvec = BitVec::with_num_bits(verifier.len() as u16);
                    for (author_idx, voting_power) in missing_batch_authors {
                        if verifier
                            .check_aggregated_voting_power(voting_power, false)
                            .is_ok()
                        {
                            aggregated_bitvec.set(author_idx as u16);
                        }
                    }
                    RoundTimeoutReason::PayloadUnavailable {
                        missing_authors: aggregated_bitvec,
                    }
                } else {
                    reason
                }
            })
            .unwrap_or(RoundTimeoutReason::Unknown)
    }
```
