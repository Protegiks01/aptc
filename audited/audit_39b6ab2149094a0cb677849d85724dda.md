# Audit Report

## Title
Lack of Freshness Validation in Peer Network Topology Data Enables Malicious Peer Selection Manipulation

## Summary
The peer monitoring service's `GetNetworkInformation` request-response protocol lacks temporal freshness guarantees, allowing malicious peers to replay cached `NetworkInformationResponse` data with stale `distance_from_validators` values. This enables manipulation of peer selection decisions across consensus observer, state sync, and mempool subsystems, potentially causing validator node slowdowns and network inefficiency.

## Finding Description

The peer monitoring service implements a stateless request protocol where `create_monitoring_service_request()` returns `PeerMonitoringServiceRequest::GetNetworkInformation` without any challenge, nonce, or timestamp to ensure response freshness: [1](#0-0) 

The `NetworkInformationResponse` structure contains only `connected_peers` and `distance_from_validators` fields, with no timestamp or freshness indicator to validate when the data was generated: [2](#0-1) 

When responses are received, the validation logic only checks response type matching and performs sanity checks on the `distance_from_validators` value based on peer role and network context. No timestamp validation, monotonic increase checks, or challenge-response validation is performed: [3](#0-2) 

While the RPC framework provides request_id matching for transport-layer correlation, this only prevents out-of-order responses or responses for expired RPC calls within the same connection. The `handle_inbound_response` method matches responses by request_id but does not validate application-layer data freshness: [4](#0-3) 

This design allows a malicious peer to respond to a current request with old but previously valid application-layer data. The attack scenario is:

1. A malicious peer at `distance_from_validators=10` (poorly connected) caches a `NetworkInformationResponse` from when it was at `distance=1` (well-connected)
2. When queried via `GetNetworkInformation`, the peer returns the cached stale response with the current RPC `request_id`
3. The victim node accepts the response because the `request_id` matches and the distance value passes role-based sanity checks
4. The victim incorrectly believes the malicious peer is close to validators and prioritizes it for critical operations

**Impact on Critical Subsystems:**

The stale `distance_from_validators` value directly affects peer selection in three critical subsystems:

**1. Consensus Observer:** The `sort_peers_by_subscription_optimality` function groups peers by distance using a BTreeMap (ascending order), then sorts within each distance group by latency. Peers with lower distances are prioritized for consensus observer subscriptions: [5](#0-4) 

**2. State Sync:** The `choose_random_peers_by_distance_and_latency` function groups peers by distance in a BTreeMap (ascending order), then performs weighted selection by latency within each group. The function explicitly prioritizes distance over latency: [6](#0-5) 

**3. Mempool:** The `get_distance_from_validators` function extracts distance from `latest_network_info_response`, and `compare_validator_distance` prioritizes peers with lower validator distances in the intelligent peer prioritization strategy: [7](#0-6) [8](#0-7) 

Network information is queried periodically at 60-second intervals as configured in the default `NetworkMonitoringConfig`: [9](#0-8) 

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty criteria, specifically matching the "Validator Node Slowdowns (High)" category.

A malicious peer falsely claiming low distance from validators would be prioritized for:
- **Consensus observer subscriptions**: Victims subscribe to the malicious peer for consensus data, receiving blocks and proofs from a suboptimal source, potentially causing slower block propagation and synchronization delays
- **State sync operations**: Victims preferentially select the malicious peer for state fetching, degrading sync performance and prolonging catch-up time
- **Mempool transaction forwarding**: Transactions are preferentially forwarded to the poorly-connected peer, delaying network-wide transaction propagation

If multiple validators are simultaneously affected, this could cause network-wide performance degradation. The 60-second refresh interval means stale data persists for up to one minute per query cycle, allowing sustained impact.

**Importantly**, this does NOT violate consensus safety—blocks and proofs are still cryptographically validated regardless of source. The impact is confined to performance and resource efficiency, not correctness or fund safety.

## Likelihood Explanation

**Likelihood: High**

- **Attacker requirements**: Only requires being a connected peer on the network (no validator privileges or stake required)
- **Exploitation complexity**: Trivial—simply cache old responses and replay them with current request_id values
- **Detection difficulty**: Victims cannot distinguish stale data from fresh data without additional timestamp validation mechanisms
- **Affected population**: All nodes that query the malicious peer during the 60-second refresh interval
- **Persistence**: Attack can be sustained indefinitely by continuously replaying cached data

The attack is highly practical and requires minimal sophistication. An attacker simply needs to:
1. Establish network connectivity as a peer
2. Cache favorable network information responses
3. Return cached data when queried, matching the current request_id

## Recommendation

Implement temporal freshness validation for `NetworkInformationResponse` data:

1. **Add timestamp field to `NetworkInformationResponse`**: Include a server-side timestamp indicating when the network information was computed

2. **Validate response freshness on client side**: Check that received timestamps are within acceptable bounds (e.g., within the last 2x refresh interval)

3. **Implement monotonic timestamp validation**: Track the latest timestamp received from each peer and reject responses with timestamps older than previously seen values

4. **Consider challenge-response mechanism**: Include a nonce or challenge in the request that must be incorporated into the response to prevent replay attacks

Example schema addition:
```rust
pub struct NetworkInformationResponse {
    pub connected_peers: BTreeMap<PeerNetworkId, ConnectionMetadata>,
    pub distance_from_validators: u64,
    pub timestamp_usecs: u64,  // Add timestamp field
}
```

Example validation logic:
```rust
// Validate response timestamp is fresh
let current_time_usecs = time_service.now_unix_time().as_micros();
let max_age_usecs = (network_monitoring_config.network_info_request_interval_ms * 2000) as u64;
if current_time_usecs.saturating_sub(network_info_response.timestamp_usecs) > max_age_usecs {
    // Reject stale response
    warn!("Received stale network info response from peer");
    self.handle_request_failure();
    return;
}
```

## Proof of Concept

The vulnerability can be demonstrated through the following attack sequence:

1. **Setup**: Attacker operates a peer node that was previously well-connected (distance=1) but is now poorly connected (distance=10)

2. **Cache Phase**: When the attacker's node was at distance=1, it receives legitimate queries and generates valid `NetworkInformationResponse` with `distance_from_validators=1`. The attacker caches this response.

3. **Attack Phase**: After network degradation (now at distance=10), when the attacker receives a new `GetNetworkInformation` request with request_id=456, it responds with the cached response data (distance=1) but uses the current request_id=456.

4. **Victim Acceptance**: The victim node's RPC framework accepts the response (request_id matches), and the application layer accepts it (distance=1 passes sanity checks for the peer's claimed role).

5. **Impact Manifestation**: The victim prioritizes the attacker's peer for consensus observer subscriptions, state sync operations, and mempool forwarding, resulting in degraded performance due to the attacker's poor actual connectivity.

The attack succeeds because there is no mechanism to verify that the `distance_from_validators` value reflects current network topology rather than cached historical data.

## Notes

This vulnerability represents a logic flaw in the peer monitoring service's trust model. While the RPC framework provides transport-level integrity (request_id matching prevents wire-level replay attacks), it does not provide application-level freshness guarantees. The peer monitoring service implicitly trusts that peers will honestly report their current network position, without mechanisms to verify temporal validity.

The distinction between `NetworkInformationResponse` (which lacks timestamps) and `NodeInformationResponse` (which includes `ledger_timestamp_usecs` for freshness validation) highlights this design inconsistency. Network topology information should receive similar freshness validation as node synchronization information.

### Citations

**File:** peer-monitoring-service/client/src/peer_states/network_info.rs (L78-80)
```rust
    fn create_monitoring_service_request(&mut self) -> PeerMonitoringServiceRequest {
        PeerMonitoringServiceRequest::GetNetworkInformation
    }
```

**File:** peer-monitoring-service/client/src/peer_states/network_info.rs (L100-154)
```rust
        let network_info_response = match monitoring_service_response {
            PeerMonitoringServiceResponse::NetworkInformation(network_information_response) => {
                network_information_response
            },
            _ => {
                warn!(LogSchema::new(LogEntry::NetworkInfoRequest)
                    .event(LogEvent::ResponseError)
                    .peer(peer_network_id)
                    .message(
                        "An unexpected response was received instead of a network info response!"
                    ));
                self.handle_request_failure();
                return;
            },
        };

        // Sanity check the response depth from the peer metadata
        let network_id = peer_network_id.network_id();
        let is_valid_depth = match network_info_response.distance_from_validators {
            0 => {
                // Verify the peer is a validator and has the correct network id
                let peer_is_validator = peer_metadata.get_connection_metadata().role.is_validator();
                let peer_has_correct_network = match self.base_config.role {
                    RoleType::Validator => network_id.is_validator_network(), // We're a validator
                    RoleType::FullNode => network_id.is_vfn_network(),        // We're a VFN
                };
                peer_is_validator && peer_has_correct_network
            },
            1 => {
                // Verify the peer is a VFN and has the correct network id
                let peer_is_vfn = peer_metadata.get_connection_metadata().role.is_vfn();
                let peer_has_correct_network = match self.base_config.role {
                    RoleType::Validator => network_id.is_vfn_network(), // We're a validator
                    RoleType::FullNode => network_id.is_public_network(), // We're a VFN or PFN
                };
                peer_is_vfn && peer_has_correct_network
            },
            distance_from_validators => {
                // The distance must be less than or equal to the max
                distance_from_validators <= MAX_DISTANCE_FROM_VALIDATORS
            },
        };

        // If the depth did not pass our sanity checks, handle a failure
        if !is_valid_depth {
            warn!(LogSchema::new(LogEntry::NetworkInfoRequest)
                .event(LogEvent::InvalidResponse)
                .peer(peer_network_id)
                .message(&format!(
                    "Peer returned invalid depth from validators: {}",
                    network_info_response.distance_from_validators
                )));
            self.handle_request_failure();
            return;
        }
```

**File:** peer-monitoring-service/types/src/response.rs (L51-55)
```rust
#[derive(Clone, Debug, Deserialize, Eq, PartialEq, Serialize)]
pub struct NetworkInformationResponse {
    pub connected_peers: BTreeMap<PeerNetworkId, ConnectionMetadata>, // Connected peers
    pub distance_from_validators: u64, // The distance of the peer from the validator set
}
```

**File:** network/framework/src/protocols/rpc/mod.rs (L688-731)
```rust
    pub fn handle_inbound_response(&mut self, response: RpcResponse) {
        let network_context = &self.network_context;
        let peer_id = &self.remote_peer_id;
        let request_id = response.request_id;

        let is_canceled = if let Some((protocol_id, response_tx)) =
            self.pending_outbound_rpcs.remove(&request_id)
        {
            self.update_inbound_rpc_response_metrics(
                protocol_id,
                response.raw_response.len() as u64,
            );
            response_tx.send(response).is_err()
        } else {
            true
        };

        if is_canceled {
            trace!(
                NetworkSchema::new(network_context).remote_peer(peer_id),
                request_id = request_id,
                "{} Received response for expired request_id {} from {}. Discarding.",
                network_context,
                request_id,
                peer_id.short_str(),
            );
            counters::rpc_messages(
                network_context,
                RESPONSE_LABEL,
                INBOUND_LABEL,
                EXPIRED_LABEL,
            )
            .inc();
        } else {
            trace!(
                NetworkSchema::new(network_context).remote_peer(peer_id),
                request_id = request_id,
                "{} Notified pending outbound rpc task of inbound response for request_id {} from {}",
                network_context,
                request_id,
                peer_id.short_str(),
            );
        }
    }
```

**File:** consensus/src/consensus_observer/observer/subscription_utils.rs (L283-350)
```rust
pub fn sort_peers_by_subscription_optimality(
    peers_and_metadata: &HashMap<PeerNetworkId, PeerMetadata>,
) -> Vec<PeerNetworkId> {
    // Group peers and latencies by validator distance, i.e., distance -> [(peer, latency)]
    let mut unsupported_peers = Vec::new();
    let mut peers_and_latencies_by_distance = BTreeMap::new();
    for (peer_network_id, peer_metadata) in peers_and_metadata {
        // Verify that the peer supports consensus observer
        if !supports_consensus_observer(peer_metadata) {
            unsupported_peers.push(*peer_network_id);
            continue; // Skip the peer
        }

        // Get the distance and latency for the peer
        let distance = get_distance_for_peer(peer_network_id, peer_metadata);
        let latency = get_latency_for_peer(peer_network_id, peer_metadata);

        // If the distance is not found, use the maximum distance
        let distance =
            distance.unwrap_or(aptos_peer_monitoring_service_types::MAX_DISTANCE_FROM_VALIDATORS);

        // If the latency is not found, use a large latency
        let latency = latency.unwrap_or(MAX_PING_LATENCY_SECS);

        // Add the peer and latency to the distance group
        peers_and_latencies_by_distance
            .entry(distance)
            .or_insert_with(Vec::new)
            .push((*peer_network_id, OrderedFloat(latency)));
    }

    // If there are peers that don't support consensus observer, log them
    if !unsupported_peers.is_empty() {
        info!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "Found {} peers that don't support consensus observer! Peers: {:?}",
                unsupported_peers.len(),
                unsupported_peers
            ))
        );
    }

    // Sort the peers by distance and latency. Note: BTreeMaps are
    // sorted by key, so the entries will be sorted by distance in ascending order.
    let mut sorted_peers_and_latencies = Vec::new();
    for (_, mut peers_and_latencies) in peers_and_latencies_by_distance {
        // Sort the peers by latency
        peers_and_latencies.sort_by_key(|(_, latency)| *latency);

        // Add the peers to the sorted list (in sorted order)
        sorted_peers_and_latencies.extend(peers_and_latencies);
    }

    // Log the sorted peers and latencies
    info!(
        LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
            "Sorted {} peers by subscription optimality! Peers and latencies: {:?}",
            sorted_peers_and_latencies.len(),
            sorted_peers_and_latencies
        ))
    );

    // Only return the sorted peers (without the latencies)
    sorted_peers_and_latencies
        .into_iter()
        .map(|(peer, _)| peer)
        .collect()
}
```

**File:** state-sync/aptos-data-client/src/utils.rs (L23-64)
```rust
/// Chooses peers weighted by distance from the validator set
/// and latency. We prioritize distance over latency as we want
/// to avoid close but not up-to-date peers.
pub fn choose_random_peers_by_distance_and_latency(
    peers: HashSet<PeerNetworkId>,
    peers_and_metadata: Arc<PeersAndMetadata>,
    num_peers_to_choose: usize,
) -> HashSet<PeerNetworkId> {
    // Group peers and latency weights by validator distance, i.e., distance -> [(peer, latency weight)]
    let mut peers_and_latencies_by_distance = BTreeMap::new();
    for peer in peers {
        if let Some((distance, latency)) =
            get_distance_and_latency_for_peer(&peers_and_metadata, peer)
        {
            let latency_weight = convert_latency_to_weight(latency);
            peers_and_latencies_by_distance
                .entry(distance)
                .or_insert_with(Vec::new)
                .push((peer, latency_weight));
        }
    }

    // Select the peers by distance and latency weights. Note: BTreeMaps are
    // sorted by key, so the entries will be sorted by distance in ascending order.
    let mut selected_peers = HashSet::new();
    for (_, peers_and_latencies) in peers_and_latencies_by_distance {
        // Select the peers by latency weights
        let num_peers_remaining = num_peers_to_choose.saturating_sub(selected_peers.len()) as u64;
        let peers = choose_random_peers_by_weight(num_peers_remaining, peers_and_latencies);

        // Add the peers to the entire set
        selected_peers.extend(peers);

        // If we have selected enough peers, return early
        if selected_peers.len() >= num_peers_to_choose {
            return selected_peers;
        }
    }

    // Return the selected peers
    selected_peers
}
```

**File:** mempool/src/shared_mempool/priority.rs (L507-516)
```rust
fn get_distance_from_validators(
    monitoring_metadata: &Option<&PeerMonitoringMetadata>,
) -> Option<u64> {
    monitoring_metadata.and_then(|metadata| {
        metadata
            .latest_network_info_response
            .as_ref()
            .map(|network_info_response| network_info_response.distance_from_validators)
    })
}
```

**File:** mempool/src/shared_mempool/priority.rs (L615-639)
```rust
fn compare_validator_distance(
    monitoring_metadata_a: &Option<&PeerMonitoringMetadata>,
    monitoring_metadata_b: &Option<&PeerMonitoringMetadata>,
) -> Ordering {
    // Get the validator distance from the monitoring metadata
    let validator_distance_a = get_distance_from_validators(monitoring_metadata_a);
    let validator_distance_b = get_distance_from_validators(monitoring_metadata_b);

    // Compare the distances
    match (validator_distance_a, validator_distance_b) {
        (Some(validator_distance_a), Some(validator_distance_b)) => {
            // Prioritize the peer with the lowest validator distance
            validator_distance_a.cmp(&validator_distance_b).reverse()
        },
        (Some(_), None) => {
            Ordering::Greater // Prioritize the peer with a validator distance
        },
        (None, Some(_)) => {
            Ordering::Less // Prioritize the peer with a validator distance
        },
        (None, None) => {
            Ordering::Equal // Neither peer has a validator distance
        },
    }
}
```

**File:** config/src/config/peer_monitoring_config.rs (L65-72)
```rust
impl Default for NetworkMonitoringConfig {
    fn default() -> Self {
        Self {
            network_info_request_interval_ms: 60_000, // 1 minute
            network_info_request_timeout_ms: 10_000,  // 10 seconds
        }
    }
}
```
