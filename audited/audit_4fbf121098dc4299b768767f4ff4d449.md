# Audit Report

## Title
VGZZ Multi-Point Evaluation Algorithm Panics on Partially Full Batches Due to Undocumented Constraint

## Summary
The VGZZ multi-point evaluation algorithm (`compute_all_vgzz_multi_point_eval`) does NOT produce equivalent behavior to the standard `compute_all()` method. When processing batches that are not completely full (fewer IDs than batch capacity), the VGZZ method triggers a panic due to a constraint violation in the underlying `multi_point_eval()` function, while the standard method handles such cases correctly. This creates a denial-of-service vulnerability if the VGZZ method is used in production code. [1](#0-0) 

## Finding Description
The security question asks whether the two methods produce equivalent proofs. The answer is: **they are not equivalent in their operational constraints**.

Both methods compute KZG evaluation proofs through different algorithmic approaches:

**Standard Method (`compute_all`)**: Uses the naive multi-point evaluation algorithm via `eval_proofs_at_x_coords_naive_multi_point_eval()`. [2](#0-1) 

**VGZZ Method (`compute_all_vgzz_multi_point_eval`)**: Uses the von zur Gathen and Gerhardt algorithm via `eval_proofs_at_x_coords()`. [1](#0-0) 

The critical difference lies in the underlying `multi_point_eval()` function used by the VGZZ method, which contains an assertion: [3](#0-2) 

This assertion requires `x_coords.len() >= f.len()`, meaning the number of evaluation points must be at least as large as the polynomial representation length.

In contrast, the naive algorithm explicitly supports arbitrary numbers of coordinates: [4](#0-3) 

**The Problem**: When computing evaluation proofs:

1. The `h_term_commitments` are computed and padded to `toeplitz_domain.dimension()`, which equals the batch capacity (e.g., 8). [5](#0-4) 

2. The x-coordinates (`poly_roots`) represent only the actual IDs in the batch (e.g., 3 IDs in a partially full batch). [6](#0-5) 

3. When `h_term_commitments.len()` (8) > `poly_roots.len()` (3), the VGZZ method calls `multi_point_eval()` with this constraint violation, triggering the assertion panic.

**Exploitation Scenario**:
- Setup: Batch encryption with capacity 8
- Attacker action: Submit only 3 ciphertexts to create a partially full batch
- If code calls `compute_all_vgzz_multi_point_eval()` instead of `compute_all()`: Node panics
- Result: Denial of service

The existing test suite validates the standard method with partially full batches: [7](#0-6) 

However, no equivalent test exists for the VGZZ method, suggesting this constraint was not validated.

## Impact Explanation
**Severity: Medium** (potentially High if used in production)

This issue constitutes a **denial-of-service vulnerability** through unhandled panic:

- **Current Impact (Medium)**: The trait documentation explicitly marks this method as "Currently for benchmarking only, not for production use," and production code uses the standard method instead. [8](#0-7) 

- **Potential Impact (High)**: The method is exposed as a public API in the `BatchThresholdEncryption` trait and could be:
  - Mistakenly used in future code changes
  - Called by external users of the library
  - Selected dynamically based on performance metrics

If the VGZZ method were used in consensus pipelines (such as the decryption pipeline), partially full batches would crash validator nodes, disrupting network availability. This maps to "State inconsistencies requiring intervention" (Medium) or "API crashes" (High) in the Aptos bug bounty categories.

## Likelihood Explanation
**Current Likelihood: Low**

The method is not currently used in production code based on codebase analysis. However, several factors increase risk:

1. **Undocumented Constraint**: The full-batch requirement is not documented at the API level, only visible in internal implementation comments
2. **Public API Exposure**: Despite being "for benchmarking only," the method is part of the public trait interface implemented by all three schemes (FPTX, FPTXSuccinct, FPTXWeighted)
3. **Performance Optimization Temptation**: Developers might switch to the VGZZ method for performance reasons without understanding the constraint
4. **No Validation**: The DigestKey and IdSet APIs don't prevent partially full batchesâ€”this is a valid use case for the standard method

## Recommendation
**Option 1: Document and Enforce Constraint (Recommended)**

Add explicit validation in `compute_all_vgzz_multi_point_eval()` to check the constraint before calling the underlying implementation:

```rust
pub fn compute_all_vgzz_multi_point_eval(&self, digest_key: &DigestKey) -> EvalProofs {
    // VGZZ algorithm requires batch to be at least as full as the polynomial degree
    let h_term_len = digest_key.fk_domain.toeplitz_domain.dimension();
    if self.ids.poly_roots.len() < h_term_len {
        panic!(
            "VGZZ multi-point eval requires at least {} IDs, but only {} provided. Use compute_all() instead.",
            h_term_len,
            self.ids.poly_roots.len()
        );
    }
    
    EvalProofs {
        computed_proofs: self
            .ids
            .compute_all_eval_proofs_with_setup_vzgg_multi_point_eval(
                digest_key,
                self.digest.round,
            ),
    }
}
```

**Option 2: Fix the Algorithm**

Modify `multi_point_eval()` to handle cases where `x_coords.len() < f.len()` by padding or returning only the requested evaluations.

**Option 3: Remove from Public API**

Mark the method as deprecated or move it to a testing/benchmarking-only module to prevent accidental production use.

## Proof of Concept

```rust
#[test]
#[should_panic(expected = "assertion failed")]
fn test_vgzz_panics_on_partial_batch() {
    use crate::shared::ids::{Id, IdSet};
    use ark_std::rand::thread_rng;
    use crate::group::Fr;
    use ark_std::{One, Zero};

    let batch_capacity = 8;
    let num_rounds = 1;
    let mut rng = thread_rng();
    let setup = DigestKey::new(&mut rng, batch_capacity, num_rounds).unwrap();

    // Create partially full batch with only 3 IDs (capacity is 8)
    let mut ids = IdSet::with_capacity(batch_capacity).unwrap();
    let mut counter = Fr::zero();
    
    for _ in 0..3 {  // Only 3 IDs, not 8
        ids.add(&Id::new(counter));
        counter += Fr::one();
    }

    let (d, pfs_promise) = setup.digest(&mut ids, 0).unwrap();
    
    // Standard method works fine
    let pfs_standard = pfs_promise.compute_all(&setup);
    setup.verify_all(&d, &pfs_standard).unwrap();  // Success
    
    // VGZZ method panics
    let pfs_vgzz = pfs_promise.compute_all_vgzz_multi_point_eval(&setup);  // PANIC!
}
```

## Notes
- The mathematical equivalence of the two algorithms when constraints are satisfied is confirmed by existing tests
- The issue is purely operational: different preconditions for valid execution
- Production code correctly uses the standard `compute_all()` method, but the public API exposes a footgun
- The VGZZ algorithm's constraint is not explicitly documented in the trait or method signature, increasing risk of misuse

### Citations

**File:** crates/aptos-batch-encryption/src/shared/digest.rs (L171-177)
```rust
    pub fn compute_all(&self, digest_key: &DigestKey) -> EvalProofs {
        EvalProofs {
            computed_proofs: self
                .ids
                .compute_all_eval_proofs_with_setup(digest_key, self.digest.round),
        }
    }
```

**File:** crates/aptos-batch-encryption/src/shared/digest.rs (L179-188)
```rust
    pub fn compute_all_vgzz_multi_point_eval(&self, digest_key: &DigestKey) -> EvalProofs {
        EvalProofs {
            computed_proofs: self
                .ids
                .compute_all_eval_proofs_with_setup_vzgg_multi_point_eval(
                    digest_key,
                    self.digest.round,
                ),
        }
    }
```

**File:** crates/aptos-batch-encryption/src/shared/digest.rs (L253-277)
```rust
    #[test]
    fn compute_and_verify_all_opening_proofs() {
        let batch_capacity = 8;
        let num_rounds = 4;
        let mut rng = thread_rng();
        let setup = DigestKey::new(&mut rng, batch_capacity, num_rounds * batch_capacity).unwrap();

        for current_batch_size in 1..=batch_capacity {
            let mut ids = IdSet::with_capacity(batch_capacity).unwrap();
            let mut counter = Fr::zero();

            for _ in 0..current_batch_size {
                ids.add(&Id::new(counter));
                counter += Fr::one();
            }

            ids.compute_poly_coeffs();

            for round in 0..num_rounds {
                let (d, pfs_promise) = setup.digest(&mut ids, round as u64).unwrap();
                let pfs = pfs_promise.compute_all(&setup);
                setup.verify_all(&d, &pfs).unwrap();
            }
        }
    }
```

**File:** crates/aptos-batch-encryption/src/shared/algebra/multi_point_eval.rs (L112-121)
```rust
pub fn multi_point_eval<F: FftField, T: DomainCoeff<F> + Mul<F, Output = T>>(
    f: &[T],
    x_coords: &[F],
) -> Vec<T> {
    // The way it is written right now, this only supports
    // evaluating a poly on a number of x coords greater than deg(f) + 1
    assert!(x_coords.len() >= f.len());
    let mult_tree = compute_mult_tree(x_coords);
    recurse(f, &mult_tree, mult_tree.len() - 1, 0)
}
```

**File:** crates/aptos-batch-encryption/src/shared/algebra/multi_point_eval.rs (L123-149)
```rust
pub fn multi_point_eval_naive<
    F: FftField,
    T: DomainCoeff<F> + Mul<F, Output = T> + VariableBaseMSM<ScalarField = F>,
>(
    f: &[T::MulBase],
    x_coords: &[F],
) -> Vec<T> {
    // Note: unlike the non-naive algorithm, this supports an arbitrary
    // number of x coords
    let powers = x_coords
        .into_par_iter()
        .map(|x| {
            let mut result = Vec::new();
            let mut x_power = F::one();
            for _i in 0..f.len() {
                result.push(x_power);
                x_power *= x;
            }
            result
        })
        .collect::<Vec<Vec<F>>>();

    powers
        .into_par_iter()
        .map(|p| T::msm(f, &p).unwrap())
        .collect()
}
```

**File:** crates/aptos-batch-encryption/src/shared/algebra/fk_algorithm.rs (L336-352)
```rust
    fn compute_h_term_commitments(&self, f: &[F], round: usize) -> Vec<T> {
        let mut f = Vec::from(f);
        f.extend(std::iter::repeat_n(
            F::zero(),
            self.toeplitz_domain.dimension() + 1 - f.len(),
        ));
        // f.len() = (degree of f) + 1. Degree of f should be equal to the toeplitz domain
        // dimension.
        debug_assert_eq!(self.toeplitz_domain.dimension(), f.len() - 1);

        self.toeplitz_domain.eval_prepared(
            &self.toeplitz_for_poly(&f),
            // The Toeplitz matrix is only evaluated on the powers up to max_poly_degree - 1,
            // since the H_j(X) polynomials have degree at most that
            &self.prepared_toeplitz_inputs[round],
        )
    }
```

**File:** crates/aptos-batch-encryption/src/shared/ids/mod.rs (L148-166)
```rust
    pub fn compute_all_eval_proofs_with_setup_vzgg_multi_point_eval(
        &self,
        setup: &crate::shared::digest::DigestKey,
        round: usize,
    ) -> HashMap<Id, G1Affine> {
        let pfs: Vec<G1Affine> = setup
            .fk_domain
            .eval_proofs_at_x_coords(&self.poly_coeffs(), &self.poly_roots, round)
            .iter()
            .map(|g| G1Affine::from(*g))
            .collect();

        HashMap::from_iter(
            self.as_vec()
                .into_iter()
                .zip(pfs)
                .collect::<Vec<(Id, G1Affine)>>(),
        )
    }
```

**File:** crates/aptos-batch-encryption/src/traits.rs (L121-127)
```rust
    /// Compute KZG eval proofs. This will be the most expensive operation in the scheme. This
    /// version uses a different (slower for our parameter regime) multi-point-eval algorithm,
    /// from von zur Gathen and Gerhardt. Currently for benchmarking only, not for production use.
    fn eval_proofs_compute_all_vzgg_multi_point_eval(
        proofs: &Self::EvalProofsPromise,
        digest_key: &Self::DigestKey,
    ) -> Self::EvalProofs;
```
