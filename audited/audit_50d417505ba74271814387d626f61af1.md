# Audit Report

## Title
Consensus Tasks Forcibly Terminated on Runtime Drop Without Graceful Shutdown - Potential SafetyRules Storage Corruption

## Summary
The `start_consensus()` function spawns long-running consensus tasks (EpochManager and NetworkTask) on a Tokio Runtime and returns this Runtime to the caller. When the Runtime is dropped, all spawned tasks are **forcibly terminated** without graceful shutdown, potentially interrupting critical SafetyRules storage writes mid-operation. Combined with the lack of fsync in `OnDiskStorage`, this can corrupt SafetyRules persistent storage, potentially enabling double-voting violations.

## Finding Description

The vulnerability stems from three interconnected design issues:

**1. Runtime Drop Behavior** [1](#0-0) 

The `start_consensus()` function spawns consensus tasks using `runtime.spawn()` on lines 119-120, then returns the Runtime. When this Runtime is dropped, Tokio forcibly terminates all spawned tasks **without waiting for graceful completion**.

**2. No Graceful Shutdown on AptosHandle Drop** [2](#0-1) 

The `AptosHandle` struct stores the consensus Runtime but has **no Drop implementation**. When the node shuts down and AptosHandle is dropped, the Runtime is simply dropped without invoking the graceful shutdown mechanisms.

**3. EpochManager Has Graceful Shutdown Logic But It's Never Called** [3](#0-2) 

The `shutdown_current_processor()` method implements proper graceful shutdown using oneshot channels for acknowledgment, but this is only called during epoch transitions, **not when the entire Runtime is dropped**.

**4. SafetyRules Storage Writes Lack Fsync** [4](#0-3) 

The `OnDiskStorage::write()` method writes data and renames the file atomically but **never calls fsync**. If the process is killed after rename but before the OS flushes dirty pages, the write can be lost on system crash.

**5. Multi-Step Vote Operation is Not Atomic** [5](#0-4) 

The `vote_block()` function performs three non-atomic steps:
1. Insert block into ConsensusDB (line 1501-1505)
2. Create vote and update SafetyRules storage (line 1520-1527)
3. Save vote to ConsensusDB (line 1539-1541)

If the Runtime is dropped after step 2 but before step 3, SafetyRules storage has the vote recorded but ConsensusDB doesn't.

**Attack Scenario:**

1. Node is running consensus normally
2. A programming error, panic, or forced shutdown causes AptosHandle to drop
3. The consensus Runtime is dropped, forcibly terminating EpochManager mid-execution
4. EpochManager was processing a vote, with SafetyRules storage updated but ConsensusDB not yet updated
5. The OnDiskStorage write to SafetyRules may not be flushed to disk (no fsync)
6. System experiences power loss or crash before OS flushes pages
7. On restart, SafetyRules storage may be corrupted or have an older `last_vote` value
8. Node may vote again for a round it already voted for, violating consensus safety [6](#0-5) 

SafetyRules prevents double-voting by checking `last_vote` (lines 70-74), but if this storage is corrupted, the check fails.

## Impact Explanation

This vulnerability has **Critical** severity potential under the Aptos bug bounty program for the following reasons:

1. **Consensus Safety Violation**: If SafetyRules storage is corrupted and `last_vote` is lost, a validator could double-vote, violating the fundamental BFT consensus safety invariant that prevents equivocation.

2. **Non-recoverable State Corruption**: If multiple validators experience this issue simultaneously (e.g., during a power outage affecting a data center), the network could experience irrecoverable state corruption requiring manual intervention or hard fork.

3. **Undermines SafetyRules Guarantees**: SafetyRules is the critical component that enforces consensus safety. Any corruption of its persistent storage undermines the entire consensus protocol's safety guarantees.

However, the **likelihood is reduced** because:
- Requires node crash or abnormal termination (not typical operation)
- OnDiskStorage README warns against production use (though config examples show it being used)
- Atomic rename provides some protection (but not durability without fsync)

## Likelihood Explanation

**Moderate to High** likelihood in practice:

1. **Node crashes do occur**: Power failures, kernel panics, OOM killers, and programming bugs can all cause abnormal termination
2. **OnDiskStorage is used in examples**: Configuration files show `on_disk_storage` being used, suggesting it's deployed in at least some environments
3. **No explicit graceful shutdown**: There's no code ensuring graceful shutdown before Runtime drop
4. **Filesystem guarantees vary**: Not all filesystems provide durability guarantees for rename without fsync

The vulnerability is **more likely** in:
- Development/testnet environments using OnDiskStorage
- Infrastructure with unreliable power
- Nodes experiencing frequent restarts
- Deployments without proper shutdown procedures

## Recommendation

Implement three layers of defense:

**1. Add Graceful Shutdown to AptosHandle**

```rust
impl Drop for AptosHandle {
    fn drop(&mut self) {
        // Signal graceful shutdown to consensus
        // Wait with timeout for tasks to complete
        if let Some(consensus_runtime) = self._consensus_runtime.take() {
            consensus_runtime.shutdown_timeout(Duration::from_secs(10));
        }
    }
}
```

**2. Add Fsync to OnDiskStorage**

```rust
fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
    let contents = serde_json::to_vec(data)?;
    let mut file = File::create(self.temp_path.path())?;
    file.write_all(&contents)?;
    file.sync_all()?;  // ADD THIS LINE
    fs::rename(&self.temp_path, &self.file_path)?;
    // Sync parent directory to ensure rename is durable
    if let Some(parent) = self.file_path.parent() {
        File::open(parent)?.sync_all()?;
    }
    Ok(())
}
```

**3. Expose Graceful Shutdown API**

Modify `start_consensus()` to return a shutdown handle along with the Runtime, allowing callers to trigger graceful shutdown before dropping the Runtime.

## Proof of Concept

```rust
// Reproduction steps demonstrating the vulnerability:

#[tokio::test]
async fn test_runtime_drop_corrupts_consensus_state() {
    // 1. Start consensus
    let (runtime, storage, _qsdb) = start_consensus(
        &node_config,
        network_client,
        network_service_events,
        state_sync_notifier,
        consensus_to_mempool_sender,
        aptos_db,
        reconfig_events,
        vtxn_pool,
        None,
    );
    
    // 2. Simulate consensus processing a vote
    // This would normally update SafetyRules storage
    
    // 3. Drop the runtime forcibly (simulating crash)
    drop(runtime);
    
    // 4. Check SafetyRules storage - it may be corrupted
    // or missing the last_vote if fsync didn't complete
    
    // 5. Restart consensus - node may be able to double-vote
    // because SafetyRules lost the last_vote record
    
    // Expected: Node should detect corruption and refuse to start
    // Actual: Node may start with inconsistent state
}
```

**Note**: A full PoC would require setting up a complete consensus environment and simulating process termination at precise timing, which is complex but demonstrates the theoretical attack path.

---

## Notes

Upon final validation against the strict criteria, this issue **may not qualify** as a traditional security vulnerability because:

1. It requires **node crash or abnormal termination**, which is not attacker-controllable in typical scenarios
2. OnDiskStorage README explicitly states it's "not for production use"
3. The issue is more about **robustness/reliability** than exploitable security

However, it represents a **critical design flaw** that could lead to consensus safety violations under specific failure conditions, particularly if OnDiskStorage is used in production despite warnings.

### Citations

**File:** consensus/src/consensus_provider.rs (L56-123)
```rust
    let runtime = aptos_runtimes::spawn_named_runtime("consensus".into(), None);
    let storage = Arc::new(StorageWriteProxy::new(node_config, aptos_db.reader.clone()));
    let quorum_store_db = Arc::new(QuorumStoreDB::new(node_config.storage.dir()));

    let txn_notifier = Arc::new(MempoolNotifier::new(
        consensus_to_mempool_sender.clone(),
        node_config.consensus.mempool_executed_txn_timeout_ms,
    ));

    let execution_proxy = ExecutionProxy::new(
        Arc::new(BlockExecutor::<AptosVMBlockExecutor>::new(aptos_db)),
        txn_notifier,
        state_sync_notifier,
        node_config.transaction_filters.execution_filter.clone(),
        node_config.consensus.enable_pre_commit,
        None,
    );

    let time_service = Arc::new(ClockTimeService::new(runtime.handle().clone()));

    let (timeout_sender, timeout_receiver) =
        aptos_channels::new(1_024, &counters::PENDING_ROUND_TIMEOUTS);
    let (self_sender, self_receiver) =
        aptos_channels::new_unbounded(&counters::PENDING_SELF_MESSAGES);
    let consensus_network_client = ConsensusNetworkClient::new(network_client);
    let bounded_executor = BoundedExecutor::new(
        node_config.consensus.num_bounded_executor_tasks as usize,
        runtime.handle().clone(),
    );
    let rand_storage = Arc::new(RandDb::new(node_config.storage.dir()));

    let execution_client = Arc::new(ExecutionProxyClient::new(
        node_config.consensus.clone(),
        Arc::new(execution_proxy),
        node_config.validator_network.as_ref().unwrap().peer_id(),
        self_sender.clone(),
        consensus_network_client.clone(),
        bounded_executor.clone(),
        rand_storage.clone(),
        node_config.consensus_observer,
        consensus_publisher.clone(),
    ));

    let epoch_mgr = EpochManager::new(
        node_config,
        time_service,
        self_sender,
        consensus_network_client,
        timeout_sender,
        consensus_to_mempool_sender,
        execution_client,
        storage.clone(),
        quorum_store_db.clone(),
        reconfig_events,
        bounded_executor,
        aptos_time_service::TimeService::real(),
        vtxn_pool,
        rand_storage,
        consensus_publisher,
    );

    let (network_task, network_receiver) = NetworkTask::new(network_service_events, self_receiver);

    runtime.spawn(network_task.start());
    runtime.spawn(epoch_mgr.start(timeout_receiver, network_receiver));

    debug!("Consensus started.");
    (runtime, storage, quorum_store_db)
```

**File:** aptos-node/src/lib.rs (L196-215)
```rust
/// Runtime handle to ensure that all inner runtimes stay in scope
pub struct AptosHandle {
    _admin_service: AdminService,
    _api_runtime: Option<Runtime>,
    _backup_runtime: Option<Runtime>,
    _consensus_observer_runtime: Option<Runtime>,
    _consensus_publisher_runtime: Option<Runtime>,
    _consensus_runtime: Option<Runtime>,
    _dkg_runtime: Option<Runtime>,
    _indexer_grpc_runtime: Option<Runtime>,
    _indexer_runtime: Option<Runtime>,
    _indexer_table_info_runtime: Option<Runtime>,
    _jwk_consensus_runtime: Option<Runtime>,
    _mempool_runtime: Runtime,
    _network_runtimes: Vec<Runtime>,
    _peer_monitoring_service_runtime: Runtime,
    _state_sync_runtimes: StateSyncRuntimes,
    _telemetry_runtime: Option<Runtime>,
    _indexer_db_runtime: Option<Runtime>,
}
```

**File:** consensus/src/epoch_manager.rs (L637-683)
```rust
    async fn shutdown_current_processor(&mut self) {
        if let Some(close_tx) = self.round_manager_close_tx.take() {
            // Release the previous RoundManager, especially the SafetyRule client
            let (ack_tx, ack_rx) = oneshot::channel();
            close_tx
                .send(ack_tx)
                .expect("[EpochManager] Fail to drop round manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop round manager");
        }
        self.round_manager_tx = None;

        if let Some(close_tx) = self.dag_shutdown_tx.take() {
            // Release the previous RoundManager, especially the SafetyRule client
            let (ack_tx, ack_rx) = oneshot::channel();
            close_tx
                .send(ack_tx)
                .expect("[EpochManager] Fail to drop DAG bootstrapper");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop DAG bootstrapper");
        }
        self.dag_shutdown_tx = None;

        // Shutdown the previous rand manager
        self.rand_manager_msg_tx = None;

        // Shutdown the previous secret share manager
        self.secret_share_manager_tx = None;

        // Shutdown the previous buffer manager, to release the SafetyRule client
        self.execution_client.end_epoch().await;

        // Shutdown the block retrieval task by dropping the sender
        self.block_retrieval_tx = None;
        self.batch_retrieval_tx = None;

        if let Some(mut quorum_store_coordinator_tx) = self.quorum_store_coordinator_tx.take() {
            let (ack_tx, ack_rx) = oneshot::channel();
            quorum_store_coordinator_tx
                .send(CoordinatorCommand::Shutdown(ack_tx))
                .await
                .expect("Could not send shutdown indicator to QuorumStore");
            ack_rx.await.expect("Failed to stop QuorumStore");
        }
    }
```

**File:** secure/storage/src/on_disk.rs (L64-70)
```rust
    fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
        let contents = serde_json::to_vec(data)?;
        let mut file = File::create(self.temp_path.path())?;
        file.write_all(&contents)?;
        fs::rename(&self.temp_path, &self.file_path)?;
        Ok(())
    }
```

**File:** consensus/src/round_manager.rs (L1500-1544)
```rust
    async fn vote_block(&mut self, proposed_block: Block) -> anyhow::Result<Vote> {
        let block_arc = self
            .block_store
            .insert_block(proposed_block)
            .await
            .context("[RoundManager] Failed to execute_and_insert the block")?;

        // Short circuit if already voted.
        ensure!(
            self.round_state.vote_sent().is_none(),
            "[RoundManager] Already vote on this round {}",
            self.round_state.current_round()
        );

        ensure!(
            !self.sync_only(),
            "[RoundManager] sync_only flag is set, stop voting"
        );

        let vote_proposal = block_arc.vote_proposal();
        let vote_result = self.safety_rules.lock().construct_and_sign_vote_two_chain(
            &vote_proposal,
            self.block_store.highest_2chain_timeout_cert().as_deref(),
        );
        let vote = vote_result.context(format!(
            "[RoundManager] SafetyRules Rejected {}",
            block_arc.block()
        ))?;
        if !block_arc.block().is_nil_block() {
            observe_block(block_arc.block().timestamp_usecs(), BlockStage::VOTED);
        }

        if block_arc.block().is_opt_block() {
            observe_block(
                block_arc.block().timestamp_usecs(),
                BlockStage::VOTED_OPT_BLOCK,
            );
        }

        self.storage
            .save_vote(&vote)
            .context("[RoundManager] Fail to persist last vote")?;

        Ok(vote)
    }
```

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L68-94)
```rust
        // if already voted on this round, send back the previous vote
        // note: this needs to happen after verifying the epoch as we just check the round here
        if let Some(vote) = safety_data.last_vote.clone() {
            if vote.vote_data().proposed().round() == proposed_block.round() {
                return Ok(vote);
            }
        }

        // Two voting rules
        self.verify_and_update_last_vote_round(
            proposed_block.block_data().round(),
            &mut safety_data,
        )?;
        self.safe_to_vote(proposed_block, timeout_cert)?;

        // Record 1-chain data
        self.observe_qc(proposed_block.quorum_cert(), &mut safety_data);
        // Construct and sign vote
        let author = self.signer()?.author();
        let ledger_info = self.construct_ledger_info_2chain(proposed_block, vote_data.hash())?;
        let signature = self.sign(&ledger_info)?;
        let vote = Vote::new_with_signature(vote_data, author, ledger_info, signature);

        safety_data.last_vote = Some(vote.clone());
        self.persistent_storage.set_safety_data(safety_data)?;

        Ok(vote)
```
