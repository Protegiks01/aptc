# Audit Report

## Title
Auto-Bootstrapping Race Condition Allows Consensus to Start with Incomplete State Synchronization

## Summary
The `enable_auto_bootstrapping` feature contains a critical race condition where the connection deadline can expire while state synchronization is in progress, causing the node to prematurely mark itself as bootstrapped and start consensus with incomplete or inconsistent state. This occurs because `check_auto_bootstrapping()` does not verify that all pending storage data has been committed before calling `bootstrapping_complete()`.

## Finding Description

The vulnerability exists in the coordination between the auto-bootstrapping timer and the storage synchronizer's pending data mechanism. The security guarantee violated is **State Consistency**: state transitions must be atomic and complete before consensus begins operation.

When the connection deadline expires and no peers are available, the driver's `drive_progress()` method returns immediately after calling `check_auto_bootstrapping()`, bypassing the normal bootstrapper progress flow. [1](#0-0) 

The `check_auto_bootstrapping()` function checks only basic conditions (not bootstrapped, consensus enabled, auto-bootstrap enabled, genesis waypoint, deadline passed) but performs NO verification of active data streams or pending storage data chunks. [2](#0-1) 

When the deadline passes, `bootstrapping_complete()` is called, which triggers `notify_listeners_if_bootstrapped()`. This function immediately calls `finish_chunk_executor()` without checking `pending_storage_data()`. [3](#0-2) 

In stark contrast, the consensus sync request handling implements the correct safety pattern by explicitly waiting for all pending data to drain before calling `finish_chunk_executor()`. [4](#0-3) 

The `pending_storage_data()` method checks if the `pending_data_chunks` atomic counter is greater than zero, indicating data chunks are still being executed, applied, or committed. [5](#0-4) 

When `finish_chunk_executor()` is called, it sets the chunk executor's inner state to `None`, releasing all in-memory resources including the commit queue. [6](#0-5) 

The critical race occurs because when `global_data_summary.is_empty()` (no peers), the code never calls `bootstrapper.drive_progress()`, which contains the pending data check at line 427. Instead, it returns immediately after checking auto-bootstrapping. [7](#0-6) 

Once bootstrapping is marked complete, `block_until_initialized()` unblocks, allowing consensus runtime creation to proceed. [8](#0-7) 

**Attack Scenario:**

1. Validator starts with `enable_auto_bootstrapping = true`, `max_connection_deadline_secs = 10`
2. Initially no peers: `global_data_summary.is_empty() = true`
3. At T+5s, peer appears and bootstrapper begins syncing epoch ending ledger infos
4. Storage synchronizer creates pending data chunks (`pending_data_chunks > 0`)
5. At T+9.5s, peer disconnects
6. At T+10s+, next `drive_progress()` interval:
   - `global_data_summary.is_empty() = true`
   - Returns at line 677 after calling `check_auto_bootstrapping()`
   - Bypasses `bootstrapper.drive_progress()` and its pending data check
   - Deadline passed, `bootstrapping_complete()` invoked
   - `finish_chunk_executor()` called while `pending_storage_data() = true`
7. Pending chunks orphaned, resources released prematurely
8. Consensus starts with potentially incomplete state

## Impact Explanation

This vulnerability meets **High Severity** criteria per Aptos bug bounty program:

**Validator Node Issues with State Inconsistencies**: Pending storage data chunks are abandoned when `finish_chunk_executor()` is called prematurely. The commit queue and in-memory state are released while background threads may still be processing chunks, leading to:

- Incomplete epoch states if epoch ending ledger infos were mid-processing
- Potentially unverified waypoint if verification was in progress
- Missing validator set information
- State root inconsistencies between validators
- Consensus liveness failures requiring manual intervention

**Consensus Safety Risk**: If multiple validators auto-bootstrap with different incomplete states during network instability, they may disagree on epoch boundaries or validator sets, potentially causing consensus failures.

The impact is **High** rather than Critical because:
- Requires specific timing conditions (peer availability during deadline)
- Most severe in test/development with short deadlines
- Production deployments use longer deadlines and have stable connectivity
- Does not directly cause fund loss
- Recovery possible through node restart

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability is likely in these scenarios:

1. **Test Deployments**: Single-node testnets commonly use `enable_auto_bootstrapping = true` with short deadlines (1-10 seconds). Network instability in test environments makes the race highly probable.

2. **Initial Network Bootstrap**: New networks with genesis waypoints where validators have intermittent peer connectivity during launch.

3. **Network Partitions**: During network instability, peers connecting/disconnecting within deadline windows.

Attack requirements:
- `enable_auto_bootstrapping = true` (common in test environments)
- Genesis waypoint (version 0) - standard for new nodes
- Peer connectivity timing manipulation (feasible via network control)

No validator private keys or special privileges required.

## Recommendation

Add a pending data check in `check_auto_bootstrapping()` before calling `bootstrapping_complete()`:

```rust
async fn check_auto_bootstrapping(&mut self) {
    if !self.bootstrapper.is_bootstrapped()
        && self.is_consensus_or_observer_enabled()
        && self.driver_configuration.config.enable_auto_bootstrapping
        && self.driver_configuration.waypoint.version() == 0
    {
        if let Some(start_time) = self.start_time {
            if let Some(connection_deadline) = start_time.checked_add(Duration::from_secs(
                self.driver_configuration.config.max_connection_deadline_secs,
            )) {
                if self.time_service.now() >= connection_deadline {
                    // NEW: Wait for pending data before completing bootstrap
                    while self.storage_synchronizer.pending_storage_data() {
                        sample!(
                            SampleRate::Duration(Duration::from_secs(PENDING_DATA_LOG_FREQ_SECS)),
                            info!("Auto-bootstrapping: waiting for pending storage data!")
                        );
                        yield_now().await;
                    }
                    
                    info!(LogSchema::new(LogEntry::AutoBootstrapping).message(
                        "Passed the connection deadline! Auto-bootstrapping the validator!"
                    ));
                    if let Err(error) = self.bootstrapper.bootstrapping_complete().await {
                        warn!(LogSchema::new(LogEntry::AutoBootstrapping)
                            .error(&error)
                            .message("Failed to mark bootstrapping as complete!"));
                    }
                }
            }
        }
    }
}
```

This ensures auto-bootstrapping follows the same safety pattern as consensus sync request handling.

## Proof of Concept

The vulnerability can be demonstrated through a timing-based integration test that:
1. Starts a node with `enable_auto_bootstrapping = true` and short deadline
2. Injects a peer that provides partial epoch ending ledger info data
3. Removes the peer mid-sync to trigger pending data chunks
4. Advances time past deadline
5. Verifies consensus starts before pending data is committed

The race condition is verifiable through code inspection without a complete PoC, as the missing safety check is clearly absent in the auto-bootstrapping path compared to the consensus sync path.

### Citations

**File:** state-sync/state-sync-driver/src/driver.rs (L556-605)
```rust
        while self.storage_synchronizer.pending_storage_data() {
            sample!(
                SampleRate::Duration(Duration::from_secs(PENDING_DATA_LOG_FREQ_SECS)),
                info!("Waiting for the storage synchronizer to handle pending data!")
            );

            // Yield to avoid starving the storage synchronizer threads.
            yield_now().await;
        }

        // If the request was to sync for a specified duration, we should only
        // stop syncing when the synced version and synced ledger info version match.
        // Otherwise, the DB will be left in an inconsistent state on handover.
        if let Some(sync_request) = consensus_sync_request.lock().as_ref() {
            if sync_request.is_sync_duration_request() {
                // Get the latest synced version and ledger info version
                let latest_synced_version =
                    utils::fetch_pre_committed_version(self.storage.clone())?;
                let latest_synced_ledger_info =
                    utils::fetch_latest_synced_ledger_info(self.storage.clone())?;
                let latest_ledger_info_version = latest_synced_ledger_info.ledger_info().version();

                // Check if the latest synced version matches the latest ledger info version
                if latest_synced_version != latest_ledger_info_version {
                    sample!(
                        SampleRate::Duration(Duration::from_secs(DRIVER_INFO_LOG_FREQ_SECS)),
                        info!(
                            "Waiting for state sync to sync to a ledger info! \
                            Latest synced version: {:?}, latest ledger info version: {:?}",
                            latest_synced_version, latest_ledger_info_version
                        )
                    );

                    return Ok(()); // State sync should continue to run
                }
            }
        }

        // Handle the satisfied sync request
        let latest_synced_ledger_info =
            utils::fetch_latest_synced_ledger_info(self.storage.clone())?;
        self.consensus_notification_handler
            .handle_satisfied_sync_request(latest_synced_ledger_info)
            .await?;

        // If the sync request was successfully handled, reset the continuous syncer
        // so that in the event another sync request occurs, we have fresh state.
        if !self.active_sync_request() {
            self.continuous_syncer.reset_active_stream(None).await?;
            self.storage_synchronizer.finish_chunk_executor(); // Consensus or consensus observer is now in control
```

**File:** state-sync/state-sync-driver/src/driver.rs (L636-657)
```rust
    async fn check_auto_bootstrapping(&mut self) {
        if !self.bootstrapper.is_bootstrapped()
            && self.is_consensus_or_observer_enabled()
            && self.driver_configuration.config.enable_auto_bootstrapping
            && self.driver_configuration.waypoint.version() == 0
        {
            if let Some(start_time) = self.start_time {
                if let Some(connection_deadline) = start_time.checked_add(Duration::from_secs(
                    self.driver_configuration
                        .config
                        .max_connection_deadline_secs,
                )) {
                    if self.time_service.now() >= connection_deadline {
                        info!(LogSchema::new(LogEntry::AutoBootstrapping).message(
                            "Passed the connection deadline! Auto-bootstrapping the validator!"
                        ));
                        if let Err(error) = self.bootstrapper.bootstrapping_complete().await {
                            warn!(LogSchema::new(LogEntry::AutoBootstrapping)
                                .error(&error)
                                .message("Failed to mark bootstrapping as complete!"));
                        }
                    }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L673-677)
```rust
        if global_data_summary.is_empty() {
            trace!(LogSchema::new(LogEntry::Driver).message(
                "The global data summary is empty! It's likely that we have no active peers."
            ));
            return self.check_auto_bootstrapping().await;
```

**File:** state-sync/state-sync-driver/src/driver.rs (L711-719)
```rust
        } else if let Err(error) = self.bootstrapper.drive_progress(&global_data_summary).await {
            sample!(
                    SampleRate::Duration(Duration::from_secs(DRIVER_ERROR_LOG_FREQ_SECS)),
                    warn!(LogSchema::new(LogEntry::Driver)
                        .error(&error)
                        .message("Error found when checking the bootstrapper progress!"));
            );
            metrics::increment_counter(&metrics::BOOTSTRAPPER_ERRORS, error.get_label());
        };
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L396-411)
```rust
    async fn notify_listeners_if_bootstrapped(&mut self) -> Result<(), Error> {
        if self.is_bootstrapped() {
            if let Some(notifier_channel) = self.bootstrap_notifier_channel.take() {
                if let Err(error) = notifier_channel.send(Ok(())) {
                    return Err(Error::CallbackSendFailed(format!(
                        "Bootstrap notification error: {:?}",
                        error
                    )));
                }
            }
            self.reset_active_stream(None).await?;
            self.storage_synchronizer.finish_chunk_executor(); // The bootstrapper is now complete
        }

        Ok(())
    }
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L408-410)
```rust
    fn pending_storage_data(&self) -> bool {
        load_pending_data_chunks(self.pending_data_chunks.clone()) > 0
    }
```

**File:** execution/executor/src/chunk_executor/mod.rs (L221-225)
```rust
    fn finish(&self) {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["chunk", "finish"]);

        *self.inner.write() = None;
    }
```

**File:** state-sync/state-sync-driver/src/driver_factory.rs (L231-235)
```rust
    pub fn block_until_initialized(&self) {
        let state_sync_client = self.state_sync.create_driver_client();
        block_on(state_sync_client.notify_once_bootstrapped())
            .expect("State sync v2 initialization failure");
    }
```
