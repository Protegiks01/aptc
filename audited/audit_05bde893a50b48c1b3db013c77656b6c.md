# Audit Report

## Title
Memory Exhaustion via Oversized Payload During Verification Bypass

## Summary
The `payload.verify()` function does not enforce maximum payload size limits before processing, allowing Byzantine validators to send proposals with payloads exceeding consensus limits (6 MB) up to network limits (64 MB), causing memory exhaustion through payload cloning during verification before size checks reject the proposal.

## Finding Description

The vulnerability exists in the payload verification flow where size limits are checked AFTER memory-intensive verification operations complete, rather than before.

**Attack Flow:**

1. A Byzantine validator crafts a `ProposalMsg` with `QuorumStoreInlineHybrid` payload containing inline batches totaling ~60 MB (below the 64 MB network limit but far exceeding the 6 MB consensus receiving limit).

2. The network layer accepts and deserializes the message into memory (60 MB allocated). [1](#0-0) 

3. When `UnverifiedEvent::ProposalMsg.verify()` is called, it invokes `ProposalMsg::verify()`: [2](#0-1) 

4. Inside `ProposalMsg::verify()`, the code calls `payload.verify()`: [3](#0-2) 

5. The `Payload::verify()` function processes inline batches by calling `verify_inline_batches()`: [4](#0-3) 

6. **Critical Issue**: `verify_inline_batches()` clones the entire transaction vector for each batch to compute its hash: [5](#0-4) 

This cloning operation doubles memory consumption (60 MB original + 60 MB clone = 120 MB total per proposal).

7. Only AFTER verification completes do size limits get checked in `process_proposal()`: [6](#0-5) 

The configured consensus limits are bypassed during verification: [7](#0-6) 

**Invariant Violation:**
This breaks the "Resource Limits" invariant: "All operations must respect gas, storage, and computational limits." The payload verification consumes memory far exceeding configured limits before rejection.

## Impact Explanation

**High Severity** per Aptos bug bounty criteria:

1. **Validator node slowdowns**: Processing oversized payloads with cloning causes significant memory pressure and GC overhead, degrading validator performance.

2. **Potential node crashes**: If multiple Byzantine validators send concurrent oversized proposals, or a single validator sends rapid bursts, accumulated memory consumption (120 MB Ã— number of concurrent proposals) could trigger OOM crashes.

3. **Consensus liveness impact**: If enough validators experience slowdowns or crashes from memory exhaustion, consensus liveness could be affected, though this requires coordination with the < 1/3 Byzantine fault tolerance.

## Likelihood Explanation

**Moderate to High Likelihood:**

- **Attacker Requirements**: Requires being a validator, which is within the Byzantine fault model (< 1/3 validators can be malicious).
- **Attack Complexity**: Low - simply send `ProposalMsg` with oversized inline batches.
- **Detection**: The attack leaves evidence in rejected proposal logs, but damage occurs before rejection.
- **Mitigating Factors**: Network limit caps maximum payload at 64 MB, limiting per-message impact. However, repeated attacks or concurrent proposals amplify the effect.

## Recommendation

Enforce payload size limits **before** expensive verification operations. Add size checks at the beginning of `Payload::verify()`:

```rust
pub fn verify(
    &self,
    verifier: &ValidatorVerifier,
    proof_cache: &ProofCache,
    quorum_store_enabled: bool,
    max_payload_bytes: u64,  // Add parameter
) -> anyhow::Result<()> {
    // Check size BEFORE any processing
    let payload_size = self.size();
    ensure!(
        payload_size as u64 <= max_payload_bytes,
        "Payload size {} exceeds maximum allowed {}",
        payload_size,
        max_payload_bytes
    );
    
    // Existing verification logic...
}
```

Call site should pass `max_receiving_block_bytes`: [3](#0-2) 

Additionally, avoid unnecessary cloning in `verify_inline_batches()` by computing hash without cloning (e.g., implement `CryptoHash` directly on the batch structure).

## Proof of Concept

```rust
#[cfg(test)]
mod memory_exhaustion_test {
    use super::*;
    use aptos_types::transaction::SignedTransaction;
    
    #[test]
    fn test_oversized_payload_memory_consumption() {
        // Create a large inline batch (50 MB of transactions)
        let large_txn = create_dummy_transaction_of_size(1024 * 1024); // 1 MB txn
        let mut txns = Vec::new();
        for _ in 0..50 {
            txns.push(large_txn.clone());
        }
        
        // Create payload with inline batch
        let batch_info = BatchInfo::new(...);
        let inline_batches = vec![(batch_info, txns)];
        let payload = Payload::QuorumStoreInlineHybrid(
            inline_batches,
            ProofWithData::empty(),
            None,
        );
        
        // Track memory before verification
        let mem_before = get_memory_usage();
        
        // Call verify - this will clone the 50 MB payload
        let _ = payload.verify(&verifier, &proof_cache, true);
        
        let mem_after = get_memory_usage();
        
        // Assert significant memory increase from cloning
        assert!(mem_after - mem_before > 50 * 1024 * 1024);
        
        // Size check happens later in process_proposal
        // The memory damage is already done
    }
}
```

## Notes

- The vulnerability affects `QuorumStoreInlineHybrid`, `QuorumStoreInlineHybridV2`, and `OptQuorumStore` payload types with inline batches.
- `DirectMempool` payloads avoid the cloning issue but still lack size validation during `verify()`.
- The TODO comment at line 545 in `common.rs` acknowledges the cloning could be avoided, indicating awareness of the inefficiency but no security consideration of the bypass.
- While the network layer limits messages to 64 MB, this still provides a ~10x gap over the consensus 6 MB receiving limit, enabling substantial memory amplification through the cloning operation.

### Citations

**File:** config/src/config/network_config.rs (L50-50)
```rust
pub const MAX_MESSAGE_SIZE: usize = 64 * 1024 * 1024; /* 64 MiB */
```

**File:** consensus/src/round_manager.rs (L120-127)
```rust
            UnverifiedEvent::ProposalMsg(p) => {
                if !self_message {
                    p.verify(peer_id, validator, proof_cache, quorum_store_enabled)?;
                    counters::VERIFY_MSG
                        .with_label_values(&["proposal"])
                        .observe(start_time.elapsed().as_secs_f64());
                }
                VerifiedEvent::ProposalMsg(p)
```

**File:** consensus/src/round_manager.rs (L1178-1193)
```rust
        let payload_len = proposal.payload().map_or(0, |payload| payload.len());
        let payload_size = proposal.payload().map_or(0, |payload| payload.size());
        ensure!(
            num_validator_txns + payload_len as u64 <= self.local_config.max_receiving_block_txns,
            "Payload len {} exceeds the limit {}",
            payload_len,
            self.local_config.max_receiving_block_txns,
        );

        ensure!(
            validator_txns_total_bytes + payload_size as u64
                <= self.local_config.max_receiving_block_bytes,
            "Payload size {} exceeds the limit {}",
            payload_size,
            self.local_config.max_receiving_block_bytes,
        );
```

**File:** consensus/consensus-types/src/proposal_msg.rs (L97-101)
```rust
        let (payload_result, sig_result) = rayon::join(
            || {
                self.proposal().payload().map_or(Ok(()), |p| {
                    p.verify(validator, proof_cache, quorum_store_enabled)
                })
```

**File:** consensus/consensus-types/src/common.rs (L544-546)
```rust
        for (batch, payload) in inline_batches {
            // TODO: Can cloning be avoided here?
            let computed_digest = BatchPayload::new(batch.author(), payload.clone()).hash();
```

**File:** consensus/consensus-types/src/common.rs (L590-596)
```rust
            (true, Payload::QuorumStoreInlineHybrid(inline_batches, proof_with_data, _))
            | (true, Payload::QuorumStoreInlineHybridV2(inline_batches, proof_with_data, _)) => {
                Self::verify_with_cache(&proof_with_data.proofs, verifier, proof_cache)?;
                Self::verify_inline_batches(
                    inline_batches.iter().map(|(info, txns)| (info, txns)),
                )?;
                Ok(())
```

**File:** config/src/config/consensus_config.rs (L227-231)
```rust
            max_sending_block_bytes: 3 * 1024 * 1024, // 3MB
            max_receiving_block_txns: *MAX_RECEIVING_BLOCK_TXNS,
            max_sending_inline_txns: 100,
            max_sending_inline_bytes: 200 * 1024,       // 200 KB
            max_receiving_block_bytes: 6 * 1024 * 1024, // 6MB
```
