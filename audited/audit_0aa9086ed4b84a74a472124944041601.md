# Audit Report

## Title
Unprotected Metrics Gathering Endpoint Enables CPU Exhaustion Attack on Validator Nodes

## Summary
The inspection service exposes metrics gathering endpoints without rate limiting, allowing external attackers to cause CPU exhaustion on validator nodes through repeated expensive `gather()` operations. While running in a separate runtime, these operations compete for CPU resources with consensus threads, potentially contributing to missed voting deadlines during high load conditions.

## Finding Description

The `get_all_metrics()` function in the inspection service performs an expensive operation that gathers all Prometheus metrics from the validator node. This function is exposed via HTTP endpoints (`/consensus_health_check` and `/forge_metrics`) without any rate limiting protection. [1](#0-0) 

The function calls `get_metric_families()` which invokes `aptos_metrics_core::gather()`, iterating through all registered metric families. The codebase explicitly acknowledges the overhead of this operation: [2](#0-1) 

The system tracks metric families exceeding 2000 dimensions and warns about them, indicating awareness of potential performance issues. With 72+ files registering metrics across the codebase, including 126 registrations in consensus alone, the total metric count can easily reach thousands of time series when accounting for label combinations. [3](#0-2) 

**Critical Design Flaw**: The `/consensus_health_check` endpoint contains a comment assuming "every few seconds" usage, but production Kubernetes deployments call it **every 1 second**: [4](#0-3) [5](#0-4) 

**Attack Vector**: The inspection service binds to `0.0.0.0:9101` by default, exposing it publicly: [6](#0-5) 

An attacker can send unlimited concurrent HTTP requests to these endpoints from multiple sources, with no rate limiting in place: [7](#0-6) 

**Resource Contention**: While the inspection service runs in a separate runtime, all threads compete for CPU cores on the same physical machine. During consensus critical operations, validators update metrics frequently: [8](#0-7) 

If multiple `gather()` operations execute concurrently while consensus is processing proposals and votes, CPU contention could push timing-sensitive operations past their deadlines. With consensus round timeouts starting at 1000ms, sustained CPU exhaustion could contribute to missed votes: [9](#0-8) 

## Impact Explanation

This issue qualifies as **Medium Severity** under the Aptos Bug Bounty program category of "Validator node slowdowns." While it does not directly break consensus safety, it provides an attack vector for degrading validator performance:

1. **Performance Degradation**: Repeated metrics gathering can consume 10-100ms per operation with thousands of metrics. An attacker sending 10 concurrent requests per second could consume 100-1000ms of CPU time per second.

2. **Amplification with Legitimate Traffic**: Kubernetes health checks already call the endpoint every second. Attacker traffic amplifies this baseline load.

3. **Consensus Timing Impact**: Validators under load could experience delays in proposal processing, vote generation, or message broadcasting, potentially missing round deadlines.

4. **No Defense Mechanism**: The absence of rate limiting means sustained attacks are trivially executable.

## Likelihood Explanation

**High Likelihood** of exploitation:
- Endpoints are publicly accessible by default (0.0.0.0:9101)
- No authentication or rate limiting required
- Attack is trivial to execute (simple HTTP GET requests)
- No special knowledge of the system needed
- Can be performed from distributed sources to evade network-level filtering

**Medium Likelihood** of actual consensus impact:
- Requires validator to be under load
- Modern multi-core systems provide some resilience
- Consensus has timeout margins (1000ms initial, exponential backoff)
- Would need sustained attack during active consensus operations

## Recommendation

Implement rate limiting on the inspection service endpoints:

```rust
// In crates/aptos-inspection-service/src/server/mod.rs
// Add rate limiter to the service

use aptos_rate_limiter::rate_limit::{Bucket, TokenBucketRateLimiter};

pub fn start_inspection_service(
    node_config: NodeConfig,
    aptos_data_client: AptosDataClient,
    peers_and_metadata: Arc<PeersAndMetadata>,
) {
    // Create rate limiter: 10 requests per second per endpoint
    let rate_limiter = Arc::new(TokenBucketRateLimiter::new(
        10, // max_tokens
        1,  // refill_rate (tokens per second)
    ));
    
    // Pass rate_limiter to service handlers
    // Check rate limit before processing expensive operations
}
```

For the metrics gathering specifically:

```rust
// In crates/aptos-inspection-service/src/server/metrics.rs
pub async fn handle_consensus_health_check(
    node_config: &NodeConfig,
    rate_limiter: &TokenBucketRateLimiter,
) -> (StatusCode, Body, String) {
    // Check rate limit
    if !rate_limiter.try_acquire(1) {
        return (
            StatusCode::TOO_MANY_REQUESTS,
            Body::from("Rate limit exceeded"),
            CONTENT_TYPE_TEXT.into(),
        );
    }
    
    // ... rest of implementation
}
```

Additional mitigations:
1. Consider caching metrics for short durations (100-500ms) for health check endpoints
2. Bind inspection service to localhost by default for production validators
3. Add configuration option to disable expensive endpoints in production
4. Implement per-IP rate limiting at the network layer

## Proof of Concept

```bash
#!/bin/bash
# PoC: Demonstrate CPU exhaustion via metrics endpoint abuse

VALIDATOR_IP="<validator-ip>"
PORT=9101

# Launch 50 concurrent attackers
for i in {1..50}; do
    (
        while true; do
            # Hammer both metrics endpoints
            curl -s "http://${VALIDATOR_IP}:${PORT}/consensus_health_check" > /dev/null &
            curl -s "http://${VALIDATOR_IP}:${PORT}/forge_metrics" > /dev/null &
            sleep 0.1
        done
    ) &
done

echo "Attack launched. Monitor validator CPU usage and consensus metrics."
echo "Expected: High CPU usage on validator, potential increase in consensus round timeouts"

# Monitor consensus metrics from another terminal:
# watch -n 1 'curl -s http://${VALIDATOR_IP}:${PORT}/metrics | grep aptos_consensus'
```

To validate impact, compare:
- Baseline: `aptos_consensus_last_committed_round` progression rate
- Under attack: Round progression rate and `aptos_consensus_timeout_count`

Expected observation: Increased CPU usage and potential slowdown in consensus round progression during sustained attack.

---

**Notes:**
- This is a defense-in-depth issue rather than a direct consensus-breaking vulnerability
- Impact severity depends on validator hardware resources and existing load
- The issue is exacerbated by Kubernetes health check frequency violating documented assumptions
- Production validators should implement network-level filtering as additional protection
- The vulnerability demonstrates the importance of rate limiting all externally accessible endpoints, even those intended for monitoring

### Citations

**File:** crates/aptos-inspection-service/src/server/utils.rs (L26-29)
```rust
pub fn get_all_metrics() -> HashMap<String, String> {
    let metric_families = get_metric_families();
    get_metrics_map(metric_families)
}
```

**File:** crates/aptos-inspection-service/src/server/utils.rs (L50-79)
```rust
fn get_metric_families() -> Vec<MetricFamily> {
    let metric_families = aptos_metrics_core::gather();
    let mut total: u64 = 0;
    let mut families_over_2000: u64 = 0;

    // Take metrics of metric gathering so we know possible overhead of this process
    for metric_family in &metric_families {
        let family_count = metric_family.get_metric().len();
        if family_count > 2000 {
            families_over_2000 = families_over_2000.saturating_add(1);
            let name = metric_family.get_name();
            warn!(
                count = family_count,
                metric_family = name,
                "Metric Family '{}' over 2000 dimensions '{}'",
                name,
                family_count
            );
        }
        total = total.saturating_add(family_count as u64);
    }

    // These metrics will be reported on the next pull, rather than create a new family
    NUM_METRICS.with_label_values(&["total"]).inc_by(total);
    NUM_METRICS
        .with_label_values(&["families_over_2000"])
        .inc_by(families_over_2000);

    metric_families
}
```

**File:** consensus/src/counters.rs (L1-20)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

#![allow(clippy::unwrap_used)]

use crate::{
    block_storage::tracing::{observe_block, BlockStage},
    quorum_store,
};
use aptos_consensus_types::{block::Block, pipelined_block::PipelinedBlock};
use aptos_crypto::HashValue;
use aptos_executor_types::{state_compute_result::StateComputeResult, ExecutorError};
use aptos_logger::prelude::warn;
use aptos_metrics_core::{
    exponential_buckets, op_counters::DurationHistogram, register_avg_counter, register_counter,
    register_gauge, register_gauge_vec, register_histogram, register_histogram_vec,
    register_int_counter, register_int_counter_vec, register_int_gauge, register_int_gauge_vec,
    Counter, Gauge, GaugeVec, Histogram, HistogramVec, IntCounter, IntCounterVec, IntGauge,
    IntGaugeVec,
};
```

**File:** crates/aptos-inspection-service/src/server/metrics.rs (L16-31)
```rust
/// Handles a consensus health check request. This method returns
/// 200 if the node is currently participating in consensus.
///
/// Note: we assume that this endpoint will only be used every few seconds.
pub async fn handle_consensus_health_check(node_config: &NodeConfig) -> (StatusCode, Body, String) {
    // Verify the node is a validator. If not, return an error.
    if !node_config.base.role.is_validator() {
        return (
            StatusCode::BAD_REQUEST,
            Body::from("This node is not a validator!"),
            CONTENT_TYPE_TEXT.into(),
        );
    }

    // Check the value of the consensus execution gauge
    let metrics = utils::get_all_metrics();
```

**File:** terraform/helm/aptos-node/templates/validator.yaml (L137-146)
```yaml
        {{- if $.Values.validator.useConsensusHealthCheckAsStartupProbe }}
        startupProbe:
          httpGet:
            path: /consensus_health_check
            port: 9101
            scheme: HTTP
          failureThreshold: 2147483647 # set it to the max value since we don't want to restart the pod automatically even if it can't participate in consensus
          periodSeconds: 1
          successThreshold: 1
          timeoutSeconds: 3
```

**File:** config/src/config/inspection_service_config.rs (L26-37)
```rust
impl Default for InspectionServiceConfig {
    fn default() -> InspectionServiceConfig {
        InspectionServiceConfig {
            address: "0.0.0.0".to_string(),
            port: 9101,
            expose_configuration: false,
            expose_identity_information: true,
            expose_peer_information: true,
            expose_system_information: true,
        }
    }
}
```

**File:** crates/aptos-inspection-service/src/server/mod.rs (L103-121)
```rust
/// A simple helper function that handles each endpoint request
async fn serve_requests(
    req: Request<Body>,
    node_config: NodeConfig,
    aptos_data_client: AptosDataClient,
    peers_and_metadata: Arc<PeersAndMetadata>,
) -> Result<Response<Body>, hyper::Error> {
    // Process the request and get the response components
    let (status_code, body, content_type) = match req.uri().path() {
        CONFIGURATION_PATH => {
            // /configuration
            // Exposes the node configuration
            configuration::handle_configuration_request(&node_config)
        },
        CONSENSUS_HEALTH_CHECK_PATH => {
            // /consensus_health_check
            // Exposes the consensus health check
            metrics::handle_consensus_health_check(&node_config).await
        },
```

**File:** consensus/src/round_manager.rs (L1500-1544)
```rust
    async fn vote_block(&mut self, proposed_block: Block) -> anyhow::Result<Vote> {
        let block_arc = self
            .block_store
            .insert_block(proposed_block)
            .await
            .context("[RoundManager] Failed to execute_and_insert the block")?;

        // Short circuit if already voted.
        ensure!(
            self.round_state.vote_sent().is_none(),
            "[RoundManager] Already vote on this round {}",
            self.round_state.current_round()
        );

        ensure!(
            !self.sync_only(),
            "[RoundManager] sync_only flag is set, stop voting"
        );

        let vote_proposal = block_arc.vote_proposal();
        let vote_result = self.safety_rules.lock().construct_and_sign_vote_two_chain(
            &vote_proposal,
            self.block_store.highest_2chain_timeout_cert().as_deref(),
        );
        let vote = vote_result.context(format!(
            "[RoundManager] SafetyRules Rejected {}",
            block_arc.block()
        ))?;
        if !block_arc.block().is_nil_block() {
            observe_block(block_arc.block().timestamp_usecs(), BlockStage::VOTED);
        }

        if block_arc.block().is_opt_block() {
            observe_block(
                block_arc.block().timestamp_usecs(),
                BlockStage::VOTED_OPT_BLOCK,
            );
        }

        self.storage
            .save_vote(&vote)
            .context("[RoundManager] Fail to persist last vote")?;

        Ok(vote)
    }
```

**File:** consensus/src/liveness/round_state.rs (L1-50)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use crate::{
    counters,
    pending_votes::{PendingVotes, VoteReceptionResult, VoteStatus},
    util::time_service::{SendTask, TimeService},
};
use aptos_consensus_types::{
    common::Round,
    round_timeout::{RoundTimeout, RoundTimeoutReason},
    sync_info::SyncInfo,
    timeout_2chain::TwoChainTimeoutWithPartialSignatures,
    vote::Vote,
};
use aptos_crypto::HashValue;
use aptos_logger::{prelude::*, Schema};
use aptos_types::validator_verifier::ValidatorVerifier;
use futures::future::AbortHandle;
use serde::Serialize;
use std::{fmt, sync::Arc, time::Duration};

/// A reason for starting a new round: introduced for monitoring / debug purposes.
#[derive(Serialize, Debug, PartialEq, Eq, Clone)]
pub enum NewRoundReason {
    QCReady,
    Timeout(RoundTimeoutReason),
}

impl fmt::Display for NewRoundReason {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        match self {
            NewRoundReason::QCReady => write!(f, "QCReady"),
            NewRoundReason::Timeout(_) => write!(f, "TCReady"),
        }
    }
}

/// NewRoundEvents produced by RoundState are guaranteed to be monotonically increasing.
/// NewRoundEvents are consumed by the rest of the system: they can cause sending new proposals
/// or voting for some proposals that wouldn't have been voted otherwise.
/// The duration is populated for debugging and testing
#[derive(Debug)]
pub struct NewRoundEvent {
    pub round: Round,
    pub reason: NewRoundReason,
    pub timeout: Duration,
    pub prev_round_votes: Vec<(HashValue, VoteStatus)>,
    pub prev_round_timeout_votes: Option<TwoChainTimeoutWithPartialSignatures>,
}
```
