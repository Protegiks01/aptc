# Audit Report

## Title
State Snapshot Restore Missing Final Root Hash Verification Allows Incomplete State Restoration

## Summary
The state snapshot restoration process verifies each chunk's `SparseMerkleRangeProof` correctly but lacks a final verification after `finish_impl()` to ensure the restored tree's root hash matches the expected root hash. This allows an attacker controlling the backup storage to modify the manifest to remove chunks, resulting in an incomplete but seemingly-validated state restoration.

## Finding Description

The vulnerability exists in the interaction between the chunk verification logic and the finalization process. 

The `StateSnapshotBackup` manifest is loaded as unauthenticated JSON, with only the `root_hash` field being authenticated via `LedgerInfoWithSignatures`. [1](#0-0) 

Each chunk's `SparseMerkleRangeProof` is verified in `add_chunk_impl()`, which checks that the accumulated state plus the proof's `right_siblings` can reconstruct the expected root hash. [2](#0-1) 

The verification uses the rightmost leaf added so far and computes left siblings from all accumulated state: [3](#0-2) 

**The critical flaw**: After all chunks are processed, `finish_impl()` freezes and writes the tree to storage WITHOUT verifying the final root hash matches the expected value. [4](#0-3) 

**Attack Vector:**
1. Attacker modifies the unauthenticated manifest's `chunks` array to remove chunks (e.g., keep only chunks 0-2, remove chunks 3-9)
2. The remaining chunks have valid proofs where `right_siblings` cryptographically commit to keys that SHOULD be in the removed chunks
3. During restoration, each chunk verifies successfully because: `accumulated_keys + proof.right_siblings = expected_root_hash`
4. The proof's `right_siblings` claim "there are more keys to be added later"
5. But no later chunks arrive (they were removed from manifest)
6. `finish_impl()` completes without checking that all promised keys were delivered
7. The final tree is incomplete with a different root hash than expected

The only root hash check occurs when creating a NEW `JellyfishMerkleRestore` instance for a completed restore, not during the initial restoration. [5](#0-4) 

## Impact Explanation

This vulnerability breaks the **State Consistency** invariant - restored state does not match the authenticated root hash, violating the guarantee that state is verifiable via Merkle proofs.

**Impact Classification: HIGH Severity**

This qualifies as a "Significant protocol violation" under the High severity category because:
- It breaks the backup/restore protocol's fundamental security guarantee
- A node restoring from malicious backup will have incomplete state but the restore process reports success
- The node will operate with incorrect state until detected through state sync failures or transaction execution errors
- This can cause operational disruption requiring manual intervention to detect and correct

While consensus itself isn't directly violated (requires validator stake), this enables:
- Operational disruption of nodes restoring from compromised backups
- Potential for nodes to execute transactions against incomplete state before detecting the issue
- State inconsistencies that require manual intervention and re-restoration

## Likelihood Explanation

**Likelihood: Medium-High**

This attack requires:
- Attacker control over backup storage (compromised backup system or malicious backup provider)
- Victim node performing state snapshot restoration from the compromised backup
- No attacker-controlled validator stake or network position required

The attack is straightforward to execute once the attacker controls backup storage:
1. Download legitimate backup manifest
2. Modify JSON manifest to remove chunks
3. Modify individual chunk files if needed (though often unnecessary - just removing manifest entries suffices)
4. Victim downloads and restores from modified backup

Common scenarios include:
- Cloud storage compromise
- Malicious third-party backup providers
- Supply chain attacks on backup infrastructure
- Man-in-the-middle attacks on backup downloads (if not properly secured)

## Recommendation

Add a final root hash verification in the `finish_impl()` method before returning successfully:

```rust
pub fn finish_impl(mut self) -> Result<()> {
    self.wait_for_async_commit()?;
    
    // ... existing freeze logic ...
    
    self.freeze(0);
    self.store.write_node_batch(&self.frozen_nodes)?;
    
    // **ADD THIS VERIFICATION:**
    // Verify the final tree root hash matches expected
    let root_node = self.store
        .get_node_option(&NodeKey::new_empty_path(self.version), "restore")?
        .ok_or_else(|| format_err!("Root node not found after restore"))?;
    
    ensure!(
        root_node.hash() == self.expected_root_hash,
        "Restored tree root hash {:x} does not match expected root hash {:x}",
        root_node.hash(),
        self.expected_root_hash,
    );
    
    Ok(())
}
```

Additionally, consider authenticating the chunk metadata in the manifest by including a Merkle tree of chunk descriptors in the signed `StateSnapshotBackup.proof`.

## Proof of Concept

```rust
// Reproduction steps (conceptual - requires backup infrastructure):

// 1. Create a legitimate state snapshot backup with 10 chunks covering keys [0..1000000]
// 2. Modify the manifest JSON:
{
  "version": 12345,
  "epoch": 100,
  "root_hash": "0xabcd...", // Keep original authenticated root hash
  "chunks": [
    // Keep only first 3 chunks covering keys [0..300000]
    // Remove chunks 4-10 that cover keys [300001..1000000]
    {"first_idx": 0, "last_idx": 100000, ...},
    {"first_idx": 100001, "last_idx": 200000, ...},
    {"first_idx": 200001, "last_idx": 300000, ...}
    // Chunks for keys [300001..1000000] removed
  ],
  "proof": "..." // Original authenticated proof
}

// 3. Victim runs: aptos-db-tool restore state-snapshot --state-manifest modified_manifest.json
// 4. Chunks 0-2 restore successfully (proofs verify with right_siblings claiming keys 300001+ exist)
// 5. finish_impl() completes successfully
// 6. Final tree only has keys [0..300000], root hash != 0xabcd...
// 7. No error reported during restore process
// 8. Error only detected later when node tries to sync or execute transactions requiring missing state
```

**Verification test:**
1. Set up state snapshot restore with modified manifest
2. Observe each chunk's `verify()` passes successfully
3. Observe `finish_impl()` returns `Ok(())` without checking final root hash
4. Query restored tree's actual root hash - it will differ from `expected_root_hash`
5. Restore reports success despite incomplete state

### Citations

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L123-136)
```rust
        let manifest: StateSnapshotBackup =
            self.storage.load_json_file(&self.manifest_handle).await?;
        let (txn_info_with_proof, li): (TransactionInfoWithProof, LedgerInfoWithSignatures) =
            self.storage.load_bcs_file(&manifest.proof).await?;
        txn_info_with_proof.verify(li.ledger_info(), manifest.version)?;
        let state_root_hash = txn_info_with_proof
            .transaction_info()
            .ensure_state_checkpoint_hash()?;
        ensure!(
            state_root_hash == manifest.root_hash,
            "Root hash mismatch with that in proof. root hash: {}, expected: {}",
            manifest.root_hash,
            state_root_hash,
        );
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L196-206)
```rust
        let (finished, partial_nodes, previous_leaf) = if let Some(root_node) =
            tree_reader.get_node_option(&NodeKey::new_empty_path(version), "restore")?
        {
            info!("Previous restore is complete, checking root hash.");
            ensure!(
                root_node.hash() == expected_root_hash,
                "Previous completed restore has root hash {}, expecting {}",
                root_node.hash(),
                expected_root_hash,
            );
            (true, vec![], None)
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L339-413)
```rust
    pub fn add_chunk_impl(
        &mut self,
        mut chunk: Vec<(&K, HashValue)>,
        proof: SparseMerkleRangeProof,
    ) -> Result<()> {
        if self.finished {
            info!("State snapshot restore already finished, ignoring entire chunk.");
            return Ok(());
        }

        if let Some(prev_leaf) = &self.previous_leaf {
            let skip_until = chunk
                .iter()
                .find_position(|(key, _hash)| key.hash() > *prev_leaf.account_key());
            chunk = match skip_until {
                None => {
                    info!("Skipping entire chunk.");
                    return Ok(());
                },
                Some((0, _)) => chunk,
                Some((num_to_skip, next_leaf)) => {
                    info!(
                        num_to_skip = num_to_skip,
                        next_leaf = next_leaf,
                        "Skipping leaves."
                    );
                    chunk.split_off(num_to_skip)
                },
            }
        };
        if chunk.is_empty() {
            return Ok(());
        }

        for (key, value_hash) in chunk {
            let hashed_key = key.hash();
            if let Some(ref prev_leaf) = self.previous_leaf {
                ensure!(
                    &hashed_key > prev_leaf.account_key(),
                    "State keys must come in increasing order.",
                )
            }
            self.previous_leaf.replace(LeafNode::new(
                hashed_key,
                value_hash,
                (key.clone(), self.version),
            ));
            self.add_one(key, value_hash);
            self.num_keys_received += 1;
        }

        // Verify what we have added so far is all correct.
        self.verify(proof)?;

        // Write the frozen nodes to storage.
        if self.async_commit {
            self.wait_for_async_commit()?;
            let (tx, rx) = channel();
            self.async_commit_result = Some(rx);

            let mut frozen_nodes = HashMap::new();
            std::mem::swap(&mut frozen_nodes, &mut self.frozen_nodes);
            let store = self.store.clone();

            IO_POOL.spawn(move || {
                let res = store.write_node_batch(&frozen_nodes);
                tx.send(res).unwrap();
            });
        } else {
            self.store.write_node_batch(&self.frozen_nodes)?;
            self.frozen_nodes.clear();
        }

        Ok(())
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L624-697)
```rust
    /// Verifies that all states that have been added so far (from the leftmost one to
    /// `self.previous_leaf`) are correct, i.e., we are able to construct `self.expected_root_hash`
    /// by combining all existing states and `proof`.
    #[allow(clippy::collapsible_if)]
    fn verify(&self, proof: SparseMerkleRangeProof) -> Result<()> {
        let previous_leaf = self
            .previous_leaf
            .as_ref()
            .expect("The previous leaf must exist.");

        let previous_key = previous_leaf.account_key();
        // If we have all siblings on the path from root to `previous_key`, we should be able to
        // compute the root hash. The siblings on the right are already in the proof. Now we
        // compute the siblings on the left side, which represent all the states that have ever
        // been added.
        let mut left_siblings = vec![];

        // The following process might add some extra placeholder siblings on the left, but it is
        // nontrivial to determine when the loop should stop. So instead we just add these
        // siblings for now and get rid of them in the next step.
        let mut num_visited_right_siblings = 0;
        for (i, bit) in previous_key.iter_bits().enumerate() {
            if bit {
                // This node is a right child and there should be a sibling on the left.
                let sibling = if i >= self.partial_nodes.len() * 4 {
                    *SPARSE_MERKLE_PLACEHOLDER_HASH
                } else {
                    Self::compute_left_sibling(
                        &self.partial_nodes[i / 4],
                        previous_key.get_nibble(i / 4),
                        (3 - i % 4) as u8,
                    )
                };
                left_siblings.push(sibling);
            } else {
                // This node is a left child and there should be a sibling on the right.
                num_visited_right_siblings += 1;
            }
        }
        ensure!(
            num_visited_right_siblings >= proof.right_siblings().len(),
            "Too many right siblings in the proof.",
        );

        // Now we remove any extra placeholder siblings at the bottom. We keep removing the last
        // sibling if 1) it's a placeholder 2) it's a sibling on the left.
        for bit in previous_key.iter_bits().rev() {
            if bit {
                if *left_siblings.last().expect("This sibling must exist.")
                    == *SPARSE_MERKLE_PLACEHOLDER_HASH
                {
                    left_siblings.pop();
                } else {
                    break;
                }
            } else if num_visited_right_siblings > proof.right_siblings().len() {
                num_visited_right_siblings -= 1;
            } else {
                break;
            }
        }

        // Left siblings must use the same ordering as the right siblings in the proof
        left_siblings.reverse();

        // Verify the proof now that we have all the siblings
        proof
            .verify(
                self.expected_root_hash,
                SparseMerkleLeafNode::new(*previous_key, previous_leaf.value_hash()),
                left_siblings,
            )
            .map_err(Into::into)
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L750-789)
```rust
    pub fn finish_impl(mut self) -> Result<()> {
        self.wait_for_async_commit()?;
        // Deal with the special case when the entire tree has a single leaf or null node.
        if self.partial_nodes.len() == 1 {
            let mut num_children = 0;
            let mut leaf = None;
            for i in 0..16 {
                if let Some(ref child_info) = self.partial_nodes[0].children[i] {
                    num_children += 1;
                    if let ChildInfo::Leaf(node) = child_info {
                        leaf = Some(node.clone());
                    }
                }
            }

            match num_children {
                0 => {
                    let node_key = NodeKey::new_empty_path(self.version);
                    assert!(self.frozen_nodes.is_empty());
                    self.frozen_nodes.insert(node_key, Node::Null);
                    self.store.write_node_batch(&self.frozen_nodes)?;
                    return Ok(());
                },
                1 => {
                    if let Some(node) = leaf {
                        let node_key = NodeKey::new_empty_path(self.version);
                        assert!(self.frozen_nodes.is_empty());
                        self.frozen_nodes.insert(node_key, node.into());
                        self.store.write_node_batch(&self.frozen_nodes)?;
                        return Ok(());
                    }
                },
                _ => (),
            }
        }

        self.freeze(0);
        self.store.write_node_batch(&self.frozen_nodes)?;
        Ok(())
    }
```
