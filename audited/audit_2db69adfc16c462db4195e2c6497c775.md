# Audit Report

## Title
Unbounded Epoch Retrieval Request Retry Loop Causing Resource Exhaustion

## Summary
A protocol-level vulnerability in the epoch synchronization mechanism allows repeated database queries and message processing without deduplication or rate limiting. When network queues are full or messages are dropped, nodes continue sending epoch retrieval requests for every consensus message received from different-epoch peers, causing resource exhaustion through repeated database operations (up to 100 epoch-ending ledger infos per query).

## Finding Description

The vulnerability exists in the epoch retrieval protocol implementation across multiple components:

**1. Error Handling Weakness:**
When `process_epoch_retrieval` attempts to send an `EpochChangeProof` response, send failures are only logged without providing feedback to the requester. [1](#0-0) 

Similarly, when `process_different_epoch` sends an `EpochRetrievalRequest`, failures are silently logged. [2](#0-1) 

**2. Unbounded Trigger Mechanism:**
The `check_epoch` function processes every consensus message type (ProposalMsg, VoteMsg, SyncInfo, etc.). When a message arrives from a different epoch, it triggers `process_different_epoch` with no deduplication. [3](#0-2) 

**3. Expensive Database Operations:**
Each epoch retrieval request triggers a database query that can fetch up to 100 epoch-ending ledger infos. [4](#0-3) 

**4. Silent Message Dropping:**
The underlying `aptos_channel::Sender::push` method only returns an error when the receiver is dropped, not when queues are full. [5](#0-4)  When queues are full, messages are silently dropped according to the QueueStyle policy, but the method still returns `Ok(())`, providing no feedback that the message was never delivered.

**5. No Deduplication or Rate Limiting:**
The EpochManager struct contains no fields for tracking pending epoch requests, implementing deduplication, or rate limiting. [6](#0-5) 

**Attack Flow:**
During epoch transitions or network congestion, when Node A (epoch N) receives consensus messages from Node B (epoch N+1):
1. Node A sends `EpochRetrievalRequest` to Node B
2. Node B queries database (up to 100 epoch infos)
3. Node B's `EpochChangeProof` response is dropped due to full network queue
4. No error is returned (silent drop), so Node A never receives the proof
5. Node B continues broadcasting consensus messages
6. Each message triggers another database query and request from Node A
7. Loop continues until network conditions improve

The code comments confirm cross-epoch communication is expected behavior during transitions. [7](#0-6) 

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under Aptos bug bounty criteria:

**Resource Exhaustion:** Both nodes experience CPU and I/O exhaustion through repeated database queries (up to 100 epoch-ending ledger infos per request), message serialization/deserialization, and network queue management overhead.

**Validator Performance Degradation:** Affected validators experience measurable slowdowns that could impact their consensus participation efficiency.

**Limited Scope:** The impact is confined to specific node pairs experiencing network issues rather than network-wide. It doesn't affect consensus safety properties, doesn't enable fund theft, and doesn't cause permanent network partitions.

**Temporary Nature:** The issue resolves when network conditions improve or when epoch synchronization succeeds through alternative paths.

This places the vulnerability firmly in Medium Severity ($10,000 tier) - worse than minor bugs but less severe than consensus safety violations or fund loss scenarios.

## Likelihood Explanation

**Likelihood: Medium**

Realistic triggering scenarios:

1. **Network Congestion:** During high load, message queues naturally fill up. The FIFO queue style used by consensus networking drops messages when full, while continuing to accept new messages that return `Ok(())`.

2. **Epoch Transitions:** The code explicitly expects nodes in different epochs to communicate during transitions, making this a normal operational scenario rather than an edge case.

3. **Partial Network Asymmetry:** Scenarios where Node A can receive from Node B but Node B's outbound queue to Node A is congested are realistic in distributed systems.

4. **No Mitigation Mechanisms:** The complete absence of deduplication, rate limiting, or backoff strategies means the vulnerability triggers automatically under the conditions above.

The likelihood is NOT High because: it requires persistent network issues or queue congestion between specific node pairs, and typically resolves when conditions normalize.

## Recommendation

Implement the following mitigations:

1. **Request Deduplication:** Track pending epoch retrieval requests per peer with timestamps to prevent duplicate requests within a time window.

2. **Exponential Backoff:** Implement backoff strategy when repeated requests to the same peer fail or timeout.

3. **Rate Limiting:** Limit epoch retrieval requests per peer to prevent unbounded loops (e.g., max 1 request per 5 seconds per peer).

4. **Request Timeout:** Track when requests are sent and consider them failed after a timeout, triggering backoff before retry.

5. **Alternative Proof Sources:** When direct delivery fails persistently, attempt to fetch epoch proofs from multiple peers in the higher epoch rather than repeatedly querying the same peer.

## Proof of Concept

While a full executable PoC would require network simulation infrastructure, the vulnerability can be demonstrated through the code flow:

1. Deploy two validator nodes in different epochs
2. Simulate network queue congestion between them (e.g., by rate limiting or filling queues with other messages)
3. Observe repeated database queries in logs from `get_epoch_ending_ledger_infos`
4. Monitor `EPOCH_MANAGER_ISSUES_DETAILS` metrics showing repeated "failed_to_send_epoch_retrieval" warnings
5. Measure CPU/IO impact from repeated database operations

The vulnerability logic is directly evident from the code structure showing no deduplication combined with trigger-on-every-message behavior.

## Notes

- The "malicious validator" scenario mentioned in the original likelihood assessment is out of scope per threat model (validators are trusted roles), but the non-malicious scenarios are sufficient to validate the vulnerability
- The issue is protocol-level, not an intentional Network DoS attack, so it falls within scope
- No evidence found that this is a known or previously reported issue

### Citations

**File:** consensus/src/epoch_manager.rs (L133-184)
```rust
pub struct EpochManager<P: OnChainConfigProvider> {
    author: Author,
    config: ConsensusConfig,
    randomness_override_seq_num: u64,
    time_service: Arc<dyn TimeService>,
    self_sender: aptos_channels::UnboundedSender<Event<ConsensusMsg>>,
    network_sender: ConsensusNetworkClient<NetworkClient<ConsensusMsg>>,
    timeout_sender: aptos_channels::Sender<Round>,
    quorum_store_enabled: bool,
    quorum_store_to_mempool_sender: Sender<QuorumStoreRequest>,
    execution_client: Arc<dyn TExecutionClient>,
    storage: Arc<dyn PersistentLivenessStorage>,
    safety_rules_manager: SafetyRulesManager,
    vtxn_pool: VTxnPoolState,
    reconfig_events: ReconfigNotificationListener<P>,
    // channels to rand manager
    rand_manager_msg_tx: Option<aptos_channel::Sender<AccountAddress, IncomingRandGenRequest>>,
    // channels to secret share manager
    secret_share_manager_tx:
        Option<aptos_channel::Sender<AccountAddress, IncomingSecretShareRequest>>,
    // channels to round manager
    round_manager_tx: Option<
        aptos_channel::Sender<(Author, Discriminant<VerifiedEvent>), (Author, VerifiedEvent)>,
    >,
    buffered_proposal_tx: Option<aptos_channel::Sender<Author, VerifiedEvent>>,
    round_manager_close_tx: Option<oneshot::Sender<oneshot::Sender<()>>>,
    epoch_state: Option<Arc<EpochState>>,
    block_retrieval_tx:
        Option<aptos_channel::Sender<AccountAddress, IncomingBlockRetrievalRequest>>,
    quorum_store_msg_tx: Option<aptos_channel::Sender<AccountAddress, (Author, VerifiedEvent)>>,
    quorum_store_coordinator_tx: Option<Sender<CoordinatorCommand>>,
    quorum_store_storage: Arc<dyn QuorumStoreStorage>,
    batch_retrieval_tx:
        Option<aptos_channel::Sender<AccountAddress, IncomingBatchRetrievalRequest>>,
    bounded_executor: BoundedExecutor,
    // recovery_mode is set to true when the recovery manager is spawned
    recovery_mode: bool,

    aptos_time_service: aptos_time_service::TimeService,
    dag_rpc_tx: Option<aptos_channel::Sender<AccountAddress, IncomingDAGRequest>>,
    dag_shutdown_tx: Option<oneshot::Sender<oneshot::Sender<()>>>,
    dag_config: DagConsensusConfig,
    payload_manager: Arc<dyn TPayloadManager>,
    rand_storage: Arc<dyn RandStorage<AugmentedData>>,
    proof_cache: ProofCache,
    consensus_publisher: Option<Arc<ConsensusPublisher>>,
    pending_blocks: Arc<Mutex<PendingBlocks>>,
    key_storage: PersistentSafetyStorage,

    consensus_txn_filter_config: BlockTransactionFilterConfig,
    quorum_store_txn_filter_config: BatchTransactionFilterConfig,
}
```

**File:** consensus/src/epoch_manager.rs (L469-475)
```rust
        if let Err(err) = self.network_sender.send_to(peer_id, msg) {
            warn!(
                "[EpochManager] Failed to send epoch proof to {}, with error: {:?}",
                peer_id, err,
            );
        }
        Ok(())
```

**File:** consensus/src/epoch_manager.rs (L497-498)
```rust
                    // Ignore message from lower epoch if we're part of the validator set, the node would eventually see messages from
                    // higher epoch and request a proof
```

**File:** consensus/src/epoch_manager.rs (L526-536)
```rust
                if let Err(err) = self.network_sender.send_to(peer_id, msg) {
                    warn!(
                        "[EpochManager] Failed to send epoch retrieval to {}, {:?}",
                        peer_id, err
                    );
                    counters::EPOCH_MANAGER_ISSUES_DETAILS
                        .with_label_values(&["failed_to_send_epoch_retrieval"])
                        .inc();
                }

                Ok(())
```

**File:** consensus/src/epoch_manager.rs (L1633-1653)
```rust
            ConsensusMsg::ProposalMsg(_)
            | ConsensusMsg::OptProposalMsg(_)
            | ConsensusMsg::SyncInfo(_)
            | ConsensusMsg::VoteMsg(_)
            | ConsensusMsg::RoundTimeoutMsg(_)
            | ConsensusMsg::OrderVoteMsg(_)
            | ConsensusMsg::CommitVoteMsg(_)
            | ConsensusMsg::CommitDecisionMsg(_)
            | ConsensusMsg::BatchMsg(_)
            | ConsensusMsg::BatchRequestMsg(_)
            | ConsensusMsg::SignedBatchInfo(_)
            | ConsensusMsg::ProofOfStoreMsg(_) => {
                let event: UnverifiedEvent = msg.into();
                if event.epoch()? == self.epoch() {
                    return Ok(Some(event));
                } else {
                    monitor!(
                        "process_different_epoch_consensus_msg",
                        self.process_different_epoch(event.epoch()?, peer_id)
                    )?;
                }
```

**File:** storage/aptosdb/src/common.rs (L9-9)
```rust
pub(crate) const MAX_NUM_EPOCH_ENDING_LEDGER_INFO: usize = 100;
```

**File:** crates/channel/src/aptos_channel.rs (L96-112)
```rust
    ) -> Result<()> {
        let mut shared_state = self.shared_state.lock();
        ensure!(!shared_state.receiver_dropped, "Channel is closed");
        debug_assert!(shared_state.num_senders > 0);

        let dropped = shared_state.internal_queue.push(key, (message, status_ch));
        // If this or an existing message had to be dropped because of the queue being full, we
        // notify the corresponding status channel if it was registered.
        if let Some((dropped_val, Some(dropped_status_ch))) = dropped {
            // Ignore errors.
            let _err = dropped_status_ch.send(ElementStatus::Dropped(dropped_val));
        }
        if let Some(w) = shared_state.waker.take() {
            w.wake();
        }
        Ok(())
    }
```
