# Audit Report

## Title
Consensus Database State Inconsistency Due to Unsynchronized Concurrent Write Operations

## Summary
The `save_blocks_and_quorum_certificates()` and `delete_blocks_and_quorum_certificates()` methods in ConsensusDB lack synchronization mechanisms, allowing concurrent execution that can lead to state inconsistency between in-memory BlockStore and persistent storage, potentially causing consensus divergence after node restart.

## Finding Description

The vulnerability exists in the interaction between ConsensusDB storage operations and the BlockStore's in-memory state management. Both methods take shared references (`&self`) and can execute concurrently without any synchronization. [1](#0-0) [2](#0-1) 

The critical issue arises in BlockStore's operations that call these methods **outside** the protection of the BlockTree's RwLock: [3](#0-2) [4](#0-3) 

The race condition occurs because:
1. Storage writes happen BEFORE acquiring the in-memory write lock
2. Storage deletes happen AFTER releasing the in-memory read lock  
3. No synchronization exists at the ConsensusDB level

**Exploitation Scenario:**

Thread A (inserting block):
- Calls `save_tree([Block_X])` → writes Block_X to persistent DB
- **RACE WINDOW**
- Calls `inner.write().insert_block(Block_X)` → adds Block_X to memory

Thread B (pruning blocks): 
- Calls `inner.read().find_blocks_to_prune()` → determines Block_X should be pruned
- **RACE WINDOW**  
- Calls `storage.prune_tree([Block_X])` → deletes Block_X from DB
- Updates in-memory state

**Result:** Block_X exists in memory but NOT in persistent storage. After crash/restart, recovery from disk will have inconsistent state compared to what other nodes expect.

This race can occur during normal consensus operation when:
- Multiple block proposals are being processed concurrently [5](#0-4) 
- Pruning operations execute simultaneously with block insertions
- The StorageWriteProxy wraps ConsensusDB in Arc for concurrent access [6](#0-5) 

While RocksDB's WriteBatch provides atomicity within each operation (preventing partial deletions within a single batch), it does NOT provide ordering guarantees between concurrent batches from different threads: [7](#0-6) [8](#0-7) 

## Impact Explanation

**Severity: High (up to $50,000)**

This vulnerability breaks **Critical Invariant #4: State Consistency** - "State transitions must be atomic and verifiable via Merkle proofs." The in-memory and persistent states can diverge, violating the fundamental assumption that persistent storage accurately reflects consensus state.

**Concrete Impacts:**

1. **Consensus Divergence After Recovery:** Nodes that restart after the race condition will recover different states, potentially causing validators to disagree on blockchain history.

2. **Protocol Violations:** The comment in the code acknowledges pruning failures are "fine" [9](#0-8)  but this race is different - it causes INSERTION failures to persist silently.

3. **Loss of Quorum Certificates:** The same race affects QC storage [10](#0-9)  which are critical for consensus proofs. Missing QCs after restart can break validator consensus.

4. **Non-Recoverable State:** Unlike pruning failures (which leave extra data), this race causes MISSING data, which cannot be recovered without re-syncing from peers.

## Likelihood Explanation

**Likelihood: Medium-High**

The race can occur during normal operation without any attacker action:

1. **High Concurrent Activity:** Consensus protocols inherently have high concurrency with multiple proposals being processed simultaneously
2. **No Synchronization:** Zero protection against concurrent access to storage layer
3. **Frequent Operations:** Block insertion and pruning happen continuously during consensus
4. **Window is Significant:** The race window spans from storage write until in-memory update, which includes disk I/O latency

A malicious validator could increase probability by:
- Submitting many proposals to maximize concurrent insertions
- Timing block commits to trigger pruning during peak insertion activity

## Recommendation

Add synchronization at the ConsensusDB level using a Mutex or RwLock:

```rust
use std::sync::Mutex;

pub struct ConsensusDB {
    db: DB,
    // Add mutex to protect concurrent write operations
    write_lock: Mutex<()>,
}

impl ConsensusDB {
    pub fn save_blocks_and_quorum_certificates(
        &self,
        block_data: Vec<Block>,
        qc_data: Vec<QuorumCert>,
    ) -> Result<(), DbError> {
        // Acquire lock before any storage operation
        let _guard = self.write_lock.lock().unwrap();
        
        if block_data.is_empty() && qc_data.is_empty() {
            return Err(anyhow::anyhow!("Consensus block and qc data is empty!").into());
        }
        let mut batch = SchemaBatch::new();
        block_data
            .iter()
            .try_for_each(|block| batch.put::<BlockSchema>(&block.id(), block))?;
        qc_data
            .iter()
            .try_for_each(|qc| batch.put::<QCSchema>(&qc.certified_block().id(), qc))?;
        self.commit(batch)
        // Lock automatically released
    }

    pub fn delete_blocks_and_quorum_certificates(
        &self,
        block_ids: Vec<HashValue>,
    ) -> Result<(), DbError> {
        // Acquire lock before any storage operation
        let _guard = self.write_lock.lock().unwrap();
        
        if block_ids.is_empty() {
            return Err(anyhow::anyhow!("Consensus block ids is empty!").into());
        }
        let mut batch = SchemaBatch::new();
        block_ids.iter().try_for_each(|hash| {
            batch.delete::<BlockSchema>(hash)?;
            batch.delete::<QCSchema>(hash)
        })?;
        self.commit(batch)
        // Lock automatically released
    }
}
```

Alternatively, move both storage operations inside the BlockTree lock to ensure atomicity:

```rust
// In BlockStore::insert_block
pub async fn insert_block(&self, pipelined_block: PipelinedBlock) -> anyhow::Result<()> {
    let mut inner = self.inner.write();  // Acquire lock FIRST
    
    // Save to storage while holding lock
    self.storage
        .save_tree(vec![pipelined_block.block().clone()], vec![])
        .context("Insert block failed when saving block")?;
    
    // Then update memory
    inner.insert_block(pipelined_block)
    // Lock released
}
```

## Proof of Concept

```rust
// Rust test demonstrating the race condition
#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_concurrent_save_delete_race() {
    use std::sync::Arc;
    use tokio::task;
    
    let consensus_db = Arc::new(ConsensusDB::new(test_db_path));
    let test_block = create_test_block();
    let block_id = test_block.id();
    
    // Spawn concurrent operations
    let db1 = consensus_db.clone();
    let block_clone = test_block.clone();
    let save_handle = task::spawn(async move {
        // Simulate block insertion
        for _ in 0..1000 {
            db1.save_blocks_and_quorum_certificates(
                vec![block_clone.clone()], 
                vec![]
            ).unwrap();
        }
    });
    
    let db2 = consensus_db.clone();
    let delete_handle = task::spawn(async move {
        // Simulate concurrent pruning
        for _ in 0..1000 {
            db2.delete_blocks_and_quorum_certificates(
                vec![block_id]
            ).unwrap_or(());
        }
    });
    
    save_handle.await.unwrap();
    delete_handle.await.unwrap();
    
    // Check final state - result is non-deterministic!
    // Block may or may not exist depending on race outcome
    let final_state = consensus_db.get::<BlockSchema>(&block_id).unwrap();
    println!("Final state non-deterministic: {:?}", final_state);
}
```

## Notes

While RocksDB's atomic WriteBatch prevents "partial deletions" within a single operation (answering part of the security question), the lack of application-level synchronization allows non-deterministic ordering between concurrent operations. This creates a state consistency vulnerability where persistent storage diverges from in-memory state, breaking consensus invariants after crash recovery.

The code comment suggesting pruning failures are "fine" does not apply here - this race causes MISSING data (saved then deleted), not extra data (failed to prune), which is more severe and non-recoverable without peer synchronization.

### Citations

**File:** consensus/src/consensusdb/mod.rs (L121-137)
```rust
    pub fn save_blocks_and_quorum_certificates(
        &self,
        block_data: Vec<Block>,
        qc_data: Vec<QuorumCert>,
    ) -> Result<(), DbError> {
        if block_data.is_empty() && qc_data.is_empty() {
            return Err(anyhow::anyhow!("Consensus block and qc data is empty!").into());
        }
        let mut batch = SchemaBatch::new();
        block_data
            .iter()
            .try_for_each(|block| batch.put::<BlockSchema>(&block.id(), block))?;
        qc_data
            .iter()
            .try_for_each(|qc| batch.put::<QCSchema>(&qc.certified_block().id(), qc))?;
        self.commit(batch)
    }
```

**File:** consensus/src/consensusdb/mod.rs (L139-152)
```rust
    pub fn delete_blocks_and_quorum_certificates(
        &self,
        block_ids: Vec<HashValue>,
    ) -> Result<(), DbError> {
        if block_ids.is_empty() {
            return Err(anyhow::anyhow!("Consensus block ids is empty!").into());
        }
        let mut batch = SchemaBatch::new();
        block_ids.iter().try_for_each(|hash| {
            batch.delete::<BlockSchema>(hash)?;
            batch.delete::<QCSchema>(hash)
        })?;
        self.commit(batch)
    }
```

**File:** consensus/src/block_storage/block_store.rs (L69-84)
```rust
/// Responsible for maintaining all the blocks of payload and the dependencies of those blocks
/// (parent and previous QC links).  It is expected to be accessed concurrently by multiple threads
/// and is thread-safe.
///
/// Example tree block structure based on parent links.
///                         ╭--> A3
/// Genesis--> B0--> B1--> B2--> B3
///             ╰--> C1--> C2
///                         ╰--> D3
///
/// Example corresponding tree block structure for the QC links (must follow QC constraints).
///                         ╭--> A3
/// Genesis--> B0--> B1--> B2--> B3
///             ├--> C1
///             ├--------> C2
///             ╰--------------> D3
```

**File:** consensus/src/block_storage/block_store.rs (L512-515)
```rust
        self.storage
            .save_tree(vec![pipelined_block.block().clone()], vec![])
            .context("Insert block failed when saving block")?;
        self.inner.write().insert_block(pipelined_block)
```

**File:** consensus/src/block_storage/block_store.rs (L552-555)
```rust
        self.storage
            .save_tree(vec![], vec![qc.clone()])
            .context("Insert block failed when saving quorum")?;
        self.inner.write().insert_quorum_cert(qc)
```

**File:** consensus/src/block_storage/block_store.rs (L843-860)
```rust
    pub(crate) fn prune_tree(&self, next_root_id: HashValue) -> VecDeque<HashValue> {
        let id_to_remove = self.inner.read().find_blocks_to_prune(next_root_id);
        if let Err(e) = self
            .storage
            .prune_tree(id_to_remove.clone().into_iter().collect())
        {
            // it's fine to fail here, as long as the commit succeeds, the next restart will clean
            // up dangling blocks, and we need to prune the tree to keep the root consistent with
            // executor.
            warn!(error = ?e, "fail to delete block");
        }

        // synchronously update both root_id and commit_root_id
        let mut wlock = self.inner.write();
        wlock.update_ordered_root(next_root_id);
        wlock.update_commit_root(next_root_id);
        wlock.update_window_root(next_root_id);
        wlock.process_pruned_blocks(id_to_remove.clone());
```

**File:** consensus/src/persistent_liveness_storage.rs (L480-489)
```rust
pub struct StorageWriteProxy {
    db: Arc<ConsensusDB>,
    aptos_db: Arc<dyn DbReader>,
}

impl StorageWriteProxy {
    pub fn new(config: &NodeConfig, aptos_db: Arc<dyn DbReader>) -> Self {
        let db = Arc::new(ConsensusDB::new(config.storage.dir()));
        StorageWriteProxy { db, aptos_db }
    }
```

**File:** storage/schemadb/src/lib.rs (L289-304)
```rust
    fn write_schemas_inner(&self, batch: impl IntoRawBatch, option: &WriteOptions) -> DbResult<()> {
        let labels = [self.name.as_str()];
        let _timer = APTOS_SCHEMADB_BATCH_COMMIT_LATENCY_SECONDS.timer_with(&labels);

        let raw_batch = batch.into_raw_batch(self)?;

        let serialized_size = raw_batch.inner.size_in_bytes();
        self.inner
            .write_opt(raw_batch.inner, option)
            .into_db_res()?;

        raw_batch.stats.commit();
        APTOS_SCHEMADB_BATCH_COMMIT_BYTES.observe_with(&[&self.name], serialized_size as f64);

        Ok(())
    }
```

**File:** storage/schemadb/src/lib.rs (L316-318)
```rust
    pub fn write_schemas_relaxed(&self, batch: impl IntoRawBatch) -> DbResult<()> {
        self.write_schemas_inner(batch, &WriteOptions::default())
    }
```
