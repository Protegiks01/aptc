# Audit Report

## Title
Global Thread Pool Contention Between Backup Service and Critical Consensus/State-Sync Operations

## Summary
The backup service, consensus message processing, and state-sync operations all use `tokio::task::spawn_blocking`, which shares a global 64-thread blocking pool across all Tokio runtimes in the validator process. Malicious or heavy backup requests can saturate this shared pool, causing critical consensus and state-sync operations to queue, leading to validator node slowdowns and potential consensus liveness degradation.

## Finding Description

The Aptos validator node creates multiple independent Tokio runtimes for different subsystems (backup, consensus, mempool, state-sync, network). However, all calls to `tokio::task::spawn_blocking` from any runtime share the same process-wide global blocking thread pool, limited to 64 threads. [1](#0-0) 

The backup service uses `spawn_blocking` for all streaming backup operations without any rate limiting or concurrency control: [2](#0-1) 

The backup service endpoints have no rate limiting or concurrency protection: [3](#0-2) 

Critical consensus operations also use the same global blocking pool for message serialization and deserialization: [4](#0-3) [5](#0-4) [6](#0-5) 

State-sync operations use the same pool for transaction execution, applying outputs, ledger updates, and chunk commits: [7](#0-6) [8](#0-7) [9](#0-8) 

The backup service is started on all nodes including validators: [10](#0-9) 

**Attack Scenario:**
1. Attacker gains access to backup service endpoint (localhost:6186 by default, but potentially exposed in cloud deployments or accessible by local processes)
2. Attacker sends many concurrent backup requests to endpoints like `state_snapshot/<version>`, `transactions/<start>/<count>`, or `state_snapshot_chunk/<version>/<start>/<limit>`
3. Each request spawns blocking tasks that read from database and stream data
4. These tasks fill the 64-thread blocking pool
5. Critical consensus message deserialization/serialization operations get queued
6. Critical state-sync operations (execute transactions, commit chunks) get queued
7. Validator experiences slowdowns in block production and state synchronization

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos bug bounty program criteria:
- **"Validator node slowdowns"** - directly causes validator performance degradation
- Potential consensus liveness issues if message processing is delayed significantly
- State-sync delays affecting validator's ability to stay synchronized

The impact affects the **Resource Limits** invariant (invariant #9) - blocking operations should not be able to exhaust shared resources and impact critical validator operations. It also risks violating the **Consensus Safety** invariant (#2) if message processing delays become severe enough to cause timeouts or missed rounds.

## Likelihood Explanation

**Likelihood: Medium**

**Attacker Requirements:**
- Access to backup service endpoint (localhost:6186 by default)
- Ability to send HTTP requests
- No authentication or rate limiting to bypass

**Factors Increasing Likelihood:**
- Default configuration binds to localhost, but cloud deployments may expose it
- Even localhost-only exposure can be exploited by other processes on the validator machine
- No concurrent request limits on the backup service
- Legitimate heavy backup usage can also trigger this issue unintentionally

**Factors Decreasing Likelihood:**
- Default localhost binding provides some protection
- Requires sustained concurrent requests to exhaust 64 threads
- Network bandwidth may limit request rate before thread pool exhaustion

## Recommendation

Implement multiple defense layers:

**1. Dedicated Blocking Pool for Backup Service:**
Create a separate blocking thread pool specifically for backup operations using a dedicated Tokio runtime with its own blocking pool configuration, isolating backup operations from critical validator operations.

**2. Rate Limiting and Concurrency Control:**
Add rate limiting and maximum concurrent request limits to the backup service:
- Limit concurrent streaming requests (e.g., max 8 concurrent backup streams)
- Implement per-IP rate limiting
- Add request queue with bounded capacity
- Reject requests when capacity is exceeded

**3. Access Control:**
- Ensure backup service is never exposed on public interfaces by default
- Add authentication/authorization for backup endpoints
- Document security implications of exposing backup service

**Example fix for dedicated blocking pool:**
```rust
// In backup-service/src/lib.rs
pub fn start_backup_service(address: SocketAddr, db: Arc<AptosDB>) -> Runtime {
    let backup_handler = db.get_backup_handler();
    let routes = get_routes(backup_handler);

    // Create runtime with dedicated blocking pool
    let runtime = tokio::runtime::Builder::new_multi_thread()
        .thread_name_fn(|| {
            static ATOMIC_ID: AtomicUsize = AtomicUsize::new(0);
            let id = ATOMIC_ID.fetch_add(1, Ordering::SeqCst);
            format!("backup-{}", id)
        })
        .max_blocking_threads(16)  // Separate pool for backup only
        .enable_all()
        .build()
        .expect("Failed to create backup runtime");

    // Bind and spawn server
    let _guard = runtime.enter();
    let server = warp::serve(routes).bind(address);
    runtime.handle().spawn(server);
    info!("Backup service spawned with dedicated blocking pool.");
    runtime
}
```

**Example fix for concurrency control:**
```rust
use tokio::sync::Semaphore;
use std::sync::Arc;

// Add semaphore to limit concurrent backup requests
pub(crate) fn get_routes(
    backup_handler: BackupHandler,
    max_concurrent_requests: usize,
) -> BoxedFilter<(impl Reply,)> {
    let semaphore = Arc::new(Semaphore::new(max_concurrent_requests));
    
    // Wrap each streaming endpoint with semaphore
    let state_snapshot = {
        let sem = semaphore.clone();
        warp::path!(Version)
            .and_then(move |version| {
                let sem = sem.clone();
                async move {
                    let _permit = sem.acquire().await
                        .map_err(|_| warp::reject::reject())?;
                    // ... existing handler logic
                }
            })
    };
    // ... apply to other streaming endpoints
}
```

## Proof of Concept

```rust
// Test demonstrating thread pool exhaustion
// Place in storage/backup/backup-service/src/lib.rs

#[cfg(test)]
mod thread_pool_contention_test {
    use super::*;
    use std::sync::atomic::{AtomicU32, Ordering};
    use std::sync::Arc;
    use std::time::Duration;
    use tokio::time::timeout;

    #[tokio::test(flavor = "multi_thread", worker_threads = 2)]
    async fn test_backup_blocks_spawn_blocking_operations() {
        // Setup: Create in-memory DB and backup service
        let tmpdir = aptos_temppath::TempPath::new();
        let db = Arc::new(AptosDB::new_for_test(&tmpdir));
        let port = aptos_config::utils::get_available_port();
        let addr = std::net::SocketAddr::from(([127, 0, 0, 1], port));
        
        let _backup_runtime = start_backup_service(addr, db.clone());
        tokio::time::sleep(Duration::from_millis(100)).await;

        // Simulate blocking pool exhaustion by spawning many blocking tasks
        // that simulate backup operations
        let counter = Arc::new(AtomicU32::new(0));
        let mut handles = vec![];
        
        // Spawn 70 blocking tasks (more than the 64 limit)
        for _ in 0..70 {
            let counter_clone = counter.clone();
            let handle = tokio::task::spawn_blocking(move || {
                counter_clone.fetch_add(1, Ordering::SeqCst);
                // Simulate slow backup read
                std::thread::sleep(Duration::from_secs(2));
            });
            handles.push(handle);
        }

        // Now try to do a critical consensus-like operation
        // This should be delayed because the blocking pool is saturated
        let critical_start = std::time::Instant::now();
        let critical_result = timeout(
            Duration::from_millis(500),
            tokio::task::spawn_blocking(|| {
                // Simulate consensus message deserialization
                "critical_operation_completed"
            })
        ).await;

        let critical_duration = critical_start.elapsed();

        // The critical operation should timeout because the pool is full
        assert!(
            critical_result.is_err() || critical_duration > Duration::from_millis(400),
            "Critical operation should be delayed or timeout due to pool saturation. \
             Duration: {:?}, Result: {:?}",
            critical_duration,
            critical_result.is_err()
        );

        // Cleanup
        for handle in handles {
            let _ = handle.await;
        }
    }
}
```

## Notes

This vulnerability demonstrates a resource isolation failure where non-critical backup operations can impact critical consensus and state-sync paths. The shared global blocking thread pool in Tokio is a known design characteristic, and proper mitigation requires either:

1. Separating backup operations into a dedicated runtime with its own blocking pool
2. Implementing strict concurrency limits on backup requests
3. Using alternative I/O approaches (async I/O, dedicated thread pools) for backup operations

The vulnerability is exacerbated by the lack of rate limiting or concurrency control on backup service endpoints, allowing unlimited concurrent requests to exhaust the shared resource.

### Citations

**File:** crates/aptos-runtimes/src/lib.rs (L27-50)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;

    // Verify the given name has an appropriate length
    if thread_name.len() > MAX_THREAD_NAME_LENGTH {
        panic!(
            "The given runtime thread name is too long! Max length: {}, given name: {}",
            MAX_THREAD_NAME_LENGTH, thread_name
        );
    }

    // Create the runtime builder
    let atomic_id = AtomicUsize::new(0);
    let thread_name_clone = thread_name.clone();
    let mut builder = Builder::new_multi_thread();
    builder
        .thread_name_fn(move || {
            let id = atomic_id.fetch_add(1, Ordering::SeqCst);
            format!("{}-{}", thread_name_clone, id)
        })
        .on_thread_start(on_thread_start)
        .disable_lifo_slot()
        // Limit concurrent blocking tasks from spawn_blocking(), in case, for example, too many
        // Rest API calls overwhelm the node.
        .max_blocking_threads(MAX_BLOCKING_THREADS)
```

**File:** storage/backup/backup-service/src/handlers/utils.rs (L46-65)
```rust
pub(super) fn reply_with_bytes_sender<F>(
    backup_handler: &BackupHandler,
    endpoint: &'static str,
    f: F,
) -> Box<dyn Reply>
where
    F: FnOnce(BackupHandler, &mut bytes_sender::BytesSender) -> DbResult<()> + Send + 'static,
{
    let (sender, stream) = bytes_sender::BytesSender::new(endpoint);

    // spawn and forget, error propagates through the `stream: TryStream<_>`
    let bh = backup_handler.clone();
    let _join_handle = tokio::task::spawn_blocking(move || {
        let _timer =
            BACKUP_TIMER.timer_with(&[&format!("backup_service_bytes_sender_{}", endpoint)]);
        abort_on_error(f)(bh, sender)
    });

    Box::new(Response::new(Body::wrap_stream(stream)))
}
```

**File:** storage/backup/backup-service/src/handlers/mod.rs (L47-79)
```rust
    // GET state_snapshot/<version>
    let bh = backup_handler.clone();
    let state_snapshot = warp::path!(Version)
        .map(move |version| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT, move |bh, sender| {
                bh.get_state_item_iter(version, 0, usize::MAX)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET state_item_count/<version>
    let bh = backup_handler.clone();
    let state_item_count = warp::path!(Version)
        .map(move |version| {
            reply_with_bcs_bytes(
                STATE_ITEM_COUNT,
                &(bh.get_state_item_count(version)? as u64),
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_snapshot_chunk/<version>/<start_idx>/<limit>
    let bh = backup_handler.clone();
    let state_snapshot_chunk = warp::path!(Version / usize / usize)
        .map(move |version, start_idx, limit| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT_CHUNK, move |bh, sender| {
                bh.get_state_item_iter(version, start_idx, limit)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);
```

**File:** consensus/src/network.rs (L320-332)
```rust
    ) -> anyhow::Result<ConsensusMsg> {
        let (tx, rx) = oneshot::channel();
        let protocol = RPC[0];
        let self_msg = Event::RpcRequest(self.author, msg.clone(), RPC[0], tx);
        self.self_sender.clone().send(self_msg).await?;
        if let Ok(Ok(Ok(bytes))) = timeout(timeout_duration, rx).await {
            let response_msg =
                tokio::task::spawn_blocking(move || protocol.from_bytes(&bytes)).await??;
            Ok(response_msg)
        } else {
            bail!("self rpc failed");
        }
    }
```

**File:** consensus/src/network.rs (L502-510)
```rust
    pub async fn broadcast_fast_share(&self, share: FastShare<Share>) {
        fail_point!("consensus::send::broadcast_share", |_| ());
        let msg = tokio::task::spawn_blocking(|| {
            RandMessage::<Share, AugmentedData>::FastShare(share).into_network_message()
        })
        .await
        .expect("task cannot fail to execute");
        self.broadcast(msg).await
    }
```

**File:** consensus/src/network.rs (L685-707)
```rust
        &self,
        receiver: Author,
        raw_message: Bytes,
        timeout: Duration,
    ) -> anyhow::Result<Res> {
        let response_msg = self
            .consensus_network_client
            .send_rpc_raw(receiver, raw_message, timeout)
            .await
            .map_err(|e| anyhow!("invalid rpc response: {}", e))?;
        tokio::task::spawn_blocking(|| TConsensusMsg::from_network_message(response_msg)).await?
    }

    async fn send_rb_rpc(
        &self,
        receiver: Author,
        message: Req,
        timeout: Duration,
    ) -> anyhow::Result<Res> {
        let consensus_msg = message.into_network_message();
        let response_msg = self.send_rpc(receiver, consensus_msg, timeout).await?;
        tokio::task::spawn_blocking(|| TConsensusMsg::from_network_message(response_msg)).await?
    }
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L983-1002)
```rust
/// Spawns a dedicated task that applies the given output chunk. We use
/// `spawn_blocking` so that the heavy synchronous function doesn't
/// block the async thread.
async fn apply_output_chunk<ChunkExecutor: ChunkExecutorTrait + 'static>(
    chunk_executor: Arc<ChunkExecutor>,
    outputs_with_proof: TransactionOutputListWithProofV2,
    target_ledger_info: LedgerInfoWithSignatures,
    end_of_epoch_ledger_info: Option<LedgerInfoWithSignatures>,
) -> anyhow::Result<()> {
    // Apply the output chunk
    let num_outputs = outputs_with_proof.get_num_outputs();
    let result = tokio::task::spawn_blocking(move || {
        chunk_executor.enqueue_chunk_by_transaction_outputs(
            outputs_with_proof,
            &target_ledger_info,
            end_of_epoch_ledger_info.as_ref(),
        )
    })
    .await
    .expect("Spawn_blocking(apply_output_chunk) failed!");
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L1023-1045)
```rust
/// Spawns a dedicated task that executes the given transaction chunk.
/// We use `spawn_blocking` so that the heavy synchronous function
/// doesn't block the async thread.
async fn execute_transaction_chunk<ChunkExecutor: ChunkExecutorTrait + 'static>(
    chunk_executor: Arc<ChunkExecutor>,
    transactions_with_proof: TransactionListWithProofV2,
    target_ledger_info: LedgerInfoWithSignatures,
    end_of_epoch_ledger_info: Option<LedgerInfoWithSignatures>,
) -> anyhow::Result<()> {
    // Execute the transaction chunk
    let num_transactions = transactions_with_proof
        .get_transaction_list_with_proof()
        .transactions
        .len();
    let result = tokio::task::spawn_blocking(move || {
        chunk_executor.enqueue_chunk_by_execution(
            transactions_with_proof,
            &target_ledger_info,
            end_of_epoch_ledger_info.as_ref(),
        )
    })
    .await
    .expect("Spawn_blocking(execute_transaction_chunk) failed!");
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L1080-1100)
```rust
/// Spawns a dedicated task that updates the ledger in storage. We use
/// `spawn_blocking` so that the heavy synchronous function doesn't
/// block the async thread.
async fn update_ledger<ChunkExecutor: ChunkExecutorTrait + 'static>(
    chunk_executor: Arc<ChunkExecutor>,
) -> anyhow::Result<()> {
    tokio::task::spawn_blocking(move || chunk_executor.update_ledger())
        .await
        .expect("Spawn_blocking(update_ledger) failed!")
}

/// Spawns a dedicated task that commits a data chunk. We use
/// `spawn_blocking` so that the heavy synchronous function doesn't
/// block the async thread.
async fn commit_chunk<ChunkExecutor: ChunkExecutorTrait + 'static>(
    chunk_executor: Arc<ChunkExecutor>,
) -> anyhow::Result<ChunkCommitNotification> {
    tokio::task::spawn_blocking(move || chunk_executor.commit_chunk())
        .await
        .expect("Spawn_blocking(commit_chunk) failed!")
}
```

**File:** aptos-node/src/storage.rs (L63-98)
```rust
    let (aptos_db_reader, db_rw, backup_service) = match FastSyncStorageWrapper::initialize_dbs(
        node_config,
        internal_indexer_db.clone(),
        update_sender,
    )? {
        Either::Left(db) => {
            let (db_arc, db_rw) = DbReaderWriter::wrap(db);
            let db_backup_service =
                start_backup_service(node_config.storage.backup_service_address, db_arc.clone());
            maybe_apply_genesis(&db_rw, node_config)?;
            (db_arc as Arc<dyn DbReader>, db_rw, Some(db_backup_service))
        },
        Either::Right(fast_sync_db_wrapper) => {
            let temp_db = fast_sync_db_wrapper.get_temporary_db_with_genesis();
            maybe_apply_genesis(&DbReaderWriter::from_arc(temp_db), node_config)?;
            let (db_arc, db_rw) = DbReaderWriter::wrap(fast_sync_db_wrapper);
            let fast_sync_db = db_arc.get_fast_sync_db();
            // FastSyncDB requires ledger info at epoch 0 to establish provenance to genesis
            let ledger_info = db_arc
                .get_temporary_db_with_genesis()
                .get_epoch_ending_ledger_info(0)
                .expect("Genesis ledger info must exist");

            if fast_sync_db
                .get_latest_ledger_info_option()
                .expect("should returns Ok results")
                .is_none()
            {
                // it means the DB is empty and we need to
                // commit the genesis ledger info to the DB.
                fast_sync_db.commit_genesis_ledger_info(&ledger_info)?;
            }
            let db_backup_service =
                start_backup_service(node_config.storage.backup_service_address, fast_sync_db);
            (db_arc as Arc<dyn DbReader>, db_rw, Some(db_backup_service))
        },
```
