# Audit Report

## Title
Consensus Safety Violation: Logical Time Desynchronization in `sync_to_target` Enables State Divergence

## Summary
The `ExecutionProxy::sync_to_target` method in `consensus/src/state_computer.rs` unconditionally updates the logical time tracker even when state synchronization fails, violating the documented API contract and creating a window where subsequent operations use stale state while believing they are synchronized. This enables consensus safety violations where nodes diverge on committed state.

## Finding Description

The vulnerability exists in the `sync_to_target` implementation where logical time is updated before verifying the state sync operation succeeded. [1](#0-0) 

The code unconditionally updates `latest_logical_time` to `target_logical_time` at line 222, regardless of whether the `state_sync_notifier.sync_to_target(target)` call succeeded or failed. The result is only checked when returning at line 229.

This violates the documented API contract for `StateComputer::sync_to_target`: [2](#0-1) 

The contract explicitly states: "In case of failure (`Result::Error`) the LI of storage remains unchanged, and the validator can assume there were no modifications to the storage made."

**Attack Scenario:**

1. Node's actual committed state: epoch 5, round 100
2. Attacker triggers `sync_to_target` with target (epoch 5, round 200) that fails due to network partition, storage error, or corrupted data
3. Line 222 updates logical time to (5, 200) despite sync failure
4. Function returns error, but `write_mutex` now contains incorrect logical time (5, 200)
5. Later, legitimate `sync_to_target` called with target (epoch 5, round 150)
6. Early return logic checks: (5, 200) >= (5, 150) â†’ true [3](#0-2) 

7. Function returns `Ok()` without performing any state sync
8. **Consensus violation**: Node believes it's at round 150 but is actually still at round 100
9. Node participates in consensus with incorrect state, leading to state divergence and potential chain splits

This is called from critical consensus paths: [4](#0-3) [5](#0-4) 

Compare this to the correct implementation in `sync_for_duration`: [6](#0-5) 

Here, logical time is only updated if the result is `Ok`, maintaining the invariant that logical time reflects actual committed state.

## Impact Explanation

**Severity: CRITICAL** (Consensus/Safety Violation)

This vulnerability breaks **Consensus Safety** (Invariant #2) and **State Consistency** (Invariant #4):

1. **Consensus Safety Violation**: Nodes can have divergent committed states while participating in consensus, potentially leading to chain splits or acceptance of invalid blocks
2. **State Inconsistency**: The logical time tracker no longer accurately reflects committed state, causing incorrect behavior in the fast-forward sync and consensus observer paths
3. **Validator Participation with Incorrect State**: Affected validators will incorrectly believe they are synced and participate in voting/proposal with wrong state root
4. **Network-Wide Impact**: If multiple validators are affected, this could cause consensus failures requiring manual intervention or hard fork

Per Aptos Bug Bounty criteria, this qualifies as **Critical Severity** as it enables "Consensus/Safety violations" that can lead to "Non-recoverable network partition (requires hardfork)".

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability can be triggered by:

1. **Network Partitions**: Temporary network failures during state sync are common in distributed systems
2. **Storage Errors**: Disk full, corruption, or I/O errors during state sync commitment
3. **State Sync Service Failures**: The `state_sync_notifier` service experiencing internal errors
4. **Race Conditions**: Concurrent epoch transitions or reconfigurations during sync operations

The fail point injection at lines 207-209 confirms that errors during sync are expected and tested scenarios: [7](#0-6) 

Given that state sync failures are anticipated and the code paths are executed during critical consensus operations (fast-forward sync, consensus observer sync), this vulnerability has a high probability of manifestation in production environments.

## Recommendation

Update `sync_to_target` to only modify logical time upon successful state synchronization, matching the pattern used in `sync_for_duration`:

```rust
async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
    let mut latest_logical_time = self.write_mutex.lock().await;
    let target_logical_time =
        LogicalTime::new(target.ledger_info().epoch(), target.ledger_info().round());

    self.executor.finish();

    if *latest_logical_time >= target_logical_time {
        warn!(
            "State sync target {:?} is lower than already committed logical time {:?}",
            target_logical_time, *latest_logical_time
        );
        return Ok(());
    }

    if let Some(inner) = self.state.read().as_ref() {
        let block_timestamp = target.commit_info().timestamp_usecs();
        inner
            .payload_manager
            .notify_commit(block_timestamp, Vec::new());
    }

    fail_point!("consensus::sync_to_target", |_| {
        Err(anyhow::anyhow!("Injected error in sync_to_target").into())
    });

    let result = monitor!(
        "sync_to_target",
        self.state_sync_notifier.sync_to_target(target).await
    );

    // CRITICAL FIX: Only update logical time if sync succeeded
    if result.is_ok() {
        *latest_logical_time = target_logical_time;
    }

    self.executor.reset()?;

    result.map_err(|error| {
        let anyhow_error: anyhow::Error = error.into();
        anyhow_error.into()
    })
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_logical_time_corruption_on_sync_failure() {
    use consensus::state_computer::ExecutionProxy;
    use aptos_types::ledger_info::{LedgerInfo, LedgerInfoWithSignatures};
    use aptos_consensus_types::common::Round;
    
    // Setup: Create ExecutionProxy with mocked dependencies
    let executor = Arc::new(MockBlockExecutor::new());
    let txn_notifier = Arc::new(MockTxnNotifier::new());
    
    // Create a state sync notifier that will fail on sync_to_target calls
    let failing_sync_notifier = Arc::new(FailingStateSyncNotifier::new());
    
    let execution_proxy = ExecutionProxy::new(
        executor,
        txn_notifier,
        failing_sync_notifier,
        BlockTransactionFilterConfig::default(),
        false,
        None,
    );
    
    // Step 1: Create target for epoch 1, round 200 (high round)
    let high_target = create_ledger_info_with_sigs(1, 200);
    
    // Step 2: Attempt sync - this will FAIL but should NOT update logical time
    let result = execution_proxy.sync_to_target(high_target).await;
    assert!(result.is_err(), "Sync should fail");
    
    // Step 3: Create legitimate target for epoch 1, round 150 (lower round)
    let low_target = create_ledger_info_with_sigs(1, 150);
    
    // Step 4: Attempt sync to lower round
    // BUG: This will return Ok() without syncing because logical time was corrupted
    let result = execution_proxy.sync_to_target(low_target).await;
    
    // VULNERABILITY: The function returns Ok() claiming sync succeeded,
    // but actual state is still at round 0, not round 150
    assert!(result.is_ok(), "Sync appears to succeed");
    
    // Verify that the node's actual committed state is still at round 0
    let actual_committed_round = executor.get_latest_committed_round();
    assert_eq!(actual_committed_round, 0, "State should still be at round 0");
    
    // Node now participates in consensus believing it's at round 150
    // but actually at round 0 - CONSENSUS SAFETY VIOLATION
}

// Helper to create mock LedgerInfoWithSignatures
fn create_ledger_info_with_sigs(epoch: u64, round: Round) -> LedgerInfoWithSignatures {
    let ledger_info = LedgerInfo::new(
        BlockInfo::empty(),
        HashValue::zero(),
    );
    LedgerInfoWithSignatures::new(ledger_info, AggregateSignature::empty())
}
```

This PoC demonstrates that after a failed `sync_to_target` to a high round, subsequent syncs to lower rounds incorrectly succeed without actually syncing, leaving the node with stale state while it believes it is synchronized.

### Citations

**File:** consensus/src/state_computer.rs (L158-163)
```rust
        // Update the latest logical time
        if let Ok(latest_synced_ledger_info) = &result {
            let ledger_info = latest_synced_ledger_info.ledger_info();
            let synced_logical_time = LogicalTime::new(ledger_info.epoch(), ledger_info.round());
            *latest_logical_time = synced_logical_time;
        }
```

**File:** consensus/src/state_computer.rs (L188-194)
```rust
        if *latest_logical_time >= target_logical_time {
            warn!(
                "State sync target {:?} is lower than already committed logical time {:?}",
                target_logical_time, *latest_logical_time
            );
            return Ok(());
        }
```

**File:** consensus/src/state_computer.rs (L207-209)
```rust
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });
```

**File:** consensus/src/state_computer.rs (L216-222)
```rust
        let result = monitor!(
            "sync_to_target",
            self.state_sync_notifier.sync_to_target(target).await
        );

        // Update the latest logical time
        *latest_logical_time = target_logical_time;
```

**File:** consensus/src/state_replication.rs (L33-37)
```rust
    /// Best effort state synchronization to the given target LedgerInfo.
    /// In case of success (`Result::Ok`) the LI of storage is at the given target.
    /// In case of failure (`Result::Error`) the LI of storage remains unchanged, and the validator
    /// can assume there were no modifications to the storage made.
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError>;
```

**File:** consensus/src/block_storage/sync_manager.rs (L512-514)
```rust
        execution_client
            .sync_to_target(highest_commit_cert.ledger_info().clone())
            .await?;
```

**File:** consensus/src/consensus_observer/observer/state_sync_manager.rs (L219-222)
```rust
                if let Err(error) = execution_client
                    .clone()
                    .sync_to_target(commit_decision.commit_proof().clone())
                    .await
```
