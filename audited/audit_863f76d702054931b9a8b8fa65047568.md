# Audit Report

## Title
Consensus Error Metrics Lack Error-Type Labels, Enabling Critical SafetyRules Failures to be Masked by High-Volume Benign Errors

## Summary
The consensus layer's `ERROR_COUNT` metric is an unlabeled `IntGauge` that aggregates all error types without distinction. While the `error_kind()` function correctly categorizes errors (SafetyRules, Execution, Mempool, etc.), this categorization is only used for logging, not for metrics. This creates a monitoring blind spot where critical SafetyRules violations can be hidden by high volumes of benign errors (e.g., mempool rejections), potentially causing operators to miss consensus safety issues. [1](#0-0) [2](#0-1) 

## Finding Description

The consensus system implements proper error categorization through the `error_kind()` function, which distinguishes between:
- "SafetyRules" - Critical consensus safety violations
- "Execution" - ExecutorError failures  
- "Mempool" - Transaction submission issues
- "QuorumStore" - Batch formation errors
- "ConsensusDb" - Database errors
- "StateSync" - State synchronization errors
- "VerifyError" - Verification failures
- "InternalError" - Catch-all category

However, when errors occur in the RoundManager's main event loop, the system only increments a single unlabeled `ERROR_COUNT` metric while logging the error_kind: [3](#0-2) [4](#0-3) 

The monitoring infrastructure has significant gaps:

1. **Alert System** - The "High consensus error rate" alert only triggers when the aggregate error rate exceeds 25%, without distinguishing error severity: [5](#0-4) 

2. **Dashboard Visualization** - The consensus dashboard shows aggregate error rate without error-type breakdown: [6](#0-5) 

3. **No SafetyRules-Specific Alerts** - Despite SafetyRules having its own labeled metrics (`aptos_safety_rules_queries` with method/result labels), there are no consensus-layer alerts specifically for SafetyRules errors.

**Attack Scenario:**

1. An attacker floods the network with invalid transactions or malformed batch requests
2. This generates high volumes of Mempool/QuorumStore errors that increment `ERROR_COUNT`
3. The baseline error rate increases, potentially triggering the "High consensus error rate" alert
4. Operators investigate and see mostly benign Mempool/QuorumStore errors in logs
5. They dismiss the alert as "network noise" or tune the threshold higher
6. Meanwhile, a critical SafetyRules error occurs (e.g., `NotSafeToVote`, `InconsistentExecutionResult`)
7. This SafetyRules error also increments the same `ERROR_COUNT` without distinction
8. The critical consensus safety violation is masked by the noise and goes undetected
9. Operators don't realize a validator is attempting to violate consensus rules

SafetyRules errors like `NotSafeToVote` and `InconsistentExecutionResult` indicate serious consensus safety violations: [7](#0-6) 

## Impact Explanation

This issue is classified as **High Severity** under the Aptos Bug Bounty program's "Significant protocol violations" category, though it represents an indirect rather than direct threat.

**Impact:**
- **Delayed Detection of Consensus Failures**: Critical SafetyRules errors (voting rule violations, inconsistent execution) may go unnoticed for extended periods
- **Alert Fatigue**: High baseline error rates from benign issues reduce operator responsiveness to genuine consensus problems  
- **Masked Safety Violations**: Consensus safety rule violations can be drowned out by transaction-level errors
- **Operational Risk**: Operators may tune down alert sensitivity after repeated false positives, reducing effectiveness of the monitoring system

**Why Not Critical:**
- The underlying consensus protocol continues to function correctly (SafetyRules prevents unsafe actions)
- SafetyRules module has its own properly-labeled metrics system
- Operators can still detect issues through SafetyRules-specific dashboards and log analysis
- No direct funds loss, consensus split, or network partition results from this issue alone

**Why High Severity:**
- Enables critical consensus failures to persist undetected longer than necessary
- Reduces the effectiveness of the operational monitoring that protects consensus safety
- In combination with other issues, could delay response to actual consensus attacks

## Likelihood Explanation

**Likelihood: Medium to High**

This issue is likely to manifest in production because:

1. **Common Trigger Conditions**: Network congestion, invalid transaction floods, and mempool pressure are common occurrences that generate high error volumes
2. **No Specialized Training Required**: Operators don't need special knowledge about the error categorization gap
3. **Natural Alert Fatigue**: The generic "High consensus error rate" alert will trigger on benign issues, leading to desensitization
4. **Stale Proposals Generate Noise**: Even normal operation generates ERROR_COUNT increments for stale proposals: [8](#0-7) 

5. **Existing Infrastructure Gap**: The monitoring system already lacks SafetyRules-specific alerts at the consensus level, indicating this is a current operational state, not a theoretical concern

## Recommendation

**Immediate Fix: Add Error-Type Labels to Consensus ERROR_COUNT Metric**

Modify the `ERROR_COUNT` metric to be an `IntCounterVec` with an "error_kind" label:

```rust
// In consensus/src/counters.rs
pub static ERROR_COUNT: Lazy<IntCounterVec> = Lazy::new(|| {
    register_int_counter_vec!(
        "aptos_consensus_error_count",
        "Total number of errors in main loop by error kind",
        &["error_kind"]
    )
    .unwrap()
});
```

Update error handling in round_manager.rs to use labeled metrics:

```rust
// In consensus/src/round_manager.rs
match result {
    Ok(_) => trace!(RoundStateLogSchema::new(round_state)),
    Err(e) => {
        let kind = error_kind(&e);
        counters::ERROR_COUNT.with_label_values(&[kind]).inc();
        warn!(kind = kind, RoundStateLogSchema::new(round_state), "Error: {:#}", e);
    }
}
```

**Additional Recommendations:**

1. **Add SafetyRules-Specific Alert**: Create a separate alert for SafetyRules errors at the consensus level:
```yaml
    - alert: Critical SafetyRules Error
  expr: rate(aptos_consensus_error_count{error_kind="SafetyRules"}[1m]) > 0
  for: 1m
  labels:
    severity: critical
  annotations:
    summary: "Critical consensus safety violation detected"
```

2. **Update Dashboards**: Modify consensus dashboard to show error breakdown by type, with SafetyRules errors prominently displayed

3. **Separate Benign from Critical Errors**: Consider not incrementing ERROR_COUNT for expected conditions like stale proposals

4. **Documentation**: Update operator runbooks to emphasize the criticality of SafetyRules errors vs. other error types

## Proof of Concept

This PoC demonstrates how the unlabeled metric treats all error types identically:

```rust
// Test in consensus/src/round_manager.rs (add to tests module)
#[tokio::test]
async fn test_error_count_lacks_labels() {
    use crate::counters::ERROR_COUNT;
    use crate::error::{MempoolError, error_kind};
    use aptos_safety_rules::Error as SafetyRulesError;
    
    let initial_count = ERROR_COUNT.get();
    
    // Simulate a benign Mempool error
    let mempool_err = MempoolError { inner: anyhow::anyhow!("transaction rejected") };
    let mempool_anyhow: anyhow::Error = mempool_err.into();
    assert_eq!(error_kind(&mempool_anyhow), "Mempool");
    ERROR_COUNT.inc(); // No label - treated same as critical errors
    
    // Simulate a critical SafetyRules error  
    let safety_err: anyhow::Error = SafetyRulesError::NotSafeToVote(10, 5, 3, 2).into();
    assert_eq!(error_kind(&safety_err), "SafetyRules");
    ERROR_COUNT.inc(); // No label - indistinguishable from benign errors
    
    // Both errors increment the same counter
    assert_eq!(ERROR_COUNT.get(), initial_count + 2);
    
    // Operators cannot distinguish between the two error types in metrics
    // A dashboard query for ERROR_COUNT will show "2 errors" without context
    // The critical SafetyRules error is hidden in the aggregate
}
```

To observe in a running system:
1. Generate high volumes of invalid transactions to trigger Mempool errors
2. Simultaneously cause a SafetyRules error (e.g., by manipulating validator state)
3. Query `rate(aptos_consensus_error_count[1m])` - shows aggregate rate only
4. Check logs - error_kind is visible but requires manual log analysis
5. The SafetyRules error is lost in the noise without specific metric labeling

## Notes

**Compensating Controls:**
- SafetyRules module has its own labeled metrics (`aptos_safety_rules_queries`)
- SafetyRules-specific dashboards exist for detailed monitoring
- Logs contain error_kind information for forensic analysis

**Why This Still Matters:**
- Consensus-level monitoring is the primary operational interface
- Operators shouldn't need to manually correlate logs with multiple dashboards during incidents
- Defense-in-depth requires proper alerting at all layers
- The error categorization infrastructure exists (`error_kind()`) but isn't utilized in metrics

**Related Code Locations:**
- Error handling in epoch_manager.rs also uses unlabeled logging
- Recovery_manager.rs and quorum_store_builder.rs have similar patterns
- All increment ERROR_COUNT or log with error_kind, but don't use labeled metrics at consensus level

### Citations

**File:** consensus/src/error.rs (L60-91)
```rust
pub fn error_kind(e: &anyhow::Error) -> &'static str {
    if e.downcast_ref::<aptos_executor_types::ExecutorError>()
        .is_some()
    {
        return "Execution";
    }
    if let Some(e) = e.downcast_ref::<StateSyncError>() {
        if e.inner
            .downcast_ref::<aptos_executor_types::ExecutorError>()
            .is_some()
        {
            return "Execution";
        }
        return "StateSync";
    }
    if e.downcast_ref::<MempoolError>().is_some() {
        return "Mempool";
    }
    if e.downcast_ref::<QuorumStoreError>().is_some() {
        return "QuorumStore";
    }
    if e.downcast_ref::<DbError>().is_some() {
        return "ConsensusDb";
    }
    if e.downcast_ref::<aptos_safety_rules::Error>().is_some() {
        return "SafetyRules";
    }
    if e.downcast_ref::<VerifyError>().is_some() {
        return "VerifyError";
    }
    "InternalError"
}
```

**File:** consensus/src/counters.rs (L69-76)
```rust
/// Counts the total number of errors
pub static ERROR_COUNT: Lazy<IntGauge> = Lazy::new(|| {
    register_int_gauge!(
        "aptos_consensus_error_count",
        "Total number of errors in main loop"
    )
    .unwrap()
});
```

**File:** consensus/src/round_manager.rs (L755-764)
```rust
                SampleRate::Duration(Duration::from_secs(30)),
                warn!(
                    "[sampled] Stale proposal {}, current round {}",
                    proposal_msg.proposal(),
                    self.round_state.current_round()
                )
            );
            counters::ERROR_COUNT.inc();
            Ok(())
        }
```

**File:** consensus/src/round_manager.rs (L2136-2142)
```rust
                        match result {
                            Ok(_) => trace!(RoundStateLogSchema::new(round_state)),
                            Err(e) => {
                                counters::ERROR_COUNT.inc();
                                warn!(kind = error_kind(&e), RoundStateLogSchema::new(round_state), "Error: {:#}", e);
                            }
                        }
```

**File:** consensus/src/round_manager.rs (L2187-2193)
```rust
                    match result {
                        Ok(_) => trace!(RoundStateLogSchema::new(round_state)),
                        Err(e) => {
                            counters::ERROR_COUNT.inc();
                            warn!(kind = error_kind(&e), RoundStateLogSchema::new(round_state), "Error: {:#}", e);
                        }
                    }
```

**File:** terraform/helm/monitoring/files/rules/alerts.yml (L20-26)
```yaml
  - alert: High consensus error rate
    expr: rate(aptos_consensus_error_count{role="validator"}[1m]) / on (role) rate(consensus_duration_count{op='main_loop', role="validator"}[1m]) > 0.25
    for: 20m
    labels:
      severity: warning
      summary: "Consensus error rate is high"
    annotations:
```

**File:** dashboards/consensus.json (L494-505)
```json
          "datasource": { "type": "prometheus", "uid": "${Datasource}" },
          "editorMode": "code",
          "expr": "rate(aptos_consensus_error_count{chain_name=~\"$chain_name\", cluster=~\"$cluster\", metrics_source=~\"$metrics_source\", namespace=~\"$namespace\", kubernetes_pod_name=~\"$kubernetes_pod_name\", role=~\"$role\"}[$interval])",
          "format": "time_series",
          "intervalFactor": 1,
          "legendFormat": "{{kubernetes_pod_name}}-{{state}}",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Round manager error rate",
      "type": "timeseries"
```

**File:** consensus/safety-rules/src/error.rs (L47-54)
```rust
    #[error("Does not satisfy 2-chain voting rule. Round {0}, Quorum round {1}, TC round {2},  HQC round in TC {3}")]
    NotSafeToVote(u64, u64, u64, u64),
    #[error("Does not satisfy 2-chain timeout rule. Round {0}, Quorum round {1}, TC round {2}, one-chain round {3}")]
    NotSafeToTimeout(u64, u64, u64, u64),
    #[error("Invalid TC: {0}")]
    InvalidTimeoutCertificate(String),
    #[error("Inconsistent Execution Result: Ordered BlockInfo doesn't match executed BlockInfo. Ordered: {0}, Executed: {1}")]
    InconsistentExecutionResult(String, String),
```
