# Audit Report

## Title
Timestamp Tampering in Peer Monitoring Allows Health Check Bypass via Future ledger_timestamp_usecs

## Summary
The peer monitoring service accepts `NodeInformationResponse` messages from peers without validating that `ledger_timestamp_usecs` is reasonable. Malicious peers can send far-future timestamps to bypass mempool health checks due to the use of `saturating_sub`, causing them to be incorrectly prioritized for transaction broadcasting and leading to network performance degradation.

## Finding Description

The vulnerability exists in the peer monitoring service's handling of `NodeInformationResponse` messages. When a peer sends node information, the `ledger_timestamp_usecs` field is accepted without any validation to check if the timestamp is in the future or unreasonably far from the current time. [1](#0-0) 

The unvalidated timestamp is stored and later used by the mempool's intelligent peer prioritization system to determine peer health. The health check compares the peer's ledger timestamp against the current time: [2](#0-1) 

The critical flaw is in the health check logic: `current_timestamp_usecs.saturating_sub(peer_ledger_timestamp_usecs) < max_sync_lag_usecs`. When `peer_ledger_timestamp_usecs` is far in the future (e.g., year 2100), `saturating_sub` returns 0 (preventing underflow), making the expression `0 < max_sync_lag_usecs` always evaluate to true. This causes the malicious peer to be incorrectly classified as healthy.

**Attack Path:**
1. Malicious peer sends `GetNodeInformation` response with `ledger_timestamp_usecs` set to a far-future value (e.g., `9999999999999999`)
2. The timestamp is stored without validation in `NodeInfoState`
3. The timestamp propagates to `PeerMonitoringMetadata.latest_node_info_response`
4. Mempool's `check_peer_metadata_health` evaluates the peer as healthy due to saturating subtraction
5. The malicious peer is prioritized first in the intelligent peer prioritization hierarchy [3](#0-2) 

The health check is the first comparison criterion in peer prioritization, meaning unhealthy peers with far-future timestamps will be prioritized over genuinely healthy peers.

## Impact Explanation

This is a **Medium Severity** vulnerability per Aptos bug bounty criteria, as it causes:

1. **State inconsistencies requiring intervention**: Malicious peers bypass health checks and are incorrectly prioritized
2. **Network performance degradation**: Transactions are preferentially broadcast to peers reporting invalid sync states
3. **Transaction propagation failures**: If malicious peers don't properly process transactions, network-wide transaction confirmation is delayed

The vulnerability affects mempool transaction broadcasting across all node types. While not causing direct fund loss or consensus violations, it enables malicious actors to degrade network performance without detection through normal health monitoring.

## Likelihood Explanation

**Likelihood: High**

- **No special privileges required**: Any peer can send malformed `NodeInformationResponse` messages
- **Trivial to exploit**: Simply set `ledger_timestamp_usecs` to a large future value
- **No cryptographic validation**: Unlike `LedgerInfoWithSignatures`, peer monitoring responses are not cryptographically signed
- **Persistent effect**: Once a peer sends a future timestamp, it remains prioritized until new responses are received
- **Multiple exploitation vectors**: Affects both mempool prioritization and potentially state-sync operations

The attack requires no validator access, no stake, and minimal technical sophistication.

## Recommendation

Add timestamp validation when receiving `NodeInformationResponse` to reject timestamps that are unreasonably far in the future or past. Implement bounds checking:

```rust
fn handle_monitoring_service_response(
    &mut self,
    peer_network_id: &PeerNetworkId,
    peer_metadata: PeerMetadata,
    monitoring_service_request: PeerMonitoringServiceRequest,
    monitoring_service_response: PeerMonitoringServiceResponse,
    response_time_secs: f64,
) {
    // Verify the response type is valid
    let node_info_response = match monitoring_service_response {
        PeerMonitoringServiceResponse::NodeInformation(node_information_response) => {
            node_information_response
        },
        _ => {
            warn!(LogSchema::new(LogEntry::NodeInfoRequest)...);
            self.handle_request_failure();
            return;
        },
    };

    // VALIDATION: Check if timestamp is reasonable
    let current_timestamp_usecs = self.time_service.now_unix_time().as_micros() as u64;
    let max_acceptable_future_drift_secs = 300; // 5 minutes tolerance
    let max_acceptable_future_drift_usecs = max_acceptable_future_drift_secs * 1_000_000;
    
    if node_info_response.ledger_timestamp_usecs > current_timestamp_usecs + max_acceptable_future_drift_usecs {
        warn!(LogSchema::new(LogEntry::NodeInfoRequest)
            .event(LogEvent::ResponseError)
            .peer(peer_network_id)
            .message("Received node info response with timestamp too far in the future"));
        self.handle_request_failure();
        return;
    }

    // Store the validated response
    self.record_node_info_response(node_info_response);
}
```

Additionally, fix the health check to properly detect future timestamps:

```rust
fn check_peer_metadata_health(
    mempool_config: &MempoolConfig,
    time_service: &TimeService,
    monitoring_metadata: &Option<&PeerMonitoringMetadata>,
) -> bool {
    monitoring_metadata
        .and_then(|metadata| {
            metadata.latest_node_info_response.as_ref().map(|node_information_response| {
                let peer_ledger_timestamp_usecs = node_information_response.ledger_timestamp_usecs;
                let current_timestamp_usecs = get_timestamp_now_usecs(time_service);
                let max_sync_lag_usecs = mempool_config.max_sync_lag_before_unhealthy_secs as u64 * MICROS_PER_SECOND;
                
                // Check timestamp is not in the future (with small tolerance)
                let max_future_drift_usecs = 60 * MICROS_PER_SECOND; // 60 seconds tolerance
                if peer_ledger_timestamp_usecs > current_timestamp_usecs + max_future_drift_usecs {
                    return false; // Timestamp is too far in the future
                }
                
                // Check timestamp is not too far in the past
                current_timestamp_usecs.saturating_sub(peer_ledger_timestamp_usecs) < max_sync_lag_usecs
            })
        })
        .unwrap_or(false)
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use aptos_config::config::MempoolConfig;
    use aptos_peer_monitoring_service_types::{
        response::NodeInformationResponse,
        PeerMonitoringMetadata,
    };
    use aptos_time_service::TimeService;
    use std::collections::BTreeMap;
    use std::time::Duration;

    #[test]
    fn test_future_timestamp_bypass() {
        // Create a mempool config with a max sync lag of 30 seconds
        let mempool_config = MempoolConfig {
            max_sync_lag_before_unhealthy_secs: 30,
            ..MempoolConfig::default()
        };

        // Create a mock time service
        let time_service = TimeService::mock();
        let current_time_usecs = time_service.now_unix_time().as_micros() as u64;

        // Create monitoring metadata with a far-future timestamp (year 2100)
        let future_timestamp_usecs = 4102444800000000u64; // Jan 1, 2100
        let malicious_metadata = PeerMonitoringMetadata {
            average_ping_latency_secs: Some(0.01),
            latest_ping_latency_secs: Some(0.01),
            latest_network_info_response: None,
            latest_node_info_response: Some(NodeInformationResponse {
                build_information: BTreeMap::new(),
                highest_synced_epoch: 100,
                highest_synced_version: 1000,
                ledger_timestamp_usecs: future_timestamp_usecs,
                lowest_available_version: 0,
                uptime: Duration::from_secs(3600),
            }),
            internal_client_state: None,
        };

        // Create legitimate metadata with current timestamp
        let legitimate_metadata = PeerMonitoringMetadata {
            average_ping_latency_secs: Some(0.01),
            latest_ping_latency_secs: Some(0.01),
            latest_network_info_response: None,
            latest_node_info_response: Some(NodeInformationResponse {
                build_information: BTreeMap::new(),
                highest_synced_epoch: 100,
                highest_synced_version: 1000,
                ledger_timestamp_usecs: current_time_usecs,
                lowest_available_version: 0,
                uptime: Duration::from_secs(3600),
            }),
            internal_client_state: None,
        };

        // Check health status - the malicious peer with future timestamp passes!
        let malicious_is_healthy = check_peer_metadata_health(
            &mempool_config,
            &time_service,
            &Some(&malicious_metadata),
        );
        let legitimate_is_healthy = check_peer_metadata_health(
            &mempool_config,
            &time_service,
            &Some(&legitimate_metadata),
        );

        // VULNERABILITY DEMONSTRATED:
        // Both peers are marked as healthy, despite the malicious peer having
        // a timestamp from the year 2100!
        assert!(malicious_is_healthy, "Malicious peer with future timestamp incorrectly marked as healthy");
        assert!(legitimate_is_healthy, "Legitimate peer should be healthy");

        println!("VULNERABILITY: Peer with timestamp in year 2100 bypassed health check!");
        println!("Current time: {}", current_time_usecs);
        println!("Malicious peer timestamp: {}", future_timestamp_usecs);
        println!("Difference: {} seconds", (future_timestamp_usecs - current_time_usecs) / 1_000_000);
    }
}
```

## Notes

The vulnerability affects the peer monitoring service integration with mempool's intelligent peer prioritization system. While the immediate impact is network performance degradation rather than fund loss, it represents a significant protocol violation that allows malicious actors to subvert health-based peer selection mechanisms. The fix requires validation at the point of receipt (peer monitoring client) and defensive checks in all consumers of timestamp data (mempool, state-sync).

### Citations

**File:** peer-monitoring-service/client/src/peer_states/node_info.rs (L79-106)
```rust
    fn handle_monitoring_service_response(
        &mut self,
        peer_network_id: &PeerNetworkId,
        _peer_metadata: PeerMetadata,
        _monitoring_service_request: PeerMonitoringServiceRequest,
        monitoring_service_response: PeerMonitoringServiceResponse,
        _response_time_secs: f64,
    ) {
        // Verify the response type is valid
        let node_info_response = match monitoring_service_response {
            PeerMonitoringServiceResponse::NodeInformation(node_information_response) => {
                node_information_response
            },
            _ => {
                warn!(LogSchema::new(LogEntry::NodeInfoRequest)
                    .event(LogEvent::ResponseError)
                    .peer(peer_network_id)
                    .message(
                        "An unexpected response was received instead of a node info response!"
                    ));
                self.handle_request_failure();
                return;
            },
        };

        // Store the new latency ping result
        self.record_node_info_response(node_info_response);
    }
```

**File:** mempool/src/shared_mempool/priority.rs (L74-92)
```rust
    fn compare_intelligent(
        &self,
        peer_a: &(PeerNetworkId, Option<&PeerMonitoringMetadata>),
        peer_b: &(PeerNetworkId, Option<&PeerMonitoringMetadata>),
    ) -> Ordering {
        // Deconstruct the peer tuples
        let (peer_network_id_a, monitoring_metadata_a) = peer_a;
        let (peer_network_id_b, monitoring_metadata_b) = peer_b;

        // First, compare the peers by health (e.g., sync lag)
        let unhealthy_ordering = compare_peer_health(
            &self.mempool_config,
            &self.time_service,
            monitoring_metadata_a,
            monitoring_metadata_b,
        );
        if !unhealthy_ordering.is_eq() {
            return unhealthy_ordering; // Only return if it's not equal
        }
```

**File:** mempool/src/shared_mempool/priority.rs (L562-589)
```rust
fn check_peer_metadata_health(
    mempool_config: &MempoolConfig,
    time_service: &TimeService,
    monitoring_metadata: &Option<&PeerMonitoringMetadata>,
) -> bool {
    monitoring_metadata
        .and_then(|metadata| {
            metadata
                .latest_node_info_response
                .as_ref()
                .map(|node_information_response| {
                    // Get the peer's ledger timestamp and the current timestamp
                    let peer_ledger_timestamp_usecs =
                        node_information_response.ledger_timestamp_usecs;
                    let current_timestamp_usecs = get_timestamp_now_usecs(time_service);

                    // Calculate the max sync lag before the peer is considered unhealthy (in microseconds)
                    let max_sync_lag_secs =
                        mempool_config.max_sync_lag_before_unhealthy_secs as u64;
                    let max_sync_lag_usecs = max_sync_lag_secs * MICROS_PER_SECOND;

                    // Determine if the peer is healthy
                    current_timestamp_usecs.saturating_sub(peer_ledger_timestamp_usecs)
                        < max_sync_lag_usecs
                })
        })
        .unwrap_or(false) // If metadata is missing, consider the peer unhealthy
}
```
