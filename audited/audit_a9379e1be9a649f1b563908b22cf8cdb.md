# Audit Report

## Title
Certified Timestamp Divergence Causes Premature Batch Expiration and Block Execution Failures

## Summary
A logic flaw in the Quorum Store batch request-response protocol causes validators with divergent certified timestamps to incorrectly fail block execution. When a validator behind in sync requests a batch from a validator ahead in sync, the requester short-circuits based on the responder's certified timestamp rather than the block's timestamp, causing block execution failures for batches that are still valid for the block being processed.

## Finding Description

The vulnerability exists in the batch retrieval mechanism between validators in the consensus layer. When a validator needs to execute a block referencing a batch it doesn't have locally, it requests the batch from other validators. The critical flaw occurs in how batch expiration is evaluated during this request-response flow.

**The Flawed Logic:**

When a batch is not found locally, responders return their current ledger info: [1](#0-0) 

The requester then evaluates whether to short-circuit the request based on the responder's timestamp: [2](#0-1) 

**Why This Is Wrong:**

The system correctly filters batches BEFORE requesting them based on the block's timestamp: [3](#0-2) 

This means if `request_batch` is called, we know `block_timestamp < batch_expiration`, so the batch is valid for the block being executed. However, the short-circuit logic checks `responder_ledger_info.timestamp > batch_expiration` instead of checking whether the batch is expired relative to the block's timestamp.

**Attack Scenario:**

1. Network latency causes validator certified timestamp divergence
2. Validator A (ahead): `certified_time = 150μs`  
3. Validator B (behind): `certified_time = 50μs`
4. Batch exists with `expiration = 100μs`
5. Validator A expires and deletes the batch: [4](#0-3) 

6. Validator B receives block proposal with `timestamp = 90μs` referencing this batch
7. Validator B correctly identifies batch as valid (90 < 100) and attempts to fetch it
8. Validator B requests from Validator A
9. Validator A responds with `NotFound(ledger_info)` where `ledger_info.timestamp = 150`
10. Validator B checks: `150 > 100` → short-circuits immediately
11. Block execution fails with `ExecutorError::CouldNotGetData`
12. Validator cannot vote (must execute before voting per consensus rules)

The certified timestamp is atomically updated when blocks commit: [5](#0-4) 

And expired batches are rejected when saving: [6](#0-5) 

## Impact Explanation

**HIGH Severity** per Aptos bug bounty criteria:

1. **Validator Node Slowdowns**: Validators with lower certified timestamps cannot execute blocks containing batches that have been expired by validators ahead in sync, causing them to fall further behind and creating a cascading effect.

2. **Protocol Violation**: Breaks the fundamental invariant that all honest validators must be able to deterministically execute the same valid blocks. A block with `timestamp T` referencing a batch with `expiration E` where `T < E` should be executable by all validators regardless of their individual certified timestamps.

3. **Liveness Impact**: If a sufficient number of validators are behind during network partition or high latency conditions, they cannot vote on blocks, potentially preventing quorum formation and halting consensus progress.

4. **Asymmetric Execution**: Different validators succeed or fail at executing identical blocks based solely on their certified timestamp relative to other validators, not based on the block's actual validity.

This qualifies as **"Validator node slowdowns"** under the HIGH severity category, with potential escalation to CRITICAL if it causes sustained liveness failures requiring manual intervention.

## Likelihood Explanation

**HIGH Likelihood:**

- **Common Occurrence**: Network latency causing validator timestamp divergence is a normal operating condition in distributed systems
- **No Malicious Behavior Required**: Happens naturally through network partitions, validator restarts, or sync lag
- **Frequent Trigger Conditions**:
  - Validators catching up after being offline
  - Network latency spikes during high load
  - Partition recovery scenarios
  - Geographic distribution of validators
- **Observable in Tests**: The test suite confirms this short-circuit behavior is intentional but doesn't account for the scenario where `block_timestamp < expiration < responder_timestamp`: [7](#0-6) 

## Recommendation

Modify the `request_batch` method to accept the `block_timestamp` parameter and check batch expiration relative to the block being executed, not the responder's current time:

```rust
pub(crate) async fn request_batch(
    &self,
    digest: HashValue,
    expiration: u64,
    block_timestamp: u64,  // ADD THIS PARAMETER
    responders: Arc<Mutex<BTreeSet<PeerId>>>,
    mut subscriber_rx: oneshot::Receiver<PersistedValue<BatchInfoExt>>,
) -> ExecutorResult<Vec<SignedTransaction>> {
    // ... existing code ...
    
    Ok(BatchResponse::NotFound(ledger_info)) => {
        counters::RECEIVED_BATCH_NOT_FOUND_COUNT.inc();
        // CHANGE THIS CONDITION:
        if ledger_info.commit_info().epoch() == epoch
            && ledger_info.commit_info().timestamp_usecs() > block_timestamp  // Use block_timestamp
            && ledger_info.verify_signatures(&validator_verifier).is_ok()
        {
            counters::RECEIVED_BATCH_EXPIRED_COUNT.inc();
            debug!("QS: batch request expired for this block, digest:{}", digest);
            return Err(ExecutorError::CouldNotGetData);
        }
    }
}
```

Update the call sites in `batch_store.rs` to pass `block_timestamp` through the call chain from `quorum_store_payload_manager.rs` where it's already available.

## Proof of Concept

A PoC would require a multi-validator test environment simulating:

1. Set up 4 validators with Quorum Store enabled
2. Create a batch with expiration T
3. Advance Validator A's certified_time to T+50 (batch expires and gets deleted)
4. Keep Validator B's certified_time at T-40
5. Propose block with timestamp T-10 containing the batch
6. Observe Validator B request batch from Validator A
7. Verify Validator B short-circuits and fails block execution despite batch being valid for the block's timestamp
8. Confirm Validator B cannot vote on the block

This can be implemented as a Rust integration test in `consensus/src/quorum_store/tests/` using the existing test infrastructure and MockBatchRequester pattern.

**Notes**

The vulnerability is confirmed through direct code analysis showing the incorrect timestamp comparison logic. The short-circuit optimization was likely intended to avoid requesting batches that have globally expired, but it fails to account for the case where validators have divergent certified timestamps. The fix requires passing block context through the batch request chain to enable correct expiration evaluation relative to the block being executed rather than the responder's current state.

### Citations

**File:** consensus/src/quorum_store/quorum_store_builder.rs (L417-418)
```rust
                    match aptos_db_clone.get_latest_ledger_info() {
                        Ok(ledger_info) => BatchResponse::NotFound(ledger_info),
```

**File:** consensus/src/quorum_store/batch_requester.rs (L142-151)
```rust
                            Ok(BatchResponse::NotFound(ledger_info)) => {
                                counters::RECEIVED_BATCH_NOT_FOUND_COUNT.inc();
                                if ledger_info.commit_info().epoch() == epoch
                                    && ledger_info.commit_info().timestamp_usecs() > expiration
                                    && ledger_info.verify_signatures(&validator_verifier).is_ok()
                                {
                                    counters::RECEIVED_BATCH_EXPIRED_COUNT.inc();
                                    debug!("QS: batch request expired, digest:{}", digest);
                                    return Err(ExecutorError::CouldNotGetData);
                                }
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L102-106)
```rust
            if block_timestamp <= batch_info.expiration() {
                futures.push(batch_reader.get_batch(batch_info, responders.clone()));
            } else {
                debug!("QSE: skipped expired batch {}", batch_info.digest());
            }
```

**File:** consensus/src/quorum_store/batch_store.rs (L419-438)
```rust
    pub(crate) fn save(&self, value: &PersistedValue<BatchInfoExt>) -> anyhow::Result<bool> {
        let last_certified_time = self.last_certified_time();
        if value.expiration() > last_certified_time {
            fail_point!("quorum_store::save", |_| {
                // Skip caching and storing value to the db
                Ok(false)
            });
            counters::GAP_BETWEEN_BATCH_EXPIRATION_AND_CURRENT_TIME_WHEN_SAVE.observe(
                Duration::from_micros(value.expiration() - last_certified_time).as_secs_f64(),
            );

            return self.insert_to_cache(value);
        }
        counters::NUM_BATCH_EXPIRED_WHEN_SAVE.inc();
        bail!(
            "Incorrect expiration {} in epoch {}, last committed timestamp {}",
            value.expiration(),
            self.epoch(),
            last_certified_time,
        );
```

**File:** consensus/src/quorum_store/batch_store.rs (L443-472)
```rust
    pub(crate) fn clear_expired_payload(&self, certified_time: u64) -> Vec<HashValue> {
        // To help slow nodes catch up via execution without going to state sync we keep the blocks for 60 extra seconds
        // after the expiration time. This will help remote peers fetch batches that just expired but are within their
        // execution window.
        let expiration_time = certified_time.saturating_sub(self.expiration_buffer_usecs);
        let expired_digests = self.expirations.lock().expire(expiration_time);
        let mut ret = Vec::new();
        for h in expired_digests {
            let removed_value = match self.db_cache.entry(h) {
                Occupied(entry) => {
                    // We need to check up-to-date expiration again because receiving the same
                    // digest with a higher expiration would update the persisted value and
                    // effectively extend the expiration.
                    if entry.get().expiration() <= expiration_time {
                        self.persist_subscribers.remove(entry.get().digest());
                        Some(entry.remove())
                    } else {
                        None
                    }
                },
                Vacant(_) => unreachable!("Expired entry not in cache"),
            };
            // No longer holding the lock on db_cache entry.
            if let Some(value) = removed_value {
                self.free_quota(value);
                ret.push(h);
            }
        }
        ret
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L530-539)
```rust
    pub fn update_certified_timestamp(&self, certified_time: u64) {
        trace!("QS: batch reader updating time {:?}", certified_time);
        self.last_certified_time
            .fetch_max(certified_time, Ordering::SeqCst);

        let expired_keys = self.clear_expired_payload(certified_time);
        if let Err(e) = self.db.delete_batches(expired_keys) {
            debug!("Error deleting batches: {:?}", e)
        }
    }
```

**File:** consensus/src/quorum_store/tests/batch_requester_test.rs (L234-277)
```rust
#[tokio::test]
async fn test_batch_request_not_exists_expired() {
    let retry_interval_ms = 1_000;
    let expiration = 10_000;

    // Batch has expired according to the ledger info that will be returned
    let (ledger_info_with_signatures, validator_verifier) =
        create_ledger_info_with_timestamp(expiration + 1);

    let batch = Batch::new(
        BatchId::new_for_test(1),
        vec![],
        1,
        expiration,
        AccountAddress::random(),
        0,
    );
    let batch_response = BatchResponse::NotFound(ledger_info_with_signatures);
    let batch_requester = BatchRequester::new(
        1,
        AccountAddress::random(),
        1,
        2,
        retry_interval_ms,
        1_000,
        MockBatchRequester::new(batch_response),
        validator_verifier.into(),
    );

    let request_start = Instant::now();
    let (_, subscriber_rx) = oneshot::channel();
    let result = batch_requester
        .request_batch(
            *batch.digest(),
            batch.expiration(),
            Arc::new(Mutex::new(btreeset![AccountAddress::random()])),
            subscriber_rx,
        )
        .await;
    let request_duration = request_start.elapsed();
    assert_err!(result);
    // No retry because of short-circuiting of expired batch
    assert!(request_duration < Duration::from_millis(retry_interval_ms as u64));
}
```
