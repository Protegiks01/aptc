# Audit Report

## Title
Irrecoverable Node Liveness Failure Due to Partial State KV Pruner Failure and Aggressive Recovery Mechanism

## Summary
When `StateKvPruner::prune()` partially fails (metadata pruner succeeds but shard pruner fails), the system creates an inconsistent state between the metadata database and shard databases. On node restart, the mandatory catch-up mechanism can cause permanent node failure if the underlying issue persists, violating the liveness invariant.

## Finding Description

The vulnerability exists in the state KV pruning mechanism when sharding is enabled. The pruning operation executes in two sequential phases that can result in partial failure:

**Phase 1: Metadata Pruning** - The metadata pruner first commits its progress to disk atomically. [1](#0-0)  This updates `DbMetadataKey::StateKvPrunerProgress` to the target version.

**Phase 2: Shard Pruning** - After metadata commits, shard pruners execute in parallel. [2](#0-1)  If any shard pruner fails, the entire operation returns an error without updating the in-memory progress tracker. [3](#0-2) 

**Runtime Behavior:** During normal operation, the pruner worker catches errors and retries using the in-memory progress. [4](#0-3)  This works as long as the node keeps running.

**Critical Failure on Restart:** When the node restarts, initialization loads the metadata progress from disk and attempts to create shard pruners. [5](#0-4)  Each shard pruner must catch up from its persisted progress to the metadata progress. [6](#0-5) 

If this catch-up fails due to persistent underlying issues (disk corruption in specific version range, I/O errors, hardware failures), the initialization propagates the error to the manager, which panics with `.expect()`. [7](#0-6) 

This creates an irrecoverable state where:
1. Metadata DB shows progress = N (advanced and committed)
2. Failed shard DB shows progress = M (M < N, not updated)
3. Unpruned data exists in shard from versions [M, N)
4. On restart, catch-up from M to N fails persistently
5. Node cannot start due to panic, requiring manual database intervention

## Impact Explanation

This qualifies as **High Severity** under Aptos bug bounty categories for the following reasons:

**Validator Unavailability:** A validator node experiencing persistent shard pruning failures becomes permanently unable to restart, removing it from consensus participation until manual database intervention occurs. This exceeds typical "Validator Node Slowdowns" as the node is completely non-functional.

**No Automatic Recovery:** The `.expect()` panic provides zero graceful degradation. Unlike runtime errors that can be retried indefinitely, the startup panic creates a binary failure state with no automated recovery path.

**Operational Fragility:** While this doesn't affect the entire network (making it sub-Critical), it represents a significant operational vulnerability. Multiple validators experiencing similar hardware/disk issues during maintenance windows or problematic blocks could simultaneously fail, potentially affecting network liveness if >1/3 validators are impacted.

**State Inconsistency:** The partial commitment creates a database state that violates pruning invariants - metadata claims data is pruned while shards retain it, yet the node cannot access or serve this data.

## Likelihood Explanation

**Moderate to High Likelihood** in production validator environments:

**Trigger Conditions:** Any I/O error, disk corruption, permission issue, or hardware failure during the critical window between metadata commit (line 72 of metadata_pruner) and shard commits (line 71 of shard_pruner) creates the inconsistent state.

**Persistence:** If the underlying cause affects a specific version range (e.g., corrupted data at versions M to N on one shard), the issue persists across restarts. The catch-up mechanism will repeatedly fail on the same corrupted data.

**Production Realities:** 
- Large-scale deployments routinely experience disk failures, especially on distributed storage systems
- I/O timeouts under high load are common
- Unclean shutdowns can corrupt specific database regions
- Filesystem permission changes during maintenance can affect specific shards

**No Graceful Path:** Unlike runtime errors that retry indefinitely, startup failures are fatal. Operators cannot "wait out" transient issues - the node simply won't start.

## Recommendation

Implement graceful degradation and recovery mechanisms:

1. **Catch-up Error Handling:** Replace the `.expect()` panic with error logging and a configurable retry strategy. Allow the node to start with degraded pruning functionality rather than complete failure.

2. **Atomic Progress Tracking:** Either make metadata and shard progress updates atomic (two-phase commit), or delay metadata progress updates until all shards confirm success.

3. **Startup Recovery Mode:** Add a recovery mode that allows operators to:
   - Start the node with pruning disabled
   - Manually reset shard progress to match metadata
   - Re-run catch-up with verbose logging

4. **Progress Validation:** On startup, validate that all shard progress values are within acceptable bounds of metadata progress. Log warnings for large gaps but proceed with catch-up attempts that have timeout/retry limits.

Example fix for the panic:
```rust
// In state_kv_pruner_manager.rs, replace line 115:
let pruner = match StateKvPruner::new(state_kv_db) {
    Ok(p) => p,
    Err(e) => {
        error!("Failed to create state kv pruner, will retry during runtime: {}", e);
        // Return a degraded pruner or allow startup to continue
        return Self::new_degraded(state_kv_db, state_kv_pruner_config);
    }
};
```

## Proof of Concept

While a full PoC requires simulating disk failures in a test environment, the vulnerability can be demonstrated by:

1. Enable sharding in StateKvDb configuration
2. Start pruning operation that commits metadata successfully
3. Inject I/O error into one shard's write operation (via fault injection or disk quota)
4. Verify metadata progress is advanced but shard progress is not
5. Restart the node
6. Observe panic during `StateKvPruner::new()` when shard catch-up fails

The panic trace will show:
```
thread 'main' panicked at 'Failed to create state kv pruner.', storage/aptosdb/src/pruner/state_kv_pruner/state_kv_pruner_manager.rs:115
```

## Notes

This vulnerability specifically affects deployments with sharding enabled. Non-sharded deployments use a different code path that doesn't have the metadata/shard split. The issue is exacerbated by the fact that the in-memory `min_readable_version` can be updated optimistically through `set_pruner_target_db_version()` [8](#0-7)  independent of actual pruning success, though this is a secondary concern compared to the startup panic.

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs (L67-72)
```rust
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        self.state_kv_db.metadata_db().write_schemas(batch)
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L67-78)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state kv shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L80-81)
```rust
            progress = current_batch_target_version;
            self.record_progress(progress);
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L117-132)
```rust
        let metadata_progress = metadata_pruner.progress()?;

        info!(
            metadata_progress = metadata_progress,
            "Created state kv metadata pruner, start catching up all shards."
        );

        let shard_pruners = if state_kv_db.enabled_sharding() {
            let num_shards = state_kv_db.num_shards();
            let mut shard_pruners = Vec::with_capacity(num_shards);
            for shard_id in 0..num_shards {
                shard_pruners.push(StateKvShardPruner::new(
                    shard_id,
                    state_kv_db.db_shard_arc(shard_id),
                    metadata_progress,
                )?);
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L56-63)
```rust
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L42-42)
```rust
        myself.prune(progress, metadata_progress)?;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_pruner_manager.rs (L115-115)
```rust
            Arc::new(StateKvPruner::new(state_kv_db).expect("Failed to create state kv pruner."));
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_pruner_manager.rs (L128-142)
```rust
    fn set_pruner_target_db_version(&self, latest_version: Version) {
        assert!(self.pruner_worker.is_some());
        let min_readable_version = latest_version.saturating_sub(self.prune_window);
        self.min_readable_version
            .store(min_readable_version, Ordering::SeqCst);

        PRUNER_VERSIONS
            .with_label_values(&["state_kv_pruner", "min_readable"])
            .set(min_readable_version as i64);

        self.pruner_worker
            .as_ref()
            .unwrap()
            .set_target_db_version(min_readable_version);
    }
```
