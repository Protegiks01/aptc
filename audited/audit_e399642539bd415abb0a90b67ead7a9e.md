# Audit Report

## Title
Layout Cache Race Condition Causes Non-Deterministic Execution and Consensus Divergence

## Summary
A race condition in the Move VM's layout cache allows validators to execute transactions with different struct/enum layouts when modules are published during parallel block execution. The race occurs between cache flushes (during commit) and cache insertions (during parallel execution), causing validators with different thread scheduling to compute different state roots for identical blocks, violating consensus safety.

## Finding Description

The vulnerability exists in the interaction between parallel transaction execution and the global layout cache in Aptos's BlockSTM executor. The critical flaw is that `store_layout_to_cache()` can insert stale layouts into the cache **after** the cache has been flushed due to module publishing, and these stale layouts lack version information to detect they were computed from outdated module definitions.

### Architecture Overview

The layout cache stores computed struct/enum layouts keyed by `(StructNameIndex, TypeArgsId)`: [1](#0-0) 

The cache uses `DashMap` for concurrent access during parallel execution: [2](#0-1) 

### The Race Condition Mechanism

**Step 1**: Layout computation and caching happens during parallel transaction execution, before validation: [3](#0-2) 

**Step 2**: Cache flush happens during commit when modules are published: [4](#0-3) 

**Step 3**: The actual cache store uses `DashMap::entry()` which only inserts if vacant: [5](#0-4) 

### The Critical Flaw

The `StructNameIndex` represents the struct NAME, not a specific module version, so it is reused across module versions: [1](#0-0) 

When a layout is loaded from cache, modules are re-read for validation purposes, but the **layout structure itself is not recomputed**: [6](#0-5) 

The module read validation only checks if the module is overridden, not if the layout matches the current module version: [7](#0-6) 

### Exploitation Scenario

Consider a block with transactions: `T5`, `T10`, `T15` (publishes module M v2 with added enum variant), `T20`, `T25`

**Validator A** (unlucky thread timing):
1. Thread 1 starts executing T5, computes layout `L_old` for enum E from module M v1 (2 variants)
2. Thread 1 is about to call `store_layout_to_cache(L_old)` but gets preempted
3. Thread 2 commits T15 sequentially, publishes M v2 (3 variants), **flushes layout cache**
4. Thread 1 resumes, `store_layout_to_cache(L_old)` completes - cache is vacant, **inserts L_old**
5. Thread 1's T5 validation fails (M v1 was overridden), T5 aborts
6. Thread 1 re-executes T5, loads `L_old` from cache (2 variants)
7. T5 re-reads module M v2 (module validation passes - v2 is current, not overridden)
8. T5 executes with **wrong layout structure** (2 variants) but module M v2 (3 variants)
9. Thread 3 executes T20, loads `L_old` from cache, same incorrect execution

**Validator B** (lucky thread timing):
1. Thread 1's `store_layout_to_cache(L_old)` completes **before** Thread 2's flush
2. Flush clears `L_old` from cache
3. All subsequent transactions compute `L_new` (3 variants) from M v2
4. All transactions use correct layout

**Result**: Validator A and Validator B compute **different state roots** for the same block!

The code even acknowledges this issue with enum layouts: [8](#0-7) 

## Impact Explanation

This is a **Critical Severity** vulnerability (Consensus/Safety violation) that aligns with Aptos bug bounty category "Consensus/Safety Violations" because:

1. **Breaks Deterministic Execution Invariant**: Validators produce different state roots for identical blocks based on non-deterministic thread scheduling, not transaction content.

2. **Consensus Safety Violation**: Different validators executing the same block compute different states. This can cause:
   - Disagreement on state roots during block commitment
   - Potential chain splits if validators diverge on whether transactions succeed/fail
   - Silent corruption where different validators maintain different ledger states

3. **Affects All Validators**: Any validator can be affected during normal operation when processing blocks with module upgrades. No Byzantine behavior is required.

4. **Silent Corruption**: The validation mechanisms pass even with stale layouts because they only check if modules are overridden (line 1061 in captured_reads.rs), not if layouts match module versions. A transaction that loads a stale layout from cache and re-reads the new module will pass all validations but execute with incorrect layout semantics.

5. **Enum Variant Additions**: The vulnerability is specifically triggered by enum variant additions, which are **allowed** by Aptos's compatibility checks. When an enum gets a new variant, the layout changes (number of variants increases), but if a stale layout is cached, transactions will attempt to deserialize new variant data using old layout information, causing:
   - Deserialization failures (transaction aborts on one validator but succeeds on another)
   - Incorrect variant interpretation (reading variant C as if it were A or B)
   - Different execution outcomes depending on which layout version is used

The vulnerability manifests whenever:
- A block contains module publishing transactions with enum modifications
- Parallel execution is enabled (default in production)
- Transactions after the module publish use enums from the published module
- Different validators have different thread scheduling patterns

## Likelihood Explanation

**High Likelihood** of occurrence:

1. **Common Trigger**: Module upgrades with enum changes are regular operations on Aptos:
   - Framework upgrades that add new enum variants
   - DApp deployments that extend existing enums
   - Compatibility checks explicitly allow adding enum variants

2. **Parallel Execution Default**: All production validators use parallel execution (BlockSTM), which is when the race occurs: [9](#0-8) 

3. **Non-Deterministic Timing**: Thread scheduling varies between:
   - Validator hardware (different CPU architectures, core counts)
   - OS scheduling (different Linux kernel versions, scheduler policies)
   - System load (other processes, network interrupts)

4. **No Special Crafting Required**: Any normal block with module publishing can trigger this. The attacker doesn't need to craft special timing - the race is inherent in the parallel execution design.

5. **Persistent Once Cached**: Once a stale layout enters the cache after a flush, it persists until the next module publish triggers another flush. This can affect multiple subsequent blocks.

The race window exists between:
- Cache flush completing (nanoseconds, line 167 in code_cache_global.rs)
- Pending `store_layout_to_cache()` calls from transactions that started execution before the flush but complete the store after

Given typical block execution with 100+ transactions and 4-32 worker threads, the probability of at least one thread being in this race window during a module publish event is significant.

## Recommendation

**Immediate Fix**: Add version information to cached layouts to detect staleness.

**Option 1 - Layout Versioning**: Include module version in `StructKey`:
```rust
pub struct StructKey {
    pub idx: StructNameIndex,
    pub ty_args_id: TypeVecId,
    pub module_version: Option<TxnIndex>, // Track when module was published
}
```

**Option 2 - Atomic Flush+Store**: Use a generation counter:
```rust
pub struct GlobalModuleCache {
    layout_cache_generation: AtomicU64,
    // ... existing fields
}

// On flush:
fn flush_layout_cache(&self) {
    self.layout_cache_generation.fetch_add(1, Ordering::SeqCst);
    self.struct_layouts.clear();
}

// On store:
fn store_struct_layout_entry(&self, key: &StructKey, entry: LayoutCacheEntry, generation: u64) {
    if generation == self.layout_cache_generation.load(Ordering::SeqCst) {
        // Only insert if generation matches
        if let dashmap::Entry::Vacant(e) = self.struct_layouts.entry(*key) {
            e.insert(entry);
        }
    }
}
```

**Option 3 - Single Variant Layouts** (as suggested by TODO comment): Store individual variant layouts instead of full enum layouts, making them immune to variant additions.

## Proof of Concept

The vulnerability can be demonstrated with a test that:
1. Publishes module M v1 with enum E having variants [A, B]
2. Executes transactions using enum E in parallel
3. Publishes module M v2 adding variant C to enum E during execution
4. Verifies that different validators (simulated with controlled thread timing) compute different state roots

A complete PoC would require modifying the block executor test framework to control thread scheduling and verify the race condition. The vulnerability is confirmed through code analysis showing:
- No version information in `StructKey` (layout_cache.rs:79-83)
- Vacant-only insertion after flush (code_cache_global.rs:186-189)
- Layout stored before validation (ty_layout_converter.rs:128)
- Module validation doesn't check layout staleness (captured_reads.rs:1060-1067)

## Notes

This vulnerability is particularly insidious because:

1. **Known Design Limitation**: The TODO comment at code_cache_global.rs:164-166 indicates the Aptos team is aware enum layouts require special handling but haven't fully addressed the race condition.

2. **Validation Blind Spot**: The module validation mechanism (captured_reads.rs:1050-1089) checks if modules are overridden but cannot detect that a cached layout was computed from an older module version.

3. **Compatibility Trade-off**: Aptos's compatibility checks allow enum variant additions to support module evolution, but the layout cache doesn't account for this, creating a mismatch between what's allowed and what's correctly handled.

4. **Non-Deterministic Manifestation**: The bug only manifests based on thread scheduling, making it extremely difficult to debug in production. Different validators would see different results seemingly randomly.

### Citations

**File:** third_party/move/move-vm/runtime/src/storage/layout_cache.rs (L79-83)
```rust
#[derive(Debug, Copy, Clone, Eq, PartialEq, Hash)]
pub struct StructKey {
    pub idx: StructNameIndex,
    pub ty_args_id: TypeVecId,
}
```

**File:** aptos-move/block-executor/src/code_cache_global.rs (L96-96)
```rust
    struct_layouts: DashMap<StructKey, LayoutCacheEntry>,
```

**File:** aptos-move/block-executor/src/code_cache_global.rs (L163-168)
```rust
    pub fn flush_layout_cache(&self) {
        // TODO(layouts):
        //   Flushing is only needed because of enums. Once we refactor layouts to store a single
        //   variant instead, this can be removed.
        self.struct_layouts.clear();
    }
```

**File:** aptos-move/block-executor/src/code_cache_global.rs (L186-189)
```rust
        if let dashmap::Entry::Vacant(e) = self.struct_layouts.entry(*key) {
            e.insert(entry);
        }
        Ok(())
```

**File:** third_party/move/move-vm/runtime/src/storage/ty_layout_converter.rs (L117-129)
```rust
                // Otherwise a cache miss, compute the result and store it.
                let mut modules = DefiningModules::new();
                let layout = self.type_to_type_layout_with_delayed_fields_impl::<false>(
                    gas_meter,
                    traversal_context,
                    &mut modules,
                    ty,
                    check_option_type,
                )?;
                let cache_entry = LayoutCacheEntry::new(layout.clone(), modules);
                self.struct_definition_loader
                    .store_layout_to_cache(&key, cache_entry)?;
                return Ok(layout);
```

**File:** aptos-move/block-executor/src/txn_last_input_output.rs (L572-576)
```rust
        if published {
            // Record validation requirements after the modules are published.
            global_module_cache.flush_layout_cache();
            scheduler.record_validation_requirements(txn_idx, module_ids_for_v2)?;
        }
```

**File:** third_party/move/move-vm/runtime/src/storage/loader/lazy.rs (L209-220)
```rust
        let entry = self.module_storage.get_struct_layout(key)?;
        let (layout, modules) = entry.unpack();
        for module_id in modules.iter() {
            // Re-read all modules for this layout, so that transaction gets invalidated
            // on module publish. Also, we re-read them in exactly the same way as they
            // were traversed during layout construction, so gas charging should be exactly
            // the same as on the cache miss.
            if let Err(err) = self.charge_module(gas_meter, traversal_context, module_id) {
                return Some(Err(err));
            }
        }
        Some(Ok(layout))
```

**File:** aptos-move/block-executor/src/captured_reads.rs (L1060-1067)
```rust
        let validate = |key: &K, read: &ModuleRead<DC, VC, S>| match read {
            ModuleRead::GlobalCache(_) => global_module_cache.contains_not_overridden(key),
            ModuleRead::PerBlockCache(previous) => {
                let current_version = per_block_module_cache.get_module_version(key);
                let previous_version = previous.as_ref().map(|(_, version)| *version);
                current_version == previous_version
            },
        };
```

**File:** aptos-move/block-executor/src/executor.rs (L1455-1472)
```rust
            while scheduler.commit_hooks_try_lock() {
                // Perform sequential commit hooks.
                while let Some((txn_idx, incarnation)) = scheduler.start_commit()? {
                    self.prepare_and_queue_commit_ready_txn(
                        txn_idx,
                        incarnation,
                        num_txns,
                        executor,
                        block,
                        num_workers as usize,
                        runtime_environment,
                        scheduler_wrapper,
                        shared_sync_params,
                    )?;
                }

                scheduler.commit_hooks_unlock();
            }
```
