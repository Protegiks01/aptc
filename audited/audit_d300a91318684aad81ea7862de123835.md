# Audit Report

## Title
Partial Checker Completion Creates State Inconsistency in Faucet Ratelimiting

## Summary
The faucet's checker completion logic lacks atomicity guarantees. When multiple checkers implement the `complete()` method and one fails mid-execution, previously completed checkers retain their state updates while subsequent checkers never execute. This creates permanent state inconsistency between different ratelimiting backends (memory vs Redis), causing unfair rate-limiting and requiring manual intervention to restore consistency.

## Finding Description

The faucet uses a checker system to validate and track funding requests. Multiple checkers can implement the `complete()` method to perform post-funding cleanup, such as decrementing rate-limit counters when a 500 error occurs (to avoid penalizing users for server failures). [1](#0-0) 

Two checkers implement this cleanup: `MemoryRatelimitChecker` and `RedisRatelimitChecker`, both decrementing their respective counters on 500 errors: [2](#0-1) [3](#0-2) 

The vulnerability occurs in how these completions are orchestrated: [4](#0-3) 

The loop uses the `?` operator to propagate errors immediately upon failure. This creates a partial completion scenario:

1. Configuration includes both `MemoryRatelimitChecker` and `RedisRatelimitChecker`
2. Both checkers increment their counters during the `check()` phase
3. Funding operation fails with a 500 error (e.g., blockchain RPC timeout)
4. `complete()` is called on all checkers to decrement counters
5. `MemoryRatelimitChecker.complete()` succeeds â†’ memory counter decremented
6. `RedisRatelimitChecker.complete()` fails (e.g., Redis connection timeout/network partition)
7. Error propagates with `?`, loop exits immediately
8. Any remaining checkers never execute their `complete()` methods

**Result**: Memory state reflects the correct count (decremented), but Redis state is incorrect (still incremented). The user is now unfairly rate-limited in Redis while appearing within limits in memory.

This violates the **State Consistency** invariant: state updates across different storage backends should be atomic. Either all checkers complete successfully, or all should maintain their pre-completion state.

## Impact Explanation

**Medium Severity** per Aptos bug bounty criteria - "State inconsistencies requiring intervention":

- **State Corruption**: Permanent divergence between memory and Redis ratelimit state for affected IPs
- **Unfair Rate-Limiting**: Users are penalized in Redis for server errors that should not count against their quota
- **Operational Impact**: Requires manual Redis intervention to identify and fix corrupted counters
- **Cascading Failures**: During Redis instability periods, many IPs accumulate incorrect state
- **User Experience Degradation**: Legitimate users receive spurious rate-limit rejections

**Not** Critical/High because:
- No fund loss or theft possible (faucet controls its own funds)
- No consensus or blockchain impact (faucet is off-chain infrastructure)
- No validator node compromise
- State can be manually repaired (though requires identifying affected IPs)

## Likelihood Explanation

**High Likelihood**:

- **Redis failures are common**: Network partitions, connection pool exhaustion, Redis server restarts, timeouts under load
- **No special attacker requirements**: The vulnerability triggers naturally during infrastructure issues
- **Observable in production**: Any faucet deployment using both memory and Redis checkers is vulnerable
- **Common configuration**: Running multiple redundant checkers is a reasonable high-availability pattern
- **Frequent 500 errors**: Blockchain RPC timeouts, validator connectivity issues regularly cause 500 responses

The vulnerability triggers whenever:
1. Multiple checkers implement `complete()` (both ratelimit checkers do)
2. A funding request results in a 500 error (common)
3. Any checker's `complete()` fails (Redis failures are routine)

## Recommendation

Implement all-or-nothing completion semantics using one of these approaches:

**Option 1: Collect All Errors (Recommended)**
```rust
// In fund.rs, replace lines 342-346:
let mut completion_errors = Vec::new();
for checker in &self.checkers {
    if let Err(e) = checker.complete(complete_data.clone()).await {
        completion_errors.push((checker, e));
    }
}

if !completion_errors.is_empty() {
    // Log all errors for monitoring
    for (checker, error) in &completion_errors {
        error!("Checker completion failed: {:?}", error);
    }
    // Return first error (or aggregate)
    return Err(completion_errors[0].1.clone().into());
}
```

**Option 2: Two-Phase Commit**
```rust
// Phase 1: Prepare all checkers
for checker in &self.checkers {
    checker.prepare_complete(complete_data.clone()).await?;
}

// Phase 2: Commit all checkers (must be infallible)
for checker in &self.checkers {
    checker.commit_complete(complete_data.clone()).await;
}
```

**Option 3: Make Operations Idempotent**
- Implement retry logic for Redis operations
- Add request IDs to track completion status
- Allow reprocessing of failed completions

**Immediate Mitigation**:
- Add comprehensive monitoring/alerting on checker completion failures
- Implement automated Redis state reconciliation scripts
- Consider making Redis operations best-effort (log failures but don't propagate)

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use std::sync::Arc;
    
    // Mock checker that fails on complete()
    struct FailingChecker;
    
    #[async_trait]
    impl CheckerTrait for FailingChecker {
        async fn check(&self, _: CheckerData, _: bool) -> Result<Vec<RejectionReason>, AptosTapError> {
            Ok(vec![])
        }
        
        async fn complete(&self, _: CompleteData) -> Result<(), AptosTapError> {
            Err(AptosTapError::new("Simulated Redis failure".to_string(), 
                AptosTapErrorCode::StorageError))
        }
        
        fn cost(&self) -> u8 { 50 }
    }
    
    #[tokio::test]
    async fn test_partial_completion_atomicity_violation() {
        // Setup: Memory checker (succeeds) + Failing checker (simulates Redis failure)
        let memory_checker = MemoryRatelimitChecker::new(MemoryRatelimitCheckerConfig {
            max_requests_per_day: 10,
            max_entries_in_map: NonZeroUsize::new(1000).unwrap(),
        });
        
        let checkers = vec![
            Checker::from(memory_checker),
            Checker::from(FailingChecker),
        ];
        
        let checker_data = CheckerData {
            receiver: AccountAddress::random(),
            source_ip: "192.168.1.1".parse().unwrap(),
            headers: Arc::new(HeaderMap::new()),
            time_request_received_secs: 1000,
        };
        
        // Phase 1: Both checkers pass check() and increment counters
        for checker in &checkers {
            checker.check(checker_data.clone(), false).await.unwrap();
        }
        
        // Verify memory counter was incremented
        let memory_count_after_check = /* access memory counter */;
        assert_eq!(memory_count_after_check, 1);
        
        // Phase 2: Simulate 500 error, call complete() on all checkers
        let complete_data = CompleteData {
            checker_data: checker_data.clone(),
            txn_hashes: vec![],
            response_is_500: true,
        };
        
        // This is what fund.rs does:
        let mut result = Ok(());
        for checker in &checkers {
            if let Err(e) = checker.complete(complete_data.clone()).await {
                result = Err(e);
                break; // Loop exits early!
            }
        }
        
        // Assertion: First checker succeeded, second failed
        assert!(result.is_err());
        
        // VULNERABILITY: Memory counter was decremented (state updated)
        let memory_count_after_complete = /* access memory counter */;
        assert_eq!(memory_count_after_complete, 0); // Decremented!
        
        // But if there was a third checker, it never ran complete()
        // This creates inconsistent state across checkers
    }
}
```

## Notes

This vulnerability affects the **aptos-faucet** component specifically, not the core Aptos blockchain consensus or execution layers. While the faucet is infrastructure for funding test accounts rather than a consensus-critical component, state consistency is still a security requirement per the bug bounty program's Medium severity criteria.

The issue manifests most severely when Redis (or other external storage) experiences transient failures, which are common in production distributed systems. The lack of atomicity guarantees means that partial state updates accumulate over time, degrading service quality and requiring manual operational intervention to identify and repair corrupted state.

### Citations

**File:** crates/aptos-faucet/core/src/checkers/mod.rs (L61-63)
```rust
    async fn complete(&self, _data: CompleteData) -> Result<(), AptosTapError> {
        Ok(())
    }
```

**File:** crates/aptos-faucet/core/src/checkers/memory_ratelimit.rs (L93-102)
```rust
    async fn complete(&self, data: CompleteData) -> Result<(), AptosTapError> {
        if data.response_is_500 {
            *self
                .ip_to_requests_today
                .lock()
                .await
                .get_or_insert_mut(data.checker_data.source_ip, || 1) -= 1;
        }
        Ok(())
    }
```

**File:** crates/aptos-faucet/core/src/checkers/redis_ratelimit.rs (L308-335)
```rust
    async fn complete(&self, data: CompleteData) -> Result<(), AptosTapError> {
        if !data.response_is_500 {
            return Ok(());
        }

        let mut conn = self
            .get_redis_connection()
            .await
            .map_err(|e| AptosTapError::new_with_error_code(e, AptosTapErrorCode::StorageError))?;

        // Generate a key corresponding to this identifier and the current day. In the
        // JWT case we re-verify the JWT. This is inefficient, but these failures are
        // extremely rare so I don't refactor for now.
        let key_prefix = self.ratelimit_key_provider.ratelimit_key_prefix();
        let key_value = self
            .ratelimit_key_provider
            .ratelimit_key_value(&data.checker_data)
            .await?;
        let (key, _) = self.get_key_and_secs_until_next_day(key_prefix, &key_value);

        let _: () = conn.decr(&key, 1).await.map_err(|e| {
            AptosTapError::new_with_error_code(
                format!("Failed to decrement value for redis key {}: {}", key, e),
                AptosTapErrorCode::StorageError,
            )
        })?;
        Ok(())
    }
```

**File:** crates/aptos-faucet/core/src/endpoints/fund.rs (L342-346)
```rust
            for checker in &self.checkers {
                checker.complete(complete_data.clone()).await.map_err(|e| {
                    AptosTapError::new_with_error_code(e, AptosTapErrorCode::CheckerError)
                })?;
            }
```
