# Audit Report

## Title
Resource Leak in ExecutorService Shutdown Leading to Thread and Memory Accumulation

## Summary
The `shutdown()` method in `ThreadExecutorService` and its underlying `ExecutorService` fails to properly clean up spawned threads, thread pools, and network resources. Across multiple shutdown/restart cycles, these resources accumulate and can lead to thread exhaustion and memory leaks.

## Finding Description

The vulnerability exists in the shutdown flow of the executor service implementation. When `ExecutorService.start()` is called, it spawns a new thread that runs the `ShardedExecutorService.start()` method: [1](#0-0) 

The spawned thread enters an infinite blocking loop waiting for commands: [2](#0-1) 

The thread blocks on receiving commands from the coordinator client: [3](#0-2) 

However, when `shutdown()` is called, it only shuts down the network controller: [4](#0-3) 

The network controller's shutdown method only sends signals to network handlers but doesn't wait for full cleanup: [5](#0-4) 

**Critical Issues:**
1. The spawned thread's `JoinHandle` is discarded and never joined
2. No `Stop` command is sent to the `ShardedExecutorService` to break its loop
3. The `command_rx` channel is never closed to signal thread termination
4. The rayon `ThreadPool` with `num_threads + 2` threads is never explicitly cleaned up
5. Network resources (tokio runtimes) are only partially cleaned up

**Per shutdown cycle, the following resources leak:**
- 1 blocked OS thread
- A rayon ThreadPool with N+2 worker threads
- Network connections and tokio runtime resources
- Memory for `Arc<ShardedExecutorService>` and related structures

## Impact Explanation

This is a **Medium severity** issue per the classification provided in the security question. While it causes resource accumulation, it has limited direct security impact because:

1. **Node Availability Impact**: In scenarios with repeated service restarts (testing, development, error recovery), accumulated resources can exhaust system thread limits or memory, leading to node crashes or degraded performance.

2. **Not Directly Exploitable**: External attackers cannot directly trigger the shutdown/restart cycle without code execution privileges on the node.

3. **Production Context Unclear**: The `ThreadExecutorService` is marked "for testing only", but the underlying `ExecutorService` implementation may be used in production scenarios where restarts occur.

The impact aligns with Medium severity: "State inconsistencies requiring intervention" in the sense that accumulated resources require node restart to clear, and could cause "Validator node slowdowns" if the service is used in production contexts.

## Likelihood Explanation

**Likelihood: Low to Medium**, depending on usage context:

- **High likelihood** in testing/development environments where services are repeatedly created and destroyed
- **Low likelihood** in production if the service is truly only used for testing
- **Medium likelihood** if similar patterns exist in production executor services

The vulnerability automatically occurs every time `shutdown()` is called, but requires scenarios where the service lifecycle is exercised multiple times.

## Recommendation

**Fix 1: Store and join the thread handle**

Modify `ExecutorService` to store the thread handle and join it during shutdown:

```rust
pub struct ExecutorService {
    shard_id: ShardId,
    controller: NetworkController,
    executor_service: Arc<ShardedExecutorService<RemoteStateViewClient>>,
    executor_thread: Option<thread::JoinHandle<()>>,
}

pub fn start(&mut self) {
    self.controller.start();
    let thread_name = format!("ExecutorService-{}", self.shard_id);
    let builder = thread::Builder::new().name(thread_name);
    let executor_service_clone = self.executor_service.clone();
    let handle = builder
        .spawn(move || {
            executor_service_clone.start();
        })
        .expect("Failed to spawn thread");
    self.executor_thread = Some(handle);
}

pub fn shutdown(&mut self) {
    self.controller.shutdown();
    // Close channels to trigger Stop command
    // Then join the thread
    if let Some(handle) = self.executor_thread.take() {
        handle.join().expect("Failed to join executor thread");
    }
}
```

**Fix 2: Implement proper channel closure**

Ensure the `command_rx` channel is closed when shutting down by implementing a proper shutdown mechanism in `RemoteCoordinatorClient` that drops the sending side of the channel.

**Fix 3: Add Drop implementation**

Add a `Drop` implementation to ensure cleanup even if `shutdown()` is not explicitly called.

## Proof of Concept

```rust
#[test]
fn test_resource_leak_on_repeated_shutdown() {
    use std::thread;
    use std::time::Duration;
    
    // Get initial thread count
    let initial_thread_count = get_current_thread_count();
    
    // Repeatedly create and shutdown services
    for i in 0..10 {
        let num_shards = 4;
        let (mut client, mut services) = create_thread_remote_executor_shards(num_shards, Some(2));
        
        // Let services start up
        thread::sleep(Duration::from_millis(100));
        
        // Shutdown all services
        for service in services.iter_mut() {
            service.shutdown();
        }
        
        // Drop everything
        drop(services);
        drop(client);
        
        thread::sleep(Duration::from_millis(100));
        
        let current_thread_count = get_current_thread_count();
        println!("Iteration {}: Thread count = {}", i, current_thread_count);
    }
    
    let final_thread_count = get_current_thread_count();
    
    // Assert that thread count has grown significantly
    assert!(
        final_thread_count > initial_thread_count + 20,
        "Thread leak detected: {} -> {} threads",
        initial_thread_count,
        final_thread_count
    );
}

fn get_current_thread_count() -> usize {
    // Platform-specific implementation to count threads
    // On Linux: read /proc/self/status
    // On macOS: use task_threads()
    std::thread::available_parallelism().unwrap().get()
}
```

## Notes

- The `ThreadExecutorService` is explicitly marked "for testing only" at line 8 of `thread_executor_service.rs`
- However, the underlying bug is in `ExecutorService` which may have production uses
- Similar issues may exist in `ProcessExecutorService` and should be audited
- The rayon ThreadPool's drop behavior needs verification - it may or may not clean up threads properly
- The NetworkController already has a TODO comment acknowledging incomplete shutdown

### Citations

**File:** execution/executor-service/src/remote_executor_service.rs (L57-67)
```rust
    pub fn start(&mut self) {
        self.controller.start();
        let thread_name = format!("ExecutorService-{}", self.shard_id);
        let builder = thread::Builder::new().name(thread_name);
        let executor_service_clone = self.executor_service.clone();
        builder
            .spawn(move || {
                executor_service_clone.start();
            })
            .expect("Failed to spawn thread");
    }
```

**File:** execution/executor-service/src/remote_executor_service.rs (L69-71)
```rust
    pub fn shutdown(&mut self) {
        self.controller.shutdown();
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L215-260)
```rust
    pub fn start(&self) {
        trace!(
            "Shard starting, shard_id={}, num_shards={}.",
            self.shard_id,
            self.num_shards
        );
        let mut num_txns = 0;
        loop {
            let command = self.coordinator_client.receive_execute_command();
            match command {
                ExecutorShardCommand::ExecuteSubBlocks(
                    state_view,
                    transactions,
                    concurrency_level_per_shard,
                    onchain_config,
                ) => {
                    num_txns += transactions.num_txns();
                    trace!(
                        "Shard {} received ExecuteBlock command of block size {} ",
                        self.shard_id,
                        num_txns
                    );
                    let exe_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "execute_block"]);
                    let ret = self.execute_block(
                        transactions,
                        state_view.as_ref(),
                        BlockExecutorConfig {
                            local: BlockExecutorLocalConfig::default_with_concurrency_level(
                                concurrency_level_per_shard,
                            ),
                            onchain: onchain_config,
                        },
                    );
                    drop(state_view);
                    drop(exe_timer);

                    let _result_tx_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "result_tx"]);
                    self.coordinator_client.send_execution_result(ret);
                },
                ExecutorShardCommand::Stop => {
                    break;
                },
            }
        }
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L80-112)
```rust
    fn receive_execute_command(&self) -> ExecutorShardCommand<RemoteStateViewClient> {
        match self.command_rx.recv() {
            Ok(message) => {
                let _rx_timer = REMOTE_EXECUTOR_TIMER
                    .with_label_values(&[&self.shard_id.to_string(), "cmd_rx"])
                    .start_timer();
                let bcs_deser_timer = REMOTE_EXECUTOR_TIMER
                    .with_label_values(&[&self.shard_id.to_string(), "cmd_rx_bcs_deser"])
                    .start_timer();
                let request: RemoteExecutionRequest = bcs::from_bytes(&message.data).unwrap();
                drop(bcs_deser_timer);

                match request {
                    RemoteExecutionRequest::ExecuteBlock(command) => {
                        let init_prefetch_timer = REMOTE_EXECUTOR_TIMER
                            .with_label_values(&[&self.shard_id.to_string(), "init_prefetch"])
                            .start_timer();
                        let state_keys = Self::extract_state_keys(&command);
                        self.state_view_client.init_for_block(state_keys);
                        drop(init_prefetch_timer);

                        let (sub_blocks, concurrency, onchain_config) = command.into();
                        ExecutorShardCommand::ExecuteSubBlocks(
                            self.state_view_client.clone(),
                            sub_blocks,
                            concurrency,
                            onchain_config,
                        )
                    },
                }
            },
            Err(_) => ExecutorShardCommand::Stop,
        }
```

**File:** secure/net/src/network_controller/mod.rs (L152-166)
```rust
    // TODO: This is still not a very clean shutdown. We don't wait for the full shutdown after
    //       sending the signal. May not matter much for now because we shutdown before exiting the
    //       process. Ideally, we want to fix this.
    pub fn shutdown(&mut self) {
        info!("Shutting down network controller at {}", self.listen_addr);
        if let Some(shutdown_signal) = self.inbound_server_shutdown_tx.take() {
            shutdown_signal.send(()).unwrap();
        }

        if let Some(shutdown_signal) = self.outbound_task_shutdown_tx.take() {
            shutdown_signal.send(Message::new(vec![])).unwrap_or_else(|_| {
                warn!("Failed to send shutdown signal to outbound task; probably already shutdown");
            })
        }
    }
```
