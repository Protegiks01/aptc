# Audit Report

## Title
TOCTOU Race Condition in OverallCommitProgress Allows Backward Progress Movement

## Summary
A time-of-check-time-of-use (TOCTOU) race condition exists between `commit_ledger()` and `finalize_state_snapshot()` functions that both write to the `OverallCommitProgress` metadata key. While `commit_ledger()` acquires `commit_lock` before reading and writing this value, `finalize_state_snapshot()` writes to it without acquiring any lock, creating a window where commit progress can move backward if both functions execute concurrently.

## Finding Description
The vulnerability exists in the storage layer where two code paths update the `OverallCommitProgress` database metadata:

**Path 1: Normal consensus commit** [1](#0-0) 

This path acquires `commit_lock`, reads the current progress via `get_synced_version()` to validate the new version, then writes the new progress value.

**Path 2: State sync snapshot finalization** [2](#0-1) 

This path writes `OverallCommitProgress` directly without acquiring `commit_lock`.

The read operation occurs here: [3](#0-2) 

The race sequence:
1. Thread A (consensus) calls `commit_ledger(102)`, acquires `commit_lock`, and reads `old_committed_ver = 100`
2. Thread A validates that `102 >= 100` âœ“
3. Thread B (state sync) calls `finalize_state_snapshot(105)` without acquiring `commit_lock`
4. Thread B writes `OverallCommitProgress = 105` to database
5. Thread A writes `OverallCommitProgress = 102` to database
6. **Result**: Progress moves backward from 105 to 102

The system design assumes coordination at a higher level (comments state "Consensus and state sync must hand over to each other after all pending execution and committing complete"), but this is not enforced at the database layer through locking. [4](#0-3) 

## Impact Explanation
**Severity: Medium to High**

If commit progress moves backward, the following issues can occur:

1. **State Inconsistency**: Nodes will report incorrect synced versions via `get_synced_version()` [5](#0-4) 

2. **Pruning Errors**: The ledger pruner and state pruners use this progress value to determine safe pruning points. Backward progress could cause incorrect pruning decisions, potentially pruning data that should be retained or failing to prune data that should be removed.

3. **Consensus Divergence**: Different validator nodes experiencing the race at different times may have inconsistent views of commit progress, potentially causing issues during epoch transitions or state sync operations.

4. **Recovery Issues**: On node restart, the backward progress value will be read, potentially causing the node to believe it needs to re-sync versions that were actually committed.

This qualifies as **High Severity** under the bug bounty criteria as it represents a "Significant protocol violation" that could affect validator node behavior and state consistency.

## Likelihood Explanation
**Likelihood: Low to Medium**

The likelihood depends on whether concurrent execution can occur:

- **Designed coordination**: The system has higher-level coordination between consensus and state sync that is intended to prevent concurrent execution
- **Defense-in-depth violation**: However, the lack of lock at the database layer means any bug in the coordination logic, timing window during handover, or incorrect usage would trigger the race
- **Multiple execution paths**: State sync can be triggered in various scenarios (fast sync, crash recovery, falling behind), increasing the surface area for concurrent execution
- **No runtime protection**: The code uses `try_lock().expect()` which panics on contention, but this only applies to paths using the lock - `finalize_state_snapshot` bypasses this entirely

## Recommendation
Add `commit_lock` acquisition to `finalize_state_snapshot()` to enforce mutual exclusion at the database layer, implementing defense-in-depth:

```rust
fn finalize_state_snapshot(
    &self,
    version: Version,
    output_with_proof: TransactionOutputListWithProofV2,
    ledger_infos: &[LedgerInfoWithSignatures],
) -> Result<()> {
    gauged_api("finalize_state_snapshot", || {
        // Add lock acquisition here
        let _lock = self
            .commit_lock
            .try_lock()
            .expect("Concurrent committing detected.");
        
        // ... rest of the function
    })
}
```

This ensures that even if higher-level coordination has bugs, the database layer enforces the mutual exclusion invariant.

## Proof of Concept

```rust
// Theoretical PoC demonstrating the race (requires test harness):
use std::sync::{Arc, Barrier};
use std::thread;

#[test]
fn test_toctou_commit_progress_race() {
    let db = setup_test_db();
    let barrier = Arc::new(Barrier::new(2));
    
    // Thread 1: commit_ledger
    let db1 = Arc::clone(&db);
    let b1 = Arc::clone(&barrier);
    let t1 = thread::spawn(move || {
        // Start commit_ledger(102)
        // After reading old value but before writing
        b1.wait(); // Synchronize to maximize race window
        db1.commit_ledger(102, None, None).unwrap();
    });
    
    // Thread 2: finalize_state_snapshot  
    let db2 = Arc::clone(&db);
    let b2 = Arc::clone(&barrier);
    let t2 = thread::spawn(move || {
        b2.wait(); // Synchronize
        db2.finalize_state_snapshot(105, output, ledger_infos).unwrap();
    });
    
    t1.join().unwrap();
    t2.join().unwrap();
    
    // Verify: progress may be 102 instead of expected 105
    let progress = db.get_synced_version().unwrap();
    assert_eq!(progress, Some(105)); // This may FAIL due to race
}
```

**Note**: A full PoC would require carefully orchestrating the timing to hit the race window and would need access to the test infrastructure to trigger both code paths concurrently.

## Notes

The vulnerability exists due to incomplete locking discipline where `finalize_state_snapshot` lacks the `commit_lock` that protects `commit_ledger`. While high-level coordination is intended to prevent concurrent execution, defense-in-depth principles require database-level enforcement of the mutual exclusion invariant to protect against coordination bugs, timing windows, or incorrect usage.

### Citations

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L46-49)
```rust
            // Pre-committing and committing in concurrency is allowed but not pre-committing at the
            // same time from multiple threads, the same for committing.
            // Consensus and state sync must hand over to each other after all pending execution and
            // committing complete.
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L78-112)
```rust
    fn commit_ledger(
        &self,
        version: Version,
        ledger_info_with_sigs: Option<&LedgerInfoWithSignatures>,
        chunk_opt: Option<ChunkToCommit>,
    ) -> Result<()> {
        gauged_api("commit_ledger", || {
            // Pre-committing and committing in concurrency is allowed but not pre-committing at the
            // same time from multiple threads, the same for committing.
            // Consensus and state sync must hand over to each other after all pending execution and
            // committing complete.
            let _lock = self
                .commit_lock
                .try_lock()
                .expect("Concurrent committing detected.");
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["commit_ledger"]);

            let old_committed_ver = self.get_and_check_commit_range(version)?;

            let mut ledger_batch = SchemaBatch::new();
            // Write down LedgerInfo if provided.
            if let Some(li) = ledger_info_with_sigs {
                self.check_and_put_ledger_info(version, li, &mut ledger_batch)?;
            }
            // Write down commit progress
            ledger_batch.put::<DbMetadataSchema>(
                &DbMetadataKey::OverallCommitProgress,
                &DbMetadataValue::Version(version),
            )?;
            self.ledger_db.metadata_db().write_schemas(ledger_batch)?;

            // Notify the pruners, invoke the indexer, and update in-memory ledger info.
            self.post_commit(old_committed_ver, version, ledger_info_with_sigs, chunk_opt)
        })
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L125-241)
```rust
    fn finalize_state_snapshot(
        &self,
        version: Version,
        output_with_proof: TransactionOutputListWithProofV2,
        ledger_infos: &[LedgerInfoWithSignatures],
    ) -> Result<()> {
        let (output_with_proof, persisted_aux_info) = output_with_proof.into_parts();
        gauged_api("finalize_state_snapshot", || {
            // Ensure the output with proof only contains a single transaction output and info
            let num_transaction_outputs = output_with_proof.get_num_outputs();
            let num_transaction_infos = output_with_proof.proof.transaction_infos.len();
            ensure!(
                num_transaction_outputs == 1,
                "Number of transaction outputs should == 1, but got: {}",
                num_transaction_outputs
            );
            ensure!(
                num_transaction_infos == 1,
                "Number of transaction infos should == 1, but got: {}",
                num_transaction_infos
            );

            // TODO(joshlind): include confirm_or_save_frozen_subtrees in the change set
            // bundle below.

            // Update the merkle accumulator using the given proof
            let frozen_subtrees = output_with_proof
                .proof
                .ledger_info_to_transaction_infos_proof
                .left_siblings();
            restore_utils::confirm_or_save_frozen_subtrees(
                self.ledger_db.transaction_accumulator_db_raw(),
                version,
                frozen_subtrees,
                None,
            )?;

            // Create a single change set for all further write operations
            let mut ledger_db_batch = LedgerDbSchemaBatches::new();
            let mut sharded_kv_batch = self.state_kv_db.new_sharded_native_batches();
            let mut state_kv_metadata_batch = SchemaBatch::new();
            // Save the target transactions, outputs, infos and events
            let (transactions, outputs): (Vec<Transaction>, Vec<TransactionOutput>) =
                output_with_proof
                    .transactions_and_outputs
                    .into_iter()
                    .unzip();
            let events = outputs
                .clone()
                .into_iter()
                .map(|output| output.events().to_vec())
                .collect::<Vec<_>>();
            let wsets: Vec<WriteSet> = outputs
                .into_iter()
                .map(|output| output.write_set().clone())
                .collect();
            let transaction_infos = output_with_proof.proof.transaction_infos;
            // We should not save the key value since the value is already recovered for this version
            restore_utils::save_transactions(
                self.state_store.clone(),
                self.ledger_db.clone(),
                version,
                &transactions,
                &persisted_aux_info,
                &transaction_infos,
                &events,
                wsets,
                Some((
                    &mut ledger_db_batch,
                    &mut sharded_kv_batch,
                    &mut state_kv_metadata_batch,
                )),
                false,
            )?;

            // Save the epoch ending ledger infos
            restore_utils::save_ledger_infos(
                self.ledger_db.metadata_db(),
                ledger_infos,
                Some(&mut ledger_db_batch.ledger_metadata_db_batches),
            )?;

            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::LedgerCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;
            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::OverallCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;

            // Apply the change set writes to the database (atomically) and update in-memory state
            //
            // state kv and SMT should use shared way of committing.
            self.ledger_db.write_schemas(ledger_db_batch)?;

            self.ledger_pruner.save_min_readable_version(version)?;
            self.state_store
                .state_merkle_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .epoch_snapshot_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .state_kv_pruner
                .save_min_readable_version(version)?;

            restore_utils::update_latest_ledger_info(self.ledger_db.metadata_db(), ledger_infos)?;
            self.state_store.reset();

            Ok(())
        })
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L522-538)
```rust
    fn get_and_check_commit_range(&self, version_to_commit: Version) -> Result<Option<Version>> {
        let old_committed_ver = self.ledger_db.metadata_db().get_synced_version()?;
        let pre_committed_ver = self.state_store.current_state_locked().version();
        ensure!(
            old_committed_ver.is_none() || version_to_commit >= old_committed_ver.unwrap(),
            "Version too old to commit. Committed: {:?}; Trying to commit with LI: {}",
            old_committed_ver,
            version_to_commit,
        );
        ensure!(
            pre_committed_ver.is_some() && version_to_commit <= pre_committed_ver.unwrap(),
            "Version too new to commit. Pre-committed: {:?}, Trying to commit with LI: {}",
            pre_committed_ver,
            version_to_commit,
        );
        Ok(old_committed_ver)
    }
```

**File:** storage/aptosdb/src/ledger_db/ledger_metadata_db.rs (L76-78)
```rust
    pub(crate) fn get_synced_version(&self) -> Result<Option<Version>> {
        get_progress(&self.db, &DbMetadataKey::OverallCommitProgress)
    }
```
