# Audit Report

## Title
Missing Chunk Range Validation Enables DoS via Over-Aggregated DKG Transcripts

## Summary
The `le_chunks_to_scalar()` function does not validate that received chunks are within the expected range [0, 2^ell). When DKG transcripts are aggregated beyond the `max_aggregation` parameter, chunk values exceed the BSGS discrete log range bound, causing BSGS to fail and nodes to crash with "BSGS dlog failed" panic.

## Finding Description [1](#0-0) 

The `le_chunks_to_scalar()` function reconstructs a scalar from chunks using the formula `scalar = chunk[0] + chunk[1] * 2^ell + chunk[2] * 2^(2*ell) + ...`. This function performs **no validation** that input chunks are within the expected range [0, 2^ell). The function only validates that `num_bits` is properly formatted but accepts arbitrarily large chunk values as input. [2](#0-1) 

The BSGS discrete log range bound is set to `2^(ell + log2(max_aggregation))`, which accommodates aggregated chunk values up to `max_aggregation` times larger than individual chunks. [3](#0-2) 

During transcript aggregation, encrypted chunks are added together via elliptic curve point addition (`self.Cs[i][j][k] += other.Cs[i][j][k]`), causing the underlying plaintext chunk values to sum. [5](#0-4) 

During decryption, if aggregated chunks exceed the BSGS range bound, `bsgs::dlog_vec()` returns `None`, triggering `.expect("BSGS dlog failed")` which **panics the validator node**. [4](#0-3) 

The same pattern exists in `decrypt_chunked_scalars()` where no validation occurs before passing values to `le_chunks_to_scalar()`.

**Attack Scenario:**

If the DKG protocol aggregates more than `max_aggregation` transcripts (either due to implementation bug, misconfiguration, or protocol violation), chunk values will exceed `2^(ell + log2(max_aggregation))`. When validators attempt to decrypt these over-aggregated transcripts, BSGS fails to find discrete logs within its search range, causing all decrypting nodes to crash simultaneously.

## Impact Explanation

This vulnerability qualifies as **High Severity** per Aptos Bug Bounty criteria:

- **Validator Node Crashes**: Nodes panic with "BSGS dlog failed", causing immediate validator unavailability
- **Potential Network-Wide DoS**: If multiple validators attempt to decrypt the same over-aggregated transcript, they all crash simultaneously, potentially causing loss of liveness
- **DKG Ceremony Failure**: The crash occurs during DKG decryption, preventing validators from completing the distributed key generation required for epoch transitions

While not reaching "total loss of liveness" (Critical severity), it represents a "significant protocol violation" that can cause "validator node slowdowns" or crashes (High severity).

## Likelihood Explanation

The likelihood is **MEDIUM-LOW** for the following reasons:

**Factors Increasing Likelihood:**
- No validation exists to prevent over-aggregation
- No validation exists to check chunk ranges before reconstruction
- The vulnerability is deterministic once triggered

**Factors Decreasing Likelihood:**
- Requires aggregating more transcripts than `max_aggregation` parameter
- Validators following correct protocol should respect `max_aggregation` limit
- No clear external attack vector for unprivileged attackers to force over-aggregation
- The `max_aggregation` parameter should be set appropriately for the deployment

The vulnerability is primarily a **defensive programming failure** that could manifest under:
1. Implementation bugs in aggregation logic
2. Misconfigured `max_aggregation` parameter (set too low)
3. Protocol violations by misbehaving (but not necessarily malicious) validators

## Recommendation

Add explicit validation at multiple layers:

**Layer 1: Validate in `le_chunks_to_scalar()`**
```rust
pub fn le_chunks_to_scalar<F: PrimeField>(num_bits: u8, chunks: &[F]) -> F {
    assert!(
        num_bits.is_multiple_of(8) && num_bits > 0 && num_bits <= 64,
        "Invalid chunk size"
    );
    
    let max_chunk_value = F::from(1u128 << num_bits);
    for (i, &chunk) in chunks.iter().enumerate() {
        assert!(
            chunk < max_chunk_value,
            "Chunk {} exceeds maximum value for {}-bit chunks",
            i, num_bits
        );
    }
    
    // ... rest of function
}
```

**Layer 2: Fail gracefully in decryption** [6](#0-5) 

Modify `decrypt_chunked_scalars()` to return `Result<Vec<C::ScalarField>, Error>` instead of panicking, and validate chunk ranges after BSGS.

**Layer 3: Validate aggregation count**

Add tracking and validation to ensure no more than `max_aggregation` transcripts are aggregated together, preventing the root cause.

## Proof of Concept

```rust
#[test]
#[should_panic(expected = "BSGS dlog failed")]
fn test_over_aggregation_causes_crash() {
    use ark_bls12_381::{Fr, G1Projective};
    use ark_ec::CurveGroup;
    
    // Simulate parameters
    let ell = 16u8; // chunks should be < 2^16 = 65536
    let max_aggregation = 4usize; // allows up to 4 aggregations
    let log2_max_agg = 2u32; // log2(4) = 2
    
    // BSGS range bound: 2^(16 + 2) = 2^18 = 262144
    let bsgs_range = 1u32 << (ell as u32 + log2_max_agg);
    
    // Simulate chunks that would result from 8 aggregations
    // (exceeding max_aggregation = 4)
    let num_aggregations = 8;
    let per_transcript_chunk = 60000u64; // valid individual chunk
    
    // After 8 aggregations: 60000 * 8 = 480000
    // This exceeds BSGS range of 262144
    let aggregated_chunk = per_transcript_chunk * num_aggregations as u64;
    assert!(aggregated_chunk > bsgs_range as u64);
    
    // When trying to decrypt this, BSGS will fail because
    // 480000 > 262144, and the .expect() will panic the node
    
    // In production, this would crash the validator when calling:
    // bsgs::dlog_vec(..., bsgs_range).expect("BSGS dlog failed")
}
```

## Notes

This vulnerability represents a **defensive programming failure** rather than a directly exploitable attack by external actors. The missing validation could lead to validator crashes if:

1. The aggregation logic has bugs allowing over-aggregation
2. The `max_aggregation` parameter is misconfigured  
3. Validators deviate from protocol (malicious or buggy)

While the immediate attack surface for unprivileged external attackers is limited, the lack of validation violates defensive programming principles and could interact with other bugs to create exploitable conditions. The fix is straightforward and should be implemented to prevent cascading failures in edge cases.

### Citations

**File:** crates/aptos-dkg/src/pvss/chunky/chunks.rs (L32-48)
```rust
pub fn le_chunks_to_scalar<F: PrimeField>(num_bits: u8, chunks: &[F]) -> F {
    assert!(
        num_bits.is_multiple_of(8) && num_bits > 0 && num_bits <= 64, // TODO: so make num_bits a u8?
        "Invalid chunk size"
    );

    let base = F::from(1u128 << num_bits); // need u128 in the case where `num_bits` is 64, because of `chunk * multiplier`
    let mut acc = F::zero();
    let mut multiplier = F::one();

    for &chunk in chunks {
        acc += chunk * multiplier;
        multiplier *= base;
    }

    acc
}
```

**File:** crates/aptos-dkg/src/pvss/chunky/public_parameters.rs (L115-117)
```rust
    pub(crate) fn get_dlog_range_bound(&self) -> u32 {
        1u32 << (self.ell as u32 + log2(self.max_aggregation))
    }
```

**File:** crates/aptos-dkg/src/pvss/chunky/weighted_transcript.rs (L357-363)
```rust
            let dealt_chunked_secret_key_share = bsgs::dlog_vec(
                pp.pp_elgamal.G.into_group(),
                &dealt_encrypted_secret_key_share_chunks,
                &pp.table,
                pp.get_dlog_range_bound(),
            )
            .expect("BSGS dlog failed");
```

**File:** crates/aptos-dkg/src/pvss/chunky/weighted_transcript.rs (L397-406)
```rust
        for i in 0..sc.get_total_num_players() {
            for j in 0..self.Vs[i].len() {
                // Aggregate the V_{i,j}s
                self.Vs[i][j] += other.Vs[i][j];
                for k in 0..self.Cs[i][j].len() {
                    // Aggregate the C_{i,j,k}s
                    self.Cs[i][j][k] += other.Cs[i][j][k];
                }
            }
        }
```

**File:** crates/aptos-dkg/src/pvss/chunky/chunked_elgamal.rs (L317-350)
```rust
pub fn decrypt_chunked_scalars<C: CurveGroup>(
    Cs_rows: &[Vec<C>],
    Rs_rows: &[Vec<C>],
    dk: &C::ScalarField,
    pp: &PublicParameters<C>,
    table: &HashMap<Vec<u8>, u32>,
    radix_exponent: u8,
) -> Vec<C::ScalarField> {
    let mut decrypted_scalars = Vec::with_capacity(Cs_rows.len());

    for (row, Rs_row) in Cs_rows.iter().zip(Rs_rows.iter()) {
        // Compute C - d_k * R for each chunk
        let exp_chunks: Vec<C> = row
            .iter()
            .zip(Rs_row.iter())
            .map(|(C_ij, &R_j)| C_ij.sub(R_j * *dk))
            .collect();

        // Recover plaintext chunks
        let chunk_values: Vec<_> =
            bsgs::dlog_vec(pp.G.into_group(), &exp_chunks, &table, 1 << radix_exponent)
                .expect("dlog_vec failed")
                .into_iter()
                .map(|x| C::ScalarField::from(x))
                .collect();

        // Convert chunks back to scalar
        let recovered = chunks::le_chunks_to_scalar(radix_exponent, &chunk_values);

        decrypted_scalars.push(recovered);
    }

    decrypted_scalars
}
```
