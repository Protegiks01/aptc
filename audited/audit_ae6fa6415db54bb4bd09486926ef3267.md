# Audit Report

## Title
Missing Configuration Validation in SharedMempool Allows Resource Exhaustion and Node Crashes

## Summary
The `SharedMempool::new()` constructor and `MempoolConfig::sanitize()` method do not validate configuration parameters for safe values. Extreme values for `usecase_stats_num_blocks_to_track`, `broadcast_buckets`, and related parameters can cause integer overflows, memory exhaustion, and validator node crashes during initialization.

## Finding Description

The `SharedMempool::new()` function accepts `MempoolConfig` and `TransactionFilterConfig` without performing any validation: [1](#0-0) 

The configuration sanitization explicitly lacks validation, as evidenced by the TODO comment: [2](#0-1) 

This leads to three critical resource exhaustion vectors:

**Vector 1: Integer Overflow in UseCaseHistory**
The `UseCaseHistory::new()` function performs `window_size + 1` without checking for overflow: [3](#0-2) 

If `usecase_stats_num_blocks_to_track` is set to `usize::MAX`, the addition `window_size + 1` causes integer overflow. On checked builds, this panics. On unchecked builds, it wraps to 0, creating a VecDeque with capacity 0, causing unexpected behavior during runtime when the code expects a non-zero capacity.

**Vector 2: Memory Exhaustion via broadcast_buckets**
The `MultiBucketTimelineIndex::new()` function creates one `TimelineIndex` instance per bucket entry: [4](#0-3) 

This is called once per sender bucket during `TransactionStore` initialization: [5](#0-4) 

If `broadcast_buckets` contains millions of entries (e.g., `[0, 1, 2, ..., 10000000]`), this creates millions of `TimelineIndex` instances, each containing a `BTreeMap`, leading to gigabytes of memory allocation during node startup.

**Vector 3: Memory Exhaustion via Large Window Size**
If `usecase_stats_num_blocks_to_track` is set to a very large value (e.g., `1_000_000_000`), the `VecDeque::with_capacity()` call attempts to allocate memory for billions of HashMap entries, causing immediate memory exhaustion and node crash.

**Exploitation Path:**
1. Node operator (malicious or misconfigured) modifies their `node.yaml` config file
2. Sets `mempool.usecase_stats_num_blocks_to_track: 5000000000` or similar extreme value
3. Or sets `mempool.broadcast_buckets: [0, 1, 2, 3, ..., 5000000]` with millions of entries
4. Node attempts to start and initialize `SharedMempool`
5. Memory allocation fails, causing immediate crash or OOM kill
6. Validator becomes unavailable, cannot participate in consensus

## Impact Explanation

This vulnerability is classified as **High Severity** per the Aptos Bug Bounty program criteria:

- **Validator node crashes**: Extreme config values cause immediate node failure during startup
- **API crashes**: The mempool initialization failure prevents the node from serving requests
- **Availability impact**: Affected validators cannot participate in consensus, reducing network resilience

While this vulnerability requires the node operator to control their config file, it represents a significant defensive programming failure. The explicit TODO comment in the sanitize method indicates the developers recognized the need for validation but it was never implemented. This creates a "foot-gun" where honest operators can accidentally misconfigure their nodes, or malicious operators can intentionally crash their nodes at critical times (e.g., during epoch transitions).

Multiple validators running with similar misconfigurations could degrade network liveness. Additionally, this could be exploited in supply chain attacks where default configs are tampered with.

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability requires:
- Node operator access to config files (standard for validator setup)
- Intentional misconfiguration or malicious modification
- No external attacker capabilities needed

While node operators are typically trusted, misconfigurations are common in production systems, especially when:
- Operators attempt performance tuning without understanding limits
- Config templates are copied from untrusted sources
- Automated deployment systems propagate bad defaults

The lack of validation violates the principle of defense-in-depth, where even trusted inputs should be validated to prevent catastrophic failures.

## Recommendation

Implement comprehensive validation in `MempoolConfig::sanitize()`:

```rust
impl ConfigSanitizer for MempoolConfig {
    fn sanitize(
        _node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        // Validate capacity limits
        const MAX_CAPACITY: usize = 10_000_000; // 10M transactions
        const MAX_CAPACITY_BYTES: usize = 100 * 1024 * 1024 * 1024; // 100GB
        const MAX_CAPACITY_PER_USER: usize = 10_000;
        
        if self.capacity > MAX_CAPACITY {
            return Err(Error::ConfigSanitizerFailed(
                format!("mempool.capacity ({}) exceeds maximum ({})", 
                    self.capacity, MAX_CAPACITY)
            ));
        }
        
        if self.capacity_bytes > MAX_CAPACITY_BYTES {
            return Err(Error::ConfigSanitizerFailed(
                format!("mempool.capacity_bytes ({}) exceeds maximum ({})", 
                    self.capacity_bytes, MAX_CAPACITY_BYTES)
            ));
        }
        
        // Validate use case history parameters
        const MAX_BLOCKS_TO_TRACK: usize = 10_000;
        const MAX_TOP_TO_TRACK: usize = 1_000;
        
        if self.usecase_stats_num_blocks_to_track > MAX_BLOCKS_TO_TRACK {
            return Err(Error::ConfigSanitizerFailed(
                format!("usecase_stats_num_blocks_to_track ({}) exceeds maximum ({})",
                    self.usecase_stats_num_blocks_to_track, MAX_BLOCKS_TO_TRACK)
            ));
        }
        
        if self.usecase_stats_num_top_to_track > MAX_TOP_TO_TRACK {
            return Err(Error::ConfigSanitizerFailed(
                format!("usecase_stats_num_top_to_track ({}) exceeds maximum ({})",
                    self.usecase_stats_num_top_to_track, MAX_TOP_TO_TRACK)
            ));
        }
        
        // Validate broadcast buckets
        const MAX_BROADCAST_BUCKETS: usize = 1_000;
        
        if self.broadcast_buckets.len() > MAX_BROADCAST_BUCKETS {
            return Err(Error::ConfigSanitizerFailed(
                format!("broadcast_buckets length ({}) exceeds maximum ({})",
                    self.broadcast_buckets.len(), MAX_BROADCAST_BUCKETS)
            ));
        }
        
        // Validate bucket values are sorted and start at 0
        if !self.broadcast_buckets.is_empty() && self.broadcast_buckets[0] != 0 {
            return Err(Error::ConfigSanitizerFailed(
                "broadcast_buckets must start at 0".to_string()
            ));
        }
        
        for i in 1..self.broadcast_buckets.len() {
            if self.broadcast_buckets[i] <= self.broadcast_buckets[i-1] {
                return Err(Error::ConfigSanitizerFailed(
                    "broadcast_buckets must be strictly increasing".to_string()
                ));
            }
        }
        
        // Validate num_sender_buckets is within reasonable range
        if self.num_sender_buckets == 0 || self.num_sender_buckets > 128 {
            return Err(Error::ConfigSanitizerFailed(
                format!("num_sender_buckets ({}) must be between 1 and 128",
                    self.num_sender_buckets)
            ));
        }
        
        Ok(())
    }
}
```

Additionally, add defensive checks in `UseCaseHistory::new()`:

```rust
pub(crate) fn new(window_size: usize, num_top_to_track: usize) -> Self {
    assert!(window_size < usize::MAX, "window_size too large, would overflow");
    assert!(window_size <= 10_000, "window_size exceeds reasonable limit");
    
    Self {
        window_size,
        num_top_to_track,
        recent: VecDeque::with_capacity(window_size.saturating_add(1)),
        total: HashMap::new(),
    }
}
```

## Proof of Concept

```rust
#[test]
#[should_panic(expected = "capacity overflow")]
fn test_mempool_config_integer_overflow() {
    use aptos_config::config::{MempoolConfig, NodeConfig};
    use aptos_infallible::Mutex;
    use std::sync::Arc;
    
    // Create config with extreme usecase_stats_num_blocks_to_track
    let mut config = NodeConfig::default();
    config.mempool.usecase_stats_num_blocks_to_track = usize::MAX;
    
    // This should panic or cause overflow when creating UseCaseHistory
    let _mempool = Arc::new(Mutex::new(CoreMempool::new(&config)));
}

#[test]
#[should_panic(expected = "memory allocation")]
fn test_mempool_config_memory_exhaustion_via_buckets() {
    use aptos_config::config::{MempoolConfig, NodeConfig};
    use aptos_infallible::Mutex;
    use std::sync::Arc;
    
    // Create config with millions of broadcast buckets
    let mut config = NodeConfig::default();
    config.mempool.broadcast_buckets = (0..5_000_000).map(|i| i as u64).collect();
    
    // This should cause massive memory allocation and likely OOM
    let _mempool = Arc::new(Mutex::new(CoreMempool::new(&config)));
}

#[test]
#[should_panic(expected = "memory allocation")]
fn test_mempool_config_memory_exhaustion_via_history() {
    use aptos_config::config::{MempoolConfig, NodeConfig};
    use aptos_infallible::Mutex;
    use std::sync::Arc;
    
    // Create config with billions of blocks to track
    let mut config = NodeConfig::default();
    config.mempool.usecase_stats_num_blocks_to_track = 5_000_000_000;
    
    // This should cause massive memory allocation for VecDeque capacity
    let _mempool = Arc::new(Mutex::new(CoreMempool::new(&config)));
}
```

## Notes

This vulnerability demonstrates a critical gap in defensive programming where configuration values from trusted sources are not validated. While the immediate impact is limited to the misconfigured node, the explicit TODO comment indicates this was a known gap that was never addressed. The lack of validation violates security best practices and could lead to production outages or be exploited in supply chain attacks.

### Citations

**File:** mempool/src/shared_mempool/types.rs (L66-93)
```rust
    pub fn new(
        mempool: Arc<Mutex<CoreMempool>>,
        config: MempoolConfig,
        transaction_filter_config: TransactionFilterConfig,
        network_client: NetworkClient,
        db: Arc<dyn DbReader>,
        validator: Arc<RwLock<TransactionValidator>>,
        subscribers: Vec<UnboundedSender<SharedMempoolNotification>>,
        node_type: NodeType,
    ) -> Self {
        let network_interface =
            MempoolNetworkInterface::new(network_client, node_type, config.clone());
        let use_case_history = UseCaseHistory::new(
            config.usecase_stats_num_blocks_to_track,
            config.usecase_stats_num_top_to_track,
        );
        SharedMempool {
            mempool,
            config,
            network_interface,
            db,
            validator,
            subscribers,
            broadcast_within_validator_network: Arc::new(RwLock::new(true)),
            use_case_history: Arc::new(Mutex::new(use_case_history)),
            transaction_filter_config,
        }
    }
```

**File:** config/src/config/mempool_config.rs (L176-183)
```rust
impl ConfigSanitizer for MempoolConfig {
    fn sanitize(
        _node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        Ok(()) // TODO: add reasonable verifications
    }
```

**File:** mempool/src/shared_mempool/use_case_history.rs (L19-27)
```rust
impl UseCaseHistory {
    pub(crate) fn new(window_size: usize, num_top_to_track: usize) -> Self {
        Self {
            window_size,
            num_top_to_track,
            recent: VecDeque::with_capacity(window_size + 1),
            total: HashMap::new(),
        }
    }
```

**File:** mempool/src/core_mempool/index.rs (L405-428)
```rust
    pub(crate) fn new(bucket_mins: Vec<RankingScore>) -> anyhow::Result<Self> {
        anyhow::ensure!(!bucket_mins.is_empty(), "Must not be empty");
        anyhow::ensure!(bucket_mins[0] == 0, "First bucket must start at 0");

        let mut prev = None;
        let mut timelines = vec![];
        for entry in bucket_mins.clone() {
            if let Some(prev) = prev {
                anyhow::ensure!(prev < entry, "Values must be sorted and not repeat");
            }
            prev = Some(entry);
            timelines.push(TimelineIndex::new());
        }

        let bucket_mins_to_string: Vec<_> = bucket_mins
            .iter()
            .map(|bucket_min| bucket_min.to_string())
            .collect();

        Ok(Self {
            timelines,
            bucket_mins,
            bucket_mins_to_string,
        })
```

**File:** mempool/src/core_mempool/transaction_store.rs (L104-141)
```rust
    pub(crate) fn new(config: &MempoolConfig) -> Self {
        let mut timeline_index = HashMap::new();
        for sender_bucket in 0..config.num_sender_buckets {
            timeline_index.insert(
                sender_bucket,
                MultiBucketTimelineIndex::new(config.broadcast_buckets.clone()).unwrap(),
            );
        }
        Self {
            // main DS
            transactions: HashMap::new(),
            account_sequence_numbers: HashMap::new(),

            // various indexes
            system_ttl_index: TTLIndex::new(Box::new(|t: &MempoolTransaction| t.expiration_time)),
            expiration_time_index: TTLIndex::new(Box::new(|t: &MempoolTransaction| {
                Duration::from_secs(t.txn.expiration_timestamp_secs())
            })),
            priority_index: PriorityIndex::new(),
            timeline_index,
            num_sender_buckets: config.num_sender_buckets,
            parking_lot_index: ParkingLotIndex::new(),
            hash_index: HashMap::new(),
            // estimated size in bytes
            size_bytes: 0,

            // configuration
            capacity: config.capacity,
            capacity_bytes: config.capacity_bytes,
            capacity_per_user: config.capacity_per_user,
            orderless_txn_capacity_per_user: config.orderless_txn_capacity_per_user,
            max_batch_bytes: config.shared_mempool_max_batch_bytes,

            // eager expiration
            eager_expire_threshold: config.eager_expire_threshold_ms.map(Duration::from_millis),
            eager_expire_time: Duration::from_millis(config.eager_expire_time_ms),
        }
    }
```
