# Audit Report

## Title
Infinite Retry Storm in gRPC Transaction Fetching Causes Resource Exhaustion

## Summary
The `fetch_transactions()` method in the indexer-grpc data service contains an infinite retry loop without backoff that can trigger aggressive retry storms when gRPC calls fail, leading to resource exhaustion on both client and server sides.

## Finding Description

The vulnerability exists in the call chain initiated by `fetch_latest_data()`. While the security question points to `fetch_manager.rs`, the actual flaw is one level deeper in the execution flow. [1](#0-0) 

The `fetch_latest_data()` function calls `fetch_and_update_cache()`, which in turn invokes `data_client.fetch_transactions()`. [2](#0-1) 

The critical vulnerability lies in the `fetch_transactions()` implementation: [3](#0-2) 

This function contains an infinite loop with **zero backoff**. When `client.get_transactions()` fails (network errors, timeouts, server overload), or when the response contains transactions with mismatched versions, the loop immediately retries without any delay. The TODO comment explicitly acknowledges the missing error handling.

The 200ms sleep in `fetch_latest_data()` only applies when `fetch_transactions()` successfully returns an empty vector—it provides no protection against failures within `fetch_transactions()` itself.

**Attack Scenario:**
1. Transient network issues or server overload cause gRPC calls to fail
2. The tight retry loop sends thousands of requests per second
3. Multiple indexer instances exhibiting this behavior create a thundering herd
4. The gRPC server becomes overwhelmed, causing more failures
5. Cascading failure exhausts network resources and CPU on both client and server

This violates the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits."

## Impact Explanation

This qualifies as **Medium Severity** per the Aptos bug bounty program criteria for the following reasons:

1. **Resource Exhaustion**: The tight retry loop can exhaust CPU, memory, and network bandwidth on indexer nodes
2. **Service Degradation**: Can overwhelm gRPC servers, causing cascading failures that affect all connected indexers
3. **Operational Impact**: Requires manual intervention to restart affected services and restore normal operation

While this doesn't directly affect consensus or blockchain state, it impacts the availability of critical indexing infrastructure that applications depend on for querying blockchain data. The issue falls under "State inconsistencies requiring intervention" as the indexer service may become unavailable or produce incomplete data until the issue is resolved.

## Likelihood Explanation

**High Likelihood:**
- Transient network failures are common in distributed systems
- Server overload during traffic spikes is a normal operational scenario
- No special attacker capabilities required—natural system conditions trigger the bug
- Multiple concurrent indexer instances amplify the impact
- The code contains a TODO acknowledging the missing error handling, indicating this is a known gap

The vulnerability will manifest whenever the gRPC server experiences temporary issues, making it highly likely to occur in production environments.

## Recommendation

Implement exponential backoff with jitter in the retry loop:

```rust
use tokio::time::{sleep, Duration};
use std::cmp::min;

pub(super) async fn fetch_transactions(&self, starting_version: u64) -> Vec<Transaction> {
    trace!("Fetching transactions from GrpcManager, start_version: {starting_version}.");

    let request = GetTransactionsRequest {
        starting_version: Some(starting_version),
        transactions_count: None,
        batch_size: None,
        transaction_filter: None,
    };
    
    let mut retry_count = 0;
    let max_retries = 10; // Add maximum retry limit
    
    loop {
        let mut client = self
            .connection_manager
            .get_grpc_manager_client_for_request();
        let response = client.get_transactions(request.clone()).await;
        
        match response {
            Ok(response) => {
                let transactions = response.into_inner().transactions;
                if transactions.is_empty() {
                    return vec![];
                }
                if transactions.first().unwrap().version == starting_version {
                    return transactions;
                }
                // Version mismatch - log and retry with backoff
                warn!("Version mismatch in response, retrying...");
            }
            Err(e) => {
                warn!("Error fetching transactions: {e}");
                if retry_count >= max_retries {
                    error!("Max retries exceeded, returning empty");
                    return vec![];
                }
            }
        }
        
        retry_count += 1;
        // Exponential backoff: 100ms, 200ms, 400ms, 800ms, ... up to 30s
        let backoff_ms = min(100 * (1 << retry_count), 30_000);
        // Add jitter to prevent thundering herd
        let jitter_ms = rand::random::<u64>() % 100;
        sleep(Duration::from_millis(backoff_ms + jitter_ms)).await;
    }
}
```

Additional improvements:
1. Add circuit breaker pattern to stop retrying when server is consistently failing
2. Implement proper timeout configuration for gRPC calls
3. Add metrics to track retry rates and failures
4. Consider returning errors instead of empty vectors after max retries

## Proof of Concept

```rust
// Integration test demonstrating the vulnerability
#[tokio::test]
async fn test_retry_storm_on_transient_failure() {
    use std::sync::atomic::{AtomicU64, Ordering};
    use std::sync::Arc;
    use std::time::Instant;
    
    // Mock a failing gRPC server
    let request_count = Arc::new(AtomicU64::new(0));
    let request_count_clone = request_count.clone();
    
    // Simulate the data_client behavior with a failing server
    let start = Instant::now();
    let timeout = Duration::from_secs(1);
    
    tokio::spawn(async move {
        // Simulate the tight retry loop from data_client.rs
        loop {
            if start.elapsed() > timeout {
                break;
            }
            // Simulate failed gRPC call
            let _ = simulate_failing_grpc_call();
            request_count_clone.fetch_add(1, Ordering::SeqCst);
            // No backoff - immediate retry like the vulnerable code
        }
    });
    
    tokio::time::sleep(timeout).await;
    
    let total_requests = request_count.load(Ordering::SeqCst);
    
    // With no backoff, expect thousands of requests in 1 second
    // This demonstrates the resource exhaustion vulnerability
    assert!(total_requests > 1000, 
        "Expected retry storm with >1000 requests/sec, got {}", 
        total_requests);
    
    println!("Retry storm generated {} requests in 1 second", total_requests);
    println!("This would overwhelm the gRPC server and exhaust client resources");
}

fn simulate_failing_grpc_call() -> Result<(), tonic::Status> {
    Err(tonic::Status::unavailable("Server temporarily unavailable"))
}
```

## Notes

While the security question identifies `fetch_manager.rs`, the actual vulnerability exists in the `fetch_transactions()` method in `data_client.rs` that is called by the fetch manager. The `fetch_latest_data()` function does have a 200ms backoff, but this only applies to the outer loop when no transactions are returned—it provides no protection against the infinite tight retry loop within `fetch_transactions()` when gRPC calls fail. The TODO comment on line 41 explicitly acknowledges this missing error handling.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/fetch_manager.rs (L48-64)
```rust
    async fn fetch_and_update_cache(
        data_client: Arc<DataClient>,
        data_manager: Arc<RwLock<DataManager>>,
        version: u64,
    ) -> usize {
        let transactions = data_client.fetch_transactions(version).await;
        let len = transactions.len();

        if len > 0 {
            data_manager
                .write()
                .await
                .update_data(version, transactions);
        }

        len
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/fetch_manager.rs (L66-87)
```rust
    async fn fetch_latest_data(&'a self) -> usize {
        let version = self.data_manager.read().await.end_version;
        info!("Fetching latest data starting from version {version}.");
        loop {
            let num_transactions = {
                let _timer = TIMER
                    .with_label_values(&["fetch_latest_data"])
                    .start_timer();
                Self::fetch_and_update_cache(
                    self.data_client.clone(),
                    self.data_manager.clone(),
                    version,
                )
                .await
            };
            if num_transactions != 0 {
                info!("Finished fetching latest data, got {num_transactions} num_transactions starting from version {version}.");
                return num_transactions;
            }
            tokio::time::sleep(Duration::from_millis(200)).await;
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/data_client.rs (L18-43)
```rust
    pub(super) async fn fetch_transactions(&self, starting_version: u64) -> Vec<Transaction> {
        trace!("Fetching transactions from GrpcManager, start_version: {starting_version}.");

        let request = GetTransactionsRequest {
            starting_version: Some(starting_version),
            transactions_count: None,
            batch_size: None,
            transaction_filter: None,
        };
        loop {
            let mut client = self
                .connection_manager
                .get_grpc_manager_client_for_request();
            let response = client.get_transactions(request.clone()).await;
            if let Ok(response) = response {
                let transactions = response.into_inner().transactions;
                if transactions.is_empty() {
                    return vec![];
                }
                if transactions.first().unwrap().version == starting_version {
                    return transactions;
                }
            }
            // TODO(grao): Error handling.
        }
    }
```
