# Audit Report

## Title
Unauthenticated Remote DoS via Array Index Out-of-Bounds Panic in RemoteStateViewService

## Summary
The `RemoteStateViewService::handle_message()` function contains an unchecked array indexing operation using an attacker-controlled `shard_id` value. Combined with an unauthenticated GRPC endpoint and a global panic handler that terminates the process, this enables any network peer to crash validator nodes running sharded execution. [1](#0-0) 

## Finding Description

While Rayon's thread pool correctly isolates panics at the thread level, preventing cross-thread contamination and shared state corruption, the Aptos node has a global panic handler that intercepts **all** panics (including those from Rayon threads) and terminates the entire process. [2](#0-1) 

This panic handler is installed during node startup: [3](#0-2) 

The critical vulnerability lies in `handle_message()` where the `shard_id` extracted from an unauthenticated remote request is used directly as an array index without bounds checking: [4](#0-3) [5](#0-4) 

The GRPC service receiving these messages has no authentication: [6](#0-5) 

**Attack Flow:**
1. Attacker crafts a `RemoteKVRequest` with `shard_id = 999999` (or any value >= number of configured shards)
2. Serializes it using BCS and sends via unauthenticated GRPC to the coordinator node
3. Message is deserialized successfully at line 86
4. At line 121, `kv_tx[999999]` triggers array index out-of-bounds panic
5. Rayon catches the panic within its thread pool (preventing cross-thread propagation)
6. Global panic handler intercepts the panic
7. Since BCS deserialization doesn't set `VMState::DESERIALIZER` (only Move bytecode deserialization does), the exception at lines 52-54 doesn't apply
8. Process terminates with exit code 12 [7](#0-6) 

This service is instantiated when remote sharded execution is enabled: [8](#0-7) [9](#0-8) 

## Impact Explanation

**Critical Severity** - This vulnerability enables **total loss of liveness/network availability**. An attacker with network access to any validator node running sharded execution can:

1. **Crash individual validator nodes** repeatedly with trivial requests
2. **Cascade failure**: If multiple validators enable sharded execution with exposed endpoints, systematic attacks could take down substantial portions of the network
3. **No recovery without restart**: Each crash requires manual intervention to restart the node

This breaks the **Consensus Safety** invariant by potentially reducing the number of active validators below the Byzantine fault tolerance threshold, and the **Resource Limits** invariant by allowing unbounded process terminations without proper resource cleanup.

## Likelihood Explanation

**High Likelihood** if sharded execution is enabled:
- Zero authentication on GRPC endpoint
- Trivial exploit requiring only network access and BCS serialization
- No rate limiting or input validation
- Attacker needs only to know the coordinator's listen address
- Single malformed message causes immediate node crash

**Medium Likelihood** overall:
- Feature must be explicitly enabled via `set_remote_addresses()`
- May be deployed only in specific high-performance configurations
- Likely intended for trusted internal networks, but network isolation failures occur

## Recommendation

Implement defense-in-depth:

1. **Immediate fix - Add bounds checking:**
```rust
pub fn handle_message(
    message: Message,
    state_view: Arc<RwLock<Option<Arc<S>>>>,
    kv_tx: Arc<Vec<Sender<Message>>>,
) {
    let req: RemoteKVRequest = match bcs::from_bytes(&message.data) {
        Ok(req) => req,
        Err(e) => {
            error!("Failed to deserialize RemoteKVRequest: {}", e);
            return;
        }
    };
    
    let (shard_id, state_keys) = req.into();
    
    // Bounds check
    if shard_id >= kv_tx.len() {
        error!("Invalid shard_id {} (max: {})", shard_id, kv_tx.len());
        return;
    }
    
    // ... rest of function with error handling instead of unwrap()
}
```

2. **Replace all `.unwrap()` calls with proper error handling** that logs and returns instead of panicking

3. **Add authentication** to the GRPC service using mTLS or token-based auth

4. **Network isolation**: Document that this service must only be exposed to trusted cluster networks

5. **Panic recovery**: Consider using `std::panic::catch_unwind` in the rayon spawn to handle panics gracefully without process termination

## Proof of Concept

```rust
use bcs;
use aptos_types::state_store::state_key::StateKey;
use execution_executor_service::{RemoteKVRequest};

#[test]
fn test_malicious_shard_id() {
    // Attacker crafts request with out-of-bounds shard_id
    let malicious_request = RemoteKVRequest::new(
        999999, // shard_id far exceeding expected range
        vec![],  // empty keys, doesn't matter
    );
    
    // Serialize with BCS (same format the service expects)
    let serialized = bcs::to_bytes(&malicious_request).unwrap();
    
    // Send this to the GRPC endpoint's simple_msg_exchange method
    // The coordinator's handle_message will panic on line 121:
    // kv_tx[999999] <- index out of bounds
    // Global panic handler will terminate the entire node process
    
    assert!(serialized.len() > 0); // PoC: serialization succeeds
    // In production: send via GRPC to crash target node
}
```

**Notes**

To answer the original security question directly:

**"Does rayon properly isolate the panic or can it affect other threads in the pool or corrupt shared state?"**

- ✅ Rayon **properly isolates** panics between threads in the pool
- ✅ Panics **cannot affect other threads** in the pool  
- ✅ Panics **cannot corrupt shared state** (only read operations performed)
- ❌ **However**, the global panic handler circumvents Rayon's isolation by terminating the entire process

The vulnerability exists not because Rayon fails to isolate panics, but because the application-level panic handler makes Rayon's isolation meaningless by killing the process. The lack of input validation combined with unauthenticated network access enables DoS attacks.

### Citations

**File:** execution/executor-service/src/remote_state_view_service.rs (L86-121)
```rust
        let req: RemoteKVRequest = bcs::from_bytes(&message.data).unwrap();
        drop(bcs_deser_timer);

        let (shard_id, state_keys) = req.into();
        trace!(
            "remote state view service - received request for shard {} with {} keys",
            shard_id,
            state_keys.len()
        );
        let resp = state_keys
            .into_iter()
            .map(|state_key| {
                let state_value = state_view
                    .read()
                    .unwrap()
                    .as_ref()
                    .unwrap()
                    .get_state_value(&state_key)
                    .unwrap();
                (state_key, state_value)
            })
            .collect_vec();
        let len = resp.len();
        let resp = RemoteKVResponse::new(resp);
        let bcs_ser_timer = REMOTE_EXECUTOR_TIMER
            .with_label_values(&["0", "kv_resp_ser"])
            .start_timer();
        let resp = bcs::to_bytes(&resp).unwrap();
        drop(bcs_ser_timer);
        trace!(
            "remote state view service - sending response for shard {} with {} keys",
            shard_id,
            len
        );
        let message = Message::new(resp);
        kv_tx[shard_id].send(message).unwrap();
```

**File:** crates/crash-handler/src/lib.rs (L26-57)
```rust
pub fn setup_panic_handler() {
    panic::set_hook(Box::new(move |pi: &PanicHookInfo<'_>| {
        handle_panic(pi);
    }));
}

// Formats and logs panic information
fn handle_panic(panic_info: &PanicHookInfo<'_>) {
    // The Display formatter for a PanicHookInfo contains the message, payload and location.
    let details = format!("{}", panic_info);
    let backtrace = format!("{:#?}", Backtrace::new());

    let info = CrashInfo { details, backtrace };
    let crash_info = toml::to_string_pretty(&info).unwrap();
    error!("{}", crash_info);
    // TODO / HACK ALARM: Write crash info synchronously via eprintln! to ensure it is written before the process exits which error! doesn't guarantee.
    // This is a workaround until https://github.com/aptos-labs/aptos-core/issues/2038 is resolved.
    eprintln!("{}", crash_info);

    // Wait till the logs have been flushed
    aptos_logger::flush();

    // Do not kill the process if the panics happened at move-bytecode-verifier.
    // This is safe because the `state::get_state()` uses a thread_local for storing states. Thus the state can only be mutated to VERIFIER by the thread that's running the bytecode verifier.
    //
    // TODO: once `can_unwind` is stable, we should assert it. See https://github.com/rust-lang/rust/issues/92988.
    if state::get_state() == VMState::VERIFIER || state::get_state() == VMState::DESERIALIZER {
        return;
    }

    // Kill the process
    process::exit(12);
```

**File:** aptos-node/src/lib.rs (L233-234)
```rust
    // Setup panic handler
    aptos_crash_handler::setup_panic_handler();
```

**File:** secure/net/src/grpc_network_service/mod.rs (L93-115)
```rust
    async fn simple_msg_exchange(
        &self,
        request: Request<NetworkMessage>,
    ) -> Result<Response<Empty>, Status> {
        let _timer = NETWORK_HANDLER_TIMER
            .with_label_values(&[&self.self_addr.to_string(), "inbound_msgs"])
            .start_timer();
        let remote_addr = request.remote_addr();
        let network_message = request.into_inner();
        let msg = Message::new(network_message.message);
        let message_type = MessageType::new(network_message.message_type);

        if let Some(handler) = self.inbound_handlers.lock().unwrap().get(&message_type) {
            // Send the message to the registered handler
            handler.send(msg).unwrap();
        } else {
            error!(
                "No handler registered for sender: {:?} and msg type {:?}",
                remote_addr, message_type
            );
        }
        Ok(Response::new(Empty {}))
    }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L121-132)
```rust
        let state_view_service = Arc::new(RemoteStateViewService::new(
            controller_mut_ref,
            remote_shard_addresses,
            None,
        ));

        let state_view_service_clone = state_view_service.clone();

        let join_handle = thread::Builder::new()
            .name("remote-state_view-service".to_string())
            .spawn(move || state_view_service_clone.start())
            .unwrap();
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L261-267)
```rust
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
```
