# Audit Report

## Title
Consensus Algorithm Upgrade Can Cause Chain Split Due to Deserialization Fallback

## Summary
During proposer election algorithm upgrades, validators running different code versions will disagree on the valid proposer for each round, causing consensus failure. This occurs because validators that fail to deserialize new algorithm variants silently fall back to a default configuration, breaking the fundamental consensus safety invariant.

## Finding Description

The Aptos consensus system allows the proposer election algorithm to be upgraded via on-chain governance through the `ConsensusConfig` stored at `@aptos_framework`. [1](#0-0) 

The `ProposerElectionType` enum defines multiple algorithm variants (FixedProposer, RotatingProposer, LeaderReputation, RoundProposer). [2](#0-1) 

During epoch transitions, validators fetch this configuration from their local database at the reconfiguration version. [3](#0-2) 

**The Critical Flaw:**

When deserialization fails (e.g., old validators encountering a new algorithm variant), validators silently fall back to `OnChainConsensusConfig::default()`, which uses `LeaderReputation` with specific default parameters. [4](#0-3) 

This creates a consensus split:
1. The `EpochManager` creates different `ProposerElection` implementations based on the deserialized config [5](#0-4) 
2. Each round, different algorithms compute different valid proposers
3. When validating proposals, `RoundManager` strictly checks if the author matches the expected proposer [6](#0-5) 
4. The validation uses `is_valid_proposer()` which compares against algorithm-specific results [7](#0-6) 

**Attack Scenario:**

1. Network has validators on code version V1
2. Governance proposal adds new `ProposerElectionType` variant or changes enum structure
3. Config update is staged via `set_for_next_epoch()` [8](#0-7) 
4. Partial validator upgrade: Some upgrade to V2, others remain on V1
5. Epoch transition applies new config via `on_new_epoch()` [9](#0-8) 
6. V1 validators fail BCS deserialization (unknown enum variant), fall back to default
7. V2 validators successfully deserialize new algorithm
8. For each round: V1 expects proposer X, V2 expects proposer Y
9. V1 validators reject proposals from Y (invalid proposer)
10. V2 validators reject proposals from X (invalid proposer)
11. **Result**: Chain split if both groups have >1/3 voting power, or liveness failure if neither has >2/3

## Impact Explanation

This meets **Critical Severity** criteria for "Consensus/Safety violations" and "Non-recoverable network partition (requires hardfork)". 

The vulnerability breaks the fundamental consensus safety invariant: "AptosBFT must prevent double-spending and chain splits under < 1/3 Byzantine". When validators disagree on valid proposers, they effectively fork into incompatible consensus groups, violating Byzantine fault tolerance guarantees even with 0% malicious validators.

The impact is network-wide and requires either:
- Emergency hardfork to revert the config change
- Forcing all validators to upgrade/downgrade simultaneously
- Manual coordination to resolve the split

## Likelihood Explanation

**Likelihood: HIGH**

This will occur during any proposer algorithm upgrade that introduces new enum variants or incompatible changes unless:
1. ALL validators upgrade synchronously (unrealistic in production)
2. The upgrade is preceded by a mandatory validator upgrade (not enforced by protocol)
3. Feature flags gate the algorithm change (no such mechanism exists)

The scenario requires:
- ✅ Governance approval (standard for protocol upgrades)
- ✅ Validators running mixed versions (common during rolling upgrades)
- ✅ No additional attacker capabilities needed

This is a protocol design flaw that manifests during legitimate operations, not a complex exploit.

## Recommendation

Implement a multi-phase upgrade coordination mechanism:

**Phase 1: Version Signaling**
- Add `supported_proposer_election_versions` field to `ValidatorConfig`
- Validators signal which algorithm versions they support
- Before applying config change, verify >2/3 validators support new version

**Phase 2: Feature Flag Gating**
- Add feature flag (e.g., `PROPOSER_ELECTION_V2_ENABLED`) to gate new algorithms
- Require feature flag activation before config using new algorithms
- Ensure all validators upgrade code before feature activation

**Phase 3: Graceful Degradation**
- Remove `unwrap_or_default()` fallback
- Panic on deserialization failure with clear error message
- Force operators to investigate and fix version mismatches immediately

**Code Fix Example:**
```rust
// In epoch_manager.rs start_new_epoch
let consensus_config = onchain_consensus_config
    .expect("CRITICAL: Failed to deserialize OnChainConsensusConfig. \
             This validator may be running outdated code. \
             Chain safety requires immediate investigation.");
// Remove: .unwrap_or_default()
```

## Proof of Concept

```rust
// Reproduction test (add to consensus/src/epoch_manager_tests.rs)
#[tokio::test]
async fn test_proposer_algorithm_mismatch_causes_split() {
    // Setup: Create two validators with different code versions
    let mut validator_v1 = create_validator_with_proposer_config(
        ProposerElectionType::LeaderReputation(
            LeaderReputationType::ProposerAndVoterV2(default_config())
        )
    );
    
    let mut validator_v2 = create_validator_with_proposer_config(
        ProposerElectionType::NewAlgorithm // Hypothetical new variant
    );
    
    // Simulate epoch transition with new config
    let new_config = create_consensus_config_with_new_algorithm();
    
    // V1 fails to deserialize, falls back to default
    // V2 successfully deserializes
    
    // For round N:
    let round = 100;
    let proposer_v1 = validator_v1.proposer_election.get_valid_proposer(round);
    let proposer_v2 = validator_v2.proposer_election.get_valid_proposer(round);
    
    // Assert: Validators disagree on valid proposer
    assert_ne!(proposer_v1, proposer_v2, 
        "Consensus split: validators disagree on proposer for round {}", round);
    
    // When proposal arrives from proposer_v2:
    let proposal = create_proposal(proposer_v2, round);
    
    // V1 rejects (invalid proposer)
    assert!(!validator_v1.proposer_election.is_valid_proposal(&proposal));
    
    // V2 accepts (valid proposer)
    assert!(validator_v2.proposer_election.is_valid_proposal(&proposal));
    
    // Result: Consensus failure - no quorum can be reached
}
```

## Notes

The vulnerability exists in the interaction between:
- On-chain config storage [10](#0-9) 
- Deserialization with silent fallback [3](#0-2) 
- Strict proposer validation [6](#0-5) 

The BCS deserialization behavior with unknown enum variants is confirmed to fail rather than gracefully handle unknowns, but the error is caught and silently replaced with defaults, breaking consensus safety during upgrades.

### Citations

**File:** aptos-move/framework/aptos-framework/sources/configs/consensus_config.move (L1-77)
```text
/// Maintains the consensus config for the blockchain. The config is stored in a
/// Reconfiguration, and may be updated by root.
module aptos_framework::consensus_config {
    use std::error;
    use std::vector;
    use aptos_framework::chain_status;
    use aptos_framework::config_buffer;

    use aptos_framework::reconfiguration;
    use aptos_framework::system_addresses;

    friend aptos_framework::genesis;
    friend aptos_framework::reconfiguration_with_dkg;

    struct ConsensusConfig has drop, key, store {
        config: vector<u8>,
    }

    /// The provided on chain config bytes are empty or invalid
    const EINVALID_CONFIG: u64 = 1;

    /// Publishes the ConsensusConfig config.
    public(friend) fun initialize(aptos_framework: &signer, config: vector<u8>) {
        system_addresses::assert_aptos_framework(aptos_framework);
        assert!(vector::length(&config) > 0, error::invalid_argument(EINVALID_CONFIG));
        move_to(aptos_framework, ConsensusConfig { config });
    }

    /// Deprecated by `set_for_next_epoch()`.
    ///
    /// WARNING: calling this while randomness is enabled will trigger a new epoch without randomness!
    ///
    /// TODO: update all the tests that reference this function, then disable this function.
    public fun set(account: &signer, config: vector<u8>) acquires ConsensusConfig {
        system_addresses::assert_aptos_framework(account);
        chain_status::assert_genesis();
        assert!(vector::length(&config) > 0, error::invalid_argument(EINVALID_CONFIG));

        let config_ref = &mut borrow_global_mut<ConsensusConfig>(@aptos_framework).config;
        *config_ref = config;

        // Need to trigger reconfiguration so validator nodes can sync on the updated configs.
        reconfiguration::reconfigure();
    }

    /// This can be called by on-chain governance to update on-chain consensus configs for the next epoch.
    /// Example usage:
    /// ```
    /// aptos_framework::consensus_config::set_for_next_epoch(&framework_signer, some_config_bytes);
    /// aptos_framework::aptos_governance::reconfigure(&framework_signer);
    /// ```
    public fun set_for_next_epoch(account: &signer, config: vector<u8>) {
        system_addresses::assert_aptos_framework(account);
        assert!(vector::length(&config) > 0, error::invalid_argument(EINVALID_CONFIG));
        std::config_buffer::upsert<ConsensusConfig>(ConsensusConfig {config});
    }

    /// Only used in reconfigurations to apply the pending `ConsensusConfig`, if there is any.
    public(friend) fun on_new_epoch(framework: &signer) acquires ConsensusConfig {
        system_addresses::assert_aptos_framework(framework);
        if (config_buffer::does_exist<ConsensusConfig>()) {
            let new_config = config_buffer::extract_v2<ConsensusConfig>();
            if (exists<ConsensusConfig>(@aptos_framework)) {
                *borrow_global_mut<ConsensusConfig>(@aptos_framework) = new_config;
            } else {
                move_to(framework, new_config);
            };
        }
    }

    public fun validator_txn_enabled(): bool acquires ConsensusConfig {
        let config_bytes = borrow_global<ConsensusConfig>(@aptos_framework).config;
        validator_txn_enabled_internal(config_bytes)
    }

    native fun validator_txn_enabled_internal(config_bytes: vector<u8>): bool;
}
```

**File:** types/src/on_chain_config/consensus_config.rs (L443-450)
```rust
impl Default for OnChainConsensusConfig {
    fn default() -> Self {
        OnChainConsensusConfig::V4 {
            alg: ConsensusAlgorithmConfig::default_if_missing(),
            vtxn: ValidatorTxnConfig::default_if_missing(),
            window_size: DEFAULT_WINDOW_SIZE,
        }
    }
```

**File:** types/src/on_chain_config/consensus_config.rs (L508-523)
```rust
#[derive(Clone, Debug, Deserialize, Eq, PartialEq, Serialize)]
#[serde(rename_all = "snake_case")] // cannot use tag = "type" as nested enums cannot work, and bcs doesn't support it
pub enum ProposerElectionType {
    // Choose the smallest PeerId as the proposer
    // with specified param contiguous_rounds
    FixedProposer(u32),
    // Round robin rotation of proposers
    // with specified param contiguous_rounds
    RotatingProposer(u32),
    // Committed history based proposer election
    LeaderReputation(LeaderReputationType),
    // Pre-specified proposers for each round,
    // or default proposer if round proposer not
    // specified
    RoundProposer(HashMap<Round, AccountAddress>),
}
```

**File:** consensus/src/epoch_manager.rs (L287-406)
```rust
    fn create_proposer_election(
        &self,
        epoch_state: &EpochState,
        onchain_config: &OnChainConsensusConfig,
    ) -> Arc<dyn ProposerElection + Send + Sync> {
        let proposers = epoch_state
            .verifier
            .get_ordered_account_addresses_iter()
            .collect::<Vec<_>>();
        match &onchain_config.proposer_election_type() {
            ProposerElectionType::RotatingProposer(contiguous_rounds) => {
                Arc::new(RotatingProposer::new(proposers, *contiguous_rounds))
            },
            // We don't really have a fixed proposer!
            ProposerElectionType::FixedProposer(contiguous_rounds) => {
                let proposer = choose_leader(proposers);
                Arc::new(RotatingProposer::new(vec![proposer], *contiguous_rounds))
            },
            ProposerElectionType::LeaderReputation(leader_reputation_type) => {
                let (
                    heuristic,
                    window_size,
                    weight_by_voting_power,
                    use_history_from_previous_epoch_max_count,
                ) = match &leader_reputation_type {
                    LeaderReputationType::ProposerAndVoter(proposer_and_voter_config)
                    | LeaderReputationType::ProposerAndVoterV2(proposer_and_voter_config) => {
                        let proposer_window_size = proposers.len()
                            * proposer_and_voter_config.proposer_window_num_validators_multiplier;
                        let voter_window_size = proposers.len()
                            * proposer_and_voter_config.voter_window_num_validators_multiplier;
                        let heuristic: Box<dyn ReputationHeuristic> =
                            Box::new(ProposerAndVoterHeuristic::new(
                                self.author,
                                proposer_and_voter_config.active_weight,
                                proposer_and_voter_config.inactive_weight,
                                proposer_and_voter_config.failed_weight,
                                proposer_and_voter_config.failure_threshold_percent,
                                voter_window_size,
                                proposer_window_size,
                                leader_reputation_type.use_reputation_window_from_stale_end(),
                            ));
                        (
                            heuristic,
                            std::cmp::max(proposer_window_size, voter_window_size),
                            proposer_and_voter_config.weight_by_voting_power,
                            proposer_and_voter_config.use_history_from_previous_epoch_max_count,
                        )
                    },
                };

                let seek_len = onchain_config.leader_reputation_exclude_round() as usize
                    + onchain_config.max_failed_authors_to_store()
                    + PROPOSER_ROUND_BEHIND_STORAGE_BUFFER;

                let backend = Arc::new(AptosDBBackend::new(
                    window_size,
                    seek_len,
                    self.storage.aptos_db(),
                ));
                let voting_powers: Vec<_> = if weight_by_voting_power {
                    proposers
                        .iter()
                        .map(|p| {
                            epoch_state
                                .verifier
                                .get_voting_power(p)
                                .expect("INVARIANT VIOLATION: proposer not in verifier set")
                        })
                        .collect()
                } else {
                    vec![1; proposers.len()]
                };

                let epoch_to_proposers = self.extract_epoch_proposers(
                    epoch_state,
                    use_history_from_previous_epoch_max_count,
                    proposers,
                    (window_size + seek_len) as u64,
                );

                info!(
                    "Starting epoch {}: proposers across epochs for leader election: {:?}",
                    epoch_state.epoch,
                    epoch_to_proposers
                        .iter()
                        .map(|(epoch, proposers)| (epoch, proposers.len()))
                        .sorted()
                        .collect::<Vec<_>>()
                );

                let proposer_election = Box::new(LeaderReputation::new(
                    epoch_state.epoch,
                    epoch_to_proposers,
                    voting_powers,
                    backend,
                    heuristic,
                    onchain_config.leader_reputation_exclude_round(),
                    leader_reputation_type.use_root_hash_for_seed(),
                    self.config.window_for_chain_health,
                ));
                // LeaderReputation is not cheap, so we can cache the amount of rounds round_manager needs.
                Arc::new(CachedProposerElection::new(
                    epoch_state.epoch,
                    proposer_election,
                    onchain_config.max_failed_authors_to_store()
                        + PROPOSER_ELECTION_CACHING_WINDOW_ADDITION,
                ))
            },
            ProposerElectionType::RoundProposer(round_proposers) => {
                // Hardcoded to the first proposer
                let default_proposer = proposers
                    .first()
                    .expect("INVARIANT VIOLATION: proposers is empty");
                Arc::new(RoundProposer::new(
                    round_proposers.clone(),
                    *default_proposer,
                ))
            },
        }
```

**File:** consensus/src/epoch_manager.rs (L1178-1201)
```rust
        let onchain_consensus_config: anyhow::Result<OnChainConsensusConfig> = payload.get();
        let onchain_execution_config: anyhow::Result<OnChainExecutionConfig> = payload.get();
        let onchain_randomness_config_seq_num: anyhow::Result<RandomnessConfigSeqNum> =
            payload.get();
        let randomness_config_move_struct: anyhow::Result<RandomnessConfigMoveStruct> =
            payload.get();
        let onchain_jwk_consensus_config: anyhow::Result<OnChainJWKConsensusConfig> = payload.get();
        let dkg_state = payload.get::<DKGState>();

        if let Err(error) = &onchain_consensus_config {
            warn!("Failed to read on-chain consensus config {}", error);
        }

        if let Err(error) = &onchain_execution_config {
            warn!("Failed to read on-chain execution config {}", error);
        }

        if let Err(error) = &randomness_config_move_struct {
            warn!("Failed to read on-chain randomness config {}", error);
        }

        self.epoch_state = Some(epoch_state.clone());

        let consensus_config = onchain_consensus_config.unwrap_or_default();
```

**File:** consensus/src/round_manager.rs (L1195-1200)
```rust
        ensure!(
            self.proposer_election.is_valid_proposal(&proposal),
            "[RoundManager] Proposer {} for block {} is not a valid proposer for this round or created duplicate proposal",
            author,
            proposal,
        );
```

**File:** consensus/src/liveness/unequivocal_proposer_election.rs (L46-60)
```rust
    pub fn is_valid_proposal(&self, block: &Block) -> bool {
        block.author().is_some_and(|author| {
            let valid_author = self.is_valid_proposer(author, block.round());
            if !valid_author {
                warn!(
                    SecurityEvent::InvalidConsensusProposal,
                    "Proposal is not from valid author {}, expected {} for round {} and id {}",
                    author,
                    self.get_valid_proposer(block.round()),
                    block.round(),
                    block.id()
                );

                return false;
            }
```
