# Audit Report

## Title
Malicious Validator Can Cause Permanent Liveness Failure via Invalid Block Data in Decoupled Execution Mode

## Summary
A malicious validator can craft a ProposalMsg with structurally valid format but containing block data that causes execution failures. Due to the decoupled execution architecture and default configuration (`discard_failed_blocks = false`), the failed block remains stuck in "Ordered" state, can never be committed, and permanently halts all consensus progress network-wide.

## Finding Description

The vulnerability exploits a critical flaw in the interaction between Aptos's decoupled execution mode and error handling in the consensus pipeline.

**Attack Flow:**

1. **Malicious Proposal Creation**: A Byzantine validator creates a ProposalMsg containing a Block with valid structural properties (correct signatures, format, round progression, QC) but with payload data that will cause execution errors. The preliminary validation in `process_proposal` only performs structural checks including proposer validation, transaction limits, and timestamp validation, but does not execute transactions. [1](#0-0) 

2. **Decoupled Voting**: Because `decoupled_execution` is hardcoded to `true`, honest validators vote on block ordering using a dummy `ACCUMULATOR_PLACEHOLDER_HASH` without waiting for execution to complete. [2](#0-1) [3](#0-2) [4](#0-3) 

3. **Block Ordering**: The malicious block receives sufficient votes, gets a QC, and is ordered by consensus. It's inserted into the buffer manager as an "Ordered" item. [5](#0-4) 

4. **Execution Failure**: The execution pipeline attempts to execute the block. With the default configuration where `discard_failed_blocks = false`, the execution fails and returns an error. [6](#0-5) [7](#0-6) 

5. **Error Handling Failure**: The `process_execution_response` function receives the error, logs it, and returns early without advancing the block from "Ordered" to "Executed" state. [8](#0-7) 

6. **Aggregation Impossibility**: When validators attempt to aggregate commit votes, the `try_advance_to_aggregated_with_ledger_info` function cannot advance an "Ordered" block to "Aggregated" state because execution never completed. The code explicitly states "can't aggregate it without execution, only store the signatures". [9](#0-8) 

7. **No Automatic Retry**: While `advance_execution_root` returns a retry signal when the execution root hasn't advanced, all call sites ignore this return value, so no retry actually occurs. [10](#0-9) [11](#0-10) [12](#0-11) 

8. **Permanent Liveness Failure**: The `advance_head` function requires blocks to be in "Aggregated" state to commit them. It calls `unwrap_aggregated()` which will panic if the item is not aggregated. Since the malicious block can never reach this state, it blocks all subsequent blocks from being committed. [13](#0-12) 

**Broken Invariants:**
- **Consensus Liveness**: The network cannot make progress past the malicious block
- **State Consistency**: Validators cannot commit new state transitions  
- **Availability**: The blockchain effectively halts for all users

## Impact Explanation

This vulnerability meets **Critical Severity** criteria per the Aptos bug bounty program for "Total Loss of Liveness/Network Availability":

- **Complete Network Halt**: Once triggered, the entire network halts and cannot commit any new blocks. All validator nodes are affected simultaneously because they all process the same ordered block that fails execution.

- **Non-recoverable without Manual Intervention**: The default configuration provides no automatic recovery mechanism. The mitigation feature `discard_failed_blocks` exists but is explicitly disabled by default. [14](#0-13)  Recovery would require:
  - A coordinated manual configuration change to enable `discard_failed_blocks = true` and restart all validators
  - A state sync from an external source
  - Potentially a hard fork

- **Maximum Impact**: A single malicious validator can execute the attack, all honest validators are affected, no transactions can be committed network-wide, user funds become inaccessible, and the attack is repeatable.

## Likelihood Explanation

**High Likelihood** due to:

1. **Low Attack Complexity**: The attacker only needs to be a validator in the rotation and craft block data that fails execution while passing preliminary structural validation.

2. **No Collusion Required**: A single Byzantine validator can execute the attack during their proposal turn (within BFT threat model of < 1/3 Byzantine validators).

3. **Immediate Impact**: The attack takes effect within one consensus round.

4. **Default Configuration Vulnerable**: The critical `discard_failed_blocks` flag is set to `false` by default. [15](#0-14) 

5. **No Pre-execution Detection**: Because of decoupled execution, the malicious block passes all preliminary checks and receives votes before execution is attempted, providing no opportunity to reject it before impact.

## Recommendation

**Immediate Mitigation:**
1. Change the default value of `discard_failed_blocks` from `false` to `true` in production configurations to enable automatic recovery from execution failures. [6](#0-5) 

2. Implement automatic retry logic by utilizing the return value from `advance_execution_root()` that currently gets ignored. [10](#0-9) 

**Long-term Solutions:**
1. Add deeper pre-execution validation that can detect blocks likely to fail execution without performing full execution.
2. Implement a circuit breaker that automatically enables `discard_failed_blocks` when execution failures are detected.
3. Add monitoring and alerting for blocks stuck in "Ordered" state.

## Proof of Concept

While a complete PoC would require deploying a test network, the vulnerability logic can be demonstrated through the following test scenario:

1. Configure validator with `discard_failed_blocks = false` (default)
2. Configure consensus with `decoupled_execution = true` (hardcoded)
3. Craft a block containing a transaction that triggers a VM panic or execution error (e.g., resource exhaustion, division by zero in Move code)
4. Propose the block - it will pass preliminary validation
5. Observe validators vote on the block without executing it
6. Block receives QC and enters buffer as "Ordered"
7. Execution is attempted and fails
8. Block remains permanently in "Ordered" state
9. All subsequent blocks cannot be committed
10. Network liveness is lost

The test in `testsuite/smoke-test/src/execution.rs` demonstrates that with `discard_failed_blocks = true`, the network can tolerate execution failures, confirming that the default `false` value is vulnerable. [16](#0-15)

### Citations

**File:** consensus/src/round_manager.rs (L1111-1286)
```rust
    async fn process_proposal(&mut self, proposal: Block) -> anyhow::Result<()> {
        let author = proposal
            .author()
            .expect("Proposal should be verified having an author");

        if !self.vtxn_config.enabled()
            && matches!(
                proposal.block_data().block_type(),
                BlockType::ProposalExt(_)
            )
        {
            counters::UNEXPECTED_PROPOSAL_EXT_COUNT.inc();
            bail!("ProposalExt unexpected while the vtxn feature is disabled.");
        }

        if let Some(vtxns) = proposal.validator_txns() {
            for vtxn in vtxns {
                let vtxn_type_name = vtxn.type_name();
                ensure!(
                    is_vtxn_expected(&self.randomness_config, &self.jwk_consensus_config, vtxn),
                    "unexpected validator txn: {:?}",
                    vtxn_type_name
                );
                vtxn.verify(self.epoch_state.verifier.as_ref())
                    .context(format!("{} verify failed", vtxn_type_name))?;
            }
        }

        let (num_validator_txns, validator_txns_total_bytes): (usize, usize) =
            proposal.validator_txns().map_or((0, 0), |txns| {
                txns.iter().fold((0, 0), |(count_acc, size_acc), txn| {
                    (count_acc + 1, size_acc + txn.size_in_bytes())
                })
            });

        let num_validator_txns = num_validator_txns as u64;
        let validator_txns_total_bytes = validator_txns_total_bytes as u64;
        let vtxn_count_limit = self.vtxn_config.per_block_limit_txn_count();
        let vtxn_bytes_limit = self.vtxn_config.per_block_limit_total_bytes();
        let author_hex = author.to_hex();
        PROPOSED_VTXN_COUNT
            .with_label_values(&[&author_hex])
            .inc_by(num_validator_txns);
        PROPOSED_VTXN_BYTES
            .with_label_values(&[&author_hex])
            .inc_by(validator_txns_total_bytes);
        info!(
            vtxn_count_limit = vtxn_count_limit,
            vtxn_count_proposed = num_validator_txns,
            vtxn_bytes_limit = vtxn_bytes_limit,
            vtxn_bytes_proposed = validator_txns_total_bytes,
            proposer = author_hex,
            "Summarizing proposed validator txns."
        );

        ensure!(
            num_validator_txns <= vtxn_count_limit,
            "process_proposal failed with per-block vtxn count limit exceeded: limit={}, actual={}",
            self.vtxn_config.per_block_limit_txn_count(),
            num_validator_txns
        );
        ensure!(
            validator_txns_total_bytes <= vtxn_bytes_limit,
            "process_proposal failed with per-block vtxn bytes limit exceeded: limit={}, actual={}",
            self.vtxn_config.per_block_limit_total_bytes(),
            validator_txns_total_bytes
        );
        let payload_len = proposal.payload().map_or(0, |payload| payload.len());
        let payload_size = proposal.payload().map_or(0, |payload| payload.size());
        ensure!(
            num_validator_txns + payload_len as u64 <= self.local_config.max_receiving_block_txns,
            "Payload len {} exceeds the limit {}",
            payload_len,
            self.local_config.max_receiving_block_txns,
        );

        ensure!(
            validator_txns_total_bytes + payload_size as u64
                <= self.local_config.max_receiving_block_bytes,
            "Payload size {} exceeds the limit {}",
            payload_size,
            self.local_config.max_receiving_block_bytes,
        );

        ensure!(
            self.proposer_election.is_valid_proposal(&proposal),
            "[RoundManager] Proposer {} for block {} is not a valid proposer for this round or created duplicate proposal",
            author,
            proposal,
        );

        // If the proposal contains any inline transactions that need to be denied
        // (e.g., due to filtering) drop the message and do not vote for the block.
        if let Err(error) = self
            .block_store
            .check_denied_inline_transactions(&proposal, &self.block_txn_filter_config)
        {
            counters::REJECTED_PROPOSAL_DENY_TXN_COUNT.inc();
            bail!(
                "[RoundManager] Proposal for block {} contains denied inline transactions: {}. Dropping proposal!",
                proposal.id(),
                error
            );
        }

        if !proposal.is_opt_block() {
            // Validate that failed_authors list is correctly specified in the block.
            let expected_failed_authors = self.proposal_generator.compute_failed_authors(
                proposal.round(),
                proposal.quorum_cert().certified_block().round(),
                false,
                self.proposer_election.clone(),
            );
            ensure!(
                proposal.block_data().failed_authors().is_some_and(|failed_authors| *failed_authors == expected_failed_authors),
                "[RoundManager] Proposal for block {} has invalid failed_authors list {:?}, expected {:?}",
                proposal.round(),
                proposal.block_data().failed_authors(),
                expected_failed_authors,
            );
        }

        let block_time_since_epoch = Duration::from_micros(proposal.timestamp_usecs());

        ensure!(
            block_time_since_epoch < self.round_state.current_round_deadline(),
            "[RoundManager] Waiting until proposal block timestamp usecs {:?} \
            would exceed the round duration {:?}, hence will not vote for this round",
            block_time_since_epoch,
            self.round_state.current_round_deadline(),
        );

        observe_block(proposal.timestamp_usecs(), BlockStage::SYNCED);
        if proposal.is_opt_block() {
            observe_block(proposal.timestamp_usecs(), BlockStage::SYNCED_OPT_BLOCK);
        }

        // Since processing proposal is delayed due to backpressure or payload availability, we add
        // the block to the block store so that we don't need to fetch it from remote once we
        // are out of the backpressure. Please note that delayed processing of proposal is not
        // guaranteed to add the block to the block store if we don't get out of the backpressure
        // before the timeout, so this is needed to ensure that the proposed block is added to
        // the block store irrespective. Also, it is possible that delayed processing of proposal
        // tries to add the same block again, which is okay as `insert_block` call
        // is idempotent.
        self.block_store
            .insert_block(proposal.clone())
            .await
            .context("[RoundManager] Failed to insert the block into BlockStore")?;

        let block_store = self.block_store.clone();
        if block_store.check_payload(&proposal).is_err() {
            debug!("Payload not available locally for block: {}", proposal.id());
            counters::CONSENSUS_PROPOSAL_PAYLOAD_AVAILABILITY
                .with_label_values(&["missing"])
                .inc();
            let start_time = Instant::now();
            let deadline = self.round_state.current_round_deadline();
            let future = async move {
                (
                    block_store.wait_for_payload(&proposal, deadline).await,
                    proposal,
                    start_time,
                )
            }
            .boxed();
            self.futures.push(future);
            return Ok(());
        }

        counters::CONSENSUS_PROPOSAL_PAYLOAD_AVAILABILITY
            .with_label_values(&["available"])
            .inc();

        self.check_backpressure_and_process_proposal(proposal).await
    }
```

**File:** types/src/on_chain_config/consensus_config.rs (L239-241)
```rust
    pub fn decoupled_execution(&self) -> bool {
        true
    }
```

**File:** consensus/consensus-types/src/vote_proposal.rs (L59-69)
```rust
    /// This function returns the vote data with a dummy executed_state_id and version
    fn vote_data_ordering_only(&self) -> VoteData {
        VoteData::new(
            self.block().gen_block_info(
                *ACCUMULATOR_PLACEHOLDER_HASH,
                0,
                self.next_epoch_state().cloned(),
            ),
            self.block().quorum_cert().certified_block().clone(),
        )
    }
```

**File:** consensus/consensus-types/src/vote_proposal.rs (L88-91)
```rust
    pub fn gen_vote_data(&self) -> anyhow::Result<VoteData> {
        if self.decoupled_execution {
            Ok(self.vote_data_ordering_only())
        } else {
```

**File:** consensus/src/pipeline/buffer_manager.rs (L382-424)
```rust
    async fn process_ordered_blocks(&mut self, ordered_blocks: OrderedBlocks) {
        let OrderedBlocks {
            ordered_blocks,
            ordered_proof,
        } = ordered_blocks;

        info!(
            "Receive {} ordered block ends with [epoch: {}, round: {}, id: {}], the queue size is {}",
            ordered_blocks.len(),
            ordered_proof.commit_info().epoch(),
            ordered_proof.commit_info().round(),
            ordered_proof.commit_info().id(),
            self.buffer.len() + 1,
        );

        let request = self.create_new_request(ExecutionRequest {
            ordered_blocks: ordered_blocks.clone(),
        });
        if let Some(consensus_publisher) = &self.consensus_publisher {
            let message = ConsensusObserverMessage::new_ordered_block_message(
                ordered_blocks.clone(),
                ordered_proof.clone(),
            );
            consensus_publisher.publish_message(message);
        }
        self.execution_schedule_phase_tx
            .send(request)
            .await
            .expect("Failed to send execution schedule request");

        let mut unverified_votes = HashMap::new();
        if let Some(block) = ordered_blocks.last() {
            if let Some(votes) = self.pending_commit_votes.remove(&block.round()) {
                for (_, vote) in votes {
                    if vote.commit_info().id() == block.id() {
                        unverified_votes.insert(vote.author(), vote);
                    }
                }
            }
        }
        let item = BufferItem::new_ordered(ordered_blocks, ordered_proof, unverified_votes);
        self.buffer.push_back(item);
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L429-452)
```rust
    fn advance_execution_root(&mut self) -> Option<HashValue> {
        let cursor = self.execution_root;
        self.execution_root = self
            .buffer
            .find_elem_from(cursor.or_else(|| *self.buffer.head_cursor()), |item| {
                item.is_ordered()
            });
        if self.execution_root.is_some() && cursor == self.execution_root {
            // Schedule retry.
            self.execution_root
        } else {
            sample!(
                SampleRate::Frequency(2),
                info!(
                    "Advance execution root from {:?} to {:?}",
                    cursor, self.execution_root
                )
            );
            // Otherwise do nothing, because the execution wait phase is driven by the response of
            // the execution schedule phase, which is in turn fed as soon as the ordered blocks
            // come in.
            None
        }
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L492-541)
```rust
    async fn advance_head(&mut self, target_block_id: HashValue) {
        let mut blocks_to_persist: Vec<Arc<PipelinedBlock>> = vec![];

        while let Some(item) = self.buffer.pop_front() {
            blocks_to_persist.extend(item.get_blocks().clone());
            if self.signing_root == Some(item.block_id()) {
                self.signing_root = None;
            }
            if self.execution_root == Some(item.block_id()) {
                self.execution_root = None;
            }
            if item.block_id() == target_block_id {
                let aggregated_item = item.unwrap_aggregated();
                let block = aggregated_item
                    .executed_blocks
                    .last()
                    .expect("executed_blocks should be not empty")
                    .block();
                observe_block(block.timestamp_usecs(), BlockStage::COMMIT_CERTIFIED);
                // As all the validators broadcast commit votes directly to all other validators,
                // the proposer do not have to broadcast commit decision again.
                let commit_proof = aggregated_item.commit_proof.clone();
                if let Some(consensus_publisher) = &self.consensus_publisher {
                    let message =
                        ConsensusObserverMessage::new_commit_decision_message(commit_proof.clone());
                    consensus_publisher.publish_message(message);
                }
                for block in &blocks_to_persist {
                    self.pending_commit_blocks
                        .insert(block.round(), block.clone());
                }
                self.persisting_phase_tx
                    .send(self.create_new_request(PersistingRequest {
                        blocks: blocks_to_persist,
                        commit_ledger_info: aggregated_item.commit_proof,
                    }))
                    .await
                    .expect("Failed to send persist request");
                if commit_proof.ledger_info().ends_epoch() {
                    // the epoch ends, reset to avoid executing more blocks, execute after
                    // this persisting request will result in BlockNotFound
                    self.reset().await;
                }
                info!("Advance head to {:?}", self.buffer.head_cursor());
                self.previous_commit_time = Instant::now();
                return;
            }
        }
        unreachable!("Aggregated item not found in the list");
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L617-626)
```rust
        let executed_blocks = match inner {
            Ok(result) => result,
            Err(e) => {
                log_executor_error_occurred(
                    e,
                    &counters::BUFFER_MANAGER_RECEIVED_EXECUTOR_ERROR_COUNT,
                    block_id,
                );
                return;
            },
```

**File:** consensus/src/pipeline/buffer_manager.rs (L943-943)
```rust
                        self.advance_execution_root();
```

**File:** consensus/src/pipeline/buffer_manager.rs (L957-957)
```rust
                    self.advance_execution_root();
```

**File:** config/src/config/execution_config.rs (L78-96)
```rust
impl Default for ExecutionConfig {
    fn default() -> ExecutionConfig {
        ExecutionConfig {
            genesis: None,
            genesis_file_location: PathBuf::new(),
            // use min of (num of cores/2, DEFAULT_CONCURRENCY_LEVEL) as default concurrency level
            concurrency_level: 0,
            num_proof_reading_threads: 32,
            paranoid_type_verification: true,
            paranoid_hot_potato_verification: true,
            discard_failed_blocks: false,
            processed_transactions_detailed_counters: false,
            genesis_waypoint: None,
            blockstm_v2_enabled: false,
            layout_caches_enabled: true,
            // TODO: consider setting to be true by default.
            async_runtime_checks: false,
        }
    }
```

**File:** types/src/block_executor/config.rs (L60-62)
```rust
    // If true, we will discard the failed blocks and continue with the next block.
    // (allow_fallback needs to be set)
    pub discard_failed_blocks: bool,
```

**File:** types/src/block_executor/config.rs (L76-76)
```rust
            discard_failed_blocks: false,
```

**File:** consensus/src/pipeline/buffer_item.rs (L272-286)
```rust
            Self::Ordered(ordered_item) => {
                let ordered = *ordered_item;
                assert!(ordered
                    .ordered_proof
                    .commit_info()
                    .match_ordered_only(commit_proof.commit_info()));
                // can't aggregate it without execution, only store the signatures
                debug!(
                    "{} received commit decision in ordered stage",
                    commit_proof.commit_info()
                );
                Self::Ordered(Box::new(OrderedItem {
                    commit_proof: Some(commit_proof),
                    ..ordered
                }))
```

**File:** testsuite/smoke-test/src/execution.rs (L19-70)
```rust
#[tokio::test]
async fn fallback_test() {
    let swarm = SwarmBuilder::new_local(1)
        .with_init_config(Arc::new(|_, config, _| {
            config.api.failpoints_enabled = true;
            config.execution.discard_failed_blocks = true;
        }))
        .with_aptos()
        .build()
        .await;

    swarm
        .wait_for_all_nodes_to_catchup_to_epoch(2, Duration::from_secs(60))
        .await
        .expect("Epoch 2 taking too long to come!");

    let client = swarm.validators().next().unwrap().rest_client();

    client
        .set_failpoint(
            "aptos_vm::vm_wrapper::execute_transaction".to_string(),
            "100%return".to_string(),
        )
        .await
        .unwrap();

    let version_milestone_0 = get_current_version(&client).await;
    let version_milestone_1 = version_milestone_0 + 1;
    // We won't reach next version.
    assert!(swarm
        .wait_for_all_nodes_to_catchup_to_version(version_milestone_1, Duration::from_secs(5))
        .await
        .is_err());

    client
        .set_failpoint(
            "aptos_vm::vm_wrapper::execute_transaction".to_string(),
            "0%return".to_string(),
        )
        .await
        .unwrap();

    let version_milestone_2 = version_milestone_1 + 5;
    println!(
        "Current version: {}, the chain should tolerate discarding failed blocks, waiting for {}.",
        version_milestone_0, version_milestone_2
    );
    swarm
        .wait_for_all_nodes_to_catchup_to_version(version_milestone_2, Duration::from_secs(30))
        .await
        .expect("milestone 2 taking too long");
}
```
