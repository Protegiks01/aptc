# Audit Report

## Title
Resource Exhaustion via Unverified Remote Batch Transactions in Consensus Deduplication

## Summary
A malicious validator can broadcast batches containing transactions with invalid signatures to other validators. These transactions bypass signature verification, get persisted in batch storage, and later reach the deduplication stage where they consume CPU resources for expensive hash calculations before ultimately failing signature verification. This creates a resource exhaustion attack vector.

## Finding Description

The transaction deduplication mechanism explicitly acknowledges that it operates before signature verification: [1](#0-0) 

The normal transaction flow ensures signature verification occurs in mempool before transactions enter the consensus pipeline: [2](#0-1) [3](#0-2) 

However, the quorum store system allows validators to broadcast batches directly to peers via `BatchMsg`, which are validated only for structural correctness, not transaction signature validity: [4](#0-3) 

The `Batch::verify()` method only checks payload structure, not transaction signatures: [5](#0-4) 

Remote batches are persisted to storage without signature verification: [6](#0-5) 

These persisted batches can later be included in block proposals as inline batches: [7](#0-6) 

Inline batch verification only checks digest correctness, not transaction signatures: [8](#0-7) 

When these transactions reach block preparation, deduplication triggers expensive hash calculations for transactions sharing the same (sender, sequence_number): [9](#0-8) [10](#0-9) 

Only after deduplication does signature verification occur: [11](#0-10) 

**Attack Path:**
1. Malicious validator creates multiple transactions with identical (sender, sequence_number) but different invalid signatures
2. Creates a batch with correct digest and broadcasts via `BatchMsg`
3. Other validators receive, validate digest (passes), and persist to batch_store
4. Batch is later included in a block proposal as an inline batch
5. Block propagates to all validators
6. All validators mark transactions as "possible duplicates" and calculate hashes in parallel
7. Different authenticators prevent deduplication, so all transactions pass through
8. Signature verification eventually catches them, but CPU resources were already consumed

## Impact Explanation

This qualifies as **Medium Severity** under the Aptos bug bounty criteria for the following reasons:

- **Resource Exhaustion**: Forces all validators to perform expensive cryptographic hash calculations (via `txn.committed_hash()`) for transactions that will ultimately fail signature verification
- **CPU Consumption**: Hash calculation is parallelized but still consumes significant CPU cycles, especially with many duplicate transactions
- **Bandwidth Waste**: Invalid transactions propagate through the network and are processed by all validators
- **Limited DOS Potential**: While not causing total network unavailability, it can degrade validator performance

This violates the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." The system performs unnecessary expensive operations on transactions guaranteed to fail later validation.

The impact is limited to resource consumption and does not affect consensus safety, fund security, or cause permanent network damage, placing it firmly in Medium severity.

## Likelihood Explanation

**Likelihood: Medium to High**

- **Attacker Requirements**: Must be a validator in the current epoch with ability to broadcast `BatchMsg`
- **Complexity**: Low - straightforward to craft transactions with invalid signatures and correct batch digest
- **Detection**: Difficult to distinguish from legitimate batches until signature verification fails
- **Byzantine Tolerance**: Within the 1/3 Byzantine validator assumption of AptosBFT
- **Repeatability**: Can be performed repeatedly across multiple epochs

The attack is realistic within the Byzantine threat model where up to 1/3 of validators may be malicious. No coordination between multiple validators is required.

## Recommendation

Implement signature verification for transactions in remote batches before persisting them to batch storage:

```rust
// In consensus/src/quorum_store/batch_coordinator.rs, modify handle_batches_msg:

pub(crate) async fn handle_batches_msg(
    &mut self,
    author: PeerId,
    batches: Vec<Batch<BatchInfoExt>>,
) {
    // ... existing checks ...
    
    // Add signature verification for remote batch transactions
    for batch in batches.iter() {
        for transaction in batch.txns() {
            if let Err(e) = transaction.check_signature() {
                error!(
                    "Invalid signature in batch {} from {}: {:?}",
                    batch.digest(),
                    author.short_str().as_str(),
                    e
                );
                counters::RECEIVED_BATCH_INVALID_SIGNATURE.inc();
                return; // Reject entire batch
            }
        }
    }
    
    // ... rest of existing code ...
}
```

Alternatively, move signature verification earlier in the pipeline before deduplication:

```rust
// In consensus/src/pipeline/pipeline_builder.rs, verify before dedup:

async fn prepare(
    decryption_fut: TaskFuture<DecryptionResult>,
    preparer: Arc<BlockPreparer>,
    block: Arc<Block>,
) -> TaskResult<PrepareResult> {
    // ... existing code to get input_txns ...
    
    // Verify signatures first
    let sig_verified_txns: Vec<SignatureVerifiedTransaction> = SIG_VERIFY_POOL.install(|| {
        input_txns
            .into_par_iter()
            .map(|t| Transaction::UserTransaction(t).into())
            .collect()
    });
    
    // Filter out invalid signatures before dedup
    let valid_txns: Vec<SignedTransaction> = sig_verified_txns
        .into_iter()
        .filter_map(|t| t.try_into_valid())
        .collect();
    
    // Then apply dedup, shuffle, etc.
    let (deduped_txns, block_gas_limit) = preparer.prepare_block(...);
    
    // ...
}
```

## Proof of Concept

```rust
// Rust unit test demonstrating the vulnerability
#[cfg(test)]
mod tests {
    use super::*;
    use aptos_crypto::ed25519::*;
    use aptos_types::transaction::*;
    
    #[test]
    fn test_invalid_signature_batch_resource_exhaustion() {
        // Create a legitimate sender account
        let (sender_privkey, sender_pubkey) = generate_keypair();
        let sender_addr = from_public_key(&sender_pubkey);
        
        // Create a fake signer (attacker)
        let (fake_privkey, fake_pubkey) = generate_keypair();
        
        // Create 100 transactions with same (sender, seq_no) but different invalid signatures
        let mut invalid_txns = vec![];
        for i in 0..100 {
            // Create different transaction payloads to get different hashes
            let raw_txn = RawTransaction::new_script(
                sender_addr,
                0, // Same sequence number
                Script::new(vec![i as u8], vec![], vec![]),
                100_000,
                100,
                0,
                ChainId::test(),
            );
            
            // Sign with WRONG key (fake_privkey instead of sender_privkey)
            let signed_txn = SignedTransaction::new(
                raw_txn,
                fake_pubkey.clone(),
                Ed25519Signature::try_from(&fake_privkey.sign(&[0u8; 32]).to_bytes()[..]).unwrap(),
            );
            
            invalid_txns.push(signed_txn);
        }
        
        // Create batch with these invalid transactions
        let batch_payload = BatchPayload::new(attacker_validator_addr, invalid_txns.clone());
        let batch_info = BatchInfo::new(
            attacker_validator_addr,
            batch_id,
            epoch,
            expiration,
            batch_payload.hash(),
            100, // num_txns
            10000, // num_bytes
            0, // gas_bucket_start
        );
        let batch = Batch::new(batch_info, batch_payload);
        
        // Verify that batch passes structural verification
        assert!(batch.verify().is_ok());
        
        // But signatures are invalid
        for txn in &invalid_txns {
            assert!(txn.check_signature().is_err());
        }
        
        // When dedup processes these:
        let deduper = TxnHashAndAuthenticatorDeduper::new();
        let start = Instant::now();
        
        // All 100 transactions trigger "possible duplicates" path
        // Forces expensive hash calculation for all
        let _deduped = deduper.dedup(invalid_txns.clone());
        
        let elapsed = start.elapsed();
        println!("Dedup with 100 invalid signature txns took: {:?}", elapsed);
        
        // This demonstrates wasted CPU resources on transactions
        // that will fail signature verification later
    }
}
```

## Notes

This vulnerability exists because remote batch validation prioritizes performance over complete validation, deferring signature checks to later pipeline stages. While the design choice (dedup before signature verification) is explicitly documented and may have been intentional, it creates an exploitable resource exhaustion vector when combined with the ability for validators to broadcast unverified batches.

The attack is realistic within the Byzantine fault tolerance model where validators can act maliciously. The mitigation should add signature verification at batch reception or move it before deduplication to prevent wasted resources on guaranteed-to-fail transactions.

### Citations

**File:** consensus/src/txn_hash_and_authenticator_deduper.rs (L13-17)
```rust
/// An implementation of TransactionDeduper. Duplicate filtering is done using the pair
/// (raw_txn.hash(), authenticator). Both the hash and signature are required because dedup
/// happens before signatures are verified and transaction prologue is checked. (So, e.g., a bad
/// transaction could contain a txn and signature that are unrelated.) If the checks are done
/// beforehand only one of the txn hash or signature would be required.
```

**File:** consensus/src/txn_hash_and_authenticator_deduper.rs (L63-71)
```rust
        let hash_and_authenticators: Vec<_> = possible_duplicates
            .into_par_iter()
            .zip(&transactions)
            .with_min_len(optimal_min_len(num_txns, 48))
            .map(|(need_hash, txn)| match need_hash {
                true => Some((txn.committed_hash(), txn.authenticator())),
                false => None,
            })
            .collect();
```

**File:** mempool/src/shared_mempool/tasks.rs (L490-503)
```rust
    let validation_results = VALIDATION_POOL.install(|| {
        transactions
            .par_iter()
            .map(|t| {
                let result = smp.validator.read().validate_transaction(t.0.clone());
                // Pre-compute the hash and length if the transaction is valid, before locking mempool
                if result.is_ok() {
                    t.0.committed_hash();
                    t.0.txn_bytes_len();
                }
                result
            })
            .collect::<Vec<_>>()
    });
```

**File:** aptos-move/aptos-vm/src/aptos_vm.rs (L3232-3236)
```rust
        let txn = match transaction.check_signature() {
            Ok(t) => t,
            _ => {
                return VMValidatorResult::error(StatusCode::INVALID_SIGNATURE);
            },
```

**File:** consensus/src/quorum_store/types.rs (L262-290)
```rust
    pub fn verify(&self) -> anyhow::Result<()> {
        ensure!(
            self.payload.author() == self.author(),
            "Payload author doesn't match the info"
        );
        ensure!(
            self.payload.hash() == *self.digest(),
            "Payload hash doesn't match the digest"
        );
        ensure!(
            self.payload.num_txns() as u64 == self.num_txns(),
            "Payload num txns doesn't match batch info"
        );
        ensure!(
            self.payload.num_bytes() as u64 == self.num_bytes(),
            "Payload num bytes doesn't match batch info"
        );
        for txn in self.payload.txns() {
            ensure!(
                txn.gas_unit_price() >= self.gas_bucket_start(),
                "Payload gas unit price doesn't match batch info"
            );
            ensure!(
                !txn.payload().is_encrypted_variant(),
                "Encrypted transaction is not supported yet"
            );
        }
        Ok(())
    }
```

**File:** consensus/src/quorum_store/types.rs (L433-461)
```rust
    pub fn verify(
        &self,
        peer_id: PeerId,
        max_num_batches: usize,
        verifier: &ValidatorVerifier,
    ) -> anyhow::Result<()> {
        ensure!(!self.batches.is_empty(), "Empty message");
        ensure!(
            self.batches.len() <= max_num_batches,
            "Too many batches: {} > {}",
            self.batches.len(),
            max_num_batches
        );
        let epoch_authors = verifier.address_to_validator_index();
        for batch in self.batches.iter() {
            ensure!(
                epoch_authors.contains_key(&batch.author()),
                "Invalid author {} for batch {} in current epoch",
                batch.author(),
                batch.digest()
            );
            ensure!(
                batch.author() == peer_id,
                "Batch author doesn't match sender"
            );
            batch.verify()?
        }
        Ok(())
    }
```

**File:** consensus/src/quorum_store/batch_coordinator.rs (L173-244)
```rust
    pub(crate) async fn handle_batches_msg(
        &mut self,
        author: PeerId,
        batches: Vec<Batch<BatchInfoExt>>,
    ) {
        if let Err(e) = self.ensure_max_limits(&batches) {
            error!("Batch from {}: {}", author, e);
            counters::RECEIVED_BATCH_MAX_LIMIT_FAILED.inc();
            return;
        }

        let Some(batch) = batches.first() else {
            error!("Empty batch received from {}", author.short_str().as_str());
            return;
        };

        // Filter the transactions in the batches. If any transaction is rejected,
        // the message will be dropped, and all batches will be rejected.
        if self.transaction_filter_config.is_enabled() {
            let transaction_filter = &self.transaction_filter_config.batch_transaction_filter();
            for batch in batches.iter() {
                for transaction in batch.txns() {
                    if !transaction_filter.allows_transaction(
                        batch.batch_info().batch_id(),
                        batch.author(),
                        batch.digest(),
                        transaction,
                    ) {
                        error!(
                            "Transaction {}, in batch {}, from {}, was rejected by the filter. Dropping {} batches!",
                            transaction.committed_hash(),
                            batch.batch_info().batch_id(),
                            author.short_str().as_str(),
                            batches.len()
                        );
                        counters::RECEIVED_BATCH_REJECTED_BY_FILTER.inc();
                        return;
                    }
                }
            }
        }

        let approx_created_ts_usecs = batch
            .info()
            .expiration()
            .saturating_sub(self.batch_expiry_gap_when_init_usecs);

        if approx_created_ts_usecs > 0 {
            observe_batch(
                approx_created_ts_usecs,
                batch.author(),
                BatchStage::RECEIVED,
            );
        }

        let mut persist_requests = vec![];
        for batch in batches.into_iter() {
            // TODO: maybe don't message batch generator if the persist is unsuccessful?
            if let Err(e) = self
                .sender_to_batch_generator
                .send(BatchGeneratorCommand::RemoteBatch(batch.clone()))
                .await
            {
                warn!("Failed to send batch to batch generator: {}", e);
            }
            persist_requests.push(batch.into());
        }
        counters::RECEIVED_BATCH_COUNT.inc_by(persist_requests.len() as u64);
        if author != self.my_peer_id {
            counters::RECEIVED_REMOTE_BATCH_COUNT.inc_by(persist_requests.len() as u64);
        }
        self.persist_and_send_digests(persist_requests, approx_created_ts_usecs);
```

**File:** consensus/consensus-types/src/common.rs (L541-556)
```rust
    pub fn verify_inline_batches<'a, T: TBatchInfo + 'a>(
        inline_batches: impl Iterator<Item = (&'a T, &'a Vec<SignedTransaction>)>,
    ) -> anyhow::Result<()> {
        for (batch, payload) in inline_batches {
            // TODO: Can cloning be avoided here?
            let computed_digest = BatchPayload::new(batch.author(), payload.clone()).hash();
            ensure!(
                computed_digest == *batch.digest(),
                "Hash of the received inline batch doesn't match the digest value for batch {:?}: {} != {}",
                batch,
                computed_digest,
                batch.digest()
            );
        }
        Ok(())
    }
```

**File:** consensus/consensus-types/src/common.rs (L574-596)
```rust
    pub fn verify(
        &self,
        verifier: &ValidatorVerifier,
        proof_cache: &ProofCache,
        quorum_store_enabled: bool,
    ) -> anyhow::Result<()> {
        match (quorum_store_enabled, self) {
            (false, Payload::DirectMempool(_)) => Ok(()),
            (true, Payload::InQuorumStore(proof_with_status)) => {
                Self::verify_with_cache(&proof_with_status.proofs, verifier, proof_cache)
            },
            (true, Payload::InQuorumStoreWithLimit(proof_with_status)) => Self::verify_with_cache(
                &proof_with_status.proof_with_data.proofs,
                verifier,
                proof_cache,
            ),
            (true, Payload::QuorumStoreInlineHybrid(inline_batches, proof_with_data, _))
            | (true, Payload::QuorumStoreInlineHybridV2(inline_batches, proof_with_data, _)) => {
                Self::verify_with_cache(&proof_with_data.proofs, verifier, proof_cache)?;
                Self::verify_inline_batches(
                    inline_batches.iter().map(|(info, txns)| (info, txns)),
                )?;
                Ok(())
```

**File:** consensus/src/block_preparer.rs (L90-99)
```rust
        let result = tokio::task::spawn_blocking(move || {
            let filtered_txns = filter_block_transactions(
                txn_filter_config,
                block_id,
                block_author,
                block_epoch,
                block_timestamp_usecs,
                txns,
            );
            let deduped_txns = txn_deduper.dedup(filtered_txns);
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L669-677)
```rust
        let sig_verification_start = Instant::now();
        let sig_verified_txns: Vec<SignatureVerifiedTransaction> = SIG_VERIFY_POOL.install(|| {
            let num_txns = input_txns.len();
            input_txns
                .into_par_iter()
                .with_min_len(optimal_min_len(num_txns, 32))
                .map(|t| Transaction::UserTransaction(t).into())
                .collect::<Vec<_>>()
        });
```
