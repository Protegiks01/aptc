# Audit Report

## Title
Epoch Transition Race Condition Causes Node Crash on Restart Due to Unfiltered Batch Loading

## Summary
The `get_all_batches()` function returns batches from all epochs without filtering. During same-epoch node restarts, `populate_cache_and_gc_expired_batches_v1()` loads these old-epoch batches into memory without epoch validation, potentially exceeding resource quotas and causing a panic that prevents node restart.

## Finding Description

The vulnerability stems from a race condition in epoch transition handling combined with missing epoch filtering during batch cache population.

**The Problem Chain:**

1. **No Epoch Filter in Database Query:**
The `get_all_batches()` function retrieves all batches from the database without any epoch filtering: [1](#0-0) 

2. **Asynchronous Epoch Cleanup:**
During epoch transitions, old batch cleanup is performed asynchronously in a background task: [2](#0-1) 

3. **Epoch Detection Logic:**
The `is_new_epoch` flag is determined by checking if the latest ledger info ends an epoch: [3](#0-2) 

4. **Missing Epoch Filter During Recovery:**
When `is_new_epoch = false` (same-epoch restart), the `populate_cache_and_gc_expired_batches_v1()` function only filters by expiration time, NOT by epoch: [4](#0-3) 

The critical line shows it only checks expiration: `if expiration < gc_timestamp` without verifying `if epoch < current_epoch`.

5. **Panic on Quota Exhaustion:**
If inserting old batches exceeds quotas, the node panics with `.expect()`: [5](#0-4) 

**Attack Scenario:**

1. Epoch N-1 â†’ N transition occurs at a validator node
2. `gc_previous_epoch_batches_from_db_v1()` spawns asynchronously to delete old batches
3. Node crashes BEFORE the cleanup task completes
4. Old epoch batches remain in the database
5. On restart, `latest_ledger_info.ends_epoch()` returns `false` (mid-epoch N)
6. `populate_cache_and_gc_expired_batches_v1()` is called instead of `gc_previous_epoch_batches_from_db_v1()`
7. ALL batches from database (including epoch N-1) are loaded via `get_all_batches()`
8. Each batch consumes quota via `insert_to_cache()`
9. If total size exceeds configured quotas (memory_quota: 120MB, db_quota: 300MB, batch_quota: 300K), the node panics
10. Node cannot restart without manual database cleanup

## Impact Explanation

**Severity: Medium** (up to $10,000 per Aptos Bug Bounty)

This vulnerability breaks the **Resource Limits** invariant ("All operations must respect gas, storage, and computational limits") and can cause validator node unavailability.

**Impact:**
- **Liveness Failure**: Affected nodes cannot restart automatically, requiring manual intervention
- **State Inconsistencies**: Nodes stuck in crash loop while network progresses
- **Validator Downtime**: Validators cannot participate in consensus until database is manually cleaned

While this doesn't directly cause consensus safety violations (epoch verification at [6](#0-5)  would reject wrong-epoch batches), it represents a denial-of-service vector against individual nodes.

The default quotas are: [7](#0-6) 

A malicious validator could create many large batches in epoch N-1 (within the allowed expiration window of 60 seconds) to maximize the chance of quota exhaustion on restart.

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability requires specific timing conditions:
1. Node crash during or shortly after epoch transition
2. Background cleanup task not completed before crash
3. Restart occurs within same epoch
4. Sufficient old-epoch batches remain unexpired in database

However, epoch transitions are frequent (typically every few hours), and node crashes during these periods are not uncommon in production environments. The asynchronous nature of cleanup makes this race condition realistic.

Batch expiration validation limits the attack window: [8](#0-7) 

Remote batches expire in 500ms, local batches in 60s, but this is still sufficient for the race condition to manifest.

## Recommendation

**Fix: Add epoch filtering in `populate_cache_and_gc_expired_batches_v1()`**

The function should filter batches by epoch during same-epoch recovery, matching the behavior of `gc_previous_epoch_batches_from_db_v1()`:

```rust
fn populate_cache_and_gc_expired_batches_v1(
    db: Arc<dyn QuorumStoreStorage>,
    current_epoch: u64,
    last_certified_time: u64,
    expiration_buffer_usecs: u64,
    batch_store: &BatchStore,
) {
    let db_content = db
        .get_all_batches()
        .expect("failed to read v1 data from db");
    
    let mut expired_keys = Vec::new();
    let gc_timestamp = last_certified_time.saturating_sub(expiration_buffer_usecs);
    
    for (digest, value) in db_content {
        let expiration = value.expiration();
        let epoch = value.epoch();  // ADD THIS
        
        // ADD EPOCH CHECK
        if epoch < current_epoch {
            expired_keys.push(digest);
            continue;
        }
        
        if expiration < gc_timestamp {
            expired_keys.push(digest);
        } else {
            batch_store
                .insert_to_cache(&value.into())
                .expect("Storage limit exceeded upon BatchReader construction");
        }
    }
    
    tokio::task::spawn_blocking(move || {
        db.delete_batches(expired_keys)
            .expect("Deletion of expired keys should not fail");
    });
}
```

Apply the same fix to `populate_cache_and_gc_expired_batches_v2()`.

**Alternative: Make epoch cleanup synchronous during epoch transitions** to eliminate the race condition entirely, though this may impact performance.

## Proof of Concept

```rust
// Reproduction steps (conceptual):
// 1. Create a test environment with BatchStore in epoch 10
// 2. Persist 1000 large batches from epoch 9 to the database
// 3. Set last_certified_time such that batches haven't expired
// 4. Create BatchStore with is_new_epoch=false (simulating same-epoch restart)
// 5. Observe panic in populate_cache_and_gc_expired_batches_v1() due to quota exhaustion

#[test]
fn test_epoch_transition_race_crash() {
    let db = Arc::new(QuorumStoreDB::new(temp_dir.path()));
    let current_epoch = 10;
    
    // Persist many batches from epoch 9
    for i in 0..1000 {
        let batch = create_large_batch(9, i); // epoch 9
        db.save_batch(batch).unwrap();
    }
    
    // Simulate same-epoch restart (is_new_epoch = false)
    // This should panic due to quota exhaustion when loading old batches
    let result = std::panic::catch_unwind(|| {
        BatchStore::new(
            current_epoch,
            false, // is_new_epoch = false (CRITICAL)
            current_time,
            db,
            120_000_000, // default memory quota
            300_000_000, // default db quota
            300_000,     // default batch quota
            validator_signer,
            60_000_000,  // expiration buffer
        )
    });
    
    assert!(result.is_err(), "Expected panic due to quota exhaustion");
}
```

**Notes:**
- Consensus confusion is prevented by epoch verification in `Payload::verify_epoch()`
- The vulnerability primarily affects node availability, not consensus safety
- Impact is amplified if multiple validators crash simultaneously during epoch transitions
- Manual intervention required: operators must delete old batches from database to restart nodes

### Citations

**File:** consensus/src/quorum_store/quorum_store_db.rs (L103-108)
```rust
    fn get_all_batches(&self) -> Result<HashMap<HashValue, PersistedValue<BatchInfo>>> {
        let mut iter = self.db.iter::<BatchSchema>()?;
        iter.seek_to_first();
        iter.map(|res| res.map_err(Into::into))
            .collect::<Result<HashMap<HashValue, PersistedValue<BatchInfo>>>>()
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L156-160)
```rust
        if is_new_epoch {
            tokio::task::spawn_blocking(move || {
                Self::gc_previous_epoch_batches_from_db_v1(db_clone.clone(), epoch);
                Self::gc_previous_epoch_batches_from_db_v2(db_clone, epoch);
            });
```

**File:** consensus/src/quorum_store/batch_store.rs (L252-279)
```rust
        let db_content = db
            .get_all_batches()
            .expect("failed to read v1 data from db");
        info!(
            epoch = current_epoch,
            "QS: Read v1 batches from storage. Len: {}, Last Cerified Time: {}",
            db_content.len(),
            last_certified_time
        );

        let mut expired_keys = Vec::new();
        let gc_timestamp = last_certified_time.saturating_sub(expiration_buffer_usecs);
        for (digest, value) in db_content {
            let expiration = value.expiration();

            trace!(
                "QS: Batchreader recovery content exp {:?}, digest {}",
                expiration,
                digest
            );

            if expiration < gc_timestamp {
                expired_keys.push(digest);
            } else {
                batch_store
                    .insert_to_cache(&value.into())
                    .expect("Storage limit exceeded upon BatchReader construction");
            }
```

**File:** consensus/src/quorum_store/quorum_store_builder.rs (L239-244)
```rust
        let latest_ledger_info_with_sigs = self
            .aptos_db
            .get_latest_ledger_info()
            .expect("could not get latest ledger info");
        let last_committed_timestamp = latest_ledger_info_with_sigs.commit_info().timestamp_usecs();
        let is_new_epoch = latest_ledger_info_with_sigs.ledger_info().ends_epoch();
```

**File:** consensus/consensus-types/src/common.rs (L634-669)
```rust
    pub(crate) fn verify_epoch(&self, epoch: u64) -> anyhow::Result<()> {
        match self {
            Payload::DirectMempool(_) => return Ok(()),
            Payload::InQuorumStore(proof_with_data) => {
                ensure!(
                    proof_with_data.proofs.iter().all(|p| p.epoch() == epoch),
                    "Payload epoch doesn't match given epoch"
                );
            },
            Payload::InQuorumStoreWithLimit(proof_with_data_with_txn_limit) => {
                ensure!(
                    proof_with_data_with_txn_limit
                        .proof_with_data
                        .proofs
                        .iter()
                        .all(|p| p.epoch() == epoch),
                    "Payload epoch doesn't match given epoch"
                );
            },
            Payload::QuorumStoreInlineHybrid(inline_batches, proof_with_data, _)
            | Payload::QuorumStoreInlineHybridV2(inline_batches, proof_with_data, _) => {
                ensure!(
                    proof_with_data.proofs.iter().all(|p| p.epoch() == epoch),
                    "Payload proof epoch doesn't match given epoch"
                );
                ensure!(
                    inline_batches.iter().all(|b| b.0.epoch() == epoch),
                    "Payload inline batch epoch doesn't match given epoch"
                )
            },
            Payload::OptQuorumStore(opt_quorum_store_payload) => {
                opt_quorum_store_payload.check_epoch(epoch)?;
            },
        };
        Ok(())
    }
```

**File:** config/src/config/quorum_store_config.rs (L133-135)
```rust
            memory_quota: 120_000_000,
            db_quota: 300_000_000,
            batch_quota: 300_000,
```

**File:** consensus/consensus-types/src/proof_of_store.rs (L469-479)
```rust
        if self.expiration()
            > aptos_infallible::duration_since_epoch().as_micros() as u64
                + max_batch_expiry_gap_usecs
        {
            bail!(
                "Batch expiration too far in future: {} > {}",
                self.expiration(),
                aptos_infallible::duration_since_epoch().as_micros() as u64
                    + max_batch_expiry_gap_usecs
            );
        }
```
