# Audit Report

## Title
Consensus Liveness Failure Due to Database State Divergence in LeaderReputation-Based Failed Authors Computation

## Summary
Validators can disagree on the `failed_authors` field when using LeaderReputation-based proposer election, causing valid proposals to be rejected and triggering consensus liveness failures. The vulnerability stems from proposer selection depending on validators' local database states, which can diverge during network delays, sync operations, or database pruning.

## Finding Description

The `failed_authors` field tracks consecutive proposers from preceding rounds that failed to produce successful blocks. When creating a proposal, validators compute this list by calling `ProposerElection::get_valid_proposer()` for each skipped round. [1](#0-0) 

For LeaderReputation-based proposer election (the default in production), proposer selection depends on local database state retrieved via `backend.get_block_metadata()`, which fetches committed block history and computes a root hash. [2](#0-1) 

The critical issue occurs when validators have different database states:

1. **Database State Divergence**: When validators have incomplete or different views of committed history (due to sync delays, pruning, or network partitions), the `AptosDBBackend` returns different `sliding_window` data and `root_hash` values. [3](#0-2) 

2. **Proposer Selection Mismatch**: When history is missing, the backend returns `HashValue::zero()` as the root hash. Different validators with different database states compute different weights and select different proposers for the same round using `choose_index()`. [4](#0-3) 

3. **Failed Authors Disagreement**: Since validators compute different proposers for past rounds, they generate different `failed_authors` lists for the same proposal round.

4. **Validation Rejection**: When a validator receives a proposal, it strictly validates that the proposal's `failed_authors` matches its own computed list. Any mismatch causes immediate rejection. [5](#0-4) 

5. **Block Hash Divergence**: The `failed_authors` field is part of the `BlockType` enum which is serialized into the block hash for non-optimistic blocks. [6](#0-5)  Validators with different `failed_authors` expectations will compute different block hashes and refuse to vote, stalling consensus.

The developers are aware of this issue, as evidenced by a warning message: "Elected proposers are unlikely to match!!" when local history is outdated. [7](#0-6) 

## Impact Explanation

**Severity: HIGH** (Consensus Liveness Failure)

This vulnerability causes consensus liveness failures that can affect the entire network:

1. **Immediate Impact**: Valid proposals are rejected because validators with different database states compute different expected `failed_authors` lists. This prevents the network from making progress.

2. **Cascading Effect**: When proposals fail due to this issue, more rounds accumulate in the failed_authors window, requiring validators to look back further in history. This increases the likelihood of database state divergence, creating a positive feedback loop that worsens the problem.

3. **Network-Wide Stall**: During periods of high network latency, database sync delays, or after validators restart, a significant portion of the network may have divergent views, causing widespread proposal rejection.

4. **Partition Risk**: Validators with similar database states may form implicit partitions, unable to accept proposals from validators with different states, effectively partitioning the network without explicit Byzantine behavior.

This meets the **High Severity** criteria per Aptos Bug Bounty: "Significant protocol violations" and "Validator node slowdowns", and approaches **Critical Severity** if the liveness failure becomes persistent.

## Likelihood Explanation

**Likelihood: MEDIUM-TO-HIGH**

This vulnerability manifests naturally under realistic network conditions:

1. **Common Triggers**:
   - Network latency causing validators to fall behind on sync
   - Validators restarting and catching up to the current state
   - Database pruning occurring at different times across validators
   - Multiple consecutive round failures (which the system is designed to handle)

2. **Production Configuration**: The default configuration uses `ProposerAndVoterV2` with `use_root_hash_for_seed=true` and `exclude_round=40`, making this vulnerability active in production deployments. [8](#0-7) 

3. **Observed in Code**: The warning message in the codebase indicates this scenario has been observed: "Elected proposers are unlikely to match!!"

4. **No Synchronization Mechanism**: There is no mechanism to ensure validators have identical database states before computing proposers, making this a race condition that naturally occurs during normal operations.

## Recommendation

Implement a deterministic proposer election mechanism that doesn't depend on potentially divergent local database states:

**Option 1: Use Consensus-Confirmed State Only**
Modify LeaderReputation to only use block history that is confirmed via QuorumCertificates (QCs), which all validators agree on, rather than relying on local database state. Compute reputation based on the QC chain up to the parent block's QC.

**Option 2: Include Reputation Seed in Parent Block**
Have each block include a deterministic seed (based on the parent block's hash) that all validators use for proposer election. This ensures all validators use the same seed for the same round, eliminating divergence.

**Option 3: Relax Failed Authors Validation**
Instead of strict equality validation, allow validators to accept proposals where the `failed_authors` list is a valid subset or superset (within bounds) of their computed list. This provides flexibility while maintaining security.

**Recommended Fix (Option 1 - Most Robust):**

In `consensus/src/liveness/leader_reputation.rs`, modify the proposer election to use only consensus-confirmed state:

```rust
// Instead of fetching from local database:
let (sliding_window, root_hash) = self.backend.get_block_metadata(self.epoch, target_round);

// Use only blocks confirmed via the QC chain from the proposal's parent:
let confirmed_history = self.extract_history_from_qc_chain(proposal.quorum_cert(), target_round);
let root_hash = proposal.quorum_cert().certified_block().id(); // Use parent block ID as seed
```

This ensures all validators computing the proposer for a given round use the same confirmed history from the QC chain, eliminating database state divergence.

## Proof of Concept

**Reproduction Steps:**

1. **Setup**: Deploy a test network with 4 validators using LeaderReputation with `use_root_hash_for_seed=true`

2. **Create Database Divergence**:
   - Stop validator V2 for 50 rounds
   - Let V1, V3, V4 continue committing blocks (rounds 1-50)
   - Restart V2 (it begins syncing but is ~30 rounds behind)

3. **Trigger Failed Authors Computation**:
   - Induce 3 consecutive proposal failures at rounds 51-53
   - V1 (fully synced) is elected proposer for round 54
   - V1 computes `failed_authors` for rounds 51-53 using its complete database state
   - V1's database has history up to round 50, so it queries rounds 14-50 for reputation (round 54 - 40 = round 14)

4. **Observe Disagreement**:
   - V2 (synced only to round 35) queries its database for rounds 14-50
   - V2's database returns empty/incomplete history for rounds 36-50
   - V2 uses `HashValue::zero()` as root_hash instead of the actual hash
   - V2 computes different proposer weights and different failed authors for rounds 51-53

5. **Validation Failure**:
   - V2 receives V1's proposal with failed_authors = [(51, Alice), (52, Bob), (53, Carol)]
   - V2 computes expected failed_authors = [(51, Bob), (52, Alice), (53, Dave)]
   - V2 rejects V1's proposal with error: "Proposal has invalid failed_authors list"
   - Consensus stalls as proposal cannot achieve 2f+1 votes

**Expected Outcome**: V1's valid proposal is rejected by V2 (and potentially V3, V4 depending on their sync states), causing consensus liveness failure.

## Notes

This vulnerability is particularly concerning because:

1. **Silent Failure**: The system provides warnings but doesn't prevent the liveness failure
2. **Cascading Nature**: Failed rounds make the problem worse by requiring deeper history lookups
3. **Production Active**: The default configuration uses the vulnerable ProposerAndVoterV2 mode
4. **No Recovery Mechanism**: Once validators disagree, there's no automatic reconciliation

The fix requires ensuring all validators use the same deterministic input for proposer election, either by synchronizing database state (difficult) or by using only consensus-confirmed data (recommended).

### Citations

**File:** consensus/src/liveness/proposal_generator.rs (L884-902)
```rust
    pub fn compute_failed_authors(
        &self,
        round: Round,
        previous_round: Round,
        include_cur_round: bool,
        proposer_election: Arc<dyn ProposerElection>,
    ) -> Vec<(Round, Author)> {
        let end_round = round + u64::from(include_cur_round);
        let mut failed_authors = Vec::new();
        let start = std::cmp::max(
            previous_round + 1,
            end_round.saturating_sub(self.max_failed_authors_to_store as u64),
        );
        for i in start..end_round {
            failed_authors.push((i, proposer_election.get_valid_proposer(i)));
        }

        failed_authors
    }
```

**File:** consensus/src/liveness/leader_reputation.rs (L109-165)
```rust
    ) -> (Vec<NewBlockEvent>, HashValue) {
        // Do not warn when round==0, because check will always be unsure of whether we have
        // all events from the previous epoch. If there is an actual issue, next round will log it.
        if target_round != 0 {
            let has_larger = events.first().is_some_and(|e| {
                (e.event.epoch(), e.event.round()) >= (target_epoch, target_round)
            });
            if !has_larger {
                // error, and not a fatal, in an unlikely scenario that we have many failed consecutive rounds,
                // and nobody has any newer successful blocks.
                warn!(
                    "Local history is too old, asking for {} epoch and {} round, and latest from db is {} epoch and {} round! Elected proposers are unlikely to match!!",
                    target_epoch, target_round, events.first().map_or(0, |e| e.event.epoch()), events.first().map_or(0, |e| e.event.round()))
            }
        }

        let mut max_version = 0;
        let mut result = vec![];
        for event in events {
            if (event.event.epoch(), event.event.round()) <= (target_epoch, target_round)
                && result.len() < self.window_size
            {
                max_version = std::cmp::max(max_version, event.version);
                result.push(event.event.clone());
            }
        }

        if result.len() < self.window_size && !hit_end {
            error!(
                "We are not fetching far enough in history, we filtered from {} to {}, but asked for {}. Target ({}, {}), received from {:?} to {:?}.",
                events.len(),
                result.len(),
                self.window_size,
                target_epoch,
                target_round,
                events.last().map_or((0, 0), |e| (e.event.epoch(), e.event.round())),
                events.first().map_or((0, 0), |e| (e.event.epoch(), e.event.round())),
            );
        }

        if result.is_empty() {
            warn!("No events in the requested window could be found");
            (result, HashValue::zero())
        } else {
            let root_hash = self
                .aptos_db
                .get_accumulator_root_hash(max_version)
                .unwrap_or_else(|_| {
                    error!(
                        "We couldn't fetch accumulator hash for the {} version, for {} epoch, {} round",
                        max_version, target_epoch, target_round,
                    );
                    HashValue::zero()
                });
            (result, root_hash)
        }
    }
```

**File:** consensus/src/liveness/leader_reputation.rs (L696-734)
```rust
    fn get_valid_proposer_and_voting_power_participation_ratio(
        &self,
        round: Round,
    ) -> (Author, VotingPowerRatio) {
        let target_round = round.saturating_sub(self.exclude_round);
        let (sliding_window, root_hash) = self.backend.get_block_metadata(self.epoch, target_round);
        let voting_power_participation_ratio =
            self.compute_chain_health_and_add_metrics(&sliding_window, round);
        let mut weights =
            self.heuristic
                .get_weights(self.epoch, &self.epoch_to_proposers, &sliding_window);
        let proposers = &self.epoch_to_proposers[&self.epoch];
        assert_eq!(weights.len(), proposers.len());

        // Multiply weights by voting power:
        let stake_weights: Vec<u128> = weights
            .iter_mut()
            .enumerate()
            .map(|(i, w)| *w as u128 * self.voting_powers[i] as u128)
            .collect();

        let state = if self.use_root_hash {
            [
                root_hash.to_vec(),
                self.epoch.to_le_bytes().to_vec(),
                round.to_le_bytes().to_vec(),
            ]
            .concat()
        } else {
            [
                self.epoch.to_le_bytes().to_vec(),
                round.to_le_bytes().to_vec(),
            ]
            .concat()
        };

        let chosen_index = choose_index(stake_weights, state);
        (proposers[chosen_index], voting_power_participation_ratio)
    }
```

**File:** consensus/src/liveness/proposer_election.rs (L38-69)
```rust
// next consumes seed and returns random deterministic u64 value in [0, max) range
fn next_in_range(state: Vec<u8>, max: u128) -> u128 {
    // hash = SHA-3-256(state)
    let hash = aptos_crypto::HashValue::sha3_256_of(&state).to_vec();
    let mut temp = [0u8; 16];
    copy_slice_to_vec(&hash[..16], &mut temp).expect("next failed");
    // return hash[0..16]
    u128::from_le_bytes(temp) % max
}

// chose index randomly, with given weight distribution
pub(crate) fn choose_index(mut weights: Vec<u128>, state: Vec<u8>) -> usize {
    let mut total_weight = 0;
    // Create cumulative weights vector
    // Since we own the vector, we can safely modify it in place
    for w in &mut weights {
        total_weight = total_weight
            .checked_add(w)
            .expect("Total stake shouldn't exceed u128::MAX");
        *w = total_weight;
    }
    let chosen_weight = next_in_range(state, total_weight);
    weights
        .binary_search_by(|w| {
            if *w <= chosen_weight {
                Ordering::Less
            } else {
                Ordering::Greater
            }
        })
        .expect_err("Comparison never returns equals, so it's always guaranteed to be error")
}
```

**File:** consensus/src/round_manager.rs (L1216-1231)
```rust
        if !proposal.is_opt_block() {
            // Validate that failed_authors list is correctly specified in the block.
            let expected_failed_authors = self.proposal_generator.compute_failed_authors(
                proposal.round(),
                proposal.quorum_cert().certified_block().round(),
                false,
                self.proposer_election.clone(),
            );
            ensure!(
                proposal.block_data().failed_authors().is_some_and(|failed_authors| *failed_authors == expected_failed_authors),
                "[RoundManager] Proposal for block {} has invalid failed_authors list {:?}, expected {:?}",
                proposal.round(),
                proposal.block_data().failed_authors(),
                expected_failed_authors,
            );
        }
```

**File:** consensus/consensus-types/src/block_data.rs (L105-134)
```rust
impl CryptoHash for BlockData {
    type Hasher = BlockDataHasher;

    fn hash(&self) -> HashValue {
        let mut state = Self::Hasher::default();
        if self.is_opt_block() {
            #[derive(Serialize)]
            struct OptBlockDataForHash<'a> {
                epoch: u64,
                round: Round,
                timestamp_usecs: u64,
                quorum_cert_vote_data: &'a VoteData,
                block_type: &'a BlockType,
            }

            let opt_block_data_for_hash = OptBlockDataForHash {
                epoch: self.epoch,
                round: self.round,
                timestamp_usecs: self.timestamp_usecs,
                quorum_cert_vote_data: self.quorum_cert.vote_data(),
                block_type: &self.block_type,
            };
            bcs::serialize_into(&mut state, &opt_block_data_for_hash)
                .expect("OptBlockDataForHash must be serializable");
        } else {
            bcs::serialize_into(&mut state, &self).expect("BlockData must be serializable");
        }
        state.finish()
    }
}
```

**File:** types/src/on_chain_config/consensus_config.rs (L525-550)
```rust
#[derive(Clone, Debug, Deserialize, Eq, PartialEq, Serialize)]
#[serde(rename_all = "snake_case")]
pub enum LeaderReputationType {
    // Proposer election based on whether nodes succeeded or failed
    // their proposer election rounds, and whether they voted.
    // Version 1:
    // * use reputation window from stale end
    // * simple (predictable) seed
    ProposerAndVoter(ProposerAndVoterConfig),
    // Version 2:
    // * use reputation window from recent end
    // * unpredictable seed, based on root hash
    ProposerAndVoterV2(ProposerAndVoterConfig),
}

impl LeaderReputationType {
    pub fn use_root_hash_for_seed(&self) -> bool {
        // all versions after V1 should use root hash
        !matches!(self, Self::ProposerAndVoter(_))
    }

    pub fn use_reputation_window_from_stale_end(&self) -> bool {
        // all versions after V1 shouldn't use from stale end
        matches!(self, Self::ProposerAndVoter(_))
    }
}
```
