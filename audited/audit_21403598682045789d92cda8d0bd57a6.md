# Audit Report

## Title
Storage Service Lacks Per-Peer Concurrent Request Limits Allowing Catch-Up Delay Attacks

## Summary
The state sync storage service lacks per-peer concurrent request limits for non-subscription requests. Malicious peers can exploit this by sending many valid requests for old state data, exhausting the shared blocking thread pool and causing significant delays for legitimate syncing nodes requesting current data.

## Finding Description

The storage service processes each incoming request by spawning a blocking task without enforcing per-peer concurrency limits for regular data requests. This design flaw allows malicious peers to monopolize the limited blocking thread pool (64 threads) by flooding the service with valid requests for historical data. [1](#0-0) 

The network layer allows up to 100 concurrent inbound RPCs per peer connection: [2](#0-1) 

However, the storage service only enforces concurrency limits for subscription requests (30 per peer): [3](#0-2) 

Regular data requests (GetTransactionsWithProof, GetStateValuesWithProof, etc.) have NO per-peer limit. The RequestModerator only validates whether requests CAN be serviced based on available data, not rate limiting: [4](#0-3) [5](#0-4) 

The runtime has only 64 blocking threads shared across ALL peers: [6](#0-5) 

**Attack Scenario:**

1. Malicious peer(s) establish connections to the storage service
2. Each malicious peer sends 100 concurrent valid requests for old historical data (e.g., GetTransactionsWithProof for versions from months ago)
3. Each request passes validation (data exists in storage) and spawns a blocking task
4. Old data requests are slower (not cached, require disk I/O)
5. With just 2 coordinating malicious peers, 200 concurrent requests can be queued, but only 64 can execute simultaneously
6. The blocking thread pool becomes saturated with malicious requests processing old data
7. Legitimate syncing nodes requesting current data experience significant delays as their requests queue behind the backlog
8. No prioritization mechanism exists - all requests are treated equally regardless of data recency or peer behavior

## Impact Explanation

This vulnerability qualifies as **Medium Severity** per the Aptos bug bounty program:

- **State inconsistencies requiring intervention**: While not causing total service failure, legitimate syncing nodes experience delays that can cause them to fall significantly behind the chain tip, requiring manual intervention or restarts
- **Limited availability impact**: The service remains functional but degraded for legitimate users
- **No direct fund loss**: Does not directly result in theft or minting of funds
- **No consensus violation**: Does not break consensus safety or cause chain splits

The attack causes service degradation rather than complete failure, fitting the Medium severity category of "State inconsistencies requiring intervention."

## Likelihood Explanation

**Likelihood: High**

- **Low barrier to entry**: Any network peer can connect and send RPC requests
- **Simple attack**: Requires only sending valid requests for historical data - no exploitation of complex logic
- **Effective with minimal resources**: Just 2-3 coordinating peers can saturate the thread pool
- **Difficult to detect**: Requests appear valid and are indistinguishable from legitimate historical data queries
- **Persistent impact**: Attack can be sustained indefinitely as long as peers maintain connections
- **No authentication required**: Public full nodes accept connections from any peer

## Recommendation

Implement per-peer concurrent request limits for all request types, not just subscriptions:

1. **Add per-peer request tracking**: Track active non-subscription requests per peer similar to how subscriptions are tracked
2. **Enforce configurable limits**: Add `max_concurrent_requests_per_peer` to `StorageServiceConfig` (e.g., default 10)
3. **Fair scheduling**: Implement weighted fair queuing to ensure no single peer monopolizes resources
4. **Prioritize recent data**: Add priority levels based on data recency - current data gets higher priority than historical data
5. **Use bounded executor**: Replace direct `spawn_blocking` with a `BoundedExecutor` that enforces global and per-peer limits

Example fix in `StorageServiceConfig`:

```rust
pub struct StorageServiceConfig {
    // ... existing fields ...
    /// Maximum number of concurrent non-subscription requests per peer
    pub max_concurrent_requests_per_peer: u64,
}

impl Default for StorageServiceConfig {
    fn default() -> Self {
        Self {
            // ... existing defaults ...
            max_concurrent_requests_per_peer: 10,
        }
    }
}
```

Track concurrent requests in `Handler::process_request_and_respond` and reject requests exceeding the limit, similar to the subscription enforcement pattern.

## Proof of Concept

```rust
// Reproduction test demonstrating the vulnerability
// File: state-sync/storage-service/server/src/tests/catch_up_attack.rs

#[tokio::test]
async fn test_catch_up_attack_concurrent_requests() {
    use aptos_config::network_id::{NetworkId, PeerNetworkId};
    use aptos_types::PeerId;
    
    // Setup storage service with default config
    let (storage_service, mut mock_client) = setup_storage_service().await;
    
    // Simulate 3 malicious peers
    let malicious_peers: Vec<PeerNetworkId> = (0..3)
        .map(|_| PeerNetworkId::new(NetworkId::Public, PeerId::random()))
        .collect();
    
    // Each malicious peer sends 50 concurrent requests for old data
    let mut handles = vec![];
    for peer in malicious_peers {
        for i in 0..50 {
            let request = StorageServiceRequest::new(
                DataRequest::GetTransactionsWithProof(TransactionsWithProofRequest {
                    proof_version: 100000,
                    start_version: i * 1000, // Old data
                    end_version: (i + 1) * 1000,
                    include_events: false,
                }),
                false,
            );
            
            let handle = tokio::spawn(async move {
                let start = Instant::now();
                mock_client.send_request(peer, request).await;
                start.elapsed()
            });
            handles.push(handle);
        }
    }
    
    // Legitimate peer tries to get current data
    let legitimate_peer = PeerNetworkId::new(NetworkId::Validator, PeerId::random());
    let current_request = StorageServiceRequest::new(
        DataRequest::GetTransactionsWithProof(TransactionsWithProofRequest {
            proof_version: 100000,
            start_version: 99000, // Current data
            end_version: 100000,
            include_events: false,
        }),
        false,
    );
    
    let legitimate_start = Instant::now();
    let legitimate_response = mock_client.send_request(legitimate_peer, current_request).await;
    let legitimate_latency = legitimate_start.elapsed();
    
    // Assert that legitimate request experienced significant delay
    // In a properly rate-limited system, this should be < 1 second
    // With the vulnerability, this can be 5-10+ seconds
    assert!(
        legitimate_latency > Duration::from_secs(5),
        "Legitimate request should be delayed by catch-up attack"
    );
}
```

The test demonstrates that when multiple peers flood the storage service with concurrent requests for old data, legitimate peers requesting current data experience significant latency increases, validating the catch-up delay vulnerability.

### Citations

**File:** state-sync/storage-service/server/src/lib.rs (L389-419)
```rust
        while let Some(network_request) = self.network_requests.next().await {
            // All handler methods are currently CPU-bound and synchronous
            // I/O-bound, so we want to spawn on the blocking thread pool to
            // avoid starving other async tasks on the same runtime.
            let storage = self.storage.clone();
            let config = self.storage_service_config;
            let cached_storage_server_summary = self.cached_storage_server_summary.clone();
            let optimistic_fetches = self.optimistic_fetches.clone();
            let subscriptions = self.subscriptions.clone();
            let lru_response_cache = self.lru_response_cache.clone();
            let request_moderator = self.request_moderator.clone();
            let time_service = self.time_service.clone();
            self.runtime.spawn_blocking(move || {
                Handler::new(
                    cached_storage_server_summary,
                    optimistic_fetches,
                    lru_response_cache,
                    request_moderator,
                    storage,
                    subscriptions,
                    time_service,
                )
                .process_request_and_respond(
                    config,
                    network_request.peer_network_id,
                    network_request.protocol_id,
                    network_request.storage_service_request,
                    network_request.response_sender,
                );
            });
        }
```

**File:** network/framework/src/protocols/rpc/mod.rs (L183-223)
```rust
    max_concurrent_inbound_rpcs: u32,
}

impl InboundRpcs {
    pub fn new(
        network_context: NetworkContext,
        time_service: TimeService,
        remote_peer_id: PeerId,
        inbound_rpc_timeout: Duration,
        max_concurrent_inbound_rpcs: u32,
    ) -> Self {
        Self {
            network_context,
            time_service,
            remote_peer_id,
            inbound_rpc_tasks: FuturesUnordered::new(),
            inbound_rpc_timeout,
            max_concurrent_inbound_rpcs,
        }
    }

    /// Handle a new inbound `RpcRequest` message off the wire.
    pub fn handle_inbound_request(
        &mut self,
        peer_notifs_tx: &aptos_channel::Sender<(PeerId, ProtocolId), ReceivedMessage>,
        mut request: ReceivedMessage,
    ) -> Result<(), RpcError> {
        let network_context = &self.network_context;

        // Drop new inbound requests if our completion queue is at capacity.
        if self.inbound_rpc_tasks.len() as u32 == self.max_concurrent_inbound_rpcs {
            // Increase counter of declined requests
            counters::rpc_messages(
                network_context,
                REQUEST_LABEL,
                INBOUND_LABEL,
                DECLINED_LABEL,
            )
            .inc();
            return Err(RpcError::TooManyPending(self.max_concurrent_inbound_rpcs));
        }
```

**File:** state-sync/storage-service/server/src/subscription.rs (L370-381)
```rust
        // Verify that the number of active subscriptions respects the maximum
        let max_num_active_subscriptions =
            storage_service_config.max_num_active_subscriptions as usize;
        if self.pending_subscription_requests.len() >= max_num_active_subscriptions {
            return Err((
                Error::InvalidRequest(format!(
                    "The maximum number of active subscriptions has been reached! Max: {:?}, found: {:?}",
                    max_num_active_subscriptions, self.pending_subscription_requests.len()
                )),
                subscription_request,
            ));
        }
```

**File:** state-sync/storage-service/server/src/moderator.rs (L134-196)
```rust
    pub fn validate_request(
        &self,
        peer_network_id: &PeerNetworkId,
        request: &StorageServiceRequest,
    ) -> Result<(), Error> {
        // Validate the request and time the operation
        let validate_request = || {
            // If the peer is being ignored, return an error
            if let Some(peer_state) = self.unhealthy_peer_states.get(peer_network_id) {
                if peer_state.is_ignored() {
                    return Err(Error::TooManyInvalidRequests(format!(
                        "Peer is temporarily ignored. Unable to handle request: {:?}",
                        request
                    )));
                }
            }

            // Get the latest storage server summary
            let storage_server_summary = self.cached_storage_server_summary.load();

            // Verify the request is serviceable using the current storage server summary
            if !storage_server_summary.can_service(
                &self.aptos_data_client_config,
                self.time_service.clone(),
                request,
            ) {
                // Increment the invalid request count for the peer
                let mut unhealthy_peer_state = self
                    .unhealthy_peer_states
                    .entry(*peer_network_id)
                    .or_insert_with(|| {
                        // Create a new unhealthy peer state (this is the first invalid request)
                        let max_invalid_requests =
                            self.storage_service_config.max_invalid_requests_per_peer;
                        let min_time_to_ignore_peers_secs =
                            self.storage_service_config.min_time_to_ignore_peers_secs;
                        let time_service = self.time_service.clone();

                        UnhealthyPeerState::new(
                            max_invalid_requests,
                            min_time_to_ignore_peers_secs,
                            time_service,
                        )
                    });
                unhealthy_peer_state.increment_invalid_request_count(peer_network_id);

                // Return the validation error
                return Err(Error::InvalidRequest(format!(
                    "The given request cannot be satisfied. Request: {:?}, storage summary: {:?}",
                    request, storage_server_summary
                )));
            }

            Ok(()) // The request is valid
        };
        utils::execute_and_time_duration(
            &metrics::STORAGE_REQUEST_VALIDATION_LATENCY,
            Some((peer_network_id, request)),
            None,
            validate_request,
            None,
        )
    }
```

**File:** state-sync/storage-service/types/src/responses.rs (L688-808)
```rust
impl DataSummary {
    /// Returns true iff the request can be serviced
    pub fn can_service(
        &self,
        aptos_data_client_config: &AptosDataClientConfig,
        time_service: TimeService,
        request: &StorageServiceRequest,
    ) -> bool {
        match &request.data_request {
            GetServerProtocolVersion | GetStorageServerSummary => true,
            GetEpochEndingLedgerInfos(request) => {
                let desired_range =
                    match CompleteDataRange::new(request.start_epoch, request.expected_end_epoch) {
                        Ok(desired_range) => desired_range,
                        Err(_) => return false,
                    };
                self.epoch_ending_ledger_infos
                    .map(|range| range.superset_of(&desired_range))
                    .unwrap_or(false)
            },
            GetNewTransactionOutputsWithProof(_) => can_service_optimistic_request(
                aptos_data_client_config,
                time_service,
                self.synced_ledger_info.as_ref(),
            ),
            GetNewTransactionsWithProof(_) => can_service_optimistic_request(
                aptos_data_client_config,
                time_service,
                self.synced_ledger_info.as_ref(),
            ),
            GetNewTransactionsOrOutputsWithProof(_) => can_service_optimistic_request(
                aptos_data_client_config,
                time_service,
                self.synced_ledger_info.as_ref(),
            ),
            GetNumberOfStatesAtVersion(version) => self
                .states
                .map(|range| range.contains(*version))
                .unwrap_or(false),
            GetStateValuesWithProof(request) => {
                let proof_version = request.version;

                let can_serve_states = self
                    .states
                    .map(|range| range.contains(request.version))
                    .unwrap_or(false);

                let can_create_proof = self
                    .synced_ledger_info
                    .as_ref()
                    .map(|li| li.ledger_info().version() >= proof_version)
                    .unwrap_or(false);

                can_serve_states && can_create_proof
            },
            GetTransactionOutputsWithProof(request) => self
                .can_service_transaction_outputs_with_proof(
                    request.start_version,
                    request.end_version,
                    request.proof_version,
                ),
            GetTransactionsWithProof(request) => self.can_service_transactions_with_proof(
                request.start_version,
                request.end_version,
                request.proof_version,
            ),
            GetTransactionsOrOutputsWithProof(request) => self
                .can_service_transactions_or_outputs_with_proof(
                    request.start_version,
                    request.end_version,
                    request.proof_version,
                ),
            SubscribeTransactionOutputsWithProof(_) => can_service_subscription_request(
                aptos_data_client_config,
                time_service,
                self.synced_ledger_info.as_ref(),
            ),
            SubscribeTransactionsOrOutputsWithProof(_) => can_service_subscription_request(
                aptos_data_client_config,
                time_service,
                self.synced_ledger_info.as_ref(),
            ),
            SubscribeTransactionsWithProof(_) => can_service_subscription_request(
                aptos_data_client_config,
                time_service,
                self.synced_ledger_info.as_ref(),
            ),

            // Transaction data v2 requests (transactions with auxiliary data)
            GetTransactionDataWithProof(request) => match request.transaction_data_request_type {
                TransactionDataRequestType::TransactionData(_) => self
                    .can_service_transactions_with_proof(
                        request.start_version,
                        request.end_version,
                        request.proof_version,
                    ),
                TransactionDataRequestType::TransactionOutputData => self
                    .can_service_transaction_outputs_with_proof(
                        request.start_version,
                        request.end_version,
                        request.proof_version,
                    ),
                TransactionDataRequestType::TransactionOrOutputData(_) => self
                    .can_service_transactions_or_outputs_with_proof(
                        request.start_version,
                        request.end_version,
                        request.proof_version,
                    ),
            },
            GetNewTransactionDataWithProof(_) => can_service_optimistic_request(
                aptos_data_client_config,
                time_service,
                self.synced_ledger_info.as_ref(),
            ),
            SubscribeTransactionDataWithProof(_) => can_service_subscription_request(
                aptos_data_client_config,
                time_service,
                self.synced_ledger_info.as_ref(),
            ),
        }
    }
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-50)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;

    // Verify the given name has an appropriate length
    if thread_name.len() > MAX_THREAD_NAME_LENGTH {
        panic!(
            "The given runtime thread name is too long! Max length: {}, given name: {}",
            MAX_THREAD_NAME_LENGTH, thread_name
        );
    }

    // Create the runtime builder
    let atomic_id = AtomicUsize::new(0);
    let thread_name_clone = thread_name.clone();
    let mut builder = Builder::new_multi_thread();
    builder
        .thread_name_fn(move || {
            let id = atomic_id.fetch_add(1, Ordering::SeqCst);
            format!("{}-{}", thread_name_clone, id)
        })
        .on_thread_start(on_thread_start)
        .disable_lifo_slot()
        // Limit concurrent blocking tasks from spawn_blocking(), in case, for example, too many
        // Rest API calls overwhelm the node.
        .max_blocking_threads(MAX_BLOCKING_THREADS)
```
