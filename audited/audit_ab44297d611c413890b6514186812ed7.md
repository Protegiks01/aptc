# Audit Report

## Title
Race Condition in `send_for_execution()` Causes Validator Panic During Concurrent Root Updates

## Summary
A critical race condition exists in the `send_for_execution()` method where non-atomic read operations on `ordered_root()` allow concurrent threads to observe inconsistent consensus state, causing validators to panic and crash during normal consensus operation.

## Finding Description
The vulnerability exists in `send_for_execution()` where two separate lock acquisitions create a time-of-check to time-of-use (TOCTOU) race condition: [1](#0-0) 

The function performs:
1. **First read lock** (line 323): Checks if `block_to_commit.round() > ordered_root().round()`
2. **Lock released**
3. **Second read lock** (line 328): Calls `path_from_ordered_root(block_id_to_commit)`
4. **Lock released**

Between these two operations, another concurrent thread can acquire a write lock and update the ordered root via: [2](#0-1) 

This creates an inconsistency where the validation at line 323 uses one root value, but the path calculation at line 328 uses a different, updated root value.

The `path_from_ordered_root()` implementation walks backwards from a block to find the root: [3](#0-2) 

When the root is updated between the two reads, the path calculation can fail because it attempts to find a path from a block that is now an **ancestor** of the new root (not a descendant). The algorithm stops when `block.round() <= root_round` and then checks if the stopped block equals the root. When they don't match, it returns `None`.

The `.unwrap_or_default()` on line 329 converts `None` to an empty vector, causing the assertion on line 331 to **panic the validator process**.

**Exploitation Scenario:**
1. Initial state: `ordered_root` = Block A (round 10)
2. Thread 1 receives finality proof for Block B (round 15)
3. Thread 2 receives finality proof for Block C (round 20)
4. Thread 1: Validates Block B round (15) > root round (10) ✓
5. Thread 2: Validates Block C round (20) > root round (10) ✓
6. Thread 2: Updates `ordered_root` to Block C (round 20)
7. Thread 1: Calculates path from **new** root C (round 20) to Block B (round 15)
8. Since Block B (round 15 ≤ 20) is an ancestor, path calculation returns `None`
9. Empty vector causes assertion failure: **Validator crashes**

This occurs naturally when multiple quorum certificates arrive simultaneously and are processed concurrently by: [4](#0-3) 

And: [5](#0-4) 

## Impact Explanation
**High Severity** per Aptos bug bounty criteria:
- **Validator node crash**: The panic causes immediate validator process termination
- **Consensus liveness impact**: Reduces the active validator set, potentially approaching the 2/3 threshold needed for progress
- **No recovery without restart**: Requires manual intervention to restart the crashed validator
- **Affects normal operation**: Does not require Byzantine behavior or malicious actors

This breaks the **consensus liveness** invariant by removing validators from the network during normal operation.

## Likelihood Explanation
**Medium to High Likelihood:**
- Occurs during standard consensus operation when multiple quorum certificates arrive within microseconds of each other
- Network conditions (latency, batching) naturally create concurrent QC processing scenarios
- Higher likelihood during high-throughput periods or network congestion
- No special attacker capabilities required
- Can be triggered inadvertently through normal network timing

The race window is small (microseconds) but consensus operates at this timescale with async message processing, making concurrent execution common.

## Recommendation
Acquire a **single write lock** for the entire critical section to make the check-and-update atomic:

```rust
pub async fn send_for_execution(
    &self,
    finality_proof: WrappedLedgerInfo,
) -> anyhow::Result<()> {
    let block_id_to_commit = finality_proof.commit_info().id();
    let block_to_commit = self
        .get_block(block_id_to_commit)
        .ok_or_else(|| format_err!("Committed block id not found"))?;

    let finality_proof_clone = finality_proof.clone();
    self.pending_blocks
        .lock()
        .gc(finality_proof.commit_info().round());

    // FIXED: Acquire write lock once for entire critical section
    let mut tree_guard = self.inner.write();
    
    // First make sure that this commit is new
    ensure!(
        block_to_commit.round() > tree_guard.ordered_root().round(),
        "Committed block round lower than root"
    );

    let blocks_to_commit = tree_guard
        .path_from_ordered_root(block_id_to_commit)
        .unwrap_or_default();

    assert!(!blocks_to_commit.is_empty());

    // Update root and insert cert atomically under same lock
    tree_guard.update_ordered_root(block_to_commit.id());
    tree_guard.insert_ordered_cert(finality_proof_clone.clone());
    
    drop(tree_guard); // Explicitly release lock
    
    update_counters_for_ordered_blocks(&blocks_to_commit);

    self.execution_client
        .finalize_order(blocks_to_commit, finality_proof.clone())
        .await
        .expect("Failed to persist commit");

    Ok(())
}
```

## Proof of Concept
```rust
#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_concurrent_send_for_execution_race() {
    // Setup: Create BlockStore with initial root at round 10
    let (block_store, blocks) = setup_block_store_with_blocks(/* rounds 1-25 */);
    
    // Create two finality proofs for different blocks
    let proof_block_15 = create_finality_proof(blocks[15].clone());
    let proof_block_20 = create_finality_proof(blocks[20].clone());
    
    // Launch concurrent sends
    let store1 = block_store.clone();
    let store2 = block_store.clone();
    
    let handle1 = tokio::spawn(async move {
        store1.send_for_execution(proof_block_15).await
    });
    
    let handle2 = tokio::spawn(async move {
        store2.send_for_execution(proof_block_20).await
    });
    
    // One of these will panic due to the race condition
    let (result1, result2) = tokio::join!(handle1, handle2);
    
    // Expected: At least one thread panics with "assertion failed: !blocks_to_commit.is_empty()"
    assert!(result1.is_err() || result2.is_err(), 
        "Race condition should cause at least one thread to panic");
}
```

## Notes
The vulnerability is subtle because RwLock correctly prevents data races at the memory level, but the **logical race** occurs between separate lock acquisitions. The consensus invariant assumes atomic root observation and update, but the implementation splits this into non-atomic operations. This is a classic TOCTOU (Time-Of-Check-Time-Of-Use) vulnerability at the application logic level.

### Citations

**File:** consensus/src/block_storage/block_store.rs (L322-331)
```rust
        ensure!(
            block_to_commit.round() > self.ordered_root().round(),
            "Committed block round lower than root"
        );

        let blocks_to_commit = self
            .path_from_ordered_root(block_id_to_commit)
            .unwrap_or_default();

        assert!(!blocks_to_commit.is_empty());
```

**File:** consensus/src/block_storage/block_store.rs (L338-338)
```rust
        self.inner.write().update_ordered_root(block_to_commit.id());
```

**File:** consensus/src/block_storage/block_tree.rs (L519-545)
```rust
    pub(super) fn path_from_root_to_block(
        &self,
        block_id: HashValue,
        root_id: HashValue,
        root_round: u64,
    ) -> Option<Vec<Arc<PipelinedBlock>>> {
        let mut res = vec![];
        let mut cur_block_id = block_id;
        loop {
            match self.get_block(&cur_block_id) {
                Some(ref block) if block.round() <= root_round => {
                    break;
                },
                Some(block) => {
                    cur_block_id = block.parent_id();
                    res.push(block);
                },
                None => return None,
            }
        }
        // At this point cur_block.round() <= self.root.round()
        if cur_block_id != root_id {
            return None;
        }
        // Called `.reverse()` to get the chronically increased order.
        res.reverse();
        Some(res)
```

**File:** consensus/src/block_storage/sync_manager.rs (L186-189)
```rust
        if self.ordered_root().round() < qc.commit_info().round() {
            SUCCESSFUL_EXECUTED_WITH_REGULAR_QC.inc();
            self.send_for_execution(qc.into_wrapped_ledger_info())
                .await?;
```

**File:** consensus/src/block_storage/sync_manager.rs (L210-219)
```rust
        if self.ordered_root().round() < ordered_cert.ledger_info().ledger_info().round() {
            if let Some(ordered_block) = self.get_block(ordered_cert.commit_info().id()) {
                if !ordered_block.block().is_nil_block() {
                    observe_block(
                        ordered_block.block().timestamp_usecs(),
                        BlockStage::OC_ADDED,
                    );
                }
                SUCCESSFUL_EXECUTED_WITH_ORDER_VOTE_QC.inc();
                self.send_for_execution(ordered_cert.clone()).await?;
```
