# Audit Report

## Title
Stale Results from Failed Block Executions Cause Consensus Split in ShardedExecutorService

## Summary
The ShardedExecutorService's result collection mechanism has a critical flaw where failed block executions in early shards leave unconsumed results from later shards in unbounded channels. These stale results are then mixed with fresh results from subsequent block executions, causing different validators to compute different state roots for the same block and breaking consensus safety.

## Finding Description

The vulnerability exists in how the ShardedExecutorService aggregates results from multiple shards executing in parallel. The issue manifests in both local and remote executor implementations.

**Root Cause:**

The channels used to communicate execution results between shards and the coordinator are unbounded and reused across multiple block executions [1](#0-0) . When collecting results, the coordinator iterates through shards sequentially and uses the `?` operator for early return on error [2](#0-1) .

**Attack Flow:**

1. **Block N Execution Fails:**
   - Coordinator dispatches ExecuteSubBlocks commands to all shards
   - Shard 0 encounters an error (e.g., VMStatus error from resource exhaustion or internal VM failure)
   - Shards 1, 2, 3 execute successfully and send their results to their respective channels
   - Coordinator reads shard 0's error result and returns immediately via `?` operator, leaving results from shards 1, 2, 3 unconsumed in channels
   - Each shard loops back to wait for the next command [3](#0-2) 

2. **Block N+1 Execution Succeeds:**
   - Coordinator dispatches new ExecuteSubBlocks commands to all shards
   - All shards (0, 1, 2, 3) execute Block N+1 successfully and send results
   - Coordinator reads results sequentially:
     - Shard 0: Reads **new result** from Block N+1 (correct)
     - Shard 1: Reads **stale result** from Block N (incorrect!)
     - Shard 2: Reads **stale result** from Block N (incorrect!)
     - Shard 3: Reads **stale result** from Block N (incorrect!)

3. **State Corruption:**
   - The coordinator aggregates mixed results (one fresh + three stale)
   - Applies transaction outputs from two different blocks to the state
   - Computes an incorrect state root

The same vulnerability exists in the remote executor implementation [4](#0-3) .

**Consensus Impact:**

Since the timing of when each validator encounters errors is non-deterministic (depends on execution speed, resource availability, etc.), different validators will have different channel states:
- Validator A might have stale results from previous block in its channels
- Validator B might have clean channels
- Both execute the same block but compute different state roots
- Consensus breaks as they cannot agree on the block's state

This directly violates the critical invariant: "All validators must produce identical state roots for identical blocks."

## Impact Explanation

**Severity: CRITICAL** (Consensus Safety Violation)

This vulnerability falls under the **Critical Severity** category per Aptos Bug Bounty rules, specifically:
- **Consensus/Safety violations**: Different validators compute different state roots for identical blocks, breaking BFT consensus safety
- **Non-recoverable network partition**: Once validators diverge in state, they cannot reach consensus on subsequent blocks without manual intervention or hard fork

The impact includes:
1. **Consensus Failure**: Validators cannot agree on state transitions, halting block production
2. **State Corruption**: Transaction outputs from different blocks are incorrectly mixed
3. **Network Partition**: The network splits between validators with clean vs. polluted channel state
4. **Non-Deterministic Execution**: Same block produces different results based on execution history

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability will trigger whenever:
1. A block execution fails with a VMStatus error in an early shard (shard 0 or 1)
2. Later shards complete successfully
3. A subsequent block execution succeeds

While VMStatus-level errors (as opposed to individual transaction failures) are less common in normal operation, they can occur due to:
- Resource exhaustion in specific shards under load
- Internal VM errors or bugs
- Serialization failures in the remote executor path [5](#0-4) 
- Edge cases in parallel execution

An attacker could potentially increase likelihood by:
- Submitting transactions that cause resource exhaustion in targeted shards
- Exploiting any transaction patterns that trigger deterministic VM errors in specific partitions

The vulnerability can also manifest during legitimate operational scenarios like network issues, resource constraints, or transient failures.

## Recommendation

**Fix: Drain channels before each block execution**

Add explicit channel cleanup to ensure no stale results remain:

```rust
// In LocalExecutorClient::execute_block, before sending commands:
fn execute_block(&self, ...) -> Result<ShardedExecutionOutput, VMStatus> {
    // Drain any stale results from previous failed executions
    for rx in self.result_rxs.iter() {
        while rx.try_recv().is_ok() {
            // Discard stale messages
        }
    }
    
    // Now proceed with execution
    for (i, sub_blocks_for_shard) in sub_blocks.into_iter().enumerate() {
        self.command_txs[i].send(ExecutorShardCommand::ExecuteSubBlocks(...)).unwrap();
    }
    
    // ... rest of execution
}
```

**Alternative Fix: Use bounded channels with capacity 1**

Replace unbounded channels with bounded channels that have capacity 1. This ensures that:
- Shards block on sending if the previous result hasn't been consumed
- Forces synchronization between coordinator and shards
- Prevents accumulation of stale results

**Alternative Fix: Use oneshot channels per execution**

Create new oneshot channels for each block execution instead of reusing channels. This guarantees fresh communication paths and prevents any possibility of stale results.

## Proof of Concept

```rust
// Rust integration test demonstrating the vulnerability
#[test]
fn test_stale_results_in_sharded_executor() {
    use aptos_vm::sharded_block_executor::local_executor_shard::LocalExecutorService;
    use crossbeam_channel::unbounded;
    
    // Setup: Create a sharded executor with 4 shards
    let num_shards = 4;
    let executor_client = LocalExecutorService::setup_local_executor_shards(num_shards, None);
    
    // Block N: Craft transactions that cause shard 0 to error
    // while shards 1, 2, 3 succeed
    let block_n = create_test_block_with_shard_0_error();
    let partitioned_n = partition_transactions(block_n, num_shards);
    
    // Execute Block N - expect error from shard 0
    let result_n = executor_client.execute_block(
        Arc::new(test_state_view),
        partitioned_n,
        16,
        test_config,
    );
    assert!(result_n.is_err()); // Shard 0 error causes early return
    
    // At this point, shards 1, 2, 3 have sent their results
    // but coordinator never read them
    
    // Block N+1: Normal block that succeeds in all shards
    let block_n_plus_1 = create_test_block_normal();
    let partitioned_n_plus_1 = partition_transactions(block_n_plus_1, num_shards);
    
    // Execute Block N+1 - should succeed
    let result_n_plus_1 = executor_client.execute_block(
        Arc::new(test_state_view),
        partitioned_n_plus_1,
        16,
        test_config,
    ).unwrap();
    
    // BUG: result_n_plus_1 contains:
    // - Outputs from shard 0 of block N+1 (correct)
    // - Outputs from shards 1, 2, 3 of block N (stale, incorrect!)
    
    // Verify the corruption by checking transaction hashes
    // Expected: All outputs should be from block N+1 transactions
    // Actual: Mixed outputs from blocks N and N+1
    let expected_txn_hashes = get_transaction_hashes(block_n_plus_1);
    let actual_txn_hashes = extract_transaction_hashes(&result_n_plus_1);
    
    assert_ne!(expected_txn_hashes, actual_txn_hashes, 
        "Stale results detected: outputs from block N mixed with block N+1");
}
```

## Notes

The vulnerability is present in both local execution [2](#0-1)  and remote execution paths [4](#0-3) . The error aggregation logic in the coordinator [6](#0-5)  assumes results are always fresh and valid for the current execution.

The error types defined in the error.rs file (InternalError and SerializationError) [7](#0-6)  are not directly mixed or confused, but the vulnerability allows entire result payloads (containing these errors or successful outputs) to be read out of order, achieving the same problematic outcome described in the security question.

### Citations

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L88-91)
```rust
        let (result_txs, result_rxs): (
            Vec<Sender<Result<Vec<Vec<TransactionOutput>>, VMStatus>>>,
            Vec<Receiver<Result<Vec<Vec<TransactionOutput>>, VMStatus>>>,
        ) = (0..num_shards).map(|_| unbounded()).unzip();
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L164-175)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        let _timer = WAIT_FOR_SHARDED_OUTPUT_SECONDS.start_timer();
        trace!("LocalExecutorClient Waiting for results");
        let mut results = vec![];
        for (i, rx) in self.result_rxs.iter().enumerate() {
            results.push(
                rx.recv()
                    .unwrap_or_else(|_| panic!("Did not receive output from shard {}", i))?,
            );
        }
        Ok(results)
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L215-260)
```rust
    pub fn start(&self) {
        trace!(
            "Shard starting, shard_id={}, num_shards={}.",
            self.shard_id,
            self.num_shards
        );
        let mut num_txns = 0;
        loop {
            let command = self.coordinator_client.receive_execute_command();
            match command {
                ExecutorShardCommand::ExecuteSubBlocks(
                    state_view,
                    transactions,
                    concurrency_level_per_shard,
                    onchain_config,
                ) => {
                    num_txns += transactions.num_txns();
                    trace!(
                        "Shard {} received ExecuteBlock command of block size {} ",
                        self.shard_id,
                        num_txns
                    );
                    let exe_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "execute_block"]);
                    let ret = self.execute_block(
                        transactions,
                        state_view.as_ref(),
                        BlockExecutorConfig {
                            local: BlockExecutorLocalConfig::default_with_concurrency_level(
                                concurrency_level_per_shard,
                            ),
                            onchain: onchain_config,
                        },
                    );
                    drop(state_view);
                    drop(exe_timer);

                    let _result_tx_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "result_tx"]);
                    self.coordinator_client.send_execution_result(ret);
                },
                ExecutorShardCommand::Stop => {
                    break;
                },
            }
        }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L163-172)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        trace!("RemoteExecutorClient Waiting for results");
        let mut results = vec![];
        for rx in self.result_rxs.iter() {
            let received_bytes = rx.recv().unwrap().to_bytes();
            let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
            results.push(result.inner?);
        }
        Ok(results)
    }
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L115-119)
```rust
    fn send_execution_result(&self, result: Result<Vec<Vec<TransactionOutput>>, VMStatus>) {
        let remote_execution_result = RemoteExecutionResult::new(result);
        let output_message = bcs::to_bytes(&remote_execution_result).unwrap();
        self.result_tx.send(Message::new(output_message)).unwrap();
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/mod.rs (L86-94)
```rust
        let (sharded_output, global_output) = self
            .executor_client
            .execute_block(
                state_view,
                transactions,
                concurrency_level_per_shard,
                onchain_config,
            )?
            .into_inner();
```

**File:** execution/executor-service/src/error.rs (L9-14)
```rust
pub enum Error {
    #[error("Internal error: {0}")]
    InternalError(String),
    #[error("Serialization error: {0}")]
    SerializationError(String),
}
```
