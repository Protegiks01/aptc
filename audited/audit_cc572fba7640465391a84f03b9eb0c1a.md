# Audit Report

## Title
Atomicity Violation in Network Message Broadcasting Causes Partial Delivery of Critical Consensus Messages

## Summary
The `send_to_peers()` function in `network/framework/src/application/interface.rs` lacks atomicity guarantees when broadcasting messages to multiple protocols and networks. If a network channel fails mid-broadcast, previously sent messages are not rolled back, resulting in partial delivery that can cause consensus state inconsistencies and liveness failures.

## Finding Description

The vulnerability exists in the `send_to_peers()` function which broadcasts messages to multiple validators grouped by protocol and network: [1](#0-0) 

The function uses nested loops to iterate through protocol groups and network groups. When `get_sender_for_network_id()` or `network_sender.send_to_many()` fails (indicated by the `?` operator), the function returns immediately without rolling back messages already enqueued in previous iterations.

The failure can occur when the underlying channel is closed, as enforced here: [2](#0-1) 

When `receiver_dropped` is true (line 98), the `push()` operation returns an error. This propagates through the call chain: [3](#0-2) 

The `send_to_many()` function (lines 76-79) explicitly states it returns early on failure, but **already-sent messages remain in the queue**.

**Attack Scenario:**

1. A validator broadcasts a critical consensus message (VoteMsg, ProposalMsg, CommitVote) to all validators
2. Peers are grouped by protocol using `group_peers_by_protocol()`: [4](#0-3) 
3. If peers support different protocols (e.g., Peer A uses ConsensusDirectSendCompressed, Peer B uses ConsensusDirectSendBcs), they are in separate groups
4. The outer loop sends to the first protocol group successfully
5. During the second protocol group iteration, the PeerManager channel closes (node shutdown, panic, crash)
6. The channel closure causes `push()` to fail with "Channel is closed" error
7. **Result:** Peers in the first group received the message, peers in the second group did not
8. No rollback mechanism exists to unsend messages from the first group

This is used by consensus for critical broadcasts: [5](#0-4) 

The `broadcast_without_self()` function calls `send_to_many()` which internally calls `send_to_peers()`. This affects:
- `broadcast_proposal()` - Block proposals
- `broadcast_vote()` - Voting messages  
- `broadcast_commit_vote()` - Commit votes
- `broadcast_proof_of_store_msg()` - Quorum Store messages

## Impact Explanation

This vulnerability has **High Severity** impact:

**Consensus Liveness Impact:**
- Partial delivery of VoteMsg means some validators count a vote while others don't
- This can prevent quorum formation on the receiving validators that didn't get the vote
- Can cause temporary stalls where validators wait for votes that were "sent" but never delivered
- May trigger unnecessary synchronization rounds and block retrievals

**State Inconsistency:**
- Different validators have different views of which messages were broadcast
- Some validators advance their state based on received messages while others remain behind
- Creates temporary network partitions at the application layer

**Quorum Store Impact:**
- Partial delivery of ProofOfStore or BatchInfo messages can cause batch availability issues
- Some validators may consider batches available while others don't
- Affects transaction throughput and latency

This meets the **High Severity** criteria: "Significant protocol violations" and "Validator node slowdowns" caused by inconsistent message delivery disrupting consensus progress.

## Likelihood Explanation

**Likelihood: Medium-to-High** during specific operational scenarios:

**Triggering Conditions:**
1. Node graceful shutdown initiated while broadcasts are in progress
2. Panic/crash in PeerManager receiver task during message processing
3. Resource exhaustion causing channel receiver to drop
4. Rapid epoch transitions with concurrent shutdowns

**Frequency Factors:**
- Happens whenever channel closes during the small time window of multi-protocol/network iteration
- More likely in networks with heterogeneous protocol support across validators
- More likely during operational events (upgrades, restarts, crashes)
- The nested loop structure increases vulnerability surface (multiple failure points)

**Practical Occurrence:**
- Production networks undergo regular upgrades and restarts
- Byzantine validators or buggy nodes may crash unexpectedly
- The race condition window is narrow but non-zero during each broadcast

## Recommendation

**Immediate Fix: Implement All-or-Nothing Semantics**

Replace the current implementation with a two-phase approach that collects all messages first, then sends atomically:

```rust
fn send_to_peers(&self, message: Message, peers: Vec<PeerNetworkId>) -> Result<(), Error> {
    let peers_per_protocol = self.group_peers_by_protocol(peers);
    
    // Phase 1: Pre-validate and prepare all sends (fail-fast)
    let mut prepared_sends = Vec::new();
    for (protocol_id, peers) in peers_per_protocol {
        for (network_id, peers) in &peers
            .iter()
            .chunk_by(|peer_network_id| peer_network_id.network_id())
        {
            let network_sender = self.get_sender_for_network_id(&network_id)?;
            let peer_ids: Vec<_> = peers.map(|peer_network_id| peer_network_id.peer_id()).collect();
            prepared_sends.push((network_sender, peer_ids, protocol_id));
        }
    }
    
    // Phase 2: Execute all sends (point of no return)
    for (network_sender, peer_ids, protocol_id) in prepared_sends {
        network_sender.send_to_many(peer_ids.into_iter(), protocol_id, message.clone())?;
    }
    
    Ok(())
}
```

**Alternative Fix: Add Transaction-Like Semantics**

Modify the underlying channel to support batched operations with rollback:
1. Add a "batch mode" to `aptos_channel` that buffers messages
2. Support commit/rollback operations on the batch
3. Only make messages visible after successful commit

**Monitoring Addition:**

Add metrics to track partial send failures:
```rust
counters::NETWORK_PARTIAL_SEND_FAILURES
    .with_label_values(&[msg_type])
    .inc();
```

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use aptos_channels;
    use std::sync::Arc;
    
    #[tokio::test]
    async fn test_partial_delivery_on_channel_closure() {
        // Setup: Create network client with two protocol groups
        let (tx1, mut rx1) = aptos_channel::new(QueueStyle::FIFO, 10, None);
        let (tx2, mut rx2) = aptos_channel::new(QueueStyle::FIFO, 10, None);
        
        let network1_sender = NetworkSender::new(
            PeerManagerRequestSender::new(tx1),
            /* ... */
        );
        let network2_sender = NetworkSender::new(
            PeerManagerRequestSender::new(tx2),
            /* ... */
        );
        
        let mut network_senders = HashMap::new();
        network_senders.insert(NetworkId::Validator, network1_sender);
        network_senders.insert(NetworkId::Vfn, network2_sender);
        
        let client = NetworkClient::new(
            vec![ProtocolId::ConsensusDirectSendBcs],
            vec![],
            network_senders,
            Arc::new(PeersAndMetadata::new(&[])),
        );
        
        // Create peers in two different networks
        let peer1 = PeerNetworkId::new(NetworkId::Validator, PeerId::random());
        let peer2 = PeerNetworkId::new(NetworkId::Vfn, PeerId::random());
        
        let msg = ConsensusMsg::VoteMsg(Box::new(/* construct vote */));
        
        // Drop rx2 to simulate channel closure for second network
        drop(rx2);
        
        // Attempt broadcast - should fail on second network
        let result = client.send_to_peers(msg, vec![peer1, peer2]);
        
        // Assert: First network received message (partial delivery)
        assert!(rx1.try_next().is_ok());
        
        // Assert: Second network failed
        assert!(result.is_err());
        
        // BUG: First network's message was not rolled back!
    }
}
```

This demonstrates that when the second network's channel is closed, the first network still receives the message without any rollback mechanism, confirming the atomicity violation.

**Notes:**

While this vulnerability requires specific timing conditions to trigger, it represents a fundamental design flaw where critical consensus operations lack atomicity guarantees. The impact is amplified during operational events (upgrades, crashes) when channel closures are more likely. The lack of rollback semantics violates the implicit expectation that broadcast operations are all-or-nothing, potentially causing subtle consensus liveness issues that are difficult to diagnose in production environments.

### Citations

**File:** network/framework/src/application/interface.rs (L160-191)
```rust
    fn group_peers_by_protocol(
        &self,
        peers: Vec<PeerNetworkId>,
    ) -> HashMap<ProtocolId, Vec<PeerNetworkId>> {
        // Sort peers by protocol
        let mut peers_per_protocol = HashMap::new();
        let mut peers_without_a_protocol = vec![];
        for peer in peers {
            match self
                .get_preferred_protocol_for_peer(&peer, &self.direct_send_protocols_and_preferences)
            {
                Ok(protocol) => peers_per_protocol
                    .entry(protocol)
                    .or_insert_with(Vec::new)
                    .push(peer),
                Err(_) => peers_without_a_protocol.push(peer),
            }
        }

        // We only periodically log any unavailable peers (to prevent log spamming)
        if !peers_without_a_protocol.is_empty() {
            sample!(
                SampleRate::Duration(Duration::from_secs(10)),
                warn!(
                    "[sampled] Unavailable peers (without a common network protocol): {:?}",
                    peers_without_a_protocol
                )
            );
        }

        peers_per_protocol
    }
```

**File:** network/framework/src/application/interface.rs (L243-258)
```rust
    fn send_to_peers(&self, message: Message, peers: Vec<PeerNetworkId>) -> Result<(), Error> {
        let peers_per_protocol = self.group_peers_by_protocol(peers);

        // Send to all peers in each protocol group and network
        for (protocol_id, peers) in peers_per_protocol {
            for (network_id, peers) in &peers
                .iter()
                .chunk_by(|peer_network_id| peer_network_id.network_id())
            {
                let network_sender = self.get_sender_for_network_id(&network_id)?;
                let peer_ids = peers.map(|peer_network_id| peer_network_id.peer_id());
                network_sender.send_to_many(peer_ids, protocol_id, message.clone())?;
            }
        }
        Ok(())
    }
```

**File:** crates/channel/src/aptos_channel.rs (L91-112)
```rust
    pub fn push_with_feedback(
        &self,
        key: K,
        message: M,
        status_ch: Option<oneshot::Sender<ElementStatus<M>>>,
    ) -> Result<()> {
        let mut shared_state = self.shared_state.lock();
        ensure!(!shared_state.receiver_dropped, "Channel is closed");
        debug_assert!(shared_state.num_senders > 0);

        let dropped = shared_state.internal_queue.push(key, (message, status_ch));
        // If this or an existing message had to be dropped because of the queue being full, we
        // notify the corresponding status channel if it was registered.
        if let Some((dropped_val, Some(dropped_status_ch))) = dropped {
            // Ignore errors.
            let _err = dropped_status_ch.send(ElementStatus::Dropped(dropped_val));
        }
        if let Some(w) = shared_state.waker.take() {
            w.wake();
        }
        Ok(())
    }
```

**File:** network/framework/src/peer_manager/senders.rs (L68-86)
```rust
    pub fn send_to_many(
        &self,
        recipients: impl Iterator<Item = PeerId>,
        protocol_id: ProtocolId,
        mdata: Bytes,
    ) -> Result<(), PeerManagerError> {
        let msg = Message { protocol_id, mdata };
        for recipient in recipients {
            // We return `Err` early here if the send fails. Since sending will
            // only fail if the queue is unexpectedly shutdown (i.e., receiver
            // dropped early), we know that we can't make further progress if
            // this send fails.
            self.inner.push(
                (recipient, protocol_id),
                PeerManagerRequest::SendDirectSend(recipient, msg.clone()),
            )?;
        }
        Ok(())
    }
```

**File:** consensus/src/network.rs (L387-408)
```rust
    pub fn broadcast_without_self(&self, msg: ConsensusMsg) {
        fail_point!("consensus::send::any", |_| ());

        let self_author = self.author;
        let mut other_validators: Vec<_> = self
            .validators
            .get_ordered_account_addresses_iter()
            .filter(|author| author != &self_author)
            .collect();
        self.sort_peers_by_latency(&mut other_validators);

        counters::CONSENSUS_SENT_MSGS
            .with_label_values(&[msg.name()])
            .inc_by(other_validators.len() as u64);
        // Broadcast message over direct-send to all other validators.
        if let Err(err) = self
            .consensus_network_client
            .send_to_many(other_validators, msg)
        {
            warn!(error = ?err, "Error broadcasting message");
        }
    }
```
