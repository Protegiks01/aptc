# Audit Report

## Title
Time-of-Check-Time-of-Use Race Condition in Storage Service Transaction Availability

## Summary
The storage service maintains a cached `StorageServerSummary` that clients use to determine data availability via `can_service_transactions()`. This cache is refreshed every 100ms, creating a race condition window where transactions can be pruned between the availability check and the actual data serving, causing state synchronization failures. [1](#0-0) 

## Finding Description
The vulnerability exists in the state synchronization layer where the storage service advertises data availability through a cached summary, but this cache can become stale due to concurrent pruning operations.

**Architecture Flow:**

1. The storage service maintains `cached_storage_server_summary` refreshed periodically: [2](#0-1) 

2. The refresh interval is 100ms by default: [3](#0-2) 

3. The cache refresh fetches transaction ranges from storage: [4](#0-3) 

4. The transaction range is based on `get_first_txn_version()` which queries the ledger pruner: [5](#0-4) 

5. When clients request transactions, the database enforces pruning checks: [6](#0-5) 

6. The pruning check validates against current minimum readable version: [7](#0-6) 

**Race Condition Scenario:**

- **T0:** Cached summary shows transactions [100, 1000] available
- **T0+10ms:** Client fetches summary, `can_service_transactions([100, 200])` returns `true`
- **T0+50ms:** Ledger pruner executes, minimum readable version becomes 150
- **T0+60ms:** Client sends `GetTransactionsWithProof` request for [100, 200]
- **T0+70ms:** Handler calls storage layer which invokes `error_if_ledger_pruned("Transaction", 100)`
- **T0+70ms:** Check fails (100 < 150), returns `AptosDbError`
- **T0+100ms:** Cache refresh occurs (too late), new summary shows [150, 1000]

The client receives an error for data it was explicitly told was available, breaking the protocol's availability guarantee.

## Impact Explanation
This qualifies as **High Severity** under the Aptos bug bounty program's "Significant protocol violations" category:

1. **State Synchronization Failures:** Clients attempting to sync state receive errors when fetching data they verified as available, causing sync failures and retry loops.

2. **Network-Wide Impact:** Multiple peers syncing from similar version ranges can be simultaneously affected, degrading overall network health.

3. **Reduced Availability:** The race condition window (up to 100ms) creates periods where advertised data cannot be served, violating availability guarantees critical for state sync.

4. **Performance Degradation:** Failed requests trigger exponential backoff and retry logic, increasing network overhead and reducing sync throughput. [8](#0-7) 

## Likelihood Explanation
**Likelihood: Medium to High**

1. **Window Size:** 100ms refresh interval provides sufficient window for the race condition to manifest.

2. **Pruning Frequency:** While pruning is periodic (not continuous), active nodes with aggressive pruning configurations increase probability.

3. **High-Traffic Scenarios:** During network stress or catch-up scenarios, multiple clients may request similar version ranges, amplifying the issue.

4. **No Synchronization:** There is no coordination between the cache refresh mechanism and the pruning process, making the race condition inherent to the design. [9](#0-8) 

## Recommendation
Implement atomic validation at request time rather than relying solely on cached data:

**Solution 1: Real-time Validation (Recommended)**
Add a validation step in the handler that re-checks data availability immediately before serving:

```rust
fn get_transactions_with_proof(
    &self,
    request: &TransactionsWithProofRequest,
) -> aptos_storage_service_types::Result<DataResponse, Error> {
    // Validate data is still available before attempting to serve
    let data_summary = self.storage.get_data_summary()?;
    let desired_range = CompleteDataRange::new(
        request.start_version,
        request.end_version,
    ).map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
    
    if !data_summary.transactions
        .map(|range| range.superset_of(&desired_range))
        .unwrap_or(false) 
    {
        return Err(Error::DataUnavailable(format!(
            "Transactions [{}, {}] no longer available (pruned)",
            request.start_version, request.end_version
        )));
    }
    
    // Proceed with original implementation
    let response = self.storage.get_transactions_with_proof(
        request.proof_version,
        request.start_version,
        request.end_version,
        request.include_events,
    )?;
    // ...
}
```

**Solution 2: Tighter Refresh Interval**
Reduce the cache refresh interval significantly (e.g., 10-20ms) to minimize the race window, though this increases database load.

**Solution 3: Version Pinning**
Include the summary version/timestamp in responses, allowing clients to understand if they're working with stale data and retry with fresh information.

## Proof of Concept

```rust
#[cfg(test)]
mod race_condition_tests {
    use super::*;
    use aptos_storage_service_types::{
        requests::{StorageServiceRequest, DataRequest, TransactionsWithProofRequest},
    };
    use std::sync::Arc;
    use std::thread;
    use std::time::Duration;

    #[tokio::test]
    async fn test_pruning_race_condition() {
        // Setup: Create storage service with mock storage
        let (mut mock_storage, storage_service) = setup_test_storage_service();
        
        // Step 1: Initialize with transactions [100, 1000]
        mock_storage.set_transaction_range(100, 1000);
        
        // Step 2: Client fetches storage summary
        let summary = storage_service.get_cached_summary();
        let can_service = summary.data_summary.can_service_transactions(
            &CompleteDataRange::new(100, 200).unwrap()
        );
        assert!(can_service, "Summary should show transactions available");
        
        // Step 3: Simulate pruning - minimum version becomes 150
        mock_storage.prune_transactions_up_to(150);
        
        // Step 4: Client attempts to fetch transactions [100, 200]
        // (before cache refresh at 100ms)
        thread::sleep(Duration::from_millis(50));
        
        let request = StorageServiceRequest {
            data_request: DataRequest::GetTransactionsWithProof(
                TransactionsWithProofRequest {
                    proof_version: 1000,
                    start_version: 100,
                    end_version: 200,
                    include_events: false,
                }
            ),
            use_compression: false,
        };
        
        // Step 5: Verify that request fails with pruning error
        let result = storage_service.process_request(&peer, request, false);
        
        assert!(result.is_err(), "Request should fail due to pruned data");
        assert!(
            matches!(result.unwrap_err(), 
                StorageServiceError::InternalError(msg) 
                if msg.contains("pruned")
            ),
            "Error should indicate data was pruned"
        );
        
        // Step 6: After cache refresh, summary should reflect new range
        thread::sleep(Duration::from_millis(60)); // Total 110ms elapsed
        let updated_summary = storage_service.get_cached_summary();
        let updated_range = updated_summary.data_summary.transactions.unwrap();
        
        assert_eq!(updated_range.lowest(), 150, "Cache should now show pruned range");
    }
}
```

**Notes:**
- The race condition naturally occurs in production without malicious actors
- It affects system reliability and availability guarantees
- The fix requires careful consideration of performance vs. consistency tradeoffs
- Current caching mechanism prioritizes performance but sacrifices atomicity

### Citations

**File:** state-sync/storage-service/types/src/responses.rs (L826-829)
```rust
    fn can_service_transactions(&self, desired_range: &CompleteDataRange<u64>) -> bool {
        self.transactions
            .map(|range| range.superset_of(desired_range))
            .unwrap_or(false)
```

**File:** state-sync/storage-service/server/src/lib.rs (L67-67)
```rust
    cached_storage_server_summary: Arc<ArcSwap<StorageServerSummary>>,
```

**File:** state-sync/storage-service/server/src/lib.rs (L512-565)
```rust
pub(crate) fn refresh_cached_storage_summary<T: StorageReaderInterface>(
    cached_storage_server_summary: Arc<ArcSwap<StorageServerSummary>>,
    storage: T,
    storage_config: StorageServiceConfig,
    cache_update_notifiers: Vec<aptos_channel::Sender<(), CachedSummaryUpdateNotification>>,
) {
    // Fetch the new data summary from storage
    let new_data_summary = match storage.get_data_summary() {
        Ok(data_summary) => data_summary,
        Err(error) => {
            error!(LogSchema::new(LogEntry::StorageSummaryRefresh)
                .error(&Error::StorageErrorEncountered(error.to_string()))
                .message("Failed to refresh the cached storage summary!"));
            return;
        },
    };

    // Initialize the protocol metadata
    let new_protocol_metadata = ProtocolMetadata {
        max_epoch_chunk_size: storage_config.max_epoch_chunk_size,
        max_transaction_chunk_size: storage_config.max_transaction_chunk_size,
        max_state_chunk_size: storage_config.max_state_chunk_size,
        max_transaction_output_chunk_size: storage_config.max_transaction_output_chunk_size,
    };

    // Create the new storage server summary
    let new_storage_server_summary = StorageServerSummary {
        protocol_metadata: new_protocol_metadata,
        data_summary: new_data_summary,
    };

    // If the new storage server summary is different to the existing one,
    // update the cache and send a notification via the notifier channel.
    let existing_storage_server_summary = cached_storage_server_summary.load().clone();
    if existing_storage_server_summary.deref().clone() != new_storage_server_summary {
        // Update the storage server summary cache
        cached_storage_server_summary.store(Arc::new(new_storage_server_summary.clone()));

        // Create an update notification
        let highest_synced_version = new_storage_server_summary
            .data_summary
            .get_synced_ledger_info_version();
        let update_notification = CachedSummaryUpdateNotification::new(highest_synced_version);

        // Send a notification via each notifier channel
        for cached_summary_update_notifier in cache_update_notifiers {
            if let Err(error) = cached_summary_update_notifier.push((), update_notification) {
                error!(LogSchema::new(LogEntry::StorageSummaryRefresh)
                    .error(&Error::StorageErrorEncountered(error.to_string()))
                    .message("Failed to send an update notification for the new cached summary!"));
            }
        }
    }
}
```

**File:** config/src/config/state_sync_config.rs (L215-215)
```rust
            storage_summary_refresh_interval_ms: 100, // Optimal for <= 10 blocks per second
```

**File:** state-sync/storage-service/server/src/storage.rs (L178-192)
```rust
    /// Returns the transaction range held in the database (lowest to highest).
    fn fetch_transaction_range(
        &self,
        latest_version: Version,
    ) -> aptos_storage_service_types::Result<Option<CompleteDataRange<Version>>, Error> {
        let first_transaction_version = self.storage.get_first_txn_version()?;
        if let Some(first_transaction_version) = first_transaction_version {
            let transaction_range =
                CompleteDataRange::new(first_transaction_version, latest_version)
                    .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
            Ok(Some(transaction_range))
        } else {
            Ok(None)
        }
    }
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L280-280)
```rust
            self.error_if_ledger_pruned("Transaction", start_version)?;
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L329-333)
```rust
    fn get_first_txn_version(&self) -> Result<Option<Version>> {
        gauged_api("get_first_txn_version", || {
            Ok(Some(self.ledger_pruner.get_min_readable_version()))
        })
    }
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L261-271)
```rust
    pub(super) fn error_if_ledger_pruned(&self, data_type: &str, version: Version) -> Result<()> {
        let min_readable_version = self.ledger_pruner.get_min_readable_version();
        ensure!(
            version >= min_readable_version,
            "{} at version {} is pruned, min available version is {}.",
            data_type,
            version,
            min_readable_version
        );
        Ok(())
    }
```

**File:** state-sync/storage-service/server/src/handler.rs (L420-422)
```rust
            DataRequest::GetTransactionsWithProof(request) => {
                self.get_transactions_with_proof(request)
            },
```
