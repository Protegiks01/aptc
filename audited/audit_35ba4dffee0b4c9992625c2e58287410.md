# Audit Report

## Title
Data Loss in Indexer Backfiller Due to Unflushed Transaction Buffer on Scope Exit

## Summary
The indexer backfiller's `backfill()` function contains a critical data loss vulnerability where buffered transactions are silently dropped when a spawned task completes without flushing its buffer. When the fullnode returns fewer transactions than requested, buffered transactions that don't trigger the flush conditions are permanently lost, and the progress file is updated to skip them, breaking the indexer's data completeness guarantee.

## Finding Description

The vulnerability exists in the transaction processing flow within `tokio_scoped::scope` at line 127-209. [1](#0-0) 

The critical flaw involves three interacting components:

**1. Buffer Flushing Conditions**: The `FileStoreOperatorV2` only flushes buffered transactions when either (a) buffer size exceeds 50 MB or (b) `end_batch` is true. [2](#0-1) 

The `end_batch` flag is calculated as `(transaction.version + 1) % self.num_txns_per_folder == 0`, meaning it only triggers when transaction version boundaries align with folder boundaries. [3](#0-2) 

**2. No Drop Implementation**: The `FileStoreOperatorV2` struct lacks a `Drop` implementation to flush remaining buffered transactions when dropped, leading to silent data loss.

**3. Progress Update Assumption**: After the scope exits, the progress file is unconditionally updated assuming all transactions were successfully processed. [4](#0-3) 

**Attack Scenario**:

When the fullnode GRPC service returns fewer transactions than requested (which happens when `highest_known_version` is less than the requested range), the spawned task processes only the available transactions. [5](#0-4) 

Example:
1. Backfiller requests 1000 transactions starting at version 5000
2. Fullnode only has transactions up to version 5399 (e.g., still syncing)
3. Task receives and buffers 400 transactions (versions 5000-5399)
4. Last transaction version 5399: `(5399 + 1) % 1000 = 400 â‰  0`, so `end_batch = false`
5. If buffer size < 50 MB (typical for 400 transactions), buffer doesn't flush
6. Task completes, dropping `file_store_operator` with 400 buffered transactions
7. Progress file updated to version 6000, permanently skipping versions 5400-5999
8. Lost transactions are never recovered in subsequent runs

The task provides misleading logging claiming success for the full range. [6](#0-5) 

## Impact Explanation

This vulnerability causes **permanent data loss** in the indexer infrastructure, classified as **HIGH severity** under the Aptos bug bounty program criteria for "significant protocol violations" and "API crashes/data inconsistencies."

**Impact Details**:
- **Data Integrity**: The indexer's fundamental guarantee of complete transaction history is violated
- **Downstream Services**: Wallets, explorers, analytics platforms, and DeFi protocols relying on complete indexer data will have gaps
- **Silent Failure**: No error is raised; data loss is only discoverable through manual verification
- **Permanent Loss**: Progress file update prevents automatic recovery; manual intervention required

While this doesn't affect consensus or validator operations (not CRITICAL), it significantly impacts the ecosystem's data infrastructure that many services depend on for accurate blockchain state queries.

## Likelihood Explanation

**Likelihood: MEDIUM to HIGH**

This vulnerability can be triggered in several realistic operational scenarios:

1. **Fullnode Syncing**: If the backfiller runs against a fullnode that hasn't fully synced historical data
2. **Network Interruptions**: GRPC stream termination before completing the batch
3. **Fullnode Restarts**: Service interruptions during backfill operations
4. **Race Conditions**: Transient inconsistencies in fullnode's `highest_known_version` 

The issue is **not** directly exploitable by malicious actors but will occur naturally in production environments with imperfect network conditions or during operational procedures like fullnode maintenance.

## Recommendation

**Immediate Fix**: Add explicit buffer flushing before task completion:

```rust
s.spawn(async move {
    // ... existing transaction processing ...
    
    // CRITICAL: Flush any remaining buffered transactions before task exits
    if !file_store_operator.buffer.is_empty() {
        file_store_operator.dump_transactions_to_file(false, tx.clone())
            .await
            .expect("Failed to flush remaining transactions");
    }
    
    info!("Backfilling versions [{task_version}, {}) is finished.", 
          task_version + num_transactions_per_folder);
});
```

**Comprehensive Fix**: Implement validation and proper cleanup:

1. **Add Drop Implementation** for `FileStoreOperatorV2` to panic if buffer is non-empty when dropped (fail-fast rather than silent data loss)

2. **Validate Transaction Count**: After stream completion, verify the expected number of transactions was received:

```rust
let expected_count = num_transactions_per_folder;
let actual_count = file_store_operator.version() - task_version;
if actual_count != expected_count {
    panic!("Expected {} transactions, received only {}. Aborting to prevent data loss.",
           expected_count, actual_count);
}
```

3. **Add Explicit Flush Method**: Create a public `flush()` method on `FileStoreOperatorV2` and call it before task completion

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_data_loss_on_incomplete_batch() {
        // Setup: Create a file store operator expecting 1000 transactions
        let mut operator = FileStoreOperatorV2::new(
            50 * (1 << 20), // 50 MB max per file
            1000,           // num_transactions_per_folder
            5000,           // starting version
            BatchMetadata::default(),
        );
        
        let (tx, mut rx) = tokio::sync::mpsc::channel(10);
        
        // Simulate receiving only 400 transactions (incomplete batch)
        for version in 5000..5400 {
            let transaction = create_test_transaction(version);
            operator.buffer_and_maybe_dump_transactions_to_file(
                transaction,
                tx.clone(),
            ).await.unwrap();
        }
        
        // Check buffer state before drop
        assert!(!operator.buffer.is_empty(), "Buffer should contain unflushed transactions");
        
        // Simulate task completion - operator is dropped
        drop(operator);
        drop(tx);
        
        // Verify no data was sent to receiver for buffered transactions
        let received_count = count_received_transactions(&mut rx).await;
        
        // VULNERABILITY: Buffered transactions (some portion of 400) are lost
        // Expected: 400 transactions
        // Actual: < 400 (only transactions that triggered flushes)
        assert!(received_count < 400, 
                "Data loss occurred: {} transactions lost", 
                400 - received_count);
    }
    
    async fn count_received_transactions(
        rx: &mut tokio::sync::mpsc::Receiver<(Vec<Transaction>, BatchMetadata, bool)>
    ) -> usize {
        let mut count = 0;
        while let Some((txns, _, _)) = rx.recv().await {
            count += txns.len();
        }
        count
    }
}
```

## Notes

This vulnerability is particularly insidious because:

1. **Silent Failure**: The system logs success messages and continues operation without indicating data loss
2. **Cascading Effect**: Once progress file is updated, the gap is permanent unless manually detected and corrected
3. **Operational Trigger**: Occurs during normal operations (network issues, maintenance) rather than requiring attack
4. **Scope Guarantee Misleading**: While `tokio_scoped::scope` guarantees spawned tasks complete, it doesn't guarantee all buffered data is persisted

The issue highlights a critical gap between task completion and data persistence guarantees in async Rust systems using buffered I/O patterns.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-v2-file-store-backfiller/src/processor.rs (L127-209)
```rust
            tokio_scoped::scope(|s| {
                for _ in 0..self.backfill_processing_task_count {
                    let task_version = version;
                    if task_version >= self.ending_version {
                        break;
                    }
                    let mut file_store_operator = FileStoreOperatorV2::new(
                        MAX_SIZE_PER_FILE,
                        self.num_transactions_per_folder,
                        version,
                        BatchMetadata::default(),
                    );

                    info!(
                        "Backfilling versions [{task_version}, {}).",
                        task_version + self.num_transactions_per_folder
                    );

                    let chain_id = self.chain_id as u32;
                    let num_transactions_per_folder = self.num_transactions_per_folder;
                    let fullnode_grpc_address = self.fullnode_grpc_address.clone();

                    let (tx, mut rx) = tokio::sync::mpsc::channel(10);

                    s.spawn(async move {
                        while let Some((transactions, batch_metadata, end_batch)) = rx.recv().await
                        {
                            self.do_upload(transactions, batch_metadata, end_batch)
                                .await
                                .unwrap();
                        }
                    });

                    s.spawn(async move {
                        // Create a grpc client to the fullnode.
                        let mut grpc_client = create_grpc_client(fullnode_grpc_address).await;
                        let request = tonic::Request::new(GetTransactionsFromNodeRequest {
                            starting_version: Some(task_version),
                            transactions_count: Some(num_transactions_per_folder),
                        });
                        let mut stream = grpc_client
                            .get_transactions_from_node(request)
                            .await
                            .unwrap()
                            .into_inner();

                        while let Some(response_item) = stream.next().await {
                            match response_item {
                                Ok(r) => {
                                    assert!(r.chain_id == chain_id);
                                    match r.response.unwrap() {
                                        Response::Data(data) => {
                                            let transactions = data.transactions;
                                            for transaction in transactions {
                                                file_store_operator
                                                    .buffer_and_maybe_dump_transactions_to_file(
                                                        transaction,
                                                        tx.clone(),
                                                    )
                                                    .await
                                                    .unwrap();
                                            }
                                        },
                                        Response::Status(_) => {
                                            continue;
                                        },
                                    }
                                },
                                Err(e) => {
                                    panic!("Error when getting transactions from fullnode: {e}.")
                                },
                            }
                        }

                        info!(
                            "Backfilling versions [{task_version}, {}) is finished.",
                            task_version + num_transactions_per_folder
                        );
                    });

                    version += self.num_transactions_per_folder;
                }
            });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-v2-file-store-backfiller/src/processor.rs (L211-220)
```rust
            // Update the progress file.
            let progress_file = ProgressFile {
                version,
                backfill_id: self.backfill_id,
            };
            let bytes =
                serde_json::to_vec(&progress_file).context("Failed to serialize progress file.")?;
            std::fs::write(&self.progress_file_path, &bytes)
                .context("Failed to write progress file.")?;
            info!("Progress file updated to version {}.", version,);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_operator.rs (L48-48)
```rust
        let end_batch = (transaction.version + 1) % self.num_txns_per_folder == 0;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_operator.rs (L59-61)
```rust
        if self.buffer_size_in_bytes >= self.max_size_per_file || end_batch {
            self.dump_transactions_to_file(end_batch, tx).await?;
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L301-301)
```rust
        let end_version = std::cmp::min(self.end_version, self.highest_known_version + 1);
```
