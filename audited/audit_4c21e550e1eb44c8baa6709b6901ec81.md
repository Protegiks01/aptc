# Audit Report

## Title
Metadata-Shard Desynchronization in StateKvPruner Causing Permanent Node Failure and State Inconsistency

## Summary
The `StateKvPruner` performs non-atomic pruning operations across multiple independent RocksDB databases. When metadata pruning succeeds but shard pruning fails, metadata progress advances beyond shard progress, creating permanent desynchronization that prevents nodes from restarting and violates state consistency invariants.

## Finding Description

The `StateKvPruner::prune()` method executes pruning in two sequential, non-atomic phases across separate RocksDB instances:

**Phase 1 (Metadata Pruning)**: The metadata pruner commits its progress marker to the `state_kv_metadata_db` database. [1](#0-0) 

This metadata pruner writes progress directly to the metadata database: [2](#0-1) 

**Phase 2 (Shard Pruning)**: Each of 16 shard pruners processes data in parallel and commits progress to its respective `state_kv_db_shards[i]` database. [3](#0-2) 

Each shard pruner writes progress to its own shard database: [4](#0-3) 

**Critical Flaw**: These databases are separate `Arc<DB>` instances with no atomic coordination. [5](#0-4) 

If Phase 1 succeeds but Phase 2 fails for any shard (disk I/O error, out-of-space, corruption, crash), the metadata database records progress to version X while shard databases remain at earlier versions.

**On Node Restart**: The initialization logic reads metadata progress and creates shard pruners that attempt to catch up to the metadata progress. [6](#0-5) 

Each shard pruner initialization includes a catch-up prune operation: [7](#0-6) 

If the underlying issue persists (corrupted data, disk error), the catch-up prune fails and the error propagates, causing initialization to fail. The node enters a permanent crash loop until manual database intervention.

**Invariant Violation**: This breaks state consistency guarantees. The metadata claims data up to version X is pruned, but shard databases contain data from earlier versions, creating an inconsistent view of pruned state across the storage system.

## Impact Explanation

**High to Critical Severity** - This vulnerability causes:

1. **Validator Node Unavailability (High)**: Individual nodes experiencing disk failures during pruning enter permanent crash loops and cannot restart without manual database intervention. This results in validator downtime and requires operational intervention.

2. **Total Loss of Liveness/Network Availability (Critical - Worst Case)**: If multiple validators experience this simultaneously (e.g., during network-wide infrastructure issues affecting disk I/O), the network loses multiple consensus participants. If >1/3 of validators are affected, consensus cannot progress, resulting in network halt.

3. **Non-Recoverable State Inconsistency**: The metadata-shard desynchronization creates persistent storage invariant violations where the system metadata claims a different pruning state than the actual shard data, requiring manual intervention to resolve.

Per Aptos bug bounty criteria, individual node failures qualify as **High Severity** ("Validator Node Slowdowns/Crashes"), while simultaneous multi-validator impact qualifies as **Critical Severity** ("Total loss of liveness/network availability").

## Likelihood Explanation

**Medium to High Likelihood** - This vulnerability can be triggered by common operational failures:

- Disk I/O errors during pruning operations
- Out-of-disk-space conditions affecting one shard before others
- Filesystem corruption in a single shard database
- System crashes between Phase 1 and Phase 2 completion
- Hardware failures affecting specific disk partitions

The pruning operation runs periodically in production, and the 16-shard architecture creates 16 independent failure points. Individual node impact has medium-to-high likelihood based on operational failure rates. Network-wide impact has lower likelihood but is realistic during systemic infrastructure issues (common cloud provider problems, shared storage failures, etc.).

## Recommendation

Implement atomic progress tracking across metadata and shard databases using one of these approaches:

**Option 1: Two-Phase Progress Commit**
- Record progress only after ALL shards successfully complete pruning
- Only update in-memory progress tracking after all database writes succeed
- On failure, none of the databases have advanced progress markers

**Option 2: Progress Verification on Restart**
- During initialization, verify all shard progress matches metadata progress
- If desynchronization detected, reset metadata progress to minimum shard progress
- Log warning and allow initialization to continue with corrected progress

**Option 3: Transactional Coordinator**
- Implement a distributed transaction coordinator that ensures all-or-nothing semantics
- Use write-ahead logging to track partial completions
- Rollback metadata progress if any shard fails

The simplest fix is Option 1 - defer progress recording until after all shards complete:

```rust
// In StateKvPruner::prune() - defer progress update
self.metadata_pruner.prune_data(progress, current_batch_target_version)?;

THREAD_MANAGER.get_background_pool().install(|| {
    self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
        shard_pruner.prune_data(progress, current_batch_target_version)
            .map_err(|err| anyhow!("Failed to prune shard {}: {err}", shard_pruner.shard_id()))
    })
})?;

// Only NOW commit progress to all databases atomically
self.metadata_pruner.commit_progress(current_batch_target_version)?;
for shard_pruner in &self.shard_pruners {
    shard_pruner.commit_progress(current_batch_target_version)?;
}

progress = current_batch_target_version;
self.record_progress(progress);
```

## Proof of Concept

This vulnerability requires simulating disk failures during pruning operations. A full PoC would involve:

1. Starting a validator node with state pruning enabled
2. Triggering pruning operations
3. Injecting a disk I/O error in one shard database after metadata pruning succeeds
4. Observing the node crash
5. Attempting to restart the node
6. Observing permanent initialization failure due to catch-up prune failing

The technical vulnerability is confirmed through code analysis showing:
- Separate database instances without atomic coordination [8](#0-7) 
- Independent write operations to metadata and shard databases [9](#0-8) [10](#0-9) 
- Catch-up logic that propagates failures during initialization [11](#0-10) 

## Notes

While the vulnerability is technically valid, the severity depends on the failure scenario:
- Single node failure: **High Severity** - requires operational intervention
- Multi-validator failure: **Critical Severity** - potential network availability impact

The consensus safety risk is overstated in the original claim. Pruning affects historical data already committed to the chain. Different pruning states don't cause consensus divergence on new blocks, though they may affect state synchronization for new nodes joining the network.

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L64-65)
```rust
            self.metadata_pruner
                .prune(progress, current_batch_target_version)?;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L67-78)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state kv shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L117-133)
```rust
        let metadata_progress = metadata_pruner.progress()?;

        info!(
            metadata_progress = metadata_progress,
            "Created state kv metadata pruner, start catching up all shards."
        );

        let shard_pruners = if state_kv_db.enabled_sharding() {
            let num_shards = state_kv_db.num_shards();
            let mut shard_pruners = Vec::with_capacity(num_shards);
            for shard_id in 0..num_shards {
                shard_pruners.push(StateKvShardPruner::new(
                    shard_id,
                    state_kv_db.db_shard_arc(shard_id),
                    metadata_progress,
                )?);
            }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs (L67-72)
```rust
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        self.state_kv_db.metadata_db().write_schemas(batch)
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L30-44)
```rust
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
            metadata_progress,
        )?;
        let myself = Self { shard_id, db_shard };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up state kv shard {shard_id}."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L66-71)
```rust
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
            &DbMetadataValue::Version(target_version),
        )?;

        self.db_shard.write_schemas(batch)
```

**File:** storage/aptosdb/src/state_kv_db.rs (L44-51)
```rust
pub struct StateKvDb {
    state_kv_metadata_db: Arc<DB>,
    state_kv_db_shards: [Arc<DB>; NUM_STATE_SHARDS],
    // TODO(HotState): no separate metadata db for hot state for now.
    #[allow(dead_code)] // TODO(HotState): can remove later.
    hot_state_kv_db_shards: Option<[Arc<DB>; NUM_STATE_SHARDS]>,
    enabled_sharding: bool,
}
```

**File:** storage/aptosdb/src/state_kv_db.rs (L92-125)
```rust
        let state_kv_metadata_db = Arc::new(Self::open_db(
            state_kv_metadata_db_path.clone(),
            STATE_KV_METADATA_DB_NAME,
            &state_kv_db_config,
            env,
            block_cache,
            readonly,
            /* is_hot = */ false,
        )?);

        info!(
            state_kv_metadata_db_path = state_kv_metadata_db_path,
            "Opened state kv metadata db!"
        );

        let state_kv_db_shards = (0..NUM_STATE_SHARDS)
            .into_par_iter()
            .map(|shard_id| {
                let shard_root_path = db_paths.state_kv_db_shard_root_path(shard_id);
                let db = Self::open_shard(
                    shard_root_path,
                    shard_id,
                    &state_kv_db_config,
                    env,
                    block_cache,
                    readonly,
                    /* is_hot = */ false,
                )
                .unwrap_or_else(|e| panic!("Failed to open state kv db shard {shard_id}: {e:?}."));
                Arc::new(db)
            })
            .collect::<Vec<_>>()
            .try_into()
            .unwrap();
```
