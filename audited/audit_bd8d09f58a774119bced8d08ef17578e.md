# Audit Report

## Title
Storage Service Blocking Thread Pool Exhaustion via Concurrent Expensive State Proof Requests

## Summary
A malicious peer can exhaust the storage service's limited blocking thread pool (64 threads) by sending concurrent requests for expensive state proofs, causing denial-of-service for legitimate state synchronization requests. The attack leverages a mismatch between the network layer's per-peer RPC concurrency limit (100) and the storage service's blocking thread pool capacity (64), with no per-peer limit on blocking task allocation.

## Finding Description

The storage service processes all incoming requests by spawning blocking tasks on a fixed-size thread pool. [1](#0-0) 

The blocking thread pool is configured with only 64 threads: [2](#0-1) 

However, the network layer allows each peer connection to send up to 100 concurrent inbound RPCs: [3](#0-2) 

The network layer enforces this per-connection limit: [4](#0-3) 

**Attack Path:**

1. A malicious peer sends 100 concurrent `GetStateValuesWithProof` RPC requests (within network layer limits)
2. Each request specifies maximum chunk size (4000 state values): [5](#0-4) 
3. Request validation only checks data availability, not request cost: [6](#0-5) 
4. Each request spawns a blocking task that performs expensive operations:
   - Iterating over 4000 state values via database reads: [7](#0-6) 
   - Generating Merkle range proof requiring tree traversal: [8](#0-7) 
5. The first 64 requests occupy all blocking threads
6. Remaining 36 requests (plus any legitimate requests) queue in the network channel (max 4000 capacity): [9](#0-8) 
7. Each blocking task processes for up to 10 seconds before timeout: [10](#0-9) 
8. By varying `start_index` values, attacker bypasses LRU cache (500 entries): [11](#0-10) 
9. Attacker continuously sends new requests to maintain thread pool saturation
10. Legitimate state sync requests from other nodes experience severe delays or are dropped

**Attack Amplification:**
With up to 100 inbound connections from unknown peers allowed: [12](#0-11) 

Multiple malicious peers can amplify the attack to fill the network channel (4000 capacity) with malicious requests while saturating all 64 blocking threads.

## Impact Explanation

This vulnerability qualifies as **HIGH severity** under Aptos bug bounty criteria:

1. **Validator node slowdowns**: Storage service becomes unresponsive, severely impacting state synchronization operations
2. **API crashes**: Storage service effectively unavailable due to thread pool exhaustion
3. **Significant protocol violations**: Violates the "Resource Limits" invariant - operations do not properly respect computational limits

**Specific Impacts:**
- New full nodes cannot synchronize state from affected validators
- Existing nodes cannot catch up after being offline
- State snapshot serving becomes unavailable
- Network health degrades as fewer nodes can successfully sync
- Does NOT affect consensus safety (storage service runs on separate runtime): [13](#0-12) 

## Likelihood Explanation

**Likelihood: HIGH**

- **Attack Complexity**: Very low - attacker only needs to establish a peer connection and send RPC requests
- **Attacker Prerequisites**: None - any peer can connect and send requests
- **Detection Difficulty**: Malicious requests appear valid (correct version ranges, within protocol limits)
- **Cost to Attacker**: Minimal - just network bandwidth
- **Proof of Feasibility**: The code paths are clearly exploitable with straightforward concurrent requests

## Recommendation

Implement per-peer limits on blocking task allocation:

```rust
// Add to StorageServiceServer struct
pub struct StorageServiceServer<T> {
    // ... existing fields ...
    
    // Track concurrent blocking tasks per peer
    peer_blocking_tasks: Arc<DashMap<PeerNetworkId, AtomicUsize>>,
    max_blocking_tasks_per_peer: usize,
}

// In the request handling loop
pub async fn start(mut self) {
    // ... existing code ...
    
    while let Some(network_request) = self.network_requests.next().await {
        let peer_id = network_request.peer_network_id;
        
        // Check per-peer blocking task limit
        let current_tasks = self.peer_blocking_tasks
            .entry(peer_id)
            .or_insert_with(|| AtomicUsize::new(0))
            .fetch_add(1, Ordering::SeqCst);
            
        if current_tasks >= self.max_blocking_tasks_per_peer {
            // Reject request - peer has too many concurrent tasks
            warn!("Rejected request from {:?}: too many concurrent blocking tasks", peer_id);
            self.peer_blocking_tasks.get(&peer_id).unwrap().fetch_sub(1, Ordering::SeqCst);
            continue;
        }
        
        let peer_blocking_tasks = self.peer_blocking_tasks.clone();
        self.runtime.spawn_blocking(move || {
            // ... existing handler code ...
            
            // Decrement counter when task completes
            peer_blocking_tasks.get(&peer_id).unwrap().fetch_sub(1, Ordering::SeqCst);
        });
    }
}
```

**Additional Mitigations:**
1. Set `max_blocking_tasks_per_peer` to a reasonable limit (e.g., 10-16 per peer)
2. Implement rate limiting based on request processing cost, not just count
3. Add circuit breaker to temporarily ban peers sending excessive expensive requests
4. Consider separate thread pools for different request types (cheap vs expensive)

## Proof of Concept

```rust
// Rust integration test demonstrating the vulnerability
#[tokio::test]
async fn test_storage_service_thread_pool_exhaustion() {
    // Setup: Create storage service with real configuration
    let (storage_service, network_tx) = setup_storage_service_test();
    
    // Attack: Send 100 concurrent expensive state proof requests from single peer
    let malicious_peer_id = PeerNetworkId::random();
    let mut request_handles = vec![];
    
    for i in 0..100 {
        let request = StorageServiceRequest::new(
            DataRequest::GetStateValuesWithProof(
                StateValuesWithProofRequest {
                    version: 1000000, // Latest version
                    start_index: i * 4000, // Vary to bypass cache
                    end_index: (i * 4000) + 3999, // Max chunk size
                }
            ),
            false, // No compression
        );
        
        let (response_tx, response_rx) = oneshot::channel();
        let network_request = NetworkRequest {
            peer_network_id: malicious_peer_id,
            protocol_id: ProtocolId::StorageServiceRpc,
            storage_service_request: request,
            response_sender: ResponseSender::new(response_tx),
        };
        
        // Send request (all 100 will be accepted by network layer)
        network_tx.send(network_request).await.unwrap();
        request_handles.push(response_rx);
    }
    
    // Legitimate request from different peer should be delayed/blocked
    let legitimate_peer_id = PeerNetworkId::random();
    let (response_tx, legitimate_response_rx) = oneshot::channel();
    
    let legitimate_request = NetworkRequest {
        peer_network_id: legitimate_peer_id,
        protocol_id: ProtocolId::StorageServiceRpc,
        storage_service_request: StorageServiceRequest::new(
            DataRequest::GetStateValuesWithProof(
                StateValuesWithProofRequest {
                    version: 1000000,
                    start_index: 0,
                    end_index: 100, // Small request
                }
            ),
            false,
        ),
        response_sender: ResponseSender::new(response_tx),
    };
    
    network_tx.send(legitimate_request).await.unwrap();
    
    // Verify: Legitimate request times out or experiences severe delay
    // because all 64 blocking threads are occupied by malicious peer's requests
    let result = timeout(Duration::from_secs(5), legitimate_response_rx).await;
    assert!(result.is_err(), "Legitimate request should timeout due to thread pool exhaustion");
    
    // Observe: Metrics showing 64 blocking tasks active, all from malicious peer
    // Network channel filling with queued requests
}
```

**Notes:**
- This vulnerability breaks the "Resource Limits" invariant
- The storage service becomes effectively unavailable for legitimate state synchronization
- Attack requires no special privileges and is trivial to execute
- Multiple malicious peers can amplify the impact significantly

### Citations

**File:** state-sync/storage-service/server/src/lib.rs (L388-419)
```rust
        // Handle the storage requests as they arrive
        while let Some(network_request) = self.network_requests.next().await {
            // All handler methods are currently CPU-bound and synchronous
            // I/O-bound, so we want to spawn on the blocking thread pool to
            // avoid starving other async tasks on the same runtime.
            let storage = self.storage.clone();
            let config = self.storage_service_config;
            let cached_storage_server_summary = self.cached_storage_server_summary.clone();
            let optimistic_fetches = self.optimistic_fetches.clone();
            let subscriptions = self.subscriptions.clone();
            let lru_response_cache = self.lru_response_cache.clone();
            let request_moderator = self.request_moderator.clone();
            let time_service = self.time_service.clone();
            self.runtime.spawn_blocking(move || {
                Handler::new(
                    cached_storage_server_summary,
                    optimistic_fetches,
                    lru_response_cache,
                    request_moderator,
                    storage,
                    subscriptions,
                    time_service,
                )
                .process_request_and_respond(
                    config,
                    network_request.peer_network_id,
                    network_request.protocol_id,
                    network_request.storage_service_request,
                    network_request.response_sender,
                );
            });
        }
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-27)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;
```

**File:** network/framework/src/constants.rs (L15-15)
```rust
pub const MAX_CONCURRENT_INBOUND_RPCS: u32 = 100;
```

**File:** network/framework/src/protocols/rpc/mod.rs (L212-223)
```rust
        // Drop new inbound requests if our completion queue is at capacity.
        if self.inbound_rpc_tasks.len() as u32 == self.max_concurrent_inbound_rpcs {
            // Increase counter of declined requests
            counters::rpc_messages(
                network_context,
                REQUEST_LABEL,
                INBOUND_LABEL,
                DECLINED_LABEL,
            )
            .inc();
            return Err(RpcError::TooManyPending(self.max_concurrent_inbound_rpcs));
        }
```

**File:** config/src/config/state_sync_config.rs (L25-25)
```rust
const MAX_STATE_CHUNK_SIZE: u64 = 4000;
```

**File:** config/src/config/state_sync_config.rs (L202-202)
```rust
            max_lru_cache_size: 500, // At ~0.6MiB per chunk, this should take no more than 0.5GiB
```

**File:** config/src/config/state_sync_config.rs (L203-203)
```rust
            max_network_channel_size: 4000,
```

**File:** config/src/config/state_sync_config.rs (L209-209)
```rust
            max_storage_read_wait_time_ms: 10_000, // 10 seconds
```

**File:** state-sync/storage-service/types/src/responses.rs (L727-742)
```rust
            GetStateValuesWithProof(request) => {
                let proof_version = request.version;

                let can_serve_states = self
                    .states
                    .map(|range| range.contains(request.version))
                    .unwrap_or(false);

                let can_create_proof = self
                    .synced_ledger_info
                    .as_ref()
                    .map(|li| li.ledger_info().version() >= proof_version)
                    .unwrap_or(false);

                can_serve_states && can_create_proof
            },
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1095-1115)
```rust
    pub fn get_value_chunk_iter(
        self: &Arc<Self>,
        version: Version,
        first_index: usize,
        chunk_size: usize,
    ) -> Result<impl Iterator<Item = Result<(StateKey, StateValue)>> + Send + Sync + use<>> {
        let store = Arc::clone(self);
        let value_chunk_iter = JellyfishMerkleIterator::new_by_index(
            Arc::clone(&self.state_merkle_db),
            version,
            first_index,
        )?
        .take(chunk_size)
        .map(move |res| {
            res.and_then(|(_, (key, version))| {
                Ok((key.clone(), store.expect_value_by_version(&key, version)?))
            })
        });

        Ok(value_chunk_iter)
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1117-1143)
```rust
    pub fn get_value_chunk_proof(
        self: &Arc<Self>,
        version: Version,
        first_index: usize,
        state_key_values: Vec<(StateKey, StateValue)>,
    ) -> Result<StateValueChunkWithProof> {
        ensure!(
            !state_key_values.is_empty(),
            "State chunk starting at {}",
            first_index,
        );
        let last_index = (state_key_values.len() - 1 + first_index) as u64;
        let first_key = state_key_values.first().expect("checked to exist").0.hash();
        let last_key = state_key_values.last().expect("checked to exist").0.hash();
        let proof = self.get_value_range_proof(last_key, version)?;
        let root_hash = self.get_root_hash(version)?;

        Ok(StateValueChunkWithProof {
            first_index: first_index as u64,
            last_index,
            first_key,
            last_key,
            raw_values: state_key_values,
            proof,
            root_hash,
        })
    }
```

**File:** config/src/config/network_config.rs (L13-13)
```rust
use aptos_secure_storage::{CryptoStorage, KVStorage, Storage};
```

**File:** aptos-node/src/state_sync.rs (L274-274)
```rust
    let storage_service_runtime = aptos_runtimes::spawn_named_runtime("stor-server".into(), None);
```
