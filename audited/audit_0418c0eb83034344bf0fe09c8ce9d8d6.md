# Audit Report

## Title
State Sync Stream Cleanup Failure Causes Persistent Node Degradation

## Summary
When `handle_storage_synchronizer_error()` fails during stream termination in the state sync driver, the error is only logged but critical cleanup operations are skipped. This leaves the node in an inconsistent state with stale stream references that cannot be cleared, preventing the creation of new streams and causing persistent sync failures requiring manual node restart.

## Finding Description

The vulnerability exists in the error-within-error handling path of the state sync driver. When the storage synchronizer encounters an error and sends an `ErrorNotification`, the driver attempts to clean up by terminating active data streams: [1](#0-0) 

The critical flaw is in how both `continuous_syncer.handle_storage_synchronizer_error()` and `bootstrapper.handle_storage_synchronizer_error()` handle failures. Both methods call `reset_active_stream()` which performs stream termination: [2](#0-1) [3](#0-2) 

The `reset_active_stream()` method has a fatal design flaw where the `?` operator causes early return if `terminate_stream_with_feedback()` fails, preventing the cleanup of `active_data_stream` and `speculative_stream_state`: [4](#0-3) 

The stream termination can fail for several reasons:
- Streaming service channel is closed (SendError)
- Stream already terminated on server side  
- Stream ID not found on server
- Notification ID mismatch [5](#0-4) 

Once the cleanup fails:
1. The `active_data_stream` field remains non-None with a stale reference
2. The `speculative_stream_state` retains invalid state
3. On subsequent `drive_progress()` calls, the check `if self.active_data_stream.is_some()` succeeds [6](#0-5) 

This prevents initialization of new streams, leaving the node stuck. Attempts to fetch notifications from the stale stream will repeatedly timeout, triggering more failed cleanup attempts and creating a cascading failure loop: [7](#0-6) 

## Impact Explanation

This vulnerability falls under **Medium Severity** per the Aptos bug bounty criteria as it causes "State inconsistencies requiring intervention." Specifically:

- **Node Liveness Degradation**: The affected node cannot synchronize state, falling behind the network
- **Availability Impact**: The node becomes unable to serve current state queries or participate in validation
- **Manual Intervention Required**: Recovery requires node restart, as there is no automatic recovery mechanism
- **Does NOT Cause**: Direct fund loss, consensus safety violations, or network-wide disruption

The impact is limited to individual nodes experiencing the error condition, not systemic network failure. However, if multiple nodes encounter this simultaneously (e.g., from malicious peer sending invalid data to many nodes), it could degrade network capacity.

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability can be triggered through multiple realistic scenarios:

1. **Malicious Peer Attack**: An attacker controlling a peer node can deliberately send invalid transaction data or proofs that cause storage synchronizer errors, then close connections to cause stream termination failures
   
2. **Network Instability**: Natural network disruptions causing streaming service channels to close during error recovery

3. **Server-Side State Mismatch**: Race conditions where the server terminates a stream just before the client attempts cleanup

4. **Concurrent Error Handling**: Multiple rapid errors on the same stream causing overlapping cleanup attempts

The vulnerability requires no special privileges - any network peer can trigger storage synchronizer errors by providing invalid data. The error handling path is exercised regularly in production environments, making exploitation highly feasible.

## Recommendation

**Fix 1: Force State Reset on Termination Failure**

Modify `reset_active_stream()` to always reset state, even if termination fails:

```rust
pub async fn reset_active_stream(
    &mut self,
    notification_and_feedback: Option<NotificationAndFeedback>,
) -> Result<(), Error> {
    let termination_result = if let Some(active_data_stream) = &self.active_data_stream {
        let data_stream_id = active_data_stream.data_stream_id;
        utils::terminate_stream_with_feedback(
            &mut self.streaming_client,
            data_stream_id,
            notification_and_feedback,
        )
        .await
    } else {
        Ok(())
    };

    // Always reset state, even if termination failed
    self.active_data_stream = None;
    self.speculative_stream_state = None;
    
    // Log termination failure but don't block cleanup
    if let Err(error) = termination_result {
        warn!(LogSchema::new(LogEntry::ContinuousSyncer)
            .message(&format!("Stream termination failed, but state was reset: {:?}", error)));
    }

    Ok(())
}
```

**Fix 2: Add Retry Logic with Exponential Backoff**

Implement retry mechanism for stream termination with eventual forced cleanup:

```rust
pub async fn reset_active_stream(
    &mut self,
    notification_and_feedback: Option<NotificationAndFeedback>,
) -> Result<(), Error> {
    const MAX_RETRIES: usize = 3;
    const RETRY_DELAY_MS: u64 = 100;
    
    if let Some(active_data_stream) = &self.active_data_stream {
        let data_stream_id = active_data_stream.data_stream_id;
        
        for attempt in 0..MAX_RETRIES {
            match utils::terminate_stream_with_feedback(
                &mut self.streaming_client,
                data_stream_id,
                notification_and_feedback.clone(),
            ).await {
                Ok(()) => break,
                Err(error) if attempt == MAX_RETRIES - 1 => {
                    // Force cleanup on final failure
                    warn!(LogSchema::new(LogEntry::ContinuousSyncer)
                        .message(&format!("Stream termination failed after {} attempts, forcing cleanup: {:?}", 
                                        MAX_RETRIES, error)));
                    break;
                },
                Err(_) => {
                    tokio::time::sleep(Duration::from_millis(RETRY_DELAY_MS * (1 << attempt))).await;
                }
            }
        }
    }

    self.active_data_stream = None;
    self.speculative_stream_state = None;
    Ok(())
}
```

Apply the same fix to `bootstrapper.rs` line 1539-1556.

**Fix 3: Propagate Critical Errors**

Modify `handle_error_notification()` to propagate termination failures that indicate unrecoverable state, triggering proper node recovery procedures rather than silently continuing with inconsistent state.

## Proof of Concept

```rust
#[cfg(test)]
mod test_stream_cleanup_failure {
    use super::*;
    use futures::channel::mpsc;
    
    // Mock streaming client that fails terminate operations
    struct FailingStreamingClient {
        terminate_should_fail: bool,
    }
    
    #[async_trait::async_trait]
    impl DataStreamingClient for FailingStreamingClient {
        async fn terminate_stream_with_feedback(
            &self,
            _data_stream_id: DataStreamId,
            _notification_and_feedback: Option<NotificationAndFeedback>,
        ) -> Result<(), Error> {
            if self.terminate_should_fail {
                Err(Error::UnexpectedErrorEncountered(
                    "Simulated stream termination failure".to_string()
                ))
            } else {
                Ok(())
            }
        }
        
        // Other trait methods omitted for brevity...
    }
    
    #[tokio::test]
    async fn test_cascading_stream_cleanup_failure() {
        // Setup continuous syncer with failing streaming client
        let failing_client = FailingStreamingClient { 
            terminate_should_fail: true 
        };
        
        let mut continuous_syncer = ContinuousSyncer::new(
            driver_config,
            failing_client,
            output_fallback_handler,
            storage,
            storage_synchronizer,
        );
        
        // Create an active stream
        continuous_syncer.active_data_stream = Some(create_mock_stream_listener());
        continuous_syncer.speculative_stream_state = Some(create_mock_speculative_state());
        
        // Simulate storage synchronizer error notification
        let notification_and_feedback = NotificationAndFeedback::new(
            123,
            NotificationFeedback::InvalidPayloadData,
        );
        
        // Attempt to handle the error - this should fail
        let result = continuous_syncer
            .handle_storage_synchronizer_error(notification_and_feedback)
            .await;
        
        assert!(result.is_err(), "Expected error from failed stream termination");
        
        // VULNERABILITY: active_data_stream should be None but isn't
        assert!(
            continuous_syncer.active_data_stream.is_some(),
            "VULNERABILITY: Stream reference not cleared after termination failure"
        );
        assert!(
            continuous_syncer.speculative_stream_state.is_some(),
            "VULNERABILITY: Speculative state not cleared after termination failure"
        );
        
        // Node is now stuck - cannot create new stream
        let progress_result = continuous_syncer
            .drive_progress(Arc::new(Mutex::new(None)))
            .await;
        
        // Will try to process from stale stream, causing cascading failures
        assert!(
            progress_result.is_err(),
            "Node stuck with stale stream, causing continued failures"
        );
    }
}
```

## Notes

This vulnerability demonstrates a critical gap in error recovery: when error handling itself fails, the system must still maintain consistent state. The current implementation prioritizes reporting stream termination failures over ensuring local state consistency, leading to unrecoverable degradation. The fix requires inverting this priority - always clean up local state, then handle termination errors separately. This ensures the node can always recover by creating fresh streams, even if server-side cleanup fails.

### Citations

**File:** state-sync/state-sync-driver/src/driver.rs (L494-533)
```rust
    /// Handles an error notification sent by the storage synchronizer
    async fn handle_error_notification(&mut self, error_notification: ErrorNotification) {
        warn!(LogSchema::new(LogEntry::SynchronizerNotification)
            .error_notification(error_notification.clone())
            .message("Received an error notification from the storage synchronizer!"));

        // Terminate the currently active streams
        let notification_id = error_notification.notification_id;
        let notification_feedback = NotificationFeedback::InvalidPayloadData;
        if self.bootstrapper.is_bootstrapped() {
            if let Err(error) = self
                .continuous_syncer
                .handle_storage_synchronizer_error(NotificationAndFeedback::new(
                    notification_id,
                    notification_feedback,
                ))
                .await
            {
                error!(LogSchema::new(LogEntry::SynchronizerNotification)
                    .message(&format!(
                        "Failed to terminate the active stream for the continuous syncer! Error: {:?}",
                        error
                    )));
            }
        } else if let Err(error) = self
            .bootstrapper
            .handle_storage_synchronizer_error(NotificationAndFeedback::new(
                notification_id,
                notification_feedback,
            ))
            .await
        {
            error!(
                LogSchema::new(LogEntry::SynchronizerNotification).message(&format!(
                    "Failed to terminate the active stream for the bootstrapper! Error: {:?}",
                    error
                ))
            );
        };
    }
```

**File:** state-sync/state-sync-driver/src/continuous_syncer.rs (L76-97)
```rust
    /// Checks if the continuous syncer is able to make progress
    pub async fn drive_progress(
        &mut self,
        consensus_sync_request: Arc<Mutex<Option<ConsensusSyncRequest>>>,
    ) -> Result<(), Error> {
        if self.active_data_stream.is_some() {
            // We have an active data stream. Process any notifications!
            self.process_active_stream_notifications(consensus_sync_request)
                .await
        } else if self.storage_synchronizer.pending_storage_data() {
            // Wait for any pending data to be processed
            sample!(
                SampleRate::Duration(Duration::from_secs(PENDING_DATA_LOG_FREQ_SECS)),
                info!("Waiting for the storage synchronizer to handle pending data!")
            );
            Ok(())
        } else {
            // Fetch a new data stream to start streaming data
            self.initialize_active_data_stream(consensus_sync_request)
                .await
        }
    }
```

**File:** state-sync/state-sync-driver/src/continuous_syncer.rs (L182-198)
```rust
    /// Attempts to fetch a data notification from the active stream
    async fn fetch_next_data_notification(&mut self) -> Result<DataNotification, Error> {
        let max_stream_wait_time_ms = self.driver_configuration.config.max_stream_wait_time_ms;
        let max_num_stream_timeouts = self.driver_configuration.config.max_num_stream_timeouts;
        let result = utils::get_data_notification(
            max_stream_wait_time_ms,
            max_num_stream_timeouts,
            self.active_data_stream.as_mut(),
        )
        .await;
        if matches!(result, Err(Error::CriticalDataStreamTimeout(_))) {
            // If the stream has timed out too many times, we need to reset it
            warn!("Resetting the currently active data stream due to too many timeouts!");
            self.reset_active_stream(None).await?;
        }
        result
    }
```

**File:** state-sync/state-sync-driver/src/continuous_syncer.rs (L500-522)
```rust
    /// Handles the storage synchronizer error sent by the driver
    pub async fn handle_storage_synchronizer_error(
        &mut self,
        notification_and_feedback: NotificationAndFeedback,
    ) -> Result<(), Error> {
        // Reset the active stream
        self.reset_active_stream(Some(notification_and_feedback))
            .await?;

        // Fallback to output syncing if we need to
        if let ContinuousSyncingMode::ExecuteTransactionsOrApplyOutputs =
            self.get_continuous_syncing_mode()
        {
            self.output_fallback_handler.fallback_to_outputs();
            metrics::set_gauge(
                &metrics::DRIVER_FALLBACK_MODE,
                ExecutingComponent::ContinuousSyncer.get_label(),
                1,
            );
        }

        Ok(())
    }
```

**File:** state-sync/state-sync-driver/src/continuous_syncer.rs (L524-542)
```rust
    /// Resets the currently active data stream and speculative state
    pub async fn reset_active_stream(
        &mut self,
        notification_and_feedback: Option<NotificationAndFeedback>,
    ) -> Result<(), Error> {
        if let Some(active_data_stream) = &self.active_data_stream {
            let data_stream_id = active_data_stream.data_stream_id;
            utils::terminate_stream_with_feedback(
                &mut self.streaming_client,
                data_stream_id,
                notification_and_feedback,
            )
            .await?;
        }

        self.active_data_stream = None;
        self.speculative_stream_state = None;
        Ok(())
    }
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L1516-1536)
```rust
    /// Handles the storage synchronizer error sent by the driver
    pub async fn handle_storage_synchronizer_error(
        &mut self,
        notification_and_feedback: NotificationAndFeedback,
    ) -> Result<(), Error> {
        // Reset the active stream
        self.reset_active_stream(Some(notification_and_feedback))
            .await?;

        // Fallback to output syncing if we need to
        if let BootstrappingMode::ExecuteOrApplyFromGenesis = self.get_bootstrapping_mode() {
            self.output_fallback_handler.fallback_to_outputs();
            metrics::set_gauge(
                &metrics::DRIVER_FALLBACK_MODE,
                ExecutingComponent::Bootstrapper.get_label(),
                1,
            );
        }

        Ok(())
    }
```

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L202-250)
```rust
    fn process_terminate_stream_request(
        &mut self,
        terminate_request: &TerminateStreamRequest,
    ) -> Result<(), Error> {
        // Grab the stream id and feedback
        let data_stream_id = &terminate_request.data_stream_id;
        let notification_and_feedback = &terminate_request.notification_and_feedback;

        // Increment the stream termination counter
        let feedback_label = match notification_and_feedback {
            Some(notification_and_feedback) => {
                notification_and_feedback.notification_feedback.get_label()
            },
            None => TERMINATE_NO_FEEDBACK,
        };
        metrics::increment_counter(&metrics::TERMINATE_DATA_STREAM, feedback_label);

        // Remove the data stream
        if let Some(data_stream) = self.data_streams.remove(data_stream_id) {
            info!(LogSchema::new(LogEntry::HandleTerminateRequest)
                .stream_id(*data_stream_id)
                .event(LogEvent::Success)
                .message(&format!(
                    "Terminating the data stream with ID: {:?}. Notification and feedback: {:?}",
                    data_stream_id, notification_and_feedback,
                )));

            // Handle any notification feedback
            if let Some(notification_and_feedback) = notification_and_feedback {
                let notification_id = &notification_and_feedback.notification_id;
                let feedback = &notification_and_feedback.notification_feedback;
                if data_stream.sent_notification(notification_id) {
                    data_stream.handle_notification_feedback(notification_id, feedback)?;
                    Ok(())
                } else {
                    Err(Error::UnexpectedErrorEncountered(format!(
                        "Data stream ID: {:?} did not appear to send notification ID: {:?}",
                        data_stream_id, notification_id,
                    )))
                }
            } else {
                Ok(())
            }
        } else {
            Err(Error::UnexpectedErrorEncountered(format!(
                "Unable to find data stream with ID: {:?}. Notification and feedback: {:?}",
                data_stream_id, notification_and_feedback,
            )))
        }
```
