# Audit Report

## Title
Non-Atomic Resource Group Write Causes Phantom Reads and Incorrect Dependency Tracking in BlockSTM v2

## Summary
The `write_v2()` function performs a non-atomic two-phase write operation where data values are written before acquiring the size lock. This creates a race window allowing concurrent readers to observe inconsistent snapshots (new data with old size), leading to corrupted read dependency tracking and potentially incorrect invalidation decisions that violate serializability guarantees.

## Finding Description

The vulnerability exists in the `write_v2()` function's non-atomic write sequence: [1](#0-0) 

The function first writes data values via `data_write_impl()`, then acquires the `group_sizes` lock to write the size. The explicit comment acknowledges this ordering: "We write data first, without holding the sizes lock, then write size."

During the race window between these operations, concurrent readers can observe an inconsistent state:

**Race Timeline:**
1. Transaction T5 calls `write_v2()` 
2. T5 executes `data_write_impl()` - data becomes visible
3. **RACE WINDOW OPENS**
4. Transaction T10 reads via `fetch_tagged_data_and_record_dependency()`: [2](#0-1) 

   - Observes T5's NEW data
   - Records dependency on T5's entry

5. T10 reads via `get_group_size_and_record_dependency()`: [3](#0-2) 

   - Observes OLD size (from transaction T3, before T5)
   - Records dependency on T3's size entry

6. **RACE WINDOW CLOSES**
7. T5 acquires `group_sizes` lock
8. T5 processes dependencies from old size entry: [4](#0-3) 

When processing dependencies, if the size value didn't change numerically (`size_entry.value.size == size`), T5 preserves T10's dependency in the new size entry, effectively marking T10's read as valid.

**The Critical Flaw:**

T10 has observed a **phantom read** - an inconsistent snapshot that never existed atomically:
- Data dependency: Recorded on T5 (index 5)  
- Size dependency: Recorded on T3 (index 3)

This violates the fundamental STM guarantee that transactions observe consistent snapshots. T10's execution is based on state `(data@T5, size@T3)` which doesn't correspond to any serial execution order.

The dependency recording in `versioned_data.rs` confirms dependencies are tracked per-entry: [5](#0-4) 

And size dependencies similarly: [6](#0-5) 

## Impact Explanation

This vulnerability qualifies as **Critical Severity** under the Aptos Bug Bounty program for the following reasons:

**1. Consensus Safety Violation**

Different validators executing the same block with different timing could make divergent invalidation decisions:
- Validator A: T10 reads during race window → dependency preserved → T10 commits
- Validator B: T10 reads after race closes → correct dependencies → different invalidation path

This breaks the **Deterministic Execution** invariant: "All validators must produce identical state roots for identical blocks."

**2. Serializability Violation**

BlockSTM guarantees serializable transaction execution. The phantom read allows transaction T10 to observe and commit based on a state that violates serializability - it's neither reading fully before T5 nor fully after T5.

**3. Incorrect Validation Logic**

The push-based validation in BlockSTM v2 relies on accurate dependency tracking to determine which transactions must be re-executed. Corrupted dependencies mean transactions that should be invalidated may be marked as valid, and vice versa.

**4. Non-Recoverable If Exploited at Scale**

If multiple transactions in a block exploit this race condition, different validators could produce fundamentally different execution results, potentially requiring a hardfork to resolve.

## Likelihood Explanation

**Likelihood: MEDIUM to HIGH**

The vulnerability triggers naturally during normal parallel execution without requiring malicious intent:

1. **No Special Privileges Required**: Any transaction writing to resource groups can trigger the race. Resource groups are commonly used in Aptos (e.g., for token standards).

2. **Parallel Execution is Default**: BlockSTM v2 is designed for maximum parallelism, meaning concurrent reads during writes are expected and frequent.

3. **Race Window Size**: The window between data write and size lock acquisition may be small, but given high transaction throughput and parallelism levels, the probability of concurrent reads during this window is non-negligible.

4. **Timing Variability Across Validators**: Different validators have different hardware, load, and network conditions, increasing the likelihood that timing-dependent behavior manifests differently across the network.

The explicit comment acknowledging the non-atomic write suggests this design choice was intentional for performance, but may not have fully considered the dependency tracking implications.

## Recommendation

**Solution: Make the write operation atomic by holding the size lock during data writes**

Modify `write_v2()` to acquire the `group_sizes` lock BEFORE calling `data_write_impl()`:

```rust
pub fn write_v2(
    &self,
    group_key: K,
    txn_idx: TxnIndex,
    incarnation: Incarnation,
    values: impl IntoIterator<Item = (T, (V, Option<Arc<MoveTypeLayout>>))>,
    size: ResourceGroupSize,
    prev_tags: HashSet<&T>,
) -> Result<BTreeMap<TxnIndex, Incarnation>, PanicError> {
    // ACQUIRE LOCK FIRST to ensure atomicity
    let mut group_sizes = self.group_sizes.get_mut(&group_key).ok_or_else(|| {
        code_invariant_error("Group (sizes) must be initialized to write to")
    })?;
    
    // Now write data while holding the lock
    let (_, mut invalidated_dependencies) =
        self.data_write_impl::<true>(&group_key, txn_idx, incarnation, values, prev_tags)?;
    
    // Size dependency processing remains the same
    let store_deps: BTreeMap<TxnIndex, Incarnation> = Self::get_latest_entry(
        &group_sizes.size_entries,
        txn_idx,
        ReadPosition::AfterCurrentTxn,
    )
    .map_or_else(BTreeMap::new, |(_, size_entry)| {
        let new_deps = size_entry.value.dependencies.lock().split_off(txn_idx + 1);
        if size_entry.value.size == size {
            new_deps
        } else {
            invalidated_dependencies.extend(new_deps);
            BTreeMap::new()
        }
    });
    
    group_sizes.size_entries.insert(
        ShiftedTxnIndex::new(txn_idx),
        SizeEntry::new(SizeAndDependencies::from_size_and_dependencies(
            size, store_deps,
        )),
    );
    
    Ok(invalidated_dependencies.take())
}
```

**Trade-off**: This increases lock contention but ensures correctness. Alternative optimizations could include:
1. Using a more fine-grained locking scheme
2. Implementing a version-based consistency check in readers
3. Recording both data and size dependencies together atomically

## Proof of Concept

The existing test cases in `versioned_group_data.rs` actually demonstrate aspects of this behavior: [7](#0-6) 

The test `test_dependency_tracking` shows scenarios where size and data validation can diverge, though it doesn't explicitly test the race condition.

A Rust reproduction would require:
1. Thread 1: Execute `write_v2()` for transaction T5
2. Thread 2: Execute concurrent reads between data write and size write
3. Verify that Thread 2's dependencies are recorded inconsistently
4. Show that invalidation decisions differ based on timing

The race window can be artificially widened using synchronization primitives (channels/barriers) between the data write and size lock acquisition to make the bug deterministically reproducible in testing.

## Notes

The vulnerability fundamentally stems from treating data and size as separately versionable entities while writing them non-atomically. The comment at line 273-274 explicitly acknowledges this design choice, suggesting it was intentional for performance. However, the dependency tracking system assumes atomic snapshots, creating a mismatch between the memory model and the consistency guarantees.

This issue specifically affects BlockSTM v2's push-based validation model where read dependencies are eagerly recorded. BlockSTM v1 uses pull-based validation which may exhibit different behavior.

### Citations

**File:** aptos-move/mvhashmap/src/versioned_group_data.rs (L270-275)
```rust
        let (_, mut invalidated_dependencies) =
            self.data_write_impl::<true>(&group_key, txn_idx, incarnation, values, prev_tags)?;

        // We write data first, without holding the sizes lock, then write size.
        // Hence when size is observed, values should already be written.
        let mut group_sizes = self.group_sizes.get_mut(&group_key).ok_or_else(|| {
```

**File:** aptos-move/mvhashmap/src/versioned_group_data.rs (L293-308)
```rust
        let store_deps: BTreeMap<TxnIndex, Incarnation> = Self::get_latest_entry(
            &group_sizes.size_entries,
            txn_idx,
            ReadPosition::AfterCurrentTxn,
        )
        .map_or_else(BTreeMap::new, |(_, size_entry)| {
            let new_deps = size_entry.value.dependencies.lock().split_off(txn_idx + 1);

            if size_entry.value.size == size {
                // Validation passed.
                new_deps
            } else {
                invalidated_dependencies.extend(new_deps);
                BTreeMap::new()
            }
        });
```

**File:** aptos-move/mvhashmap/src/versioned_group_data.rs (L436-458)
```rust
    pub fn fetch_tagged_data_and_record_dependency(
        &self,
        group_key: &K,
        tag: &T,
        txn_idx: TxnIndex,
        incarnation: Incarnation,
    ) -> Result<(Version, ValueWithLayout<V>), MVGroupError> {
        let key_ref = GroupKeyRef { group_key, tag };

        // We are accessing group_sizes and values non-atomically, hence the order matters.
        // It is important that initialization check happens before fetch data below. O.w.
        // we could incorrectly get a TagNotFound error (do not find data, but then find
        // size initialized in between the calls). In fact, we always write size after data,
        // and sometimes (e.g. during initialization) even hold the sizes lock during writes.
        // It is fine to observe initialized = false, but find data, in convert_tagged_data.
        // TODO(BlockSTMv2): complete overhaul of initialization logic.
        let initialized = self.group_sizes.contains_key(group_key);

        let data_value =
            self.values
                .fetch_data_and_record_dependency(&key_ref, txn_idx, incarnation);
        self.convert_tagged_data(data_value, initialized)
    }
```

**File:** aptos-move/mvhashmap/src/versioned_group_data.rs (L485-502)
```rust
    pub fn get_group_size_and_record_dependency(
        &self,
        group_key: &K,
        txn_idx: TxnIndex,
        incarnation: Incarnation,
    ) -> Result<ResourceGroupSize, MVGroupError> {
        match self.group_sizes.get(group_key) {
            Some(g) => {
                Self::get_latest_entry(&g.size_entries, txn_idx, ReadPosition::BeforeCurrentTxn)
                    .map_or(Err(MVGroupError::Uninitialized), |(_, size)| {
                        // TODO(BlockSTMv2): convert to PanicErrors after MVHashMap refactoring.
                        assert_ok!(size.value.dependencies.lock().insert(txn_idx, incarnation));
                        Ok(size.value.size)
                    })
            },
            None => Err(MVGroupError::Uninitialized),
        }
    }
```

**File:** aptos-move/mvhashmap/src/versioned_group_data.rs (L711-915)
```rust
    // Test for dependency tracking in V2 interfaces. Notice that due to the implementation,
    // while the size of the base entry is computed at the time of the write, we can determine
    // the recorded sizes for other entries - which allows flexibility in test cases. We test
    // values and sizes separately (and keep the other from invalidating the same dependencies).
    #[test_case(true, true, false; "value test: mid value high, mid size low")]
    #[test_case(true, false, false; "value test: mid value low, mid size low")]
    #[test_case(false, false, true; "size test: mid size high, mid value low")]
    #[test_case(false, false, false; "size test: mid size low, mid value low")]
    fn test_dependency_tracking(
        test_value_or_size: bool,
        mid_value_matches_low: bool,
        mid_size_matches_low: bool,
    ) {
        // Initialize test data
        let group_key = KeyType(b"/group/test".to_vec());
        let tag: usize = 1;

        // Create values and determine which one to use for mid_value
        let base_value = TestValue::creation_with_len(1);
        let high_value = TestValue::creation_with_len(2);
        let mid_value = if mid_value_matches_low {
            base_value.clone()
        } else {
            high_value.clone()
        };

        // Calculate base_size based on the actual values (as it will be computed by set_raw_base_values).
        // Set high size arbitrary and determine mid_size.
        let one_entry_len = base_value.bytes().unwrap().len();
        let base_size = group_size_as_sum(vec![(&tag, one_entry_len)].into_iter()).unwrap();
        let high_size = ResourceGroupSize::Combined {
            num_tagged_resources: 3,
            all_tagged_resources_size: 20,
        };
        let mid_size = if mid_size_matches_low {
            base_size
        } else {
            high_size
        };

        // Fixed indices for our test
        let mid_idx: TxnIndex = 5; // Middle index for the write that may invalidate
        let high_idx: TxnIndex = 10; // High index for the first write
        let inc_1: Incarnation = 1; // Fixed incarnation for simplicity

        // Create a new VersionedGroupData instance and initialize it with base & high values.
        let group_data = VersionedGroupData::<KeyType<Vec<u8>>, usize, TestValue>::empty();
        assert_ok!(
            group_data.set_raw_base_values(group_key.clone(), vec![(tag, base_value.clone())],)
        );
        assert_ok!(group_data.write_v2(
            group_key.clone(),
            high_idx,
            inc_1,
            vec![(tag, (high_value.clone(), None))],
            high_size,
            HashSet::new(),
        ));

        // Define indices for dependencies
        let dependency_indices = [
            15, 20, // > high_idx
            6, 10, // (mid_idx, high_idx]
            2, 5, // ..=mid_idx
        ];

        let mut all_value_deps = BTreeMap::new();
        let mut all_size_deps = BTreeMap::new();
        for idx in dependency_indices {
            if test_value_or_size {
                // Create value dependency
                let res = group_data
                    .fetch_tagged_data_and_record_dependency(&group_key, &tag, idx, inc_1)
                    .unwrap();
                assert_eq!(
                    res.0,
                    if idx > high_idx {
                        Ok((high_idx, inc_1))
                    } else {
                        Err(StorageVersion)
                    }
                );
                assert_eq!(
                    res.1,
                    if idx > high_idx {
                        ValueWithLayout::Exchanged(Arc::new(high_value.clone()), None)
                    } else {
                        ValueWithLayout::RawFromStorage(Arc::new(base_value.clone()))
                    }
                );
                all_value_deps.insert(idx, inc_1);
            } else {
                // Create size dependency
                // Create size dependency
                assert_ok!(group_data.get_group_size_and_record_dependency(&group_key, idx, inc_1));
                all_size_deps.insert(idx, inc_1);
            }
        }

        // Make sure the base value actually matches based on layout / Exchanged.
        group_data.update_tagged_base_value_with_layout(
            group_key.clone(),
            tag,
            base_value.clone(),
            None,
        );
        assert_eq!(
            group_data
                .fetch_tagged_data_and_record_dependency(&group_key, &tag, 2, 1)
                .unwrap(),
            (
                Err(StorageVersion),
                ValueWithLayout::Exchanged(Arc::new(base_value.clone()), None)
            )
        );

        // Write another value at a middle index and check dependency handling.
        let write_invalidated_deps = group_data
            .write_v2(
                group_key.clone(),
                mid_idx,
                inc_1,
                vec![(tag, (mid_value.clone(), None))],
                mid_size,
                HashSet::new(),
            )
            .unwrap();
        let expected_invalidated = all_value_deps
            .clone()
            .into_iter()
            .filter(|&(idx, _)| idx > mid_idx && idx <= high_idx && !mid_value_matches_low)
            .chain(
                all_size_deps
                    .clone()
                    .into_iter()
                    .filter(|&(idx, _)| idx > mid_idx && idx <= high_idx && !mid_size_matches_low),
            )
            .collect::<BTreeMap<_, _>>();
        assert_eq!(write_invalidated_deps, expected_invalidated);
        // Remove the high index entry and check dependency handling
        let remove_invalidated_deps = group_data
            .remove_v2(&group_key, high_idx, HashSet::from([&tag]))
            .unwrap();
        let expected_invalidated = all_value_deps
            .into_iter()
            // matching low value means not matching high in the test
            .filter(|&(idx, _)| idx > high_idx && mid_value_matches_low)
            .chain(
                all_size_deps
                    .clone()
                    .into_iter()
                    .filter(|&(idx, _)| idx > high_idx && mid_size_matches_low),
            )
            .collect::<BTreeMap<_, _>>();
        assert_eq!(remove_invalidated_deps, expected_invalidated);

        // Verify stored size dependencies in the data structure
        let group_sizes = group_data.group_sizes.get(&group_key).unwrap();
        {
            let mid_deps = &group_sizes
                .size_entries
                .get(&ShiftedTxnIndex::new(mid_idx))
                .unwrap()
                .value
                .dependencies;
            assert_eq!(
                take_dependencies(mid_deps),
                all_size_deps
                    .clone()
                    .into_iter()
                    .filter(
                        |&(idx, _)| (idx > mid_idx && idx <= high_idx && mid_size_matches_low)
                            || (idx > high_idx && !mid_size_matches_low)
                    )
                    .collect::<BTreeMap<_, _>>()
            );
        }
        {
            let base_deps = &group_sizes
                .size_entries
                .get(&ShiftedTxnIndex::zero_idx())
                .unwrap()
                .value
                .dependencies;
            assert_eq!(
                take_dependencies(base_deps),
                all_size_deps
                    .into_iter()
                    .filter(|&(idx, _)| idx <= mid_idx)
                    .collect::<BTreeMap<_, _>>()
            );
        }

        // Verify we can access the value and size from mid write
        let (_, value) = group_data
            .fetch_tagged_data_and_record_dependency(
                &group_key, &tag, 21, inc_1, // higher than any idx.
            )
            .unwrap();
        assert_eq!(value, ValueWithLayout::Exchanged(Arc::new(mid_value), None));
        let size = group_data
            .get_group_size_and_record_dependency(&group_key, 21, inc_1)
            .unwrap();
        assert_eq!(size, mid_size);
    }
```

**File:** aptos-move/mvhashmap/src/versioned_data.rs (L279-285)
```rust
                    // Record the read dependency (only in V2 case, not to add contention to V1).
                    if let Some(reader_incarnation) = maybe_reader_incarnation {
                        // TODO(BlockSTMv2): convert to PanicErrors after MVHashMap refactoring.
                        assert_ok!(dependencies
                            .lock()
                            .insert(reader_txn_idx, reader_incarnation));
                    }
```
