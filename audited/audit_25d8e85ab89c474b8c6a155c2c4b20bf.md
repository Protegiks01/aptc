# Audit Report

## Title
Consensus Split via Non-Deterministic VTxn Config Parsing During Epoch Transitions

## Summary
The `initialize_shared_component()` function silently falls back to a default `ValidatorTxnConfig` when on-chain config deserialization fails, causing validators to use different VTxn limits (enabled vs. disabled). This creates a consensus split where validators disagree on proposal validity, violating BFT safety guarantees.

## Finding Description

The vulnerability exists in the epoch manager's config parsing logic. When starting a new epoch, the code attempts to deserialize the on-chain consensus configuration but uses `unwrap_or_default()` as a fallback: [1](#0-0) 

When deserialization fails, only a warning is logged, and the system continues with the **default** config. The default config has VTxn disabled: [2](#0-1) [3](#0-2) 

When VTxn is disabled (V0), both limits return 0: [4](#0-3) 

The `effective_vtxn_config` is then used in `initialize_shared_component()`: [5](#0-4) 

During proposal validation, validators check VTxn limits strictly: [6](#0-5) 

**The Consensus Split:**

1. On-chain config has `ValidatorTxnConfig::V1 { per_block_limit_txn_count: 2, per_block_limit_total_bytes: 2097152 }`
2. **Validator A** successfully deserializes → gets limits (2, 2MB)
3. **Validator B** fails deserialization (version mismatch, corrupted state, BCS incompatibility) → falls back to V0 → gets limits (0, 0)
4. Proposer creates block with 1 validator transaction
5. **Validator A validates**: `1 <= 2` ✓ **ACCEPTS**
6. **Validator B validates**: `1 <= 0` ✗ **REJECTS**
7. **Consensus split occurs** - validators permanently disagree on chain history

**Root Causes for Deserialization Failure:**
- **Version mismatches**: During framework upgrades, if on-chain config uses a newer enum variant (e.g., hypothetical V6) that some validators don't recognize
- **State corruption**: Database issues or incomplete state sync
- **BCS incompatibilities**: Library version differences across validators

## Impact Explanation

This is a **CRITICAL** severity vulnerability per Aptos bug bounty criteria:

1. **Consensus/Safety Violation**: Validators disagree on block validity, violating the fundamental safety property of BFT consensus that honest validators must agree on committed blocks.

2. **Non-Recoverable Network Partition**: Once validators diverge on which blocks are valid, they form separate consensus groups. The network cannot recover without manual intervention (hardfork to force all validators to the same config).

3. **Breaks Invariant #2**: "Consensus Safety: AptosBFT must prevent double-spending and chain splits under < 1/3 Byzantine" - this bug causes splits even with 0% Byzantine validators.

4. **Breaks Invariant #1**: "Deterministic Execution: All validators must produce identical state roots for identical blocks" - validators produce different results for the same block based on their local config state.

The silent failure makes this particularly dangerous - validators continue operating while unknowingly participating in a consensus split. No alerts or errors indicate the network is partitioned until transaction finality breaks.

## Likelihood Explanation

**HIGH** likelihood during:

1. **Network Upgrades**: Framework upgrades that modify `OnChainConsensusConfig` enum variants are high-risk periods. If validators upgrade at different times, they may temporarily have incompatible BCS deserializers.

2. **State Sync Issues**: Validators performing state sync or recovery from snapshots may have incomplete or corrupted on-chain config data, triggering deserialization failures.

3. **Node Restarts During Epoch Transitions**: Validators restarting during epoch changes are vulnerable to reading partially-committed config state.

The bug requires no attacker action - it's a latent vulnerability triggered by operational conditions that occur regularly in production networks. The use of `.unwrap_or_default()` means failures are always silently masked.

## Recommendation

**Remove the silent fallback behavior**. Validators MUST NOT continue with mismatched configs. The fix should:

1. **Panic if config deserialization fails** - forcing operators to investigate and resolve the issue before the validator rejoins consensus:

```rust
let consensus_config = onchain_consensus_config
    .expect("FATAL: Failed to deserialize on-chain consensus config. This validator cannot safely participate in consensus with an incompatible config. Investigate the error above and ensure all validators are running compatible software versions.");
```

2. **Add config hash to epoch state** - include a hash of the serialized config in the epoch proof so validators can verify they all have the same config:

```rust
let config_hash = CryptoHash::hash(&bcs::to_bytes(&consensus_config)?);
// Verify config_hash matches across all validators before starting epoch
```

3. **Add runtime config compatibility checks** - validate that the deserialized config's discriminants match expected versions:

```rust
if let Err(error) = &onchain_consensus_config {
    error!("Failed to read on-chain consensus config: {}", error);
    panic!("Cannot continue with incompatible consensus config");
}
```

The same fix must be applied to all other config parsing locations: [7](#0-6) 

## Proof of Concept

**Reproduction Steps:**

1. **Setup**: Deploy a test network with 4 validators, all running identical software initially.

2. **Trigger Condition**: Simulate a scenario where Validator D has a corrupted on-chain config by:
   - Manually corrupting the consensus config bytes in Validator D's state DB
   - Or use a version mismatch: upgrade Validator D to a version with an incompatible `OnChainConsensusConfig` enum

3. **Epoch Transition**: Trigger an epoch change with VTxn enabled:
   ```rust
   // On-chain config has:
   ValidatorTxnConfig::V1 {
       per_block_limit_txn_count: 2,
       per_block_limit_total_bytes: 2097152,
   }
   ```

4. **Observe Logs**:
   - Validators A, B, C log: `"effective_vtxn_config=V1 { per_block_limit_txn_count: 2, per_block_limit_total_bytes: 2097152 }"`
   - Validator D logs: `"Failed to read on-chain consensus config"` followed by `"effective_vtxn_config=V0"`

5. **Create Proposal with VTxn**: Have a proposer (say A) create a block with 1 validator transaction.

6. **Verify Split**:
   - Validators A, B, C accept the proposal (1 ≤ 2)
   - Validator D rejects the proposal (1 > 0) with error: `"process_proposal failed with per-block vtxn count limit exceeded: limit=0, actual=1"`
   - Consensus cannot progress - 3/4 validators accept, 1/4 rejects
   - If this continues, network partitions based on who has correct vs. default config

**Rust Unit Test** (to be added to `consensus/src/epoch_manager_tests.rs`):

```rust
#[tokio::test]
async fn test_vtxn_config_split_on_deserialization_failure() {
    // Setup: Create corrupted config bytes that will fail deserialization
    let invalid_config_bytes = vec![0xFF; 100]; // Invalid BCS bytes
    
    // Simulate what happens when validator parses this
    let result = bcs::from_bytes::<OnChainConsensusConfig>(&invalid_config_bytes);
    assert!(result.is_err(), "Config should fail to deserialize");
    
    // Current behavior: Falls back to default with VTxn disabled
    let config = result.unwrap_or_default();
    assert_eq!(config.effective_validator_txn_config().per_block_limit_txn_count(), 0);
    
    // Expected behavior: Should panic/error, not silently continue
    // This test documents the vulnerability
}
```

## Notes

This vulnerability demonstrates why silent fallbacks to defaults are dangerous in distributed consensus systems. All validators must operate with **identical** configurations to maintain consensus safety. The fix should follow the "fail-fast" principle: if a validator cannot parse critical configuration, it should halt rather than risk causing a network split.

The same pattern appears in multiple locations in the codebase and should be audited comprehensively. Any config that affects block validation logic must be parsed deterministically and identically across all validators, with no silent fallbacks.

### Citations

**File:** consensus/src/epoch_manager.rs (L1178-1201)
```rust
        let onchain_consensus_config: anyhow::Result<OnChainConsensusConfig> = payload.get();
        let onchain_execution_config: anyhow::Result<OnChainExecutionConfig> = payload.get();
        let onchain_randomness_config_seq_num: anyhow::Result<RandomnessConfigSeqNum> =
            payload.get();
        let randomness_config_move_struct: anyhow::Result<RandomnessConfigMoveStruct> =
            payload.get();
        let onchain_jwk_consensus_config: anyhow::Result<OnChainJWKConsensusConfig> = payload.get();
        let dkg_state = payload.get::<DKGState>();

        if let Err(error) = &onchain_consensus_config {
            warn!("Failed to read on-chain consensus config {}", error);
        }

        if let Err(error) = &onchain_execution_config {
            warn!("Failed to read on-chain execution config {}", error);
        }

        if let Err(error) = &randomness_config_move_struct {
            warn!("Failed to read on-chain randomness config {}", error);
        }

        self.epoch_state = Some(epoch_state.clone());

        let consensus_config = onchain_consensus_config.unwrap_or_default();
```

**File:** consensus/src/epoch_manager.rs (L1331-1365)
```rust
    async fn initialize_shared_component(
        &mut self,
        epoch_state: &EpochState,
        consensus_config: &OnChainConsensusConfig,
        consensus_key: Arc<PrivateKey>,
    ) -> (
        NetworkSender,
        Arc<dyn PayloadClient>,
        Arc<dyn TPayloadManager>,
    ) {
        self.set_epoch_start_metrics(epoch_state);
        self.quorum_store_enabled = self.enable_quorum_store(consensus_config);
        let network_sender = self.create_network_sender(epoch_state);
        let (payload_manager, quorum_store_client, quorum_store_builder) = self
            .init_payload_provider(
                epoch_state,
                network_sender.clone(),
                consensus_config,
                consensus_key,
            )
            .await;
        let effective_vtxn_config = consensus_config.effective_validator_txn_config();
        debug!("effective_vtxn_config={:?}", effective_vtxn_config);
        let mixed_payload_client = MixedPayloadClient::new(
            effective_vtxn_config,
            Arc::new(self.vtxn_pool.clone()),
            Arc::new(quorum_store_client),
        );
        self.start_quorum_store(quorum_store_builder);
        (
            network_sender,
            Arc::new(mixed_payload_client),
            payload_manager,
        )
    }
```

**File:** types/src/on_chain_config/consensus_config.rs (L147-149)
```rust
    pub fn default_if_missing() -> Self {
        Self::V0
    }
```

**File:** types/src/on_chain_config/consensus_config.rs (L169-187)
```rust
    pub fn per_block_limit_txn_count(&self) -> u64 {
        match self {
            ValidatorTxnConfig::V0 => 0,
            ValidatorTxnConfig::V1 {
                per_block_limit_txn_count,
                ..
            } => *per_block_limit_txn_count,
        }
    }

    pub fn per_block_limit_total_bytes(&self) -> u64 {
        match self {
            ValidatorTxnConfig::V0 => 0,
            ValidatorTxnConfig::V1 {
                per_block_limit_total_bytes,
                ..
            } => *per_block_limit_total_bytes,
        }
    }
```

**File:** types/src/on_chain_config/consensus_config.rs (L443-450)
```rust
impl Default for OnChainConsensusConfig {
    fn default() -> Self {
        OnChainConsensusConfig::V4 {
            alg: ConsensusAlgorithmConfig::default_if_missing(),
            vtxn: ValidatorTxnConfig::default_if_missing(),
            window_size: DEFAULT_WINDOW_SIZE,
        }
    }
```

**File:** consensus/src/round_manager.rs (L1146-1177)
```rust
        let num_validator_txns = num_validator_txns as u64;
        let validator_txns_total_bytes = validator_txns_total_bytes as u64;
        let vtxn_count_limit = self.vtxn_config.per_block_limit_txn_count();
        let vtxn_bytes_limit = self.vtxn_config.per_block_limit_total_bytes();
        let author_hex = author.to_hex();
        PROPOSED_VTXN_COUNT
            .with_label_values(&[&author_hex])
            .inc_by(num_validator_txns);
        PROPOSED_VTXN_BYTES
            .with_label_values(&[&author_hex])
            .inc_by(validator_txns_total_bytes);
        info!(
            vtxn_count_limit = vtxn_count_limit,
            vtxn_count_proposed = num_validator_txns,
            vtxn_bytes_limit = vtxn_bytes_limit,
            vtxn_bytes_proposed = validator_txns_total_bytes,
            proposer = author_hex,
            "Summarizing proposed validator txns."
        );

        ensure!(
            num_validator_txns <= vtxn_count_limit,
            "process_proposal failed with per-block vtxn count limit exceeded: limit={}, actual={}",
            self.vtxn_config.per_block_limit_txn_count(),
            num_validator_txns
        );
        ensure!(
            validator_txns_total_bytes <= vtxn_bytes_limit,
            "process_proposal failed with per-block vtxn bytes limit exceeded: limit={}, actual={}",
            self.vtxn_config.per_block_limit_total_bytes(),
            validator_txns_total_bytes
        );
```

**File:** consensus/src/consensus_observer/observer/epoch_state.rs (L156-166)
```rust
    // Extract the consensus config (or use the default if it's missing)
    let onchain_consensus_config: anyhow::Result<OnChainConsensusConfig> = on_chain_configs.get();
    if let Err(error) = &onchain_consensus_config {
        error!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "Failed to read on-chain consensus config! Error: {:?}",
                error
            ))
        );
    }
    let consensus_config = onchain_consensus_config.unwrap_or_default();
```
