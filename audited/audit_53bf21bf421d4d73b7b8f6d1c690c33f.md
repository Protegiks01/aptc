# Audit Report

## Title
State Root Divergence Between Local and Remote Sharded Execution Due to Missing Total Supply Aggregation

## Summary
The `RemoteExecutorClient` does not perform total supply aggregation after sharded execution, while `LocalExecutorClient` does. This causes validators using different execution modes to compute different state roots for identical blocks, breaking consensus safety.

## Finding Description

The sharded block executor supports two execution modes with critically different post-execution logic:

**LocalExecutorClient performs total supply aggregation:** [1](#0-0) 

After receiving execution results from shards, it calls `aggregate_and_update_total_supply()` which modifies transaction outputs.

**RemoteExecutorClient does NOT perform total supply aggregation:** [2](#0-1) 

It returns execution results directly without calling the aggregation function.

**The aggregation function modifies write sets:** [3](#0-2) 

This function computes cumulative total supply deltas across shards and updates each transaction output's `TOTAL_SUPPLY_STATE_KEY` value in the write set.

**The update mechanism directly modifies the write set:** [4](#0-3) 

The `update_total_supply` method inserts a new WriteOp into the BTreeMap, changing the write set content.

**Write sets are cryptographically hashed:** [5](#0-4) 

`WriteSetV0` derives `BCSCryptoHash` and `CryptoHasher`, meaning any change to write set content changes its cryptographic hash, which contributes to the state commitment.

**Validators independently choose execution mode based on configuration:** [6](#0-5) 

The decision is based on whether `get_remote_addresses()` returns a non-empty vector, which is stored in process-local static variables: [7](#0-6) 

Each validator independently configures these addresses, allowing different validators to use different execution modes.

## Impact Explanation

**Critical Severity** - This is a consensus-breaking vulnerability:

- **Consensus Safety Violation**: Validators using different execution modes produce different write sets for the same transactions, leading to different write_set_hash values in TransactionInfo, which results in different state roots.

- **Network Partition**: The network splits into two groups (local vs. remote executors) that reject each other's blocks because they disagree on the state commitment.

- **Non-Recoverable Without Hardfork**: Once divergence occurs, validators cannot automatically reconcile because they fundamentally disagree on the correct state for the same block execution.

This qualifies as **Critical Severity** per Aptos bug bounty criteria under "Consensus/Safety violations" and "Non-recoverable network partition (requires hardfork)".

## Likelihood Explanation

**Conditional Likelihood:**

The vulnerability is deterministic and will trigger if validators use different execution modes. However, important caveats exist: [8](#0-7) 

The remote execution feature panics if global transactions exist, indicating it is incomplete and possibly not production-ready. [9](#0-8) 

The TODO comment suggests developers are aware of incomplete functionality.

**Likelihood Assessment:**
- **HIGH** if mixed execution modes are deployed
- **LOW** if the feature is experimental and not production-deployed
- The vulnerability is latent but becomes active upon any mixed deployment

## Recommendation

**Immediate Fix:** Add the aggregation call to `RemoteExecutorClient`:

```rust
// In RemoteExecutorClient::execute_block, after line 208:
let mut execution_results = self.get_output_from_shards()?;

// Add aggregation (similar to LocalExecutorClient):
sharded_aggregator_service::aggregate_and_update_total_supply(
    &mut execution_results,
    &mut vec![], // global_output placeholder until global txns are supported
    state_view.as_ref(),
    // Need to provide thread pool reference
);

self.state_view_service.drop_state_view();
Ok(ShardedExecutionOutput::new(execution_results, vec![]))
```

**Long-term Fix:** Complete the remote execution implementation including global transaction support, or add validation to prevent mixed execution mode deployments.

## Proof of Concept

No executable PoC provided. The vulnerability can be demonstrated by:
1. Configuring one validator with `set_remote_addresses(vec![...])` 
2. Configuring another validator without remote addresses
3. Executing identical blocks with transactions that modify total supply
4. Comparing the resulting state roots - they will differ

## Notes

This is a **logic vulnerability** with verified code evidence. The execution path difference is real and deterministic. However, the remote execution feature appears incomplete (panics on global transactions), suggesting it may not be production-deployed. If all validators currently use the same execution mode, the vulnerability is latent but would activate immediately upon any mixed deployment scenario.

### Citations

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L215-220)
```rust
        sharded_aggregator_service::aggregate_and_update_total_supply(
            &mut sharded_output,
            &mut global_output,
            state_view.as_ref(),
            self.global_executor.get_executor_thread_pool(),
        );
```

**File:** execution/executor-service/src/remote_executor_client.rs (L32-44)
```rust
static REMOTE_ADDRESSES: OnceCell<Vec<SocketAddr>> = OnceCell::new();
static COORDINATOR_ADDRESS: OnceCell<SocketAddr> = OnceCell::new();

pub fn set_remote_addresses(addresses: Vec<SocketAddr>) {
    REMOTE_ADDRESSES.set(addresses).ok();
}

pub fn get_remote_addresses() -> Vec<SocketAddr> {
    match REMOTE_ADDRESSES.get() {
        Some(value) => value.clone(),
        None => vec![],
    }
}
```

**File:** execution/executor-service/src/remote_executor_client.rs (L190-192)
```rust
        if !global_txns.is_empty() {
            panic!("Global transactions are not supported yet");
        }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L208-211)
```rust
        let execution_results = self.get_output_from_shards()?;

        self.state_view_service.drop_state_view();
        Ok(ShardedExecutionOutput::new(execution_results, vec![]))
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_aggregator_service.rs (L168-257)
```rust
pub fn aggregate_and_update_total_supply<S: StateView>(
    sharded_output: &mut Vec<Vec<Vec<TransactionOutput>>>,
    global_output: &mut [TransactionOutput],
    state_view: &S,
    executor_thread_pool: Arc<rayon::ThreadPool>,
) {
    let num_shards = sharded_output.len();
    let num_rounds = sharded_output[0].len();

    // The first element is 0, which is the delta for shard 0 in round 0. +1 element will contain
    // the delta for the global shard
    let mut aggr_total_supply_delta = vec![DeltaU128::default(); num_shards * num_rounds + 1];

    // No need to parallelize this as the runtime is O(num_shards * num_rounds)
    // TODO: Get this from the individual shards while getting 'sharded_output'
    let mut aggr_ts_idx = 1;
    for round in 0..num_rounds {
        sharded_output.iter().for_each(|shard_output| {
            let mut curr_delta = DeltaU128::default();
            // Though we expect all the txn_outputs to have total_supply, there can be
            // exceptions like 'block meta' (first txn in the block) and 'chkpt info' (last txn
            // in the block) which may not have total supply. Hence we iterate till we find the
            // last txn with total supply.
            for txn in shard_output[round].iter().rev() {
                if let Some(last_txn_total_supply) = txn.write_set().get_total_supply() {
                    curr_delta =
                        DeltaU128::get_delta(last_txn_total_supply, TOTAL_SUPPLY_AGGR_BASE_VAL);
                    break;
                }
            }
            aggr_total_supply_delta[aggr_ts_idx] =
                curr_delta + aggr_total_supply_delta[aggr_ts_idx - 1];
            aggr_ts_idx += 1;
        });
    }

    // The txn_outputs contain 'txn_total_supply' with
    // 'CrossShardStateViewAggrOverride::total_supply_aggr_base_val' as the base value.
    // The actual 'total_supply_base_val' is in the state_view.
    // The 'delta' for the shard/round is in aggr_total_supply_delta[round * num_shards + shard_id + 1]
    // For every txn_output, we have to compute
    //      txn_total_supply = txn_total_supply - CrossShardStateViewAggrOverride::total_supply_aggr_base_val + total_supply_base_val + delta
    // While 'txn_total_supply' is u128, the intermediate computation can be negative. So we use
    // DeltaU128 to handle any intermediate underflow of u128.
    let total_supply_base_val: u128 = get_state_value(&TOTAL_SUPPLY_STATE_KEY, state_view).unwrap();
    let base_val_delta = DeltaU128::get_delta(total_supply_base_val, TOTAL_SUPPLY_AGGR_BASE_VAL);

    let aggr_total_supply_delta_ref = &aggr_total_supply_delta;
    // Runtime is O(num_txns), hence parallelized at the shard level and at the txns level.
    executor_thread_pool.scope(|_| {
        sharded_output
            .par_iter_mut()
            .enumerate()
            .for_each(|(shard_id, shard_output)| {
                for (round, txn_outputs) in shard_output.iter_mut().enumerate() {
                    let delta_for_round =
                        aggr_total_supply_delta_ref[round * num_shards + shard_id] + base_val_delta;
                    let num_txn_outputs = txn_outputs.len();
                    txn_outputs
                        .par_iter_mut()
                        .with_min_len(optimal_min_len(num_txn_outputs, 32))
                        .for_each(|txn_output| {
                            if let Some(txn_total_supply) =
                                txn_output.write_set().get_total_supply()
                            {
                                txn_output.update_total_supply(
                                    delta_for_round.add_delta(txn_total_supply),
                                );
                            }
                        });
                }
            });
    });

    let delta_for_global_shard = aggr_total_supply_delta[num_shards * num_rounds] + base_val_delta;
    let delta_for_global_shard_ref = &delta_for_global_shard;
    executor_thread_pool.scope(|_| {
        let num_txn_outputs = global_output.len();
        global_output
            .par_iter_mut()
            .with_min_len(optimal_min_len(num_txn_outputs, 32))
            .for_each(|txn_output| {
                if let Some(txn_total_supply) = txn_output.write_set().get_total_supply() {
                    txn_output.update_total_supply(
                        delta_for_global_shard_ref.add_delta(txn_total_supply),
                    );
                }
            });
    });
}
```

**File:** types/src/write_set.rs (L692-695)
```rust
#[derive(
    BCSCryptoHash, Clone, CryptoHasher, Debug, Default, Eq, PartialEq, Serialize, Deserialize,
)]
pub struct WriteSetV0(WriteSetMut);
```

**File:** types/src/write_set.rs (L730-739)
```rust
    fn update_total_supply(&mut self, value: u128) {
        assert!(self
            .0
            .write_set
            .insert(
                TOTAL_SUPPLY_STATE_KEY.clone(),
                WriteOp::legacy_modification(bcs::to_bytes(&value).unwrap().into())
            )
            .is_some());
    }
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L261-275)
```rust
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        } else {
            Ok(V::execute_block_sharded(
                &SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        }
```
