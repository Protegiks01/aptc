# Audit Report

## Title
Pre-Validation Memory Exhaustion via Oversized Inline Transaction Batches in Block Proposals

## Summary
A malicious block proposer can cause unbounded memory allocation on validator nodes by including hundreds of thousands of transactions in inline batches within block proposals. The memory allocation occurs during BCS deserialization before validation checks are enforced, allowing an attacker to exhaust validator memory and cause crashes through repeated malicious proposals.

## Finding Description

The security question asks whether the `TransactionDeduper::dedup()` trait contract specifies maximum input size bounds. While the trait itself does not specify bounds, the actual vulnerability exists earlier in the message processing flow—during BCS deserialization, not during deduplication. [1](#0-0) 

The `Payload` enum includes `QuorumStoreInlineHybrid` and `QuorumStoreInlineHybridV2` variants that directly embed transaction vectors: [2](#0-1) 

When a `ProposalMsg` containing these inline transactions is received from the network, the entire message undergoes BCS deserialization. The network layer enforces a maximum message size of approximately 62 MiB: [3](#0-2) 

The BCS deserialization uses a recursion limit (64) for nesting depth, not for vector length: [4](#0-3) 

This means a malicious proposer can pack approximately 400,000 minimal transactions (~150 bytes each) into a 62 MiB message. During deserialization, memory is allocated for ALL these transactions before any validation occurs.

Only after deserialization does `process_proposal()` validate the transaction count: [5](#0-4) 

The `max_receiving_block_txns` limit (default 10,000) causes the proposal to be rejected: [6](#0-5) 

However, the damage is already done—approximately 40-80 MB of memory was allocated during deserialization. The `verify_inline_batches()` function only verifies cryptographic hashes after deserialization: [7](#0-6) 

**Attack Scenario:**
1. Malicious proposer creates block with ~400,000 inline transactions (fits in ~60 MiB)
2. Validator receives and deserializes the `ProposalMsg`, allocating ~40-80 MB
3. Validation in `process_proposal()` rejects the block for exceeding limits
4. Attacker repeats rapidly, sending multiple oversized proposals
5. Memory accumulates if concurrent deserialization occurs
6. Validator memory exhaustion leads to crashes or severe slowdowns

By the time `dedup()` is called, transactions are already bounded by validation: [8](#0-7) 

## Impact Explanation

**Severity: HIGH**

This vulnerability enables:
- **Validator node slowdowns**: Memory pressure from repeated large allocations degrades performance
- **Validator crashes**: Sustained attacks can exhaust available memory, causing OOM kills
- **Protocol violations**: Temporarily offline validators impact consensus liveness

Per Aptos bug bounty criteria, this qualifies as **High Severity** due to validator node slowdowns and potential crashes. While not immediately causing consensus safety violations or fund loss, repeated exploitation can degrade network health and availability.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

- **Attacker requirements**: Any validator who is the designated proposer for a round can craft malicious proposals
- **Detection**: Proposals are rejected after deserialization, generating logs, but memory allocation has already occurred
- **Rate limiting**: An attacker controlling proposer rotation (even briefly) can send multiple malicious proposals before detection
- **Mitigation gap**: No pre-deserialization size checking prevents the initial memory allocation

## Recommendation

Implement size validation BEFORE full deserialization:

1. **Immediate fix**: Add a custom BCS deserializer for `Payload` that validates inline batch sizes before deserializing transaction contents:

```rust
// In consensus-types/src/common.rs
impl Payload {
    pub fn from_bytes_with_limit(bytes: &[u8], max_inline_txns: usize) -> anyhow::Result<Self> {
        // Deserialize only the enum discriminant and BatchInfo metadata first
        // Validate num_txns in each BatchInfo against max_inline_txns
        // Only then deserialize full transaction vectors
    }
}
```

2. **Network layer protection**: Enforce inline transaction count limits at the protocol level:

```rust
// In network/framework/src/protocols/wire/handshake/v1/mod.rs
// Add validation in from_bytes before full deserialization
pub fn validate_consensus_msg_size(bytes: &[u8]) -> anyhow::Result<()> {
    // Check inline batch metadata before full deserialization
}
```

3. **Configuration**: Add `max_receiving_inline_txns` config parameter and enforce it pre-deserialization.

## Proof of Concept

```rust
// This PoC demonstrates the vulnerability conceptually
// In a test environment with instrumented memory tracking:

#[test]
fn test_memory_exhaustion_via_oversized_inline_batches() {
    // Setup validator node
    let validator = create_test_validator();
    
    // Malicious proposer creates block with 400,000 minimal transactions
    let mut inline_batches = vec![];
    for batch_id in 0..40 {
        let mut txns = vec![];
        for i in 0..10000 {
            // Create minimal valid SignedTransaction (~150 bytes)
            txns.push(create_minimal_transaction(batch_id, i));
        }
        let batch_info = create_batch_info(&txns);
        inline_batches.push((batch_info, txns));
    }
    
    let payload = Payload::QuorumStoreInlineHybrid(
        inline_batches,
        ProofWithData::new(vec![]),
        None,
    );
    
    // Serialize to bytes (should be ~60 MiB)
    let proposal_bytes = bcs::to_bytes(&ProposalMsg::new(
        create_block_with_payload(payload),
        sync_info,
    )).unwrap();
    
    assert!(proposal_bytes.len() < MAX_APPLICATION_MESSAGE_SIZE);
    
    // Track memory before deserialization
    let mem_before = get_process_memory();
    
    // Validator receives and deserializes - THIS ALLOCATES MEMORY
    let proposal_msg: ProposalMsg = bcs::from_bytes(&proposal_bytes).unwrap();
    
    let mem_after = get_process_memory();
    assert!(mem_after - mem_before > 40_000_000); // At least 40 MB allocated
    
    // Validation rejects the proposal
    let result = validator.process_proposal(proposal_msg.take_proposal()).await;
    assert!(result.is_err()); // Exceeds max_receiving_block_txns
    
    // But memory was already allocated and consumed
    // Repeat 100 times to exhaust memory...
}
```

## Notes

While the security question specifically asks about the `TransactionDeduper::dedup()` trait, the actual vulnerability occurs in the BCS deserialization flow before dedup is ever called. The dedup function itself operates on already-validated, bounded transaction sets (≤10,000 transactions) and is not directly vulnerable. However, the overall attack vector—malicious proposers including excessive transactions—is valid and exploitable through the deserialization path.

### Citations

**File:** consensus/src/transaction_deduper.rs (L10-12)
```rust
pub trait TransactionDeduper: Send + Sync {
    fn dedup(&self, txns: Vec<SignedTransaction>) -> Vec<SignedTransaction>;
}
```

**File:** consensus/consensus-types/src/common.rs (L213-224)
```rust
    QuorumStoreInlineHybrid(
        Vec<(BatchInfo, Vec<SignedTransaction>)>,
        ProofWithData,
        Option<u64>,
    ),
    OptQuorumStore(OptQuorumStorePayload),
    QuorumStoreInlineHybridV2(
        Vec<(BatchInfo, Vec<SignedTransaction>)>,
        ProofWithData,
        PayloadExecutionLimit,
    ),
}
```

**File:** consensus/consensus-types/src/common.rs (L541-556)
```rust
    pub fn verify_inline_batches<'a, T: TBatchInfo + 'a>(
        inline_batches: impl Iterator<Item = (&'a T, &'a Vec<SignedTransaction>)>,
    ) -> anyhow::Result<()> {
        for (batch, payload) in inline_batches {
            // TODO: Can cloning be avoided here?
            let computed_digest = BatchPayload::new(batch.author(), payload.clone()).hash();
            ensure!(
                computed_digest == *batch.digest(),
                "Hash of the received inline batch doesn't match the digest value for batch {:?}: {} != {}",
                batch,
                computed_digest,
                batch.digest()
            );
        }
        Ok(())
    }
```

**File:** config/src/config/network_config.rs (L47-50)
```rust
pub const MAX_APPLICATION_MESSAGE_SIZE: usize =
    (MAX_MESSAGE_SIZE - MAX_MESSAGE_METADATA_SIZE) - MESSAGE_PADDING_SIZE; /* The message size that applications should check against */
pub const MAX_FRAME_SIZE: usize = 4 * 1024 * 1024; /* 4 MiB large messages will be chunked into multiple frames and streamed */
pub const MAX_MESSAGE_SIZE: usize = 64 * 1024 * 1024; /* 64 MiB */
```

**File:** network/framework/src/protocols/wire/handshake/v1/mod.rs (L156-172)
```rust
    fn encoding(self) -> Encoding {
        match self {
            ProtocolId::ConsensusDirectSendJson | ProtocolId::ConsensusRpcJson => Encoding::Json,
            ProtocolId::ConsensusDirectSendCompressed | ProtocolId::ConsensusRpcCompressed => {
                Encoding::CompressedBcs(RECURSION_LIMIT)
            },
            ProtocolId::ConsensusObserver => Encoding::CompressedBcs(RECURSION_LIMIT),
            ProtocolId::DKGDirectSendCompressed | ProtocolId::DKGRpcCompressed => {
                Encoding::CompressedBcs(RECURSION_LIMIT)
            },
            ProtocolId::JWKConsensusDirectSendCompressed
            | ProtocolId::JWKConsensusRpcCompressed => Encoding::CompressedBcs(RECURSION_LIMIT),
            ProtocolId::MempoolDirectSend => Encoding::CompressedBcs(USER_INPUT_RECURSION_LIMIT),
            ProtocolId::MempoolRpc => Encoding::Bcs(USER_INPUT_RECURSION_LIMIT),
            _ => Encoding::Bcs(RECURSION_LIMIT),
        }
    }
```

**File:** consensus/src/round_manager.rs (L1180-1193)
```rust
        ensure!(
            num_validator_txns + payload_len as u64 <= self.local_config.max_receiving_block_txns,
            "Payload len {} exceeds the limit {}",
            payload_len,
            self.local_config.max_receiving_block_txns,
        );

        ensure!(
            validator_txns_total_bytes + payload_size as u64
                <= self.local_config.max_receiving_block_bytes,
            "Payload size {} exceeds the limit {}",
            payload_size,
            self.local_config.max_receiving_block_bytes,
        );
```

**File:** config/src/config/consensus_config.rs (L23-24)
```rust
pub(crate) static MAX_RECEIVING_BLOCK_TXNS: Lazy<u64> =
    Lazy::new(|| 10000.max(2 * MAX_SENDING_BLOCK_TXNS));
```

**File:** consensus/src/block_preparer.rs (L99-99)
```rust
            let deduped_txns = txn_deduper.dedup(filtered_txns);
```
