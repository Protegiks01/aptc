# Audit Report

## Title
Memory Exhaustion via Unbounded Deferred Module Validation Accumulation in BlockSTMv2

## Summary
The `defer_module_validation()` function in the BlockSTMv2 scheduler allows unbounded accumulation of `ModuleId` entries in a transaction's `stored_requirements` BTreeSet. An attacker can exploit this by submitting multiple module-publishing transactions that cause long-running transactions to accumulate deferred validation requirements, potentially exhausting validator node memory.

## Finding Description

The vulnerability exists in the interaction between module publishing and BlockSTMv2's speculative parallel execution model. When a transaction publishes modules, the system records validation requirements for all subsequent executing or executed transactions. [1](#0-0) 

These requirements are processed by a dedicated worker that activates pending requirements and calls `defer_module_validation()` for transactions still executing. [2](#0-1) 

The critical issue is that `defer_module_validation()` extends the `stored_requirements` BTreeSet without any size checks: [3](#0-2) 

**Attack Flow:**

1. Attacker submits Block N containing:
   - Transactions 0-999: Each publishes 10 unique modules (limited by gas/size constraints)
   - Transactions 1000-9999: Slow-executing transactions (e.g., with read-write conflicts causing multiple incarnations)

2. BlockSTMv2 speculatively executes transactions in parallel, so transactions 1000+ start executing before transactions 0-999 commit.

3. As each publishing transaction (0-999) commits sequentially, it triggers: [4](#0-3) 

4. The dedicated worker activates pending requirements and merges them into the shared `active_requirements.requirements` BTreeSet. [5](#0-4) 

5. For each transaction still executing, `defer_module_validation()` is called, cloning and extending its `stored_requirements` with all accumulated `ModuleId` entries.

6. A transaction executing slowly (due to aborts and re-executions) can receive multiple calls to `defer_module_validation()` as new publishing transactions commit, each time extending its BTreeSet.

7. **Memory Consumption Calculation:**
   - 1,000 publishing transactions × 10 modules each = 10,000 unique `ModuleId` entries
   - Each `ModuleId`: ~32 bytes (address) + ~32 bytes (module name avg) + ~64 bytes (BTreeSet overhead) ≈ 128 bytes
   - Per slow transaction: 10,000 × 128 bytes = 1.28 MB
   - With 1,000 slow transactions: 1,000 × 1.28 MB = **1.28 GB**

8. Across multiple blocks, an attacker could cause sustained memory pressure leading to validator node degradation or crashes.

**Invariant Violation:**

This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." The memory consumption from deferred validation requirements is not bounded by gas metering and can grow based on block composition.

## Impact Explanation

This vulnerability qualifies as **Medium to High severity** per the Aptos bug bounty program:

- **High Severity** ($50,000): "Validator node slowdowns" - Memory exhaustion causing increased GC pressure, slower transaction processing, and potential out-of-memory crashes fits this category.

- **Medium Severity** ($10,000): If the impact is limited to specific conditions requiring sustained attacks across multiple blocks.

The memory is allocated per transaction and persists until execution completes or aborts. [6](#0-5) 

While module publishing is expensive (limited by gas), an attacker with sufficient resources or control over block composition could craft adversarial blocks. The lack of any size limit makes this a clear resource exhaustion vector.

## Likelihood Explanation

**Likelihood: Medium**

**Factors increasing likelihood:**
- No code-level protection against unbounded accumulation
- Exploitable by any attacker who can afford gas costs for module publishing
- BlockSTM's speculative execution naturally creates the timing conditions needed

**Factors decreasing likelihood:**
- Module publishing is expensive (gas costs)
- Requires publishing many unique modules (not just re-publishing the same module)
- Needs slow-executing transactions to accumulate validations (requires contention/conflicts)
- Block size limits constrain total number of transactions (~10,000 typical max)
- Validators may have sufficient memory to handle moderate cases

The attack is feasible but requires significant resources. An attacker would need to:
1. Pay substantial gas fees for hundreds of module-publishing transactions
2. Either control the block proposer or rely on transaction ordering
3. Create transactions that execute slowly enough to accumulate multiple rounds of deferred validations

However, even a partial exploitation (e.g., 100 publishing transactions causing 100 MB memory growth) repeated across multiple blocks could degrade validator performance.

## Recommendation

Implement a size limit on the `stored_requirements` BTreeSet in `defer_module_validation()`:

```rust
pub(crate) fn defer_module_validation(
    &self,
    txn_idx: TxnIndex,
    incarnation: Incarnation,
    requirements: &BTreeSet<ModuleId>,
) -> Result<Option<bool>, PanicError> {
    const MAX_DEFERRED_MODULE_REQUIREMENTS: usize = 1000; // Configurable limit
    
    let status = &self.statuses[txn_idx as usize];
    let mut status_guard = status.status_with_incarnation.lock();

    // ... existing validation logic ...

    match &mut status_guard.status {
        SchedulingStatus::Executing(stored_requirements) => {
            // Check size before extending
            let new_total = stored_requirements.len() + requirements.len();
            if new_total > MAX_DEFERRED_MODULE_REQUIREMENTS {
                // Option 1: Return error to abort execution
                return Err(code_invariant_error(format!(
                    "Deferred module requirements limit exceeded for txn {}: {} + {} > {}",
                    txn_idx, stored_requirements.len(), requirements.len(), 
                    MAX_DEFERRED_MODULE_REQUIREMENTS
                )));
                
                // Option 2: Truncate (less safe, could miss validations)
                // Or: Force immediate validation instead of deferring
            }
            stored_requirements.extend(requirements.iter().cloned());
            Ok(Some(true))
        },
        // ... rest of match arms ...
    }
}
```

**Additional mitigations:**
1. Implement a per-block limit on module publishing transactions
2. Add monitoring/metrics for deferred validation requirements size
3. Consider limiting the number of times `defer_module_validation()` can be called per incarnation
4. Clear deferred requirements more aggressively on transaction abort

## Proof of Concept

```rust
#[cfg(test)]
mod memory_exhaustion_test {
    use super::*;
    use move_core_types::{account_address::AccountAddress, identifier::Identifier};
    
    #[test]
    fn test_unbounded_deferred_validation_accumulation() {
        let num_txns = 1000;
        let statuses = ExecutionStatuses::new(num_txns);
        
        // Simulate transaction 500 starting execution
        let txn_idx = 500;
        statuses.start_executing(txn_idx).unwrap();
        
        // Simulate 100 rounds of module publishing, each adding 10 unique modules
        let mut total_modules = BTreeSet::new();
        for round in 0..100 {
            let mut requirements = BTreeSet::new();
            for module_num in 0..10 {
                let module_id = ModuleId::new(
                    AccountAddress::random(),
                    Identifier::new(format!("Module_{}_{}", round, module_num)).unwrap()
                );
                requirements.insert(module_id.clone());
                total_modules.insert(module_id);
            }
            
            // Defer validation for transaction 500
            let result = statuses.defer_module_validation(
                txn_idx,
                0, // incarnation
                &requirements
            );
            assert!(result.is_ok());
        }
        
        // At this point, transaction 500 has accumulated 1000 ModuleIds
        // Memory consumption: ~128 KB (1000 * ~128 bytes per entry)
        // With 1000 such transactions across a block: ~128 MB
        
        // Finish execution to verify accumulated requirements are returned
        let accumulated_requirements = statuses.finish_execution(txn_idx, 0).unwrap().unwrap();
        assert_eq!(accumulated_requirements.len(), total_modules.len());
        assert_eq!(accumulated_requirements.len(), 1000);
        
        println!("Successfully accumulated {} unique ModuleIds without any size limits", 
                 accumulated_requirements.len());
    }
}
```

## Notes

This vulnerability is particularly concerning because:

1. **No gas metering**: The memory consumption from deferred validation is not charged to any transaction's gas budget.

2. **Cascade effect**: In a block with many publishing transactions, ALL slow-executing transactions accumulate the same growing set of requirements.

3. **BTreeSet overhead**: Beyond the `ModuleId` size, BTreeSet internal nodes add ~40-60% overhead for tree structure.

4. **String allocations**: Module names are heap-allocated strings, and Rust allocates separate heap memory for each cloned string in `requirements.iter().cloned()`.

5. **No cleanup on abort**: While the BTreeSet is eventually dropped, a transaction going through many incarnations due to conflicts could accumulate requirements multiple times during its lifetime.

The fix should balance security (preventing memory exhaustion) with functionality (not breaking legitimate module publishing workflows). A reasonable limit of 1,000-10,000 deferred module requirements per transaction should be sufficient for legitimate use cases while preventing abuse.

### Citations

**File:** aptos-move/block-executor/src/txn_last_input_output.rs (L558-576)
```rust
        let mut module_ids_for_v2 = BTreeSet::new();
        for write in output_before_guard.module_write_set().values() {
            published = true;
            if scheduler.is_v2() {
                module_ids_for_v2.insert(write.module_id().clone());
            }
            add_module_write_to_module_cache::<T>(
                write,
                txn_idx,
                runtime_environment,
                global_module_cache,
                versioned_cache.module_cache(),
            )?;
        }
        if published {
            // Record validation requirements after the modules are published.
            global_module_cache.flush_layout_cache();
            scheduler.record_validation_requirements(txn_idx, module_ids_for_v2)?;
        }
```

**File:** aptos-move/block-executor/src/cold_validation.rs (L483-499)
```rust
        let new_versions: BTreeMap<TxnIndex, (Incarnation, bool)> = (starting_idx..ending_idx)
            .filter_map(|txn_idx| {
                statuses
                    .requires_module_validation(txn_idx)
                    .map(|(incarnation, is_executing)| (txn_idx, (incarnation, is_executing)))
            })
            .collect();
        let new_requirements = pending_reqs
            .into_iter()
            .fold(BTreeSet::new(), |mut acc, req| {
                acc.extend(req.requirements);
                acc
            });

        let active_reqs = self.active_requirements.dereference_mut();
        active_reqs.requirements.extend(new_requirements);
        active_reqs.versions.extend(new_versions);
```

**File:** aptos-move/block-executor/src/scheduler_status.rs (L591-612)
```rust
        match status_guard.status {
            SchedulingStatus::Executing(_) => {
                let requirements = if let SchedulingStatus::Executing(requirements) =
                    std::mem::replace(&mut status_guard.status, SchedulingStatus::Executed)
                {
                    requirements
                } else {
                    unreachable!("In Executing variant match arm");
                };

                let new_status_flag = if status.is_stalled() {
                    DependencyStatus::ShouldDefer
                } else {
                    DependencyStatus::IsSafe
                };
                status.swap_dependency_status_any(
                    &[DependencyStatus::WaitForExecution],
                    new_status_flag,
                    "finish_execution",
                )?;

                Ok(Some(requirements))
```

**File:** aptos-move/block-executor/src/scheduler_status.rs (L849-852)
```rust
            SchedulingStatus::Executing(stored_requirements) => {
                // Note: we can move the clone out of the critical section if needed.
                stored_requirements.extend(requirements.iter().cloned());
                Ok(Some(true))
```

**File:** aptos-move/block-executor/src/scheduler_v2.rs (L1018-1049)
```rust
    pub(crate) fn record_validation_requirements(
        &self,
        worker_id: u32,
        txn_idx: TxnIndex,
        module_ids: BTreeSet<ModuleId>,
    ) -> Result<(), PanicError> {
        if worker_id >= self.num_workers {
            return Err(code_invariant_error(format!(
                "Worker ID {} must be less than the number of workers {}",
                worker_id, self.num_workers
            )));
        }
        if txn_idx >= self.num_txns {
            return Err(code_invariant_error(format!(
                "Txn index {} must be less than the number of transactions {}",
                txn_idx, self.num_txns
            )));
        }

        let min_never_scheduled_idx = self.min_never_scheduled_idx()?;
        if txn_idx >= min_never_scheduled_idx {
            return Err(code_invariant_error(format!(
                "Calling txn idx {} must be less than min_never_scheduled_idx {}",
                txn_idx, min_never_scheduled_idx
            )));
        }
        self.cold_validation_requirements.record_requirements(
            worker_id,
            txn_idx,
            min_never_scheduled_idx,
            module_ids,
        )
```
