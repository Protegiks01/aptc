# Audit Report

## Title
Double-Checked Locking Bug in OutputCache Causes Resource Exhaustion and Amplification Attacks on Node-Checker Service

## Summary
The `OutputCache::get()` function in the node-checker contains a classic double-checked locking bug where concurrent threads can all execute expensive network operations despite only the first execution being necessary. This occurs because the implementation fails to re-verify cache staleness after acquiring write locks, enabling denial-of-service attacks and resource exhaustion on the node-checker infrastructure. [1](#0-0) 

## Finding Description
The vulnerability exists in the cache invalidation and refresh logic. The function performs an initial staleness check under read locks, but after multiple threads detect a stale cache and queue for write locks, each thread sequentially executes the expensive `func.await` operation without re-checking if another thread already refreshed the cache.

**Attack Flow:**

1. **Initial State**: Baseline providers are shared across requests via Arc wrappers [2](#0-1) 

2. **Cache Expiration**: Default TTL is 1000ms, creating a recurring attack window every second [3](#0-2) 

3. **Concurrent Requests**: When 100 concurrent requests arrive:
   - All threads check `last_run.read().await.elapsed() < self.cache_ttl` and see the cache is stale
   - All threads release read locks and proceed to line 48
   - Thread 1 acquires write locks (lines 48-49), executes `func.await` (line 50)
   - Threads 2-100 queue waiting for write locks
   - **Critical Bug**: Thread 2 acquires locks but doesn't re-check cache staleness before calling `func.await`
   - Each subsequent thread repeats the expensive operation despite cache being fresh

4. **Resource Amplification**: If the network operation takes 100ms, total execution time becomes 100 threads × 100ms = 10 seconds of sequential blocking, during which:
   - Write locks prevent ANY reader from accessing the cache
   - The baseline node receives 100× the necessary network requests
   - Legitimate health check requests are blocked

**Why This Differs from Lock Scope Question:**

The security question asks whether locks should be released before `func.await` to reduce contention. The answer is **neither approach is correct**:
- **Current implementation** (locks held): Has missing double-check causing sequential redundant fetches
- **Releasing locks**: Would cause concurrent redundant fetches (cache stampede)
- **Correct solution**: Keep locks held but add double-check after acquiring them

The comment at lines 46-47 explicitly states the intention to avoid redundant fetches, but this is defeated by the missing re-check: [4](#0-3) 

## Impact Explanation
This vulnerability enables **Medium severity** impact through resource exhaustion and availability disruption:

1. **Node-Checker Service DoS**: Concurrent attacks cause cascading delays, making the service unresponsive to legitimate health check requests

2. **Baseline Node Amplification**: Attackers amplify their requests 100× against baseline nodes (which may be production validators), potentially causing:
   - Metrics endpoint overload
   - API rate limiting triggering
   - Network bandwidth exhaustion

3. **Infrastructure Monitoring Degradation**: The node-checker is critical infrastructure for validator health monitoring. Disruption affects: [5](#0-4) 

While this doesn't directly impact consensus, it degrades the Aptos ecosystem's ability to monitor and respond to validator health issues. The classification as Medium severity aligns with "State inconsistencies requiring intervention" where the "state" refers to infrastructure monitoring state requiring manual recovery.

## Likelihood Explanation
**High Likelihood** - The attack requires minimal sophistication:

1. **No Authentication**: The node-checker `/check` endpoint has no rate limiting [6](#0-5) 

2. **Public Accessibility**: Node-checkers are designed to be publicly accessible for health monitoring

3. **Recurring Attack Window**: The 1-second cache TTL creates a predictable attack opportunity every second

4. **Low Complexity**: Attacker only needs to send concurrent HTTP GET requests to trigger the vulnerability

## Recommendation
Implement proper double-checked locking by adding a cache staleness re-check after acquiring write locks:

```rust
pub async fn get(
    &self,
    func: impl Future<Output = Result<T, ProviderError>>,
) -> Result<T, ProviderError> {
    // First check (without write locks)
    if self.last_run.read().await.elapsed() < self.cache_ttl {
        if let Some(last_output) = &*self.last_output.read().await {
            return Ok(last_output.clone());
        }
    }

    // Acquire write locks
    let mut last_output = self.last_output.write().await;
    let mut last_run = self.last_run.write().await;
    
    // SECOND CHECK - Re-verify staleness after acquiring locks
    if last_run.elapsed() < self.cache_ttl {
        if let Some(output) = &*last_output {
            return Ok(output.clone());
        }
    }
    
    // Now safely fetch knowing we're the only thread doing so
    let new_output = func.await?;
    *last_output = Some(new_output.clone());
    *last_run = Instant::now();
    Ok(new_output)
}
```

Additionally, consider implementing rate limiting on the `/check` endpoint to mitigate concurrent request floods.

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::atomic::{AtomicUsize, Ordering};
    use std::sync::Arc;
    use tokio::time::{sleep, Duration};

    #[tokio::test]
    async fn test_cache_stampede() {
        let fetch_count = Arc::new(AtomicUsize::new(0));
        let cache = Arc::new(OutputCache::new(Duration::from_millis(100)));
        
        // Simulate 10 concurrent requests after cache expiration
        sleep(Duration::from_millis(150)).await; // Wait for cache to expire
        
        let mut handles = vec![];
        for _ in 0..10 {
            let cache_clone = cache.clone();
            let counter_clone = fetch_count.clone();
            handles.push(tokio::spawn(async move {
                cache_clone.get(async {
                    counter_clone.fetch_add(1, Ordering::SeqCst);
                    sleep(Duration::from_millis(50)).await; // Simulate slow network
                    Ok::<_, ProviderError>(42)
                }).await
            }));
        }
        
        for handle in handles {
            handle.await.unwrap().unwrap();
        }
        
        let actual_fetches = fetch_count.load(Ordering::SeqCst);
        println!("Expected: 1 fetch, Actual: {} fetches", actual_fetches);
        
        // BUG: Without double-check, this will be 10 instead of 1
        assert!(actual_fetches > 1, "Cache stampede vulnerability confirmed");
    }
}
```

**Expected Behavior**: Only 1 fetch should occur when 10 concurrent requests arrive
**Actual Behavior**: All 10 requests execute the expensive operation sequentially
**Total Execution Time**: 10 requests × 50ms = 500ms instead of 50ms

## Notes
While this vulnerability doesn't directly compromise blockchain consensus or cause fund loss, it impacts critical infrastructure used for validator health monitoring. The explicit marking of this issue as "(Medium)" severity in the security question indicates that infrastructure availability issues within the Aptos ecosystem are considered reportable. The bug represents a classic synchronization error that violates the stated intent of preventing redundant work while holding locks.

### Citations

**File:** ecosystem/node-checker/src/provider/cache.rs (L35-54)
```rust
    pub async fn get(
        &self,
        func: impl Future<Output = Result<T, ProviderError>>,
    ) -> Result<T, ProviderError> {
        // If the cache isn't too old and there is a value, return it.
        if self.last_run.read().await.elapsed() < self.cache_ttl {
            if let Some(last_output) = &*self.last_output.read().await {
                return Ok(last_output.clone());
            }
        }

        // Otherwise fetch the value and update the cache. We take the locks while
        // fetching the new value so we don't waste effort fetching it multiple times.
        let mut last_output = self.last_output.write().await;
        let mut last_run = self.last_run.write().await;
        let new_output = func.await?;
        *last_output = Some(new_output.clone());
        *last_run = Instant::now();
        Ok(new_output)
    }
```

**File:** ecosystem/node-checker/src/server/build.rs (L98-98)
```rust
            provider_collection.baseline_metrics_provider = Some(Arc::new(metrics_provider));
```

**File:** ecosystem/node-checker/src/provider/mod.rs (L60-62)
```rust
    fn default_cache_ttl_ms() -> u64 {
        1000
    }
```

**File:** ecosystem/node-checker/src/checker/node_identity.rs (L86-86)
```rust
        let baseline_response = baseline_api_index_provider.provide().await?;
```

**File:** ecosystem/node-checker/src/server/run.rs (L51-56)
```rust
    let cors = Cors::new().allow_methods(vec![Method::GET]);

    Server::new(TcpListener::bind((
        args.server_args.listen_address,
        args.server_args.listen_port,
    )))
```
