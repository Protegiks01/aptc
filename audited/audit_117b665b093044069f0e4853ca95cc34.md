# Audit Report

## Title
VFN Transaction Broadcast Single Point of Failure: Zero Failover Peers Eliminate Resilience to Primary Peer Degradation

## Summary
Setting `default_failovers=0` for Validator Full Nodes (VFNs) in the mempool configuration eliminates transaction broadcast redundancy during normal operation. This creates a single point of failure where VFNs rely entirely on one primary validator peer for transaction propagation, with no backup peers to provide resilience against degraded or slow (but connected) primary peers. [1](#0-0) 

## Finding Description

The `optimize()` function explicitly sets `default_failovers=0` for VFNs, which directly impacts the failover peer assignment logic in the mempool broadcast system.

**How Failover Assignment Works:**

The `update_sender_bucket_for_peers()` function assigns sender buckets with failover priority by iterating `default_failovers` times: [2](#0-1) 

When `default_failovers=0`, this loop executes zero times, meaning **no peers are assigned as failover peers**. Combined with the VFN-specific logic that selects only one primary peer from the VFN network: [3](#0-2) 

This results in VFNs having:
- **Exactly 1 primary peer** (the VFN network peer with lowest ping latency)
- **Zero failover peers** (because `default_failovers=0`)

**Broadcast Behavior Without Failovers:**

During broadcast, peers without assigned sender buckets are rejected: [4](#0-3) 

**The Vulnerability:**

When the primary peer becomes degraded (high latency, packet loss, processing delays) but remains connected:
1. No peer disconnection event occurs
2. No peer reassignment is triggered
3. The VFN continues broadcasting exclusively to the degraded primary peer
4. Transactions accumulate in mempool without reaching validators efficiently
5. **No backup peers exist to provide redundancy**

This violates the availability invariant that transactions should propagate reliably to validators.

**Attack Scenario:**

An adversary controlling network infrastructure could:
1. Introduce targeted latency or packet loss to the primary validator peer
2. Keep the connection alive (avoiding disconnection detection)
3. Force the VFN to queue transactions without reliable propagation
4. Cause transaction delays affecting dependent applications

## Impact Explanation

**High Severity** per Aptos bug bounty criteria:

1. **Validator node slowdowns**: Indirectly causes slowdowns by delaying transaction propagation from VFNs to validators, reducing network throughput.

2. **Significant protocol violations**: Eliminates the expected resilience property that nodes should have redundancy in critical operations. Transaction propagation is a critical path in the blockchain protocol.

3. **Availability Impact**: During primary peer degradation (not full failure), transactions experience unbounded delays. This affects:
   - Users submitting transactions through VFNs
   - Applications depending on timely transaction confirmation
   - Overall network responsiveness

4. **Real-world Likelihood**: Network degradation is common in production environments (congestion, routing issues, partial failures). Unlike complete disconnections, degraded connections are harder to detect and more persistent.

## Likelihood Explanation

**High Likelihood** due to:

1. **Default Configuration**: This is the optimized default for all VFNs, not an edge case configuration.

2. **Common Network Conditions**: Partial network failures, congestion, and degraded connections are routine in production blockchain networks.

3. **No Alternative Mechanism**: There is no alternative redundancy mechanism during normal operation. Failover only occurs through peer reassignment after disconnection detection.

4. **Detection Lag**: The health check mechanism requires sync lag to exceed `max_sync_lag_before_unhealthy_secs` (default 30 seconds), creating a significant window of degraded service: [5](#0-4) 

5. **Observable in Tests**: The test suite explicitly validates this configuration but does not test degraded-but-connected peer scenarios: [6](#0-5) 

## Recommendation

**Immediate Fix**: Restore minimum failover redundancy for VFNs by setting `default_failovers=1` instead of 0:

```rust
// In config/src/config/mempool_config.rs, line 224:
if local_mempool_config_yaml["default_failovers"].is_null() {
    mempool_config.default_failovers = 1;  // Changed from 0 to 1
    modified_config = true;
}
```

**Rationale**: This provides one backup peer during normal operation while maintaining most of the performance optimization. The backup peer receives transactions with a delay (default 500ms), providing resilience without doubling immediate network traffic.

**Alternative Approaches**:

1. **Adaptive Failover**: Enable failover peers only when primary peer latency exceeds a threshold
2. **Health-Based Failover**: Dynamically adjust `default_failovers` based on primary peer health metrics
3. **Configurable Policy**: Allow operators to choose between performance (0 failovers) and resilience (â‰¥1 failovers) based on their deployment requirements

## Proof of Concept

**Scenario**: Demonstrate transaction propagation failure when primary VFN peer is degraded but connected.

**Setup Requirements**:
1. Deploy a test network with 4 validators and 4 VFNs (matching the test setup)
2. Configure VFNs with `default_failovers=0`
3. Introduce artificial network latency to the primary validator peer

**Expected Behavior** (with `default_failovers=0`):
- Transactions submitted to VFN continue using only the degraded primary peer
- No failover peers exist to provide backup
- Transaction confirmation times increase proportionally to primary peer latency
- Peer reassignment does NOT occur (connection remains active)

**Expected Behavior** (with `default_failovers=1`):
- Failover peer receives transactions after 500ms delay
- If primary peer is degraded, failover peer successfully propagates transactions
- Transaction confirmation times remain bounded

**Test Implementation** (Rust smoke test):

```rust
#[tokio::test]
async fn test_vfn_degraded_primary_peer_resilience() {
    // Create VFN config with default_failovers=0
    let mut vfn_config = NodeConfig::get_default_vfn_config();
    vfn_config.mempool.default_failovers = 0;
    
    let mut swarm = SwarmBuilder::new_local(4)
        .with_num_fullnodes(4)
        .with_vfn_config(vfn_config)
        .build()
        .await;
    
    // Wait for connectivity
    swarm.wait_for_connectivity(/* ... */).await.unwrap();
    
    // Identify primary validator peer for VFN[1]
    let vfn = swarm.full_nodes().nth(1).unwrap();
    let vfn_client = vfn.rest_client();
    
    // Introduce network latency to primary validator peer (simulated degradation)
    // This would require network manipulation capabilities
    inject_network_latency(vfn, primary_validator, Duration::from_millis(2000));
    
    // Submit transactions through VFN
    let start = Instant::now();
    let txn = transfer_coins(/* ... */).await;
    let duration = start.elapsed();
    
    // With default_failovers=0, transaction propagation is bounded by degraded primary peer
    // Expected: duration >> normal_propagation_time
    assert!(duration > Duration::from_secs(2), 
        "Transaction should be delayed by degraded primary peer");
    
    // No failover occurred - verify transaction went through degraded primary only
    verify_single_peer_broadcast(vfn).await;
}
```

**Notes**

The test case `test_vfn_failover` validates that VFNs can recover from complete validator failure through peer reassignment, but does not test resilience during the more common scenario of degraded-but-connected peers. The current configuration trades resilience for performance optimization, which may be acceptable in some deployments but violates the principle of defense-in-depth for critical transaction propagation paths. 

The issue is exacerbated by VFNs' special peer selection logic that restricts the primary peer to the VFN network, limiting the diversity of potential failover peers. Setting `default_failovers=1` restores minimal redundancy at a small cost of additional network traffic delayed by 500ms.

### Citations

**File:** config/src/config/mempool_config.rs (L222-226)
```rust
            // Set the default_failovers to 0 (default is 1)
            if local_mempool_config_yaml["default_failovers"].is_null() {
                mempool_config.default_failovers = 0;
                modified_config = true;
            }
```

**File:** mempool/src/shared_mempool/priority.rs (L347-359)
```rust
        if self.node_type.is_validator_fullnode() {
            // Use the peer on the VFN network with lowest ping latency as the primary peer
            let peers_in_vfn_network = self
                .prioritized_peers
                .read()
                .iter()
                .cloned()
                .filter(|peer| peer.network_id() == NetworkId::Vfn)
                .collect::<Vec<_>>();

            if !peers_in_vfn_network.is_empty() {
                top_peers = vec![peers_in_vfn_network[0]];
            }
```

**File:** mempool/src/shared_mempool/priority.rs (L411-431)
```rust
            // Assign sender buckets with Failover priority. Use Round Robin.
            peer_index = 0;
            let num_prioritized_peers = self.prioritized_peers.read().len();
            for _ in 0..self.mempool_config.default_failovers {
                for bucket_index in 0..self.mempool_config.num_sender_buckets {
                    // Find the first peer that already doesn't have the sender bucket, and add the bucket
                    for _ in 0..num_prioritized_peers {
                        let peer = self.prioritized_peers.read()[peer_index];
                        let sender_bucket_list =
                            self.peer_to_sender_buckets.entry(peer).or_default();
                        if let std::collections::hash_map::Entry::Vacant(e) =
                            sender_bucket_list.entry(bucket_index)
                        {
                            e.insert(BroadcastPeerPriority::Failover);
                            break;
                        }
                        peer_index = (peer_index + 1) % num_prioritized_peers;
                    }
                }
            }
        }
```

**File:** mempool/src/shared_mempool/priority.rs (L577-589)
```rust

                    // Calculate the max sync lag before the peer is considered unhealthy (in microseconds)
                    let max_sync_lag_secs =
                        mempool_config.max_sync_lag_before_unhealthy_secs as u64;
                    let max_sync_lag_usecs = max_sync_lag_secs * MICROS_PER_SECOND;

                    // Determine if the peer is healthy
                    current_timestamp_usecs.saturating_sub(peer_ledger_timestamp_usecs)
                        < max_sync_lag_usecs
                })
        })
        .unwrap_or(false) // If metadata is missing, consider the peer unhealthy
}
```

**File:** mempool/src/shared_mempool/network.rs (L502-513)
```rust
                            self.prioritized_peers_state
                                .get_sender_buckets_for_peer(&peer)
                                .ok_or_else(|| {
                                    BroadcastError::PeerNotPrioritized(
                                        peer,
                                        self.prioritized_peers_state.get_peer_priority(&peer),
                                    )
                                })?
                                .clone()
                                .into_iter()
                                .collect()
                        };
```

**File:** testsuite/smoke-test/src/full_nodes.rs (L111-115)
```rust
async fn test_vfn_failover() {
    // VFN failover happens when validator is down even for default_failovers = 0
    let mut vfn_config = NodeConfig::get_default_vfn_config();
    vfn_config.mempool.default_failovers = 0;
    let mut swarm = SwarmBuilder::new_local(4)
```
