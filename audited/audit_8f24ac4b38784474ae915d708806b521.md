# Audit Report

## Title
Consensus Node Crash via Duplicate BatchKey with Different gas_bucket_start Values in BatchProofQueue

## Summary
A Byzantine validator can cause all honest validators to crash by sending batch summaries and proofs for the same batch (same author and batch_id) but with different `gas_bucket_start` values. This creates multiple entries in the expiration tracking system that map to a single entry in the items storage, leading to a panic when both entries expire and are processed sequentially.

## Finding Description

The `BatchProofQueue` maintains three data structures to track batches:
- `items: HashMap<BatchKey, QueueItem>` where `BatchKey = (author, batch_id)`
- `author_to_batches: HashMap<PeerId, BTreeMap<BatchSortKey, BatchInfoExt>>` where `BatchSortKey = (author, batch_id, gas_bucket_start)`
- `expirations: TimeExpirations<BatchSortKey>` 

The vulnerability arises because `BatchKey` and `BatchSortKey` use different fields for uniqueness. A malicious validator can exploit this by:

1. Broadcasting a batch summary via `insert_batches()` with `(author=V, batch_id=1, gas_bucket_start=100, expiration=T)` [1](#0-0) 

2. Later broadcasting a proof via `insert_proof()` for the "same" batch with `(author=V, batch_id=1, gas_bucket_start=200, expiration=T)` [2](#0-1) 

The duplicate check in `insert_proof()` only validates whether a proof already exists or the batch is committed, but does NOT validate consistency of `gas_bucket_start`: [3](#0-2) 

This results in:
- Two entries in `author_to_batches`: `(V, 1, 100)` and `(V, 1, 200)`
- Two entries in `expirations`: `(V, 1, 100)` and `(V, 1, 200)` 
- ONE entry in `items`: `(V, 1)`

When `handle_updated_block_timestamp(T)` is called at expiration:
1. Both `BatchSortKey` entries are returned by `expirations.expire(T)` [4](#0-3) 

2. First iteration processes `(V, 1, 100)`:
   - Removes and processes the item from `items` successfully
   - Re-inserts the queue still containing `(V, 1, 200)` [5](#0-4) 

3. Second iteration processes `(V, 1, 200)`:
   - Finds it in `author_to_batches` (line 732-733)
   - Attempts to get from `items` at line 734-737, but the item was already removed
   - **PANIC**: `expect("Entry for unexpired batch must exist")` [6](#0-5) 
   - If that expect were removed, the `assert_some!` at line 760 would panic instead [7](#0-6) 

## Impact Explanation

**Severity: HIGH**

This vulnerability can cause **validator node crashes**, impacting consensus liveness:

1. **Consensus Availability Impact**: When all validators process the malicious batches at expiration time, they will all panic and crash simultaneously, causing total network liveness loss until nodes are manually restarted.

2. **Deterministic Crash**: The panic is deterministic once the malicious data is in the queue and the block timestamp reaches the expiration time. This is not a rare race condition.

3. **Network-Wide Impact**: Since the malicious batches are broadcast to all validators and the expiration logic is deterministic, all honest validators will crash at the same block height.

Per the Aptos Bug Bounty program, this qualifies as **High Severity** due to "Validator node slowdowns" and "Significant protocol violations" (validator crashes), potentially escalating to **Critical** if it causes "Total loss of liveness/network availability" network-wide.

## Likelihood Explanation

**Likelihood: MEDIUM to HIGH**

The attack is feasible under these conditions:

1. **Attacker Requirements**: The attacker must be a validator with the ability to create batches and broadcast batch summaries and proofs. This is a reasonable assumption under the Byzantine threat model (up to 1/3 malicious validators).

2. **Execution Complexity**: The attack is straightforward:
   - Reuse the same `batch_id` for two different `BatchInfo` structures with different `gas_bucket_start`
   - Broadcast both the summary and proof
   - Wait for expiration

3. **Detection Difficulty**: The inconsistency is not detected by the current validation logic, as the duplicate check only verifies if a proof exists, not if the `gas_bucket_start` matches.

4. **Real-World Trigger**: This could also be triggered unintentionally by:
   - A buggy validator implementation that reuses batch IDs
   - Network message reordering causing stale batch summaries to be processed after new proofs
   - Software upgrades that change batch creation logic

## Recommendation

Add validation in `insert_proof()` to verify that if a batch summary already exists for the same `BatchKey`, the `gas_bucket_start` (and other critical fields) must match:

```rust
pub(crate) fn insert_proof(&mut self, proof: ProofOfStore<BatchInfoExt>) {
    // ... existing expiration check ...
    
    let batch_key = BatchKey::from_info(proof.info());
    
    // Enhanced duplicate check
    if let Some(existing_item) = self.items.get(&batch_key) {
        if existing_item.proof.is_some() || existing_item.is_committed() {
            counters::inc_rejected_pos_count(counters::POS_DUPLICATE_LABEL);
            return;
        }
        
        // NEW: Validate consistency of gas_bucket_start
        if existing_item.info.gas_bucket_start() != proof.gas_bucket_start() {
            counters::inc_rejected_pos_count(counters::POS_INCONSISTENT_LABEL);
            warn!(
                "Rejecting proof with inconsistent gas_bucket_start: existing={}, proof={}",
                existing_item.info.gas_bucket_start(),
                proof.gas_bucket_start()
            );
            return;
        }
    }
    
    // ... rest of insertion logic ...
}
```

Similarly, add validation in `insert_batches()` to reject batch summaries that conflict with existing proofs.

Additionally, consider using `BatchKey` consistently as the index key or including `gas_bucket_start` in `BatchKey` to prevent such inconsistencies by design.

## Proof of Concept

```rust
#[tokio::test]
async fn test_duplicate_batch_key_different_gas_bucket_panic() {
    let my_peer_id = PeerId::random();
    let batch_store = batch_store_for_test(5 * 1024 * 1024);
    let mut proof_queue = BatchProofQueue::new(my_peer_id, batch_store, 1);
    
    let author = PeerId::random();
    let batch_id = BatchId::new_for_test(1);
    let expiration = 1000;
    
    // Step 1: Insert batch summary with gas_bucket_start=100
    let batch_info_v1 = BatchInfo::new(
        author,
        batch_id,
        0,
        expiration,
        HashValue::random(),
        10,
        1000,
        100, // gas_bucket_start=100
    );
    let txn_summaries = vec![];
    proof_queue.insert_batches(vec![(batch_info_v1.into(), txn_summaries)]);
    
    // Step 2: Insert proof with gas_bucket_start=200 (different!)
    let batch_info_v2 = BatchInfo::new(
        author,
        batch_id,
        0,
        expiration,
        HashValue::random(),
        10,
        1000,
        200, // gas_bucket_start=200
    );
    let proof = ProofOfStore::new(
        batch_info_v2.into(),
        AggregateSignature::empty(),
    );
    proof_queue.insert_proof(proof);
    
    // Step 3: Advance block timestamp to expiration
    // This should panic at line 737 or 760
    proof_queue.handle_updated_block_timestamp(expiration);
    
    // If we reach here, the bug is not triggered (test should panic)
    panic!("Expected panic did not occur!");
}
```

**Note**: This PoC demonstrates the logic vulnerability. In a real scenario, the proof would need valid signatures, but the structural issue in the data management remains.

### Citations

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L175-256)
```rust
    pub(crate) fn insert_proof(&mut self, proof: ProofOfStore<BatchInfoExt>) {
        if proof.expiration() <= self.latest_block_timestamp {
            counters::inc_rejected_pos_count(counters::POS_EXPIRED_LABEL);
            return;
        }
        let batch_key = BatchKey::from_info(proof.info());
        if self
            .items
            .get(&batch_key)
            .is_some_and(|item| item.proof.is_some() || item.is_committed())
        {
            counters::inc_rejected_pos_count(counters::POS_DUPLICATE_LABEL);
            return;
        }

        let author = proof.author();
        let bucket = proof.gas_bucket_start();
        let num_txns = proof.num_txns();
        let expiration = proof.expiration();

        let batch_sort_key = BatchSortKey::from_info(proof.info());
        let batches_for_author = self.author_to_batches.entry(author).or_default();
        batches_for_author.insert(batch_sort_key.clone(), proof.info().clone());

        // Check if a batch with a higher batch Id (reverse sorted) exists
        if let Some((prev_batch_key, _)) = batches_for_author
            .range((Bound::Unbounded, Bound::Excluded(batch_sort_key.clone())))
            .next_back()
        {
            if prev_batch_key.gas_bucket_start() == batch_sort_key.gas_bucket_start() {
                counters::PROOF_MANAGER_OUT_OF_ORDER_PROOF_INSERTION
                    .with_label_values(&[author.short_str().as_str()])
                    .inc();
            }
        }

        self.expirations.add_item(batch_sort_key, expiration);

        // If we are here, then proof is added for the first time. Otherwise, we will
        // return early. We only count when proof is added for the first time and txn
        // summary exists.
        if let Some(txn_summaries) = self
            .items
            .get(&batch_key)
            .and_then(|item| item.txn_summaries.as_ref())
        {
            for txn_summary in txn_summaries {
                *self
                    .txn_summary_num_occurrences
                    .entry(*txn_summary)
                    .or_insert(0) += 1;
            }
        }

        match self.items.entry(batch_key) {
            Entry::Occupied(mut entry) => {
                let item = entry.get_mut();
                item.proof = Some(proof);
                item.proof_insertion_time = Some(Instant::now());
            },
            Entry::Vacant(entry) => {
                entry.insert(QueueItem {
                    info: proof.info().clone(),
                    proof: Some(proof),
                    proof_insertion_time: Some(Instant::now()),
                    txn_summaries: None,
                });
            },
        }

        if author == self.my_peer_id {
            counters::inc_local_pos_count(bucket);
        } else {
            counters::inc_remote_pos_count(bucket);
        }
        self.inc_remaining_proofs(&author, num_txns);

        sample!(
            SampleRate::Duration(Duration::from_millis(500)),
            self.gc_expired_batch_summaries_without_proofs()
        );
    }
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L258-320)
```rust
    pub fn insert_batches(
        &mut self,
        batches_with_txn_summaries: Vec<(BatchInfoExt, Vec<TxnSummaryWithExpiration>)>,
    ) {
        let start = Instant::now();

        for (batch_info, txn_summaries) in batches_with_txn_summaries.into_iter() {
            let batch_sort_key = BatchSortKey::from_info(&batch_info);
            let batch_key = BatchKey::from_info(&batch_info);

            // If the batch is either committed or the txn summary already exists, skip
            // inserting this batch.
            if self
                .items
                .get(&batch_key)
                .is_some_and(|item| item.is_committed() || item.txn_summaries.is_some())
            {
                continue;
            }

            self.author_to_batches
                .entry(batch_info.author())
                .or_default()
                .insert(batch_sort_key.clone(), batch_info.clone());
            self.expirations
                .add_item(batch_sort_key, batch_info.expiration());

            // We only count txn summaries first time it is added to the queue
            // and only if the proof already exists.
            if self
                .items
                .get(&batch_key)
                .is_some_and(|item| item.proof.is_some())
            {
                for txn_summary in &txn_summaries {
                    *self
                        .txn_summary_num_occurrences
                        .entry(*txn_summary)
                        .or_insert(0) += 1;
                }
            }

            match self.items.entry(batch_key) {
                Entry::Occupied(mut entry) => {
                    entry.get_mut().txn_summaries = Some(txn_summaries);
                },
                Entry::Vacant(entry) => {
                    entry.insert(QueueItem {
                        info: batch_info,
                        proof: None,
                        proof_insertion_time: None,
                        txn_summaries: Some(txn_summaries),
                    });
                },
            }
        }

        sample!(
            SampleRate::Duration(Duration::from_millis(500)),
            self.gc_expired_batch_summaries_without_proofs()
        );
        counters::PROOF_QUEUE_ADD_BATCH_SUMMARIES_DURATION.observe_duration(start.elapsed());
    }
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L729-729)
```rust
        let expired = self.expirations.expire(block_timestamp);
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L732-765)
```rust
            if let Some(mut queue) = self.author_to_batches.remove(&key.author()) {
                if let Some(batch) = queue.remove(key) {
                    let item = self
                        .items
                        .get(&key.batch_key)
                        .expect("Entry for unexpired batch must exist");
                    if item.proof.is_some() {
                        // not committed proof that is expired
                        num_expired_but_not_committed += 1;
                        counters::GAP_BETWEEN_BATCH_EXPIRATION_AND_CURRENT_TIME_WHEN_COMMIT
                            .observe((block_timestamp - batch.expiration()) as f64);
                        if let Some(ref txn_summaries) = item.txn_summaries {
                            for txn_summary in txn_summaries {
                                if let Some(count) =
                                    self.txn_summary_num_occurrences.get_mut(txn_summary)
                                {
                                    *count -= 1;
                                    if *count == 0 {
                                        self.txn_summary_num_occurrences.remove(txn_summary);
                                    }
                                };
                            }
                        }
                        self.dec_remaining_proofs(&batch.author(), batch.num_txns());
                        counters::GARBAGE_COLLECTED_IN_PROOF_QUEUE_COUNTER
                            .with_label_values(&["expired_proof"])
                            .inc();
                    }
                    claims::assert_some!(self.items.remove(&key.batch_key));
                }
                if !queue.is_empty() {
                    self.author_to_batches.insert(key.author(), queue);
                }
            }
```
