# Audit Report

## Title
RocksDB Write Stalls Due to Insufficient max_total_wal_size Can Delay Consensus Block Commits

## Summary
The default `max_total_wal_size` configuration of 1GB in RocksDB is insufficient for high-throughput scenarios, causing write stalls that block storage operations. This directly delays consensus block commits through the `pre_commit_ledger()` and `commit_ledger()` operations, degrading validator node performance and potentially impacting network liveness.

## Finding Description

The vulnerability exists in the RocksDB configuration applied to AptosDB storage layer. The `gen_rocksdb_options()` function sets `max_total_wal_size` to 1GB by default: [1](#0-0) 

When write throughput is high (e.g., sustained >5K TPS with large transactions), the Write-Ahead Log (WAL) can grow faster than the background flush jobs (limited to 4 by default) can persist memtables to SST files: [2](#0-1) 

When the total WAL size exceeds this limit, RocksDB enters write stall mode to force memtable flushes. This causes all `write_opt()` calls to block: [3](#0-2) 

The write stalls propagate through the storage stack to the consensus pipeline. During block commitment, the `pre_commit()` phase calls `executor.pre_commit_block()`: [4](#0-3) 

And the `commit_ledger()` phase calls `executor.commit_ledger()`: [5](#0-4) 

Both operations are wrapped in `spawn_blocking` and will hang during RocksDB write stalls. This blocks the entire consensus pipeline, preventing new blocks from being committed.

**Attack Path:**
1. Attacker sends high volume of transactions (legitimate or spam)
2. Multiple RocksDB instances (ledger_db, state_kv_db, state_merkle_db) all write simultaneously
3. WAL files accumulate faster than 4 background jobs can flush them
4. Total WAL size exceeds 1GB limit
5. RocksDB triggers write stall
6. All storage write operations block
7. `pre_commit_ledger()` and `commit_ledger()` operations block
8. Consensus pipeline stalls at pre-commit or commit phases
9. Block commits delayed, consensus throughput drops
10. Network-wide performance degradation as multiple validators experience the same issue

The system does monitor write stalls through RocksDB properties: [6](#0-5) 

However, monitoring alone does not prevent or mitigate the issueâ€”it only allows detection after the problem has already impacted consensus.

## Impact Explanation

This vulnerability qualifies as **HIGH severity** under the Aptos Bug Bounty program for "Validator node slowdowns" (up to $50,000). The impact includes:

1. **Consensus Liveness Degradation**: Block commits are delayed, reducing network throughput
2. **Validator Performance Impact**: Affected validators cannot keep up with consensus rounds
3. **Network-Wide Effects**: If multiple validators hit this limit simultaneously during peak load, the entire network experiences slowdowns
4. **Denial of Service Vector**: Malicious actors can deliberately trigger this by flooding the network with transactions

If the write stall persists long enough to completely halt block commits network-wide, this could escalate to **CRITICAL severity** under "Total loss of liveness/network availability."

## Likelihood Explanation

**Likelihood: MEDIUM to HIGH**

This issue is likely to occur because:

1. **Natural Network Growth**: As Aptos adoption increases, sustained transaction volumes above 5K TPS become common
2. **Peak Load Scenarios**: During NFT mints, airdrops, or popular dApp launches, transaction spikes can easily exceed capacity
3. **Multiple Database Instances**: Aptos uses separate RocksDB instances (ledger_db, state_kv_db, state_merkle_db), each with the 1GB limit, increasing the probability that at least one hits the threshold
4. **Limited Background Jobs**: With only 4 background jobs by default, the system may not keep up with flush requirements under stress
5. **No Dynamic Adjustment**: The configuration is static and doesn't adapt to load conditions

The issue is more likely during:
- Mainnet/testnet peak usage periods
- Spam attack scenarios
- State sync operations that generate high write volumes
- Epoch transitions with large state changes

## Recommendation

**Immediate Fixes:**

1. **Increase Default max_total_wal_size**: Set to at least 8GB-16GB to provide adequate buffer:

```rust
// In config/src/config/storage_config.rs
impl Default for RocksdbConfig {
    fn default() -> Self {
        Self {
            // ... other fields ...
            // Increase from 1GB to 8GB for high-throughput scenarios
            max_total_wal_size: 8u64 << 30,  // 8GB instead of 1GB
            // Increase background jobs for better flush/compaction parallelism
            max_background_jobs: 16,  // 16 instead of 4
            // ... other fields ...
        }
    }
}
```

2. **Add Dynamic Monitoring and Alerting**: Implement automatic detection and alerting when approaching WAL limits:

```rust
// Add to rocksdb_property_reporter.rs monitoring
const WAL_SIZE_WARNING_THRESHOLD: f64 = 0.8; // 80% of max

// Check if approaching limit and log warning
if wal_size > (max_total_wal_size as f64 * WAL_SIZE_WARNING_THRESHOLD) {
    warn!("WAL size approaching limit: {} / {}", wal_size, max_total_wal_size);
}
```

3. **Document Safe Configuration**: Add clear documentation on tuning these parameters for different deployment sizes:

```yaml
# For high-throughput validators (>10K TPS):
max_total_wal_size: 16GB
max_background_jobs: 32

# For medium validators (5K-10K TPS):
max_total_wal_size: 8GB
max_background_jobs: 16

# For low-throughput validators (<5K TPS):
max_total_wal_size: 4GB
max_background_jobs: 8
```

**Long-term Solutions:**

1. Implement dynamic WAL size adjustment based on measured write throughput
2. Add circuit breakers that pause non-critical operations when approaching limits
3. Separate consensus-critical writes from state sync writes using different RocksDB instances with independent limits
4. Consider using RocksDB's `max_write_buffer_number` and related tuning parameters for better write amplification control

## Proof of Concept

**Reproduction Steps:**

1. Deploy a test validator node with default configuration (1GB `max_total_wal_size`, 4 `max_background_jobs`)

2. Generate sustained high transaction load:

```rust
// High TPS load generator test
#[tokio::test]
async fn test_wal_size_write_stall() {
    let validator = start_test_validator_with_default_config().await;
    
    // Generate 10K TPS for 300 seconds with 1KB transactions
    // This generates ~3GB of data, exceeding the 1GB WAL limit
    let txn_generator = TransactionGenerator::new(10_000, 1024);
    
    // Monitor consensus commit latency
    let mut latency_monitor = LatencyMonitor::new();
    
    for _ in 0..300 {
        // Submit 10K transactions per second
        let txns = txn_generator.generate_batch(10_000);
        validator.submit_transactions(txns).await;
        
        // Measure time to commit blocks
        let commit_latency = latency_monitor.measure_commit_latency().await;
        
        // After ~100 seconds, expect latency to spike as WAL exceeds 1GB
        if commit_latency > Duration::from_secs(5) {
            // Check RocksDB write stall metrics
            let is_write_stopped = validator.get_rocksdb_property(
                "rocksdb.is-write-stopped"
            ).await;
            
            assert_eq!(is_write_stopped, 1, "Write stall detected");
            return; // Test confirms vulnerability
        }
        
        tokio::time::sleep(Duration::from_secs(1)).await;
    }
    
    panic!("Expected write stall did not occur");
}
```

3. Observe RocksDB metrics showing write stalls:
   - `rocksdb.is-write-stopped` becomes 1
   - `rocksdb.actual-delayed-write-rate` shows throttling
   - Consensus commit latency increases dramatically

4. Monitor consensus pipeline logs showing blocked `pre_commit` and `commit_ledger` operations

5. Verify network throughput degradation as blocks take longer to commit

**Expected Results:**
- After ~100-200 seconds of sustained 10K TPS, WAL size exceeds 1GB
- RocksDB enters write stall mode
- Block commit latency increases from milliseconds to seconds
- Consensus throughput drops significantly
- System recovers once load decreases and WAL is flushed

## Notes

This vulnerability represents a real security concern rather than just a performance optimization issue because:

1. **Consensus Impact**: It directly affects the consensus layer's ability to commit blocks, violating liveness guarantees
2. **External Trigger**: Can be triggered by unprivileged actors (transaction senders) without validator access
3. **Systemic Risk**: Affects multiple validators simultaneously during network-wide high load
4. **No Automatic Mitigation**: The system has no built-in mechanism to prevent or recover from this condition

The default configuration values were likely chosen for resource-constrained environments but are inadequate for production mainnet validators handling high throughput. The 1GB limit is particularly problematic given that Aptos uses multiple separate RocksDB instances, each with this limit, and all writing concurrently during block commits.

While operators can manually tune these values, the hardcoded defaults in the codebase create a security vulnerability for validators using standard configurations, especially during peak network usage or attack scenarios.

### Citations

**File:** config/src/config/storage_config.rs (L170-172)
```rust
            // For now we set the max total WAL size to be 1G. This config can be useful when column
            // families are updated at non-uniform frequencies.
            max_total_wal_size: 1u64 << 30,
```

**File:** config/src/config/storage_config.rs (L173-174)
```rust
            // This includes jobs for flush and compaction.
            max_background_jobs: 4,
```

**File:** storage/schemadb/src/lib.rs (L289-303)
```rust
    fn write_schemas_inner(&self, batch: impl IntoRawBatch, option: &WriteOptions) -> DbResult<()> {
        let labels = [self.name.as_str()];
        let _timer = APTOS_SCHEMADB_BATCH_COMMIT_LATENCY_SECONDS.timer_with(&labels);

        let raw_batch = batch.into_raw_batch(self)?;

        let serialized_size = raw_batch.inner.size_in_bytes();
        self.inner
            .write_opt(raw_batch.inner, option)
            .into_db_res()?;

        raw_batch.stats.commit();
        APTOS_SCHEMADB_BATCH_COMMIT_BYTES.observe_with(&[&self.name], serialized_size as f64);

        Ok(())
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L1067-1074)
```rust
        tokio::task::spawn_blocking(move || {
            executor
                .pre_commit_block(block.id())
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
        Ok(compute_result)
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L1098-1104)
```rust
        tokio::task::spawn_blocking(move || {
            executor
                .commit_ledger(ledger_info_with_sigs_clone)
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
```

**File:** storage/aptosdb/src/rocksdb_property_reporter.rs (L61-62)
```rust
        "rocksdb.actual-delayed-write-rate",
        "rocksdb.is-write-stopped",
```
