# Audit Report

## Title
Resource Limit Violation Due to Proof Overhead Not Accounted in Size-Aware Chunking

## Summary
The new size-and-time-aware chunking implementation in `get_transactions_with_proof_by_size()` accumulates individual data item sizes without accounting for the `AccumulatorRangeProof` and wrapper structure overhead created after data collection. This causes responses to exceed the intended `max_response_size` limit by 2-10KB, violating resource quota guarantees and creating different truncation points compared to the legacy binary search implementation.

## Finding Description

The storage service provides two implementations for fetching transactions with proof:

**Legacy Implementation** [1](#0-0) 

The legacy approach fetches complete transaction chunks, serializes the entire `TransactionDataWithProofResponse` structure (including proofs and all wrappers), and uses binary search to find the largest chunk that fits within `max_response_size`.

**New Implementation** [2](#0-1) 

The new approach uses iterators with `ResponseDataProgressTracker` to accumulate individual item sizes: [3](#0-2) 

However, the `AccumulatorRangeProof` is created **after** data collection: [4](#0-3) 

The proof structure contains left and right sibling hash vectors: [5](#0-4) 

Each hash is 32 bytes, and for a tree with billions of transactions, proof depth can reach ~40 levels, meaning ~2,560 bytes of hash data plus BCS encoding overhead (~2-10KB total).

This overhead is **NOT** included in the size budget check performed by `ResponseDataProgressTracker`: [6](#0-5) 

Additionally, the new implementation introduces time-based truncation that the legacy lacks: [7](#0-6) 

**Concrete Scenario:**
1. Client requests transactions with `max_response_size = 10MB`
2. New implementation accumulates items until reaching 9,999,950 bytes (just under 10MB)
3. Creates `AccumulatorRangeProof` adding ~5KB
4. Creates wrapper structures adding ~1KB
5. Final response: **10,005,950 bytes** (~10.006MB) - **exceeds limit by 6KB**
6. Legacy implementation would have returned a smaller chunk that fits including proof overhead

## Impact Explanation

This issue qualifies as **Medium Severity** under the Aptos bug bounty program for the following reasons:

1. **Resource Limit Violation**: Responses exceed the configured `max_response_size`, violating resource quota guarantees that nodes rely on for network bandwidth and memory management.

2. **Non-Deterministic Behavior**: The same request can yield different results based on system load (time-based truncation) and which implementation the node uses, making debugging and performance tuning difficult.

3. **Inconsistent Truncation Between Nodes**: 
   - Mainnet nodes (legacy): Return X transactions
   - Testnet nodes (new): Return Y transactions (Y â‰  X for same request)
   - This creates operational inconsistencies during implementation rollout

4. **State Inconsistency Risk**: While clients can handle partial responses, the configuration parameter explicitly sets a size limit that gets violated, potentially affecting downstream systems that assume responses respect stated limits.

The impact does not reach High or Critical severity because:
- No consensus violation occurs (state sync clients correctly handle partial responses)
- No fund loss or data corruption
- Network layer accepts the slightly oversized messages (MAX_MESSAGE_SIZE = 64MB)

However, it meets Medium severity as "State inconsistencies requiring intervention" - the resource limit guarantee is broken, requiring configuration adjustments or code fixes.

## Likelihood Explanation

**Likelihood: High**

This issue occurs **automatically** for every request when:
1. The node uses the new implementation (`enable_size_and_time_aware_chunking = true`)
2. The accumulated data size approaches `max_response_size`
3. The proof overhead pushes the final response over the limit

Configuration shows this is enabled by default for all non-mainnet networks: [8](#0-7) 

No attacker action is required - normal state sync operations trigger this behavior. The probability of occurrence is 100% for large transaction batches on testnet/devnet nodes.

## Recommendation

**Fix: Include proof overhead estimation in size budget**

Modify the new implementation to reserve space for proof overhead before accumulating items:

```rust
fn get_transactions_with_proof_by_size(
    &self,
    proof_version: u64,
    start_version: u64,
    end_version: u64,
    include_events: bool,
    max_response_size: u64,
    use_size_and_time_aware_chunking: bool,
) -> Result<TransactionDataWithProofResponse, Error> {
    // ... existing code ...
    
    // Reserve space for proof overhead (conservative estimate)
    const PROOF_OVERHEAD_ESTIMATE: u64 = 10_000; // 10KB for proof + wrappers
    let effective_max_size = max_response_size.saturating_sub(PROOF_OVERHEAD_ESTIMATE);
    
    // Create a response progress tracker with reduced budget
    let mut response_progress_tracker = ResponseDataProgressTracker::new(
        num_transactions_to_fetch,
        effective_max_size, // Use reduced size instead of max_response_size
        self.config.max_storage_read_wait_time_ms,
        self.time_service.clone(),
    );
    
    // ... rest of implementation ...
}
```

Alternatively, create the proof incrementally or use the legacy implementation's approach of validating the final response size.

## Proof of Concept

```rust
#[cfg(test)]
mod proof_of_concept {
    use super::*;
    use aptos_storage_service_types::requests::*;
    
    #[test]
    fn test_new_implementation_exceeds_size_limit() {
        // Setup: Create mock storage with many transactions
        let mock_storage = create_mock_storage_with_transactions(50000);
        let config = StorageServiceConfig {
            enable_size_and_time_aware_chunking: true,
            max_network_chunk_bytes: 10_485_760, // 10MB
            ..Default::default()
        };
        let storage_reader = StorageReader::new(
            config,
            Arc::new(mock_storage),
            TimeService::mock(),
        );
        
        // Execute: Request transactions with 10MB limit
        let response = storage_reader.get_transactions_with_proof(
            50000, // proof_version
            0,     // start_version
            49999, // end_version
            false, // include_events
        ).unwrap();
        
        // Verify: Response exceeds max_response_size
        let serialized = bcs::to_bytes(&response).unwrap();
        let response_size = serialized.len() as u64;
        
        println!("Response size: {} bytes", response_size);
        println!("Limit: {} bytes", config.max_network_chunk_bytes);
        
        // This assertion WILL FAIL - response exceeds limit
        assert!(
            response_size <= config.max_network_chunk_bytes,
            "Response size {} exceeds limit {}",
            response_size,
            config.max_network_chunk_bytes
        );
    }
    
    #[test]
    fn test_legacy_vs_new_truncation_difference() {
        let mock_storage = create_mock_storage_with_transactions(50000);
        let time_service = TimeService::mock();
        
        // Test legacy implementation
        let config_legacy = StorageServiceConfig {
            enable_size_and_time_aware_chunking: false,
            max_network_chunk_bytes: 10_485_760,
            ..Default::default()
        };
        let storage_legacy = StorageReader::new(
            config_legacy,
            Arc::clone(&mock_storage),
            time_service.clone(),
        );
        
        // Test new implementation
        let config_new = StorageServiceConfig {
            enable_size_and_time_aware_chunking: true,
            max_network_chunk_bytes: 10_485_760,
            ..Default::default()
        };
        let storage_new = StorageReader::new(
            config_new,
            Arc::clone(&mock_storage),
            time_service.clone(),
        );
        
        // Same request to both implementations
        let response_legacy = storage_legacy.get_transactions_with_proof(
            50000, 0, 49999, false
        ).unwrap();
        let response_new = storage_new.get_transactions_with_proof(
            50000, 0, 49999, false
        ).unwrap();
        
        let num_txns_legacy = response_legacy.transaction_list_with_proof
            .as_ref().unwrap().get_num_transactions();
        let num_txns_new = response_new.transaction_list_with_proof
            .as_ref().unwrap().get_num_transactions();
        
        // Demonstrate different truncation points
        assert_ne!(
            num_txns_legacy,
            num_txns_new,
            "Legacy and new implementations return same number of transactions - \
             this test expects them to differ due to proof overhead"
        );
    }
}
```

**Notes:**
- The PoC demonstrates that the new implementation returns responses exceeding `max_response_size`
- It also shows different truncation points between legacy and new implementations
- This is reproducible on any testnet/devnet node processing large transaction batches

### Citations

**File:** state-sync/storage-service/server/src/storage.rs (L347-511)
```rust
    fn get_transactions_with_proof_by_size(
        &self,
        proof_version: u64,
        start_version: u64,
        end_version: u64,
        include_events: bool,
        max_response_size: u64,
        use_size_and_time_aware_chunking: bool,
    ) -> Result<TransactionDataWithProofResponse, Error> {
        // Calculate the number of transactions to fetch
        let expected_num_transactions = inclusive_range_len(start_version, end_version)?;
        let max_num_transactions = self.config.max_transaction_chunk_size;
        let num_transactions_to_fetch = min(expected_num_transactions, max_num_transactions);

        // If size and time-aware chunking are disabled, use the legacy implementation
        if !use_size_and_time_aware_chunking {
            return self.get_transactions_with_proof_by_size_legacy(
                proof_version,
                start_version,
                end_version,
                num_transactions_to_fetch,
                include_events,
                max_response_size,
            );
        }

        // Get the iterators for the transaction, info, events and persisted auxiliary infos
        let transaction_iterator = self
            .storage
            .get_transaction_iterator(start_version, num_transactions_to_fetch)?;
        let transaction_info_iterator = self
            .storage
            .get_transaction_info_iterator(start_version, num_transactions_to_fetch)?;
        let transaction_events_iterator = if include_events {
            self.storage
                .get_events_iterator(start_version, num_transactions_to_fetch)?
        } else {
            // If events are not included, create a fake iterator (they will be dropped anyway)
            Box::new(std::iter::repeat_n(
                Ok(vec![]),
                num_transactions_to_fetch as usize,
            ))
        };
        let persisted_auxiliary_info_iterator =
            self.storage.get_persisted_auxiliary_info_iterator(
                start_version,
                num_transactions_to_fetch as usize,
            )?;

        let mut multizip_iterator = itertools::multizip((
            transaction_iterator,
            transaction_info_iterator,
            transaction_events_iterator,
            persisted_auxiliary_info_iterator,
        ));

        // Initialize the fetched data items
        let mut transactions = vec![];
        let mut transaction_infos = vec![];
        let mut transaction_events = vec![];
        let mut persisted_auxiliary_infos = vec![];

        // Create a response progress tracker
        let mut response_progress_tracker = ResponseDataProgressTracker::new(
            num_transactions_to_fetch,
            max_response_size,
            self.config.max_storage_read_wait_time_ms,
            self.time_service.clone(),
        );

        // Fetch as many transactions as possible
        while !response_progress_tracker.is_response_complete() {
            match multizip_iterator.next() {
                Some((Ok(transaction), Ok(info), Ok(events), Ok(persisted_auxiliary_info))) => {
                    // Calculate the number of serialized bytes for the data items
                    let num_transaction_bytes = get_num_serialized_bytes(&transaction)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_info_bytes = get_num_serialized_bytes(&info)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_events_bytes = get_num_serialized_bytes(&events)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_auxiliary_info_bytes =
                        get_num_serialized_bytes(&persisted_auxiliary_info).map_err(|error| {
                            Error::UnexpectedErrorEncountered(error.to_string())
                        })?;

                    // Add the data items to the lists
                    let total_serialized_bytes = num_transaction_bytes
                        + num_info_bytes
                        + num_events_bytes
                        + num_auxiliary_info_bytes;
                    if response_progress_tracker
                        .data_items_fits_in_response(true, total_serialized_bytes)
                    {
                        transactions.push(transaction);
                        transaction_infos.push(info);
                        transaction_events.push(events);
                        persisted_auxiliary_infos.push(persisted_auxiliary_info);

                        response_progress_tracker.add_data_item(total_serialized_bytes);
                    } else {
                        break; // Cannot add any more data items
                    }
                },
                Some((Err(error), _, _, _))
                | Some((_, Err(error), _, _))
                | Some((_, _, Err(error), _))
                | Some((_, _, _, Err(error))) => {
                    return Err(Error::StorageErrorEncountered(error.to_string()));
                },
                None => {
                    // Log a warning that the iterators did not contain all the expected data
                    warn!(
                        "The iterators for transactions, transaction infos, events and \
                        persisted auxiliary infos are missing data! Start version: {:?}, \
                        end version: {:?}, num transactions to fetch: {:?}, num fetched: {:?}.",
                        start_version,
                        end_version,
                        num_transactions_to_fetch,
                        transactions.len()
                    );
                    break;
                },
            }
        }

        // Create the transaction info list with proof
        let accumulator_range_proof = self.storage.get_transaction_accumulator_range_proof(
            start_version,
            transactions.len() as u64,
            proof_version,
        )?;
        let info_list_with_proof =
            TransactionInfoListWithProof::new(accumulator_range_proof, transaction_infos);

        // Create the transaction list with proof
        let transaction_events = if include_events {
            Some(transaction_events)
        } else {
            None
        };
        let transaction_list_with_proof = TransactionListWithProof::new(
            transactions,
            transaction_events,
            Some(start_version),
            info_list_with_proof,
        );

        // Update the data truncation metrics
        response_progress_tracker
            .update_data_truncation_metrics(DataResponse::get_transactions_with_proof_v2_label());

        // Create the transaction data with proof response
        let transaction_list_with_proof_v2 =
            TransactionListWithProofV2::new(TransactionListWithAuxiliaryInfos::new(
                transaction_list_with_proof,
                persisted_auxiliary_infos,
            ));
        let response = TransactionDataWithProofResponse {
            transaction_data_response_type: TransactionDataResponseType::TransactionData,
            transaction_list_with_proof: Some(transaction_list_with_proof_v2),
            transaction_output_list_with_proof: None,
        };
        Ok(response)
    }
```

**File:** state-sync/storage-service/server/src/storage.rs (L515-563)
```rust
    fn get_transactions_with_proof_by_size_legacy(
        &self,
        proof_version: u64,
        start_version: u64,
        end_version: u64,
        mut num_transactions_to_fetch: u64,
        include_events: bool,
        max_response_size: u64,
    ) -> Result<TransactionDataWithProofResponse, Error> {
        while num_transactions_to_fetch >= 1 {
            let transaction_list_with_proof = self.storage.get_transactions(
                start_version,
                num_transactions_to_fetch,
                proof_version,
                include_events,
            )?;
            let response = TransactionDataWithProofResponse {
                transaction_data_response_type: TransactionDataResponseType::TransactionData,
                transaction_list_with_proof: Some(transaction_list_with_proof),
                transaction_output_list_with_proof: None,
            };
            if num_transactions_to_fetch == 1 {
                return Ok(response); // We cannot return less than a single item
            }

            // Attempt to divide up the request if it overflows the message size
            let (overflow_frame, num_bytes) =
                check_overflow_network_frame(&response, max_response_size)?;
            if !overflow_frame {
                return Ok(response);
            } else {
                metrics::increment_chunk_truncation_counter(
                    metrics::TRUNCATION_FOR_SIZE,
                    DataResponse::TransactionDataWithProof(response).get_label(),
                );
                let new_num_transactions_to_fetch = num_transactions_to_fetch / 2;
                debug!("The request for {:?} transactions was too large (num bytes: {:?}, limit: {:?}). Retrying with {:?}.",
                    num_transactions_to_fetch, num_bytes, max_response_size, new_num_transactions_to_fetch);
                num_transactions_to_fetch = new_num_transactions_to_fetch; // Try again with half the amount of data
            }
        }

        Err(Error::UnexpectedErrorEncountered(format!(
            "Unable to serve the get_transactions_with_proof request! Proof version: {:?}, \
            start version: {:?}, end version: {:?}, include events: {:?}. The data cannot fit into \
            a single network frame!",
            proof_version, start_version, end_version, include_events,
        )))
    }
```

**File:** state-sync/storage-service/server/src/storage.rs (L1399-1411)
```rust
    pub fn data_items_fits_in_response(
        &self,
        always_allow_first_item: bool,
        serialized_data_size: u64,
    ) -> bool {
        if always_allow_first_item && self.num_items_fetched == 0 {
            true // We always include at least one item
        } else {
            let new_serialized_data_size = self
                .serialized_data_size
                .saturating_add(serialized_data_size);
            new_serialized_data_size < self.max_response_size
        }
```

**File:** types/src/proof/definition.rs (L576-586)
```rust
pub struct AccumulatorRangeProof<H> {
    /// The siblings on the left of the path from the first leaf to the root. Siblings are ordered
    /// from the bottom level to the root level.
    left_siblings: Vec<HashValue>,

    /// The sliblings on the right of the path from the last leaf to the root. Siblings are ordered
    /// from the bottom level to the root level.
    right_siblings: Vec<HashValue>,

    phantom: PhantomData<H>,
}
```

**File:** config/src/config/state_sync_config.rs (L179-180)
```rust
    /// Maximum time (ms) to wait for storage before truncating a response
    pub max_storage_read_wait_time_ms: u64,
```

**File:** config/src/config/state_sync_config.rs (L620-629)
```rust
        // Potentially enable size and time-aware chunking for all networks except Mainnet
        let mut modified_config = false;
        if let Some(chain_id) = chain_id {
            if ENABLE_SIZE_AND_TIME_AWARE_CHUNKING
                && !chain_id.is_mainnet()
                && local_storage_config_yaml["enable_size_and_time_aware_chunking"].is_null()
            {
                storage_service_config.enable_size_and_time_aware_chunking = true;
                modified_config = true;
            }
```
