# Audit Report

## Title
Validator Set Ordering Mismatch Between DKG Transcript and Epoch State Causes Honest Randomness Share Rejection

## Summary
The `pk_shares` vector in `RandKeys` can have inconsistent ordering across validators due to a mismatch between the validator set used during DKG transcript creation and the actual validator set at epoch transition. This causes honest validators' randomness shares to be rejected when their `validator_index` differs between the DKG target set and the live epoch state.

## Finding Description

The vulnerability arises from a timing discrepancy in how validator sets are computed for randomness generation:

**Phase 1: DKG Setup (Epoch X)** [1](#0-0) 

The system calls `stake::next_validator_consensus_infos()` to predict the validator set for epoch X+1 and uses this as the `target_validator_set` for DKG. [2](#0-1) 

This function computes the next epoch's validators by iterating through current active and pending_active validators, filtering by minimum stake, and assigning sequential `validator_index` values (lines 1541, 1554).

**Phase 2: DKG Transcript Creation** [3](#0-2) 

The DKG public parameters are built using `target_validator_consensus_infos_cloned()` which preserves the ordering from Phase 1, creating a mapping where `Player{id: i}` corresponds to the validator at position `i` in the target set.

**Phase 3: Epoch Transition (Entering Epoch X+1)** [4](#0-3) 

When the epoch actually transitions, `on_new_epoch()` merges `active_validators` and `pending_active` (line 1364), then re-filters by minimum stake and assigns NEW sequential `validator_index` values (lines 1426-1428). If validator stakes have changed between DKG start and epoch transition, the filtering produces a different validator set with different index assignments.

**Phase 4: Loading DKG Keys in Epoch X+1** [5](#0-4) 

The system constructs `pk_shares` by iterating from `0..new_epoch_state.verifier.len()` and fetching `get_public_key_share()` for each `Player{id}`. However, `new_epoch_state.verifier` reflects the ACTUAL validator set from `on_new_epoch()`, not the DKG target set.

**The Verification Failure** [6](#0-5) 

When verifying a share, the code looks up the validator's index using `address_to_validator_index()` from the actual epoch state (lines 58-62), then retrieves the public key from `certified_apks[index]`. But this index corresponds to the validator's position in the ACTUAL validator set, while `pk_shares[index]` contains the public key share for `Player{id: index}` from the DKG transcript, which corresponds to a potentially DIFFERENT validator from the target set.

**Concrete Attack Scenario:**
1. Epoch X: Active validators [A, B], Pending active [C, D]
2. DKG starts: Target set computed as [A, B, C, D] with indices [0, 1, 2, 3]
3. DKG transcript: `Player{0}→A, Player{1}→B, Player{2}→C, Player{3}→D`
4. Between DKG and epoch transition: Validator B's stake drops below minimum
5. Epoch transition: Actual validator set becomes [A, C, D] with indices [0, 1, 2]
6. Epoch X+1 key loading:
   - `pk_shares[0] = PK_share(Player{0})` = A's key ✓
   - `pk_shares[1] = PK_share(Player{1})` = B's key ✗
   - `pk_shares[2] = PK_share(Player{2})` = C's key ✗
7. When C submits a share:
   - `index = address_to_validator_index[C] = 1` (C is at position 1)
   - Uses `pk_shares[1]` = B's public key
   - Verification fails because C's share was created with C's secret key, not B's

## Impact Explanation

**Severity: High**

This vulnerability breaks the **Deterministic Execution** invariant as different validators could reject different sets of honest randomness shares, and the **Consensus Safety** invariant by potentially causing randomness generation failures.

Impact categories:
- **Randomness Generation Failure**: If enough validators are affected by the ordering mismatch, the system cannot aggregate sufficient shares to generate on-chain randomness
- **Consensus Liveness Impact**: Failed randomness generation can block consensus progress if randomness is required for block production
- **Validator Reputation Damage**: Honest validators' shares are incorrectly rejected, making them appear malicious or faulty

The vulnerability affects all validators whose `validator_index` differs between the DKG target set and the actual epoch state. In scenarios with high validator churn or stake volatility, multiple validators could be simultaneously affected.

## Likelihood Explanation

**Likelihood: Medium-High**

The vulnerability triggers whenever:
1. The validator set changes between DKG start (`reconfiguration_with_dkg::try_start()`) and epoch transition (`stake::on_new_epoch()`)
2. Changes include validators crossing the minimum stake threshold, changing rankings, or joining/leaving

Common scenarios that trigger this:
- Validators staking or unstaking during DKG
- Reward distribution causing stake changes
- Penalties reducing validator stakes below minimum
- Natural stake pool operations (deposits, withdrawals)
- Validator configuration changes

There are no visible guards in the codebase preventing validator set modifications during active DKG sessions. [7](#0-6) 

The reconfiguration state tracking doesn't enforce stake freezing, allowing the validator set to drift between DKG prediction and actual transition.

## Recommendation

**Fix: Persist and enforce the DKG target validator set through epoch transition**

1. **Store the canonical validator set ordering in DKGSessionMetadata:**
   - When `dkg::start()` is called, capture the exact `target_validator_set` ordering
   - Persist this in the DKG session state
   
2. **Use the persisted ordering when loading keys:**
   - Modify `epoch_manager.rs` to use the validator ordering from `dkg_session.metadata.target_validator_set` instead of `new_epoch_state.verifier`
   - Build `pk_shares` using the DKG target set ordering
   - Create a mapping from actual validator indices to DKG target indices

3. **Add validation that the validator sets match:**
   - Before loading DKG keys, verify that the actual validator set (addresses) matches the target set
   - If there's a mismatch, reject the DKG transcript and require a new DKG round
   - This ensures the system fails safely rather than silently using mismatched keys

4. **Alternative: Freeze validator set during DKG:**
   - Block validator set modifications when `reconfiguration_state::is_in_progress()` returns true
   - Reject stake operations that would change the validator set composition
   - This prevents the validator set from drifting during DKG

**Code Fix Example (Conceptual):**

```rust
// In epoch_manager.rs try_get_rand_config_for_new_epoch():
let target_validators = dkg_pub_params.session_metadata.target_validator_set;
let actual_validators = new_epoch_state.verifier.get_ordered_account_addresses();

// Validate sets match
ensure!(
    target_validators.len() == actual_validators.len(),
    "Validator set size mismatch between DKG target and actual epoch state"
);

// Build pk_shares using target set ordering
let pk_shares = target_validators.iter().enumerate()
    .map(|(id, _validator_info)| {
        transcript.main.get_public_key_share(&dkg_pub_params.pvss_config.wconfig, &Player { id })
    })
    .collect::<Vec<_>>();

// Create index mapping for verification
let target_to_actual_index: HashMap<AccountAddress, usize> = ...;
```

## Proof of Concept

This vulnerability can be demonstrated through a scenario-based test:

```rust
// Pseudocode PoC demonstrating the vulnerability

#[test]
fn test_validator_set_mismatch_rejects_honest_shares() {
    // Setup: Epoch X with 4 validators
    let validators = vec![
        validator_a, // stake = 100 (active)
        validator_b, // stake = 100 (active) 
        validator_c, // stake = 100 (pending_active)
        validator_d, // stake = 100 (pending_active)
    ];
    
    // Phase 1: Start DKG with predicted validator set [A, B, C, D]
    let target_set = stake::next_validator_consensus_infos(); 
    // Returns [A:idx=0, B:idx=1, C:idx=2, D:idx=3]
    
    dkg::start(epoch_x, ..., target_set);
    
    // DKG runs, creates transcript with Player{0}→A, Player{1}→B, Player{2}→C, Player{3}→D
    
    // Phase 2: Validator B's stake drops below minimum
    reduce_stake(validator_b, amount = 60); // Now below minimum of 100
    
    // Phase 3: Epoch transition
    stake::on_new_epoch();
    // Actual validator set: [A:idx=0, C:idx=1, D:idx=2]
    // B is excluded due to insufficient stake
    
    // Phase 4: Load DKG keys in epoch X+1
    let pk_shares = (0..3).map(|id| 
        transcript.get_public_key_share(Player{id})
    ).collect();
    // pk_shares[0] = A's key
    // pk_shares[1] = B's key (but B is not in validator set!)
    // pk_shares[2] = C's key
    
    // Phase 5: Validator C tries to submit a randomness share
    let c_share = WVUF::create_share(&c_secret_key, metadata);
    let c_index = address_to_validator_index[C]; // = 1
    
    // Verification attempts to use pk_shares[1] = B's key to verify C's share
    let result = WVUF::verify_share(&vuf_pp, &pk_shares[c_index], metadata, &c_share);
    
    // ASSERTION: Verification FAILS even though C is honest
    assert!(result.is_err(), "Honest validator C's share incorrectly rejected");
}
```

The PoC demonstrates that honest validator C's shares are rejected because the system attempts to verify them using validator B's public key due to the index mismatch.

## Notes

This vulnerability is particularly insidious because:
1. It affects honest validators, not malicious ones
2. The failure is silent from the validator's perspective - they submit correct shares that get rejected
3. No single validator has visibility into whether the ordering matches across the network
4. The impact scales with validator churn - higher churn increases probability of mismatches

The root cause is the separation between DKG prediction (`next_validator_consensus_infos()`) and actual state transition (`on_new_epoch()`), combined with the lack of validation that these produce identical results.

### Citations

**File:** aptos-move/framework/aptos-framework/sources/reconfiguration_with_dkg.move (L34-39)
```text
        dkg::start(
            cur_epoch,
            randomness_config::current(),
            stake::cur_validator_consensus_infos(),
            stake::next_validator_consensus_infos(),
        );
```

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L1364-1451)
```text
        append(&mut validator_set.active_validators, &mut validator_set.pending_active);

        // Officially deactivate all pending_inactive validators. They will now no longer receive rewards.
        validator_set.pending_inactive = vector::empty();

        // Update active validator set so that network address/public key change takes effect.
        // Moreover, recalculate the total voting power, and deactivate the validator whose
        // voting power is less than the minimum required stake.
        let next_epoch_validators = vector::empty();
        let (minimum_stake, _) = staking_config::get_required_stake(&config);
        let vlen = vector::length(&validator_set.active_validators);
        let total_voting_power = 0;
        let i = 0;
        while ({
            spec {
                invariant spec_validators_are_initialized(next_epoch_validators);
                invariant i <= vlen;
            };
            i < vlen
        }) {
            let old_validator_info = vector::borrow_mut(&mut validator_set.active_validators, i);
            let pool_address = old_validator_info.addr;
            let validator_config = borrow_global<ValidatorConfig>(pool_address);
            let stake_pool = borrow_global<StakePool>(pool_address);
            let new_validator_info = generate_validator_info(pool_address, stake_pool, *validator_config);

            // A validator needs at least the min stake required to join the validator set.
            if (new_validator_info.voting_power >= minimum_stake) {
                spec {
                    assume total_voting_power + new_validator_info.voting_power <= MAX_U128;
                };
                total_voting_power = total_voting_power + (new_validator_info.voting_power as u128);
                vector::push_back(&mut next_epoch_validators, new_validator_info);
            };
            i = i + 1;
        };

        validator_set.active_validators = next_epoch_validators;
        validator_set.total_voting_power = total_voting_power;
        validator_set.total_joining_power = 0;

        // Update validator indices, reset performance scores, and renew lockups.
        validator_perf.validators = vector::empty();
        let recurring_lockup_duration_secs = staking_config::get_recurring_lockup_duration(&config);
        let vlen = vector::length(&validator_set.active_validators);
        let validator_index = 0;
        while ({
            spec {
                invariant spec_validators_are_initialized(validator_set.active_validators);
                invariant len(validator_set.pending_active) == 0;
                invariant len(validator_set.pending_inactive) == 0;
                invariant 0 <= validator_index && validator_index <= vlen;
                invariant vlen == len(validator_set.active_validators);
                invariant forall i in 0..validator_index:
                    global<ValidatorConfig>(validator_set.active_validators[i].addr).validator_index < validator_index;
                invariant forall i in 0..validator_index:
                    validator_set.active_validators[i].config.validator_index < validator_index;
                invariant len(validator_perf.validators) == validator_index;
            };
            validator_index < vlen
        }) {
            let validator_info = vector::borrow_mut(&mut validator_set.active_validators, validator_index);
            validator_info.config.validator_index = validator_index;
            let validator_config = borrow_global_mut<ValidatorConfig>(validator_info.addr);
            validator_config.validator_index = validator_index;

            vector::push_back(&mut validator_perf.validators, IndividualValidatorPerformance {
                successful_proposals: 0,
                failed_proposals: 0,
            });

            // Automatically renew a validator's lockup for validators that will still be in the validator set in the
            // next epoch.
            let stake_pool = borrow_global_mut<StakePool>(validator_info.addr);
            let now_secs = timestamp::now_seconds();
            let reconfig_start_secs = if (chain_status::is_operating()) {
                get_reconfig_start_time_secs()
            } else {
                now_secs
            };
            if (stake_pool.locked_until_secs <= reconfig_start_secs) {
                spec {
                    assume now_secs + recurring_lockup_duration_secs <= MAX_U64;
                };
                stake_pool.locked_until_secs = now_secs + recurring_lockup_duration_secs;
            };

            validator_index = validator_index + 1;
```

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L1473-1569)
```text
    public fun next_validator_consensus_infos(): vector<ValidatorConsensusInfo> acquires ValidatorSet, ValidatorPerformance, StakePool, ValidatorConfig {
        // Init.
        let cur_validator_set = borrow_global<ValidatorSet>(@aptos_framework);
        let staking_config = staking_config::get();
        let validator_perf = borrow_global<ValidatorPerformance>(@aptos_framework);
        let (minimum_stake, _) = staking_config::get_required_stake(&staking_config);
        let (rewards_rate, rewards_rate_denominator) = staking_config::get_reward_rate(&staking_config);

        // Compute new validator set.
        let new_active_validators = vector[];
        let num_new_actives = 0;
        let candidate_idx = 0;
        let new_total_power = 0;
        let num_cur_actives = vector::length(&cur_validator_set.active_validators);
        let num_cur_pending_actives = vector::length(&cur_validator_set.pending_active);
        spec {
            assume num_cur_actives + num_cur_pending_actives <= MAX_U64;
        };
        let num_candidates = num_cur_actives + num_cur_pending_actives;
        while ({
            spec {
                invariant candidate_idx <= num_candidates;
                invariant spec_validators_are_initialized(new_active_validators);
                invariant len(new_active_validators) == num_new_actives;
                invariant forall i in 0..len(new_active_validators):
                    new_active_validators[i].config.validator_index == i;
                invariant num_new_actives <= candidate_idx;
                invariant spec_validators_are_initialized(new_active_validators);
            };
            candidate_idx < num_candidates
        }) {
            let candidate_in_current_validator_set = candidate_idx < num_cur_actives;
            let candidate = if (candidate_idx < num_cur_actives) {
                vector::borrow(&cur_validator_set.active_validators, candidate_idx)
            } else {
                vector::borrow(&cur_validator_set.pending_active, candidate_idx - num_cur_actives)
            };
            let stake_pool = borrow_global<StakePool>(candidate.addr);
            let cur_active = coin::value(&stake_pool.active);
            let cur_pending_active = coin::value(&stake_pool.pending_active);
            let cur_pending_inactive = coin::value(&stake_pool.pending_inactive);

            let cur_reward = if (candidate_in_current_validator_set && cur_active > 0) {
                spec {
                    assert candidate.config.validator_index < len(validator_perf.validators);
                };
                let cur_perf = vector::borrow(&validator_perf.validators, candidate.config.validator_index);
                spec {
                    assume cur_perf.successful_proposals + cur_perf.failed_proposals <= MAX_U64;
                };
                calculate_rewards_amount(cur_active, cur_perf.successful_proposals, cur_perf.successful_proposals + cur_perf.failed_proposals, rewards_rate, rewards_rate_denominator)
            } else {
                0
            };

            let lockup_expired = get_reconfig_start_time_secs() >= stake_pool.locked_until_secs;
            spec {
                assume cur_active + cur_pending_active + cur_reward <= MAX_U64;
                assume cur_active + cur_pending_inactive + cur_pending_active + cur_reward <= MAX_U64;
            };
            let new_voting_power =
                cur_active
                + if (lockup_expired) { 0 } else { cur_pending_inactive }
                + cur_pending_active
                + cur_reward;

            if (new_voting_power >= minimum_stake) {
                let config = *borrow_global<ValidatorConfig>(candidate.addr);
                config.validator_index = num_new_actives;
                let new_validator_info = ValidatorInfo {
                    addr: candidate.addr,
                    voting_power: new_voting_power,
                    config,
                };

                // Update ValidatorSet.
                spec {
                    assume new_total_power + new_voting_power <= MAX_U128;
                };
                new_total_power = new_total_power + (new_voting_power as u128);
                vector::push_back(&mut new_active_validators, new_validator_info);
                num_new_actives = num_new_actives + 1;

            };
            candidate_idx = candidate_idx + 1;
        };

        let new_validator_set = ValidatorSet {
            consensus_scheme: cur_validator_set.consensus_scheme,
            active_validators: new_active_validators,
            pending_inactive: vector[],
            pending_active: vector[],
            total_voting_power: new_total_power,
            total_joining_power: 0,
        };

        validator_consensus_infos_from_validator_set(&new_validator_set)
```

**File:** types/src/dkg/real_dkg/mod.rs (L211-217)
```rust
        let pvss_config = build_dkg_pvss_config(
            dkg_session_metadata.dealer_epoch,
            secrecy_threshold,
            reconstruct_threshold,
            maybe_fast_path_secrecy_threshold,
            &dkg_session_metadata.target_validator_consensus_infos_cloned(),
        );
```

**File:** consensus/src/epoch_manager.rs (L1080-1126)
```rust
        let pk_shares = (0..new_epoch_state.verifier.len())
            .map(|id| {
                transcript
                    .main
                    .get_public_key_share(&dkg_pub_params.pvss_config.wconfig, &Player { id })
            })
            .collect::<Vec<_>>();

        // Recover existing augmented key pair or generate a new one
        let (augmented_key_pair, fast_augmented_key_pair) = if let Some((_, key_pair)) = self
            .rand_storage
            .get_key_pair_bytes()
            .map_err(NoRandomnessReason::RandDbNotAvailable)?
            .filter(|(epoch, _)| *epoch == new_epoch)
        {
            info!(epoch = new_epoch, "Recovering existing augmented key");
            bcs::from_bytes(&key_pair).map_err(NoRandomnessReason::KeyPairDeserializationError)?
        } else {
            info!(
                epoch = new_epoch_state.epoch,
                "Generating a new augmented key"
            );
            let mut rng =
                StdRng::from_rng(thread_rng()).map_err(NoRandomnessReason::RngCreationError)?;
            let augmented_key_pair = WVUF::augment_key_pair(&vuf_pp, sk.main, pk.main, &mut rng);
            let fast_augmented_key_pair = if fast_randomness_is_enabled {
                if let (Some(sk), Some(pk)) = (sk.fast, pk.fast) {
                    Some(WVUF::augment_key_pair(&vuf_pp, sk, pk, &mut rng))
                } else {
                    None
                }
            } else {
                None
            };
            self.rand_storage
                .save_key_pair_bytes(
                    new_epoch,
                    bcs::to_bytes(&(augmented_key_pair.clone(), fast_augmented_key_pair.clone()))
                        .map_err(NoRandomnessReason::KeyPairSerializationError)?,
                )
                .map_err(NoRandomnessReason::KeyPairPersistError)?;
            (augmented_key_pair, fast_augmented_key_pair)
        };

        let (ask, apk) = augmented_key_pair;

        let keys = RandKeys::new(ask, apk, pk_shares, new_epoch_state.verifier.len());
```

**File:** consensus/src/rand/rand_gen/types.rs (L52-81)
```rust
    fn verify(
        &self,
        rand_config: &RandConfig,
        rand_metadata: &RandMetadata,
        author: &Author,
    ) -> anyhow::Result<()> {
        let index = *rand_config
            .validator
            .address_to_validator_index()
            .get(author)
            .ok_or_else(|| anyhow!("Share::verify failed with unknown author"))?;
        let maybe_apk = &rand_config.keys.certified_apks[index];
        if let Some(apk) = maybe_apk.get() {
            WVUF::verify_share(
                &rand_config.vuf_pp,
                apk,
                bcs::to_bytes(&rand_metadata)
                    .map_err(|e| anyhow!("Serialization failed: {}", e))?
                    .as_slice(),
                &self.share,
            )?;
        } else {
            bail!(
                "[RandShare] No augmented public key for validator id {}, {}",
                index,
                author
            );
        }
        Ok(())
    }
```

**File:** aptos-move/framework/aptos-framework/sources/reconfiguration_state.move (L67-77)
```text
    public(friend) fun on_reconfig_start() acquires State {
        if (exists<State>(@aptos_framework)) {
            let state = borrow_global_mut<State>(@aptos_framework);
            let variant_type_name = *string::bytes(copyable_any::type_name(&state.variant));
            if (variant_type_name == b"0x1::reconfiguration_state::StateInactive") {
                state.variant = copyable_any::pack(StateActive {
                    start_time_secs: timestamp::now_seconds()
                });
            }
        };
    }
```
