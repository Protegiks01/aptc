# Audit Report

## Title
Remote Executor Lacks Message Loss Detection and Recovery, Causing Indefinite Hangs and Process Crashes on Network Partitions

## Summary
The remote executor system in Aptos Core has no mechanism to detect or recover from message loss caused by network partitions. When the coordinator sends execution commands to remote shards, it blocks indefinitely waiting for responses with no timeout. Additionally, gRPC send failures cause the entire process to panic rather than retry. This leads to either indefinite hangs or process crashes, halting block execution and violating liveness guarantees.

## Finding Description

The remote executor system enables distributed block execution across multiple shards. The coordinator (`RemoteExecutorClient`) sends execution commands to remote executor shards and waits for results. However, this system completely lacks message loss handling:

**Issue 1: Indefinite Blocking on Message Loss**

When waiting for execution results from shards, the coordinator uses blocking receive with no timeout: [1](#0-0) 

The `rx.recv().unwrap()` call at line 167 blocks indefinitely. If any shard loses the execution command message due to network partition, crashes, or fails to respond, the coordinator will hang forever waiting for that shard's result. There is no timeout mechanism, no way to detect the message was lost, and no recovery path.

**Issue 2: Panic on Send Failure**

When sending messages via gRPC, the system panics on any send failure: [2](#0-1) 

At line 154, if `simple_msg_exchange` returns an error (due to network partition, connection failure, or other network issues), the entire process panics. The TODO comment at line 150 explicitly acknowledges the missing retry mechanism: "// TODO: Retry with exponential backoff on failures".

**Issue 3: No Recovery Infrastructure**

The message sending path lacks any retry, acknowledgment, or recovery mechanism: [3](#0-2) 

The execution flow at lines 193-206 sends messages to all shards, then immediately calls `get_output_from_shards()` at line 208. There's no sequence numbering, no acknowledgments, and no way to detect or retry lost messages.

**Attack Vector:**

1. Coordinator sends execution commands to N remote shards
2. Network partition occurs, causing message loss to one or more shards
3. Two scenarios:
   - **Scenario A (Send Failure)**: gRPC send fails → process panics → node crashes
   - **Scenario B (Lost Response)**: Message sent but response lost → coordinator hangs indefinitely waiting for response

**Broken Invariants:**
- **Liveness Guarantee**: Block execution cannot proceed, halting consensus
- **Fault Tolerance**: System cannot tolerate transient network failures
- **Resource Limits**: Indefinite blocking violates operational timeouts

## Impact Explanation

This vulnerability meets **High Severity** criteria per Aptos bug bounty guidelines:

1. **Validator Node Slowdowns**: When messages are lost, the coordinator hangs indefinitely, preventing any further block execution. The validator becomes unresponsive.

2. **API Crashes**: When gRPC sends fail, the panic causes the entire process to crash, requiring manual restart.

3. **Significant Protocol Violations**: Block execution is a critical path in the consensus protocol. Halting execution breaks liveness guarantees and can prevent the blockchain from making progress.

The remote executor is used in the sharded block execution path. If it fails, blocks cannot be executed, transactions cannot be committed, and the blockchain halts. This affects all validators using remote execution and impacts the entire network's ability to process transactions.

While this doesn't directly violate consensus safety (different nodes won't commit different states), it severely impacts **liveness** - the blockchain's ability to continue producing blocks.

## Likelihood Explanation

**Likelihood: High**

Network partitions and transient failures are common in distributed systems:

1. **Frequency**: Network issues occur regularly in production distributed systems
2. **Complexity**: No special attack required - normal network failures trigger this
3. **Prerequisites**: None - any validator using remote execution is vulnerable
4. **Detection**: System provides no warning before hanging or crashing

The TODO comment explicitly acknowledges this is a known gap that needs fixing. The vulnerability will manifest during any network instability, which is inevitable in real-world deployments.

## Recommendation

Implement comprehensive message loss detection and recovery:

**1. Add Timeout to Receive Operations**

Replace blocking `recv()` with `recv_timeout()`:

```rust
fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
    trace!("RemoteExecutorClient Waiting for results");
    let mut results = vec![];
    let timeout = Duration::from_secs(30); // Configurable timeout
    
    for rx in self.result_rxs.iter() {
        match rx.recv_timeout(timeout) {
            Ok(received_bytes) => {
                let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes.to_bytes()).unwrap();
                results.push(result.inner?);
            }
            Err(RecvTimeoutError::Timeout) => {
                return Err(VMStatus::Error(
                    StatusCode::REMOTE_EXECUTION_TIMEOUT,
                    Some("Shard failed to respond within timeout".to_string())
                ));
            }
            Err(RecvTimeoutError::Disconnected) => {
                return Err(VMStatus::Error(
                    StatusCode::REMOTE_EXECUTION_ERROR,
                    Some("Shard connection lost".to_string())
                ));
            }
        }
    }
    Ok(results)
}
```

**2. Implement Retry with Exponential Backoff**

In the gRPC client wrapper, replace panic with retry logic using the existing `aptos-retrier` infrastructure: [4](#0-3) 

```rust
pub async fn send_message(
    &mut self,
    sender_addr: SocketAddr,
    message: Message,
    mt: &MessageType,
) -> Result<(), String> {
    let request = tonic::Request::new(NetworkMessage {
        message: message.data,
        message_type: mt.get_type(),
    });
    
    let mut retry_delay = Duration::from_millis(100);
    let max_retries = 5;
    
    for attempt in 0..max_retries {
        match self.remote_channel.simple_msg_exchange(request.clone()).await {
            Ok(_) => return Ok(()),
            Err(e) => {
                if attempt == max_retries - 1 {
                    return Err(format!(
                        "Failed to send message to {} after {} attempts: {}",
                        self.remote_addr, max_retries, e
                    ));
                }
                tokio::time::sleep(retry_delay).await;
                retry_delay *= 2; // Exponential backoff
            }
        }
    }
    unreachable!()
}
```

**3. Add Message Acknowledgment and Sequence Numbers**

Implement message IDs and acknowledgments to detect lost messages and enable retransmission.

## Proof of Concept

```rust
#[test]
fn test_network_partition_causes_hang() {
    use std::net::{IpAddr, Ipv4Addr, SocketAddr};
    use std::sync::Arc;
    use std::thread;
    use std::time::Duration;
    
    // Setup coordinator
    let coordinator_addr = SocketAddr::new(
        IpAddr::V4(Ipv4Addr::LOCALHOST), 
        52200
    );
    
    // Setup remote shard that will "lose" messages (never responds)
    let shard_addr = SocketAddr::new(
        IpAddr::V4(Ipv4Addr::LOCALHOST), 
        52201
    );
    
    // Create remote executor client
    let mut controller = NetworkController::new(
        "test-coordinator".to_string(),
        coordinator_addr,
        5000,
    );
    
    let client = RemoteExecutorClient::<TestStateView>::new(
        vec![shard_addr],
        controller,
        Some(4),
    );
    
    // Simulate execution - this will hang indefinitely
    let handle = thread::spawn(move || {
        let state_view = Arc::new(TestStateView::new());
        let txns = PartitionedTransactions::new(vec![vec![]], vec![]);
        
        // This call will block forever waiting for shard response
        let result = client.execute_block(
            state_view,
            txns,
            4,
            BlockExecutorConfigFromOnchain::default()
        );
        
        result
    });
    
    // Wait to see if it completes (it won't)
    thread::sleep(Duration::from_secs(10));
    
    // The thread is still blocked, demonstrating the hang
    assert!(!handle.is_finished(), "Execution should hang indefinitely");
}

#[test]
fn test_grpc_send_failure_causes_panic() {
    // Setup coordinator with invalid remote address
    let invalid_addr = SocketAddr::new(
        IpAddr::V4(Ipv4Addr::new(192, 0, 2, 1)), // TEST-NET-1 (non-routable)
        52201
    );
    
    let mut controller = NetworkController::new(
        "test-coordinator".to_string(),
        SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 52200),
        5000,
    );
    
    let client = RemoteExecutorClient::<TestStateView>::new(
        vec![invalid_addr],
        controller,
        Some(4),
    );
    
    let state_view = Arc::new(TestStateView::new());
    let txns = PartitionedTransactions::new(vec![vec![]], vec![]);
    
    // This will panic when gRPC send fails
    let result = std::panic::catch_unwind(|| {
        client.execute_block(
            state_view,
            txns,
            4,
            BlockExecutorConfigFromOnchain::default()
        )
    });
    
    assert!(result.is_err(), "Should panic on gRPC send failure");
}
```

## Notes

This vulnerability is explicitly acknowledged in the codebase via the TODO comment but remains unimplemented. The infrastructure for exponential backoff exists in `aptos-retrier` but is not integrated into the remote executor. This represents a critical gap in the system's fault tolerance that must be addressed before production use of sharded remote execution.

### Citations

**File:** execution/executor-service/src/remote_executor_client.rs (L163-172)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        trace!("RemoteExecutorClient Waiting for results");
        let mut results = vec![];
        for rx in self.result_rxs.iter() {
            let received_bytes = rx.recv().unwrap().to_bytes();
            let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
            results.push(result.inner?);
        }
        Ok(results)
    }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L180-212)
```rust
    fn execute_block(
        &self,
        state_view: Arc<S>,
        transactions: PartitionedTransactions,
        concurrency_level_per_shard: usize,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<ShardedExecutionOutput, VMStatus> {
        trace!("RemoteExecutorClient Sending block to shards");
        self.state_view_service.set_state_view(state_view);
        let (sub_blocks, global_txns) = transactions.into();
        if !global_txns.is_empty() {
            panic!("Global transactions are not supported yet");
        }
        for (shard_id, sub_blocks) in sub_blocks.into_iter().enumerate() {
            let senders = self.command_txs.clone();
            let execution_request = RemoteExecutionRequest::ExecuteBlock(ExecuteBlockCommand {
                sub_blocks,
                concurrency_level: concurrency_level_per_shard,
                onchain_config: onchain_config.clone(),
            });

            senders[shard_id]
                .lock()
                .unwrap()
                .send(Message::new(bcs::to_bytes(&execution_request).unwrap()))
                .unwrap();
        }

        let execution_results = self.get_output_from_shards()?;

        self.state_view_service.drop_state_view();
        Ok(ShardedExecutionOutput::new(execution_results, vec![]))
    }
```

**File:** secure/net/src/grpc_network_service/mod.rs (L140-160)
```rust
    pub async fn send_message(
        &mut self,
        sender_addr: SocketAddr,
        message: Message,
        mt: &MessageType,
    ) {
        let request = tonic::Request::new(NetworkMessage {
            message: message.data,
            message_type: mt.get_type(),
        });
        // TODO: Retry with exponential backoff on failures
        match self.remote_channel.simple_msg_exchange(request).await {
            Ok(_) => {},
            Err(e) => {
                panic!(
                    "Error '{}' sending message to {} on node {:?}",
                    e, self.remote_addr, sender_addr
                );
            },
        }
    }
```
