# Audit Report

## Title
Indexer File Store Backfiller Lacks Cryptographic Verification of Fullnode Transaction Data

## Summary
The indexer-grpc file store backfiller accepts transaction data from fullnodes without any cryptographic verification, performing only superficial validation of batch size and version numbers. A malicious or compromised fullnode can send fabricated transaction data that passes all validation checks and corrupts the file store, affecting all downstream indexer consumers.

## Finding Description

The backfiller worker tasks validate incoming transactions with only basic checks: [1](#0-0) 

These checks verify:
1. Batch contains exactly 1000 transactions
2. Starting version is divisible by 1000
3. Version numbers are sequential

**Critical Missing Validation:**
The backfiller performs NO cryptographic verification of transaction authenticity. The Aptos codebase provides mechanisms for verifying transactions against a trusted LedgerInfo using transaction accumulator proofs: [2](#0-1) 

This verification ensures each transaction hash is cryptographically committed to the blockchain's transaction accumulator, which is signed by validators. The backfiller does not use this mechanism.

**Attack Scenario:**
1. Attacker operates a modified fullnode serving the gRPC stream
2. The fullnode's `IndexerStreamCoordinator` normally fetches transactions from AptosDB: [3](#0-2) 

3. Attacker modifies this code to return fabricated transactions with correct version numbers but false content (wrong signatures, payloads, execution results)
4. Backfiller receives these transactions and validates only version numbers (passes)
5. Fabricated data is serialized and uploaded to file store: [4](#0-3) 

6. File store now contains permanent corrupted transaction history
7. All downstream consumers (block explorers, wallets, DEXs, analytics tools) serve incorrect data

**Invariant Violated:**
**State Consistency** - The file store should be a cryptographically verifiable archive of blockchain history, but lacks verification mechanisms to ensure data integrity against the canonical blockchain state.

## Impact Explanation

**Severity: Medium** (State inconsistencies requiring intervention)

**Affected Systems:**
- File store contains corrupted transaction history
- Downstream indexer databases built from file store are corrupted
- Public APIs serving transaction data return false information
- User-facing applications (wallets, explorers) display incorrect transaction details
- Analytics and monitoring tools produce wrong metrics

**Remediation Required:**
- Manual intervention to identify corrupted batches
- Delete corrupted files from storage
- Re-run backfiller from trusted source
- Potential service interruption during recovery

**Scope Limitation:**
This does NOT affect:
- Blockchain consensus or validator operations
- Actual on-chain state or fund security
- Transaction execution or mempool processing

The vulnerability is limited to the indexer infrastructure ecosystem, which is why this qualifies as Medium (not High/Critical) severity.

## Likelihood Explanation

**Likelihood: Medium-High**

**Attacker Requirements:**
- Ability to run a modified fullnode (low barrier - anyone can compile modified code)
- Social engineering to get backfiller operators to connect to malicious fullnode (moderate difficulty)
- OR compromise of existing trusted fullnode infrastructure (higher difficulty)

**Feasibility:**
The attack is technically straightforward - an attacker only needs to:
1. Fork the Aptos codebase
2. Modify transaction serving code to inject false data
3. Run the modified fullnode
4. Advertise it as a "fast reliable fullnode for indexing"

**Real-World Scenarios:**
- Compromised cloud infrastructure hosting fullnode
- Malicious insider at organization running fullnode
- Supply chain attack on fullnode dependencies
- Configuration error pointing backfiller to untrusted node

## Recommendation

Implement cryptographic verification using transaction accumulator proofs. Add verification after receiving transactions:

```rust
// In processor.rs worker task, after line 200:
let transaction_infos = transactions
    .iter()
    .map(|t| extract_transaction_info(t))
    .collect::<Result<Vec<_>>>()?;

// Fetch ledger info and proof from fullnode
let ledger_info = fetch_trusted_ledger_info(starting_version).await?;
let proof = fetch_transaction_accumulator_proof(
    starting_version, 
    1000
).await?;

// Verify all transactions against ledger info
for (idx, (txn_info, txn)) in transaction_infos.iter()
    .zip(transactions.iter())
    .enumerate() 
{
    let version = starting_version + idx as u64;
    verify_transaction_info(
        &ledger_info,
        version,
        txn_info,
        &proof.get_proof_for_version(version)?,
    )?;
}
```

**Additional Recommendations:**
1. Cross-validate with multiple independent fullnodes
2. Implement periodic integrity checks against known state roots
3. Add digital signatures from trusted fullnode operators
4. Log all data sources for forensic analysis
5. Implement file store integrity checksums

## Proof of Concept

**Setup:**
1. Deploy modified fullnode that injects fabricated transaction at version 1000:

```rust
// In stream_coordinator.rs, modify fetch_raw_txns_with_retries
pub async fn fetch_raw_txns_with_retries(
    context: Arc<Context>,
    ledger_version: u64,
    batch: TransactionBatchInfo,
) -> Vec<TransactionOnChainData> {
    let mut txns = context.get_transactions(
        batch.start_version,
        batch.num_transactions_to_fetch,
        ledger_version,
    ).unwrap();
    
    // MALICIOUS INJECTION
    if batch.start_version == 1000 {
        // Replace transaction at version 1000 with fabricated data
        if let Some(txn) = txns.get_mut(0) {
            txn.transaction = fabricate_fake_transaction();
        }
    }
    txns
}
```

2. Run backfiller pointing to malicious fullnode:
```bash
cargo run --bin aptos-indexer-grpc-file-store-backfiller -- \
  --fullnode-grpc-address http://malicious-node:50051 \
  --file-store-type gcs \
  --gcs-bucket corrupted-test-bucket
```

**Expected Result:**
- Backfiller accepts fabricated transaction (version checks pass)
- File `files/1000.json` contains corrupted transaction data
- No error or warning logged
- File store permanently corrupted

**Verification:**
```bash
# Download and inspect file
gsutil cp gs://corrupted-test-bucket/files/1000.json .
# Transaction at index 0 has wrong signature/payload/outcome
# But file was accepted and uploaded successfully
```

## Notes

While the indexer infrastructure is separate from core blockchain consensus, it represents critical infrastructure for the Aptos ecosystem. The lack of cryptographic verification creates a single point of trust failure that can cascade to all downstream consumers of blockchain data.

Defense-in-depth principles dictate that even operational tools should validate their inputs using available cryptographic mechanisms, especially when handling data that will be served to potentially millions of users through public APIs and applications.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store-backfiller/src/processor.rs (L188-199)
```rust
                        // Data quality check.
                        ensure!(transactions.len() == 1000, "Unexpected transaction count");
                        ensure!(
                            transactions[0].version % 1000 == 0,
                            "Unexpected starting version"
                        );
                        for (ide, t) in transactions.iter().enumerate() {
                            ensure!(
                                t.version == transactions[0].version + ide as u64,
                                "Unexpected version"
                            );
                        }
```

**File:** types/src/proof/mod.rs (L39-61)
```rust
/// Verifies that a given `transaction_info` exists in the ledger using provided proof.
fn verify_transaction_info(
    ledger_info: &LedgerInfo,
    transaction_version: Version,
    transaction_info: &TransactionInfo,
    ledger_info_to_transaction_info_proof: &TransactionAccumulatorProof,
) -> Result<()> {
    ensure!(
        transaction_version <= ledger_info.version(),
        "Transaction version {} is newer than LedgerInfo version {}.",
        transaction_version,
        ledger_info.version(),
    );

    let transaction_info_hash = transaction_info.hash();
    ledger_info_to_transaction_info_proof.verify(
        ledger_info.transaction_accumulator_hash(),
        transaction_info_hash,
        transaction_version,
    )?;

    Ok(())
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L320-332)
```rust
    pub async fn fetch_raw_txns_with_retries(
        context: Arc<Context>,
        ledger_version: u64,
        batch: TransactionBatchInfo,
    ) -> Vec<TransactionOnChainData> {
        let mut retries = 0;
        loop {
            match context.get_transactions(
                batch.start_version,
                batch.num_transactions_to_fetch,
                ledger_version,
            ) {
                Ok(raw_txns) => return raw_txns,
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/compression_util.rs (L191-214)
```rust
    pub fn from_transactions(
        transactions: Vec<Transaction>,
        storage_format: StorageFormat,
    ) -> Self {
        let mut bytes = Vec::new();
        let starting_version = transactions
            .first()
            .expect("Cannot build empty file")
            .version;
        match storage_format {
            StorageFormat::Lz4CompressedProto => {
                let t = TransactionsInStorage {
                    starting_version: Some(transactions.first().unwrap().version),
                    transactions,
                };
                t.encode(&mut bytes).expect("proto serialization failed.");
                let mut compressed = EncoderBuilder::new()
                    .level(0)
                    .build(Vec::new())
                    .expect("Lz4 compression failed.");
                compressed
                    .write_all(&bytes)
                    .expect("Lz4 compression failed.");
                FileEntry::Lz4CompressionProto(compressed.finish().0)
```
