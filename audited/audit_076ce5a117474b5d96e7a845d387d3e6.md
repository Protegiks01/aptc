# Audit Report

## Title
Concurrent Database Instance Corruption: create_checkpoint() Opens New RocksDB Instance While Active Instance Performs Writes

## Summary
The `LedgerDb::create_checkpoint()` static method creates a new RocksDB instance pointing to the same database directory that is already open by an active `AptosDB` instance. This allows concurrent access to the same RocksDB files by two independent instances within the same process, causing undefined behavior that can lead to checkpoint corruption or database corruption. [1](#0-0) 

## Finding Description

The vulnerability occurs because `LedgerDb::create_checkpoint()` is implemented as a static method that opens an entirely new `LedgerDb` instance rather than using an existing instance: [2](#0-1) 

This new instance opens RocksDB in **read-write mode** (`readonly=false`), creating a second handle to the same database files that are already open by the operational node's `AptosDB` instance.

Meanwhile, the active `AptosDB` instance performs write operations through `write_schemas()`: [3](#0-2) 

The write operations eventually call RocksDB's `write_opt()`: [4](#0-3) 

While checkpoint creation calls RocksDB's native checkpoint API: [5](#0-4) 

**The Critical Flaw:**

RocksDB's file locks are process-level, not instance-level. Within the same process, opening multiple `rocksdb::DB` instances to the same directory creates **undefined behavior** because:

1. Each instance maintains independent in-memory state (memtables, write buffers, caches)
2. Instances don't coordinate or share state
3. The checkpoint instance doesn't see uncommitted data from the active instance's write buffers
4. The checkpoint captures an inconsistent point-in-time snapshot mixing old disk data with new in-memory data

The active `AptosDB` instance uses locks to prevent concurrent writes: [6](#0-5) 

However, these locks are **instance-level** and don't protect against a completely separate `LedgerDb` instance created by the static `create_checkpoint()` method.

**Breaking State Consistency Invariant:**

This violates **Critical Invariant #4**: "State transitions must be atomic and verifiable via Merkle proofs." The checkpoint may contain:
- Partial transaction data (some components written, others not)
- Mixed state from different versions
- Inconsistent Merkle tree nodes

## Impact Explanation

This qualifies as **High Severity** under Aptos bug bounty criteria:

1. **State inconsistencies requiring intervention**: Corrupted checkpoints could be used for node recovery, causing nodes to restart with inconsistent state.

2. **Potential database corruption**: In worst-case scenarios, concurrent file operations from both instances could corrupt the active database, causing validator node crashes or requiring manual intervention.

3. **Consensus impact**: If checkpoints are used for state sync or backup/restore, nodes restored from corrupted checkpoints may have inconsistent state, breaking consensus determinism.

4. **Validator node slowdowns**: Database corruption or inconsistencies would cause performance degradation.

## Likelihood Explanation

**Current Likelihood: Medium-Low**

The vulnerability is currently exploitable only in limited scenarios:

1. **Node operators running checkpoint tools while node is active**: The `db_debugger` CLI tools could be run by operators on a live database. [7](#0-6) 

2. **Future API exposure**: If checkpoint functionality is exposed via RPC or admin APIs, the attack surface expands significantly.

3. **Internal code paths**: Future code changes might inadvertently call `create_checkpoint()` during active operations.

However, the vulnerability represents a **design flaw** that should be fixed defensively, regardless of current exposure.

## Recommendation

**Fix 1: Use Existing Instance for Checkpoints**

Modify `create_checkpoint()` to be an instance method that uses the existing DB instance, or accept a reference to an existing `LedgerDb`:

```rust
pub fn create_checkpoint_from_instance(
    &self,
    cp_root_path: impl AsRef<Path>,
) -> Result<()> {
    let cp_ledger_db_folder = cp_root_path.as_ref().join(LEDGER_DB_FOLDER_NAME);
    
    std::fs::remove_dir_all(&cp_ledger_db_folder).unwrap_or(());
    if self.enable_storage_sharding {
        std::fs::create_dir_all(&cp_ledger_db_folder).unwrap_or(());
    }
    
    // Use existing instance's DB handles
    self.metadata_db().create_checkpoint(
        Self::metadata_db_path(cp_root_path.as_ref(), self.enable_storage_sharding)
    )?;
    
    if self.enable_storage_sharding {
        self.event_db().create_checkpoint(cp_ledger_db_folder.join(EVENT_DB_NAME))?;
        // ... other sub-databases
    }
    
    Ok(())
}
```

**Fix 2: Open in Secondary/Read-Only Mode**

If a new instance must be opened, use RocksDB's secondary instance mode: [8](#0-7) 

However, this requires a separate directory for the secondary instance.

**Fix 3: Add Explicit Locking**

Add a process-wide or file-based lock that prevents checkpoint operations during active writes. However, this doesn't address the fundamental issue of multiple instances.

**Recommended Approach: Fix 1** - Use the existing instance, which is the safest and most straightforward solution.

## Proof of Concept

```rust
// Conceptual PoC demonstrating the vulnerability
// This would need to be adapted to the actual test framework

use std::sync::{Arc, Barrier};
use std::thread;
use tempfile::TempDir;

#[test]
fn test_concurrent_checkpoint_corruption() {
    let tmp_dir = TempDir::new().unwrap();
    let db_path = tmp_dir.path().to_path_buf();
    
    // Create and initialize a database
    let db1 = AptosDB::new_for_test(&db_path);
    let db1_arc = Arc::new(db1);
    
    let barrier = Arc::new(Barrier::new(2));
    let db_path_clone = db_path.clone();
    let barrier_clone = barrier.clone();
    
    // Thread 1: Continuously writes transactions
    let write_handle = thread::spawn(move || {
        barrier_clone.wait();
        for i in 0..1000 {
            let mut batch = LedgerDbSchemaBatches::new();
            // Add transaction data to batch
            db1_arc.ledger_db.write_schemas(batch).unwrap();
        }
    });
    
    // Thread 2: Attempts to create checkpoint while writes are ongoing
    let checkpoint_handle = thread::spawn(move || {
        barrier.wait();
        let cp_path = tmp_dir.path().join("checkpoint");
        // This opens a NEW RocksDB instance!
        LedgerDb::create_checkpoint(&db_path_clone, &cp_path, true).unwrap();
    });
    
    write_handle.join().unwrap();
    checkpoint_handle.join().unwrap();
    
    // Verify checkpoint consistency
    // Expected: Checkpoint should be consistent
    // Actual: Checkpoint may contain mixed state from different versions
}
```

## Notes

This vulnerability exists due to a fundamental architectural decision to implement `create_checkpoint()` as a static method that creates a new database instance. While RocksDB's native checkpoint API is designed to work safely on an active database, the implementation bypasses this safety by opening a second independent instance within the same process.

The current exposure is limited because checkpoint operations are primarily triggered by CLI tools rather than runtime APIs. However, this represents a critical design flaw that should be addressed proactively to prevent future exploitation as the codebase evolves.

### Citations

**File:** storage/aptosdb/src/ledger_db/mod.rs (L311-328)
```rust
    pub(crate) fn create_checkpoint(
        db_root_path: impl AsRef<Path>,
        cp_root_path: impl AsRef<Path>,
        sharding: bool,
    ) -> Result<()> {
        let rocksdb_configs = RocksdbConfigs {
            enable_storage_sharding: sharding,
            ..Default::default()
        };
        let env = None;
        let block_cache = None;
        let ledger_db = Self::new(
            db_root_path,
            rocksdb_configs,
            env,
            block_cache,
            /*readonly=*/ false,
        )?;
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L531-548)
```rust
    pub fn write_schemas(&self, schemas: LedgerDbSchemaBatches) -> Result<()> {
        self.write_set_db
            .write_schemas(schemas.write_set_db_batches)?;
        self.transaction_info_db
            .write_schemas(schemas.transaction_info_db_batches)?;
        self.transaction_db
            .write_schemas(schemas.transaction_db_batches)?;
        self.persisted_auxiliary_info_db
            .write_schemas(schemas.persisted_auxiliary_info_db_batches)?;
        self.event_db.write_schemas(schemas.event_db_batches)?;
        self.transaction_accumulator_db
            .write_schemas(schemas.transaction_accumulator_db_batches)?;
        self.transaction_auxiliary_data_db
            .write_schemas(schemas.transaction_auxiliary_data_db_batches)?;
        // TODO: remove this after sharding migration
        self.ledger_metadata_db
            .write_schemas(schemas.ledger_metadata_db_batches)
    }
```

**File:** storage/schemadb/src/lib.rs (L101-115)
```rust
    pub fn open_cf_as_secondary<P: AsRef<Path>>(
        opts: &Options,
        primary_path: P,
        secondary_path: P,
        name: &str,
        cfds: Vec<ColumnFamilyDescriptor>,
    ) -> DbResult<DB> {
        Self::open_cf_impl(
            opts,
            primary_path,
            name,
            cfds,
            OpenMode::Secondary(secondary_path.as_ref()),
        )
    }
```

**File:** storage/schemadb/src/lib.rs (L289-304)
```rust
    fn write_schemas_inner(&self, batch: impl IntoRawBatch, option: &WriteOptions) -> DbResult<()> {
        let labels = [self.name.as_str()];
        let _timer = APTOS_SCHEMADB_BATCH_COMMIT_LATENCY_SECONDS.timer_with(&labels);

        let raw_batch = batch.into_raw_batch(self)?;

        let serialized_size = raw_batch.inner.size_in_bytes();
        self.inner
            .write_opt(raw_batch.inner, option)
            .into_db_res()?;

        raw_batch.stats.commit();
        APTOS_SCHEMADB_BATCH_COMMIT_BYTES.observe_with(&[&self.name], serialized_size as f64);

        Ok(())
    }
```

**File:** storage/schemadb/src/lib.rs (L356-362)
```rust
    pub fn create_checkpoint<P: AsRef<Path>>(&self, path: P) -> DbResult<()> {
        rocksdb::checkpoint::Checkpoint::new(&self.inner)
            .into_db_res()?
            .create_checkpoint(path)
            .into_db_res()?;
        Ok(())
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L50-53)
```rust
            let _lock = self
                .pre_commit_lock
                .try_lock()
                .expect("Concurrent committing detected.");
```

**File:** storage/aptosdb/src/db_debugger/checkpoint/mod.rs (L20-29)
```rust
    pub fn run(self) -> Result<()> {
        ensure!(!self.output_dir.exists(), "Output dir already exists.");
        fs::create_dir_all(&self.output_dir)?;
        let sharding_config = self.db_dir.sharding_config.clone();
        AptosDB::create_checkpoint(
            self.db_dir,
            self.output_dir,
            sharding_config.enable_storage_sharding,
        )
    }
```
