# Audit Report

## Title
Non-Atomic OnDiskStorage Write Operations Can Corrupt Consensus Private Keys and Safety Data Leading to Network Liveness Failure

## Summary
The `OnDiskStorage` implementation used by validators for persisting consensus keys and safety data does not call `fsync()` after writing files, violating atomicity guarantees. System crashes during write operations can result in corrupted storage files containing neither old nor new data, causing validators to panic on restart and preventing network participation.

## Finding Description

The `OnDiskStorage::write()` method uses a write-to-temporary-then-rename pattern but omits the critical `fsync()` call required for durability. [1](#0-0) 

This implementation performs:
1. Serialization to JSON
2. Write to temporary file (no fsync)
3. Rename temporary file to target file

The rename operation updates file metadata, which may be journaled and persisted by the filesystem. However, without `fsync()`, the actual data blocks written in step 2 may remain in OS buffers and never reach disk before a crash.

When a system crash occurs after the rename but before buffer flush, the file's directory entry points to the new location, but the file's data blocks contain uninitialized data, zeros, or garbage. This leaves the storage file in a corrupted state that is neither the old value (lost during rename) nor the new value (never flushed to disk).

This storage backend is actively used in production validator configurations: [2](#0-1) 

The storage persists critical consensus data including:
- Consensus private keys (BLS12381)
- SafetyData (last_voted_round, epoch, preferred_round)
- Validator account information
- Waypoint data [3](#0-2) 

The implementation explicitly expects sync guarantees that OnDiskStorage does not provide.

When validators restart after corruption:

**Consensus Key Corruption:** The validator attempts to load the consensus private key. If deserialization fails due to corrupted JSON, the system panics: [4](#0-3) 

**SafetyData Corruption:** Vote verification requires loading SafetyData. Corruption prevents the validator from participating in consensus: [5](#0-4) 

## Impact Explanation

**Critical Severity** - This vulnerability meets multiple Critical severity criteria from the Aptos bug bounty program:

1. **Total loss of liveness/network availability**: If multiple validators experience simultaneous crashes (datacenter power failure, infrastructure issues), and their consensus keys become corrupted, they cannot rejoin the network. With >1/3 of validators offline due to corruption, the network halts.

2. **Non-recoverable network partition**: Validators with corrupted keys require manual intervention to restore from backups. If backups are unavailable or outdated, validators may need to be re-initialized, potentially requiring a hardfork to recover the network.

3. **Consensus violations**: The inability of validators to load consensus keys directly violates the consensus liveness guarantee and prevents the network from making progress.

Even a single validator corruption causes that validator to be permanently offline until manual recovery, reducing network resilience and increasing the risk of reaching the Byzantine threshold.

## Likelihood Explanation

**High Likelihood**:

1. **Frequent write operations**: SafetyData is updated on every consensus vote, potentially multiple times per second during normal operation. Each write is a corruption opportunity.

2. **Production deployment**: OnDiskStorage is the configured backend in production validator templates, not just test environments.

3. **Infrastructure realities**: Datacenter power failures, kernel panics, OOM kills, and hardware failures are inevitable in distributed systems. These events create the exact crash-during-write conditions that trigger corruption.

4. **Correlated failures**: Infrastructure issues often affect multiple validators simultaneously (regional power outage, cloud provider incidents), maximizing the likelihood of exceeding the Byzantine threshold.

5. **No recovery mechanism**: Unlike databases with WAL (Write-Ahead Logging), there is no automatic recovery mechanism. Corruption requires manual intervention.

## Recommendation

Add `fsync()` calls to ensure durability before the rename operation:

```rust
fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
    let contents = serde_json::to_vec(data)?;
    let mut file = File::create(self.temp_path.path())?;
    file.write_all(&contents)?;
    
    // CRITICAL: Sync data to disk before rename
    file.sync_all()?;
    
    fs::rename(&self.temp_path, &self.file_path)?;
    
    // Sync directory metadata to persist rename
    let dir_path = self.file_path.parent().unwrap();
    File::open(dir_path)?.sync_all()?;
    
    Ok(())
}
```

Additionally, consider:
1. Implementing checksum validation on read to detect corruption early
2. Maintaining versioned backups of critical keys
3. Adding automated recovery mechanisms from backup storage
4. Logging warnings when OnDiskStorage is used in production environments

## Proof of Concept

```rust
#[test]
fn test_ondisk_corruption_on_crash() {
    use std::fs::{self, File};
    use std::io::Write;
    use aptos_secure_storage::{OnDiskStorage, KVStorage};
    use aptos_crypto::{bls12381::PrivateKey, Uniform};
    use aptos_temppath::TempPath;
    
    let temp_path = TempPath::new();
    temp_path.create_as_file().unwrap();
    
    let mut storage = OnDiskStorage::new(temp_path.path().to_path_buf());
    
    // Write a private key
    let mut rng = rand::rngs::StdRng::from_seed([0u8; 32]);
    let private_key = PrivateKey::generate(&mut rng);
    storage.set("consensus_key", private_key.clone()).unwrap();
    
    // Simulate crash by killing process after write but before sync
    // In real scenario, the rename would be persisted but data wouldn't be
    
    // Attempt to read back
    let result: Result<_, _> = storage.get("consensus_key");
    
    // With proper fsync, this should always succeed
    // Without fsync, crash scenarios can cause deserialization failure
    assert!(result.is_ok());
    
    // To truly demonstrate the bug, you would need to:
    // 1. Write to OnDiskStorage
    // 2. Force a system crash (kill -9, power off VM)
    // 3. Restart and observe corrupted file
    // 4. Attempt deserialization and observe panic
}
```

To reproduce in a real environment:
1. Configure a validator with OnDiskStorage backend
2. Start the validator and let it participate in consensus
3. While SafetyData is being updated, forcibly crash the system (e.g., `kill -9` the process or power off the VM)
4. Restart the validator
5. Observe deserialization failures and validator panic during consensus key loading

## Notes

This vulnerability specifically answers the security question: "If the underlying KVStorage.set() is not atomic, can partial writes leave private keys in a corrupted state that's neither the old nor new value?"

**Answer: YES.** The OnDiskStorage implementation's lack of `fsync()` means that while individual write operations appear atomic at the application level (due to rename), they are not durable. System crashes can leave files in a state containing corrupted data that represents neither the previous value (lost during rename) nor the new value (never flushed to disk), directly compromising consensus private keys and safety-critical data.

### Citations

**File:** secure/storage/src/on_disk.rs (L64-70)
```rust
    fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
        let contents = serde_json::to_vec(data)?;
        let mut file = File::create(self.temp_path.path())?;
        file.write_all(&contents)?;
        fs::rename(&self.temp_path, &self.file_path)?;
        Ok(())
    }
```

**File:** terraform/helm/aptos-node/files/configs/validator-base.yaml (L14-16)
```yaml
    backend:
      type: "on_disk_storage"
      path: secure-data.json
```

**File:** consensus/safety-rules/src/persistent_safety_storage.rs (L17-18)
```rust
/// persistent data to local disk, cloud, secrets managers, or even memory (for tests)
/// Any set function is expected to sync to the remote system before returning.
```

**File:** consensus/src/epoch_manager.rs (L1228-1233)
```rust
        let loaded_consensus_key = match self.load_consensus_key(&epoch_state.verifier) {
            Ok(k) => Arc::new(k),
            Err(e) => {
                panic!("load_consensus_key failed: {e}");
            },
        };
```

**File:** consensus/safety-rules/src/safety_rules.rs (L67-68)
```rust
        let proposed_block = vote_proposal.block();
        let safety_data = self.persistent_storage.safety_data()?;
```
