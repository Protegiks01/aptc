# Audit Report

## Title
Unbounded SchemaBatch Size in Transaction Pruner Initialization Can Cause Node Failure

## Summary
The `TransactionPruner` lacks batch size limits during initialization, allowing unbounded memory consumption when catching up on large version gaps. This can cause out-of-memory errors and prevent node startup after extended downtime.

## Finding Description

The `TransactionPruner::prune()` function creates a single `SchemaBatch` to accumulate all deletion operations for a given version range without any size validation. [1](#0-0) 

The `SchemaBatch` data structure has no built-in size limits - it simply accumulates operations in a HashMap: [2](#0-1) 

The `get_pruning_candidate_transactions()` method loads ALL transactions in the range into memory with a comment incorrectly claiming the capacity is "capped": [3](#0-2) 

The configuration allows arbitrary `batch_size` values with no validation: [4](#0-3) 

**Critical vulnerability during initialization**: When a `TransactionPruner` is constructed, it immediately calls `prune()` to catch up any gap between its saved progress and the metadata progress, processing the ENTIRE gap in a single batch: [5](#0-4) 

This bypasses the normal chunking logic in `LedgerPruner::prune()` which respects the `batch_size` parameter: [6](#0-5) 

**Transaction size limits** allow up to 64 KB per transaction (1 MB for governance transactions): [7](#0-6) 

**Attack scenarios:**

1. **Initialization after downtime**: Node offline for days creates gap of 500,000+ versions. On restart, `TransactionPruner::new()` attempts to load and prune all transactions in one batch: 500,000 × 64 KB = 32 GB potential batch size → OOM crash, node cannot start

2. **Operator misconfiguration**: Setting `batch_size: 1000000` in config during normal operation: 1M × 64 KB = 64 GB batch → Memory exhaustion, node crash

3. **Governance transaction pruning**: With default `batch_size: 5000` and governance transactions (1 MB each): 5,000 × 1 MB = 5 GB batch → Memory pressure, potential OOM

This breaks the **Resource Limits** invariant: operations must respect memory and computational limits.

## Impact Explanation

**Medium Severity** per Aptos bug bounty criteria: "State inconsistencies requiring intervention"

**Concrete impacts:**
- Node unable to restart after extended downtime (requires manual database intervention)
- Memory exhaustion causing node crashes during pruning operations
- Database write failures leaving pruner in inconsistent state
- Accumulation of unpruned data leading to unbounded database growth
- Potential validator set disruption if multiple nodes experience simultaneous failures

The vulnerability doesn't directly cause fund loss or consensus violations, but creates operational availability issues requiring manual intervention, meeting the Medium severity threshold.

## Likelihood Explanation

**High likelihood** for natural occurrence:

1. **Initialization gap scenario**: Common in production when nodes experience extended downtime (maintenance, hardware issues, network partitions). A node offline for 1-2 days on mainnet could accumulate 200,000+ versions requiring catch-up.

2. **Default configuration vulnerability**: While the default `batch_size: 5000` provides some protection, it's insufficient for governance transactions or misconfiguration scenarios.

3. **No input validation**: The system accepts any `batch_size` value without bounds checking, making misconfiguration trivial.

4. **Affects all sub-pruners**: The same pattern exists across 8 different pruner components: [8](#0-7) 

This amplifies the likelihood as any sub-pruner can trigger the issue.

## Recommendation

Implement batch size limits and chunking in initialization code:

```rust
fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
    const MAX_BATCH_SIZE: usize = 5000; // Enforce reasonable limit
    
    let mut progress = current_progress;
    while progress < target_version {
        let batch_end = std::cmp::min(
            progress + MAX_BATCH_SIZE as Version,
            target_version
        );
        
        let mut batch = SchemaBatch::new();
        let candidate_transactions =
            self.get_pruning_candidate_transactions(progress, batch_end)?;
        
        self.ledger_db
            .transaction_db()
            .prune_transaction_by_hash_indices(
                candidate_transactions.iter().map(|(_, txn)| txn.hash()),
                &mut batch,
            )?;
        self.ledger_db.transaction_db().prune_transactions(
            progress,
            batch_end,
            &mut batch,
        )?;
        self.transaction_store
            .prune_transaction_summaries_by_account(&candidate_transactions, &mut batch)?;
        
        if batch_end == target_version {
            batch.put::<DbMetadataSchema>(
                &DbMetadataKey::TransactionPrunerProgress,
                &DbMetadataValue::Version(batch_end),
            )?;
        }
        
        self.ledger_db.transaction_db().write_schemas(batch)?;
        progress = batch_end;
    }
    
    Ok(())
}
```

Additionally, add configuration validation: [9](#0-8) 

Add to the `ConfigSanitizer::sanitize()` method:

```rust
if config.storage_pruner_config.ledger_pruner_config.batch_size > 50_000 {
    return Err(Error::ConfigSanitizerFailed(
        sanitizer_name,
        "ledger pruner batch_size too large (max 50,000 recommended)".to_string(),
    ));
}
```

## Proof of Concept

**Reproduction steps:**

1. Set up Aptos node with custom configuration:
```yaml
storage:
  storage_pruner_config:
    ledger_pruner_config:
      enable: true
      batch_size: 500000  # Dangerously large
      prune_window: 90000000
```

2. Run node until it processes significant transactions

3. Stop node, manually modify DB to create artificial gap:
```rust
// In test environment
db.put::<DbMetadataSchema>(
    &DbMetadataKey::TransactionPrunerProgress,
    &DbMetadataValue::Version(0),
)?;
```

4. Restart node - `TransactionPruner::new()` will attempt to process 500,000 versions in single batch

5. Observe OOM error or memory exhaustion preventing node startup

**Expected result**: Node fails to start with memory-related errors during `TransactionPruner` initialization.

**Verification**: Monitor memory consumption during initialization - will spike to multiple GB before crashing.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L37-74)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        let candidate_transactions =
            self.get_pruning_candidate_transactions(current_progress, target_version)?;
        self.ledger_db
            .transaction_db()
            .prune_transaction_by_hash_indices(
                candidate_transactions.iter().map(|(_, txn)| txn.hash()),
                &mut batch,
            )?;
        self.ledger_db.transaction_db().prune_transactions(
            current_progress,
            target_version,
            &mut batch,
        )?;
        self.transaction_store
            .prune_transaction_summaries_by_account(&candidate_transactions, &mut batch)?;
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::TransactionPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
        if let Some(indexer_db) = self.internal_indexer_db.as_ref() {
            if indexer_db.transaction_enabled() {
                let mut index_batch = SchemaBatch::new();
                self.transaction_store
                    .prune_transaction_by_account(&candidate_transactions, &mut index_batch)?;
                index_batch.put::<InternalIndexerMetadataSchema>(
                    &IndexerMetadataKey::TransactionPrunerProgress,
                    &IndexerMetadataValue::Version(target_version),
                )?;
                indexer_db.get_inner_db_ref().write_schemas(index_batch)?;
            } else {
                self.transaction_store
                    .prune_transaction_by_account(&candidate_transactions, &mut batch)?;
            }
        }
        self.ledger_db.transaction_db().write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L77-104)
```rust
impl TransactionPruner {
    pub(in crate::pruner) fn new(
        transaction_store: Arc<TransactionStore>,
        ledger_db: Arc<LedgerDb>,
        metadata_progress: Version,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.transaction_db_raw(),
            &DbMetadataKey::TransactionPrunerProgress,
            metadata_progress,
        )?;

        let myself = TransactionPruner {
            transaction_store,
            ledger_db,
            internal_indexer_db,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up TransactionPruner."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L106-131)
```rust
    fn get_pruning_candidate_transactions(
        &self,
        start: Version,
        end: Version,
    ) -> Result<Vec<(Version, Transaction)>> {
        ensure!(end >= start, "{} must be >= {}", end, start);

        let mut iter = self
            .ledger_db
            .transaction_db_raw()
            .iter::<TransactionSchema>()?;
        iter.seek(&start)?;

        // The capacity is capped by the max number of txns we prune in a single batch. It's a
        // relatively small number set in the config, so it won't cause high memory usage here.
        let mut txns = Vec::with_capacity((end - start) as usize);
        for item in iter {
            let (version, txn) = item?;
            if version >= end {
                break;
            }
            txns.push((version, txn));
        }

        Ok(txns)
    }
```

**File:** storage/schemadb/src/batch.rs (L127-173)
```rust
/// `SchemaBatch` holds a collection of updates that can be applied to a DB atomically. The updates
/// will be applied in the order in which they are added to the `SchemaBatch`.
#[derive(Debug, Default)]
pub struct SchemaBatch {
    rows: DropHelper<HashMap<ColumnFamilyName, Vec<WriteOp>>>,
    stats: SampledBatchStats,
}

impl SchemaBatch {
    /// Creates an empty batch.
    pub fn new() -> Self {
        Self::default()
    }

    /// keep these on the struct itself so that we don't need to update each call site.
    pub fn put<S: Schema>(&mut self, key: &S::Key, value: &S::Value) -> DbResult<()> {
        <Self as WriteBatch>::put::<S>(self, key, value)
    }

    pub fn delete<S: Schema>(&mut self, key: &S::Key) -> DbResult<()> {
        <Self as WriteBatch>::delete::<S>(self, key)
    }
}

impl WriteBatch for SchemaBatch {
    fn stats(&mut self) -> &mut SampledBatchStats {
        &mut self.stats
    }

    fn raw_put(&mut self, cf_name: ColumnFamilyName, key: Vec<u8>, value: Vec<u8>) -> DbResult<()> {
        self.rows
            .entry(cf_name)
            .or_default()
            .push(WriteOp::Value { key, value });

        Ok(())
    }

    fn raw_delete(&mut self, cf_name: ColumnFamilyName, key: Vec<u8>) -> DbResult<()> {
        self.rows
            .entry(cf_name)
            .or_default()
            .push(WriteOp::Deletion { key });

        Ok(())
    }
}
```

**File:** config/src/config/storage_config.rs (L387-395)
```rust
impl Default for LedgerPrunerConfig {
    fn default() -> Self {
        LedgerPrunerConfig {
            enable: true,
            prune_window: 90_000_000,
            batch_size: 5_000,
            user_pruning_window_offset: 200_000,
        }
    }
```

**File:** config/src/config/storage_config.rs (L682-729)
```rust
impl ConfigSanitizer for StorageConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let config = &node_config.storage;

        let ledger_prune_window = config
            .storage_pruner_config
            .ledger_pruner_config
            .prune_window;
        let state_merkle_prune_window = config
            .storage_pruner_config
            .state_merkle_pruner_config
            .prune_window;
        let epoch_snapshot_prune_window = config
            .storage_pruner_config
            .epoch_snapshot_pruner_config
            .prune_window;
        let user_pruning_window_offset = config
            .storage_pruner_config
            .ledger_pruner_config
            .user_pruning_window_offset;

        if ledger_prune_window < 50_000_000 {
            warn!("Ledger prune_window is too small, harming network data availability.");
        }
        if state_merkle_prune_window < 100_000 {
            warn!("State Merkle prune_window is too small, node might stop functioning.");
        }
        if epoch_snapshot_prune_window < 50_000_000 {
            warn!("Epoch snapshot prune_window is too small, harming network data availability.");
        }
        if user_pruning_window_offset > 1_000_000 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "user_pruning_window_offset too large, so big a buffer is unlikely necessary. Set something < 1 million.".to_string(),
            ));
        }
        if user_pruning_window_offset > ledger_prune_window {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "user_pruning_window_offset is larger than the ledger prune window, the API will refuse to return any data.".to_string(),
            ));
        }

```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L62-92)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning ledger data."
            );
            self.ledger_metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning ledger data is done.");
        }

        Ok(target_version)
    }
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L73-81)
```rust
            max_transaction_size_in_bytes: NumBytes,
            "max_transaction_size_in_bytes",
            64 * 1024
        ],
        [
            max_transaction_size_in_bytes_gov: NumBytes,
            { RELEASE_V1_13.. => "max_transaction_size_in_bytes.gov" },
            1024 * 1024
        ],
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L106-106)
```rust
        myself.prune(progress, metadata_progress)?;
```
