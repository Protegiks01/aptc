# Audit Report

## Title
Byzantine Backup Storage Can Cause Indefinite Restoration Hang via FuturesOrderedX Ordering Deadlock

## Summary
A Byzantine backup storage service can cause backup restoration to hang indefinitely by making a single chunk download never complete. The `FuturesOrderedX` stream enforces strict ordering of outputs, and if a future with index N never completes, `poll_next()` will never return futures with indices greater than N, even if they complete successfully. No timeouts exist in the restoration pipeline, causing permanent loss of node liveness.

## Finding Description

The vulnerability exists in the interaction between `FuturesOrderedX` and the state snapshot restoration process. [1](#0-0) 

The `poll_next()` function enforces strict ordering by checking if `output.index == this.next_outgoing_index`. If the next expected index never arrives, all subsequent completed futures accumulate in `queued_outputs` but are never returned.

The state snapshot restoration process uses this stream to download chunks: [2](#0-1) 

Futures are created with sequential indices (0, 1, 2, ...) and streamed through `buffered_x`, which internally uses `FuturesOrderedX`. The restoration process then awaits each chunk with `futs_stream.try_next().await?` on line 201.

**Attack Scenario:**
1. Attacker controls or compromises a backup storage service (e.g., S3/GCS bucket)
2. Node operator configures restoration from the Byzantine source
3. Byzantine storage responds normally to most chunks but makes one specific chunk (e.g., chunk index 5) hang by:
   - Not sending any data after TCP connection established
   - Sending only the 4-byte size header but no actual data
   - Sending data at 1 byte per hour [3](#0-2) 

The `read_full_buf_or_none` function will block indefinitely waiting for data with no timeout.

4. Chunks 0-4 complete and are processed successfully
5. Chunk 5 future hangs waiting for storage response
6. Chunks 6+ complete and get pushed to `queued_outputs` BinaryHeap
7. `poll_next()` waits forever for index 5 since `next_outgoing_index == 5`
8. The restoration process hangs indefinitely at line 201

**No Timeout Protection:** [4](#0-3) 

The `run()` method tracks elapsed time but has no timeout mechanism. [5](#0-4) 

The storage layer's `open_for_read` implementation spawns commands with no timeout.

This breaks the **availability invariant** - nodes cannot complete state restoration and join the network.

## Impact Explanation

**Critical Severity** - This vulnerability meets multiple Critical severity criteria from the Aptos bug bounty program:

1. **Total loss of liveness/network availability**: Affected nodes cannot complete restoration and join the network. If the restoration is part of disaster recovery or initial node setup, this completely prevents node operation.

2. **Non-recoverable without manual intervention**: There are no automatic timeouts, retries with different sources, or health checks. Operators must manually diagnose the hanging restoration and reconfigure to use a different backup source.

3. **Network-wide impact potential**: If a commonly-recommended backup source is compromised (e.g., official Aptos-maintained backup), many node operators would be affected simultaneously, severely degrading network participation.

4. **Secondary impact - Memory exhaustion**: As more out-of-order chunks complete and accumulate in `queued_outputs`, memory consumption grows unbounded, potentially causing OOM crashes.

The attack is trivial to execute - the Byzantine storage only needs to hang a single chunk download by not responding. This is fundamentally different from a network-level DoS (which is out of scope) because it exploits application-level logic in the ordered streaming mechanism.

## Likelihood Explanation

**High Likelihood:**

1. **Low attacker requirements**: Attacker only needs to control or compromise a backup storage service. This could be:
   - Compromising cloud storage credentials
   - Running a malicious backup service that nodes are configured to use
   - Man-in-the-middle attacks on backup downloads (if not using HTTPS/authenticated storage)

2. **Trivial exploitation**: Making a chunk download hang requires only not sending data - no sophisticated attack needed

3. **Common scenario**: State restoration is a critical operation performed during:
   - Initial node setup
   - Disaster recovery after node failure
   - Validator onboarding
   - Network upgrades requiring state migration

4. **No existing defenses**: Complete absence of timeouts, health checks, or automatic failover to alternative backup sources

5. **Difficult to detect**: Operators may not immediately realize the restoration is permanently hung vs. just slow, delaying mitigation

## Recommendation

Implement multiple layers of timeout protection:

**1. Per-chunk download timeout:**
```rust
// In StateSnapshotRestoreController::run_impl()
use tokio::time::{timeout, Duration};

const CHUNK_DOWNLOAD_TIMEOUT: Duration = Duration::from_secs(300); // 5 minutes

let storage = self.storage.clone();
let futs_iter = chunks.into_iter().enumerate().map(|(chunk_idx, chunk)| {
    let storage = storage.clone();
    async move {
        timeout(CHUNK_DOWNLOAD_TIMEOUT, tokio::spawn(async move {
            let blobs = Self::read_state_value(&storage, chunk.blobs.clone()).await?;
            let proof = storage.load_bcs_file(&chunk.proof).await?;
            Result::<_>::Ok((chunk_idx, chunk, blobs, proof))
        }))
        .await
        .map_err(|_| anyhow!("Chunk download timeout after {:?}", CHUNK_DOWNLOAD_TIMEOUT))?
        .map_err(|e| anyhow!("Task join error: {}", e))?
    }
});
```

**2. Global restoration timeout:**
```rust
// In StateSnapshotRestoreController::run()
const RESTORE_TIMEOUT: Duration = Duration::from_secs(3600 * 4); // 4 hours

pub async fn run(self) -> Result<()> {
    let name = self.name();
    let start = Instant::now();
    info!("{} started. Manifest: {}", name, self.manifest_handle);
    
    timeout(RESTORE_TIMEOUT, self.run_impl())
        .await
        .map_err(|_| anyhow!("{} timed out after {:?}", name, RESTORE_TIMEOUT))?
        .map_err(|e| anyhow!("{} failed: {}", name, e))?;
        
    info!(time = start.elapsed().as_secs(), "{} succeeded.", name);
    Ok(())
}
```

**3. Progress monitoring:**
Add a watchdog that aborts restoration if no progress is made for a configurable period:
```rust
// Track last progress timestamp
let last_progress = Arc::new(Mutex::new(Instant::now()));
let watchdog_interval = Duration::from_secs(300);

// Spawn watchdog task
let watchdog_last_progress = last_progress.clone();
let watchdog_handle = tokio::spawn(async move {
    loop {
        tokio::time::sleep(watchdog_interval).await;
        if watchdog_last_progress.lock().elapsed() > watchdog_interval {
            return Err(anyhow!("No restoration progress for {:?}", watchdog_interval));
        }
    }
});

// Update progress in chunk processing loop
*last_progress.lock() = Instant::now();
```

**4. Retry with exponential backoff:**
Add retry logic for failed chunk downloads with different backup sources if available.

## Proof of Concept

```rust
#[cfg(test)]
mod vulnerability_poc {
    use super::*;
    use futures::StreamExt;
    use tokio::time::{sleep, Duration};

    #[tokio::test]
    async fn test_futures_ordered_x_hang_on_missing_index() {
        // Create FuturesOrderedX with 3 max in-progress
        let mut ordered_stream = FuturesOrderedX::new(3);
        
        // Push 5 futures with indices 0, 1, 2, 3, 4
        for i in 0..5 {
            let delay = if i == 1 {
                // Future with index 1 never completes
                Duration::from_secs(999999)
            } else {
                // All other futures complete quickly
                Duration::from_millis(10)
            };
            
            ordered_stream.push(async move {
                sleep(delay).await;
                i
            });
        }
        
        // Should get future 0
        assert_eq!(ordered_stream.next().await, Some(0));
        
        // Now waiting for future 1, but it never completes
        // Futures 2, 3, 4 have completed and are queued
        // This next() call will hang forever
        let result = tokio::time::timeout(
            Duration::from_secs(1),
            ordered_stream.next()
        ).await;
        
        // Demonstrates the hang - timeout expires
        assert!(result.is_err(), "Stream should hang waiting for index 1");
        
        // In production, this causes:
        // 1. Restoration process hangs indefinitely
        // 2. Node cannot join network
        // 3. Memory accumulates with queued futures 2, 3, 4
        // 4. No automatic recovery without manual intervention
    }
    
    #[tokio::test]
    async fn test_byzantine_backup_source_simulation() {
        use std::sync::Arc;
        use tokio::sync::Mutex;
        
        // Simulate Byzantine backup storage that hangs on chunk 2
        let byzantine_chunk_index = 2;
        let chunks_completed = Arc::new(Mutex::new(vec![]));
        
        let mut ordered_stream = FuturesOrderedX::new(5);
        
        for i in 0..5 {
            let completed = chunks_completed.clone();
            ordered_stream.push(async move {
                if i == byzantine_chunk_index {
                    // Byzantine storage hangs on this chunk
                    sleep(Duration::from_secs(999999)).await;
                } else {
                    // Normal chunks complete quickly
                    sleep(Duration::from_millis(50)).await;
                }
                completed.lock().await.push(i);
                i
            });
        }
        
        // Successfully restore chunks 0 and 1
        assert_eq!(ordered_stream.next().await, Some(0));
        assert_eq!(ordered_stream.next().await, Some(1));
        
        // Restoration hangs here waiting for chunk 2
        let hang_result = tokio::time::timeout(
            Duration::from_secs(2),
            ordered_stream.next()
        ).await;
        
        assert!(hang_result.is_err(), "Restoration hangs on Byzantine chunk");
        
        // Verify chunks 3 and 4 actually completed but are stuck in queue
        sleep(Duration::from_millis(200)).await;
        let completed = chunks_completed.lock().await;
        assert!(completed.contains(&3), "Chunk 3 completed but not returned");
        assert!(completed.contains(&4), "Chunk 4 completed but not returned");
        assert!(!completed.contains(&2), "Byzantine chunk 2 never completes");
        
        // This demonstrates the critical vulnerability:
        // - Byzantine storage hangs ONE chunk
        // - ALL subsequent restoration is blocked forever
        // - Node cannot join network
        // - Requires manual intervention to recover
    }
}
```

## Notes

This vulnerability is particularly dangerous because:

1. **Silent failure mode**: The restoration appears to be making progress initially, then hangs without clear error messages
2. **Cascading impact**: If backup storage is centralized, many nodes could be affected simultaneously
3. **No automatic recovery**: Operators must manually diagnose and reconfigure
4. **Memory leak potential**: Unbounded accumulation in `queued_outputs` can cause OOM crashes
5. **Trust assumption violation**: The code assumes backup storage is benign, but restoration should be Byzantine-fault-tolerant with cryptographic verification (which exists) AND liveness guarantees (which don't exist)

The vulnerability fundamentally stems from the ordered stream guarantee being incompatible with untrusted/Byzantine data sources without timeout protection.

### Citations

**File:** storage/backup/backup-cli/src/utils/stream/futures_ordered_x.rs (L124-148)
```rust
    fn poll_next(mut self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Option<Self::Item>> {
        let this = &mut *self;

        // Check to see if we've already received the next value
        if let Some(next_output) = this.queued_outputs.peek_mut() {
            if next_output.index == this.next_outgoing_index {
                this.next_outgoing_index += 1;
                return Poll::Ready(Some(PeekMut::pop(next_output).data));
            }
        }

        loop {
            match ready!(this.in_progress_queue.poll_next_unpin(cx)) {
                Some(output) => {
                    if output.index == this.next_outgoing_index {
                        this.next_outgoing_index += 1;
                        return Poll::Ready(Some(output.data));
                    } else {
                        this.queued_outputs.push(output)
                    }
                },
                None => return Poll::Ready(None),
            }
        }
    }
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L96-105)
```rust
    pub async fn run(self) -> Result<()> {
        let name = self.name();
        let start = Instant::now();
        info!("{} started. Manifest: {}", name, self.manifest_handle);
        self.run_impl()
            .await
            .map_err(|e| anyhow!("{} failed: {}", name, e))?;
        info!(time = start.elapsed().as_secs(), "{} succeeded.", name);
        Ok(())
    }
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L187-226)
```rust
        let futs_iter = chunks.into_iter().enumerate().map(|(chunk_idx, chunk)| {
            let storage = storage.clone();
            async move {
                tokio::spawn(async move {
                    let blobs = Self::read_state_value(&storage, chunk.blobs.clone()).await?;
                    let proof = storage.load_bcs_file(&chunk.proof).await?;
                    Result::<_>::Ok((chunk_idx, chunk, blobs, proof))
                })
                .await?
            }
        });
        let con = self.concurrent_downloads;
        let mut futs_stream = stream::iter(futs_iter).buffered_x(con * 2, con);
        let mut start = None;
        while let Some((chunk_idx, chunk, mut blobs, proof)) = futs_stream.try_next().await? {
            start = start.or_else(|| Some(Instant::now()));
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["add_state_chunk"]);
            let receiver = receiver.clone();
            if self.validate_modules {
                blobs = tokio::task::spawn_blocking(move || {
                    Self::validate_modules(&blobs);
                    blobs
                })
                .await?;
            }
            tokio::task::spawn_blocking(move || {
                receiver.lock().as_mut().unwrap().add_chunk(blobs, proof)
            })
            .await??;
            leaf_idx.set(chunk.last_idx as i64);
            info!(
                chunk = chunk_idx,
                chunks_to_add = chunks_to_add,
                last_idx = chunk.last_idx,
                values_per_second = ((chunk.last_idx + 1 - start_idx) as f64
                    / start.as_ref().unwrap().elapsed().as_secs_f64())
                    as u64,
                "State chunk added.",
            );
        }
```

**File:** storage/backup/backup-cli/src/utils/read_record_bytes.rs (L20-42)
```rust
    async fn read_full_buf_or_none(&mut self, buf: &mut BytesMut) -> Result<()> {
        assert_eq!(buf.len(), 0);
        let n_expected = buf.capacity();

        loop {
            let n_read = self.read_buf(buf).await.err_notes("")?;
            let n_read_total = buf.len();
            if n_read_total == n_expected {
                return Ok(());
            }
            if n_read == 0 {
                if n_read_total == 0 {
                    return Ok(());
                } else {
                    bail!(
                        "Hit EOF before filling the whole buffer, read {}, expected {}",
                        n_read_total,
                        n_expected
                    );
                }
            }
        }
    }
```

**File:** storage/backup/backup-cli/src/storage/command_adapter/mod.rs (L114-124)
```rust
    async fn open_for_read(
        &self,
        file_handle: &FileHandleRef,
    ) -> Result<Box<dyn AsyncRead + Send + Unpin>> {
        let child = self
            .cmd(&self.config.commands.open_for_read, vec![
                EnvVar::file_handle(file_handle.to_string()),
            ])
            .spawn()?;
        Ok(Box::new(child.into_data_source()))
    }
```
