# Audit Report

## Title
Cache Worker Denial of Service via Unrecoverable Task Failure Panic

## Summary
The indexer-grpc-cache-worker crashes completely when any Redis operation or transaction processing task fails, due to an unconditional `panic!()` at line 429 that terminates the process instead of allowing error recovery and reconnection. This creates a denial-of-service vulnerability where operational issues or resource exhaustion triggered by high transaction volume can crash the indexer API infrastructure.

## Finding Description

The cache worker's `process_streaming_response()` function processes transaction batches from a fullnode gRPC stream by spawning parallel async tasks. Each task encodes transactions and writes them to Redis cache. [1](#0-0) 

When a `BatchEnd` signal is received, the worker awaits all accumulated tasks using `join_all()`. If any task fails, the worker unconditionally panics: [2](#0-1) 

**Task Failure Scenarios:**
1. **Redis Operation Failures**: Network issues, Redis OOM, connection timeouts [3](#0-2) 

2. **Resource Exhaustion**: The cache worker spawns tasks without concurrency limits and accumulates them until BatchEnd. [4](#0-3)  With gRPC configured to accept unlimited message sizes [5](#0-4) , high transaction volume can cause memory pressure.

**Attack Path:**
An attacker can flood the blockchain with transactions (up to consensus limits of 64KB per regular transaction, 1MB for governance transactions [6](#0-5) ). While each transaction has bounded size, sustained high volume causes:
- Many concurrent tasks spawned and held in memory
- Redis receives bursts of SET commands
- Memory pressure in cache worker or Redis
- Task failures trigger panic, crashing the entire process

**Why Error Recovery Fails:**
The `run()` method has a reconnection loop designed to restart after errors: [7](#0-6) 

However, `panic!()` terminates the process immediately, bypassing this error recovery mechanism. If the panic were replaced with `return Err()`, the worker would reconnect and resume.

## Impact Explanation

**Severity: High** per Aptos Bug Bounty criteria - "API crashes"

The cache worker is a critical component of the indexer infrastructure that serves transaction data to dApps and external services. Crashing it causes:
- Indexer API unavailability
- Disruption of downstream applications relying on real-time transaction data
- Manual intervention required to restart the service
- Potential data gaps if not restarted promptly

While this doesn't affect consensus or blockchain state (the cache worker is off-chain infrastructure), it directly impacts the availability of indexer APIs, which matches the High Severity category.

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability can be triggered by:
1. **Operational Issues**: Redis connection problems, memory constraints (common in production)
2. **High Load Conditions**: Network congestion, transaction spikes during busy periods
3. **Intentional Attack**: Adversary floods blockchain with maximum-size transactions (requires gas payment but feasible)

The lack of concurrency limits and backpressure mechanisms makes the system fragile under load. Any transient Redis failure becomes a complete crash rather than a recoverable error.

## Recommendation

Replace the panic with proper error handling that returns to the reconnection loop:

```rust
// In process_streaming_response() at line 418-430
GrpcDataStatus::BatchEnd {
    start_version,
    num_of_transactions,
} => {
    let result = join_all(tasks_to_run).await;
    if result
        .iter()
        .any(|r| r.is_err() || r.as_ref().unwrap().is_err())
    {
        error!(
            start_version = start_version,
            num_of_transactions = num_of_transactions,
            "[Indexer Cache] Process transactions from fullnode failed."
        );
        ERROR_COUNT.with_label_values(&["response_error"]).inc();
        // Return error instead of panic to allow reconnection
        return Err(anyhow::anyhow!(
            "Task failure during batch processing at version {}",
            start_version
        ));
    }
    // ... rest of the code
}
```

**Additional Mitigations:**
1. Add concurrency limits using a semaphore to bound parallel task count
2. Implement circuit breaker pattern for Redis operations
3. Add retry logic with exponential backoff for transient failures
4. Monitor task queue size and apply backpressure when needed

## Proof of Concept

```rust
// Reproduction steps:
// 1. Start cache worker connected to Redis
// 2. Temporarily disrupt Redis (network partition, stop Redis server, or exhaust memory)
// 3. Let cache worker receive transaction batch from fullnode
// 4. When BatchEnd arrives, tasks fail due to Redis unavailability
// 5. Observe: Worker panics and terminates completely
// 6. Expected: Worker should log error and reconnect

// Simulated test scenario:
#[tokio::test]
async fn test_cache_worker_redis_failure_recovery() {
    // Setup mock Redis that fails after N operations
    let failing_redis = setup_failing_redis_mock(failure_after: 10);
    
    // Setup cache worker with failing Redis
    let mut worker = Worker::new(
        fullnode_url,
        failing_redis,
        file_store_config,
        true
    ).await.unwrap();
    
    // Simulate transaction stream with BatchEnd
    // Current behavior: panic! terminates process
    // Expected behavior: Error returned, reconnection loop retries
    
    let result = worker.run().await;
    // This test would currently fail with panic
    // After fix: should return Err and allow reconnection
    assert!(result.is_err());
}
```

**Notes:**

This vulnerability is specific to the cache worker's error handling design. While individual transaction data cannot directly cause encoding failures (protobuf/LZ4 encoding is robust), the panic-on-any-failure pattern creates brittleness that makes the system vulnerable to operational issues and resource exhaustion under high load. The fix is straightforward: replace panic with error return to enable the existing reconnection logic.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L109-180)
```rust
    pub async fn run(&mut self) -> Result<()> {
        // Re-connect if lost.
        loop {
            let conn = self
                .redis_client
                .get_tokio_connection_manager()
                .await
                .context("Get redis connection failed.")?;
            let mut rpc_client = create_grpc_client(self.fullnode_grpc_address.clone()).await;

            // 1. Fetch metadata.
            let file_store_operator: Box<dyn FileStoreOperator> = self.file_store.create();
            // TODO: move chain id check somewhere around here
            // This ensures that metadata is created before we start the cache worker
            let mut starting_version = file_store_operator.get_latest_version().await;
            while starting_version.is_none() {
                starting_version = file_store_operator.get_latest_version().await;
                tracing::warn!(
                    "[Indexer Cache] File store metadata not found. Waiting for {} ms.",
                    FILE_STORE_METADATA_WAIT_MS
                );
                tokio::time::sleep(std::time::Duration::from_millis(
                    FILE_STORE_METADATA_WAIT_MS,
                ))
                .await;
            }

            // There's a guarantee at this point that starting_version is not null
            let starting_version = starting_version.unwrap();

            let file_store_metadata = file_store_operator.get_file_store_metadata().await.unwrap();

            tracing::info!(
                service_type = SERVICE_TYPE,
                "[Indexer Cache] Starting cache worker with version {}",
                starting_version
            );

            // 2. Start streaming RPC.
            let request = tonic::Request::new(GetTransactionsFromNodeRequest {
                starting_version: Some(starting_version),
                ..Default::default()
            });

            let response = rpc_client
                .get_transactions_from_node(request)
                .await
                .with_context(|| {
                    format!(
                        "Failed to get transactions from node at starting version {}",
                        starting_version
                    )
                })?;
            info!(
                service_type = SERVICE_TYPE,
                "[Indexer Cache] Streaming RPC started."
            );
            // 3&4. Infinite streaming until error happens. Either stream ends or worker crashes.
            process_streaming_response(
                conn,
                self.cache_storage_format,
                file_store_metadata,
                response.into_inner(),
            )
            .await?;

            info!(
                service_type = SERVICE_TYPE,
                "[Indexer Cache] Streaming RPC ended."
            );
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L213-275)
```rust
            let task: JoinHandle<anyhow::Result<()>> = tokio::spawn({
                let first_transaction = data
                    .transactions
                    .first()
                    .context("There were unexpectedly no transactions in the response")?;
                let first_transaction_version = first_transaction.version;
                let last_transaction = data
                    .transactions
                    .last()
                    .context("There were unexpectedly no transactions in the response")?;
                let last_transaction_version = last_transaction.version;
                let start_version = first_transaction.version;
                let first_transaction_pb_timestamp = first_transaction.timestamp;
                let last_transaction_pb_timestamp = last_transaction.timestamp;

                log_grpc_step(
                    SERVICE_TYPE,
                    IndexerGrpcStep::CacheWorkerReceivedTxns,
                    Some(start_version as i64),
                    Some(last_transaction_version as i64),
                    first_transaction_pb_timestamp.as_ref(),
                    last_transaction_pb_timestamp.as_ref(),
                    Some(data_download_duration_in_secs),
                    Some(size_in_bytes),
                    Some((last_transaction_version + 1 - first_transaction_version) as i64),
                    None,
                );

                let cache_update_start_time = std::time::Instant::now();

                async move {
                    // Push to cache.
                    match cache_operator_clone
                        .update_cache_transactions(data.transactions)
                        .await
                    {
                        Ok(_) => {
                            log_grpc_step(
                                SERVICE_TYPE,
                                IndexerGrpcStep::CacheWorkerTxnsProcessed,
                                Some(first_transaction_version as i64),
                                Some(last_transaction_version as i64),
                                first_transaction_pb_timestamp.as_ref(),
                                last_transaction_pb_timestamp.as_ref(),
                                Some(cache_update_start_time.elapsed().as_secs_f64()),
                                Some(size_in_bytes),
                                Some(
                                    (last_transaction_version + 1 - first_transaction_version)
                                        as i64,
                                ),
                                None,
                            );
                            Ok(())
                        },
                        Err(e) => {
                            ERROR_COUNT
                                .with_label_values(&["failed_to_update_cache_version"])
                                .inc();
                            bail!("Update cache with version failed: {}", e);
                        },
                    }
                }
            });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L354-403)
```rust
    let mut tasks_to_run = vec![];
    // 4. Process the streaming response.
    loop {
        let download_start_time = std::time::Instant::now();
        let received = match resp_stream.next().await {
            Some(r) => r,
            _ => {
                error!(
                    service_type = SERVICE_TYPE,
                    "[Indexer Cache] Streaming error: no response."
                );
                ERROR_COUNT.with_label_values(&["streaming_error"]).inc();
                break;
            },
        };
        // 10 batches doewnload + slowest processing& uploading task
        let received: TransactionsFromNodeResponse = match received {
            Ok(r) => r,
            Err(err) => {
                error!(
                    service_type = SERVICE_TYPE,
                    "[Indexer Cache] Streaming error: {}", err
                );
                ERROR_COUNT.with_label_values(&["streaming_error"]).inc();
                break;
            },
        };

        if received.chain_id as u64 != fullnode_chain_id as u64 {
            panic!("[Indexer Cache] Chain id mismatch happens during data streaming.");
        }

        let size_in_bytes = received.encoded_len();
        match process_transactions_from_node_response(
            received,
            &mut cache_operator,
            download_start_time,
        )
        .await
        {
            Ok(status) => match status {
                GrpcDataStatus::ChunkDataOk {
                    num_of_transactions,
                    task,
                } => {
                    current_version += num_of_transactions;
                    transaction_count += num_of_transactions;
                    tps_calculator.tick_now(num_of_transactions);

                    tasks_to_run.push(task);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L418-430)
```rust
                    let result = join_all(tasks_to_run).await;
                    if result
                        .iter()
                        .any(|r| r.is_err() || r.as_ref().unwrap().is_err())
                    {
                        error!(
                            start_version = start_version,
                            num_of_transactions = num_of_transactions,
                            "[Indexer Cache] Process transactions from fullnode failed."
                        );
                        ERROR_COUNT.with_label_values(&["response_error"]).inc();
                        panic!("Error happens when processing transactions from fullnode.");
                    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/cache_operator.rs (L306-313)
```rust
        let redis_result: RedisResult<()> =
            redis_pipeline.query_async::<_, _>(&mut self.conn).await;

        match redis_result {
            Ok(_) => Ok(()),
            Err(err) => Err(err.into()),
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/lib.rs (L44-46)
```rust
                Ok(client
                    .max_decoding_message_size(usize::MAX)
                    .max_encoding_message_size(usize::MAX)
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L73-81)
```rust
            max_transaction_size_in_bytes: NumBytes,
            "max_transaction_size_in_bytes",
            64 * 1024
        ],
        [
            max_transaction_size_in_bytes_gov: NumBytes,
            { RELEASE_V1_13.. => "max_transaction_size_in_bytes.gov" },
            1024 * 1024
        ],
```
