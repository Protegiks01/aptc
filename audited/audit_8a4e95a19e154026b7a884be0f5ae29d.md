# Audit Report

## Title
Out-of-Memory Vulnerability in TransactionPruner Initialization Causing Unrecoverable Validator Node Crash

## Summary
The `get_pruning_candidate_transactions()` function in TransactionPruner pre-allocates a vector with capacity `(end - start)` without any bounds checking. During pruner initialization/catch-up, if the TransactionPruner has fallen behind due to operational issues, the gap between its stored progress and the current metadata progress can be millions of versions, causing the function to attempt allocating memory for millions of Transaction objects, leading to Out-of-Memory (OOM) crashes and creating an unrecoverable crash loop on validator node restart.

## Finding Description

The vulnerability exists in the initialization flow of the ledger pruner subsystem. When a validator node starts, `TransactionPruner::new()` is called, which attempts to "catch up" the pruner by calling `prune()` with the gap between its last stored progress and the current metadata progress. [1](#0-0) 

The critical issue is on line 101, where `myself.prune(progress, metadata_progress)` is called directly during initialization without any batch size enforcement. This calls the `DBSubPruner::prune()` implementation: [2](#0-1) 

Which in turn calls `get_pruning_candidate_transactions()` with the full gap: [3](#0-2) 

Line 121 allocates a Vec with capacity `(end - start)`, and the misleading comment on lines 119-120 claims this is safe because "it's capped by the max number of txns we prune in a single batch." **This is only true during normal operation, NOT during initialization.**

During normal operation, the LedgerPruner correctly enforces batch_size limits: [4](#0-3) 

However, this batch limiting logic is bypassed during the initialization catch-up phase in each sub-pruner's constructor.

**How the gap forms:**

1. During normal operation, all sub-pruners (including TransactionPruner) are called in parallel and should stay in sync
2. If TransactionPruner encounters an error, bug, or crash that prevents it from updating its progress metadata while other pruners continue, a gap forms
3. The stored progress in the database becomes stale (e.g., version 0 or 10M) while metadata_progress advances (e.g., to 90M)
4. On node restart, `get_or_initialize_subpruner_progress()` retrieves the old stored progress [5](#0-4) 

5. The catch-up then tries to allocate Vec for 90 million transactions

**Memory calculation:**
The Transaction enum is large (note the clippy warning): [6](#0-5) 

With variants containing SignedTransaction, WriteSetPayload, BlockMetadata, etc., each transaction easily exceeds 500 bytes. For 90 million transactions:
- Memory needed: 90M Ã— ~500 bytes = **45+ GB**
- Most validator nodes cannot allocate this much memory at once
- Results in OOM crash during initialization

## Impact Explanation

This vulnerability meets **High Severity** criteria per the Aptos bug bounty program:

**Validator node crashes**: The OOM condition causes immediate validator node termination during initialization.

**Unrecoverable crash loop**: Since the issue occurs during initialization, the node crashes every time it attempts to restart, creating a persistent availability failure that cannot be recovered without manual database intervention.

**Network availability impact**: If multiple validators are affected by the same underlying issue (e.g., a bug that causes TransactionPruner progress updates to fail), this could impact network liveness and consensus participation.

The default prune_window is 90 million versions, providing ample opportunity for large gaps to form: [7](#0-6) 

## Likelihood Explanation

**Likelihood: Medium-High**

This vulnerability can be triggered by realistic operational scenarios:

1. **Silent pruner failures**: A subtle bug in TransactionPruner that causes silent failures or prevents progress updates while the node continues operating
2. **Crash during pruning**: Node crashes after some sub-pruners update progress but before TransactionPruner persists its progress
3. **Database corruption**: Partial corruption affecting TransactionPruner progress metadata
4. **Extended downtime with pruning issues**: Node runs for weeks with pruning partially disabled or failing

The gap accumulates over time - with default batch_size of 5,000 and if pruning runs multiple times per day, a gap of millions of versions can form over days or weeks of degraded operation.

## Recommendation

**Fix: Enforce batch size limits during initialization catch-up**

Modify `TransactionPruner::new()` to respect batch_size when catching up:

```rust
pub(in crate::pruner) fn new(
    transaction_store: Arc<TransactionStore>,
    ledger_db: Arc<LedgerDb>,
    metadata_progress: Version,
    internal_indexer_db: Option<InternalIndexerDB>,
) -> Result<Self> {
    let progress = get_or_initialize_subpruner_progress(
        ledger_db.transaction_db_raw(),
        &DbMetadataKey::TransactionPrunerProgress,
        metadata_progress,
    )?;

    let myself = TransactionPruner {
        transaction_store,
        ledger_db,
        internal_indexer_db,
    };

    // Catch up in batches to avoid OOM
    const CATCH_UP_BATCH_SIZE: u64 = 5_000;
    let mut current = progress;
    while current < metadata_progress {
        let target = std::cmp::min(current + CATCH_UP_BATCH_SIZE, metadata_progress);
        info!(
            progress = current,
            target = target,
            "Catching up TransactionPruner in batches."
        );
        myself.prune(current, target)?;
        current = target;
    }

    Ok(myself)
}
```

**Alternative: Add capacity limit in `get_pruning_candidate_transactions()`**

```rust
fn get_pruning_candidate_transactions(
    &self,
    start: Version,
    end: Version,
) -> Result<Vec<(Version, Transaction)>> {
    ensure!(end >= start, "{} must be >= {}", end, start);
    
    // Enforce maximum capacity to prevent OOM
    const MAX_BATCH_CAPACITY: usize = 10_000;
    let requested_capacity = (end - start) as usize;
    ensure!(
        requested_capacity <= MAX_BATCH_CAPACITY,
        "Requested capacity {} exceeds maximum {}", 
        requested_capacity, 
        MAX_BATCH_CAPACITY
    );

    let mut iter = self
        .ledger_db
        .transaction_db_raw()
        .iter::<TransactionSchema>()?;
    iter.seek(&start)?;

    let mut txns = Vec::with_capacity(requested_capacity);
    for item in iter {
        let (version, txn) = item?;
        if version >= end {
            break;
        }
        txns.push((version, txn));
    }

    Ok(txns)
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use aptos_temppath::TempPath;
    use aptos_types::transaction::Transaction;
    
    #[test]
    #[should_panic(expected = "memory allocation")]
    fn test_transaction_pruner_oom_on_large_gap() {
        // Setup: Create a database with transactions
        let tmp_dir = TempPath::new();
        let db = AptosDB::new_for_test(&tmp_dir);
        
        // Insert 100 transactions
        for i in 0..100 {
            let txn = Transaction::StateCheckpoint(HashValue::random());
            db.save_transactions(&[txn], i, i, None, true, None).unwrap();
        }
        
        // Manually set TransactionPruner progress to 0
        db.ledger_db
            .transaction_db_raw()
            .put::<DbMetadataSchema>(
                &DbMetadataKey::TransactionPrunerProgress,
                &DbMetadataValue::Version(0),
            )
            .unwrap();
        
        // Set metadata progress to simulate a large gap
        // In reality this could be 90M, but even 1M will demonstrate the issue
        let large_gap = 1_000_000_u64;
        
        // This will attempt to allocate Vec for 1M transactions, causing OOM
        let transaction_store = Arc::new(TransactionStore::new(Arc::clone(&db.ledger_db)));
        let _pruner = TransactionPruner::new(
            transaction_store,
            Arc::clone(&db.ledger_db),
            large_gap,
            None,
        );
        
        // If we reach here, the test failed - it should have OOM'd
        panic!("Expected OOM but allocation succeeded");
    }
}
```

**Notes:**
- This vulnerability affects all sub-pruners that load data in bulk during catch-up (EventStorePruner may have similar issues)
- The issue is masked in normal operation because batch_size is properly enforced by LedgerPruner
- The comment claiming safety is misleading and should be updated even after the fix
- Monitoring should be added to detect when sub-pruner progress lags significantly behind metadata progress

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L37-41)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        let candidate_transactions =
            self.get_pruning_candidate_transactions(current_progress, target_version)?;
        self.ledger_db
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L78-104)
```rust
    pub(in crate::pruner) fn new(
        transaction_store: Arc<TransactionStore>,
        ledger_db: Arc<LedgerDb>,
        metadata_progress: Version,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.transaction_db_raw(),
            &DbMetadataKey::TransactionPrunerProgress,
            metadata_progress,
        )?;

        let myself = TransactionPruner {
            transaction_store,
            ledger_db,
            internal_indexer_db,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up TransactionPruner."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L106-131)
```rust
    fn get_pruning_candidate_transactions(
        &self,
        start: Version,
        end: Version,
    ) -> Result<Vec<(Version, Transaction)>> {
        ensure!(end >= start, "{} must be >= {}", end, start);

        let mut iter = self
            .ledger_db
            .transaction_db_raw()
            .iter::<TransactionSchema>()?;
        iter.seek(&start)?;

        // The capacity is capped by the max number of txns we prune in a single batch. It's a
        // relatively small number set in the config, so it won't cause high memory usage here.
        let mut txns = Vec::with_capacity((end - start) as usize);
        for item in iter {
            let (version, txn) = item?;
            if version >= end {
                break;
            }
            txns.push((version, txn));
        }

        Ok(txns)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L62-76)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning ledger data."
            );
            self.ledger_metadata_pruner
                .prune(progress, current_batch_target_version)?;
```

**File:** storage/aptosdb/src/pruner/pruner_utils.rs (L44-60)
```rust
pub(crate) fn get_or_initialize_subpruner_progress(
    sub_db: &DB,
    progress_key: &DbMetadataKey,
    metadata_progress: Version,
) -> Result<Version> {
    Ok(
        if let Some(v) = sub_db.get::<DbMetadataSchema>(progress_key)? {
            v.expect_version()
        } else {
            sub_db.put::<DbMetadataSchema>(
                progress_key,
                &DbMetadataValue::Version(metadata_progress),
            )?;
            metadata_progress
        },
    )
}
```

**File:** types/src/transaction/mod.rs (L2943-2977)
```rust
#[allow(clippy::large_enum_variant)]
#[cfg_attr(any(test, feature = "fuzzing"), derive(Arbitrary))]
#[derive(Clone, Debug, Eq, PartialEq, Serialize, Deserialize, CryptoHasher, BCSCryptoHash)]
pub enum Transaction {
    /// Transaction submitted by the user. e.g: P2P payment transaction, publishing module
    /// transaction, etc.
    /// TODO: We need to rename SignedTransaction to SignedUserTransaction, as well as all the other
    ///       transaction types we had in our codebase.
    UserTransaction(SignedTransaction),

    /// Transaction that applies a WriteSet to the current storage, it's applied manually via aptos-db-bootstrapper.
    GenesisTransaction(WriteSetPayload),

    /// Transaction to update the block metadata resource at the beginning of a block,
    /// when on-chain randomness is disabled.
    BlockMetadata(BlockMetadata),

    /// Transaction to let the executor update the global state tree and record the root hash
    /// in the TransactionInfo
    /// The hash value inside is unique block id which can generate unique hash of state checkpoint transaction
    StateCheckpoint(HashValue),

    /// Transaction that only proposed by a validator mainly to update on-chain configs.
    ValidatorTransaction(ValidatorTransaction),

    /// Transaction to update the block metadata resource at the beginning of a block,
    /// when on-chain randomness is enabled.
    BlockMetadataExt(BlockMetadataExt),

    /// Transaction to let the executor update the global state tree and record the root hash
    /// in the TransactionInfo
    /// The hash value inside is unique block id which can generate unique hash of state checkpoint transaction
    /// Replaces StateCheckpoint, with optionally having more data.
    BlockEpilogue(BlockEpiloguePayload),
}
```

**File:** config/src/config/storage_config.rs (L387-396)
```rust
impl Default for LedgerPrunerConfig {
    fn default() -> Self {
        LedgerPrunerConfig {
            enable: true,
            prune_window: 90_000_000,
            batch_size: 5_000,
            user_pruning_window_offset: 200_000,
        }
    }
}
```
