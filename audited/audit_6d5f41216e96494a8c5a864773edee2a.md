# Audit Report

## Title
Memory Exhaustion via Unbounded Fragment Accumulation in Network Streaming Protocol

## Summary
The network layer's fragment reassembly mechanism lacks timeout protection, allowing attackers to cause memory exhaustion on validator nodes by initiating multiple incomplete message streams. While `read_u16frame()` itself imposes no rate limiting or cumulative size limits (by design as a low-level framing utility), the vulnerability manifests at the application layer where fragments accumulate in unbounded vectors without timeout enforcement.

## Finding Description

The `read_u16frame()` function in `network/netcore/src/framing.rs` is a low-level utility that reads length-prefixed frames up to 65535 bytes (u16::MAX): [1](#0-0) 

This function imposes **no rate limiting** and **no cumulative size limits** - it simply reads one frame at a time. However, the actual vulnerability exists in how these frames are processed at higher layers.

The Noise protocol layer uses fixed-size buffers (65535 bytes) that are reused, preventing memory growth: [2](#0-1) [3](#0-2) 

The critical vulnerability occurs at the application streaming layer. Large messages exceeding `MAX_FRAME_SIZE` (4 MiB) are split into fragments that are reassembled by `InboundStream`: [4](#0-3) 

**Key Issues:**
1. Fragment data is appended to growing `Vec` buffers (lines 206-209) up to `MAX_MESSAGE_SIZE` (64 MiB)
2. **No timeout exists** for fragment reassembly - streams can remain incomplete indefinitely
3. Each connection maintains one `InboundStreamBuffer` with accumulated fragment data [5](#0-4) 

**Attack Path:**
1. Attacker opens `MAX_INBOUND_CONNECTIONS` (100) connections to a validator
2. For each connection, sends a `StreamHeader` indicating maximum fragments (16 fragments, calculated as 64 MiB / 4 MiB)
3. Sends first fragment to each connection, then stops or sends subsequent fragments very slowly (under rate limit of 100 KiB/s)
4. Keeps connections alive by responding to health check pings
5. Each incomplete stream holds up to 64 MiB of allocated memory
6. Total memory consumed: 100 connections × 64 MiB = **6.4 GB** [6](#0-5) 

The health checker will not disconnect these peers as long as they respond to pings: [7](#0-6) 

## Impact Explanation

This qualifies as **High Severity** under the Aptos bug bounty program criteria:
- **Validator node slowdowns**: Memory pressure from 6.4 GB allocation causes performance degradation
- **Potential total loss of liveness**: On validators with constrained memory (<16 GB), this could trigger OOM conditions or severe swap-induced performance collapse

While not guaranteed to cause total validator failure on all systems, the impact escalates based on:
- Available system memory vs attack-induced allocation (6.4 GB)
- Concurrent network/consensus/execution memory demands
- Operating system OOM killer behavior

The invariant **"Resource Limits: All operations must respect gas, storage, and computational limits"** is violated - there is no limit on how long fragment data can be held in memory.

## Likelihood Explanation

**Likelihood: Medium to High**

**Attacker Requirements:**
- Ability to establish 100 inbound connections (standard network peer capability)
- Optionally, multiple IP addresses to bypass per-IP rate limiting (but not required - attacker can send slowly from one IP)
- Basic understanding of the streaming protocol

**Feasibility:**
- Attack requires no special privileges or validator access
- Health check mechanism does not prevent the attack (attacker can respond to pings while holding memory)
- Rate limiting slows but does not prevent the attack (100 KiB/s × 100 connections = 10 MB/s aggregate)
- Connection limits bound the maximum impact but do not prevent exploitation

**Time to Impact:**
- With single IP (100 KiB/s): ~18 hours to reach 6.4 GB
- With 10 IPs (1 MB/s aggregate): ~1.8 hours to reach 6.4 GB
- Attack can be sustained indefinitely once memory is allocated

## Recommendation

Implement timeout-based cleanup for incomplete fragment streams:

```rust
// In network/framework/src/protocols/stream/mod.rs
pub struct InboundStreamBuffer {
    stream: Option<InboundStream>,
    max_fragments: usize,
    // Add timestamp tracking
    stream_started_at: Option<std::time::Instant>,
}

impl InboundStreamBuffer {
    pub fn new(max_fragments: usize) -> Self {
        Self {
            stream: None,
            max_fragments,
            stream_started_at: None,
        }
    }

    pub fn new_stream(&mut self, header: StreamHeader) -> anyhow::Result<()> {
        let inbound_stream = InboundStream::new(header, self.max_fragments)?;
        self.stream_started_at = Some(std::time::Instant::now());
        if let Some(old) = self.stream.replace(inbound_stream) {
            bail!("Discarding existing stream for request ID: {}", old.request_id)
        } else {
            Ok(())
        }
    }

    // Add timeout check method
    pub fn check_timeout(&mut self, timeout: std::time::Duration) -> bool {
        if let Some(started_at) = self.stream_started_at {
            if started_at.elapsed() > timeout {
                warn!("Stream fragment reassembly timed out after {:?}", timeout);
                self.stream = None;
                self.stream_started_at = None;
                return true;
            }
        }
        false
    }
}
```

Add periodic timeout checks in the peer event loop and configure reasonable timeout (e.g., 60 seconds) for fragment reassembly completion.

## Proof of Concept

```rust
// PoC demonstrating memory accumulation without cleanup
use aptos_network::protocols::stream::{InboundStreamBuffer, StreamHeader, StreamFragment};
use aptos_network::protocols::wire::messaging::v1::NetworkMessage;
use std::time::Duration;

#[tokio::test]
async fn test_fragment_memory_exhaustion() {
    const MAX_FRAGMENTS: usize = 16;
    const FRAGMENT_SIZE: usize = 4 * 1024 * 1024; // 4 MiB
    const NUM_STREAMS: usize = 100;
    
    let mut buffers = Vec::new();
    
    // Simulate attacker opening 100 connections
    for stream_id in 0..NUM_STREAMS {
        let mut buffer = InboundStreamBuffer::new(MAX_FRAGMENTS);
        
        // Send header for large message
        let header = StreamHeader {
            request_id: stream_id as u32,
            num_fragments: MAX_FRAGMENTS as u8,
            message: NetworkMessage::DirectSendMsg(/* ... */),
        };
        buffer.new_stream(header).unwrap();
        
        // Send only first fragment, leaving stream incomplete
        let fragment = StreamFragment {
            request_id: stream_id as u32,
            fragment_id: 1,
            raw_data: vec![0u8; FRAGMENT_SIZE],
        };
        buffer.append_fragment(fragment).unwrap();
        
        buffers.push(buffer);
    }
    
    // Memory now holds: 100 streams × 4 MiB (first fragment) = 400 MiB minimum
    // If attacker sends more fragments: up to 100 × 64 MiB = 6.4 GB
    
    // Sleep to simulate time passing - streams never timeout
    tokio::time::sleep(Duration::from_secs(3600)).await;
    
    // Memory still held - no cleanup mechanism exists
    assert_eq!(buffers.len(), NUM_STREAMS);
}
```

## Notes

While the question specifically focuses on `read_u16frame()`, the function itself is a low-level framing utility that correctly performs its intended purpose. The actual vulnerability lies in the **absence of timeout enforcement** at the application layer's fragment reassembly mechanism, which allows unbounded memory accumulation across multiple connections. The attack leverages the composition of multiple system properties: connection limits (100), message size limits (64 MiB), and lack of reassembly timeouts.

### Citations

**File:** network/netcore/src/framing.rs (L9-22)
```rust
pub async fn read_u16frame<'stream, 'buf, 'c, TSocket>(
    mut stream: &'stream mut TSocket,
    buf: &'buf mut BytesMut,
) -> Result<()>
where
    'stream: 'c,
    'buf: 'c,
    TSocket: AsyncRead + Unpin,
{
    let len = read_u16frame_len(&mut stream).await?;
    buf.resize(len as usize, 0);
    stream.read_exact(buf.as_mut()).await?;
    Ok(())
}
```

**File:** crates/aptos-crypto/src/noise.rs (L79-80)
```rust
/// A noise message cannot be larger than 65535 bytes as per the specification.
pub const MAX_SIZE_NOISE_MSG: usize = 65535;
```

**File:** network/framework/src/noise/stream.rs (L408-422)
```rust
struct NoiseBuffers {
    /// A read buffer, used for both a received ciphertext and then for its decrypted content.
    read_buffer: [u8; noise::MAX_SIZE_NOISE_MSG],
    /// A write buffer, used for both a plaintext to send, and then its encrypted version.
    write_buffer: [u8; noise::MAX_SIZE_NOISE_MSG],
}

impl NoiseBuffers {
    fn new() -> Self {
        Self {
            read_buffer: [0; noise::MAX_SIZE_NOISE_MSG],
            write_buffer: [0; noise::MAX_SIZE_NOISE_MSG],
        }
    }
}
```

**File:** network/framework/src/protocols/stream/mod.rs (L164-214)
```rust
    fn append_fragment(&mut self, mut fragment: StreamFragment) -> anyhow::Result<bool> {
        // Verify the stream request ID and fragment request ID
        ensure!(
            self.request_id == fragment.request_id,
            "Stream fragment from a different request! Expected {}, got {}.",
            self.request_id,
            fragment.request_id
        );

        // Verify the fragment ID
        let fragment_id = fragment.fragment_id;
        ensure!(fragment_id > 0, "Fragment ID must be greater than zero!");
        ensure!(
            fragment_id <= self.num_fragments,
            "Fragment ID {} exceeds number of fragments {}!",
            fragment_id,
            self.num_fragments
        );

        // Verify the fragment ID is the expected next fragment
        let expected_fragment_id = self.received_fragment_id.checked_add(1).ok_or_else(|| {
            anyhow::anyhow!(
                "Current fragment ID overflowed when adding 1: {}",
                self.received_fragment_id
            )
        })?;
        ensure!(
            expected_fragment_id == fragment_id,
            "Unexpected fragment ID, expected {}, got {}!",
            expected_fragment_id,
            fragment_id
        );

        // Update the received fragment ID
        self.received_fragment_id = expected_fragment_id;

        // Append the fragment data to the message
        let raw_data = &mut fragment.raw_data;
        match &mut self.message {
            NetworkMessage::Error(_) => {
                panic!("StreamHeader for NetworkMessage::Error(_) should be rejected!")
            },
            NetworkMessage::RpcRequest(request) => request.raw_request.append(raw_data),
            NetworkMessage::RpcResponse(response) => response.raw_response.append(raw_data),
            NetworkMessage::DirectSendMsg(message) => message.raw_msg.append(raw_data),
        }

        // Return whether the stream is complete
        let is_stream_complete = self.received_fragment_id == self.num_fragments;
        Ok(is_stream_complete)
    }
```

**File:** network/framework/src/peer/mod.rs (L168-194)
```rust
        let max_fragments = max_message_size / max_frame_size;
        Self {
            network_context,
            executor,
            time_service: time_service.clone(),
            connection_metadata,
            connection: Some(socket),
            connection_notifs_tx,
            peer_reqs_rx,
            upstream_handlers,
            inbound_rpcs: InboundRpcs::new(
                network_context,
                time_service.clone(),
                remote_peer_id,
                inbound_rpc_timeout,
                max_concurrent_inbound_rpcs,
            ),
            outbound_rpcs: OutboundRpcs::new(
                network_context,
                time_service,
                remote_peer_id,
                max_concurrent_outbound_rpcs,
            ),
            state: State::Connected,
            max_frame_size,
            max_message_size,
            inbound_stream: InboundStreamBuffer::new(max_fragments),
```

**File:** config/src/config/network_config.rs (L38-40)
```rust
pub const PING_INTERVAL_MS: u64 = 10_000;
pub const PING_TIMEOUT_MS: u64 = 20_000;
pub const PING_FAILURES_TOLERATED: u64 = 3;
```

**File:** config/src/config/network_config.rs (L44-53)
```rust
pub const MAX_INBOUND_CONNECTIONS: usize = 100;
pub const MAX_MESSAGE_METADATA_SIZE: usize = 128 * 1024; /* 128 KiB: a buffer for metadata that might be added to messages by networking */
pub const MESSAGE_PADDING_SIZE: usize = 2 * 1024 * 1024; /* 2 MiB: a safety buffer to allow messages to get larger during serialization */
pub const MAX_APPLICATION_MESSAGE_SIZE: usize =
    (MAX_MESSAGE_SIZE - MAX_MESSAGE_METADATA_SIZE) - MESSAGE_PADDING_SIZE; /* The message size that applications should check against */
pub const MAX_FRAME_SIZE: usize = 4 * 1024 * 1024; /* 4 MiB large messages will be chunked into multiple frames and streamed */
pub const MAX_MESSAGE_SIZE: usize = 64 * 1024 * 1024; /* 64 MiB */
pub const CONNECTION_BACKOFF_BASE: u64 = 2;
pub const IP_BYTE_BUCKET_RATE: usize = 102400 /* 100 KiB */;
pub const IP_BYTE_BUCKET_SIZE: usize = IP_BYTE_BUCKET_RATE;
```
