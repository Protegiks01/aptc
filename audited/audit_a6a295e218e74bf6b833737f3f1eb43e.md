# Audit Report

## Title
Infinite Loop in State KV Pruner Due to Zero Batch Size Configuration

## Summary
The `StateKvPruner::prune()` function contains an infinite loop vulnerability when `max_versions` (derived from `batch_size` configuration) is set to 0. The function attempts to prune from `progress` to `target_version`, but when `max_versions` is 0, `current_batch_target_version` equals `progress`, causing no progress to be made and the while loop to never terminate. [1](#0-0) 

## Finding Description
The vulnerability exists in the pruner's batch processing logic. When the pruner is enabled with `batch_size: 0` in the configuration, the following execution path occurs:

1. Configuration loading: The `LedgerPrunerConfig` struct allows `batch_size: 0` through YAML deserialization with no validation [2](#0-1) 

2. The `NO_OP_STORAGE_PRUNER_CONFIG` constant explicitly sets `batch_size: 0` when pruning is disabled [3](#0-2) , but nothing prevents setting `enable: true` with `batch_size: 0`.

3. Configuration validation: The `ConfigSanitizer` implementation validates `prune_window` and `user_pruning_window_offset` but completely omits `batch_size` validation [4](#0-3) 

4. Pruner initialization: `StateKvPrunerManager` creates a `PrunerWorker` with the configured `batch_size` when `enable: true` [5](#0-4) 

5. Pruner execution: The worker continuously calls `pruner.prune(batch_size)` in its work loop [6](#0-5) 

6. Infinite loop trigger: When `max_versions == 0`, the calculation `current_batch_target_version = min(progress + 0, target_version)` yields `progress`, causing `progress` to never advance beyond its initial value, making the condition `progress < target_version` perpetually true.

**The same vulnerability exists in `LedgerPruner::prune()`** [7](#0-6) , affecting multiple critical pruning subsystems.

This breaks the **Resource Limits** invariant (operations must respect computational limits) and causes a liveness failure.

## Impact Explanation
This is a **High Severity** vulnerability per Aptos bug bounty criteria for "Validator node slowdowns":

- **Immediate impact**: The pruner worker thread enters an infinite busy loop consuming 100% CPU on that thread
- **Database growth**: Without functional pruning, the database grows unbounded, eventually exhausting disk space
- **Node degradation**: High CPU usage impacts other node operations including consensus participation
- **Operational failure**: Once disk space is exhausted, the node stops functioning entirely
- **Network-wide risk**: If multiple validators misconfigure this parameter, network health degrades

While not directly causing consensus violations, this severely impacts node **availability** and **liveness**, which are critical for blockchain operations.

## Likelihood Explanation
**Likelihood: Medium to High**

This vulnerability can occur through:

1. **Honest misconfiguration**: An operator accidentally sets `batch_size: 0` in the YAML config while intending to disable the pruner (should set `enable: false` instead)
2. **Configuration template errors**: Copy-pasting the `NO_OP_STORAGE_PRUNER_CONFIG` values with `enable: true`
3. **Automated configuration systems**: Scripts that generate configs with edge case values
4. **Configuration updates**: Operators modifying configs without understanding the implications

The lack of any validation or warnings makes this error easy to introduce and difficult to diagnose before deployment.

## Recommendation
Add strict validation for `batch_size` in multiple layers:

**1. Configuration validation in `ConfigSanitizer`:**

Add to the `ConfigSanitizer` implementation in `storage_config.rs`:
```rust
// After line 706, add:
let ledger_batch_size = config.storage_pruner_config.ledger_pruner_config.batch_size;
let state_merkle_batch_size = config.storage_pruner_config.state_merkle_pruner_config.batch_size;
let epoch_snapshot_batch_size = config.storage_pruner_config.epoch_snapshot_pruner_config.batch_size;

if config.storage_pruner_config.ledger_pruner_config.enable && ledger_batch_size == 0 {
    return Err(Error::ConfigSanitizerFailed(
        sanitizer_name,
        "ledger_pruner batch_size cannot be 0 when pruner is enabled. Set to at least 1.".to_string(),
    ));
}

if config.storage_pruner_config.state_merkle_pruner_config.enable && state_merkle_batch_size == 0 {
    return Err(Error::ConfigSanitizerFailed(
        sanitizer_name,
        "state_merkle_pruner batch_size cannot be 0 when pruner is enabled. Set to at least 1.".to_string(),
    ));
}

if config.storage_pruner_config.epoch_snapshot_pruner_config.enable && epoch_snapshot_batch_size == 0 {
    return Err(Error::ConfigSanitizerFailed(
        sanitizer_name,
        "epoch_snapshot_pruner batch_size cannot be 0 when pruner is enabled. Set to at least 1.".to_string(),
    ));
}
```

**2. Runtime guard in pruner implementations:**

Add defensive check at the start of each `prune()` function:
```rust
fn prune(&self, max_versions: usize) -> Result<Version> {
    if max_versions == 0 {
        return Err(anyhow::anyhow!(
            "{} pruner called with max_versions=0, which would cause infinite loop",
            self.name()
        ));
    }
    // ... rest of function
}
```

**3. Update default configurations and documentation** to explicitly warn against setting `batch_size: 0`.

## Proof of Concept

**Rust Test Reproduction:**

```rust
#[test]
#[should_panic(expected = "infinite loop")]
fn test_state_kv_pruner_zero_batch_size_infinite_loop() {
    use std::sync::Arc;
    use std::time::Duration;
    use std::thread;
    
    // Setup: Create a StateKvPruner instance with test StateKvDb
    let tmpdir = tempfile::tempdir().unwrap();
    let state_kv_db = Arc::new(StateKvDb::new_for_test(&tmpdir));
    let pruner = Arc::new(StateKvPruner::new(state_kv_db).unwrap());
    
    // Set target version ahead of progress to trigger pruning
    pruner.set_target_version(100);
    pruner.record_progress(0);
    
    // Spawn pruning thread with batch_size = 0
    let pruner_clone = Arc::clone(&pruner);
    let handle = thread::spawn(move || {
        // This should hang indefinitely
        let _ = pruner_clone.prune(0);
    });
    
    // Wait 5 seconds - if thread is still running, we have an infinite loop
    thread::sleep(Duration::from_secs(5));
    
    // Thread should have completed by now if working correctly
    assert!(
        handle.is_finished(),
        "infinite loop detected: pruner thread still running after 5 seconds with batch_size=0"
    );
}
```

**Configuration-based reproduction:**

1. Create a node config YAML with:
```yaml
storage:
  storage_pruner_config:
    ledger_pruner_config:
      enable: true
      prune_window: 90000000
      batch_size: 0  # Triggers infinite loop
      user_pruning_window_offset: 200000
```

2. Start the validator node with this configuration
3. Once pruning is triggered (by committing transactions beyond the prune window), observe:
   - High CPU usage on pruner thread
   - Progress metrics frozen at initial value
   - Database continuing to grow without pruning
   - Node logs showing repeated "Pruning state kv data" messages with no progress

**Notes:**
This vulnerability affects production deployments where configuration errors can have severe consequences. The lack of validation transforms an operator mistake into a critical availability issue requiring node restart and configuration correction.

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L55-83)
```rust
        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning state kv data."
            );
            self.metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state kv shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning state kv data is done.");
        }
```

**File:** config/src/config/storage_config.rs (L306-323)
```rust
pub const NO_OP_STORAGE_PRUNER_CONFIG: PrunerConfig = PrunerConfig {
    ledger_pruner_config: LedgerPrunerConfig {
        enable: false,
        prune_window: 0,
        batch_size: 0,
        user_pruning_window_offset: 0,
    },
    state_merkle_pruner_config: StateMerklePrunerConfig {
        enable: false,
        prune_window: 0,
        batch_size: 0,
    },
    epoch_snapshot_pruner_config: EpochSnapshotPrunerConfig {
        enable: false,
        prune_window: 0,
        batch_size: 0,
    },
};
```

**File:** config/src/config/storage_config.rs (L327-341)
```rust
pub struct LedgerPrunerConfig {
    /// Boolean to enable/disable the ledger pruner. The ledger pruner is responsible for pruning
    /// everything else except for states (e.g. transactions, events etc.)
    pub enable: bool,
    /// This is the default pruning window for any other store except for state store. State store
    /// being big in size, we might want to configure a smaller window for state store vs other
    /// store.
    pub prune_window: u64,
    /// Batch size of the versions to be sent to the ledger pruner - this is to avoid slowdown due to
    /// issuing too many DB calls and batch prune instead. For ledger pruner, this means the number
    /// of versions to prune a time.
    pub batch_size: usize,
    /// The offset for user pruning window to adjust
    pub user_pruning_window_offset: u64,
}
```

**File:** config/src/config/storage_config.rs (L682-798)
```rust
impl ConfigSanitizer for StorageConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let config = &node_config.storage;

        let ledger_prune_window = config
            .storage_pruner_config
            .ledger_pruner_config
            .prune_window;
        let state_merkle_prune_window = config
            .storage_pruner_config
            .state_merkle_pruner_config
            .prune_window;
        let epoch_snapshot_prune_window = config
            .storage_pruner_config
            .epoch_snapshot_pruner_config
            .prune_window;
        let user_pruning_window_offset = config
            .storage_pruner_config
            .ledger_pruner_config
            .user_pruning_window_offset;

        if ledger_prune_window < 50_000_000 {
            warn!("Ledger prune_window is too small, harming network data availability.");
        }
        if state_merkle_prune_window < 100_000 {
            warn!("State Merkle prune_window is too small, node might stop functioning.");
        }
        if epoch_snapshot_prune_window < 50_000_000 {
            warn!("Epoch snapshot prune_window is too small, harming network data availability.");
        }
        if user_pruning_window_offset > 1_000_000 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "user_pruning_window_offset too large, so big a buffer is unlikely necessary. Set something < 1 million.".to_string(),
            ));
        }
        if user_pruning_window_offset > ledger_prune_window {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "user_pruning_window_offset is larger than the ledger prune window, the API will refuse to return any data.".to_string(),
            ));
        }

        if let Some(db_path_overrides) = config.db_path_overrides.as_ref() {
            if !config.rocksdb_configs.enable_storage_sharding {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "db_path_overrides is allowed only if sharding is enabled.".to_string(),
                ));
            }

            if let Some(ledger_db_path) = db_path_overrides.ledger_db_path.as_ref() {
                if !ledger_db_path.is_absolute() {
                    return Err(Error::ConfigSanitizerFailed(
                        sanitizer_name,
                        format!(
                            "Path {ledger_db_path:?} in db_path_overrides is not an absolute path."
                        ),
                    ));
                }
            }

            if let Some(state_kv_db_path) = db_path_overrides.state_kv_db_path.as_ref() {
                if let Some(metadata_path) = state_kv_db_path.metadata_path.as_ref() {
                    if !metadata_path.is_absolute() {
                        return Err(Error::ConfigSanitizerFailed(
                            sanitizer_name,
                            format!("Path {metadata_path:?} in db_path_overrides is not an absolute path."),
                        ));
                    }
                }

                if let Err(e) = state_kv_db_path.get_shard_paths() {
                    return Err(Error::ConfigSanitizerFailed(sanitizer_name, e.to_string()));
                }
            }

            if let Some(state_merkle_db_path) = db_path_overrides.state_merkle_db_path.as_ref() {
                if let Some(metadata_path) = state_merkle_db_path.metadata_path.as_ref() {
                    if !metadata_path.is_absolute() {
                        return Err(Error::ConfigSanitizerFailed(
                            sanitizer_name,
                            format!("Path {metadata_path:?} in db_path_overrides is not an absolute path."),
                        ));
                    }
                }

                if let Err(e) = state_merkle_db_path.get_shard_paths() {
                    return Err(Error::ConfigSanitizerFailed(sanitizer_name, e.to_string()));
                }
            }

            if let Some(hot_state_merkle_db_path) =
                db_path_overrides.hot_state_merkle_db_path.as_ref()
            {
                if let Some(metadata_path) = hot_state_merkle_db_path.metadata_path.as_ref() {
                    if !metadata_path.is_absolute() {
                        return Err(Error::ConfigSanitizerFailed(
                            sanitizer_name,
                            format!("Path {metadata_path:?} in db_path_overrides is not an absolute path."),
                        ));
                    }
                }

                if let Err(e) = hot_state_merkle_db_path.get_shard_paths() {
                    return Err(Error::ConfigSanitizerFailed(sanitizer_name, e.to_string()));
                }
            }
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_pruner_manager.rs (L84-126)
```rust
    pub fn new(state_kv_db: Arc<StateKvDb>, state_kv_pruner_config: LedgerPrunerConfig) -> Self {
        let pruner_worker = if state_kv_pruner_config.enable {
            Some(Self::init_pruner(
                Arc::clone(&state_kv_db),
                state_kv_pruner_config,
            ))
        } else {
            None
        };

        let min_readable_version =
            pruner_utils::get_state_kv_pruner_progress(&state_kv_db).expect("Must succeed.");

        PRUNER_VERSIONS
            .with_label_values(&["state_kv_pruner", "min_readable"])
            .set(min_readable_version as i64);

        Self {
            state_kv_db,
            prune_window: state_kv_pruner_config.prune_window,
            pruner_worker,
            pruning_batch_size: state_kv_pruner_config.batch_size,
            min_readable_version: AtomicVersion::new(min_readable_version),
        }
    }

    fn init_pruner(
        state_kv_db: Arc<StateKvDb>,
        state_kv_pruner_config: LedgerPrunerConfig,
    ) -> PrunerWorker {
        let pruner =
            Arc::new(StateKvPruner::new(state_kv_db).expect("Failed to create state kv pruner."));

        PRUNER_WINDOW
            .with_label_values(&["state_kv_pruner"])
            .set(state_kv_pruner_config.prune_window as i64);

        PRUNER_BATCH_SIZE
            .with_label_values(&["state_kv_pruner"])
            .set(state_kv_pruner_config.batch_size as i64);

        PrunerWorker::new(pruner, state_kv_pruner_config.batch_size, "state_kv")
    }
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L53-69)
```rust
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L62-92)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning ledger data."
            );
            self.ledger_metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning ledger data is done.");
        }

        Ok(target_version)
    }
```
