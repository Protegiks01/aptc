# Audit Report

## Title
Consensus Round Advancement Without State Synchronization on ResetDropped Error

## Summary
When `sync_to_target()` encounters a `ResetDropped` error due to a crashed buffer manager, the consensus round is advanced via `process_certificates()` without actually synchronizing the validator's state or fetching missing blocks. This creates a round/state inconsistency where the validator advances to a higher round but lacks the corresponding blockchain data.

## Finding Description
The vulnerability exists in the error handling flow of the consensus synchronization mechanism. The attack path is:

1. **Buffer Manager Crash**: The buffer manager task exits (due to epoch end, panic, or other failure), dropping its `reset_rx` receiver. [1](#0-0) 

2. **Sync Attempt with ResetDropped**: When `sync_to_target()` is called, it attempts to reset the buffer manager first. The reset fails because the receiver is dropped, returning `Error::ResetDropped`. The actual state sync is never executed. [2](#0-1) [3](#0-2) 

3. **Round Advancement Despite Failure**: In `sync_up()`, the error from `add_certs()` is stored, but `process_certificates()` is called regardless with the `await?` operator. This advances the validator's round to `sync_info.highest_round() + 1` even though the blocks were never fetched. [4](#0-3) [5](#0-4) 

4. **Error Logging Without Recovery**: The error propagates up to the event loop where it's logged but the round advancement persists. [6](#0-5) 

**Broken Invariant**: The validator's consensus round no longer matches its actual synchronized state, violating the requirement that validators only participate at rounds they have fully synchronized to.

## Impact Explanation
This qualifies as **Medium Severity** based on the following:

- **State Inconsistency**: The validator's `current_round` advances without corresponding block data, creating a round/ledger mismatch that requires manual intervention or automatic recovery mechanisms.

- **Individual Validator Liveness Impact**: The affected validator cannot vote on subsequent proposals because `insert_block()` will fail when the parent block doesn't exist (due to failed sync). This prevents the validator from participating until recovery. [7](#0-6) 

- **Not Network-Wide**: The issue affects only individual validators whose buffer managers have crashed, not the entire network. Other validators continue normal operation.

- **No Consensus Safety Violation**: Because the affected validator cannot vote without having the parent blocks, it cannot cause consensus splits or safety violations.

## Likelihood Explanation  
**Likelihood: Medium**

The vulnerability triggers when:
1. A buffer manager crashes or exits abnormally (can occur due to bugs, race conditions, or resource exhaustion)
2. The validator receives sync info from peers and attempts to synchronize
3. No recovery mechanism exists to detect the round/state mismatch

This can occur in production environments during:
- Epoch transitions if timing is incorrect
- Resource exhaustion scenarios
- Software bugs causing buffer manager panics
- Abnormal shutdown sequences

## Recommendation
Implement proper state validation after `process_certificates()` to ensure round advancement only occurs when sync succeeds:

```rust
async fn sync_up(&mut self, sync_info: &SyncInfo, author: Author) -> anyhow::Result<()> {
    let local_sync_info = self.block_store.sync_info();
    if sync_info.has_newer_certificates(&local_sync_info) {
        // Verify sync_info
        sync_info.verify(&self.epoch_state.verifier).map_err(|e| {
            error!(SecurityEvent::InvalidSyncInfoMsg, ...);
            VerifyError::from(e)
        })?;
        
        SYNC_INFO_RECEIVED_WITH_NEWER_CERT.inc();
        
        // FIX: Only process certificates if add_certs succeeds
        let result = self
            .block_store
            .add_certs(sync_info, self.create_block_retriever(author))
            .await;
        
        // Only advance round if sync succeeded
        if result.is_ok() {
            self.process_certificates().await?;
        }
        
        result
    } else {
        Ok(())
    }
}
```

Additionally, detect and handle buffer manager failures explicitly:
- Check buffer manager health before advancing rounds
- Implement automatic recovery when ResetDropped is detected
- Add fail-safe mechanisms to prevent participation when execution pipeline is dead

## Proof of Concept

```rust
// Reproduction scenario (conceptual - would require integration test setup)

// 1. Start a validator with buffer manager running
// 2. Crash the buffer manager task (simulate via kill or panic injection)
// 3. Trigger sync via SyncInfo message from peer with higher round
// 4. Observe:
//    - sync_to_target() fails with ResetDropped error
//    - process_certificates() still advances current_round
//    - Validator is now at round N but only has blocks up to round M (M < N)
//    - Validator cannot vote on new proposals (parent not found error)
//    - Error is logged but validator remains in broken state

// Key assertion to verify the vulnerability:
assert!(validator.current_round() > validator.committed_round() + expected_gap);
// This gap should not exist after a failed sync
```

To test, inject a fail point in the buffer manager to cause early exit, then send SyncInfo to trigger synchronization and observe the round/state mismatch.

## Notes
The TODO comment at line 669-670 of `execution_client.rs` acknowledges a related issue about handling sync failures after reset succeeds, but the current vulnerability is about reset itself failing. Both scenarios need proper recovery mechanisms to maintain consistency.

### Citations

**File:** consensus/src/pipeline/buffer_manager.rs (L912-996)
```rust
    pub async fn start(mut self) {
        info!("Buffer manager starts.");
        let (verified_commit_msg_tx, mut verified_commit_msg_rx) = create_channel();
        let mut interval = tokio::time::interval(Duration::from_millis(LOOP_INTERVAL_MS));
        let mut commit_msg_rx = self.commit_msg_rx.take().expect("commit msg rx must exist");
        let epoch_state = self.epoch_state.clone();
        let bounded_executor = self.bounded_executor.clone();
        spawn_named!("buffer manager verification", async move {
            while let Some((sender, commit_msg)) = commit_msg_rx.next().await {
                let tx = verified_commit_msg_tx.clone();
                let epoch_state_clone = epoch_state.clone();
                bounded_executor
                    .spawn(async move {
                        match commit_msg.req.verify(sender, &epoch_state_clone.verifier) {
                            Ok(_) => {
                                let _ = tx.unbounded_send(commit_msg);
                            },
                            Err(e) => warn!("Invalid commit message: {}", e),
                        }
                    })
                    .await;
            }
        });
        while !self.stop {
            // advancing the root will trigger sending requests to the pipeline
            ::tokio::select! {
                Some(blocks) = self.block_rx.next(), if !self.need_back_pressure() => {
                    self.latest_round = blocks.latest_round();
                    monitor!("buffer_manager_process_ordered", {
                    self.process_ordered_blocks(blocks).await;
                    if self.execution_root.is_none() {
                        self.advance_execution_root();
                    }});
                },
                Some(reset_event) = self.reset_rx.next() => {
                    monitor!("buffer_manager_process_reset",
                    self.process_reset_request(reset_event).await);
                },
                Some(response) = self.execution_schedule_phase_rx.next() => {
                    monitor!("buffer_manager_process_execution_schedule_response", {
                    self.process_execution_schedule_response(response).await;
                })},
                Some(response) = self.execution_wait_phase_rx.next() => {
                    monitor!("buffer_manager_process_execution_wait_response", {
                    self.process_execution_response(response).await;
                    self.advance_execution_root();
                    if self.signing_root.is_none() {
                        self.advance_signing_root().await;
                    }});
                },
                Some(response) = self.signing_phase_rx.next() => {
                    monitor!("buffer_manager_process_signing_response", {
                    self.process_signing_response(response).await;
                    self.advance_signing_root().await
                    })
                },
                Some(Ok(round)) = self.persisting_phase_rx.next() => {
                    // see where `need_backpressure()` is called.
                    self.pending_commit_votes = self.pending_commit_votes.split_off(&(round + 1));
                    self.highest_committed_round = round;
                    self.pending_commit_blocks = self.pending_commit_blocks.split_off(&(round + 1));
                },
                Some(rpc_request) = verified_commit_msg_rx.next() => {
                    monitor!("buffer_manager_process_commit_message",
                    if let Some(aggregated_block_id) = self.process_commit_message(rpc_request) {
                        self.advance_head(aggregated_block_id).await;
                        if self.execution_root.is_none() {
                            self.advance_execution_root();
                        }
                        if self.signing_root.is_none() {
                            self.advance_signing_root().await;
                        }
                    });
                }
                _ = interval.tick().fuse() => {
                    monitor!("buffer_manager_process_interval_tick", {
                    self.update_buffer_manager_metrics();
                    self.rebroadcast_commit_votes_if_needed().await
                    });
                },
                // no else branch here because interval.tick will always be available
            }
        }
        info!("Buffer manager stops.");
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L661-672)
```rust
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });

        // Reset the rand and buffer managers to the target round
        self.reset(&target).await?;

        // TODO: handle the state sync error (e.g., re-push the ordered
        // blocks to the buffer manager when it's reset but sync fails).
        self.execution_proxy.sync_to_target(target).await
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L674-709)
```rust
    async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
        let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager) = {
            let handle = self.handle.read();
            (
                handle.reset_tx_to_rand_manager.clone(),
                handle.reset_tx_to_buffer_manager.clone(),
            )
        };

        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        }

        if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
            // reset execution phase and commit phase
            let (tx, rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::ResetDropped)?;
            rx.await.map_err(|_| Error::ResetDropped)?;
        }

        Ok(())
    }
```

**File:** consensus/src/round_manager.rs (L878-907)
```rust
    async fn sync_up(&mut self, sync_info: &SyncInfo, author: Author) -> anyhow::Result<()> {
        let local_sync_info = self.block_store.sync_info();
        if sync_info.has_newer_certificates(&local_sync_info) {
            info!(
                self.new_log(LogEvent::ReceiveNewCertificate)
                    .remote_peer(author),
                "Local state {},\n remote state {}", local_sync_info, sync_info
            );
            // Some information in SyncInfo is ahead of what we have locally.
            // First verify the SyncInfo (didn't verify it in the yet).
            sync_info.verify(&self.epoch_state.verifier).map_err(|e| {
                error!(
                    SecurityEvent::InvalidSyncInfoMsg,
                    sync_info = sync_info,
                    remote_peer = author,
                    error = ?e,
                );
                VerifyError::from(e)
            })?;
            SYNC_INFO_RECEIVED_WITH_NEWER_CERT.inc();
            let result = self
                .block_store
                .add_certs(sync_info, self.create_block_retriever(author))
                .await;
            self.process_certificates().await?;
            result
        } else {
            Ok(())
        }
    }
```

**File:** consensus/src/round_manager.rs (L1500-1505)
```rust
    async fn vote_block(&mut self, proposed_block: Block) -> anyhow::Result<Vote> {
        let block_arc = self
            .block_store
            .insert_block(proposed_block)
            .await
            .context("[RoundManager] Failed to execute_and_insert the block")?;
```

**File:** consensus/src/round_manager.rs (L2136-2142)
```rust
                        match result {
                            Ok(_) => trace!(RoundStateLogSchema::new(round_state)),
                            Err(e) => {
                                counters::ERROR_COUNT.inc();
                                warn!(kind = error_kind(&e), RoundStateLogSchema::new(round_state), "Error: {:#}", e);
                            }
                        }
```

**File:** consensus/src/liveness/round_state.rs (L245-289)
```rust
    pub fn process_certificates(
        &mut self,
        sync_info: SyncInfo,
        verifier: &ValidatorVerifier,
    ) -> Option<NewRoundEvent> {
        if sync_info.highest_ordered_round() > self.highest_ordered_round {
            self.highest_ordered_round = sync_info.highest_ordered_round();
        }
        let new_round = sync_info.highest_round() + 1;
        if new_round > self.current_round {
            let (prev_round_votes, prev_round_timeout_votes) = self.pending_votes.drain_votes();

            // Start a new round.
            self.current_round = new_round;
            self.pending_votes = PendingVotes::new();
            self.vote_sent = None;
            self.timeout_sent = None;
            let timeout = self.setup_timeout(1);

            let (prev_round_timeout_votes, prev_round_timeout_reason) = prev_round_timeout_votes
                .map(|votes| votes.unpack_aggregate(verifier))
                .unzip();

            // The new round reason is QCReady in case both QC.round + 1 == new_round, otherwise
            // it's Timeout and TC.round + 1 == new_round.
            let new_round_reason = if sync_info.highest_certified_round() + 1 == new_round {
                NewRoundReason::QCReady
            } else {
                let prev_round_timeout_reason =
                    prev_round_timeout_reason.unwrap_or(RoundTimeoutReason::Unknown);
                NewRoundReason::Timeout(prev_round_timeout_reason)
            };

            let new_round_event = NewRoundEvent {
                round: self.current_round,
                reason: new_round_reason,
                timeout,
                prev_round_votes,
                prev_round_timeout_votes,
            };
            info!(round = new_round, "Starting new round: {}", new_round_event);
            return Some(new_round_event);
        }
        None
    }
```
