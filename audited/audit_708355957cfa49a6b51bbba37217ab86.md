# Audit Report

## Title
BatchKey Collision Vulnerability Enabling Byzantine Validator to Cause Batch State Confusion

## Summary
The `BatchKey` structure uses only `(author, batch_id)` as the unique identifier for batches in the `BatchProofQueue`, omitting the cryptographic `digest` field. This allows a Byzantine validator to create two different batches with identical `BatchKey` values but different transaction contents, causing internal state inconsistencies in victim validators' proof queues. [1](#0-0) 

## Finding Description

The `BatchKey::from_info()` function creates a key using only the `author` (PeerId) and `batch_id` fields, completely ignoring the `digest` field that contains the cryptographic hash of the batch's transactions. [2](#0-1) 

This key is used as the primary identifier in `BatchProofQueue.items` HashMap: [3](#0-2) 

**Attack Scenario:**

1. Byzantine validator M creates two batches with the same `(author, batch_id)` but different transaction contents:
   - Batch1: `(author=M, batch_id=100, digest=Hash(TxnSet1), txns=TxnSet1)`
   - Batch2: `(author=M, batch_id=100, digest=Hash(TxnSet2), txns=TxnSet2)`

2. Both batches are validly signed and pass verification individually.

3. The `BatchStore` accepts both batches because it keys by `digest` (not BatchKey): [4](#0-3) 

4. When victim validator receives batches in sequence:
   - First: `insert_batches()` receives Batch1's transaction summaries
   - Second: `insert_proof()` receives Batch2's proof

5. Due to BatchKey collision at `(M, 100)`, the `QueueItem` becomes inconsistent: [5](#0-4) 

Result: `QueueItem.info = Batch1`, `QueueItem.proof = ProofOfStore(Batch2)`, `QueueItem.txn_summaries = TxnSet1`

6. When `pull_proofs()` is called, it uses Batch1's `txn_summaries` for duplicate filtering but returns Batch2's proof: [6](#0-5) 

This causes incorrect duplicate detection: transactions in TxnSet2 are not properly filtered because the filtering logic uses TxnSet1's summaries.

## Impact Explanation

**High Severity** - This qualifies as a "Significant protocol violation" under the Aptos bug bounty program because:

1. **Transaction Filtering Bypass**: The duplicate detection mechanism is subverted. Transactions that should be filtered as duplicates may be included because filtering uses wrong transaction summaries.

2. **Accounting Corruption**: The `txn_summary_num_occurrences` tracking becomes corrupted, affecting back-pressure calculations: [7](#0-6) 

3. **Non-Deterministic Block Construction**: Different validators may pull different proofs (Batch1 vs Batch2) depending on timing, leading to divergent block proposals. While consensus eventually agrees on one block, this increases latency and creates unnecessary disagreement.

4. **Potential Double-Processing**: If both batches propagate through different code paths, their transactions might both be processed, violating the "Deterministic Execution" invariant.

## Likelihood Explanation

**Medium Likelihood**:
- Requires a Byzantine validator to deliberately violate the batch_id increment protocol
- The validator must control batch creation and network propagation timing
- The attack is not detected by existing validation logic
- The Byzantine validator can be any validator in the active set (no collusion required beyond the 1/3 Byzantine assumption)
- The attack window exists whenever batches are being created and proofs generated

## Recommendation

Include the `digest` field in the `BatchKey` to ensure uniqueness:

```rust
#[derive(PartialEq, Eq, Hash, Clone, Debug)]
pub struct BatchKey {
    author: PeerId,
    batch_id: BatchId,
    digest: HashValue,  // Add this field
}

impl BatchKey {
    pub fn from_info(info: &BatchInfoExt) -> Self {
        Self {
            author: info.author(),
            batch_id: info.batch_id(),
            digest: *info.digest(),  // Include digest
        }
    }
}
```

This ensures that batches with different transaction contents cannot collide in the HashMap, maintaining the integrity of the proof queue's internal state.

## Proof of Concept

**Note**: This vulnerability requires validator-level access and cannot be demonstrated with a simple Move test. A full PoC would require:

1. Modified validator that deliberately reuses batch_ids
2. Network simulation with multiple validators
3. Observation of inconsistent QueueItem states

**Conceptual Rust test outline**:

```rust
#[test]
fn test_batch_key_collision() {
    // Create two different batches with same (author, batch_id)
    let batch1 = create_batch(author, batch_id, txns1);
    let batch2 = create_batch(author, batch_id, txns2);
    
    // Both have different digests
    assert_ne!(batch1.digest(), batch2.digest());
    
    // But same BatchKey
    assert_eq!(
        BatchKey::from_info(&batch1.info()),
        BatchKey::from_info(&batch2.info())
    );
    
    // Insert batch1 summaries, then batch2 proof
    queue.insert_batches(vec![(batch1.info(), batch1.summaries())]);
    queue.insert_proof(batch2.proof());
    
    // Verify inconsistent state
    let item = queue.items.get(&BatchKey::from_info(&batch1.info()));
    assert_eq!(item.info, batch1.info());  // Batch1 info
    assert_eq!(item.proof.info(), batch2.info());  // Batch2 proof
    // Inconsistency demonstrated
}
```

## Notes

While this vulnerability requires Byzantine validator behavior (which is within the < 1/3 Byzantine assumption of AptosBFT), the lack of digest in BatchKey violates the principle of unique identification for batches. The current implementation assumes validators won't reuse batch_ids, but Byzantine fault tolerance should not rely on such assumptions. Including the digest in BatchKey provides defense-in-depth against Byzantine behavior.

### Citations

**File:** consensus/src/quorum_store/utils.rs (L150-163)
```rust
#[derive(PartialEq, Eq, Hash, Clone, Debug)]
pub struct BatchKey {
    author: PeerId,
    batch_id: BatchId,
}

impl BatchKey {
    pub fn from_info(info: &BatchInfoExt) -> Self {
        Self {
            author: info.author(),
            batch_id: info.batch_id(),
        }
    }
}
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L56-61)
```rust
pub struct BatchProofQueue {
    my_peer_id: PeerId,
    // Queue per peer to ensure fairness between peers and priority within peer
    author_to_batches: HashMap<PeerId, BTreeMap<BatchSortKey, BatchInfoExt>>,
    // Map of Batch key to QueueItem containing Batch data and proofs
    items: HashMap<BatchKey, QueueItem>,
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L216-227)
```rust
        if let Some(txn_summaries) = self
            .items
            .get(&batch_key)
            .and_then(|item| item.txn_summaries.as_ref())
        {
            for txn_summary in txn_summaries {
                *self
                    .txn_summary_num_occurrences
                    .entry(*txn_summary)
                    .or_insert(0) += 1;
            }
        }
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L229-243)
```rust
        match self.items.entry(batch_key) {
            Entry::Occupied(mut entry) => {
                let item = entry.get_mut();
                item.proof = Some(proof);
                item.proof_insertion_time = Some(Instant::now());
            },
            Entry::Vacant(entry) => {
                entry.insert(QueueItem {
                    info: proof.info().clone(),
                    proof: Some(proof),
                    proof_insertion_time: Some(Instant::now()),
                    txn_summaries: None,
                });
            },
        }
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L636-650)
```rust
                    } else {
                        // Calculate the number of unique transactions if this batch is included in the result
                        let unique_txns = if let Some(ref txn_summaries) = item.txn_summaries {
                            cur_unique_txns
                                + txn_summaries
                                    .iter()
                                    .filter(|txn_summary| {
                                        !filtered_txns.contains(txn_summary)
                                            && block_timestamp.as_secs()
                                                < txn_summary.expiration_timestamp_secs
                                    })
                                    .count() as u64
                        } else {
                            cur_unique_txns + batch.num_txns()
                        };
```

**File:** consensus/src/quorum_store/batch_store.rs (L358-382)
```rust
    pub(crate) fn insert_to_cache(
        &self,
        value: &PersistedValue<BatchInfoExt>,
    ) -> anyhow::Result<bool> {
        let digest = *value.digest();
        let author = value.author();
        let expiration_time = value.expiration();

        {
            // Acquire dashmap internal lock on the entry corresponding to the digest.
            let cache_entry = self.db_cache.entry(digest);

            if let Occupied(entry) = &cache_entry {
                match entry.get().expiration().cmp(&expiration_time) {
                    std::cmp::Ordering::Equal => return Ok(false),
                    std::cmp::Ordering::Greater => {
                        debug!(
                            "QS: already have the digest with higher expiration {}",
                            digest
                        );
                        return Ok(false);
                    },
                    std::cmp::Ordering::Less => {},
                }
            };
```
