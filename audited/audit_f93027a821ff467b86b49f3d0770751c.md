# Audit Report

## Title
Non-Deterministic Leader Election Due to Database Query Failure Fallback in `extract_epoch_proposers()`

## Summary
The `extract_epoch_proposers()` function in `EpochManager` silently falls back to using only the current epoch's proposers when the database query for historical epoch data fails. This creates inconsistent leader election across validators, causing them to disagree on the valid proposer for each round, leading to consensus liveness failures.

## Finding Description

The vulnerability exists in the `extract_epoch_proposers()` function which is responsible for building the historical proposer map used by the leader reputation system: [1](#0-0) 

When `get_epoch_ending_ledger_infos()` fails (line 430) or when `extract_epoch_to_proposers()` fails (line 437), the code falls back to returning only the current epoch's proposers (line 444). This fallback is silent except for an error log.

**How the vulnerability manifests:**

1. During epoch initialization, validators call `create_proposer_election()` which invokes `extract_epoch_proposers()`: [2](#0-1) 

2. Different validators may experience the database query failure at different times or not at all due to:
   - Database pruning differences
   - Storage I/O errors
   - Race conditions during epoch transitions
   - Network synchronization delays

3. This creates inconsistent `epoch_to_proposers` maps across validators:
   - Successful validators: `{epoch-N: [...], ..., epoch: [...]}`
   - Failed validators: `{epoch: [...]}`

4. The `LeaderReputation` system uses this map to filter historical NewBlockEvents when computing reputation weights: [3](#0-2) 

5. Validators with incomplete `epoch_to_proposers` data will filter out all historical events from previous epochs, resulting in all validators receiving `inactive_weight` instead of reputation-based weights.

6. Different reputation weights lead to different leader selections via `choose_index()`: [4](#0-3) 

7. When proposals arrive, validators validate the proposer using `is_valid_proposal()`: [5](#0-4) 

This check delegates to: [6](#0-5) 

8. Validators with different leader election results will reject proposals that others accept, causing consensus to stall as the network cannot reach agreement on valid blocks.

**Invariant Violation:**
This breaks the **Deterministic Execution** invariant: "All validators must produce identical state roots for identical blocks." More specifically, it violates the requirement that all validators must deterministically agree on who the valid proposer is for any given round.

## Impact Explanation

**Severity: Medium**

This vulnerability causes **state inconsistencies requiring intervention** and **consensus liveness failures**, meeting the Medium severity criteria in the Aptos bug bounty program.

**Impact Details:**
- **Consensus Liveness Failure**: Validators cannot reach consensus because they disagree on valid proposers, causing block production to halt
- **Network Availability Impact**: The blockchain stops making progress until the inconsistency is resolved
- **No Safety Violation**: Does not cause double-spending, forks, or fund loss because proposals are still cryptographically validated
- **Requires Manual Intervention**: Operators must restart validators or manually synchronize database state to recover

The impact is limited to Medium (not High or Critical) because:
- No fund loss or theft occurs
- No permanent network partition (recoverable with restarts)
- No consensus safety violations (no conflicting blocks committed)
- Network degradation rather than complete failure

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability is likely to occur in production because:

1. **No Malicious Action Required**: The bug can trigger naturally during normal operation
2. **Common Trigger Conditions**:
   - Database pruning operations that remove historical ledger infos
   - Storage hardware failures or I/O errors
   - Validators starting at slightly different times during epoch transitions
   - Network delays causing validators to sync at different rates

3. **Increased Risk During**:
   - Epoch boundaries (when historical data is fetched)
   - Network upgrades (when validators restart)
   - Storage maintenance operations

4. **Silent Failure**: The error is only logged, not exposed to operators, making it hard to detect before consensus failures occur

The vulnerability becomes more likely as:
- The network ages and database pruning becomes more aggressive
- The validator set grows (more opportunity for inconsistency)
- Historical window sizes increase (more epochs to fetch)

## Recommendation

**Fix: Enforce consistency by failing epoch initialization on database query errors**

Instead of silently falling back to incomplete data, the function should fail fast and require operators to resolve the database issue:

```rust
fn extract_epoch_proposers(
    &self,
    epoch_state: &EpochState,
    use_history_from_previous_epoch_max_count: u32,
    proposers: Vec<AccountAddress>,
    needed_rounds: u64,
) -> anyhow::Result<HashMap<u64, Vec<AccountAddress>>> {
    let first_epoch_to_consider = std::cmp::max(
        if epoch_state.epoch == 1 { 1 } else { 2 },
        epoch_state
            .epoch
            .saturating_sub(use_history_from_previous_epoch_max_count as u64),
    );
    
    if epoch_state.epoch > first_epoch_to_consider {
        let proof = self.storage
            .aptos_db()
            .get_epoch_ending_ledger_infos(first_epoch_to_consider - 1, epoch_state.epoch)
            .context("Failed to fetch epoch ending ledger infos for leader reputation")?;
        
        ensure!(
            proof.ledger_info_with_sigs.len() as u64
                == (epoch_state.epoch - (first_epoch_to_consider - 1)),
            "Incomplete epoch ending ledger infos"
        );
        
        extract_epoch_to_proposers(proof, epoch_state.epoch, &proposers, needed_rounds)
    } else {
        Ok(HashMap::from([(epoch_state.epoch, proposers)]))
    }
}
```

**Alternative Fix: Force consistent fallback behavior**

If failing fast is too disruptive, ensure ALL validators use the same fallback:

```rust
// In create_proposer_election, disable leader reputation if historical data unavailable
let epoch_to_proposers = match self.extract_epoch_proposers(...) {
    Ok(map) => map,
    Err(e) => {
        error!("Failed to fetch historical proposers: {:?}. Falling back to rotating proposer.", e);
        // Force all validators to use RotatingProposer instead of LeaderReputation
        return Arc::new(RotatingProposer::new(proposers, 1));
    }
};
```

## Proof of Concept

```rust
#[cfg(test)]
mod test_leader_election_inconsistency {
    use super::*;
    use aptos_consensus_types::common::Author;
    use std::collections::HashMap;

    #[test]
    fn test_inconsistent_epoch_proposers_leads_to_different_leaders() {
        // Simulate two validators with different epoch_to_proposers maps
        let epoch = 10;
        let proposers = vec![
            Author::random(),
            Author::random(), 
            Author::random(),
        ];
        
        // Validator 1: Successfully fetched historical data
        let mut validator1_epoch_to_proposers = HashMap::new();
        validator1_epoch_to_proposers.insert(8, proposers.clone());
        validator1_epoch_to_proposers.insert(9, proposers.clone());
        validator1_epoch_to_proposers.insert(10, proposers.clone());
        
        // Validator 2: DB query failed, only has current epoch
        let mut validator2_epoch_to_proposers = HashMap::new();
        validator2_epoch_to_proposers.insert(10, proposers.clone());
        
        // Both create LeaderReputation with same config but different historical data
        let backend1 = Arc::new(MockBackend::new(/* with historical events */));
        let backend2 = Arc::new(MockBackend::new(/* without historical events */));
        
        let leader_rep1 = LeaderReputation::new(
            epoch,
            validator1_epoch_to_proposers,
            vec![1, 1, 1],
            backend1,
            Box::new(/* heuristic */),
            0,
            false,
            100,
        );
        
        let leader_rep2 = LeaderReputation::new(
            epoch,
            validator2_epoch_to_proposers,
            vec![1, 1, 1],
            backend2,
            Box::new(/* heuristic */),
            0,
            false,
            100,
        );
        
        // ASSERTION: For the same round, they select DIFFERENT leaders
        for round in 1..100 {
            let leader1 = leader_rep1.get_valid_proposer(round);
            let leader2 = leader_rep2.get_valid_proposer(round);
            
            if leader1 != leader2 {
                println!("VULNERABILITY CONFIRMED: Round {} has different leaders!", round);
                println!("Validator 1 expects: {}", leader1);
                println!("Validator 2 expects: {}", leader2);
                return; // Test passes - inconsistency demonstrated
            }
        }
        
        panic!("Expected to find inconsistent leader election");
    }
}
```

**Notes:**
- This vulnerability affects the leader reputation mode specifically when `use_history_from_previous_epoch_max_count > 0`
- The issue is masked in networks using `RotatingProposer` or `FixedProposer` election types
- The error log at line 440-443 provides the only indication this failure occurred
- Recovery requires validator restarts or manual database synchronization
- The issue becomes more severe as the validator set size increases (more validators = higher probability of inconsistency)

### Citations

**File:** consensus/src/epoch_manager.rs (L361-366)
```rust
                let epoch_to_proposers = self.extract_epoch_proposers(
                    epoch_state,
                    use_history_from_previous_epoch_max_count,
                    proposers,
                    (window_size + seek_len) as u64,
                );
```

**File:** consensus/src/epoch_manager.rs (L409-449)
```rust
    fn extract_epoch_proposers(
        &self,
        epoch_state: &EpochState,
        use_history_from_previous_epoch_max_count: u32,
        proposers: Vec<AccountAddress>,
        needed_rounds: u64,
    ) -> HashMap<u64, Vec<AccountAddress>> {
        // Genesis is epoch=0
        // First block (after genesis) is epoch=1, and is the only block in that epoch.
        // It has no votes, so we skip it unless we are in epoch 1, as otherwise it will
        // skew leader elections for exclude_round number of rounds.
        let first_epoch_to_consider = std::cmp::max(
            if epoch_state.epoch == 1 { 1 } else { 2 },
            epoch_state
                .epoch
                .saturating_sub(use_history_from_previous_epoch_max_count as u64),
        );
        // If we are considering beyond the current epoch, we need to fetch validators for those epochs
        if epoch_state.epoch > first_epoch_to_consider {
            self.storage
                .aptos_db()
                .get_epoch_ending_ledger_infos(first_epoch_to_consider - 1, epoch_state.epoch)
                .map_err(Into::into)
                .and_then(|proof| {
                    ensure!(
                        proof.ledger_info_with_sigs.len() as u64
                            == (epoch_state.epoch - (first_epoch_to_consider - 1))
                    );
                    extract_epoch_to_proposers(proof, epoch_state.epoch, &proposers, needed_rounds)
                })
                .unwrap_or_else(|err| {
                    error!(
                        "Couldn't create leader reputation with history across epochs, {:?}",
                        err
                    );
                    HashMap::from([(epoch_state.epoch, proposers)])
                })
        } else {
            HashMap::from([(epoch_state.epoch, proposers)])
        }
    }
```

**File:** consensus/src/liveness/leader_reputation.rs (L322-326)
```rust
        };
        sub_history
            .iter()
            .filter(move |&meta| epoch_to_candidates.contains_key(&meta.epoch()))
    }
```

**File:** consensus/src/liveness/leader_reputation.rs (L704-733)
```rust
        let mut weights =
            self.heuristic
                .get_weights(self.epoch, &self.epoch_to_proposers, &sliding_window);
        let proposers = &self.epoch_to_proposers[&self.epoch];
        assert_eq!(weights.len(), proposers.len());

        // Multiply weights by voting power:
        let stake_weights: Vec<u128> = weights
            .iter_mut()
            .enumerate()
            .map(|(i, w)| *w as u128 * self.voting_powers[i] as u128)
            .collect();

        let state = if self.use_root_hash {
            [
                root_hash.to_vec(),
                self.epoch.to_le_bytes().to_vec(),
                round.to_le_bytes().to_vec(),
            ]
            .concat()
        } else {
            [
                self.epoch.to_le_bytes().to_vec(),
                round.to_le_bytes().to_vec(),
            ]
            .concat()
        };

        let chosen_index = choose_index(stake_weights, state);
        (proposers[chosen_index], voting_power_participation_ratio)
```

**File:** consensus/src/round_manager.rs (L1195-1200)
```rust
        ensure!(
            self.proposer_election.is_valid_proposal(&proposal),
            "[RoundManager] Proposer {} for block {} is not a valid proposer for this round or created duplicate proposal",
            author,
            proposal,
        );
```

**File:** consensus/src/liveness/unequivocal_proposer_election.rs (L46-60)
```rust
    pub fn is_valid_proposal(&self, block: &Block) -> bool {
        block.author().is_some_and(|author| {
            let valid_author = self.is_valid_proposer(author, block.round());
            if !valid_author {
                warn!(
                    SecurityEvent::InvalidConsensusProposal,
                    "Proposal is not from valid author {}, expected {} for round {} and id {}",
                    author,
                    self.get_valid_proposer(block.round()),
                    block.round(),
                    block.id()
                );

                return false;
            }
```
