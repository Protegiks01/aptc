# Audit Report

## Title
Missing Bounds Validation in Node-Checker Baseline Configuration Allows Resource Exhaustion and API Denial of Service

## Summary
The `configuration::run_cmd()` validation function fails to validate numeric bounds for checker configuration parameters, allowing malicious baseline configurations to cause resource exhaustion, indefinite API hangs, and denial of service attacks against both the node-checker service and target nodes being evaluated.

## Finding Description

The node-checker validation logic only verifies that checkers can be instantiated but does not validate the reasonableness of configuration parameter values. [1](#0-0) 

This allows unbounded values in multiple checkers:

**LatencyChecker** accepts `num_samples` (u16, max 65,535) and `delay_between_samples_ms` (u64, max ~18 quintillion milliseconds) without validation. [2](#0-1) 

The checker loops through all samples with sleep delays between each: [3](#0-2) 

**TpsChecker** accepts `repeat_target_count` (usize) which creates that many concurrent connections to the target node: [4](#0-3) 

This value is used directly to duplicate target URLs: [5](#0-4) 

**SyncRunnerConfig** accepts `num_retries` (u8, max 255) and `retry_delay_secs` (u16, max 65,535) without bounds: [6](#0-5) 

These values control retry loops: [7](#0-6) 

When the server loads baseline configurations at startup, it attempts to run checkers against the baseline itself but this validation only catches hard errors, not resource exhaustion issues: [8](#0-7) 

During API requests, the runner executes all configured checkers concurrently: [9](#0-8) 

**Attack Scenario:**
1. Attacker distributes malicious baseline configuration with `num_samples=65535` and `delay_between_samples_ms=3600000` (1 hour per sample)
2. Operator loads this configuration when starting the node-checker service
3. Each health check request triggers the LatencyChecker to run for 65,535 hours (~7.5 years)
4. API becomes completely unresponsive, hanging indefinitely
5. Alternatively, with `repeat_target_count=100000` in TpsChecker, the service creates 100,000 concurrent connections to each target node, causing resource exhaustion on both the node-checker and target nodes

## Impact Explanation

This vulnerability qualifies as **High Severity** per Aptos bug bounty criteria due to "API crashes" - the node-checker API becomes unavailable or unresponsive. While the node-checker is not part of core consensus infrastructure, it:

1. **API Denial of Service**: Causes complete unavailability of the health checking API used by node operators
2. **Resource Exhaustion**: Consumes CPU, memory, and network resources on the node-checker server
3. **Cascading DoS**: TpsChecker with excessive `repeat_target_count` enables DoS attacks against innocent target nodes being evaluated
4. **Operational Impact**: Prevents node operators from validating their infrastructure meets network requirements

The impact is amplified because baseline configurations are typically provided by trusted sources (Aptos Foundation), making operators more likely to trust and load them without scrutiny.

## Likelihood Explanation

**Likelihood: Medium**

The attack requires:
- Social engineering or supply chain compromise to distribute malicious baseline configurations
- Operator action to load the configuration

However:
- Configuration files are loaded directly from filesystem without authentication
- No warnings are shown for extreme parameter values
- Validation passes successfully for malicious configs
- Operators may download baseline configs from community sources or automated pipelines
- Once loaded, every API request triggers the resource exhaustion

The ease of creating malicious configurations (simple YAML file) and the lack of any defensive checks make exploitation straightforward once distribution is achieved.

## Recommendation

Implement comprehensive bounds validation in the configuration validation logic:

```rust
pub fn validate_configuration(node_configuration: &BaselineConfiguration) -> Result<()> {
    build_checkers(&node_configuration.checkers).context("Failed to build Checkers")?;
    
    // Validate runner config bounds
    if node_configuration.runner_config.retry_delay_secs > 300 {
        bail!("retry_delay_secs cannot exceed 300 seconds");
    }
    
    // Validate each checker's configuration
    for checker_config in &node_configuration.checkers {
        match checker_config {
            CheckerConfig::Latency(config) => {
                if config.num_samples > 100 {
                    bail!("num_samples cannot exceed 100");
                }
                if config.delay_between_samples_ms > 10000 {
                    bail!("delay_between_samples_ms cannot exceed 10000ms");
                }
            },
            CheckerConfig::Tps(config) => {
                if config.repeat_target_count > 100 {
                    bail!("repeat_target_count cannot exceed 100");
                }
            },
            CheckerConfig::Hardware(config) => {
                if config.min_cpu_cores > 256 {
                    bail!("min_cpu_cores cannot exceed 256");
                }
                if config.min_ram_gb > 2048 {
                    bail!("min_ram_gb cannot exceed 2048GB");
                }
            },
            _ => {}
        }
    }
    
    Ok(())
}
```

Additionally, implement request timeouts at the API layer to prevent indefinite hangs.

## Proof of Concept

Create a malicious baseline configuration file `malicious_baseline.yaml`:

```yaml
configuration_id: "malicious_test"
configuration_name: "Malicious Test Configuration"
node_address:
  url: "http://localhost"
  api_port: 8080
  metrics_port: 9101

checkers:
  - type: Latency
    num_samples: 65535
    delay_between_samples_ms: 3600000
    num_allowed_errors: 0
    max_api_latency_ms: 1000
```

Steps to reproduce:
1. Start node-checker server with the malicious configuration: `aptos-node-checker server run --baseline-config-paths ./malicious_baseline.yaml`
2. Send a health check request: `curl "http://localhost:20121/api/check?baseline_configuration_id=malicious_test&node_url=http://localhost&api_port=8080"`
3. Observe that the request hangs indefinitely, consuming resources
4. Server becomes unresponsive to all subsequent requests

For TpsChecker DoS amplification, use:
```yaml
checkers:
  - type: Tps
    repeat_target_count: 100000
    minimum_tps: 100
```

This creates 100,000 concurrent connections to the target node, exhausting its resources.

## Notes

While the node-checker is not part of core consensus or validator infrastructure, it provides critical operational tooling for node operators. The complete absence of bounds validation combined with the trust placed in baseline configurations from official sources creates a significant attack surface for resource exhaustion and denial of service.

### Citations

**File:** ecosystem/node-checker/src/configuration/validate.rs (L24-27)
```rust
pub fn validate_configuration(node_configuration: &BaselineConfiguration) -> Result<()> {
    build_checkers(&node_configuration.checkers).context("Failed to build Checkers")?;
    Ok(())
}
```

**File:** ecosystem/node-checker/src/checker/latency.rs (L16-40)
```rust
#[derive(Clone, Debug, Deserialize, Serialize)]
#[serde(deny_unknown_fields)]
pub struct LatencyCheckerConfig {
    #[serde(flatten)]
    pub common: CommonCheckerConfig,

    /// The number of times to hit the node to check latency.
    #[serde(default = "LatencyCheckerConfig::default_num_samples")]
    pub num_samples: u16,

    /// The delay between each call.
    #[serde(default = "LatencyCheckerConfig::default_delay_between_samples_ms")]
    pub delay_between_samples_ms: u64,

    /// The number of responses that are allowed to be errors.
    #[serde(default)]
    pub num_allowed_errors: u16,

    /// If the average latency exceeds this value, it will fail the evaluation.
    /// This value is not the same as regular latency , e.g. with the ping tool.
    /// Instead, this measures the total RTT for an API call to the node. See
    /// https://aptos.dev/nodes/node-health-checker/node-health-checker-faq#how-does-the-latency-evaluator-work
    /// for more information.
    pub max_api_latency_ms: u64,
}
```

**File:** ecosystem/node-checker/src/checker/latency.rs (L84-105)
```rust
        for _ in 0..self.config.num_samples {
            match self.get_latency_datapoint(target_api_index_provider).await {
                Ok(latency) => latencies.push(latency),
                Err(e) => errors.push(e),
            }
            if errors.len() as u16 > self.config.num_allowed_errors {
                return Ok(vec![
                    Self::build_result(
                        "Node returned too many errors while checking API latency".to_string(),
                        0,
                        format!(
                            "The node returned too many errors while checking API RTT (Round trip time), the tolerance was {} errors out of {} calls: {}. Note, this latency is not the same as standard ping latency, see the attached link.",
                            self.config.num_allowed_errors, self.config.num_samples, errors.into_iter().map(|e| e.to_string()).collect::<Vec<String>>().join(", "),
                        )
                    ).links(vec![LINK.to_string()])
                ]);
            }
            tokio::time::sleep(std::time::Duration::from_millis(
                self.config.delay_between_samples_ms,
            ))
            .await;
        }
```

**File:** ecosystem/node-checker/src/checker/tps.rs (L68-75)
```rust
    /// The minimum TPS required to pass the test.
    pub minimum_tps: u64,

    /// The number of times to repeat the target. This influences thread
    /// count and rest client count.
    #[serde(default = "TpsCheckerConfig::default_repeat_target_count")]
    pub repeat_target_count: usize,
}
```

**File:** ecosystem/node-checker/src/checker/tps.rs (L130-136)
```rust
        let cluster_config = ClusterArgs {
            targets: Some(vec![target_url; self.config.repeat_target_count]),
            targets_file: None,
            coin_source_args: self.config.coin_source_args.clone(),
            chain_id: Some(chain_id),
            node_api_key: None,
        };
```

**File:** ecosystem/node-checker/src/runner/sync_runner.rs (L21-38)
```rust
#[derive(Clone, Debug, Deserialize, Serialize)]
#[serde(deny_unknown_fields)]
pub struct SyncRunnerConfig {
    /// If > 0, Checkers that failed with a retryable error will be retried
    /// this many times with the configured delay.
    #[serde(default)]
    pub num_retries: u8,

    /// If num_retries > 0, this is the delay in seconds between retries.
    #[serde(default = "SyncRunnerConfig::default_retry_delay_secs")]
    pub retry_delay_secs: u16,
}

impl SyncRunnerConfig {
    fn default_retry_delay_secs() -> u16 {
        2
    }
}
```

**File:** ecosystem/node-checker/src/runner/sync_runner.rs (L187-210)
```rust
        let mut num_attempts = 0;
        let check_result = loop {
            match checker.check(provider_collection).await {
                Ok(check_result) => break check_result,
                Err(err) => {
                    if num_attempts < self.config.num_retries {
                        num_attempts += 1;
                        warn!(
                            "Checker failed with a retryable error: {:#}. Retrying in {} seconds.",
                            err, self.config.retry_delay_secs
                        );
                        tokio::time::sleep(Duration::from_secs(
                            self.config.retry_delay_secs.into(),
                        ))
                        .await;
                    } else {
                        error!(
                            "Checker failed with a retryable error too many times ({}): {:#}.",
                            self.config.num_retries, err
                        );
                        return Err(err);
                    }
                },
            }
```

**File:** ecosystem/node-checker/src/server/build.rs (L166-199)
```rust
    // Finally, if a baseline was provided in the config, we run the runner against
    // the baseline itself. If the configuration listed a Checker that requires a
    // particular Provider, this will fail if that Provider was not created. If we
    // don't do this now, it'll fail later when NHC handles requests. With this run
    // we're only looking for hard errors or check results that appeared to fail
    // because of a missing Provider.
    if let Some(node_address) = &configuration.node_address {
        let message = format!(
            "Failed to run the Checker suite against the baseline node itself \
                for {}. This implies that a Checker has been enabled in the baseline \
                config but the necessary baseline information was not provided. For \
                example, this error might happen if the NodeIdentityChecker was enabled \
                but the API port for the baseline was not provided, since that checker \
                needs to be able to query the API of the baseline node. ",
            configuration.configuration_id
        );
        let results = runner
            .run(node_address)
            .await
            .with_context(|| message.clone())?;
        let mut missing_provider_results = Vec::new();
        for result in results.check_results.into_iter() {
            if result.score < 100 && result.headline.ends_with(MISSING_PROVIDER_MESSAGE) {
                missing_provider_results.push(result);
            }
        }
        if !missing_provider_results.is_empty() {
            bail!(
                "{} The following check results should explain the problem: {:#?}",
                message,
                missing_provider_results
            );
        }
    }
```

**File:** ecosystem/node-checker/src/server/api.rs (L89-92)
```rust
        let complete_evaluation_result = baseline_configuration
            .runner
            .run(&target_node_address)
            .await;
```
