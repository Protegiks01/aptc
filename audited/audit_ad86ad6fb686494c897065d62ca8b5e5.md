# Audit Report

## Title
Epoch Snapshot Pruner Can Delete Required State Sync Data Due to Unbounded Progress Metadata

## Summary
The epoch snapshot pruner calculates its target version based on the latest state checkpoint version minus the prune window, without validating that this target doesn't exceed the latest epoch ending version. This architectural flaw allows the pruner to delete epoch ending snapshot data that are still needed for state synchronization, breaking the ability of new or recovering nodes to fast sync to recent epochs.

## Finding Description

The vulnerability exists in how the epoch snapshot pruner determines its target pruning version. The pruner is designed to maintain epoch ending snapshots for state sync, but it lacks bounds checking to ensure it doesn't prune beyond the latest epoch ending.

**Root Cause Flow:**

1. **Pruner Target Calculation Without Epoch Awareness**

The epoch snapshot pruner's target is set by calling `maybe_set_pruner_target_db_version()` with the current state checkpoint version. [1](#0-0)  This current version represents the latest state checkpoint, not necessarily an epoch ending version. [2](#0-1) 

2. **Unconstrained Target Version Calculation**

The pruner manager calculates the target as `latest_version.saturating_sub(self.prune_window)` without any validation against epoch boundaries. [3](#0-2)  This uses a simple subtraction without checking if the result would prune needed epoch ending snapshots.

3. **Epoch Ending Nodes Stored Separately**

Nodes from epoch ending snapshots are correctly identified and stored in `StaleNodeIndexCrossEpochSchema` when their version is at or before the previous epoch ending. [4](#0-3)  However, this classification alone doesn't protect them from premature pruning.

4. **Unconditional Deletion Based on Target**

The pruner retrieves all stale node indices where `stale_since_version <= target_version` and deletes them unconditionally. [5](#0-4) [6](#0-5)  No validation prevents this from deleting epoch ending snapshot nodes.

**Test Code Reveals the Bug:**

The test implementation shows the **correct** approach that production code should implement: `epoch_snapshot_pruner.set_worker_target_version(std::cmp::min(*snapshots.first().unwrap(), *epoch_snapshots.first().unwrap_or(&Version::MAX)))` [7](#0-6)  This explicitly takes the minimum of the first snapshot and first epoch snapshot to ensure the pruner never prunes beyond the oldest epoch snapshot that needs to be kept. However, this safeguard exists **only in test code**, not in production.

**Attack Scenario (Natural Occurrence):**

Consider a network where:
- Epochs are time-based (every 2 hours as configured)
- An epoch runs longer than expected due to low transaction throughput or governance delays
- Epoch 3 ends at version 30M (latest completed epoch)
- Node continues processing transactions to version 120M (regular checkpoints occurring frequently)
- Pruner calculates target: 120M - 80M (prune_window) = 40M
- Pruner deletes all stale nodes with `stale_since_version ≤ 40M`
- **This includes the epoch 3 snapshot at version 30M**
- New nodes attempting to fast sync to epoch 3 fail - required merkle tree nodes are missing

This violates the documented invariant that "complete state trees (or 'epoch snapshots') at the end of each recent epochs are available for peers to access, which is important for the health of the chain." [8](#0-7) 

## Impact Explanation

**High Severity** - This vulnerability causes significant protocol violations affecting network availability:

1. **State Sync Protocol Violation**: The configuration explicitly documents that "epoch ending snapshots are used by state sync in fast sync mode." [9](#0-8)  Deleting these snapshots breaks the state sync protocol's fast sync capability, forcing nodes to fall back to slower transaction replay methods.

2. **Network Resilience Impact**: New validators cannot efficiently join the network, and existing validators that go offline cannot quickly rejoin if they need to sync from a pruned epoch. This reduces network resilience and increases barriers to participation, directly impacting decentralization.

3. **Operational Cost Increase**: Nodes must either maintain unnecessarily long history to avoid this issue, or risk extended downtime during recovery. This increases operational costs and complexity for validator operators.

The impact is **not** Critical because:
- Does not directly cause consensus failure or funds loss
- Nodes can still sync using slower methods (transaction replay)
- Does not cause permanent network partition

However, it meets **High Severity** criteria as a significant protocol violation that degrades validator operations and network availability, which are critical for blockchain health.

## Likelihood Explanation

**Highly Likely** - This vulnerability occurs naturally under normal network operation:

1. **Architectural Mismatch**: Epochs are **time-based** (every 2 hours by default), while the prune window is **version-based** (80M versions). [10](#0-9) [11](#0-10)  This creates an inherent mismatch: at variable TPS, the number of versions per epoch varies, but the prune window remains fixed.

2. **Default Configuration Vulnerability**: The default prune window of 80M versions is designed for "~5K TPS * 2h/epoch * 2 epochs" (≈2.2 epochs at constant 5K TPS). However, this assumes constant throughput and regular epoch transitions. Any deviation creates the vulnerability conditions.

3. **Checkpoint-Epoch Frequency Gap**: State checkpoints occur very frequently (potentially every block), while epochs occur every ~2 hours. The `current_version` used to set the pruner target naturally advances far beyond the latest epoch ending version, especially during long epochs.

4. **No Production Safeguards**: There are **zero validation checks** in the production code path to prevent this scenario. The vulnerability triggers automatically when `(latest_checkpoint - prune_window) > some_older_epoch_ending`. The test code proves this safeguard should exist, but it's absent in production. [12](#0-11) 

5. **Natural Triggering Conditions**: Networks with variable transaction throughput, governance-delayed reconfigurations, or epoch duration changes will naturally experience this issue. It requires no attacker action and occurs automatically through normal pruning operations.

This is not a theoretical edge case - it represents a fundamental architectural flaw where the test suite demonstrates the correct behavior that production code fails to implement.

## Recommendation

Implement the same epoch boundary validation that exists in the test code:

1. **Add Epoch Boundary Constraint**: When setting the epoch_snapshot_pruner target version, calculate the minimum of:
   - `current_version - prune_window` (existing calculation)
   - The oldest epoch ending version that should be kept (based on epoch_snapshot_prune_window)

2. **Suggested Fix Location**: Modify `StateMerklePrunerManager::maybe_set_pruner_target_db_version` or `set_pruner_target_db_version` in `storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_pruner_manager.rs` to query the ledger metadata DB for epoch ending versions and constrain the target accordingly.

3. **Alternative Approach**: Track epoch ending versions separately and ensure the epoch_snapshot_pruner target never exceeds the oldest epoch ending version within the configured retention window, similar to how the test code implements it with `std::cmp::min()`.

## Proof of Concept

While a complete executable PoC would require setting up a full Aptos node and manipulating epoch/checkpoint timing, the vulnerability is clearly demonstrated by the divergence between test code and production code:

**Test Code (Correct Behavior):**
The test at `storage/aptosdb/src/db/aptosdb_test.rs:298-301` explicitly uses `std::cmp::min(*snapshots.first().unwrap(), *epoch_snapshots.first().unwrap_or(&Version::MAX))` to ensure the epoch_snapshot_pruner doesn't prune beyond the oldest epoch snapshot.

**Production Code (Vulnerable Behavior):**
The production code at `storage/aptosdb/src/state_store/state_merkle_batch_committer.rs:97-98` simply calls `epoch_snapshot_pruner.maybe_set_pruner_target_db_version(current_version)` without any epoch boundary constraint, relying only on the prune_window subtraction.

The test suite itself proves that:
1. The developers understood epoch boundaries must constrain the pruner
2. The correct implementation exists in test code
3. Production code lacks this critical safeguard

A reproduction scenario would involve:
1. Running a network with default configuration (80M version prune window)
2. Allowing epochs to be longer than expected (e.g., through low TPS or delayed governance actions)
3. Observing that `latest_checkpoint_version - 80M > latest_epoch_ending_version`
4. New nodes attempting fast sync would fail to retrieve epoch ending snapshots

## Notes

This vulnerability represents a **logic flaw** in the pruning architecture where the distinction between frequent state checkpoints and infrequent epoch ending snapshots is not properly maintained. The test code explicitly demonstrates the correct behavior, making this a clear case where production code fails to implement a necessary safeguard that the test suite validates.

### Citations

**File:** storage/aptosdb/src/state_store/state_merkle_batch_committer.rs (L96-98)
```rust
                    self.state_db
                        .epoch_snapshot_pruner
                        .maybe_set_pruner_target_db_version(current_version);
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L812-819)
```rust
    fn get_latest_state_checkpoint_version(&self) -> Result<Option<Version>> {
        gauged_api("get_latest_state_checkpoint_version", || {
            Ok(self
                .state_store
                .current_state_locked()
                .last_checkpoint()
                .version())
        })
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_pruner_manager.rs (L159-174)
```rust
    fn set_pruner_target_db_version(&self, latest_version: Version) {
        assert!(self.pruner_worker.is_some());

        let min_readable_version = latest_version.saturating_sub(self.prune_window);
        self.min_readable_version
            .store(min_readable_version, Ordering::SeqCst);

        PRUNER_VERSIONS
            .with_label_values(&[S::name(), "min_readable"])
            .set(min_readable_version as i64);

        self.pruner_worker
            .as_ref()
            .unwrap()
            .set_target_db_version(min_readable_version);
    }
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L378-386)
```rust
            if previous_epoch_ending_version.is_some()
                && row.node_key.version() <= previous_epoch_ending_version.unwrap()
            {
                batch.put::<StaleNodeIndexCrossEpochSchema>(row, &())
            } else {
                // These are processed by the state merkle pruner.
                batch.put::<StaleNodeIndexSchema>(row, &())
            }
        })?;
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L191-217)
```rust
    pub(in crate::pruner::state_merkle_pruner) fn get_stale_node_indices(
        state_merkle_db_shard: &DB,
        start_version: Version,
        target_version: Version,
        limit: usize,
    ) -> Result<(Vec<StaleNodeIndex>, Option<Version>)> {
        let mut indices = Vec::new();
        let mut iter = state_merkle_db_shard.iter::<S>()?;
        iter.seek(&StaleNodeIndex {
            stale_since_version: start_version,
            node_key: NodeKey::new_empty_path(0),
        })?;

        let mut next_version = None;
        while indices.len() < limit {
            if let Some((index, _)) = iter.next().transpose()? {
                next_version = Some(index.stale_since_version);
                if index.stale_since_version <= target_version {
                    indices.push(index);
                    continue;
                }
            }
            break;
        }

        Ok((indices, next_version))
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L73-76)
```rust
            indices.into_iter().try_for_each(|index| {
                batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
                batch.delete::<S>(&index)
            })?;
```

**File:** storage/aptosdb/src/db/aptosdb_test.rs (L293-323)
```rust

        // Prune till the oldest snapshot readable.
        let pruner = &db.state_store.state_db.state_merkle_pruner;
        let epoch_snapshot_pruner = &db.state_store.state_db.epoch_snapshot_pruner;
        pruner.set_worker_target_version(*snapshots.first().unwrap());
        epoch_snapshot_pruner.set_worker_target_version(std::cmp::min(
            *snapshots.first().unwrap(),
            *epoch_snapshots.first().unwrap_or(&Version::MAX),
        ));
        pruner.wait_for_pruner().unwrap();
        epoch_snapshot_pruner.wait_for_pruner().unwrap();

        // Check strictly that all trees in the window accessible and all those nodes not needed
        // must be gone.
        let non_pruned_versions: HashSet<_> = snapshots
            .into_iter()
            .chain(epoch_snapshots.into_iter())
            .collect();

        let expected_nodes: HashSet<_> = non_pruned_versions
            .iter()
            .flat_map(|v| db.state_store.get_all_jmt_nodes_referenced(*v).unwrap())
            .collect();
        let all_nodes: HashSet<_> = db
            .state_store
            .get_all_jmt_nodes()
            .unwrap()
            .into_iter()
            .collect();

        assert_eq!(expected_nodes, all_nodes);
```

**File:** storage/README.md (L111-119)
```markdown
    # This configures the inter-epoch state tree pruner. If a state tree node is
    # overwritten by a later transaction that's in a later epoch, it's gonna be
    # pruned later by this pruner according to these configs. The prune window
    # looks large (the unit is number of transactions) but at each position in
    # the tree only the last node in its epoch among all updates to the same
    # position is kept (or pruned) by this pruner. Effectively these configs
    # guarantees complete state trees (or "epoch snapshots") at the end of
    # each recent epochs are available for peers to access, which is important
    # for the health of the chain.
```

**File:** config/src/config/storage_config.rs (L415-430)
```rust
impl Default for EpochSnapshotPrunerConfig {
    fn default() -> Self {
        Self {
            enable: true,
            // This is based on ~5K TPS * 2h/epoch * 2 epochs. -- epoch ending snapshots are used
            // by state sync in fast sync mode.
            // The setting is in versions, not epochs, because this makes it behave more like other
            // pruners: a slower network will have longer history in db with the same pruner
            // settings, but the disk space take will be similar.
            // settings.
            prune_window: 80_000_000,
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
        }
    }
```

**File:** aptos-move/framework/aptos-framework/sources/block.move (L1-50)
```text
/// This module defines a struct storing the metadata of the block and new block events.
module aptos_framework::block {
    use std::error;
    use std::vector;
    use std::option;
    use aptos_std::table_with_length::{Self, TableWithLength};
    use std::option::Option;
    use aptos_framework::randomness;

    use aptos_framework::account;
    use aptos_framework::event::{Self, EventHandle};
    use aptos_framework::reconfiguration;
    use aptos_framework::reconfiguration_with_dkg;
    use aptos_framework::stake;
    use aptos_framework::state_storage;
    use aptos_framework::system_addresses;
    use aptos_framework::timestamp;

    friend aptos_framework::genesis;

    const MAX_U64: u64 = 18446744073709551615;

    /// Should be in-sync with BlockResource rust struct in new_block.rs
    struct BlockResource has key {
        /// Height of the current block
        height: u64,
        /// Time period between epochs.
        epoch_interval: u64,
        /// Handle where events with the time of new blocks are emitted
        new_block_events: EventHandle<NewBlockEvent>,
        update_epoch_interval_events: EventHandle<UpdateEpochIntervalEvent>,
    }

    /// Store new block events as a move resource, internally using a circular buffer.
    struct CommitHistory has key {
        max_capacity: u32,
        next_idx: u32,
        table: TableWithLength<u32, NewBlockEvent>,
    }

    /// Should be in-sync with NewBlockEvent rust struct in new_block.rs
    struct NewBlockEvent has copy, drop, store {
        hash: address,
        epoch: u64,
        round: u64,
        height: u64,
        previous_block_votes_bitvec: vector<u8>,
        proposer: address,
        failed_proposer_indices: vector<u64>,
        /// On-chain time during the block at the given height
```
