# Audit Report

## Title
Division by Zero Panic in State Sync Data Poller Due to Unvalidated peer_bucket_size Configuration

## Summary
The `calculate_num_peers_to_poll()` function in the state sync data client performs division by the `peer_bucket_size` configuration parameter without validation. Setting `peer_bucket_size` to 0 in the node configuration causes an immediate panic and node crash during the state synchronization polling loop. [1](#0-0) 

## Finding Description
The vulnerability exists in the state sync data client's peer polling calculation logic. The `AptosDataPollerConfig` struct allows `peer_bucket_size` to be deserialized from YAML/TOML configuration files without any validation to ensure it's non-zero. [2](#0-1) 

The configuration has a default value of 10, but no `ConfigSanitizer` implementation exists to validate the value when loaded from external sources. [3](#0-2) 

When the state sync poller starts, it continuously executes a polling loop that alternates between priority and regular peers. For each polling round, it calls `calculate_num_peers_to_poll()` which performs the vulnerable division: [4](#0-3) 

The vulnerable division occurs at line 361: `potential_peers.len() as u64 / peer_bucket_sizes`. If `peer_bucket_sizes` is 0, this causes a Rust panic with a division by zero error.

The polling loop is invoked from both priority and regular peer polling paths: [5](#0-4) [6](#0-5) 

## Impact Explanation
This vulnerability qualifies as **High Severity** under the Aptos bug bounty program criteria ("API crashes" and "Validator node slowdowns"). 

**Impact:**
- **Complete node crash**: The panic occurs in the main state sync poller thread, causing the entire node process to crash
- **State synchronization failure**: Nodes cannot sync state from the network while crashed, falling behind the blockchain
- **Validator downtime**: If a validator node is misconfigured, it becomes unavailable, affecting network participation
- **Cascading failures**: If multiple nodes use centralized configuration management with this misconfiguration, it could affect multiple nodes simultaneously

The state sync data client is critical infrastructure for keeping nodes synchronized with the network. Without it, nodes cannot:
- Bootstrap from genesis
- Catch up after being offline
- Sync with the latest blockchain state
- Serve state data to other peers

## Likelihood Explanation
**Likelihood: Medium**

The vulnerability requires configuration file access, which limits the attack surface. However, several realistic scenarios make this exploitable:

1. **Honest operator error**: An administrator might accidentally set `peer_bucket_size: 0` while testing or troubleshooting
2. **Compromised configuration management**: If centralized configuration systems (Ansible, Kubernetes ConfigMaps, etc.) are compromised, attackers could inject malicious configurations
3. **Supply chain attacks**: Malicious configuration templates could be distributed to node operators
4. **Internal threats**: Malicious insiders with configuration access could intentionally crash nodes

The lack of any validation makes this trivial to trigger once configuration access is obtained.

## Recommendation
Implement a `ConfigSanitizer` for `AptosDataPollerConfig` to validate that `peer_bucket_size` is non-zero:

```rust
impl ConfigSanitizer for AptosDataPollerConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let data_poller_config = &node_config.state_sync.aptos_data_client.data_poller_config;

        // Validate peer_bucket_size is non-zero
        if data_poller_config.peer_bucket_size == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "peer_bucket_size must be greater than 0 to avoid division by zero".to_string(),
            ));
        }

        Ok(())
    }
}
```

Then register this sanitizer in the `StateSyncConfig::sanitize()` method to ensure validation occurs during configuration loading. [7](#0-6) 

## Proof of Concept

```rust
#[test]
#[should_panic(expected = "attempt to divide by zero")]
fn test_peer_bucket_size_zero_causes_panic() {
    use aptos_config::config::{AptosDataClientConfig, AptosDataPollerConfig};
    use std::collections::HashSet;
    use aptos_config::network_id::{NetworkId, PeerNetworkId};
    use aptos_types::PeerId;

    // Create config with peer_bucket_size set to 0
    let data_poller_config = AptosDataPollerConfig {
        additional_polls_per_peer_bucket: 1,
        min_polls_per_second: 5,
        max_polls_per_second: 20,
        peer_bucket_size: 0, // Vulnerable configuration
        poll_loop_interval_ms: 100,
        max_num_in_flight_priority_polls: 30,
        max_num_in_flight_regular_polls: 30,
    };

    // Create a set of potential peers
    let mut potential_peers = HashSet::new();
    for i in 0..10 {
        potential_peers.insert(PeerNetworkId::new(NetworkId::Validator, PeerId::random()));
    }

    // This will panic with division by zero
    crate::poller::calculate_num_peers_to_poll(
        &potential_peers,
        10,
        data_poller_config,
    );
}
```

**Steps to reproduce:**
1. Create a node configuration file with `peer_bucket_size: 0` in the state sync section
2. Start the Aptos node with this configuration
3. Observe the node crash with a division by zero panic when the state sync poller initializes

**Notes**

This vulnerability demonstrates a fundamental failure in defense-in-depth configuration validation. While node operators are generally trusted, robust systems should validate all inputs to prevent crashes from honest mistakes, compromised tooling, or malicious insiders. The fix is straightforward and follows existing patterns in the codebase for configuration sanitization.

### Citations

**File:** state-sync/aptos-data-client/src/poller.rs (L110-116)
```rust
        // Calculate the number of peers to poll this round
        let max_num_peers_to_poll = max_num_in_flight_polls.saturating_sub(num_in_flight_polls);
        let num_peers_to_poll = calculate_num_peers_to_poll(
            &all_priority_peers,
            max_num_peers_to_poll,
            self.data_client_config.data_poller_config,
        );
```

**File:** state-sync/aptos-data-client/src/poller.rs (L139-145)
```rust
        // Calculate the number of peers to poll this round
        let max_num_peers_to_poll = max_num_in_flight_polls.saturating_sub(num_in_flight_polls);
        let num_peers_to_poll = calculate_num_peers_to_poll(
            &all_regular_peers,
            max_num_peers_to_poll,
            self.data_client_config.data_poller_config,
        );
```

**File:** state-sync/aptos-data-client/src/poller.rs (L350-362)
```rust
/// Calculates the number of peers to poll this round
pub(crate) fn calculate_num_peers_to_poll(
    potential_peers: &HashSet<PeerNetworkId>,
    max_num_peers_to_poll: u64,
    data_poller_config: AptosDataPollerConfig,
) -> u64 {
    // Calculate the total number of peers to poll (per second)
    let min_polls_per_second = data_poller_config.min_polls_per_second;
    let peer_bucket_sizes = data_poller_config.peer_bucket_size;
    let additional_polls_per_bucket = data_poller_config.additional_polls_per_peer_bucket;
    let total_polls_per_second = min_polls_per_second
        + (additional_polls_per_bucket * (potential_peers.len() as u64 / peer_bucket_sizes));

```

**File:** config/src/config/state_sync_config.rs (L327-344)
```rust
#[derive(Clone, Copy, Debug, Deserialize, PartialEq, Eq, Serialize)]
#[serde(default, deny_unknown_fields)]
pub struct AptosDataPollerConfig {
    /// The additional number of polls to send per peer bucket (per second)
    pub additional_polls_per_peer_bucket: u64,
    /// The minimum number of polls that should be sent per second
    pub min_polls_per_second: u64,
    /// The maximum number of in-flight polls for priority peers
    pub max_num_in_flight_priority_polls: u64,
    /// The maximum number of in-flight polls for regular peers
    pub max_num_in_flight_regular_polls: u64,
    /// The maximum number of polls that should be sent per second
    pub max_polls_per_second: u64,
    /// The number of peers per bucket
    pub peer_bucket_size: u64,
    /// Interval (in ms) between summary poll loop executions
    pub poll_loop_interval_ms: u64,
}
```

**File:** config/src/config/state_sync_config.rs (L346-358)
```rust
impl Default for AptosDataPollerConfig {
    fn default() -> Self {
        Self {
            additional_polls_per_peer_bucket: 1,
            min_polls_per_second: 5,
            max_num_in_flight_priority_polls: 30,
            max_num_in_flight_regular_polls: 30,
            max_polls_per_second: 20,
            peer_bucket_size: 10,
            poll_loop_interval_ms: 100,
        }
    }
}
```

**File:** config/src/config/state_sync_config.rs (L487-496)
```rust
impl ConfigSanitizer for StateSyncConfig {
    fn sanitize(
        node_config: &NodeConfig,
        node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        // Sanitize the state sync driver config
        StateSyncDriverConfig::sanitize(node_config, node_type, chain_id)
    }
}
```
