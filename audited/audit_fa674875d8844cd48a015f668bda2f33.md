# Audit Report

## Title
Missing Retry Mechanism for Execution Failures Causes Permanent Liveness Loss

## Summary
When the execution wait phase returns an `ExecutorError::CouldNotGetData` timeout error, the affected block remains permanently stuck in "Ordered" state and is never re-executed. This causes complete liveness failure for the validator as all subsequent blocks are blocked from processing. Unlike the signing phase which implements retry logic, the execution phase ignores the retry signal, creating an unrecoverable deadlock situation.

## Finding Description
The vulnerability exists in the execution response handling logic within the buffer manager's main event loop. When blocks are ordered and sent for execution, if the execution fails with a `CouldNotGetData` error (commonly triggered by quorum store batch request timeouts), the block processing follows this flawed path:

1. The `ExecutionWaitPhase` awaits compute results and returns an `ExecutionResponse` containing the error [1](#0-0) 

2. The buffer manager's `process_execution_response` method receives this error response, logs it, and returns early without advancing the block from "Ordered" to "Executed" state [2](#0-1) 

3. The `advance_execution_root` method is designed to detect this stuck situation and return `Some(block_id)` to signal that a retry is needed [3](#0-2) 

4. However, in the main event loop, this return value is completely ignored and no retry is scheduled [4](#0-3) 

This is in stark contrast to the signing phase, which properly handles retry scenarios by spawning a delayed retry request when the signing root hasn't advanced [5](#0-4) 

The `CouldNotGetData` error occurs in several realistic scenarios:
- Batch request timeout after exhausting retry attempts [6](#0-5) 
- Batch expiration based on ledger timestamp [7](#0-6) 
- Batch unavailable in database or cache [8](#0-7) 

Once a block becomes stuck, execution is only triggered once when ordered blocks are first received [9](#0-8)  and there is no mechanism to retry failed executions during normal operation.

## Impact Explanation
This vulnerability qualifies as **High Severity** under the Aptos bug bounty program for the following reasons:

**Total Loss of Liveness**: When a block gets stuck due to `CouldNotGetData`, the validator completely stops making consensus progress. The buffer manager cannot advance past the stuck block, preventing all subsequent blocks from being executed, signed, or committed. This represents a "significant protocol violation" causing validator node to become non-functional.

**No Automatic Recovery**: The only recovery mechanisms are:
- Epoch boundary reset [10](#0-9) 
- Manual state synchronization intervention [11](#0-10) 

During normal epoch operation, there is no automatic recovery, meaning validators remain stuck until external intervention.

**Network-Wide Impact**: If multiple validators encounter batch timeout issues simultaneously (e.g., during network partitions or storage slowdowns), this can lead to widespread liveness failures across the validator set, potentially preventing the network from reaching consensus.

## Likelihood Explanation
This vulnerability has **HIGH likelihood** of occurring:

**Common Trigger Conditions**:
- Network latency or partitions causing batch request timeouts
- Storage I/O delays preventing timely batch retrieval
- Quorum store synchronization issues between validators
- High load conditions causing request queue backlogs

**No Special Privileges Required**: This can happen to any validator during normal operations without requiring malicious activity. The batch request timeout mechanism has built-in limits that will eventually return `CouldNotGetData` under adverse conditions [12](#0-11) 

**Production Environment Realistic**: Network and storage issues are common in distributed systems, making this a realistic failure mode that could affect mainnet validators.

## Recommendation
Implement retry logic for execution failures similar to the signing phase retry mechanism. The fix should:

1. Capture the return value from `advance_execution_root()` in the main event loop
2. When a retry is needed (i.e., `Some(block_id)` is returned), spawn a delayed retry request to re-execute the block

**Suggested Code Fix** (in `buffer_manager.rs` main event loop):

```rust
Some(response) = self.execution_wait_phase_rx.next() => {
    monitor!("buffer_manager_process_execution_wait_response", {
        self.process_execution_response(response).await;
        
        // Handle retry if execution root hasn't advanced
        if let Some(block_id) = self.advance_execution_root() {
            // Find the item that needs retry
            if let Some(cursor) = self.buffer.find_elem_by_key(self.execution_root, block_id) {
                let item = self.buffer.get(&cursor);
                if let Some(ordered_item) = item.as_ordered() {
                    let request = self.create_new_request(ExecutionRequest {
                        ordered_blocks: ordered_item.ordered_blocks.clone(),
                    });
                    let sender = self.execution_schedule_phase_tx.clone();
                    Self::spawn_retry_request(sender, request, Duration::from_millis(100));
                }
            }
        }
        
        if self.signing_root.is_none() {
            self.advance_signing_root().await;
        }
    });
},
```

Additionally, consider implementing:
- Maximum retry attempts with exponential backoff
- Metrics/alerts when blocks require multiple retry attempts  
- Fallback mechanisms if retries continue to fail after a threshold

## Proof of Concept

**Rust Integration Test** (add to `consensus/src/pipeline/tests/buffer_manager_tests.rs`):

```rust
#[tokio::test]
async fn test_execution_timeout_retry() {
    // Setup buffer manager with execution pipeline
    let (execution_schedule_tx, mut execution_schedule_rx) = create_channel();
    let (execution_wait_tx, execution_wait_rx) = create_channel();
    // ... setup other channels and buffer manager ...
    
    // Start buffer manager in background
    tokio::spawn(async move {
        buffer_manager.start().await;
    });
    
    // Send ordered blocks
    block_tx.send(ordered_blocks).await.unwrap();
    
    // Receive execution schedule request
    let schedule_req = execution_schedule_rx.next().await.unwrap();
    
    // Simulate CouldNotGetData timeout error
    let error_response = ExecutionResponse {
        block_id: schedule_req.block_id,
        inner: Err(ExecutorError::CouldNotGetData),
    };
    execution_wait_tx.send(error_response).await.unwrap();
    
    // Wait and verify that NO retry request is sent (demonstrates bug)
    tokio::time::sleep(Duration::from_millis(500)).await;
    assert!(execution_schedule_rx.try_next().is_err(), 
            "Bug: No retry request sent after CouldNotGetData error");
    
    // Verify block remains stuck in Ordered state
    // (buffer manager metrics would show pending_ordered > 0)
}
```

**Reproduction Steps**:
1. Configure quorum store with low batch request timeout
2. Simulate network partition preventing batch retrieval
3. Send ordered blocks requiring the unavailable batches
4. Observe `CouldNotGetData` error in logs [13](#0-12) 
5. Verify validator stops processing subsequent blocks (check `NUM_BLOCKS_IN_PIPELINE` metric showing blocks stuck in "ordered" state)
6. Confirm no automatic recovery occurs until epoch boundary or manual reset

### Citations

**File:** consensus/src/pipeline/execution_wait_phase.rs (L49-56)
```rust
    async fn process(&self, req: ExecutionWaitRequest) -> ExecutionResponse {
        let ExecutionWaitRequest { block_id, fut } = req;

        ExecutionResponse {
            block_id,
            inner: fut.await,
        }
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L397-410)
```rust
        let request = self.create_new_request(ExecutionRequest {
            ordered_blocks: ordered_blocks.clone(),
        });
        if let Some(consensus_publisher) = &self.consensus_publisher {
            let message = ConsensusObserverMessage::new_ordered_block_message(
                ordered_blocks.clone(),
                ordered_proof.clone(),
            );
            consensus_publisher.publish_message(message);
        }
        self.execution_schedule_phase_tx
            .send(request)
            .await
            .expect("Failed to send execution schedule request");
```

**File:** consensus/src/pipeline/buffer_manager.rs (L436-451)
```rust
        if self.execution_root.is_some() && cursor == self.execution_root {
            // Schedule retry.
            self.execution_root
        } else {
            sample!(
                SampleRate::Frequency(2),
                info!(
                    "Advance execution root from {:?} to {:?}",
                    cursor, self.execution_root
                )
            );
            // Otherwise do nothing, because the execution wait phase is driven by the response of
            // the execution schedule phase, which is in turn fed as soon as the ordered blocks
            // come in.
            None
        }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L478-480)
```rust
            if cursor == self.signing_root {
                let sender = self.signing_phase_tx.clone();
                Self::spawn_retry_request(sender, request, Duration::from_millis(100));
```

**File:** consensus/src/pipeline/buffer_manager.rs (L530-534)
```rust
                if commit_proof.ledger_info().ends_epoch() {
                    // the epoch ends, reset to avoid executing more blocks, execute after
                    // this persisting request will result in BlockNotFound
                    self.reset().await;
                }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L579-595)
```rust
    async fn process_reset_request(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        info!("Receive reset");

        match signal {
            ResetSignal::Stop => self.stop = true,
            ResetSignal::TargetRound(round) => {
                self.highest_committed_round = round;
                self.latest_round = round;

                let _ = self.drain_pending_commit_proof_till(round);
            },
        }

        self.reset().await;
        let _ = tx.send(ResetAck::default());
        info!("Reset finishes");
```

**File:** consensus/src/pipeline/buffer_manager.rs (L617-626)
```rust
        let executed_blocks = match inner {
            Ok(result) => result,
            Err(e) => {
                log_executor_error_occurred(
                    e,
                    &counters::BUFFER_MANAGER_RECEIVED_EXECUTOR_ERROR_COUNT,
                    block_id,
                );
                return;
            },
```

**File:** consensus/src/pipeline/buffer_manager.rs (L954-960)
```rust
                Some(response) = self.execution_wait_phase_rx.next() => {
                    monitor!("buffer_manager_process_execution_wait_response", {
                    self.process_execution_response(response).await;
                    self.advance_execution_root();
                    if self.signing_root.is_none() {
                        self.advance_signing_root().await;
                    }});
```

**File:** consensus/src/quorum_store/batch_requester.rs (L108-132)
```rust
        let validator_verifier = self.validator_verifier.clone();
        let mut request_state = BatchRequesterState::new(responders, self.retry_limit);
        let network_sender = self.network_sender.clone();
        let request_num_peers = self.request_num_peers;
        let my_peer_id = self.my_peer_id;
        let epoch = self.epoch;
        let retry_interval = Duration::from_millis(self.retry_interval_ms as u64);
        let rpc_timeout = Duration::from_millis(self.rpc_timeout_ms as u64);

        monitor!("batch_request", {
            let mut interval = time::interval(retry_interval);
            let mut futures = FuturesUnordered::new();
            let request = BatchRequest::new(my_peer_id, epoch, digest);
            loop {
                tokio::select! {
                    _ = interval.tick() => {
                        // send batch request to a set of peers of size request_num_peers
                        if let Some(request_peers) = request_state.next_request_peers(request_num_peers) {
                            for peer in request_peers {
                                futures.push(network_sender.request_batch(request.clone(), peer, rpc_timeout));
                            }
                        } else if futures.is_empty() {
                            // end the loop when the futures are drained
                            break;
                        }
```

**File:** consensus/src/quorum_store/batch_requester.rs (L148-150)
```rust
                                    counters::RECEIVED_BATCH_EXPIRED_COUNT.inc();
                                    debug!("QS: batch request expired, digest:{}", digest);
                                    return Err(ExecutorError::CouldNotGetData);
```

**File:** consensus/src/quorum_store/batch_requester.rs (L176-178)
```rust
            counters::RECEIVED_BATCH_REQUEST_TIMEOUT_COUNT.inc();
            debug!("QS: batch request timed out, digest:{}", digest);
            Err(ExecutorError::CouldNotGetData)
```

**File:** consensus/src/quorum_store/batch_store.rs (L555-558)
```rust
                Ok(None) | Err(_) => {
                    warn!("Could not get batch from db");
                    Err(ExecutorError::CouldNotGetData)
                },
```

**File:** consensus/src/counters.rs (L1190-1195)
```rust
        ExecutorError::CouldNotGetData => {
            counter.with_label_values(&["CouldNotGetData"]).inc();
            warn!(
                block_id = block_id,
                "Execution error - CouldNotGetData {}", block_id
            );
```
