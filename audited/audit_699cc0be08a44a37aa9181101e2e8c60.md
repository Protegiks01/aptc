# Audit Report

## Title
Event V2 Translator Cache-DB Desynchronization Causing Sequence Number Gaps

## Summary
A critical race condition exists in the Event V2 translation system where sequence numbers are cached before DB commit. If a batch processing error occurs after caching but before DB commit, the in-memory cache becomes desynchronized from the database, causing permanent sequence number gaps that break event queries and violate state consistency guarantees.

## Finding Description

The vulnerability exists in the event indexing flow where Event V2 events are translated to V1 format for backward compatibility. [1](#0-0) 

The sequence number assignment and caching flow is:

1. **Sequence Number Generation**: When translating an Event V2, `get_next_sequence_number()` retrieves the next sequence number, checking the cache first before falling back to the database. [2](#0-1) 

2. **Immediate Cache Update**: After translation, the sequence number is immediately cached in memory. [3](#0-2) 

3. **Delayed DB Commit**: The database is only updated much later, after all events in the batch are processed. [4](#0-3) 

**The Critical Gap**: Between lines 462 (cache update) and 547 (batch send for commit), any error causes the function to return early, leaving the cache updated but the database unchanged. There is **no rollback mechanism** for the cache.

**Error Scenarios**:
- Iterator failures during `try_for_each` (line 418)
- Channel send failures (line 547)
- Resource exhaustion (memory, disk space)
- I/O errors during batch operations

When a batch fails and is retried:
1. The cache still contains the previously assigned sequence numbers
2. `get_next_sequence_number()` returns `cached_value + 1`
3. This skips sequence numbers that were never committed
4. Creates permanent gaps in the sequence number space

**Impact on Event Queries**: The event lookup function explicitly validates sequence number continuity and fails when gaps are detected. [5](#0-4) 

This breaks the fundamental invariant that event sequence numbers must be continuous and sequential.

## Impact Explanation

**Severity: HIGH**

This vulnerability qualifies as HIGH severity under Aptos Bug Bounty criteria for multiple reasons:

1. **API Crashes**: Event query APIs fail with "DB corruption: Sequence number not continuous" errors, affecting all users and applications that rely on event data. External indexers, wallets, and dApps that query events become dysfunctional.

2. **Significant Protocol Violations**: Violates the critical invariant that event sequence numbers are continuous and monotonically increasing. This breaks state consistency guarantees.

3. **State Inconsistency**: Creates persistent data corruption in the indexer database that requires manual intervention to fix. The cache-DB desynchronization persists until node restart (at which point the cache is cleared), but the gaps remain in the DB.

4. **Cascading Failures**: Once sequence number gaps exist, all subsequent event queries for that event key fail, creating a denial-of-service condition for event-based functionality.

The vulnerability affects critical blockchain infrastructure components and breaks fundamental data integrity guarantees.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

This vulnerability has realistic triggering conditions:

1. **Natural Occurrence**: I/O errors, resource exhaustion (disk full, memory pressure), and transient failures during batch processing are common in production systems.

2. **Attacker-Induced Errors**: An attacker can increase likelihood by:
   - Sending transactions that maximize resource usage during indexer processing
   - Triggering edge cases in event translation that cause errors
   - Exploiting resource exhaustion vulnerabilities to cause I/O failures

3. **No Automatic Recovery**: The vulnerability persists after the error is resolved because:
   - The cache is not cleared on error
   - The database is not rolled back
   - Only a node restart clears the cache (but gaps remain in DB)

4. **Production Impact**: Multiple nodes in a production network could be affected simultaneously if they encounter similar error conditions during synchronization or high load periods.

The combination of realistic error conditions and lack of recovery mechanisms makes this vulnerability likely to occur in production environments.

## Recommendation

Implement proper transaction semantics for the cache-DB synchronization:

**Solution 1: Atomic Cache-DB Update (Recommended)**
```rust
// In process_a_batch(), defer cache updates until after successful DB commit
// Store pending cache updates in a local HashMap
let mut pending_cache_updates: HashMap<EventKey, u64> = HashMap::new();

// During event processing (line 462), collect updates instead of caching:
pending_cache_updates.insert(key, sequence_number);

// After successful batch send and commit confirmation:
for (event_key, seq_num) in pending_cache_updates {
    self.event_v2_translation_engine.cache_sequence_number(&event_key, seq_num);
}
```

**Solution 2: Cache Rollback on Error**
```rust
// In process_a_batch(), track cache updates for rollback
let mut cached_keys_this_batch: Vec<(EventKey, Option<u64>)> = Vec::new();

// Before caching, store previous value:
let prev_value = self.event_v2_translation_engine.get_cached_sequence_number(&key);
cached_keys_this_batch.push((key, prev_value));
self.event_v2_translation_engine.cache_sequence_number(&key, sequence_number);

// On error, restore previous cache state:
// (Add this in error handling path)
for (key, prev_value) in cached_keys_this_batch {
    if let Some(prev) = prev_value {
        self.event_v2_translation_engine.cache_sequence_number(&key, prev);
    } else {
        self.event_v2_translation_engine.clear_cached_sequence_number(&key);
    }
}
```

**Solution 3: Validate Cache-DB Consistency**
Add validation logic to detect and recover from desynchronization:
```rust
// Before using cached value, validate against DB:
pub fn get_next_sequence_number_validated(&self, event_key: &EventKey, default: u64) -> Result<u64> {
    let cached = self.get_cached_sequence_number(event_key);
    let db_value = self.internal_indexer_db.get::<EventSequenceNumberSchema>(event_key)?;
    
    // If cache and DB differ, prefer DB (authoritative source)
    match (cached, db_value) {
        (Some(c), Some(d)) if c != d => {
            warn!("Cache-DB desync detected for {:?}: cache={}, db={}", event_key, c, d);
            Ok(d + 1) // Use DB value
        },
        (Some(c), None) => {
            warn!("Cached value exists but DB empty for {:?}", event_key);
            Ok(default + 1) // Use default
        },
        _ => self.get_next_sequence_number(event_key, default),
    }
}
```

## Proof of Concept

```rust
// Rust unit test demonstrating the vulnerability
#[cfg(test)]
mod test {
    use super::*;
    
    #[test]
    fn test_cache_db_desynchronization() {
        // Setup: Create indexer with event translation enabled
        let (indexer_db, main_db) = setup_test_dbs();
        let indexer = DBIndexer::new(indexer_db, main_db);
        
        // Step 1: Process a batch with a valid Event V2
        let event_key = EventKey::new(1, AccountAddress::random());
        let v2_event = create_test_v2_event(event_key);
        
        // Simulate first successful translation
        let result = indexer.translate_event_v2_to_v1(&v2_event);
        assert!(result.is_ok());
        let v1_event = result.unwrap().unwrap();
        
        // Cache is updated (simulating line 462)
        indexer.event_v2_translation_engine
            .cache_sequence_number(&event_key, v1_event.sequence_number());
        
        // Verify cache has the value
        let cached = indexer.event_v2_translation_engine
            .get_cached_sequence_number(&event_key);
        assert_eq!(cached, Some(0)); // First event has seq 0
        
        // Step 2: SIMULATE ERROR - batch fails before DB commit
        // (DB is not updated, but cache remains)
        
        // Step 3: Retry the batch - translate the same event again
        let next_seq = indexer.event_v2_translation_engine
            .get_next_sequence_number(&event_key, 0)
            .unwrap();
        
        // BUG: Returns cached_value + 1 = 0 + 1 = 1
        // But DB is still empty, so it should return 0
        assert_eq!(next_seq, 1); // Gap created! Sequence 0 is skipped
        
        // Step 4: Verify DB is still empty
        let db_value = indexer.indexer_db.db
            .get::<EventSequenceNumberSchema>(&event_key)
            .unwrap();
        assert!(db_value.is_none()); // DB has nothing
        
        // Step 5: Now if we commit this event with seq=1, 
        // sequence number 0 is permanently lost - creating a gap
        println!("GAP CREATED: Event sequence jumped from None to 1, skipping 0");
        
        // Step 6: Demonstrate query failure
        // When querying events starting from seq 0:
        let query_result = indexer.indexer_db.lookup_events_by_key(
            &event_key,
            0,  // start_seq_num
            10, // limit
            100 // ledger_version
        );
        
        // This will eventually fail when it detects the gap
        // with error: "DB corruption: Sequence number not continuous"
    }
}
```

## Notes

This vulnerability demonstrates a critical failure in transaction semantics between the in-memory cache and persistent database. The Event V2 translation system assumes that cache updates and DB commits happen atomically, but they are actually separated by multiple error-prone operations. The lack of rollback or validation mechanisms allows this desynchronization to persist and corrupt the event sequence number space.

The vulnerability is particularly severe because:
1. It affects production infrastructure components (indexers, event APIs)
2. It creates permanent data corruption that persists across retries
3. It has no automatic recovery mechanism
4. It can cascade to affect all event queries for impacted event keys

The recommended fix requires implementing proper two-phase commit semantics or pessimistic validation to ensure cache-DB consistency is maintained even under error conditions.

### Citations

**File:** storage/indexer/src/db_indexer.rs (L232-239)
```rust
            if seq != cur_seq {
                let msg = if cur_seq == start_seq_num {
                    "First requested event is probably pruned."
                } else {
                    "DB corruption: Sequence number not continuous."
                };
                bail!("{} expected: {}, actual: {}", msg, cur_seq, seq);
            }
```

**File:** storage/indexer/src/db_indexer.rs (L448-487)
```rust
                    if self.indexer_db.event_v2_translation_enabled() {
                        if let ContractEvent::V2(v2) = event {
                            if let Some(translated_v1_event) =
                                self.translate_event_v2_to_v1(v2).map_err(|e| {
                                    anyhow::anyhow!(
                                        "Failed to translate event: {:?}. Error: {}",
                                        v2,
                                        e
                                    )
                                })?
                            {
                                let key = *translated_v1_event.key();
                                let sequence_number = translated_v1_event.sequence_number();
                                self.event_v2_translation_engine
                                    .cache_sequence_number(&key, sequence_number);
                                event_keys.insert(key);
                                batch
                                    .put::<EventByKeySchema>(
                                        &(key, sequence_number),
                                        &(version, idx as u64),
                                    )
                                    .expect("Failed to put events by key to a batch");
                                batch
                                    .put::<EventByVersionSchema>(
                                        &(key, version, sequence_number),
                                        &(idx as u64),
                                    )
                                    .expect("Failed to put events by version to a batch");
                                batch
                                    .put::<TranslatedV1EventSchema>(
                                        &(version, idx as u64),
                                        &translated_v1_event,
                                    )
                                    .expect("Failed to put translated v1 events to a batch");
                            }
                        }
                    }
                    Ok::<(), AptosDbError>(())
                })?;
            }
```

**File:** storage/indexer/src/db_indexer.rs (L511-521)
```rust
            for event_key in event_keys {
                batch
                    .put::<EventSequenceNumberSchema>(
                        &event_key,
                        &self
                            .event_v2_translation_engine
                            .get_cached_sequence_number(&event_key)
                            .unwrap_or(0),
                    )
                    .expect("Failed to put events by key to a batch");
            }
```

**File:** storage/indexer/src/event_v2_translator.rs (L179-182)
```rust
    pub fn cache_sequence_number(&self, event_key: &EventKey, sequence_number: u64) {
        self.event_sequence_number_cache
            .insert(*event_key, sequence_number);
    }
```

**File:** storage/indexer/src/event_v2_translator.rs (L190-200)
```rust
    pub fn get_next_sequence_number(&self, event_key: &EventKey, default: u64) -> Result<u64> {
        if let Some(seq) = self.get_cached_sequence_number(event_key) {
            Ok(seq + 1)
        } else {
            let seq = self
                .internal_indexer_db
                .get::<EventSequenceNumberSchema>(event_key)?
                .map_or(default, |seq| seq + 1);
            Ok(seq)
        }
    }
```
