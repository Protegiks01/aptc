# Audit Report

## Title
In-Progress JWK Consensus Sessions Lost During Consensus Key Rotation

## Summary
The `IssuerLevelConsensusManager` does not persist or transfer in-progress JWK consensus sessions when the consensus key is rotated during epoch transitions. This results in aborted consensus sessions and delayed JWK security updates, potentially extending the vulnerability window for compromised authentication keys.

## Finding Description

The JWK (JSON Web Key) consensus system in Aptos manages cryptographic key rotations for OpenID Connect (OIDC) authentication. When a validator's consensus key is rotated (which takes effect at epoch boundaries), the system creates a new `IssuerLevelConsensusManager` with the new key but **does not preserve in-progress consensus sessions** from the old manager.

The vulnerability manifests through the following code path:

1. **Consensus Key Rotation Initiated**: A validator calls `stake::rotate_consensus_key()` which updates the on-chain consensus key. [1](#0-0) 

2. **Epoch Transition Triggered**: When the new epoch begins, `EpochManager::on_new_epoch()` is invoked, which calls `shutdown_current_processor()` to terminate the old manager. [2](#0-1) 

3. **In-Progress Sessions Aborted**: The shutdown process calls `tear_down()` on the `IssuerLevelConsensusManager`, which sets `stopped = true` and drops the `states_by_issuer` map containing all consensus state. [3](#0-2) 

4. **Consensus Processes Cancelled**: When `ConsensusState::InProgress` is dropped, the `QuorumCertProcessGuard` destructor calls `abort()` on the underlying async task, cancelling the in-progress reliable broadcast. [4](#0-3) 

5. **State Not Recovered**: The new manager is created with the rotated consensus key and initializes state only from on-chain data via `reset_with_on_chain_state()`, which creates fresh `PerProviderState` objects with `ConsensusState::NotStarted`. [5](#0-4) 

The critical issue is that the `consensus_key` field is immutable after construction and there is no mechanism to update it or transfer in-progress sessions to a new manager instance: [6](#0-5) 

**Attack Scenario**: An attacker who has compromised an OIDC provider's old JWK keys can benefit from this vulnerability by:
1. Monitoring on-chain state to predict when validator epoch transitions occur
2. Timing a legitimate JWK rotation to occur just before multiple validators undergo epoch transitions
3. If the JWK update consensus is `InProgress` when validators transition epochs, those sessions are aborted
4. The JWK observers must re-observe the update (10-second polling interval), creating a delay
5. During this extended window, the attacker's compromised old JWK remains valid for authentication

## Impact Explanation

This is a **Medium Severity** vulnerability per the Aptos bug bounty criteria:

- **State Inconsistencies**: In-progress consensus sessions are lost without completion or recovery, creating inconsistent state across validators at different stages of epoch transition
- **Extended Vulnerability Window**: When JWK rotations are security-critical (e.g., responding to key compromise), this mechanism delays the update propagation by aborting in-progress consensus and requiring re-observation
- **No Direct Funds Loss**: This does not directly result in theft or minting of funds
- **Authentication Security Impact**: The extended validity period of old JWKs could allow unauthorized authentication attempts using compromised keys

The impact aligns with "State inconsistencies requiring intervention" and extends the attack surface for authentication-based exploits.

## Likelihood Explanation

The likelihood is **Medium to High**:

**Factors Increasing Likelihood**:
- Epoch transitions are deterministic and publicly observable on-chain
- JWK observations occur continuously every 10 seconds [7](#0-6) 
- Multiple validators may rotate consensus keys during network operation
- The timing window for in-progress sessions is significant (consensus can take minutes to reach quorum in adverse network conditions)

**Factors Decreasing Likelihood**:
- Requires either OIDC provider compromise or pre-existing JWK compromise to exploit meaningfully
- Natural JWK rotations by legitimate providers are not synchronized with validator epochs
- The system will eventually re-observe and re-consensus the JWK update

**Overall Assessment**: The vulnerability will manifest frequently during normal operations (validators rotating keys), but meaningful exploitation requires additional compromise of OIDC infrastructure.

## Recommendation

Implement a state persistence and recovery mechanism for in-progress JWK consensus sessions across epoch transitions:

**Option 1: Persist Pending Sessions**
```rust
// In IssuerLevelConsensusManager
pub struct PerProviderState {
    pub on_chain: Option<ProviderJWKs>,
    pub observed: Option<Vec<JWKMoveStruct>>,
    pub consensus_state: ConsensusState<ObservedUpdate>,
    // Add: last observation timestamp for recovery
    pub last_observation_time: Option<Instant>,
}

// Modify tear_down to persist in-progress sessions
async fn tear_down(&mut self, ack_tx: Option<oneshot::Sender<()>>) -> Result<()> {
    self.stopped = true;
    
    // Extract in-progress observations before dropping
    let pending_observations: Vec<(Issuer, ObservedUpdate)> = self.states_by_issuer
        .iter()
        .filter_map(|(issuer, state)| {
            if let ConsensusState::InProgress { my_proposal, .. } = &state.consensus_state {
                Some((issuer.clone(), my_proposal.clone()))
            } else {
                None
            }
        })
        .collect();
    
    // Persist pending observations (could write to storage or pass to epoch manager)
    // For now, log them for recovery
    for (issuer, proposal) in pending_observations {
        info!("Persisting in-progress JWK consensus for issuer {:?}", issuer);
        // TODO: Store in persistent storage
    }
    
    let futures = std::mem::take(&mut self.jwk_observers)
        .into_iter()
        .map(JWKObserver::shutdown)
        .collect::<Vec<_>>();
    join_all(futures).await;
    
    if let Some(tx) = ack_tx {
        let _ = tx.send(());
    }
    Ok(())
}
```

**Option 2: Complete In-Progress Sessions Before Shutdown**
```rust
// Modify shutdown_current_processor to wait for in-progress sessions
async fn shutdown_current_processor(&mut self) {
    if let Some(tx) = self.jwk_manager_close_tx.take() {
        // Send shutdown signal with grace period
        let (ack_tx, ack_rx) = oneshot::channel();
        let _ = tx.send(ack_tx);
        
        // Wait with timeout for in-progress sessions to complete
        let timeout_result = tokio::time::timeout(
            Duration::from_secs(30),
            ack_rx
        ).await;
        
        if timeout_result.is_err() {
            warn!("JWK consensus manager shutdown timeout - in-progress sessions may be lost");
        }
    }
    
    self.jwk_updated_event_txs = None;
}

// In IssuerLevelConsensusManager, modify tear_down to complete pending work
async fn tear_down(&mut self, ack_tx: Option<oneshot::Sender<()>>) -> Result<()> {
    self.stopped = true;
    
    // Allow some time for in-progress consensus to complete
    let in_progress_count = self.states_by_issuer.values()
        .filter(|s| matches!(s.consensus_state, ConsensusState::InProgress { .. }))
        .count();
    
    if in_progress_count > 0 {
        info!("Waiting for {} in-progress JWK consensus sessions", in_progress_count);
        tokio::time::sleep(Duration::from_secs(5)).await;
    }
    
    // Continue with normal shutdown...
    let futures = std::mem::take(&mut self.jwk_observers)
        .into_iter()
        .map(JWKObserver::shutdown)
        .collect::<Vec<_>>();
    join_all(futures).await;
    
    if let Some(tx) = ack_tx {
        let _ = tx.send(());
    }
    Ok(())
}
```

**Recommended Approach**: Implement Option 2 (graceful completion) as the primary fix, with Option 1 (persistence) as a fallback for sessions that cannot complete within the grace period.

## Proof of Concept

```rust
#[tokio::test]
async fn test_in_progress_sessions_lost_on_epoch_transition() {
    use aptos_crypto::{bls12381::PrivateKey, SigningKey};
    use aptos_types::{
        account_address::AccountAddress,
        epoch_state::EpochState,
        jwks::{issuer_from_str, jwk::JWK, unsupported::UnsupportedJWK, ProviderJWKs},
        validator_verifier::{ValidatorConsensusInfo, ValidatorVerifier},
    };
    use std::sync::Arc;
    
    // Setup validator set
    let private_key = Arc::new(PrivateKey::generate_for_testing());
    let public_key = PublicKey::from(private_key.as_ref());
    let addr = AccountAddress::random();
    let validator_info = ValidatorConsensusInfo::new(addr, public_key, 1);
    let epoch_state = Arc::new(EpochState {
        epoch: 1,
        verifier: ValidatorVerifier::new(vec![validator_info]).into(),
    });
    
    // Create manager with consensus key
    let update_certifier = Arc::new(DummyUpdateCertifier::default());
    let vtxn_pool = VTxnPoolState::default();
    let mut manager = IssuerLevelConsensusManager::new(
        private_key.clone(),
        addr,
        epoch_state.clone(),
        update_certifier.clone(),
        vtxn_pool.clone(),
    );
    
    // Initialize with on-chain state
    let issuer = issuer_from_str("https://example.com");
    let initial_jwks = vec![
        JWK::Unsupported(UnsupportedJWK::new_for_testing("kid1", "payload1")).into(),
    ];
    let on_chain = ProviderJWKs {
        issuer: issuer.clone(),
        version: 1,
        jwks: initial_jwks,
    };
    manager.reset_with_on_chain_state(AllProvidersJWKs {
        entries: vec![on_chain],
    }).unwrap();
    
    // Observe a new JWK update, triggering InProgress state
    let new_jwks = vec![
        JWK::Unsupported(UnsupportedJWK::new_for_testing("kid2", "payload2")).into(),
    ];
    manager.process_new_observation(issuer.clone(), new_jwks.clone()).unwrap();
    
    // Verify consensus is InProgress
    let state = manager.states_by_issuer.get(&issuer).unwrap();
    assert!(matches!(state.consensus_state, ConsensusState::InProgress { .. }));
    
    // Simulate epoch transition by tearing down the manager
    let (ack_tx, ack_rx) = oneshot::channel();
    manager.tear_down(Some(ack_tx)).await.unwrap();
    ack_rx.await.unwrap();
    
    // Create new manager with rotated key (simulating epoch transition)
    let new_private_key = Arc::new(PrivateKey::generate_for_testing());
    let mut new_manager = IssuerLevelConsensusManager::new(
        new_private_key,
        addr,
        epoch_state.clone(),
        update_certifier,
        vtxn_pool,
    );
    
    // Reset with same on-chain state (the in-progress update wasn't committed)
    let on_chain = ProviderJWKs {
        issuer: issuer.clone(),
        version: 1,
        jwks: vec![
            JWK::Unsupported(UnsupportedJWK::new_for_testing("kid1", "payload1")).into(),
        ],
    };
    new_manager.reset_with_on_chain_state(AllProvidersJWKs {
        entries: vec![on_chain],
    }).unwrap();
    
    // Verify: The in-progress consensus session was lost
    let new_state = new_manager.states_by_issuer.get(&issuer).unwrap();
    assert!(matches!(new_state.consensus_state, ConsensusState::NotStarted));
    
    // The new JWKs that were being consensus'd are lost
    // They must be re-observed, causing a delay in security updates
    assert_eq!(new_state.observed, None);
    
    println!("âœ— VULNERABILITY CONFIRMED: In-progress JWK consensus session lost during epoch transition");
    println!("  - Old manager had InProgress consensus for new JWKs");
    println!("  - After epoch transition, new manager has NotStarted state");
    println!("  - Security update delayed until re-observation and re-consensus");
}
```

**Notes**:
- This PoC requires the test helper types (`DummyUpdateCertifier`) from the existing test module
- The test demonstrates that in-progress consensus state is not transferred across epoch boundaries
- In production, this creates a vulnerability window where compromised JWKs remain valid longer than intended

### Citations

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L909-932)
```text
    /// Rotate the consensus key of the validator, it'll take effect in next epoch.
    public entry fun rotate_consensus_key(
        operator: &signer,
        pool_address: address,
        new_consensus_pubkey: vector<u8>,
        proof_of_possession: vector<u8>,
    ) acquires StakePool, ValidatorConfig {
        check_stake_permission(operator);
        assert_reconfig_not_in_progress();
        assert_stake_pool_exists(pool_address);

        let stake_pool = borrow_global_mut<StakePool>(pool_address);
        assert!(signer::address_of(operator) == stake_pool.operator_address, error::unauthenticated(ENOT_OPERATOR));

        assert!(exists<ValidatorConfig>(pool_address), error::not_found(EVALIDATOR_CONFIG));
        let validator_info = borrow_global_mut<ValidatorConfig>(pool_address);
        let old_consensus_pubkey = validator_info.consensus_pubkey;
        // Checks the public key has a valid proof-of-possession to prevent rogue-key attacks.
        let pubkey_from_pop = &bls12381::public_key_from_bytes_with_pop(
            new_consensus_pubkey,
            &proof_of_possession_from_bytes(proof_of_possession)
        );
        assert!(option::is_some(pubkey_from_pop), error::invalid_argument(EINVALID_PUBLIC_KEY));
        validator_info.consensus_pubkey = new_consensus_pubkey;
```

**File:** crates/aptos-jwk-consensus/src/epoch_manager.rs (L122-122)
```rust
    pub async fn start(mut self, mut network_receivers: NetworkReceivers) {
```

**File:** crates/aptos-jwk-consensus/src/epoch_manager.rs (L259-264)
```rust
    async fn on_new_epoch(&mut self, reconfig_notification: ReconfigNotification<P>) -> Result<()> {
        self.shutdown_current_processor().await;
        self.start_new_epoch(reconfig_notification.on_chain_configs)
            .await?;
        Ok(())
    }
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager/mod.rs (L39-62)
```rust
pub struct IssuerLevelConsensusManager {
    /// Some useful metadata.
    my_addr: AccountAddress,
    epoch_state: Arc<EpochState>,

    /// Used to sign JWK observations before sharing them with peers.
    consensus_key: Arc<PrivateKey>,

    /// The sub-process that collects JWK updates from peers and aggregate them into a quorum-certified JWK update.
    update_certifier: Arc<dyn TUpdateCertifier<PerIssuerMode>>,

    /// When a quorum-certified JWK update is available, use this to put it into the validator transaction pool.
    vtxn_pool: VTxnPoolState,

    /// The JWK consensus states of all the issuers.
    states_by_issuer: HashMap<Issuer, PerProviderState>,

    /// Whether a CLOSE command has been received.
    stopped: bool,

    qc_update_tx: aptos_channel::Sender<Issuer, QuorumCertifiedUpdate>,
    qc_update_rx: aptos_channel::Receiver<Issuer, QuorumCertifiedUpdate>,
    jwk_observers: Vec<JWKObserver>,
}
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager/mod.rs (L170-181)
```rust
    async fn tear_down(&mut self, ack_tx: Option<oneshot::Sender<()>>) -> Result<()> {
        self.stopped = true;
        let futures = std::mem::take(&mut self.jwk_observers)
            .into_iter()
            .map(JWKObserver::shutdown)
            .collect::<Vec<_>>();
        join_all(futures).await;
        if let Some(tx) = ack_tx {
            let _ = tx.send(());
        }
        Ok(())
    }
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager/mod.rs (L231-291)
```rust
    pub fn reset_with_on_chain_state(&mut self, on_chain_state: AllProvidersJWKs) -> Result<()> {
        info!(
            epoch = self.epoch_state.epoch,
            "reset_with_on_chain_state starting."
        );
        let onchain_issuer_set: HashSet<Issuer> = on_chain_state
            .entries
            .iter()
            .map(|entry| entry.issuer.clone())
            .collect();
        let local_issuer_set: HashSet<Issuer> = self.states_by_issuer.keys().cloned().collect();

        for issuer in local_issuer_set.difference(&onchain_issuer_set) {
            info!(
                epoch = self.epoch_state.epoch,
                op = "delete",
                issuer = issuer.clone(),
                "reset_with_on_chain_state"
            );
        }

        self.states_by_issuer
            .retain(|issuer, _| onchain_issuer_set.contains(issuer));
        for on_chain_provider_jwks in on_chain_state.entries {
            let issuer = on_chain_provider_jwks.issuer.clone();
            let locally_cached = self
                .states_by_issuer
                .get(&on_chain_provider_jwks.issuer)
                .and_then(|s| s.on_chain.as_ref());
            if locally_cached == Some(&on_chain_provider_jwks) {
                // The on-chain update did not touch this provider.
                // The corresponding local state does not have to be reset.
                info!(
                    epoch = self.epoch_state.epoch,
                    op = "no-op",
                    issuer = issuer,
                    "reset_with_on_chain_state"
                );
            } else {
                let old_value = self.states_by_issuer.insert(
                    on_chain_provider_jwks.issuer.clone(),
                    PerProviderState::new(on_chain_provider_jwks),
                );
                let op = if old_value.is_some() {
                    "update"
                } else {
                    "insert"
                };
                info!(
                    epoch = self.epoch_state.epoch,
                    op = op,
                    issuer = issuer,
                    "reset_with_on_chain_state"
                );
            }
        }
        info!(
            epoch = self.epoch_state.epoch,
            "reset_with_on_chain_state finished."
        );
        Ok(())
```

**File:** crates/aptos-jwk-consensus/src/types.rs (L96-101)
```rust
impl Drop for QuorumCertProcessGuard {
    fn drop(&mut self) {
        let QuorumCertProcessGuard { handle } = self;
        handle.abort();
    }
}
```
