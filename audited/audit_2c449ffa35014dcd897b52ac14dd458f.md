# Audit Report

## Title
Partial Genesis Commit Without Recovery Mechanism Leads to Validator Inconsistent State

## Summary
The genesis initialization process uses parallel database writes without atomic commit guarantees across all databases. If a validator crashes mid-execution during genesis application, some databases are written while others remain empty, leaving the validator in an inconsistent state with no automatic recovery mechanism. On restart, the validator attempts to reapply genesis on top of partially written data, causing state corruption that requires manual intervention.

## Finding Description

The genesis application process violates the atomic state transition invariant through a multi-step commit process that is not crash-safe: [1](#0-0) 

This function spawns multiple parallel tasks to write to different databases (events_db, write_set_db, transaction_db, transaction_info_db, state_kv_db, transaction_accumulator_db). Each task uses `.unwrap()` which causes a panic on any error, but critically, if the process crashes (power failure, OOM kill, validator hardware failure), some tasks may have already successfully committed their data to RocksDB while others have not.

The commit progress metadata is only written AFTER all parallel writes complete: [2](#0-1) 

The `OverallCommitProgress` is written at the end of `commit_ledger`, but if the crash occurs during `pre_commit_ledger`, this metadata is never set.

On node restart, the recovery mechanism checks for commit progress: [3](#0-2) 

When `OverallCommitProgress` is None (line 417-420), the sync logic simply logs "No overall commit progress was found!" and returns without performing any truncation, leaving partial data in place.

The genesis bootstrap logic then checks if genesis should be applied: [4](#0-3) 

Since the state_store version is still 0 (no progress was committed), the system attempts to apply genesis again. The validation check passes because the expected version matches: [5](#0-4) 

The code contains an explicit TODO acknowledging this issue: [6](#0-5) 

## Impact Explanation

This issue qualifies as **High Severity** under the Aptos Bug Bounty criteria for the following reasons:

1. **Validator Node Issues**: Affected validators become stuck in an inconsistent state where they cannot successfully complete genesis initialization, requiring manual database cleanup and restart - qualifying as "Validator node slowdowns" and operational issues.

2. **State Inconsistencies Requiring Intervention**: The partial commit leaves databases in an inconsistent state that cannot be automatically recovered, requiring manual intervention (database wipe and restart) - qualifying as "State inconsistencies requiring intervention" (Medium severity minimum).

3. **Protocol Violation**: This breaks the fundamental "State Consistency" invariant that "State transitions must be atomic and verifiable via Merkle proofs" - qualifying as "Significant protocol violations" (High severity).

4. **Network Impact**: If multiple validators experience similar crashes (e.g., due to insufficient memory configuration causing OOM kills), multiple nodes could be stuck, impacting network liveness during genesis or hard fork scenarios.

## Likelihood Explanation

The likelihood of this vulnerability being triggered is **MODERATE to HIGH**:

1. **Genesis Scenarios**: This affects initial network genesis and hard fork genesis transactions. While infrequent, these are critical moments when all validators must coordinate.

2. **Realistic Crash Scenarios**:
   - Power failures during data center operations
   - Out-of-Memory (OOM) kills if validator is misconfigured with insufficient memory
   - Hardware failures during the I/O-intensive parallel write phase
   - Network partitions causing timeouts and process kills by orchestration systems

3. **Long-Running Writes**: The parallel database writes for genesis (which includes writing the entire framework, all validator accounts, and initial state) can take significant time, increasing the crash window.

4. **No Automatic Recovery**: The absence of automatic recovery means every crash requires manual intervention, making this a persistent operational risk.

## Recommendation

Implement a crash-safe genesis commit mechanism using one of these approaches:

**Option 1: Write-Ahead Logging with Rollback**
```rust
// Before attempting genesis, write a "genesis_in_progress" marker
// If found on restart, truncate ALL databases to version 0

fn pre_commit_ledger(&self, chunk: ChunkToCommit, sync_commit: bool) -> Result<()> {
    // 1. Write genesis-in-progress marker
    if chunk.first_version == 0 {
        self.ledger_db.metadata_db().mark_genesis_in_progress()?;
    }
    
    // 2. Perform writes
    self.calculate_and_commit_ledger_and_state_kv(&chunk, self.skip_index_and_usage)?;
    
    // 3. Update state
    self.state_store.buffered_state().lock().update(
        chunk.result_ledger_state_with_summary(),
        chunk.estimated_total_state_updates(),
        sync_commit || chunk.is_reconfig,
    )?;
    
    // 4. Clear marker only after commit_ledger succeeds
    Ok(())
}
```

**Option 2: Enhanced Recovery in sync_commit_progress**
```rust
pub fn sync_commit_progress(...) {
    let ledger_metadata_db = ledger_db.metadata_db();
    
    // Check for genesis-in-progress marker
    if ledger_metadata_db.is_genesis_in_progress()? {
        info!("Detected incomplete genesis, truncating all databases to version 0");
        truncate_all_databases_to_genesis(...)?;
        ledger_metadata_db.clear_genesis_in_progress()?;
        return;
    }
    
    // Existing logic...
}
```

**Option 3: Two-Phase Commit**
Write all data to a temporary staging area first, then atomically promote to the main database after all parallel writes complete successfully. This ensures either all data is written or none.

The code should also replace `.unwrap()` calls with proper error propagation to enable graceful recovery rather than panicking.

## Proof of Concept

```rust
// Test to reproduce the vulnerability
#[test]
fn test_partial_genesis_recovery() {
    use aptos_db::AptosDB;
    use aptos_executor::db_bootstrapper::{calculate_genesis, maybe_bootstrap};
    use aptos_vm::aptos_vm::AptosVMBlockExecutor;
    use std::sync::Arc;
    
    // 1. Create a fresh database
    let tmpdir = aptos_temppath::TempPath::new();
    let db = AptosDB::new_for_test(&tmpdir);
    let db_rw = DbReaderWriter::new(db);
    
    // 2. Start genesis commit but simulate crash during parallel writes
    let genesis_txn = test_genesis_transaction();
    let waypoint = Waypoint::new_epoch_boundary(...)?;
    
    // Begin genesis - this will spawn parallel tasks
    let committer = calculate_genesis::<AptosVMBlockExecutor>(
        &db_rw, 
        db_rw.reader.get_pre_committed_ledger_summary()?,
        &genesis_txn
    )?;
    
    // 3. Simulate crash by dropping committer WITHOUT calling commit()
    // This leaves some databases written (from parallel tasks) but no progress metadata
    drop(committer);
    drop(db_rw);
    
    // 4. Reopen database (simulating restart)
    let db = AptosDB::open(...)?;  // sync_commit_progress runs here
    let db_rw = DbReaderWriter::new(db);
    
    // 5. Attempt to apply genesis again
    let result = maybe_bootstrap::<AptosVMBlockExecutor>(&db_rw, &genesis_txn, waypoint);
    
    // 6. Verify the database is now in an inconsistent state
    // - Some column families have genesis data
    // - Some are empty  
    // - No way to recover automatically
    
    // Expected: Should detect partial commit and truncate
    // Actual: Attempts to write on top of partial data, causing corruption
    assert!(result.is_err(), "Should detect inconsistent state");
}
```

## Notes

The TODO comment in the code explicitly acknowledges this issue: "Write progress for each of the following databases, and handle the inconsistency at the startup time." However, the inconsistency handling at startup time has not been implemented, leaving validators vulnerable to partial genesis commits. This is particularly critical because genesis is a special transaction that initializes the entire blockchain state, and any corruption at this level affects all subsequent operations.

### Citations

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L78-112)
```rust
    fn commit_ledger(
        &self,
        version: Version,
        ledger_info_with_sigs: Option<&LedgerInfoWithSignatures>,
        chunk_opt: Option<ChunkToCommit>,
    ) -> Result<()> {
        gauged_api("commit_ledger", || {
            // Pre-committing and committing in concurrency is allowed but not pre-committing at the
            // same time from multiple threads, the same for committing.
            // Consensus and state sync must hand over to each other after all pending execution and
            // committing complete.
            let _lock = self
                .commit_lock
                .try_lock()
                .expect("Concurrent committing detected.");
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["commit_ledger"]);

            let old_committed_ver = self.get_and_check_commit_range(version)?;

            let mut ledger_batch = SchemaBatch::new();
            // Write down LedgerInfo if provided.
            if let Some(li) = ledger_info_with_sigs {
                self.check_and_put_ledger_info(version, li, &mut ledger_batch)?;
            }
            // Write down commit progress
            ledger_batch.put::<DbMetadataSchema>(
                &DbMetadataKey::OverallCommitProgress,
                &DbMetadataValue::Version(version),
            )?;
            self.ledger_db.metadata_db().write_schemas(ledger_batch)?;

            // Notify the pruners, invoke the indexer, and update in-memory ledger info.
            self.post_commit(old_committed_ver, version, ledger_info_with_sigs, chunk_opt)
        })
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L245-261)
```rust
    fn pre_commit_validation(&self, chunk: &ChunkToCommit) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["save_transactions_validation"]);

        ensure!(!chunk.is_empty(), "chunk is empty, nothing to save.");

        let next_version = self.state_store.current_state_locked().next_version();
        // Ensure the incoming committing requests are always consecutive and the version in
        // buffered state is consistent with that in db.
        ensure!(
            chunk.first_version == next_version,
            "The first version passed in ({}), and the next version expected by db ({}) are inconsistent.",
            chunk.first_version,
            next_version,
        );

        Ok(())
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L263-322)
```rust
    fn calculate_and_commit_ledger_and_state_kv(
        &self,
        chunk: &ChunkToCommit,
        skip_index_and_usage: bool,
    ) -> Result<HashValue> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["save_transactions__work"]);

        let mut new_root_hash = HashValue::zero();
        THREAD_MANAGER.get_non_exe_cpu_pool().scope(|s| {
            // TODO(grao): Write progress for each of the following databases, and handle the
            // inconsistency at the startup time.
            //
            // TODO(grao): Consider propagating the error instead of panic, if necessary.
            s.spawn(|_| {
                self.commit_events(
                    chunk.first_version,
                    chunk.transaction_outputs,
                    skip_index_and_usage,
                )
                .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .write_set_db()
                    .commit_write_sets(chunk.first_version, chunk.transaction_outputs)
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .transaction_db()
                    .commit_transactions(
                        chunk.first_version,
                        chunk.transactions,
                        skip_index_and_usage,
                    )
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .persisted_auxiliary_info_db()
                    .commit_auxiliary_info(chunk.first_version, chunk.persisted_auxiliary_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_state_kv_and_ledger_metadata(chunk, skip_index_and_usage)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_transaction_infos(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                new_root_hash = self
                    .commit_transaction_accumulator(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
        });

        Ok(new_root_hash)
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L410-502)
```rust
    pub fn sync_commit_progress(
        ledger_db: Arc<LedgerDb>,
        state_kv_db: Arc<StateKvDb>,
        state_merkle_db: Arc<StateMerkleDb>,
        crash_if_difference_is_too_large: bool,
    ) {
        let ledger_metadata_db = ledger_db.metadata_db();
        if let Some(overall_commit_progress) = ledger_metadata_db
            .get_synced_version()
            .expect("DB read failed.")
        {
            info!(
                overall_commit_progress = overall_commit_progress,
                "Start syncing databases..."
            );
            let ledger_commit_progress = ledger_metadata_db
                .get_ledger_commit_progress()
                .expect("Failed to read ledger commit progress.");
            assert_ge!(ledger_commit_progress, overall_commit_progress);

            let state_kv_commit_progress = state_kv_db
                .metadata_db()
                .get::<DbMetadataSchema>(&DbMetadataKey::StateKvCommitProgress)
                .expect("Failed to read state K/V commit progress.")
                .expect("State K/V commit progress cannot be None.")
                .expect_version();
            assert_ge!(state_kv_commit_progress, overall_commit_progress);

            // LedgerCommitProgress was not guaranteed to commit after all ledger changes finish,
            // have to attempt truncating every column family.
            info!(
                ledger_commit_progress = ledger_commit_progress,
                "Attempt ledger truncation...",
            );
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");

            // State K/V commit progress isn't (can't be) written atomically with the data,
            // because there are shards, so we have to attempt truncation anyway.
            info!(
                state_kv_commit_progress = state_kv_commit_progress,
                "Start state KV truncation..."
            );
            let difference = state_kv_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_state_kv_db(
                &state_kv_db,
                state_kv_commit_progress,
                overall_commit_progress,
                std::cmp::max(difference as usize, 1), /* batch_size */
            )
            .expect("Failed to truncate state K/V db.");

            let state_merkle_max_version = get_max_version_in_state_merkle_db(&state_merkle_db)
                .expect("Failed to get state merkle max version.")
                .expect("State merkle max version cannot be None.");
            if state_merkle_max_version > overall_commit_progress {
                let difference = state_merkle_max_version - overall_commit_progress;
                if crash_if_difference_is_too_large {
                    assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
                }
            }
            let state_merkle_target_version = find_tree_root_at_or_before(
                ledger_metadata_db,
                &state_merkle_db,
                overall_commit_progress,
            )
            .expect("DB read failed.")
            .unwrap_or_else(|| {
                panic!(
                    "Could not find a valid root before or at version {}, maybe it was pruned?",
                    overall_commit_progress
                )
            });
            if state_merkle_target_version < state_merkle_max_version {
                info!(
                    state_merkle_max_version = state_merkle_max_version,
                    target_version = state_merkle_target_version,
                    "Start state merkle truncation..."
                );
                truncate_state_merkle_db(&state_merkle_db, state_merkle_target_version)
                    .expect("Failed to truncate state merkle db.");
            }
        } else {
            info!("No overall commit progress was found!");
        }
    }
```

**File:** execution/executor/src/db_bootstrapper/mod.rs (L48-71)
```rust
pub fn maybe_bootstrap<V: VMBlockExecutor>(
    db: &DbReaderWriter,
    genesis_txn: &Transaction,
    waypoint: Waypoint,
) -> Result<Option<LedgerInfoWithSignatures>> {
    let ledger_summary = db.reader.get_pre_committed_ledger_summary()?;
    // if the waypoint is not targeted with the genesis txn, it may be either already bootstrapped, or
    // aiming for state sync to catch up.
    if ledger_summary.version().map_or(0, |v| v + 1) != waypoint.version() {
        info!(waypoint = %waypoint, "Skip genesis txn.");
        return Ok(None);
    }

    let committer = calculate_genesis::<V>(db, ledger_summary, genesis_txn)?;
    ensure!(
        waypoint == committer.waypoint(),
        "Waypoint verification failed. Expected {:?}, got {:?}.",
        waypoint,
        committer.waypoint(),
    );
    let ledger_info = committer.output.ledger_info_opt.clone();
    committer.commit()?;
    Ok(ledger_info)
}
```
