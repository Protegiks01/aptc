# Audit Report

## Title
Race Condition in BatchStore Persistence Causes Batch Unavailability During Block Proposal

## Summary
A race condition exists between in-memory cache insertion and database persistence in the `BatchStore` implementation. When memory quota is exceeded, batches are stored with `StorageMode::PersistedOnly` in the cache before being written to the database. During this window, concurrent reads via `get_batch_from_local()` fail, causing batches to be silently skipped during block proposal creation. [1](#0-0) 

## Finding Description

The vulnerability occurs in the `persist_inner()` method's non-atomic two-phase persistence operation:

**Phase 1: Cache Insertion** [2](#0-1) 

The `save()` method calls `insert_to_cache()`, which determines storage mode based on available quota: [3](#0-2) 

When memory quota is exceeded, the batch is stored **without payload** (`StorageMode::PersistedOnly`), containing only metadata. The batch immediately becomes visible in the `db_cache` DashMap.

**Phase 2: Database Persistence** [4](#0-3) 

The database write happens **after** `insert_to_cache()` returns. There is no atomicity guarantee between these operations.

**The Race Window**

Between Phase 1 and Phase 2, any concurrent call to `get_batch_from_local()` will: [5](#0-4) 

1. Find the batch in cache with `StorageMode::PersistedOnly` (line 576)
2. Attempt to fetch payload from database via `get_batch_from_db()` (line 577)
3. Fail because the database write hasn't completed yet (line 557)
4. Return `Err(ExecutorError::CouldNotGetData)`

**Exploitation Path**

This race directly impacts `pull_batches_with_transactions()`, which is called by `ProofManager` during block proposal creation: [6](#0-5) 

When `get_batch_from_local()` fails, the batch is **silently skipped** with only a warning logged (lines 544-549). The block proposal proceeds with incomplete transaction content.

**Concurrent Execution Context**

The race occurs because persistence happens asynchronously in spawned tasks: [7](#0-6) 

Meanwhile, `ProofManager` handles proposal requests concurrently: [8](#0-7) 

## Impact Explanation

This vulnerability violates the **State Consistency** invariant (#4 in the specification): "State transitions must be atomic and verifiable." The batch's availability state is inconsistent—it exists in the system but is reported as unavailable.

**Impact Severity: Medium**

According to Aptos bug bounty criteria, this qualifies as "State inconsistencies requiring intervention":

1. **Batch Availability Violation**: Certified batches that should be available are incorrectly reported as unavailable
2. **Non-Deterministic Block Content**: Under race conditions, proposal leaders may include different transaction sets depending on timing
3. **Throughput Degradation**: Available batches are unnecessarily skipped, reducing network throughput
4. **Batch Serving Failures**: The batch retrieval task may return `NotFound` for batches that exist: [9](#0-8) 

This does NOT constitute a consensus safety violation because:
- Only the leader creates proposals for a given round
- All validators vote on the same proposal content
- No blockchain fork or double-spend is possible

However, it breaks the quorum store's correctness guarantees and requires careful timing analysis to debug.

## Likelihood Explanation

**Likelihood: Medium-High under load**

The race window depends on:
1. **Memory quota exhaustion**: Requires high batch volume to trigger `PersistedOnly` storage mode
2. **Database I/O latency**: Slower storage increases the race window
3. **Concurrent proposal creation**: ProofManager must request batches during the persistence window

Under high network load with many validators submitting batches:
- Memory quotas are frequently exceeded (demonstrated in test code at line 192-194)
- Multiple components access `BatchStore` concurrently
- Disk I/O latency can be 10-100ms, creating a realistic exploitation window

The vulnerability is deterministic once the race conditions align—no special attacker capabilities required beyond generating normal transaction load.

## Recommendation

**Solution: Implement atomic cache-and-persist operation**

Modify `insert_to_cache()` to accept a database persistence callback and ensure atomicity:

```rust
pub(crate) fn insert_to_cache_and_persist(
    &self,
    value: &PersistedValue<BatchInfoExt>,
    persist_to_db: bool,
) -> anyhow::Result<bool> {
    let digest = *value.digest();
    let author = value.author();
    let expiration_time = value.expiration();
    
    // Determine storage mode BEFORE making batch visible
    let storage_mode = self
        .peer_quota
        .entry(author)
        .or_insert(QuotaManager::new(
            self.db_quota,
            self.memory_quota,
            self.batch_quota,
        ))
        .update_quota(value.num_bytes() as usize)?;
    
    // If PersistedOnly mode, write to DB FIRST
    if persist_to_db && storage_mode == StorageMode::PersistedOnly {
        if !value.batch_info().is_v2() {
            let persist_request: PersistedValue<BatchInfo> = 
                value.clone().try_into().expect("Must be a V1 batch");
            self.db.save_batch(persist_request)?;
        } else {
            self.db.save_batch_v2(value.clone())?;
        }
    }
    
    // Now insert to cache with correct storage mode
    let value_to_be_stored = if storage_mode == StorageMode::PersistedOnly {
        PersistedValue::new(value.batch_info().clone(), None)
    } else {
        value.clone()
    };
    
    // Proceed with cache insertion...
    let cache_entry = self.db_cache.entry(digest);
    // ... rest of insertion logic
    
    Ok(true)
}
```

**Key changes:**
1. For `PersistedOnly` mode, persist to database **before** adding to cache
2. Only make batch visible in cache after DB write completes
3. Maintain atomicity: external readers never see partial state

Alternative: Add a pending persistence marker to prevent reads during the race window.

## Proof of Concept

```rust
#[tokio::test]
async fn test_batch_store_race_condition() {
    use std::sync::Arc;
    use tokio::time::{sleep, Duration};
    
    // Create BatchStore with very low memory quota to force PersistedOnly mode
    let storage = Arc::new(MockQuorumStoreDB::new());
    let batch_store = Arc::new(BatchStore::new(
        1, // epoch
        false,
        0, // last_certified_time
        storage,
        10, // memory_quota - very small
        1000, // db_quota
        100, // batch_quota
        create_signer(),
        Duration::from_secs(60).as_micros() as u64,
    ));
    
    // Create a large batch that will exceed memory quota
    let large_batch = create_batch_with_transactions(100); // 100 transactions
    let persist_value = PersistedValue::new(
        large_batch.batch_info().clone(),
        Some(large_batch.into_transactions()),
    );
    
    let batch_store_clone = batch_store.clone();
    let digest = *persist_value.digest();
    
    // Spawn persistence task (simulates BatchCoordinator behavior)
    let persist_handle = tokio::spawn(async move {
        // Add small delay to simulate processing
        sleep(Duration::from_millis(10)).await;
        batch_store_clone.persist(vec![persist_value])
    });
    
    // Immediately try to read the batch (simulates ProofManager)
    let read_handle = tokio::spawn(async move {
        sleep(Duration::from_millis(5)).await; // Race window
        batch_store.get_batch_from_local(&digest)
    });
    
    let persist_result = persist_handle.await.unwrap();
    let read_result = read_handle.await.unwrap();
    
    // Vulnerability: read fails even though persist succeeds
    assert!(!persist_result.is_empty(), "Persistence should succeed");
    assert!(read_result.is_err(), "Read fails due to race condition");
    assert_eq!(read_result.unwrap_err(), ExecutorError::CouldNotGetData);
}
```

**Notes**

This race condition is subtle but real. Under production conditions with high transaction volume, memory quota exhaustion, and concurrent consensus operations, the vulnerability manifests as unexplained batch unavailability and reduced block throughput. The fix requires careful architectural changes to ensure atomicity between cache and persistent storage operations.

### Citations

**File:** consensus/src/quorum_store/batch_store.rs (L383-397)
```rust
            let value_to_be_stored = if self
                .peer_quota
                .entry(author)
                .or_insert(QuotaManager::new(
                    self.db_quota,
                    self.memory_quota,
                    self.batch_quota,
                ))
                .update_quota(value.num_bytes() as usize)?
                == StorageMode::PersistedOnly
            {
                PersistedValue::new(value.batch_info().clone(), None)
            } else {
                value.clone()
            };
```

**File:** consensus/src/quorum_store/batch_store.rs (L488-528)
```rust
    fn persist_inner(
        &self,
        batch_info: BatchInfoExt,
        persist_request: PersistedValue<BatchInfoExt>,
    ) -> Option<SignedBatchInfo<BatchInfoExt>> {
        assert!(
            &batch_info == persist_request.batch_info(),
            "Provided batch info doesn't match persist request batch info"
        );
        match self.save(&persist_request) {
            Ok(needs_db) => {
                trace!("QS: sign digest {}", persist_request.digest());
                if needs_db {
                    if !batch_info.is_v2() {
                        let persist_request =
                            persist_request.try_into().expect("Must be a V1 batch");
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch(persist_request)
                            .expect("Could not write to DB");
                    } else {
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch_v2(persist_request)
                            .expect("Could not write to DB")
                    }
                }
                if !batch_info.is_v2() {
                    self.generate_signed_batch_info(batch_info.info().clone())
                        .ok()
                        .map(|inner| inner.into())
                } else {
                    self.generate_signed_batch_info(batch_info).ok()
                }
            },
            Err(e) => {
                debug!("QS: failed to store to cache {:?}", e);
                None
            },
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L571-585)
```rust
    pub(crate) fn get_batch_from_local(
        &self,
        digest: &HashValue,
    ) -> ExecutorResult<PersistedValue<BatchInfoExt>> {
        if let Some(value) = self.db_cache.get(digest) {
            if value.payload_storage_mode() == StorageMode::PersistedOnly {
                self.get_batch_from_db(digest, value.batch_info().is_v2())
            } else {
                // Available in memory.
                Ok(value.clone())
            }
        } else {
            Err(ExecutorError::CouldNotGetData)
        }
    }
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L539-550)
```rust
        for batch in batches.into_iter() {
            if let Ok(mut persisted_value) = self.batch_store.get_batch_from_local(batch.digest()) {
                if let Some(txns) = persisted_value.take_payload() {
                    result.push((batch, txns));
                }
            } else {
                warn!(
                    "Couldn't find a batch in local storage while creating inline block: {:?}",
                    batch.digest()
                );
            }
        }
```

**File:** consensus/src/quorum_store/batch_coordinator.rs (L87-90)
```rust
        let batch_store = self.batch_store.clone();
        let network_sender = self.network_sender.clone();
        let sender_to_proof_manager = self.sender_to_proof_manager.clone();
        tokio::spawn(async move {
```

**File:** consensus/src/quorum_store/proof_manager.rs (L167-180)
```rust
                let (inline_batches, inline_payload_size, _) =
                    self.batch_proof_queue.pull_batches_with_transactions(
                        &excluded_batches
                            .iter()
                            .cloned()
                            .chain(proof_block.iter().map(|proof| proof.info().clone()))
                            .chain(opt_batches.clone())
                            .collect(),
                        max_inline_txns_to_pull,
                        request.max_txns_after_filtering,
                        request.soft_max_txns_after_filtering,
                        request.return_non_full,
                        request.block_timestamp,
                    );
```

**File:** consensus/src/quorum_store/quorum_store_builder.rs (L408-416)
```rust
                let response = if let Ok(value) =
                    batch_store.get_batch_from_local(&rpc_request.req.digest())
                {
                    let batch: Batch<BatchInfoExt> = value.try_into().unwrap();
                    let batch: Batch<BatchInfo> = batch
                        .try_into()
                        .expect("Batch retieval requests must be for V1 batch");
                    BatchResponse::Batch(batch)
                } else {
```
