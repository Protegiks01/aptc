# Audit Report

## Title
Indexer gRPC Manager Permanent Unavailability Due to Missing Request Timeouts and Connection Leak

## Summary
The `FullnodeDataClient` uses a single gRPC connection per fullnode without timeout configuration or connection cleanup mechanisms. When a fullnode becomes slow or unresponsive, the indexer's main data-fetching loop can hang indefinitely, blocking all transaction caching operations and causing permanent service unavailability.

## Finding Description
The indexer gRPC manager creates one `FullnodeDataClient` per fullnode address and stores them permanently in a `DashMap`. [1](#0-0) 

The client wraps a single `tonic::client::Grpc<T>` connection with no connection pooling. [2](#0-1) 

**Critical Issues:**

1. **No Request Timeout**: The gRPC channel is created with `connect_lazy()` and no timeout configuration. [3](#0-2) 

2. **Blocking Ready Check**: Each request calls `self.inner.ready().await` which can hang indefinitely if the connection is in a bad state. [4](#0-3) 

3. **No Connection Cleanup**: Fullnode connections are intentionally never removed from the registry, even when unhealthy. [5](#0-4) 

4. **Single-Threaded Bottleneck**: The `DataManager` runs a single sequential loop that fetches transactions from fullnodes. [6](#0-5) 

When the loop makes a request without timeout, it can hang indefinitely, preventing any further transaction caching.

**Attack Scenario:**
1. A fullnode becomes slow due to network issues, database problems, or resource exhaustion
2. The DataManager selects this fullnode and calls `get_transactions_from_node()` without timeout
3. The request hangs in the `.ready().await` or during the streaming response
4. The entire DataManager loop is blocked, stopping all new transaction caching
5. The indexer's cache becomes stale, making it unable to serve current blockchain data
6. Client requests to the gRPC manager API may also trigger fallback requests to slow fullnodes, causing cascading timeouts

## Impact Explanation
This is **High Severity** per the Aptos bug bounty program criteria: "API crashes" and "Significant protocol violations."

The indexer gRPC manager provides critical infrastructure for querying blockchain data. When it becomes unavailable:
- Applications relying on indexer APIs cannot query current blockchain state
- Users cannot check balances, view transaction history, or interact with dApps
- The service requires manual restart to recover, with no automatic healing

While this doesn't directly affect consensus or validator operations, it causes significant availability disruption to the broader Aptos ecosystem infrastructure.

## Likelihood Explanation
**Very High Likelihood:**
- Fullnodes becoming slow or unresponsive is a common operational scenario (network issues, load spikes, database maintenance)
- No timeout means any slowdown causes indefinite hangs
- The single-threaded DataManager loop ensures one slow fullnode blocks everything
- Connections are never cleaned up, so the problem persists until manual intervention
- No retry or failover logic exists at the connection level

This will occur naturally in production without requiring any malicious action.

## Recommendation
Implement comprehensive timeout and connection health management:

```rust
// In Fullnode::new() - Add timeout configuration
fn new(address: GrpcAddress) -> Self {
    let channel = Channel::from_shared(address)
        .expect("Bad address.")
        .timeout(Duration::from_secs(30))  // Add request timeout
        .connect_timeout(Duration::from_secs(10))  // Add connect timeout
        .tcp_keepalive(Some(Duration::from_secs(30)))  // Add TCP keepalive
        .http2_keep_alive_interval(Duration::from_secs(30))  // Add HTTP2 keepalive
        .keep_alive_timeout(Duration::from_secs(10))
        .connect_lazy();
    // ... rest of implementation
}

// In data_manager.rs - Add timeout wrapper around requests
use tokio::time::timeout;

let response = timeout(
    Duration::from_secs(60),  // 60 second timeout
    fullnode_client.get_transactions_from_node(request)
).await;

match response {
    Ok(Ok(resp)) => { /* process response */ },
    Ok(Err(e)) => { /* handle gRPC error */ },
    Err(_) => { 
        warn!("Request to fullnode ({address}) timed out");
        // Mark fullnode as unhealthy and select different one
        continue;
    }
}

// In metadata_manager.rs - Add connection health tracking and removal
struct Fullnode {
    client: FullnodeDataClient<Channel>,
    recent_states: VecDeque<FullnodeInfo>,
    consecutive_failures: AtomicU32,  // Add failure tracking
    last_success: AtomicU64,  // Add timestamp tracking
}

// Remove unhealthy fullnodes periodically
if fullnode.consecutive_failures.load(Ordering::SeqCst) > 5 
    || elapsed_since_last_success > Duration::from_secs(300) {
    self.fullnodes.remove(&address);
    warn!("Removed unhealthy fullnode: {address}");
}
```

## Proof of Concept
```rust
// Create a test that simulates a slow fullnode
#[tokio::test]
async fn test_slow_fullnode_blocks_indexer() {
    // Start a mock gRPC server that delays responses
    let slow_server = tokio::spawn(async {
        // Simulate slow server that never responds
        tokio::time::sleep(Duration::from_secs(3600)).await;
    });
    
    // Configure GrpcManager with the slow fullnode
    let config = IndexerGrpcManagerConfig {
        fullnode_addresses: vec!["http://127.0.0.1:50051".to_string()],
        // ... other config
    };
    
    let manager = GrpcManager::new(&config).await;
    
    // Start DataManager - it will hang on the first request
    let start_time = Instant::now();
    tokio::time::timeout(
        Duration::from_secs(5),
        manager.data_manager.start(false, rx)
    ).await.expect_err("DataManager should timeout, proving it's blocked");
    
    // Verify that the manager is stuck and cannot cache new transactions
    assert!(start_time.elapsed() >= Duration::from_secs(5));
}
```

## Notes
This vulnerability affects the indexer infrastructure layer, not core blockchain consensus. However, it represents a significant availability issue that can cause permanent service disruption requiring manual intervention. The root cause is the combination of: (1) no timeout configuration on gRPC requests, (2) permanent connection retention without health checks, and (3) a single-threaded processing loop that cannot proceed when blocked on one slow fullnode.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L67-80)
```rust
    fn new(address: GrpcAddress) -> Self {
        let channel = Channel::from_shared(address)
            .expect("Bad address.")
            .connect_lazy();
        let client = FullnodeDataClient::new(channel)
            .send_compressed(CompressionEncoding::Zstd)
            .accept_compressed(CompressionEncoding::Zstd)
            .max_encoding_message_size(MAX_MESSAGE_SIZE)
            .max_decoding_message_size(MAX_MESSAGE_SIZE);
        Self {
            client,
            recent_states: VecDeque::new(),
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L306-306)
```rust
            // NOTE: We don't remove FNs and GrpcManagers here intentionally.
```

**File:** protos/rust/src/pb/aptos.internal.fullnode.v1.tonic.rs (L11-14)
```rust
    #[derive(Debug, Clone)]
    pub struct FullnodeDataClient<T> {
        inner: tonic::client::Grpc<T>,
    }
```

**File:** protos/rust/src/pb/aptos.internal.fullnode.v1.tonic.rs (L129-137)
```rust
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::new(
                        tonic::Code::Unknown,
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L217-220)
```rust
            let (address, mut fullnode_client) =
                self.metadata_manager.get_fullnode_for_request(&request);
            trace!("Fullnode ({address}) is picked for request.");
            let response = fullnode_client.get_transactions_from_node(request).await;
```
