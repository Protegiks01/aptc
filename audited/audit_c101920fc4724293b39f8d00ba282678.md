# Audit Report

## Title
DKG Transcript Aggregation Memory Exhaustion via Unbounded Deserialization

## Summary
Byzantine validators can craft malicious DKG transcripts with oversized internal vectors that bypass size validation, causing honest validators to exhaust memory during the aggregation process. The vulnerability exists because BCS deserialization allocates memory before validation checks can reject malformed transcripts.

## Finding Description

During the DKG (Distributed Key Generation) process, validators broadcast their transcripts to peers for aggregation. The aggregation logic in `TranscriptAggregationState::add()` deserializes incoming transcripts before performing validation checks. [1](#0-0) 

The deserialization uses `bcs::from_bytes` without any size limit or pre-validation of the `transcript_bytes` field. This allows a Byzantine validator to craft a malicious `Transcript` with an extremely large `soks` vector (signatures of knowledge).

The PVSS transcript structure contains several vector fields: [2](#0-1) 

While the `verify()` function validates the sizes of V, V_hat, R, R_hat, and C vectors: [3](#0-2) 

The `check_sizes()` function notably **does NOT validate the size of the `soks` vector**. This oversight allows an attacker to include hundreds of thousands of sok entries.

During aggregation, soks from each contributor are appended: [4](#0-3) 

**Attack Flow:**

1. Byzantine validator creates a `Transcript` with 300,000+ sok entries (each ~200 bytes = ~60 MB)
2. Other vectors (V, V_hat, R, R_hat, C) are correctly sized to pass `check_sizes()`
3. BCS serializes this into `transcript_bytes` (~60 MB)
4. Wraps in `DKGTranscript` and `DKGMessage::TranscriptResponse`
5. Network layer compresses and sends (within the ~62 MiB limit): [5](#0-4) 

6. Honest validator receives and deserializes the message envelope
7. Calls `TranscriptAggregationState::add()` which deserializes `transcript_bytes` **without size limits**
8. BCS allocates ~60 MB for the malicious soks vector
9. Validation checks execute AFTER memory allocation:
   - `verify_transcript_extra()` extracts dealers from soks and checks for duplicates [6](#0-5) 

10. Check at line 311 fails (duplicate dealers) or line 315 fails (wrong dealer set)
11. Transcript is rejected, but ~60 MB was already allocated and processed

With multiple colluding Byzantine validators (e.g., 10 out of 100), each sending malicious transcripts, honest nodes experience:
- 10 Ã— 60 MB = 600 MB memory spike
- Repeated memory allocation/deallocation causing GC pressure
- Potential OOM on memory-constrained validator nodes

## Impact Explanation

**Severity: High** (Validator node slowdowns)

This vulnerability enables Byzantine validators to cause memory exhaustion on honest validator nodes during DKG transcript aggregation. While the memory is eventually freed after validation rejection, the temporary spike can:

1. **Slow down validator nodes** through excessive memory allocation and GC pressure
2. **Crash nodes with limited memory** via OOM conditions  
3. **Degrade DKG performance** across the network
4. **Be repeated every epoch** as DKG runs at epoch boundaries

Per the Aptos bug bounty severity criteria, this constitutes "Validator node slowdowns" which falls under **High Severity** (up to $50,000).

The attack is amplified when multiple Byzantine validators collude, as each can send one malicious transcript per DKG session, compounding the memory pressure on honest nodes.

## Likelihood Explanation

**Likelihood: High**

The vulnerability is highly likely to be exploitable because:

1. **Low barrier to entry**: Any validator (even with minimal stake) can participate in DKG
2. **No authentication of transcript size**: No pre-validation checks before deserialization
3. **Reliable broadcast distribution**: The reliable broadcast mechanism ensures all validators receive responses
4. **Recurring opportunity**: DKG runs at every epoch transition, providing repeated attack windows
5. **No rate limiting**: Each Byzantine validator can send one malicious transcript per session

The attack requires:
- Byzantine validator status (achievable with minimal stake)
- Ability to craft malicious BCS-serialized transcripts
- Network connectivity to other validators

No special privileges or complex coordination beyond basic validator operation is needed.

## Recommendation

Implement size validation on `transcript_bytes` before deserialization:

```rust
fn add(
    &self,
    sender: Author,
    dkg_transcript: DKGTranscript,
) -> anyhow::Result<Option<Self::Aggregated>> {
    let DKGTranscript {
        metadata,
        transcript_bytes,
    } = dkg_transcript;
    
    // Add size check before deserialization
    const MAX_TRANSCRIPT_SIZE: usize = 10 * 1024 * 1024; // 10 MB reasonable limit
    ensure!(
        transcript_bytes.len() <= MAX_TRANSCRIPT_SIZE,
        "[DKG] transcript_bytes exceeds maximum size: {} > {}",
        transcript_bytes.len(),
        MAX_TRANSCRIPT_SIZE
    );
    
    ensure!(
        metadata.epoch == self.epoch_state.epoch,
        "[DKG] adding peer transcript failed with invalid node epoch",
    );
    // ... rest of function
```

Additionally, add validation for the `soks` vector size in `check_sizes()`:

```rust
fn check_sizes(&self, sc: &WeightedConfigBlstrs) -> anyhow::Result<()> {
    let W = sc.get_total_weight();
    
    // Add check for soks vector
    let num_dealers = self.soks.len();
    let num_validators = sc.get_total_num_players();
    if num_dealers > num_validators {
        bail!(
            "Expected at most {} dealers, but got {}",
            num_validators,
            num_dealers
        );
    }
    
    // ... existing checks for V, V_hat, R, R_hat, C
}
```

Consider using `bcs::from_bytes_with_limit` for transcript deserialization to prevent stack overflow from deeply nested structures.

## Proof of Concept

```rust
#[cfg(test)]
mod malicious_transcript_test {
    use super::*;
    use aptos_dkg::pvss::das::weighted_protocol::Transcript;
    
    #[test]
    fn test_oversized_soks_memory_exhaustion() {
        // Setup: Create DKG public params with small validator set
        let validator_infos = create_test_validators(10);
        let dkg_session_metadata = create_test_session_metadata(&validator_infos);
        let pub_params = RealDKG::new_public_params(&dkg_session_metadata);
        
        // Create malicious transcript with 100,000 sok entries
        let mut malicious_transcript = create_valid_transcript(&pub_params, 0);
        
        // Duplicate the first sok entry 100,000 times
        let first_sok = malicious_transcript.main.soks[0].clone();
        malicious_transcript.main.soks.clear();
        for _ in 0..100_000 {
            malicious_transcript.main.soks.push(first_sok.clone());
        }
        
        // Serialize to transcript_bytes
        let transcript_bytes = bcs::to_bytes(&malicious_transcript)
            .expect("serialization should succeed");
        
        println!("Malicious transcript size: {} bytes", transcript_bytes.len());
        assert!(transcript_bytes.len() > 10_000_000); // > 10 MB
        
        // Create DKGTranscript
        let dkg_transcript = DKGTranscript {
            metadata: DKGTranscriptMetadata {
                epoch: 1,
                author: validator_infos[0].addr,
            },
            transcript_bytes,
        };
        
        // Attempt aggregation - this will allocate large memory before rejecting
        let aggregation_state = create_aggregation_state(&pub_params);
        
        // This deserialization allocates memory for 100,000 soks
        let result = aggregation_state.add(validator_infos[0].addr, dkg_transcript);
        
        // Should fail at verification, but memory was already allocated
        assert!(result.is_err());
        assert!(result.unwrap_err().to_string().contains("dealer"));
    }
}
```

## Notes

The vulnerability is in the aggregation PROCESS (deserializing individual peer transcripts), not the final aggregated transcript at line 402. The final aggregated transcript has legitimate size bounded by the number of honest contributors and the total weight parameter. However, the process of getting there allows Byzantine validators to force honest nodes to allocate excessive memory during deserialization of malicious individual transcripts.

The network layer's ~62 MiB decompressed message limit provides some protection, but still allows transcripts large enough to cause significant memory pressure when multiple Byzantine validators attack simultaneously.

### Citations

**File:** dkg/src/transcript_aggregation/mod.rs (L88-90)
```rust
        let transcript = bcs::from_bytes(transcript_bytes.as_slice()).map_err(|e| {
            anyhow!("[DKG] adding peer transcript failed with trx deserialization error: {e}")
        })?;
```

**File:** crates/aptos-dkg/src/pvss/das/weighted_protocol.rs (L50-72)
```rust
pub struct Transcript {
    /// Proofs-of-knowledge (PoKs) for the dealt secret committed in $c = g_2^{p(0)}$.
    /// Since the transcript could have been aggregated from other transcripts with their own
    /// committed secrets in $c_i = g_2^{p_i(0)}$, this is a vector of PoKs for all these $c_i$'s
    /// such that $\prod_i c_i = c$.
    ///
    /// Also contains BLS signatures from each player $i$ on that player's contribution $c_i$, the
    /// player ID $i$ and auxiliary information `aux[i]` provided during dealing.
    soks: Vec<SoK<G1Projective>>,
    /// Commitment to encryption randomness $g_1^{r_j} \in G_1, \forall j \in [W]$
    R: Vec<G1Projective>,
    /// Same as $R$ except uses $g_2$.
    R_hat: Vec<G2Projective>,
    /// First $W$ elements are commitments to the evaluations of $p(X)$: $g_1^{p(\omega^i)}$,
    /// where $i \in [W]$. Last element is $g_1^{p(0)}$ (i.e., the dealt public key).
    V: Vec<G1Projective>,
    /// Same as $V$ except uses $g_2$.
    V_hat: Vec<G2Projective>,
    /// ElGamal encryption of the $j$th share of player $i$:
    /// i.e., $C[s_i+j-1] = h_1^{p(\omega^{s_i + j - 1})} ek_i^{r_j}, \forall i \in [n], j \in [w_i]$.
    /// We sometimes denote $C[s_i+j-1]$ by C_{i, j}.
    C: Vec<G1Projective>,
}
```

**File:** crates/aptos-dkg/src/pvss/das/weighted_protocol.rs (L405-407)
```rust
        for sok in &other.soks {
            self.soks.push(sok.clone());
        }
```

**File:** crates/aptos-dkg/src/pvss/das/weighted_protocol.rs (L415-455)
```rust
    fn check_sizes(&self, sc: &WeightedConfigBlstrs) -> anyhow::Result<()> {
        let W = sc.get_total_weight();

        if self.V.len() != W + 1 {
            bail!(
                "Expected {} G_2 (polynomial) commitment elements, but got {}",
                W + 1,
                self.V.len()
            );
        }

        if self.V_hat.len() != W + 1 {
            bail!(
                "Expected {} G_2 (polynomial) commitment elements, but got {}",
                W + 1,
                self.V_hat.len()
            );
        }

        if self.R.len() != W {
            bail!(
                "Expected {} G_1 commitment(s) to ElGamal randomness, but got {}",
                W,
                self.R.len()
            );
        }

        if self.R_hat.len() != W {
            bail!(
                "Expected {} G_2 commitment(s) to ElGamal randomness, but got {}",
                W,
                self.R_hat.len()
            );
        }

        if self.C.len() != W {
            bail!("Expected C of length {}, but got {}", W, self.C.len());
        }

        Ok(())
    }
```

**File:** config/src/config/network_config.rs (L47-50)
```rust
pub const MAX_APPLICATION_MESSAGE_SIZE: usize =
    (MAX_MESSAGE_SIZE - MAX_MESSAGE_METADATA_SIZE) - MESSAGE_PADDING_SIZE; /* The message size that applications should check against */
pub const MAX_FRAME_SIZE: usize = 4 * 1024 * 1024; /* 4 MiB large messages will be chunked into multiple frames and streamed */
pub const MAX_MESSAGE_SIZE: usize = 64 * 1024 * 1024; /* 64 MiB */
```

**File:** types/src/dkg/real_dkg/mod.rs (L302-316)
```rust
        let main_trx_dealers = trx.main.get_dealers();
        let mut dealer_set = HashSet::with_capacity(main_trx_dealers.len());
        for dealer in main_trx_dealers.iter() {
            if let Some(dealer_addr) = all_validator_addrs.get(dealer.id) {
                dealer_set.insert(*dealer_addr);
            } else {
                bail!("invalid dealer idx");
            }
        }
        ensure!(main_trx_dealers.len() == dealer_set.len());
        if ensures_single_dealer.is_some() {
            let expected_dealer_set: HashSet<AccountAddress> =
                ensures_single_dealer.into_iter().collect();
            ensure!(expected_dealer_set == dealer_set);
        }
```
