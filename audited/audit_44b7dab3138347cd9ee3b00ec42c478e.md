# Audit Report

## Title
Race Condition in HotState::get_committed() Causes State Version Mismatch Leading to Consensus Violation

## Summary
A critical race condition exists in `HotState::get_committed()` where the committed state metadata and hot state entries can be cloned from different versions. This causes validators to read inconsistent state during block execution, potentially producing different state roots for identical blocks and violating consensus safety.

## Finding Description

The vulnerability exists in `HotState::get_committed()` where two separate operations fetch state components without atomic synchronization. [1](#0-0) 

The function performs two distinct operations:
1. Locks `self.committed`, clones the `State` object containing version metadata, then **releases the lock**
2. Clones the `Arc<HotStateBase>` reference pointing to shared DashMaps **without any lock protection**

Between these operations, the Committer thread can update both components. The Committer's update sequence is: [2](#0-1) 

1. Line 196: Calls `commit()` which modifies the shared `HotStateBase` DashMaps to version N+1 [3](#0-2) 
2. Line 197: Updates `self.committed` to version N+1

**Race Scenario:**
- Thread A (block execution): Locks and clones `committed` → gets State at version N
- Thread A: **Releases lock**
- Thread B (Committer): Calls `commit()` → updates `base` DashMaps with version N+1 entries via `insert()` and `remove()` operations
- Thread B: Updates `committed` to version N+1
- Thread A: Clones `base` Arc → gets reference to HotStateBase containing version N+1 entries
- Thread A: Returns inconsistent tuple `(base_with_v(N+1)_entries, state_v(N))`

This inconsistent tuple is used to create `CachedStateView` during block execution. [4](#0-3)  The state view is created via `CachedStateView::new()`, which calls `get_persisted_state()` [5](#0-4) , which in turn calls `hot_state.get_committed()` [6](#0-5) 

The `CachedStateView` uses these inconsistent components where `persisted_state` (version N) becomes the base for the `speculative` delta, but `hot_state` contains entries from version N+1. [7](#0-6) 

When state values are queried during transaction execution, the lookup checks hot state which may return values from version N+1, even though `base_version()` reports version N. [8](#0-7)  The hot state contains actual `StateValue` objects with version information. [9](#0-8) 

**Consensus Impact:**

If different validators hit the race at different times when executing the same block:
- Validator V1: Gets `(hot_v(N), state_v(N))` → reads key K from hot state or falls back to cold at version N
- Validator V2: Gets `(hot_v(N+1), state_v(N))` → reads key K from hot state with version N+1 value
- Both validators execute identical block inputs but see different state values
- They produce **different state roots** for the same block execution
- **Consensus safety violation**: validators disagree on block results, potentially causing chain splits

## Impact Explanation

This is a **Critical Severity** vulnerability per Aptos Bug Bounty criteria under "Consensus/Safety Violations":

The core consensus invariant "Deterministic Execution: All validators must produce identical state roots for identical blocks" is broken. Different validators can produce different execution results for the same block, leading to:

1. **Chain Splits**: Validators may commit different state roots and diverge permanently
2. **Liveness Failures**: Consensus may stall when validators cannot reach agreement on block results
3. **Requires Hard Fork**: Recovery requires coordinated network-wide intervention to reconcile divergent chains

The impact affects the entire validator network and fundamentally breaks the safety guarantees of the AptosBFT consensus protocol. This matches the Critical category: "Different validators commit different blocks" with potential for "Chain splits without hardfork requirement."

## Likelihood Explanation

**Likelihood: Medium to High**

The race condition occurs during normal network operation whenever:
1. Block execution happens concurrently with hot state commits (continuous during normal operation)
2. The timing window between line 132 (lock release) and line 133 (Arc clone) coincides with Committer updates at lines 196-197

Given that:
- The Committer runs continuously in a background thread processing state updates
- Block execution happens frequently across all validators  
- Different validators execute blocks at different wall-clock times due to network propagation
- The race window is narrow but real and unprotected by any synchronization mechanism

The probability that different validators experience different race outcomes for the same block is non-negligible. While individual race occurrence may be rare per validator, over thousands of blocks across hundreds of validators operating independently, the cumulative probability becomes significant.

**No malicious behavior is required** - this occurs naturally during normal network operation due to timing variations between validators.

## Recommendation

Implement atomic acquisition of both the `State` and `HotStateBase` reference by holding the lock across both operations:

```rust
pub fn get_committed(&self) -> (Arc<dyn HotStateView>, State) {
    let guard = self.committed.lock();
    let state = guard.clone();
    let base = self.base.clone();
    drop(guard);
    (base, state)
}
```

Alternatively, use a read-write lock (`RwLock`) or ensure the `base` Arc is updated atomically together with `committed` to maintain version consistency between the two components.

## Proof of Concept

While a complete runnable PoC would require complex multi-threaded timing, the vulnerability is demonstrable through code inspection:

1. The `get_committed()` function releases the lock after line 132 before cloning `base` at line 133
2. The `Committer::run()` can interleave between these two operations
3. The `commit()` function modifies the shared DashMaps in place via `insert()` and `remove()`
4. Since `base` is an `Arc`, cloning it provides a reference to the same mutated DashMaps
5. Block execution uses the inconsistent tuple, leading to different state reads across validators

The race window and consensus impact are clearly evident from the code structure and execution flow through the block executor.

### Citations

**File:** storage/aptosdb/src/state_store/hot_state.rs (L131-136)
```rust
    pub fn get_committed(&self) -> (Arc<dyn HotStateView>, State) {
        let state = self.committed.lock().clone();
        let base = self.base.clone();

        (base, state)
    }
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L195-197)
```rust
        while let Some(to_commit) = self.next_to_commit() {
            self.commit(&to_commit);
            *self.committed.lock() = to_commit;
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L235-275)
```rust
    fn commit(&mut self, to_commit: &State) {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["hot_state_commit"]);

        let mut n_insert = 0;
        let mut n_update = 0;
        let mut n_evict = 0;

        let delta = to_commit.make_delta(&self.committed.lock());
        for shard_id in 0..NUM_STATE_SHARDS {
            for (key, slot) in delta.shards[shard_id].iter() {
                if slot.is_hot() {
                    let key_size = key.size();
                    self.total_key_bytes += key_size;
                    self.total_value_bytes += slot.size();
                    if let Some(old_slot) = self.base.shards[shard_id].insert(key, slot) {
                        self.total_key_bytes -= key_size;
                        self.total_value_bytes -= old_slot.size();
                        n_update += 1;
                    } else {
                        n_insert += 1;
                    }
                } else if let Some((key, old_slot)) = self.base.shards[shard_id].remove(&key) {
                    self.total_key_bytes -= key.size();
                    self.total_value_bytes -= old_slot.size();
                    n_evict += 1;
                }
            }
            self.heads[shard_id] = to_commit.latest_hot_key(shard_id);
            self.tails[shard_id] = to_commit.oldest_hot_key(shard_id);
            assert_eq!(
                self.base.shards[shard_id].len(),
                to_commit.num_hot_items(shard_id)
            );

            debug_assert!(self.validate_lru(shard_id).is_ok());
        }

        COUNTER.inc_with_by(&["hot_state_insert"], n_insert);
        COUNTER.inc_with_by(&["hot_state_update"], n_update);
        COUNTER.inc_with_by(&["hot_state_evict"], n_evict);
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L226-233)
```rust
                let state_view = {
                    let _timer = OTHER_TIMERS.timer_with(&["get_state_view"]);
                    CachedStateView::new(
                        StateViewId::BlockExecution { block_id },
                        Arc::clone(&self.db.reader),
                        parent_output.result_state().latest().clone(),
                    )?
                };
```

**File:** storage/storage-interface/src/state_store/state_view/cached_state_view.rs (L126-135)
```rust
    pub fn new(id: StateViewId, reader: Arc<dyn DbReader>, state: State) -> StateViewResult<Self> {
        let (hot_state, persisted_state) = reader.get_persisted_state()?;
        Ok(Self::new_impl(
            id,
            reader,
            hot_state,
            persisted_state,
            state,
        ))
    }
```

**File:** storage/storage-interface/src/state_store/state_view/cached_state_view.rs (L147-163)
```rust
    pub fn new_with_config(
        id: StateViewId,
        reader: Arc<dyn DbReader>,
        hot_state: Arc<dyn HotStateView>,
        persisted_state: State,
        state: State,
    ) -> Self {
        let version = state.version();

        Self {
            id,
            speculative: state.into_delta(persisted_state),
            hot: hot_state,
            cold: reader,
            memorized: ShardedStateCache::new_empty(version),
        }
    }
```

**File:** storage/storage-interface/src/state_store/state_view/cached_state_view.rs (L233-253)
```rust
    fn get_unmemorized(&self, state_key: &StateKey) -> Result<StateSlot> {
        COUNTER.inc_with(&["sv_unmemorized"]);

        let ret = if let Some(slot) = self.speculative.get_state_slot(state_key) {
            COUNTER.inc_with(&["sv_hit_speculative"]);
            slot
        } else if let Some(slot) = self.hot.get_state_slot(state_key) {
            COUNTER.inc_with(&["sv_hit_hot"]);
            slot
        } else if let Some(base_version) = self.base_version() {
            COUNTER.inc_with(&["sv_cold"]);
            StateSlot::from_db_get(
                self.cold
                    .get_state_value_with_version_by_version(state_key, base_version)?,
            )
        } else {
            StateSlot::ColdVacant
        };

        Ok(ret)
    }
```

**File:** storage/aptosdb/src/state_store/persisted_state.rs (L46-48)
```rust
    pub fn get_state(&self) -> (Arc<dyn HotStateView>, State) {
        self.hot_state.get_committed()
    }
```

**File:** types/src/state_store/state_slot.rs (L34-39)
```rust
    HotOccupied {
        value_version: Version,
        value: StateValue,
        hot_since_version: Version,
        lru_info: LRUEntry<StateKey>,
    },
```
