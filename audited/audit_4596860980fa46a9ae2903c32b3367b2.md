# Audit Report

## Title
Race Condition in DAG Certified Node Storage Allows State Desynchronization

## Summary
A race condition exists in the DAG consensus node storage mechanism where concurrent processing of the same certified node can result in desynchronization between the persistent database and in-memory DAG state. This occurs due to a split critical section between validation and insertion operations, allowing storage to be overwritten after validation but before in-memory insertion completes. [1](#0-0) 

## Finding Description
The vulnerability exists in the `DagStore::add_node` method which handles the insertion of certified nodes. The method performs three distinct operations with improper synchronization:

1. **Validation Phase**: Acquires write lock, validates the node, releases write lock
2. **Storage Phase**: Saves to persistent storage WITHOUT holding any lock
3. **Insertion Phase**: Acquires write lock again, inserts into in-memory DAG, releases write lock

The existence check performed before calling `add_node` uses only a read lock: [2](#0-1) 

This creates a race condition window where:
- Thread 1 and Thread 2 both pass the read-locked existence check
- Thread 1 validates (write lock) → passes → releases lock  
- Thread 2 validates (write lock) → passes (node not inserted yet) → releases lock
- Thread 1 saves to storage with certificate C1
- Thread 2 saves to storage with certificate C2, **overwriting C1** (same digest key)
- Thread 1 inserts into in-memory DAG successfully  
- Thread 2 attempts insertion but fails with "race during insertion" error

The storage schema uses the node's digest as the key, which is computed from node content excluding the certificate: [3](#0-2) 

The digest calculation excludes signatures: [4](#0-3) 

**Attack Scenario**: A Byzantine validator or network-level message duplication causes two versions of a certified node to arrive concurrently at an honest validator. If these have different quorum certificates (different 2f+1 validator subsets), the race condition results in:
- **Persistent storage**: Contains CertifiedNode with certificate C2
- **In-memory DAG**: Contains CertifiedNode with certificate C1
- **State desynchronization**: On validator restart, recovery loads C2 from storage, but consensus decisions were made using C1

This violates the **State Consistency** invariant requiring atomic state transitions and the **Deterministic Execution** invariant requiring all validators to have consistent views.

## Impact Explanation
**Medium Severity** - "State inconsistencies requiring intervention"

Impact includes:
1. **Consensus Divergence**: Different validators experiencing different race timings may persist different certificates for the same node, leading to divergent views after restart
2. **Verification Failures**: The certificate loaded from storage may have different signers than the one used in consensus decisions, potentially causing verification failures in dependent operations
3. **Non-deterministic Recovery**: Validator restart behavior becomes non-deterministic based on which certificate was written last during the race

While this does not immediately break consensus safety (requires < 1/3 Byzantine validators), it creates state inconsistencies that could amplify under network stress or when combined with other timing-dependent bugs. The issue qualifies as Medium severity per the Aptos bug bounty criteria for state inconsistencies requiring manual intervention.

## Likelihood Explanation
**Medium to High Likelihood**

The vulnerability can be triggered by:
1. **Network-level duplication**: Reliable broadcast retries or network layer packet duplication causing the same CertifiedNode message to arrive twice
2. **Byzantine behavior**: A malicious validator creating multiple valid certificates by collecting different 2f+1 quorum subsets and broadcasting both
3. **High concurrency scenarios**: During network stress or high round progression rates, concurrent message processing increases race window exposure

The race window is small but non-zero, and the existence check using a read lock makes concurrent entry into the critical section likely under load. Modern multi-core validator nodes with concurrent RPC handlers make this race realistic.

## Recommendation
Implement atomic check-and-set semantics by holding the write lock throughout the entire validation-storage-insertion sequence:

```rust
pub fn add_node(&self, node: CertifiedNode) -> anyhow::Result<()> {
    let mut dag_guard = self.dag.write();
    dag_guard.validate_new_node(&node)?;
    
    // Save to storage while still holding the lock
    self.storage.save_certified_node(&node)?;
    
    debug!("Added node {}", node.id());
    self.payload_manager.prefetch_payload_data(
        node.payload(),
        *node.author(),
        node.metadata().timestamp(),
    );
    
    dag_guard.add_validated_node(node)?;
    // Lock released here when dag_guard goes out of scope
    Ok(())
}
```

Alternatively, add an explicit existence check after acquiring the write lock but before storage:

```rust
pub fn add_node(&self, node: CertifiedNode) -> anyhow::Result<()> {
    self.dag.write().validate_new_node(&node)?;
    
    // Atomic check-and-set: verify node still doesn't exist before storage
    let mut dag_guard = self.dag.write();
    let index = *dag_guard.author_to_index.get(node.author())
        .ok_or_else(|| anyhow!("unknown author"))?;
    let round_ref = dag_guard.nodes_by_round
        .entry(node.round())
        .or_insert_with(|| vec![None; dag_guard.author_to_index.len()]);
    ensure!(round_ref[index].is_none(), "duplicate node detected after validation");
    
    self.storage.save_certified_node(&node)?;
    
    dag_guard.add_validated_node_without_check(node)
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod race_condition_test {
    use super::*;
    use std::sync::{Arc, Barrier};
    use std::thread;
    
    #[test]
    fn test_concurrent_certified_node_insertion_race() {
        // Setup: Create a DagStore and a valid CertifiedNode
        let epoch_state = Arc::new(create_test_epoch_state());
        let storage = Arc::new(create_mock_storage());
        let payload_manager = Arc::new(create_mock_payload_manager());
        let dag_store = Arc::new(DagStore::new_empty(
            epoch_state.clone(),
            storage.clone(),
            payload_manager,
            1,
            100,
        ));
        
        // Create a certified node with certificate C1
        let node = create_test_node(epoch_state.epoch, 2, author1);
        let cert1 = create_test_certificate(&node, vec![validator1, validator2, validator3]);
        let certified_node1 = CertifiedNode::new(node.clone(), cert1);
        
        // Create same node with different certificate C2 (different quorum subset)
        let cert2 = create_test_certificate(&node, vec![validator1, validator4, validator5]);
        let certified_node2 = CertifiedNode::new(node, cert2);
        
        // Barrier to synchronize thread start
        let barrier = Arc::new(Barrier::new(2));
        let dag_store1 = dag_store.clone();
        let dag_store2 = dag_store.clone();
        let barrier1 = barrier.clone();
        let barrier2 = barrier.clone();
        
        // Thread 1: Process certified_node1
        let handle1 = thread::spawn(move || {
            barrier1.wait();
            dag_store1.add_node(certified_node1)
        });
        
        // Thread 2: Process certified_node2 (same node, different cert)
        let handle2 = thread::spawn(move || {
            barrier2.wait();
            dag_store2.add_node(certified_node2)
        });
        
        let result1 = handle1.join().unwrap();
        let result2 = handle2.join().unwrap();
        
        // One should succeed, one should fail with "race during insertion"
        assert!(result1.is_ok() ^ result2.is_ok());
        
        // Critical: Check if storage and in-memory DAG are desynchronized
        let stored_node = storage.get_certified_nodes().unwrap()
            .into_iter()
            .find(|(digest, _)| *digest == node.digest())
            .map(|(_, node)| node);
        
        let dag_node = dag_store.read().get_node(&node.metadata());
        
        // VULNERABILITY: Storage might have cert2 while DAG has cert1 (or vice versa)
        if let (Some(stored), Some(dag)) = (stored_node, dag_node) {
            // This assertion may FAIL, demonstrating the desynchronization
            assert_eq!(
                stored.signatures(),
                dag.signatures(),
                "Storage and in-memory DAG have different certificates!"
            );
        }
    }
}
```

## Notes
The vulnerability stems from the architectural decision to split the critical section for performance reasons, as evidenced by the comment in the code about pruning operations. However, this optimization introduces a race condition that violates state consistency guarantees. The fix requires either holding locks longer (reducing concurrency) or implementing lock-free atomic operations at the storage layer, both of which have performance trade-offs that should be carefully evaluated.

### Citations

**File:** consensus/src/dag/dag_store.rs (L518-536)
```rust
    pub fn add_node(&self, node: CertifiedNode) -> anyhow::Result<()> {
        self.dag.write().validate_new_node(&node)?;

        // Note on concurrency: it is possible that a prune operation kicks in here and
        // moves the window forward making the `node` stale. Any stale node inserted
        // due to this race will be cleaned up with the next prune operation.

        // mutate after all checks pass
        self.storage.save_certified_node(&node)?;

        debug!("Added node {}", node.id());
        self.payload_manager.prefetch_payload_data(
            node.payload(),
            *node.author(),
            node.metadata().timestamp(),
        );

        self.dag.write().add_validated_node(node)
    }
```

**File:** consensus/src/dag/dag_driver.rs (L399-400)
```rust
        if self.dag.read().exists(certified_node.metadata()) {
            return Ok(CertifiedAck::new(epoch));
```

**File:** consensus/src/consensusdb/schema/dag/mod.rs (L71-86)
```rust
define_schema!(
    CertifiedNodeSchema,
    HashValue,
    CertifiedNode,
    CERTIFIED_NODE_CF_NAME
);

impl KeyCodec<CertifiedNodeSchema> for HashValue {
    fn encode_key(&self) -> Result<Vec<u8>> {
        Ok(self.to_vec())
    }

    fn decode_key(data: &[u8]) -> Result<Self> {
        Ok(HashValue::from_slice(data)?)
    }
}
```

**File:** consensus/src/dag/types.rs (L216-251)
```rust
    /// Calculate the node digest based on all fields in the node
    fn calculate_digest_internal(
        epoch: u64,
        round: Round,
        author: Author,
        timestamp: u64,
        validator_txns: &Vec<ValidatorTransaction>,
        payload: &Payload,
        parents: &Vec<NodeCertificate>,
        extensions: &Extensions,
    ) -> HashValue {
        let node_with_out_digest = NodeWithoutDigest {
            epoch,
            round,
            author,
            timestamp,
            validator_txns,
            payload,
            parents,
            extensions,
        };
        node_with_out_digest.hash()
    }

    fn calculate_digest(&self) -> HashValue {
        Self::calculate_digest_internal(
            self.metadata.epoch,
            self.metadata.round,
            self.metadata.author,
            self.metadata.timestamp,
            &self.validator_txns,
            &self.payload,
            &self.parents,
            &self.extensions,
        )
    }
```
