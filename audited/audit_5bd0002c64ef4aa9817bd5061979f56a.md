# Audit Report

## Title
Mempool Peer Health Check Bypass Through Self-Reported Monitoring Metadata

## Summary
The mempool's intelligent peer prioritization system relies on self-reported health metrics from peers without validation. Malicious peers can manipulate their reported `ledger_timestamp_usecs` and `distance_from_validators` values to bypass health-based prioritization, maintaining high priority for transaction broadcasts despite being unhealthy or stale.

## Finding Description

The mempool's `compare_intelligent()` function prioritizes peers for transaction broadcasts based on health metrics obtained from `PeerMonitoringMetadata`. [1](#0-0) 

The health check in `check_peer_metadata_health()` determines if a peer is healthy by comparing the peer's self-reported `ledger_timestamp_usecs` against the current time. [2](#0-1) 

The critical vulnerability is that `ledger_timestamp_usecs` comes from the peer's own monitoring service response without any validation. The peer monitoring server simply returns whatever value it reads from its local storage. [3](#0-2) 

**Attack Path:**

1. A malicious peer modifies their peer monitoring service to return a fake `NodeInformationResponse` with a recent `ledger_timestamp_usecs` (even if they're days behind in sync)
2. When honest nodes query the malicious peer's monitoring service, they receive and store this fake timestamp in `PeerMonitoringMetadata` [4](#0-3) 
3. The honest node's mempool uses this fake data in `compare_peer_health()`, incorrectly classifying the malicious peer as healthy [5](#0-4) 
4. The malicious peer receives higher priority and gets assigned sender buckets for transaction broadcasts [6](#0-5) 
5. Honest nodes waste bandwidth broadcasting transactions to the malicious peer, which may drop them or fail to propagate them due to being out of sync

Similarly, malicious peers can fake their `distance_from_validators` to appear closer to the validator set than they actually are. [7](#0-6) 

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty criteria for the following reasons:

1. **Protocol Violation**: The peer prioritization mechanism is designed to preferentially broadcast to healthy, well-synced peers. This vulnerability allows malicious peers to completely bypass this security mechanism.

2. **Network Performance Degradation**: If multiple malicious peers exploit this vulnerability, they can collectively degrade transaction propagation efficiency across the network, leading to increased latency and reduced throughput.

3. **Resource Exhaustion**: Honest nodes waste bandwidth, CPU cycles, and memory broadcasting transactions to unhealthy peers that cannot properly handle or forward them.

4. **No Consensus Impact But Significant Service Disruption**: While this doesn't directly break consensus, it can severely impact the network's ability to propagate transactions efficiently, affecting user experience and potentially enabling targeted DoS scenarios.

## Likelihood Explanation

**Likelihood: High**

This vulnerability is highly likely to be exploited because:

1. **Low Barrier to Entry**: Any node operator can modify their local peer monitoring service to return arbitrary values. No special privileges, stake, or validator status is required.

2. **No Detection Mechanism**: There is no cross-validation of peer-reported metrics against consensus state or other authoritative sources. Malicious peers can maintain the deception indefinitely.

3. **Clear Incentive**: Malicious actors have multiple incentives to exploit this:
   - Eclipse attacks: Ensure their nodes remain prioritized while being stale
   - Censorship: Receive transactions and selectively drop or delay them
   - Network disruption: Coordinate multiple fake-healthy nodes to degrade overall performance

4. **Difficult to Attribute**: Since peer health can legitimately fluctuate, it's hard to distinguish between genuinely unhealthy nodes and malicious ones faking health status.

## Recommendation

Implement independent verification of peer health rather than trusting self-reported metrics:

**Option 1 - Consensus-Based Verification:**
Track and verify peer sync progress by cross-referencing with consensus state. When a peer claims a certain `ledger_timestamp_usecs`, verify this against:
- The peer's ability to serve recent blocks
- Their participation in consensus voting (for validators)
- Cross-validation with other trusted peers' observations

**Option 2 - Cryptographic Proofs:**
Require peers to provide cryptographic proofs of their claimed state:
- State commitment signatures from validators
- Merkle proofs showing they possess recent state
- Signed ledger info proving their claimed timestamp

**Option 3 - Behavioral Monitoring:**
Track actual peer behavior independently of self-reported metrics:
- Monitor transaction acceptance/rejection rates
- Track response times and success rates
- Measure actual state freshness through queries
- Implement reputation scoring based on observed behavior

**Example Fix Approach:**

Instead of solely trusting `peer_ledger_timestamp_usecs` from `latest_node_info_response`, implement a verification layer that:

1. Tracks the peer's actual responsiveness and transaction forwarding behavior
2. Periodically queries the peer for recent blocks to verify sync progress
3. Compares the peer's reported metrics against consensus-observed state
4. Uses a weighted score combining self-reported metrics (with low trust) and independently verified metrics (with high trust)

The health check should be modified to incorporate both self-reported and verified metrics, with appropriate weighting that prevents complete reliance on untrusted data.

## Proof of Concept

**Malicious Peer Setup:**

1. Fork the Aptos Core repository
2. Modify `peer-monitoring-service/server/src/lib.rs` in the `get_node_information()` function to always return a recent timestamp:

```rust
// Around line 266, replace:
let ledger_timestamp_usecs = self.storage.get_ledger_timestamp_usecs()?;

// With:
let ledger_timestamp_usecs = self.time_service.now_unix_time().as_micros() as u64; // Always report current time
```

3. Modify distance calculation to always return 0:

```rust
// In get_network_information() around line 241
let distance_from_validators = 0; // Always claim to be in validator set
```

**Attack Demonstration:**

1. Start the modified malicious node and connect it to the network
2. Let the malicious node fall far behind in sync (stop syncing or disconnect from other peers)
3. Reconnect to honest nodes
4. The honest nodes will query the malicious node's monitoring service
5. The malicious node returns fake health metrics showing it's up-to-date
6. Monitor the honest node's mempool logs - it will show the malicious peer with high priority
7. Observe that honest nodes waste bandwidth broadcasting transactions to the stale malicious peer

**Observable Impact:**

- Mempool priority logs will show the malicious peer ranked higher than actually healthy peers
- Network traffic analysis will show broadcasts being sent to the stale malicious peer
- Transaction propagation latency increases as bandwidth is wasted on unhealthy peers
- With multiple coordinated malicious peers, network-wide transaction propagation can be significantly degraded

## Notes

This vulnerability represents a fundamental trust boundary violation where external, untrusted peer data is used for critical prioritization decisions without verification. While the peer monitoring service is useful for gathering information, its self-reported nature makes it unsuitable as the sole source of truth for security-critical decisions like peer prioritization.

The issue is exacerbated in non-validator nodes (VFNs and PFNs) where intelligent peer prioritization is most heavily relied upon, as they don't have direct consensus participation to cross-validate peer claims.

### Citations

**File:** mempool/src/shared_mempool/priority.rs (L74-92)
```rust
    fn compare_intelligent(
        &self,
        peer_a: &(PeerNetworkId, Option<&PeerMonitoringMetadata>),
        peer_b: &(PeerNetworkId, Option<&PeerMonitoringMetadata>),
    ) -> Ordering {
        // Deconstruct the peer tuples
        let (peer_network_id_a, monitoring_metadata_a) = peer_a;
        let (peer_network_id_b, monitoring_metadata_b) = peer_b;

        // First, compare the peers by health (e.g., sync lag)
        let unhealthy_ordering = compare_peer_health(
            &self.mempool_config,
            &self.time_service,
            monitoring_metadata_a,
            monitoring_metadata_b,
        );
        if !unhealthy_ordering.is_eq() {
            return unhealthy_ordering; // Only return if it's not equal
        }
```

**File:** mempool/src/shared_mempool/priority.rs (L562-589)
```rust
fn check_peer_metadata_health(
    mempool_config: &MempoolConfig,
    time_service: &TimeService,
    monitoring_metadata: &Option<&PeerMonitoringMetadata>,
) -> bool {
    monitoring_metadata
        .and_then(|metadata| {
            metadata
                .latest_node_info_response
                .as_ref()
                .map(|node_information_response| {
                    // Get the peer's ledger timestamp and the current timestamp
                    let peer_ledger_timestamp_usecs =
                        node_information_response.ledger_timestamp_usecs;
                    let current_timestamp_usecs = get_timestamp_now_usecs(time_service);

                    // Calculate the max sync lag before the peer is considered unhealthy (in microseconds)
                    let max_sync_lag_secs =
                        mempool_config.max_sync_lag_before_unhealthy_secs as u64;
                    let max_sync_lag_usecs = max_sync_lag_secs * MICROS_PER_SECOND;

                    // Determine if the peer is healthy
                    current_timestamp_usecs.saturating_sub(peer_ledger_timestamp_usecs)
                        < max_sync_lag_usecs
                })
        })
        .unwrap_or(false) // If metadata is missing, consider the peer unhealthy
}
```

**File:** mempool/src/shared_mempool/priority.rs (L593-611)
```rust
fn compare_peer_health(
    mempool_config: &MempoolConfig,
    time_service: &TimeService,
    monitoring_metadata_a: &Option<&PeerMonitoringMetadata>,
    monitoring_metadata_b: &Option<&PeerMonitoringMetadata>,
) -> Ordering {
    // Check the health of the peer monitoring metadata
    let is_healthy_a =
        check_peer_metadata_health(mempool_config, time_service, monitoring_metadata_a);
    let is_healthy_b =
        check_peer_metadata_health(mempool_config, time_service, monitoring_metadata_b);

    // Compare the health statuses
    match (is_healthy_a, is_healthy_b) {
        (true, false) => Ordering::Greater, // A is healthy, B is unhealthy
        (false, true) => Ordering::Less,    // A is unhealthy, B is healthy
        _ => Ordering::Equal,               // Both are healthy or unhealthy
    }
}
```

**File:** peer-monitoring-service/server/src/lib.rs (L259-280)
```rust
    fn get_node_information(&self) -> Result<PeerMonitoringServiceResponse, Error> {
        // Get the node information
        let build_information = aptos_build_info::get_build_information();
        let current_time: Instant = self.time_service.now();
        let uptime = current_time.duration_since(self.start_time);
        let (highest_synced_epoch, highest_synced_version) =
            self.storage.get_highest_synced_epoch_and_version()?;
        let ledger_timestamp_usecs = self.storage.get_ledger_timestamp_usecs()?;
        let lowest_available_version = self.storage.get_lowest_available_version()?;

        // Create and return the response
        let node_information_response = NodeInformationResponse {
            build_information,
            highest_synced_epoch,
            highest_synced_version,
            ledger_timestamp_usecs,
            lowest_available_version,
            uptime,
        };
        Ok(PeerMonitoringServiceResponse::NodeInformation(
            node_information_response,
        ))
```

**File:** peer-monitoring-service/server/src/lib.rs (L296-340)
```rust
/// Returns the distance from the validators using the given base config
/// and the peers and metadata information.
fn get_distance_from_validators(
    base_config: &BaseConfig,
    peers_and_metadata: Arc<PeersAndMetadata>,
) -> u64 {
    // Get the connected peers and metadata
    let connected_peers_and_metadata = match peers_and_metadata.get_connected_peers_and_metadata() {
        Ok(connected_peers_and_metadata) => connected_peers_and_metadata,
        Err(error) => {
            warn!(LogSchema::new(LogEntry::PeerMonitoringServiceError).error(&error.into()));
            return MAX_DISTANCE_FROM_VALIDATORS;
        },
    };

    // If we're a validator and we have active validator peers, we're in the validator set.
    // TODO: figure out if we need to deal with validator set forks here.
    if base_config.role.is_validator() {
        for peer_metadata in connected_peers_and_metadata.values() {
            if peer_metadata.get_connection_metadata().role.is_validator() {
                return 0;
            }
        }
    }

    // Otherwise, go through our peers, find the min, and return a distance relative to the min
    let mut min_peer_distance_from_validators = MAX_DISTANCE_FROM_VALIDATORS;
    for peer_metadata in connected_peers_and_metadata.values() {
        if let Some(ref latest_network_info_response) = peer_metadata
            .get_peer_monitoring_metadata()
            .latest_network_info_response
        {
            min_peer_distance_from_validators = min(
                min_peer_distance_from_validators,
                latest_network_info_response.distance_from_validators,
            );
        }
    }

    // We're one hop away from the peer
    min(
        MAX_DISTANCE_FROM_VALIDATORS,
        min_peer_distance_from_validators + 1,
    )
}
```

**File:** peer-monitoring-service/client/src/peer_states/peer_state.rs (L187-214)
```rust
    pub fn extract_peer_monitoring_metadata(&self) -> Result<PeerMonitoringMetadata, Error> {
        // Create an empty metadata entry for the peer
        let mut peer_monitoring_metadata = PeerMonitoringMetadata::default();

        // Get and store the average latency ping
        let latency_info_state = self.get_latency_info_state()?;
        let average_latency_ping_secs = latency_info_state.get_average_latency_ping_secs();
        peer_monitoring_metadata.average_ping_latency_secs = average_latency_ping_secs;

        let latest_ping_latency_secs = latency_info_state.get_latest_latency_ping_secs();
        peer_monitoring_metadata.latest_ping_latency_secs = latest_ping_latency_secs;

        // Get and store the detailed monitoring metadata
        let internal_client_state = self.get_internal_client_state()?;
        peer_monitoring_metadata.internal_client_state = internal_client_state;

        // Get and store the latest network info response
        let network_info_state = self.get_network_info_state()?;
        let network_info_response = network_info_state.get_latest_network_info_response();
        peer_monitoring_metadata.latest_network_info_response = network_info_response;

        // Get and store the latest node info response
        let node_info_state = self.get_node_info_state()?;
        let node_info_response = node_info_state.get_latest_node_info_response();
        peer_monitoring_metadata.latest_node_info_response = node_info_response;

        Ok(peer_monitoring_metadata)
    }
```

**File:** mempool/src/shared_mempool/network.rs (L502-509)
```rust
                            self.prioritized_peers_state
                                .get_sender_buckets_for_peer(&peer)
                                .ok_or_else(|| {
                                    BroadcastError::PeerNotPrioritized(
                                        peer,
                                        self.prioritized_peers_state.get_peer_priority(&peer),
                                    )
                                })?
```
