# Audit Report

## Title
Resource Exhaustion in QuorumStore V2 Batch Recovery During Node Restart

## Summary
The QuorumStore batch recovery mechanism loads all persisted V2 batches into memory without pagination during node restart, potentially causing out-of-memory (OOM) crashes when large numbers of batches have accumulated. This prevents validator nodes from successfully restarting and participating in consensus.

## Finding Description

The vulnerability exists in the batch recovery flow when a consensus node restarts within the same epoch (not during epoch transitions). The system attempts to repopulate its in-memory cache by loading all persisted batches from the database.

The `get_all_batches_v2()` function creates an iterator over the entire `BatchV2Schema` column family and collects all entries into a HashMap without any streaming or pagination: [1](#0-0) 

This function is invoked during `BatchStore::new()` construction when `is_new_epoch` is false: [2](#0-1) 

The critical issue occurs in `populate_cache_and_gc_expired_batches_v2()` where all batches are loaded into the `db_content` HashMap before any filtering or garbage collection: [3](#0-2) 

The filtering to remove expired batches happens only after all data is already resident in memory (lines 309-326).

**Resource Bounds Analysis:**

Each validator can accumulate batches subject to per-peer quotas: [4](#0-3) 

With the default quota of 300,000 batches per peer and 100 validators in the network:
- Maximum batches: 100 Ã— 300,000 = 30,000,000 batches
- Each `PersistedValue<BatchInfoExt>` contains both metadata and optionally transaction payloads
- Metadata alone: ~350 bytes per entry = ~10.5 GB for 30M batches
- With partial payloads: Memory usage can exceed 50+ GB

**Execution Flow:**

1. During normal operation, validators create batches that persist to the database
2. Expired batches are cleaned up via `update_certified_timestamp()` during runtime
3. When a node restarts mid-epoch, `BatchStore::new()` is called with `is_new_epoch=false`
4. `populate_cache_and_gc_expired_batches_v2()` invokes `get_all_batches_v2()`
5. All batches (including metadata and any stored payloads) load into memory simultaneously
6. If accumulated batches exceed available memory, the node crashes with OOM
7. The node cannot restart, preventing consensus participation

The `PersistedValue<T>` structure contains both batch metadata and optional payload: [5](#0-4) 

The `BatchInfoExt` enum includes extended metadata beyond the basic `BatchInfo`: [6](#0-5) 

## Impact Explanation

**Severity: HIGH to CRITICAL**

This vulnerability qualifies as **HIGH severity** under Aptos bug bounty criteria:
- **Validator Node Crashes**: Directly matches the "Validator node slowdowns" category through OOM crashes during restart
- The affected validator cannot participate in consensus until the issue is resolved
- Requires operational intervention to recover (manual database cleanup or configuration changes)

The severity escalates to **CRITICAL** if:
- Multiple validators restart simultaneously during coordinated network upgrades
- Sufficient validators crash to threaten network liveness
- Could constitute **"Total loss of liveness/network availability"** if enough nodes cannot restart

The vulnerability breaks resource constraint guarantees by loading unbounded amounts of data relative to available system memory, despite the existence of per-peer quotas. The quotas limit what can be stored but not what must be loaded during recovery.

## Likelihood Explanation

**Likelihood: MEDIUM to HIGH**

This issue will naturally manifest during normal network operation:

1. **Natural Accumulation**: Batches accumulate during regular validator operation within designed quota limits (300,000 per peer). No malicious behavior required.

2. **Common Trigger Events**: Node restarts occur frequently for:
   - Planned software upgrades
   - Crash recovery from unrelated issues
   - Routine maintenance operations
   - Infrastructure events (hardware failures, network issues)

3. **Time Dependency**: The longer an epoch runs, the more batches accumulate. Longer epochs increase risk.

4. **Network Scale Dependency**: Larger validator sets (100+ validators) create more peer quotas and higher total batch counts.

5. **Configuration Dependent**: Nodes with limited memory relative to the network's batch accumulation are at higher risk.

The likelihood increases over time within an epoch and scales with network size. In production networks with many validators and moderate epoch durations, this poses a real operational risk.

## Recommendation

Implement streaming or paginated recovery instead of loading all batches into memory at once:

```rust
fn populate_cache_and_gc_expired_batches_v2(
    db: Arc<dyn QuorumStoreStorage>,
    current_epoch: u64,
    last_certified_time: u64,
    expiration_buffer_usecs: u64,
    batch_store: &BatchStore,
) {
    let mut iter = db.iter_batches_v2().expect("failed to create iterator");
    iter.seek_to_first();
    
    let gc_timestamp = last_certified_time.saturating_sub(expiration_buffer_usecs);
    let mut expired_keys = Vec::new();
    let mut batch_count = 0;
    const MAX_BATCH_SIZE: usize = 10_000;
    
    for result in iter {
        let (digest, value) = result.expect("failed to read batch");
        
        if value.expiration() < gc_timestamp {
            expired_keys.push(digest);
        } else {
            batch_store
                .insert_to_cache(&value)
                .expect("Storage limit exceeded upon BatchReader construction");
        }
        
        batch_count += 1;
        
        // Periodically flush expired keys to avoid unbounded memory growth
        if expired_keys.len() >= MAX_BATCH_SIZE {
            let keys_to_delete = std::mem::take(&mut expired_keys);
            tokio::task::spawn_blocking({
                let db = db.clone();
                move || {
                    db.delete_batches_v2(keys_to_delete)
                        .expect("Deletion of expired keys should not fail");
                }
            });
        }
    }
    
    // Final cleanup
    if !expired_keys.is_empty() {
        tokio::task::spawn_blocking(move || {
            db.delete_batches_v2(expired_keys)
                .expect("Deletion of expired keys should not fail");
        });
    }
    
    info!(
        "QS: Batch store bootstrap processed {} batches",
        batch_count
    );
}
```

Additionally, consider:
1. Adding memory usage monitoring during recovery
2. Implementing recovery checkpoints for very large batch sets
3. Adjusting quota limits based on available system memory
4. Adding alerts when batch accumulation approaches dangerous levels

## Proof of Concept

While a full PoC would require a running testnet with multiple validators accumulating batches over time, the vulnerability is evident from code inspection. The execution path is:

1. Node restart triggers `BatchStore::new()` with `is_new_epoch=false`
2. Calls `populate_cache_and_gc_expired_batches_v2()`  
3. Invokes `get_all_batches_v2()` which calls `.collect()` on the entire database iterator
4. With 30M batches accumulated, this loads gigabytes of data into memory before any filtering
5. System OOM kills the process or the Rust allocator panics

The code path is deterministic and requires no special inputs beyond normal batch accumulation during validator operation.

### Citations

**File:** consensus/src/quorum_store/quorum_store_db.rs (L133-138)
```rust
    fn get_all_batches_v2(&self) -> Result<HashMap<HashValue, PersistedValue<BatchInfoExt>>> {
        let mut iter = self.db.iter::<BatchV2Schema>()?;
        iter.seek_to_first();
        iter.map(|res| res.map_err(Into::into))
            .collect::<Result<HashMap<HashValue, PersistedValue<BatchInfoExt>>>>()
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L161-176)
```rust
        } else {
            Self::populate_cache_and_gc_expired_batches_v1(
                db_clone.clone(),
                epoch,
                last_certified_time,
                expiration_buffer_usecs,
                &batch_store,
            );
            Self::populate_cache_and_gc_expired_batches_v2(
                db_clone,
                epoch,
                last_certified_time,
                expiration_buffer_usecs,
                &batch_store,
            );
        }
```

**File:** consensus/src/quorum_store/batch_store.rs (L299-326)
```rust
        let db_content = db
            .get_all_batches_v2()
            .expect("failed to read v1 data from db");
        info!(
            epoch = current_epoch,
            "QS: Read v1 batches from storage. Len: {}, Last Cerified Time: {}",
            db_content.len(),
            last_certified_time
        );

        let mut expired_keys = Vec::new();
        let gc_timestamp = last_certified_time.saturating_sub(expiration_buffer_usecs);
        for (digest, value) in db_content {
            let expiration = value.expiration();
            trace!(
                "QS: Batchreader recovery content exp {:?}, digest {}",
                expiration,
                digest
            );

            if expiration < gc_timestamp {
                expired_keys.push(digest);
            } else {
                batch_store
                    .insert_to_cache(&value)
                    .expect("Storage limit exceeded upon BatchReader construction");
            }
        }
```

**File:** config/src/config/quorum_store_config.rs (L133-135)
```rust
            memory_quota: 120_000_000,
            db_quota: 300_000_000,
            batch_quota: 300_000,
```

**File:** consensus/src/quorum_store/types.rs (L21-39)
```rust
#[derive(Clone, Eq, Deserialize, Serialize, PartialEq, Debug)]
pub struct PersistedValue<T> {
    info: T,
    maybe_payload: Option<Vec<SignedTransaction>>,
}

#[derive(PartialEq, Debug)]
pub(crate) enum StorageMode {
    PersistedOnly,
    MemoryAndPersisted,
}

impl<T: TBatchInfo> PersistedValue<T> {
    pub(crate) fn new(info: T, maybe_payload: Option<Vec<SignedTransaction>>) -> Self {
        Self {
            info,
            maybe_payload,
        }
    }
```

**File:** consensus/consensus-types/src/proof_of_store.rs (L192-273)
```rust
#[derive(
    Clone, Debug, Deserialize, Serialize, CryptoHasher, BCSCryptoHash, PartialEq, Eq, Hash,
)]
pub enum BatchInfoExt {
    V1 {
        info: BatchInfo,
    },
    V2 {
        info: BatchInfo,
        extra: ExtraBatchInfo,
    },
}

impl BatchInfoExt {
    pub fn new_v1(
        author: PeerId,
        batch_id: BatchId,
        epoch: u64,
        expiration: u64,
        digest: HashValue,
        num_txns: u64,
        num_bytes: u64,
        gas_bucket_start: u64,
    ) -> Self {
        Self::V1 {
            info: BatchInfo::new(
                author,
                batch_id,
                epoch,
                expiration,
                digest,
                num_txns,
                num_bytes,
                gas_bucket_start,
            ),
        }
    }

    pub fn new_v2(
        author: PeerId,
        batch_id: BatchId,
        epoch: u64,
        expiration: u64,
        digest: HashValue,
        num_txns: u64,
        num_bytes: u64,
        gas_bucket_start: u64,
        kind: BatchKind,
    ) -> Self {
        Self::V2 {
            info: BatchInfo::new(
                author,
                batch_id,
                epoch,
                expiration,
                digest,
                num_txns,
                num_bytes,
                gas_bucket_start,
            ),
            extra: ExtraBatchInfo { batch_kind: kind },
        }
    }

    pub fn info(&self) -> &BatchInfo {
        match self {
            BatchInfoExt::V1 { info } => info,
            BatchInfoExt::V2 { info, .. } => info,
        }
    }

    pub fn is_v2(&self) -> bool {
        matches!(self, Self::V2 { .. })
    }

    pub fn unpack_info(self) -> BatchInfo {
        match self {
            BatchInfoExt::V1 { info } => info,
            BatchInfoExt::V2 { info, .. } => info,
        }
    }
}
```
