# Audit Report

## Title
DKG Runtime Permanent Liveness Loss Due to Unhandled Initialization Failure on Cold Start

## Summary
The DKG (Distributed Key Generation) runtime contains a critical error handling flaw in `EpochManager::await_reconfig_notification()` that causes permanent liveness loss when consensus key retrieval fails during initial startup. The `.unwrap()` call on line 154 of `epoch_manager.rs` panics when key lookup fails, terminating the DKG runtime task with no recovery mechanism. [1](#0-0) 

## Finding Description
On cold start, the DKG runtime follows this initialization path:

1. `start_dkg_runtime()` spawns the `EpochManager::start()` task
2. `EpochManager::start()` immediately calls `await_reconfig_notification()`
3. This function waits for the first reconfig notification from the blockchain
4. Upon receiving it, calls `start_new_epoch()` to initialize the epoch state
5. **Critical flaw**: The result is unwrapped with `.unwrap()` - if `start_new_epoch()` returns an error, the task panics [2](#0-1) 

The `start_new_epoch()` function attempts to retrieve the validator's consensus private key from persistent storage using `consensus_sk_by_pk()`. This can fail if:
- The validator's public key is in the validator set but the corresponding private key is not in storage
- Key storage was not properly initialized during genesis
- There's a mismatch between the epoch state and stored keys [3](#0-2) 

When key lookup fails, `consensus_sk_by_pk()` returns `Error::ValidatorKeyNotFound`. This error propagates through the `?` operator and causes the `.unwrap()` to panic, immediately terminating the entire DKG EpochManager task. [4](#0-3) 

**Critical Issue**: The panic occurs in the spawned task, which terminates silently without restarting. The DKG runtime has no supervision or recovery mechanism, resulting in permanent liveness loss for that validator's DKG participation. [5](#0-4) 

## Impact Explanation
This vulnerability meets **Critical Severity** criteria per Aptos Bug Bounty rules:

- **"Total loss of liveness/network availability"**: The affected validator permanently loses DKG functionality, preventing participation in randomness generation
- **"Non-recoverable network partition"**: If multiple validators are affected during genesis or epoch transition, the network cannot achieve the quorum needed for DKG completion, blocking randomness-dependent features indefinitely

The impact is severe because:
1. DKG is required for on-chain randomness generation
2. Loss of DKG capability blocks critical network functionality
3. No automatic recovery exists - requires manual node restart
4. Multiple affected validators could halt randomness generation network-wide

## Likelihood Explanation
**Likelihood: Medium to High**

This failure can occur in several realistic scenarios:

1. **Genesis misconfiguration**: If the genesis setup doesn't properly initialize validator keys in storage matching the validator set
2. **Key rotation errors**: During validator set updates, if new keys are added to the validator set but not provisioned in local storage
3. **Storage initialization failures**: If persistent storage initialization is incomplete or corrupted
4. **Deployment errors**: Common in validator onboarding when key setup steps are missed

The existing test suite demonstrates awareness of this failure mode: [6](#0-5) 

The test explicitly checks that `initialize()` fails when a validator's key is not in storage, confirming this is a known failure scenario in consensus components.

## Recommendation

Replace the `.unwrap()` with proper error handling that logs the error and allows graceful degradation:

```rust
async fn await_reconfig_notification(&mut self) {
    let reconfig_notification = self
        .reconfig_events
        .next()
        .await
        .expect("Reconfig sender dropped, unable to start new epoch");
    
    match self.start_new_epoch(reconfig_notification.on_chain_configs).await {
        Ok(()) => {
            info!("[DKG] Successfully started new epoch");
        }
        Err(e) => {
            error!(
                epoch = self.epoch_state.as_ref().map(|s| s.epoch).unwrap_or(0),
                "[DKG] Failed to start new epoch: {}. DKG will not be available for this validator.", 
                e
            );
            // Continue running to handle subsequent reconfig events
            // DKG will be disabled for this validator until the issue is resolved
        }
    }
}
```

Additionally, the main event loop should be modified to continue processing events even if epoch initialization fails:

```rust
pub async fn start(mut self, mut network_receivers: NetworkReceivers) {
    // Initial reconfig handling with error recovery
    self.await_reconfig_notification().await;
    
    loop {
        let handling_result = tokio::select! {
            notification = self.dkg_start_events.select_next_some() => {
                self.on_dkg_start_notification(notification)
            },
            reconfig_notification = self.reconfig_events.select_next_some() => {
                // Wrap in Result to prevent panic
                match self.on_new_epoch(reconfig_notification).await {
                    Ok(()) => Ok(()),
                    Err(e) => {
                        error!("[DKG] Epoch transition failed: {}", e);
                        Ok(()) // Continue despite error
                    }
                }
            },
            // ... rest of select branches
        };

        if let Err(e) = handling_result {
            error!("{}", e);
        }
    }
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_dkg_cold_start_key_not_found_panic() {
    use aptos_crypto::bls12381;
    use aptos_secure_storage::{InMemoryStorage, Storage};
    use aptos_types::{
        validator_signer::ValidatorSigner,
        validator_verifier::ValidatorVerifier,
    };
    
    // Create validator with one key
    let validator_signer = ValidatorSigner::random([0u8; 32]);
    let validator_addr = validator_signer.author();
    let validator_pubkey = validator_signer.public_key();
    
    // Create a different key that will be in storage
    let different_signer = ValidatorSigner::random([1u8; 32]);
    let different_private_key = different_signer.private_key().clone();
    
    // Initialize storage with different key (mimicking misconfiguration)
    let storage = Storage::from(InMemoryStorage::new());
    let safety_rules_config = SafetyRulesConfig::new(storage);
    let key_storage = storage::storage(&safety_rules_config);
    
    // Storage has the different key, not the one in validator set
    key_storage.internal_store()
        .set(CONSENSUS_KEY, different_private_key)
        .unwrap();
    
    // Create epoch state with validator_pubkey (different from stored key)
    let validator_verifier = ValidatorVerifier::new_single(
        validator_addr,
        validator_pubkey,
    );
    let epoch_state = EpochState::new(1, validator_verifier.into());
    
    // Create mock reconfig events
    let (reconfig_tx, reconfig_rx) = aptos_channels::new_test(1);
    let payload = OnChainConfigPayload::new(
        1,
        Arc::new(OnChainConfigMap::new()),
    );
    reconfig_tx.push((), ReconfigNotification { on_chain_configs: payload });
    
    // Create EpochManager
    let mut epoch_manager = EpochManager::new(
        &safety_rules_config,
        validator_addr,
        reconfig_rx,
        // ... other params
    );
    
    // This should panic when it tries to get consensus_sk_by_pk
    // because the stored key doesn't match the validator set key
    let result = std::panic::catch_unwind(std::panic::AssertUnwindSafe(|| {
        epoch_manager.await_reconfig_notification().await
    }));
    
    assert!(result.is_err(), "Expected panic due to key not found");
}
```

## Notes

This vulnerability represents a critical robustness failure in the DKG initialization path. While it requires a configuration or setup error to trigger (rather than being directly exploitable by an external attacker), the consequences are severe: permanent loss of DKG functionality without recovery. The issue is particularly concerning because:

1. The error occurs during the critical cold start phase when operators may not have full monitoring in place
2. The silent task termination makes diagnosis difficult
3. Genesis and validator onboarding are error-prone processes where such misconfigurations can realistically occur
4. The existing consensus layer acknowledges this failure mode (as evidenced by the test case) but DKG doesn't handle it gracefully

The fix is straightforward: replace panic-inducing error handling with graceful degradation that logs errors and allows continued operation.

### Citations

**File:** dkg/src/epoch_manager.rs (L146-155)
```rust
    async fn await_reconfig_notification(&mut self) {
        let reconfig_notification = self
            .reconfig_events
            .next()
            .await
            .expect("Reconfig sender dropped, unable to start new epoch");
        self.start_new_epoch(reconfig_notification.on_chain_configs)
            .await
            .unwrap();
    }
```

**File:** dkg/src/epoch_manager.rs (L238-243)
```rust
            let dealer_sk = self
                .key_storage
                .consensus_sk_by_pk(my_pk.clone())
                .map_err(|e| {
                    anyhow!("dkg new epoch handling failed with consensus sk lookup err: {e}")
                })?;
```

**File:** consensus/safety-rules/src/persistent_safety_storage.rs (L106-132)
```rust
    pub fn consensus_sk_by_pk(
        &self,
        pk: bls12381::PublicKey,
    ) -> Result<bls12381::PrivateKey, Error> {
        let _timer = counters::start_timer("get", CONSENSUS_KEY);
        let pk_hex = hex::encode(pk.to_bytes());
        let explicit_storage_key = format!("{}_{}", CONSENSUS_KEY, pk_hex);
        let explicit_sk = self
            .internal_store
            .get::<bls12381::PrivateKey>(explicit_storage_key.as_str())
            .map(|v| v.value);
        let default_sk = self.default_consensus_sk();
        let key = match (explicit_sk, default_sk) {
            (Ok(sk_0), _) => sk_0,
            (Err(_), Ok(sk_1)) => sk_1,
            (Err(_), Err(_)) => {
                return Err(Error::ValidatorKeyNotFound("not found!".to_string()));
            },
        };
        if key.public_key() != pk {
            return Err(Error::SecureStorageMissingDataError(format!(
                "Incorrect sk saved for {:?} the expected pk",
                pk
            )));
        }
        Ok(key)
    }
```

**File:** consensus/safety-rules/src/error.rs (L41-42)
```rust
    #[error("Validator key not found: {0}")]
    ValidatorKeyNotFound(String),
```

**File:** dkg/src/lib.rs (L26-56)
```rust
pub fn start_dkg_runtime(
    my_addr: AccountAddress,
    safety_rules_config: &SafetyRulesConfig,
    network_client: NetworkClient<DKGMessage>,
    network_service_events: NetworkServiceEvents<DKGMessage>,
    reconfig_events: ReconfigNotificationListener<DbBackedOnChainConfig>,
    dkg_start_events: EventNotificationListener,
    vtxn_pool: VTxnPoolState,
    rb_config: ReliableBroadcastConfig,
    randomness_override_seq_num: u64,
) -> Runtime {
    let runtime = aptos_runtimes::spawn_named_runtime("dkg".into(), Some(4));
    let (self_sender, self_receiver) = aptos_channels::new(1_024, &counters::PENDING_SELF_MESSAGES);
    let dkg_network_client = DKGNetworkClient::new(network_client);

    let dkg_epoch_manager = EpochManager::new(
        safety_rules_config,
        my_addr,
        reconfig_events,
        dkg_start_events,
        self_sender,
        dkg_network_client,
        vtxn_pool,
        rb_config,
        randomness_override_seq_num,
    );
    let (network_task, network_receiver) = NetworkTask::new(network_service_events, self_receiver);
    runtime.spawn(network_task.start());
    runtime.spawn(dkg_epoch_manager.start(network_receiver));
    runtime
}
```

**File:** consensus/safety-rules/src/tests/suite.rs (L642-676)
```rust
fn test_key_not_in_store(safety_rules: &Callback) {
    let (mut safety_rules, signer) = safety_rules();
    let (mut proof, genesis_qc) = test_utils::make_genesis(&signer);
    let round = genesis_qc.certified_block().round();

    safety_rules.initialize(&proof).unwrap();

    let a1 = test_utils::make_proposal_with_qc(round + 1, genesis_qc, &signer);

    // Update to an epoch where the validator fails to retrive the respective key
    // from persistent storage
    let mut next_epoch_state = EpochState::empty();
    next_epoch_state.epoch = 1;
    let rand_signer = ValidatorSigner::random([0xFu8; 32]);
    next_epoch_state.verifier =
        ValidatorVerifier::new_single(signer.author(), rand_signer.public_key()).into();
    let a2 = test_utils::make_proposal_with_parent_and_overrides(
        Payload::empty(false, true),
        round + 2,
        &a1,
        Some(&a1),
        &signer,
        Some(1),
        Some(next_epoch_state),
    );
    proof
        .ledger_info_with_sigs
        .push(a2.block().quorum_cert().ledger_info().clone());

    // Expected failure due to validator key not being found.
    safety_rules.initialize(&proof).unwrap_err();

    let state = safety_rules.consensus_state().unwrap();
    assert!(!state.in_validator_set());
}
```
