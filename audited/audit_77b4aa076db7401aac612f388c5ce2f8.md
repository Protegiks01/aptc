# Audit Report

## Title
Sentinel Index Underflow in Block Partitioner Causes Missing Cross-Shard Dependencies to Global Executor

## Summary
The sentinel value used at line 329 in `take_txn_with_dep()` to bound dependency range queries is incorrectly calculated and fails to include transactions assigned to the global executor sub-block. This causes cross-shard dependency edges to be omitted, leading to non-deterministic execution across validators and consensus safety violations. [1](#0-0) 

## Finding Description

The block partitioner's `take_txn_with_dep()` function builds cross-shard dependency edges between transactions. When constructing dependent edges for a transaction that writes to a storage key, it must find all subsequent "follower" transactions that access the same key.

The code uses a sentinel value as the upper bound for a range query to find these followers: [2](#0-1) 

The sentinel is constructed as `ShardedTxnIndexV2::new(self.num_rounds(), self.num_executor_shards, 0)` with the comment claiming it's "Guaranteed to be greater than any invalid idx."

**However, this is false.** 

Transactions in the final partitioning round can be assigned to a special "global" sub-block when `partition_last_round` is false (the default). These global transactions have indices using special constants: [3](#0-2) [4](#0-3) 

The global sub-block has `round_id = GLOBAL_ROUND_ID = 9` and `shard_id = GLOBAL_SHARD_ID = usize::MAX`.

`ShardedTxnIndexV2` uses tuple ordering `(round_id, shard_id, pre_partitioned_txn_idx)`: [5](#0-4) 

With the default configuration where `max_partitioning_rounds = 4`: [6](#0-5) 

The sentinel becomes `(4, num_executor_shards, 0)` while global transactions have indices like `(9, usize::MAX, txn_idx)`. Since `4 < 9`, the sentinel is **less than** all global transactions, causing the range query to exclude them: [7](#0-6) 

**Impact Chain:**
1. Follower transactions in the global executor are not found by `all_txns_in_sub_block_range()`
2. No dependent edges are created from source transactions to global followers
3. When source transactions commit, `CrossShardCommitSender` doesn't send write sets to the global round because the dependent edges are missing: [8](#0-7) 

4. Global transactions that should have required edges don't receive cross-shard state updates
5. Global transactions execute with stale or missing state values
6. Different validators produce different execution results and state roots
7. **Consensus safety violation** - the fundamental invariant of deterministic execution is broken

## Impact Explanation

This vulnerability directly violates **Critical Invariant #1: Deterministic Execution** - "All validators must produce identical state roots for identical blocks."

When cross-shard dependencies are missing, transactions in the global executor will read stale state instead of waiting for updated values from prior transactions. Since different validators may have different timing or execution orders for when they process these transactions, they will compute different state transitions and arrive at different state roots for the same block.

This qualifies as **Critical Severity** under the Aptos bug bounty program:
- **Consensus/Safety violations** - Different validators will fail to agree on the canonical state
- **Non-recoverable network partition** - Once validators diverge on state roots, the network cannot recover without a hardfork to correct the dependency tracking logic

The vulnerability affects the core execution layer and impacts all blocks that use sharded execution with cross-shard dependencies involving the final round.

## Likelihood Explanation

This vulnerability has **HIGH likelihood** of occurrence:

**Required Conditions (all easily met):**
1. `partition_last_round = false` (default configuration)
2. `num_rounds() â‰¤ 9` (default max is 4, always true)
3. A transaction in an earlier round writes to a storage key
4. A transaction in the final round (assigned to global executor) reads the same key

**Frequency:** This happens naturally whenever:
- Blocks are large enough to require multiple partitioning rounds
- There are storage dependencies spanning from earlier rounds to the final round
- The system uses sharded execution (increasingly common as throughput scales)

The default configuration makes this inevitable rather than requiring special attacker manipulation. Any workload with cross-shard storage dependencies can trigger this naturally.

## Recommendation

The sentinel value must be greater than any possible transaction index, including those in the global sub-block. Since `GLOBAL_SHARD_ID = usize::MAX`, the only way to exceed it is to use a round_id greater than `GLOBAL_ROUND_ID`.

**Fix:**

```rust
let end_follower = match next_writer {
    None => ShardedTxnIndexV2::new(GLOBAL_ROUND_ID + 1, 0, 0), // Guaranteed to exceed all valid indices including global
    Some(idx) => ShardedTxnIndexV2::new(idx.round_id(), idx.shard_id() + 1, 0),
};
```

By using `GLOBAL_ROUND_ID + 1` as the round_id and `0` as the shard_id, the sentinel becomes `(10, 0, 0)` which is guaranteed to be greater than any global transaction `(9, usize::MAX, _)` because `10 > 9` in the tuple comparison.

**Alternative Fix (more explicit):**

```rust
let end_follower = match next_writer {
    None => {
        // Create a sentinel that's guaranteed to be greater than any valid index,
        // including global transactions at (GLOBAL_ROUND_ID, GLOBAL_SHARD_ID)
        ShardedTxnIndexV2::new(usize::MAX, usize::MAX, usize::MAX)
    },
    Some(idx) => ShardedTxnIndexV2::new(idx.round_id(), idx.shard_id() + 1, 0),
};
```

The fix must be accompanied by comprehensive integration tests that verify cross-shard dependencies are correctly created for transactions in the global executor.

## Proof of Concept

```rust
#[cfg(test)]
mod test_global_dependency_bug {
    use super::*;
    use aptos_types::block_executor::partitioner::{GLOBAL_ROUND_ID, GLOBAL_SHARD_ID};
    
    #[test]
    fn test_sentinel_less_than_global_index() {
        // Simulate default config with 4 rounds completed
        let num_rounds = 4;
        let num_executor_shards = 8;
        
        // The buggy sentinel value
        let sentinel = ShardedTxnIndexV2::new(num_rounds, num_executor_shards, 0);
        
        // A transaction in the global sub-block
        let global_txn = ShardedTxnIndexV2::new(GLOBAL_ROUND_ID, GLOBAL_SHARD_ID, 0);
        
        // The bug: sentinel should be > all valid indices, but it's not
        assert!(sentinel < global_txn, 
            "BUG: Sentinel {:?} is less than global transaction {:?}. \
             Range queries using this sentinel will exclude global transactions!",
            sentinel, global_txn);
        
        // Range query [start, sentinel) will NOT include global_txn
        let start = ShardedTxnIndexV2::new(0, 0, 0);
        let in_range = start <= global_txn && global_txn < sentinel;
        assert!(!in_range, "Global transaction incorrectly excluded from range");
    }
    
    #[test]
    fn test_corrected_sentinel() {
        let num_rounds = 4;
        
        // The correct sentinel: use GLOBAL_ROUND_ID + 1
        let correct_sentinel = ShardedTxnIndexV2::new(GLOBAL_ROUND_ID + 1, 0, 0);
        
        // A transaction in the global sub-block  
        let global_txn = ShardedTxnIndexV2::new(GLOBAL_ROUND_ID, GLOBAL_SHARD_ID, 0);
        
        // Now the sentinel correctly exceeds global transactions
        assert!(correct_sentinel > global_txn,
            "Fixed sentinel {:?} is greater than global transaction {:?}",
            correct_sentinel, global_txn);
    }
}
```

**Integration Test Scenario:**
1. Create a block with transactions that span multiple rounds
2. Set `partition_last_round = false` to enable global executor
3. Add transaction T1 in round 0 that writes to state key K
4. Add transaction T2 in the final round that reads state key K  
5. Run partitioner and verify that T1's dependent edges include T2
6. Execute the block and verify T2 receives T1's write via cross-shard messaging
7. Without the fix, T2's dependent edge will be missing and execution will be non-deterministic

## Notes

The vulnerability exists because the sentinel calculation incorrectly assumes `num_rounds()` will always exceed special round IDs like `GLOBAL_ROUND_ID`. The default configuration explicitly violates this assumption. This is a critical oversight in the dependency tracking logic that directly undermines consensus safety.

### Citations

**File:** execution/block-partitioner/src/v2/state.rs (L328-330)
```rust
                let end_follower = match next_writer {
                    None => ShardedTxnIndexV2::new(self.num_rounds(), self.num_executor_shards, 0), // Guaranteed to be greater than any invalid idx...
                    Some(idx) => ShardedTxnIndexV2::new(idx.round_id(), idx.shard_id() + 1, 0),
```

**File:** execution/block-partitioner/src/v2/state.rs (L332-333)
```rust
                for follower_txn_idx in
                    self.all_txns_in_sub_block_range(key_idx, start_of_next_sub_block, end_follower)
```

**File:** types/src/block_executor/partitioner.rs (L20-22)
```rust
pub static MAX_ALLOWED_PARTITIONING_ROUNDS: usize = 8;
pub static GLOBAL_ROUND_ID: usize = MAX_ALLOWED_PARTITIONING_ROUNDS + 1;
pub static GLOBAL_SHARD_ID: usize = usize::MAX;
```

**File:** execution/block-partitioner/src/v2/types.rs (L35-37)
```rust
    pub fn global() -> SubBlockIdx {
        SubBlockIdx::new(GLOBAL_ROUND_ID, GLOBAL_SHARD_ID)
    }
```

**File:** execution/block-partitioner/src/v2/types.rs (L65-69)
```rust
impl Ord for ShardedTxnIndexV2 {
    fn cmp(&self, other: &Self) -> cmp::Ordering {
        (self.sub_block_idx, self.pre_partitioned_txn_idx)
            .cmp(&(other.sub_block_idx, other.pre_partitioned_txn_idx))
    }
```

**File:** execution/block-partitioner/src/v2/config.rs (L58-61)
```rust
            max_partitioning_rounds: 4,
            cross_shard_dep_avoid_threshold: 0.9,
            dashmap_num_shards: 64,
            partition_last_round: false,
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L114-133)
```rust
        for (state_key, write_op) in write_set.expect_write_op_iter() {
            if let Some(dependent_shard_ids) = edges.get(state_key) {
                for (dependent_shard_id, round_id) in dependent_shard_ids.iter() {
                    trace!("Sending remote update for success for shard id {:?} and txn_idx: {:?}, state_key: {:?}, dependent shard id: {:?}", self.shard_id, txn_idx, state_key, dependent_shard_id);
                    let message = RemoteTxnWriteMsg(RemoteTxnWrite::new(
                        state_key.clone(),
                        Some(write_op.clone()),
                    ));
                    if *round_id == GLOBAL_ROUND_ID {
                        self.cross_shard_client.send_global_msg(message);
                    } else {
                        self.cross_shard_client.send_cross_shard_msg(
                            *dependent_shard_id,
                            *round_id,
                            message,
                        );
                    }
                }
            }
        }
```
