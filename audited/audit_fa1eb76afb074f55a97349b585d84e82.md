# Audit Report

## Title
FastSyncStorageWrapper Status Remains Stuck in STARTED State After get_state_snapshot_receiver() Failure

## Summary
The `FastSyncStorageWrapper::get_state_snapshot_receiver()` function sets the fast sync status to `STARTED` before calling the underlying database operation. If this operation fails due to storage I/O errors, root hash mismatches, or other errors, the status remains stuck in `STARTED` state without a valid snapshot receiver, causing a read/write split and preventing proper recovery.

## Finding Description

The vulnerability exists in the `get_state_snapshot_receiver()` method of `FastSyncStorageWrapper`: [1](#0-0) 

The function immediately sets the status to `STARTED` on line 149, then calls the underlying DB's `get_state_snapshot_receiver()` on lines 150-151. If the underlying call fails, the function returns an error but the status remains `STARTED`.

**Failure Scenarios:**

The underlying `JellyfishMerkleRestore::new()` can fail in several ways: [2](#0-1) 

1. **Storage I/O errors** when reading nodes (line 197, 207, 212)
2. **Root hash mismatch** if a previous restore exists with different hash (lines 200-205)
3. **Partial node recovery errors** during restoration of incomplete snapshots (line 212)

**State Machine Violation:**

Once status is stuck in `STARTED`, the wrapper's routing logic creates an inconsistent state: [3](#0-2) 

- **Reads** continue from `temporary_db_with_genesis` (lines 127-132 - only `FINISHED` switches to main DB)
- **Writes** are directed to `db_for_fast_sync` (lines 135-139 - both `STARTED` and `FINISHED` use main DB)

This creates a **read/write split** where the node reads from one database while writing to another.

**No Recovery Mechanism:**

Investigation of the codebase confirms there is no mechanism to reset the status back to `UNKNOWN`: [4](#0-3) 

The `FastSyncStatus` enum only transitions forward (`UNKNOWN` → `STARTED` → `FINISHED`), with no reset path implemented in the wrapper or higher-level error handlers.

**Caller Impact:**

The state sync driver calls this function with `.expect()`, causing a panic on failure: [5](#0-4) 

The async task panics, but the `FastSyncStorageWrapper` status remains corrupted.

## Impact Explanation

This is a **High Severity** vulnerability according to Aptos bug bounty criteria:

1. **Validator node operational issues**: Nodes stuck in `STARTED` state cannot complete fast sync properly and experience state inconsistency
2. **Significant protocol violations**: Violates the state consistency invariant - "State transitions must be atomic and verifiable via Merkle proofs"
3. **Node availability degradation**: Affected nodes require manual intervention or restart to recover, impacting network reliability

While this doesn't directly cause consensus violations or fund loss (not Critical), it creates operational issues that can affect validator node availability and require manual intervention.

## Likelihood Explanation

**Likelihood: Medium-High**

- **Trigger conditions**: Storage I/O errors, disk space exhaustion, database corruption, or root hash mismatches during fast sync initialization
- **Occurrence probability**: These failure scenarios are realistic in production environments where nodes experience hardware issues, disk failures, or interrupted restore attempts
- **Attacker capability**: An attacker with access to a node's storage layer (e.g., through resource exhaustion attacks, disk corruption, or filesystem manipulation) could intentionally trigger this condition
- **Complexity**: Low - straightforward to trigger through storage manipulation or resource exhaustion

## Recommendation

Implement atomic status management with proper rollback on failure:

```rust
fn get_state_snapshot_receiver(
    &self,
    version: Version,
    expected_root_hash: HashValue,
) -> Result<Box<dyn StateSnapshotReceiver<StateKey, StateValue>>> {
    // Try to get the snapshot receiver first
    let receiver = self.get_aptos_db_write_ref()
        .get_state_snapshot_receiver(version, expected_root_hash)?;
    
    // Only set status to STARTED after successful initialization
    *self.fast_sync_status.write() = FastSyncStatus::STARTED;
    
    Ok(receiver)
}
```

**Alternative approach**: Add explicit error recovery:

```rust
fn get_state_snapshot_receiver(
    &self,
    version: Version,
    expected_root_hash: HashValue,
) -> Result<Box<dyn StateSnapshotReceiver<StateKey, StateValue>>> {
    *self.fast_sync_status.write() = FastSyncStatus::STARTED;
    
    match self.get_aptos_db_write_ref()
        .get_state_snapshot_receiver(version, expected_root_hash) {
        Ok(receiver) => Ok(receiver),
        Err(e) => {
            // Revert status on failure
            *self.fast_sync_status.write() = FastSyncStatus::UNKNOWN;
            Err(e)
        }
    }
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use aptos_storage_interface::DbWriter;
    use aptos_crypto::HashValue;
    
    #[test]
    fn test_status_stuck_on_snapshot_receiver_failure() {
        // Setup: Create a FastSyncStorageWrapper with mocked DBs
        // that will fail on get_state_snapshot_receiver()
        let wrapper = create_test_wrapper_with_failing_db();
        
        // Verify initial status is UNKNOWN
        assert_eq!(wrapper.get_fast_sync_status(), FastSyncStatus::UNKNOWN);
        
        // Attempt to get snapshot receiver (this will fail)
        let result = wrapper.get_state_snapshot_receiver(
            100,
            HashValue::zero(),
        );
        
        // Verify the call failed
        assert!(result.is_err());
        
        // BUG: Status is now STARTED despite failure
        assert_eq!(wrapper.get_fast_sync_status(), FastSyncStatus::STARTED);
        
        // Demonstrate the read/write split:
        // - Writes go to db_for_fast_sync (because status is STARTED)
        let write_db = wrapper.get_aptos_db_write_ref();
        // - Reads come from temporary_db_with_genesis (because status is not FINISHED)
        let read_db = wrapper.get_aptos_db_read_ref();
        
        // These are different databases, creating inconsistency
        assert!(std::ptr::eq(write_db, wrapper.db_for_fast_sync.as_ref()));
        assert!(std::ptr::eq(read_db, wrapper.temporary_db_with_genesis.as_ref()));
        
        // Node is now in an inconsistent state with no recovery path
    }
}
```

## Notes

This vulnerability specifically affects nodes performing fast sync bootstrapping. The issue breaks the atomic state transition invariant and can leave nodes in an unrecoverable state without manual intervention. The lack of any status reset mechanism in the codebase (confirmed through comprehensive search) makes this a systemic design flaw rather than a simple oversight.

### Citations

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L23-28)
```rust
#[derive(Clone, Copy, Debug, Eq, PartialEq)]
pub enum FastSyncStatus {
    UNKNOWN,
    STARTED,
    FINISHED,
}
```

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L126-140)
```rust
    pub(crate) fn get_aptos_db_read_ref(&self) -> &AptosDB {
        if self.is_fast_sync_bootstrap_finished() {
            self.db_for_fast_sync.as_ref()
        } else {
            self.temporary_db_with_genesis.as_ref()
        }
    }

    pub(crate) fn get_aptos_db_write_ref(&self) -> &AptosDB {
        if self.is_fast_sync_bootstrap_started() || self.is_fast_sync_bootstrap_finished() {
            self.db_for_fast_sync.as_ref()
        } else {
            self.temporary_db_with_genesis.as_ref()
        }
    }
```

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L144-152)
```rust
    fn get_state_snapshot_receiver(
        &self,
        version: Version,
        expected_root_hash: HashValue,
    ) -> Result<Box<dyn StateSnapshotReceiver<StateKey, StateValue>>> {
        *self.fast_sync_status.write() = FastSyncStatus::STARTED;
        self.get_aptos_db_write_ref()
            .get_state_snapshot_receiver(version, expected_root_hash)
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L190-234)
```rust
        store: Arc<D>,
        version: Version,
        expected_root_hash: HashValue,
        async_commit: bool,
    ) -> Result<Self> {
        let tree_reader = Arc::clone(&store);
        let (finished, partial_nodes, previous_leaf) = if let Some(root_node) =
            tree_reader.get_node_option(&NodeKey::new_empty_path(version), "restore")?
        {
            info!("Previous restore is complete, checking root hash.");
            ensure!(
                root_node.hash() == expected_root_hash,
                "Previous completed restore has root hash {}, expecting {}",
                root_node.hash(),
                expected_root_hash,
            );
            (true, vec![], None)
        } else if let Some((node_key, leaf_node)) = tree_reader.get_rightmost_leaf(version)? {
            // If the system crashed in the middle of the previous restoration attempt, we need
            // to recover the partial nodes to the state right before the crash.
            (
                false,
                Self::recover_partial_nodes(tree_reader.as_ref(), version, node_key)?,
                Some(leaf_node),
            )
        } else {
            (
                false,
                vec![InternalInfo::new_empty(NodeKey::new_empty_path(version))],
                None,
            )
        };

        Ok(Self {
            store,
            version,
            partial_nodes,
            frozen_nodes: HashMap::new(),
            previous_leaf,
            num_keys_received: 0,
            expected_root_hash,
            finished,
            async_commit,
            async_commit_result: None,
        })
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L856-860)
```rust
        // Create the snapshot receiver
        let mut state_snapshot_receiver = storage
            .writer
            .get_state_snapshot_receiver(version, expected_root_hash)
            .expect("Failed to initialize the state snapshot receiver!");
```
