# Audit Report

## Title
Unbounded Transaction Batch Size in MempoolSyncMsg Enables Resource Exhaustion Attack

## Summary
The `MempoolSyncMsg::BroadcastTransactionsRequest` message handler does not validate the number of transactions in incoming broadcast messages, allowing malicious peers to send arbitrarily large transaction batches (limited only by the 64 MiB network message size) and trigger resource exhaustion through excessive CPU consumption, memory allocation, and storage I/O operations.

## Finding Description

The Aptos mempool implements strict limits on **outgoing** transaction broadcasts (`shared_mempool_batch_size` of 300 transactions by default), but fails to enforce equivalent limits on **incoming** broadcasts from network peers. [1](#0-0) 

When a peer sends a `BroadcastTransactionsRequest`, the message is processed in `handle_network_event` without any validation on the `transactions` vector length. The entire batch is passed directly to `process_received_txns`: [2](#0-1) 

This spawns a task that processes **all transactions** in the batch, including:

1. **Storage I/O**: Fetching account sequence numbers for each transaction from disk
2. **VM Validation**: Running CPU-intensive Move VM validation in parallel via `VALIDATION_POOL`
3. **Mempool Operations**: Adding transactions while holding the mempool lock [3](#0-2) 

The bounded executor limits concurrent processing tasks to 4 (default) or 16 (VFNs), but each task processes an unbounded number of transactions: [4](#0-3) 

**Attack Scenario:**
A malicious peer can craft `BroadcastTransactionsRequest` messages containing thousands of small transactions:
- With ~200 bytes per minimal transaction, a 64 MiB message could contain ~320,000 transactions
- Multiple malicious peers could coordinate to send such messages simultaneously
- Even with only 4 concurrent tasks processing, each task handling 10,000+ transactions causes severe resource exhaustion

**Comparison with API Batch Submission:**
The REST API enforces explicit batch size limits to prevent similar attacks: [5](#0-4) [6](#0-5) 

The API limits batch submissions to 10 transactions by default, while mempool sync messages have **no such protection**.

## Impact Explanation

This vulnerability enables **High Severity** attacks per the Aptos bug bounty program:

1. **Validator Node Slowdowns**: Excessive VM validation and storage I/O consumption degrades validator performance, potentially causing consensus participation issues
2. **Memory Exhaustion**: Large transaction vectors held in memory during deserialization and processing
3. **CPU Resource Exhaustion**: Parallel VM validation consumes all available CPU cores
4. **Lock Contention**: Extended mempool lock holding blocks other critical operations

This breaks the **Resource Limits** invariant (#9): "All operations must respect gas, storage, and computational limits."

While not directly causing consensus safety violations, sustained resource exhaustion could:
- Delay block proposals from affected validators
- Cause timeouts in consensus communication
- Force validator operators to manually intervene

## Likelihood Explanation

**Likelihood: High**

- **Low Barrier to Entry**: Any network peer can connect and send messages; no special privileges required
- **No Authentication**: The system accepts sync messages from any connected peer
- **Amplification Factor**: High - single 64 MiB message can trigger processing of hundreds of thousands of transactions
- **Detection Difficulty**: Appears as legitimate network traffic; difficult to distinguish from normal high-load scenarios
- **Coordination Potential**: Multiple attackers can amplify the effect

The attack requires only:
1. Network connectivity to target node
2. Ability to serialize large transaction batches (trivial with standard serialization libraries)
3. No cryptographic material or validator credentials

## Recommendation

Implement strict validation on incoming `MempoolSyncMsg` batch sizes, similar to the API's batch submission limits:

```rust
// In mempool/src/shared_mempool/coordinator.rs, modify handle_network_event:

async fn handle_network_event<NetworkClient, TransactionValidator>(
    bounded_executor: &BoundedExecutor,
    smp: &mut SharedMempool<NetworkClient, TransactionValidator>,
    network_id: NetworkId,
    event: Event<MempoolSyncMsg>,
) where
    NetworkClient: NetworkClientInterface<MempoolSyncMsg> + 'static,
    TransactionValidator: TransactionValidation + 'static,
{
    match event {
        Event::Message(peer_id, msg) => {
            counters::shared_mempool_event_inc("message");
            match msg {
                MempoolSyncMsg::BroadcastTransactionsRequest {
                    message_id,
                    transactions,
                } => {
                    // ADD VALIDATION HERE
                    let max_batch_size = smp.config.shared_mempool_batch_size * 2; // Allow 2x of normal batch
                    if transactions.len() > max_batch_size {
                        warn!(
                            "Peer {:?} sent oversized batch: {} transactions (max: {})",
                            peer_id,
                            transactions.len(),
                            max_batch_size
                        );
                        counters::shared_mempool_event_inc("oversized_batch_rejected");
                        return; // Reject the message
                    }
                    
                    process_received_txns(
                        bounded_executor,
                        smp,
                        network_id,
                        message_id,
                        transactions.into_iter().map(|t| (t, None, None)).collect(),
                        peer_id,
                    )
                    .await;
                },
                MempoolSyncMsg::BroadcastTransactionsRequestWithReadyTime {
                    message_id,
                    transactions,
                } => {
                    // ADD VALIDATION HERE (same check)
                    let max_batch_size = smp.config.shared_mempool_batch_size * 2;
                    if transactions.len() > max_batch_size {
                        warn!(
                            "Peer {:?} sent oversized batch: {} transactions (max: {})",
                            peer_id,
                            transactions.len(),
                            max_batch_size
                        );
                        counters::shared_mempool_event_inc("oversized_batch_rejected");
                        return;
                    }
                    
                    process_received_txns(
                        bounded_executor,
                        smp,
                        network_id,
                        message_id,
                        transactions
                            .into_iter()
                            .map(|t| (t.0, Some(t.1), Some(t.2)))
                            .collect(),
                        peer_id,
                    )
                    .await;
                },
                // ... rest of the match arms
            }
        },
        // ... rest of the match arms
    }
}
```

Additional recommendations:
1. Add a configuration parameter `max_inbound_mempool_batch_size` with a reasonable default (e.g., 500-1000 transactions)
2. Track and monitor oversized batch attempts per peer
3. Consider implementing peer reputation or temporary banning for repeated violations
4. Add rate limiting on the number of messages processed per peer per time window

## Proof of Concept

```rust
// This would be added to mempool/src/tests/integration_tests.rs

#[tokio::test]
async fn test_oversized_batch_resource_exhaustion() {
    use aptos_config::config::NodeConfig;
    use aptos_types::transaction::SignedTransaction;
    use std::time::Instant;
    
    // Setup test node
    let mut node_config = NodeConfig::get_default_vfn_config();
    let (mut env, _) = test_framework::setup_mempool_with_config(&node_config);
    
    // Create a large batch of minimal transactions (e.g., 10,000)
    let large_batch_size = 10_000;
    let mut transactions = vec![];
    for i in 0..large_batch_size {
        let txn = create_test_transaction(i as u64);
        transactions.push(txn);
    }
    
    // Send the oversized batch from a malicious peer
    let malicious_peer = create_test_peer();
    
    let start_time = Instant::now();
    
    // Send BroadcastTransactionsRequest with oversized batch
    env.send_message_from_peer(
        malicious_peer,
        MempoolSyncMsg::BroadcastTransactionsRequest {
            message_id: MempoolMessageId::default(),
            transactions: transactions.clone(),
        }
    );
    
    // Measure processing time and resource consumption
    let processing_time = start_time.elapsed();
    
    // Verify that processing takes excessive time compared to normal batches
    println!("Processing time for {} transactions: {:?}", large_batch_size, processing_time);
    
    // Normal batch of 300 should process quickly
    let normal_batch: Vec<_> = transactions.into_iter().take(300).collect();
    let start_normal = Instant::now();
    env.send_message_from_peer(
        malicious_peer,
        MempoolSyncMsg::BroadcastTransactionsRequest {
            message_id: MempoolMessageId::default(),
            transactions: normal_batch,
        }
    );
    let normal_time = start_normal.elapsed();
    
    println!("Processing time for 300 transactions: {:?}", normal_time);
    
    // Demonstrate resource exhaustion:
    // Large batch takes disproportionately longer than (size_ratio * normal_batch_time)
    let size_ratio = large_batch_size / 300;
    assert!(
        processing_time > normal_time * size_ratio * 2,
        "Resource exhaustion demonstrated: large batch processing time is disproportionate"
    );
}
```

## Notes

This vulnerability represents a **design-level asymmetry**: the system carefully limits outgoing broadcasts to prevent overwhelming peers, but fails to protect itself against receiving oversized batches. This is a common pattern in distributed systems where defensive programming focuses on "good citizen" behavior without adequate protection against malicious actors.

The issue is particularly severe because:
1. The bounded executor provides a false sense of security - it limits concurrent tasks but not work per task
2. VM validation is CPU-intensive and cannot be easily interrupted once started
3. Multiple attack vectors exist (different malicious peers, repeated messages)
4. The gap between API limits (10) and absence of mempool sync limits creates a glaring inconsistency

### Citations

**File:** mempool/src/shared_mempool/coordinator.rs (L293-342)
```rust
async fn process_received_txns<NetworkClient, TransactionValidator>(
    bounded_executor: &BoundedExecutor,
    smp: &mut SharedMempool<NetworkClient, TransactionValidator>,
    network_id: NetworkId,
    message_id: MempoolMessageId,
    transactions: Vec<(
        SignedTransaction,
        Option<u64>,
        Option<BroadcastPeerPriority>,
    )>,
    peer_id: PeerId,
) where
    NetworkClient: NetworkClientInterface<MempoolSyncMsg> + 'static,
    TransactionValidator: TransactionValidation + 'static,
{
    smp.network_interface
        .num_mempool_txns_received_since_peers_updated += transactions.len() as u64;
    let smp_clone = smp.clone();
    let peer = PeerNetworkId::new(network_id, peer_id);
    let ineligible_for_broadcast = (smp.network_interface.is_validator()
        && !smp.broadcast_within_validator_network())
        || smp.network_interface.is_upstream_peer(&peer, None);
    let timeline_state = if ineligible_for_broadcast {
        TimelineState::NonQualified
    } else {
        TimelineState::NotReady
    };
    // This timer measures how long it took for the bounded executor to
    // *schedule* the task.
    let _timer = counters::task_spawn_latency_timer(
        counters::PEER_BROADCAST_EVENT_LABEL,
        counters::SPAWN_LABEL,
    );
    // This timer measures how long it took for the task to go from scheduled
    // to started.
    let task_start_timer = counters::task_spawn_latency_timer(
        counters::PEER_BROADCAST_EVENT_LABEL,
        counters::START_LABEL,
    );
    bounded_executor
        .spawn(tasks::process_transaction_broadcast(
            smp_clone,
            transactions,
            message_id,
            timeline_state,
            peer,
            task_start_timer,
        ))
        .await;
}
```

**File:** mempool/src/shared_mempool/coordinator.rs (L360-372)
```rust
                MempoolSyncMsg::BroadcastTransactionsRequest {
                    message_id,
                    transactions,
                } => {
                    process_received_txns(
                        bounded_executor,
                        smp,
                        network_id,
                        message_id,
                        transactions.into_iter().map(|t| (t, None, None)).collect(),
                        peer_id,
                    )
                    .await;
```

**File:** mempool/src/shared_mempool/tasks.rs (L304-403)
```rust
pub(crate) fn process_incoming_transactions<NetworkClient, TransactionValidator>(
    smp: &SharedMempool<NetworkClient, TransactionValidator>,
    transactions: Vec<(
        SignedTransaction,
        Option<u64>,
        Option<BroadcastPeerPriority>,
    )>,
    timeline_state: TimelineState,
    client_submitted: bool,
) -> Vec<SubmissionStatusBundle>
where
    NetworkClient: NetworkClientInterface<MempoolSyncMsg>,
    TransactionValidator: TransactionValidation,
{
    // Filter out any disallowed transactions
    let mut statuses = vec![];
    let transactions =
        filter_transactions(&smp.transaction_filter_config, transactions, &mut statuses);

    // If there are no transactions left after filtering, return early
    if transactions.is_empty() {
        return statuses;
    }

    let start_storage_read = Instant::now();
    let state_view = smp
        .db
        .latest_state_checkpoint_view()
        .expect("Failed to get latest state checkpoint view.");

    // Track latency: fetching seq number
    let account_seq_numbers = IO_POOL.install(|| {
        transactions
            .par_iter()
            .map(|(t, _, _)| match t.replay_protector() {
                ReplayProtector::Nonce(_) => Ok(None),
                ReplayProtector::SequenceNumber(_) => {
                    get_account_sequence_number(&state_view, t.sender())
                        .map(Some)
                        .inspect_err(|e| {
                            error!(LogSchema::new(LogEntry::DBError).error(e));
                            counters::DB_ERROR.inc();
                        })
                },
            })
            .collect::<Vec<_>>()
    });

    // Track latency for storage read fetching sequence number
    let storage_read_latency = start_storage_read.elapsed();
    counters::PROCESS_TXN_BREAKDOWN_LATENCY
        .with_label_values(&[counters::FETCH_SEQ_NUM_LABEL])
        .observe(storage_read_latency.as_secs_f64() / transactions.len() as f64);

    let transactions: Vec<_> = transactions
        .into_iter()
        .enumerate()
        .filter_map(|(idx, (t, ready_time_at_sender, priority))| {
            if let Ok(account_sequence_num) = account_seq_numbers[idx] {
                match account_sequence_num {
                    Some(sequence_num) => {
                        if t.sequence_number() >= sequence_num {
                            return Some((t, Some(sequence_num), ready_time_at_sender, priority));
                        } else {
                            statuses.push((
                                t,
                                (
                                    MempoolStatus::new(MempoolStatusCode::VmError),
                                    Some(DiscardedVMStatus::SEQUENCE_NUMBER_TOO_OLD),
                                ),
                            ));
                        }
                    },
                    None => {
                        return Some((t, None, ready_time_at_sender, priority));
                    },
                }
            } else {
                // Failed to get account's onchain sequence number
                statuses.push((
                    t,
                    (
                        MempoolStatus::new(MempoolStatusCode::VmError),
                        Some(DiscardedVMStatus::RESOURCE_DOES_NOT_EXIST),
                    ),
                ));
            }
            None
        })
        .collect();

    validate_and_add_transactions(
        transactions,
        smp,
        timeline_state,
        &mut statuses,
        client_submitted,
    );
    notify_subscribers(SharedMempoolNotification::NewTransactions, &smp.subscribers);
    statuses
```

**File:** config/src/config/mempool_config.rs (L116-116)
```rust
            shared_mempool_max_concurrent_inbound_syncs: 4,
```

**File:** api/src/transactions.rs (L550-560)
```rust
        if self.context.max_submit_transaction_batch_size() < signed_transactions_batch.len() {
            return Err(SubmitTransactionError::bad_request_with_code(
                format!(
                    "Submitted too many transactions: {}, while limit is {}",
                    signed_transactions_batch.len(),
                    self.context.max_submit_transaction_batch_size(),
                ),
                AptosErrorCode::InvalidInput,
                &ledger_info,
            ));
        }
```

**File:** config/src/config/api_config.rs (L98-98)
```rust
pub const DEFAULT_MAX_SUBMIT_TRANSACTION_BATCH_SIZE: usize = 10;
```
