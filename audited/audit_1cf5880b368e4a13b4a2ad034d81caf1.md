# Audit Report

## Title
Consensus Observer Liveness Failure Due to Pending Block Garbage Collection

## Summary
The consensus observer's pending block store can garbage collect blocks before their payloads arrive, causing permanent loss of required blocks and triggering liveness failure. When pending blocks accumulate to the maximum limit (150 by default), the oldest pending blocks are removed to make room for new ones. If a garbage-collected block's payload arrives later, the block cannot be recovered, breaking the sequential block processing requirement and preventing consensus progress until fallback mode is triggered after 10+ seconds.

## Finding Description
The consensus observer architecture separates block ordering information (`OrderedBlock` messages) from transaction payloads (`BlockPayload` messages). When an `OrderedBlock` arrives without its corresponding payload, it's stored in the `PendingBlockStore` to await the payload's arrival. [1](#0-0) 

The pending block store enforces a maximum capacity configured by `max_num_pending_blocks` (default 150): [2](#0-1) 

When this limit is reached, garbage collection automatically removes the oldest pending blocks: [3](#0-2) 

The vulnerability occurs in this sequence:

1. **Block Accumulation**: Network delays cause 150+ `OrderedBlock` messages to arrive without their payloads, filling the pending store to capacity.

2. **Garbage Collection**: When new ordered blocks arrive, the oldest pending blocks (e.g., round 101) are removed to make room.

3. **Late Payload Arrival**: The payload for the garbage-collected block (round 101) finally arrives and is stored in the payload store.

4. **Recovery Attempt Fails**: The system attempts to process the now-ready block: [4](#0-3) 

However, the pending block was already garbage collected, so `remove_ready_pending_block` returns `None`.

5. **Sequential Processing Requirement**: The consensus observer requires blocks to form a sequential parent-child chain: [5](#0-4) 

Without block 101, subsequent blocks (102, 103, etc.) cannot be processed even if their payloads exist, because they don't extend from the last processed block (100).

6. **Liveness Failure**: The node cannot make consensus progress. After 10 seconds without version increases, the fallback manager detects the stall: [6](#0-5) 

7. **Fallback Recovery**: The system enters fallback mode, clearing all state and using state sync to recover: [7](#0-6) 

## Impact Explanation
This vulnerability meets **High severity** criteria per the Aptos bug bounty program for the following reasons:

1. **Validator Fullnode Impact**: Consensus observers are enabled by default on Validator Fullnodes (VFNs): [8](#0-7) 

VFNs are critical infrastructure that serve consensus data to downstream clients and public fullnodes.

2. **Temporary Liveness Failure**: Affected nodes experience 10+ seconds of inability to process consensus updates, qualifying as "validator node slowdowns" under High severity.

3. **Service Degradation**: During the liveness failure, the VFN cannot serve clients with current consensus state, degrading the reliability of dependent services.

4. **Repeated Occurrence**: In high block rate environments (testnet, mainnet during peak usage), the condition can recur repeatedly, causing persistent degradation.

5. **Fallback Overhead**: Recovery requires falling back to state sync, which is slower and more resource-intensive than direct consensus observation.

## Likelihood Explanation
This vulnerability has **Medium-to-High** likelihood of occurrence:

1. **Natural Network Conditions**: Message reordering and delays are common in distributed systems. The issue doesn't require malicious actorsâ€”normal network variability can trigger it.

2. **High Block Rate Environments**: Test networks use increased pending block limits (300) specifically to accommodate higher block rates: [9](#0-8) 

This indicates the issue is anticipated during high throughput periods.

3. **No Recovery Mechanism**: Once a pending block is garbage collected, there's no mechanism to re-request the `OrderedBlock` message. The block is permanently lost from the observer's perspective.

4. **Cascading Effect**: A single garbage-collected block breaks the entire chain of subsequent blocks, amplifying the impact.

## Recommendation
Implement one or more of the following mitigations:

**Option 1: Block Re-request Mechanism**
When a payload arrives for a missing pending block, send a request to the publisher to re-send the `OrderedBlock` message:

```rust
// In remove_ready_pending_block
pub fn remove_ready_block(
    &mut self,
    received_payload_epoch: u64,
    received_payload_round: Round,
    block_payload_store: &mut BlockPayloadStore,
) -> Option<Arc<PendingBlockWithMetadata>> {
    // ... existing logic ...
    
    // NEW: Check if we received a payload without a pending block
    if ready_block.is_none() {
        // Check if this payload was for a potentially garbage-collected block
        if self.blocks_without_payloads.is_empty() || 
           self.blocks_without_payloads.first_key_value()
               .map(|(epoch_round, _)| *epoch_round > (received_payload_epoch, received_payload_round))
               .unwrap_or(false)
        {
            // Signal that we need to re-request this block
            return Some(RequestMissingBlock(received_payload_epoch, received_payload_round));
        }
    }
    
    ready_block
}
```

**Option 2: Smarter Garbage Collection**
Only garbage collect blocks that have both expired time-wise AND have later blocks that can make progress:

```rust
fn garbage_collect_pending_blocks(&mut self) {
    let num_blocks_to_remove = num_pending_blocks.saturating_sub(max_pending_blocks);
    
    // NEW: Only remove blocks older than a time threshold
    let now = Instant::now();
    let max_age = Duration::from_secs(60); // 1 minute threshold
    
    for _ in 0..num_blocks_to_remove {
        if let Some((oldest_epoch_round, pending_block)) = self.blocks_without_payloads.first_key_value() {
            // Check if block is old enough to garbage collect
            if now.duration_since(pending_block.block_receipt_time) < max_age {
                break; // Don't remove recent blocks
            }
            
            // Remove the block
            self.blocks_without_payloads.pop_first();
            // ...
        }
    }
}
```

**Option 3: Increase Pending Block Limit Dynamically**
Monitor the rate of payload arrivals and dynamically increase the pending block limit when delays are detected, preventing premature garbage collection.

## Proof of Concept

```rust
#[cfg(test)]
mod liveness_failure_test {
    use super::*;
    
    #[tokio::test]
    async fn test_garbage_collection_causes_liveness_failure() {
        // Setup: Create consensus observer with default config (150 pending blocks)
        let config = ConsensusObserverConfig::default();
        let mut observer_block_data = ObserverBlockData::new_with_root(
            config.clone(),
            create_test_ledger_info(0, 100), // Starting at round 100
        );
        
        // Step 1: Send 150 OrderedBlock messages for rounds 101-250 WITHOUT payloads
        for round in 101..=250 {
            let ordered_block = create_test_ordered_block(0, round);
            let pending_block = PendingBlockWithMetadata::new_with_arc(
                PeerNetworkId::random(),
                Instant::now(),
                ObservedOrderedBlock::new(ordered_block),
            );
            observer_block_data.insert_pending_block(pending_block);
        }
        
        // Verify: 150 pending blocks exist
        assert_eq!(observer_block_data.pending_block_store.len(), 150);
        
        // Step 2: Send one more OrderedBlock for round 251
        // This triggers garbage collection of round 101
        let ordered_block = create_test_ordered_block(0, 251);
        let pending_block = PendingBlockWithMetadata::new_with_arc(
            PeerNetworkId::random(),
            Instant::now(),
            ObservedOrderedBlock::new(ordered_block),
        );
        observer_block_data.insert_pending_block(pending_block);
        
        // Verify: Round 101 was garbage collected
        assert!(!observer_block_data.existing_pending_block(
            &create_test_ordered_block(0, 101)
        ));
        
        // Step 3: Payload for round 101 arrives (too late!)
        let payload_101 = create_test_payload(0, 101);
        observer_block_data.insert_block_payload(payload_101, true);
        
        // Step 4: Attempt to process round 101
        let ready_block = observer_block_data.remove_ready_pending_block(0, 101);
        
        // Verify: No block is returned because it was garbage collected
        assert!(ready_block.is_none());
        
        // Step 5: Try to process round 102 (payload exists, block exists)
        let payload_102 = create_test_payload(0, 102);
        observer_block_data.insert_block_payload(payload_102, true);
        let ready_block = observer_block_data.remove_ready_pending_block(0, 102);
        
        // Even though block 102 has both ordered block and payload,
        // it cannot be processed because:
        // - Last ordered block is still round 100
        // - Block 102's parent is block 101 (which is missing)
        // - The parent_id check will fail in process_ordered_block
        
        // This creates a permanent gap in the block chain until fallback triggers
        println!("Liveness failure demonstrated: gap at round 101 prevents progress");
    }
}
```

## Notes

The vulnerability specifically affects Validator Fullnodes (VFNs) which run the consensus observer component. Validators themselves are not affected as they only run the consensus publisher, not the observer. However, VFNs are critical infrastructure operated by validator operators and serve as the primary source of consensus data for downstream public fullnodes and clients.

The automatic fallback mechanism after 10 seconds prevents this from being a Critical severity issue (permanent liveness failure), but the repeated degradation and forced reliance on state sync represents a significant operational impact qualifying as High severity under the "validator node slowdowns" and "significant protocol violations" categories.

### Citations

**File:** consensus/src/consensus_observer/observer/block_data.rs (L40-52)
```rust
pub struct ObserverBlockData {
    // The block payload store (containing the block transaction payloads)
    block_payload_store: BlockPayloadStore,

    // The ordered block store (containing ordered blocks that are ready for execution)
    ordered_block_store: OrderedBlockStore,

    // The pending block store (containing pending blocks that are without payloads)
    pending_block_store: PendingBlockStore,

    // The latest ledger info
    root: LedgerInfoWithSignatures,
}
```

**File:** config/src/config/consensus_observer_config.rs (L16-17)
```rust
// Maximum number of pending blocks for test networks (e.g., devnet)
const MAX_NUM_PENDING_BLOCKS_FOR_TEST_NETWORKS: u64 = 300;
```

**File:** config/src/config/consensus_observer_config.rs (L72-72)
```rust
            max_num_pending_blocks: 150, // 150 blocks (sufficient for existing production networks)
```

**File:** config/src/config/consensus_observer_config.rs (L119-128)
```rust
            NodeType::ValidatorFullnode => {
                if ENABLE_ON_VALIDATOR_FULLNODES
                    && !observer_manually_set
                    && !publisher_manually_set
                {
                    // Enable both the observer and the publisher for VFNs
                    consensus_observer_config.observer_enabled = true;
                    consensus_observer_config.publisher_enabled = true;
                    modified_config = true;
                }
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L156-195)
```rust
    /// Garbage collects the pending blocks store by removing
    /// the oldest blocks if the store is too large.
    fn garbage_collect_pending_blocks(&mut self) {
        // Verify that both stores have the same number of entries.
        // If not, log an error as this should never happen.
        let num_pending_blocks = self.blocks_without_payloads.len() as u64;
        let num_pending_blocks_by_hash = self.blocks_without_payloads_by_hash.len() as u64;
        if num_pending_blocks != num_pending_blocks_by_hash {
            error!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "The pending block stores have different numbers of entries: {} and {} (by hash)",
                    num_pending_blocks, num_pending_blocks_by_hash
                ))
            );
        }

        // Calculate the number of blocks to remove
        let max_pending_blocks = self.consensus_observer_config.max_num_pending_blocks;
        let num_blocks_to_remove = num_pending_blocks.saturating_sub(max_pending_blocks);

        // Remove the oldest blocks if the store is too large
        for _ in 0..num_blocks_to_remove {
            if let Some((oldest_epoch_round, pending_block)) =
                self.blocks_without_payloads.pop_first()
            {
                // Log a warning message for the removed block
                warn!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "The pending block store is too large: {:?} blocks. Removing the block for the oldest epoch and round: {:?}",
                        num_pending_blocks, oldest_epoch_round
                    ))
                );

                // Remove the block from the hash store
                let first_block = pending_block.ordered_block().first_block();
                self.blocks_without_payloads_by_hash
                    .remove(&first_block.id());
            }
        }
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L236-246)
```rust
    /// Enters fallback mode for consensus observer by invoking state sync
    async fn enter_fallback_mode(&mut self) {
        // Terminate all active subscriptions (to ensure we don't process any more messages)
        self.subscription_manager.terminate_all_subscriptions();

        // Clear all the pending block state
        self.clear_pending_block_state().await;

        // Start syncing for the fallback
        self.state_sync_manager.sync_for_fallback();
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L341-353)
```rust
    async fn order_ready_pending_block(&mut self, block_epoch: u64, block_round: Round) {
        // Remove any ready pending block
        let pending_block_with_metadata = self
            .observer_block_data
            .lock()
            .remove_ready_pending_block(block_epoch, block_round);

        // Process the ready ordered block (if it exists)
        if let Some(pending_block_with_metadata) = pending_block_with_metadata {
            self.process_ordered_block(pending_block_with_metadata)
                .await;
        }
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L773-800)
```rust
        // The block was verified correctly. If the block is a child of our
        // last block, we can insert it into the ordered block store.
        let last_ordered_block = self.observer_block_data.lock().get_last_ordered_block();
        if last_ordered_block.id() == ordered_block.first_block().parent_id() {
            // Update the latency metrics for ordered block processing
            update_message_processing_latency_metrics(
                message_received_time,
                &peer_network_id,
                metrics::ORDERED_BLOCK_LABEL,
            );

            // Insert the ordered block into the pending blocks
            self.observer_block_data
                .lock()
                .insert_ordered_block(observed_ordered_block.clone());

            // If state sync is not syncing to a commit, finalize the ordered blocks
            if !self.state_sync_manager.is_syncing_to_commit() {
                self.finalize_ordered_block(ordered_block).await;
            }
        } else {
            warn!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Parent block for ordered block is missing! Ignoring: {:?}",
                    ordered_block.proof_block_info()
                ))
            );
        }
```

**File:** consensus/src/consensus_observer/observer/fallback_manager.rs (L89-117)
```rust
    fn verify_increasing_sync_versions(
        &mut self,
        latest_ledger_info_version: Version,
        time_now: Instant,
    ) -> Result<(), Error> {
        // Verify that the synced version is increasing appropriately
        let (highest_synced_version, highest_version_timestamp) =
            self.highest_synced_version_and_time;
        if latest_ledger_info_version <= highest_synced_version {
            // The synced version hasn't increased. Check if we should enter fallback mode.
            let duration_since_highest_seen = time_now.duration_since(highest_version_timestamp);
            let fallback_threshold = Duration::from_millis(
                self.consensus_observer_config
                    .observer_fallback_progress_threshold_ms,
            );
            if duration_since_highest_seen > fallback_threshold {
                Err(Error::ObserverProgressStopped(format!(
                    "Consensus observer is not making progress! Highest synced version: {}, elapsed: {:?}",
                    highest_synced_version, duration_since_highest_seen
                )))
            } else {
                Ok(()) // We haven't passed the fallback threshold yet
            }
        } else {
            // The synced version has increased. Update the highest synced version and time.
            self.highest_synced_version_and_time = (latest_ledger_info_version, time_now);
            Ok(())
        }
    }
```
