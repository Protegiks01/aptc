# Audit Report

## Title
Timeout Certificate Loss During Cross-Epoch Recovery with Execution Pool Window

## Summary
The timeout certificate (TC) filtering logic in `RecoveryData::new()` can incorrectly discard valid TCs from the current epoch when the window root block is selected from a previous epoch, causing potential liveness degradation during recovery after epoch transitions.

## Finding Description

In `RecoveryData::new()`, the epoch used for filtering timeout certificates and last votes is derived from either `commit_root_block` or `window_root_block`: [1](#0-0) 

The timeout certificate is then filtered based on this epoch: [2](#0-1) 

**Vulnerability Scenario:**

When the execution pool is enabled with a window size, during an epoch transition, the following state can occur:

1. Storage ledger ends epoch N-1 (with `ends_epoch = true`)
2. A genesis block for epoch N is created as `commit_root_block`
3. Consensus proceeds in epoch N, generating and persisting a TC from epoch N to ConsensusDB
4. Node crashes before committing blocks from epoch N to storage
5. On restart, recovery loads the genesis block as `commit_root_block` (epoch N)
6. The window calculation in `find_root_with_window` walks backwards from the commit block
7. If the window size is configured and blocks from epoch N-1 exist in ConsensusDB, the `window_root_block` could be selected from epoch N-1
8. The `epoch` variable is set to N-1 (from `window_root_block`)
9. The TC from epoch N is discarded because `tc.epoch() (N) != epoch (N-1)` [3](#0-2) 

While the walk-back logic stops at genesis blocks via the `is_genesis_block()` check: [4](#0-3) 

The vulnerability depends on the specific timing and state of blocks in ConsensusDB during the epoch transition window.

## Impact Explanation

**Severity: High**

This issue causes **liveness degradation** during recovery:

1. **Single node recovery**: A node loses its highest TC, falling behind peers who retained their TCs. The node must re-sync TCs from the network, causing delayed consensus participation.

2. **Network-wide restart**: If all nodes restart simultaneously (e.g., coordinated upgrade or network outage), ALL nodes would discard their TCs from the current epoch. Without any node having a valid TC to advance rounds, the network could experience **prolonged liveness delay** until new TCs are generated.

3. **Round advancement**: TCs are critical for 2-chain consensus to advance past rounds where no block was committed. Loss of TCs forces nodes to wait for new timeout votes rather than immediately jumping to higher rounds. [5](#0-4) 

The TC is used in sync_info propagation: [6](#0-5) 

And is critical for liveness in the 2-chain timeout mechanism: [7](#0-6) 

This qualifies as **High Severity** per Aptos bug bounty criteria: "Significant protocol violations" and "Validator node slowdowns".

## Likelihood Explanation

**Likelihood: Medium to Low**

The vulnerability requires specific conditions:
- Execution pool must be enabled with window size configured
- Epoch transition must occur
- Node must crash after TC persistence but before block commitment
- Window calculation must select a root from the previous epoch

However, epoch transitions are regular events in Aptos, and the window execution pool feature is enabled in production, making this scenario plausible in practice.

The code attempts to prevent this via the `is_genesis_block()` check in the window walk-back logic, but the interaction between epoch transitions and window boundaries creates a subtle edge case.

## Recommendation

**Fix: Use commit_root_block epoch for TC filtering when available**

Modify the epoch derivation logic to always use `commit_root_block.epoch()` for filtering TCs and votes, as the commit root represents the latest committed state that should align with persisted consensus artifacts:

```rust
// In RecoveryData::new(), around lines 386-397:
let (root_id, epoch) = match &root.window_root_block {
    None => {
        let commit_root_id = root.commit_root_block.id();
        let epoch = root.commit_root_block.epoch();
        (commit_root_id, epoch)
    },
    Some(window_root_block) => {
        let window_start_id = window_root_block.id();
        // Use commit_root epoch for TC filtering, not window_root epoch
        let epoch = root.commit_root_block.epoch();
        (window_start_id, epoch)
    },
};
```

**Alternative: Add epoch validation to ensure window doesn't cross epoch boundaries**

Add an explicit check after window calculation:

```rust
// After finding window_root_block in find_root_with_window:
ensure!(
    window_start_block.epoch() == commit_block.epoch(),
    "Window root and commit root must be from the same epoch"
);
```

## Proof of Concept

```rust
// Reproduction scenario (conceptual - requires integration test framework):

#[tokio::test]
async fn test_tc_loss_on_cross_epoch_recovery() {
    // 1. Setup: End of epoch N-1
    let storage = setup_storage_with_end_epoch_ledger(epoch_n_minus_1);
    let consensus_db = setup_consensus_db();
    
    // 2. Start epoch N - genesis block created
    let genesis_block = Block::make_genesis_block_from_ledger_info(
        storage.get_latest_ledger_info().ledger_info()
    );
    
    // 3. Generate TC from epoch N
    let tc_epoch_n = create_timeout_certificate(epoch_n, round_5);
    consensus_db.save_highest_2chain_timeout_certificate(tc_epoch_n);
    
    // 4. Save some blocks from epoch N-1 to ConsensusDB
    // (simulating incomplete cleanup)
    consensus_db.save_blocks(blocks_from_epoch_n_minus_1);
    
    // 5. Simulate restart with window enabled
    let recovery_data = storage.start(
        order_vote_enabled = true,
        window_size = Some(100)
    );
    
    // 6. Assert: TC from epoch N should be present but may be lost
    match recovery_data {
        LivenessStorageData::FullRecoveryData(data) => {
            // TC should exist but gets filtered out
            assert!(data.highest_2chain_timeout_certificate().is_none());
            // This demonstrates the liveness issue
        }
        _ => panic!("Expected full recovery data"),
    }
}
```

## Notes

The core issue stems from the semantic difference between `commit_root_block` (the latest committed state, which should be from the current epoch after epoch transition) and `window_root_block` (an execution window optimization that may reference older blocks). Using the window root's epoch for filtering artifacts that should align with the commit state creates a mismatch during epoch transitions.

The genesis block check in the window walk-back logic provides partial mitigation, but the interaction with ConsensusDB state and the timing of epoch transitions creates edge cases where this protection may not be sufficient.

### Citations

**File:** consensus/src/persistent_liveness_storage.rs (L165-187)
```rust
        let window_start_round = calculate_window_start_round(commit_block.round(), window_size);
        let mut id_to_blocks = HashMap::new();
        blocks.iter().for_each(|block| {
            id_to_blocks.insert(block.id(), block);
        });

        let mut current_block = &commit_block;
        while !current_block.is_genesis_block()
            && current_block.quorum_cert().certified_block().round() >= window_start_round
        {
            if let Some(parent_block) = id_to_blocks.get(&current_block.parent_id()) {
                current_block = *parent_block;
            } else {
                bail!("Parent block not found for block {}", current_block.id());
            }
        }
        let window_start_id = current_block.id();

        let window_start_idx = blocks
            .iter()
            .position(|block| block.id() == window_start_id)
            .ok_or_else(|| format_err!("unable to find window root: {}", window_start_id))?;
        let window_start_block = blocks.remove(window_start_idx);
```

**File:** consensus/src/persistent_liveness_storage.rs (L386-397)
```rust
        let (root_id, epoch) = match &root.window_root_block {
            None => {
                let commit_root_id = root.commit_root_block.id();
                let epoch = root.commit_root_block.epoch();
                (commit_root_id, epoch)
            },
            Some(window_root_block) => {
                let window_start_id = window_root_block.id();
                let epoch = window_root_block.epoch();
                (window_start_id, epoch)
            },
        };
```

**File:** consensus/src/persistent_liveness_storage.rs (L414-418)
```rust
            highest_2chain_timeout_certificate: match highest_2chain_timeout_cert {
                Some(tc) if tc.epoch() == epoch => Some(tc),
                _ => None,
            },
        })
```

**File:** consensus/consensus-types/src/block_data.rs (L211-213)
```rust
    pub fn is_genesis_block(&self) -> bool {
        matches!(self.block_type, BlockType::Genesis)
    }
```

**File:** consensus/src/block_storage/block_store.rs (L560-575)
```rust
    pub fn insert_2chain_timeout_certificate(
        &self,
        tc: Arc<TwoChainTimeoutCertificate>,
    ) -> anyhow::Result<()> {
        let cur_tc_round = self
            .highest_2chain_timeout_cert()
            .map_or(0, |tc| tc.round());
        if tc.round() <= cur_tc_round {
            return Ok(());
        }
        self.storage
            .save_highest_2chain_timeout_cert(tc.as_ref())
            .context("Timeout certificate insert failed when persisting to DB")?;
        self.inner.write().replace_2chain_timeout_cert(tc);
        Ok(())
    }
```

**File:** consensus/src/block_storage/block_store.rs (L680-688)
```rust
    fn sync_info(&self) -> SyncInfo {
        SyncInfo::new_decoupled(
            self.highest_quorum_cert().as_ref().clone(),
            self.highest_ordered_cert().as_ref().clone(),
            self.highest_commit_cert().as_ref().clone(),
            self.highest_2chain_timeout_cert()
                .map(|tc| tc.as_ref().clone()),
        )
    }
```

**File:** consensus/consensus-types/src/timeout_2chain.rs (L105-112)
```rust
/// TimeoutCertificate is a proof that 2f+1 participants in epoch i
/// have voted in round r and we can now move to round r+1. AptosBFT v4 requires signature to sign on
/// the TimeoutSigningRepr and carry the TimeoutWithHighestQC with highest quorum cert among 2f+1.
#[derive(Debug, Clone, Serialize, Deserialize, Eq, PartialEq)]
pub struct TwoChainTimeoutCertificate {
    timeout: TwoChainTimeout,
    signatures_with_rounds: AggregateSignatureWithRounds,
}
```
