# Audit Report

## Title
Silent Storage Commit Failure Leads to Consensus Safety Violation and Ledger Inconsistency

## Summary
The persisting phase ignores storage commit errors, causing the buffer manager to incorrectly mark blocks as committed even when persistence fails. This creates ledger inconsistencies between validators and violates consensus safety guarantees.

## Finding Description

The vulnerability exists in the interaction between the persisting phase and buffer manager during block commitment. When blocks are ready to be persisted, the buffer manager calls `advance_head()` which adds blocks to `pending_commit_blocks` and sends them to the persisting phase. [1](#0-0) 

The persisting phase processes these blocks by calling `wait_for_commit_ledger()` on each block: [2](#0-1) 

However, `wait_for_commit_ledger()` explicitly ignores the result of the commit operation: [3](#0-2) 

The actual commit operation in the pipeline can fail for multiple reasons (storage I/O errors, validation failures, disk space issues): [4](#0-3) 

The storage layer's `commit_ledger` can fail during validation or database writes: [5](#0-4) 

Despite potential commit failures, the persisting phase always returns `Ok(round)`: [6](#0-5) 

When the buffer manager receives this `Ok(round)`, it unconditionally cleans up pending state and updates the committed round, even though the actual storage commit may have failed: [7](#0-6) 

**Attack Scenario:**

1. Validator receives aggregated block and sends to persisting phase
2. Storage commit fails (disk full, I/O error, database corruption, etc.)
3. Error propagates through `commit_ledger` future as `TaskError`
4. `wait_for_commit_ledger()` ignores the error with `let _ = ...`
5. Persisting phase returns `Ok(round)` to buffer manager
6. Buffer manager updates `highest_committed_round = round`
7. Buffer manager cleans up `pending_commit_blocks` for that round
8. Buffer manager cleans up `pending_commit_votes` for that round
9. Node believes block is committed, but it's NOT in storage
10. If node crashes and restarts, committed blocks are lost
11. Other validators may have successfully committed â†’ ledger fork

The system even has a fail point for testing commit failures, confirming this is an expected error path: [8](#0-7) 

## Impact Explanation

This is a **Critical Severity** vulnerability per the Aptos bug bounty criteria because:

1. **Consensus Safety Violation**: Violates the fundamental invariant that "AptosBFT must prevent double-spending and chain splits under < 1/3 Byzantine". When one validator fails to persist while others succeed, the network has inconsistent ledger states.

2. **State Inconsistency**: Breaks the "State transitions must be atomic and verifiable" invariant. The in-memory state (highest_committed_round) diverges from the actual persisted state.

3. **Non-Recoverable Network Partition**: If enough validators experience storage failures simultaneously (e.g., due to correlated infrastructure issues), the network could split into incompatible states requiring manual intervention or a hard fork to resolve.

4. **Data Loss**: Validators that experience commit failures will lose blocks they believed were committed if they crash and restart, creating permanent ledger inconsistencies.

The vulnerability affects the core consensus mechanism and can lead to irrecoverable state divergence across the validator set.

## Likelihood Explanation

**High Likelihood** - This vulnerability will trigger in realistic production scenarios:

1. **Storage failures are common**: Disk full conditions, I/O errors, database corruption, network-attached storage issues, and filesystem quota limits are routine operational issues.

2. **No special privileges required**: This is triggered by normal system operation under failure conditions, not by attacker action.

3. **Distributed systems amplify risk**: In a network of hundreds of validators with diverse infrastructure, storage failures will occur regularly. Each failure risks creating ledger inconsistency.

4. **No error visibility**: Since errors are silently swallowed, operators won't know their validators are in an inconsistent state until serious divergence occurs.

5. **Fail point exists for testing**: The codebase includes fail point infrastructure specifically for testing commit failures, indicating the developers recognize this is a realistic failure mode.

## Recommendation

Properly handle commit errors at both the persisting phase and buffer manager levels:

**Fix 1 - Persisting Phase Error Handling:**

Modify `wait_for_commit_ledger()` to return and check the result:

```rust
// In consensus/consensus-types/src/pipelined_block.rs
pub async fn wait_for_commit_ledger(&self) -> TaskResult<()> {
    if let Some(fut) = self.pipeline_futs() {
        fut.commit_ledger_fut.await?;
    }
    Ok(())
}
```

**Fix 2 - Persisting Phase Process:**

Return errors from the persisting phase:

```rust
// In consensus/src/pipeline/persisting_phase.rs
async fn process(&self, req: PersistingRequest) -> PersistingResponse {
    let PersistingRequest {
        blocks,
        commit_ledger_info,
    } = req;

    for b in &blocks {
        if let Some(tx) = b.pipeline_tx().lock().as_mut() {
            tx.commit_proof_tx
                .take()
                .map(|tx| tx.send(commit_ledger_info.clone()));
        }
        // Propagate errors instead of ignoring them
        b.wait_for_commit_ledger().await
            .map_err(|e| anyhow::anyhow!("Commit ledger failed: {}", e))?;
    }

    let response = Ok(blocks.last().expect("Blocks can't be empty").round());
    if commit_ledger_info.ledger_info().ends_epoch() {
        self.commit_msg_tx
            .send_epoch_change(EpochChangeProof::new(vec![commit_ledger_info], false))
            .await;
    }
    response
}
```

**Fix 3 - Buffer Manager Error Handling:**

Handle `Err` results from the persisting phase:

```rust
// In consensus/src/pipeline/buffer_manager.rs (in the start() function)
Some(response) = self.persisting_phase_rx.next() => {
    match response {
        Ok(round) => {
            self.pending_commit_votes = self.pending_commit_votes.split_off(&(round + 1));
            self.highest_committed_round = round;
            self.pending_commit_blocks = self.pending_commit_blocks.split_off(&(round + 1));
        }
        Err(e) => {
            error!("Persistence failed: {:?}. Initiating recovery.", e);
            // Trigger node crash or state sync recovery
            // Do NOT clean up pending state
            // Consider calling self.reset().await or panic to force recovery
            panic!("Critical: Block persistence failed - {}", e);
        }
    }
}
```

**Additional Safety:** Consider adding explicit logging in `commit_ledger` to track all commit attempts and their results for debugging and monitoring.

## Proof of Concept

This vulnerability can be demonstrated using the existing fail point infrastructure:

```rust
#[tokio::test]
async fn test_commit_failure_causes_inconsistency() {
    // Enable the fail point to inject commit errors
    fail::cfg("executor::commit_blocks", "return").unwrap();
    
    // Set up consensus environment with buffer manager
    // ... (setup code) ...
    
    // 1. Create and order a block
    let block = create_test_block(round);
    buffer_manager.process_ordered_blocks(ordered_blocks).await;
    
    // 2. Execute the block successfully
    // ... (execution phase) ...
    
    // 3. Sign the block and aggregate votes
    // ... (signing and aggregation) ...
    
    // 4. Advance head to trigger persisting
    buffer_manager.advance_head(block_id).await;
    
    // 5. The persisting phase will fail due to injected error
    // but buffer_manager will still receive Ok(round)
    
    // 6. Verify the inconsistency:
    // - highest_committed_round is updated to the failed round
    assert_eq!(buffer_manager.highest_committed_round, round);
    
    // - pending_commit_blocks is cleaned up
    assert!(!buffer_manager.pending_commit_blocks.contains_key(&round));
    
    // - BUT storage does NOT contain the block
    let storage_version = db.get_latest_version().unwrap();
    assert!(storage_version < expected_version); // Storage is behind!
    
    // This proves the node believes it committed a block that isn't in storage
}
```

The fail point can also be triggered manually during integration testing or chaos engineering exercises to observe the vulnerability in a controlled environment.

**Notes**

This vulnerability is particularly insidious because:

1. It only manifests during storage failures, which may be rare in testing but common in production
2. The error is completely silent - no logs, no alerts, no visibility
3. Validators will continue operating normally while having divergent state
4. The issue compounds over time as more failed commits accumulate
5. Recovery requires careful state synchronization or potentially a node restart with state sync

The root cause is the unsafe pattern of ignoring `Result` types in critical paths. The `let _ = fut.await` pattern should never be used for operations that can fail in consensus-critical code.

### Citations

**File:** consensus/src/pipeline/buffer_manager.rs (L519-522)
```rust
                for block in &blocks_to_persist {
                    self.pending_commit_blocks
                        .insert(block.round(), block.clone());
                }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L968-973)
```rust
                Some(Ok(round)) = self.persisting_phase_rx.next() => {
                    // see where `need_backpressure()` is called.
                    self.pending_commit_votes = self.pending_commit_votes.split_off(&(round + 1));
                    self.highest_committed_round = round;
                    self.pending_commit_blocks = self.pending_commit_blocks.split_off(&(round + 1));
                },
```

**File:** consensus/src/pipeline/persisting_phase.rs (L65-72)
```rust
        for b in &blocks {
            if let Some(tx) = b.pipeline_tx().lock().as_mut() {
                tx.commit_proof_tx
                    .take()
                    .map(|tx| tx.send(commit_ledger_info.clone()));
            }
            b.wait_for_commit_ledger().await;
        }
```

**File:** consensus/src/pipeline/persisting_phase.rs (L74-74)
```rust
        let response = Ok(blocks.last().expect("Blocks can't be empty").round());
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L562-568)
```rust
    pub async fn wait_for_commit_ledger(&self) {
        // may be aborted (e.g. by reset)
        if let Some(fut) = self.pipeline_futs() {
            // this may be cancelled
            let _ = fut.commit_ledger_fut.await;
        }
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L1098-1105)
```rust
        tokio::task::spawn_blocking(move || {
            executor
                .commit_ledger(ledger_info_with_sigs_clone)
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
        Ok(Some(ledger_info_with_sigs))
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L95-107)
```rust
            let old_committed_ver = self.get_and_check_commit_range(version)?;

            let mut ledger_batch = SchemaBatch::new();
            // Write down LedgerInfo if provided.
            if let Some(li) = ledger_info_with_sigs {
                self.check_and_put_ledger_info(version, li, &mut ledger_batch)?;
            }
            // Write down commit progress
            ledger_batch.put::<DbMetadataSchema>(
                &DbMetadataKey::OverallCommitProgress,
                &DbMetadataValue::Version(version),
            )?;
            self.ledger_db.metadata_db().write_schemas(ledger_batch)?;
```

**File:** execution/executor/src/block_executor/mod.rs (L383-385)
```rust
        fail_point!("executor::commit_blocks", |_| {
            Err(anyhow::anyhow!("Injected error in commit_blocks.").into())
        });
```
