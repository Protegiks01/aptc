# Audit Report

## Title
State Sync Request Cleanup Failure Causes Consensus Deadlock and Memory Leak

## Summary
When `handle_satisfied_sync_request()` fails to send a response to consensus (e.g., due to channel closure or timeout), the sync request state is prematurely removed but critical cleanup operations (`finish_chunk_executor()` and `reset_active_stream()`) never execute. This leaves consensus waiting indefinitely for a response, causes memory leaks in the ChunkExecutor, and leaves the storage synchronizer in an inconsistent state, blocking future sync operations.

## Finding Description

The vulnerability exists in the `check_sync_request_progress()` function's error handling logic. The function performs state modifications in a non-atomic manner: [1](#0-0) 

When this code calls `handle_satisfied_sync_request()`, the following sequence occurs: [2](#0-1) 

The critical flaw is at line 328 where `sync_request_lock.take()` **immediately removes the sync request from state** before attempting to send responses to consensus. If the response operations fail (lines 333-359), the function returns an error, but the sync request has already been removed.

The callback send can fail when: [3](#0-2) [4](#0-3) 

The oneshot channel send fails if consensus has dropped the receiver (e.g., due to timeout). When this happens:

1. The sync request is removed from state (line 328)
2. The response fails with `Error::CallbackSendFailed`
3. The error propagates back to `check_sync_request_progress()` via the `?` operator at line 599
4. Execution returns early, **never reaching the cleanup code at lines 604-605**
5. The error is logged in `drive_progress()` but execution continues [5](#0-4) 

The cleanup code that never executes is critical:
- `finish_chunk_executor()` releases Sparse Merkle Tree memory structures
- `reset_active_stream()` terminates the active data stream [6](#0-5) [7](#0-6) 

**Attack Scenario:**
1. Consensus issues a sync request to state sync (either SyncTarget or SyncDuration)
2. State sync satisfies the request and reaches `check_sync_request_progress()`
3. The sync request is detected as satisfied and removed from state
4. State sync attempts to send a response to consensus
5. Consensus has already timed out and dropped the receiver channel
6. The callback send fails, returning `Error::CallbackSendFailed`
7. Cleanup code never executes
8. Consensus remains blocked waiting for a response
9. ChunkExecutor memory is never released
10. Active data streams continue running
11. Future sync requests may fail due to inconsistent state

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos bug bounty criteria:

**Consensus Liveness Failure:**
- Consensus waits indefinitely for a response that will never arrive
- The node cannot make progress on consensus operations
- Validators become stuck, unable to participate in block production
- Meets "Validator node slowdowns" and "Significant protocol violations" criteria

**Memory Leak:**
- The ChunkExecutor's internal Sparse Merkle Tree structures are never released
- Each failed sync request accumulates unreleased memory
- Repeated failures lead to memory exhaustion
- Node eventually crashes due to out-of-memory conditions

**Resource Exhaustion:**
- Active data streams continue running unnecessarily
- Network bandwidth is wasted on streaming data that won't be used
- Storage synchronizer remains in an inconsistent state

**No Automatic Recovery:**
- Once the sync request state is removed, there's no mechanism to detect or recover from the failure
- The cleanup code check at line 603 uses `!self.active_sync_request()` which returns false (since the request was removed)
- Manual intervention or node restart is required

**Cascading Failures:**
- Future sync requests may fail because the storage synchronizer is in an inconsistent state
- The chunk executor hasn't been properly reset for new operations
- Multiple validators experiencing this issue simultaneously could impact network liveness

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability can occur through natural network conditions without any malicious actor:

**Consensus Timeout Scenario:**
Consensus components have timeouts when waiting for state sync responses. For commit notifications: [8](#0-7) 

If state sync takes longer than the timeout to satisfy the request (e.g., due to network delays, heavy load, or slow peer responses), consensus will timeout and drop the receiver. When state sync finally tries to respond, the send will fail.

**Node Restart/Crash:**
If consensus restarts or crashes while state sync is processing a request, the receiver channel is dropped, causing the same failure.

**Channel Closure:**
Various internal errors or state transitions can cause the consensus notification channels to close unexpectedly.

**Triggering Conditions:**
- High network latency between state sync and data sources
- Slow peer responses during state synchronization
- Heavy node load causing processing delays
- Consensus restarts or upgrades
- Internal errors in consensus or state sync components

The vulnerability is particularly concerning because:
- It requires no attacker action
- It can occur during normal operations under stress
- The failure is silent (only logged as a warning)
- There's no automatic recovery mechanism

## Recommendation

Implement atomic state cleanup by deferring the sync request removal until after all operations succeed:

```rust
pub async fn handle_satisfied_sync_request(
    &mut self,
    latest_synced_ledger_info: LedgerInfoWithSignatures,
) -> Result<(), Error> {
    // Get a reference to the sync request WITHOUT removing it yet
    let mut sync_request_lock = self.consensus_sync_request.lock();
    let consensus_sync_request = match sync_request_lock.as_ref() {
        Some(req) => req.clone(),
        None => return Ok(()), // No request to handle
    };
    drop(sync_request_lock); // Release lock before async operations

    // Attempt to notify consensus of the satisfied request
    let result = match consensus_sync_request {
        ConsensusSyncRequest::SyncDuration(_, sync_duration_notification) => {
            self.respond_to_sync_duration_notification(
                sync_duration_notification,
                Ok(()),
                Some(latest_synced_ledger_info),
            )
        },
        ConsensusSyncRequest::SyncTarget(sync_target_notification) => {
            let sync_target = sync_target_notification.get_target();
            let sync_target_version = sync_target.ledger_info().version();
            let latest_synced_version = latest_synced_ledger_info.ledger_info().version();

            if latest_synced_version > sync_target_version {
                let error = Err(Error::SyncedBeyondTarget(
                    latest_synced_version,
                    sync_target_version,
                ));
                self.respond_to_sync_target_notification(
                    sync_target_notification,
                    error.clone(),
                )?;
                return error;
            }

            self.respond_to_sync_target_notification(sync_target_notification, Ok(()))
        },
    };

    // Only remove the sync request if notification succeeded
    result?;
    *self.consensus_sync_request.lock() = None;
    Ok(())
}
```

Additionally, add error recovery in `check_sync_request_progress()`:

```rust
async fn check_sync_request_progress(&mut self) -> Result<(), Error> {
    // ... existing code ...

    // Handle the satisfied sync request
    let latest_synced_ledger_info =
        utils::fetch_latest_synced_ledger_info(self.storage.clone())?;
    
    match self.consensus_notification_handler
        .handle_satisfied_sync_request(latest_synced_ledger_info)
        .await 
    {
        Ok(_) => {
            // Success - perform cleanup
            if !self.active_sync_request() {
                self.continuous_syncer.reset_active_stream(None).await?;
                self.storage_synchronizer.finish_chunk_executor();
            }
            Ok(())
        },
        Err(error) => {
            // Failure - still perform cleanup to prevent resource leaks
            error!(LogSchema::new(LogEntry::Driver)
                .error(&error)
                .message("Failed to handle satisfied sync request, performing cleanup"));
            
            // Always clean up resources even on failure
            if !self.active_sync_request() {
                let _ = self.continuous_syncer.reset_active_stream(None).await;
                self.storage_synchronizer.finish_chunk_executor();
            }
            
            Err(error)
        }
    }
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_sync_request_cleanup_on_callback_failure() {
    use aptos_consensus_notifications::ConsensusNotificationListener;
    use futures::channel::mpsc;
    
    // Setup state sync driver components
    let (notification_sender, notification_receiver) = mpsc::unbounded();
    let mut consensus_handler = ConsensusNotificationHandler::new(
        ConsensusNotificationListener::new(notification_receiver),
        TimeService::mock(),
    );
    
    // Create a sync target notification
    let target_li = create_test_ledger_info(100);
    let (sync_notification, callback_receiver) = 
        ConsensusSyncTargetNotification::new(target_li.clone());
    
    // Initialize sync request
    let latest_pre_committed = 50;
    consensus_handler.initialize_sync_target_request(
        sync_notification,
        latest_pre_committed,
        create_test_ledger_info(50),
    ).await.unwrap();
    
    // Verify sync request exists
    assert!(consensus_handler.active_sync_request());
    
    // Drop the callback receiver to simulate consensus timeout
    drop(callback_receiver);
    
    // Attempt to handle satisfied request - this should fail
    let synced_li = create_test_ledger_info(100);
    let result = consensus_handler.handle_satisfied_sync_request(synced_li).await;
    
    // Verify the failure occurred
    assert!(result.is_err());
    assert!(matches!(result, Err(Error::CallbackSendFailed(_))));
    
    // BUG: Sync request was removed despite the failure
    assert!(!consensus_handler.active_sync_request());
    
    // At this point, in the actual driver:
    // - Consensus is still waiting for a response
    // - Sync request state is gone
    // - Cleanup code (finish_chunk_executor, reset_active_stream) never ran
    // - Memory leak and resource leak occurred
    // - System is in inconsistent state
}
```

## Notes

This vulnerability violates the **State Consistency** invariant that "state transitions must be atomic." The premature removal of the sync request before confirming successful notification creates a non-atomic operation that leaves the system in an inconsistent state.

The issue is particularly severe because:
1. It can occur during normal operations without attacker involvement
2. The failure mode is silent (only logged as warning)
3. There's no automatic recovery mechanism
4. Multiple validators could be affected simultaneously under stress
5. Memory leaks accumulate over time leading to node crashes

The fix requires ensuring that all state modifications are committed atomically only after all dependent operations succeed, and implementing proper cleanup even in error paths to prevent resource leaks.

### Citations

**File:** state-sync/state-sync-driver/src/driver.rs (L594-609)
```rust
        // Handle the satisfied sync request
        let latest_synced_ledger_info =
            utils::fetch_latest_synced_ledger_info(self.storage.clone())?;
        self.consensus_notification_handler
            .handle_satisfied_sync_request(latest_synced_ledger_info)
            .await?;

        // If the sync request was successfully handled, reset the continuous syncer
        // so that in the event another sync request occurs, we have fresh state.
        if !self.active_sync_request() {
            self.continuous_syncer.reset_active_stream(None).await?;
            self.storage_synchronizer.finish_chunk_executor(); // Consensus or consensus observer is now in control
        }

        Ok(())
    }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L680-685)
```rust
        // Check the progress of any sync requests
        if let Err(error) = self.check_sync_request_progress().await {
            warn!(LogSchema::new(LogEntry::Driver)
                .error(&error)
                .message("Error found when checking the sync request progress!"));
        }
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L322-365)
```rust
    pub async fn handle_satisfied_sync_request(
        &mut self,
        latest_synced_ledger_info: LedgerInfoWithSignatures,
    ) -> Result<(), Error> {
        // Remove the active sync request
        let mut sync_request_lock = self.consensus_sync_request.lock();
        let consensus_sync_request = sync_request_lock.take();

        // Notify consensus of the satisfied request
        match consensus_sync_request {
            Some(ConsensusSyncRequest::SyncDuration(_, sync_duration_notification)) => {
                self.respond_to_sync_duration_notification(
                    sync_duration_notification,
                    Ok(()),
                    Some(latest_synced_ledger_info),
                )?;
            },
            Some(ConsensusSyncRequest::SyncTarget(sync_target_notification)) => {
                // Get the sync target version and latest synced version
                let sync_target = sync_target_notification.get_target();
                let sync_target_version = sync_target.ledger_info().version();
                let latest_synced_version = latest_synced_ledger_info.ledger_info().version();

                // Check if we've synced beyond the target. If so, notify consensus with an error.
                if latest_synced_version > sync_target_version {
                    let error = Err(Error::SyncedBeyondTarget(
                        latest_synced_version,
                        sync_target_version,
                    ));
                    self.respond_to_sync_target_notification(
                        sync_target_notification,
                        error.clone(),
                    )?;
                    return error;
                }

                // Otherwise, notify consensus that the target has been reached
                self.respond_to_sync_target_notification(sync_target_notification, Ok(()))?;
            },
            None => { /* Nothing needs to be done */ },
        }

        Ok(())
    }
```

**File:** state-sync/inter-component/consensus-notifications/src/lib.rs (L122-137)
```rust
        if let Ok(response) = timeout(
            Duration::from_millis(self.commit_timeout_ms),
            callback_receiver,
        )
        .await
        {
            match response {
                Ok(consensus_notification_response) => consensus_notification_response.get_result(),
                Err(error) => Err(Error::UnexpectedErrorEncountered(format!(
                    "Consensus commit notification failure: {:?}",
                    error
                ))),
            }
        } else {
            Err(Error::TimeoutWaitingForStateSync)
        }
```

**File:** state-sync/inter-component/consensus-notifications/src/lib.rs (L224-232)
```rust
    fn send_result_to_callback(
        &self,
        callback: oneshot::Sender<ConsensusNotificationResponse>,
        result: Result<(), Error>,
    ) -> Result<(), Error> {
        callback
            .send(ConsensusNotificationResponse::new(result))
            .map_err(|error| Error::UnexpectedErrorEncountered(format!("{:?}", error)))
    }
```

**File:** state-sync/inter-component/consensus-notifications/src/lib.rs (L254-259)
```rust
        // Send the response to the callback
        sync_duration_notification
            .callback
            .send(response)
            .map_err(|error| Error::UnexpectedErrorEncountered(format!("{:?}", error)))
    }
```

**File:** execution/executor/src/chunk_executor/mod.rs (L221-225)
```rust
    fn finish(&self) {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["chunk", "finish"]);

        *self.inner.write() = None;
    }
```

**File:** execution/executor-types/src/lib.rs (L113-114)
```rust
    /// Finishes the chunk executor by releasing memory held by inner data structures(SMT).
    fn finish(&self);
```
