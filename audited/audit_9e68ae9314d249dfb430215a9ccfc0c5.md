# Audit Report

## Title
State Merkle Shard Pruner Progress Tracking Failure Causes State Inconsistency After Crash

## Summary
The `StateMerkleShardPruner::prune()` function contains a critical bug where `current_progress` is never updated within the pruning loop. When the pruning process is interrupted (crash, panic, or restart) after deleting nodes but before completing all iterations, the per-shard progress remains at its original value despite nodes being permanently deleted. This causes the system to claim data availability for versions that have been pruned, leading to `MissingRootError` on queries and potential consensus divergence across validators.

## Finding Description

The vulnerability exists in the main pruning loop where `current_progress` is passed to `get_stale_node_indices()` on every iteration without being updated: [1](#0-0) 

The critical flaw is that:
1. The function receives `current_progress` as a parameter
2. Each loop iteration calls `get_stale_node_indices()` with the same `current_progress` value
3. Progress is only persisted when `done = true` at the very end (lines 86-89)
4. If the process is interrupted mid-loop after batch commits (line 92), multiple batches of nodes are deleted but progress never advances

**State Inconsistency Scenario:**

1. **Initial State**: Shard progress = 5000, stale nodes exist for versions 5000-6000, batch_size = 100
2. **Iteration 1**: `get_stale_node_indices(5000, 6000, 100)` returns 100 indices for versions 5000-5050. These are deleted and committed. Progress still = 5000.
3. **Iteration 2**: `get_stale_node_indices(5000, 6000, 100)` still uses 5000! Iterator skips deleted nodes, returns versions 5051-5100. Deleted and committed. Progress still = 5000.
4. **CRASH** occurs before loop completion
5. **On Restart**: Shard progress reads 5000 from database [2](#0-1) 

During initialization, the shard calls `prune(5000, 5000, usize::MAX)` for catchup, which does nothing since current_progress == metadata_progress.

6. **Query Failure**: A query for version 5025 passes the min_readable_version check: [3](#0-2) 

But when retrieving the actual merkle node, it fails with `MissingRootError`: [4](#0-3) 

This violates the **State Consistency** invariant: versions >= min_readable_version must be queryable with complete merkle proofs.

## Impact Explanation

This is **HIGH Severity** (up to $50,000) under "Significant protocol violations" and **MEDIUM Severity** (up to $10,000) under "State inconsistencies requiring intervention":

1. **Data Availability Failure**: Nodes cannot serve legitimate queries for versions they claim to support, breaking the promise that all versions >= min_readable_version are available.

2. **Consensus Divergence Risk**: Different validators may crash at different points during pruning, resulting in different missing data ranges across the network. This could cause validators to diverge when processing state-dependent transactions.

3. **State Sync Failures**: New validators attempting to sync from affected nodes will fail to retrieve merkle proofs for supposedly available versions.

4. **Operational Impact**: Requires manual intervention to identify inconsistent shards, potentially requiring database recovery or re-sync from genesis.

The impact extends beyond a single node because crashes during pruning are common operational events, and the outer `StateMerklePruner` will record overall progress even if individual shards are inconsistent: [5](#0-4) 

## Likelihood Explanation

**HIGH Likelihood** - This bug will manifest naturally during normal operations:

1. **Common Trigger**: Process crashes, OOM kills, restarts, and panics occur regularly in distributed systems, especially during high load
2. **Frequent Operation**: Pruning runs continuously as a background task when enabled
3. **Multiple Iterations**: With default batch_size (100) and thousands of versions to prune, multi-iteration loops are common
4. **No Prevention**: No safeguards exist to prevent or detect this inconsistency
5. **Silent Failure**: The bug is silent until a query attempts to access the missing range

The parallel execution of shard pruners increases likelihood: [6](#0-5) 

Multiple shards can enter inconsistent states independently.

## Recommendation

**Fix**: Update `current_progress` after each batch is successfully committed, and persist incremental progress:

```rust
pub(in crate::pruner) fn prune(
    &self,
    current_progress: Version,
    target_version: Version,
    max_nodes_to_prune: usize,
) -> Result<()> {
    let mut progress = current_progress;  // Make progress mutable
    
    loop {
        let mut batch = SchemaBatch::new();
        let (indices, next_version) = StateMerklePruner::get_stale_node_indices(
            &self.db_shard,
            progress,  // Use updated progress
            target_version,
            max_nodes_to_prune,
        )?;

        indices.into_iter().try_for_each(|index| {
            batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
            batch.delete::<S>(&index)
        })?;

        let mut done = true;
        let new_progress = if let Some(next_version) = next_version {
            if next_version <= target_version {
                done = false;
                next_version  // Advance to next_version
            } else {
                target_version
            }
        } else {
            target_version
        };

        // Always update progress in the batch
        batch.put::<DbMetadataSchema>(
            &S::progress_metadata_key(Some(self.shard_id)),
            &DbMetadataValue::Version(new_progress),
        )?;

        self.db_shard.write_schemas(batch)?;
        
        progress = new_progress;  // Update loop variable
        
        if done {
            break;
        }
    }

    Ok(())
}
```

This ensures:
1. Progress is updated after each successful batch commit
2. If interrupted, progress reflects actual deleted ranges
3. On restart, pruning continues from correct position
4. No missing data ranges exist

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use crate::schema::jellyfish_merkle_node::JellyfishMerkleNodeSchema;
    use aptos_temppath::TempPath;
    
    #[test]
    fn test_shard_pruner_crash_inconsistency() {
        // Setup: Create shard DB with stale nodes at versions 1000-1200
        let tmpdir = TempPath::new();
        let db = Arc::new(DB::open(
            &tmpdir,
            "test_db",
            &[JellyfishMerkleNodeSchema::COLUMN_FAMILY_NAME, 
              DbMetadataSchema::COLUMN_FAMILY_NAME]
        ).unwrap());
        
        // Insert 200 stale node indices for versions 1000-1200
        let mut batch = SchemaBatch::new();
        for version in 1000..1200 {
            let index = StaleNodeIndex {
                stale_since_version: version,
                node_key: NodeKey::new_empty_path(version),
            };
            let node = create_test_node();
            batch.put::<JellyfishMerkleNodeSchema>(&index.node_key, &node).unwrap();
            batch.put::<StaleNodeIndexSchema>(&index, &()).unwrap();
        }
        db.write_schemas(batch).unwrap();
        
        // Set initial progress to 1000
        db.put::<DbMetadataSchema>(
            &DbMetadataKey::StateMerkleShardPrunerProgress(0),
            &DbMetadataValue::Version(1000)
        ).unwrap();
        
        // Simulate pruning with small batch size
        let pruner = StateMerkleShardPruner::<StaleNodeIndexSchema>::new(
            0, db.clone(), 1000
        ).unwrap();
        
        // Start pruning from 1000 to 1200 with batch_size=50
        // This requires multiple iterations
        let prune_result = std::panic::catch_unwind(|| {
            // Simulate crash after first iteration
            let mut batch = SchemaBatch::new();
            let (indices, _) = StateMerklePruner::get_stale_node_indices(
                &db, 1000, 1200, 50
            ).unwrap();
            
            // Delete first batch (versions 1000-1049)
            for index in indices {
                batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key).unwrap();
                batch.delete::<StaleNodeIndexSchema>(&index).unwrap();
            }
            db.write_schemas(batch).unwrap();
            
            // CRASH HERE - progress never updated!
            panic!("Simulated crash");
        });
        
        assert!(prune_result.is_err());
        
        // Verify inconsistency: progress still at 1000
        let progress = db.get::<DbMetadataSchema>(
            &DbMetadataKey::StateMerkleShardPrunerProgress(0)
        ).unwrap().unwrap().expect_version();
        assert_eq!(progress, 1000);
        
        // But nodes 1000-1049 are deleted!
        let node = db.get::<JellyfishMerkleNodeSchema>(
            &NodeKey::new_empty_path(1025)
        ).unwrap();
        assert!(node.is_none(), "Node should be deleted but progress says available");
        
        // This creates state inconsistency:
        // - System claims version 1025 is available (>= progress 1000)
        // - But querying it will fail with MissingRootError
    }
}
```

## Notes

The contrasting implementation in `StateMerkleMetadataPruner` correctly updates progress after each single version: [7](#0-6) 

This metadata pruner updates progress at line 66-68 within each call, demonstrating the correct pattern that should be followed by the shard pruner.

### Citations

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L31-56)
```rust
    pub(in crate::pruner) fn new(
        shard_id: usize,
        db_shard: Arc<DB>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &S::progress_metadata_key(Some(shard_id)),
            metadata_progress,
        )?;
        let myself = Self {
            shard_id,
            db_shard,
            _phantom: PhantomData,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up {} shard {shard_id}.",
            S::name(),
        );
        myself.prune(progress, metadata_progress, usize::MAX)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L58-100)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
        max_nodes_to_prune: usize,
    ) -> Result<()> {
        loop {
            let mut batch = SchemaBatch::new();
            let (indices, next_version) = StateMerklePruner::get_stale_node_indices(
                &self.db_shard,
                current_progress,
                target_version,
                max_nodes_to_prune,
            )?;

            indices.into_iter().try_for_each(|index| {
                batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
                batch.delete::<S>(&index)
            })?;

            let mut done = true;
            if let Some(next_version) = next_version {
                if next_version <= target_version {
                    done = false;
                }
            }

            if done {
                batch.put::<DbMetadataSchema>(
                    &S::progress_metadata_key(Some(self.shard_id)),
                    &DbMetadataValue::Version(target_version),
                )?;
            }

            self.db_shard.write_schemas(batch)?;

            if done {
                break;
            }
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L273-302)
```rust
    pub(super) fn error_if_state_merkle_pruned(
        &self,
        data_type: &str,
        version: Version,
    ) -> Result<()> {
        let min_readable_version = self
            .state_store
            .state_db
            .state_merkle_pruner
            .get_min_readable_version();
        if version >= min_readable_version {
            return Ok(());
        }

        let min_readable_epoch_snapshot_version = self
            .state_store
            .state_db
            .epoch_snapshot_pruner
            .get_min_readable_version();
        if version >= min_readable_epoch_snapshot_version {
            self.ledger_db.metadata_db().ensure_epoch_ending(version)
        } else {
            bail!(
                "{} at version {} is pruned. snapshots are available at >= {}, epoch snapshots are available at >= {}",
                data_type,
                version,
                min_readable_version,
                min_readable_epoch_snapshot_version,
            )
        }
```

**File:** storage/jellyfish-merkle/src/lib.rs (L732-741)
```rust
            let next_node = self
                .reader
                .get_node_with_tag(&next_node_key, "get_proof")
                .map_err(|err| {
                    if nibble_depth == 0 {
                        AptosDbError::MissingRootError(version)
                    } else {
                        err
                    }
                })?;
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L76-89)
```rust
        while progress < target_version {
            if let Some(target_version_for_this_round) = self
                .metadata_pruner
                .maybe_prune_single_version(progress, target_version)?
            {
                self.prune_shards(progress, target_version_for_this_round, batch_size)?;
                progress = target_version_for_this_round;
                info!(name = S::name(), progress = progress);
                self.record_progress(target_version_for_this_round);
            } else {
                self.prune_shards(progress, target_version, batch_size)?;
                self.record_progress(target_version);
                break;
            }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L168-189)
```rust
    fn prune_shards(
        &self,
        current_progress: Version,
        target_version: Version,
        batch_size: usize,
    ) -> Result<()> {
        THREAD_MANAGER
            .get_background_pool()
            .install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(current_progress, target_version, batch_size)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state merkle shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })
            .map_err(Into::into)
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs (L40-79)
```rust
    pub(in crate::pruner) fn maybe_prune_single_version(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<Option<Version>> {
        let next_version = self.next_version.load(Ordering::SeqCst);
        // This max here is only to handle the case when next version is not initialized.
        let target_version_for_this_round = max(next_version, current_progress);
        if target_version_for_this_round > target_version {
            return Ok(None);
        }

        // When next_version is not initialized, this call is used to initialize it.
        let (indices, next_version) = StateMerklePruner::get_stale_node_indices(
            &self.metadata_db,
            current_progress,
            target_version_for_this_round,
            usize::MAX,
        )?;

        let mut batch = SchemaBatch::new();
        indices.into_iter().try_for_each(|index| {
            batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
            batch.delete::<S>(&index)
        })?;

        batch.put::<DbMetadataSchema>(
            &S::progress_metadata_key(None),
            &DbMetadataValue::Version(target_version_for_this_round),
        )?;

        self.metadata_db.write_schemas(batch)?;

        self.next_version
            // If next_version is None, meaning we've already reached the end of stale index.
            // Updating it to the target_version to make sure it's still making progress.
            .store(next_version.unwrap_or(target_version), Ordering::SeqCst);

        Ok(Some(target_version_for_this_round))
    }
```
