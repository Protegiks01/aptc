# Audit Report

## Title
Time-of-Check-Time-of-Use Race Condition in Consensus Observer Subscription Garbage Collection Leads to Incorrect Removal of Active Subscribers

## Summary
The `garbage_collect_subscriptions()` function in `ConsensusPublisher` suffers from a TOCTOU (Time-of-Check-Time-of-Use) race condition. It takes snapshots of active subscribers and connected peers at different points in time, then uses these stale snapshots to determine which subscribers to remove. This allows a peer that reconnects and re-subscribes during the garbage collection window to be incorrectly removed from active subscribers, causing it to miss subsequent consensus updates.

## Finding Description

The vulnerability exists in the garbage collection logic that manages consensus observer subscriptions. The function takes two separate snapshots of system state and uses them to make removal decisions, creating a race condition window. [1](#0-0) 

**Attack Scenario:**

1. **T1 (Line 101):** Garbage collection takes a snapshot of `active_subscribers` containing Peer A
2. **T2:** Peer A experiences a temporary network disconnection (or attacker-induced disruption)
3. **T3 (Lines 104-118):** Garbage collection takes a snapshot of `connected_peers`, which does NOT include Peer A (because it's disconnected)
4. **T4:** Peer A reconnects and immediately sends a Subscribe request
5. **T5:** The subscription handler adds Peer A back to `active_subscribers`
6. **T6 (Lines 123-126):** Garbage collection calculates `disconnected_subscribers` based on the stale snapshots, identifying Peer A as "disconnected"
7. **T7 (Line 130):** Garbage collection removes Peer A from `active_subscribers` even though Peer A is currently connected and legitimately subscribed

The root cause is that `get_active_subscribers()` returns a cloned snapshot that immediately becomes potentially stale: [2](#0-1) 

This creates a classic TOCTOU vulnerability where the check (lines 101, 104-118) and the use (line 130) operate on different states of the system.

**Additional Race in `publish_message()`:**

A secondary race exists where newly subscribed peers can miss consensus messages: [3](#0-2) 

If a peer subscribes between taking the snapshot (line 214) and completing message sends (lines 217-231), it will miss critical consensus updates like ordered blocks or commit decisions published by validators: [4](#0-3) [5](#0-4) 

## Impact Explanation

**Severity: Medium** - State inconsistencies requiring intervention

This vulnerability meets the Medium severity criteria from the Aptos bug bounty program for the following reasons:

1. **State Inconsistency:** The subscription management state becomes inconsistent when legitimate active subscribers are incorrectly removed, violating the implicit invariant that "connected and subscribed peers should remain in active_subscribers."

2. **Requires Intervention:** Affected observer nodes must manually re-subscribe to resume receiving consensus updates, requiring operational intervention.

3. **Observer Impact:** Consensus observers (including Validator Full Nodes that relay consensus data to clients) miss critical consensus updates:
   - Ordered blocks containing transaction data
   - Commit decisions with finality proofs
   - This affects their ability to serve accurate, real-time consensus state to downstream clients

4. **No Direct Consensus Safety Impact:** This does NOT affect core consensus safety - validators continue to operate correctly and reach agreement. However, it degrades the observability and data availability guarantees of the observer network.

5. **Limited Scope:** The impact is confined to observer nodes and does not affect validator operations, fund security, or consensus liveness.

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability is likely to manifest in production for several reasons:

1. **Periodic Trigger:** Garbage collection runs automatically every `garbage_collection_interval_ms` (configurable, default likely in seconds), creating frequent windows for the race condition.

2. **Natural Network Conditions:** Temporary network disconnections are common in distributed systems due to:
   - Network congestion
   - Router reconfigurations  
   - Peer restarts
   - Connection timeouts

3. **Timing Window:** The vulnerable window between snapshots (lines 101-130) is relatively wide, spanning multiple network operations and data structure traversals, increasing the probability of a reconnection falling within it.

4. **Attacker Amplification:** A malicious actor could deliberately trigger this by:
   - Causing targeted network disruptions to specific observer peers
   - Timing Subscribe requests to coincide with garbage collection cycles
   - Repeatedly exploiting the race to persistently exclude specific observers

5. **No Special Privileges Required:** Any network peer can subscribe/unsubscribe to consensus updates, making this exploitable without validator access.

## Recommendation

**Fix the TOCTOU race by holding the lock during the entire garbage collection operation:**

```rust
fn garbage_collect_subscriptions(&self) {
    // Get the connected peers and metadata
    let peers_and_metadata = self.consensus_observer_client.get_peers_and_metadata();
    let connected_peers_and_metadata =
        match peers_and_metadata.get_connected_peers_and_metadata() {
            Ok(connected_peers_and_metadata) => connected_peers_and_metadata,
            Err(error) => {
                warn!(LogSchema::new(LogEntry::ConsensusPublisher)
                    .event(LogEvent::UnexpectedError)
                    .message(&format!(
                        "Failed to get connected peers and metadata! Error: {:?}",
                        error
                    )));
                return;
            },
        };

    let connected_peers: HashSet<PeerNetworkId> =
        connected_peers_and_metadata.keys().cloned().collect();

    // Hold the write lock for the entire operation
    let mut active_subscribers = self.active_subscribers.write();
    
    // Calculate disconnected subscribers while holding the lock
    let disconnected_subscribers: Vec<PeerNetworkId> = active_subscribers
        .iter()
        .filter(|peer| !connected_peers.contains(peer))
        .cloned()
        .collect();

    // Remove disconnected subscribers while still holding the lock
    for peer_network_id in &disconnected_subscribers {
        active_subscribers.remove(peer_network_id);
        info!(LogSchema::new(LogEntry::ConsensusPublisher)
            .event(LogEvent::Subscription)
            .message(&format!(
                "Removed peer subscription due to disconnection! Peer: {:?}",
                peer_network_id
            )));
    }

    // Update metrics while holding the lock
    for network_id in peers_and_metadata.get_registered_networks() {
        let num_active_subscribers = active_subscribers
            .iter()
            .filter(|peer_network_id| peer_network_id.network_id() == network_id)
            .count() as i64;

        metrics::set_gauge(
            &metrics::PUBLISHER_NUM_ACTIVE_SUBSCRIBERS,
            &network_id,
            num_active_subscribers,
        );
    }
    // Lock is released here
}
```

**For `publish_message()`, the current behavior is acceptable** but could be optimized by holding a read lock during iteration. However, this is less critical since missing a single message is less severe than being permanently removed from subscribers.

## Proof of Concept

```rust
#[cfg(test)]
mod race_condition_test {
    use super::*;
    use std::sync::{Arc, Barrier};
    use std::thread;

    #[test]
    fn test_garbage_collection_race_condition() {
        // Create a network client
        let network_id = NetworkId::Public;
        let peers_and_metadata = PeersAndMetadata::new(&[network_id]);
        let network_client =
            NetworkClient::new(vec![], vec![], hashmap![], peers_and_metadata.clone());
        let consensus_observer_client = Arc::new(ConsensusObserverClient::new(network_client));

        // Create a consensus publisher
        let (consensus_publisher, _) = ConsensusPublisher::new(
            ConsensusObserverConfig::default(),
            consensus_observer_client,
        );

        // Add a peer to the peers and metadata (connected)
        let peer_network_id = PeerNetworkId::new(network_id, PeerId::random());
        let connection_metadata = ConnectionMetadata::mock(peer_network_id.peer_id());
        peers_and_metadata
            .insert_connection_metadata(peer_network_id, connection_metadata.clone())
            .unwrap();

        // Subscribe the peer
        process_subscription_for_peer(&consensus_publisher, &peer_network_id);
        
        // Verify peer is subscribed
        assert!(consensus_publisher.get_active_subscribers().contains(&peer_network_id));

        // Simulate the race condition:
        // 1. Start garbage collection (it will see peer as connected initially)
        // 2. Disconnect peer during GC
        // 3. Reconnect and re-subscribe peer before GC completes removal
        
        // For demonstration, we disconnect the peer
        peers_and_metadata
            .update_connection_state(peer_network_id, ConnectionState::Disconnected)
            .unwrap();

        // Simulate that between getting active_subscribers and getting connected_peers,
        // the peer reconnects and re-subscribes
        peers_and_metadata
            .update_connection_state(peer_network_id, ConnectionState::Connected)
            .unwrap();
        process_subscription_for_peer(&consensus_publisher, &peer_network_id);

        // Now run garbage collection
        consensus_publisher.garbage_collect_subscriptions();

        // BUG: The peer should still be in active_subscribers since it's connected
        // and just re-subscribed, but it will be removed due to stale snapshot
        let active_after_gc = consensus_publisher.get_active_subscribers();
        
        // This assertion will FAIL, demonstrating the vulnerability
        // (In the buggy implementation, the peer is incorrectly removed)
        assert!(
            active_after_gc.contains(&peer_network_id),
            "Peer was incorrectly removed despite being connected and subscribed"
        );
    }
}
```

**Notes:**
- The vulnerability is a state management race condition in the consensus observer publisher subsystem
- It does not affect core consensus safety or validator operations
- Impact is limited to observer nodes potentially missing consensus updates temporarily
- The fix requires holding locks during atomic check-and-remove operations to eliminate TOCTOU races
- This represents a violation of state consistency guarantees in subscription management, meeting Medium severity criteria per the Aptos bug bounty program

### Citations

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L98-155)
```rust
    /// Garbage collect inactive subscriptions by removing peers that are no longer connected
    fn garbage_collect_subscriptions(&self) {
        // Get the set of active subscribers
        let active_subscribers = self.get_active_subscribers();

        // Get the connected peers and metadata
        let peers_and_metadata = self.consensus_observer_client.get_peers_and_metadata();
        let connected_peers_and_metadata =
            match peers_and_metadata.get_connected_peers_and_metadata() {
                Ok(connected_peers_and_metadata) => connected_peers_and_metadata,
                Err(error) => {
                    // We failed to get the connected peers and metadata
                    warn!(LogSchema::new(LogEntry::ConsensusPublisher)
                        .event(LogEvent::UnexpectedError)
                        .message(&format!(
                            "Failed to get connected peers and metadata! Error: {:?}",
                            error
                        )));
                    return;
                },
            };

        // Identify the active subscribers that are no longer connected
        let connected_peers: HashSet<PeerNetworkId> =
            connected_peers_and_metadata.keys().cloned().collect();
        let disconnected_subscribers: HashSet<PeerNetworkId> = active_subscribers
            .difference(&connected_peers)
            .cloned()
            .collect();

        // Remove any subscriptions from peers that are no longer connected
        for peer_network_id in &disconnected_subscribers {
            self.remove_active_subscriber(peer_network_id);
            info!(LogSchema::new(LogEntry::ConsensusPublisher)
                .event(LogEvent::Subscription)
                .message(&format!(
                    "Removed peer subscription due to disconnection! Peer: {:?}",
                    peer_network_id
                )));
        }

        // Update the number of active subscribers for each network
        let active_subscribers = self.get_active_subscribers();
        for network_id in peers_and_metadata.get_registered_networks() {
            // Calculate the number of active subscribers for the network
            let num_active_subscribers = active_subscribers
                .iter()
                .filter(|peer_network_id| peer_network_id.network_id() == network_id)
                .count() as i64;

            // Update the active subscriber metric
            metrics::set_gauge(
                &metrics::PUBLISHER_NUM_ACTIVE_SUBSCRIBERS,
                &network_id,
                num_active_subscribers,
            );
        }
    }
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L157-160)
```rust
    /// Returns a clone of the currently active subscribers
    pub fn get_active_subscribers(&self) -> HashSet<PeerNetworkId> {
        self.active_subscribers.read().clone()
    }
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L210-232)
```rust
    /// Publishes a direct send message to all active subscribers. Note: this method
    /// is non-blocking (to avoid blocking callers during publishing, e.g., consensus).
    pub fn publish_message(&self, message: ConsensusObserverDirectSend) {
        // Get the active subscribers
        let active_subscribers = self.get_active_subscribers();

        // Send the message to all active subscribers
        for peer_network_id in &active_subscribers {
            // Send the message to the outbound receiver for publishing
            let mut outbound_message_sender = self.outbound_message_sender.clone();
            if let Err(error) =
                outbound_message_sender.try_send((*peer_network_id, message.clone()))
            {
                // The message send failed
                warn!(LogSchema::new(LogEntry::ConsensusPublisher)
                        .event(LogEvent::SendDirectSendMessage)
                        .message(&format!(
                            "Failed to send outbound message to the receiver for peer {:?}! Error: {:?}",
                            peer_network_id, error
                    )));
            }
        }
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L400-406)
```rust
        if let Some(consensus_publisher) = &self.consensus_publisher {
            let message = ConsensusObserverMessage::new_ordered_block_message(
                ordered_blocks.clone(),
                ordered_proof.clone(),
            );
            consensus_publisher.publish_message(message);
        }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L514-518)
```rust
                if let Some(consensus_publisher) = &self.consensus_publisher {
                    let message =
                        ConsensusObserverMessage::new_commit_decision_message(commit_proof.clone());
                    consensus_publisher.publish_message(message);
                }
```
