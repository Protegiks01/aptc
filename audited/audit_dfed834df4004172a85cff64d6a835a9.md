# Audit Report

## Title
Indexer Race Condition Causes Silent Loss of Object Deletion Events in Parallel Batch Processing

## Summary
The Aptos indexer's parallel batch processing architecture can cause object deletion events to be silently dropped when transaction batches complete out of order. This results in the `current_objects` table showing objects as existing when they were actually deleted at later blockchain versions, creating permanent data integrity inconsistencies.

## Finding Description

The indexer processes transactions in parallel batches (default: 5 concurrent tasks) to improve throughput. When processing object deletions via `Object::from_delete_resource()`, the code requires the previous object state to create the deletion record. It first checks an in-batch HashMap, then queries the database as a fallback. [1](#0-0) 

The critical vulnerability occurs when:

1. **Batch A (versions 100-199)**: Contains object creation at version 100
2. **Batch B (versions 200-299)**: Contains object deletion at version 200
3. **Batch B processes first** (due to parallel execution completing out of order)
4. When processing the deletion, `get_object_owner()` queries the database but finds nothing (Batch A hasn't committed yet) [2](#0-1) 

The retry mechanism (5 attempts, 500ms delay) provides only 2.5 seconds maximum wait time. If Batch A takes longer to process, the deletion lookup fails, logs an error, and **returns `Ok(None)`**, silently dropping the deletion event. [3](#0-2) 

When Batch A finally completes, it writes the object creation at version 100 with `is_deleted=false`. The final state shows the object as existing, with no deletion record ever created.

The parallel processing architecture enables this race: [4](#0-3) 

Default configuration allows 5 concurrent processor tasks: [5](#0-4) 

While the `insert_current_objects()` function has a WHERE clause to prevent older versions from overwriting newer ones, this protection is irrelevant when the deletion record is never created: [6](#0-5) 

## Impact Explanation

This qualifies as **Medium Severity** per Aptos bug bounty criteria ("State inconsistencies requiring intervention").

**Impact:**
- **Data Integrity Violation**: The indexer database permanently loses critical state transitions (deletions)
- **API Incorrectness**: Queries return deleted objects as existing, misleading users and applications
- **Cascading Effects**: Downstream systems relying on indexer data receive incorrect information about object existence and ownership
- **Requires Manual Intervention**: Recovery requires stopping the indexer, clearing affected data, and reindexing from scratch

**Scope:** While this affects the indexer (not consensus), the indexer is a critical component for:
- REST API queries
- Explorer interfaces  
- Analytics dashboards
- Third-party integrations

The developer comment acknowledges known limitations with the approach: [7](#0-6) 

## Likelihood Explanation

**High Likelihood** under normal production conditions:

1. **Default Configuration**: Parallel processing is enabled by default with 5 concurrent tasks
2. **Natural Occurrence**: No attacker action required - happens naturally during normal operation
3. **Timing Factors**: 
   - Batch processing times vary based on transaction complexity
   - Database write performance fluctuations
   - System load variations
   - 2.5 second retry window is often insufficient for real processing delays
4. **Frequency**: More likely during:
   - High transaction volume periods
   - System resource contention
   - Large batch sizes
   - Complex transactions requiring more processing time

The error logging shows developers anticipated lookup failures but may not have fully considered the parallel processing race condition: [8](#0-7) 

## Recommendation

**Option 1: Serialize Deletion Processing**
Defer deletion processing until all earlier batches have committed. Track pending deletions and process them after verifying prerequisite data exists.

**Option 2: Extend Retry Window with Exponential Backoff**
Increase retry attempts and use exponential backoff (e.g., 10 retries with exponential delay up to 30 seconds) to handle longer batch processing delays.

**Option 3: Batch-Level Ordering Guarantee**
Ensure batches commit to the database in strict version order, even if processed in parallel. Use a coordination mechanism to serialize commits.

**Option 4: Transaction-Level Dependency Tracking**
Implement explicit dependency tracking between transactions across batches, preventing deletion processing until creation transactions are committed.

**Recommended Fix (Option 3 - Strictest):**
Add a batch ordering coordinator in `runtime.rs` to ensure database commits occur in version order, while still allowing parallel processing of batch contents.

## Proof of Concept

```rust
// Reproduction scenario:
// 1. Configure indexer with processor_tasks = 5 (default)
// 2. Prepare two batches:
//    - Batch A [100-199]: Contains ObjectCore resource creation at version 100
//    - Batch B [200-299]: Contains ObjectGroup resource deletion at version 200
// 3. Artificially delay Batch A processing (e.g., add sleep or complex transactions)
// 4. Batch B processes first, attempts to delete object
// 5. get_object_owner() fails after 5 retries (2.5 seconds)
// 6. Deletion returns Ok(None), no deletion record created
// 7. Batch A completes, writes object creation
// 8. Query current_objects: object exists with is_deleted=false
// 9. Query objects: only version 100 record exists
// Expected: version 200 deletion record should exist
// Actual: deletion record missing, object incorrectly shown as existing

// To test, modify runtime.rs to add delays and observe error logs:
// "Missing object owner for object. You probably should backfill db."
```

## Notes

This vulnerability specifically affects the **indexer component**, not blockchain consensus. However, the indexer is critical infrastructure for:
- Block explorers
- API services
- Analytics platforms
- User-facing applications

The issue represents a fundamental race condition in the parallel batch processing architecture that violates the expected invariant: *all blockchain state transitions must be accurately reflected in the indexer database*.

### Citations

**File:** crates/indexer/src/models/v2_objects.rs (L111-139)
```rust
    pub fn from_delete_resource(
        delete_resource: &DeleteResource,
        txn_version: i64,
        write_set_change_index: i64,
        object_mapping: &HashMap<CurrentObjectPK, CurrentObject>,
        conn: &mut PgPoolConnection,
    ) -> anyhow::Result<Option<(Self, CurrentObject)>> {
        if delete_resource.resource.to_string() == "0x1::object::ObjectGroup" {
            let resource = MoveResource::from_delete_resource(
                delete_resource,
                0, // Placeholder, this isn't used anyway
                txn_version,
                0, // Placeholder, this isn't used anyway
            );
            let previous_object = if let Some(object) = object_mapping.get(&resource.address) {
                object.clone()
            } else {
                match Self::get_object_owner(conn, &resource.address) {
                    Ok(owner) => owner,
                    Err(_) => {
                        aptos_logger::error!(
                            transaction_version = txn_version,
                            lookup_key = &resource.address,
                            "Missing object owner for object. You probably should backfill db.",
                        );
                        return Ok(None);
                    },
                }
            };
```

**File:** crates/indexer/src/models/v2_objects.rs (L166-192)
```rust
    /// This is actually not great because object owner can change. The best we can do now though
    fn get_object_owner(
        conn: &mut PgPoolConnection,
        object_address: &str,
    ) -> anyhow::Result<CurrentObject> {
        let mut retried = 0;
        while retried < QUERY_RETRIES {
            retried += 1;
            match CurrentObjectQuery::get_by_address(object_address, conn) {
                Ok(res) => {
                    return Ok(CurrentObject {
                        object_address: res.object_address,
                        owner_address: res.owner_address,
                        state_key_hash: res.state_key_hash,
                        allow_ungated_transfer: res.allow_ungated_transfer,
                        last_guid_creation_num: res.last_guid_creation_num,
                        last_transaction_version: res.last_transaction_version,
                        is_deleted: res.is_deleted,
                    })
                },
                Err(_) => {
                    std::thread::sleep(std::time::Duration::from_millis(QUERY_RETRY_DELAY_MS));
                },
            }
        }
        Err(anyhow::anyhow!("Failed to get object owner"))
    }
```

**File:** crates/indexer/src/runtime.rs (L210-219)
```rust
        let mut tasks = vec![];
        for _ in 0..processor_tasks {
            let other_tailer = tailer.clone();
            let task = tokio::spawn(async move { other_tailer.process_next_batch().await });
            tasks.push(task);
        }
        let batches = match futures::future::try_join_all(tasks).await {
            Ok(res) => res,
            Err(err) => panic!("Error processing transaction batches: {:?}", err),
        };
```

**File:** config/src/config/indexer_config.rs (L22-22)
```rust
pub const DEFAULT_PROCESSOR_TASKS: u8 = 5;
```

**File:** crates/indexer/src/processors/default_processor.rs (L444-469)
```rust
fn insert_current_objects(
    conn: &mut PgConnection,
    items_to_insert: &[CurrentObject],
) -> Result<(), diesel::result::Error> {
    use schema::current_objects::dsl::*;
    let chunks = get_chunks(items_to_insert.len(), CurrentObject::field_count());
    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::current_objects::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict(object_address)
                .do_update()
                .set((
                    owner_address.eq(excluded(owner_address)),
                    state_key_hash.eq(excluded(state_key_hash)),
                    allow_ungated_transfer.eq(excluded(allow_ungated_transfer)),
                    last_guid_creation_num.eq(excluded(last_guid_creation_num)),
                    last_transaction_version.eq(excluded(last_transaction_version)),
                    is_deleted.eq(excluded(is_deleted)),
                    inserted_at.eq(excluded(inserted_at)),
                )),
                Some(" WHERE current_objects.last_transaction_version <= excluded.last_transaction_version "),
        )?;
    }
    Ok(())
```
