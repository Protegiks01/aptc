# Audit Report

## Title
Unbounded Iterator in StateKvShardPruner Causes Memory Exhaustion During Node Initialization

## Summary
The `StateKvShardPruner::prune()` function lacks batching controls and accumulates all deletion operations in a single unbounded `SchemaBatch` during shard catch-up. When a shard falls behind and the node restarts, the initialization process attempts to process potentially millions of stale entries at once, leading to excessive memory consumption and node crashes.

## Finding Description

The vulnerability exists in the `prune()` function which is called during shard pruner initialization to catch up with the metadata pruner's progress. [1](#0-0) 

The function creates a single `SchemaBatch` and iterates through all stale state value indices from `current_progress` to `target_version` without any batching mechanism. Each stale entry requires two deletions (index and value), and all operations accumulate in memory before being written.

During node initialization, `StateKvShardPruner::new()` calls this function to catch up: [2](#0-1) 

The `SchemaBatch` structure has no inherent memory limits: [3](#0-2) 

In contrast, the similar `StateMerkleShardPruner` implements internal batching with a loop that processes entries incrementally: [4](#0-3) 

**Attack Scenario:**

1. An attacker sends high-throughput transactions that update state frequently (within gas limits). According to configuration comments, a 10k transaction block can touch 60k state values. [5](#0-4) 

2. Stale entries accumulate across thousands of versions (e.g., 10,000 versions × 60k entries = 600 million stale entries)

3. Due to operational issues (crash, disk slowdown, restart), one or more shards fall behind the metadata pruner

4. On node restart, `StateKvPruner::new()` initializes all shard pruners sequentially: [6](#0-5) 

5. Each shard attempts to catch up by processing all accumulated stale entries at once
6. Memory consumption: 600M entries × ~48 bytes per key = ~28.8 GB just for deletion keys, plus batch overhead
7. Node runs out of memory and crashes
8. Node cannot restart (persistent DoS condition)

## Impact Explanation

**Severity: Medium**

This vulnerability causes **node unavailability** and falls under the Medium severity category: "State inconsistencies requiring intervention." Specifically:

- **Validator node unavailability**: Affected nodes cannot start up, requiring manual intervention to resolve
- **Liveness impact**: Individual validator nodes become unavailable, though network consensus can continue with remaining validators
- **DoS vector**: Attackers can amplify normal operational issues into complete node failure
- **Recovery complexity**: Requires manual database intervention or restoration from backup

The issue does not directly lead to:
- Consensus safety violations (other nodes continue operating)
- Loss of funds or state corruption
- Network-wide partition (only affects individual nodes)

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability occurs under specific but realistic conditions:

**Natural occurrence scenarios:**
- Shard disk slowdown causing pruner lag
- Node crash during pruning operations
- Network interruptions preventing pruner progress
- Resource exhaustion on specific shard storage

**Attacker-amplified scenarios:**
- Sending high-throughput transactions to maximize stale entries (limited by gas/fees but feasible)
- Exploiting other crash vulnerabilities to force restarts
- Timing attacks during operational maintenance windows

**Frequency factors:**
- Default prune window is 90 million versions for ledger pruning
- Shards can naturally fall behind during high-load periods
- Node restarts are common in production environments (updates, maintenance, crashes)
- The gap only needs to be ~10,000 versions with normal traffic to cause memory issues

## Recommendation

Implement internal batching similar to `StateMerkleShardPruner` to limit memory consumption per iteration:

```rust
pub(in crate::pruner) fn prune(
    &self,
    current_progress: Version,
    target_version: Version,
) -> Result<()> {
    const MAX_ENTRIES_PER_BATCH: usize = 10_000;
    
    let mut progress = current_progress;
    while progress < target_version {
        let mut batch = SchemaBatch::new();
        let mut entries_in_batch = 0;

        let mut iter = self
            .db_shard
            .iter::<StaleStateValueIndexByKeyHashSchema>()?;
        iter.seek(&progress)?;
        
        for item in iter {
            let (index, _) = item?;
            if index.stale_since_version > target_version {
                break;
            }
            
            batch.delete::<StaleStateValueIndexByKeyHashSchema>(&index)?;
            batch.delete::<StateValueByKeyHashSchema>(&(index.state_key_hash, index.version))?;
            
            progress = index.stale_since_version;
            entries_in_batch += 1;
            
            if entries_in_batch >= MAX_ENTRIES_PER_BATCH {
                break;
            }
        }
        
        if entries_in_batch == 0 {
            break;
        }
        
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
            &DbMetadataValue::Version(progress),
        )?;

        self.db_shard.write_schemas(batch)?;
    }
    
    // Final progress update
    let mut batch = SchemaBatch::new();
    batch.put::<DbMetadataSchema>(
        &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
        &DbMetadataValue::Version(target_version),
    )?;
    self.db_shard.write_schemas(batch)
}
```

This approach processes entries in bounded batches of 10,000 entries, writing progress incrementally to prevent memory exhaustion.

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use crate::schema::{
        stale_state_value_index_by_key_hash::StaleStateValueIndexByKeyHashSchema,
        state_value_by_key_hash::StateValueByKeyHashSchema,
    };
    use aptos_crypto::HashValue;
    use aptos_schemadb::DB;
    use aptos_temppath::TempPath;
    use aptos_types::state_store::state_value::StaleStateValueByKeyHashIndex;

    #[test]
    fn test_unbounded_catch_up_memory_exhaustion() {
        // Create test database
        let tmpdir = TempPath::new();
        let db = Arc::new(DB::open(
            tmpdir.path(),
            "test_db",
            vec![
                StaleStateValueIndexByKeyHashSchema::COLUMN_FAMILY_NAME,
                StateValueByKeyHashSchema::COLUMN_FAMILY_NAME,
                DbMetadataSchema::COLUMN_FAMILY_NAME,
            ],
            &Default::default(),
        ).unwrap());

        // Simulate large gap: create 100,000 stale entries
        // In production, this could be millions of entries
        let num_stale_entries = 100_000;
        let start_version = 0;
        
        for i in 0..num_stale_entries {
            let version = start_version + i;
            let index = StaleStateValueByKeyHashIndex {
                stale_since_version: version,
                version: version,
                state_key_hash: HashValue::random(),
            };
            
            db.put::<StaleStateValueIndexByKeyHashSchema>(&index, &()).unwrap();
            db.put::<StateValueByKeyHashSchema>(
                &(index.state_key_hash, version),
                &vec![0u8; 100] // Dummy state value
            ).unwrap();
        }

        // Initialize metadata progress ahead of shard
        let metadata_progress = start_version + num_stale_entries;
        db.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(0),
            &DbMetadataValue::Version(start_version),
        ).unwrap();

        // Attempt to create pruner - this will trigger catch-up
        // With 100k entries, this accumulates ~5MB in batch
        // With 1M entries (realistic production), this would be ~50MB+
        // With 10M+ entries, this causes OOM
        let result = StateKvShardPruner::new(0, db, metadata_progress);
        
        // In a real attack scenario with millions of entries,
        // this would panic with OOM before completing
        assert!(result.is_ok());
    }
}
```

**To reproduce OOM in production-like scenario:**
1. Configure node with sharding enabled
2. Generate high transaction throughput for extended period
3. Introduce artificial delay in one shard's pruner
4. Let shard fall behind by 100k+ versions
5. Restart node
6. Observe memory exhaustion during initialization

## Notes

This vulnerability demonstrates a critical difference in implementation between `StateKvShardPruner` and `StateMerkleShardPruner`. While both perform similar catch-up operations during initialization, only the Merkle pruner implements proper batching controls. The state KV pruner's unbounded iteration violates the "Resource Limits" invariant that all operations must respect memory constraints.

The issue is particularly severe because it occurs during node initialization, making it a persistent DoS condition that prevents node restart without manual database intervention.

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L25-45)
```rust
    pub(in crate::pruner) fn new(
        shard_id: usize,
        db_shard: Arc<DB>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
            metadata_progress,
        )?;
        let myself = Self { shard_id, db_shard };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up state kv shard {shard_id}."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L47-72)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();

        let mut iter = self
            .db_shard
            .iter::<StaleStateValueIndexByKeyHashSchema>()?;
        iter.seek(&current_progress)?;
        for item in iter {
            let (index, _) = item?;
            if index.stale_since_version > target_version {
                break;
            }
            batch.delete::<StaleStateValueIndexByKeyHashSchema>(&index)?;
            batch.delete::<StateValueByKeyHashSchema>(&(index.state_key_hash, index.version))?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
            &DbMetadataValue::Version(target_version),
        )?;

        self.db_shard.write_schemas(batch)
    }
```

**File:** storage/schemadb/src/batch.rs (L127-149)
```rust
/// `SchemaBatch` holds a collection of updates that can be applied to a DB atomically. The updates
/// will be applied in the order in which they are added to the `SchemaBatch`.
#[derive(Debug, Default)]
pub struct SchemaBatch {
    rows: DropHelper<HashMap<ColumnFamilyName, Vec<WriteOp>>>,
    stats: SampledBatchStats,
}

impl SchemaBatch {
    /// Creates an empty batch.
    pub fn new() -> Self {
        Self::default()
    }

    /// keep these on the struct itself so that we don't need to update each call site.
    pub fn put<S: Schema>(&mut self, key: &S::Key, value: &S::Value) -> DbResult<()> {
        <Self as WriteBatch>::put::<S>(self, key, value)
    }

    pub fn delete<S: Schema>(&mut self, key: &S::Key) -> DbResult<()> {
        <Self as WriteBatch>::delete::<S>(self, key)
    }
}
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L58-100)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
        max_nodes_to_prune: usize,
    ) -> Result<()> {
        loop {
            let mut batch = SchemaBatch::new();
            let (indices, next_version) = StateMerklePruner::get_stale_node_indices(
                &self.db_shard,
                current_progress,
                target_version,
                max_nodes_to_prune,
            )?;

            indices.into_iter().try_for_each(|index| {
                batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
                batch.delete::<S>(&index)
            })?;

            let mut done = true;
            if let Some(next_version) = next_version {
                if next_version <= target_version {
                    done = false;
                }
            }

            if done {
                batch.put::<DbMetadataSchema>(
                    &S::progress_metadata_key(Some(self.shard_id)),
                    &DbMetadataValue::Version(target_version),
                )?;
            }

            self.db_shard.write_schemas(batch)?;

            if done {
                break;
            }
        }

        Ok(())
    }
```

**File:** config/src/config/storage_config.rs (L408-410)
```rust
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L124-134)
```rust
        let shard_pruners = if state_kv_db.enabled_sharding() {
            let num_shards = state_kv_db.num_shards();
            let mut shard_pruners = Vec::with_capacity(num_shards);
            for shard_id in 0..num_shards {
                shard_pruners.push(StateKvShardPruner::new(
                    shard_id,
                    state_kv_db.db_shard_arc(shard_id),
                    metadata_progress,
                )?);
            }
            shard_pruners
```
