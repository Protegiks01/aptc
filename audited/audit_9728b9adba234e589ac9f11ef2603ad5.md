# Audit Report

## Title
Fast Sync Storage Wrapper Race Condition Causes Inconsistent Transaction Proof Generation During Status Transitions

## Summary
The `FastSyncStorageWrapper` delegates all `DbReader` operations through `get_read_delegatee()`, which dynamically selects between two databases based on fast sync status. Long-running read operations that create iterators before a status transition can continue reading from the old database while subsequent proof generation calls read from the new database, producing cryptographically inconsistent responses that violate state consistency invariants.

## Finding Description

The `FastSyncStorageWrapper` maintains two separate databases during fast sync bootstrapping:
- `temporary_db_with_genesis`: Contains only genesis data (version 0)
- `db_for_fast_sync`: Contains the restored state snapshot (version N >> 0) [1](#0-0) 

The `DbReader` trait implementation delegates all operations to `get_read_delegatee()`, which internally calls `get_aptos_db_read_ref()`: [2](#0-1) [3](#0-2) 

The delegation macro generates methods that call `get_read_delegatee()` on each invocation: [4](#0-3) [5](#0-4) 

**The Vulnerability**: When the storage service processes transaction requests with size-aware chunking, it creates iterators from one database, then after consuming them, generates proofs by calling `get_transaction_accumulator_range_proof()`. If `finalize_state_snapshot()` executes between these operations, the status changes from STARTED to FINISHED: [6](#0-5) 

This causes:
1. Iterators created read from `temporary_db_with_genesis` (version 0 data)
2. Status transitions to FINISHED during iteration
3. Proof generation reads from `db_for_fast_sync` (version N data) [7](#0-6) [8](#0-7) 

The same issue occurs in `get_transaction_outputs_with_proof_by_size`: [9](#0-8) 

**Result**: The storage service returns a response containing:
- Transactions from version 0 (genesis)
- Accumulator proof from version N (snapshot)
- Transaction infos from version 0
- Mismatched cryptographic proofs

This violates the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs."

## Impact Explanation

**Severity: HIGH**

This vulnerability meets the High severity criteria under "Significant protocol violations" because:

1. **State Synchronization Corruption**: Peer nodes receiving mismatched transaction data and proofs will fail cryptographic verification, causing state sync to abort or enter error states. The accumulator proof cannot verify transactions from a different ledger version.

2. **Network-Wide Impact**: During fast sync bootstrapping, nodes serve data to peers through the storage service. Multiple bootstrapping nodes could simultaneously serve corrupted data, propagating the inconsistency across the network.

3. **Protocol Guarantee Violation**: This directly breaks the fundamental guarantee that all data in a cryptographic proof bundle must originate from the same consistent ledger state, making Merkle proofs unverifiable.

The storage service operates independently and continues serving requests during bootstrapping with no synchronization preventing this race: [10](#0-9) 

## Likelihood Explanation

**Likelihood: MEDIUM**

The vulnerability requires specific timing conditions but is practically exploitable:

**Timing Requirements**:
- A storage service request must start during STARTED status
- The request must use size-aware chunking with iterators (enabled by default)
- `finalize_state_snapshot()` must complete during iterator consumption
- The window spans the duration of iterator processing (configurable via `max_storage_read_wait_time_ms`)

**Favorable Factors for Exploitation**:
1. Fast sync is a one-time event per node but affects ALL bootstrapping nodes simultaneously
2. Storage service requests can be triggered by any peer node
3. The ResponseDataProgressTracker allows configurable time limits (default values could span seconds)
4. Large transaction batches increase the window duration
5. No locks or synchronization prevent concurrent access during status transition

**Real-World Scenario**: When multiple validator nodes bootstrap simultaneously (common during network upgrades or testnet resets), they request data from each other while fast syncing, creating natural conditions for this race.

## Recommendation

Implement atomic database selection for multi-step read operations. Options include:

1. **Session-based locking**: Acquire and hold a read lock on `fast_sync_status` for the entire duration of a multi-step read operation (iterator creation through proof generation).

2. **Database reference caching**: Have the storage service cache the database reference at the start of a request and use that reference for all operations within that request, rather than calling `get_read_delegatee()` multiple times.

3. **Atomic read operations**: Refactor the storage service to use single atomic read operations that return both data and proofs together, ensuring they come from the same database state.

Example fix (option 2):
```rust
// In storage service handler
let db_snapshot = self.storage.get_read_delegatee(); // Get once
let iterator = db_snapshot.get_transaction_iterator(...);
// ... consume iterator ...
let proof = db_snapshot.get_transaction_accumulator_range_proof(...);
```

## Proof of Concept

A full PoC would require:
1. Setting up a node in fast sync mode
2. Triggering storage service requests from a peer during bootstrapping
3. Precisely timing `finalize_state_snapshot()` to occur between iterator creation and proof generation
4. Capturing and verifying the cryptographic inconsistency in the response

The race window can be artificially widened for testing by adding delays in `finalize_state_snapshot()` or by processing large transaction batches that increase iterator consumption time.

## Notes

The vulnerability is rooted in the architectural decision to delegate reads through a status-dependent function that can change between method calls. While the RwLock protects the status variable itself, it does not provide atomicity across the multi-step read operations performed by the storage service. This creates a time-of-check-to-time-of-use (TOCTOU) race condition where the "check" is the first `get_read_delegatee()` call (for iterators) and the "use" is the second `get_read_delegatee()` call (for proofs).

### Citations

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L31-38)
```rust
pub struct FastSyncStorageWrapper {
    // Used for storing genesis data during fast sync
    temporary_db_with_genesis: Arc<AptosDB>,
    // Used for restoring fast sync snapshot and all the read/writes afterwards
    db_for_fast_sync: Arc<AptosDB>,
    // This is for reading the fast_sync status to determine which db to use
    fast_sync_status: Arc<RwLock<FastSyncStatus>>,
}
```

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L126-132)
```rust
    pub(crate) fn get_aptos_db_read_ref(&self) -> &AptosDB {
        if self.is_fast_sync_bootstrap_finished() {
            self.db_for_fast_sync.as_ref()
        } else {
            self.temporary_db_with_genesis.as_ref()
        }
    }
```

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L154-170)
```rust
    fn finalize_state_snapshot(
        &self,
        version: Version,
        output_with_proof: TransactionOutputListWithProofV2,
        ledger_infos: &[LedgerInfoWithSignatures],
    ) -> Result<()> {
        let status = self.get_fast_sync_status();
        assert_eq!(status, FastSyncStatus::STARTED);
        self.get_aptos_db_write_ref().finalize_state_snapshot(
            version,
            output_with_proof,
            ledger_infos,
        )?;
        let mut status = self.fast_sync_status.write();
        *status = FastSyncStatus::FINISHED;
        Ok(())
    }
```

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L188-192)
```rust
impl DbReader for FastSyncStorageWrapper {
    fn get_read_delegatee(&self) -> &dyn DbReader {
        self.get_aptos_db_read_ref()
    }
}
```

**File:** storage/storage-interface/src/lib.rs (L99-111)
```rust
macro_rules! delegate_read {
    ($(
        $(#[$($attr:meta)*])*
        fn $name:ident(&self $(, $arg: ident : $ty: ty)* $(,)?) -> $return_type:ty;
    )+) => {
        $(
            $(#[$($attr)*])*
            fn $name(&self, $($arg: $ty),*) -> $return_type {
                self.get_read_delegatee().$name($($arg),*)
            }
        )+
    };
}
```

**File:** storage/storage-interface/src/lib.rs (L217-246)
```rust
        fn get_transaction_iterator(
            &self,
            start_version: Version,
            limit: u64,
        ) -> Result<Box<dyn Iterator<Item = Result<Transaction>> + '_>>;

        fn get_transaction_info_iterator(
            &self,
            start_version: Version,
            limit: u64,
        ) -> Result<Box<dyn Iterator<Item = Result<TransactionInfo>> + '_>>;

        fn get_events_iterator(
            &self,
            start_version: Version,
            limit: u64,
        ) -> Result<Box<dyn Iterator<Item = Result<Vec<ContractEvent>>> + '_>>;

        fn get_write_set_iterator(
            &self,
            start_version: Version,
            limit: u64,
        ) -> Result<Box<dyn Iterator<Item = Result<WriteSet>> + '_>>;

        fn get_transaction_accumulator_range_proof(
            &self,
            start_version: Version,
            limit: u64,
            ledger_version: Version,
        ) -> Result<TransactionAccumulatorRangeProof>;
```

**File:** state-sync/storage-service/server/src/storage.rs (L373-394)
```rust
        // Get the iterators for the transaction, info, events and persisted auxiliary infos
        let transaction_iterator = self
            .storage
            .get_transaction_iterator(start_version, num_transactions_to_fetch)?;
        let transaction_info_iterator = self
            .storage
            .get_transaction_info_iterator(start_version, num_transactions_to_fetch)?;
        let transaction_events_iterator = if include_events {
            self.storage
                .get_events_iterator(start_version, num_transactions_to_fetch)?
        } else {
            // If events are not included, create a fake iterator (they will be dropped anyway)
            Box::new(std::iter::repeat_n(
                Ok(vec![]),
                num_transactions_to_fetch as usize,
            ))
        };
        let persisted_auxiliary_info_iterator =
            self.storage.get_persisted_auxiliary_info_iterator(
                start_version,
                num_transactions_to_fetch as usize,
            )?;
```

**File:** state-sync/storage-service/server/src/storage.rs (L418-478)
```rust
        while !response_progress_tracker.is_response_complete() {
            match multizip_iterator.next() {
                Some((Ok(transaction), Ok(info), Ok(events), Ok(persisted_auxiliary_info))) => {
                    // Calculate the number of serialized bytes for the data items
                    let num_transaction_bytes = get_num_serialized_bytes(&transaction)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_info_bytes = get_num_serialized_bytes(&info)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_events_bytes = get_num_serialized_bytes(&events)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_auxiliary_info_bytes =
                        get_num_serialized_bytes(&persisted_auxiliary_info).map_err(|error| {
                            Error::UnexpectedErrorEncountered(error.to_string())
                        })?;

                    // Add the data items to the lists
                    let total_serialized_bytes = num_transaction_bytes
                        + num_info_bytes
                        + num_events_bytes
                        + num_auxiliary_info_bytes;
                    if response_progress_tracker
                        .data_items_fits_in_response(true, total_serialized_bytes)
                    {
                        transactions.push(transaction);
                        transaction_infos.push(info);
                        transaction_events.push(events);
                        persisted_auxiliary_infos.push(persisted_auxiliary_info);

                        response_progress_tracker.add_data_item(total_serialized_bytes);
                    } else {
                        break; // Cannot add any more data items
                    }
                },
                Some((Err(error), _, _, _))
                | Some((_, Err(error), _, _))
                | Some((_, _, Err(error), _))
                | Some((_, _, _, Err(error))) => {
                    return Err(Error::StorageErrorEncountered(error.to_string()));
                },
                None => {
                    // Log a warning that the iterators did not contain all the expected data
                    warn!(
                        "The iterators for transactions, transaction infos, events and \
                        persisted auxiliary infos are missing data! Start version: {:?}, \
                        end version: {:?}, num transactions to fetch: {:?}, num fetched: {:?}.",
                        start_version,
                        end_version,
                        num_transactions_to_fetch,
                        transactions.len()
                    );
                    break;
                },
            }
        }

        // Create the transaction info list with proof
        let accumulator_range_proof = self.storage.get_transaction_accumulator_range_proof(
            start_version,
            transactions.len() as u64,
            proof_version,
        )?;
```

**File:** state-sync/storage-service/server/src/storage.rs (L591-708)
```rust
        // Get the iterators for the transaction, info, write set, events,
        // auxiliary data and persisted auxiliary infos.
        let transaction_iterator = self
            .storage
            .get_transaction_iterator(start_version, num_outputs_to_fetch)?;
        let transaction_info_iterator = self
            .storage
            .get_transaction_info_iterator(start_version, num_outputs_to_fetch)?;
        let transaction_write_set_iterator = self
            .storage
            .get_write_set_iterator(start_version, num_outputs_to_fetch)?;
        let transaction_events_iterator = self
            .storage
            .get_events_iterator(start_version, num_outputs_to_fetch)?;
        let persisted_auxiliary_info_iterator = self
            .storage
            .get_persisted_auxiliary_info_iterator(start_version, num_outputs_to_fetch as usize)?;
        let mut multizip_iterator = itertools::multizip((
            transaction_iterator,
            transaction_info_iterator,
            transaction_write_set_iterator,
            transaction_events_iterator,
            persisted_auxiliary_info_iterator,
        ));

        // Initialize the fetched data items
        let mut transactions_and_outputs = vec![];
        let mut transaction_infos = vec![];
        let mut persisted_auxiliary_infos = vec![];

        // Create a response progress tracker
        let mut response_progress_tracker = ResponseDataProgressTracker::new(
            num_outputs_to_fetch,
            max_response_size,
            self.config.max_storage_read_wait_time_ms,
            self.time_service.clone(),
        );

        // Fetch as many transaction outputs as possible
        while !response_progress_tracker.is_response_complete() {
            match multizip_iterator.next() {
                Some((
                    Ok(transaction),
                    Ok(info),
                    Ok(write_set),
                    Ok(events),
                    Ok(persisted_auxiliary_info),
                )) => {
                    // Create the transaction output
                    let output = TransactionOutput::new(
                        write_set,
                        events,
                        info.gas_used(),
                        info.status().clone().into(),
                        TransactionAuxiliaryData::None, // Auxiliary data is no longer supported
                    );

                    // Calculate the number of serialized bytes for the data items
                    let num_transaction_bytes = get_num_serialized_bytes(&transaction)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_info_bytes = get_num_serialized_bytes(&info)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_output_bytes = get_num_serialized_bytes(&output)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_auxiliary_info_bytes =
                        get_num_serialized_bytes(&persisted_auxiliary_info).map_err(|error| {
                            Error::UnexpectedErrorEncountered(error.to_string())
                        })?;

                    // Add the data items to the lists
                    let total_serialized_bytes = num_transaction_bytes
                        + num_info_bytes
                        + num_output_bytes
                        + num_auxiliary_info_bytes;
                    if response_progress_tracker.data_items_fits_in_response(
                        !is_transaction_or_output_request,
                        total_serialized_bytes,
                    ) {
                        transactions_and_outputs.push((transaction, output));
                        transaction_infos.push(info);
                        persisted_auxiliary_infos.push(persisted_auxiliary_info);

                        response_progress_tracker.add_data_item(total_serialized_bytes);
                    } else {
                        break; // Cannot add any more data items
                    }
                },
                Some((Err(error), _, _, _, _))
                | Some((_, Err(error), _, _, _))
                | Some((_, _, Err(error), _, _))
                | Some((_, _, _, Err(error), _))
                | Some((_, _, _, _, Err(error))) => {
                    return Err(Error::StorageErrorEncountered(error.to_string()));
                },
                None => {
                    // Log a warning that the iterators did not contain all the expected data
                    warn!(
                        "The iterators for transactions, transaction infos, write sets, events, \
                        auxiliary data and persisted auxiliary infos are missing data! Start version: {:?}, \
                        end version: {:?}, num outputs to fetch: {:?}, num fetched: {:?}.",
                        start_version, end_version, num_outputs_to_fetch, transactions_and_outputs.len()
                    );
                    break;
                },
            }
        }

        // Create the transaction output list with proof
        let num_fetched_outputs = transactions_and_outputs.len();
        let accumulator_range_proof = if num_fetched_outputs == 0 {
            AccumulatorRangeProof::new_empty() // Return an empty proof if no outputs were fetched
        } else {
            self.storage.get_transaction_accumulator_range_proof(
                start_version,
                num_fetched_outputs as u64,
                proof_version,
            )?
        };
```

**File:** aptos-node/src/state_sync.rs (L274-291)
```rust
    let storage_service_runtime = aptos_runtimes::spawn_named_runtime("stor-server".into(), None);

    // Spawn the state sync storage service servers on the runtime
    let storage_reader = StorageReader::new(
        config.storage_service,
        Arc::clone(&db_rw.reader),
        TimeService::real(),
    );
    let service = StorageServiceServer::new(
        config,
        storage_service_runtime.handle().clone(),
        storage_reader,
        TimeService::real(),
        peers_and_metadata,
        StorageServiceNetworkEvents::new(network_service_events),
        storage_service_listener,
    );
    storage_service_runtime.spawn(service.start());
```
