# Audit Report

## Title
Split-Brain Consensus Safety Violation via OnDiskStorage Race Condition in SafetyData Persistence

## Summary
Multiple validator processes accessing the same OnDiskStorage backend can experience race conditions during SafetyData persistence, potentially causing consensus equivocation (double voting) and violating BFT safety guarantees.

## Finding Description

While the question focuses on `save_config()` for SafetyRulesConfig, the actual vulnerability lies in the **SafetyData persistence mechanism** used by the consensus safety rules. When OnDiskStorage is configured as the SecureBackend, concurrent access from multiple validator processes with the same identity creates a critical race condition. [1](#0-0) 

The `OnDiskStorage::set()` method implements a non-atomic read-modify-write pattern without any concurrency control. When multiple validator processes call this method concurrently to persist SafetyData:

1. Both processes read the current state (e.g., `last_voted_round = 10`)
2. Both verify they can vote on round 11 (checks pass since both see round 10)
3. Both construct votes for round 11 (potentially different blocks)
4. Both call `set_safety_data()` which triggers `OnDiskStorage::set()`
   - Process A: reads file → inserts round 11, vote X → writes file
   - Process B: reads file → inserts round 11, vote Y → **overwrites A's update**
5. Both votes are sent to the network before persistence completes
6. **Result: Equivocation detected** - the validator voted twice on the same round for different blocks [2](#0-1) 

The voting logic constructs and signs the vote (lines 86-89), then persists to storage (line 92). The persistence call goes through PersistentSafetyStorage: [3](#0-2) 

This ultimately invokes the racy `OnDiskStorage::set()` method. The documentation explicitly acknowledges this limitation: [4](#0-3) [5](#0-4) 

Despite these warnings, OnDiskStorage is used in production-like configurations: [6](#0-5) [7](#0-6) 

The split-brain scenario is demonstrated in the test suite: [8](#0-7) 

## Impact Explanation

**Severity: Critical** - This breaks the fundamental consensus safety invariant: "AptosBFT must prevent double-spending and chain splits under < 1/3 Byzantine validators."

When a validator equivocates (votes twice on the same round for different blocks), it can cause:
- **Consensus safety violations**: Different honest validators may commit different blocks at the same height
- **Chain splits**: The network may permanently fork, requiring manual intervention or hardfork
- **Loss of BFT safety**: A single compromised validator exhibiting split-brain behavior could contribute to breaking the 1/3 Byzantine fault tolerance threshold

The Aptos bug bounty categorizes this as **Critical Severity** (up to $1,000,000): "Consensus/Safety violations" and "Non-recoverable network partition (requires hardfork)."

## Likelihood Explanation

**Likelihood: Medium-Low** (but impact warrants attention)

This requires specific conditions:
1. **OnDiskStorage configured as backend** - Used in Docker/Helm configs despite README warnings
2. **Multiple validator processes with same identity** - Requires operator error (accidental duplicate startup) or orchestration failure in containerized environments
3. **Concurrent voting** - Both processes must receive proposals and vote simultaneously

While not trivial to trigger accidentally, containerized deployments (Kubernetes, Docker Swarm) can experience scenarios where:
- Pod restarts don't fully terminate old processes
- Network splits cause orchestrators to start duplicate instances
- Manual operator errors during maintenance

The sanitizer only forbids InMemoryStorage for mainnet, not OnDiskStorage: [9](#0-8) 

## Recommendation

**Immediate Fix:**
1. Add file-based locking to `OnDiskStorage` to prevent concurrent access:

```rust
use fs2::FileExt;

pub struct OnDiskStorage {
    file_path: PathBuf,
    temp_path: TempPath,
    time_service: TimeService,
    lock_file: File, // Add lock file handle
}

impl OnDiskStorage {
    fn read(&self) -> Result<HashMap<String, Value>, Error> {
        self.lock_file.lock_exclusive()?; // Acquire exclusive lock
        // ... existing read logic
        self.lock_file.unlock()?;
        // return data
    }
    
    fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
        self.lock_file.lock_exclusive()?; // Acquire exclusive lock
        // ... existing write logic (already atomic via rename)
        self.lock_file.unlock()?;
        Ok(())
    }
}
```

**Long-term Recommendations:**
1. **Enforce Vault for production**: Update sanitizer to forbid OnDiskStorage for mainnet validators
2. **Add runtime detection**: Implement process-level checks to detect duplicate validator instances and shut down gracefully
3. **Update documentation**: Clarify that OnDiskStorage MUST NOT be used with multiple processes

## Proof of Concept

The vulnerability is demonstrated by the existing twin validator test: [10](#0-9) 

**Reproduction steps:**
1. Start a validator with OnDiskStorage backend
2. Copy the validator identity and storage to a second process
3. Start the second process with the same identity
4. Send different block proposals to both processes for the same round
5. Observe both processes vote on the same round with different votes
6. Other validators will detect equivocation via: [11](#0-10) 

**Expected result**: The validator sends two different votes for the same round, causing `VoteReceptionResult::EquivocateVote` to be logged by other validators, demonstrating the consensus safety violation.

---

**Notes:**
- The question asks about `save_config()` for SafetyRulesConfig, but the actual vulnerability is in SafetyData persistence through OnDiskStorage
- While OnDiskStorage documents its limitations, it remains usable in production configs, creating an operational hazard
- This is partially a known limitation (documented in README), but the lack of runtime protection and continued use in production configs elevates it to a reportable vulnerability
- The fix should prevent split-brain scenarios rather than relying solely on operator best practices

### Citations

**File:** secure/storage/src/on_disk.rs (L16-22)
```rust
/// OnDiskStorage represents a key value store that is persisted to the local filesystem and is
/// intended for single threads (or must be wrapped by a Arc<RwLock<>>). This provides no permission
/// checks and simply offers a proof of concept to unblock building of applications without more
/// complex data stores. Internally, it reads and writes all data to a file, which means that it
/// must make copies of all key material which violates the code base. It violates it because
/// the anticipation is that data stores would securely handle key material. This should not be used
/// in production.
```

**File:** secure/storage/src/on_disk.rs (L85-93)
```rust
    fn set<V: Serialize>(&mut self, key: &str, value: V) -> Result<(), Error> {
        let now = self.time_service.now_secs();
        let mut data = self.read()?;
        data.insert(
            key.to_string(),
            serde_json::to_value(GetResponse::new(value, now))?,
        );
        self.write(&data)
    }
```

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L53-95)
```rust
    pub(crate) fn guarded_construct_and_sign_vote_two_chain(
        &mut self,
        vote_proposal: &VoteProposal,
        timeout_cert: Option<&TwoChainTimeoutCertificate>,
    ) -> Result<Vote, Error> {
        // Exit early if we cannot sign
        self.signer()?;

        let vote_data = self.verify_proposal(vote_proposal)?;
        if let Some(tc) = timeout_cert {
            self.verify_tc(tc)?;
        }
        let proposed_block = vote_proposal.block();
        let mut safety_data = self.persistent_storage.safety_data()?;

        // if already voted on this round, send back the previous vote
        // note: this needs to happen after verifying the epoch as we just check the round here
        if let Some(vote) = safety_data.last_vote.clone() {
            if vote.vote_data().proposed().round() == proposed_block.round() {
                return Ok(vote);
            }
        }

        // Two voting rules
        self.verify_and_update_last_vote_round(
            proposed_block.block_data().round(),
            &mut safety_data,
        )?;
        self.safe_to_vote(proposed_block, timeout_cert)?;

        // Record 1-chain data
        self.observe_qc(proposed_block.quorum_cert(), &mut safety_data);
        // Construct and sign vote
        let author = self.signer()?.author();
        let ledger_info = self.construct_ledger_info_2chain(proposed_block, vote_data.hash())?;
        let signature = self.sign(&ledger_info)?;
        let vote = Vote::new_with_signature(vote_data, author, ledger_info, signature);

        safety_data.last_vote = Some(vote.clone());
        self.persistent_storage.set_safety_data(safety_data)?;

        Ok(vote)
    }
```

**File:** consensus/safety-rules/src/persistent_safety_storage.rs (L150-169)
```rust
    pub fn set_safety_data(&mut self, data: SafetyData) -> Result<(), Error> {
        let _timer = counters::start_timer("set", SAFETY_DATA);
        counters::set_state(counters::EPOCH, data.epoch as i64);
        counters::set_state(counters::LAST_VOTED_ROUND, data.last_voted_round as i64);
        counters::set_state(
            counters::HIGHEST_TIMEOUT_ROUND,
            data.highest_timeout_round as i64,
        );
        counters::set_state(counters::PREFERRED_ROUND, data.preferred_round as i64);

        match self.internal_store.set(SAFETY_DATA, data.clone()) {
            Ok(_) => {
                self.cached_safety_data = Some(data);
                Ok(())
            },
            Err(error) => {
                self.cached_safety_data = None;
                Err(Error::SecureStorageUnexpectedError(error.to_string()))
            },
        }
```

**File:** secure/storage/README.md (L37-42)
```markdown
- `OnDisk`: Similar to InMemory, the OnDisk secure storage implementation provides another
useful testing implementation: an on-disk storage engine, where the storage backend is
implemented using a single file written to local disk. In a similar fashion to the in-memory
storage, on-disk should not be used in production environments as it provides no security
guarantees (e.g., encryption before writing to disk). Moreover, OnDisk storage does not
currently support concurrent data accesses.
```

**File:** docker/compose/aptos-node/validator.yaml (L11-13)
```yaml
    backend:
      type: "on_disk_storage"
      path: secure-data.json
```

**File:** terraform/helm/aptos-node/files/configs/validator-base.yaml (L14-16)
```yaml
    backend:
      type: "on_disk_storage"
      path: secure-data.json
```

**File:** testsuite/testcases/src/twin_validator_test.rs (L14-82)
```rust
pub struct TwinValidatorTest;

impl Test for TwinValidatorTest {
    fn name(&self) -> &'static str {
        "twin validator"
    }
}

impl NetworkLoadTest for TwinValidatorTest {}

#[async_trait]
impl NetworkTest for TwinValidatorTest {
    async fn run<'a>(&self, ctxa: NetworkContextSynchronizer<'a>) -> anyhow::Result<()> {
        {
            let mut ctx_locker = ctxa.ctx.lock().await;
            let ctx = ctx_locker.deref_mut();

            let all_validators_ids = {
                ctx.swarm
                    .read()
                    .await
                    .validators()
                    .map(|v| v.peer_id())
                    .collect::<Vec<_>>()
            };
            let validator_count = all_validators_ids.len();
            let twin_count = 2;

            for i in 0..twin_count {
                let main_id: AccountAddress = all_validators_ids[i];
                let twin_id = all_validators_ids[i + validator_count - twin_count];
                let swarm = ctx.swarm.read().await;
                swarm
                    .validator(twin_id)
                    .unwrap()
                    .clear_storage()
                    .await
                    .context(format!(
                        "Error while clearing storage and stopping {twin_id}"
                    ))?;
                let main_identity = swarm
                    .validator(main_id)
                    .unwrap()
                    .get_identity()
                    .await
                    .context(format!("Error while getting identity for {main_id}"))?;
                swarm
                    .validator(twin_id)
                    .unwrap()
                    .set_identity(main_identity)
                    .await
                    .context(format!("Error while setting identity for {twin_id}"))?;
                swarm
                    .validator(twin_id)
                    .unwrap()
                    .start()
                    .await
                    .context(format!("Error while starting {twin_id}"))?;
                swarm
                    .validator(twin_id)
                    .unwrap()
                    .wait_until_healthy(Instant::now() + Duration::from_secs(300))
                    .await
                    .context(format!("Error while waiting for {twin_id}"))?;
            }
        }
        <dyn NetworkLoadTest>::run(self, ctxa).await
    }
}
```

**File:** config/src/config/safety_rules_config.rs (L86-96)
```rust
            // Verify that the secure backend is appropriate for mainnet validators
            if chain_id.is_mainnet()
                && node_type.is_validator()
                && safety_rules_config.backend.is_in_memory()
            {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "The secure backend should not be set to in memory storage in mainnet!"
                        .to_string(),
                ));
            }
```

**File:** consensus/src/pending_votes.rs (L180-200)
```rust
    pub fn new() -> Self {
        PendingVotes {
            li_digest_to_votes: HashMap::new(),
            maybe_2chain_timeout_votes: None,
            author_to_vote: HashMap::new(),
            echo_timeout: false,
        }
    }

    /// Insert a RoundTimeout and return a TimeoutCertificate if it can be formed
    pub fn insert_round_timeout(
        &mut self,
        round_timeout: &RoundTimeout,
        validator_verifier: &ValidatorVerifier,
    ) -> VoteReceptionResult {
        //
        // Let's check if we can create a TC
        //

        let timeout = round_timeout.two_chain_timeout();
        let signature = round_timeout.signature();
```
