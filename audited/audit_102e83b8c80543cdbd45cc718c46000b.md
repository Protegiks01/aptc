# Audit Report

## Title
RwLock Poisoning in Health Checker Causes Complete Validator Node Termination

## Summary
The health checker's `health_check_data` RwLock uses `aptos_infallible::RwLock`, which calls `.expect()` instead of handling poisoned locks. If any thread panics while holding the write lock, all subsequent lock operations will panic, triggering the global panic handler that terminates the entire validator process with `process::exit(12)`. This is worse than just killing peer connectivityâ€”it kills the entire node.

## Finding Description

The vulnerability exists in the interaction between three architectural components:

**1. RwLock Implementation** [1](#0-0) 

The `aptos_infallible::RwLock` wrapper calls `.expect("Cannot currently handle a poisoned lock")` on both read and write operations. When a `std::sync::RwLock` becomes poisoned (due to a panic while the lock is held), all subsequent lock attempts return a `PoisonError`, which the `.expect()` call converts into an immediate panic.

**2. Health Checker Usage** [2](#0-1) 

The `health_check_data` field is accessed throughout the health checker interface via multiple methods that acquire write locks: [3](#0-2) [4](#0-3) 

**3. Global Panic Handler** [5](#0-4) 

When any panic occurs outside the Move verifier/deserializer, the global panic handler logs the crash and calls `process::exit(12)`, terminating the entire validator node process.

**Cascading Failure Path:**
1. Thread A acquires write lock on `health_check_data`
2. Thread A panics due to any reason (OOM, bug, edge case)
3. The `std::sync::RwLock` becomes poisoned
4. Thread B attempts to acquire the lock (read or write)
5. Thread B's `.expect()` call panics with "Cannot currently handle a poisoned lock"
6. Global panic handler executes `process::exit(12)`
7. **Entire validator node terminates**

The health checker is actively used in the main event loop: [6](#0-5) 

## Impact Explanation

**Critical Severity** - This qualifies as "Total loss of liveness/network availability" per the Aptos bug bounty criteria:

- **Complete Node Termination**: Unlike just losing peer connectivity, the entire validator process exits
- **Unrecoverable**: Requires manual node restart by operator
- **Network Impact**: One less validator participating in consensus
- **Cascading Risk**: If multiple validators encounter the same panic condition, the network could lose significant consensus participation

The impact is magnified because:
- No recovery mechanism exists
- No graceful degradation
- The validator is completely removed from the network until manually restarted
- If exploitable conditions are reproducible, an attacker could repeatedly kill validators

## Likelihood Explanation

**Medium Likelihood** - While the architectural vulnerability is certain, triggering requires a panic to occur while holding the lock:

**Panic Triggers:**
- Out-of-memory during HashMap operations
- Integer overflow in debug builds (though production likely uses release mode)
- Unhandled edge cases or bugs in health checker logic
- Stack overflow conditions
- Any panic in code executed while the lock is held

**Mitigating Factors:**
- Panics are intended to be rare in Rust
- Production code is typically well-tested
- Release builds disable some panic sources (e.g., integer overflow)

**Aggravating Factors:**
- No defensive programming around lock poisoning
- Once poisoned, guaranteed process termination
- No catch_unwind protection in network layer
- Widespread use of this pattern throughout critical components

## Recommendation

Replace `aptos_infallible::RwLock` with proper poison handling in critical paths:

**Option 1 - Handle Poisoning Gracefully:**
```rust
// In crates/aptos-infallible/src/rwlock.rs
pub fn read(&self) -> RwLockReadGuard<'_, T> {
    match self.0.read() {
        Ok(guard) => guard,
        Err(poisoned) => {
            error!("RwLock was poisoned, recovering by clearing poison");
            poisoned.into_inner()
        }
    }
}

pub fn write(&self) -> RwLockWriteGuard<'_, T> {
    match self.0.write() {
        Ok(guard) => guard,
        Err(poisoned) => {
            error!("RwLock was poisoned, recovering by clearing poison");
            poisoned.into_inner()
        }
    }
}
```

**Option 2 - Use parking_lot::RwLock:**
The `parking_lot` crate provides a RwLock that doesn't poison on panics, eliminating the issue entirely.

**Option 3 - Add catch_unwind Protection:**
Wrap critical health checker operations with `std::panic::catch_unwind` to prevent panics from poisoning the lock.

## Proof of Concept

```rust
#[cfg(test)]
mod test_rwlock_poisoning {
    use super::*;
    use aptos_infallible::RwLock;
    use std::sync::Arc;
    use std::thread;

    #[test]
    #[should_panic(expected = "Cannot currently handle a poisoned lock")]
    fn test_health_checker_rwlock_poisoning() {
        let data = Arc::new(RwLock::new(HashMap::<PeerId, HealthCheckData>::new()));
        
        // Thread 1: Panic while holding write lock
        let data_clone = data.clone();
        let handle = thread::spawn(move || {
            let mut guard = data_clone.write();
            guard.insert(PeerId::random(), HealthCheckData::new(1));
            panic!("Simulated panic while holding lock");
        });
        
        // Wait for thread to panic and poison the lock
        let _ = handle.join();
        
        // Thread 2: Attempt to read - this will panic due to poisoning
        // In production, this panic triggers process::exit(12)
        let _read_guard = data.read(); // PANICS HERE
    }
}
```

**Notes:**
- This vulnerability is architectural and affects multiple critical components throughout the codebase that use `aptos_infallible::RwLock`
- The exploitation vector is limited because external attackers cannot directly trigger panics, but internal bugs or resource exhaustion could trigger the cascading failure
- The severity is critical due to complete node termination, even though direct exploitation is difficult
- This represents a **systemic reliability and availability risk** where any panic in a critical code path can cause complete node failure rather than graceful degradation

### Citations

**File:** crates/aptos-infallible/src/rwlock.rs (L19-30)
```rust
    pub fn read(&self) -> RwLockReadGuard<'_, T> {
        self.0
            .read()
            .expect("Cannot currently handle a poisoned lock")
    }

    /// lock the rwlock in write mode
    pub fn write(&self) -> RwLockWriteGuard<'_, T> {
        self.0
            .write()
            .expect("Cannot currently handle a poisoned lock")
    }
```

**File:** network/framework/src/protocols/health_checker/interface.rs (L39-43)
```rust
pub struct HealthCheckNetworkInterface<NetworkClient> {
    health_check_data: RwLock<HashMap<PeerId, HealthCheckData>>,
    network_client: NetworkClient,
    receiver: HealthCheckerNetworkEvents,
}
```

**File:** network/framework/src/protocols/health_checker/interface.rs (L95-101)
```rust
    pub fn create_peer_and_health_data(&mut self, peer_id: PeerId, round: u64) {
        self.health_check_data
            .write()
            .entry(peer_id)
            .and_modify(|health_check_data| health_check_data.round = round)
            .or_insert_with(|| HealthCheckData::new(round));
    }
```

**File:** network/framework/src/protocols/health_checker/interface.rs (L110-116)
```rust
    pub fn increment_peer_round_failure(&mut self, peer_id: PeerId, round: u64) {
        if let Some(health_check_data) = self.health_check_data.write().get_mut(&peer_id) {
            if health_check_data.round <= round {
                health_check_data.failures += 1;
            }
        }
    }
```

**File:** crates/crash-handler/src/lib.rs (L48-57)
```rust
    // Do not kill the process if the panics happened at move-bytecode-verifier.
    // This is safe because the `state::get_state()` uses a thread_local for storing states. Thus the state can only be mutated to VERIFIER by the thread that's running the bytecode verifier.
    //
    // TODO: once `can_unwind` is stable, we should assert it. See https://github.com/rust-lang/rust/issues/92988.
    if state::get_state() == VMState::VERIFIER || state::get_state() == VMState::DESERIALIZER {
        return;
    }

    // Kill the process
    process::exit(12);
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L209-227)
```rust
                conn_event = connection_events.select_next_some() => {
                    match conn_event {
                        ConnectionNotification::NewPeer(metadata, network_id) => {
                            // PeersAndMetadata is a global singleton across all networks; filter connect/disconnect events to the NetworkId that this HealthChecker instance is watching
                            if network_id == self_network_id {
                                self.network_interface.create_peer_and_health_data(
                                    metadata.remote_peer_id, self.round
                                );
                            }
                        }
                        ConnectionNotification::LostPeer(metadata, network_id) => {
                            // PeersAndMetadata is a global singleton across all networks; filter connect/disconnect events to the NetworkId that this HealthChecker instance is watching
                            if network_id == self_network_id {
                                self.network_interface.remove_peer_and_health_data(
                                    &metadata.remote_peer_id
                                );
                            }
                        }
                    }
```
