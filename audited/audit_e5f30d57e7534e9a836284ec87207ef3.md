# Audit Report

## Title
Race Condition in new_epoch() Allows LogicalTime Desynchronization During Epoch Transitions

## Summary
The `new_epoch()` function in `ExecutionProxy` modifies shared state (`self.state`) without acquiring the `write_mutex` lock, creating a race condition with concurrent `sync_to_target()` and `sync_for_duration()` operations. This allows the logical time tracking to become desynchronized with the actual epoch state, potentially causing consensus state inconsistencies.

## Finding Description

The `ExecutionProxy` structure maintains two critical synchronization primitives: [1](#0-0) 

The `sync_to_target()` function acquires `write_mutex` to serialize state sync operations and updates the logical time: [2](#0-1) 

Similarly, `sync_for_duration()` follows the same pattern: [3](#0-2) 

However, `new_epoch()` only modifies `self.state` via the RwLock without coordinating with `write_mutex`: [4](#0-3) 

The vulnerability arises because sync operations can be spawned in concurrent tasks by the `ConsensusObserver`'s `StateSyncManager`: [5](#0-4) [6](#0-5) 

**Race Condition Scenario:**

1. **Thread A (Observer)**: Spawned task begins `sync_to_target(ledger_info: epoch 5, round 150)`
   - Acquires `write_mutex`, `latest_logical_time = (epoch 5, round 100)`
   - Calls `executor.finish()`
   - Begins state synchronization

2. **Thread B (EpochManager)**: Calls `new_epoch()` for epoch 6
   - Writes `self.state = MutableState(epoch 6)` **without acquiring write_mutex**

3. **Thread A continues**:
   - Reads `self.state.read()` at line 199 → now returns **epoch 6** state
   - Calls `payload_manager.notify_commit()` with epoch 6's payload manager but epoch 5's timestamp
   - Completes sync to epoch 5 target
   - Updates `*latest_logical_time = (5, 150)` at line 222 → **sets logical time back to old epoch!**

**Result:** The system now has:
- `write_mutex` (LogicalTime) = (5, 150) ← old epoch
- `self.state` = MutableState(epoch 6) ← new epoch

This violates the **State Consistency** invariant (#4) which requires state transitions to be atomic. The logical time tracking becomes desynchronized from the actual epoch state.

## Impact Explanation

This is a **High Severity** vulnerability per Aptos bug bounty criteria:

1. **Significant Protocol Violation**: The logical time tracking mechanism is used to prevent out-of-order state sync operations. When desynchronized, it can cause:
   - Incorrect ordering of sync operations
   - State sync believing the node is at an older epoch than reality
   - Potential for processing old epoch data in a new epoch context

2. **Consensus State Inconsistency**: Different validators experiencing this race at different times during epoch transitions could have inconsistent views of their sync state, potentially leading to:
   - Divergent sync behavior
   - Liveness issues if nodes cannot agree on epoch state
   - Potential for validator slowdowns or stalls

3. **Epoch Transition Vulnerability**: This specifically affects the critical epoch transition path, one of the most security-sensitive operations in consensus systems.

While this does not directly cause fund loss or chain splits, it represents a "Significant protocol violation" that can lead to "Validator node slowdowns" and state inconsistencies requiring manual intervention.

## Likelihood Explanation

**Likelihood: Medium-High**

The race condition can occur naturally without attacker intervention:

1. **Concurrent Components**: The Consensus Observer runs independently and spawns sync tasks concurrently with epoch transitions
2. **Common Trigger**: During every epoch change, if the observer is actively syncing (fallback mode or commit sync), the race window exists
3. **Realistic Timing**: The race window spans the entire duration of a state sync operation (potentially seconds), making it statistically likely
4. **No Privilege Required**: This can occur through normal node operation without any malicious actor

The attack becomes more likely during:
- Network partitions where nodes fall behind and trigger observer fallback syncing
- Epoch boundaries with active state synchronization
- High transaction throughput scenarios where sync operations are frequent

## Recommendation

Acquire `write_mutex` in `new_epoch()` to coordinate with ongoing sync operations:

```rust
fn new_epoch(
    &self,
    epoch_state: &EpochState,
    payload_manager: Arc<dyn TPayloadManager>,
    transaction_shuffler: Arc<dyn TransactionShuffler>,
    block_executor_onchain_config: BlockExecutorConfigFromOnchain,
    transaction_deduper: Arc<dyn TransactionDeduper>,
    randomness_enabled: bool,
    consensus_onchain_config: OnChainConsensusConfig,
    persisted_auxiliary_info_version: u8,
    network_sender: Arc<NetworkSender>,
) {
    // NEW: Acquire write_mutex to coordinate with sync operations
    let _guard = self.write_mutex.blocking_lock();
    
    *self.state.write() = Some(MutableState {
        validators: epoch_state
            .verifier
            .get_ordered_account_addresses_iter()
            .collect::<Vec<_>>()
            .into(),
        payload_manager,
        transaction_shuffler,
        block_executor_onchain_config,
        transaction_deduper,
        is_randomness_enabled: randomness_enabled,
        consensus_onchain_config,
        persisted_auxiliary_info_version,
        network_sender,
    });
    
    // After updating state, also update logical time to reflect new epoch
    // Note: blocking_lock() is used since new_epoch() is not async
}
```

Alternatively, make `new_epoch()` async and use proper async locking:

```rust
async fn new_epoch(...) {
    let mut latest_logical_time = self.write_mutex.lock().await;
    
    *self.state.write() = Some(MutableState { ... });
    
    // Update logical time to new epoch, round 0
    *latest_logical_time = LogicalTime::new(epoch_state.epoch, 0);
}
```

This would also require updating the trait definition in `StateComputer`.

## Proof of Concept

**Reproduction Steps:**

1. Set up a validator node with Consensus Observer enabled
2. Configure observer to use fallback syncing with aggressive timeout
3. During epoch N, trigger a fallback sync by simulating network delay
4. While the fallback sync is executing `sync_for_duration()`:
   - Initiate epoch N+1 transition via normal consensus path
   - `new_epoch()` will be called for epoch N+1
5. Observe the state after sync completes:
   - Check `write_mutex` logical time → should be (N, R) for some round R
   - Check `self.state` epoch → should be N+1
   - State is now inconsistent

**Rust Test Skeleton:**

```rust
#[tokio::test]
async fn test_new_epoch_race_condition() {
    // Setup ExecutionProxy
    let execution_proxy = /* create ExecutionProxy */;
    
    // Spawn sync_to_target for epoch 5
    let proxy_clone = execution_proxy.clone();
    let sync_handle = tokio::spawn(async move {
        let target = /* LedgerInfo for epoch 5, round 150 */;
        proxy_clone.sync_to_target(target).await
    });
    
    // Small delay to ensure sync started
    tokio::time::sleep(Duration::from_millis(10)).await;
    
    // Call new_epoch for epoch 6 while sync is running
    execution_proxy.new_epoch(
        &epoch_6_state,
        /* ... other parameters ... */
    );
    
    // Wait for sync to complete
    sync_handle.await;
    
    // Verify inconsistency:
    // - write_mutex should have epoch 5 logical time
    // - state should have epoch 6 data
    assert_inconsistent_state(&execution_proxy);
}
```

The test demonstrates that without proper synchronization, the logical time can be set to an old epoch after `new_epoch()` updates to a new epoch, creating the exact race condition described.

---

**Notes**

This vulnerability specifically affects the synchronization between the consensus observer's independent sync operations and epoch transitions managed by the epoch manager. While the epoch manager itself follows a proper sequence (shutdown → sync → new_epoch), the observer spawns concurrent tasks that don't coordinate with this sequence. The fix requires ensuring `new_epoch()` acquires the same lock used by sync operations to maintain atomicity of the state transition.

### Citations

**File:** consensus/src/state_computer.rs (L54-63)
```rust
pub struct ExecutionProxy {
    executor: Arc<dyn BlockExecutorTrait>,
    txn_notifier: Arc<dyn TxnNotifier>,
    state_sync_notifier: Arc<dyn ConsensusNotificationSender>,
    write_mutex: AsyncMutex<LogicalTime>,
    txn_filter_config: Arc<BlockTransactionFilterConfig>,
    state: RwLock<Option<MutableState>>,
    enable_pre_commit: bool,
    secret_share_config: Option<SecretShareConfig>,
}
```

**File:** consensus/src/state_computer.rs (L132-174)
```rust
    async fn sync_for_duration(
        &self,
        duration: Duration,
    ) -> Result<LedgerInfoWithSignatures, StateSyncError> {
        // Grab the logical time lock
        let mut latest_logical_time = self.write_mutex.lock().await;

        // Before state synchronization, we have to call finish() to free the
        // in-memory SMT held by the BlockExecutor to prevent a memory leak.
        self.executor.finish();

        // Inject an error for fail point testing
        fail_point!("consensus::sync_for_duration", |_| {
            Err(anyhow::anyhow!("Injected error in sync_for_duration").into())
        });

        // Invoke state sync to synchronize for the specified duration. Here, the
        // ChunkExecutor will process chunks and commit to storage. However, after
        // block execution and commits, the internal state of the ChunkExecutor may
        // not be up to date. So, it is required to reset the cache of the
        // ChunkExecutor in state sync when requested to sync.
        let result = monitor!(
            "sync_for_duration",
            self.state_sync_notifier.sync_for_duration(duration).await
        );

        // Update the latest logical time
        if let Ok(latest_synced_ledger_info) = &result {
            let ledger_info = latest_synced_ledger_info.ledger_info();
            let synced_logical_time = LogicalTime::new(ledger_info.epoch(), ledger_info.round());
            *latest_logical_time = synced_logical_time;
        }

        // Similarly, after state synchronization, we have to reset the cache of
        // the BlockExecutor to guarantee the latest committed state is up to date.
        self.executor.reset()?;

        // Return the result
        result.map_err(|error| {
            let anyhow_error: anyhow::Error = error.into();
            anyhow_error.into()
        })
    }
```

**File:** consensus/src/state_computer.rs (L177-233)
```rust
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        // Grab the logical time lock and calculate the target logical time
        let mut latest_logical_time = self.write_mutex.lock().await;
        let target_logical_time =
            LogicalTime::new(target.ledger_info().epoch(), target.ledger_info().round());

        // Before state synchronization, we have to call finish() to free the
        // in-memory SMT held by BlockExecutor to prevent a memory leak.
        self.executor.finish();

        // The pipeline phase already committed beyond the target block timestamp, just return.
        if *latest_logical_time >= target_logical_time {
            warn!(
                "State sync target {:?} is lower than already committed logical time {:?}",
                target_logical_time, *latest_logical_time
            );
            return Ok(());
        }

        // This is to update QuorumStore with the latest known commit in the system,
        // so it can set batches expiration accordingly.
        // Might be none if called in the recovery path, or between epoch stop and start.
        if let Some(inner) = self.state.read().as_ref() {
            let block_timestamp = target.commit_info().timestamp_usecs();
            inner
                .payload_manager
                .notify_commit(block_timestamp, Vec::new());
        }

        // Inject an error for fail point testing
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });

        // Invoke state sync to synchronize to the specified target. Here, the
        // ChunkExecutor will process chunks and commit to storage. However, after
        // block execution and commits, the internal state of the ChunkExecutor may
        // not be up to date. So, it is required to reset the cache of the
        // ChunkExecutor in state sync when requested to sync.
        let result = monitor!(
            "sync_to_target",
            self.state_sync_notifier.sync_to_target(target).await
        );

        // Update the latest logical time
        *latest_logical_time = target_logical_time;

        // Similarly, after state synchronization, we have to reset the cache of
        // the BlockExecutor to guarantee the latest committed state is up to date.
        self.executor.reset()?;

        // Return the result
        result.map_err(|error| {
            let anyhow_error: anyhow::Error = error.into();
            anyhow_error.into()
        })
    }
```

**File:** consensus/src/state_computer.rs (L235-262)
```rust
    fn new_epoch(
        &self,
        epoch_state: &EpochState,
        payload_manager: Arc<dyn TPayloadManager>,
        transaction_shuffler: Arc<dyn TransactionShuffler>,
        block_executor_onchain_config: BlockExecutorConfigFromOnchain,
        transaction_deduper: Arc<dyn TransactionDeduper>,
        randomness_enabled: bool,
        consensus_onchain_config: OnChainConsensusConfig,
        persisted_auxiliary_info_version: u8,
        network_sender: Arc<NetworkSender>,
    ) {
        *self.state.write() = Some(MutableState {
            validators: epoch_state
                .verifier
                .get_ordered_account_addresses_iter()
                .collect::<Vec<_>>()
                .into(),
            payload_manager,
            transaction_shuffler,
            block_executor_onchain_config,
            transaction_deduper,
            is_randomness_enabled: randomness_enabled,
            consensus_onchain_config,
            persisted_auxiliary_info_version,
            network_sender,
        });
    }
```

**File:** consensus/src/consensus_observer/observer/state_sync_manager.rs (L134-153)
```rust
        // Spawn a task to sync for the fallback
        let (abort_handle, abort_registration) = AbortHandle::new_pair();
        tokio::spawn(Abortable::new(
            async move {
                // Update the state sync metrics now that we're syncing for the fallback
                metrics::set_gauge_with_label(
                    &metrics::OBSERVER_STATE_SYNC_EXECUTING,
                    metrics::STATE_SYNCING_FOR_FALLBACK,
                    1, // We're syncing for the fallback
                );

                // Get the fallback duration
                let fallback_duration =
                    Duration::from_millis(consensus_observer_config.observer_fallback_duration_ms);

                // Sync for the fallback duration
                let latest_synced_ledger_info = match execution_client
                    .clone()
                    .sync_for_duration(fallback_duration)
                    .await
```

**File:** consensus/src/consensus_observer/observer/state_sync_manager.rs (L207-222)
```rust
        // Spawn a task to sync to the commit decision
        let (abort_handle, abort_registration) = AbortHandle::new_pair();
        tokio::spawn(Abortable::new(
            async move {
                // Update the state sync metrics now that we're syncing to a commit
                metrics::set_gauge_with_label(
                    &metrics::OBSERVER_STATE_SYNC_EXECUTING,
                    metrics::STATE_SYNCING_TO_COMMIT,
                    1, // We're syncing to a commit decision
                );

                // Sync to the commit decision
                if let Err(error) = execution_client
                    .clone()
                    .sync_to_target(commit_decision.commit_proof().clone())
                    .await
```
