# Audit Report

## Title
Silent Message Drop in gRPC Network Service Causes Sharded Executor Deadlock

## Summary
The `simple_msg_exchange` implementation in the gRPC network service returns a successful `Empty` response even when no handler is registered for the message type, causing the message to be silently dropped. This leads to deadlocks in the sharded block executor where clients wait indefinitely for responses that will never arrive.

## Finding Description

The vulnerability exists in the `simple_msg_exchange` server implementation where messages are routed to registered handlers based on message type. [1](#0-0) 

When a message arrives, the server checks if a handler is registered for the message type. If no handler exists, the error is only logged, but the function still returns `Ok(Response::new(Empty {}))`, indicating successful message delivery to the client. [2](#0-1) 

The client-side implementation treats this successful response as confirmation that the message was delivered and processed: [3](#0-2) 

This breaks the **Deterministic Execution** and **Liveness** invariants because:

1. **Critical execution messages are silently dropped**: The sharded block executor uses this network service to send execution commands and receive results. [4](#0-3) 

2. **Clients wait indefinitely**: When waiting for execution results, the client blocks on `recv()` which will never return if the command message was dropped: [5](#0-4) 

3. **Race conditions during startup**: The test suite explicitly acknowledges a timing issue where messages can arrive before handlers are registered: [6](#0-5) 

**Attack Scenarios:**

1. **Startup Race Condition**: An attacker sends execution commands to a shard immediately after it starts but before all handlers are registered. The messages are accepted (200 OK) but silently dropped, causing the coordinator to hang waiting for results.

2. **Configuration Error Exploitation**: If a shard is misconfigured with the wrong message type strings or missing handler registrations, legitimate execution requests are silently dropped, causing system-wide execution failures.

3. **Message Type Confusion**: Sending messages with incorrect `message_type` values results in silent drops rather than explicit errors, making debugging extremely difficult.

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos bug bounty program criteria:

- **State inconsistencies requiring intervention**: When execution commands are silently dropped, the sharded executor enters a deadlock state where the coordinator waits indefinitely for results. This requires manual intervention to restart affected nodes.

- **Validator node availability**: While not a total network failure, affected executor shards become unresponsive, degrading overall system performance and potentially preventing block execution entirely if enough shards are affected.

- **Silent failure mode**: The most concerning aspect is that failures are completely silent from the client perspective. The gRPC call succeeds, metrics show successful message delivery, but the message is actually dropped. This makes detection and diagnosis extremely difficult.

The impact does not reach High/Critical severity because:
- It does not directly cause loss of funds
- It does not break consensus safety guarantees
- It is limited to the sharded executor service rather than core consensus
- It can be mitigated by proper handler registration and startup sequencing

## Likelihood Explanation

**Likelihood: Medium-High**

This vulnerability is highly likely to occur in production because:

1. **Race condition is acknowledged but not fixed**: The test code includes explicit TODOs about the race condition and uses sleep delays as workarounds, indicating the developers are aware of the timing issue but haven't implemented a proper fix. [7](#0-6) 

2. **String-based message type routing**: The system uses string-based message type identifiers (`format!("execute_command_{}", shard_id)`) which are error-prone and can lead to mismatches between sender and receiver. [8](#0-7) 

3. **Multiple registration points**: Handlers are registered in multiple places (RemoteCoordinatorClient, RemoteCrossShardClient, RemoteStateViewService) making it easy to miss a registration or introduce typos.

4. **No health check mechanism**: The comments indicate there's "no easy way to know if/when the server has started successfully", leaving a window where clients can send messages before the server is ready. [9](#0-8) 

5. **Production deployment complexity**: In distributed deployments with multiple shards starting asynchronously, the race window increases significantly.

## Recommendation

**Immediate Fix: Return gRPC error when handler not found**

The server should return a gRPC error status instead of `Ok(Empty {})` when no handler is registered:

```rust
async fn simple_msg_exchange(
    &self,
    request: Request<NetworkMessage>,
) -> Result<Response<Empty>, Status> {
    let _timer = NETWORK_HANDLER_TIMER
        .with_label_values(&[&self.self_addr.to_string(), "inbound_msgs"])
        .start_timer();
    let remote_addr = request.remote_addr();
    let network_message = request.into_inner();
    let msg = Message::new(network_message.message);
    let message_type = MessageType::new(network_message.message_type);

    if let Some(handler) = self.inbound_handlers.lock().unwrap().get(&message_type) {
        // Send the message to the registered handler
        handler.send(msg).unwrap();
        Ok(Response::new(Empty {}))
    } else {
        error!(
            "No handler registered for sender: {:?} and msg type {:?}",
            remote_addr, message_type
        );
        // Return error instead of Ok
        Err(Status::not_found(format!(
            "No handler registered for message type: {:?}",
            message_type
        )))
    }
}
```

**Additional Mitigations:**

1. **Implement server readiness check**: Add a health check endpoint that confirms all expected handlers are registered before marking the server as ready.

2. **Client-side retry with backoff**: Implement retry logic in the client's `send_message` as indicated by the TODO comment. [10](#0-9) 

3. **Handler registration validation**: Assert at startup that all required handlers are registered before starting the gRPC server.

4. **Add timeouts to blocking receives**: Replace `rx.recv().unwrap()` with `rx.recv_timeout()` to prevent indefinite hangs.

5. **Strongly-typed message types**: Replace string-based message types with enums to prevent typos and mismatches.

## Proof of Concept

```rust
// Add this test to secure/net/src/grpc_network_service/mod.rs

#[test]
fn test_message_drop_on_missing_handler() {
    use aptos_config::utils;
    use std::{
        net::{IpAddr, Ipv4Addr},
        thread,
        time::Duration,
    };

    let server_addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), utils::get_available_port());
    let server_handlers: Arc<Mutex<HashMap<MessageType, Sender<Message>>>> =
        Arc::new(Mutex::new(HashMap::new()));

    // Start server with NO handlers registered
    let server = GRPCNetworkMessageServiceServerWrapper::new(server_handlers, server_addr);

    let rt = Runtime::new().unwrap();
    let (server_shutdown_tx, server_shutdown_rx) = oneshot::channel();
    server.start(
        &rt,
        "test server".to_string(),
        server_addr,
        1000,
        server_shutdown_rx,
    );

    thread::sleep(Duration::from_millis(100));

    let mut grpc_client = GRPCNetworkMessageServiceClientWrapper::new(&rt, server_addr);
    let client_addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), utils::get_available_port());
    
    // Send message to unregistered handler - this should fail but currently succeeds
    rt.block_on(async {
        grpc_client
            .send_message(
                client_addr,
                Message::new(b"test message".to_vec()),
                &MessageType::new("unregistered_type".to_string()),
            )
            .await;
    });
    
    // The message was "successfully" sent but silently dropped
    // In real usage, any code waiting for a response would hang indefinitely
    
    server_shutdown_tx.send(()).unwrap();
}
```

**Expected behavior**: The `send_message` call should panic with a gRPC error indicating no handler is registered.

**Actual behavior**: The `send_message` call succeeds, returning `Ok(())`, even though the message is dropped and logged as an error on the server side.

**Notes**

This vulnerability is particularly dangerous in the sharded block executor context because:

1. The executor relies on request-response patterns where the coordinator sends execution commands and waits for results
2. Silent message drops break this pattern, causing deadlocks that appear as "hung" nodes
3. The error handling is insufficient - logging alone doesn't prevent the system failure
4. The race condition is a known issue (acknowledged in test comments) but hasn't been properly addressed

The fix is straightforward: return gRPC errors for unregistered handlers instead of silent success responses. This allows clients to detect failures immediately and implement proper retry/fallback logic.

### Citations

**File:** secure/net/src/grpc_network_service/mod.rs (L69-74)
```rust
        // NOTE: (1) serve_with_shutdown() starts the server, if successful the task does not return
        //           till the server is shutdown. Hence this should be called as a separate
        //           non-blocking task. Signal handler 'server_shutdown_rx' is needed to shutdown
        //           the server
        //       (2) There is no easy way to know if/when the server has started successfully. Hence
        //           we may need to implement a healthcheck service to check if the server is up
```

**File:** secure/net/src/grpc_network_service/mod.rs (L93-115)
```rust
    async fn simple_msg_exchange(
        &self,
        request: Request<NetworkMessage>,
    ) -> Result<Response<Empty>, Status> {
        let _timer = NETWORK_HANDLER_TIMER
            .with_label_values(&[&self.self_addr.to_string(), "inbound_msgs"])
            .start_timer();
        let remote_addr = request.remote_addr();
        let network_message = request.into_inner();
        let msg = Message::new(network_message.message);
        let message_type = MessageType::new(network_message.message_type);

        if let Some(handler) = self.inbound_handlers.lock().unwrap().get(&message_type) {
            // Send the message to the registered handler
            handler.send(msg).unwrap();
        } else {
            error!(
                "No handler registered for sender: {:?} and msg type {:?}",
                remote_addr, message_type
            );
        }
        Ok(Response::new(Empty {}))
    }
```

**File:** secure/net/src/grpc_network_service/mod.rs (L150-150)
```rust
        // TODO: Retry with exponential backoff on failures
```

**File:** secure/net/src/grpc_network_service/mod.rs (L151-159)
```rust
        match self.remote_channel.simple_msg_exchange(request).await {
            Ok(_) => {},
            Err(e) => {
                panic!(
                    "Error '{}' sending message to {} on node {:?}",
                    e, self.remote_addr, sender_addr
                );
            },
        }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L163-172)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        trace!("RemoteExecutorClient Waiting for results");
        let mut results = vec![];
        for rx in self.result_rxs.iter() {
            let received_bytes = rx.recv().unwrap().to_bytes();
            let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
            results.push(result.inner?);
        }
        Ok(results)
    }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L194-206)
```rust
            let senders = self.command_txs.clone();
            let execution_request = RemoteExecutionRequest::ExecuteBlock(ExecuteBlockCommand {
                sub_blocks,
                concurrency_level: concurrency_level_per_shard,
                onchain_config: onchain_config.clone(),
            });

            senders[shard_id]
                .lock()
                .unwrap()
                .send(Message::new(bcs::to_bytes(&execution_request).unwrap()))
                .unwrap();
        }
```

**File:** secure/net/src/network_controller/mod.rs (L199-204)
```rust
        network_controller1.start();
        network_controller2.start();

        // wait for the server to be ready to serve
        // TODO: We need to pass this test without this sleep
        thread::sleep(std::time::Duration::from_millis(100));
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L32-36)
```rust
        let execute_command_type = format!("execute_command_{}", shard_id);
        let execute_result_type = format!("execute_result_{}", shard_id);
        let command_rx = controller.create_inbound_channel(execute_command_type);
        let result_tx =
            controller.create_outbound_channel(coordinator_address, execute_result_type);
```
