# Audit Report

## Title
Lack of Validation for Batch Request Configuration Parameters Enabling Network Amplification Attacks

## Summary
The `QuorumStoreConfig` structure lacks validation for critical batch request parameters (`batch_request_num_peers`, `batch_request_retry_limit`, `batch_request_retry_interval_ms`, and `batch_request_rpc_timeout_ms`), allowing validator operators to set extreme values that can generate massive network traffic amplification during batch fetch operations, degrading network availability during attacks or network disruptions.

## Finding Description

The quorum store batch request configuration parameters have no bounds checking or validation in the configuration sanitizer. [1](#0-0) 

The `QuorumStoreConfig::sanitize()` method only validates send/recv batch limits and batch/total limits, but completely omits validation for batch request retry parameters. [2](#0-1) 

These unvalidated parameters are passed directly to `BatchRequester::new()` in the `create_batch_store()` function. [3](#0-2) 

During batch fetch operations, the `BatchRequester::request_batch()` method uses these parameters to control the retry loop, sending requests to `request_num_peers` peers every `retry_interval_ms` for up to `retry_limit` attempts. [4](#0-3) 

The `next_request_peers()` method cycles through available signers, allowing requests to the same peers multiple times if `num_peers` exceeds the validator count. [5](#0-4) 

**Attack Scenario:**
A validator operator (malicious, compromised, or misconfigured) could set:
- `batch_request_num_peers` = 1000 (vs default 5)
- `batch_request_retry_limit` = 10000 (vs default 10)  
- `batch_request_retry_interval_ms` = 10 (vs default 500)

For each missing batch, this would generate: 1000 peers × 10000 retries = 10 million request attempts over 100 seconds. During Byzantine attacks where batches are withheld, or network partitions causing batch unavailability, this creates a severe amplification effect that overwhelms the validator network.

## Impact Explanation

This qualifies as **Medium severity** per Aptos bug bounty criteria due to validator network performance degradation and amplification of attack traffic. While it doesn't directly cause fund loss or consensus safety violations, it creates significant availability issues:

- Network congestion affecting consensus liveness during attacks
- Resource exhaustion (CPU, memory, bandwidth) on peer validators receiving excessive requests
- Amplification factor of 200x in peer requests and 1000x in retry attempts compared to defaults
- Coordinated exploitation across multiple misconfigured validators could severely degrade network performance

This aligns with Medium severity: "State inconsistencies requiring intervention" and network availability degradation that requires operator intervention to resolve.

## Likelihood Explanation

**Likelihood: Medium**

While this requires validator operator access (either through misconfiguration, compromise, or malicious intent), several factors increase likelihood:

1. **No validation barriers**: The absence of any bounds checking means accidental misconfiguration goes undetected
2. **Amplification during attacks**: The impact is most severe during Byzantine attacks or network issues, which are realistic threat scenarios
3. **Defense-in-depth failure**: Configuration validation is a fundamental security control that should exist regardless of operator trust level
4. **Cascading effect**: One misconfigured validator can impact the entire network during batch unavailability scenarios

## Recommendation

Add validation in `QuorumStoreConfig::sanitize()` to enforce reasonable bounds:

```rust
fn sanitize_batch_request_limits(
    sanitizer_name: &str,
    config: &QuorumStoreConfig,
) -> Result<(), Error> {
    // Validate batch_request_num_peers
    if config.batch_request_num_peers == 0 {
        return Err(Error::ConfigSanitizerFailed(
            sanitizer_name.to_owned(),
            "batch_request_num_peers must be greater than 0".to_string(),
        ));
    }
    if config.batch_request_num_peers > 50 {
        return Err(Error::ConfigSanitizerFailed(
            sanitizer_name.to_owned(),
            format!("batch_request_num_peers too high: {} (max: 50)", config.batch_request_num_peers),
        ));
    }

    // Validate batch_request_retry_limit
    if config.batch_request_retry_limit > 100 {
        return Err(Error::ConfigSanitizerFailed(
            sanitizer_name.to_owned(),
            format!("batch_request_retry_limit too high: {} (max: 100)", config.batch_request_retry_limit),
        ));
    }

    // Validate retry interval is reasonable
    if config.batch_request_retry_interval_ms < 100 {
        return Err(Error::ConfigSanitizerFailed(
            sanitizer_name.to_owned(),
            format!("batch_request_retry_interval_ms too low: {} (min: 100)", config.batch_request_retry_interval_ms),
        ));
    }

    // Validate RPC timeout is reasonable
    if config.batch_request_rpc_timeout_ms < 1000 || config.batch_request_rpc_timeout_ms > 30000 {
        return Err(Error::ConfigSanitizerFailed(
            sanitizer_name.to_owned(),
            format!("batch_request_rpc_timeout_ms out of range: {} (range: 1000-30000)", config.batch_request_rpc_timeout_ms),
        ));
    }

    Ok(())
}
```

Call this from the main `sanitize()` method:
```rust
Self::sanitize_batch_request_limits(&sanitizer_name, &node_config.consensus.quorum_store)?;
```

## Proof of Concept

Create a test demonstrating the lack of validation:

```rust
#[cfg(test)]
mod test {
    use super::*;
    use crate::config::ConsensusConfig;

    #[test]
    fn test_batch_request_params_lack_validation() {
        // Create a node config with extreme batch request values
        let node_config = NodeConfig {
            consensus: ConsensusConfig {
                quorum_store: QuorumStoreConfig {
                    batch_request_num_peers: 10000, // Extreme value
                    batch_request_retry_limit: 100000, // Extreme value
                    batch_request_retry_interval_ms: 1, // Too aggressive
                    ..Default::default()
                },
                ..Default::default()
            },
            ..Default::default()
        };

        // Current behavior: sanitize passes with extreme values
        let result = QuorumStoreConfig::sanitize(
            &node_config,
            NodeType::Validator,
            Some(ChainId::mainnet()),
        );
        
        // This currently PASSES when it should FAIL
        // Demonstrates missing validation
        assert!(result.is_ok(), "Extreme batch request params should be rejected but currently pass validation");
        
        // After fix, this test should fail, proving validation now exists
    }
}
```

To demonstrate the amplification, calculate the request volume:
- Default: 5 peers × 10 retries × 500ms = 50 requests over 5 seconds per batch
- Malicious: 10000 peers × 100000 retries × 1ms = 1 billion requests over 100 seconds per batch
- Amplification: 20,000,000x increase in total request volume

## Notes

This vulnerability represents a **defense-in-depth failure** where configuration validation should prevent extreme values regardless of operator trust level. While validator operators are generally trusted, defense-in-depth principles and the principle of least privilege dictate that all user-controllable inputs—including configuration files—should have appropriate bounds checking to prevent accidental or malicious misconfiguration from degrading system security and availability.

### Citations

**File:** config/src/config/quorum_store_config.rs (L84-87)
```rust
    pub batch_request_num_peers: usize,
    pub batch_request_retry_limit: usize,
    pub batch_request_retry_interval_ms: usize,
    pub batch_request_rpc_timeout_ms: usize,
```

**File:** config/src/config/quorum_store_config.rs (L253-271)
```rust
impl ConfigSanitizer for QuorumStoreConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();

        // Sanitize the send/recv batch limits
        Self::sanitize_send_recv_batch_limits(
            &sanitizer_name,
            &node_config.consensus.quorum_store,
        )?;

        // Sanitize the batch total limits
        Self::sanitize_batch_total_limits(&sanitizer_name, &node_config.consensus.quorum_store)?;

        Ok(())
    }
```

**File:** consensus/src/quorum_store/quorum_store_builder.rs (L246-255)
```rust
        let batch_requester = BatchRequester::new(
            self.epoch,
            self.author,
            self.config.batch_request_num_peers,
            self.config.batch_request_retry_limit,
            self.config.batch_request_retry_interval_ms,
            self.config.batch_request_rpc_timeout_ms,
            self.network_sender.clone(),
            self.verifier.clone(),
        );
```

**File:** consensus/src/quorum_store/batch_requester.rs (L40-64)
```rust
    fn next_request_peers(&mut self, num_peers: usize) -> Option<Vec<PeerId>> {
        let signers = self.signers.lock();
        if self.num_retries == 0 {
            let mut rng = rand::thread_rng();
            // make sure nodes request from the different set of nodes
            self.next_index = rng.r#gen::<usize>() % signers.len();
            counters::SENT_BATCH_REQUEST_COUNT.inc_by(num_peers as u64);
        } else {
            counters::SENT_BATCH_REQUEST_RETRY_COUNT.inc_by(num_peers as u64);
        }
        if self.num_retries < self.retry_limit {
            self.num_retries += 1;
            let ret = signers
                .iter()
                .cycle()
                .skip(self.next_index)
                .take(num_peers)
                .cloned()
                .collect();
            self.next_index = (self.next_index + num_peers) % signers.len();
            Some(ret)
        } else {
            None
        }
    }
```

**File:** consensus/src/quorum_store/batch_requester.rs (L101-132)
```rust
    pub(crate) async fn request_batch(
        &self,
        digest: HashValue,
        expiration: u64,
        responders: Arc<Mutex<BTreeSet<PeerId>>>,
        mut subscriber_rx: oneshot::Receiver<PersistedValue<BatchInfoExt>>,
    ) -> ExecutorResult<Vec<SignedTransaction>> {
        let validator_verifier = self.validator_verifier.clone();
        let mut request_state = BatchRequesterState::new(responders, self.retry_limit);
        let network_sender = self.network_sender.clone();
        let request_num_peers = self.request_num_peers;
        let my_peer_id = self.my_peer_id;
        let epoch = self.epoch;
        let retry_interval = Duration::from_millis(self.retry_interval_ms as u64);
        let rpc_timeout = Duration::from_millis(self.rpc_timeout_ms as u64);

        monitor!("batch_request", {
            let mut interval = time::interval(retry_interval);
            let mut futures = FuturesUnordered::new();
            let request = BatchRequest::new(my_peer_id, epoch, digest);
            loop {
                tokio::select! {
                    _ = interval.tick() => {
                        // send batch request to a set of peers of size request_num_peers
                        if let Some(request_peers) = request_state.next_request_peers(request_num_peers) {
                            for peer in request_peers {
                                futures.push(network_sender.request_batch(request.clone(), peer, rpc_timeout));
                            }
                        } else if futures.is_empty() {
                            // end the loop when the futures are drained
                            break;
                        }
```
