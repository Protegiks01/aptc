# Audit Report

## Title
Indexer Event Batch Insertion Vulnerable to Memory Exhaustion Due to Unbounded JSONB Data Size

## Summary
The indexer's event batch insertion mechanism chunks data based only on parameter count limits, not actual data size. This allows multi-gigabyte INSERT statements when processing transactions with maximum-sized event payloads, potentially causing memory exhaustion, protocol message size violations, and indexer service disruption.

## Finding Description

The `Event::from_events()` function returns a `Vec<EventModel>` that is subsequently inserted via `insert_events()` in the default processor. [1](#0-0) 

The insertion uses chunking to respect Diesel's parameter limit: [2](#0-1) 

The chunking function only considers parameter count: [3](#0-2) 

**The Critical Flaw:**

With `MAX_DIESEL_PARAM_SIZE = 65,535` and `EventModel` having 8 fields, each chunk can contain up to 8,191 events. However, each event's `data` field is JSONB with a maximum size of 1 MB per event (enforced at transaction validation): [4](#0-3) 

Per transaction, up to 10 MB of event data is allowed: [5](#0-4) 

The indexer processes transactions in batches of 500 (default): [6](#0-5) 

**Worst Case Calculation:**
- Batch size: 500 transactions
- Per transaction: 10 MB of event data (10 events Ã— 1 MB each)
- Total event data per batch: 5 GB
- Number of events: ~5,000
- Chunk size: All fit in one chunk (5,000 < 8,191)
- **Result: Single INSERT statement with ~5 GB of JSONB data**

This violates PostgreSQL practical limits:
1. Protocol message size constraints (~1 GB typical limit)
2. Memory consumption during query construction in Rust
3. Memory consumption during query parsing in PostgreSQL
4. Potential statement timeout

The events table schema stores data as JSONB without size constraints: [7](#0-6) 

## Impact Explanation

**Medium Severity** - This issue affects indexer availability, qualifying as "State inconsistencies requiring intervention" under the bug bounty program's Medium severity category.

**Impact Scope:**
- **Indexer Service Disruption**: Memory exhaustion or protocol violations can crash the indexer process
- **Query API Unavailability**: Applications depending on historical transaction queries lose functionality
- **Operational Intervention Required**: Manual restart and potential batch size reconfiguration needed

**NOT Affected:**
- Consensus layer (validators continue operating normally)
- Transaction processing (blockchain state unaffected)
- Fund security (no financial impact)
- Validator nodes (indexer is separate service)

The indexer is an auxiliary service providing query functionality. While its failure doesn't compromise blockchain integrity, it affects the ecosystem's operational capabilities and user experience.

## Likelihood Explanation

**Moderate Likelihood:**

**Natural Occurrence:**
- High-activity periods with event-heavy contracts
- Batch processing of historical blocks with many large events
- DeFi protocols emitting detailed event logs

**Amplification Scenario:**
An attacker could deliberately create transactions that:
1. Generate maximum allowed events (10 MB per transaction)
2. Use contracts designed to emit large structured data
3. Coordinate multiple such transactions

**Constraints:**
- Requires paying gas for legitimate transactions
- Events must be valid contract outputs (not arbitrary data)
- Attack cost scales with gas prices
- Only affects indexer, not core blockchain

**Real-World Triggers:**
- Popular NFT mints with metadata events
- Large DEX swaps with detailed event logs
- Governance proposals with extensive parameter data
- Airdrops to many recipients

## Recommendation

Implement data size-aware chunking for event insertion:

```rust
// In crates/indexer/src/database.rs
pub const MAX_CHUNK_SIZE_BYTES: usize = 100 * 1024 * 1024; // 100 MB limit

pub fn get_chunks_with_size_limit<T>(
    items: &[T],
    column_count: usize,
    size_estimator: impl Fn(&T) -> usize,
) -> Vec<(usize, usize)> {
    let max_items_by_params = MAX_DIESEL_PARAM_SIZE as usize / column_count;
    let mut chunks = vec![];
    let mut start = 0;
    
    while start < items.len() {
        let mut end = start;
        let mut current_size = 0;
        
        while end < items.len() 
            && (end - start) < max_items_by_params 
            && current_size < MAX_CHUNK_SIZE_BYTES 
        {
            current_size += size_estimator(&items[end]);
            end += 1;
        }
        
        // Ensure at least one item per chunk
        if end == start {
            end = start + 1;
        }
        
        chunks.push((start, end));
        start = end;
    }
    chunks
}

// Update insert_events to use size-aware chunking
fn insert_events(
    conn: &mut PgConnection,
    items_to_insert: &[EventModel],
) -> Result<(), diesel::result::Error> {
    use schema::events::dsl::*;
    
    let size_estimator = |event: &EventModel| {
        // Estimate: base fields + JSON data size
        100 + event.data.to_string().len()
    };
    
    let chunks = get_chunks_with_size_limit(
        items_to_insert,
        EventModel::field_count(),
        size_estimator,
    );
    
    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::events::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict((account_address, creation_number, sequence_number))
                .do_update()
                .set((
                    inserted_at.eq(excluded(inserted_at)),
                    event_index.eq(excluded(event_index)),
                )),
            None,
        )?;
    }
    Ok(())
}
```

**Additional Safeguards:**
1. Configure PostgreSQL `statement_timeout` for indexer connection pool
2. Add monitoring for large event batches
3. Implement backpressure when batch sizes exceed thresholds
4. Consider streaming inserts for very large batches

## Proof of Concept

```rust
// Test to demonstrate the issue
#[cfg(test)]
mod test_large_event_batch {
    use super::*;
    use serde_json::json;
    
    #[test]
    fn test_large_event_data_exceeds_practical_limits() {
        // Create events with maximum allowed data size
        let large_data = json!({
            "data": "x".repeat(1_000_000) // 1 MB JSON string
        });
        
        // Simulate 500 transactions with 10 events each (10 MB per transaction)
        let mut events = vec![];
        for tx_version in 0..500 {
            for event_idx in 0..10 {
                events.push(EventModel {
                    sequence_number: event_idx,
                    creation_number: tx_version,
                    account_address: "0x1".to_string(),
                    transaction_version: tx_version,
                    transaction_block_height: tx_version,
                    type_: "test::Event".to_string(),
                    data: large_data.clone(),
                    event_index: Some(event_idx),
                });
            }
        }
        
        // Calculate chunk size
        let chunks = get_chunks(events.len(), EventModel::field_count());
        
        // Verify the problem: 5000 events fit in one chunk
        assert_eq!(chunks.len(), 1);
        assert_eq!(chunks[0], (0, 5000));
        
        // Calculate approximate data size
        let total_data_size: usize = events.iter()
            .map(|e| e.data.to_string().len())
            .sum();
        
        // Demonstrate: ~5 GB in a single INSERT
        println!("Total data size: {} GB", total_data_size / (1024 * 1024 * 1024));
        assert!(total_data_size > 4_000_000_000); // > 4 GB
        
        // This would cause memory exhaustion when building the INSERT query
    }
}
```

## Notes

This vulnerability specifically affects the **indexer service**, not the core blockchain consensus. The indexer is an auxiliary component that provides query APIs for transaction history. While its disruption impacts user experience and application functionality, it does not compromise blockchain integrity, consensus safety, or fund security.

The issue stems from the mismatch between parameter-based chunking (which prevents Diesel query construction errors) and actual data size considerations (which affect memory usage and protocol limits). Event data size is validated at the transaction level before consensus, ensuring malicious oversized events cannot enter the system. However, the aggregation of many legitimate large events during batch processing can exceed PostgreSQL's practical operational limits.

### Citations

**File:** crates/indexer/src/models/events.rs (L61-78)
```rust
    pub fn from_events(
        events: &[APIEvent],
        transaction_version: i64,
        transaction_block_height: i64,
    ) -> Vec<Self> {
        events
            .iter()
            .enumerate()
            .map(|(index, event)| {
                Self::from_event(
                    event,
                    transaction_version,
                    transaction_block_height,
                    index as i64,
                )
            })
            .collect::<Vec<EventModel>>()
    }
```

**File:** crates/indexer/src/processors/default_processor.rs (L276-297)
```rust
fn insert_events(
    conn: &mut PgConnection,
    items_to_insert: &[EventModel],
) -> Result<(), diesel::result::Error> {
    use schema::events::dsl::*;
    let chunks = get_chunks(items_to_insert.len(), EventModel::field_count());
    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::events::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict((account_address, creation_number, sequence_number))
                .do_update()
                .set((
                    inserted_at.eq(excluded(inserted_at)),
                    event_index.eq(excluded(event_index)),
                )),
            None,
        )?;
    }
    Ok(())
}
```

**File:** crates/indexer/src/database.rs (L27-44)
```rust
pub const MAX_DIESEL_PARAM_SIZE: u16 = u16::MAX;

/// Given diesel has a limit of how many parameters can be inserted in a single operation (u16::MAX)
/// we may need to chunk an array of items based on how many columns are in the table.
/// This function returns boundaries of chunks in the form of (start_index, end_index)
pub fn get_chunks(num_items_to_insert: usize, column_count: usize) -> Vec<(usize, usize)> {
    let max_item_size = MAX_DIESEL_PARAM_SIZE as usize / column_count;
    let mut chunk: (usize, usize) = (0, min(num_items_to_insert, max_item_size));
    let mut chunks = vec![chunk];
    while chunk.1 != num_items_to_insert {
        chunk = (
            chunk.0 + max_item_size,
            min(num_items_to_insert, chunk.1 + max_item_size),
        );
        chunks.push(chunk);
    }
    chunks
}
```

**File:** aptos-move/aptos-vm-types/src/storage/change_set_configs.rs (L68-72)
```rust
    fn for_feature_version_3() -> Self {
        const MB: u64 = 1 << 20;

        Self::new_impl(3, MB, u64::MAX, MB, 10 * MB, u64::MAX)
    }
```

**File:** aptos-move/aptos-vm-types/src/storage/change_set_configs.rs (L115-125)
```rust
        let mut total_event_size = 0;
        for event in change_set.events_iter() {
            let size = event.event_data().len() as u64;
            if size > self.max_bytes_per_event {
                return storage_write_limit_reached(None);
            }
            total_event_size += size;
            if total_event_size > self.max_bytes_all_events_per_transaction {
                return storage_write_limit_reached(None);
            }
        }
```

**File:** crates/indexer/src/indexer/fetcher.rs (L15-15)
```rust
const TRANSACTION_FETCH_BATCH_SIZE: u16 = 500;
```

**File:** crates/indexer/migrations/2022-08-08-043603_core_tables/up.sql (L208-224)
```sql
CREATE TABLE events (
  sequence_number BIGINT NOT NULL,
  creation_number BIGINT NOT NULL,
  account_address VARCHAR(66) NOT NULL,
  transaction_version BIGINT NOT NULL,
  transaction_block_height BIGINT NOT NULL,
  type TEXT NOT NULL,
  data jsonb NOT NULL,
  inserted_at TIMESTAMP NOT NULL DEFAULT NOW(),
  -- Constraints
  PRIMARY KEY (
    account_address,
    creation_number,
    sequence_number
  ),
  CONSTRAINT fk_transaction_versions FOREIGN KEY (transaction_version) REFERENCES transactions (version)
);
```
