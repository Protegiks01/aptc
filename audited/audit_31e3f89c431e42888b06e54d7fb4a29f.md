# Audit Report

## Title
Cascading Sub-Pruner Failures Block Storage Pruning and Cause Uncontrolled Storage Bloat

## Summary
The AptosDB pruning subsystem uses `try_for_each()` with short-circuit error handling when coordinating multiple sub-pruners in parallel. If a single sub-pruner persistently fails, it blocks the entire pruner from advancing to newer versions, even though other independent sub-pruners are functioning correctly. This causes unpruned blockchain data to accumulate indefinitely, leading to storage bloat, disk exhaustion, and eventual node failure.

## Finding Description

The vulnerability exists in three critical pruning subsystems: **LedgerPruner**, **StateKvPruner**, and **StateMerklePruner**. All three use the same flawed error-handling pattern when coordinating parallel sub-pruner execution.

### Root Cause Architecture

In `LedgerPruner`, the pruning logic coordinates 7 independent sub-pruners (EventStorePruner, TransactionPruner, WriteSetPruner, TransactionInfoPruner, TransactionAccumulatorPruner, TransactionAuxiliaryDataPruner, and PersistedAuxiliaryInfoPruner). These sub-pruners execute in parallel and each maintains its own progress in the database. [1](#0-0) 

The critical flaw is on these lines where `try_for_each()` is used with the `?` operator. The `try_for_each()` method **short-circuits on the first error**, immediately returning without allowing other sub-pruners to complete. The overall pruner progress is only recorded if ALL sub-pruners succeed: [2](#0-1) 

### Cascading Failure Mechanism

When a sub-pruner fails:

1. Each sub-pruner successfully updates its own individual progress in the database (e.g., EventStorePruner writes to `DbMetadataKey::EventPrunerProgress`): [3](#0-2) 

2. However, if any sub-pruner fails, the `LedgerPruner`'s overall atomic progress counter is **never updated** because line 87 is never reached.

3. The pruner worker catches the error and simply retries indefinitely: [4](#0-3) 

4. On the next iteration, ALL sub-pruners are called again with the **same stale version range** using `LedgerPruner.progress` (not individual sub-pruner progress): [5](#0-4) 

5. The healthy sub-pruners waste resources re-processing already-pruned ranges (though this is mostly idempotent).

6. **Most critically**: The pruner remains stuck at the old progress value and **cannot advance to prune newer blockchain data**, even though 6 out of 7 sub-pruners are working perfectly.

### Attack Scenario

**Initial State:**
- LedgerPruner overall progress: version 1000
- Target version: 2000
- Blockchain continuing to grow (versions 3000, 4000, 5000...)

**Triggering Condition (realistic failure cases):**
- Disk corruption affecting one sub-pruner's column family
- Lock contention or deadlock in one sub-pruner's database operations  
- Resource exhaustion (file descriptors, memory) affecting one sub-pruner
- Bug in one sub-pruner implementation causing persistent errors

**Exploitation Path:**

1. **Batch 1 (versions 1000-1100):**
   - `ledger_metadata_pruner.prune(1000, 1100)` succeeds
   - Parallel sub-pruner execution begins:
     - EventStorePruner: SUCCESS → writes progress 1100
     - TransactionPruner: SUCCESS → writes progress 1100
     - WriteSetPruner: **FAILS** (disk I/O error on corrupted sector)
     - Other 4 sub-pruners: SUCCESS → write progress 1100
   - `try_for_each()` returns error due to WriteSetPruner failure
   - LedgerPruner overall progress **remains at 1000**

2. **Batch 2 (still 1000-1100, infinite retry loop):**
   - System attempts same batch again
   - WriteSetPruner still fails (persistent disk corruption)
   - Cycle repeats indefinitely

3. **Storage Bloat Accumulation:**
   - Blockchain continues growing to version 5000, 10000, 20000...
   - Versions 1100+ are **never pruned** because pruner is stuck at 1000
   - Storage consumption grows unbounded
   - Eventually: **Disk full → Node crash → Network liveness impact**

### Systemic Issue

The same vulnerability pattern exists in **all three pruning subsystems**:

**StateKvPruner** with shard pruners: [6](#0-5) 

**StateMerklePruner** with shard pruners: [7](#0-6) 

This indicates a fundamental architectural flaw in the pruning subsystem design that affects **all major storage components**.

## Impact Explanation

**Severity: Medium** per Aptos Bug Bounty Program criteria.

**Impacts:**

1. **"State inconsistencies requiring intervention"** - Manual operator intervention required to:
   - Identify the failing sub-pruner
   - Fix underlying issue (disk repair, resource allocation)
   - Potentially manually clean up accumulated data
   - Restart pruning process

2. **Validator Operational Issues:**
   - Gradual storage exhaustion leading to disk full
   - Node crashes requiring manual recovery
   - Service degradation as disk space approaches limits
   - Monitoring alerts and paging escalations

3. **Network-Wide Availability Risk:**
   - If multiple validators hit the same disk-related bug simultaneously (e.g., after software upgrade)
   - Cascading validator outages
   - Potential liveness degradation if enough validators go offline

4. **Data Availability:**
   - Historical data queries may slow down as DB grows beyond intended size
   - State sync operations degraded
   - API performance impact

**Why Not Higher Severity:**
- Not immediately exploitable by external attacker (requires natural failure conditions)
- Gradual degradation rather than instant failure
- Recoverable with manual intervention
- Doesn't directly affect consensus safety or funds

**Why Not Lower Severity:**
- Real operational risk requiring urgent intervention
- Can affect network availability if widespread
- Storage exhaustion is a serious node failure mode
- Violates critical invariant: "Resource Limits: All operations must respect storage and computational limits"

## Likelihood Explanation

**Likelihood: Medium to High**

**Factors Increasing Likelihood:**

1. **Persistent Disk Errors Common in Production:**
   - SSDs develop bad sectors over time
   - RAID controller failures
   - File system corruption
   - Database column family-specific issues

2. **Resource Contention Realistic:**
   - High transaction throughput causing lock contention
   - Memory pressure affecting specific sub-pruner operations
   - File descriptor exhaustion hitting specific DB operations

3. **Implementation Bugs Possible:**
   - Any bug in one of 7 sub-pruners blocks entire system
   - Code changes to one sub-pruner can break pruning for all
   - Complex pruning logic increases bug surface area

4. **Long-Running Operations:**
   - Validators run continuously for months
   - Accumulated storage makes failure probability increase over time
   - No automatic recovery mechanism

**Factors Decreasing Likelihood:**
- Requires persistent (not transient) failure
- Individual sub-pruner implementations are relatively simple
- Database operations are generally reliable

**Historical Precedent:**
Similar cascading failure patterns in distributed systems have caused major incidents (Cloudflare 2020, AWS 2017). Storage exhaustion from failed cleanup jobs is a known operational risk category.

## Recommendation

Implement **failure isolation with independent progress tracking** to prevent cascading failures. The fix requires three key changes:

### 1. Change Error Handling Strategy

Replace `try_for_each()` with `for_each()` and collect errors independently:

```rust
// In ledger_pruner/mod.rs, lines 78-84
let errors: Vec<_> = THREAD_MANAGER.get_background_pool().install(|| {
    self.sub_pruners.par_iter()
        .filter_map(|sub_pruner| {
            sub_pruner
                .prune(progress, current_batch_target_version)
                .map_err(|err| (sub_pruner.name(), err))
                .err()
        })
        .collect()
});

// Log all errors but don't fail the batch
for (name, err) in &errors {
    error!(
        pruner_name = name,
        error = ?err,
        "Sub-pruner failed, continuing with others"
    );
}

// Record progress even if some sub-pruners failed
// (they'll catch up next iteration using their individual progress)
progress = current_batch_target_version;
self.record_progress(progress);
```

### 2. Use Individual Sub-Pruner Progress

Modify sub-pruner invocation to use each sub-pruner's actual progress from the database:

```rust
// Read each sub-pruner's individual progress
let sub_pruner_progresses: HashMap<&str, Version> = self.sub_pruners
    .iter()
    .map(|sp| (sp.name(), sp.get_individual_progress()))
    .collect();

// Call each with its own progress
self.sub_pruners.par_iter().for_each(|sub_pruner| {
    let individual_progress = sub_pruner_progresses[sub_pruner.name()];
    if individual_progress < current_batch_target_version {
        if let Err(err) = sub_pruner.prune(individual_progress, current_batch_target_version) {
            error!("Sub-pruner {} failed: {}", sub_pruner.name(), err);
        }
    }
});
```

### 3. Add Circuit Breaker for Persistent Failures

```rust
// Track consecutive failures per sub-pruner
struct SubPrunerHealth {
    consecutive_failures: u32,
    last_successful_version: Version,
}

// Skip sub-pruner if it has failed too many times
const MAX_CONSECUTIVE_FAILURES: u32 = 10;

if sub_pruner_health.consecutive_failures > MAX_CONSECUTIVE_FAILURES {
    warn!(
        "Sub-pruner {} has failed {} times, skipping to prevent blocking. Manual intervention required.",
        sub_pruner.name(),
        sub_pruner_health.consecutive_failures
    );
    continue;
}
```

Apply the same fixes to:
- `storage/aptosdb/src/pruner/state_kv_pruner/mod.rs`
- `storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs`

## Proof of Concept

### Rust Test Demonstrating the Vulnerability

```rust
#[cfg(test)]
mod cascading_failure_test {
    use super::*;
    use std::sync::{Arc, atomic::{AtomicU32, Ordering}};
    
    // Mock sub-pruner that fails after N calls
    struct FailingSubPruner {
        name: String,
        failure_count: Arc<AtomicU32>,
        max_calls_before_failure: u32,
        call_count: Arc<AtomicU32>,
    }
    
    impl DBSubPruner for FailingSubPruner {
        fn name(&self) -> &str {
            &self.name
        }
        
        fn prune(&self, _current: Version, _target: Version) -> Result<()> {
            let call_num = self.call_count.fetch_add(1, Ordering::SeqCst);
            
            if call_num >= self.max_calls_before_failure {
                self.failure_count.fetch_add(1, Ordering::SeqCst);
                anyhow::bail!("Simulated persistent failure (e.g., disk corruption)");
            }
            
            Ok(())
        }
    }
    
    #[test]
    fn test_one_failed_sub_pruner_blocks_all_progress() {
        // Setup: 3 sub-pruners, one will fail persistently after 1 success
        let failure_count = Arc::new(AtomicU32::new(0));
        
        let sub_pruners: Vec<Box<dyn DBSubPruner + Send + Sync>> = vec![
            Box::new(FailingSubPruner {
                name: "healthy_1".to_string(),
                failure_count: Arc::clone(&failure_count),
                max_calls_before_failure: u32::MAX, // Never fails
                call_count: Arc::new(AtomicU32::new(0)),
            }),
            Box::new(FailingSubPruner {
                name: "failing_pruner".to_string(),
                failure_count: Arc::clone(&failure_count),
                max_calls_before_failure: 1, // Fails after first call
                call_count: Arc::new(AtomicU32::new(0)),
            }),
            Box::new(FailingSubPruner {
                name: "healthy_2".to_string(),
                failure_count: Arc::clone(&failure_count),
                max_calls_before_failure: u32::MAX, // Never fails
                call_count: Arc::new(AtomicU32::new(0)),
            }),
        ];
        
        // Simulate pruner loop with current implementation
        let mut progress = 0;
        let target_version = 1000;
        let batch_size = 100;
        
        let mut iterations = 0;
        while progress < target_version && iterations < 20 {
            let current_batch_target = std::cmp::min(progress + batch_size, target_version);
            
            // This is the vulnerable pattern from ledger_pruner/mod.rs:78-84
            let result = sub_pruners.par_iter().try_for_each(|sub_pruner| {
                sub_pruner.prune(progress, current_batch_target)
                    .map_err(|err| anyhow!("{} failed: {}", sub_pruner.name(), err))
            });
            
            match result {
                Ok(_) => {
                    // Only update progress if ALL succeed
                    progress = current_batch_target;
                },
                Err(_) => {
                    // Error: progress NOT updated, will retry same batch
                    // This is the bug!
                }
            }
            
            iterations += 1;
        }
        
        // VULNERABILITY DEMONSTRATED:
        // After 20 iterations, progress should be at 1000 if healthy sub-pruners
        // could advance independently. But due to the failing sub-pruner blocking,
        // progress is stuck at 100 (or even 0 if failure happens immediately).
        assert!(
            progress < 200,
            "BUG CONFIRMED: Progress is {} (should be 1000 if healthy pruners could continue)",
            progress
        );
        
        // The failing sub-pruner is called repeatedly (19 times after initial success)
        assert!(
            failure_count.load(Ordering::SeqCst) > 15,
            "Failing sub-pruner was retried {} times, blocking all progress",
            failure_count.load(Ordering::SeqCst)
        );
    }
}
```

### Reproduction Steps on Live Node

1. Deploy instrumented node with injected failure in one sub-pruner:
   ```rust
   // In write_set_pruner.rs, inject artificial failure
   fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
       if current_progress > 1000 && current_progress < 2000 {
           anyhow::bail!("Simulated disk sector corruption");
       }
       // ... normal implementation
   }
   ```

2. Monitor pruner progress metrics:
   ```
   ledger_pruner_progress (stuck at ~1000)
   ledger_pruner_target (advancing: 5000, 10000, 20000...)
   disk_usage (growing unbounded)
   ```

3. Observe logs showing repeated retries:
   ```
   ERROR pruner: WriteSetPruner failed to prune
   INFO ledger_pruner: Retrying batch 1000-1100...
   ERROR pruner: WriteSetPruner failed to prune
   INFO ledger_pruner: Retrying batch 1000-1100...
   [repeated indefinitely]
   ```

4. Eventually: Disk full → Node crash

---

**Notes:**

This vulnerability represents a systemic architectural issue in the AptosDB pruning subsystem affecting all three major pruners (Ledger, StateKv, and StateMerkle). The impact is operational rather than consensus-breaking, but the likelihood is significant given realistic failure modes in production environments (disk errors, resource contention, implementation bugs). The fix requires redesigning the error handling to isolate failures and track individual sub-pruner progress independently.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L62-76)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning ledger data."
            );
            self.ledger_metadata_pruner
                .prune(progress, current_batch_target_version)?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L78-84)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L86-87)
```rust
            progress = current_batch_target_version;
            self.record_progress(progress);
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L66-69)
```rust
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::EventPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L56-63)
```rust
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L68-78)
```rust
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state kv shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L177-188)
```rust
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(current_progress, target_version, batch_size)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state merkle shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })
            .map_err(Into::into)
```
