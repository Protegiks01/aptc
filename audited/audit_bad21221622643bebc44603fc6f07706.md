# Audit Report

## Title
Byzantine State Corruption via Race Condition in Snapshot Verification with Error Masking

## Summary
A race condition in `StateSnapshotRestore::add_chunk` combined with indiscriminate error masking allows malicious validators to inject corrupted state values into honest nodes during state snapshot synchronization. The vulnerability enables permanent state corruption that remains undetected, violating the State Consistency invariant.

## Finding Description

The vulnerability exists in the state snapshot restoration process where KV data and Merkle tree verification occur in parallel threads without proper synchronization or error handling.

**Root Cause 1: Parallel Processing Without Atomicity**

In `StateSnapshotRestore::add_chunk`, KV writes and cryptographic verification execute in parallel: [1](#0-0) 

The `kv_fn` writes state values to RocksDB immediately with no cryptographic verification, while `tree_fn` performs Merkle proof verification. These execute concurrently via `IO_POOL.join()`, meaning KV data commits to storage before verification completes.

**Root Cause 2: Indiscriminate Error Masking**

All state snapshot errors, including cryptographic verification failures, are converted to `UnexpectedError`: [2](#0-1) [3](#0-2) 

This masks Byzantine attacks as transient network errors, causing the bootstrapper to retry with a different peer while preserving corrupted data.

**Root Cause 3: Progress-Based Skip Logic**

When retrying after a verification failure, the KV restoration skips already-written entries: [4](#0-3) 

The skip logic at lines 92-99 prevents overwriting corrupted data that was written before verification failed.

**Attack Execution Flow:**

1. **Initial Corruption**: Malicious validator sends `StateValueChunkWithProof` with:
   - Corrupted `raw_values` for keys K₁...Kₙ
   - Valid `root_hash` (copied from expected transaction info)
   - Invalid `SparseMerkleRangeProof` (doesn't match corrupted values)

2. **Bootstrapper Validation**: Node performs superficial root hash check: [5](#0-4) 

This check compares `state_value_chunk_with_proof.root_hash` against `expected_root_hash` from the transaction info, which passes because the attacker copied the correct root hash.

3. **Parallel Processing**:
   - Thread A (`kv_fn`): Writes corrupted values `(K₁, corrupted_V₁)...(Kₙ, corrupted_Vₙ)` to RocksDB
   - Thread B (`tree_fn`): Computes `hash(corrupted_V₁)...hash(corrupted_Vₙ)` and calls `verify(proof)`

4. **Verification Failure**: The Merkle proof verification fails because the proof is for correct values, not corrupted ones: [6](#0-5) 

The `verify()` call at line 391 fails, but Thread A has already committed corrupted KV data to storage.

5. **Error Masking & Retry**: The verification error becomes `UnexpectedError`, causing stream reset: [7](#0-6) 

6. **Honest Peer Retry**: Node requests chunk from honest validator. The skip logic prevents overwriting corrupted data, while tree verification passes with correct hashes from the new chunk.

7. **No Final Verification**: Neither `finish()` nor `finalize_state_snapshot()` verify KV data consistency: [8](#0-7) 

The finalization process saves transactions and ledger infos without verifying that KV storage matches the Merkle tree.

**Result**: Node completes snapshot sync with corrupted state values in KV storage. When these values are read during transaction execution, they will cause state divergence and consensus failures.

## Impact Explanation

**Severity: Critical ($1,000,000 category)**

This vulnerability enables:

1. **State Consistency Violation**: Breaks invariant #4 - state transitions are no longer verifiable via Merkle proofs. The Merkle tree contains correct hashes while KV storage contains corrupted values.

2. **Deterministic Execution Violation**: Breaks invariant #1 - nodes with corrupted state will execute transactions differently, producing different state roots.

3. **Consensus Safety Violation**: Different nodes may commit different blocks due to state divergence, potentially causing chain splits or requiring manual intervention (hard fork).

4. **Permanent Corruption**: The corrupted state persists indefinitely as there's no background verification process to detect inconsistencies between the Merkle tree and KV storage.

This meets the Critical severity criteria:
- Consensus/Safety violations ✓
- State inconsistencies requiring intervention ✓
- Potential non-recoverable network partition ✓

## Likelihood Explanation

**Likelihood: High**

Attack requirements:
- Single malicious network peer capable of serving state sync data
- No validator privileges required
- No cryptographic breaks needed
- Exploits normal state sync protocol

The attack is:
- **Realistic**: Nodes regularly perform state snapshot sync during bootstrapping or fast sync
- **Stealthy**: Errors appear as network issues, not Byzantine attacks
- **Persistent**: Corrupted state remains undetected indefinitely
- **Scalable**: Can corrupt multiple state keys in a single chunk

The only complexity is crafting the malicious chunk, which requires:
1. Valid root hash from transaction info (publicly available)
2. Arbitrary corrupted state values
3. Invalid proof (any proof that doesn't match the corrupted values)

## Recommendation

**Immediate Fix**: Implement atomic verification before any storage writes.

```rust
// In StateSnapshotRestore::add_chunk (state_restore/mod.rs)
pub fn add_chunk(&mut self, chunk: Vec<(K, V)>, proof: SparseMerkleRangeProof) -> Result<()> {
    // 1. VERIFY FIRST - before any writes
    let chunk_hashes: Vec<(&K, HashValue)> = chunk.iter()
        .map(|(k, v)| (k, v.hash()))
        .collect();
    
    self.tree_restore.lock().as_mut().unwrap()
        .add_chunk_impl(chunk_hashes, proof)?;
    
    // 2. ONLY WRITE KV AFTER VERIFICATION PASSES
    self.kv_restore.lock().as_mut().unwrap()
        .add_chunk(chunk)?;
    
    Ok(())
}
```

**Additional Fixes**:

1. **Preserve Error Context**: Don't mask cryptographic verification failures
```rust
// In send_storage_synchronizer_error (storage_synchronizer.rs)
async fn send_storage_synchronizer_error(
    mut error_notification_sender: mpsc::UnboundedSender<ErrorNotification>,
    notification_id: NotificationId,
    error_message: String,
) {
    // Classify errors properly
    let error = if error_message.contains("Root hashes do not match") || 
                   error_message.contains("proof") {
        Error::VerificationError(error_message)
    } else {
        Error::UnexpectedError(error_message)
    };
    // ... rest of function
}
```

2. **Final State Consistency Check**: Add verification in `finalize_state_snapshot` to ensure KV data matches tree hashes.

3. **Cleanup on Failure**: Implement rollback of KV writes when verification fails.

## Proof of Concept

**Setup**: Modify a state sync peer to send corrupted chunks:

```rust
// Malicious peer modification
fn create_corrupted_chunk(
    correct_values: Vec<(StateKey, StateValue)>,
    expected_root_hash: HashValue,
    valid_proof: SparseMerkleRangeProof,
) -> StateValueChunkWithProof {
    // Corrupt the values
    let corrupted_values: Vec<(StateKey, StateValue)> = correct_values.iter()
        .map(|(k, v)| {
            let mut corrupted = v.clone();
            // Flip first byte of value
            if let Some(byte) = corrupted.bytes_mut().get_mut(0) {
                *byte = !*byte;
            }
            (k.clone(), corrupted)
        })
        .collect();
    
    StateValueChunkWithProof {
        first_index: 0,
        last_index: corrupted_values.len() as u64 - 1,
        first_key: corrupted_values[0].0.hash(),
        last_key: corrupted_values.last().unwrap().0.hash(),
        raw_values: corrupted_values,
        proof: valid_proof, // This won't match corrupted values
        root_hash: expected_root_hash, // Use correct root hash to pass initial check
    }
}
```

**Verification Steps**:
1. Start victim node in bootstrapping mode
2. Configure malicious peer as state sync source
3. Observe error logs showing verification failure converted to `UnexpectedError`
4. Observe node retrying with honest peer
5. After completion, query corrupted keys and observe incorrect values
6. Attempt to execute transactions using corrupted state - observe state root mismatch

**Expected Result**: Node completes state sync with corrupted values in storage, causing subsequent transaction execution failures and consensus divergence.

## Notes

This vulnerability is particularly severe because:

1. **Silent Corruption**: The Merkle tree appears valid, only the underlying KV data is corrupted
2. **Detection Difficulty**: No alerts are raised; errors appear as transient network issues
3. **Widespread Impact**: Affects all nodes performing state snapshot sync
4. **No Recovery Path**: Corrupted state persists until manually detected and the node is re-synced

The fundamental flaw is the assumption that parallel execution with post-hoc error checking provides atomicity - it does not when one operation is persistent (RocksDB write) and the other is validation.

### Citations

**File:** storage/aptosdb/src/state_restore/mod.rs (L88-127)
```rust
    pub fn add_chunk(&mut self, mut chunk: Vec<(K, V)>) -> Result<()> {
        // load progress
        let progress_opt = self.db.get_progress(self.version)?;

        // skip overlaps
        if let Some(progress) = progress_opt {
            let idx = chunk
                .iter()
                .position(|(k, _v)| CryptoHash::hash(k) > progress.key_hash)
                .unwrap_or(chunk.len());
            chunk = chunk.split_off(idx);
        }

        // quit if all skipped
        if chunk.is_empty() {
            return Ok(());
        }

        // save
        let mut usage = progress_opt.map_or(StateStorageUsage::zero(), |p| p.usage);
        let (last_key, _last_value) = chunk.last().unwrap();
        let last_key_hash = CryptoHash::hash(last_key);

        // In case of TreeOnly Restore, we only restore the usage of KV without actually writing KV into DB
        for (k, v) in chunk.iter() {
            usage.add_item(k.key_size() + v.value_size());
        }

        // prepare the sharded kv batch
        let kv_batch: StateValueBatch<K, Option<V>> = chunk
            .into_iter()
            .map(|(k, v)| ((k, self.version), Some(v)))
            .collect();

        self.db.write_kv_batch(
            self.version,
            &kv_batch,
            StateSnapshotProgress::new(last_key_hash, usage),
        )
    }
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L228-258)
```rust
    fn add_chunk(&mut self, chunk: Vec<(K, V)>, proof: SparseMerkleRangeProof) -> Result<()> {
        let kv_fn = || {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_value_add_chunk"]);
            self.kv_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk(chunk.clone())
        };

        let tree_fn = || {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["jmt_add_chunk"]);
            self.tree_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk_impl(chunk.iter().map(|(k, v)| (k, v.hash())).collect(), proof)
        };
        match self.restore_mode {
            StateSnapshotRestoreMode::KvOnly => kv_fn()?,
            StateSnapshotRestoreMode::TreeOnly => tree_fn()?,
            StateSnapshotRestoreMode::Default => {
                // We run kv_fn with TreeOnly to restore the usage of DB
                let (r1, r2) = IO_POOL.join(kv_fn, tree_fn);
                r1?;
                r2?;
            },
        }

        Ok(())
    }
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L956-965)
```rust
                        Err(error) => {
                            let error =
                                format!("Failed to commit state value chunk! Error: {:?}", error);
                            send_storage_synchronizer_error(
                                error_notification_sender.clone(),
                                notification_id,
                                error,
                            )
                            .await;
                        },
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L1321-1347)
```rust
async fn send_storage_synchronizer_error(
    mut error_notification_sender: mpsc::UnboundedSender<ErrorNotification>,
    notification_id: NotificationId,
    error_message: String,
) {
    // Log the storage synchronizer error
    let error_message = format!("Storage synchronizer error: {:?}", error_message);
    error!(LogSchema::new(LogEntry::StorageSynchronizer).message(&error_message));

    // Update the storage synchronizer error metrics
    let error = Error::UnexpectedError(error_message);
    metrics::increment_counter(&metrics::STORAGE_SYNCHRONIZER_ERRORS, error.get_label());

    // Send an error notification to the driver
    let error_notification = ErrorNotification {
        error: error.clone(),
        notification_id,
    };
    if let Err(error) = error_notification_sender.send(error_notification).await {
        error!(
            LogSchema::new(LogEntry::StorageSynchronizer).message(&format!(
                "Failed to send error notification! Error: {:?}",
                error
            ))
        );
    }
}
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L1007-1031)
```rust
        // Verify the chunk root hash matches the expected root hash
        let first_transaction_info = transaction_output_to_sync
            .get_output_list_with_proof()
            .proof
            .transaction_infos
            .first()
            .ok_or_else(|| {
                Error::UnexpectedError("Target transaction info does not exist!".into())
            })?;
        let expected_root_hash = first_transaction_info
            .ensure_state_checkpoint_hash()
            .map_err(|error| {
                Error::UnexpectedError(format!("State checkpoint must exist! Error: {:?}", error))
            })?;
        if state_value_chunk_with_proof.root_hash != expected_root_hash {
            self.reset_active_stream(Some(NotificationAndFeedback::new(
                notification_id,
                NotificationFeedback::InvalidPayloadData,
            )))
            .await?;
            return Err(Error::VerificationError(format!(
                "The states chunk with proof root hash: {:?} didn't match the expected hash: {:?}!",
                state_value_chunk_with_proof.root_hash, expected_root_hash,
            )));
        }
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L1517-1536)
```rust
    pub async fn handle_storage_synchronizer_error(
        &mut self,
        notification_and_feedback: NotificationAndFeedback,
    ) -> Result<(), Error> {
        // Reset the active stream
        self.reset_active_stream(Some(notification_and_feedback))
            .await?;

        // Fallback to output syncing if we need to
        if let BootstrappingMode::ExecuteOrApplyFromGenesis = self.get_bootstrapping_mode() {
            self.output_fallback_handler.fallback_to_outputs();
            metrics::set_gauge(
                &metrics::DRIVER_FALLBACK_MODE,
                ExecutingComponent::Bootstrapper.get_label(),
                1,
            );
        }

        Ok(())
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L339-413)
```rust
    pub fn add_chunk_impl(
        &mut self,
        mut chunk: Vec<(&K, HashValue)>,
        proof: SparseMerkleRangeProof,
    ) -> Result<()> {
        if self.finished {
            info!("State snapshot restore already finished, ignoring entire chunk.");
            return Ok(());
        }

        if let Some(prev_leaf) = &self.previous_leaf {
            let skip_until = chunk
                .iter()
                .find_position(|(key, _hash)| key.hash() > *prev_leaf.account_key());
            chunk = match skip_until {
                None => {
                    info!("Skipping entire chunk.");
                    return Ok(());
                },
                Some((0, _)) => chunk,
                Some((num_to_skip, next_leaf)) => {
                    info!(
                        num_to_skip = num_to_skip,
                        next_leaf = next_leaf,
                        "Skipping leaves."
                    );
                    chunk.split_off(num_to_skip)
                },
            }
        };
        if chunk.is_empty() {
            return Ok(());
        }

        for (key, value_hash) in chunk {
            let hashed_key = key.hash();
            if let Some(ref prev_leaf) = self.previous_leaf {
                ensure!(
                    &hashed_key > prev_leaf.account_key(),
                    "State keys must come in increasing order.",
                )
            }
            self.previous_leaf.replace(LeafNode::new(
                hashed_key,
                value_hash,
                (key.clone(), self.version),
            ));
            self.add_one(key, value_hash);
            self.num_keys_received += 1;
        }

        // Verify what we have added so far is all correct.
        self.verify(proof)?;

        // Write the frozen nodes to storage.
        if self.async_commit {
            self.wait_for_async_commit()?;
            let (tx, rx) = channel();
            self.async_commit_result = Some(rx);

            let mut frozen_nodes = HashMap::new();
            std::mem::swap(&mut frozen_nodes, &mut self.frozen_nodes);
            let store = self.store.clone();

            IO_POOL.spawn(move || {
                let res = store.write_node_batch(&frozen_nodes);
                tx.send(res).unwrap();
            });
        } else {
            self.store.write_node_batch(&self.frozen_nodes)?;
            self.frozen_nodes.clear();
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L125-241)
```rust
    fn finalize_state_snapshot(
        &self,
        version: Version,
        output_with_proof: TransactionOutputListWithProofV2,
        ledger_infos: &[LedgerInfoWithSignatures],
    ) -> Result<()> {
        let (output_with_proof, persisted_aux_info) = output_with_proof.into_parts();
        gauged_api("finalize_state_snapshot", || {
            // Ensure the output with proof only contains a single transaction output and info
            let num_transaction_outputs = output_with_proof.get_num_outputs();
            let num_transaction_infos = output_with_proof.proof.transaction_infos.len();
            ensure!(
                num_transaction_outputs == 1,
                "Number of transaction outputs should == 1, but got: {}",
                num_transaction_outputs
            );
            ensure!(
                num_transaction_infos == 1,
                "Number of transaction infos should == 1, but got: {}",
                num_transaction_infos
            );

            // TODO(joshlind): include confirm_or_save_frozen_subtrees in the change set
            // bundle below.

            // Update the merkle accumulator using the given proof
            let frozen_subtrees = output_with_proof
                .proof
                .ledger_info_to_transaction_infos_proof
                .left_siblings();
            restore_utils::confirm_or_save_frozen_subtrees(
                self.ledger_db.transaction_accumulator_db_raw(),
                version,
                frozen_subtrees,
                None,
            )?;

            // Create a single change set for all further write operations
            let mut ledger_db_batch = LedgerDbSchemaBatches::new();
            let mut sharded_kv_batch = self.state_kv_db.new_sharded_native_batches();
            let mut state_kv_metadata_batch = SchemaBatch::new();
            // Save the target transactions, outputs, infos and events
            let (transactions, outputs): (Vec<Transaction>, Vec<TransactionOutput>) =
                output_with_proof
                    .transactions_and_outputs
                    .into_iter()
                    .unzip();
            let events = outputs
                .clone()
                .into_iter()
                .map(|output| output.events().to_vec())
                .collect::<Vec<_>>();
            let wsets: Vec<WriteSet> = outputs
                .into_iter()
                .map(|output| output.write_set().clone())
                .collect();
            let transaction_infos = output_with_proof.proof.transaction_infos;
            // We should not save the key value since the value is already recovered for this version
            restore_utils::save_transactions(
                self.state_store.clone(),
                self.ledger_db.clone(),
                version,
                &transactions,
                &persisted_aux_info,
                &transaction_infos,
                &events,
                wsets,
                Some((
                    &mut ledger_db_batch,
                    &mut sharded_kv_batch,
                    &mut state_kv_metadata_batch,
                )),
                false,
            )?;

            // Save the epoch ending ledger infos
            restore_utils::save_ledger_infos(
                self.ledger_db.metadata_db(),
                ledger_infos,
                Some(&mut ledger_db_batch.ledger_metadata_db_batches),
            )?;

            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::LedgerCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;
            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::OverallCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;

            // Apply the change set writes to the database (atomically) and update in-memory state
            //
            // state kv and SMT should use shared way of committing.
            self.ledger_db.write_schemas(ledger_db_batch)?;

            self.ledger_pruner.save_min_readable_version(version)?;
            self.state_store
                .state_merkle_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .epoch_snapshot_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .state_kv_pruner
                .save_min_readable_version(version)?;

            restore_utils::update_latest_ledger_info(self.ledger_db.metadata_db(), ledger_infos)?;
            self.state_store.reset();

            Ok(())
        })
    }
```
