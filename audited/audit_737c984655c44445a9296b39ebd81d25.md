# Audit Report

## Title
CPU Exhaustion via Closed Oneshot Channel in Batch Requester Select Loop

## Summary
The `request_batch()` function in the batch requester contains a `tokio::select!` loop that polls a oneshot receiver without proper handling when the sender is dropped. When the sender is dropped (e.g., during batch expiration), the receiver immediately returns errors on every poll, creating a busy loop that consumes 100% CPU until the retry limit is exhausted.

## Finding Description

The vulnerability exists in the `request_batch()` function's select loop. [1](#0-0) 

The select loop has three branches:
1. **Interval tick** - properly waits asynchronously
2. **Futures completion** - properly waits asynchronously  
3. **Subscriber receiver** - **PROBLEMATIC**

When the `subscriber_rx` oneshot channel's sender is dropped, the receiver returns `Err(oneshot::Canceled)`. The error handling branch only logs the error and continues the loop without breaking or returning. [2](#0-1) 

Once a oneshot receiver is in the error state, subsequent polls immediately return `Poll::Ready(Err(...))` without waiting. This causes the select! loop to repeatedly poll the closed receiver on every iteration, creating a CPU-intensive busy loop.

**Trigger Path:**

The subscriber sender is stored in `persist_subscribers` when `subscribe()` is called. [3](#0-2) 

When batches expire, the `clear_expired_batches()` function removes entries from `persist_subscribers`, dropping all senders without sending values. [4](#0-3) 

**Contrast with Correct Pattern:**

The codebase shows the correct pattern in other modules where oneshot receivers in select loops always break regardless of Ok or Err. [5](#0-4) 

## Impact Explanation

This is a **Medium Severity** vulnerability per Aptos bug bounty criteria:

- **Validator Node Slowdowns**: The CPU exhaustion directly impacts validator performance. A validator stuck in this busy loop consumes excessive CPU resources, reducing its ability to participate effectively in consensus.

- **Resource Limits Invariant Violation**: The vulnerability violates the invariant that "All operations must respect gas, storage, and computational limits." The spinning loop consumes unbounded CPU without proper backoff.

- **Consensus Performance Impact**: While this doesn't break consensus safety, multiple validators experiencing this simultaneously (e.g., all requesting the same expired batch) could degrade network performance and potentially impact liveness.

The impact is limited to availability/performance degradation rather than safety violations or fund loss, placing it in the Medium severity category.

## Likelihood Explanation

**High Likelihood** - This vulnerability triggers through normal operation without malicious actors:

1. **Natural Occurrence**: Batches expire naturally as time passes. When `clear_expired_batches()` runs (typically on a schedule), it removes subscriber entries for expired batches.

2. **Race Condition**: If `request_batch()` is called for a batch that expires during the request, the sender will be dropped mid-flight.

3. **No Special Conditions**: No attacker input, malicious validator, or special network conditions are required. This is purely a timing-based issue that occurs during normal validator operation.

4. **Duration**: The CPU spinning continues until either:
   - The retry limit is reached (controlled by `retry_limit` parameter)
   - All pending network futures complete
   - This could be seconds to minutes of 100% CPU usage

## Recommendation

The fix is to break from the loop when the subscriber channel returns an error, matching the pattern used elsewhere in the codebase:

```rust
result = &mut subscriber_rx => {
    match result {
        Ok(persisted_value) => {
            counters::RECEIVED_BATCH_FROM_SUBSCRIPTION_COUNT.inc();
            let (_, maybe_payload) = persisted_value.unpack();
            return Ok(maybe_payload.expect("persisted value must exist"));
        }
        Err(err) => {
            debug!("channel closed: {}", err);
            // FIXED: Break from the loop instead of continuing
            break;
        }
    };
},
```

Alternatively, convert the oneshot receiver to a stream and use `select_next_some()` which will only match when data is available, preventing the busy loop entirely.

## Proof of Concept

```rust
#[tokio::test]
async fn test_cpu_exhaustion_on_closed_subscriber() {
    use tokio::sync::oneshot;
    use tokio::time::{interval, Duration};
    use std::time::Instant;
    
    // Simulate the problematic pattern
    let (tx, mut rx) = oneshot::channel::<String>();
    let mut interval = interval(Duration::from_millis(100));
    
    // Drop the sender immediately to simulate batch expiration
    drop(tx);
    
    let start = Instant::now();
    let mut iterations = 0u64;
    
    // Run the loop for a short time and count iterations
    let timeout = tokio::time::sleep(Duration::from_millis(50));
    tokio::pin!(timeout);
    
    loop {
        tokio::select! {
            _ = &mut timeout => {
                break;
            }
            _ = interval.tick() => {
                // This branch rarely executes
            }
            result = &mut rx => {
                // This branch executes on EVERY iteration after sender is dropped
                match result {
                    Ok(_) => break,
                    Err(_) => {
                        iterations += 1;
                        // BUG: Should break here, but continues
                        continue;
                    }
                }
            }
        }
    }
    
    let elapsed = start.elapsed();
    println!("Iterations: {} in {:?}", iterations, elapsed);
    
    // If the bug exists, we'll see thousands/millions of iterations
    // because the closed receiver returns immediately on every poll.
    // Expected: iterations >> 10 (normal would be ~0-1)
    assert!(iterations > 1000, 
        "Bug detected: {} iterations in {:?} indicates busy loop",
        iterations, elapsed);
}
```

**Expected Result**: The test will demonstrate that the closed oneshot receiver causes the select loop to iterate thousands of times in 50ms, proving the CPU exhaustion issue. In a properly functioning implementation with a break statement, iterations would be 0-1.

## Notes

- This vulnerability affects all validators running consensus with the quorum store enabled
- The CPU exhaustion is self-inflicted (not attacker-controlled) but still impacts validator performance
- The fix is straightforward and follows established patterns elsewhere in the codebase
- No data loss or consensus safety violations occur, only performance degradation
- The retry interval provides some natural throttling, but doesn't prevent the busy loop between retry ticks

### Citations

**File:** consensus/src/quorum_store/batch_requester.rs (L122-174)
```rust
                tokio::select! {
                    _ = interval.tick() => {
                        // send batch request to a set of peers of size request_num_peers
                        if let Some(request_peers) = request_state.next_request_peers(request_num_peers) {
                            for peer in request_peers {
                                futures.push(network_sender.request_batch(request.clone(), peer, rpc_timeout));
                            }
                        } else if futures.is_empty() {
                            // end the loop when the futures are drained
                            break;
                        }
                    },
                    Some(response) = futures.next() => {
                        match response {
                            Ok(BatchResponse::Batch(batch)) => {
                                counters::RECEIVED_BATCH_RESPONSE_COUNT.inc();
                                let payload = batch.into_transactions();
                                return Ok(payload);
                            }
                            // Short-circuit if the chain has moved beyond expiration
                            Ok(BatchResponse::NotFound(ledger_info)) => {
                                counters::RECEIVED_BATCH_NOT_FOUND_COUNT.inc();
                                if ledger_info.commit_info().epoch() == epoch
                                    && ledger_info.commit_info().timestamp_usecs() > expiration
                                    && ledger_info.verify_signatures(&validator_verifier).is_ok()
                                {
                                    counters::RECEIVED_BATCH_EXPIRED_COUNT.inc();
                                    debug!("QS: batch request expired, digest:{}", digest);
                                    return Err(ExecutorError::CouldNotGetData);
                                }
                            }
                            Ok(BatchResponse::BatchV2(_)) => {
                                error!("Batch V2 response is not supported");
                            }
                            Err(e) => {
                                counters::RECEIVED_BATCH_RESPONSE_ERROR_COUNT.inc();
                                debug!("QS: batch request error, digest:{}, error:{:?}", digest, e);
                            }
                        }
                    },
                    result = &mut subscriber_rx => {
                        match result {
                            Ok(persisted_value) => {
                                counters::RECEIVED_BATCH_FROM_SUBSCRIPTION_COUNT.inc();
                                let (_, maybe_payload) = persisted_value.unpack();
                                return Ok(maybe_payload.expect("persisted value must exist"));
                            }
                            Err(err) => {
                                debug!("channel closed: {}", err);
                            }
                        };
                    },
                }
```

**File:** consensus/src/quorum_store/batch_store.rs (L456-458)
```rust
                    if entry.get().expiration() <= expiration_time {
                        self.persist_subscribers.remove(entry.get().digest());
                        Some(entry.remove())
```

**File:** consensus/src/quorum_store/batch_store.rs (L591-601)
```rust
    fn subscribe(&self, digest: HashValue) -> oneshot::Receiver<PersistedValue<BatchInfoExt>> {
        let (tx, rx) = oneshot::channel();
        self.persist_subscribers.entry(digest).or_default().push(tx);

        // This is to account for the race where this subscribe call happens after the
        // persist call.
        if let Ok(value) = self.get_batch_from_local(&digest) {
            self.notify_subscribers(value)
        }

        rx
```

**File:** consensus/src/recovery_manager.rs (L164-169)
```rust
                close_req = close_rx.select_next_some() => {
                    if let Ok(ack_sender) = close_req {
                        ack_sender.send(()).expect("[RecoveryManager] Fail to ack shutdown");
                    }
                    break;
                }
```
