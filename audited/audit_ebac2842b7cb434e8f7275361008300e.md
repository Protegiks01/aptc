# Audit Report

## Title
Silent Event V2 Data Loss Due to Primary Key Collision in Indexer

## Summary
The indexer's `insert_events` function silently corrupts and loses Event V2 data when multiple V2 events are emitted across different transactions. Event V2 uses dummy GUID values (creation_number=0, account_address=0x0, sequence_number=0), causing all V2 events to share the same primary key and resulting in data overwrites that preserve stale transaction metadata while updating only timestamps.

## Finding Description
Event V2 (module events) were introduced to replace Event V1 (instance events). Unlike V1 events which have unique EventKeys, V2 events do not have handles or sequence numbers. The API layer compensates by returning dummy GUID values for V2 events. [1](#0-0) [2](#0-1) 

The indexer extracts these dummy values without validation: [3](#0-2) 

The events table uses a composite primary key: [4](#0-3) 

When inserting events, the code handles primary key conflicts with UPDATE instead of failure: [5](#0-4) 

**Attack Scenario:**
1. Transaction A (version 1000) emits Event V2 with type `0x1::module::EventType1` and data `{"value": "A"}`
2. Transaction B (version 1001) emits Event V2 with type `0x1::module::EventType2` and data `{"value": "B"}`
3. Both events have PK: (`0x0`, 0, 0)
4. First insert succeeds, second triggers UPDATE
5. Result: Database contains ONE row with:
   - `transaction_version`: 1000 (from Event A - WRONG)
   - `type`: `0x1::module::EventType1` (from Event A - WRONG)
   - `data`: `{"value": "A"}` (from Event A - WRONG)
   - `event_index`: 0 (from Event B)
   - `inserted_at`: current_timestamp (from Event B)

Event B is completely lost. Queries filtering by transaction_version, type, or data will return incorrect results. Event history is corrupted.

## Impact Explanation
This is a **Medium Severity** issue per Aptos Bug Bounty criteria:

- **State inconsistencies requiring intervention**: The indexer database contains corrupted event data where transaction_version, type, and data fields are inconsistent with event_index and inserted_at fields
- **Data loss**: All Event V2 events except the first one per dummy GUID combination are permanently lost from the indexer
- **Application-level impact**: DApps relying on indexer event data will receive incorrect or incomplete information, potentially leading to incorrect UI displays, failed transaction processing, or incorrect business logic execution

While this does not directly affect consensus or on-chain state (the blockchain itself remains correct), it violates the critical invariant that the indexer must accurately reflect on-chain data. This impacts ecosystem reliability as developers cannot trust indexed event data.

## Likelihood Explanation
**High likelihood** of occurrence:

- Event V2 is the modern event system replacing deprecated Event V1
- Any Move module using `event::emit()` creates Event V2 events
- Multiple transactions emitting Event V2 events is the normal operation mode for active contracts
- The bug triggers automatically without requiring malicious intent
- As adoption of Event V2 increases (Event V1 is deprecated), this issue becomes more prevalent

The issue has likely already occurred in production environments using Event V2.

## Recommendation
**Immediate Fix:** Add `transaction_version` to the primary key to ensure uniqueness across transactions:

```sql
ALTER TABLE events DROP CONSTRAINT events_pkey;
ALTER TABLE events ADD PRIMARY KEY (account_address, creation_number, sequence_number, transaction_version);
```

For the insertion code, update the conflict resolution to include transaction_version:

```rust
fn insert_events(
    conn: &mut PgConnection,
    items_to_insert: &[EventModel],
) -> Result<(), diesel::result::Error> {
    use schema::events::dsl::*;
    let chunks = get_chunks(items_to_insert.len(), EventModel::field_count());
    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::events::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict((account_address, creation_number, sequence_number, transaction_version))
                .do_update()
                .set((
                    inserted_at.eq(excluded(inserted_at)),
                    event_index.eq(excluded(event_index)),
                )),
            None,
        )?;
    }
    Ok(())
}
```

**Alternative Fix (Event V2 specific):** Create a synthetic unique identifier for V2 events using transaction_version and event_index, since the GUID is not meaningful for V2 events.

## Proof of Concept
Create a Move module that emits multiple Event V2 events:

```move
module test_addr::event_test {
    use std::signer;
    use aptos_framework::event;
    
    struct TestEvent has drop, store {
        value: u64,
    }
    
    public entry fun emit_multiple_events(account: &signer) {
        // Emit first event
        event::emit(TestEvent { value: 1 });
        
        // Emit second event  
        event::emit(TestEvent { value: 2 });
        
        // Emit third event
        event::emit(TestEvent { value: 3 });
    }
}
```

Execute this from multiple transactions. Query the indexer events table:

```sql
SELECT transaction_version, type, data, event_index, inserted_at 
FROM events 
WHERE account_address = '0x0000000000000000000000000000000000000000000000000000000000000000'
  AND creation_number = 0 
  AND sequence_number = 0;
```

**Expected**: Multiple rows showing all emitted events
**Actual**: Single row with mixed data from different events, with later events completely lost

## Notes
This vulnerability affects the indexer component specifically, not the blockchain consensus layer. However, indexer data integrity is critical for ecosystem functionality as most DApps and tools rely on indexed data rather than directly querying nodes. The silent nature of the failure (no error thrown) makes this particularly insidious as developers may not realize their event data is incomplete until investigating specific discrepancies.

### Citations

**File:** api/types/src/transaction.rs (L48-52)
```rust
static DUMMY_GUID: Lazy<EventGuid> = Lazy::new(|| EventGuid {
    creation_number: U64::from(0u64),
    account_address: Address::from(AccountAddress::ZERO),
});
static DUMMY_SEQUENCE_NUMBER: Lazy<U64> = Lazy::new(|| U64::from(0));
```

**File:** api/types/src/transaction.rs (L886-891)
```rust
            ContractEvent::V2(v2) => Self {
                guid: *DUMMY_GUID,
                sequence_number: *DUMMY_SEQUENCE_NUMBER,
                typ: v2.type_tag().into(),
                data,
            },
```

**File:** crates/indexer/src/models/events.rs (L43-59)
```rust
    pub fn from_event(
        event: &APIEvent,
        transaction_version: i64,
        transaction_block_height: i64,
        event_index: i64,
    ) -> Self {
        Event {
            account_address: standardize_address(&event.guid.account_address.to_string()),
            creation_number: event.guid.creation_number.0 as i64,
            sequence_number: event.sequence_number.0 as i64,
            transaction_version,
            transaction_block_height,
            type_: event.typ.to_string(),
            data: event.data.clone(),
            event_index: Some(event_index),
        }
    }
```

**File:** crates/indexer/migrations/2022-08-08-043603_core_tables/up.sql (L218-222)
```sql
  PRIMARY KEY (
    account_address,
    creation_number,
    sequence_number
  ),
```

**File:** crates/indexer/src/processors/default_processor.rs (L276-296)
```rust
fn insert_events(
    conn: &mut PgConnection,
    items_to_insert: &[EventModel],
) -> Result<(), diesel::result::Error> {
    use schema::events::dsl::*;
    let chunks = get_chunks(items_to_insert.len(), EventModel::field_count());
    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::events::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict((account_address, creation_number, sequence_number))
                .do_update()
                .set((
                    inserted_at.eq(excluded(inserted_at)),
                    event_index.eq(excluded(event_index)),
                )),
            None,
        )?;
    }
    Ok(())
```
