# Audit Report

## Title
DKG Transcript Deserialization DoS via Unbounded Vector Allocation

## Summary
The DKG (Distributed Key Generation) transcript deserialization in `weighted_protocol.rs` lacks size validation before deserializing BCS-encoded data, allowing malicious validators to send transcripts with extremely large vectors that consume excessive CPU and memory during elliptic curve point validation, causing validator node slowdowns during epoch transitions.

## Finding Description

The vulnerability exists in the DKG transcript deserialization flow where size validation occurs **after** expensive cryptographic operations: [1](#0-0) 

The `Transcript` struct contains multiple vectors of elliptic curve points: [2](#0-1) 

When a peer validator sends a transcript via RPC, it is deserialized **before** any size checks: [3](#0-2) 

The size validation in `check_sizes()` only occurs during `verify()`, which is called **after** deserialization: [4](#0-3) 

**Attack Path:**
1. Malicious validator crafts a transcript with vectors containing 150,000 elements each (vs. legitimate ~414 elements for mainnet)
2. Serializes it to ~50 MB (within 64 MiB network limit)
3. Sends via DKG RPC to target validators
4. Target nodes deserialize, allocating memory and performing ~750,000 elliptic curve point validations (via `GroupEncoding::from_bytes`)
5. Each point validation requires expensive cryptographic operations
6. Only after all deserialization completes, `verify()` rejects the oversized transcript
7. Attacker repeats from multiple validator identities or same identity repeatedly

**Broken Invariant:** "Resource Limits: All operations must respect gas, storage, and computational limits"

The legitimate total weight W for mainnet is ~414 validators: [5](#0-4) 

The network allows messages up to 64 MiB: [6](#0-5) 

## Impact Explanation

This qualifies as **MEDIUM severity** per Aptos bug bounty criteria:
- Causes validator node resource exhaustion during DKG protocol execution
- Each malicious transcript consumes seconds of CPU time and ~50 MB memory
- Multiple concurrent attacks could significantly slow validator operations
- Could delay or disrupt epoch transitions, affecting network liveness
- Does not directly cause fund loss or consensus violations, but affects availability

The impact is limited to DKG phase (during epoch transitions) and requires attacker to be a validator, preventing CRITICAL/HIGH classification.

## Likelihood Explanation

**Likelihood: MEDIUM to HIGH**
- Any malicious validator in the active set can exploit this
- No collusion required - single validator can attack
- Attack is simple to execute (craft oversized vectors, send via RPC)
- DKG runs during every epoch transition (regular occurrence)
- Mitigation requires validator operators to detect and isolate malicious peer
- Attack can be repeated from different validator identities if attacker controls multiple validators

## Recommendation

**Add size validation before deserialization:**

```rust
impl TryFrom<&[u8]> for Transcript {
    type Error = CryptoMaterialError;

    fn try_from(bytes: &[u8]) -> Result<Self, Self::Error> {
        // Add max size check before deserialization
        const MAX_TRANSCRIPT_SIZE: usize = 10 * 1024 * 1024; // 10 MB reasonable limit
        if bytes.len() > MAX_TRANSCRIPT_SIZE {
            return Err(CryptoMaterialError::DeserializationError);
        }
        
        let transcript = bcs::from_bytes::<Transcript>(bytes)
            .map_err(|_| CryptoMaterialError::DeserializationError)?;
        
        // Add element count validation immediately after deserialization
        const MAX_VECTOR_ELEMENTS: usize = 10000; // Conservative upper bound
        if transcript.R.len() > MAX_VECTOR_ELEMENTS 
            || transcript.R_hat.len() > MAX_VECTOR_ELEMENTS
            || transcript.V.len() > MAX_VECTOR_ELEMENTS
            || transcript.V_hat.len() > MAX_VECTOR_ELEMENTS
            || transcript.C.len() > MAX_VECTOR_ELEMENTS {
            return Err(CryptoMaterialError::DeserializationError);
        }
        
        Ok(transcript)
    }
}
```

Apply the same fix to all transcript types:
- `crates/aptos-dkg/src/pvss/insecure_field/transcript.rs`
- `crates/aptos-dkg/src/pvss/das/unweighted_protocol.rs`
- `crates/aptos-dkg/src/pvss/chunky/weighted_transcript.rs`

## Proof of Concept

```rust
#[test]
fn test_oversized_transcript_dos() {
    use blstrs::{G1Projective, G2Projective, Scalar};
    use group::Group;
    
    // Create malicious transcript with 150,000 elements
    let malicious_size = 150_000;
    let malicious_transcript = Transcript {
        soks: vec![],
        R: vec![G1Projective::generator(); malicious_size],
        R_hat: vec![G2Projective::generator(); malicious_size],
        V: vec![G1Projective::generator(); malicious_size + 1],
        V_hat: vec![G2Projective::generator(); malicious_size + 1],
        C: vec![G1Projective::generator(); malicious_size],
    };
    
    // Serialize
    let bytes = bcs::to_bytes(&malicious_transcript).unwrap();
    println!("Malicious transcript size: {} MB", bytes.len() / (1024 * 1024));
    
    // Measure deserialization time
    let start = std::time::Instant::now();
    let result = Transcript::try_from(bytes.as_slice());
    let elapsed = start.elapsed();
    
    println!("Deserialization took: {:?}", elapsed);
    assert!(elapsed.as_secs() > 1, "Deserialization should take significant time");
    
    // Should fail verification due to size check, but damage already done
    // In production, this would have consumed significant CPU/memory
}
```

### Citations

**File:** crates/aptos-dkg/src/pvss/das/weighted_protocol.rs (L48-72)
```rust
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq, Eq, BCSCryptoHash, CryptoHasher)]
#[allow(non_snake_case)]
pub struct Transcript {
    /// Proofs-of-knowledge (PoKs) for the dealt secret committed in $c = g_2^{p(0)}$.
    /// Since the transcript could have been aggregated from other transcripts with their own
    /// committed secrets in $c_i = g_2^{p_i(0)}$, this is a vector of PoKs for all these $c_i$'s
    /// such that $\prod_i c_i = c$.
    ///
    /// Also contains BLS signatures from each player $i$ on that player's contribution $c_i$, the
    /// player ID $i$ and auxiliary information `aux[i]` provided during dealing.
    soks: Vec<SoK<G1Projective>>,
    /// Commitment to encryption randomness $g_1^{r_j} \in G_1, \forall j \in [W]$
    R: Vec<G1Projective>,
    /// Same as $R$ except uses $g_2$.
    R_hat: Vec<G2Projective>,
    /// First $W$ elements are commitments to the evaluations of $p(X)$: $g_1^{p(\omega^i)}$,
    /// where $i \in [W]$. Last element is $g_1^{p(0)}$ (i.e., the dealt public key).
    V: Vec<G1Projective>,
    /// Same as $V$ except uses $g_2$.
    V_hat: Vec<G2Projective>,
    /// ElGamal encryption of the $j$th share of player $i$:
    /// i.e., $C[s_i+j-1] = h_1^{p(\omega^{s_i + j - 1})} ek_i^{r_j}, \forall i \in [n], j \in [w_i]$.
    /// We sometimes denote $C[s_i+j-1]$ by C_{i, j}.
    C: Vec<G1Projective>,
}
```

**File:** crates/aptos-dkg/src/pvss/das/weighted_protocol.rs (L82-90)
```rust
impl TryFrom<&[u8]> for Transcript {
    type Error = CryptoMaterialError;

    fn try_from(bytes: &[u8]) -> Result<Self, Self::Error> {
        // NOTE: The `serde` implementation in `blstrs` already performs the necessary point validation
        // by ultimately calling `GroupEncoding::from_bytes`.
        bcs::from_bytes::<Transcript>(bytes).map_err(|_| CryptoMaterialError::DeserializationError)
    }
}
```

**File:** crates/aptos-dkg/src/pvss/das/weighted_protocol.rs (L415-455)
```rust
    fn check_sizes(&self, sc: &WeightedConfigBlstrs) -> anyhow::Result<()> {
        let W = sc.get_total_weight();

        if self.V.len() != W + 1 {
            bail!(
                "Expected {} G_2 (polynomial) commitment elements, but got {}",
                W + 1,
                self.V.len()
            );
        }

        if self.V_hat.len() != W + 1 {
            bail!(
                "Expected {} G_2 (polynomial) commitment elements, but got {}",
                W + 1,
                self.V_hat.len()
            );
        }

        if self.R.len() != W {
            bail!(
                "Expected {} G_1 commitment(s) to ElGamal randomness, but got {}",
                W,
                self.R.len()
            );
        }

        if self.R_hat.len() != W {
            bail!(
                "Expected {} G_2 commitment(s) to ElGamal randomness, but got {}",
                W,
                self.R_hat.len()
            );
        }

        if self.C.len() != W {
            bail!("Expected C of length {}, but got {}", W, self.C.len());
        }

        Ok(())
    }
```

**File:** dkg/src/transcript_aggregation/mod.rs (L88-101)
```rust
        let transcript = bcs::from_bytes(transcript_bytes.as_slice()).map_err(|e| {
            anyhow!("[DKG] adding peer transcript failed with trx deserialization error: {e}")
        })?;
        let mut trx_aggregator = self.trx_aggregator.lock();
        if trx_aggregator.contributors.contains(&metadata.author) {
            return Ok(None);
        }

        S::verify_transcript_extra(&transcript, &self.epoch_state.verifier, false, Some(sender))
            .context("extra verification failed")?;

        S::verify_transcript(&self.dkg_pub_params, &transcript).map_err(|e| {
            anyhow!("[DKG] adding peer transcript failed with trx verification failure: {e}")
        })?;
```

**File:** types/src/dkg/real_dkg/rounding/tests.rs (L16-26)
```rust
fn compute_mainnet_rounding() {
    let validator_stakes = MAINNET_STAKES.to_vec();
    let dkg_rounding = DKGRounding::new(
        &validator_stakes,
        *DEFAULT_SECRECY_THRESHOLD.deref(),
        *DEFAULT_RECONSTRUCT_THRESHOLD.deref(),
        Some(*DEFAULT_FAST_PATH_SECRECY_THRESHOLD.deref()),
    );
    println!("mainnet rounding profile: {:?}", dkg_rounding.profile);
    // Result:
    // mainnet rounding profile: total_weight: 414, secrecy_threshold_in_stake_ratio: 0.5, reconstruct_threshold_in_stake_ratio: 0.60478401144595166257, reconstruct_threshold_in_weights: 228, fast_reconstruct_threshold_in_stake_ratio: Some(0.7714506781126183292), fast_reconstruct_threshold_in_weights: Some(335), validator_weights: [7, 5, 6, 6, 5, 1, 6, 6, 1, 5, 6, 5, 1, 7, 1, 6, 6, 1, 2, 1, 6, 3, 2, 1, 1, 4, 3, 2, 5, 5, 5, 1, 1, 4, 1, 1, 1, 7, 5, 1, 1, 2, 6, 1, 6, 1, 3, 5, 5, 1, 5, 5, 3, 2, 5, 1, 6, 3, 6, 1, 1, 3, 1, 5, 1, 9, 1, 1, 1, 6, 1, 5, 7, 4, 6, 1, 5, 6, 5, 5, 3, 1, 6, 7, 6, 1, 3, 1, 1, 1, 1, 1, 1, 7, 2, 1, 6, 7, 1, 1, 1, 1, 5, 3, 1, 2, 3, 1, 1, 1, 1, 4, 1, 1, 1, 2, 1, 6, 7, 5, 1, 5, 1, 6, 1, 2, 3, 2, 2]
```

**File:** config/src/config/network_config.rs (L45-50)
```rust
pub const MAX_MESSAGE_METADATA_SIZE: usize = 128 * 1024; /* 128 KiB: a buffer for metadata that might be added to messages by networking */
pub const MESSAGE_PADDING_SIZE: usize = 2 * 1024 * 1024; /* 2 MiB: a safety buffer to allow messages to get larger during serialization */
pub const MAX_APPLICATION_MESSAGE_SIZE: usize =
    (MAX_MESSAGE_SIZE - MAX_MESSAGE_METADATA_SIZE) - MESSAGE_PADDING_SIZE; /* The message size that applications should check against */
pub const MAX_FRAME_SIZE: usize = 4 * 1024 * 1024; /* 4 MiB large messages will be chunked into multiple frames and streamed */
pub const MAX_MESSAGE_SIZE: usize = 64 * 1024 * 1024; /* 64 MiB */
```
