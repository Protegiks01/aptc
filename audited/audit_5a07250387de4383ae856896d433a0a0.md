# Audit Report

## Title
Race Condition in Fast-Forward Sync Causes Consensus Node Crash Due to Storage Inconsistency

## Summary
A race condition exists in `fast_forward_sync()` where concurrent commits to AptosDB can occur between saving blocks to ConsensusDB and syncing execution state. This causes `storage.start()` to fail reconciling inconsistent database states, resulting in a node panic and consensus failure.

## Finding Description

The vulnerability exists in the `fast_forward_sync()` method where storage and execution operations are not properly synchronized: [1](#0-0) 

The critical issue occurs in this sequence:

1. **Line 503**: `storage.save_tree()` saves fetched blocks/QCs to ConsensusDB
2. **Lines 506-511**: Pipeline abort (but concurrent commits may have already completed)
3. **Lines 512-514**: `execution_client.sync_to_target()` syncs AptosDB to target
4. **Line 519**: `storage.start()` reads from BOTH databases to construct recovery data

The `sync_to_target()` implementation contains early-return logic when AptosDB is already ahead: [2](#0-1) 

If concurrent consensus operations commit new blocks to AptosDB between steps 1 and 3, then:
- ConsensusDB contains blocks up to round X (saved at line 503)
- AptosDB advances to round Y (where Y > X) due to concurrent commits
- `sync_to_target()` sees AptosDB is already at round Y ≥ X and returns early without syncing
- `storage.start()` attempts to reconcile the inconsistent states

The `storage.start()` method reads from both databases: [3](#0-2) 

It retrieves blocks from ConsensusDB (lines 521-534) and latest ledger info from AptosDB (lines 549-552), then calls `RecoveryData::new()` which invokes `find_root()` to match them.

The `find_root()` method searches for the committed block from AptosDB's ledger info within the blocks retrieved from ConsensusDB: [4](#0-3) 

When the block cannot be found (because AptosDB is ahead of ConsensusDB), `find_root()` returns an error, causing `storage.start()` to return `PartialRecoveryData` instead of `FullRecoveryData`.

The `fast_forward_sync()` expects `FullRecoveryData` and panics otherwise: [5](#0-4) 

**Broken Invariants:**
- **State Consistency**: State transitions are not atomic across ConsensusDB and AptosDB
- **Consensus Safety**: Node crashes during recovery can cause liveness failures

## Impact Explanation

This is a **HIGH severity** vulnerability per Aptos bug bounty criteria:
- **Validator node crashes**: Panic causes immediate node termination
- **API crashes**: Node becomes unavailable requiring manual restart
- **Significant protocol violations**: Breaks atomicity of state synchronization

If multiple nodes trigger this race condition simultaneously (e.g., during network partition recovery), it could escalate to **CRITICAL severity** causing total loss of liveness.

The vulnerability affects normal consensus operations - any node performing fast-forward sync is at risk when concurrent executions commit to AptosDB.

## Likelihood Explanation

**HIGH likelihood** - This race condition can occur naturally without attacker involvement:

1. **Common trigger**: Fast-forward sync is initiated when nodes receive sync_info from peers (called via `add_certs()` → `sync_to_highest_quorum_cert()`)
2. **Concurrent operations**: Aptos uses decoupled execution where consensus and execution pipelines run concurrently
3. **No synchronization**: No global lock prevents commits to AptosDB during fast-forward sync
4. **Race window**: The window between `save_tree()` (line 503) and `sync_to_target()` (line 512) is several operations long, providing ample opportunity for race conditions

The `write_mutex` in ExecutionProxy only protects operations WITHIN `sync_to_target()`, not the entire fast-forward sync sequence: [6](#0-5) 

## Recommendation

**Fix: Implement atomic state synchronization with proper locking**

```rust
async fn sync_to_highest_quorum_cert(
    &self,
    highest_quorum_cert: QuorumCert,
    highest_commit_cert: WrappedLedgerInfo,
    retriever: &mut BlockRetriever,
) -> anyhow::Result<()> {
    // Acquire execution lock BEFORE starting fast_forward_sync
    let execution_guard = self.execution_client.acquire_sync_lock().await;
    
    let (root, root_metadata, blocks, quorum_certs) = Self::fast_forward_sync(
        &highest_quorum_cert,
        &highest_commit_cert,
        retriever,
        self.storage.clone(),
        self.execution_client.clone(),
        self.payload_manager.clone(),
        self.order_vote_enabled,
        self.window_size,
        Some(self),
    )
    .await?
    .take();
    
    // Release lock after rebuild completes
    drop(execution_guard);
    
    self.rebuild(root, root_metadata, blocks, quorum_certs).await;
    // ... rest of function
}
```

Additionally, modify `storage.start()` to validate consistency:

```rust
// In storage.start(), after getting data from both DBs:
if latest_ledger_info.ledger_info().round() > blocks.iter().map(|b| b.round()).max().unwrap_or(0) {
    warn!("AptosDB ahead of ConsensusDB during recovery, triggering re-sync");
    return LivenessStorageData::PartialRecoveryData(ledger_recovery_data);
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_fast_forward_sync_race_condition() {
    // Setup: Create a BlockStore with mocked storage and execution client
    let (storage, execution_client, block_store) = setup_test_environment();
    
    // Thread 1: Start fast_forward_sync
    let sync_handle = tokio::spawn(async move {
        let retriever = create_block_retriever();
        block_store.sync_to_highest_quorum_cert(
            create_qc_for_round(100),
            create_commit_cert_for_round(100),
            &mut retriever,
        ).await
    });
    
    // Thread 2: Simulate concurrent commit to AptosDB
    tokio::time::sleep(Duration::from_millis(10)).await; // Wait for save_tree()
    let commit_handle = tokio::spawn(async move {
        // This advances AptosDB to round 110
        execution_client.commit_blocks(create_blocks_for_rounds(101..=110)).await;
    });
    
    // Wait for both operations
    let sync_result = sync_handle.await.unwrap();
    commit_handle.await.unwrap();
    
    // Assert: Should panic with "Failed to construct recovery data"
    assert!(sync_result.is_err());
    assert!(sync_result.unwrap_err().to_string().contains("Failed to construct recovery data"));
}
```

## Notes

This vulnerability demonstrates a critical synchronization gap in the consensus recovery path. The issue stems from the architectural decision to use separate databases (ConsensusDB for consensus state, AptosDB for execution state) without proper atomicity guarantees during fast-forward sync. The Arc::clone() operations at lines 299-301 are not the root cause - they correctly share the same underlying objects. The real issue is the lack of transactional consistency across the two databases during the multi-step sync process.

### Citations

**File:** consensus/src/block_storage/sync_manager.rs (L503-524)
```rust
        storage.save_tree(blocks.clone(), quorum_certs.clone())?;
        // abort any pending executor tasks before entering state sync
        // with zaptos, things can run before hitting buffer manager
        if let Some(block_store) = maybe_block_store {
            monitor!(
                "abort_pipeline_for_state_sync",
                block_store.abort_pipeline_for_state_sync().await
            );
        }
        execution_client
            .sync_to_target(highest_commit_cert.ledger_info().clone())
            .await?;

        // we do not need to update block_tree.highest_commit_decision_ledger_info here
        // because the block_tree is going to rebuild itself.

        let recovery_data = match storage.start(order_vote_enabled, window_size) {
            LivenessStorageData::FullRecoveryData(recovery_data) => recovery_data,
            _ => panic!("Failed to construct recovery data after fast forward sync"),
        };

        Ok(recovery_data)
```

**File:** consensus/src/state_computer.rs (L178-179)
```rust
        // Grab the logical time lock and calculate the target logical time
        let mut latest_logical_time = self.write_mutex.lock().await;
```

**File:** consensus/src/state_computer.rs (L187-194)
```rust
        // The pipeline phase already committed beyond the target block timestamp, just return.
        if *latest_logical_time >= target_logical_time {
            warn!(
                "State sync target {:?} is lower than already committed logical time {:?}",
                target_logical_time, *latest_logical_time
            );
            return Ok(());
        }
```

**File:** consensus/src/persistent_liveness_storage.rs (L233-236)
```rust
        let root_idx = blocks
            .iter()
            .position(|block| block.id() == root_id)
            .ok_or_else(|| format_err!("unable to find root: {}", root_id))?;
```

**File:** consensus/src/persistent_liveness_storage.rs (L519-596)
```rust
    fn start(&self, order_vote_enabled: bool, window_size: Option<u64>) -> LivenessStorageData {
        info!("Start consensus recovery.");
        let raw_data = self
            .db
            .get_data()
            .expect("unable to recover consensus data");

        let last_vote = raw_data
            .0
            .map(|bytes| bcs::from_bytes(&bytes[..]).expect("unable to deserialize last vote"));

        let highest_2chain_timeout_cert = raw_data.1.map(|b| {
            bcs::from_bytes(&b).expect("unable to deserialize highest 2-chain timeout cert")
        });
        let blocks = raw_data.2;
        let quorum_certs: Vec<_> = raw_data.3;
        let blocks_repr: Vec<String> = blocks.iter().map(|b| format!("\n\t{}", b)).collect();
        info!(
            "The following blocks were restored from ConsensusDB : {}",
            blocks_repr.concat()
        );
        let qc_repr: Vec<String> = quorum_certs
            .iter()
            .map(|qc| format!("\n\t{}", qc))
            .collect();
        info!(
            "The following quorum certs were restored from ConsensusDB: {}",
            qc_repr.concat()
        );
        // find the block corresponding to storage latest ledger info
        let latest_ledger_info = self
            .aptos_db
            .get_latest_ledger_info()
            .expect("Failed to get latest ledger info.");
        let accumulator_summary = self
            .aptos_db
            .get_accumulator_summary(latest_ledger_info.ledger_info().version())
            .expect("Failed to get accumulator summary.");
        let ledger_recovery_data = LedgerRecoveryData::new(latest_ledger_info);

        match RecoveryData::new(
            last_vote,
            ledger_recovery_data.clone(),
            blocks,
            accumulator_summary.into(),
            quorum_certs,
            highest_2chain_timeout_cert,
            order_vote_enabled,
            window_size,
        ) {
            Ok(mut initial_data) => {
                (self as &dyn PersistentLivenessStorage)
                    .prune_tree(initial_data.take_blocks_to_prune())
                    .expect("unable to prune dangling blocks during restart");
                if initial_data.last_vote.is_none() {
                    self.db
                        .delete_last_vote_msg()
                        .expect("unable to cleanup last vote");
                }
                if initial_data.highest_2chain_timeout_certificate.is_none() {
                    self.db
                        .delete_highest_2chain_timeout_certificate()
                        .expect("unable to cleanup highest 2-chain timeout cert");
                }
                info!(
                    "Starting up the consensus state machine with recovery data - [last_vote {}], [highest timeout certificate: {}]",
                    initial_data.last_vote.as_ref().map_or_else(|| "None".to_string(), |v| v.to_string()),
                    initial_data.highest_2chain_timeout_certificate().as_ref().map_or_else(|| "None".to_string(), |v| v.to_string()),
                );

                LivenessStorageData::FullRecoveryData(initial_data)
            },
            Err(e) => {
                error!(error = ?e, "Failed to construct recovery data");
                LivenessStorageData::PartialRecoveryData(ledger_recovery_data)
            },
        }
    }
```
