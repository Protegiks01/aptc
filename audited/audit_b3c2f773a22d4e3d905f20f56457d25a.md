# Audit Report

## Title
Unbounded Write Set Deserialization Denial of Service via get_write_set_iterator()

## Summary
The `get_write_set_iterator()` function in AptosDB lacks size validation before deserializing individual WriteSet objects, allowing attackers to cause memory exhaustion and CPU overload by requesting iterators over versions with extremely large write sets (such as genesis). This vulnerability can be exploited by any network peer through the storage service API to cause validator node slowdowns or crashes.

## Finding Description

The vulnerability exists in the write set iterator implementation. When external peers request transaction data through the state sync storage service, the system creates iterators to fetch write sets from the database. The critical flaw is that **individual WriteSet objects are deserialized without any size limit checks**, even though the total number of transactions is bounded.

**Attack Flow:**

1. An attacker (any network peer - validator, VFN, or public full node) sends a storage service request for `TransactionOutputsWithProofRequest` or `TransactionsWithProofRequest` starting from version 0 (genesis) [1](#0-0) 

2. The storage service handler creates a write set iterator via `get_write_set_iterator()` [2](#0-1) 

3. The function only validates the **count** of transactions (MAX_REQUEST_LIMIT = 20,000) but does NOT check the **size** of individual WriteSet objects [3](#0-2) 

4. When `multizip_iterator.next()` is called, it invokes the write set iterator which deserializes the WriteSet using `bcs::from_bytes()` **without any size limit** [4](#0-3) 

5. For genesis (version 0), the WriteSet can be **arbitrarily large** because genesis uses unlimited change set configs that bypass the normal 10 MB limit [5](#0-4) 

6. The response size check happens **AFTER** deserialization, meaning the memory/CPU damage is already done [6](#0-5) 

**Why Genesis is Problematic:**

Genesis write sets contain all framework modules, initial resources, and genesis state initialization. The normal transaction limit of 10 MB (`max_bytes_all_write_ops_per_transaction`) is bypassed for genesis using unlimited configs: [7](#0-6) 

The WriteSet structure is a `BTreeMap<StateKey, WriteOp>` where each WriteOp can contain arbitrarily large data: [8](#0-7) 

## Impact Explanation

This is a **HIGH severity** vulnerability per Aptos bug bounty criteria:

1. **Validator Node Slowdowns**: Deserializing large write sets (potentially hundreds of megabytes for genesis) consumes significant CPU time, blocking the storage service thread and degrading node performance.

2. **Potential API Crashes**: If the genesis write set exceeds available memory, repeated requests could trigger OOM (Out Of Memory) conditions, crashing the node or requiring restart.

3. **Network-Wide DoS Potential**: Since the storage service is accessible to all network peers (validators, VFNs, public full nodes), multiple attackers could simultaneously request large write sets, amplifying the resource exhaustion across the network.

4. **No Rate Limiting on Size**: While the request moderator validates that requests are serviceable, it only checks version range validity, not the size impact of the data being requested.

## Likelihood Explanation

**Likelihood: HIGH**

The attack is trivially easy to execute:
- **No special privileges required**: Any network peer can send storage service requests
- **Simple attack vector**: Single request for transactions starting from version 0
- **No detection**: The request appears legitimate as genesis is a valid version
- **Repeatable**: Attacker can send multiple requests to sustain resource exhaustion
- **Wide attack surface**: All nodes serving state sync data (validators, VFNs, full nodes) are vulnerable

The only limiting factor is that the attacker must be connected as a network peer, but public full nodes can connect to the network freely.

## Recommendation

Implement size validation **before** deserializing WriteSet objects in the iterator. Add a configuration parameter for maximum individual write set size and check it during iteration:

**Fix 1: Add size check in write set iterator**

Modify `storage/aptosdb/src/ledger_db/write_set_db.rs` to check serialized size before deserialization:

```rust
pub(crate) fn get_write_set_iter(
    &self,
    start_version: Version,
    num_transactions: usize,
) -> Result<impl Iterator<Item = Result<WriteSet>> + '_> {
    const MAX_WRITE_SET_SIZE: usize = 50 * 1024 * 1024; // 50 MB limit
    
    let mut iter = self.db.iter::<WriteSetSchema>()?;
    iter.seek(&start_version)?;
    
    Ok(iter
        .expect_continuous_versions(start_version, num_transactions)?
        .map(move |result| {
            result.and_then(|write_set| {
                let serialized_size = bcs::to_bytes(&write_set)?.len();
                if serialized_size > MAX_WRITE_SET_SIZE {
                    Err(AptosDbError::TooManyRequested(
                        start_version,
                        serialized_size as u64,
                    ).into())
                } else {
                    Ok(write_set)
                }
            })
        }))
}
```

**Fix 2: Add pre-deserialization size check in schema**

Modify `storage/aptosdb/src/schema/write_set/mod.rs` to enforce size limits during decoding:

```rust
impl ValueCodec<WriteSetSchema> for WriteSet {
    fn encode_value(&self) -> Result<Vec<u8>> {
        bcs::to_bytes(self).map_err(Into::into)
    }

    fn decode_value(data: &[u8]) -> Result<Self> {
        const MAX_WRITE_SET_BYTES: usize = 50 * 1024 * 1024; // 50 MB
        ensure!(
            data.len() <= MAX_WRITE_SET_BYTES,
            "WriteSet exceeds maximum size: {} bytes (max: {})",
            data.len(),
            MAX_WRITE_SET_BYTES
        );
        bcs::from_bytes(data).map_err(Into::into)
    }
}
```

## Proof of Concept

**Rust Integration Test:**

```rust
use aptos_storage_interface::DbReader;
use aptos_types::transaction::Version;

#[test]
fn test_large_write_set_dos() {
    // Setup: Create a test database with genesis
    let (db, _) = create_test_db_with_genesis();
    
    // Attack: Request write set iterator starting from genesis
    let start_version: Version = 0;
    let limit = 1; // Only need one item to trigger deserialization
    
    // This should either:
    // 1. Take excessive time to deserialize (DoS via CPU)
    // 2. Consume excessive memory (DoS via OOM)
    // 3. Be rejected with a size limit error (if fix is applied)
    
    let result = db.get_write_set_iterator(start_version, limit);
    
    match result {
        Ok(mut iter) => {
            let start_time = std::time::Instant::now();
            let start_mem = get_current_memory_usage();
            
            // Attempt to deserialize first write set (genesis)
            let write_set_result = iter.next();
            
            let elapsed = start_time.elapsed();
            let mem_delta = get_current_memory_usage() - start_mem;
            
            // Assert that either:
            // - Deserialization took > 1 second (CPU DoS)
            // - Memory increased by > 50 MB (memory DoS)
            // - Request was rejected (proper fix)
            
            assert!(
                elapsed.as_secs() > 1 || mem_delta > 50_000_000,
                "Genesis write set deserialization vulnerable to DoS"
            );
        }
        Err(e) => {
            // If properly fixed, should get size limit error
            assert!(e.to_string().contains("size") || e.to_string().contains("limit"));
        }
    }
}
```

**Network Attack Simulation:**

An attacker can send storage service requests programmatically:

```rust
// Attacker code (pseudocode)
use aptos_storage_service_types::requests::*;

// Connect to target node as a peer
let peer_network_id = connect_to_aptos_network();

// Send malicious request for genesis transactions
let request = StorageServiceRequest::new(
    DataRequest::GetTransactionOutputsWithProof(
        TransactionOutputsWithProofRequest {
            proof_version: 0,
            start_version: 0,  // Genesis
            end_version: 1,    // Just one transaction
        }
    )
);

// This triggers the vulnerable code path
// Repeat to sustain DoS
loop {
    send_request(peer_network_id, request.clone());
    std::thread::sleep(Duration::from_millis(100));
}
```

**Notes:**
The exact impact depends on the actual size of genesis in the deployed network. Mainnet genesis likely contains substantial data (framework modules, genesis validators, initial resources), making this a practical attack vector. The vulnerability violates the **Resource Limits** invariant that "all operations must respect gas, storage, and computational limits."

### Citations

**File:** state-sync/storage-service/server/src/storage.rs (L599-601)
```rust
        let transaction_write_set_iterator = self
            .storage
            .get_write_set_iterator(start_version, num_outputs_to_fetch)?;
```

**File:** state-sync/storage-service/server/src/storage.rs (L649-676)
```rust
                    let num_transaction_bytes = get_num_serialized_bytes(&transaction)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_info_bytes = get_num_serialized_bytes(&info)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_output_bytes = get_num_serialized_bytes(&output)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_auxiliary_info_bytes =
                        get_num_serialized_bytes(&persisted_auxiliary_info).map_err(|error| {
                            Error::UnexpectedErrorEncountered(error.to_string())
                        })?;

                    // Add the data items to the lists
                    let total_serialized_bytes = num_transaction_bytes
                        + num_info_bytes
                        + num_output_bytes
                        + num_auxiliary_info_bytes;
                    if response_progress_tracker.data_items_fits_in_response(
                        !is_transaction_or_output_request,
                        total_serialized_bytes,
                    ) {
                        transactions_and_outputs.push((transaction, output));
                        transaction_infos.push(info);
                        persisted_auxiliary_infos.push(persisted_auxiliary_info);

                        response_progress_tracker.add_data_item(total_serialized_bytes);
                    } else {
                        break; // Cannot add any more data items
                    }
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L531-546)
```rust
    fn get_write_set_iterator(
        &self,
        start_version: Version,
        limit: u64,
    ) -> Result<Box<dyn Iterator<Item = Result<WriteSet>> + '_>> {
        gauged_api("get_write_set_iterator", || {
            error_if_too_many_requested(limit, MAX_REQUEST_LIMIT)?;
            self.error_if_ledger_pruned("Transaction", start_version)?;

            let iter = self
                .ledger_db
                .write_set_db()
                .get_write_set_iter(start_version, limit as usize)?;
            Ok(Box::new(iter) as Box<dyn Iterator<Item = Result<WriteSet>> + '_>)
        })
    }
```

**File:** storage/storage-interface/src/lib.rs (L56-58)
```rust
// This is last line of defense against large queries slipping through external facing interfaces,
// like the API and State Sync, etc.
pub const MAX_REQUEST_LIMIT: u64 = 20_000;
```

**File:** storage/aptosdb/src/schema/write_set/mod.rs (L39-47)
```rust
impl ValueCodec<WriteSetSchema> for WriteSet {
    fn encode_value(&self) -> Result<Vec<u8>> {
        bcs::to_bytes(self).map_err(Into::into)
    }

    fn decode_value(data: &[u8]) -> Result<Self> {
        bcs::from_bytes(data).map_err(Into::into)
    }
}
```

**File:** aptos-move/aptos-vm-types/src/storage/change_set_configs.rs (L20-38)
```rust
    pub fn unlimited_at_gas_feature_version(gas_feature_version: u64) -> Self {
        Self::new_impl(
            gas_feature_version,
            u64::MAX,
            u64::MAX,
            u64::MAX,
            u64::MAX,
            u64::MAX,
        )
    }

    pub fn new(feature_version: u64, gas_params: &AptosGasParameters) -> Self {
        if feature_version >= 5 {
            Self::from_gas_params(feature_version, gas_params)
        } else if feature_version >= 3 {
            Self::for_feature_version_3()
        } else {
            Self::unlimited_at_gas_feature_version(feature_version)
        }
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L154-162)
```rust
            max_bytes_per_write_op: NumBytes,
            { 5.. => "max_bytes_per_write_op" },
            1 << 20, // a single state item is 1MB max
        ],
        [
            max_bytes_all_write_ops_per_transaction: NumBytes,
            { 5.. => "max_bytes_all_write_ops_per_transaction" },
            10 << 20, // all write ops from a single transaction are 10MB max
        ],
```

**File:** types/src/write_set.rs (L548-554)
```rust
#[derive(BCSCryptoHash, Clone, CryptoHasher, Debug, Default, Eq, PartialEq)]
pub struct WriteSet {
    value: ValueWriteSet,
    /// TODO(HotState): this field is not serialized for now.
    hotness: BTreeMap<StateKey, HotStateOp>,
}

```
