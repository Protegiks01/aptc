# Audit Report

## Title
Fast Sync Status Inconsistency Due to Non-Atomic State Transition

## Summary
The `get_state_snapshot_receiver()` function in `FastSyncStorageWrapper` sets the fast sync status to `STARTED` before attempting to create the snapshot receiver. If the underlying receiver creation fails, the status remains `STARTED` while no valid receiver exists, creating a read/write database split that violates state consistency guarantees.

## Finding Description

The vulnerability exists in the `FastSyncStorageWrapper::get_state_snapshot_receiver()` implementation. The function performs a non-atomic state transition by setting `fast_sync_status` to `STARTED` before calling the potentially-failing underlying database operation. [1](#0-0) 

When `fast_sync_status` is set to `STARTED`, the wrapper's routing logic changes:
- **Write operations** route to `db_for_fast_sync`
- **Read operations** route to `temporary_db_with_genesis` (until status becomes `FINISHED`) [2](#0-1) 

The underlying `get_state_snapshot_receiver()` call chain can fail during:

1. **Database reads** in `JellyfishMerkleRestore::new()` when checking for existing root nodes or rightmost leaves: [3](#0-2) 

2. **Root hash validation** when a previous restore exists with mismatched hash: [4](#0-3) 

3. **Partial node recovery** which performs multiple database reads: [5](#0-4) 

The caller (state sync driver) expects this call to succeed and panics on failure: [6](#0-5) 

When the panic occurs, `fast_sync_status` remains `STARTED`, causing:
- All subsequent reads to go to `temporary_db_with_genesis`
- All subsequent writes to go to `db_for_fast_sync`
- State divergence between the two databases
- Violation of the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs"

## Impact Explanation

This constitutes a **Medium severity** issue per the Aptos bug bounty criteria: "State inconsistencies requiring intervention."

While the status remains `STARTED`:
1. The node enters an inconsistent state with split read/write databases
2. `finalize_state_snapshot()` would pass its assertion check but operate on corrupted state [7](#0-6) 
3. Manual intervention is required to recover the node
4. The node cannot properly complete fast sync bootstrap

This does NOT reach Critical/High severity because:
- No funds are at risk
- The issue affects individual node availability, not network-wide consensus
- Recovery is possible through node restart and database cleanup

## Likelihood Explanation

**Likelihood: Low to Medium**

The vulnerability can be triggered by:
- Database read errors (disk I/O failures, corruption)
- Root hash mismatches from incomplete previous sync attempts
- Storage layer exceptions during `recover_partial_nodes()`

These scenarios are not directly exploitable by external attackers but can occur in production environments due to:
- Hardware failures
- Disk space exhaustion
- Database corruption
- Interrupted sync processes

The existing test confirms developers are aware of the panic behavior but not the status inconsistency: [8](#0-7) 

## Recommendation

Implement atomic state transition with rollback on failure:

```rust
fn get_state_snapshot_receiver(
    &self,
    version: Version,
    expected_root_hash: HashValue,
) -> Result<Box<dyn StateSnapshotReceiver<StateKey, StateValue>>> {
    // Attempt to create receiver BEFORE changing status
    let receiver = self.get_aptos_db_write_ref()
        .get_state_snapshot_receiver(version, expected_root_hash)?;
    
    // Only set status to STARTED after successful receiver creation
    *self.fast_sync_status.write() = FastSyncStatus::STARTED;
    
    Ok(receiver)
}
```

Additionally, add status reset logic in error paths or implement a `FAILED` status state to explicitly track failures.

## Proof of Concept

```rust
use aptos_storage_interface::AptosDbError;

#[tokio::test]
async fn test_fast_sync_status_inconsistency() {
    // Create FastSyncStorageWrapper with mock DB
    let mut mock_db = MockAptosDB::new();
    
    // Configure DB to fail on get_state_snapshot_receiver
    mock_db.expect_get_state_snapshot_receiver()
        .returning(|_, _| Err(AptosDbError::Other("DB corruption".into())));
    
    let wrapper = FastSyncStorageWrapper {
        db_for_fast_sync: Arc::new(mock_db),
        temporary_db_with_genesis: Arc::new(create_genesis_db()),
        fast_sync_status: Arc::new(RwLock::new(FastSyncStatus::UNKNOWN)),
    };
    
    // Verify initial status
    assert_eq!(wrapper.get_fast_sync_status(), FastSyncStatus::UNKNOWN);
    
    // Call get_state_snapshot_receiver - will fail
    let result = wrapper.get_state_snapshot_receiver(100, HashValue::zero());
    assert!(result.is_err());
    
    // BUG: Status is now STARTED despite failure
    assert_eq!(wrapper.get_fast_sync_status(), FastSyncStatus::STARTED);
    
    // Demonstrate read/write split:
    // Reads go to temporary_db_with_genesis
    assert!(std::ptr::eq(
        wrapper.get_aptos_db_read_ref(),
        wrapper.temporary_db_with_genesis.as_ref()
    ));
    
    // Writes go to db_for_fast_sync
    assert!(std::ptr::eq(
        wrapper.get_aptos_db_write_ref(),
        wrapper.db_for_fast_sync.as_ref()
    ));
    
    // State inconsistency confirmed
}
```

**Notes:**
This vulnerability stems from violating the atomicity principle in state transitions. The status change should only be committed after all fallible operations succeed. While not directly exploitable by external attackers, it represents a significant reliability issue that can cause node failures during fast sync operations, requiring manual intervention to recover.

### Citations

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L126-140)
```rust
    pub(crate) fn get_aptos_db_read_ref(&self) -> &AptosDB {
        if self.is_fast_sync_bootstrap_finished() {
            self.db_for_fast_sync.as_ref()
        } else {
            self.temporary_db_with_genesis.as_ref()
        }
    }

    pub(crate) fn get_aptos_db_write_ref(&self) -> &AptosDB {
        if self.is_fast_sync_bootstrap_started() || self.is_fast_sync_bootstrap_finished() {
            self.db_for_fast_sync.as_ref()
        } else {
            self.temporary_db_with_genesis.as_ref()
        }
    }
```

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L144-152)
```rust
    fn get_state_snapshot_receiver(
        &self,
        version: Version,
        expected_root_hash: HashValue,
    ) -> Result<Box<dyn StateSnapshotReceiver<StateKey, StateValue>>> {
        *self.fast_sync_status.write() = FastSyncStatus::STARTED;
        self.get_aptos_db_write_ref()
            .get_state_snapshot_receiver(version, expected_root_hash)
    }
```

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L154-170)
```rust
    fn finalize_state_snapshot(
        &self,
        version: Version,
        output_with_proof: TransactionOutputListWithProofV2,
        ledger_infos: &[LedgerInfoWithSignatures],
    ) -> Result<()> {
        let status = self.get_fast_sync_status();
        assert_eq!(status, FastSyncStatus::STARTED);
        self.get_aptos_db_write_ref().finalize_state_snapshot(
            version,
            output_with_proof,
            ledger_infos,
        )?;
        let mut status = self.fast_sync_status.write();
        *status = FastSyncStatus::FINISHED;
        Ok(())
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L189-235)
```rust
    pub fn new<D: 'static + TreeReader<K> + TreeWriter<K>>(
        store: Arc<D>,
        version: Version,
        expected_root_hash: HashValue,
        async_commit: bool,
    ) -> Result<Self> {
        let tree_reader = Arc::clone(&store);
        let (finished, partial_nodes, previous_leaf) = if let Some(root_node) =
            tree_reader.get_node_option(&NodeKey::new_empty_path(version), "restore")?
        {
            info!("Previous restore is complete, checking root hash.");
            ensure!(
                root_node.hash() == expected_root_hash,
                "Previous completed restore has root hash {}, expecting {}",
                root_node.hash(),
                expected_root_hash,
            );
            (true, vec![], None)
        } else if let Some((node_key, leaf_node)) = tree_reader.get_rightmost_leaf(version)? {
            // If the system crashed in the middle of the previous restoration attempt, we need
            // to recover the partial nodes to the state right before the crash.
            (
                false,
                Self::recover_partial_nodes(tree_reader.as_ref(), version, node_key)?,
                Some(leaf_node),
            )
        } else {
            (
                false,
                vec![InternalInfo::new_empty(NodeKey::new_empty_path(version))],
                None,
            )
        };

        Ok(Self {
            store,
            version,
            partial_nodes,
            frozen_nodes: HashMap::new(),
            previous_leaf,
            num_keys_received: 0,
            expected_root_hash,
            finished,
            async_commit,
            async_commit_result: None,
        })
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L267-300)
```rust
    fn recover_partial_nodes(
        store: &dyn TreeReader<K>,
        version: Version,
        rightmost_leaf_node_key: NodeKey,
    ) -> Result<Vec<InternalInfo<K>>> {
        ensure!(
            !rightmost_leaf_node_key.nibble_path().is_empty(),
            "Root node would not be written until entire restoration process has completed \
             successfully.",
        );

        // Start from the parent of the rightmost leaf. If this internal node exists in storage, it
        // is not a partial node. Go to the parent node and repeat until we see a node that does
        // not exist. This node and all its ancestors will be the partial nodes.
        let mut node_key = rightmost_leaf_node_key.gen_parent_node_key();
        while store.get_node_option(&node_key, "restore")?.is_some() {
            node_key = node_key.gen_parent_node_key();
        }

        // Next we reconstruct all the partial nodes up to the root node, starting from the bottom.
        // For all of them, we scan all its possible child positions and see if there is one at
        // each position. If the node is not the bottom one, there is additionally a partial node
        // child at the position `previous_child_index`.
        let mut partial_nodes = vec![];
        // Initialize `previous_child_index` to `None` for the first iteration of the loop so the
        // code below treats it differently.
        let mut previous_child_index = None;

        loop {
            let mut internal_info = InternalInfo::new_empty(node_key.clone());

            for i in 0..previous_child_index.unwrap_or(16) {
                let child_node_key = node_key.gen_child_node_key(version, (i as u8).into());
                if let Some(node) = store.get_node_option(&child_node_key, "restore")? {
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L857-860)
```rust
        let mut state_snapshot_receiver = storage
            .writer
            .get_state_snapshot_receiver(version, expected_root_hash)
            .expect("Failed to initialize the state snapshot receiver!");
```

**File:** state-sync/state-sync-driver/src/tests/storage_synchronizer.rs (L655-684)
```rust
#[should_panic]
async fn test_initialize_state_synchronizer_receiver_error() {
    // Setup the mock db writer. The db writer should always fail.
    let mut db_writer = create_mock_db_writer();
    db_writer
        .expect_get_state_snapshot_receiver()
        .returning(|_, _| {
            Err(AptosDbError::Other(
                "Failed to get snapshot receiver!".to_string(),
            ))
        });

    // Create the storage synchronizer
    let (_, _, _, _, _, mut storage_synchronizer, _) = create_storage_synchronizer(
        create_mock_executor(),
        create_mock_reader_writer(None, Some(db_writer)),
    );

    // Initialize the state synchronizer
    let state_synchronizer_handle = storage_synchronizer
        .initialize_state_synchronizer(
            vec![create_epoch_ending_ledger_info()],
            create_epoch_ending_ledger_info(),
            create_output_list_with_proof(),
        )
        .unwrap();

    // The handler should panic as storage failed to return a snapshot receiver
    state_synchronizer_handle.await.unwrap();
}
```
