# Audit Report

## Title
Epoch Snapshot Pruner Can Delete Required State Sync Data Due to Unbounded Progress Metadata

## Summary
The epoch snapshot pruner calculates its target version based on the latest state checkpoint version minus the prune window, without validating that this target doesn't exceed the latest epoch ending version. This allows the pruner to delete epoch ending snapshot data (stale merkle tree nodes) that are still needed for ongoing state synchronization, breaking the ability of new or recovering nodes to fast sync to recent epochs.

## Finding Description

The vulnerability exists in how the epoch snapshot pruner determines its target pruning version. The pruner is designed to maintain epoch ending snapshots for state sync, but it lacks bounds checking to ensure it doesn't prune beyond the latest epoch ending.

**Root Cause Flow:**

1. **Pruner Target Calculation**: The epoch snapshot pruner's target is set using `get_latest_state_checkpoint_version()`, which returns the most recent committed state checkpoint - not necessarily an epoch ending version. [1](#0-0) 

2. **No Epoch Boundary Validation**: The target version is calculated as `latest_version.saturating_sub(self.prune_window)` without any check that this value doesn't exceed the latest epoch ending version. [2](#0-1) 

3. **Stale Node Classification**: Stale nodes with versions at or before the previous epoch ending are stored in `StaleNodeIndexCrossEpochSchema` - these represent the epoch ending snapshots required for state sync. [3](#0-2) 

4. **Unconditional Pruning**: The pruner deletes all stale nodes with `stale_since_version <= target_version` without verifying these aren't part of a needed epoch ending snapshot. [4](#0-3) [5](#0-4) 

**The Critical Logic Flaw:**

The pruner uses `stale_since_version` (when the node was overwritten) as the criterion for deletion, not the epoch the node belongs to. When nodes from an epoch ending snapshot (e.g., epoch 3 at version 30M) are overwritten shortly after in epoch 4 (versions 30M-40M), they receive `stale_since_version` values in that range. If the pruner target is 40M (calculated as 120M - 80M prune_window), these nodes are deleted even though they belong to an epoch snapshot that should be retained.

**Attack Scenario (Natural Occurrence):**
- Epoch 3 ends at version 30M
- Node continues processing to version 120M
- Pruner calculates target: 120M - 80M = 40M  
- Stale nodes from epoch 3 that were overwritten at versions 30M-40M are pruned
- New node attempting to fast sync to epoch 3 fails - required merkle tree nodes are missing

This violates the design intent documented in the configuration that "epoch ending snapshots are used by state sync in fast sync mode." [6](#0-5) 

**State Sync Dependency on Merkle Nodes:**

When serving state values with proofs, the system must generate merkle range proofs and retrieve root hashes, which require reading nodes from `JellyfishMerkleNodeSchema`: [7](#0-6) [8](#0-7) [9](#0-8) [10](#0-9) 

When these nodes are deleted by the pruner, the `get_node_option` call returns `None`, causing proof generation to fail with "Missing node" errors.

## Impact Explanation

**Medium Severity** - This vulnerability causes protocol violations affecting network availability:

1. **State Sync Degradation**: Nodes attempting to fast sync to recent epochs will encounter errors when the serving node cannot retrieve required merkle tree nodes for epoch ending snapshots. The validation logic in `error_if_state_merkle_pruned` will detect the missing data and return an error. [11](#0-10) 

2. **Availability Impact**: New validators and full nodes must use slower synchronization methods (transaction replay) or wait for peers with longer history, increasing operational barriers and reducing network resilience.

3. **Operational Costs**: Node operators must maintain unnecessarily long history or risk being unable to serve state sync requests effectively.

The impact is **not** High/Critical because:
- Doesn't directly cause consensus failure or funds loss
- Nodes can still sync using transaction replay from genesis
- Doesn't cause permanent network partition or total liveness failure
- The error is detected and returned gracefully

This represents a **limited protocol violation** requiring manual intervention or workarounds, fitting the Medium severity category.

## Likelihood Explanation

**Likely** - This occurs naturally under normal network operation:

1. **Default Configuration Creates Vulnerability**: The default prune window is 80M versions (~2.2 epochs at 5K TPS). [12](#0-11) 

2. **Checkpoint-Epoch Timing Gap**: State checkpoints occur frequently (potentially every block), while epochs occur every ~2 hours. The latest checkpoint version naturally advances beyond epoch boundaries.

3. **No Validation Safeguards**: The pruning code path contains no checks to prevent pruning nodes from recent epoch snapshots. The vulnerability triggers automatically when state churn causes nodes from epoch snapshots to be overwritten within the prune window.

4. **State Churn**: As transactions modify state in the epochs following an epoch ending, nodes from the previous epoch snapshot become stale. Their `stale_since_version` values fall within the range that gets pruned.

## Recommendation

Modify the epoch snapshot pruner to validate the target version against the latest epoch ending version before pruning:

**In `state_merkle_pruner_manager.rs`, add validation:**

```rust
fn set_pruner_target_db_version(&self, latest_version: Version) {
    assert!(self.pruner_worker.is_some());
    
    let min_readable_version = latest_version.saturating_sub(self.prune_window);
    
    // For epoch snapshot pruner, ensure we don't prune beyond latest epoch ending
    let validated_min_readable_version = if S::name() == "epoch_snapshot_pruner" {
        // Query latest epoch ending version from ledger metadata
        let latest_epoch_ending = self.state_merkle_db
            .get_latest_epoch_ending_version()
            .unwrap_or(0);
        
        // Don't set min_readable_version beyond the latest epoch ending
        min_readable_version.max(latest_epoch_ending.saturating_sub(self.prune_window))
    } else {
        min_readable_version
    };
    
    self.min_readable_version
        .store(validated_min_readable_version, Ordering::SeqCst);
    
    self.pruner_worker
        .as_ref()
        .unwrap()
        .set_target_db_version(validated_min_readable_version);
}
```

Alternatively, change the pruning criterion to use epoch boundaries rather than `stale_since_version` for epoch snapshots, or track epoch ending versions separately to ensure complete snapshots are preserved.

## Proof of Concept

While a complete PoC would require a long-running network to demonstrate the natural occurrence of this issue, the logic vulnerability can be validated through code inspection. The vulnerability exists in the production code path where:

1. `maybe_set_pruner_target_db_version` is called with `get_latest_state_checkpoint_version()`
2. Target is calculated without epoch boundary validation  
3. Stale nodes are pruned based on `stale_since_version` comparison
4. State sync attempts to read pruned nodes for proof generation

A test demonstrating the issue would need to:
1. Create multiple epochs with state changes
2. Advance the checkpoint version beyond epoch boundaries
3. Trigger the epoch snapshot pruner
4. Verify that nodes from recent epoch snapshots are deleted
5. Attempt to serve state sync at the pruned epoch ending and observe failure

## Notes

This is a design-level logic flaw where the pruning strategy (based on when nodes become stale) conflicts with the retention requirement (complete snapshots at epoch boundaries). The configuration documentation explicitly states epoch snapshots should be available for state sync, but the implementation doesn't enforce this invariant. The severity is assessed as Medium rather than High because alternative sync methods exist and the system degrades gracefully rather than failing catastrophically.

### Citations

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L172-180)
```rust
            if let Some(version) = myself.get_latest_state_checkpoint_version()? {
                myself
                    .state_store
                    .state_merkle_pruner
                    .maybe_set_pruner_target_db_version(version);
                myself
                    .state_store
                    .epoch_snapshot_pruner
                    .maybe_set_pruner_target_db_version(version);
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L273-303)
```rust
    pub(super) fn error_if_state_merkle_pruned(
        &self,
        data_type: &str,
        version: Version,
    ) -> Result<()> {
        let min_readable_version = self
            .state_store
            .state_db
            .state_merkle_pruner
            .get_min_readable_version();
        if version >= min_readable_version {
            return Ok(());
        }

        let min_readable_epoch_snapshot_version = self
            .state_store
            .state_db
            .epoch_snapshot_pruner
            .get_min_readable_version();
        if version >= min_readable_epoch_snapshot_version {
            self.ledger_db.metadata_db().ensure_epoch_ending(version)
        } else {
            bail!(
                "{} at version {} is pruned. snapshots are available at >= {}, epoch snapshots are available at >= {}",
                data_type,
                version,
                min_readable_version,
                min_readable_epoch_snapshot_version,
            )
        }
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_pruner_manager.rs (L159-174)
```rust
    fn set_pruner_target_db_version(&self, latest_version: Version) {
        assert!(self.pruner_worker.is_some());

        let min_readable_version = latest_version.saturating_sub(self.prune_window);
        self.min_readable_version
            .store(min_readable_version, Ordering::SeqCst);

        PRUNER_VERSIONS
            .with_label_values(&[S::name(), "min_readable"])
            .set(min_readable_version as i64);

        self.pruner_worker
            .as_ref()
            .unwrap()
            .set_target_db_version(min_readable_version);
    }
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L378-385)
```rust
            if previous_epoch_ending_version.is_some()
                && row.node_key.version() <= previous_epoch_ending_version.unwrap()
            {
                batch.put::<StaleNodeIndexCrossEpochSchema>(row, &())
            } else {
                // These are processed by the state merkle pruner.
                batch.put::<StaleNodeIndexSchema>(row, &())
            }
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L856-864)
```rust
    fn get_node_option(&self, node_key: &NodeKey, tag: &str) -> Result<Option<Node>> {
        let start_time = Instant::now();
        if !self.cache_enabled() {
            let node_opt = self
                .db_by_key(node_key)
                .get::<JellyfishMerkleNodeSchema>(node_key)?;
            NODE_CACHE_SECONDS
                .observe_with(&[tag, "cache_disabled"], start_time.elapsed().as_secs_f64());
            return Ok(node_opt);
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L191-217)
```rust
    pub(in crate::pruner::state_merkle_pruner) fn get_stale_node_indices(
        state_merkle_db_shard: &DB,
        start_version: Version,
        target_version: Version,
        limit: usize,
    ) -> Result<(Vec<StaleNodeIndex>, Option<Version>)> {
        let mut indices = Vec::new();
        let mut iter = state_merkle_db_shard.iter::<S>()?;
        iter.seek(&StaleNodeIndex {
            stale_since_version: start_version,
            node_key: NodeKey::new_empty_path(0),
        })?;

        let mut next_version = None;
        while indices.len() < limit {
            if let Some((index, _)) = iter.next().transpose()? {
                next_version = Some(index.stale_since_version);
                if index.stale_since_version <= target_version {
                    indices.push(index);
                    continue;
                }
            }
            break;
        }

        Ok((indices, next_version))
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs (L61-64)
```rust
        indices.into_iter().try_for_each(|index| {
            batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
            batch.delete::<S>(&index)
        })?;
```

**File:** config/src/config/storage_config.rs (L415-430)
```rust
impl Default for EpochSnapshotPrunerConfig {
    fn default() -> Self {
        Self {
            enable: true,
            // This is based on ~5K TPS * 2h/epoch * 2 epochs. -- epoch ending snapshots are used
            // by state sync in fast sync mode.
            // The setting is in versions, not epochs, because this makes it behave more like other
            // pruners: a slower network will have longer history in db with the same pruner
            // settings, but the disk space take will be similar.
            // settings.
            prune_window: 80_000_000,
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
        }
    }
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L880-891)
```rust
    fn get_state_value_chunk_with_proof(
        &self,
        version: Version,
        first_index: usize,
        chunk_size: usize,
    ) -> Result<StateValueChunkWithProof> {
        gauged_api("get_state_value_chunk_with_proof", || {
            self.error_if_state_merkle_pruned("State merkle", version)?;
            self.state_store
                .get_value_chunk_with_proof(version, first_index, chunk_size)
        })
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1083-1093)
```rust
    pub fn get_value_chunk_with_proof(
        self: &Arc<Self>,
        version: Version,
        first_index: usize,
        chunk_size: usize,
    ) -> Result<StateValueChunkWithProof> {
        let state_key_values: Vec<(StateKey, StateValue)> = self
            .get_value_chunk_iter(version, first_index, chunk_size)?
            .collect::<Result<Vec<_>>>()?;
        self.get_value_chunk_proof(version, first_index, state_key_values)
    }
```

**File:** storage/jellyfish-merkle/src/lib.rs (L801-824)
```rust
    pub fn get_range_proof(
        &self,
        rightmost_key_to_prove: HashValue,
        version: Version,
    ) -> Result<SparseMerkleRangeProof> {
        let (account, proof) = self.get_with_proof(rightmost_key_to_prove, version)?;
        ensure!(account.is_some(), "rightmost_key_to_prove must exist.");

        let siblings = proof
            .siblings()
            .iter()
            .zip(rightmost_key_to_prove.iter_bits())
            .filter_map(|(sibling, bit)| {
                // We only need to keep the siblings on the right.
                if !bit {
                    Some(*sibling)
                } else {
                    None
                }
            })
            .rev()
            .collect();
        Ok(SparseMerkleRangeProof::new(siblings))
    }
```
