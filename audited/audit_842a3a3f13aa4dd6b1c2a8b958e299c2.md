# Audit Report

## Title
Shard Pruning Atomicity Violation Causing Node Startup Failure and Inconsistent Pruning State

## Summary
The StateKV pruner in sharded mode commits global pruning progress to the metadata database BEFORE all individual shard pruners complete their work. When a shard pruner fails due to database corruption or I/O errors, the global progress is already updated while some shards remain unpruned. On node restart, the catch-up mechanism fails with a panic if shard corruption persists, rendering the node permanently unavailable without manual intervention.

## Finding Description

The vulnerability exists in the pruning orchestration logic where metadata updates and actual shard deletions are not atomic. [1](#0-0) 

The execution flow is:

1. **Metadata Update (Line 64-65)**: `metadata_pruner.prune()` is called first and updates the global `StateKvPrunerProgress` in the metadata database to the target version. [2](#0-1) 

In sharded mode, this function merely iterates through shards without performing actual deletions, then commits the global progress marker at line 67-72.

2. **Shard Pruning (Lines 67-78)**: After metadata is updated, all shard pruners execute in parallel. Each shard performs actual deletions and updates its individual shard progress. [3](#0-2) 

3. **Failure Scenario**: If any shard fails during parallel execution (disk corruption, I/O error, etc.), the `try_for_each` operation aborts via the `?` operator at line 78, but the global metadata progress has already been committed.

**State After Partial Failure:**
- Global `StateKvPrunerProgress` in metadata DB = target version (COMMITTED)
- Successful shards' progress = target version (COMMITTED) 
- Failed shard's progress = old version (NOT UPDATED)
- In-memory atomic progress = old version (lines 80-81 never reached)

**Node Restart Catastrophe:**

On restart, the initialization attempts to catch up all shards to the metadata progress: [4](#0-3) 

For each shard, `StateKvShardPruner::new()` calls `prune()` to catch up: [5](#0-4) 

If the corrupted shard still fails during catch-up (line 42), the error propagates through the `?` operator, causing `StateKvPruner::new()` to fail, which triggers a panic: [6](#0-5) 

**Result**: The node cannot start and requires manual database recovery.

This breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs." The pruning state across shards is not consistent, and the system cannot recover automatically.

## Impact Explanation

This qualifies as **HIGH Severity** per Aptos bug bounty criteria:

1. **Validator node unavailability**: When a shard has persistent corruption, the node panics during initialization and cannot restart. This directly maps to "Validator node slowdowns" and availability issues worth up to $50,000.

2. **State inconsistencies requiring intervention**: The inconsistent pruning state across shards violates data integrity guarantees and requires manual database recovery, qualifying as "Significant protocol violations" (High Severity).

3. **Storage bloat and resource exhaustion**: Failed shards accumulate unpruned stale data indefinitely, potentially leading to resource exhaustion over time.

The impact affects:
- **All validator nodes** running with sharding enabled (production configuration)
- Requires operator intervention and potential database restoration from backup
- Can lead to prolonged node downtime during critical network operations

## Likelihood Explanation

**HIGH Likelihood** - This will occur naturally during:

1. **Disk failures**: Hardware failures are common in long-running distributed systems
2. **I/O errors**: Transient or persistent I/O errors during database operations
3. **Database corruption**: Power failures, filesystem issues, or RocksDB internal errors
4. **Storage exhaustion**: When disk fills up during pruning operations

The vulnerability requires NO attacker action - it's triggered by normal operational failures that occur in production systems. Given that Aptos validators run continuously and perform regular pruning operations, encountering this scenario is inevitable over time.

The catch-up logic at startup makes recovery impossible without manual intervention, increasing the severity.

## Recommendation

**Solution**: Implement two-phase commit for pruning operations:

1. **Phase 1**: All shard pruners execute and prepare their batches, but DO NOT commit
2. **Phase 2**: Only after ALL shards succeed, commit shard batches AND metadata progress atomically

```rust
// In StateKvPruner::prune()
fn prune(&self, max_versions: usize) -> Result<Version> {
    // ... existing code ...
    
    while progress < target_version {
        let current_batch_target_version = 
            min(progress + max_versions as Version, target_version);
        
        // Step 1: Prepare all shard batches (don't commit yet)
        let shard_batches: Vec<_> = THREAD_MANAGER.get_background_pool().install(|| {
            self.shard_pruners.par_iter().map(|shard_pruner| {
                shard_pruner.prepare_prune(progress, current_batch_target_version)
            }).collect::<Result<Vec<_>>>()
        })?;
        
        // Step 2: Only if ALL shards prepared successfully, commit atomically
        for (shard_pruner, batch) in self.shard_pruners.iter().zip(shard_batches.iter()) {
            shard_pruner.commit_batch(batch)?;
        }
        
        // Step 3: Finally update metadata progress
        self.metadata_pruner.prune(progress, current_batch_target_version)?;
        
        progress = current_batch_target_version;
        self.record_progress(progress);
    }
    
    Ok(target_version)
}
```

**Alternative**: Add error recovery logic in `StateKvPruner::new()` to gracefully handle catch-up failures:

```rust
// In StateKvPruner::new()
for shard_id in 0..num_shards {
    match StateKvShardPruner::new(shard_id, db_shard_arc(shard_id), metadata_progress) {
        Ok(pruner) => shard_pruners.push(pruner),
        Err(e) => {
            // Log error but continue with degraded mode
            error!("Failed to initialize shard {}: {}. Running without this shard.", shard_id, e);
            // OR: Reset metadata progress to minimum of all shard progresses
        }
    }
}
```

## Proof of Concept

```rust
// Test demonstrating the vulnerability
#[test]
fn test_partial_shard_failure_causes_inconsistency() {
    // Setup: Create StateKvDb with sharding enabled
    let state_kv_db = create_test_state_kv_db_with_sharding();
    let pruner = StateKvPruner::new(state_kv_db.clone()).unwrap();
    
    // Write state data to version 200
    populate_test_data(&state_kv_db, 200);
    
    // Simulate corruption in shard 15 by injecting error
    inject_corruption_into_shard(&state_kv_db, 15);
    
    // Attempt pruning from 0 to 100
    let result = pruner.prune(100);
    assert!(result.is_err()); // Pruning fails
    
    // Check state after failure:
    let metadata_progress = get_metadata_pruner_progress(&state_kv_db);
    assert_eq!(metadata_progress, 100); // Global progress UPDATED
    
    // Check shard 15 progress
    let shard_15_progress = get_shard_progress(&state_kv_db, 15);
    assert_eq!(shard_15_progress, 0); // Shard 15 NOT updated - INCONSISTENT!
    
    // Now simulate node restart
    drop(pruner);
    let result = StateKvPruner::new(state_kv_db.clone());
    
    // Initialization PANICS or FAILS due to catch-up failure on corrupted shard
    assert!(result.is_err()); // Node cannot start!
}
```

## Notes

This vulnerability demonstrates a critical atomicity violation in distributed database operations. The root cause is updating global metadata before ensuring all shards have successfully completed their local operations. This pattern should be reviewed across all similar distributed operations in AptosDB.

The issue is particularly severe because:
1. It occurs naturally without attacker involvement
2. Recovery requires manual intervention (cannot self-heal)
3. Affects production validator configurations with sharding enabled
4. Violates fundamental consistency guarantees expected from storage systems

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L64-78)
```rust
            self.metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state kv shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L117-132)
```rust
        let metadata_progress = metadata_pruner.progress()?;

        info!(
            metadata_progress = metadata_progress,
            "Created state kv metadata pruner, start catching up all shards."
        );

        let shard_pruners = if state_kv_db.enabled_sharding() {
            let num_shards = state_kv_db.num_shards();
            let mut shard_pruners = Vec::with_capacity(num_shards);
            for shard_id in 0..num_shards {
                shard_pruners.push(StateKvShardPruner::new(
                    shard_id,
                    state_kv_db.db_shard_arc(shard_id),
                    metadata_progress,
                )?);
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs (L35-72)
```rust
        if self.state_kv_db.enabled_sharding() {
            let num_shards = self.state_kv_db.num_shards();
            // NOTE: This can be done in parallel if it becomes the bottleneck.
            for shard_id in 0..num_shards {
                let mut iter = self
                    .state_kv_db
                    .db_shard(shard_id)
                    .iter::<StaleStateValueIndexByKeyHashSchema>()?;
                iter.seek(&current_progress)?;
                for item in iter {
                    let (index, _) = item?;
                    if index.stale_since_version > target_version {
                        break;
                    }
                }
            }
        } else {
            let mut iter = self
                .state_kv_db
                .metadata_db()
                .iter::<StaleStateValueIndexSchema>()?;
            iter.seek(&current_progress)?;
            for item in iter {
                let (index, _) = item?;
                if index.stale_since_version > target_version {
                    break;
                }
                batch.delete::<StaleStateValueIndexSchema>(&index)?;
                batch.delete::<StateValueSchema>(&(index.state_key, index.version))?;
            }
        }

        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        self.state_kv_db.metadata_db().write_schemas(batch)
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L24-45)
```rust
impl StateKvShardPruner {
    pub(in crate::pruner) fn new(
        shard_id: usize,
        db_shard: Arc<DB>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
            metadata_progress,
        )?;
        let myself = Self { shard_id, db_shard };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up state kv shard {shard_id}."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L47-72)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();

        let mut iter = self
            .db_shard
            .iter::<StaleStateValueIndexByKeyHashSchema>()?;
        iter.seek(&current_progress)?;
        for item in iter {
            let (index, _) = item?;
            if index.stale_since_version > target_version {
                break;
            }
            batch.delete::<StaleStateValueIndexByKeyHashSchema>(&index)?;
            batch.delete::<StateValueByKeyHashSchema>(&(index.state_key_hash, index.version))?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
            &DbMetadataValue::Version(target_version),
        )?;

        self.db_shard.write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_pruner_manager.rs (L114-115)
```rust
        let pruner =
            Arc::new(StateKvPruner::new(state_kv_db).expect("Failed to create state kv pruner."));
```
