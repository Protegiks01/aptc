# Audit Report

## Title
Unbounded Batch Size During State Merkle Pruner Initialization Causes Extended Database Lock Contention and Validator Slowdowns

## Summary
The state merkle pruner initialization uses `usize::MAX` as the batch size limit, allowing it to collect potentially millions of stale nodes into a single in-memory batch and write them in one massive database operation. This causes extended database write lock contention that blocks other critical write operations (consensus commits, state sync) for potentially minutes, resulting in validator node slowdowns.

## Finding Description

The vulnerability exists in the state merkle pruner's initialization and catch-up logic. When a `StateMerkleShardPruner` is created or needs to catch up after a period of inactivity, it calls the `prune()` function with `usize::MAX` as the `max_nodes_to_prune` parameter. [1](#0-0) 

This unbounded limit is passed to `get_stale_node_indices()`, which iterates through the database and collects stale node indices into a `Vec` until reaching the limit. [2](#0-1) 

With `usize::MAX` as the limit, this function will collect ALL available stale nodes (potentially millions) into memory. Each stale node index is then added as a deletion operation to a `SchemaBatch`. [3](#0-2) 

The massive batch is then written to the database using `write_schemas()`, which calls `write_schemas_inner()` with `sync_write_option()` that has `set_sync(true)`. [4](#0-3) [5](#0-4) [6](#0-5) 

The synchronous write with fsync enabled means RocksDB must serialize the entire batch and wait for disk synchronization to complete. For millions of deletions, this operation can take minutes while holding RocksDB's write lock, blocking other write operations to the same database shard.

The same issue exists in `StateMerkleMetadataPruner`: [7](#0-6) 

**Attack Scenario:**
1. A validator node runs with pruning disabled or falls behind in pruning due to configuration or operational issues
2. Pruning is re-enabled or the node is restarted
3. During initialization, the pruner attempts to catch up by pruning all accumulated stale nodes
4. With potentially millions of stale nodes accumulated over weeks/months, the pruner collects them all into a single batch
5. The massive database write holds the write lock for minutes
6. During this time, consensus cannot commit new blocks and state sync stalls
7. The validator experiences severe performance degradation

**Invariant Violation:**
This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." The pruner operation does not respect memory or time limits, allowing unbounded resource consumption.

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos bug bounty program criteria: "Validator node slowdowns."

The impact includes:
- **Consensus Delays**: Block commits to the state merkle database are blocked, preventing consensus from making progress
- **State Sync Stalls**: State sync operations that write to the state merkle database are blocked
- **Validator Performance Degradation**: The affected validator experiences multi-minute periods where it cannot participate effectively in consensus
- **Network Health Impact**: Multiple validators experiencing this simultaneously could impact network liveness

The default batch size for normal operations is 1,000 nodes, but during initialization, this safety limit is bypassed entirely. [8](#0-7) 

## Likelihood Explanation

**Likelihood: Medium to High**

This issue will occur in the following scenarios:
1. **Node restart after extended downtime**: Any node that has been offline or has had pruning disabled will accumulate stale nodes
2. **Pruner configuration changes**: Changing pruner settings that require catch-up
3. **Network upgrades**: Nodes catching up after missing blocks during upgrades
4. **Normal operations over time**: Even in normal operation, if the pruner falls behind, catch-up operations use unbounded batches

The issue is deterministic and does not require any attacker action - it's a natural consequence of the code logic. Operators running production validators are likely to encounter this during normal operational scenarios.

## Recommendation

Implement a maximum batch size limit even during initialization and catch-up operations. The initialization should be modified to respect reasonable batch size limits and iterate multiple times if necessary.

**Recommended Fix:**

Replace the `usize::MAX` parameter with a configurable maximum initialization batch size (e.g., 10,000 or 100,000 nodes):

```rust
// In state_merkle_shard_pruner.rs
const MAX_INITIALIZATION_BATCH_SIZE: usize = 10_000;

pub(in crate::pruner) fn new(
    shard_id: usize,
    db_shard: Arc<DB>,
    metadata_progress: Version,
) -> Result<Self> {
    let progress = get_or_initialize_subpruner_progress(
        &db_shard,
        &S::progress_metadata_key(Some(shard_id)),
        metadata_progress,
    )?;
    let myself = Self {
        shard_id,
        db_shard,
        _phantom: PhantomData,
    };

    info!(
        progress = progress,
        metadata_progress = metadata_progress,
        "Catching up {} shard {shard_id}.",
        S::name(),
    );
    // Use bounded batch size instead of usize::MAX
    myself.prune(progress, metadata_progress, MAX_INITIALIZATION_BATCH_SIZE)?;

    Ok(myself)
}
```

Apply the same fix to `state_merkle_metadata_pruner.rs`.

Additionally, consider:
1. Making the maximum initialization batch size configurable via `StateMerklePrunerConfig`
2. Adding metrics to track batch sizes and write durations
3. Implementing incremental catch-up with progress updates between batches

## Proof of Concept

**Reproduction Steps:**

1. **Setup**: Deploy a validator node with pruning enabled and default configuration
2. **Accumulate stale nodes**: 
   - Disable pruning by setting `enable: false` in `state_merkle_pruner_config`
   - Run the node for an extended period (days/weeks) with active transaction processing
   - This will accumulate millions of stale nodes in the database
3. **Trigger vulnerability**:
   - Re-enable pruning or restart the node with pruning enabled
   - The pruner initialization will attempt to catch up
4. **Observe impact**:
   - Monitor database write latency (should spike to seconds/minutes)
   - Monitor consensus commit times (should show delays during pruning)
   - Check validator metrics for block commit delays
   - Observe memory consumption spike as millions of nodes are loaded into the batch

**Observable Symptoms:**
- Log message: "Catching up state_merkle shard" followed by extended silence
- Prometheus metric `aptos_schemadb_batch_commit_latency_seconds` showing multi-second/minute latencies
- Consensus metrics showing block commit delays
- Memory usage spike of potentially gigabytes

**Rust Test Scenario:**
```rust
#[test]
fn test_pruner_initialization_with_many_stale_nodes() {
    // 1. Create a state merkle db with millions of stale nodes
    // 2. Initialize a StateMerkleShardPruner
    // 3. Measure time taken and verify it blocks concurrent writes
    // 4. Verify other database operations are delayed during pruning
}
```

## Notes

While RocksDB does allow concurrent reads during writes (so read operations remain functional), write operations are serialized. The state merkle database is critical for:
- **Block execution and commitment**: Writing new Jellyfish Merkle Tree nodes during state updates
- **State synchronization**: Writing state data during fast sync or state sync operations

Both of these operations require write access to the state merkle database shards. Extended write lock contention directly impacts validator performance and consensus participation, making this a High severity issue that requires immediate attention.

The vulnerability is exacerbated by the use of `sync_write_option()` which forces fsync, making large batch writes even slower. The parallel execution across shards helps somewhat, but each shard can still experience extended lock times independently.

### Citations

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L53-53)
```rust
        myself.prune(progress, metadata_progress, usize::MAX)?;
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L73-76)
```rust
            indices.into_iter().try_for_each(|index| {
                batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
                batch.delete::<S>(&index)
            })?;
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L92-92)
```rust
            self.db_shard.write_schemas(batch)?;
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L191-217)
```rust
    pub(in crate::pruner::state_merkle_pruner) fn get_stale_node_indices(
        state_merkle_db_shard: &DB,
        start_version: Version,
        target_version: Version,
        limit: usize,
    ) -> Result<(Vec<StaleNodeIndex>, Option<Version>)> {
        let mut indices = Vec::new();
        let mut iter = state_merkle_db_shard.iter::<S>()?;
        iter.seek(&StaleNodeIndex {
            stale_since_version: start_version,
            node_key: NodeKey::new_empty_path(0),
        })?;

        let mut next_version = None;
        while indices.len() < limit {
            if let Some((index, _)) = iter.next().transpose()? {
                next_version = Some(index.stale_since_version);
                if index.stale_since_version <= target_version {
                    indices.push(index);
                    continue;
                }
            }
            break;
        }

        Ok((indices, next_version))
    }
```

**File:** storage/schemadb/src/lib.rs (L307-309)
```rust
    pub fn write_schemas(&self, batch: impl IntoRawBatch) -> DbResult<()> {
        self.write_schemas_inner(batch, &sync_write_option())
    }
```

**File:** storage/schemadb/src/lib.rs (L374-378)
```rust
fn sync_write_option() -> rocksdb::WriteOptions {
    let mut opts = rocksdb::WriteOptions::default();
    opts.set_sync(true);
    opts
}
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs (L53-58)
```rust
        let (indices, next_version) = StateMerklePruner::get_stale_node_indices(
            &self.metadata_db,
            current_progress,
            target_version_for_this_round,
            usize::MAX,
        )?;
```

**File:** config/src/config/storage_config.rs (L398-412)
```rust
impl Default for StateMerklePrunerConfig {
    fn default() -> Self {
        StateMerklePrunerConfig {
            enable: true,
            // This allows a block / chunk being executed to have access to a non-latest state tree.
            // It needs to be greater than the number of versions the state committing thread is
            // able to commit during the execution of the block / chunk. If the bad case indeed
            // happens due to this being too small, a node restart should recover it.
            // Still, defaulting to 1M to be super safe.
            prune_window: 1_000_000,
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
        }
    }
```
