# Audit Report

## Title
State KV Pruner Transaction Inconsistency Causes Node Startup Failures

## Summary
The state KV pruner in sharded mode writes pruning progress (`StateKvPrunerProgress`) in a separate transaction from the actual shard data deletions, creating a window where progress can advance without actual deletion. If a shard pruner fails after the metadata pruner succeeds, the node will panic and fail to start on restart when attempting to catch up.

## Finding Description

The vulnerability exists in the transaction boundaries of the state KV pruning operation when sharding is enabled. The pruning process consists of two separate, non-atomic operations:

**Step 1: Metadata Pruner Commits Progress** [1](#0-0) 

In sharded mode, the metadata pruner only iterates through shard indices without performing deletions, but writes `StateKvPrunerProgress` to the database and commits the transaction.

**Step 2: Shard Pruners Execute in Parallel** [2](#0-1) 

The shard pruners execute in separate transactions. If any shard pruner fails after the metadata pruner has committed, an error is returned and in-memory progress is not updated, but the database `StateKvPrunerProgress` has already been advanced.

**Critical Failure Path on Restart:**

When the node restarts after this inconsistent state:

1. The pruner manager reads the advanced `StateKvPrunerProgress` from the database: [3](#0-2) 

2. During initialization, each shard pruner attempts to catch up to the metadata progress: [4](#0-3) 

3. If the catch-up pruning fails (due to persistent database corruption, I/O errors, or disk issues), the initialization panics: [5](#0-4) 

This breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs." The pruning operation is not atomic - the progress marker can advance without corresponding data deletion.

## Impact Explanation

This is a **High Severity** vulnerability per Aptos bug bounty criteria:

1. **Validator Node Slowdowns/Failures**: If the catch-up fails during node restart, the validator cannot start, causing complete node unavailability. The `.expect()` call causes a panic that prevents node initialization.

2. **Significant Protocol Violations**: The storage layer maintains inconsistent state where the pruner progress marker does not accurately reflect the actual pruned data. This violates the atomicity guarantee of database operations.

3. **Manual Intervention Required**: Recovery requires manual database intervention to either:
   - Roll back the `StateKvPrunerProgress` to match the shard pruner progress
   - Manually fix the underlying database corruption preventing catch-up
   - Restore from backup

The impact is particularly severe because:
- A transient error during normal operation becomes a permanent liveness failure
- Multiple validators experiencing the same issue could reduce network liveness
- No automatic recovery mechanism exists

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability can be triggered by:

1. **Transient Database Errors**: Temporary I/O errors, disk full conditions, or network storage issues during shard pruner execution
2. **Database Corruption**: Corruption in shard databases that doesn't affect the metadata database
3. **Resource Exhaustion**: Out of memory or disk space during pruning operations
4. **Concurrent Access Issues**: Race conditions in database access during parallel shard pruning

The likelihood is elevated because:
- Pruning runs continuously in the background on all validators
- Shard pruners execute in parallel, increasing the chance of at least one failing
- Database operations can fail for many reasons (hardware, OS, resource limits)
- The failure window exists on every pruning batch execution

Once the inconsistency occurs, node restart will fail with high probability if:
- The underlying issue persists (e.g., permanent database corruption)
- The catch-up encounters the same failure condition
- Disk space or resources remain insufficient

## Recommendation

**Solution: Atomic Two-Phase Commit for Pruning Progress**

The pruning progress should only be committed after ALL shard pruners succeed. Implement a two-phase approach:

1. Execute all shard pruner deletions first
2. Only write `StateKvPrunerProgress` after all shards have successfully completed
3. Use a distributed transaction or collect all batches before committing

**Proposed Fix:**

Modify `StateKvPruner::prune()` to defer the metadata pruner's progress write until after shard pruners succeed:

```rust
// In StateKvPruner::prune()
// First, ensure all shard deletions complete successfully
THREAD_MANAGER.get_background_pool().install(|| {
    self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
        shard_pruner.prune(progress, current_batch_target_version)
            .map_err(|err| anyhow!("Failed to prune state kv shard {}: {err}", shard_pruner.shard_id()))
    })
})?;

// Only AFTER all shards succeed, update the metadata progress
self.metadata_pruner.update_progress_only(current_batch_target_version)?;
```

Modify `StateKvMetadataPruner::prune()` to separate iteration/validation from progress updates, or only write progress after verification that shard pruners will succeed.

**Alternative: Rollback on Failure**

Implement a rollback mechanism that reverts `StateKvPrunerProgress` if any shard pruner fails, ensuring consistency is maintained.

## Proof of Concept

```rust
// Reproduction steps (pseudo-code for clarity):

// 1. Setup: Enable sharding, initialize state KV database
let state_kv_db = StateKvDb::new_sharded(...);
let pruner = StateKvPruner::new(state_kv_db)?;

// 2. Trigger pruning with injected failure in shard pruner
pruner.set_target_version(100);

// Simulate: metadata_pruner.prune() succeeds, writes StateKvPrunerProgress=100
// Then: shard_pruner[0].prune() fails with I/O error
// Result: StateKvPrunerProgress=100 in DB, but shard data still exists

// 3. Restart the node
// StateKvPrunerManager::new() will attempt initialization
let result = StateKvPrunerManager::new(state_kv_db, config);

// Expected: If catch-up fails, panic occurs:
// "thread panicked at 'Failed to create state kv pruner.'"
// Node cannot start

// 4. Verification
// Check database state shows:
// - StateKvPrunerProgress = 100
// - StateKvShardPrunerProgress(0) = 50 (old value)
// - Shard data for versions 50-100 still exists
// - Node is unable to restart without manual intervention
```

To actually reproduce, modify the shard pruner to inject a failure after the metadata pruner succeeds, then restart the node and observe the panic during initialization.

## Notes

The vulnerability specifically affects **sharded mode** deployments. In non-sharded mode, the metadata pruner performs the actual deletions and writes progress in the same transaction, maintaining atomicity [6](#0-5) .

The `save_min_readable_version()` function mentioned in the security question is primarily used during fast sync/restoration [7](#0-6) , not during normal pruning operations. However, it also writes to the same `StateKvPrunerProgress` key [8](#0-7) , which could potentially overwrite the metadata pruner's progress in certain edge cases, though this is a separate concern from the primary vulnerability described above.

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs (L28-73)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();

        if self.state_kv_db.enabled_sharding() {
            let num_shards = self.state_kv_db.num_shards();
            // NOTE: This can be done in parallel if it becomes the bottleneck.
            for shard_id in 0..num_shards {
                let mut iter = self
                    .state_kv_db
                    .db_shard(shard_id)
                    .iter::<StaleStateValueIndexByKeyHashSchema>()?;
                iter.seek(&current_progress)?;
                for item in iter {
                    let (index, _) = item?;
                    if index.stale_since_version > target_version {
                        break;
                    }
                }
            }
        } else {
            let mut iter = self
                .state_kv_db
                .metadata_db()
                .iter::<StaleStateValueIndexSchema>()?;
            iter.seek(&current_progress)?;
            for item in iter {
                let (index, _) = item?;
                if index.stale_since_version > target_version {
                    break;
                }
                batch.delete::<StaleStateValueIndexSchema>(&index)?;
                batch.delete::<StateValueSchema>(&(index.state_key, index.version))?;
            }
        }

        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        self.state_kv_db.metadata_db().write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L49-86)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_pruner__prune"]);

        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning state kv data."
            );
            self.metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state kv shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning state kv data is done.");
        }

        Ok(target_version)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_pruner_manager.rs (L94-95)
```rust
        let min_readable_version =
            pruner_utils::get_state_kv_pruner_progress(&state_kv_db).expect("Must succeed.");
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_pruner_manager.rs (L114-115)
```rust
        let pruner =
            Arc::new(StateKvPruner::new(state_kv_db).expect("Failed to create state kv pruner."));
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L25-45)
```rust
    pub(in crate::pruner) fn new(
        shard_id: usize,
        db_shard: Arc<DB>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
            metadata_progress,
        )?;
        let myself = Self { shard_id, db_shard };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up state kv shard {shard_id}."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/pruner_manager.rs (L33-35)
```rust
    // Only used at the end of fast sync to store the min_readable_version to db and update the
    // in memory progress.
    fn save_min_readable_version(&self, min_readable_version: Version) -> Result<()>;
```

**File:** storage/aptosdb/src/state_kv_db.rs (L217-222)
```rust
    pub(crate) fn write_pruner_progress(&self, version: Version) -> Result<()> {
        self.state_kv_metadata_db.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvPrunerProgress,
            &DbMetadataValue::Version(version),
        )
    }
```
