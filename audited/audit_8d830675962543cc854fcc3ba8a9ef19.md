# Audit Report

## Title
Backup Compaction Propagates Corrupted Merkle Tree Hash Chains Without Verification

## Summary
The `BackupCompactor` in Aptos merges backup metadata files without verifying the integrity of the underlying Merkle tree hashes. If backup data becomes corrupted (through storage bit rot, attacker modification, or system errors), compaction will silently propagate these corrupted hash chains. When the original backups are eventually deleted, the network may become unrestorable, causing a non-recoverable network partition scenario.

## Finding Description
The backup compaction process consolidates multiple metadata files into compacted versions to reduce storage overhead and improve metadata loading performance. However, the compaction operation performs **no cryptographic verification** of the Merkle tree hashes stored in state snapshot backups.

**How the vulnerability manifests:**

1. State snapshot backups contain:
   - `root_hash`: The Jellyfish Merkle tree root hash
   - `chunks`: State data chunks with `SparseMerkleRangeProof`s
   - `proof`: `TransactionInfoWithProof` that authenticates the root hash [1](#0-0) 

2. During backup creation, these hashes are properly set and verified: [2](#0-1) 

3. However, `BackupCompactor::run()` only consolidates metadata references without touching or verifying the manifest files: [3](#0-2) 

4. The compaction functions merely organize metadata into chunks and create new filenames: [4](#0-3) 

5. After `remove_compacted_file_after` seconds (default 86400 = 1 day), original metadata files are deleted, leaving only references to potentially corrupted backup data. [5](#0-4) 

**Attack scenario:**

1. An attacker gains access to backup storage (compromised cloud credentials, insider threat, or storage system malfunction causes bit rot)
2. Attacker modifies a state snapshot manifest to corrupt the `root_hash` or chunk proofs
3. The backup compaction CronJob runs (daily schedule in production): [6](#0-5) 

4. Compaction merges the corrupted backup metadata without verification
5. After 24 hours, original uncompacted files are deleted
6. Now all backup references point to corrupted data
7. When operators attempt disaster recovery, restore fails at hash verification: [7](#0-6) 

## Impact Explanation
This vulnerability meets **Critical Severity** ($1,000,000) criteria under "Non-recoverable network partition (requires hardfork)."

If all backups become corrupted and compacted:
- Operators cannot restore the network from backup
- State verification fails during restore due to hash mismatches
- The network cannot recover from catastrophic failures
- A hardfork would be required to establish a new genesis state

The separate backup verification CronJob runs independently and may not detect corruption before compaction happens: [8](#0-7) 

Even in tests, verification runs **after** compaction, not before: [9](#0-8) 

## Likelihood Explanation
**Medium to High Likelihood:**

- **Storage access is realistic**: Cloud credential compromises, insider threats, and storage system failures (bit rot) are well-documented attack vectors
- **Time window exists**: Between backup creation and verification, corruption can occur
- **No safeguards in code**: The compaction process has zero cryptographic verification
- **Production deployment confirms risk**: Separate CronJobs for compaction and verification mean no guaranteed ordering

The vulnerability doesn't require sophisticated exploitationâ€”simple file modification or natural storage degradation can trigger it.

## Recommendation
Implement mandatory hash verification before compaction. The `BackupCompactor` should verify all manifests being compacted:

```rust
// In BackupCompactor::run(), before compacting state snapshots:
for range in metaview.compact_state_backups(self.state_snapshot_file_compact_factor)? {
    // VERIFICATION STEP: Load and verify each manifest before compacting
    for backup_meta in &range {
        let manifest: StateSnapshotBackup = 
            self.storage.load_json_file(&backup_meta.manifest).await?;
        let (txn_info_with_proof, li): (TransactionInfoWithProof, LedgerInfoWithSignatures) = 
            self.storage.load_bcs_file(&manifest.proof).await?;
        
        // Verify the proof is valid
        txn_info_with_proof.verify(li.ledger_info(), manifest.version)?;
        
        // Verify root hash matches
        let state_root_hash = txn_info_with_proof
            .transaction_info()
            .ensure_state_checkpoint_hash()?;
        ensure!(
            state_root_hash == manifest.root_hash,
            "Root hash mismatch in backup being compacted: expected {}, got {}",
            state_root_hash,
            manifest.root_hash
        );
    }
    
    // Now safe to compact
    let (state_range, file_name) =
        Metadata::compact_statesnapshot_backup_range(range.to_vec())?;
    // ... rest of compaction logic
}
```

**Alternative approach**: Make the backup verification CronJob a prerequisite for compaction by adding a verification check at the start of `BackupCompactor::run()`.

## Proof of Concept

```rust
#[tokio::test]
async fn test_compaction_with_corrupted_manifest() {
    use aptos_backup_cli::{
        coordinators::backup::BackupCompactor,
        metadata::{cache::MetadataCacheOpt, Metadata},
        storage::{local_fs::LocalFs, BackupStorage},
    };
    use aptos_temppath::TempPath;
    use std::sync::Arc;
    
    // Setup: Create backup directory with a state snapshot
    let backup_dir = TempPath::new();
    backup_dir.create_as_dir().unwrap();
    let local_fs = LocalFs::new(backup_dir.path().to_path_buf());
    let store: Arc<dyn BackupStorage> = Arc::new(local_fs);
    
    // Create a legitimate backup (assume db and backup service setup)
    // ... backup creation code ...
    
    // ATTACK: Corrupt the manifest file by modifying root_hash
    let manifest_path = backup_dir.path().join("state_snapshot_ver_100.manifest");
    let mut manifest_content = std::fs::read_to_string(&manifest_path).unwrap();
    let mut manifest: StateSnapshotBackup = serde_json::from_str(&manifest_content).unwrap();
    
    // Corrupt the root hash
    manifest.root_hash = HashValue::zero();
    std::fs::write(&manifest_path, serde_json::to_string(&manifest).unwrap()).unwrap();
    
    // Run compaction - it should fail but currently succeeds
    let metadata_cache_dir = TempPath::new();
    let metadata_opt = MetadataCacheOpt::new(Some(metadata_cache_dir.path().to_path_buf()));
    let compactor = BackupCompactor::new(2, 2, 2, metadata_opt, store.clone(), 1, 1);
    
    // BUG: Compaction succeeds even with corrupted hash
    compactor.run().await.unwrap(); // Should fail but doesn't!
    
    // Wait for deletion period
    std::thread::sleep(std::time::Duration::from_secs(2));
    compactor.run().await.unwrap();
    
    // Now try to restore - this will fail
    // StateSnapshotRestoreController will detect the corruption at line 131-136
    // but it's too late - original backup is deleted
}
```

This PoC demonstrates that compaction succeeds even when manifests contain corrupted hashes, and the corruption is only detected during restore when it may be too late to recover.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/manifest.rs (L29-51)
```rust
/// State snapshot backup manifest, representing a complete state view at specified version.
#[derive(Deserialize, Serialize)]
pub struct StateSnapshotBackup {
    /// Version at which this state snapshot is taken.
    pub version: Version,
    /// Epoch in which this state snapshot is taken.
    pub epoch: u64,
    /// Hash of the state tree root.
    pub root_hash: HashValue,
    /// All account blobs in chunks.
    pub chunks: Vec<StateSnapshotChunk>,
    /// BCS serialized
    /// `Tuple(TransactionInfoWithProof, LedgerInfoWithSignatures)`.
    ///   - The `TransactionInfoWithProof` is at `Version` above, and carries the same `root_hash`
    /// above; It proves that at specified version the root hash is as specified in a chain
    /// represented by the LedgerInfo below.
    ///   - The signatures on the `LedgerInfoWithSignatures` has a version greater than or equal to
    /// the version of this backup but is within the same epoch, so the signatures on it can be
    /// verified by the validator set in the same epoch, which can be provided by an
    /// `EpochStateBackup` recovered prior to this to the DB; Requiring it to be in the same epoch
    /// limits the requirement on such `EpochStateBackup` to no older than the same epoch.
    pub proof: FileHandle,
}
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/backup.rs (L449-492)
```rust
    async fn write_manifest(
        &self,
        backup_handle: &BackupHandleRef,
        chunks: Vec<StateSnapshotChunk>,
    ) -> Result<FileHandle> {
        let proof_bytes = self.client.get_state_root_proof(self.version()).await?;
        let (txn_info, _): (TransactionInfoWithProof, LedgerInfoWithSignatures) =
            bcs::from_bytes(&proof_bytes)?;

        let (proof_handle, mut proof_file) = self
            .storage
            .create_for_write(backup_handle, Self::proof_name())
            .await?;
        proof_file.write_all(&proof_bytes).await?;
        proof_file.shutdown().await?;

        let manifest = StateSnapshotBackup {
            epoch: self.epoch,
            version: self.version(),
            root_hash: txn_info.transaction_info().ensure_state_checkpoint_hash()?,
            chunks,
            proof: proof_handle,
        };

        let (manifest_handle, mut manifest_file) = self
            .storage
            .create_for_write(backup_handle, Self::manifest_name())
            .await?;
        manifest_file
            .write_all(&serde_json::to_vec(&manifest)?)
            .await?;
        manifest_file.shutdown().await?;

        let metadata = Metadata::new_state_snapshot_backup(
            self.epoch,
            self.version(),
            manifest_handle.clone(),
        );
        self.storage
            .save_metadata_line(&metadata.name(), &metadata.to_text_line()?)
            .await?;

        Ok(manifest_handle)
    }
```

**File:** storage/backup/backup-cli/src/coordinators/backup.rs (L409-475)
```rust
    pub async fn run(self) -> Result<()> {
        info!("Backup compaction started");
        // sync the metadata from backup storage
        let mut metaview = metadata::cache::sync_and_load(
            &self.metadata_cache_opt,
            Arc::clone(&self.storage),
            self.concurrent_downloads,
        )
        .await?;

        let files = metaview.get_file_handles();

        info!("Start compacting backup metadata files.");
        let mut new_files: HashSet<FileHandle> = HashSet::new(); // record overwrite file names
        for range in metaview.compact_epoch_ending_backups(self.epoch_ending_file_compact_factor)? {
            let (epoch_range, file_name) =
                Metadata::compact_epoch_ending_backup_range(range.to_vec())?;
            let file_handle = self
                .storage
                .save_metadata_lines(&file_name, epoch_range.as_slice())
                .await?;
            new_files.insert(file_handle);
        }
        for range in metaview.compact_transaction_backups(self.transaction_file_compact_factor)? {
            let (txn_range, file_name) =
                Metadata::compact_transaction_backup_range(range.to_vec())?;
            let file_handle = self
                .storage
                .save_metadata_lines(&file_name, txn_range.as_slice())
                .await?;
            new_files.insert(file_handle);
        }
        for range in metaview.compact_state_backups(self.state_snapshot_file_compact_factor)? {
            let (state_range, file_name) =
                Metadata::compact_statesnapshot_backup_range(range.to_vec())?;
            let file_handle = self
                .storage
                .save_metadata_lines(&file_name, state_range.as_slice())
                .await?;
            new_files.insert(file_handle);
        }

        // Move expired files to the metadata backup folder
        let (to_move, compaction_meta) =
            self.update_compaction_timestamps(&mut metaview, files, new_files)?;
        for file in to_move {
            info!(file = file, "Backup metadata file.");
            self.storage
                .backup_metadata_file(&file)
                .await
                .map_err(|err| {
                    error!(
                        file = file,
                        error = %err,
                        "Backup metadata file failed, ignoring.",
                    )
                })
                .ok();
        }
        // save the metadata compaction timestamps
        let metadata = Metadata::new_compaction_timestamps(compaction_meta);
        self.storage
            .save_metadata_line(&metadata.name(), &metadata.to_text_line()?)
            .await?;

        Ok(())
    }
```

**File:** storage/backup/backup-cli/src/metadata/mod.rs (L95-112)
```rust
    pub fn compact_statesnapshot_backup_range(
        backup_metas: Vec<StateSnapshotBackupMeta>,
    ) -> Result<(Vec<TextLine>, ShellSafeName)> {
        ensure!(
            !backup_metas.is_empty(),
            "compacting an empty metadata vector"
        );
        let name = format!(
            "state_snapshot_compacted_epoch_{}_{}.meta",
            backup_metas[0].epoch,
            backup_metas[backup_metas.len() - 1].epoch
        );
        let res: Vec<TextLine> = backup_metas
            .into_iter()
            .map(|e| Metadata::StateSnapshotBackup(e).to_text_line())
            .collect::<Result<_>>()?;
        Ok((res, name.parse()?))
    }
```

**File:** storage/db-tool/src/backup_maintenance.rs (L41-46)
```rust
    #[clap(
        long,
        default_value_t = 86400,
        help = "Remove metadata files replaced by compaction after specified seconds. They were not replaced right away after compaction in case they are being read then."
    )]
    pub remove_compacted_file_after: u64,
```

**File:** terraform/helm/fullnode/templates/backup-compaction.yaml (L37-52)
```yaml
            command:
            - /usr/local/bin/aptos-debugger
            - aptos-db
            - backup-maintenance
            - compact
            - --state-snapshot-file-compact-factor
            - "100"
            - --transaction-file-compact-factor
            - "100"
            - --epoch-ending-file-compact-factor
            - "100"
            - --metadata-cache-dir
            - /tmp/aptos-backup-compaction-metadata
            - --command-adapter-config
            # use the same config with the backup sts
            - /opt/aptos/etc/{{ .Values.backup.config.location }}.yaml
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L127-136)
```rust
        txn_info_with_proof.verify(li.ledger_info(), manifest.version)?;
        let state_root_hash = txn_info_with_proof
            .transaction_info()
            .ensure_state_checkpoint_hash()?;
        ensure!(
            state_root_hash == manifest.root_hash,
            "Root hash mismatch with that in proof. root hash: {}, expected: {}",
            manifest.root_hash,
            state_root_hash,
        );
```

**File:** terraform/helm/fullnode/templates/backup-verify.yaml (L36-54)
```yaml
            imagePullPolicy: {{ .Values.backup.image.pullPolicy }}
            command:
            - /usr/local/bin/aptos-debugger
            - aptos-db
            - backup
            - verify
            {{- range $.Values.restore.config.trusted_waypoints }}
            - --trust-waypoint
            - {{ . }}
            {{- end }}
            - --metadata-cache-dir
            - /tmp/aptos-backup-verify-metadata
            {{- if .Values.backup_verify.config.concurrent_downloads }}
            - --concurrent-downloads
            - "{{ .Values.backup_verify.config.concurrent_downloads }}"
            {{- end }}
            - --command-adapter-config
            # use the same config with the backup sts
            - /opt/aptos/etc/{{ .Values.backup.config.location }}.yaml
```

**File:** testsuite/smoke-test/src/storage.rs (L379-408)
```rust
    // start the backup compaction
    let compaction = Command::new(bin_path.as_path())
        .current_dir(workspace_root())
        .args([
            "aptos-db",
            "backup-maintenance",
            "compact",
            "--epoch-ending-file-compact-factor",
            "2",
            "--state-snapshot-file-compact-factor",
            "2",
            "--transaction-file-compact-factor",
            "2",
            "--metadata-cache-dir",
            metadata_cache_path1.path().to_str().unwrap(),
            "--concurrent-downloads",
            "4",
            "--local-fs-dir",
            backup_path.path().to_str().unwrap(),
        ])
        .output()
        .unwrap();
    assert!(
        compaction.status.success(),
        "{}",
        std::str::from_utf8(&compaction.stderr).unwrap()
    );
    backup_coordinator.kill().unwrap();
    let snapshot_ver = wait_res.unwrap();
    replay_verify(backup_path.path(), trusted_waypoints);
```
