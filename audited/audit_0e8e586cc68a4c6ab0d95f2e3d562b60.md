# Audit Report

## Title
Insufficient Discrete Log Range Bound for Non-Power-of-2 Aggregation Values Causes DKG Decryption Failure

## Summary
The DKG public parameters construction uses `log2(max_aggregation)` with floor semantics when computing the discrete logarithm range bound. When `max_aggregation` is not a power of 2, the resulting range is insufficient to accommodate aggregated chunk values, causing deterministic decryption failures that break the DKG protocol and prevent epoch transitions.

## Finding Description

The vulnerability exists in the table size and range bound calculations: [1](#0-0) 

The `build_dlog_table` function computes the baby-step table size as `1u32 << ((ell as u32 + log2(max_aggregation)) / 2)`, and `get_dlog_range_bound` returns `1u32 << (ell as u32 + log2(max_aggregation))`.

The critical issue arises from `ark_std::log2`, which returns the floor of the logarithm. When `max_aggregation` is not a power of 2 (e.g., 3, 5, 7), `log2(max_aggregation)` underestimates the required range.

**Concrete Example:**
- `max_aggregation = 3`, `ell = 8`
- `log2(3) = floor(log2(3)) = floor(1.585) = 1`
- `range_bound = 2^(8+1) = 512`
- Maximum chunk value after aggregating 3 transcripts: `3 × (2^8 - 1) = 3 × 255 = 765`
- **765 > 512** ✗ Out of range!

During DKG, when transcripts are aggregated via the `aggregate_with` method: [2](#0-1) 

The ciphertext chunks are additively combined. After aggregating `N` transcripts, each chunk value becomes the sum of `N` original chunks, with maximum value `N × (2^ell - 1)`.

When decryption attempts to recover these chunks using BSGS: [3](#0-2) 

The BSGS algorithm can only find discrete logs within `[0, range_bound - 1]`. When chunk values exceed this range due to insufficient `log2` precision, the algorithm returns `None`, causing the `.expect()` to panic and decryption to fail.

The baby-step giant-step algorithm implementation confirms this limitation: [4](#0-3) 

With `m = table.len()` and `n = range_limit.div_ceil(m)`, the maximum discoverable discrete log is `n × m - 1`. For non-power-of-2 `max_aggregation` values, this upper bound is less than the actual maximum chunk value after aggregation.

**Attack Path:**
1. Network is configured with `max_aggregation` set to a non-power-of-2 value (common in weighted configurations with weights like `[2, 3, 2]` yielding `max_weight = 3`)
2. During normal DKG operation, transcripts from multiple dealers are aggregated
3. When the number of aggregated transcripts approaches or equals `max_aggregation`, chunk values exceed the computed range bound
4. All validators attempting to decrypt fail with "BSGS dlog failed" panic
5. DKG cannot complete, preventing epoch transitions and validator set updates

This breaks **Deterministic Execution** and **Consensus Safety** invariants, as validators cannot progress through epoch changes without successful DKG completion.

## Impact Explanation

**Severity: High** (up to $50,000)

This qualifies as a **significant protocol violation** under the Aptos bug bounty program. The vulnerability causes:

1. **DKG Protocol Failure**: Systematic decryption failures prevent completion of Distributed Key Generation
2. **Epoch Transition Blocking**: Without successful DKG, the network cannot transition to new epochs or update validator sets
3. **Network Liveness Impact**: While not causing total unavailability, this creates operational disruptions requiring configuration changes or validator coordination to resolve

The issue does not cause loss of funds or permanent state corruption, but significantly impacts protocol operation and validator functionality, meeting the "High Severity" criteria of validator node functional disruption and protocol violations.

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability is highly likely to manifest because:

1. **Common Configuration**: Weighted validator configurations with non-power-of-2 weights are realistic. Test configurations explicitly include examples like `weights: [2, 3, 2]` with `max_weight = 3` [5](#0-4) 

2. **Automatic Triggering**: The bug triggers automatically during normal DKG operation when the configured `max_aggregation` is non-power-of-2 and sufficient transcripts are aggregated

3. **No Validation**: The codebase lacks validation to prevent setting non-power-of-2 values or to limit aggregation to safe bounds

4. **Deterministic Failure**: Once the misconfiguration exists, every DKG round will fail deterministically when aggregation reaches the problematic threshold

## Recommendation

**Fix the range bound calculation to properly account for non-power-of-2 aggregation values:**

```rust
pub(crate) fn get_dlog_range_bound(&self) -> u32 {
    // Use ceiling of log2 to ensure sufficient range for non-power-of-2 values
    // For max_aggregation = 3: need range >= 3 * 2^ell, not just 2 * 2^ell
    let log2_ceil = if self.max_aggregation.is_power_of_two() {
        self.max_aggregation.trailing_zeros()
    } else {
        (self.max_aggregation - 1).ilog2() + 1
    };
    
    1u32 << (self.ell as u32 + log2_ceil)
}

pub(crate) fn build_dlog_table(
    G: E::G1,
    ell: u8,
    max_aggregation: usize,
) -> HashMap<Vec<u8>, u32> {
    let log2_ceil = if max_aggregation.is_power_of_two() {
        max_aggregation.trailing_zeros()
    } else {
        (max_aggregation - 1).ilog2() + 1
    };
    
    dlog::table::build::<E::G1>(
        G,
        1u32 << ((ell as u32 + log2_ceil) / 2)
    )
}
```

Alternatively, add validation to enforce power-of-2 constraints:

```rust
pub fn new<R: RngCore + CryptoRng>(
    max_num_shares: usize,
    ell: u8,
    max_aggregation: usize,
    rng: &mut R,
) -> Self {
    assert!(
        max_aggregation.is_power_of_two(),
        "max_aggregation must be a power of 2 to ensure correct range bound calculation"
    );
    // ... rest of implementation
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod test_aggregation_overflow {
    use super::*;
    use ark_bn254::Bn254;
    use ark_std::test_rng;
    
    #[test]
    #[should_panic(expected = "BSGS dlog failed")]
    fn test_non_power_of_2_aggregation_overflow() {
        let mut rng = test_rng();
        let ell = 8u8;
        let max_aggregation = 3usize; // Non-power-of-2
        
        // Create public parameters with max_aggregation = 3
        let pp = PublicParameters::<Bn254>::new(1, ell, max_aggregation, &mut rng);
        
        // Verify the range bound is insufficient
        let range_bound = pp.get_dlog_range_bound();
        assert_eq!(range_bound, 512); // 2^9
        
        // Maximum chunk value after 3 aggregations
        let max_chunk_value = 3 * ((1u32 << ell) - 1);
        assert_eq!(max_chunk_value, 765);
        assert!(max_chunk_value > range_bound); // 765 > 512
        
        // Create a test point representing an out-of-range discrete log
        let G = pp.pp_elgamal.G.into_group();
        let out_of_range_value = 600u32; // Within [512, 765]
        let H = G * ark_bn254::Fr::from(out_of_range_value);
        
        // This will panic because the discrete log is out of range
        let result = bsgs::dlog(
            G,
            H,
            &pp.table,
            pp.get_dlog_range_bound()
        );
        
        assert!(result.is_none()); // Cannot find discrete log
    }
}
```

**Notes**

The vulnerability stems from a subtle semantic mismatch: `max_aggregation` appears to be set based on `max_weight` from weighted validator configurations, which commonly includes non-power-of-2 values. However, the discrete log range calculations assume power-of-2 semantics due to the floor behavior of `log2()`. This creates a systematic failure mode in realistic validator configurations with non-uniform voting weights.

The fix requires either enforcing power-of-2 constraints on `max_aggregation` or using ceiling semantics in the logarithm calculations to ensure the range bound properly accommodates all possible aggregated chunk values.

### Citations

**File:** crates/aptos-dkg/src/pvss/chunky/public_parameters.rs (L107-117)
```rust
    pub(crate) fn build_dlog_table(
        G: E::G1,
        ell: u8,
        max_aggregation: usize,
    ) -> HashMap<Vec<u8>, u32> {
        dlog::table::build::<E::G1>(G, 1u32 << ((ell as u32 + log2(max_aggregation)) / 2))
    }

    pub(crate) fn get_dlog_range_bound(&self) -> u32 {
        1u32 << (self.ell as u32 + log2(self.max_aggregation))
    }
```

**File:** crates/aptos-dkg/src/pvss/chunky/weighted_transcript.rs (L357-363)
```rust
            let dealt_chunked_secret_key_share = bsgs::dlog_vec(
                pp.pp_elgamal.G.into_group(),
                &dealt_encrypted_secret_key_share_chunks,
                &pp.table,
                pp.get_dlog_range_bound(),
            )
            .expect("BSGS dlog failed");
```

**File:** crates/aptos-dkg/src/pvss/chunky/weighted_transcript.rs (L401-403)
```rust
                for k in 0..self.Cs[i][j].len() {
                    // Aggregate the C_{i,j,k}s
                    self.Cs[i][j][k] += other.Cs[i][j][k];
```

**File:** crates/aptos-dkg/src/dlog/bsgs.rs (L25-46)
```rust
    let m = baby_table
        .len()
        .try_into()
        .expect("Table seems rather large");
    let n = range_limit.div_ceil(m);

    let G_neg_m = G * -C::ScalarField::from(m);

    let mut gamma = H;

    for i in 0..n {
        let mut buf = vec![0u8; byte_size];
        gamma.serialize_compressed(&mut buf[..]).unwrap();

        if let Some(&j) = baby_table.get(&buf) {
            return Some(i * m + j);
        }

        gamma += G_neg_m;
    }

    None
```

**File:** crates/aptos-dkg/src/pvss/test_utils.rs (L234-235)
```rust
    // 3-out-of-7, weights 2 3 2
    wcs.push(WeightedConfig::<T>::new(3, vec![2, 3, 2]).unwrap());
```
