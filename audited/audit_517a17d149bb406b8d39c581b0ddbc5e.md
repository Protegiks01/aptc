# Audit Report

## Title
Thread Pool Saturation in GlobalExecutor Causing Validator Performance Degradation

## Summary
The `GlobalExecutor` in the sharded block execution system creates a thread pool with exactly `num_threads + 2` threads and operates at full capacity during global transaction execution. When processing a large batch of global transactions with extensive cross-shard dependencies, all worker threads can become blocked waiting for remote state values (using indefinite `Condvar.wait()` without timeouts), leading to thread pool saturation and significant validator performance degradation.

## Finding Description

The vulnerability exists in the thread pool architecture for global transaction execution: [1](#0-0) 

The thread pool is created with exactly `num_threads + 2` threads. During execution via `execute_transactions_with_dependencies`: [2](#0-1) 

This creates a scope that spawns two tasks: the `CrossShardCommitReceiver` and an executor task that creates a nested scope spawning `num_workers` (equal to `num_threads`) worker threads: [3](#0-2) 

**Thread Accounting at Full Capacity:**
- CrossShardCommitReceiver: 1 thread (blocked on channel recv)
- Executor thread: 1 thread (blocked waiting for nested scope)  
- Worker threads: `num_threads` threads (executing/potentially blocked)
- **Total: `num_threads + 2` threads = exactly the pool capacity**

When global transactions have cross-shard dependencies, worker threads call `RemoteStateValue.get_value()` which blocks indefinitely using `Condvar.wait()`: [4](#0-3) 

The developers themselves acknowledged this concern: [5](#0-4) 

**Attack Scenario:**
1. Attacker crafts transactions with read/write patterns spanning multiple shards, causing them to be classified as global transactions with many cross-shard dependencies
2. These transactions are included in a block and routed to global execution
3. Worker threads execute these transactions and encounter cross-shard state reads
4. Most/all `num_threads` workers block on `RemoteStateValue.get_value()` via `Condvar.wait()`
5. With all threads either blocked or busy, the thread pool operates at saturation
6. Block execution stalls or becomes extremely slow
7. Validator cannot process blocks efficiently, causing performance degradation

**Absence of Safeguards:**
- No timeouts on `RemoteStateValue.get_value()` blocking operations
- No explicit limit on the number of global transactions per block
- Zero thread buffer (operates at exact capacity with no headroom)

## Impact Explanation

This vulnerability falls under **High Severity** per Aptos bug bounty criteria due to "Validator node slowdowns." 

**Quantified Impact:**
- **Affected Nodes:** All validators processing blocks with large batches of global transactions
- **Performance Impact:** Severe degradation in block execution time when thread pool saturates
- **Liveness Risk:** If block execution is significantly delayed, validators may fall behind consensus, affecting network liveness
- **Cascade Effect:** Slow validators can impact overall network throughput and finality

The vulnerability doesn't cause:
- Direct fund loss or theft
- Consensus safety violations (determinism is maintained)
- Permanent network damage

However, it enables a practical DoS attack vector against validator performance by exploiting the tight thread pool resource constraints.

## Likelihood Explanation

**Likelihood: Medium to High**

**Attacker Requirements:**
- Ability to submit transactions (publicly available)
- Knowledge of transaction partitioning heuristics to craft cross-shard dependencies
- No special validator access required

**Feasibility:**
- Attackers can craft transactions with specific storage access patterns to maximize cross-shard dependencies
- The partitioner deterministically assigns these to global execution based on dependencies
- Block proposers will include valid transactions regardless of their global classification
- Attack requires moderate sophistication but is technically feasible

**Limiting Factors:**
- Gas costs for transactions (but attacker can use multiple accounts)
- Block size limits (but even a fraction of a block with many global txns causes issues)
- Partitioner may limit global transactions, but no hard cap is evident in the code

## Recommendation

**Recommended Fixes:**

1. **Add Timeout Protection:**
```rust
// In remote_state_value.rs
pub fn get_value_with_timeout(&self, timeout: Duration) -> Result<Option<StateValue>, TimeoutError> {
    let (lock, cvar) = &*self.value_condition;
    let mut status = lock.lock().unwrap();
    let timeout_result = cvar.wait_timeout_while(
        status,
        timeout,
        |s| matches!(s, RemoteValueStatus::Waiting)
    ).unwrap();
    
    if timeout_result.1.timed_out() {
        return Err(TimeoutError::RemoteStateTimeout);
    }
    
    match &*timeout_result.0 {
        RemoteValueStatus::Ready(value) => Ok(value.clone()),
        RemoteValueStatus::Waiting => unreachable!(),
    }
}
```

2. **Increase Thread Pool Buffer:**
```rust
// In global_executor.rs - increase buffer from +2 to +4 or +8
.num_threads(num_threads + 8)  // More headroom for blocked threads
```

3. **Limit Global Transactions:**
```rust
// Add validation in partitioner or block processing
const MAX_GLOBAL_TRANSACTIONS_PER_BLOCK: usize = 1000;
if global_txns.len() > MAX_GLOBAL_TRANSACTIONS_PER_BLOCK {
    return Err(VMStatus::Error {
        status_code: StatusCode::TOO_MANY_GLOBAL_TRANSACTIONS,
        ...
    });
}
```

4. **Monitor and Alert:**
    - Add metrics for thread pool saturation levels
    - Alert operators when global transaction counts are abnormally high
    - Track blocking duration on remote state waits

## Proof of Concept

```rust
// Proof of Concept test demonstrating thread pool saturation
#[test]
fn test_global_executor_thread_saturation() {
    use std::sync::Arc;
    use std::sync::atomic::{AtomicUsize, Ordering};
    use std::time::{Duration, Instant};
    
    // Create global executor with small thread pool for testing
    let num_threads = 4;
    let cross_shard_client = Arc::new(GlobalCrossShardClient::new(...));
    let global_executor = GlobalExecutor::new(cross_shard_client, num_threads);
    
    // Create many global transactions with cross-shard dependencies
    let num_global_txns = 100;
    let mut transactions = Vec::new();
    
    for i in 0..num_global_txns {
        // Craft transaction that reads from multiple shards
        let mut cross_shard_deps = CrossShardDependencies::new();
        
        // Add dependencies on multiple shards to force blocking
        for shard_id in 0..8 {
            cross_shard_deps.add_dependency(
                TxnIdWithShard { shard_id, round_id: 0, txn_id: i },
                vec![StateKey::raw(format!("cross_shard_key_{}_{}", shard_id, i).as_bytes())],
            );
        }
        
        transactions.push(TransactionWithDependencies {
            txn: create_test_transaction(i),
            cross_shard_dependencies: cross_shard_deps,
        });
    }
    
    let blocked_count = Arc::new(AtomicUsize::new(0));
    let blocked_count_clone = blocked_count.clone();
    
    // Instrument RemoteStateValue to track blocking
    let start = Instant::now();
    
    // Execute global transactions
    let result = global_executor.execute_global_txns(
        transactions,
        &mock_state_view,
        BlockExecutorConfigFromOnchain::default(),
    );
    
    let duration = start.elapsed();
    
    // Verify thread pool saturation caused significant delay
    assert!(duration > Duration::from_secs(5), 
        "Expected significant delay due to thread saturation, got {:?}", duration);
    
    // With proper timeouts, this should fail gracefully instead of hanging
    assert!(result.is_err(), "Should timeout or fail gracefully with saturated thread pool");
}
```

**Notes:**
- The thread pool design operates at exact capacity with zero buffer for handling blocked threads
- Cross-shard dependency blocking uses `Condvar.wait()` without timeouts, allowing indefinite blocks
- Developers acknowledged thread contention concerns in code comments but did not implement safeguards
- An attacker can craft transaction patterns to maximize global transaction classification and cross-shard dependencies
- The vulnerability affects validator performance and network liveness without violating consensus safety

### Citations

**File:** aptos-move/aptos-vm/src/sharded_block_executor/global_executor.rs (L27-42)
```rust
    pub fn new(cross_shard_client: Arc<GlobalCrossShardClient>, num_threads: usize) -> Self {
        let executor_thread_pool = Arc::new(
            rayon::ThreadPoolBuilder::new()
                // We need two extra threads for the cross-shard commit receiver and the thread
                // that is blocked on waiting for execute block to finish.
                .num_threads(num_threads + 2)
                .build()
                .unwrap(),
        );
        Self {
            global_cross_shard_client: cross_shard_client,
            executor_thread_pool,
            phantom: std::marker::PhantomData,
            concurrency_level: num_threads,
        }
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L134-180)
```rust
        executor_thread_pool.clone().scope(|s| {
            s.spawn(move |_| {
                CrossShardCommitReceiver::start(
                    cross_shard_state_view_clone,
                    cross_shard_client,
                    round,
                );
            });
            s.spawn(move |_| {
                let txn_provider =
                    DefaultTxnProvider::new_without_info(signature_verified_transactions);
                let ret = AptosVMBlockExecutorWrapper::execute_block_on_thread_pool(
                    executor_thread_pool,
                    &txn_provider,
                    aggr_overridden_state_view.as_ref(),
                    // Since we execute blocks in parallel, we cannot share module caches, so each
                    // thread has its own caches.
                    &AptosModuleCacheManager::new(),
                    config,
                    TransactionSliceMetadata::unknown(),
                    cross_shard_commit_sender,
                )
                .map(BlockOutput::into_transaction_outputs_forced);
                if let Some(shard_id) = shard_id {
                    trace!(
                        "executed sub block for shard {} and round {}",
                        shard_id,
                        round
                    );
                    // Send a self message to stop the cross-shard commit receiver.
                    cross_shard_client_clone.send_cross_shard_msg(
                        shard_id,
                        round,
                        CrossShardMsg::StopMsg,
                    );
                } else {
                    trace!("executed block for global shard and round {}", round);
                    // Send a self message to stop the cross-shard commit receiver.
                    cross_shard_client_clone.send_global_msg(CrossShardMsg::StopMsg);
                }
                callback.send(ret).unwrap();
                executor_thread_pool_clone.spawn(move || {
                    // Explicit async drop
                    drop(txn_provider);
                });
            });
        });
```

**File:** aptos-move/block-executor/src/executor.rs (L1718-1767)
```rust
        let num_workers = self.config.local.concurrency_level.min(num_txns / 2).max(2) as u32;
        // +1 for potential BlockEpilogue txn.
        let final_results = ExplicitSyncWrapper::new(
            (0..num_txns + 1)
                .map(|_| E::Output::skip_output())
                .collect::<Vec<_>>(),
        );

        let block_limit_processor = ExplicitSyncWrapper::new(BlockGasLimitProcessor::new(
            self.config.onchain.block_gas_limit_type.clone(),
            self.config.onchain.block_gas_limit_override(),
            num_txns,
        ));
        let block_epilogue_txn_idx = ExplicitSyncWrapper::new(None);
        let num_txns = num_txns as u32;

        let start_delayed_field_id_counter = gen_id_start_value(false);
        let delayed_field_id_counter = AtomicU32::new(start_delayed_field_id_counter);

        let shared_maybe_error = AtomicBool::new(false);

        // +1 for potential BlockEpilogue txn.
        let last_input_output = TxnLastInputOutput::new(num_txns + 1);
        let mut versioned_cache = MVHashMap::new();
        let scheduler = SchedulerV2::new(num_txns, num_workers);

        let shared_sync_params: SharedSyncParams<'_, T, E, S> = SharedSyncParams {
            base_view,
            versioned_cache: &versioned_cache,
            global_module_cache: module_cache_manager_guard.module_cache(),
            last_input_output: &last_input_output,
            delayed_field_id_counter: &delayed_field_id_counter,
            start_shared_counter: start_delayed_field_id_counter,
            block_limit_processor: &block_limit_processor,
            final_results: &final_results,
            maybe_block_epilogue_txn_idx: &block_epilogue_txn_idx,
        };

        let async_runtime_checks_enabled = should_perform_async_runtime_checks_for_block(
            module_cache_manager_guard.environment(),
            num_txns,
            num_workers,
        );

        let timer = RAYON_EXECUTION_SECONDS.start_timer();
        let worker_ids: Vec<u32> = (0..num_workers).collect();
        let maybe_executor = ExplicitSyncWrapper::new(None);
        self.executor_thread_pool.scope(|s| {
            for worker_id in &worker_ids {
                s.spawn(|_| {
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/remote_state_value.rs (L29-39)
```rust
    pub fn get_value(&self) -> Option<StateValue> {
        let (lock, cvar) = &*self.value_condition;
        let mut status = lock.lock().unwrap();
        while let RemoteValueStatus::Waiting = *status {
            status = cvar.wait(status).unwrap();
        }
        match &*status {
            RemoteValueStatus::Ready(value) => value.clone(),
            RemoteValueStatus::Waiting => unreachable!(),
        }
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L203-211)
```rust
        // This means that we are executing the global transactions concurrently with the individual shards but the
        // global transactions will be blocked for cross shard transaction results. This hopefully will help with
        // finishing the global transactions faster but we need to evaluate if this causes thread contention. If it
        // does, then we can simply move this call to the end of the function.
        let mut global_output = self.global_executor.execute_global_txns(
            global_txns,
            state_view.as_ref(),
            onchain_config,
        )?;
```
