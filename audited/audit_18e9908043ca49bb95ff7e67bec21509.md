# Audit Report

## Title
Incomplete Upsert Logic in move_resources Table Prevents Data Correction

## Summary
The `insert_move_resources` function in the Aptos indexer uses an incomplete upsert operation that only updates `inserted_at` and `state_key_hash` fields on conflict, but does NOT update critical fields including `is_deleted`, `data`, `name`, `type_`, `address`, `module`, and `generic_type_params`. This prevents correction of corrupted data during reindexing operations. [1](#0-0) 

## Finding Description

The indexer is designed to support idempotent reprocessing of transactions, as evidenced by explicit testing of this behavior: [2](#0-1) 

However, the upsert logic in `insert_move_resources` has a critical limitation. When a conflict occurs on the primary key `(transaction_version, write_set_change_index)`, only two fields are updated:
- `inserted_at`
- `state_key_hash`

The `is_deleted` field (and other critical fields) are NOT updated during conflict resolution. This creates a **data corruption persistence** problem where:

1. If the indexer initially writes incorrect `is_deleted` values due to a bug, crash, or data corruption
2. And operators attempt to fix the issue by upgrading the indexer code and reprocessing transactions
3. The reprocessing will NOT correct the `is_deleted` field values
4. Applications querying the `move_resources` table will continue to read incorrect deletion status

The `move_resources` table structure shows that `is_deleted` is a critical field for tracking resource lifecycle: [3](#0-2) 

The correct logic for creating `MoveResource` records distinguishes between writes and deletes: [4](#0-3) 

However, other "current" tables in the indexer properly update ALL fields including `is_deleted` during upserts: [5](#0-4) 

Note that `current_objects` correctly updates `is_deleted` at line 463 and includes a WHERE clause to ensure version ordering.

## Impact Explanation

This issue qualifies as **Medium Severity** under "State inconsistencies requiring intervention" because:

1. **Data Integrity**: Once incorrect `is_deleted` values enter the database, they cannot be corrected through standard reindexing procedures
2. **Application Impact**: Applications relying on the indexer's `move_resources` table will receive incorrect resource deletion status, potentially causing them to:
   - Attempt to read resources that were deleted on-chain
   - Ignore resources that still exist on-chain
   - Make incorrect business logic decisions based on stale data
3. **Operational Complexity**: Fixing corrupted data requires manual database intervention rather than automated reindexing

However, this is NOT Critical severity because:
- It does not affect blockchain consensus or execution
- It does not affect the actual on-chain state
- It only impacts the indexer's database view, not the canonical blockchain state

## Likelihood Explanation

This vulnerability can manifest in several realistic scenarios:

1. **Indexer Bug Remediation**: If a bug in the indexer code causes incorrect `is_deleted` values to be written, upgrading the indexer and reprocessing will not fix the corrupted data

2. **Crash Recovery**: If the indexer crashes after writing transaction data but before updating processor status, reprocessing those transactions will not update existing records due to the incomplete upsert

3. **Schema Evolution**: Database migrations or data corrections that rely on reprocessing will fail to update `is_deleted` values

The likelihood is MODERATE because:
- The indexer is explicitly designed for idempotent reprocessing (as shown in tests)
- Operators reasonably expect reprocessing to correct data inconsistencies
- The incomplete upsert silently fails to update fields without error indication

## Recommendation

Update the `insert_move_resources` function to update ALL fields on conflict, matching the pattern used in `insert_current_objects`:

```rust
fn insert_move_resources(
    conn: &mut PgConnection,
    items_to_insert: &[MoveResource],
) -> Result<(), diesel::result::Error> {
    use schema::move_resources::dsl::*;
    let chunks = get_chunks(items_to_insert.len(), MoveResource::field_count());
    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::move_resources::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict((transaction_version, write_set_change_index))
                .do_update()
                .set((
                    transaction_block_height.eq(excluded(transaction_block_height)),
                    name.eq(excluded(name)),
                    address.eq(excluded(address)),
                    type_.eq(excluded(type_)),
                    module.eq(excluded(module)),
                    generic_type_params.eq(excluded(generic_type_params)),
                    data.eq(excluded(data)),
                    is_deleted.eq(excluded(is_deleted)),  // CRITICAL FIX
                    state_key_hash.eq(excluded(state_key_hash)),
                    inserted_at.eq(excluded(inserted_at)),
                )),
            None,
        )?;
    }
    Ok(())
}
```

## Proof of Concept

**Reproduction Steps:**

1. Set up an Aptos indexer with PostgreSQL backend
2. Manually corrupt a record in `move_resources` table:
```sql
UPDATE move_resources 
SET is_deleted = false 
WHERE transaction_version = 100 
  AND write_set_change_index = 0 
  AND is_deleted = true;
```
3. Configure indexer to reprocess transaction version 100
4. Observe that `is_deleted` remains `false` even though reprocessing should have set it to `true`
5. Query the table to verify corruption persists:
```sql
SELECT transaction_version, write_set_change_index, is_deleted, name 
FROM move_resources 
WHERE transaction_version = 100 
ORDER BY write_set_change_index;
```

**Expected vs Actual Behavior:**
- **Expected**: Reprocessing corrects the `is_deleted` value to match the actual write set change type
- **Actual**: The `is_deleted` value remains incorrectly set to `false`, causing applications to read stale deletion status

---

**Notes:**

While this vulnerability does not directly allow `is_deleted` to be false for deleted resources under normal operation (the initial write logic is correct), it prevents correction of any data corruption that might occur. The security guarantee "applications will read consistent resource deletion status from the indexer" is violated when data correction is needed but impossible due to incomplete upsert logic.

### Citations

**File:** crates/indexer/src/processors/default_processor.rs (L337-358)
```rust
fn insert_move_resources(
    conn: &mut PgConnection,
    items_to_insert: &[MoveResource],
) -> Result<(), diesel::result::Error> {
    use schema::move_resources::dsl::*;
    let chunks = get_chunks(items_to_insert.len(), MoveResource::field_count());
    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::move_resources::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict((transaction_version, write_set_change_index))
                .do_update()
                .set((
                    inserted_at.eq(excluded(inserted_at)),
                    state_key_hash.eq(excluded(state_key_hash)),
                )),
            None,
        )?;
    }
    Ok(())
}
```

**File:** crates/indexer/src/processors/default_processor.rs (L444-470)
```rust
fn insert_current_objects(
    conn: &mut PgConnection,
    items_to_insert: &[CurrentObject],
) -> Result<(), diesel::result::Error> {
    use schema::current_objects::dsl::*;
    let chunks = get_chunks(items_to_insert.len(), CurrentObject::field_count());
    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::current_objects::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict(object_address)
                .do_update()
                .set((
                    owner_address.eq(excluded(owner_address)),
                    state_key_hash.eq(excluded(state_key_hash)),
                    allow_ungated_transfer.eq(excluded(allow_ungated_transfer)),
                    last_guid_creation_num.eq(excluded(last_guid_creation_num)),
                    last_transaction_version.eq(excluded(last_transaction_version)),
                    is_deleted.eq(excluded(is_deleted)),
                    inserted_at.eq(excluded(inserted_at)),
                )),
                Some(" WHERE current_objects.last_transaction_version <= excluded.last_transaction_version "),
        )?;
    }
    Ok(())
}
```

**File:** crates/indexer/src/indexer/tailer.rs (L815-825)
```rust
        // We run it twice to ensure we don't explode. Idempotency!
        tailer
            .processor
            .process_transactions_with_status(vec![user_txn.clone()])
            .await
            .unwrap();
        tailer
            .processor
            .process_transactions_with_status(vec![user_txn.clone()])
            .await
            .unwrap();
```

**File:** crates/indexer/src/schema.rs (L557-574)
```rust
    move_resources (transaction_version, write_set_change_index) {
        transaction_version -> Int8,
        write_set_change_index -> Int8,
        transaction_block_height -> Int8,
        name -> Text,
        #[max_length = 66]
        address -> Varchar,
        #[sql_name = "type"]
        type_ -> Text,
        module -> Text,
        generic_type_params -> Nullable<Jsonb>,
        data -> Nullable<Jsonb>,
        is_deleted -> Bool,
        inserted_at -> Timestamp,
        #[max_length = 66]
        state_key_hash -> Varchar,
    }
}
```

**File:** crates/indexer/src/models/move_resources.rs (L36-78)
```rust
    pub fn from_write_resource(
        write_resource: &WriteResource,
        write_set_change_index: i64,
        transaction_version: i64,
        transaction_block_height: i64,
    ) -> Self {
        let parsed_data = Self::convert_move_struct_tag(&write_resource.data.typ);
        Self {
            transaction_version,
            transaction_block_height,
            write_set_change_index,
            type_: write_resource.data.typ.to_string(),
            name: parsed_data.name.clone(),
            address: standardize_address(&write_resource.address.to_string()),
            module: parsed_data.module.clone(),
            generic_type_params: parsed_data.generic_type_params,
            data: Some(serde_json::to_value(&write_resource.data.data).unwrap()),
            is_deleted: false,
            state_key_hash: standardize_address(write_resource.state_key_hash.as_str()),
        }
    }

    pub fn from_delete_resource(
        delete_resource: &DeleteResource,
        write_set_change_index: i64,
        transaction_version: i64,
        transaction_block_height: i64,
    ) -> Self {
        let parsed_data = Self::convert_move_struct_tag(&delete_resource.resource);
        Self {
            transaction_version,
            transaction_block_height,
            write_set_change_index,
            type_: delete_resource.resource.to_string(),
            name: parsed_data.name.clone(),
            address: standardize_address(&delete_resource.address.to_string()),
            module: parsed_data.module.clone(),
            generic_type_params: parsed_data.generic_type_params,
            data: None,
            is_deleted: true,
            state_key_hash: standardize_address(delete_resource.state_key_hash.as_str()),
        }
    }
```
