# Audit Report

## Title
TOCTOU Race Condition in send_for_execution() Causing Out-of-Order Block Delivery to Execution Pipeline

## Summary
The `send_for_execution()` function in `consensus/src/block_storage/block_store.rs` contains a Time-of-Check-Time-of-Use (TOCTOU) race condition due to non-atomic lock acquisitions. When multiple threads concurrently process finality proofs, the `ordered_root` state can change between the validation check and the path computation, causing blocks to be sent to the execution pipeline in incorrect order, violating the sequential round ordering invariant required for consensus safety.

## Finding Description

The vulnerability exists in the `send_for_execution()` function which performs critical state updates through separate, non-atomic lock acquisitions: [1](#0-0) 

The function executes in this sequence:

1. **Line 323**: Validates that `block_to_commit.round() > self.ordered_root().round()` (acquires and releases read lock on `inner`)
2. **Line 327-329**: Computes `path_from_ordered_root(block_id_to_commit)` (acquires and releases read lock on `inner` again)
3. **Line 334-336**: Acquires `pending_blocks.lock()`, calls `gc()`, then releases
4. **Line 338**: Acquires `inner.write()`, updates `ordered_root`, then releases  
5. **Line 339-341**: Acquires `inner.write()` again, calls `insert_ordered_cert()`, then releases
6. **Line 344-347**: Sends blocks to execution via `finalize_order()`

**The Race Condition:**

Between steps 1-2 and steps 4-5, another thread can execute the entire sequence and update `ordered_root`. This causes the second thread to compute its block path from a different root than it validated against.

**Concrete Attack Scenario:**

Initial state: `ordered_root = Block B8 (round 8)`, chain contains `B8 → B9 → B10`

Two threads process concurrent finality proofs:
- Thread A: finality_proof for B10 (round 10)
- Thread B: finality_proof for B9 (round 9)

**Interleaved Execution Timeline:**

1. Thread A: Checks `B10.round(10) > ordered_root.round(8)` ✓ PASS
2. Thread B: Checks `B9.round(9) > ordered_root.round(8)` ✓ PASS  
3. Thread B: Computes `path_from_ordered_root(B9)` from root B8 → returns `[B9]`
4. Thread B: Calls `pending_blocks.lock().gc(9)`
5. Thread B: Calls `inner.write().update_ordered_root(B9)` — **ordered_root now B9**
6. Thread B: Calls `inner.write().insert_ordered_cert()`
7. Thread A: Computes `path_from_ordered_root(B10)` — **now reads root B9** → returns `[B10]` only (should be `[B9, B10]` from B8!)
8. Thread A: Calls `pending_blocks.lock().gc(10)`
9. Thread A: Calls `inner.write().update_ordered_root(B10)`
10. Thread A: Calls `inner.write().insert_ordered_cert()`
11. Thread A: Calls `finalize_order([B10], proof_10).await` — sends to execution channel
12. Thread B: Calls `finalize_order([B9], proof_9).await` — sends to execution channel

**The Violation:**

If Thread A's async `finalize_order` at step 11 completes before Thread B's at step 12, the execution pipeline receives blocks in wrong order: `[B10]` arrives before `[B9]`, violating the sequential round ordering invariant.

The `path_from_ordered_root()` implementation reads the current `ordered_root` state at call time: [2](#0-1) 

This function is called while holding no locks, allowing `ordered_root_id` and `ordered_root().round()` to change between Thread A's validation check and path computation.

The execution client sends blocks through an unbounded channel: [3](#0-2) 

The channel operations are async, and completion order depends on task scheduling, not call order. This allows blocks to arrive at the buffer manager out of sequence.

The consensus observer's verification function explicitly checks that blocks are correctly chained: [4](#0-3) 

This verification enforces that blocks must form a valid chain, but this check happens in a different code path (consensus observer) and doesn't protect the main execution pipeline from the race condition in `send_for_execution()`.

## Impact Explanation

**Severity: HIGH** (qualifies for up to $50,000 under Aptos Bug Bounty program)

This vulnerability causes **significant protocol violations** through:

1. **Consensus Safety Violation**: Blocks arriving at the execution layer out of round order can cause validators to execute blocks in different sequences, potentially leading to state divergence between honest validators.

2. **Invariant Violation**: Breaks the critical invariant that blocks must be executed in sequential round order. The system assumes `ordered_root` represents the highest round successfully sent for execution, but the race allows `ordered_root` to be updated before blocks are actually delivered.

3. **State Inconsistency**: Creates a window where `ordered_root` indicates blocks are ordered, but they haven't been delivered to execution yet, or have been delivered in wrong order.

4. **Deterministic Execution Violation**: If different validators experience different thread scheduling, they may execute blocks in different orders, violating the deterministic execution requirement where "all validators must produce identical state roots for identical blocks."

This does not qualify as Critical severity because it doesn't directly cause fund loss or network partition, but it represents a significant threat to consensus correctness.

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability has high probability of occurrence because:

1. **Normal Operation Trigger**: Concurrent finality proofs for consecutive or nearby blocks are expected during normal blockchain operation, especially under high throughput when blocks are being proposed and certified rapidly.

2. **No Special Permissions Required**: Any normal consensus operation can trigger this race condition—no Byzantine behavior, validator collusion, or special network conditions are needed.

3. **Timing Window**: The race window exists between lines 323 and 338 (approximately 15 lines of code execution), providing sufficient opportunity for interleaving on multi-core validator nodes.

4. **Multi-threaded Environment**: Validator nodes process consensus messages concurrently across multiple threads, making race conditions in shared state updates highly likely.

5. **Asynchronous Execution**: The `.await` on `finalize_order()` introduces non-deterministic scheduling, increasing the probability of out-of-order delivery.

## Recommendation

**Fix: Atomic Lock Acquisition for State Consistency**

The fix requires making all validation, path computation, and state updates atomic by holding the necessary locks throughout the entire operation:

```rust
pub async fn send_for_execution(
    &self,
    finality_proof: WrappedLedgerInfo,
) -> anyhow::Result<()> {
    let block_id_to_commit = finality_proof.commit_info().id();
    let block_to_commit = self
        .get_block(block_id_to_commit)
        .ok_or_else(|| format_err!("Committed block id not found"))?;

    // ATOMIC SECTION: Acquire all necessary locks before any state reads/updates
    let mut pending_blocks_guard = self.pending_blocks.lock();
    let mut inner_guard = self.inner.write();
    
    // Perform validation and path computation while holding locks
    ensure!(
        block_to_commit.round() > inner_guard.ordered_root().round(),
        "Committed block round lower than root"
    );

    let blocks_to_commit = inner_guard
        .path_from_ordered_root(block_id_to_commit)
        .unwrap_or_default();
    
    assert!(!blocks_to_commit.is_empty());

    // Perform all state updates atomically
    pending_blocks_guard.gc(finality_proof.commit_info().round());
    inner_guard.update_ordered_root(block_to_commit.id());
    inner_guard.insert_ordered_cert(finality_proof.clone());
    
    // Release locks before async operation
    drop(pending_blocks_guard);
    drop(inner_guard);
    
    update_counters_for_ordered_blocks(&blocks_to_commit);

    self.execution_client
        .finalize_order(blocks_to_commit, finality_proof.clone())
        .await
        .expect("Failed to persist commit");

    Ok(())
}
```

**Key Changes:**
1. Acquire both `pending_blocks.lock()` and `inner.write()` at the beginning
2. Perform all validation and path computation while holding both locks
3. Perform all state updates atomically before releasing locks
4. Release locks explicitly before the async `finalize_order()` call to avoid holding locks across await points

This ensures that the validation check, path computation, and state updates all see a consistent snapshot of `ordered_root`, preventing the TOCTOU race condition.

## Proof of Concept

```rust
// Rust test demonstrating the race condition
#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_send_for_execution_race_condition() {
    // Setup: Create block store with genesis block
    let mut inserter = TreeInserter::default();
    let block_store = inserter.block_store();
    let genesis = block_store.ordered_root();
    
    // Insert blocks B8 -> B9 -> B10
    let block_8 = inserter.insert_block_with_qc(
        certificate_for_genesis(), &genesis, 8
    ).await;
    let block_9 = inserter.insert_block(&block_8, 9, None).await;
    let block_10 = inserter.insert_block(&block_9, 10, None).await;
    
    // Create finality proofs for B9 and B10
    let proof_9 = gen_test_certificate(
        vec![&block_9.block()],
        block_9.quorum_cert().clone(),
        block_9.id(),
        9,
    );
    let proof_10 = gen_test_certificate(
        vec![&block_10.block()],
        block_10.quorum_cert().clone(), 
        block_10.id(),
        10,
    );
    
    // Spawn concurrent tasks to trigger race condition
    let block_store_clone = block_store.clone();
    let task_a = tokio::spawn(async move {
        block_store_clone
            .send_for_execution(proof_10.into_wrapped_ledger_info())
            .await
            .expect("Failed to send B10");
    });
    
    let block_store_clone2 = block_store.clone();
    let task_b = tokio::spawn(async move {
        block_store_clone2
            .send_for_execution(proof_9.into_wrapped_ledger_info())
            .await
            .expect("Failed to send B9");
    });
    
    // Wait for both tasks
    let _ = tokio::join!(task_a, task_b);
    
    // Verify execution received blocks in correct order
    // This test should fail intermittently when the race occurs
    let execution_channel = block_store.execution_client.get_execution_channel();
    // Check that B9 was received before B10
    // (Implementation would need access to execution channel internals)
}
```

**Notes:**
- The PoC requires access to internal execution channel state to verify block ordering
- The race is timing-dependent and may not reproduce on every run
- Running with thread sanitizers (TSAN) or under high concurrency increases detection probability
- The test demonstrates that concurrent `send_for_execution()` calls for consecutive blocks can cause the race condition described

### Citations

**File:** consensus/src/block_storage/block_store.rs (L312-350)
```rust
    pub async fn send_for_execution(
        &self,
        finality_proof: WrappedLedgerInfo,
    ) -> anyhow::Result<()> {
        let block_id_to_commit = finality_proof.commit_info().id();
        let block_to_commit = self
            .get_block(block_id_to_commit)
            .ok_or_else(|| format_err!("Committed block id not found"))?;

        // First make sure that this commit is new.
        ensure!(
            block_to_commit.round() > self.ordered_root().round(),
            "Committed block round lower than root"
        );

        let blocks_to_commit = self
            .path_from_ordered_root(block_id_to_commit)
            .unwrap_or_default();

        assert!(!blocks_to_commit.is_empty());

        let finality_proof_clone = finality_proof.clone();
        self.pending_blocks
            .lock()
            .gc(finality_proof.commit_info().round());

        self.inner.write().update_ordered_root(block_to_commit.id());
        self.inner
            .write()
            .insert_ordered_cert(finality_proof_clone.clone());
        update_counters_for_ordered_blocks(&blocks_to_commit);

        self.execution_client
            .finalize_order(blocks_to_commit, finality_proof.clone())
            .await
            .expect("Failed to persist commit");

        Ok(())
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L548-553)
```rust
    pub(super) fn path_from_ordered_root(
        &self,
        block_id: HashValue,
    ) -> Option<Vec<Arc<PipelinedBlock>>> {
        self.path_from_root_to_block(block_id, self.ordered_root_id, self.ordered_root().round())
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L590-620)
```rust
    async fn finalize_order(
        &self,
        blocks: Vec<Arc<PipelinedBlock>>,
        ordered_proof: WrappedLedgerInfo,
    ) -> ExecutorResult<()> {
        assert!(!blocks.is_empty());
        let mut execute_tx = match self.handle.read().execute_tx.clone() {
            Some(tx) => tx,
            None => {
                debug!("Failed to send to buffer manager, maybe epoch ends");
                return Ok(());
            },
        };

        for block in &blocks {
            block.set_insertion_time();
            if let Some(tx) = block.pipeline_tx().lock().as_mut() {
                tx.order_proof_tx
                    .take()
                    .map(|tx| tx.send(ordered_proof.clone()));
            }
        }

        if execute_tx
            .send(OrderedBlocks {
                ordered_blocks: blocks,
                ordered_proof: ordered_proof.ledger_info().clone(),
            })
            .await
            .is_err()
        {
```

**File:** consensus/src/consensus_observer/network/observer_message.rs (L227-266)
```rust
    pub fn verify_ordered_blocks(&self) -> Result<(), Error> {
        // Verify that we have at least one ordered block
        if self.blocks.is_empty() {
            return Err(Error::InvalidMessageError(
                "Received empty ordered block!".to_string(),
            ));
        }

        // Verify the last block ID matches the ordered proof block ID
        if self.last_block().id() != self.proof_block_info().id() {
            return Err(Error::InvalidMessageError(
                format!(
                    "Last ordered block ID does not match the ordered proof ID! Number of blocks: {:?}, Last ordered block ID: {:?}, Ordered proof ID: {:?}",
                    self.blocks.len(),
                    self.last_block().id(),
                    self.proof_block_info().id()
                )
            ));
        }

        // Verify the blocks are correctly chained together (from the last block to the first)
        let mut expected_parent_id = None;
        for block in self.blocks.iter().rev() {
            if let Some(expected_parent_id) = expected_parent_id {
                if block.id() != expected_parent_id {
                    return Err(Error::InvalidMessageError(
                        format!(
                            "Block parent ID does not match the expected parent ID! Block ID: {:?}, Expected parent ID: {:?}",
                            block.id(),
                            expected_parent_id
                        )
                    ));
                }
            }

            expected_parent_id = Some(block.parent_id());
        }

        Ok(())
    }
```
