# Audit Report

## Title
Retry Amplification DoS in Indexer gRPC File Store Reader

## Summary
The indexer-grpc file store reader implements an aggressive retry mechanism that amplifies storage load by up to 4x when handling invalid transaction version requests. Attackers can exploit this by sending concurrent requests with versions that trigger transient storage errors, overwhelming the GCS backend and degrading indexer API availability.

## Finding Description

The vulnerability exists in the file store transaction retrieval flow where external clients can trigger retry amplification on storage operations.

**Attack Flow:**

1. External client sends `GetTransactionsRequest` to the GrpcManager service with an attacker-controlled `starting_version` parameter [1](#0-0) 

2. The request reaches `DataManager.get_transactions()` which, for versions older than the cache, invokes the file store reader with a hardcoded retry count of 3 [2](#0-1) 

3. The `FileStoreReader.get_transaction_batch()` calls `get_transaction_file_at_version()` for each file, passing through the retry parameter [3](#0-2) 

4. The retry logic in `get_transaction_file_at_version()` implements an aggressive retry loop that on each storage error, retries up to 3 additional times with only a 10ms sleep between attempts [4](#0-3) 

5. Each retry invokes `get_raw_file()` on the GCS backend, which performs a full object download operation [5](#0-4) 

**Vulnerability Characteristics:**

- **No input validation**: The `starting_version` parameter is passed directly to storage operations without checking if the version exists or is within valid bounds
- **Short retry delay**: The 10ms sleep between retries is insufficient for transient storage issues to resolve, ensuring all retries hit the storage backend
- **No rate limiting**: The gRPC endpoint has no concurrency limits or rate limiting on requests
- **Load amplification**: Each request generates up to 4 storage operations (1 initial + 3 retries), creating a 4x amplification factor
- **Cascading failure potential**: If enough concurrent requests trigger GCS rate limiting, subsequent requests will also fail and retry, further amplifying the load

**Attack Scenario:**

An attacker identifies versions in the range `[0, cache_start_version)` and sends hundreds of concurrent `GetTransactionsRequest` messages targeting versions that may cause transient storage errors (network issues, rate limiting, etc.). Each failed request retries 3 times with minimal delay, amplifying the load on GCS by 4x. This can trigger GCS rate limiting (1 request per second per object), causing legitimate indexer queries to fail and degrading API availability.

## Impact Explanation

This qualifies as **Medium severity** per Aptos bug bounty criteria because it enables denial of service against the indexer-grpc API infrastructure. While it does not directly impact consensus, validator operations, or blockchain state, it breaks the **Resource Limits** invariant (#9) by failing to enforce proper limits on storage operations in the indexer subsystem.

The impact includes:
- Degraded indexer API response times for all clients
- Potential cascading failures if GCS rate limits are reached
- Increased infrastructure costs from amplified storage operations
- Reduced reliability of applications depending on the indexer API

The issue falls under "API crashes" or service degradation, which is categorized as High severity in the bug bounty program, though the specific file store amplification impact is more appropriately Medium severity as indicated in the security question.

## Likelihood Explanation

**Likelihood: High**

The attack is highly feasible because:
- No authentication or privileged access required to call the gRPC endpoint
- Attacker can easily determine valid version ranges by querying metadata
- No rate limiting or request validation prevents malicious requests
- Simple to execute with standard gRPC clients
- Low technical barrier - requires only network access to the indexer service

The only limiting factor is that the attacker must have network access to the indexer-grpc service, but this is typically exposed publicly to serve client applications.

## Recommendation

Implement multiple defense layers:

**1. Version Validation**: Add validation before hitting storage to check if the requested version is within servable bounds and exists in the file store metadata.

**2. Rate Limiting**: Implement per-client rate limiting on the gRPC endpoint, possibly using `tonic-ratelimit` or similar middleware.

**3. Retry Backoff**: Replace the fixed 10ms delay with exponential backoff (e.g., 50ms, 200ms, 800ms) to give storage systems time to recover.

**4. Reduce Retry Count**: Decrease the hardcoded retry count from 3 to 1 or 0 for initial requests, reserving retries only for known-good operations.

**5. Circuit Breaker**: Implement a circuit breaker pattern that stops sending requests to storage after a threshold of failures is reached.

Example fix for the retry logic in `file_store_reader.rs`:

```rust
async fn get_transaction_file_at_version(
    &self,
    version: u64,
    suffix: Option<u64>,
    retries: u8,
) -> Result<Vec<Transaction>> {
    let mut retries = retries;
    let mut backoff_ms = 50; // Start with 50ms
    let bytes = loop {
        let path = self.get_path_for_version(version, suffix);
        match self.reader.get_raw_file(path.clone()).await {
            Ok(bytes) => break bytes.unwrap_or_else(|| panic!("File should exist: {path:?}.")),
            Err(err) => {
                if retries == 0 {
                    return Err(err);
                }
                retries -= 1;
                // Exponential backoff instead of fixed 10ms
                tokio::time::sleep(std::time::Duration::from_millis(backoff_ms)).await;
                backoff_ms *= 4; // Exponential increase
            },
        }
    };
    // ... rest of function
}
```

And reduce the hardcoded retry count in `data_manager.rs`:

```rust
self.file_store_reader
    .get_transaction_batch(
        start_version,
        /*retries=*/ 1, // Reduced from 3
        /*max_files=*/ Some(1),
        /*filter=*/ None,
        /*ending_version=*/ None,
        tx,
    )
    .await;
```

## Proof of Concept

```rust
// Rust PoC demonstrating the retry amplification attack
use aptos_protos::indexer::v1::{GetTransactionsRequest, grpc_manager_client::GrpcManagerClient};
use tokio::time::{Duration, Instant};
use std::sync::Arc;
use std::sync::atomic::{AtomicU64, Ordering};

#[tokio::test]
async fn test_retry_amplification_dos() {
    // Connect to indexer-grpc-manager service
    let mut client = GrpcManagerClient::connect("http://localhost:50051")
        .await
        .expect("Failed to connect");

    let storage_request_count = Arc::new(AtomicU64::new(0));
    
    // Simulate 100 concurrent malicious requests targeting invalid versions
    let start = Instant::now();
    let mut handles = vec![];
    
    for i in 0..100 {
        let mut client_clone = client.clone();
        let counter = storage_request_count.clone();
        
        handles.push(tokio::spawn(async move {
            // Request a version that's likely to cause retries
            // (older than cache but potentially not in file store)
            let request = GetTransactionsRequest {
                starting_version: Some(i * 1000), // Invalid versions
                ..Default::default()
            };
            
            // Each failed request will trigger 4 storage operations (1 + 3 retries)
            let result = client_clone.get_transactions(request).await;
            
            // Track that we triggered retries
            if result.is_err() {
                counter.fetch_add(4, Ordering::SeqCst); // 4x amplification
            }
        }));
    }
    
    for handle in handles {
        let _ = handle.await;
    }
    
    let elapsed = start.elapsed();
    let total_storage_requests = storage_request_count.load(Ordering::SeqCst);
    
    println!("Attack completed in {:?}", elapsed);
    println!("Total amplified storage requests: {}", total_storage_requests);
    
    // With 100 malicious requests and 4x amplification factor,
    // we expect approximately 400 storage operations to be triggered
    assert!(total_storage_requests >= 300, 
        "Expected significant retry amplification, got {}", total_storage_requests);
}
```

**Notes**

This vulnerability affects the indexer-grpc auxiliary service, not the core Aptos consensus or execution layers. While it does not compromise blockchain safety or validator operations, it degrades the availability of the indexer API which is critical infrastructure for applications and block explorers built on Aptos. The retry amplification pattern violates resource limit best practices and should be addressed to ensure robust API availability.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/service.rs (L129-146)
```rust
    async fn get_transactions(
        &self,
        request: Request<GetTransactionsRequest>,
    ) -> Result<Response<TransactionsResponse>, Status> {
        let request = request.into_inner();
        let transactions = self
            .data_manager
            .get_transactions(request.starting_version(), MAX_SIZE_BYTES_FROM_CACHE)
            .await
            .map_err(|e| Status::internal(format!("{e}")))?;

        Ok(Response::new(TransactionsResponse {
            transactions,
            chain_id: Some(self.chain_id),
            // Not used.
            processed_range: None,
        }))
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L342-352)
```rust
        let (tx, mut rx) = channel(1);
        self.file_store_reader
            .get_transaction_batch(
                start_version,
                /*retries=*/ 3,
                /*max_files=*/ Some(1),
                /*filter=*/ None,
                /*ending_version=*/ None,
                tx,
            )
            .await;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_reader.rs (L122-124)
```rust
            let transactions = self
                .get_transaction_file_at_version(current_version, batch_metadata.suffix, retries)
                .await;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_reader.rs (L219-232)
```rust
        let mut retries = retries;
        let bytes = loop {
            let path = self.get_path_for_version(version, suffix);
            match self.reader.get_raw_file(path.clone()).await {
                Ok(bytes) => break bytes.unwrap_or_else(|| panic!("File should exist: {path:?}.")),
                Err(err) => {
                    if retries == 0 {
                        return Err(err);
                    }
                    retries -= 1;
                    tokio::time::sleep(std::time::Duration::from_millis(10)).await;
                },
            }
        };
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/gcs.rs (L95-115)
```rust
    async fn get_raw_file(&self, file_path: PathBuf) -> Result<Option<Vec<u8>>> {
        let path = self.get_path(file_path);
        trace!(
            "Downloading object at {}/{}.",
            self.bucket_name,
            path.as_str()
        );
        match Object::download(&self.bucket_name, path.as_str()).await {
            Ok(file) => Ok(Some(file)),
            Err(cloud_storage::Error::Other(err)) => {
                if err.contains("No such object: ") {
                    Ok(None)
                } else {
                    bail!("[Indexer File] Error happens when downloading file at {path:?}. {err}",);
                }
            },
            Err(err) => {
                bail!("[Indexer File] Error happens when downloading file at {path:?}. {err}");
            },
        }
    }
```
