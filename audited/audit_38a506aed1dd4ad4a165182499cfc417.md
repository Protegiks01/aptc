# Audit Report

## Title
Consensus Sync Request State Machine Corruption via TOCTOU Race Condition During Concurrent Sync Duration Notifications

## Summary
A Time-of-Check-Time-of-Use (TOCTOU) race condition exists in the state sync driver's handling of concurrent sync duration notifications. When a new sync notification arrives during the async yield point while processing a satisfied sync request, the state machine responds to the wrong request callback, causing the original consensus task to hang indefinitely and corrupting the sync state.

## Finding Description

The vulnerability exists in the interaction between `check_sync_request_progress()` and `initialize_sync_duration_request()` across an async yield boundary.

The critical flaw occurs in this execution sequence:

1. **Initial capture**: `check_sync_request_progress()` captures an Arc reference to the current sync request (Arc1/SyncRequest1) via `get_sync_request()`. [1](#0-0) 

2. **Satisfaction check**: The function determines SyncRequest1 has completed its duration. [2](#0-1) 

3. **Async yield point**: The function enters a while loop waiting for the storage synchronizer to drain pending data, calling `yield_now().await` which yields control back to the async runtime. [3](#0-2) 

4. **Race window**: During the yield, the `futures::select!` in the main driver loop processes a new consensus sync duration notification, triggering `handle_consensus_sync_duration_notification()`. [4](#0-3) 

5. **Arc replacement without validation**: The handler calls `initialize_sync_duration_request()` which creates a NEW Arc (Arc2/SyncRequest2) and replaces `self.consensus_sync_request` without any validation of existing active requests. [5](#0-4) 

6. **Validation using stale reference**: When `check_sync_request_progress()` resumes after the yield, it continues using the local variable (still pointing to Arc1) for validation checks. [6](#0-5) 

7. **Wrong request handled**: The function calls `handle_satisfied_sync_request()`, which accesses `self.consensus_sync_request` directly (not the local variable). [7](#0-6) 

8. **State corruption**: `handle_satisfied_sync_request()` locks `self.consensus_sync_request` (now Arc2), takes SyncRequest2 out, and responds to SyncRequest2's callback. [8](#0-7) 

**Result**: SyncRequest1's callback is never invoked, causing the consensus task to hang forever. SyncRequest2's callback receives a premature response for an incomplete sync operation.

Each sync notification contains a `oneshot::Sender` callback that consensus awaits on. [9](#0-8)  When consensus calls `sync_for_duration()`, it blocks on `callback_receiver.await` with no timeout. [10](#0-9)  If this callback is never sent a response due to the race condition, the consensus task is permanently blocked.

The vulnerability is enabled because consensus and consensus observer use separate `ExecutionProxy` instances but share the same `state_sync_notifier` channel. [11](#0-10) [12](#0-11)  This allows them to send concurrent sync requests that bypass the `write_mutex` protection in `ExecutionProxy`. [13](#0-12) 

## Impact Explanation

This vulnerability meets **Critical Severity** criteria under the Aptos bug bounty program:

**Total Loss of Liveness/Network Availability**: When a consensus task is permanently blocked waiting for a sync response that never arrives, it can prevent:
- Block proposals from being generated
- Votes from being cast on proposed blocks
- Epoch transitions from completing
- The validator from participating in consensus

**Consensus Protocol Violations**: The state sync component violates its contract with consensus by:
- Failing to respond to valid sync requests
- Sending responses for the wrong sync operations
- Corrupting the sync state machine that consensus depends on

**Significant Protocol Violations**: The bug causes:
- Orphaned oneshot callback channels that can never be completed
- Mismatched sync request/response pairs
- Unpredictable behavior when multiple sync requests are in flight

The vulnerability is exploitable under normal network conditions when consensus sends multiple sync requests (e.g., during network partitions, fallback mode, or rapid epoch changes). No Byzantine behavior or validator collusion is required.

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability is triggered when:
1. A sync duration request is in the process of completing (has satisfied its duration)
2. The storage synchronizer still has pending data to drain (causing yield_now() loop)
3. A second sync duration notification arrives during this window

This scenario occurs naturally in several situations:
- **Consensus fallback mode**: When consensus observer enters fallback synchronization [14](#0-13) 
- **Network instability**: Causing consensus to retry sync operations
- **Epoch transitions**: Where multiple sync operations may be initiated
- **Validator catchup**: After downtime, multiple sync requests may be queued

The race window can persist for seconds or longer if the storage synchronizer has significant pending data, making the race window substantial.

## Recommendation

Add validation to check for existing active sync requests before creating a new one:

```rust
pub async fn initialize_sync_duration_request(
    &mut self,
    sync_duration_notification: ConsensusSyncDurationNotification,
) -> Result<(), Error> {
    // Check if there's already an active sync request
    if self.consensus_sync_request.lock().is_some() {
        let error = Err(Error::UnexpectedError(
            "Cannot initialize new sync request while another is active".into()
        ));
        self.respond_to_sync_duration_notification(
            sync_duration_notification,
            error.clone(),
            None,
        )?;
        return error;
    }
    
    // Get the current time
    let start_time = self.time_service.now();
    
    // Save the request
    let consensus_sync_request =
        ConsensusSyncRequest::new_with_duration(start_time, sync_duration_notification);
    self.consensus_sync_request = Arc::new(Mutex::new(Some(consensus_sync_request)));
    
    Ok(())
}
```

Additionally, use the same Arc reference throughout `check_sync_request_progress()` instead of accessing `self.consensus_sync_request` directly in `handle_satisfied_sync_request()`.

## Proof of Concept

A PoC would require:
1. Setting up two ExecutionProxy instances (simulating consensus and consensus observer)
2. Sending a sync_for_duration request from the first instance
3. During the yield_now() loop in check_sync_request_progress, sending another sync_for_duration from the second instance
4. Observing that the first request's callback is never invoked while the second receives a premature response

The race condition is inherent in the async control flow and Arc replacement logic shown in the citations above.

## Notes

This is a genuine concurrency bug in the state sync driver that can cause critical consensus liveness issues. The vulnerability stems from the lack of synchronization between capturing the sync request Arc reference and handling the satisfied request, combined with the async yield point that allows concurrent request processing. The separate ExecutionProxy instances for consensus and consensus observer, while using the same notification channel, enable this race condition to occur in production environments.

### Citations

**File:** state-sync/state-sync-driver/src/driver.rs (L222-238)
```rust
            ::futures::select! {
                notification = self.client_notification_listener.select_next_some() => {
                    self.handle_client_notification(notification).await;
                },
                notification = self.commit_notification_listener.select_next_some() => {
                    self.handle_snapshot_commit_notification(notification).await;
                }
                notification = self.consensus_notification_handler.select_next_some() => {
                    self.handle_consensus_or_observer_notification(notification).await;
                }
                notification = self.error_notification_listener.select_next_some() => {
                    self.handle_error_notification(notification).await;
                }
                _ = progress_check_interval.select_next_some() => {
                    self.drive_progress().await;
                }
            }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L538-538)
```rust
        let consensus_sync_request = self.consensus_notification_handler.get_sync_request();
```

**File:** state-sync/state-sync-driver/src/driver.rs (L543-546)
```rust
                if !consensus_sync_request
                    .sync_request_satisfied(&latest_synced_ledger_info, self.time_service.clone())
                {
                    return Ok(()); // The sync request hasn't been satisfied yet
```

**File:** state-sync/state-sync-driver/src/driver.rs (L556-564)
```rust
        while self.storage_synchronizer.pending_storage_data() {
            sample!(
                SampleRate::Duration(Duration::from_secs(PENDING_DATA_LOG_FREQ_SECS)),
                info!("Waiting for the storage synchronizer to handle pending data!")
            );

            // Yield to avoid starving the storage synchronizer threads.
            yield_now().await;
        }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L569-591)
```rust
        if let Some(sync_request) = consensus_sync_request.lock().as_ref() {
            if sync_request.is_sync_duration_request() {
                // Get the latest synced version and ledger info version
                let latest_synced_version =
                    utils::fetch_pre_committed_version(self.storage.clone())?;
                let latest_synced_ledger_info =
                    utils::fetch_latest_synced_ledger_info(self.storage.clone())?;
                let latest_ledger_info_version = latest_synced_ledger_info.ledger_info().version();

                // Check if the latest synced version matches the latest ledger info version
                if latest_synced_version != latest_ledger_info_version {
                    sample!(
                        SampleRate::Duration(Duration::from_secs(DRIVER_INFO_LOG_FREQ_SECS)),
                        info!(
                            "Waiting for state sync to sync to a ledger info! \
                            Latest synced version: {:?}, latest ledger info version: {:?}",
                            latest_synced_version, latest_ledger_info_version
                        )
                    );

                    return Ok(()); // State sync should continue to run
                }
            }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L597-599)
```rust
        self.consensus_notification_handler
            .handle_satisfied_sync_request(latest_synced_ledger_info)
            .await?;
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L246-259)
```rust
    pub async fn initialize_sync_duration_request(
        &mut self,
        sync_duration_notification: ConsensusSyncDurationNotification,
    ) -> Result<(), Error> {
        // Get the current time
        let start_time = self.time_service.now();

        // Save the request so we can notify consensus once we've hit the duration
        let consensus_sync_request =
            ConsensusSyncRequest::new_with_duration(start_time, sync_duration_notification);
        self.consensus_sync_request = Arc::new(Mutex::new(Some(consensus_sync_request)));

        Ok(())
    }
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L322-365)
```rust
    pub async fn handle_satisfied_sync_request(
        &mut self,
        latest_synced_ledger_info: LedgerInfoWithSignatures,
    ) -> Result<(), Error> {
        // Remove the active sync request
        let mut sync_request_lock = self.consensus_sync_request.lock();
        let consensus_sync_request = sync_request_lock.take();

        // Notify consensus of the satisfied request
        match consensus_sync_request {
            Some(ConsensusSyncRequest::SyncDuration(_, sync_duration_notification)) => {
                self.respond_to_sync_duration_notification(
                    sync_duration_notification,
                    Ok(()),
                    Some(latest_synced_ledger_info),
                )?;
            },
            Some(ConsensusSyncRequest::SyncTarget(sync_target_notification)) => {
                // Get the sync target version and latest synced version
                let sync_target = sync_target_notification.get_target();
                let sync_target_version = sync_target.ledger_info().version();
                let latest_synced_version = latest_synced_ledger_info.ledger_info().version();

                // Check if we've synced beyond the target. If so, notify consensus with an error.
                if latest_synced_version > sync_target_version {
                    let error = Err(Error::SyncedBeyondTarget(
                        latest_synced_version,
                        sync_target_version,
                    ));
                    self.respond_to_sync_target_notification(
                        sync_target_notification,
                        error.clone(),
                    )?;
                    return error;
                }

                // Otherwise, notify consensus that the target has been reached
                self.respond_to_sync_target_notification(sync_target_notification, Ok(()))?;
            },
            None => { /* Nothing needs to be done */ },
        }

        Ok(())
    }
```

**File:** state-sync/inter-component/consensus-notifications/src/lib.rs (L140-179)
```rust
    async fn sync_for_duration(
        &self,
        duration: Duration,
    ) -> Result<LedgerInfoWithSignatures, Error> {
        // Create a consensus sync duration notification
        let (notification, callback_receiver) = ConsensusSyncDurationNotification::new(duration);
        let sync_duration_notification = ConsensusNotification::SyncForDuration(notification);

        // Send the notification to state sync
        if let Err(error) = self
            .notification_sender
            .clone()
            .send(sync_duration_notification)
            .await
        {
            return Err(Error::NotificationError(format!(
                "Failed to notify state sync of sync duration! Error: {:?}",
                error
            )));
        }

        // Process the response
        match callback_receiver.await {
            Ok(response) => match response.get_result() {
                Ok(_) => response.get_latest_synced_ledger_info().ok_or_else(|| {
                    Error::UnexpectedErrorEncountered(
                        "Sync for duration returned an empty latest synced ledger info!".into(),
                    )
                }),
                Err(error) => Err(Error::UnexpectedErrorEncountered(format!(
                    "Sync for duration returned an error: {:?}",
                    error
                ))),
            },
            Err(error) => Err(Error::UnexpectedErrorEncountered(format!(
                "Sync for duration failure: {:?}",
                error
            ))),
        }
    }
```

**File:** state-sync/inter-component/consensus-notifications/src/lib.rs (L360-379)
```rust
/// A notification for state sync to synchronize for the specified duration
#[derive(Debug)]
pub struct ConsensusSyncDurationNotification {
    duration: Duration,
    callback: oneshot::Sender<ConsensusNotificationResponse>,
}

impl ConsensusSyncDurationNotification {
    pub fn new(duration: Duration) -> (Self, oneshot::Receiver<ConsensusNotificationResponse>) {
        let (callback, callback_receiver) = oneshot::channel();
        let notification = ConsensusSyncDurationNotification { duration, callback };

        (notification, callback_receiver)
    }

    /// Returns the duration of the notification
    pub fn get_duration(&self) -> Duration {
        self.duration
    }
}
```

**File:** consensus/src/consensus_provider.rs (L65-72)
```rust
    let execution_proxy = ExecutionProxy::new(
        Arc::new(BlockExecutor::<AptosVMBlockExecutor>::new(aptos_db)),
        txn_notifier,
        state_sync_notifier,
        node_config.transaction_filters.execution_filter.clone(),
        node_config.consensus.enable_pre_commit,
        None,
    );
```

**File:** consensus/src/consensus_provider.rs (L158-165)
```rust
        let execution_proxy = ExecutionProxy::new(
            Arc::new(BlockExecutor::<AptosVMBlockExecutor>::new(aptos_db.clone())),
            txn_notifier,
            state_sync_notifier,
            node_config.transaction_filters.execution_filter.clone(),
            node_config.consensus.enable_pre_commit,
            None,
        );
```

**File:** consensus/src/state_computer.rs (L132-137)
```rust
    async fn sync_for_duration(
        &self,
        duration: Duration,
    ) -> Result<LedgerInfoWithSignatures, StateSyncError> {
        // Grab the logical time lock
        let mut latest_logical_time = self.write_mutex.lock().await;
```

**File:** consensus/src/consensus_observer/observer/state_sync_manager.rs (L116-187)
```rust
    /// Invokes state sync to synchronize in fallback mode
    pub fn sync_for_fallback(&mut self) {
        // Log that we're starting to sync in fallback mode
        info!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "Started syncing in fallback mode! Syncing duration: {:?} ms!",
                self.consensus_observer_config.observer_fallback_duration_ms
            ))
        );

        // Update the state sync fallback counter
        metrics::increment_counter_without_labels(&metrics::OBSERVER_STATE_SYNC_FALLBACK_COUNTER);

        // Clone the required components for the state sync task
        let consensus_observer_config = self.consensus_observer_config;
        let execution_client = self.execution_client.clone();
        let sync_notification_sender = self.state_sync_notification_sender.clone();

        // Spawn a task to sync for the fallback
        let (abort_handle, abort_registration) = AbortHandle::new_pair();
        tokio::spawn(Abortable::new(
            async move {
                // Update the state sync metrics now that we're syncing for the fallback
                metrics::set_gauge_with_label(
                    &metrics::OBSERVER_STATE_SYNC_EXECUTING,
                    metrics::STATE_SYNCING_FOR_FALLBACK,
                    1, // We're syncing for the fallback
                );

                // Get the fallback duration
                let fallback_duration =
                    Duration::from_millis(consensus_observer_config.observer_fallback_duration_ms);

                // Sync for the fallback duration
                let latest_synced_ledger_info = match execution_client
                    .clone()
                    .sync_for_duration(fallback_duration)
                    .await
                {
                    Ok(latest_synced_ledger_info) => latest_synced_ledger_info,
                    Err(error) => {
                        error!(LogSchema::new(LogEntry::ConsensusObserver)
                            .message(&format!("Failed to sync for fallback! Error: {:?}", error)));
                        return;
                    },
                };

                // Notify consensus observer that we've synced for the fallback
                let state_sync_notification =
                    StateSyncNotification::fallback_sync_completed(latest_synced_ledger_info);
                if let Err(error) = sync_notification_sender.send(state_sync_notification) {
                    error!(
                        LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                            "Failed to send state sync notification for fallback! Error: {:?}",
                            error
                        ))
                    );
                }

                // Clear the state sync metrics now that we're done syncing
                metrics::set_gauge_with_label(
                    &metrics::OBSERVER_STATE_SYNC_EXECUTING,
                    metrics::STATE_SYNCING_FOR_FALLBACK,
                    0, // We're no longer syncing for the fallback
                );
            },
            abort_registration,
        ));

        // Save the sync task handle
        self.fallback_sync_handle = Some(DropGuard::new(abort_handle));
    }
```
