# Audit Report

## Title
Partial Epoch Transition During State Sync Error Handling Causes Inconsistent Node State and Fork Risk

## Summary
During epoch transitions in state synchronization, the `finalize_state_snapshot` function in AptosDB performs a database commit atomically but then executes several non-atomic cleanup operations. If any of these non-atomic operations fail after the atomic commit, the database will have transitioned to the new epoch (including updated validator sets and commit progress), but consensus and other in-memory components will remain in the old epoch. This creates an inconsistent state that violates consensus safety and can cause network forks.

## Finding Description

The vulnerability exists in the epoch transition finalization flow during state synchronization. The critical code path is: [1](#0-0) 

The `finalize_state_snapshot` function performs epoch transition updates in two phases:

**Phase 1 (Atomic, line 223):** The function creates database batches containing:
- Transaction outputs and infos
- Epoch ending ledger infos (saved via `restore_utils::save_ledger_infos`)
- Commit progress metadata (`LedgerCommitProgress` and `OverallCommitProgress` set to the new version)

All these are written atomically via `self.ledger_db.write_schemas(ledger_db_batch)`.

**Phase 2 (Non-Atomic, lines 225-237):** After the atomic commit, the function executes:
- Pruner min readable version updates for four different pruners
- Latest ledger info update in metadata DB
- State store reset

If ANY of these non-atomic operations fail, the function returns an error. However, the database has ALREADY committed the new epoch state.

The error propagates to: [2](#0-1) 

When `finalize_state_snapshot` fails, the `finalize_storage_and_send_commit` function:
1. Attempts to reset the chunk executor (line 1150)
2. Fails to send the commit notification to consensus (lines 1163-1171)
3. Returns an error to the state sync driver

The chunk executor reset reads the NEW committed version from the database: [3](#0-2) 

When `ChunkExecutorInner::new()` is called during reset, it reads `next_synced_version` from the database reader, which now reflects the NEW epoch state that was already committed.

The error notification is then handled by: [4](#0-3) 

The driver terminates the active stream but does NOT roll back the database changes. The node is now in an inconsistent state where:
- **Database:** Has epoch N+1 committed with new validator set
- **Consensus layer:** Still operating in epoch N with old validator set
- **In-memory components:** Still in epoch N

The recovery mechanism that could fix this inconsistency: [5](#0-4) 

The `sync_commit_progress` function can detect and fix such inconsistencies, but it is ONLY called during database initialization (line 354 in the StateStore constructor), not during runtime when the error occurs: [6](#0-5) 

## Impact Explanation

This is **CRITICAL** severity per the Aptos bug bounty criteria:

1. **Consensus Safety Violation:** The node has different epoch states in different components, violating the fundamental invariant that all validators must have consistent views of the current epoch and validator set. This directly breaks "Consensus Safety: AptosBFT must prevent chain splits under < 1/3 Byzantine".

2. **Network Partition Risk:** If multiple nodes experience this error at different points during an epoch transition:
   - Node A: Database in epoch N+1, consensus in epoch N (partial failure after commit)
   - Node B: Database in epoch N, consensus in epoch N (failure before commit)
   - Node C: Fully transitioned to epoch N+1

   These nodes cannot reach consensus because they disagree on which epoch they're in and which validator set is active. This creates a non-recoverable network partition requiring manual intervention or a hard fork.

3. **State Consistency Violation:** The atomic state transition guarantee is broken - the database has transitioned to a new epoch, but the consensus layer hasn't been notified, violating "State Consistency: State transitions must be atomic and verifiable".

4. **Requires Node Restart:** The inconsistency can only be resolved by restarting the node, during which `sync_commit_progress` will truncate the database back to a consistent state. However, this causes downtime and if multiple validators are affected, the network loses liveness.

## Likelihood Explanation

This vulnerability has **MEDIUM to HIGH** likelihood:

1. **Realistic Trigger Conditions:**
   - Disk I/O errors during pruner updates
   - Resource exhaustion (memory, disk space)
   - Process termination signals received between atomic commit and cleanup
   - File system errors during metadata updates

2. **Epoch Transitions Are Frequent:** In production Aptos networks, epochs change regularly (e.g., every few hours to days depending on configuration), providing multiple opportunities for this error condition.

3. **No Defensive Checks:** There are no checks in the error handling path to detect that the database has already committed the new epoch state before returning an error.

4. **State Sync Is Common:** Nodes performing state sync (new nodes joining, nodes recovering from downtime, fast sync scenarios) routinely execute this code path during epoch transitions.

## Recommendation

The issue should be fixed by ensuring either:

**Option 1 (Preferred): Make all operations atomic**
Modify `finalize_state_snapshot` to include all cleanup operations within the atomic transaction batch, or use a two-phase commit approach where cleanup operations are retried until they succeed.

**Option 2: Add runtime consistency check**
Before returning an error, check if the database commit has already been persisted. If so, either:
- Complete the remaining non-atomic operations with retries
- Call `sync_commit_progress` immediately to restore consistency
- Trigger an emergency rollback of the database commit

**Option 3: Transactional wrapper**
Wrap the entire finalization in a transaction that can be rolled back if any step fails:

```rust
fn finalize_state_snapshot(
    &self,
    version: Version,
    output_with_proof: TransactionOutputListWithProofV2,
    ledger_infos: &[LedgerInfoWithSignatures],
) -> Result<()> {
    // ... existing validation code ...
    
    // Atomic phase
    self.ledger_db.write_schemas(ledger_db_batch)?;
    
    // Non-atomic cleanup - wrap in retry logic
    let cleanup_result = self.execute_finalization_cleanup(version, ledger_infos);
    
    if cleanup_result.is_err() {
        // Detect if we've already committed
        let current_version = self.ledger_db.metadata_db()
            .get_synced_version()?.unwrap_or(0);
            
        if current_version >= version {
            // Database commit succeeded, must complete cleanup or rollback
            // Option: Call sync_commit_progress to restore consistency
            StateStore::sync_commit_progress(
                self.ledger_db.clone(),
                self.state_kv_db.clone(),
                self.state_merkle_db.clone(),
                false, // Don't crash on large differences during error recovery
            );
        }
        
        return cleanup_result;
    }
    
    Ok(())
}
```

## Proof of Concept

```rust
// Reproduction steps (pseudo-code for integration test):

#[test]
fn test_epoch_transition_partial_failure() {
    // 1. Setup: Create a node syncing during epoch transition
    let mut node = create_test_node_with_state_sync();
    
    // 2. Prepare epoch transition data
    let epoch_change_proofs = create_epoch_ending_ledger_infos(epoch_0, epoch_1);
    let state_snapshot = create_state_snapshot_for_epoch_1();
    
    // 3. Inject failure after database commit but before cleanup
    // This simulates a disk I/O error during pruner update
    inject_failure_after_atomic_commit(&mut node, |db| {
        // After line 223 (atomic commit) succeeds
        // Fail at line 225 (pruner update)
        return Err(anyhow!("Simulated pruner update failure"));
    });
    
    // 4. Attempt to finalize state snapshot
    let result = node.storage_synchronizer.finalize_state_snapshot(
        version_in_epoch_1,
        state_snapshot,
        &epoch_change_proofs,
    );
    
    // 5. Verify the inconsistent state
    assert!(result.is_err(), "Expected error from finalization");
    
    // Database should have epoch 1 committed
    let db_epoch = node.db.reader.get_latest_ledger_info()
        .unwrap().ledger_info().epoch();
    assert_eq!(db_epoch, 1, "Database should be in epoch 1");
    
    // But consensus should still be in epoch 0
    let consensus_epoch = node.consensus.get_current_epoch();
    assert_eq!(consensus_epoch, 0, "Consensus should still be in epoch 0");
    
    // 6. Verify that consensus operations fail due to epoch mismatch
    let block_proposal = create_block_proposal_for_epoch_1();
    let verify_result = node.consensus.verify_proposal(&block_proposal);
    assert!(verify_result.is_err(), "Proposal verification should fail due to epoch mismatch");
    
    // 7. Demonstrate that only restart can fix this
    node.restart();
    
    // After restart, sync_commit_progress restores consistency
    let db_epoch_after = node.db.reader.get_latest_ledger_info()
        .unwrap().ledger_info().epoch();
    let consensus_epoch_after = node.consensus.get_current_epoch();
    assert_eq!(db_epoch_after, consensus_epoch_after, 
        "Epochs should be consistent after restart");
}
```

### Citations

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L125-241)
```rust
    fn finalize_state_snapshot(
        &self,
        version: Version,
        output_with_proof: TransactionOutputListWithProofV2,
        ledger_infos: &[LedgerInfoWithSignatures],
    ) -> Result<()> {
        let (output_with_proof, persisted_aux_info) = output_with_proof.into_parts();
        gauged_api("finalize_state_snapshot", || {
            // Ensure the output with proof only contains a single transaction output and info
            let num_transaction_outputs = output_with_proof.get_num_outputs();
            let num_transaction_infos = output_with_proof.proof.transaction_infos.len();
            ensure!(
                num_transaction_outputs == 1,
                "Number of transaction outputs should == 1, but got: {}",
                num_transaction_outputs
            );
            ensure!(
                num_transaction_infos == 1,
                "Number of transaction infos should == 1, but got: {}",
                num_transaction_infos
            );

            // TODO(joshlind): include confirm_or_save_frozen_subtrees in the change set
            // bundle below.

            // Update the merkle accumulator using the given proof
            let frozen_subtrees = output_with_proof
                .proof
                .ledger_info_to_transaction_infos_proof
                .left_siblings();
            restore_utils::confirm_or_save_frozen_subtrees(
                self.ledger_db.transaction_accumulator_db_raw(),
                version,
                frozen_subtrees,
                None,
            )?;

            // Create a single change set for all further write operations
            let mut ledger_db_batch = LedgerDbSchemaBatches::new();
            let mut sharded_kv_batch = self.state_kv_db.new_sharded_native_batches();
            let mut state_kv_metadata_batch = SchemaBatch::new();
            // Save the target transactions, outputs, infos and events
            let (transactions, outputs): (Vec<Transaction>, Vec<TransactionOutput>) =
                output_with_proof
                    .transactions_and_outputs
                    .into_iter()
                    .unzip();
            let events = outputs
                .clone()
                .into_iter()
                .map(|output| output.events().to_vec())
                .collect::<Vec<_>>();
            let wsets: Vec<WriteSet> = outputs
                .into_iter()
                .map(|output| output.write_set().clone())
                .collect();
            let transaction_infos = output_with_proof.proof.transaction_infos;
            // We should not save the key value since the value is already recovered for this version
            restore_utils::save_transactions(
                self.state_store.clone(),
                self.ledger_db.clone(),
                version,
                &transactions,
                &persisted_aux_info,
                &transaction_infos,
                &events,
                wsets,
                Some((
                    &mut ledger_db_batch,
                    &mut sharded_kv_batch,
                    &mut state_kv_metadata_batch,
                )),
                false,
            )?;

            // Save the epoch ending ledger infos
            restore_utils::save_ledger_infos(
                self.ledger_db.metadata_db(),
                ledger_infos,
                Some(&mut ledger_db_batch.ledger_metadata_db_batches),
            )?;

            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::LedgerCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;
            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::OverallCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;

            // Apply the change set writes to the database (atomically) and update in-memory state
            //
            // state kv and SMT should use shared way of committing.
            self.ledger_db.write_schemas(ledger_db_batch)?;

            self.ledger_pruner.save_min_readable_version(version)?;
            self.state_store
                .state_merkle_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .epoch_snapshot_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .state_kv_pruner
                .save_min_readable_version(version)?;

            restore_utils::update_latest_ledger_info(self.ledger_db.metadata_db(), ledger_infos)?;
            self.state_store.reset();

            Ok(())
        })
    }
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L1104-1182)
```rust
async fn finalize_storage_and_send_commit<
    'receiver_lifetime, // Required because of https://github.com/rust-lang/rust/issues/63033
    ChunkExecutor: ChunkExecutorTrait + 'static,
    MetadataStorage: MetadataStorageInterface + Clone + Send + Sync + 'static,
>(
    chunk_executor: Arc<ChunkExecutor>,
    commit_notification_sender: &mut mpsc::UnboundedSender<CommitNotification>,
    metadata_storage: MetadataStorage,
    state_snapshot_receiver: Box<
        dyn StateSnapshotReceiver<StateKey, StateValue> + 'receiver_lifetime,
    >,
    storage: DbReaderWriter,
    epoch_change_proofs: &[LedgerInfoWithSignatures],
    target_output_with_proof: TransactionOutputListWithProofV2,
    version: Version,
    target_ledger_info: &LedgerInfoWithSignatures,
    last_committed_state_index: u64,
) -> Result<(), String> {
    // Finalize the state snapshot
    state_snapshot_receiver.finish_box().map_err(|error| {
        format!(
            "Failed to finish the state value synchronization! Error: {:?}",
            error
        )
    })?;
    storage
        .writer
        .finalize_state_snapshot(
            version,
            target_output_with_proof.clone(),
            epoch_change_proofs,
        )
        .map_err(|error| format!("Failed to finalize the state snapshot! Error: {:?}", error))?;

    info!("All states have synced, version: {}", version);

    // Update the metadata storage
    metadata_storage.update_last_persisted_state_value_index(
            target_ledger_info,
            last_committed_state_index,
            true,
        ).map_err(|error| {
        format!("All states have synced, but failed to update the metadata storage at version {:?}! Error: {:?}", version, error)
    })?;

    // Reset the chunk executor
    chunk_executor.reset().map_err(|error| {
        format!(
            "Failed to reset the chunk executor after state snapshot synchronization! Error: {:?}",
            error
        )
    })?;

    // Create and send the commit notification
    let commit_notification = create_commit_notification(
        target_output_with_proof,
        last_committed_state_index,
        version,
    );
    commit_notification_sender
        .send(commit_notification)
        .await
        .map_err(|error| {
            format!(
                "Failed to send the final state commit notification! Error: {:?}",
                error
            )
        })?;

    // Update the counters
    utils::initialize_sync_gauges(storage.reader).map_err(|error| {
        format!(
            "Failed to initialize the state sync version gauges! Error: {:?}",
            error
        )
    })?;

    Ok(())
}
```

**File:** execution/executor/src/chunk_executor/mod.rs (L214-250)
```rust
    fn reset(&self) -> Result<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["chunk", "reset"]);

        *self.inner.write() = Some(ChunkExecutorInner::new(self.db.clone())?);
        Ok(())
    }

    fn finish(&self) {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["chunk", "finish"]);

        *self.inner.write() = None;
    }
}

struct ChunkExecutorInner<V> {
    db: DbReaderWriter,
    commit_queue: Mutex<ChunkCommitQueue>,
    has_pending_pre_commit: AtomicBool,
    _phantom: PhantomData<V>,
}

impl<V: VMBlockExecutor> ChunkExecutorInner<V> {
    pub fn new(db: DbReaderWriter) -> Result<Self> {
        let commit_queue = ChunkCommitQueue::new_from_db(&db.reader)?;

        let next_pre_committed_version = commit_queue.expecting_version();
        let next_synced_version = db.reader.get_synced_version()?.map_or(0, |v| v + 1);
        assert!(next_synced_version <= next_pre_committed_version);
        let has_pending_pre_commit = next_synced_version < next_pre_committed_version;

        Ok(Self {
            db,
            commit_queue: Mutex::new(commit_queue),
            has_pending_pre_commit: AtomicBool::new(has_pending_pre_commit),
            _phantom: PhantomData,
        })
    }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L494-533)
```rust
    /// Handles an error notification sent by the storage synchronizer
    async fn handle_error_notification(&mut self, error_notification: ErrorNotification) {
        warn!(LogSchema::new(LogEntry::SynchronizerNotification)
            .error_notification(error_notification.clone())
            .message("Received an error notification from the storage synchronizer!"));

        // Terminate the currently active streams
        let notification_id = error_notification.notification_id;
        let notification_feedback = NotificationFeedback::InvalidPayloadData;
        if self.bootstrapper.is_bootstrapped() {
            if let Err(error) = self
                .continuous_syncer
                .handle_storage_synchronizer_error(NotificationAndFeedback::new(
                    notification_id,
                    notification_feedback,
                ))
                .await
            {
                error!(LogSchema::new(LogEntry::SynchronizerNotification)
                    .message(&format!(
                        "Failed to terminate the active stream for the continuous syncer! Error: {:?}",
                        error
                    )));
            }
        } else if let Err(error) = self
            .bootstrapper
            .handle_storage_synchronizer_error(NotificationAndFeedback::new(
                notification_id,
                notification_feedback,
            ))
            .await
        {
            error!(
                LogSchema::new(LogEntry::SynchronizerNotification).message(&format!(
                    "Failed to terminate the active stream for the bootstrapper! Error: {:?}",
                    error
                ))
            );
        };
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L352-360)
```rust
    ) -> Self {
        if !hack_for_tests && !empty_buffered_state_for_restore {
            Self::sync_commit_progress(
                Arc::clone(&ledger_db),
                Arc::clone(&state_kv_db),
                Arc::clone(&state_merkle_db),
                /*crash_if_difference_is_too_large=*/ true,
            );
        }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L410-502)
```rust
    pub fn sync_commit_progress(
        ledger_db: Arc<LedgerDb>,
        state_kv_db: Arc<StateKvDb>,
        state_merkle_db: Arc<StateMerkleDb>,
        crash_if_difference_is_too_large: bool,
    ) {
        let ledger_metadata_db = ledger_db.metadata_db();
        if let Some(overall_commit_progress) = ledger_metadata_db
            .get_synced_version()
            .expect("DB read failed.")
        {
            info!(
                overall_commit_progress = overall_commit_progress,
                "Start syncing databases..."
            );
            let ledger_commit_progress = ledger_metadata_db
                .get_ledger_commit_progress()
                .expect("Failed to read ledger commit progress.");
            assert_ge!(ledger_commit_progress, overall_commit_progress);

            let state_kv_commit_progress = state_kv_db
                .metadata_db()
                .get::<DbMetadataSchema>(&DbMetadataKey::StateKvCommitProgress)
                .expect("Failed to read state K/V commit progress.")
                .expect("State K/V commit progress cannot be None.")
                .expect_version();
            assert_ge!(state_kv_commit_progress, overall_commit_progress);

            // LedgerCommitProgress was not guaranteed to commit after all ledger changes finish,
            // have to attempt truncating every column family.
            info!(
                ledger_commit_progress = ledger_commit_progress,
                "Attempt ledger truncation...",
            );
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");

            // State K/V commit progress isn't (can't be) written atomically with the data,
            // because there are shards, so we have to attempt truncation anyway.
            info!(
                state_kv_commit_progress = state_kv_commit_progress,
                "Start state KV truncation..."
            );
            let difference = state_kv_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_state_kv_db(
                &state_kv_db,
                state_kv_commit_progress,
                overall_commit_progress,
                std::cmp::max(difference as usize, 1), /* batch_size */
            )
            .expect("Failed to truncate state K/V db.");

            let state_merkle_max_version = get_max_version_in_state_merkle_db(&state_merkle_db)
                .expect("Failed to get state merkle max version.")
                .expect("State merkle max version cannot be None.");
            if state_merkle_max_version > overall_commit_progress {
                let difference = state_merkle_max_version - overall_commit_progress;
                if crash_if_difference_is_too_large {
                    assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
                }
            }
            let state_merkle_target_version = find_tree_root_at_or_before(
                ledger_metadata_db,
                &state_merkle_db,
                overall_commit_progress,
            )
            .expect("DB read failed.")
            .unwrap_or_else(|| {
                panic!(
                    "Could not find a valid root before or at version {}, maybe it was pruned?",
                    overall_commit_progress
                )
            });
            if state_merkle_target_version < state_merkle_max_version {
                info!(
                    state_merkle_max_version = state_merkle_max_version,
                    target_version = state_merkle_target_version,
                    "Start state merkle truncation..."
                );
                truncate_state_merkle_db(&state_merkle_db, state_merkle_target_version)
                    .expect("Failed to truncate state merkle db.");
            }
        } else {
            info!("No overall commit progress was found!");
        }
    }
```
