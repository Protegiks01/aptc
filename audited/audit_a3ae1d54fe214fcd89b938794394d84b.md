# Audit Report

## Title
Memory Leak in ConsensusObserverPayloadManager: Old Epoch Payloads Never Cleaned Up During State Sync

## Summary
The `ConsensusObserverPayloadManager` suffers from a memory leak where block payloads from old epochs that arrive during state synchronization are never cleaned up, leading to unbounded memory growth and performance degradation over multiple epoch transitions.

## Finding Description

The vulnerability exists in how the consensus observer handles block payloads during epoch transitions via the commit sync path. The `ConsensusObserverPayloadManager` stores payloads in a shared `BTreeMap<(u64, Round), BlockPayloadStatus>` indexed by `(epoch, round)`. [1](#0-0) 

The critical issue is that the `notify_commit` method is empty and performs no cleanup: [2](#0-1) 

During epoch transitions via the commit sync path, the cleanup mechanism has a race condition:

1. When a commit decision for epoch N+1 arrives, `update_blocks_for_state_sync_commit` is called, which cleans up blocks before the N+1 commit point.

2. However, state sync then begins and can take seconds to minutes. During this time, the node continues to receive messages from peers.

3. Block payload messages for late epoch N blocks arrive and are inserted into the map: [3](#0-2) 

4. When state sync completes and the epoch transitions to N+1, only `verify_payload_signatures` is called: [4](#0-3) 

5. The `verify_payload_signatures` method **only processes payloads for the current epoch** and ignores old epoch payloads: [5](#0-4) [6](#0-5) 

The method iterates through all payloads but only processes those where `epoch == current_epoch`. Payloads where `epoch < current_epoch` are **never removed**, causing them to remain in the map forever.

This is in contrast to the fallback sync path, which properly calls `clear_pending_block_state()` to clean up all old data: [7](#0-6) 

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos bug bounty program:

- **Memory Exhaustion**: Each block payload contains transaction data (potentially megabytes per payload). Over multiple epoch transitions, hundreds or thousands of orphaned payloads accumulate, consuming gigabytes of memory.

- **Performance Degradation**: BTreeMap operations have O(log n) complexity. As the map grows from thousands to millions of entries, all payload lookups and insertions slow down proportionally.

- **Node Instability**: Eventually, nodes may experience out-of-memory crashes or become unresponsive, affecting network health and requiring manual intervention.

- **No Natural Recovery**: Once payloads accumulate, they remain forever until the node restarts, as there is no pruning mechanism.

This matches the Medium severity category: "State inconsistencies requiring intervention" and "Validator node slowdowns."

## Likelihood Explanation

This vulnerability has **HIGH likelihood** of occurring:

- **Frequent Trigger**: Epochs change regularly in Aptos (typically every few hours based on governance configuration).

- **Inevitable Accumulation**: State sync is a normal operation that happens during epoch transitions. Delayed or buffered messages from peers arriving during sync are expected network behavior.

- **No Attack Required**: This happens naturally without any attacker action - it's a logic bug in the cleanup flow.

- **Cumulative Effect**: Each epoch transition via commit sync potentially leaves orphaned payloads, and the effect compounds over weeks/months of operation.

## Recommendation

Add proper cleanup of old epoch payloads in the `verify_payload_signatures` method or during epoch transitions. The fix should remove payloads where `epoch < current_epoch`:

```rust
// In payload_store.rs, in verify_payload_signatures method:
pub fn verify_payload_signatures(&mut self, epoch_state: &EpochState) -> Vec<Round> {
    let current_epoch = epoch_state.epoch;
    let payload_epochs_and_rounds: Vec<(u64, Round)> =
        self.block_payloads.lock().keys().cloned().collect();

    let mut verified_payloads_to_update = vec![];
    for (epoch, round) in payload_epochs_and_rounds {
        // NEW: Remove old epoch payloads
        if epoch < current_epoch {
            self.block_payloads.lock().remove(&(epoch, round));
            continue;
        }
        
        // Break if future epoch
        if epoch > current_epoch {
            break;
        }

        // Process current epoch payloads (existing logic)
        if epoch == current_epoch {
            // ... existing verification logic ...
        }
    }
    
    // ... rest of method unchanged ...
}
```

Alternatively, explicitly clean up old epoch payloads in the epoch transition handler:

```rust
// In consensus_observer.rs, in process_commit_sync_notification:
if synced_epoch > current_epoch_state.epoch {
    self.execution_client.end_epoch().await;
    self.wait_for_epoch_start().await;
    
    // NEW: Clean up old epoch payloads before verifying new ones
    let new_epoch = self.get_epoch_state().epoch;
    self.observer_block_data.lock()
        .get_block_payloads()
        .lock()
        .retain(|(epoch, _), _| *epoch >= new_epoch);
    
    // Verify the block payloads for the new epoch
    let new_epoch_state = self.get_epoch_state();
    let verified_payload_rounds = self.observer_block_data
        .lock()
        .verify_payload_signatures(&new_epoch_state);
    // ... rest unchanged ...
}
```

## Proof of Concept

To reproduce this vulnerability:

1. Set up a consensus observer node in a test environment
2. Monitor the size of `block_payloads` map in `BlockPayloadStore`
3. Trigger multiple epoch transitions using commit sync (not fallback sync)
4. During each state sync, send delayed payload messages for the previous epoch
5. Observe that old epoch payloads accumulate in the map without being cleaned up
6. Memory usage and map size grow unbounded over time

**Rust reproduction steps:**

```rust
#[tokio::test]
async fn test_old_epoch_payload_leak() {
    // Create observer with initial epoch 0
    let mut observer = create_test_observer();
    
    // Insert payloads for epoch 0
    for round in 0..100 {
        let payload = create_test_payload(0, round);
        observer.observer_block_data.lock()
            .insert_block_payload(payload, true);
    }
    
    // Trigger commit sync to epoch 1
    let commit_decision = create_commit_decision(1, 0);
    observer.process_commit_decision(commit_decision).await;
    
    // DURING state sync, insert late epoch 0 payloads
    for round in 100..200 {
        let payload = create_test_payload(0, round);
        observer.observer_block_data.lock()
            .insert_block_payload(payload, false);
    }
    
    // Complete state sync and transition to epoch 1
    observer.process_commit_sync_notification(
        create_ledger_info(1, 0)
    ).await;
    
    // VERIFY: Old epoch 0 payloads (rounds 100-200) still exist
    let payloads = observer.observer_block_data.lock().get_block_payloads();
    let old_epoch_count = payloads.lock()
        .keys()
        .filter(|(epoch, _)| *epoch == 0)
        .count();
    
    assert!(old_epoch_count > 0, "Memory leak: {} old epoch payloads remain", old_epoch_count);
    
    // Repeat for multiple epochs - memory grows unbounded
    for new_epoch in 2..10 {
        // Similar process, inserting payloads during sync
        // Each iteration leaves more orphaned payloads
    }
}
```

## Notes

This vulnerability specifically affects the commit sync path for epoch transitions. The fallback sync path correctly clears all pending block state. The inconsistency between these two code paths is the root cause of the vulnerability.

The empty `notify_commit` implementation in `ConsensusObserverPayloadManager` also contributes to the issue, as it prevents cleanup through the normal payload manager interface used by other components.

### Citations

**File:** consensus/src/payload_manager/co_payload_manager.rs (L78-81)
```rust
pub struct ConsensusObserverPayloadManager {
    txns_pool: Arc<Mutex<BTreeMap<(u64, Round), BlockPayloadStatus>>>,
    consensus_publisher: Option<Arc<ConsensusPublisher>>,
}
```

**File:** consensus/src/payload_manager/co_payload_manager.rs (L97-97)
```rust
    fn notify_commit(&self, _block_timestamp: u64, _payloads: Vec<Payload>) {}
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L427-430)
```rust
        // Update the payload store with the payload
        self.observer_block_data
            .lock()
            .insert_block_payload(block_payload, verified_payload);
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L952-961)
```rust
        // If the epoch has changed, end the current epoch and start the latest one
        let current_epoch_state = self.get_epoch_state();
        if epoch > current_epoch_state.epoch {
            // Wait for the latest epoch to start
            self.execution_client.end_epoch().await;
            self.wait_for_epoch_start().await;
        };

        // Reset the pending block state
        self.clear_pending_block_state().await;
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L1033-1038)
```rust
            // Verify the block payloads for the new epoch
            let new_epoch_state = self.get_epoch_state();
            let verified_payload_rounds = self
                .observer_block_data
                .lock()
                .verify_payload_signatures(&new_epoch_state);
```

**File:** consensus/src/consensus_observer/observer/payload_store.rs (L227-231)
```rust
        for (epoch, round) in payload_epochs_and_rounds {
            // Check if we can break early (BtreeMaps are sorted by key)
            if epoch > current_epoch {
                break;
            }
```

**File:** consensus/src/consensus_observer/observer/payload_store.rs (L234-257)
```rust
            if epoch == current_epoch {
                if let Entry::Occupied(mut entry) = self.block_payloads.lock().entry((epoch, round))
                {
                    if let BlockPayloadStatus::AvailableAndUnverified(block_payload) =
                        entry.get_mut()
                    {
                        if let Err(error) = block_payload.verify_payload_signatures(epoch_state) {
                            // Log the verification failure
                            error!(
                                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                                    "Failed to verify the block payload signatures for epoch: {:?} and round: {:?}. Error: {:?}",
                                    epoch, round, error
                                ))
                            );

                            // Remove the block payload from the store
                            entry.remove();
                        } else {
                            // Save the block payload for reinsertion
                            verified_payloads_to_update.push(block_payload.clone());
                        }
                    }
                }
            }
```
