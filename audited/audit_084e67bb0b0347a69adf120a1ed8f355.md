# Audit Report

## Title
State Merkle Pruner Catch-Up Causes Unbounded Memory Allocation Leading to OOM Crashes

## Summary
During node initialization, the `StateMerkleShardPruner::new()` function performs a catch-up operation by passing `usize::MAX` as the `max_nodes_to_prune` parameter. This causes `get_stale_node_indices()` to load all stale nodes between the current progress and target version into memory at once, potentially consuming gigabytes of RAM and causing Out-of-Memory (OOM) crashes when millions of stale nodes have accumulated. [1](#0-0) 

## Finding Description

The vulnerability exists in the state merkle pruner's initialization logic. When a `StateMerkleShardPruner` is created, it performs a "catch-up" operation to process any accumulated stale nodes: [2](#0-1) 

The `usize::MAX` parameter is passed as `max_nodes_to_prune`, which translates to 18,446,744,073,709,551,615 on 64-bit systems. This value flows into `get_stale_node_indices()`: [3](#0-2) 

The critical issue is at line 205: `while indices.len() < limit` - when `limit = usize::MAX`, this loop effectively becomes unbounded and will continue pushing `StaleNodeIndex` entries into the `indices` vector until either:
1. All stale nodes in the version range are exhausted
2. The process runs out of memory

Each `StaleNodeIndex` contains:
- `stale_since_version: Version` (u64 = 8 bytes)
- `node_key: NodeKey` which contains:
  - `version: Version` (8 bytes)
  - `nibble_path: NibblePath` with `num_nibbles: usize` (8 bytes) and `bytes: Vec<u8>` (24 bytes + up to 32 bytes data) [4](#0-3) [5](#0-4) 

This results in approximately 48-80 bytes per `StaleNodeIndex`. With millions of accumulated stale nodes:
- 10 million nodes = ~800 MB
- 100 million nodes = ~8 GB

Stale nodes accumulate when:
1. State merkle pruning is disabled (`enable: false` in config)
2. The node falls behind or is offline for extended periods
3. The prune window is very large (default is 1,000,000 versions) [6](#0-5) 

In contrast, normal pruning operations use `batch_size: 1_000` to process nodes incrementally: [7](#0-6) 

The vulnerability occurs during node startup when `StateMerklePruner::new()` creates shard pruners (up to 16 shards): [8](#0-7) 

**This violates the documented invariant**: "Resource Limits: All operations must respect gas, storage, and computational limits" - the unbounded memory allocation violates memory resource limits.

## Impact Explanation

This vulnerability qualifies as **HIGH severity** per the Aptos bug bounty criteria because it causes:

1. **Validator node crashes**: When a node with accumulated stale nodes restarts, it will attempt to allocate gigabytes of memory during initialization, potentially causing OOM kills by the operating system.

2. **Network availability degradation**: If multiple validators experience this issue simultaneously (e.g., after a coordinated restart or configuration change), it reduces the number of active validators, impacting consensus performance and potentially threatening liveness if enough validators are affected.

3. **Denial of Service**: Operators cannot start their validator nodes successfully, leading to extended downtime until manual intervention (database recovery, pruning, or reconfiguration).

The impact maps to the HIGH severity category: "Validator node slowdowns" and can escalate to availability issues affecting network participation.

## Likelihood Explanation

The likelihood is **MEDIUM to HIGH** because:

**Realistic Scenarios:**
1. **Configuration Changes**: Operators who initially run with pruning disabled and later enable it will trigger this during the next restart
2. **Missed Pruning**: If pruning falls behind due to performance issues or bugs, millions of nodes can accumulate
3. **Node Recovery**: Nodes recovering from extended downtime must catch up on pruning, triggering unbounded allocation
4. **Default Window**: The 1,000,000 version prune window means substantial accumulation can occur even during normal operation if pruning lags

**No Attacker Required**: This is a self-inflicted condition - no malicious actor is needed. The vulnerability triggers automatically during normal operational scenarios.

**Frequency**: Given that validator nodes restart periodically for upgrades, configuration changes, or recovery, this condition will be encountered by operators running nodes with accumulated stale states.

## Recommendation

Replace the unbounded `usize::MAX` parameter in catch-up with the configured `batch_size`. Modify the catch-up logic to process nodes incrementally in batches:

```rust
pub(in crate::pruner) fn new(
    shard_id: usize,
    db_shard: Arc<DB>,
    metadata_progress: Version,
    batch_size: usize,  // Add batch_size parameter
) -> Result<Self> {
    let progress = get_or_initialize_subpruner_progress(
        &db_shard,
        &S::progress_metadata_key(Some(shard_id)),
        metadata_progress,
    )?;
    let myself = Self {
        shard_id,
        db_shard,
        _phantom: PhantomData,
    };

    info!(
        progress = progress,
        metadata_progress = metadata_progress,
        "Catching up {} shard {shard_id}.",
        S::name(),
    );
    // Use batch_size instead of usize::MAX
    myself.prune(progress, metadata_progress, batch_size)?;

    Ok(myself)
}
```

And update the caller to pass the batch_size:

```rust
for shard_id in 0..num_shards {
    shard_pruners.push(StateMerkleShardPruner::new(
        shard_id,
        state_merkle_db.db_shard_arc(shard_id),
        metadata_progress,
        state_merkle_pruner_config.batch_size,  // Pass configured batch_size
    )?);
}
```

The existing `prune()` function already handles batching correctly with its loop structure, so using the configured `batch_size` (default 1,000) will process catch-up incrementally without unbounded memory allocation.

## Proof of Concept

```rust
#[test]
fn test_catchup_oom_vulnerability() {
    use tempfile::TempDir;
    use aptos_jellyfish_merkle::StaleNodeIndex;
    use crate::schema::stale_node_index::StaleNodeIndexSchema;
    
    // Setup test database
    let tmpdir = TempDir::new().unwrap();
    let db = DB::open(
        tmpdir.path(),
        "test_catchup",
        /* ... rocksdb config ... */
    ).unwrap();
    
    // Simulate millions of accumulated stale nodes
    let mut batch = SchemaBatch::new();
    let num_stale_nodes = 10_000_000; // 10 million nodes
    
    for i in 0..num_stale_nodes {
        let index = StaleNodeIndex {
            stale_since_version: i,
            node_key: NodeKey::new_empty_path(i),
        };
        batch.put::<StaleNodeIndexSchema>(&index, &()).unwrap();
    }
    db.write_schemas(batch).unwrap();
    
    // Attempt to create shard pruner with catch-up
    // This will try to allocate ~800 MB in one vector
    // With usize::MAX limit, all 10M nodes get loaded into memory
    let result = StateMerkleShardPruner::<StaleNodeIndexSchema>::new(
        0,
        Arc::new(db),
        num_stale_nodes - 1,
    );
    
    // Monitor memory allocation during this operation
    // Expected: Single allocation of ~800 MB for Vec<StaleNodeIndex>
    // With batch_size=1000: Only ~80 KB per batch iteration
}
```

**Notes:**
This vulnerability demonstrates a classic unbounded resource allocation bug that violates memory safety principles in production systems. The fix is straightforward - use the existing batch_size configuration parameter consistently across both normal pruning and catch-up operations. The current discrepancy between normal operation (batch_size=1,000) and catch-up (usize::MAX) is inconsistent and dangerous.

### Citations

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L31-56)
```rust
    pub(in crate::pruner) fn new(
        shard_id: usize,
        db_shard: Arc<DB>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &S::progress_metadata_key(Some(shard_id)),
            metadata_progress,
        )?;
        let myself = Self {
            shard_id,
            db_shard,
            _phantom: PhantomData,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up {} shard {shard_id}.",
            S::name(),
        );
        myself.prune(progress, metadata_progress, usize::MAX)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L136-149)
```rust
        let shard_pruners = if state_merkle_db.sharding_enabled() {
            let num_shards = state_merkle_db.num_shards();
            let mut shard_pruners = Vec::with_capacity(num_shards);
            for shard_id in 0..num_shards {
                shard_pruners.push(StateMerkleShardPruner::new(
                    shard_id,
                    state_merkle_db.db_shard_arc(shard_id),
                    metadata_progress,
                )?);
            }
            shard_pruners
        } else {
            Vec::new()
        };
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L168-189)
```rust
    fn prune_shards(
        &self,
        current_progress: Version,
        target_version: Version,
        batch_size: usize,
    ) -> Result<()> {
        THREAD_MANAGER
            .get_background_pool()
            .install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(current_progress, target_version, batch_size)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state merkle shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })
            .map_err(Into::into)
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L191-217)
```rust
    pub(in crate::pruner::state_merkle_pruner) fn get_stale_node_indices(
        state_merkle_db_shard: &DB,
        start_version: Version,
        target_version: Version,
        limit: usize,
    ) -> Result<(Vec<StaleNodeIndex>, Option<Version>)> {
        let mut indices = Vec::new();
        let mut iter = state_merkle_db_shard.iter::<S>()?;
        iter.seek(&StaleNodeIndex {
            stale_since_version: start_version,
            node_key: NodeKey::new_empty_path(0),
        })?;

        let mut next_version = None;
        while indices.len() < limit {
            if let Some((index, _)) = iter.next().transpose()? {
                next_version = Some(index.stale_since_version);
                if index.stale_since_version <= target_version {
                    indices.push(index);
                    continue;
                }
            }
            break;
        }

        Ok((indices, next_version))
    }
```

**File:** storage/jellyfish-merkle/src/lib.rs (L195-201)
```rust
pub struct StaleNodeIndex {
    /// The version since when the node is overwritten and becomes stale.
    pub stale_since_version: Version,
    /// The [`NodeKey`](node_type/struct.NodeKey.html) identifying the node associated with this
    /// record.
    pub node_key: NodeKey,
}
```

**File:** storage/jellyfish-merkle/src/node_type/mod.rs (L47-54)
```rust
#[derive(Clone, Debug, Hash, Eq, PartialEq, Ord, PartialOrd)]
#[cfg_attr(any(test, feature = "fuzzing"), derive(Arbitrary))]
pub struct NodeKey {
    // The version at which the node is created.
    version: Version,
    // The nibble path this node represents in the tree.
    nibble_path: NibblePath,
}
```

**File:** config/src/config/storage_config.rs (L398-412)
```rust
impl Default for StateMerklePrunerConfig {
    fn default() -> Self {
        StateMerklePrunerConfig {
            enable: true,
            // This allows a block / chunk being executed to have access to a non-latest state tree.
            // It needs to be greater than the number of versions the state committing thread is
            // able to commit during the execution of the block / chunk. If the bad case indeed
            // happens due to this being too small, a node restart should recover it.
            // Still, defaulting to 1M to be super safe.
            prune_window: 1_000_000,
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
        }
    }
```
