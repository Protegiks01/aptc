# Audit Report

## Title
Internal Indexer Service Blocked After Fast Sync Due to Missing Version Update Notification

## Summary
During fast sync mode, the `finalize_state_snapshot()` function commits the state snapshot to the database without sending a version update notification to the internal indexer service. This causes the internal indexer to wait indefinitely for a notification that was never sent, blocking indexer availability until a new transaction is committed through the normal path.

## Finding Description

The vulnerability occurs in the fast sync state snapshot finalization flow:

**Root Cause**: The `finalize_state_snapshot()` method in [1](#0-0)  writes state snapshot data directly to the database and updates version metadata [2](#0-1)  but does NOT call `post_commit()` which is responsible for sending version update notifications.

In contrast, normal transaction commits call `post_commit()` which sends notifications: [3](#0-2) 

**The Flow**:

1. During state snapshot restoration, the internal indexer DB metadata is updated to `snapshot_version - 1` via `kv_finish()`: [4](#0-3) 

2. When `finalize_state_snapshot()` commits the final snapshot transaction at `snapshot_version`, no notification is sent to `update_subscriber`.

3. The internal indexer service starts and waits for fast sync to complete: [5](#0-4) 

4. It then enters the main processing loop where `start_version = snapshot_version` (next version to process) and `target_version = snapshot_version` (from main DB): [6](#0-5) 

5. Since `target_version <= start_version`, the indexer waits for an update notification: [7](#0-6) 

6. **The internal indexer blocks indefinitely** until a new transaction is committed through the normal path, which triggers the first notification.

**Note on secondary_db**: While the security question mentions that `secondary_db` doesn't receive `update_sender` [8](#0-7) , this is not the primary issue. Writes during fast sync go to `db_for_fast_sync` (db_main) which DOES have the update_sender [9](#0-8) . The real problem is that `finalize_state_snapshot()` doesn't trigger notifications even though it writes to a DB with `update_subscriber` configured.

## Impact Explanation

This qualifies as **High Severity** per Aptos bug bounty criteria:

- **API service degradation**: The internal indexer service is used by the API to serve account-based queries [10](#0-9) . When blocked, the API cannot serve up-to-date indexed data.

- **Service availability impact**: In networks with low transaction activity, the internal indexer could remain blocked for extended periods (minutes to hours), causing prolonged service degradation.

- **Upstream dependency breakage**: Any service depending on version update notifications experiences delays, potentially affecting node observability and monitoring systems.

While not a complete service failure, this constitutes a "significant protocol violation" where the protocol design assumes version updates are consistently sent but fast sync violates this assumption.

## Likelihood Explanation

**Probability**: High
- Every node performing fast sync will encounter this issue
- Fast sync is a standard bootstrapping mode for new full nodes: [11](#0-10) 

**Complexity**: Automatic
- No attacker action required - this is a protocol implementation bug
- Occurs naturally during standard node operations

**Duration of Impact**: Variable
- In high-activity networks: Seconds (next transaction triggers notification)
- In low-activity networks: Minutes to hours until next transaction
- Critical during network initialization or quiet periods

## Recommendation

**Fix**: Modify `finalize_state_snapshot()` to send version update notifications after committing the snapshot data.

Add notification sending at the end of `finalize_state_snapshot()`:

```rust
// In storage/aptosdb/src/db/aptosdb_writer.rs, after line 237:
restore_utils::update_latest_ledger_info(self.ledger_db.metadata_db(), ledger_infos)?;
self.state_store.reset();

// ADD THIS:
if let Some(update_sender) = &self.update_subscriber {
    update_sender
        .send((Instant::now(), version))
        .map_err(|err| {
            AptosDbError::Other(format!("Failed to send update to subscriber: {}", err))
        })?;
}

Ok(())
```

**Alternative**: The internal indexer could implement a timeout-based polling mechanism as a fallback when waiting for updates, periodically checking the main DB version even without notifications.

## Proof of Concept

**Setup**:
1. Configure a node with fast sync enabled and internal indexer enabled
2. Start the node to bootstrap from an existing network
3. Monitor the internal indexer service logs

**Expected Behavior** (with bug):
1. Node performs fast sync, downloads state snapshot
2. `finalize_state_snapshot()` commits snapshot without notification
3. Internal indexer service starts, enters wait loop
4. Internal indexer remains blocked (logs show waiting for update)
5. Only resumes when first new transaction is committed

**Reproduction Steps**:
```rust
// In a smoke test, configure node with:
let mut node_config = NodeConfig::get_default_vfn_config();
node_config.state_sync.state_sync_driver.bootstrapping_mode = 
    BootstrappingMode::DownloadLatestStates;
enable_internal_indexer(&mut node_config);

// Start node and observe internal indexer service
// Add logging in InternalIndexerDBService::run() to show:
// 1. When waiting for update at line 174
// 2. How long the wait persists
// 3. What triggers the first update
```

The test at [12](#0-11)  may not catch this if transactions are committed quickly enough after fast sync completion to mask the delay.

---

**Notes**

While the original security question focused on `secondary_db` not receiving `update_sender`, the actual vulnerability is in `finalize_state_snapshot()` not sending notifications for the database that DOES have the subscriber configured. This causes a service availability issue affecting all nodes performing fast sync with internal indexer enabled.

### Citations

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L125-241)
```rust
    fn finalize_state_snapshot(
        &self,
        version: Version,
        output_with_proof: TransactionOutputListWithProofV2,
        ledger_infos: &[LedgerInfoWithSignatures],
    ) -> Result<()> {
        let (output_with_proof, persisted_aux_info) = output_with_proof.into_parts();
        gauged_api("finalize_state_snapshot", || {
            // Ensure the output with proof only contains a single transaction output and info
            let num_transaction_outputs = output_with_proof.get_num_outputs();
            let num_transaction_infos = output_with_proof.proof.transaction_infos.len();
            ensure!(
                num_transaction_outputs == 1,
                "Number of transaction outputs should == 1, but got: {}",
                num_transaction_outputs
            );
            ensure!(
                num_transaction_infos == 1,
                "Number of transaction infos should == 1, but got: {}",
                num_transaction_infos
            );

            // TODO(joshlind): include confirm_or_save_frozen_subtrees in the change set
            // bundle below.

            // Update the merkle accumulator using the given proof
            let frozen_subtrees = output_with_proof
                .proof
                .ledger_info_to_transaction_infos_proof
                .left_siblings();
            restore_utils::confirm_or_save_frozen_subtrees(
                self.ledger_db.transaction_accumulator_db_raw(),
                version,
                frozen_subtrees,
                None,
            )?;

            // Create a single change set for all further write operations
            let mut ledger_db_batch = LedgerDbSchemaBatches::new();
            let mut sharded_kv_batch = self.state_kv_db.new_sharded_native_batches();
            let mut state_kv_metadata_batch = SchemaBatch::new();
            // Save the target transactions, outputs, infos and events
            let (transactions, outputs): (Vec<Transaction>, Vec<TransactionOutput>) =
                output_with_proof
                    .transactions_and_outputs
                    .into_iter()
                    .unzip();
            let events = outputs
                .clone()
                .into_iter()
                .map(|output| output.events().to_vec())
                .collect::<Vec<_>>();
            let wsets: Vec<WriteSet> = outputs
                .into_iter()
                .map(|output| output.write_set().clone())
                .collect();
            let transaction_infos = output_with_proof.proof.transaction_infos;
            // We should not save the key value since the value is already recovered for this version
            restore_utils::save_transactions(
                self.state_store.clone(),
                self.ledger_db.clone(),
                version,
                &transactions,
                &persisted_aux_info,
                &transaction_infos,
                &events,
                wsets,
                Some((
                    &mut ledger_db_batch,
                    &mut sharded_kv_batch,
                    &mut state_kv_metadata_batch,
                )),
                false,
            )?;

            // Save the epoch ending ledger infos
            restore_utils::save_ledger_infos(
                self.ledger_db.metadata_db(),
                ledger_infos,
                Some(&mut ledger_db_batch.ledger_metadata_db_batches),
            )?;

            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::LedgerCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;
            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::OverallCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;

            // Apply the change set writes to the database (atomically) and update in-memory state
            //
            // state kv and SMT should use shared way of committing.
            self.ledger_db.write_schemas(ledger_db_batch)?;

            self.ledger_pruner.save_min_readable_version(version)?;
            self.state_store
                .state_merkle_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .epoch_snapshot_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .state_kv_pruner
                .save_min_readable_version(version)?;

            restore_utils::update_latest_ledger_info(self.ledger_db.metadata_db(), ledger_infos)?;
            self.state_store.reset();

            Ok(())
        })
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L618-624)
```rust
            if let Some(update_sender) = &self.update_subscriber {
                update_sender
                    .send((Instant::now(), version))
                    .map_err(|err| {
                        AptosDbError::Other(format!("Failed to send update to subscriber: {}", err))
                    })?;
            }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1283-1310)
```rust
        if let Some(internal_indexer_db) = self.internal_indexer_db.as_ref() {
            if version > 0 {
                let mut batch = SchemaBatch::new();
                batch.put::<InternalIndexerMetadataSchema>(
                    &MetadataKey::LatestVersion,
                    &MetadataValue::Version(version - 1),
                )?;
                if internal_indexer_db.statekeys_enabled() {
                    batch.put::<InternalIndexerMetadataSchema>(
                        &MetadataKey::StateVersion,
                        &MetadataValue::Version(version - 1),
                    )?;
                }
                if internal_indexer_db.transaction_enabled() {
                    batch.put::<InternalIndexerMetadataSchema>(
                        &MetadataKey::TransactionVersion,
                        &MetadataValue::Version(version - 1),
                    )?;
                }
                if internal_indexer_db.event_enabled() {
                    batch.put::<InternalIndexerMetadataSchema>(
                        &MetadataKey::EventVersion,
                        &MetadataValue::Version(version - 1),
                    )?;
                }
                internal_indexer_db
                    .get_inner_db_ref()
                    .write_schemas(batch)?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/internal_indexer_db_service.rs (L94-100)
```rust
        let mut main_db_synced_version = self.db_indexer.main_db_reader.ensure_synced_version()?;

        // Wait till fast sync is done
        while fast_sync_enabled && main_db_synced_version == 0 {
            tokio::time::sleep(std::time::Duration::from_secs(1)).await;
            main_db_synced_version = self.db_indexer.main_db_reader.ensure_synced_version()?;
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/internal_indexer_db_service.rs (L168-183)
```rust
        let mut start_version = self.get_start_version(node_config).await?;
        let mut target_version = self.db_indexer.main_db_reader.ensure_synced_version()?;
        let mut step_timer = std::time::Instant::now();

        loop {
            if target_version <= start_version {
                match self.update_receiver.changed().await {
                    Ok(_) => {
                        (step_timer, target_version) = *self.update_receiver.borrow();
                    },
                    Err(e) => {
                        panic!("Failed to get update from update_receiver: {}", e);
                    },
                }
            }
            let next_version = self.db_indexer.process(start_version, target_version)?;
```

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L59-61)
```rust
        .map_err(|err| anyhow!("fast sync DB failed to open {}", err))?;
        if let Some(sender) = update_sender {
            db_main.add_version_update_subscriber(sender)?;
```

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L66-77)
```rust
        if config
            .state_sync
            .state_sync_driver
            .bootstrapping_mode
            .is_fast_sync()
            && (db_main
                .ledger_db
                .metadata_db()
                .get_synced_version()?
                .map_or(0, |v| v)
                == 0)
        {
```

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L79-90)
```rust
            let secondary_db = AptosDB::open(
                StorageDirPaths::from_path(db_dir.as_path()),
                /*readonly=*/ false,
                config.storage.storage_pruner_config,
                config.storage.rocksdb_configs,
                config.storage.enable_indexer,
                config.storage.buffered_state_target_items,
                config.storage.max_num_nodes_per_lru_cache_shard,
                None,
                config.storage.hot_state_config,
            )
            .map_err(|err| anyhow!("Secondary DB failed to open {}", err))?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/runtime.rs (L34-38)
```rust
    let mut indexer_service = InternalIndexerDBService::new(
        db_rw.reader,
        internal_indexer_db.unwrap(),
        update_receiver.expect("Internal indexer db update receiver is missing"),
    );
```

**File:** testsuite/smoke-test/src/fullnode.rs (L118-173)
```rust
async fn test_internal_indexer_with_fast_sync() {
    // Create a swarm with 2 validators
    let mut swarm = new_local_swarm_with_aptos(1).await;

    let validator_peer_id = swarm.validators().next().unwrap().peer_id();
    let validator_client = swarm.validator(validator_peer_id).unwrap().rest_client();
    let (mut account_0, account_1) = create_test_accounts(&mut swarm).await;

    execute_transactions(
        &mut swarm,
        &validator_client,
        &mut account_0,
        &account_1,
        true,
    )
    .await;

    let ledger_info = validator_client.get_ledger_information().await.unwrap();
    println!("ledger_info: {:?}", ledger_info);
    let mut vfn_config = NodeConfig::get_default_vfn_config();
    vfn_config.storage.rocksdb_configs.enable_storage_sharding = true;
    vfn_config.state_sync.state_sync_driver.bootstrapping_mode =
        BootstrappingMode::DownloadLatestStates;
    vfn_config
        .storage
        .storage_pruner_config
        .ledger_pruner_config
        .enable = true;
    vfn_config
        .storage
        .storage_pruner_config
        .ledger_pruner_config
        .prune_window = 100;
    vfn_config
        .storage
        .storage_pruner_config
        .ledger_pruner_config
        .batch_size = 50;
    vfn_config
        .storage
        .storage_pruner_config
        .ledger_pruner_config
        .user_pruning_window_offset = 30;

    enable_internal_indexer(&mut vfn_config);

    let peer_id = create_fullnode(vfn_config.clone(), &mut swarm).await;
    swarm
        .wait_for_all_nodes_to_catchup(Duration::from_secs(60))
        .await
        .unwrap();
    let node = swarm.full_node(peer_id).unwrap();
    let node_config = node.config().to_owned();
    node.stop().await.unwrap();
    check_indexer_db(&node_config);
}
```
