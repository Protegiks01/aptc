# Audit Report

## Title
Infinite Database Connection Retry Loop Causes Resource Exhaustion in Indexer Transaction Processor

## Summary
The indexer's `TransactionProcessor::get_conn()` method implements an infinite retry loop without exponential backoff, maximum attempt limits, or circuit breakers. When database connections fail, processor threads become stuck indefinitely attempting to reconnect every 30 seconds, causing thread exhaustion, connection pool depletion, and amplifying cascading failures during database outages. [1](#0-0) 

## Finding Description

The vulnerability exists in the database connection acquisition logic used throughout transaction processing. The `get_conn()` helper method enters an infinite loop when the connection pool cannot provide a connection, with no mechanism to eventually give up or back off.

**Attack Flow:**

1. **Trigger Condition**: Database becomes unavailable or connection pool is exhausted (can occur naturally during maintenance, network issues, or resource constraints)

2. **Infinite Retry Activation**: When `apply_processor_status()` is called during transaction processing, it invokes `get_conn()`: [2](#0-1) 

3. **Thread Starvation**: Multiple processor threads enter the retry loop simultaneously, each waiting 30 seconds per attempt (the default diesel r2d2 connection timeout) before retrying: [3](#0-2) 

4. **Resource Exhaustion**:
   - Processor threads remain blocked indefinitely
   - Each thread continuously attempts database connections
   - Metrics counter increments unboundedly
   - Log spam accumulates
   - Thread pool exhaustion prevents new work

5. **Cascading Failure Amplification**:
   - Continuous connection attempts stress an already-struggling database
   - No exponential backoff means constant hammering
   - Multiple indexer instances multiply the effect
   - Database cannot recover under sustained connection pressure

**Critical Call Paths Affected:**

The infinite retry affects all status update operations:
- `mark_versions_started()` - called before processing
- `update_status_success()` - called after successful processing  
- `update_status_err()` - called after processing errors [4](#0-3) 

**Contrast with Proper Implementation:**

The codebase contains correct retry implementations that this code fails to use. The `aptos-retrier` crate provides `ExponentWithLimitDelay` with proper timeout bounds: [5](#0-4) 

Other components like the NFT metadata crawler properly implement exponential backoff with maximum elapsed time: [6](#0-5) 

**Similar Vulnerability:**

The same pattern exists in the table info indexer with even worse characteristics (10ms retry delay enables ~100 retries/second): [7](#0-6) 

## Impact Explanation

**Severity: Medium**

This qualifies as **Medium severity** per Aptos bug bounty criteria because it causes:

1. **Service Availability Degradation**: Indexer becomes unresponsive, affecting API query functionality but not consensus or transaction execution
2. **Resource Exhaustion**: Thread pool depletion prevents the indexer from processing new transactions
3. **Cascading Failure Amplification**: Makes database recovery harder by maintaining connection pressure
4. **State Inconsistencies**: Transactions may be processed but status not recorded, requiring manual intervention

While the indexer is not on the consensus critical path, it's an essential service component. The bug bounty program lists "API crashes" as High severity, and this represents systematic resource exhaustion affecting indexer availability, justifying Medium severity.

This does NOT affect:
- Consensus safety or liveness
- Transaction execution correctness
- Validator operations
- On-chain state or funds

## Likelihood Explanation

**Likelihood: High**

Database connection failures are common operational scenarios:
- Planned maintenance and upgrades
- Network partitions between indexer and database
- Database resource exhaustion under load
- Connection pool saturation during traffic spikes
- Database crashes or restarts

Once triggered, the vulnerability activates deterministically:
- No special attacker capabilities required
- Every processor thread that needs a connection will hang
- Effect compounds with multiple concurrent transactions
- Recovery requires service restart

The lack of proper retry mechanisms despite their availability in the codebase (and correct usage elsewhere) indicates this is a latent defect affecting production deployments.

## Recommendation

Replace the infinite retry loop with bounded, exponential backoff using the existing `aptos-retrier` infrastructure:

```rust
use aptos_retrier::{retry, ExponentWithLimitDelay};

fn get_conn(&self) -> Result<PgPoolConnection, PoolError> {
    let pool = self.connection_pool();
    
    // Retry for up to 2 minutes with exponential backoff
    // Start: 100ms, Max: 10s per attempt, Total timeout: 120s
    let delays = ExponentWithLimitDelay::new(100, 10_000, 120_000);
    
    retry(delays, || {
        match pool.get() {
            Ok(conn) => {
                GOT_CONNECTION.inc();
                Ok(conn)
            },
            Err(err) => {
                UNABLE_TO_GET_CONNECTION.inc();
                aptos_logger::warn!(
                    "Could not get DB connection from pool: {:?}",
                    err
                );
                Err(err)
            }
        }
    })
}
```

**Additional Improvements:**

1. **Circuit Breaker Pattern**: Implement circuit breaking to stop retry attempts during sustained failures
2. **Graceful Degradation**: Allow status updates to fail without blocking transaction processing
3. **Health Check Integration**: Surface connection pool health via readiness probes
4. **Monitoring**: Alert on sustained connection failures before thread exhaustion
5. **Apply Same Fix**: Update `db_v2.rs::get_table_info_with_retry()` with similar bounded retry logic

## Proof of Concept

```rust
// Test demonstrating the vulnerability
#[tokio::test]
async fn test_infinite_retry_resource_exhaustion() {
    use std::sync::{Arc, atomic::{AtomicUsize, Ordering}};
    use std::time::{Duration, Instant};
    
    // Create a connection pool that will fail
    let database_url = "postgresql://invalid_host:5432/nonexistent";
    let pool = match new_db_pool(database_url) {
        Ok(p) => p,
        Err(_) => {
            // Expected: pool creation might fail
            return;
        }
    };
    
    let attempt_counter = Arc::new(AtomicUsize::new(0));
    let counter_clone = attempt_counter.clone();
    
    // Spawn task that will get stuck in infinite retry
    let handle = tokio::spawn(async move {
        let start = Instant::now();
        loop {
            match pool.get() {
                Ok(_) => break,
                Err(_) => {
                    counter_clone.fetch_add(1, Ordering::SeqCst);
                    // Simulate the retry behavior
                    if start.elapsed() > Duration::from_secs(10) {
                        // After 10 seconds, we should have hit timeout
                        // But in actual code, this never happens
                        break;
                    }
                    tokio::time::sleep(Duration::from_millis(100)).await;
                }
            }
        }
    });
    
    // Give it 3 seconds to demonstrate continuous retries
    tokio::time::sleep(Duration::from_secs(3)).await;
    
    let attempts = attempt_counter.load(Ordering::SeqCst);
    
    // Actual code would continue retrying indefinitely
    // This demonstrates the retry behavior continues without bounds
    assert!(attempts > 5, 
        "Should have made multiple retry attempts: {}", attempts);
    
    // In production, this task would never complete without external intervention
    handle.abort();
}
```

**To reproduce in a live system:**

1. Deploy indexer with PostgreSQL backend
2. Start processing transactions normally
3. Simulate database failure:
   - Stop PostgreSQL service, OR
   - Block network access to database, OR
   - Exhaust connection pool via external load
4. Observe indexer threads becoming stuck
5. Monitor metrics: `UNABLE_TO_GET_CONNECTION` counter increases unboundedly
6. Thread pool becomes exhausted
7. Indexer stops processing new transactions
8. Service requires restart to recover

**Notes**

The indexer component processes blockchain data into a PostgreSQL database for API queries. While not on the consensus critical path, it's an essential infrastructure service. The infinite retry vulnerability represents a failure in defensive programming that:

- Violates the "Resource Limits" invariant by allowing unbounded resource consumption
- Contradicts established patterns in the same codebase (proper retry with backoff exists and is used elsewhere)
- Creates operational fragility during common failure scenarios
- Amplifies rather than mitigates cascading failures

The vulnerability is particularly concerning because the infrastructure for proper retries (`aptos-retrier`, `backoff` crate) exists and is correctly used in other components like the NFT metadata crawler, indicating this is a gap in defensive implementation rather than missing capability.

### Citations

**File:** crates/indexer/src/indexer/transaction_processor.rs (L45-63)
```rust
    fn get_conn(&self) -> PgPoolConnection {
        let pool = self.connection_pool();
        loop {
            match pool.get() {
                Ok(conn) => {
                    GOT_CONNECTION.inc();
                    return conn;
                },
                Err(err) => {
                    UNABLE_TO_GET_CONNECTION.inc();
                    aptos_logger::error!(
                        "Could not get DB connection from pool, will retry in {:?}. Err: {:?}",
                        pool.connection_timeout(),
                        err
                    );
                },
            };
        }
    }
```

**File:** crates/indexer/src/indexer/transaction_processor.rs (L93-143)
```rust
    /// Writes that a version has been started for this `TransactionProcessor` to the DB
    fn mark_versions_started(&self, start_version: u64, end_version: u64) {
        aptos_logger::debug!(
            "[{}] Marking processing versions started from versions {} to {}",
            self.name(),
            start_version,
            end_version
        );
        let psms = ProcessorStatusModel::from_versions(
            self.name(),
            start_version,
            end_version,
            false,
            None,
        );
        self.apply_processor_status(&psms);
    }

    /// Writes that a version has been completed successfully for this `TransactionProcessor` to the DB
    fn update_status_success(&self, processing_result: &ProcessingResult) {
        aptos_logger::debug!(
            "[{}] Marking processing version OK from versions {} to {}",
            self.name(),
            processing_result.start_version,
            processing_result.end_version
        );
        PROCESSOR_SUCCESSES.with_label_values(&[self.name()]).inc();
        LATEST_PROCESSED_VERSION
            .with_label_values(&[self.name()])
            .set(processing_result.end_version as i64);
        let psms = ProcessorStatusModel::from_versions(
            self.name(),
            processing_result.start_version,
            processing_result.end_version,
            true,
            None,
        );
        self.apply_processor_status(&psms);
    }

    /// Writes that a version has errored for this `TransactionProcessor` to the DB
    fn update_status_err(&self, tpe: &TransactionProcessingError) {
        aptos_logger::debug!(
            "[{}] Marking processing version Err: {:?}",
            self.name(),
            tpe
        );
        PROCESSOR_ERRORS.with_label_values(&[self.name()]).inc();
        let psm = ProcessorStatusModel::from_transaction_processing_err(tpe);
        self.apply_processor_status(&psm);
    }
```

**File:** crates/indexer/src/indexer/transaction_processor.rs (L146-165)
```rust
    fn apply_processor_status(&self, psms: &[ProcessorStatusModel]) {
        let mut conn = self.get_conn();
        let chunks = get_chunks(psms.len(), ProcessorStatusModel::field_count());
        for (start_ind, end_ind) in chunks {
            execute_with_better_error(
                &mut conn,
                diesel::insert_into(processor_statuses::table)
                    .values(&psms[start_ind..end_ind])
                    .on_conflict((dsl::name, dsl::version))
                    .do_update()
                    .set((
                        dsl::success.eq(excluded(dsl::success)),
                        dsl::details.eq(excluded(dsl::details)),
                        dsl::last_updated.eq(excluded(dsl::last_updated)),
                    )),
                None,
            )
            .expect("Error updating Processor Status!");
        }
    }
```

**File:** crates/indexer/src/database.rs (L59-62)
```rust
pub fn new_db_pool(database_url: &str) -> Result<PgDbPool, PoolError> {
    let manager = ConnectionManager::<PgConnection>::new(database_url);
    PgPool::builder().build(manager).map(Arc::new)
}
```

**File:** crates/aptos-retrier/src/lib.rs (L85-124)
```rust
impl ExponentWithLimitDelay {
    pub fn new(start_ms: u64, limit_ms: u64, timeout_ms: u64) -> Self {
        ExponentWithLimitDelay {
            current: Duration::from_millis(start_ms),
            limit: Duration::from_millis(limit_ms),
            start: Instant::now(),
            timeout: Duration::from_millis(timeout_ms),
            exp: 1.5,
        }
    }
}

impl Iterator for FixedDelay {
    type Item = Duration;

    fn next(&mut self) -> Option<Duration> {
        Some(self.duration)
    }
}

impl Iterator for ExponentWithLimitDelay {
    type Item = Duration;

    fn next(&mut self) -> Option<Duration> {
        // If we've hit the timeout, no more delays.
        // This may go slightly over the limit, but it's close enough
        let elapsed = self.start.elapsed();
        if elapsed + self.current > self.timeout {
            return None;
        }

        // Backoff up to the max limit
        let duration = self.current;
        self.current = min(
            Duration::from_millis((self.current.as_millis() as f64 * self.exp) as u64),
            self.limit,
        );
        Some(duration)
    }
}
```

**File:** ecosystem/nft-metadata-crawler/src/models/parsed_asset_uris_query.rs (L47-56)
```rust
        let backoff = ExponentialBackoff {
            max_elapsed_time: Some(Duration::from_secs(MAX_RETRY_TIME_SECONDS)),
            ..Default::default()
        };

        retry(backoff, &mut op).unwrap_or_else(|e| {
            error!(asset_uri = asset_uri, error=?e, "Failed to get_by_asset_uri");
            None
        })
    }
```

**File:** storage/indexer/src/db_v2.rs (L153-173)
```rust
    pub fn get_table_info_with_retry(&self, handle: TableHandle) -> Result<Option<TableInfo>> {
        let mut retried = 0;
        loop {
            if let Ok(Some(table_info)) = self.get_table_info(handle) {
                return Ok(Some(table_info));
            }

            // Log the first failure, and then sample subsequent failures to avoid log spam
            if retried == 0 {
                log_table_info_failure(handle, retried);
            } else {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    log_table_info_failure(handle, retried)
                );
            }

            retried += 1;
            std::thread::sleep(Duration::from_millis(TABLE_INFO_RETRY_TIME_MILLIS));
        }
    }
```
