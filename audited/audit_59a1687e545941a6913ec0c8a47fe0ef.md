# Audit Report

## Title
File Descriptor Exhaustion in NFT Metadata Crawler HTTP Client Creation

## Summary
The NFT metadata crawler repeatedly creates new `reqwest::Client` instances for each HTTP request without reusing connection pools, leading to potential file descriptor exhaustion in long-running deployments processing high volumes of NFT metadata.

## Finding Description

The NFT metadata crawler's `run()` method [1](#0-0)  initiates a long-running HTTP server that processes NFT metadata parsing requests. For each incoming request, the system creates multiple HTTP clients across several code paths:

1. **URI Metadata Fetching**: The `get_uri_metadata()` function creates a new client for HEAD requests [2](#0-1) 

2. **JSON Parsing**: The `JSONParser::parse()` method creates a new client inside the retry closure [3](#0-2)  which can be invoked multiple times during exponential backoff retries [4](#0-3) 

3. **Image Optimization**: The `ImageOptimizer::optimize()` method creates a new client inside its retry closure [5](#0-4)  which is also subject to retry logic [6](#0-5) 

Each `reqwest::Client` instantiation creates internal connection pools and TCP sockets that consume system file descriptors. For a single NFT metadata request, the Worker can create 6+ client instances [7](#0-6)  (metadata check for JSON, JSON parsing with retries, metadata check for image, image optimization with retries, optional animation processing). Under sustained high load, this pattern exhausts the default file descriptor limit (typically 1024 on Linux systems).

## Impact Explanation

This qualifies as **Medium Severity** under the Aptos bug bounty program for the following reasons:

While the NFT metadata crawler is an auxiliary ecosystem service and not part of core blockchain consensus, it represents an **API crash** scenario. The service becomes unavailable when file descriptor limits are exhausted, requiring manual intervention to restart. This affects the broader Aptos ecosystem's ability to serve NFT metadata to wallets, explorers, and dApps.

The issue does NOT affect:
- Blockchain consensus or validator operations
- Move VM execution or state management  
- On-chain governance or staking mechanisms
- Core blockchain availability

However, it does impact the availability of an officially maintained indexing service that many ecosystem participants depend on.

## Likelihood Explanation

**Likelihood: Medium to High** in production environments with sustained traffic.

- **Trigger Condition**: Processing moderate to high volumes of NFT metadata requests (hundreds to thousands per hour)
- **Attacker Requirements**: No special privileges required; simply submitting NFT URIs for indexing (normal usage)
- **Complexity**: Low - this is triggered by normal service operation, not requiring sophisticated exploitation
- **Detection**: System monitoring would show increasing file descriptor usage until ulimit is hit, causing connection failures

In deployment scenarios where the crawler processes large NFT collections or experiences traffic spikes, file descriptor exhaustion can occur within hours of operation.

## Recommendation

**Refactor HTTP client instantiation to use a shared, reusable client instance.** The solution involves:

1. **Create a single shared client at the ParserContext level**: Store one `Arc<Client>` in the `ParserContext` struct and pass it to all workers instead of creating clients per-request.

2. **Pass the shared client to utility functions**: Modify `get_uri_metadata()`, `JSONParser::parse()`, and `ImageOptimizer::optimize()` to accept a `&Client` parameter rather than creating new instances.

3. **Remove client creation from retry closures**: Move client initialization outside of the retry logic in `json_parser.rs` and `image_optimizer.rs`.

Example fix for `get_uri_metadata()`:

```rust
pub async fn get_uri_metadata(client: &Client, url: &str) -> anyhow::Result<(String, u32)> {
    let request = client.head(url.trim());
    // ... rest of implementation
}
```

Similarly refactor `JSONParser::parse()` and `ImageOptimizer::optimize()` to accept `&Client` parameters, removing lines that create new clients inside retry closures.

## Proof of Concept

```rust
// Rust test demonstrating file descriptor leak
#[tokio::test]
async fn test_file_descriptor_leak() {
    use std::fs::File;
    use std::os::unix::io::AsRawFd;
    
    // Get initial file descriptor count
    let proc_fd_path = format!("/proc/{}/fd", std::process::id());
    let initial_fd_count = std::fs::read_dir(&proc_fd_path).unwrap().count();
    
    // Simulate 100 NFT metadata parsing operations
    for i in 0..100 {
        // Each call creates new Client instances (simplified simulation)
        let _ = get_uri_metadata("https://example.com/nft.json").await;
        let _ = JSONParser::parse("https://example.com/nft.json".to_string(), 1000000).await;
        let _ = ImageOptimizer::optimize("https://example.com/image.png", 1000000, 85, 512).await;
    }
    
    // Check file descriptor count after
    let final_fd_count = std::fs::read_dir(&proc_fd_path).unwrap().count();
    let fd_leaked = final_fd_count - initial_fd_count;
    
    println!("File descriptors leaked: {}", fd_leaked);
    assert!(fd_leaked < 50, "Excessive file descriptor leak detected: {} FDs", fd_leaked);
}
```

Under current implementation, this test would show significant file descriptor growth. After the fix with shared client instances, file descriptor count should remain relatively stable.

**Notes**

While this is a real operational issue affecting service availability, it's important to clarify that the NFT metadata crawler is an **off-chain auxiliary service** in the `ecosystem/` directory, separate from core blockchain functionality. The Aptos blockchain itself (consensus, execution, state management, validators) would continue operating normally even if this service crashes. The classification as Medium severity reflects its impact on ecosystem tooling rather than core protocol security.

### Citations

**File:** ecosystem/nft-metadata-crawler/src/config.rs (L87-104)
```rust
    async fn run(&self) -> anyhow::Result<()> {
        info!("[NFT Metadata Crawler] Starting with config: {:?}", self);

        info!("[NFT Metadata Crawler] Connecting to database");
        let pool = establish_connection_pool(&self.database_url);
        info!("[NFT Metadata Crawler] Database connection successful");

        info!("[NFT Metadata Crawler] Running migrations");
        run_migrations(&pool);
        info!("[NFT Metadata Crawler] Finished migrations");

        // Create request context
        let context = self.server_config.build_context(pool).await;
        let listener = TcpListener::bind(format!("0.0.0.0:{}", self.server_port)).await?;
        axum::serve(listener, context.build_router()).await?;

        Ok(())
    }
```

**File:** ecosystem/nft-metadata-crawler/src/lib.rs (L18-21)
```rust
    let client = Client::builder()
        .timeout(Duration::from_secs(MAX_HEAD_REQUEST_RETRY_SECONDS))
        .build()
        .context("Failed to build reqwest client")?;
```

**File:** ecosystem/nft-metadata-crawler/src/utils/json_parser.rs (L55-58)
```rust
                let client = Client::builder()
                    .timeout(Duration::from_secs(MAX_JSON_REQUEST_RETRY_SECONDS))
                    .build()
                    .context("Failed to build reqwest client")?;
```

**File:** ecosystem/nft-metadata-crawler/src/utils/json_parser.rs (L85-85)
```rust
        match retry(backoff, op).await {
```

**File:** ecosystem/nft-metadata-crawler/src/utils/image_optimizer.rs (L56-59)
```rust
                let client = Client::builder()
                    .timeout(Duration::from_secs(MAX_IMAGE_REQUEST_RETRY_SECONDS))
                    .build()
                    .context("Failed to build reqwest client")?;
```

**File:** ecosystem/nft-metadata-crawler/src/utils/image_optimizer.rs (L99-99)
```rust
        match retry(backoff, op).await {
```

**File:** ecosystem/nft-metadata-crawler/src/parser/worker.rs (L78-374)
```rust
    pub async fn parse(&mut self) -> anyhow::Result<()> {
        // Deduplicate asset_uri
        // Exit if not force or if asset_uri has already been parsed
        let prev_model = ParsedAssetUrisQuery::get_by_asset_uri(&mut self.conn, &self.asset_uri);
        if let Some(pm) = prev_model {
            DUPLICATE_ASSET_URI_COUNT.inc();
            self.model = pm.into();
            if !self.force && self.model.get_do_not_parse() {
                self.log_info("asset_uri has been marked as do_not_parse, skipping parse");
                SKIP_URI_COUNT.with_label_values(&["do_not_parse"]).inc();
                self.upsert();
                return Ok(());
            }
        }

        // Check asset_uri against the URI blacklist
        if self.is_blacklisted_uri(&self.asset_uri.clone()) {
            self.log_info("Found match in URI blacklist, marking as do_not_parse");
            self.model.set_do_not_parse(true);
            self.upsert();
            SKIP_URI_COUNT.with_label_values(&["blacklist"]).inc();
            return Ok(());
        }

        // Skip if asset_uri is not a valid URI, do not write invalid URI to Postgres
        if Url::parse(&self.asset_uri).is_err() {
            self.log_info("URI is invalid, skipping parse, marking as do_not_parse");
            self.model.set_do_not_parse(true);
            SKIP_URI_COUNT.with_label_values(&["invalid"]).inc();
            return Ok(());
        }

        if self.force || self.model.get_cdn_json_uri().is_none() {
            // Parse asset_uri
            self.log_info("Parsing asset_uri");
            let json_uri = URIParser::parse(
                &self.parser_config.ipfs_prefix,
                &self.model.get_asset_uri(),
                self.parser_config.ipfs_auth_key.as_deref(),
            )
            .unwrap_or_else(|_| {
                self.log_warn("Failed to parse asset_uri", None);
                PARSE_URI_TYPE_COUNT.with_label_values(&["other"]).inc();
                self.model.get_asset_uri()
            });

            // Parse JSON for raw_image_uri and raw_animation_uri
            self.log_info("Starting JSON parsing");
            let (raw_image_uri, raw_animation_uri, json) =
                JSONParser::parse(json_uri, self.parser_config.max_file_size_bytes)
                    .await
                    .unwrap_or_else(|e| {
                        // Increment retry count if JSON parsing fails
                        self.log_warn("JSON parsing failed", Some(&e));
                        self.model.increment_json_parser_retry_count();
                        (None, None, Value::Null)
                    });

            self.model.set_raw_image_uri(raw_image_uri);
            self.model.set_raw_animation_uri(raw_animation_uri);

            // Save parsed JSON to GCS
            if json != Value::Null {
                self.log_info("Writing JSON to GCS");
                let cdn_json_uri_result = write_json_to_gcs(
                    &self.parser_config.bucket,
                    &self.asset_uri,
                    &json,
                    &self.gcs_client,
                )
                .await;

                if let Err(e) = cdn_json_uri_result.as_ref() {
                    self.log_warn(
                        "Failed to write JSON to GCS, maybe upload timed out?",
                        Some(e),
                    );
                }

                let cdn_json_uri = cdn_json_uri_result
                    .map(|value| format!("{}{}", self.parser_config.cdn_prefix, value))
                    .ok();
                self.model.set_cdn_json_uri(cdn_json_uri);
            }

            // Commit model to Postgres
            self.log_info("Committing JSON to Postgres");
            self.upsert();
        }

        // Should I optimize image?
        // if force: true
        // else if cdn_image_uri already exists: false
        // else: perform lookup
        //     if found: set cdn_image_uri, false
        //     else: true
        let should_optimize_image = if self.force {
            true
        } else if self.model.get_cdn_image_uri().is_some() {
            false
        } else {
            self.model.get_raw_image_uri().is_none_or(|uri| {
                match ParsedAssetUrisQuery::get_by_raw_image_uri(
                    &mut self.conn,
                    &self.asset_uri,
                    &uri,
                ) {
                    Some(uris) => {
                        self.log_info("Duplicate raw_image_uri found");
                        DUPLICATE_RAW_IMAGE_URI_COUNT.inc();
                        self.model.set_cdn_image_uri(uris.cdn_image_uri);
                        self.upsert();
                        false
                    },
                    None => true,
                }
            })
        };

        if should_optimize_image {
            // Parse raw_image_uri, use asset_uri if parsing fails
            self.log_info("Parsing raw_image_uri");
            let raw_image_uri = self
                .model
                .get_raw_image_uri()
                .unwrap_or(self.model.get_asset_uri());

            // Check raw_image_uri against the URI blacklist
            if self.is_blacklisted_uri(&raw_image_uri) {
                self.log_info("Found match in URI blacklist, marking as do_not_parse");
                self.model.set_do_not_parse(true);
                self.upsert();
                SKIP_URI_COUNT.with_label_values(&["blacklist"]).inc();
                return Ok(());
            }

            let img_uri = URIParser::parse(
                &self.parser_config.ipfs_prefix,
                &raw_image_uri,
                self.parser_config.ipfs_auth_key.as_deref(),
            )
            .unwrap_or_else(|_| {
                self.log_warn("Failed to parse raw_image_uri", None);
                PARSE_URI_TYPE_COUNT.with_label_values(&["other"]).inc();
                raw_image_uri.clone()
            });

            // Resize and optimize image
            self.log_info("Starting image optimization");
            OPTIMIZE_IMAGE_TYPE_COUNT
                .with_label_values(&["image"])
                .inc();
            let (image, format) = ImageOptimizer::optimize(
                &img_uri,
                self.parser_config.max_file_size_bytes,
                self.parser_config.image_quality,
                self.parser_config.max_image_dimensions,
            )
            .await
            .unwrap_or_else(|e| {
                // Increment retry count if image is None
                self.log_warn("Image optimization failed", Some(&e));
                self.model.increment_image_optimizer_retry_count();
                (vec![], ImageFormat::Png)
            });

            // Save resized and optimized image to GCS
            if !image.is_empty() {
                self.log_info("Writing image to GCS");
                let cdn_image_uri_result = write_image_to_gcs(
                    format,
                    &self.parser_config.bucket,
                    &raw_image_uri,
                    image,
                    &self.gcs_client,
                )
                .await;

                if let Err(e) = cdn_image_uri_result.as_ref() {
                    self.log_warn(
                        "Failed to write image to GCS, maybe upload timed out?",
                        Some(e),
                    );
                }

                let cdn_image_uri = cdn_image_uri_result
                    .map(|value| format!("{}{}", self.parser_config.cdn_prefix, value))
                    .ok();
                self.model.set_cdn_image_uri(cdn_image_uri);
                self.model.reset_json_parser_retry_count();
            }

            // Commit model to Postgres
            self.log_info("Committing image to Postgres");
            self.upsert();
        }

        // Should I optimize animation?
        // if force: true
        // else if cdn_animation_uri already exists: false
        // else: perform lookup
        //     if found: set cdn_animation_uri, false
        //     else: true
        let raw_animation_uri_option = if self.force {
            self.model.get_raw_animation_uri()
        } else if self.model.get_cdn_animation_uri().is_some() {
            None
        } else {
            self.model.get_raw_animation_uri().and_then(|uri| {
                match ParsedAssetUrisQuery::get_by_raw_animation_uri(
                    &mut self.conn,
                    &self.asset_uri,
                    &uri,
                ) {
                    Some(uris) => {
                        self.log_info("Duplicate raw_image_uri found");
                        DUPLICATE_RAW_ANIMATION_URI_COUNT.inc();
                        self.model.set_cdn_animation_uri(uris.cdn_animation_uri);
                        self.upsert();
                        None
                    },
                    None => Some(uri),
                }
            })
        };

        // If raw_animation_uri_option is None, skip
        if let Some(raw_animation_uri) = raw_animation_uri_option {
            self.log_info("Parsing raw_animation_uri");
            let animation_uri = URIParser::parse(
                &self.parser_config.ipfs_prefix,
                &raw_animation_uri,
                self.parser_config.ipfs_auth_key.as_deref(),
            )
            .unwrap_or_else(|_| {
                self.log_warn("Failed to parse raw_animation_uri", None);
                PARSE_URI_TYPE_COUNT.with_label_values(&["other"]).inc();
                raw_animation_uri.clone()
            });

            // Resize and optimize animation
            self.log_info("Starting animation optimization");
            OPTIMIZE_IMAGE_TYPE_COUNT
                .with_label_values(&["animation"])
                .inc();
            let (animation, format) = ImageOptimizer::optimize(
                &animation_uri,
                self.parser_config.max_file_size_bytes,
                self.parser_config.image_quality,
                self.parser_config.max_image_dimensions,
            )
            .await
            .unwrap_or_else(|e| {
                // Increment retry count if animation is None
                self.log_warn("Animation optimization failed", Some(&e));
                self.model.increment_animation_optimizer_retry_count();
                (vec![], ImageFormat::Png)
            });

            // Save resized and optimized animation to GCS
            if !animation.is_empty() {
                self.log_info("Writing animation to GCS");
                let cdn_animation_uri_result = write_image_to_gcs(
                    format,
                    &self.parser_config.bucket,
                    &raw_animation_uri,
                    animation,
                    &self.gcs_client,
                )
                .await;

                if let Err(e) = cdn_animation_uri_result.as_ref() {
                    self.log_error("Failed to write animation to GCS", e);
                }

                let cdn_animation_uri = cdn_animation_uri_result
                    .map(|value| format!("{}{}", self.parser_config.cdn_prefix, value))
                    .ok();
                self.model.set_cdn_animation_uri(cdn_animation_uri);
            }

            // Commit model to Postgres
            self.log_info("Committing animation to Postgres");
            self.upsert();
        }

        if self.model.get_json_parser_retry_count() >= self.max_num_retries
            || self.model.get_image_optimizer_retry_count() >= self.max_num_retries
            || self.model.get_animation_optimizer_retry_count() >= self.max_num_retries
        {
            self.log_info("Retry count exceeded, marking as do_not_parse");
            self.model.set_do_not_parse(true);
            self.upsert();
        }

        PARSER_SUCCESSES_COUNT.inc();
        Ok(())
```
