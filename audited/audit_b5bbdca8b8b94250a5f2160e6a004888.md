# Audit Report

## Title
Concurrent Backup Maintenance Operations Cause Race Conditions Leading to Partial Reads and File Access Failures

## Summary
The backup maintenance system's `Compact` and `ReadMetadata` operations lack synchronization mechanisms when executed concurrently on the same storage backend. This creates race conditions that can result in partial reads of incomplete metadata files, file-not-found errors during file relocation, and TOCTOU vulnerabilities. These issues compromise backup integrity and disaster recovery reliability.

## Finding Description

The backup maintenance system in `storage/db-tool/src/backup_maintenance.rs` defines two operations that can access the same storage backend concurrently without coordination: [1](#0-0) 

The `Compact` operation creates new compacted metadata files and moves expired files to a backup folder: [2](#0-1) 

The file writing operation in `LocalFs::save_metadata_lines` uses a non-atomic multi-step process: [3](#0-2) 

The critical flaw is between lines 162-170: after the file is created atomically with `.create_new(true)`, there is a non-atomic window where:
1. Data is written with `write_all()` 
2. File is flushed/closed with `shutdown()`

During this window, the `ReadMetadata` operation can open and read the file: [4](#0-3) [5](#0-4) 

**Race Condition #1 - Partial Reads During File Creation:**
1. Compact creates `transaction_compacted_0-99999.meta` (atomic file creation)
2. Compact starts writing metadata content via `write_all()`
3. **BEFORE** `shutdown()` completes, ReadMetadata opens the same file
4. ReadMetadata reads incomplete data and attempts JSON deserialization
5. Result: Either deserialization error OR successfully parsing incomplete/corrupted metadata

**Race Condition #2 - File Access During Relocation:**
The compaction process moves expired files to a backup folder: [6](#0-5) 

If ReadMetadata attempts to read a file while it's being moved by `rename()` at line 144:
- The read operation may fail with "file not found"
- On Windows systems, the rename may fail due to file locking
- File handles may become stale mid-operation

**Race Condition #3 - TOCTOU in Directory Creation:**
Lines 130-133 show a check-then-act pattern without synchronization, creating a TOCTOU vulnerability if multiple processes try to back up files concurrently.

The BackupStorage trait documentation explicitly acknowledges undefined behavior for concurrent operations: [7](#0-6) 

## Impact Explanation

This vulnerability falls under **Medium Severity** per the Aptos bug bounty program ("State inconsistencies requiring intervention"), with potential elevation to **High Severity** if it causes validator operational issues.

**Impact on Disaster Recovery:**
- Corrupted metadata prevents accurate backup restoration
- Validators recovering from disasters may restore to incorrect state versions
- Partial reads during backup verification jobs cause false negatives
- Silent corruption (valid but incomplete JSON) is more dangerous than obvious failures

**Operational Impact:**
- Scheduled backup verification jobs running concurrently with compaction experience random failures
- Manual backup inspections during compaction windows encounter inconsistent state
- Backup monitoring systems receive false alerts

**Production Scenarios:**
The Kubernetes CronJob configuration shows compaction runs on a schedule: [8](#0-7) 

While `concurrencyPolicy: Replace` prevents overlapping scheduled compactions, it does NOT prevent:
- Manual `read-metadata` commands during scheduled compaction
- Backup verification jobs accessing storage during compaction
- Restore operations reading metadata while compaction runs
- Multiple manual tool invocations

## Likelihood Explanation

**High Likelihood** in production environments due to:

1. **Concurrent Operations by Design**: The system schedules multiple backup-related jobs (compaction, verification, monitoring) that naturally access the same storage

2. **Long Compaction Windows**: With compact factors of 100 files, compaction operations take significant time, creating large windows for concurrent access

3. **No Synchronization**: Complete absence of locking, coordination, or atomic file operations

4. **Operational Reality**: Operators frequently inspect backups manually, overlapping with automated jobs

5. **No Warnings**: The CLI provides no warnings about concurrent access risks

## Recommendation

Implement proper synchronization and atomic file operations:

**1. Atomic File Writes Using Temporary Files:**
```rust
async fn save_metadata_lines(
    &self,
    name: &ShellSafeName,
    lines: &[TextLine],
) -> Result<FileHandle> {
    let dir = self.metadata_dir();
    create_dir_all(&dir).await.err_notes(name)?;
    
    let content = lines.iter().map(|e| e.as_ref()).collect::<Vec<&str>>().join("");
    
    // Write to temporary file first
    let temp_name = format!("{}.tmp.{}", name.as_ref(), std::process::id());
    let temp_path = dir.join(&temp_name);
    let final_path = dir.join(name.as_ref());
    
    let mut file = OpenOptions::new()
        .write(true)
        .create_new(true)
        .open(&temp_path)
        .await?;
    
    file.write_all(content.as_bytes()).await?;
    file.sync_all().await?;  // Ensure data is flushed
    drop(file);  // Close file
    
    // Atomic rename
    rename(&temp_path, &final_path).await?;
    
    let fh = PathBuf::from(Self::METADATA_DIR)
        .join(name.as_ref())
        .path_to_string()?;
    Ok(fh)
}
```

**2. Add Advisory File Locking:**
Use filesystem advisory locks (flock on Unix, LockFile on Windows) to coordinate access between Compact and ReadMetadata operations.

**3. Add Operation Coordination:**
Implement a lock file mechanism where Compact creates a `.compacting` lock file that ReadMetadata checks before accessing storage.

**4. Document Concurrent Access Limitations:**
Add CLI warnings and documentation clearly stating that Compact and ReadMetadata should not run concurrently.

## Proof of Concept

```rust
// Test demonstrating concurrent access race condition
#[tokio::test]
async fn test_concurrent_compact_read_race() {
    use std::sync::Arc;
    use tokio::task::JoinHandle;
    
    // Setup shared storage backend
    let temp_dir = tempfile::tempdir().unwrap();
    let storage = Arc::new(LocalFs::new(temp_dir.path().to_path_buf()));
    
    // Create initial metadata files
    for i in 0..10 {
        let name: ShellSafeName = format!("test_{}.meta", i).parse().unwrap();
        let line = TextLine::new(&format!("{{\"version\": {}}}", i)).unwrap();
        storage.save_metadata_line(&name, &line).await.unwrap();
    }
    
    let storage_clone = Arc::clone(&storage);
    
    // Spawn compaction task
    let compact_handle: JoinHandle<()> = tokio::spawn(async move {
        // Simulate compaction: create new compacted file
        let compacted_name: ShellSafeName = "test_compacted_0-9.meta".parse().unwrap();
        let mut lines = Vec::new();
        for i in 0..10 {
            lines.push(TextLine::new(&format!("{{\"version\": {}}}", i)).unwrap());
        }
        storage_clone.save_metadata_lines(&compacted_name, &lines).await.unwrap();
    });
    
    // Small delay to ensure file creation starts
    tokio::time::sleep(tokio::time::Duration::from_millis(1)).await;
    
    // Spawn concurrent read task
    let storage_clone2 = Arc::clone(&storage);
    let read_handle: JoinHandle<Result<serde_json::Value>> = tokio::spawn(async move {
        // Try to read the file being written
        let file_path = "metadata/test_compacted_0-9.meta";
        storage_clone2.load_json_file::<serde_json::Value>(file_path).await
    });
    
    compact_handle.await.unwrap();
    let read_result = read_handle.await.unwrap();
    
    // Race condition: read may fail with partial data or succeed with incomplete JSON
    match read_result {
        Ok(value) => {
            // May have read partial data if timing aligned incorrectly
            println!("Read succeeded, but may have partial data: {:?}", value);
        }
        Err(e) => {
            // Expected: JSON parse error from partial read
            println!("Read failed as expected during concurrent write: {}", e);
        }
    }
}
```

**Notes:**
- This vulnerability requires access to backup maintenance tools or backup storage, typically available to validator operators
- While not exploitable by completely unprivileged attackers, it represents a critical reliability flaw in disaster recovery infrastructure
- The race conditions occur naturally during normal operations when scheduled jobs overlap with manual inspections or restore operations
- Silent corruption (successful parsing of incomplete metadata) is particularly dangerous as it may go undetected until restoration is attempted

### Citations

**File:** storage/db-tool/src/backup_maintenance.rs (L14-21)
```rust
pub enum Command {
    #[clap(about = "Compact metdata files")]
    Compact(CompactionOpt),
    #[clap(about = "Cleanup the backup metadata files")]
    Cleanup(CleanupOpt),
    #[clap(about = "Display the backup meatdata in human-readable JSON format.")]
    ReadMetadata(ReadMetadataOpt),
}
```

**File:** storage/db-tool/src/backup_maintenance.rs (L80-87)
```rust
            Command::ReadMetadata(opt) => {
                println!("Reading metadata file at: {}...", opt.path);
                let storage = opt.storage.init_storage().await?;
                let json_value = storage
                    .load_json_file::<serde_json::Value>(&opt.path)
                    .await?;
                println!("{}", serde_json::to_string_pretty(&json_value).unwrap());
            },
```

**File:** storage/backup/backup-cli/src/coordinators/backup.rs (L409-475)
```rust
    pub async fn run(self) -> Result<()> {
        info!("Backup compaction started");
        // sync the metadata from backup storage
        let mut metaview = metadata::cache::sync_and_load(
            &self.metadata_cache_opt,
            Arc::clone(&self.storage),
            self.concurrent_downloads,
        )
        .await?;

        let files = metaview.get_file_handles();

        info!("Start compacting backup metadata files.");
        let mut new_files: HashSet<FileHandle> = HashSet::new(); // record overwrite file names
        for range in metaview.compact_epoch_ending_backups(self.epoch_ending_file_compact_factor)? {
            let (epoch_range, file_name) =
                Metadata::compact_epoch_ending_backup_range(range.to_vec())?;
            let file_handle = self
                .storage
                .save_metadata_lines(&file_name, epoch_range.as_slice())
                .await?;
            new_files.insert(file_handle);
        }
        for range in metaview.compact_transaction_backups(self.transaction_file_compact_factor)? {
            let (txn_range, file_name) =
                Metadata::compact_transaction_backup_range(range.to_vec())?;
            let file_handle = self
                .storage
                .save_metadata_lines(&file_name, txn_range.as_slice())
                .await?;
            new_files.insert(file_handle);
        }
        for range in metaview.compact_state_backups(self.state_snapshot_file_compact_factor)? {
            let (state_range, file_name) =
                Metadata::compact_statesnapshot_backup_range(range.to_vec())?;
            let file_handle = self
                .storage
                .save_metadata_lines(&file_name, state_range.as_slice())
                .await?;
            new_files.insert(file_handle);
        }

        // Move expired files to the metadata backup folder
        let (to_move, compaction_meta) =
            self.update_compaction_timestamps(&mut metaview, files, new_files)?;
        for file in to_move {
            info!(file = file, "Backup metadata file.");
            self.storage
                .backup_metadata_file(&file)
                .await
                .map_err(|err| {
                    error!(
                        file = file,
                        error = %err,
                        "Backup metadata file failed, ignoring.",
                    )
                })
                .ok();
        }
        // save the metadata compaction timestamps
        let metadata = Metadata::new_compaction_timestamps(compaction_meta);
        self.storage
            .save_metadata_line(&metadata.name(), &metadata.to_text_line()?)
            .await?;

        Ok(())
    }
```

**File:** storage/backup/backup-cli/src/storage/local_fs/mod.rs (L127-147)
```rust
    async fn backup_metadata_file(&self, file_handle: &FileHandleRef) -> Result<()> {
        let dir = self.metadata_backup_dir();

        // Check if the backup directory exists, create it if it doesn't
        if !dir.exists() {
            create_dir_all(&dir).await?;
        }

        // Get the file name and the backup file path
        let name = Path::new(file_handle)
            .file_name()
            .and_then(OsStr::to_str)
            .ok_or_else(|| format_err!("cannot extract filename from {}", file_handle))?;
        let mut backup_path = PathBuf::from(&dir);
        backup_path.push(name);

        // Move the file to the backup directory
        rename(&self.dir.join(file_handle), &backup_path).await?;

        Ok(())
    }
```

**File:** storage/backup/backup-cli/src/storage/local_fs/mod.rs (L149-181)
```rust
    async fn save_metadata_lines(
        &self,
        name: &ShellSafeName,
        lines: &[TextLine],
    ) -> Result<FileHandle> {
        let dir = self.metadata_dir();
        create_dir_all(&dir).await.err_notes(name)?; // in case not yet created
        let content = lines
            .iter()
            .map(|e| e.as_ref())
            .collect::<Vec<&str>>()
            .join("");
        let path = dir.join(name.as_ref());
        let file = OpenOptions::new()
            .write(true)
            .create_new(true)
            .open(&path)
            .await;
        match file {
            Ok(mut f) => {
                f.write_all(content.as_bytes()).await.err_notes(&path)?;
                f.shutdown().await.err_notes(&path)?;
            },
            Err(e) if e.kind() == io::ErrorKind::AlreadyExists => {
                info!("File {} already exists, Skip", name.as_ref());
            },
            _ => bail!("Unexpected Error in saving metadata file {}", name.as_ref()),
        }
        let fh = PathBuf::from(Self::METADATA_DIR)
            .join(name.as_ref())
            .path_to_string()?;
        Ok(fh)
    }
```

**File:** storage/backup/backup-cli/src/utils/storage_ext.rs (L24-29)
```rust
    async fn read_all(&self, file_handle: &FileHandleRef) -> Result<Vec<u8>> {
        let mut file = self.open_for_read(file_handle).await?;
        let mut bytes = Vec::new();
        file.read_to_end(&mut bytes).await?;
        Ok(bytes)
    }
```

**File:** storage/backup/backup-cli/src/storage/mod.rs (L161-162)
```rust
    /// Behavior on duplicated names is undefined, overwriting the content upon an existing name
    /// is straightforward and acceptable.
```

**File:** terraform/helm/fullnode/templates/backup-compaction.yaml (L11-12)
```yaml
  concurrencyPolicy: Replace
  schedule: {{ .Values.backup_compaction.schedule | quote }}
```
