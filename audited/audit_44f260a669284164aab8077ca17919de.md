# Audit Report

## Title
Off-By-One Error in Transaction Version Calculation Causes Indexer Service Crash

## Summary
A critical off-by-one error in the `update_data()` function causes transactions to be stored at incorrect cache slot positions. When the service attempts to access the last transaction to update latency metrics, it triggers a panic due to calling `.unwrap()` on a None value, crashing the indexer-grpc data service. [1](#0-0) 

## Finding Description

The vulnerability exists in the version calculation logic within the transaction storage loop. The function receives transactions with their starting version number and stores them in a circular buffer. When transactions need to be partially skipped (because they're older than the cache's start version), the code calculates which transactions to skip but then incorrectly calculates the version numbers for storage.

**The Bug:** At line 70, `start_version` is reassigned to `max(param_start_version, self.start_version)`, shadowing the function parameter. Then at line 80, the code calculates `version = start_version + i`, where `i` is the enumeration index from the original transactions vector. Since `enumerate()` is called before `skip()`, the index `i` includes the skipped elements in its count. This causes a double-counting of the offset, storing transactions at incorrect version numbers.

**Exploitation Scenario:**
1. Cache initialized with `start_version=100, end_version=100`
2. Incoming data: `start_version=50`, 60 transactions (covering versions 50-109)
3. `num_to_skip = 100 - 50 = 50` transactions should be skipped
4. Loop processes transactions[50..59] with enumeration indices i=50..59
5. Transactions stored at versions 100+50=150 through 100+59=159 (INCORRECT - should be 100-109)
6. At line 94, code attempts to access transaction at version 109 to update latency metrics
7. Transaction at version 109 is not in slot 109 (it's in slot 159)
8. `.unwrap()` panics because slot 109 is None, crashing the service [2](#0-1) 

This breaks the service availability invariant - the indexer should reliably serve transaction data to clients without crashing.

## Impact Explanation

**High Severity** - This qualifies as "API crashes" per the Aptos bug bounty program. The indexer-grpc-data-service-v2 is a critical API component that serves blockchain transaction data to external applications, wallets, and services. When this service crashes:

1. **Service Unavailability**: The indexer service becomes unavailable, preventing clients from querying historical transaction data
2. **Application Failures**: Downstream applications and services that depend on the indexer for transaction data will experience failures
3. **User Impact**: End users of applications relying on this indexer cannot access transaction history, account states, or other indexed data
4. **Operational Burden**: Requires manual service restart and investigation, causing operational overhead

While this does not directly affect blockchain consensus or validator operations, it severely impacts the ecosystem's infrastructure layer that applications depend on.

## Likelihood Explanation

**High Likelihood** - This vulnerability will trigger in normal production operation whenever:

1. The cache receives transaction data with a starting version lower than the cache's current start version
2. The incoming data extends beyond the cache's current end version
3. Both conditions commonly occur during:
   - Service startup/initialization when catching up with the blockchain
   - Network delays causing out-of-order data arrival
   - Cache eviction followed by backfill operations

The vulnerability does NOT require:
- Malicious actors or compromised infrastructure
- Special privileges or validator access
- Corrupted or malformed protobuf data
- Any attack coordination

It's a deterministic logic bug that will consistently crash the service under specific but common operational conditions, making it highly likely to occur in production environments.

## Recommendation

The version calculation must use the original `start_version` parameter value, not the adjusted cache-aligned value. There are two ways to fix this:

**Option 1** - Preserve the original start_version:
```rust
pub(super) fn update_data(&mut self, start_version: u64, transactions: Vec<Transaction>) {
    let end_version = start_version + transactions.len() as u64;
    // ... validation checks ...
    
    let original_start_version = start_version; // Preserve original
    let num_to_skip = self.start_version.saturating_sub(start_version);
    let start_version = start_version.max(self.start_version);
    
    for (i, transaction) in transactions
        .into_iter()
        .enumerate()
        .skip(num_to_skip as usize)
    {
        let version = original_start_version + i as u64; // Use original
        // ... rest of loop ...
    }
}
```

**Option 2** - Adjust for skipped elements:
```rust
let version = start_version + (i as u64 - num_to_skip);
```

Additionally, the `.unwrap()` at line 94 should be replaced with proper error handling:
```rust
if let Some(transaction) = self.get_data(end_version - 1).as_ref() {
    if let Some(txn_timestamp) = transaction.timestamp {
        // ... timestamp processing ...
    }
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use aptos_protos::transaction::v1::Transaction;

    #[test]
    #[should_panic(expected = "called `Option::unwrap()` on a `None` value")]
    fn test_version_calculation_panic() {
        // Initialize cache at version 100
        let mut data_manager = DataManager::new(100, 1000, 1_000_000);
        
        // Create 60 transactions for versions 50-109
        let mut transactions = Vec::new();
        for version in 50..110 {
            let mut tx = Transaction::default();
            tx.version = version;
            // Add required timestamp to trigger the panic path
            tx.timestamp = Some(aptos_protos::util::timestamp::Timestamp {
                seconds: 1234567890,
                nanos: 0,
            });
            transactions.push(tx);
        }
        
        // This should store transactions at versions 100-109
        // but instead stores them at 150-159 due to the bug
        // Then tries to access version 109 which is None, causing panic
        data_manager.update_data(50, transactions);
    }

    #[test]
    fn test_wrong_slot_storage() {
        let mut data_manager = DataManager::new(100, 1000, 1_000_000);
        
        let mut transactions = Vec::new();
        for version in 50..60 {
            let mut tx = Transaction::default();
            tx.version = version;
            transactions.push(tx);
        }
        
        data_manager.update_data(50, transactions);
        
        // Transaction for blockchain version 50 should be in slot 50
        // but due to the bug, it's actually in slot 150
        assert!(data_manager.get_data(50).is_none());  // Wrong slot is empty
        // Would need to check slot 150 to find it (if num_slots > 150)
    }
}
```

## Notes

While the security question focuses on "corrupted protobuf data" as the potential cause of crashes, the actual vulnerability is a logic error in version calculation that occurs with valid, well-formed data. The crash at line 94 results from the `.unwrap()` being called on None due to transactions being stored at incorrect version numbers, not from malformed protobuf data.

There is a secondary potential vulnerability related to invalid timestamp values: if a Transaction's `timestamp.nanos` field contains values â‰¥ 1,000,000,000 or negative values (which would cast to large u32 values), the `Duration::new()` call at line 97 would panic. However, this requires corrupted upstream data and is less likely in practice since Aptos nodes should produce valid timestamps. [3](#0-2) 

The `get_data()` function itself is safe and simply returns a reference to the Option. The unsafe operations occur during `update_data()` when it calls `get_data()` and then immediately unwraps the result without checking.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/data_manager.rs (L40-42)
```rust
    pub(super) fn get_data(&self, version: u64) -> &Option<Box<Transaction>> {
        &self.data[version as usize % self.num_slots]
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/data_manager.rs (L69-87)
```rust
        let num_to_skip = self.start_version.saturating_sub(start_version);
        let start_version = start_version.max(self.start_version);

        let mut size_increased = 0;
        let mut size_decreased = 0;

        for (i, transaction) in transactions
            .into_iter()
            .enumerate()
            .skip(num_to_skip as usize)
        {
            let version = start_version + i as u64;
            let slot_index = version as usize % self.num_slots;
            if let Some(transaction) = self.data[slot_index].take() {
                size_decreased += transaction.encoded_len();
            }
            size_increased += transaction.encoded_len();
            self.data[version as usize % self.num_slots] = Some(Box::new(transaction));
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/data_manager.rs (L94-101)
```rust
            if let Some(txn_timestamp) = self.get_data(end_version - 1).as_ref().unwrap().timestamp
            {
                let timestamp_since_epoch =
                    Duration::new(txn_timestamp.seconds as u64, txn_timestamp.nanos as u32);
                let now_since_epoch = SystemTime::now().duration_since(UNIX_EPOCH).unwrap();
                let latency = now_since_epoch.saturating_sub(timestamp_since_epoch);
                LATENCY_MS.set(latency.as_millis() as i64);
            }
```
