# Audit Report

## Title
Mempool Channel Overflow and Coordinator Deadlock Enables API Denial of Service

## Summary
The `MempoolClientSender` uses a bounded channel with a buffer size of 1,024 messages, but lacks proper backpressure handling. An attacker can exploit the interaction between the bounded channel, the mempool coordinator's event loop, and the bounded executor to cause API request handlers to hang indefinitely, resulting in denial of service. [1](#0-0) 

## Finding Description

The vulnerability stems from a deadlock condition between the bounded channel and the mempool coordinator's event processing:

1. **Bounded Channel Creation**: The API and mempool communicate via a bounded `mpsc::channel` with buffer size 1,024: [2](#0-1) 

2. **API Submission Blocks on Full Channel**: When the API submits transactions, it calls `mp_sender.send().await` which blocks indefinitely if the channel is full: [3](#0-2) 

3. **No Request Timeout**: The Poem web server has no timeout configuration, allowing requests to hang indefinitely: [4](#0-3) 

4. **Coordinator Event Loop Blocking**: The coordinator processes client requests using a bounded executor with only 4 workers (default): [5](#0-4) [6](#0-5) 

5. **Critical Blocking Point**: When handling client requests, the coordinator awaits on `bounded_executor.spawn()`, which blocks if all workers are busy: [7](#0-6) 

**Attack Scenario**:
1. Attacker rapidly submits 1,024+ transactions via API POST `/v1/transactions`
2. These fill the bounded channel buffer
3. Attacker submits 4 slow-processing transactions that occupy all bounded executor workers
4. The coordinator blocks in `bounded_executor.spawn().await` waiting for worker permits
5. While coordinator is blocked, it cannot pull new messages from `client_events` channel
6. Channel remains full at 1,024 messages
7. All subsequent API requests calling `mp_sender.send().await` hang indefinitely
8. API becomes completely unresponsive - Denial of Service achieved

## Impact Explanation

**High Severity** per Aptos bug bounty criteria:
- **API crashes**: The API becomes completely unresponsive to transaction submissions
- **Validator node slowdowns**: The node's API service is unavailable, affecting network accessibility
- Violates the **Resource Limits** invariant: operations must respect computational limits, but unbounded blocking exhausts API handler threads/resources

This breaks normal transaction submission flow and can render the node's API unusable, requiring restart to recover.

## Likelihood Explanation

**High Likelihood**:
- Requires no special privileges - any user can submit transactions via public API
- Attack is simple: send HTTP POST requests rapidly
- Small worker pool (4 workers) and moderate channel buffer (1,024) make exploitation feasible
- No rate limiting or timeout protections visible in the code
- Attacker only needs to sustain request rate slightly faster than 4 workers can process

## Recommendation

Implement proper timeout and backpressure handling:

1. **Add Timeout to Channel Send**: Use `tokio::time::timeout()` around the `send()` call to prevent indefinite blocking:

```rust
pub async fn submit_transaction(&self, txn: SignedTransaction) -> Result<SubmissionStatus> {
    let (req_sender, callback) = oneshot::channel();
    
    // Add 5-second timeout for channel send
    tokio::time::timeout(
        Duration::from_secs(5),
        self.mp_sender.clone().send(MempoolClientRequest::SubmitTransaction(txn, req_sender))
    )
    .await
    .map_err(|_| anyhow!("Mempool submission timeout - system overloaded"))??;
    
    callback.await?
}
```

2. **Use try_spawn Instead of spawn**: Modify coordinator to fail fast when workers are exhausted rather than blocking:

```rust
// Use try_spawn and handle capacity exhaustion gracefully
match bounded_executor.try_spawn(tasks::process_client_transaction_submission(...)) {
    Ok(_) => {},
    Err(_) => {
        // Send back error through callback indicating overload
        let _ = callback.send(Err(anyhow!("Mempool processing capacity exhausted")));
        counters::MEMPOOL_OVERLOAD_REJECTIONS.inc();
    }
}
```

3. **Increase Worker Pool Size**: Consider increasing `shared_mempool_max_concurrent_inbound_syncs` from 4 to a higher value based on expected load.

4. **Add HTTP Server Timeout**: Configure request timeout in Poem server setup.

## Proof of Concept

```rust
// Integration test demonstrating the DoS
#[tokio::test]
async fn test_mempool_channel_dos() {
    use aptos_rest_client::Client;
    use aptos_types::transaction::SignedTransaction;
    
    // Setup test node
    let (node, client) = setup_test_node().await;
    
    // Phase 1: Fill the channel with 1024 transactions rapidly
    let mut futures = vec![];
    for i in 0..1024 {
        let client = client.clone();
        let txn = create_test_transaction(i);
        futures.push(tokio::spawn(async move {
            client.submit(&txn).await
        }));
    }
    
    // Send all rapidly to fill channel buffer
    let _ = futures::future::join_all(futures).await;
    
    // Phase 2: Send 4 complex transactions to occupy all workers
    for i in 0..4 {
        let txn = create_slow_validation_transaction(i);
        tokio::spawn(async move {
            client.submit(&txn).await
        });
    }
    
    // Phase 3: Verify API is now unresponsive
    let timeout_txn = create_test_transaction(9999);
    let result = tokio::time::timeout(
        Duration::from_secs(10),
        client.submit(&timeout_txn)
    ).await;
    
    // This should timeout, proving DoS
    assert!(result.is_err(), "API should be unresponsive due to channel overflow");
}
```

**Notes**:
- The vulnerability exists in the interaction between bounded channel (1,024 buffer), small worker pool (4 workers), and blocking coordinator loop
- Even though the channel is bounded, the lack of timeout on `send().await` and blocking in coordinator creates exploitable deadlock
- This affects all nodes running the API service, making it a significant availability issue

### Citations

**File:** aptos-node/src/services.rs (L46-70)
```rust
const AC_SMP_CHANNEL_BUFFER_SIZE: usize = 1_024;
const INTRA_NODE_CHANNEL_BUFFER_SIZE: usize = 1;

/// Bootstraps the API and the indexer. Returns the Mempool client
/// receiver, and both the api and indexer runtimes.
pub fn bootstrap_api_and_indexer(
    node_config: &NodeConfig,
    db_rw: DbReaderWriter,
    chain_id: ChainId,
    internal_indexer_db: Option<InternalIndexerDB>,
    update_receiver: Option<WatchReceiver<(Instant, Version)>>,
    api_port_tx: Option<oneshot::Sender<u16>>,
    indexer_grpc_port_tx: Option<oneshot::Sender<u16>>,
) -> anyhow::Result<(
    Receiver<MempoolClientRequest>,
    Option<Runtime>,
    Option<Runtime>,
    Option<Runtime>,
    Option<Runtime>,
    Option<Runtime>,
    MempoolClientSender,
)> {
    // Create the mempool client and sender
    let (mempool_client_sender, mempool_client_receiver) =
        mpsc::channel(AC_SMP_CHANNEL_BUFFER_SIZE);
```

**File:** api/src/context.rs (L217-225)
```rust
    pub async fn submit_transaction(&self, txn: SignedTransaction) -> Result<SubmissionStatus> {
        let (req_sender, callback) = oneshot::channel();
        self.mp_sender
            .clone()
            .send(MempoolClientRequest::SubmitTransaction(txn, req_sender))
            .await?;

        callback.await?
    }
```

**File:** api/src/runtime.rs (L260-263)
```rust
        Server::new_with_acceptor(acceptor)
            .run(route)
            .await
            .map_err(anyhow::Error::msg)
```

**File:** config/src/config/mempool_config.rs (L116-116)
```rust
            shared_mempool_max_concurrent_inbound_syncs: 4,
```

**File:** mempool/src/shared_mempool/coordinator.rs (L92-93)
```rust
    let workers_available = smp.config.shared_mempool_max_concurrent_inbound_syncs;
    let bounded_executor = BoundedExecutor::new(workers_available, executor.clone());
```

**File:** mempool/src/shared_mempool/coordinator.rs (L189-196)
```rust
            bounded_executor
                .spawn(tasks::process_client_transaction_submission(
                    smp.clone(),
                    txn,
                    callback,
                    task_start_timer,
                ))
                .await;
```
