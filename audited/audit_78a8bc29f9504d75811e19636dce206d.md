# Audit Report

## Title
Event Store Pruner Database Inconsistency from Non-Atomic Cross-Database Writes

## Summary
The `EventStorePruner` writes to two separate databases (main event DB and internal indexer DB) in a non-atomic manner. When the internal indexer write succeeds but the main event DB write fails due to persistent infrastructure issues, the system enters an inconsistent state requiring manual intervention to resolve.

## Finding Description

The `EventStorePruner::prune()` method performs pruning across two independent databases when the internal indexer is enabled: [1](#0-0) 

The critical issue occurs in the write sequence:

1. **First Write (Line 76-78)**: Writes to the internal indexer database, including deletion of event indices (`EventByKeySchema`, `EventByVersionSchema`) and updating `EventPrunerProgress` to the target version.

2. **Second Write (Line 80)**: Writes to the main event database, including deletion of events (`EventSchema`) and updating `EventPrunerProgress` to the target version.

These are **two separate, non-atomic database operations**. Each individual write is atomic within its own database via RocksDB's WriteBatch: [2](#0-1) 

However, there is no transaction mechanism spanning both databases.

The retry logic exists at the `PrunerWorker` level: [3](#0-2) 

On retry, the progress is read only from the main event database: [4](#0-3) 

**Failure Scenario:**

1. Initial state: Main DB has events at versions 100-200 (progress=100), Indexer DB has indices for versions 100-200 (progress=100)

2. Pruning attempt: `prune(100, 200)` is called

3. Indexer write **succeeds** (line 78): Indices deleted, progress=200

4. Main DB write **fails** (line 80) due to persistent disk issue (full disk, corruption, permission error): Events NOT deleted, progress=100

5. Error logged, worker retries reading progress from main DB (=100)

6. If the main DB write failure is **persistent** (not transient):
   - Worker continuously retries and fails
   - Indexer DB: progress=200, indices deleted
   - Main DB: progress=100, events NOT deleted
   - **Permanent inconsistency**: events exist without their secondary indices

7. Impact of inconsistency:
   - Query-by-event-key fails (requires `EventByKeySchema`/`EventByVersionSchema` indices from indexer)
   - Query-by-version still works (reads `EventSchema` directly from main DB)
   - Pruning completely stalled
   - Disk space not reclaimed from main DB
   - **Manual intervention required** to either fix the underlying disk issue or reset/rebuild the indexer database

This breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs." While each database is internally consistent, the cross-database state is inconsistent.

## Impact Explanation

This qualifies as **Medium Severity** under the Aptos Bug Bounty criteria: "State inconsistencies requiring intervention."

**Why not higher severity:**
- No permanent data loss occurs (events are preserved in main DB)
- No funds are lost or at risk
- No consensus safety violation
- The node continues functioning for most operations

**Why not lower severity:**
- Requires manual intervention to resolve
- Causes operational disruption (pruning stalled, query failures)
- Creates inconsistent database state that cannot self-heal
- Affects data availability (query-by-key functionality broken)

## Likelihood Explanation

**Likelihood: Low to Medium**

This requires specific conditions:
1. Internal indexer must be enabled (optional feature)
2. A persistent write failure must occur specifically in the main event DB after the indexer write succeeds
3. The failure must not be transient (disk full, corruption, permissions)

**Factors increasing likelihood:**
- Different databases may be on different disks/partitions with different health states
- Disk space exhaustion can affect one database before another
- Production environments with high write volumes increase disk wear

**Factors decreasing likelihood:**
- Modern infrastructure typically has monitoring and alerting
- Transient errors (the common case) are handled correctly by retry logic
- The window for failure is narrow (between two write operations)

## Recommendation

Implement one of the following solutions:

**Option 1: Reverse Write Order (Simplest)**
Write to the main event DB first, then the indexer DB. If the main write fails, neither database is updated. If the indexer write fails after main succeeds, the retry will safely re-prune the same range (events are idempotent to delete).

**Option 2: Progress Consistency Check**
On initialization, check both progress markers and handle mismatches:

```rust
pub(in crate::pruner) fn new(
    ledger_db: Arc<LedgerDb>,
    metadata_progress: Version,
    internal_indexer_db: Option<InternalIndexerDB>,
) -> Result<Self> {
    let main_progress = get_or_initialize_subpruner_progress(
        ledger_db.event_db_raw(),
        &DbMetadataKey::EventPrunerProgress,
        metadata_progress,
    )?;
    
    // Check indexer progress consistency
    if let Some(ref indexer_db) = internal_indexer_db {
        let indexer_progress = indexer_db.get_event_pruner_progress()?;
        if indexer_progress > main_progress {
            warn!(
                main_progress = main_progress,
                indexer_progress = indexer_progress,
                "Indexer pruner progress ahead of main DB, resetting to main DB progress"
            );
            // Reset indexer progress to match main DB
            indexer_db.reset_event_pruner_progress(main_progress)?;
        }
    }
    
    let myself = EventStorePruner {
        ledger_db,
        internal_indexer_db,
    };
    
    myself.prune(main_progress, metadata_progress)?;
    Ok(myself)
}
```

**Option 3: Two-Phase Commit Protocol**
Implement a proper distributed transaction protocol with prepare/commit/rollback phases, though this adds significant complexity.

**Recommended: Option 1** (reverse write order) provides the best tradeoff between safety and simplicity.

## Proof of Concept

This issue can be reproduced with the following test scenario:

```rust
#[test]
fn test_event_pruner_write_failure_inconsistency() {
    // Setup: Create event store pruner with indexer enabled
    let tmpdir = TempPath::new();
    let ledger_db = create_test_ledger_db(&tmpdir);
    let indexer_db = create_test_indexer_db(&tmpdir);
    
    // Write test events at versions 0-99
    populate_test_events(&ledger_db, 0, 100);
    populate_test_event_indices(&indexer_db, 0, 100);
    
    let pruner = EventStorePruner::new(
        Arc::new(ledger_db),
        0,
        Some(indexer_db.clone()),
    ).unwrap();
    
    // Simulate write failure by making main DB read-only
    // (In production, this would be disk full, permissions, etc.)
    ledger_db.set_readonly(true);
    
    // Attempt to prune - indexer write succeeds, main write fails
    let result = pruner.prune(0, 50);
    assert!(result.is_err());
    
    // Verify inconsistent state
    let main_progress = ledger_db.get_event_pruner_progress().unwrap();
    let indexer_progress = indexer_db.get_event_pruner_progress().unwrap();
    
    assert_eq!(main_progress, 0);  // Not updated due to failure
    assert_eq!(indexer_progress, 50);  // Updated successfully
    
    // Verify events still exist in main DB
    for v in 0..50 {
        assert!(ledger_db.get_events_by_version(v).is_ok());
    }
    
    // Verify indices deleted from indexer DB
    for v in 0..50 {
        assert!(indexer_db.get_event_by_key_at_version(v).is_err());
    }
    
    // Retry after fixing main DB still succeeds
    ledger_db.set_readonly(false);
    assert!(pruner.prune(0, 50).is_ok());
    
    // Final state: both DBs consistent again
    assert_eq!(ledger_db.get_event_pruner_progress().unwrap(), 50);
    assert_eq!(indexer_db.get_event_pruner_progress().unwrap(), 50);
}
```

## Notes

**Mitigation in Current Code:**
- Transient errors (disk busy, temporary I/O issues) are correctly handled by the retry logic at the worker level
- The retry mechanism successfully resolves most failure cases
- Data is never permanently lost (events remain in main DB)

**Remaining Risk:**
- Persistent infrastructure failures create unrecoverable inconsistency without manual intervention
- No automatic detection or recovery mechanism for cross-database progress mismatches
- Operators must manually diagnose and fix the inconsistency

**Related Code Paths:**
All other ledger sub-pruners follow the same pattern and may have similar issues if they write to multiple databases.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L43-81)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        let mut indexer_batch = None;

        let indices_batch = if let Some(indexer_db) = self.indexer_db() {
            if indexer_db.event_enabled() {
                indexer_batch = Some(SchemaBatch::new());
            }
            indexer_batch.as_mut()
        } else {
            Some(&mut batch)
        };
        let num_events_per_version = self.ledger_db.event_db().prune_event_indices(
            current_progress,
            target_version,
            indices_batch,
        )?;
        self.ledger_db.event_db().prune_events(
            num_events_per_version,
            current_progress,
            target_version,
            &mut batch,
        )?;
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::EventPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        if let Some(mut indexer_batch) = indexer_batch {
            indexer_batch.put::<InternalIndexerMetadataSchema>(
                &IndexerMetadataKey::EventPrunerProgress,
                &IndexerMetadataValue::Version(target_version),
            )?;
            self.expect_indexer_db()
                .get_inner_db_ref()
                .write_schemas(indexer_batch)?;
        }
        self.ledger_db.event_db().write_schemas(batch)
    }
```

**File:** storage/schemadb/src/lib.rs (L289-309)
```rust
    fn write_schemas_inner(&self, batch: impl IntoRawBatch, option: &WriteOptions) -> DbResult<()> {
        let labels = [self.name.as_str()];
        let _timer = APTOS_SCHEMADB_BATCH_COMMIT_LATENCY_SECONDS.timer_with(&labels);

        let raw_batch = batch.into_raw_batch(self)?;

        let serialized_size = raw_batch.inner.size_in_bytes();
        self.inner
            .write_opt(raw_batch.inner, option)
            .into_db_res()?;

        raw_batch.stats.commit();
        APTOS_SCHEMADB_BATCH_COMMIT_BYTES.observe_with(&[&self.name], serialized_size as f64);

        Ok(())
    }

    /// Writes a group of records wrapped in a [`SchemaBatch`].
    pub fn write_schemas(&self, batch: impl IntoRawBatch) -> DbResult<()> {
        self.write_schemas_inner(batch, &sync_write_option())
    }
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L52-69)
```rust
    // Loop that does the real pruning job.
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
    }
```

**File:** storage/aptosdb/src/pruner/pruner_utils.rs (L44-60)
```rust
pub(crate) fn get_or_initialize_subpruner_progress(
    sub_db: &DB,
    progress_key: &DbMetadataKey,
    metadata_progress: Version,
) -> Result<Version> {
    Ok(
        if let Some(v) = sub_db.get::<DbMetadataSchema>(progress_key)? {
            v.expect_version()
        } else {
            sub_db.put::<DbMetadataSchema>(
                progress_key,
                &DbMetadataValue::Version(metadata_progress),
            )?;
            metadata_progress
        },
    )
}
```
