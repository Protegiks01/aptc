# Audit Report

## Title
Remote Sharding Deployment Without Total Supply Aggregation Causes Immediate Consensus Failure

## Summary
The temporary workaround in `AggregatorOverriddenStateView` for handling total supply during sharded execution is **not removed** before remote sharding deployment, causing all remote shards to commit transaction outputs with incorrect total supply values. This breaks deterministic execution across validators, leading to state root divergence and immediate consensus failure requiring emergency hard fork.

## Finding Description

The TODO comment indicates a temporary implementation for total supply handling during sharded execution: [1](#0-0) 

During sharded execution, `AggregatorOverriddenStateView` overrides all reads of the total supply state key to return a fixed base value (`TOTAL_SUPPLY_AGGR_BASE_VAL = u128::MAX >> 1`) instead of the actual state value: [2](#0-1) 

This overridden view is used by all shards during transaction execution: [3](#0-2) 

**Critical Divergence Between Local and Remote Sharding:**

In **local sharding** (all shards in same process), the `LocalExecutorClient` correctly aggregates and fixes the total supply values BEFORE returning results: [4](#0-3) 

However, in **remote sharding** (shards on different machines), the `RemoteExecutorClient` completely **omits** this aggregation step: [5](#0-4) 

The aggregation function computes deltas from the fake base value and adjusts all transaction outputs to contain correct total supply values: [6](#0-5) 

**The Vulnerability:**

When remote sharding is enabled without removing this temporary implementation:

1. Each remote shard executes transactions with total supply reads returning `TOTAL_SUPPLY_AGGR_BASE_VAL` (u128::MAX >> 1)
2. Transaction write sets contain total supply values based on this fake base value
3. `RemoteExecutorClient` returns these incorrect outputs directly **without aggregation**
4. The coordinator in `ShardedBlockExecutor::execute_block` simply reorders results without any correction: [7](#0-6) 

5. These incorrect total supply values get written to the Jellyfish Merkle tree
6. State root calculation includes these wrong values
7. Different validators compute different state roots
8. Consensus breaks - validators cannot agree on block commitments

**Invariant Violations:**

- **Deterministic Execution (Invariant #1)**: Validators no longer produce identical state roots for identical blocks
- **Consensus Safety (Invariant #2)**: AptosBFT cannot reach agreement on state transitions

## Impact Explanation

**CRITICAL Severity** - This meets the highest severity category per Aptos bug bounty:

- **Consensus/Safety violations**: Direct violation of consensus safety - validators cannot agree on state roots
- **Non-recoverable network partition (requires hardfork)**: Once blocks with incorrect total supply are committed, the chain state is corrupted and requires emergency hard fork to recover
- **Total loss of liveness/network availability**: Network cannot make progress as validators disagree on every block

The total supply state key is fundamental to the Aptos token economics: [8](#0-7) 

Every transaction that modifies token balances updates this value. With incorrect values committed to state, the entire economic integrity of the chain is compromised.

## Likelihood Explanation

**Likelihood: HIGH** - This vulnerability triggers **automatically** upon enabling remote sharding, requiring no attacker action:

1. Remote sharding is already implemented and can be enabled via configuration flags: [9](#0-8) 

2. The code path is deterministic - any block execution with remote sharding will produce incorrect results
3. No special transaction crafting or validator collusion required
4. The TODO comment explicitly warns this must be removed before remote sharding deployment
5. The aggregation logic exists but is simply not called in the remote path

## Recommendation

**Immediate Action:** Do NOT enable remote sharding until this is fixed.

**Fix:** Add the missing aggregation call in `RemoteExecutorClient::execute_block`:

```rust
// In execution/executor-service/src/remote_executor_client.rs
impl<S: StateView + Sync + Send + 'static> ExecutorClient<S> for RemoteExecutorClient<S> {
    fn execute_block(
        &self,
        state_view: Arc<S>,
        transactions: PartitionedTransactions,
        concurrency_level_per_shard: usize,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<ShardedExecutionOutput, VMStatus> {
        // ... existing code ...
        
        let mut execution_results = self.get_output_from_shards()?;
        let mut global_output = vec![]; // TODO: support global transactions
        
        // ADD THIS: Aggregate and fix total supply values
        aptos_vm::sharded_block_executor::sharded_aggregator_service::aggregate_and_update_total_supply(
            &mut execution_results,
            &mut global_output,
            state_view.as_ref(),
            self.thread_pool.clone(),
        );
        
        self.state_view_service.drop_state_view();
        Ok(ShardedExecutionOutput::new(execution_results, global_output))
    }
}
```

**Long-term Solution:** Implement proper aggregated total supply handling for remote sharding as intended by the TODO comment, eliminating the need for post-execution correction.

## Proof of Concept

```rust
// Integration test demonstrating the vulnerability
#[test]
fn test_remote_sharding_total_supply_divergence() {
    // Setup: Create a remote sharded executor with 2 shards
    let remote_addresses = vec![
        "127.0.0.1:52201".parse().unwrap(),
        "127.0.0.1:52202".parse().unwrap(),
    ];
    let coordinator_addr = "127.0.0.1:52200".parse().unwrap();
    
    let remote_executor = RemoteExecutorClient::create_remote_sharded_block_executor(
        coordinator_addr,
        remote_addresses,
        None,
    );
    
    // Create test block with transactions that modify total supply
    let transactions = create_token_transfer_transactions(100);
    let partitioned = partition_transactions(transactions, 2);
    
    // Execute block through remote sharding
    let state_view = create_test_state_view();
    let result = remote_executor.execute_block(
        Arc::new(state_view),
        partitioned,
        4,
        BlockExecutorConfigFromOnchain::default(),
    ).unwrap();
    
    // VULNERABILITY: Check that total supply values are INCORRECT
    for output in result.inner().0.iter().flatten().flatten() {
        if let Some(total_supply) = output.write_set().get_total_supply() {
            // These values will be based on TOTAL_SUPPLY_AGGR_BASE_VAL
            // instead of actual state, causing consensus failure
            assert_ne!(total_supply, get_expected_total_supply());
            
            // Verify it's using the fake base value
            let base_val = TOTAL_SUPPLY_AGGR_BASE_VAL;
            assert!(total_supply > base_val - 1000 && total_supply < base_val + 1000);
        }
    }
    
    // Two validators executing the same block will compute different state roots
    // because the total supply values in the write set are incorrect
}
```

## Notes

This vulnerability exists because the aggregation logic was implemented only for local sharding (same-process execution) but not ported to remote sharding (distributed execution). The TODO comment at line 43-45 explicitly indicates this is temporary, but without proper aggregation in the remote path, enabling remote sharding would immediately break consensus across all validators. This represents a **critical deployment gate** that must not be crossed until fixed.

### Citations

**File:** aptos-move/aptos-vm/src/sharded_block_executor/aggr_overridden_state_view.rs (L41-50)
```rust
    fn get_state_value(&self, state_key: &StateKey) -> Result<Option<StateValue>> {
        if *state_key == *TOTAL_SUPPLY_STATE_KEY {
            // TODO: Remove this when we have aggregated total supply implementation for remote
            //       sharding. For now we need this because after all the txns are executed, the
            //       proof checker expects the total_supply to read/written to the tree.
            self.base_view.get_state_value(state_key)?;
            return self.total_supply_base_view_override();
        }
        self.base_view.get_state_value(state_key)
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L123-126)
```rust
        let aggr_overridden_state_view = Arc::new(AggregatorOverriddenStateView::new(
            cross_shard_state_view.as_ref(),
            TOTAL_SUPPLY_AGGR_BASE_VAL,
        ));
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L215-220)
```rust
        sharded_aggregator_service::aggregate_and_update_total_supply(
            &mut sharded_output,
            &mut global_output,
            state_view.as_ref(),
            self.global_executor.get_executor_thread_pool(),
        );
```

**File:** execution/executor-service/src/remote_executor_client.rs (L57-72)
```rust
pub static REMOTE_SHARDED_BLOCK_EXECUTOR: Lazy<
    Arc<
        aptos_infallible::Mutex<
            ShardedBlockExecutor<CachedStateView, RemoteExecutorClient<CachedStateView>>,
        >,
    >,
> = Lazy::new(|| {
    info!("REMOTE_SHARDED_BLOCK_EXECUTOR created");
    Arc::new(aptos_infallible::Mutex::new(
        RemoteExecutorClient::create_remote_sharded_block_executor(
            get_coordinator_address(),
            get_remote_addresses(),
            None,
        ),
    ))
});
```

**File:** execution/executor-service/src/remote_executor_client.rs (L180-212)
```rust
    fn execute_block(
        &self,
        state_view: Arc<S>,
        transactions: PartitionedTransactions,
        concurrency_level_per_shard: usize,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<ShardedExecutionOutput, VMStatus> {
        trace!("RemoteExecutorClient Sending block to shards");
        self.state_view_service.set_state_view(state_view);
        let (sub_blocks, global_txns) = transactions.into();
        if !global_txns.is_empty() {
            panic!("Global transactions are not supported yet");
        }
        for (shard_id, sub_blocks) in sub_blocks.into_iter().enumerate() {
            let senders = self.command_txs.clone();
            let execution_request = RemoteExecutionRequest::ExecuteBlock(ExecuteBlockCommand {
                sub_blocks,
                concurrency_level: concurrency_level_per_shard,
                onchain_config: onchain_config.clone(),
            });

            senders[shard_id]
                .lock()
                .unwrap()
                .send(Message::new(bcs::to_bytes(&execution_request).unwrap()))
                .unwrap();
        }

        let execution_results = self.get_output_from_shards()?;

        self.state_view_service.drop_state_view();
        Ok(ShardedExecutionOutput::new(execution_results, vec![]))
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_aggregator_service.rs (L168-220)
```rust
pub fn aggregate_and_update_total_supply<S: StateView>(
    sharded_output: &mut Vec<Vec<Vec<TransactionOutput>>>,
    global_output: &mut [TransactionOutput],
    state_view: &S,
    executor_thread_pool: Arc<rayon::ThreadPool>,
) {
    let num_shards = sharded_output.len();
    let num_rounds = sharded_output[0].len();

    // The first element is 0, which is the delta for shard 0 in round 0. +1 element will contain
    // the delta for the global shard
    let mut aggr_total_supply_delta = vec![DeltaU128::default(); num_shards * num_rounds + 1];

    // No need to parallelize this as the runtime is O(num_shards * num_rounds)
    // TODO: Get this from the individual shards while getting 'sharded_output'
    let mut aggr_ts_idx = 1;
    for round in 0..num_rounds {
        sharded_output.iter().for_each(|shard_output| {
            let mut curr_delta = DeltaU128::default();
            // Though we expect all the txn_outputs to have total_supply, there can be
            // exceptions like 'block meta' (first txn in the block) and 'chkpt info' (last txn
            // in the block) which may not have total supply. Hence we iterate till we find the
            // last txn with total supply.
            for txn in shard_output[round].iter().rev() {
                if let Some(last_txn_total_supply) = txn.write_set().get_total_supply() {
                    curr_delta =
                        DeltaU128::get_delta(last_txn_total_supply, TOTAL_SUPPLY_AGGR_BASE_VAL);
                    break;
                }
            }
            aggr_total_supply_delta[aggr_ts_idx] =
                curr_delta + aggr_total_supply_delta[aggr_ts_idx - 1];
            aggr_ts_idx += 1;
        });
    }

    // The txn_outputs contain 'txn_total_supply' with
    // 'CrossShardStateViewAggrOverride::total_supply_aggr_base_val' as the base value.
    // The actual 'total_supply_base_val' is in the state_view.
    // The 'delta' for the shard/round is in aggr_total_supply_delta[round * num_shards + shard_id + 1]
    // For every txn_output, we have to compute
    //      txn_total_supply = txn_total_supply - CrossShardStateViewAggrOverride::total_supply_aggr_base_val + total_supply_base_val + delta
    // While 'txn_total_supply' is u128, the intermediate computation can be negative. So we use
    // DeltaU128 to handle any intermediate underflow of u128.
    let total_supply_base_val: u128 = get_state_value(&TOTAL_SUPPLY_STATE_KEY, state_view).unwrap();
    let base_val_delta = DeltaU128::get_delta(total_supply_base_val, TOTAL_SUPPLY_AGGR_BASE_VAL);

    let aggr_total_supply_delta_ref = &aggr_total_supply_delta;
    // Runtime is O(num_txns), hence parallelized at the shard level and at the txns level.
    executor_thread_pool.scope(|_| {
        sharded_output
            .par_iter_mut()
            .enumerate()
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/mod.rs (L86-115)
```rust
        let (sharded_output, global_output) = self
            .executor_client
            .execute_block(
                state_view,
                transactions,
                concurrency_level_per_shard,
                onchain_config,
            )?
            .into_inner();
        // wait for all remote executors to send the result back and append them in order by shard id
        info!("ShardedBlockExecutor Received all results");
        let _aggregation_timer = SHARDED_EXECUTION_RESULT_AGGREGATION_SECONDS.start_timer();
        let num_rounds = sharded_output[0].len();
        let mut aggregated_results = vec![];
        let mut ordered_results = vec![vec![]; num_executor_shards * num_rounds];
        // Append the output from individual shards in the round order
        for (shard_id, results_from_shard) in sharded_output.into_iter().enumerate() {
            for (round, result) in results_from_shard.into_iter().enumerate() {
                ordered_results[round * num_executor_shards + shard_id] = result;
            }
        }

        for result in ordered_results.into_iter() {
            aggregated_results.extend(result);
        }

        // Lastly append the global output
        aggregated_results.extend(global_output);

        Ok(aggregated_results)
```

**File:** types/src/write_set.rs (L27-37)
```rust
pub static TOTAL_SUPPLY_STATE_KEY: Lazy<StateKey> = Lazy::new(|| {
    StateKey::table_item(
        &"1b854694ae746cdbd8d44186ca4929b2b337df21d1c74633be19b2710552fdca"
            .parse()
            .unwrap(),
        &[
            6, 25, 220, 41, 160, 170, 200, 250, 20, 103, 20, 5, 142, 141, 214, 210, 208, 243, 189,
            245, 246, 51, 25, 7, 191, 145, 243, 172, 216, 30, 105, 53,
        ],
    )
});
```
