# Audit Report

## Title
State KV Database Shard Version Misalignment During Truncation Leads to Potential Node Availability Failure

## Summary
The `truncate_state_kv_db()` function in `storage/aptosdb/src/utils/truncation_helper.rs` writes the overall commit progress **before** truncating shards, and uses parallel shard truncation without atomic commit guarantees. This design allows partial truncation failures to create persistent shard version misalignment, potentially causing validator nodes to enter an unrecoverable crash loop.

## Finding Description

The truncation logic violates atomicity guarantees through an incorrect ordering of operations: [1](#0-0) 

The code writes the overall progress marker **first** (line 101), then attempts to truncate all shards in parallel (line 106). The parallel shard truncation uses `try_for_each()` from rayon: [2](#0-1) 

Each shard is independently truncated and commits its own progress marker: [3](#0-2) 

**Failure Scenario:**
1. Overall `StateKvCommitProgress` is written as version 100
2. Shard 0 truncates successfully: deletes data 101-150, commits shard progress = 100  
3. Shard 1 fails during `commit_single_shard()` due to I/O error, disk full, or corruption
4. Shards 2-15 may complete or fail depending on timing
5. `try_for_each()` returns error, propagating to caller

**Resulting On-Disk State:**
- Overall progress: 100 (already written!)
- Shard 0: truncated to 100, `StateKvShardCommitProgress(0)` = 100
- Shard 1: NOT truncated, data still exists at versions 101-150
- Other shards: variable states

This violates the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs."

**Crash Loop Impact:**

The truncation function is called during node startup via `sync_commit_progress()`: [4](#0-3) 

When truncation fails, line 467 calls `.expect()` which **panics and crashes the node**. On restart:

1. Recovery attempts to truncate shards again
2. If the failure is **persistent** (e.g., disk corruption in shard 1), the same error occurs
3. Node panics again
4. **Infinite crash loop** - node cannot start

Additionally, the StateKvDb constructor has its own recovery mechanism: [5](#0-4) 

If this also fails due to the same persistent error, the node cannot initialize.

**Comparison with Normal Commit Flow:**

The normal commit operation does the opposite - commits shards **first**, then writes progress: [6](#0-5) 

Shards are committed in parallel (lines 186-200), and if any shard fails, the process **panics** (line 196). The overall progress is only written **after** all shards succeed (line 207). This ensures atomicity.

## Impact Explanation

**Severity: High**

This qualifies as **High Severity** per Aptos bug bounty criteria: "Validator node slowdowns, API crashes, Significant protocol violations."

**Specific Impacts:**

1. **Node Availability Failure**: If disk corruption or persistent I/O errors affect one shard, the validator node enters an unrecoverable crash loop, unable to start or participate in consensus.

2. **Network Liveness Risk**: If multiple validators experience this simultaneously (e.g., during coordinated database maintenance or if a common storage infrastructure issue affects multiple nodes), the network could lose quorum and halt.

3. **Database Integrity**: The on-disk database contains inconsistent shard versions between crashes, complicating forensic analysis, backup/restore operations, and manual recovery.

4. **Operational Burden**: Requires manual database repair or restoration from backup to recover, causing extended downtime.

While this does not directly enable consensus safety violations (the node crashes rather than producing incorrect state roots), it represents a **significant protocol violation** by making validator nodes unable to fulfill their consensus duties.

## Likelihood Explanation

**Likelihood: Medium-High**

This issue will occur whenever:
- Disk I/O errors happen during truncation operations
- Disk space exhaustion occurs mid-truncation
- Storage hardware failures affect specific shards
- File system corruption impacts shard databases
- Database lock conflicts occur during parallel writes

The truncation operation runs:
1. At every node startup via `sync_commit_progress()`
2. During manual database maintenance operations
3. As part of state synchronization recovery

Given that AptosDB uses RocksDB with 16 independent shard databases, storage I/O failures are a realistic operational concern, especially at scale.

## Recommendation

**Fix: Implement Atomic Truncation with Progress-Last Ordering**

The truncation logic should follow the same pattern as normal commits:

1. **Truncate all shards first** (with failure rollback)
2. **Only write overall progress after all shards succeed**
3. **Use atomic batch commits** where possible

**Proposed Code Fix:**

```rust
pub(crate) fn truncate_state_kv_db(
    state_kv_db: &StateKvDb,
    current_version: Version,
    target_version: Version,
    batch_size: usize,
) -> Result<()> {
    assert!(batch_size > 0);
    let status = StatusLine::new(Progress::new("Truncating State KV DB", target_version));
    status.set_current_version(current_version);

    let mut current_version = current_version;
    loop {
        let target_version_for_this_batch = std::cmp::max(
            current_version.saturating_sub(batch_size as Version),
            target_version,
        );
        
        // CHANGED: Truncate shards FIRST
        truncate_state_kv_db_shards(state_kv_db, target_version_for_this_batch)?;
        
        // CHANGED: Write progress LAST, only after all shards succeed
        state_kv_db.write_progress(target_version_for_this_batch)?;
        
        current_version = target_version_for_this_batch;
        status.set_current_version(current_version);

        if current_version <= target_version {
            break;
        }
    }
    assert_eq!(current_version, target_version);
    Ok(())
}
```

**Additional Safety Enhancement:**

Consider adding transactional semantics with rollback capability if any shard fails, similar to how normal commits use `.unwrap_or_else(|err| panic!())` to ensure all-or-nothing behavior.

## Proof of Concept

**Simulation Steps:**

1. **Setup**: Create a test AptosDB with sharded StateKvDb containing data at versions 0-150
2. **Inject Failure**: Mock the `commit_single_shard()` function to fail for shard_id=1 with an I/O error
3. **Trigger Truncation**: Call `truncate_state_kv_db()` to truncate to version 100
4. **Observe State**:
   - Overall `StateKvCommitProgress` = 100
   - Shard 0: `StateKvShardCommitProgress(0)` = 100, no data beyond version 100
   - Shard 1: No progress marker update, data still exists at versions 101-150
5. **Simulate Restart**: Call `StateStore::sync_commit_progress()` 
6. **Expected Result**: Node enters crash loop as shard 1 continues to fail

**Rust Test Skeleton:**

```rust
#[test]
fn test_truncation_shard_misalignment() {
    // Create test DB with 16 shards, data at versions 0-150
    let tmp_dir = TempPath::new();
    let db = AptosDB::new_for_test_with_sharding(&tmp_dir, 0);
    
    // Commit transactions to version 150
    for v in 0..150 {
        db.save_transactions_for_test(/* ... */, v, None, true).unwrap();
    }
    
    // Mock shard 1 to fail on next commit
    inject_io_error_for_shard(&db, 1);
    
    // Attempt truncation to version 100
    let result = truncate_state_kv_db(&db.state_kv_db, 150, 100, 50);
    assert!(result.is_err());
    
    // Verify misalignment
    let overall_progress = get_state_kv_commit_progress(&db.state_kv_db).unwrap().unwrap();
    assert_eq!(overall_progress, 100);
    
    let shard_0_progress = db.state_kv_db.db_shard(0)
        .get::<DbMetadataSchema>(&DbMetadataKey::StateKvShardCommitProgress(0))
        .unwrap().unwrap().expect_version();
    assert_eq!(shard_0_progress, 100);
    
    let shard_1_data_exists = db.state_kv_db.db_shard(1)
        .get::<StateValueByKeyHashSchema>(/* key at version 101 */)
        .unwrap().is_some();
    assert!(shard_1_data_exists, "Shard 1 should still have data beyond version 100");
    
    // Verify crash loop on restart
    drop(db);
    let restart_result = AptosDB::new_for_test_with_sharding(&tmp_dir, 0);
    // Should panic during StateStore::sync_commit_progress()
}
```

## Notes

This vulnerability demonstrates a critical design flaw where the atomicity assumption for distributed storage operations was violated. The write-progress-first approach was likely intended to maintain a conservative invariant ("progress <= actual shard state"), but it creates a worse failure mode where partial truncation leaves the database in an inconsistent state requiring manual intervention.

The issue is exacerbated by the lack of transactional semantics across shards - RocksDB batch commits are atomic per-shard, but there's no two-phase commit or rollback mechanism across all 16 shards.

### Citations

**File:** storage/aptosdb/src/utils/truncation_helper.rs (L99-106)
```rust
        // By writing the progress first, we still maintain that it is less than or equal to the
        // actual progress per shard, even if it dies in the middle of truncation.
        state_kv_db.write_progress(target_version_for_this_batch)?;
        // the first batch can actually delete more versions than the target batch size because
        // we calculate the start version of this batch assuming the latest data is at
        // `current_version`. Otherwise, we need to seek all shards to determine the
        // actual latest version of data.
        truncate_state_kv_db_shards(state_kv_db, target_version_for_this_batch)?;
```

**File:** storage/aptosdb/src/utils/truncation_helper.rs (L118-127)
```rust
pub(crate) fn truncate_state_kv_db_shards(
    state_kv_db: &StateKvDb,
    target_version: Version,
) -> Result<()> {
    (0..state_kv_db.hack_num_real_shards())
        .into_par_iter()
        .try_for_each(|shard_id| {
            truncate_state_kv_db_single_shard(state_kv_db, shard_id, target_version)
        })
}
```

**File:** storage/aptosdb/src/utils/truncation_helper.rs (L129-142)
```rust
pub(crate) fn truncate_state_kv_db_single_shard(
    state_kv_db: &StateKvDb,
    shard_id: usize,
    target_version: Version,
) -> Result<()> {
    let mut batch = SchemaBatch::new();
    delete_state_value_and_index(
        state_kv_db.db_shard(shard_id),
        target_version + 1,
        &mut batch,
        state_kv_db.enabled_sharding(),
    )?;
    state_kv_db.commit_single_shard(target_version, shard_id, batch)
}
```

**File:** storage/aptosdb/src/state_store/mod.rs (L461-467)
```rust
            truncate_state_kv_db(
                &state_kv_db,
                state_kv_commit_progress,
                overall_commit_progress,
                std::cmp::max(difference as usize, 1), /* batch_size */
            )
            .expect("Failed to truncate state K/V db.");
```

**File:** storage/aptosdb/src/state_kv_db.rs (L164-168)
```rust
        if !readonly {
            if let Some(overall_kv_commit_progress) = get_state_kv_commit_progress(&state_kv_db)? {
                truncate_state_kv_db_shards(&state_kv_db, overall_kv_commit_progress)?;
            }
        }
```

**File:** storage/aptosdb/src/state_kv_db.rs (L186-207)
```rust
            THREAD_MANAGER.get_io_pool().scope(|s| {
                let mut batches = sharded_state_kv_batches.into_iter();
                for shard_id in 0..NUM_STATE_SHARDS {
                    let state_kv_batch = batches
                        .next()
                        .expect("Not sufficient number of sharded state kv batches");
                    s.spawn(move |_| {
                        // TODO(grao): Consider propagating the error instead of panic, if necessary.
                        self.commit_single_shard(version, shard_id, state_kv_batch)
                            .unwrap_or_else(|err| {
                                panic!("Failed to commit shard {shard_id}: {err}.")
                            });
                    });
                }
            });
        }
        if let Some(batch) = state_kv_metadata_batch {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_db__commit_metadata"]);
            self.state_kv_metadata_db.write_schemas(batch)?;
        }

        self.write_progress(version)
```
