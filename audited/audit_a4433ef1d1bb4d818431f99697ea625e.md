# Audit Report

## Title
Remote Executor Lacks Per-Peer Rate Limiting Enabling Targeted DoS Attacks

## Summary
The remote executor system lacks per-peer rate limiting on incoming messages, allowing a single compromised remote executor shard to flood other shards or the coordinator with unlimited execution commands or state view requests. This causes memory exhaustion through unbounded message queues and CPU exhaustion from processing malicious messages, leading to targeted denial-of-service attacks against specific validator nodes.

## Finding Description

The remote executor architecture enables distributed execution of transaction blocks across multiple shards via network communication. However, the implementation contains a critical security flaw: **no rate limiting exists on messages received from remote peers**.

The vulnerability manifests through several interconnected components:

**1. Unbounded Channel Creation**

The `NetworkController` creates all message channels using unbounded crossbeam channels: [1](#0-0) [2](#0-1) 

**2. No Rate Limiting in GRPC Message Handler**

When messages arrive via GRPC, they are immediately forwarded to registered handlers without any rate limiting checks: [3](#0-2) 

**3. Remote Coordinator Client Blocks Without Limits**

The `RemoteCoordinatorClient` receives execution commands through an unbounded channel and blocks waiting for messages indefinitely: [4](#0-3) 

**4. Sharded Executor Service Processing Loop**

The service processes messages in an infinite loop without any throttling or backpressure mechanisms: [5](#0-4) 

**5. State View Service Vulnerable to Flooding**

The `RemoteStateViewService` similarly processes KV requests in an unbounded loop: [6](#0-5) 

**Attack Scenario:**

1. Attacker compromises or operates a malicious remote executor shard
2. Attacker creates a malicious client that continuously sends:
   - `ExecuteBlockCommand` messages to target shards [7](#0-6) 
   - `RemoteKVRequest` messages to the coordinator's state view service
3. Target's unbounded channels accumulate messages without limit, consuming memory
4. Target's CPU is overwhelmed processing or queueing malicious messages
5. Legitimate execution requests are delayed or dropped entirely
6. Target node experiences severe slowdown or complete service disruption

**Broken Invariant:** This violates Critical Invariant #9: "Resource Limits: All operations must respect gas, storage, and computational limits." The unbounded channels allow unlimited memory consumption without any resource controls.

While a general `TokenBucketRateLimiter` exists in the codebase, it is **not integrated** with the remote executor system: [8](#0-7) 

## Impact Explanation

This vulnerability qualifies as **HIGH severity** according to the Aptos Bug Bounty program criteria:

- **Validator node slowdowns**: Unbounded message queues cause memory pressure and CPU exhaustion
- **API crashes**: Service becomes unresponsive or crashes due to resource exhaustion
- **Targeted attacks**: Attacker can selectively DoS specific shards to disrupt execution

The impact is significant because:
- Remote executors are critical infrastructure for distributed transaction execution
- A single compromised shard can attack multiple targets simultaneously
- Attack requires minimal resources from attacker but causes maximum damage to victim
- No authentication or authorization prevents arbitrary senders from flooding messages
- Memory exhaustion can lead to node crashes requiring operator intervention

## Likelihood Explanation

The likelihood of exploitation is **HIGH** because:

1. **Low attack complexity**: Simply sending GRPC messages to exposed endpoints
2. **No authentication required**: Any entity that can reach the remote executor ports can attack
3. **Minimal attacker requirements**: No special privileges or insider access needed
4. **High attacker motivation**: DoS attacks against validator infrastructure are valuable
5. **Architectural exposure**: Remote executor endpoints must be network-accessible for legitimate operation

The attack is particularly concerning because:
- Remote executor architecture is designed for multi-shard deployment
- Each shard trusts messages from other shards without verification
- No mechanism exists to identify or block malicious senders
- Unbounded channels provide no backpressure to slow attackers

## Recommendation

Implement comprehensive rate limiting and resource controls for the remote executor system:

**1. Integrate Token Bucket Rate Limiter Per Peer**

Add per-peer rate limiting in the GRPC message handler:

```rust
// In grpc_network_service/mod.rs
use aptos_rate_limiter::rate_limit::TokenBucketRateLimiter;
use std::net::SocketAddr;

pub struct GRPCNetworkMessageServiceServerWrapper {
    inbound_handlers: Arc<Mutex<HashMap<MessageType, Sender<Message>>>>,
    self_addr: SocketAddr,
    rate_limiter: Arc<TokenBucketRateLimiter<SocketAddr>>, // Add this
}

#[tonic::async_trait]
impl NetworkMessageService for GRPCNetworkMessageServiceServerWrapper {
    async fn simple_msg_exchange(
        &self,
        request: Request<NetworkMessage>,
    ) -> Result<Response<Empty>, Status> {
        let remote_addr = request.remote_addr()
            .ok_or_else(|| Status::unauthenticated("No remote address"))?;
        
        // Rate limit per peer
        let bucket = self.rate_limiter.bucket(remote_addr);
        if bucket.lock().acquire_all_tokens(1).is_err() {
            return Err(Status::resource_exhausted("Rate limit exceeded"));
        }
        
        // ... rest of handler
    }
}
```

**2. Replace Unbounded Channels with Bounded Channels**

Use bounded channels to provide backpressure:

```rust
// In network_controller/mod.rs
pub fn create_outbound_channel(
    &mut self,
    remote_peer_addr: SocketAddr,
    message_type: String,
) -> Sender<Message> {
    let (outbound_sender, outbound_receiver) = bounded(1000); // Bounded channel
    // ... rest of implementation
}
```

**3. Add Message Authentication**

Implement cryptographic authentication for remote executor messages:
- Sign messages with shard private keys
- Verify signatures before processing
- Maintain allowlist of authorized shard addresses

**4. Implement Connection-Level Throttling**

Add connection limits and per-connection message quotas in the GRPC server configuration.

## Proof of Concept

```rust
// PoC demonstrating unbounded message flooding
// File: execution/executor-service/src/tests/dos_poc.rs

#[cfg(test)]
mod dos_attack_test {
    use super::*;
    use std::net::{IpAddr, Ipv4Addr, SocketAddr};
    use std::thread;
    use std::time::Duration;
    use aptos_secure_net::network_controller::{NetworkController, Message};
    use aptos_config::utils;
    
    #[test]
    #[ignore] // Remove to run the attack simulation
    fn test_remote_executor_flooding_dos() {
        // Setup victim remote executor service
        let victim_port = utils::get_available_port();
        let victim_addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), victim_port);
        
        let mut victim_controller = NetworkController::new(
            "victim".to_string(), 
            victim_addr, 
            5000
        );
        let victim_rx = victim_controller.create_inbound_channel("execute_command_0".to_string());
        victim_controller.start();
        
        // Setup malicious attacker
        let attacker_port = utils::get_available_port();
        let attacker_addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), attacker_port);
        
        let mut attacker_controller = NetworkController::new(
            "attacker".to_string(),
            attacker_addr,
            5000
        );
        let attacker_tx = attacker_controller.create_outbound_channel(
            victim_addr,
            "execute_command_0".to_string()
        );
        attacker_controller.start();
        
        thread::sleep(Duration::from_millis(100)); // Wait for servers to start
        
        // Launch flooding attack - send 10,000 messages rapidly
        let flood_count = 10000;
        let flood_thread = thread::spawn(move || {
            for i in 0..flood_count {
                let malicious_payload = vec![0u8; 1024]; // 1KB payload
                attacker_tx.send(Message::new(malicious_payload)).unwrap();
                
                if i % 1000 == 0 {
                    println!("Sent {} malicious messages", i);
                }
            }
        });
        
        // Victim attempts to process messages - will experience severe slowdown
        let mut received_count = 0;
        let start_time = std::time::Instant::now();
        
        // Try to receive just 100 messages - should be instant but will be delayed
        for _ in 0..100 {
            victim_rx.recv().unwrap();
            received_count += 1;
        }
        
        let elapsed = start_time.elapsed();
        flood_thread.join().unwrap();
        
        println!("Victim processed {} messages in {:?}", received_count, elapsed);
        println!("Remaining messages in queue: {}", 
                 flood_count - received_count);
        
        // Demonstrates:
        // 1. Unbounded channel accepts all 10,000 messages
        // 2. Memory consumption grows without limit
        // 3. Victim experiences processing delays
        // 4. No rate limiting prevents the attack
        
        assert!(elapsed > Duration::from_secs(1), 
                "Victim should experience significant slowdown");
    }
}
```

## Notes

The vulnerability is particularly severe because:

1. The remote executor system is designed for production use in distributed validator setups
2. The attack requires no special privileges - any network peer can flood messages
3. The codebase already contains a rate limiter implementation but it's not integrated
4. Multiple attack vectors exist (execution commands, KV requests, cross-shard messages)
5. The unbounded channels provide no memory protection or backpressure mechanisms

This finding demonstrates a fundamental architectural flaw where distributed system security was not prioritized during the remote executor design. The fix requires comprehensive integration of rate limiting, authentication, and resource controls throughout the message handling pipeline.

### Citations

**File:** secure/net/src/network_controller/mod.rs (L115-126)
```rust
    pub fn create_outbound_channel(
        &mut self,
        remote_peer_addr: SocketAddr,
        message_type: String,
    ) -> Sender<Message> {
        let (outbound_sender, outbound_receiver) = unbounded();

        self.outbound_handler
            .register_handler(message_type, remote_peer_addr, outbound_receiver);

        outbound_sender
    }
```

**File:** secure/net/src/network_controller/mod.rs (L128-137)
```rust
    pub fn create_inbound_channel(&mut self, message_type: String) -> Receiver<Message> {
        let (inbound_sender, inbound_receiver) = unbounded();

        self.inbound_handler
            .lock()
            .unwrap()
            .register_handler(message_type, inbound_sender);

        inbound_receiver
    }
```

**File:** secure/net/src/grpc_network_service/mod.rs (L91-116)
```rust
#[tonic::async_trait]
impl NetworkMessageService for GRPCNetworkMessageServiceServerWrapper {
    async fn simple_msg_exchange(
        &self,
        request: Request<NetworkMessage>,
    ) -> Result<Response<Empty>, Status> {
        let _timer = NETWORK_HANDLER_TIMER
            .with_label_values(&[&self.self_addr.to_string(), "inbound_msgs"])
            .start_timer();
        let remote_addr = request.remote_addr();
        let network_message = request.into_inner();
        let msg = Message::new(network_message.message);
        let message_type = MessageType::new(network_message.message_type);

        if let Some(handler) = self.inbound_handlers.lock().unwrap().get(&message_type) {
            // Send the message to the registered handler
            handler.send(msg).unwrap();
        } else {
            error!(
                "No handler registered for sender: {:?} and msg type {:?}",
                remote_addr, message_type
            );
        }
        Ok(Response::new(Empty {}))
    }
}
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L79-113)
```rust
impl CoordinatorClient<RemoteStateViewClient> for RemoteCoordinatorClient {
    fn receive_execute_command(&self) -> ExecutorShardCommand<RemoteStateViewClient> {
        match self.command_rx.recv() {
            Ok(message) => {
                let _rx_timer = REMOTE_EXECUTOR_TIMER
                    .with_label_values(&[&self.shard_id.to_string(), "cmd_rx"])
                    .start_timer();
                let bcs_deser_timer = REMOTE_EXECUTOR_TIMER
                    .with_label_values(&[&self.shard_id.to_string(), "cmd_rx_bcs_deser"])
                    .start_timer();
                let request: RemoteExecutionRequest = bcs::from_bytes(&message.data).unwrap();
                drop(bcs_deser_timer);

                match request {
                    RemoteExecutionRequest::ExecuteBlock(command) => {
                        let init_prefetch_timer = REMOTE_EXECUTOR_TIMER
                            .with_label_values(&[&self.shard_id.to_string(), "init_prefetch"])
                            .start_timer();
                        let state_keys = Self::extract_state_keys(&command);
                        self.state_view_client.init_for_block(state_keys);
                        drop(init_prefetch_timer);

                        let (sub_blocks, concurrency, onchain_config) = command.into();
                        ExecutorShardCommand::ExecuteSubBlocks(
                            self.state_view_client.clone(),
                            sub_blocks,
                            concurrency,
                            onchain_config,
                        )
                    },
                }
            },
            Err(_) => ExecutorShardCommand::Stop,
        }
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L215-260)
```rust
    pub fn start(&self) {
        trace!(
            "Shard starting, shard_id={}, num_shards={}.",
            self.shard_id,
            self.num_shards
        );
        let mut num_txns = 0;
        loop {
            let command = self.coordinator_client.receive_execute_command();
            match command {
                ExecutorShardCommand::ExecuteSubBlocks(
                    state_view,
                    transactions,
                    concurrency_level_per_shard,
                    onchain_config,
                ) => {
                    num_txns += transactions.num_txns();
                    trace!(
                        "Shard {} received ExecuteBlock command of block size {} ",
                        self.shard_id,
                        num_txns
                    );
                    let exe_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "execute_block"]);
                    let ret = self.execute_block(
                        transactions,
                        state_view.as_ref(),
                        BlockExecutorConfig {
                            local: BlockExecutorLocalConfig::default_with_concurrency_level(
                                concurrency_level_per_shard,
                            ),
                            onchain: onchain_config,
                        },
                    );
                    drop(state_view);
                    drop(exe_timer);

                    let _result_tx_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "result_tx"]);
                    self.coordinator_client.send_execution_result(ret);
                },
                ExecutorShardCommand::Stop => {
                    break;
                },
            }
        }
```

**File:** execution/executor-service/src/remote_state_view_service.rs (L64-72)
```rust
    pub fn start(&self) {
        while let Ok(message) = self.kv_rx.recv() {
            let state_view = self.state_view.clone();
            let kv_txs = self.kv_tx.clone();
            self.thread_pool.spawn(move || {
                Self::handle_message(message, state_view, kv_txs);
            });
        }
    }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L180-206)
```rust
    fn execute_block(
        &self,
        state_view: Arc<S>,
        transactions: PartitionedTransactions,
        concurrency_level_per_shard: usize,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<ShardedExecutionOutput, VMStatus> {
        trace!("RemoteExecutorClient Sending block to shards");
        self.state_view_service.set_state_view(state_view);
        let (sub_blocks, global_txns) = transactions.into();
        if !global_txns.is_empty() {
            panic!("Global transactions are not supported yet");
        }
        for (shard_id, sub_blocks) in sub_blocks.into_iter().enumerate() {
            let senders = self.command_txs.clone();
            let execution_request = RemoteExecutionRequest::ExecuteBlock(ExecuteBlockCommand {
                sub_blocks,
                concurrency_level: concurrency_level_per_shard,
                onchain_config: onchain_config.clone(),
            });

            senders[shard_id]
                .lock()
                .unwrap()
                .send(Message::new(bcs::to_bytes(&execution_request).unwrap()))
                .unwrap();
        }
```

**File:** crates/aptos-rate-limiter/src/rate_limit.rs (L54-89)
```rust
pub struct TokenBucketRateLimiter<Key: Eq + Hash + Clone + Debug> {
    label: &'static str,
    log_info: String,
    buckets: RwLock<HashMap<Key, SharedBucket>>,
    new_bucket_start_percentage: u8,
    default_bucket_size: usize,
    default_fill_rate: usize,
    enabled: bool,
    metrics: Option<HistogramVec>,
}

impl<Key: Eq + Hash + Clone + Debug> TokenBucketRateLimiter<Key> {
    pub fn new(
        label: &'static str,
        log_info: String,
        new_bucket_start_percentage: u8,
        default_bucket_size: usize,
        default_fill_rate: usize,
        metrics: Option<HistogramVec>,
    ) -> Self {
        // Ensure that we can actually use the rate limiter
        assert!(new_bucket_start_percentage <= 100);
        assert!(default_bucket_size > 0);
        assert!(default_fill_rate > 0);

        Self {
            label,
            log_info,
            buckets: RwLock::new(HashMap::new()),
            new_bucket_start_percentage,
            default_bucket_size,
            default_fill_rate,
            enabled: true,
            metrics,
        }
    }
```
