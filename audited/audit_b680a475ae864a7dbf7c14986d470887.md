# Audit Report

## Title
Cache-DB Inconsistency in Quorum Store Batch Persistence Causes Data Loss on Validator Restart

## Summary
The `persist_inner()` function in `consensus/src/quorum_store/batch_store.rs` has a non-atomic two-phase persistence operation where `save()` succeeds (adding batch to cache) but `db.save_batch()` can fail, causing an immediate validator crash via panic while leaving the batch permanently lost after restart. This violates atomicity and durability guarantees for batch persistence.

## Finding Description

The vulnerability exists in the batch persistence logic that separates cache insertion from database persistence without transactional atomicity. [1](#0-0) 

The execution flow is:

1. **Phase 1 - Cache Insertion**: The `save()` method is called, which internally calls `insert_to_cache()` to add the batch to the in-memory `db_cache` DashMap. [2](#0-1) 

2. **Phase 2 - DB Persistence**: If `save()` returns `Ok(true)`, the code attempts to persist to the database using either `db.save_batch()` or `db.save_batch_v2()`, both wrapped with `.expect("Could not write to DB")`. [3](#0-2) 

The database write operation can fail due to realistic production scenarios:
- Disk full conditions
- I/O errors (disk corruption, hardware failure)
- Database corruption
- File system permission issues
- Out of disk quota [4](#0-3) 

The database operations use relaxed (non-sync) writes, meaning failures can occur during the write operation itself: [5](#0-4) 

**The Critical Issue**: When the DB write fails, the `.expect()` causes an immediate panic, crashing the validator. However, the batch was already inserted into the cache in Phase 1. Upon validator restart, the recovery process only loads batches from the database into the cache: [6](#0-5) 

Since the batch never made it to the database (Phase 2 failed), it is **permanently lost** after restart, despite being accepted into the cache during Phase 1.

**Concurrent Access Window**: Between Phase 1 completion and Phase 2 failure, other threads can read the batch from the concurrent `db_cache` DashMap: [7](#0-6) 

This creates a window where the batch appears available but isn't durably persisted, violating consistency guarantees.

## Impact Explanation

This vulnerability has **HIGH severity** impact per Aptos bug bounty criteria:

1. **Validator Node Crashes** (HIGH): Any database write failure causes immediate panic, resulting in validator unavailability. The `.expect()` provides no graceful degradation.

2. **State Inconsistencies Requiring Intervention** (MEDIUM): Batches accepted into the system are lost on restart, requiring:
   - Re-fetching from peers (if they still have the batch)
   - Potential consensus delays or failures if locally-generated batches are referenced in blocks
   - Manual intervention to recover or resync state

3. **Consensus Impact**: If a validator generates a batch, accepts it into cache, but loses it on crash before other validators can retrieve it, this can cause:
   - Block proposal failures (missing batch data)
   - Consensus stalls requiring timeout and re-proposal
   - Reduced network liveness

4. **Violation of Critical Invariants**:
   - **State Consistency** (Invariant #4): The cache and persistent storage are inconsistent between Phase 1 and Phase 2
   - **Atomicity**: Batch acceptance should be atomic (all-or-nothing), but the two-phase approach violates this

## Likelihood Explanation

**MODERATE to HIGH likelihood** in production environments:

- **Disk Full Scenarios**: Validators handling high transaction volumes can fill disk space, especially with insufficient monitoring
- **Hardware Failures**: I/O errors from failing disks are common in production systems
- **Filesystem Issues**: Permission changes, quota enforcement, or filesystem corruption can cause write failures
- **Operational Errors**: Misconfigurations or maintenance operations can trigger database write failures

The likelihood increases during:
- Network congestion (high batch creation rate)
- Disk capacity management issues
- Hardware degradation
- Multiple concurrent batch persistence operations

## Recommendation

Implement atomic persistence with proper error handling and rollback mechanisms:

```rust
fn persist_inner(
    &self,
    batch_info: BatchInfoExt,
    persist_request: PersistedValue<BatchInfoExt>,
) -> Option<SignedBatchInfo<BatchInfoExt>> {
    assert!(
        &batch_info == persist_request.batch_info(),
        "Provided batch info doesn't match persist request batch info"
    );
    
    // OPTION 1: Write to DB first, then cache (durability-first approach)
    let needs_cache = match self.try_persist_to_db(&persist_request, &batch_info) {
        Ok(needs_cache) => needs_cache,
        Err(e) => {
            error!("QS: failed to persist to DB: {:?}", e);
            counters::BATCH_DB_WRITE_FAILURES.inc();
            return None; // Graceful failure instead of panic
        }
    };
    
    if needs_cache {
        if let Err(e) = self.save(&persist_request) {
            // Rollback: remove from DB if cache insertion fails
            error!("QS: failed to store to cache, rolling back DB: {:?}", e);
            let _ = self.delete_from_db(persist_request.digest(), batch_info.is_v2());
            return None;
        }
    }
    
    // Only generate signature after both cache and DB are consistent
    self.generate_signed_batch_info_internal(&batch_info)
}

fn try_persist_to_db(
    &self,
    persist_request: &PersistedValue<BatchInfoExt>,
    batch_info: &BatchInfoExt,
) -> anyhow::Result<bool> {
    if !batch_info.is_v2() {
        let persist_request = persist_request.clone().try_into()?;
        self.db.save_batch(persist_request)?; // Propagate error instead of panic
    } else {
        self.db.save_batch_v2(persist_request.clone())?; // Propagate error instead of panic
    }
    Ok(true)
}
```

**Key improvements**:
1. **Remove panic on DB failure**: Replace `.expect()` with proper error propagation
2. **DB-first approach**: Write to persistent storage before cache to ensure durability
3. **Rollback mechanism**: If cache insertion fails after DB write, remove from DB
4. **Graceful degradation**: Return `None` on failure instead of crashing validator
5. **Metrics**: Add counters for DB write failures to aid monitoring

## Proof of Concept

```rust
#[cfg(test)]
mod cache_db_inconsistency_test {
    use super::*;
    use aptos_crypto::HashValue;
    use std::sync::Arc;
    
    struct FailingDB {
        should_fail: AtomicBool,
    }
    
    impl QuorumStoreStorage for FailingDB {
        fn save_batch(&self, _batch: PersistedValue<BatchInfo>) -> Result<(), DbError> {
            if self.should_fail.load(Ordering::SeqCst) {
                Err(DbError::Other("Simulated disk full".to_string()))
            } else {
                Ok(())
            }
        }
        
        fn get_all_batches(&self) -> Result<HashMap<HashValue, PersistedValue<BatchInfo>>> {
            Ok(HashMap::new()) // Empty DB after restart
        }
        
        // ... other methods return Ok or empty results ...
    }
    
    #[test]
    #[should_panic(expected = "Could not write to DB")]
    fn test_cache_db_inconsistency_causes_panic() {
        let failing_db = Arc::new(FailingDB {
            should_fail: AtomicBool::new(true),
        });
        
        let batch_store = BatchStore::new(
            1, // epoch
            false, // is_new_epoch
            0, // last_certified_time
            failing_db.clone(),
            1000, // memory_quota
            2000, // db_quota
            10, // batch_quota
            test_validator_signer(),
            60_000_000, // expiration_buffer
        );
        
        let batch = create_test_batch();
        let persist_request = PersistedValue::new(batch, Some(vec![test_transaction()]));
        
        // This will succeed in adding to cache
        // Then panic when DB write fails
        // Demonstrating the inconsistency window
        batch_store.persist(vec![persist_request]);
        
        // After panic and restart, cache is empty and DB has no batch
        // Batch is permanently lost
    }
    
    #[test]
    fn test_batch_lost_after_restart() {
        // Setup batch store with failing DB
        let failing_db = Arc::new(FailingDB {
            should_fail: AtomicBool::new(true),
        });
        
        let batch_store = BatchStore::new(...);
        
        // Catch the panic
        let result = std::panic::catch_unwind(|| {
            batch_store.persist(vec![test_batch]);
        });
        
        assert!(result.is_err(), "Expected panic on DB failure");
        
        // Simulate restart: create new BatchStore
        failing_db.should_fail.store(false, Ordering::SeqCst);
        let batch_store_after_restart = BatchStore::new(
            1, false, 0, failing_db, 1000, 2000, 10, test_validator_signer(), 60_000_000
        );
        
        // Batch is not in cache (volatile, lost on crash)
        // Batch is not in DB (write failed)
        let result = batch_store_after_restart.get_batch_from_local(&test_digest);
        assert!(result.is_err(), "Batch should be lost after restart");
    }
}
```

**Notes:**

1. The vulnerability is confirmed through code analysis - the two-phase persistence (cache then DB) without atomicity guarantees causes data loss when DB writes fail.

2. The `.expect()` panic strategy prevents further inconsistencies but doesn't prevent the initial inconsistency or data loss on restart.

3. Production validators are at risk during disk space exhaustion, hardware failures, or I/O errors - all realistic scenarios.

4. The recommended fix ensures atomicity by writing to DB first (durability) and providing rollback if cache insertion fails, while also handling errors gracefully without crashing the validator.

### Citations

**File:** consensus/src/quorum_store/batch_store.rs (L245-290)
```rust
    fn populate_cache_and_gc_expired_batches_v1(
        db: Arc<dyn QuorumStoreStorage>,
        current_epoch: u64,
        last_certified_time: u64,
        expiration_buffer_usecs: u64,
        batch_store: &BatchStore,
    ) {
        let db_content = db
            .get_all_batches()
            .expect("failed to read v1 data from db");
        info!(
            epoch = current_epoch,
            "QS: Read v1 batches from storage. Len: {}, Last Cerified Time: {}",
            db_content.len(),
            last_certified_time
        );

        let mut expired_keys = Vec::new();
        let gc_timestamp = last_certified_time.saturating_sub(expiration_buffer_usecs);
        for (digest, value) in db_content {
            let expiration = value.expiration();

            trace!(
                "QS: Batchreader recovery content exp {:?}, digest {}",
                expiration,
                digest
            );

            if expiration < gc_timestamp {
                expired_keys.push(digest);
            } else {
                batch_store
                    .insert_to_cache(&value.into())
                    .expect("Storage limit exceeded upon BatchReader construction");
            }
        }

        info!(
            "QS: Batch store bootstrap expired keys len {}",
            expired_keys.len()
        );
        tokio::task::spawn_blocking(move || {
            db.delete_batches(expired_keys)
                .expect("Deletion of expired keys should not fail");
        });
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L419-439)
```rust
    pub(crate) fn save(&self, value: &PersistedValue<BatchInfoExt>) -> anyhow::Result<bool> {
        let last_certified_time = self.last_certified_time();
        if value.expiration() > last_certified_time {
            fail_point!("quorum_store::save", |_| {
                // Skip caching and storing value to the db
                Ok(false)
            });
            counters::GAP_BETWEEN_BATCH_EXPIRATION_AND_CURRENT_TIME_WHEN_SAVE.observe(
                Duration::from_micros(value.expiration() - last_certified_time).as_secs_f64(),
            );

            return self.insert_to_cache(value);
        }
        counters::NUM_BATCH_EXPIRED_WHEN_SAVE.inc();
        bail!(
            "Incorrect expiration {} in epoch {}, last committed timestamp {}",
            value.expiration(),
            self.epoch(),
            last_certified_time,
        );
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L488-528)
```rust
    fn persist_inner(
        &self,
        batch_info: BatchInfoExt,
        persist_request: PersistedValue<BatchInfoExt>,
    ) -> Option<SignedBatchInfo<BatchInfoExt>> {
        assert!(
            &batch_info == persist_request.batch_info(),
            "Provided batch info doesn't match persist request batch info"
        );
        match self.save(&persist_request) {
            Ok(needs_db) => {
                trace!("QS: sign digest {}", persist_request.digest());
                if needs_db {
                    if !batch_info.is_v2() {
                        let persist_request =
                            persist_request.try_into().expect("Must be a V1 batch");
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch(persist_request)
                            .expect("Could not write to DB");
                    } else {
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch_v2(persist_request)
                            .expect("Could not write to DB")
                    }
                }
                if !batch_info.is_v2() {
                    self.generate_signed_batch_info(batch_info.info().clone())
                        .ok()
                        .map(|inner| inner.into())
                } else {
                    self.generate_signed_batch_info(batch_info).ok()
                }
            },
            Err(e) => {
                debug!("QS: failed to store to cache {:?}", e);
                None
            },
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L571-585)
```rust
    pub(crate) fn get_batch_from_local(
        &self,
        digest: &HashValue,
    ) -> ExecutorResult<PersistedValue<BatchInfoExt>> {
        if let Some(value) = self.db_cache.get(digest) {
            if value.payload_storage_mode() == StorageMode::PersistedOnly {
                self.get_batch_from_db(digest, value.batch_info().is_v2())
            } else {
                // Available in memory.
                Ok(value.clone())
            }
        } else {
            Err(ExecutorError::CouldNotGetData)
        }
    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L82-89)
```rust
    /// Relaxed writes instead of sync writes.
    pub fn put<S: Schema>(&self, key: &S::Key, value: &S::Value) -> Result<(), DbError> {
        // Not necessary to use a batch, but we'd like a central place to bump counters.
        let mut batch = self.db.new_native_batch();
        batch.put::<S>(key, value)?;
        self.db.write_schemas_relaxed(batch)?;
        Ok(())
    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L110-117)
```rust
    fn save_batch(&self, batch: PersistedValue<BatchInfo>) -> Result<(), DbError> {
        trace!(
            "QS: db persists digest {} expiration {:?}",
            batch.digest(),
            batch.expiration()
        );
        self.put::<BatchSchema>(batch.digest(), &batch)
    }
```
