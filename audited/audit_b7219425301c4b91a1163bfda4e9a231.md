# Audit Report

## Title
Cross-Block Message Contamination via Shared Global Channel in Sharded Executor

## Summary
The global cross-shard communication channel is reused across multiple block executions without proper cleanup, allowing messages from Block N to contaminate Block N+1's execution. This causes non-deterministic execution and consensus safety violations as different validators may consume messages in different orders.

## Finding Description

The vulnerability exists in the sharded block executor's global message channel lifecycle. The channel is created once during executor setup and reused across all block executions: [1](#0-0) 

This channel is stored in `GlobalExecutor` and persists across all blocks: [2](#0-1) 

During block execution, the global executor spawns a `CrossShardCommitReceiver` that processes messages until it receives a `StopMsg`: [3](#0-2) 

The critical timing issue occurs because:

1. Global execution completes and sends `StopMsg` before waiting for shards: [4](#0-3) 

2. Shards continue executing and may send messages to the global channel after the global receiver has stopped: [5](#0-4) 

3. When the receiver exits upon receiving `StopMsg`, it does NOT drain remaining messages from the unbounded channel.

4. The next block's execution spawns a new `CrossShardCommitReceiver` that consumes leftover messages from the previous block.

5. These stale messages either: (a) update the `CrossShardStateView` with incorrect state values if the state key exists, or (b) cause a panic if the state key is not expected: [6](#0-5) 

The `unwrap()` at line 52 will panic if the state key from a stale message is not in the current block's expected cross-shard keys, crashing the executor.

**Attack Scenario:**

Block N:
- Shard 0 commits transaction T1 writing to state key K1 (global dependency)
- Global executor completes and sends `StopMsg`
- Global `CrossShardCommitReceiver` receives `StopMsg` and exits
- Shard 0 still executing, commits T2 writing to K2 with global dependency
- Shard 0 sends `RemoteTxnWriteMsg(K2, V2)` to global channel
- Message remains in channel (receiver already stopped)

Block N+1:
- New `CrossShardCommitReceiver` spawned for global executor
- First message received: `RemoteTxnWriteMsg(K2, V2)` from Block N
- If K2 not in Block N+1's `CrossShardStateView`: **PANIC and executor crash**
- If K2 exists in Block N+1's keys: **State contamination with stale value V2**

This violates **Invariant #1: Deterministic Execution** - validators will have different execution results based on timing races.

## Impact Explanation

**CRITICAL Severity** - This meets multiple critical impact categories:

1. **Consensus/Safety Violations**: Different validators may process messages in different orders due to timing variations, producing different state roots for the same block. This breaks consensus safety and can lead to chain splits.

2. **Non-Deterministic Execution**: The core deterministic execution invariant is violated. Validators executing the same block with the same transactions will produce different results based on:
   - How many stale messages are in the channel
   - Which state keys the stale messages reference
   - Timing of when previous block's shards completed

3. **Validator Node Crashes**: If a stale message references a state key not in the current block's `CrossShardStateView`, the `unwrap()` panic crashes the executor, causing total loss of liveness for that validator.

4. **State Corruption**: Transactions may read incorrect cross-shard state values from previous blocks, leading to wrong execution results, incorrect balances, and potential double-spend scenarios.

This directly maps to Aptos Bug Bounty **Critical Severity** categories:
- Consensus/Safety violations (up to $1,000,000)
- Non-recoverable network partition if majority of validators experience different message contamination patterns

## Likelihood Explanation

**HIGH Likelihood**:

1. **Natural Occurrence**: This is not an attack requiring special conditions - it happens during normal operation whenever:
   - Global transactions complete before all shard transactions
   - Shard transactions have cross-shard dependencies to global round
   - High transaction throughput with many shards

2. **Timing Race**: The concurrent execution model explicitly states that global execution runs concurrently with shards: [7](#0-6) 

3. **No Synchronization**: There is no barrier or synchronization ensuring all shards finish sending messages before global receiver stops.

4. **Unbounded Channel**: The channel is unbounded, so messages accumulate indefinitely: [8](#0-7) 

5. **No Cleanup**: The `shutdown()` method is empty, providing no cleanup mechanism: [9](#0-8) 

## Recommendation

**Immediate Fix**: Drain the global channel between block executions:

```rust
fn setup_global_executor() -> (GlobalExecutor<S>, Sender<CrossShardMsg>, Receiver<CrossShardMsg>) {
    let (cross_shard_tx, cross_shard_rx) = unbounded();
    let cross_shard_client = Arc::new(GlobalCrossShardClient::new(
        cross_shard_tx.clone(),
        cross_shard_rx.clone(),
    ));
    let executor_threads = num_cpus::get().min(32);
    let global_executor = GlobalExecutor::new(cross_shard_client, executor_threads);
    (global_executor, cross_shard_tx, cross_shard_rx)
}

// In execute_block, before starting new execution:
fn execute_block(...) -> Result<ShardedExecutionOutput, VMStatus> {
    // Drain any leftover messages from previous block
    while let Ok(_) = self.global_cross_shard_rx.try_recv() {
        // Discard stale messages
    }
    
    // Continue with normal execution...
}
```

**Better Long-term Fix**: Create a fresh channel per block execution:

```rust
pub fn execute_block(...) -> Result<ShardedExecutionOutput, VMStatus> {
    // Create fresh channel for this block
    let (cross_shard_tx, cross_shard_rx) = unbounded();
    let cross_shard_client = Arc::new(GlobalCrossShardClient::new(
        cross_shard_tx,
        cross_shard_rx,
    ));
    
    // Execute with isolated channel
    let global_output = execute_global_txns_with_client(
        global_txns,
        state_view.as_ref(),
        onchain_config,
        cross_shard_client,
    )?;
    
    // Channel automatically cleaned up when dropped
}
```

**Additional Safeguards**:
1. Add synchronization barrier ensuring all shards complete before global receiver stops
2. Add assertions to detect stale messages (e.g., timestamp or epoch markers)
3. Change `unwrap()` to proper error handling to prevent crashes

## Proof of Concept

```rust
// Reproduction test demonstrating the vulnerability
#[test]
fn test_cross_block_message_contamination() {
    use std::sync::Arc;
    use aptos_types::state_store::MockStateView;
    
    // Setup executor with multiple shards
    let num_shards = 4;
    let executor = LocalExecutorService::setup_local_executor_shards(num_shards, None);
    
    // Block N: Create transactions with cross-shard dependencies to global round
    let block_n_txns = create_test_transactions_with_global_deps();
    let state_view = Arc::new(MockStateView::empty());
    
    // Execute Block N
    let result_n = executor.execute_block(
        state_view.clone(),
        block_n_txns,
        8,
        BlockExecutorConfigFromOnchain::default(),
    ).unwrap();
    
    // Block N+1: Execute immediately with different transaction set
    let block_n1_txns = create_different_test_transactions();
    
    // This execution will consume leftover messages from Block N
    let result_n1 = executor.execute_block(
        state_view.clone(),
        block_n1_txns,
        8,
        BlockExecutorConfigFromOnchain::default(),
    );
    
    // Expected: Either panic from unwrap() or non-deterministic results
    // Verify: Run this test multiple times - results should vary based on timing
    match result_n1 {
        Err(_) => println!("Executor crashed due to unexpected stale message"),
        Ok(output) => {
            // Compare with clean execution - outputs will differ due to contamination
            let clean_executor = LocalExecutorService::setup_local_executor_shards(num_shards, None);
            let clean_result = clean_executor.execute_block(
                state_view.clone(),
                create_different_test_transactions(),
                8,
                BlockExecutorConfigFromOnchain::default(),
            ).unwrap();
            
            assert_ne!(output, clean_result, "State contamination detected!");
        }
    }
}
```

**Notes**

This vulnerability is particularly severe because:

1. It affects the core deterministic execution guarantee that all validators depend on
2. It can manifest as either silent state corruption or executor crashes
3. The timing-dependent nature makes it difficult to diagnose in production
4. It only requires normal transaction flow with cross-shard dependencies - no malicious input needed
5. The unbounded channel means contamination can accumulate across many blocks

The fix must ensure complete message isolation between block executions while maintaining the performance benefits of the sharded architecture.

### Citations

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L65-75)
```rust
    fn setup_global_executor() -> (GlobalExecutor<S>, Sender<CrossShardMsg>) {
        let (cross_shard_tx, cross_shard_rx) = unbounded();
        let cross_shard_client = Arc::new(GlobalCrossShardClient::new(
            cross_shard_tx.clone(),
            cross_shard_rx,
        ));
        // Limit the number of global executor threads to 32 as parallel execution doesn't scale well beyond that.
        let executor_threads = num_cpus::get().min(32);
        let global_executor = GlobalExecutor::new(cross_shard_client, executor_threads);
        (global_executor, cross_shard_tx)
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L183-223)
```rust
    fn execute_block(
        &self,
        state_view: Arc<S>,
        transactions: PartitionedTransactions,
        concurrency_level_per_shard: usize,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<ShardedExecutionOutput, VMStatus> {
        assert_eq!(transactions.num_shards(), self.num_shards());
        let (sub_blocks, global_txns) = transactions.into();
        for (i, sub_blocks_for_shard) in sub_blocks.into_iter().enumerate() {
            self.command_txs[i]
                .send(ExecutorShardCommand::ExecuteSubBlocks(
                    state_view.clone(),
                    sub_blocks_for_shard,
                    concurrency_level_per_shard,
                    onchain_config.clone(),
                ))
                .unwrap();
        }

        // This means that we are executing the global transactions concurrently with the individual shards but the
        // global transactions will be blocked for cross shard transaction results. This hopefully will help with
        // finishing the global transactions faster but we need to evaluate if this causes thread contention. If it
        // does, then we can simply move this call to the end of the function.
        let mut global_output = self.global_executor.execute_global_txns(
            global_txns,
            state_view.as_ref(),
            onchain_config,
        )?;

        let mut sharded_output = self.get_output_from_shards()?;

        sharded_aggregator_service::aggregate_and_update_total_supply(
            &mut sharded_output,
            &mut global_output,
            state_view.as_ref(),
            self.global_executor.get_executor_thread_pool(),
        );

        Ok(ShardedExecutionOutput::new(sharded_output, global_output))
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L225-225)
```rust
    fn shutdown(&mut self) {}
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L26-45)
```rust
    pub fn start<S: StateView + Sync + Send>(
        cross_shard_state_view: Arc<CrossShardStateView<S>>,
        cross_shard_client: Arc<dyn CrossShardClient>,
        round: RoundId,
    ) {
        loop {
            let msg = cross_shard_client.receive_cross_shard_msg(round);
            match msg {
                RemoteTxnWriteMsg(txn_commit_msg) => {
                    let (state_key, write_op) = txn_commit_msg.take();
                    cross_shard_state_view
                        .set_value(&state_key, write_op.and_then(|w| w.as_state_value()));
                },
                CrossShardMsg::StopMsg => {
                    trace!("Cross shard commit receiver stopped for round {}", round);
                    break;
                },
            }
        }
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L122-130)
```rust
                    if *round_id == GLOBAL_ROUND_ID {
                        self.cross_shard_client.send_global_msg(message);
                    } else {
                        self.cross_shard_client.send_cross_shard_msg(
                            *dependent_shard_id,
                            *round_id,
                            message,
                        );
                    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L169-173)
```rust
                } else {
                    trace!("executed block for global shard and round {}", round);
                    // Send a self message to stop the cross-shard commit receiver.
                    cross_shard_client_clone.send_global_msg(CrossShardMsg::StopMsg);
                }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_state_view.rs (L49-56)
```rust
    pub fn set_value(&self, state_key: &StateKey, state_value: Option<StateValue>) {
        self.cross_shard_data
            .get(state_key)
            .unwrap()
            .set_value(state_value);
        // uncomment the following line to debug waiting count
        // trace!("waiting count for shard id {} is {}", self.shard_id, self.waiting_count());
    }
```
