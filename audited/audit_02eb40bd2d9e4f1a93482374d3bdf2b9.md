# Audit Report

## Title
Hardcoded Seed Peers Cannot Be Updated Through Governance, Causing Bootstrap Failures When Infrastructure Migrates

## Summary
Seed peer constants (`MAINNET_SEED_PEERS`, `TESTNET_SEED_PEERS`) in the Aptos Core codebase are hardcoded at compile-time and cannot be updated through on-chain governance mechanisms. When seed operators migrate their infrastructure, new fullnodes attempting to bootstrap cannot connect to any peers and become permanently unable to join the network. [1](#0-0) 

## Finding Description

The seed peer discovery mechanism exhibits a critical operational vulnerability due to its hardcoded, non-updatable design:

**1. Hardcoded Seed Peers**: Seed peer addresses, public keys, and network addresses are defined as compile-time constants that can only be changed through source code modifications and binary redeployment. [2](#0-1) 

**2. No On-Chain Governance Update Path**: Unlike validator network addresses which can be updated on-chain via `stake::update_network_and_fullnode_addresses`, seed peer constants have no runtime update mechanism. [3](#0-2) 

**3. Bootstrap Dependency Chain**: New fullnodes rely on this discovery priority: [4](#0-3) 

The discovery sources are prioritized: OnChain > File > Rest > Config (seeds). However, on-chain discovery requires:
- An existing peer connection to perform state sync
- State sync to download the current validator set
- The validator set to populate on-chain discovery [5](#0-4) 

**4. Bootstrap Deadlock**: When all seed peers have migrated infrastructure:
- New fullnodes cannot connect to any seed peer (stale addresses)
- Without peer connections, `global_data_summary.is_empty()` remains true
- State sync cannot proceed without peers
- On-chain discovery cannot fetch validator addresses without state sync
- The node is stuck in a permanent bootstrap failure [6](#0-5) 

**5. Auto-Bootstrapping Does Not Apply**: The auto-bootstrapping mechanism only applies to validators with genesis waypoints, not public fullnodes. [7](#0-6) [8](#0-7) 

## Impact Explanation

This vulnerability meets **Medium Severity** criteria per the Aptos bug bounty program:

**"State inconsistencies requiring intervention"** - When seed peer infrastructure migrates without coordinated binary updates, new fullnodes experience permanent bootstrap failures requiring manual intervention (source code updates and node binary redeployment).

**Network Availability Impact**:
- New fullnodes cannot join mainnet or testnet
- Network participation becomes gated on having the latest binary with updated seed peers
- Affects network decentralization as new operators cannot onboard
- Requires urgent coordination between core developers and node operators

This does NOT reach Critical/High severity because:
- Existing connected nodes remain unaffected
- Does not cause consensus violations
- Does not affect fund security
- The main network continues operating normally

## Likelihood Explanation

**High Likelihood**: This scenario is inevitable in production networks:

1. **Natural Infrastructure Migration**: Seed operators routinely migrate infrastructure for:
   - Cloud provider changes
   - Geographic relocation
   - Security key rotation
   - DNS/hostname updates
   - Network topology changes

2. **Coordination Challenge**: When multiple seed operators migrate independently, the hardcoded addresses become progressively stale.

3. **Silent Failure**: The issue only manifests when new nodes attempt to bootstrap, creating a delayed operational impact that may go unnoticed initially.

4. **Already Observable**: The mainnet seed peers list contains only 1 seed peer, while testnet has 4. This suggests past seed peers may have already been removed due to migrations. [9](#0-8) 

## Recommendation

Implement a runtime-updatable seed peer discovery mechanism with on-chain governance:

**Solution 1: On-Chain Seed Peer Registry**
- Create a Move module `aptos_framework::seed_peers` with governance-controlled seed peer list
- Allow governance proposals to add/remove seed peers
- Modify `config_optimizer.rs` to fetch seeds from on-chain config during initialization
- Maintain hardcoded seeds as fallback for genesis bootstrap

**Solution 2: DNS-Based Seed Discovery**
- Use DNS SRV records for seed peer discovery (e.g., `_aptos._tcp.seeds.mainnet.aptoslabs.com`)
- Allow dynamic updates without binary changes
- Implement DNS resolution with multiple authoritative nameservers
- Keep hardcoded seeds as bootstrap fallback

**Solution 3: Hybrid Approach**
- Combine on-chain registry with DNS-based discovery
- Use hardcoded seeds only for initial genesis bootstrap
- Transition to on-chain/DNS after first successful sync

**Implementation Priority**:
```rust
// In config/src/config/config_optimizer.rs
fn get_seed_peers(chain_id: ChainId, db_reader: Option<&dyn DbReader>) -> Result<PeerSet, Error> {
    // 1. Try on-chain seed peer registry (if node has synced state)
    if let Some(reader) = db_reader {
        if let Ok(onchain_seeds) = fetch_onchain_seed_peers(reader, chain_id) {
            if !onchain_seeds.is_empty() {
                return Ok(onchain_seeds);
            }
        }
    }
    
    // 2. Fallback to hardcoded constants for bootstrap
    match chain_id {
        ChainId::Mainnet => create_seed_peers(MAINNET_SEED_PEERS.into()),
        ChainId::Testnet => create_seed_peers(TESTNET_SEED_PEERS.into()),
        _ => Ok(HashMap::new()),
    }
}
```

## Proof of Concept

**Scenario**: Demonstrate bootstrap failure when all seed peers are unreachable.

**Test Setup**:
```rust
// In network/framework/src/connectivity_manager/test.rs

#[tokio::test]
async fn test_bootstrap_failure_with_stale_seeds() {
    // 1. Create a new fullnode with stale seed peer addresses
    let stale_seeds = create_seed_peers(vec![
        ("dead0000", "0xdead0000", "/ip4/192.0.2.1/tcp/6182/noise-ik/0xdead0000/handshake/0"),
        ("dead0001", "0xdead0001", "/ip4/192.0.2.2/tcp/6182/noise-ik/0xdead0001/handshake/0"),
    ]).unwrap();
    
    let network_context = NetworkContext::mock();
    let (connection_reqs_tx, _rx) = aptos_channels::new_test(1);
    let (notifs_tx, notifs_rx) = conn_notifs_channel::new();
    let (reqs_tx, reqs_rx) = aptos_channels::new_test(1);
    
    let mut conn_mgr = ConnectivityManager::new(
        network_context,
        TimeService::mock(),
        Arc::new(PeersAndMetadata::new(&[NetworkId::Public])),
        stale_seeds, // Stale seed peers
        connection_reqs_tx,
        notifs_rx,
        reqs_rx,
        Duration::from_secs(1),
        ExponentialBackoff::from_millis(10).map(jitter).take(10),
        Duration::from_secs(60),
        None,
        false,
        false,
    );
    
    // 2. Start connectivity manager
    let handle = tokio::spawn(conn_mgr.start());
    
    // 3. Wait and verify no successful connections
    tokio::time::sleep(Duration::from_secs(5)).await;
    
    // 4. Query connected peers - should be empty
    let (tx, rx) = oneshot::channel();
    reqs_tx.push((), ConnectivityRequest::GetConnectedSize(tx)).unwrap();
    let connected_size = rx.await.unwrap();
    assert_eq!(connected_size, 0, "Should have zero connections with stale seeds");
    
    // 5. Node is stuck and cannot bootstrap
    // In production, this would prevent state sync from proceeding
}
```

**Real-World Reproduction Steps**:
1. Deploy a new public fullnode with the current Aptos binary
2. Modify local config to use non-existent seed peers (simulating migration)
3. Start the node and observe logs: "The global data summary is empty! It's likely that we have no active peers."
4. Verify the node never progresses past bootstrap phase
5. Confirm that updating to working seed peers allows bootstrap to proceed

## Notes

This vulnerability represents a **fundamental architectural limitation** in the current seed peer discovery design. While validators can update their network addresses on-chain through governance, the bootstrap mechanism for new nodes relies on immutable compile-time constants.

The issue is exacerbated by:
- Only 1 seed peer currently configured for mainnet (single point of failure)
- No monitoring/alerting when seed peers become unreachable
- No graceful degradation or alternative bootstrap paths

This finding emphasizes the need for **runtime-updatable infrastructure configuration** in production blockchain networks, especially for critical bootstrap components that cannot be updated through normal on-chain governance mechanisms.

### Citations

**File:** config/src/config/config_optimizer.rs (L30-61)
```rust
// Mainnet seed peers. Each seed peer entry is a tuple
// of (account address, public key, network address).
const MAINNET_SEED_PEERS: [(&str, &str, &str); 1] = [(
    "568fdb6acf26aae2a84419108ff13baa3ebf133844ef18e23a9f47b5af16b698",
    "0x003cc2ed36e7d486539ac2c411b48d962f1ef17d884c3a7109cad43f16bd5008",
    "/dns/node1.cloud-b.mainnet.aptoslabs.com/tcp/6182/noise-ik/0x003cc2ed36e7d486539ac2c411b48d962f1ef17d884c3a7109cad43f16bd5008/handshake/0",
)];

// Testnet seed peers. Each seed peer entry is a tuple
// of (account address, public key, network address).
const TESTNET_SEED_PEERS: [(&str, &str, &str); 4] = [
    (
        "31e55012a7d439dcd16fee0509cd5855c1fbdc62057ba7fac3f7c88f5453dd8e",
        "0x87bb19b02580b7e2a91a8e9342ec77ffd8f3ad967f54e77b22aaf558c5c11755",
        "/dns/seed0.testnet.aptoslabs.com/tcp/6182/noise-ik/0x87bb19b02580b7e2a91a8e9342ec77ffd8f3ad967f54e77b22aaf558c5c11755/handshake/0",
    ),
    (
        "116176e2af223a8b7f8db80dc52f7a423b4d7f8c0553a1747e92ef58849aff4f",
        "0xc2f24389f31c9c18d2ceb69d153ad9299e0ea7bbd66f457e0a28ef41c77c2b64",
        "/dns/seed1.testnet.aptoslabs.com/tcp/6182/noise-ik/0xc2f24389f31c9c18d2ceb69d153ad9299e0ea7bbd66f457e0a28ef41c77c2b64/handshake/0",
    ),
    (
        "12000330d7cd8a748f46c25e6ce5d236a27e13d0b510d4516ac84ecc5fddd002",
        "0x171c661e5b785283978a74eafc52a906e68c73ae78119737b92f93507c753933",
        "/dns/seed2.testnet.aptoslabs.com/tcp/6182/noise-ik/0x171c661e5b785283978a74eafc52a906e68c73ae78119737b92f93507c753933/handshake/0",
    ),
    (
        "03c04549114877c55f45649aba48ac0a4ff086ab7bdce3b8cc8d3d9947bc0d99",
        "0xafc38bf177bd825326a1c314748612137d2b35dae6472932806806a32c23174a",
        "/dns/seed3.testnet.aptoslabs.com/tcp/6182/noise-ik/0xafc38bf177bd825326a1c314748612137d2b35dae6472932806806a32c23174a/handshake/0",
    ),
];
```

**File:** config/src/config/config_optimizer.rs (L202-215)
```rust
            // Only add seeds to testnet and mainnet (as they are long living networks)
            if local_network_config_yaml["seeds"].is_null() {
                if let Some(chain_id) = chain_id {
                    if chain_id.is_testnet() {
                        fullnode_network_config.seeds =
                            create_seed_peers(TESTNET_SEED_PEERS.into())?;
                        modified_config = true;
                    } else if chain_id.is_mainnet() {
                        fullnode_network_config.seeds =
                            create_seed_peers(MAINNET_SEED_PEERS.into())?;
                        modified_config = true;
                    }
                }
            }
```

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L1-19)
```text
///
/// Validator lifecycle:
/// 1. Prepare a validator node set up and call stake::initialize_validator
/// 2. Once ready to deposit stake (or have funds assigned by a staking service in exchange for ownership capability),
/// call stake::add_stake (or *_with_cap versions if called from the staking service)
/// 3. Call stake::join_validator_set (or _with_cap version) to join the active validator set. Changes are effective in
/// the next epoch.
/// 4. Validate and gain rewards. The stake will automatically be locked up for a fixed duration (set by governance) and
/// automatically renewed at expiration.
/// 5. At any point, if the validator operator wants to update the consensus key or network/fullnode addresses, they can
/// call stake::rotate_consensus_key and stake::update_network_and_fullnode_addresses. Similar to changes to stake, the
/// changes to consensus key/network/fullnode addresses are only effective in the next epoch.
/// 6. Validator can request to unlock their stake at any time. However, their stake will only become withdrawable when
/// their current lockup expires. This can be at most as long as the fixed lockup duration.
/// 7. After exiting, the validator can either explicitly leave the validator set by calling stake::leave_validator_set
/// or if their stake drops below the min required, they would get removed at the end of the epoch.
/// 8. Validator can always rejoin the validator set by going through steps 2-3 again.
/// 9. An owner can always switch operators by calling stake::set_operator.
/// 10. An owner can always switch designated voter by calling stake::set_designated_voter.
```

**File:** network/framework/src/connectivity_manager/mod.rs (L11-27)
```rust
//! Different discovery sources notify the ConnectivityManager of updates to
//! peers' addresses. Currently, there are 2 discovery sources (ordered by
//! decreasing dial priority, i.e., first is highest priority):
//!
//! 1. Onchain discovery protocol
//! 2. Seed peers from config
//!
//! In other words, if a we have some addresses discovered via onchain discovery
//! and some seed addresses from our local config, we will try the onchain
//! discovery addresses first and the local seed addresses after.
//!
//! When dialing a peer with a given list of addresses, we attempt each address
//! in order with a capped exponential backoff delay until we eventually connect
//! to the peer. The backoff is capped since, for validators specifically, it is
//! absolutely important that we maintain connectivity with all peers and heal
//! any partitions asap, as we aren't currently gossiping consensus messages or
//! using a relay protocol.
```

**File:** state-sync/state-sync-driver/src/driver.rs (L636-664)
```rust
    async fn check_auto_bootstrapping(&mut self) {
        if !self.bootstrapper.is_bootstrapped()
            && self.is_consensus_or_observer_enabled()
            && self.driver_configuration.config.enable_auto_bootstrapping
            && self.driver_configuration.waypoint.version() == 0
        {
            if let Some(start_time) = self.start_time {
                if let Some(connection_deadline) = start_time.checked_add(Duration::from_secs(
                    self.driver_configuration
                        .config
                        .max_connection_deadline_secs,
                )) {
                    if self.time_service.now() >= connection_deadline {
                        info!(LogSchema::new(LogEntry::AutoBootstrapping).message(
                            "Passed the connection deadline! Auto-bootstrapping the validator!"
                        ));
                        if let Err(error) = self.bootstrapper.bootstrapping_complete().await {
                            warn!(LogSchema::new(LogEntry::AutoBootstrapping)
                                .error(&error)
                                .message("Failed to mark bootstrapping as complete!"));
                        }
                    }
                } else {
                    warn!(LogSchema::new(LogEntry::AutoBootstrapping)
                        .message("The connection deadline overflowed! Unable to auto-bootstrap!"));
                }
            }
        }
    }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L670-678)
```rust

        // Fetch the global data summary and verify we have active peers
        let global_data_summary = self.aptos_data_client.get_global_data_summary();
        if global_data_summary.is_empty() {
            trace!(LogSchema::new(LogEntry::Driver).message(
                "The global data summary is empty! It's likely that we have no active peers."
            ));
            return self.check_auto_bootstrapping().await;
        }
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L35-38)
```rust
// Useful bootstrapper constants
const BOOTSTRAPPER_LOG_INTERVAL_SECS: u64 = 3;
pub const GENESIS_TRANSACTION_VERSION: u64 = 0; // The expected version of the genesis transaction

```

**File:** config/src/config/state_sync_config.rs (L110-143)
```rust
    /// Enable auto-bootstrapping if no peers are found after `max_connection_deadline_secs`
    pub enable_auto_bootstrapping: bool,
    /// The interval (ms) to refresh the storage summary
    pub fallback_to_output_syncing_secs: u64,
    /// The interval (ms) at which to check state sync progress
    pub progress_check_interval_ms: u64,
    /// The maximum time (secs) to wait for connections from peers before auto-bootstrapping
    pub max_connection_deadline_secs: u64,
    /// The maximum number of notifications to process per driver loop
    pub max_consecutive_stream_notifications: u64,
    /// The maximum number of stream timeouts allowed before termination
    pub max_num_stream_timeouts: u64,
    /// The maximum number of data chunks pending execution or commit
    pub max_pending_data_chunks: u64,
    /// The maximum number of pending mempool commit notifications
    pub max_pending_mempool_notifications: u64,
    /// The maximum time (ms) to wait for a data stream notification
    pub max_stream_wait_time_ms: u64,
    /// The version lag we'll tolerate before snapshot syncing
    pub num_versions_to_skip_snapshot_sync: u64,
}

/// The default state sync driver config will be the one that gets (and keeps)
/// the node up-to-date as quickly and cheaply as possible.
impl Default for StateSyncDriverConfig {
    fn default() -> Self {
        Self {
            bootstrapping_mode: BootstrappingMode::ExecuteOrApplyFromGenesis,
            commit_notification_timeout_ms: 5000,
            continuous_syncing_mode: ContinuousSyncingMode::ExecuteTransactionsOrApplyOutputs,
            enable_auto_bootstrapping: false,
            fallback_to_output_syncing_secs: 180, // 3 minutes
            progress_check_interval_ms: 100,
            max_connection_deadline_secs: 10,
```
