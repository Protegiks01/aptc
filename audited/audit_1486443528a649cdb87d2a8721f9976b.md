# Audit Report

## Title
Unbounded Computational Complexity in DKG Transcript Verification Allows DoS Attack

## Summary
A malicious validator can craft a DKG transcript containing a Sigma protocol proof with an arbitrarily large `Witness` structure. During verification, honest validators must iterate over all elements in this structure and perform expensive field arithmetic operations, causing excessive CPU consumption before the malformed proof can be rejected. This can prevent DKG ceremony completion and delay epoch transitions.

## Finding Description

The DKG (Distributed Key Generation) system uses Sigma protocols to prove knowledge of chunked plaintexts. The proof structure contains a response field `z` of type `Witness`: [1](#0-0) 

This `Witness` structure has no bounds on the dimensions of its nested `Vec<Vec<Vec<Scalar<F>>>>` field. During transcript verification, the following execution path occurs:

1. **Transcript Verification Entry**: The VM calls `verify_transcript()` which invokes the PVSS verification: [2](#0-1) 

2. **Sigma Protocol Verification**: This calls the tuple homomorphism verification which computes MSM terms from the proof response: [3](#0-2) 

3. **Unbounded Iteration**: The `msm_terms()` function iterates over ALL elements in the attacker-controlled `Witness.chunked_values`: [4](#0-3) 

4. **Expensive Operation Per Element**: For each chunk vector, `le_chunks_to_scalar()` performs O(num_chunks) field multiplications and additions: [5](#0-4) 

**Attack Scenario:**
- A Byzantine validator creates a transcript with `proof.z.chunked_values` having dimensions like `[1000][1000][1000]` instead of the expected `[4][10][8]`
- This forces honest validators to perform ~1 billion field operations during `msm_terms()` computation
- The verification will eventually fail due to dimension mismatch in `merge_msm_terms()`, but only AFTER the computational cost has been incurred
- There are no size limits during deserialization (the `CanonicalDeserialize` trait provides no bounds checking)
- No timeout mechanism exists for transcript verification

The vulnerability violates the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits."

## Impact Explanation

This qualifies as **Medium Severity** per the Aptos bug bounty criteria:

1. **Validator Node Slowdowns** (High severity criterion): Honest validators processing the malicious transcript experience significant CPU consumption, potentially causing:
   - Block proposal delays
   - Consensus participation degradation  
   - Resource exhaustion on validator nodes

2. **DKG Ceremony Failure**: If validators timeout during transcript verification, the DKG ceremony cannot complete, preventing:
   - Epoch transitions
   - Randomness generation for the next epoch
   - Validator set updates

3. **State Inconsistencies** (Medium severity criterion): Failed DKG ceremonies require manual intervention and may cause validators to disagree on epoch state.

The attack does not directly cause fund loss or permanent network partition, but can severely degrade network availability and require coordinated recovery efforts.

## Likelihood Explanation

**Likelihood: High**

- **Attacker Requirements**: The attacker must be a validator (or compromise one), which is feasible under the Byzantine fault tolerance model assuming up to 1/3 malicious validators
- **Ease of Exploitation**: Simple to execute - just craft a transcript with oversized `Witness.chunked_values` dimensions during serialization
- **Detection Difficulty**: The malicious transcript appears valid during initial deserialization; the issue only manifests during verification
- **No Existing Protections**: There are no size checks during deserialization, no timeouts on verification, and no early validation of proof structure dimensions

## Recommendation

Add explicit bounds checking on `Witness` dimensions before processing. Validate that the proof structure matches expected dimensions based on the secret sharing configuration:

```rust
// In weighted_transcriptv2.rs, before line 514 (hom.verify call):
fn validate_proof_dimensions<E: Pairing>(
    proof: &hkzg_chunked_elgamal_commit::Proof<E>,
    sc: &SecretSharingConfig<E>,
    pp: &PublicParameters<E>,
) -> anyhow::Result<()> {
    let expected_num_players = sc.get_total_num_players();
    let expected_max_weight = sc.get_max_weight();
    let expected_num_chunks = num_chunks_per_scalar::<E::ScalarField>(pp.ell);
    
    // Validate Witness dimensions match configuration
    if proof.z.chunked_plaintexts.len() != expected_num_players {
        bail!("Invalid proof: unexpected number of players in witness");
    }
    
    for (i, player_chunks) in proof.z.chunked_plaintexts.iter().enumerate() {
        let player_weight = sc.get_player_weight(&sc.get_player(i));
        if player_chunks.len() != player_weight {
            bail!("Invalid proof: unexpected weight for player {}", i);
        }
        
        for chunks in player_chunks.iter() {
            if chunks.len() > expected_num_chunks {
                bail!("Invalid proof: too many chunks");
            }
        }
    }
    
    Ok(())
}

// Call before verification:
validate_proof_dimensions(&self.sharing_proof.SoK, sc, pp)?;
```

Additionally, implement a timeout mechanism for transcript verification to bound worst-case execution time.

## Proof of Concept

```rust
#[test]
fn test_dos_via_oversized_witness() {
    use ark_bls12_381::Fr;
    use crate::pvss::chunky::chunked_scalar_mul::Witness;
    use crate::Scalar;
    
    // Create a legitimate-looking but oversized Witness
    let attacker_witness = Witness {
        chunked_values: vec![
            vec![
                vec![
                    vec![Scalar(Fr::from(1u64)); 1000]; // 1000 chunks per value
                    1000 // 1000 values per player
                ]; 
                1000 // 1000 players
            ]
        ]
    };
    
    // Serialize this into a proof
    let mut proof_bytes = Vec::new();
    attacker_witness.serialize_compressed(&mut proof_bytes).unwrap();
    
    println!("Malicious proof size: {} bytes", proof_bytes.len());
    
    // When a validator receives and deserializes this proof, 
    // the msm_terms() call will iterate over 1_000_000_000 elements,
    // each requiring le_chunks_to_scalar() computation (1000 field ops each),
    // totaling ~1 trillion field operations before verification fails.
    
    // Time the msm_terms computation
    let hom = chunked_scalar_mul::Homomorphism {
        base: G2Affine::generator(),
        ell: 16,
    };
    
    let start = std::time::Instant::now();
    let _terms = hom.msm_terms(&attacker_witness);
    let duration = start.elapsed();
    
    println!("msm_terms() took {:?} for oversized witness", duration);
    // Expected: several minutes of CPU time before verification can proceed
}
```

## Notes

The vulnerability is exploitable during DKG transcript verification when validators process proofs from potentially malicious peers. The lack of input validation on proof dimensions allows computational DoS attacks that can prevent DKG ceremony completion. This is a protocol-level issue requiring consensus among validators to patch, as all validators must enforce the same dimension bounds to maintain deterministic verification.

### Citations

**File:** crates/aptos-dkg/src/pvss/chunky/chunked_scalar_mul.rs (L73-78)
```rust
#[derive(
    SigmaProtocolWitness, CanonicalSerialize, CanonicalDeserialize, Clone, Debug, PartialEq, Eq,
)]
pub struct Witness<F: PrimeField> {
    pub chunked_values: Vec<Vec<Vec<Scalar<F>>>>,
}
```

**File:** crates/aptos-dkg/src/pvss/chunky/chunked_scalar_mul.rs (L98-116)
```rust
    fn msm_terms(&self, input: &Self::Domain) -> Self::CodomainShape<Self::MsmInput> {
        let rows: Vec<Vec<Self::MsmInput>> = input
            .chunked_values
            .iter()
            .map(|row| {
                row.iter()
                    .map(|chunks| MsmInput {
                        bases: vec![self.base.clone()],
                        scalars: vec![le_chunks_to_scalar(
                            self.ell,
                            &Scalar::slice_as_inner(chunks),
                        )],
                    })
                    .collect()
            })
            .collect();

        CodomainShape(rows)
    }
```

**File:** types/src/dkg/real_dkg/mod.rs (L368-374)
```rust
        trx.main.verify(
            &params.pvss_config.wconfig,
            &params.pvss_config.pp,
            &spks,
            &all_eks,
            &aux,
        )?;
```

**File:** crates/aptos-dkg/src/sigma_protocol/homomorphism/tuple.rs (L358-358)
```rust
        let (first_msm_terms_of_response, second_msm_terms_of_response) = self.msm_terms(&proof.z);
```

**File:** crates/aptos-dkg/src/pvss/chunky/chunks.rs (L32-48)
```rust
pub fn le_chunks_to_scalar<F: PrimeField>(num_bits: u8, chunks: &[F]) -> F {
    assert!(
        num_bits.is_multiple_of(8) && num_bits > 0 && num_bits <= 64, // TODO: so make num_bits a u8?
        "Invalid chunk size"
    );

    let base = F::from(1u128 << num_bits); // need u128 in the case where `num_bits` is 64, because of `chunk * multiplier`
    let mut acc = F::zero();
    let mut multiplier = F::one();

    for &chunk in chunks {
        acc += chunk * multiplier;
        multiplier *= base;
    }

    acc
}
```
