# Audit Report

## Title
Resource Group Operations Bypass Write Operation Limit via Inner Operation Miscounting

## Summary
Resource group operations in `check_change_set()` count multiple inner resource operations as a single write operation, allowing transactions to bypass the `max_write_ops_per_transaction` limit of 8192. A single resource group containing thousands of individual resource operations is validated as only one write operation, enabling resource exhaustion attacks on validator nodes.

## Finding Description

The vulnerability exists in how resource groups are counted when validating transaction write operation limits.

In `check_change_set()`, the validation checks: [1](#0-0) 

The `num_write_ops()` implementation only counts top-level entries in the resource write set: [2](#0-1) 

For resource groups, the `GroupWrite` structure contains multiple `inner_ops` representing individual resource operations within the group: [3](#0-2) 

When resources are added/modified/deleted within a group, each operation is stored in `inner_ops`: [4](#0-3) 

The critical issue: A resource group containing N inner operations (e.g., 10,000 individual resource writes) is counted as **1 write operation** instead of N operations. The `max_write_ops_per_transaction` limit is set to 8192: [5](#0-4) 

**Attack Scenario:**
1. Attacker creates a transaction that adds 10,000 small resources to a single resource group
2. The validation counts this as 1 write operation (passes the 8192 limit)
3. The transaction is processed with 10,000 actual storage modifications
4. This bypasses the intended write operation limit by 21%+

This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits."

Additionally, individual resources within `inner_ops` are never validated against `max_bytes_per_write_op` - only the aggregate group size is checked via `materialized_size()`: [6](#0-5) 

## Impact Explanation

This vulnerability qualifies as **HIGH severity** under the Aptos bug bounty program for the following reasons:

1. **Validator Node Slowdowns**: Transactions with abnormally high numbers of write operations can cause performance degradation on validator nodes during transaction processing and state commitment phases.

2. **Resource Exhaustion**: The `max_write_ops_per_transaction` limit exists to prevent excessive storage I/O operations. Bypassing this limit allows attackers to force validators to perform significantly more work than intended.

3. **Protocol Violation**: The write operation limit is an explicit protocol parameter designed to bound transaction execution costs. This bypass violates that security guarantee.

The attack requires no special privileges - any transaction sender can exploit this by using resource groups. While not a consensus safety violation, it represents a significant operational security issue that could impact network performance and availability.

## Likelihood Explanation

**Likelihood: HIGH**

The attack is highly likely to be exploitable because:

1. **Easy to Execute**: Any user can create transactions with resource groups containing many inner operations
2. **No Special Requirements**: Requires no validator access, governance control, or special permissions
3. **Common Feature**: Resource groups are a standard Aptos feature, making this attack surface readily available
4. **Clear Bypass**: The counting logic definitively treats N operations as 1, making the bypass deterministic

The only limitation is that the total size of all operations must still fit within `max_bytes_all_write_ops_per_transaction`, but this is a separate limit that can be satisfied with many small resources.

## Recommendation

Modify `num_write_ops()` to count individual resource operations within resource groups. Update the implementation to:

```rust
fn num_write_ops(&self) -> usize {
    let resource_ops = self.resource_write_set()
        .iter()
        .map(|(_, v)| match v {
            AbstractResourceWriteOp::WriteResourceGroup(group_write) => {
                // Count each inner operation in the group
                group_write.inner_ops().len()
            },
            _ => 1,
        })
        .sum::<usize>();
    
    resource_ops + self.aggregator_v1_write_set().len()
}
```

Additionally, add individual size validation for resources within groups in `check_change_set()` to ensure each inner operation respects `max_bytes_per_write_op`.

## Proof of Concept

```rust
// Rust test demonstrating the bypass
#[test]
fn test_resource_group_write_ops_bypass() {
    use aptos_vm_types::abstract_write_op::*;
    use aptos_vm_types::change_set::*;
    use aptos_types::state_store::state_key::StateKey;
    use std::collections::BTreeMap;
    
    // Create a resource group with 10,000 inner operations
    let mut inner_ops = BTreeMap::new();
    for i in 0..10000 {
        let tag = /* create StructTag */;
        let write_op = WriteOp::legacy_creation(vec![0u8; 10].into());
        inner_ops.insert(tag, (write_op, None));
    }
    
    let group_key = StateKey::raw(&[0u8; 32]);
    let group_write = GroupWrite::new(
        WriteOp::legacy_creation(Bytes::new()),
        inner_ops,
        ResourceGroupSize::Concrete(100000),
        0,
    );
    
    let mut resource_write_set = BTreeMap::new();
    resource_write_set.insert(
        group_key,
        AbstractResourceWriteOp::WriteResourceGroup(group_write)
    );
    
    let change_set = VMChangeSet::new(
        resource_write_set,
        vec![],
        BTreeMap::new(),
        BTreeMap::new(),
        BTreeMap::new(),
    );
    
    // This should be 10,000 but is actually 1
    assert_eq!(change_set.num_write_ops(), 1);
    
    // With max_write_ops_per_transaction = 8192, this bypasses the limit
    // as 10,000 operations are counted as just 1
}
```

**Notes**

The vulnerability specifically violates the question's concern about "both individual and group contexts" - individual resource operations within groups are neither counted separately for operation limits nor validated individually for size limits. Only aggregate group-level checks are performed, creating a significant bypass of intended resource limits.

### Citations

**File:** aptos-move/aptos-vm-types/src/storage/change_set_configs.rs (L95-98)
```rust
        if self.max_write_ops_per_transaction != 0
            && change_set.num_write_ops() as u64 > self.max_write_ops_per_transaction
        {
            return storage_write_limit_reached(Some("Too many write ops."));
```

**File:** aptos-move/aptos-vm-types/src/change_set.rs (L856-860)
```rust
    fn num_write_ops(&self) -> usize {
        // Note: we only use resources and aggregators because they use write ops directly,
        // and deltas & events are not part of these.
        self.resource_write_set().len() + self.aggregator_v1_write_set().len()
    }
```

**File:** aptos-move/aptos-vm-types/src/abstract_write_op.rs (L44-68)
```rust
    pub fn materialized_size(&self) -> WriteOpSize {
        use AbstractResourceWriteOp::*;
        match self {
            Write(write) => write.write_op_size(),
            WriteWithDelayedFields(WriteWithDelayedFieldsOp {
                write_op,
                materialized_size,
                ..
            }) => write_op.project_write_op_size(|| *materialized_size),
            WriteResourceGroup(GroupWrite {
                metadata_op: write_op,
                maybe_group_op_size,
                ..
            }) => write_op.project_write_op_size(|| maybe_group_op_size.map(|x| x.get())),
            InPlaceDelayedFieldChange(InPlaceDelayedFieldChangeOp {
                materialized_size, ..
            })
            | ResourceGroupInPlaceDelayedFieldChange(ResourceGroupInPlaceDelayedFieldChangeOp {
                materialized_size,
                ..
            }) => WriteOpSize::Modification {
                write_len: *materialized_size,
            },
        }
    }
```

**File:** aptos-move/aptos-vm-types/src/abstract_write_op.rs (L150-172)
```rust
#[derive(PartialEq, Eq, Clone, Debug)]
pub struct GroupWrite {
    /// Op of the correct kind (creation / modification / deletion) and metadata, and
    /// the size of the group after the updates encoded in the bytes (no bytes for
    /// deletion). Relevant during block execution, where the information read to
    /// derive metadata_op will be validated during parallel execution to make sure
    /// it is correct, and the bytes will be replaced after the transaction is committed
    /// with correct serialized group update to obtain storage WriteOp.
    pub metadata_op: WriteOp,
    /// Updates to individual group members. WriteOps are 'legacy', i.e. no metadata.
    /// If the metadata_op is a deletion, all (correct) inner_ops should be deletions,
    /// and if metadata_op is a creation, then there may not be a creation inner op.
    /// Not vice versa, e.g. for deleted inner ops, other untouched resources may still
    /// exist in the group. Note: During parallel block execution, due to speculative
    /// reads, this invariant may be violated (and lead to speculation error if observed)
    /// but guaranteed to fail validation and lead to correct re-execution in that case.
    pub(crate) inner_ops: BTreeMap<StructTag, (WriteOp, Option<TriompheArc<MoveTypeLayout>>)>,
    /// Group size as used for gas charging, None if (metadata_)op is Deletion.
    pub(crate) maybe_group_op_size: Option<ResourceGroupSize>,
    // TODO: consider Option<u64> to be able to represent a previously non-existent group,
    //       if useful
    pub(crate) prev_group_size: u64,
}
```

**File:** aptos-move/aptos-vm/src/move_vm_ext/write_op_converter.rs (L173-205)
```rust
        for (tag, current_op) in group_changes {
            // We take speculative group size prior to the transaction, and update it based on the change-set.
            // For each tagged resource in the change set, we subtract the previous size tagged resource size,
            // and then add new tagged resource size.
            //
            // The reason we do not instead get and add the sizes of the resources in the group,
            // but not in the change-set, is to avoid creating unnecessary R/W conflicts (the resources
            // in the change-set are already read, but the other resources are not).
            if !matches!(current_op, MoveStorageOp::New(_)) {
                let old_tagged_value_size = self.remote.resource_size_in_group(state_key, &tag)?;
                let old_size = group_tagged_resource_size(&tag, old_tagged_value_size)?;
                decrement_size_for_remove_tag(&mut post_group_size, old_size)?;
            }

            match &current_op {
                MoveStorageOp::Modify((data, _)) | MoveStorageOp::New((data, _)) => {
                    let new_size = group_tagged_resource_size(&tag, data.len())?;
                    increment_size_for_add_tag(&mut post_group_size, new_size)?;
                },
                MoveStorageOp::Delete => {},
            };

            let legacy_op = match current_op {
                MoveStorageOp::Delete => (WriteOp::legacy_deletion(), None),
                MoveStorageOp::Modify((data, maybe_layout)) => {
                    (WriteOp::legacy_modification(data), maybe_layout)
                },
                MoveStorageOp::New((data, maybe_layout)) => {
                    (WriteOp::legacy_creation(data), maybe_layout)
                },
            };
            inner_ops.insert(tag, legacy_op);
        }
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L173-177)
```rust
        [
            max_write_ops_per_transaction: NumSlots,
            { 11.. => "max_write_ops_per_transaction" },
            8192,
        ],
```
