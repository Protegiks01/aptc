# Audit Report

## Title
Compression Bomb Denial of Service via Unbounded Decompression in Backup Restore

## Summary
The `read_all()` function in the backup CLI loads entire decompressed backup files into memory without size limits, enabling compression bomb attacks that can crash validator nodes during restore operations.

## Finding Description

The backup restore system uses gzip compression for all backup data stored in cloud storage (GCS, S3, Azure). When reading backup files, the system decompresses data through shell commands without validating the decompressed size.

**Vulnerable Code Flow:**

1. **Decompression Configuration**: All CommandAdapter configurations pipe backup data through `gzip -cd` for decompression: [1](#0-0) [2](#0-1) 

2. **Unbounded Memory Allocation**: The `read_all()` function reads the entire decompressed stream into memory: [3](#0-2) 

3. **Critical Usage Points**: These functions load compressed backup metadata and proofs: [4](#0-3) [5](#0-4) 

**Attack Scenario:**

If an attacker gains access to backup storage (via compromise, malicious insider, or supply chain attack), they can replace legitimate backup files with compression bombs:
- A 1KB gzipped file expanding to 10GB when decompressed
- When `read_all()` calls `read_to_end()`, it allocates memory for the entire 10GB
- The process exhausts available memory and crashes

**Invariant Violated:**
**Resource Limits** (Invariant #9): All operations must respect gas, storage, and computational limits. The unbounded memory allocation violates this invariant by allowing decompression to consume arbitrary amounts of memory.

## Impact Explanation

**High Severity** - Meets "Validator node slowdowns" and "API crashes" criteria:

- **Validator Bootstrap Failure**: Validators restoring from backup cannot complete the operation, preventing node startup
- **Recovery Prevention**: Nodes recovering from data corruption cannot restore, causing extended downtime
- **Availability Impact**: While not permanent network failure, it prevents individual validators from joining/recovering

The impact is contained to nodes attempting restore operations, not the entire network, qualifying this as High rather than Critical severity.

## Likelihood Explanation

**Medium-to-Low Likelihood**:

**Required Conditions:**
1. Attacker must compromise backup storage (GCS/S3/Azure buckets) OR be a malicious insider
2. Validator must attempt restore operation (not frequent - only during bootstrap/recovery)
3. No additional validation or size checks exist in the restore pipeline

**Mitigating Factors:**
- Backup storage is typically access-controlled and monitored
- Restore operations are infrequent
- Operators can validate backup integrity through other means

**However**: Given the high impact when exploited, and increasing supply chain attacks on cloud infrastructure, this represents a realistic defense-in-depth concern.

## Recommendation

Implement size limits on decompressed data to prevent unbounded memory allocation:

```rust
async fn read_all(&self, file_handle: &FileHandleRef) -> Result<Vec<u8>> {
    const MAX_DECOMPRESSED_SIZE: usize = 100 * 1024 * 1024; // 100MB limit
    
    let mut file = self.open_for_read(file_handle).await?;
    let mut bytes = Vec::new();
    
    // Use take() to limit maximum bytes read
    let mut limited_reader = file.take(MAX_DECOMPRESSED_SIZE as u64);
    let bytes_read = limited_reader.read_to_end(&mut bytes).await?;
    
    // Check if we hit the limit (possible compression bomb)
    if bytes_read >= MAX_DECOMPRESSED_SIZE {
        bail!(
            "Decompressed file size exceeded maximum allowed ({} bytes). \
             Possible compression bomb in: {}",
            MAX_DECOMPRESSED_SIZE,
            file_handle
        );
    }
    
    Ok(bytes)
}
```

**Additional Recommendations:**
1. Add configuration parameter for maximum file size
2. Log warnings when files approach size limits
3. Implement streaming parsers for large files (JSON/BCS) to avoid loading entire files into memory
4. Add checksum validation before decompression

## Proof of Concept

```rust
// Create a compression bomb test
#[tokio::test]
async fn test_compression_bomb_protection() {
    use std::io::Write;
    use flate2::Compression;
    use flate2::write::GzEncoder;
    
    // Create a 10MB file of zeros (highly compressible)
    let uncompressed = vec![0u8; 10 * 1024 * 1024];
    
    // Compress it (will be very small, ~10KB)
    let mut encoder = GzEncoder::new(Vec::new(), Compression::best());
    encoder.write_all(&uncompressed).unwrap();
    let compressed = encoder.finish().unwrap();
    
    println!("Compression ratio: {} bytes -> {} bytes", 
             uncompressed.len(), compressed.len());
    
    // Attempt to restore this through backup-cli
    // Expected: Should fail with size limit exceeded
    // Actual (without fix): Will allocate 10MB and potentially crash
    
    // Setup test backup storage with compressed bomb
    // ... (full integration test would require backup storage setup)
    
    assert!(compressed.len() < 100_000); // Compressed is small
    assert!(uncompressed.len() > 1_000_000); // Uncompressed is large
}
```

**Reproduction Steps:**
1. Create a highly compressible file (e.g., 100MB of zeros)
2. Gzip compress it (produces ~100KB file)
3. Upload as backup manifest or proof file to backup storage
4. Attempt restore operation: `aptos-db-tool restore bootstrap-db --metadata-cache-dir <dir> --target-version <version>`
5. Observe memory consumption spike and potential OOM crash

## Notes

**Trust Boundary Consideration**: While backup storage is typically controlled by trusted validator operators, this vulnerability represents a defense-in-depth issue. Compromised backup infrastructure (via supply chain attacks, credential theft, or insider threats) should not enable trivial DoS attacks on validators.

**Cloud Storage Context**: The vulnerability affects all cloud storage backends (GCP, S3, Azure) identically, as all use `gzip -cd` for decompression without size validation.

### Citations

**File:** storage/backup/backup-cli/src/storage/command_adapter/sample_configs/gcp.sample.yaml (L19-21)
```yaml
  open_for_read: |
    # route file handle content to stdout
    gsutil -q cp "gs://$BUCKET/$SUB_DIR/$FILE_HANDLE" - | gzip -cd
```

**File:** storage/backup/backup-cli/src/storage/command_adapter/sample_configs/s3.sample.yaml (L19-21)
```yaml
  open_for_read: |
    # route file handle content to stdout
    aws s3 cp "s3://$BUCKET/$SUB_DIR/$FILE_HANDLE" - | gzip -cd
```

**File:** storage/backup/backup-cli/src/utils/storage_ext.rs (L24-29)
```rust
    async fn read_all(&self, file_handle: &FileHandleRef) -> Result<Vec<u8>> {
        let mut file = self.open_for_read(file_handle).await?;
        let mut bytes = Vec::new();
        file.read_to_end(&mut bytes).await?;
        Ok(bytes)
    }
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L123-126)
```rust
        let manifest: StateSnapshotBackup =
            self.storage.load_json_file(&self.manifest_handle).await?;
        let (txn_info_with_proof, li): (TransactionInfoWithProof, LedgerInfoWithSignatures) =
            self.storage.load_bcs_file(&manifest.proof).await?;
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L191-193)
```rust
                    let blobs = Self::read_state_value(&storage, chunk.blobs.clone()).await?;
                    let proof = storage.load_bcs_file(&chunk.proof).await?;
                    Result::<_>::Ok((chunk_idx, chunk, blobs, proof))
```
