# Audit Report

## Title
Missing Application-Level Integrity Verification for NewBlockEvents in Consensus Leader Selection

## Summary
The consensus layer reads `NewBlockEvents` from storage for leader reputation calculations without verifying them against their cryptographic hash stored in `TransactionInfo`. If storage corruption creates structurally valid but semantically incorrect BCS data, the corrupted events will be used in consensus decisions, potentially causing liveness issues and non-deterministic leader selection across nodes.

## Finding Description

The Aptos consensus layer uses `NewBlockEvents` to calculate leader reputation scores, which directly influence proposer selection in AptosBFT. These events are read from local storage and deserialized via BCS without cryptographic verification against the `event_root_hash` stored in `TransactionInfo`.

**Storage Layer**: Events are stored in RocksDB with XXH3 block-level checksums [1](#0-0) 

**Event Accumulator**: During writes, event hashes are computed and stored in a Merkle accumulator [2](#0-1) 

**Consensus Read Path**: When reading NewBlockEvents for leader reputation, the consensus layer calls `get_latest_block_events` [3](#0-2) , which retrieves events from storage [4](#0-3)  and deserializes them directly via `bcs::from_bytes` [5](#0-4)  **without** verifying against the event_root_hash.

**Missing Verification**: While the codebase includes `verify_events_against_root_hash` for transaction proofs [6](#0-5) , this verification is **not performed** when consensus reads events from local storage.

**BCS Limitation**: BCS deserialization only validates structural correctness [7](#0-6) . A bit flip changing epoch from `100` to `101`, or corrupting the `proposer` field, produces valid BCS that deserializes successfully.

**Consensus Impact**: Corrupted NewBlockEvents directly affect leader selection through vote counting, proposal counting, and failed proposer tracking [8](#0-7) , which compute reputation weights used in proposer election [9](#0-8) .

## Impact Explanation

This vulnerability meets **High Severity** criteria per the Aptos bug bounty program:
- **Validator node slowdowns**: Nodes with corrupted NewBlockEvents will calculate different leader weights, potentially selecting different proposers than the rest of the network, causing proposal failures and round delays
- **Significant protocol violations**: Breaks the "Deterministic Execution" invariant - validators must produce identical behavior for identical inputs, but corrupted local storage creates non-deterministic leader selection

If storage corruption occurs on multiple validators with different corruption patterns, the network could experience persistent liveness degradation due to leader selection mismatches. While this doesn't break consensus safety (wrong proposals are rejected), it violates the principle that all nodes should have identical views of leader reputation history.

## Likelihood Explanation

**Likelihood: Medium-Low**

The vulnerability requires storage corruption to occur, which can happen through:
1. **In-memory corruption**: RAM bit flips after RocksDB read but before BCS deserialization (bypasses RocksDB checksums entirely)
2. **Hardware failures**: Disk corruption, CPU bugs, or DMA errors
3. **Software bugs**: Errors in the storage stack between RocksDB verification and application use

While RocksDB checksums provide strong protection, they only detect corruption at the block level during disk I/O. The RocksDB error handling maps `ErrorKind::Corruption` to application errors [10](#0-9) , but this doesn't protect against post-read corruption.

The likelihood is elevated because:
- Validators run continuously, increasing cumulative probability of hardware errors
- Consensus-critical data deserves defense-in-depth protection beyond storage layer checksums
- The event_root_hash is already available and computed during block execution but unused during reads

## Recommendation

Implement application-level integrity verification when reading NewBlockEvents for consensus operations:

```rust
// In storage/aptosdb/src/db/aptosdb_internal.rs, modify to_api_block_info:
pub(super) fn to_api_block_info(
    &self,
    block_height: u64,
    block_info: BlockInfo,
) -> Result<(Version, Version, NewBlockEvent)> {
    let committed_version = self.get_latest_ledger_info_version()?;
    ensure!(
        block_info.first_version() <= committed_version,
        "block first version {} > committed version {committed_version}",
        block_info.first_version(),
    );

    let new_block_event = self
        .ledger_db
        .event_db()
        .expect_new_block_event(block_info.first_version())?;

    // ADD: Verify event against TransactionInfo
    let txn_info = self.ledger_db.transaction_info_db()
        .get_transaction_info(block_info.first_version())?;
    let event_hash = CryptoHash::hash(&new_block_event);
    let computed_root = InMemoryEventAccumulator::from_leaves(&[event_hash]).root_hash();
    ensure!(
        computed_root == txn_info.event_root_hash(),
        "NewBlockEvent integrity check failed: computed hash {:?} != stored hash {:?}",
        computed_root, txn_info.event_root_hash()
    );

    let last_version = match self.get_raw_block_info_by_height(block_height + 1) {
        Ok(next_block_info) => next_block_info.first_version() - 1,
        Err(AptosDbError::NotFound(..)) => committed_version,
        Err(err) => return Err(err),
    };

    Ok((
        block_info.first_version(),
        last_version,
        bcs::from_bytes(new_block_event.event_data())?,
    ))
}
```

This adds cryptographic verification using the existing event accumulator infrastructure, ensuring corrupted events are detected before affecting consensus.

## Proof of Concept

The following demonstrates that BCS deserialization cannot detect semantic corruption:

```rust
#[test]
fn test_bcs_cannot_detect_corruption() {
    use aptos_types::account_config::NewBlockEvent;
    use aptos_types::account_address::AccountAddress;
    
    // Create a valid NewBlockEvent
    let original = NewBlockEvent::new(
        AccountAddress::from_hex_literal("0x1").unwrap(),
        100, // epoch
        200, // round  
        300, // height
        vec![0xFF, 0xFF], // votes
        AccountAddress::from_hex_literal("0xA").unwrap(),
        vec![1, 2, 3], // failed proposers
        1234567890, // timestamp
    );
    
    // Serialize to BCS
    let mut bytes = bcs::to_bytes(&original).unwrap();
    
    // Corrupt the epoch field (flip one bit)
    // This changes epoch from 100 to 101, creating wrong consensus data
    bytes[32] ^= 0x01;
    
    // BCS deserialization SUCCEEDS with corrupted data
    let corrupted: NewBlockEvent = bcs::from_bytes(&bytes).unwrap();
    
    // The corrupted event has wrong epoch, but BCS doesn't detect it
    assert_eq!(corrupted.epoch(), 101); // Corruption not detected!
    assert_ne!(corrupted.epoch(), original.epoch());
    
    // This corrupted event would be used in consensus leader selection,
    // causing the node to have a different view of history than other validators
}
```

## Notes

This vulnerability represents a violation of defense-in-depth principles for consensus-critical data. While RocksDB checksums provide primary protection, application-level verification using the existing event_root_hash would prevent corrupted data from affecting consensus decisions. The fix leverages already-computed cryptographic commitments stored in TransactionInfo, adding minimal performance overhead while significantly improving system resilience against storage layer failures.

### Citations

**File:** storage/aptosdb/src/ledger_db/event_db.rs (L169-170)
```rust
                batch.put::<EventSchema>(&(version, idx as u64), event)
            })?;
```

**File:** storage/aptosdb/src/ledger_db/event_db.rs (L174-184)
```rust
            let event_hashes: Vec<HashValue> = events.iter().map(ContractEvent::hash).collect();
            let (_root_hash, writes) =
                MerkleAccumulator::<EmptyReader, EventAccumulatorHasher>::append(
                    &EmptyReader,
                    0,
                    &event_hashes,
                )?;

            writes.into_iter().try_for_each(|(pos, hash)| {
                batch.put::<EventAccumulatorSchema>(&(version, pos), &hash)
            })?;
```

**File:** consensus/src/liveness/leader_reputation.rs (L78-78)
```rust
        let events = self.aptos_db.get_latest_block_events(limit)?;
```

**File:** consensus/src/liveness/leader_reputation.rs (L353-461)
```rust
    pub fn count_votes(
        &self,
        epoch_to_candidates: &HashMap<u64, Vec<Author>>,
        history: &[NewBlockEvent],
    ) -> HashMap<Author, u32> {
        Self::count_votes_custom(
            epoch_to_candidates,
            history,
            self.voter_window_size,
            self.reputation_window_from_stale_end,
        )
    }

    pub fn count_votes_custom(
        epoch_to_candidates: &HashMap<u64, Vec<Author>>,
        history: &[NewBlockEvent],
        window_size: usize,
        from_stale_end: bool,
    ) -> HashMap<Author, u32> {
        Self::history_iter(history, epoch_to_candidates, window_size, from_stale_end).fold(
            HashMap::new(),
            |mut map, meta| {
                match Self::bitvec_to_voters(
                    &epoch_to_candidates[&meta.epoch()],
                    &meta.previous_block_votes_bitvec().clone().into(),
                ) {
                    Ok(voters) => {
                        for &voter in voters {
                            let count = map.entry(voter).or_insert(0);
                            *count += 1;
                        }
                    },
                    Err(msg) => {
                        error!(
                            "Voter conversion from bitmap failed at epoch {}, round {}: {}",
                            meta.epoch(),
                            meta.round(),
                            msg
                        )
                    },
                }
                map
            },
        )
    }

    pub fn count_proposals(
        &self,
        epoch_to_candidates: &HashMap<u64, Vec<Author>>,
        history: &[NewBlockEvent],
    ) -> HashMap<Author, u32> {
        Self::count_proposals_custom(
            epoch_to_candidates,
            history,
            self.proposer_window_size,
            self.reputation_window_from_stale_end,
        )
    }

    pub fn count_proposals_custom(
        epoch_to_candidates: &HashMap<u64, Vec<Author>>,
        history: &[NewBlockEvent],
        window_size: usize,
        from_stale_end: bool,
    ) -> HashMap<Author, u32> {
        Self::history_iter(history, epoch_to_candidates, window_size, from_stale_end).fold(
            HashMap::new(),
            |mut map, meta| {
                let count = map.entry(meta.proposer()).or_insert(0);
                *count += 1;
                map
            },
        )
    }

    pub fn count_failed_proposals(
        &self,
        epoch_to_candidates: &HashMap<u64, Vec<Author>>,
        history: &[NewBlockEvent],
    ) -> HashMap<Author, u32> {
        Self::history_iter(
            history,
            epoch_to_candidates,
            self.proposer_window_size,
            self.reputation_window_from_stale_end,
        )
        .fold(HashMap::new(), |mut map, meta| {
            match Self::indices_to_validators(
                &epoch_to_candidates[&meta.epoch()],
                meta.failed_proposer_indices(),
            ) {
                Ok(failed_proposers) => {
                    for &failed_proposer in failed_proposers {
                        let count = map.entry(failed_proposer).or_insert(0);
                        *count += 1;
                    }
                },
                Err(msg) => {
                    error!(
                        "Failed proposer conversion from indices failed at epoch {}, round {}: {}",
                        meta.epoch(),
                        meta.round(),
                        msg
                    )
                },
            }
            map
        })
    }
```

**File:** consensus/src/liveness/leader_reputation.rs (L521-550)
```rust
impl ReputationHeuristic for ProposerAndVoterHeuristic {
    fn get_weights(
        &self,
        epoch: u64,
        epoch_to_candidates: &HashMap<u64, Vec<Author>>,
        history: &[NewBlockEvent],
    ) -> Vec<u64> {
        assert!(epoch_to_candidates.contains_key(&epoch));

        let (votes, proposals, failed_proposals) =
            self.aggregation
                .get_aggregated_metrics(epoch_to_candidates, history, &self.author);

        epoch_to_candidates[&epoch]
            .iter()
            .map(|author| {
                let cur_votes = *votes.get(author).unwrap_or(&0);
                let cur_proposals = *proposals.get(author).unwrap_or(&0);
                let cur_failed_proposals = *failed_proposals.get(author).unwrap_or(&0);

                if cur_failed_proposals * 100
                    > (cur_proposals + cur_failed_proposals) * self.failure_threshold_percent
                {
                    self.failed_weight
                } else if cur_proposals > 0 || cur_votes > 0 {
                    self.active_weight
                } else {
                    self.inactive_weight
                }
            })
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L742-776)
```rust
    fn get_latest_block_events(&self, num_events: usize) -> Result<Vec<EventWithVersion>> {
        gauged_api("get_latest_block_events", || {
            let latest_version = self.get_synced_version()?;
            if !self.skip_index_and_usage {
                return self.get_events(
                    &new_block_event_key(),
                    u64::MAX,
                    Order::Descending,
                    num_events as u64,
                    latest_version.unwrap_or(0),
                );
            }

            let db = self.ledger_db.metadata_db_arc();
            let mut iter = db.rev_iter::<BlockInfoSchema>()?;
            iter.seek_to_last();

            let mut events = Vec::with_capacity(num_events);
            for item in iter {
                let (_block_height, block_info) = item?;
                let first_version = block_info.first_version();
                if latest_version.as_ref().is_some_and(|v| first_version <= *v) {
                    let event = self
                        .ledger_db
                        .event_db()
                        .expect_new_block_event(first_version)?;
                    events.push(EventWithVersion::new(first_version, event));
                    if events.len() == num_events {
                        break;
                    }
                }
            }

            Ok(events)
        })
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L403-403)
```rust
            bcs::from_bytes(new_block_event.event_data())?,
```

**File:** types/src/transaction/mod.rs (L2629-2643)
```rust
fn verify_events_against_root_hash(
    events: &[ContractEvent],
    transaction_info: &TransactionInfo,
) -> Result<()> {
    let event_hashes: Vec<_> = events.iter().map(CryptoHash::hash).collect();
    let event_root_hash = InMemoryEventAccumulator::from_leaves(&event_hashes).root_hash();
    ensure!(
        event_root_hash == transaction_info.event_root_hash(),
        "The event root hash calculated doesn't match that carried on the \
                         transaction info! Calculated hash {:?}, transaction info hash {:?}",
        event_root_hash,
        transaction_info.event_root_hash()
    );
    Ok(())
}
```

**File:** types/src/account_config/events/new_block.rs (L69-71)
```rust
    pub fn try_from_bytes(bytes: &[u8]) -> Result<Self> {
        bcs::from_bytes(bytes).map_err(Into::into)
    }
```

**File:** storage/schemadb/src/lib.rs (L389-407)
```rust
fn to_db_err(rocksdb_err: rocksdb::Error) -> AptosDbError {
    match rocksdb_err.kind() {
        ErrorKind::Incomplete => AptosDbError::RocksDbIncompleteResult(rocksdb_err.to_string()),
        ErrorKind::NotFound
        | ErrorKind::Corruption
        | ErrorKind::NotSupported
        | ErrorKind::InvalidArgument
        | ErrorKind::IOError
        | ErrorKind::MergeInProgress
        | ErrorKind::ShutdownInProgress
        | ErrorKind::TimedOut
        | ErrorKind::Aborted
        | ErrorKind::Busy
        | ErrorKind::Expired
        | ErrorKind::TryAgain
        | ErrorKind::CompactionTooLarge
        | ErrorKind::ColumnFamilyDropped
        | ErrorKind::Unknown => AptosDbError::OtherRocksDbError(rocksdb_err.to_string()),
    }
```
