# Audit Report

## Title
Capacity Management Bypass Through Multiple BoundedExecutor Instances Sharing Same Runtime Handle in DAG Consensus

## Summary
The DAG consensus implementation creates two separate `BoundedExecutor` instances that wrap the same underlying tokio runtime `Handle`, allowing a total of 24 concurrent tasks (16 + 8) instead of the intended capacity limit of 16. This bypasses resource limits and could potentially lead to runtime resource exhaustion under specific conditions.

## Finding Description

The `BoundedExecutor` struct is designed to limit concurrent task execution on a tokio runtime using a semaphore. Each `BoundedExecutor` instance created via `new()` gets its own independent semaphore for capacity tracking. [1](#0-0) 

In the DAG consensus implementation, two separate `BoundedExecutor` instances are created that wrap the same tokio runtime `Handle`:

**Executor A (Primary Consensus Executor)**: Created during consensus initialization with configurable capacity (default 16): [2](#0-1) 

This executor is passed to `EpochManager` and stored: [3](#0-2) 

It's then passed to `DagBootstrapper`: [4](#0-3) 

**Executor B (DAG Message Verification Executor)**: Created inside the DAG handler's `run` method with hardcoded capacity of 8: [5](#0-4) 

When `handler.run()` is called from the consensus runtime context, both executors wrap the same `Handle`: [6](#0-5) 

Since both executors operate in the consensus runtime spawned at: [7](#0-6) 

The `Handle::current()` call in Executor B returns the same handle as Executor A's `runtime.handle().clone()`.

**The Problem:**
- Each executor has its own semaphore tracking capacity independently
- Executor A limits to 16 concurrent tasks
- Executor B limits to 8 concurrent tasks  
- Both spawn tasks on the same underlying tokio runtime
- Total possible concurrent tasks: 24 (bypassing intended limit)

This breaks the **Resource Limits** invariant that "All operations must respect gas, storage, and computational limits."

## Impact Explanation

This issue is classified as **Medium severity** for the following reasons:

1. **Resource Limit Bypass**: The intended capacity management is circumvented, allowing 50% more concurrent tasks than configured (24 vs 16).

2. **Potential Performance Degradation**: Under high load, the additional concurrent tasks could degrade consensus performance or cause resource contention on the runtime's thread pool.

3. **Not Critical Severity** because:
   - Does not break consensus safety guarantees
   - Does not cause loss of funds
   - Does not enable consensus splits or double-spending
   - The capacity increase is limited (8 additional tasks)
   - Does not directly lead to network partition

However, the actual exploitability is **LIMITED** because:
- DAG messages are part of the validator consensus protocol
- Network message handling typically has rate limiting and backpressure mechanisms
- The tokio runtime may have sufficient resources to handle 24 concurrent tasks
- The separate executor appears intentional per the code comment

## Likelihood Explanation

**Likelihood: Low to Medium**

The issue manifests automatically whenever the DAG consensus is active, as both executors are created during normal operation. However, actual resource exhaustion requires:

1. High concurrent load on both executors simultaneously
2. Sufficient incoming DAG messages to saturate Executor B's 8-task capacity
3. Concurrent consensus execution operations saturating Executor A's 16-task capacity
4. The underlying runtime having insufficient resources to handle 24 tasks

The design appears intentional based on the comment "A separate executor to ensure the message verification sender (above) and receiver (below) are not blocking each other", suggesting the developers wanted independent concurrency control for verification tasks.

## Recommendation

**Option 1 (Shared Capacity):** Use a single shared `BoundedExecutor` for both verification and execution tasks to enforce unified capacity limits:

```rust
// In dag_handler.rs, use the passed-in executor instead of creating a new one:
// Remove line 127: let executor = BoundedExecutor::new(8, Handle::current());
// Use the executor parameter directly for both verification and processing
```

**Option 2 (Aggregate Capacity Tracking):** If separate executors are required for isolation, create them with coordinated capacities that sum to the desired total:

```rust
// Adjust capacities so they sum to the intended limit
// E.g., Executor A: 12 tasks, Executor B: 4 tasks = 16 total
```

**Option 3 (Explicit Runtime Isolation):** Document that the separate executors are intentional and ensure the runtime is sized appropriately to handle the combined load.

## Proof of Concept

The issue exists in the production code structure. To demonstrate capacity bypass:

1. Configure consensus with `num_bounded_executor_tasks = 16`
2. Start DAG consensus which creates both executors
3. Spawn 16 tasks via Executor A (fills its capacity)
4. Spawn 8 tasks via Executor B (fills its capacity)  
5. Observe that 24 tasks run concurrently on the same runtime

However, demonstrating actual resource exhaustion or performance impact requires load testing under specific network conditions, which is beyond the scope of static analysis.

---

**Notes:**

While this finding demonstrates the pattern described in the security question (multiple `BoundedExecutor` instances wrapping the same `Handle`), the practical security impact is limited. The issue appears to be a **design consideration** rather than a critical exploitable vulnerability, as:

1. The separate executor is explicitly created with a comment explaining its purpose
2. The capacity increase is bounded and relatively small (50%)
3. Exploiting this for DoS would require either validator access or the ability to flood DAG messages
4. Modern tokio runtimes can typically handle dozens of concurrent tasks efficiently
5. Additional network-level protections (rate limiting, backpressure) likely exist

The finding is valid as an answer to the security question about whether multiple executors can interfere with capacity management (yes, they can), but may not meet the threshold for a critical security vulnerability requiring immediate remediation.

### Citations

**File:** crates/bounded-executor/src/executor.rs (L22-31)
```rust
impl BoundedExecutor {
    /// Create a new `BoundedExecutor` from an existing tokio [`Handle`]
    /// with a maximum concurrent task capacity of `capacity`.
    pub fn new(capacity: usize, executor: Handle) -> Self {
        let semaphore = Arc::new(Semaphore::new(capacity));
        Self {
            semaphore,
            executor,
        }
    }
```

**File:** consensus/src/consensus_provider.rs (L56-56)
```rust
    let runtime = aptos_runtimes::spawn_named_runtime("consensus".into(), None);
```

**File:** consensus/src/consensus_provider.rs (L81-84)
```rust
    let bounded_executor = BoundedExecutor::new(
        node_config.consensus.num_bounded_executor_tasks as usize,
        runtime.handle().clone(),
    );
```

**File:** consensus/src/epoch_manager.rs (L167-167)
```rust
    bounded_executor: BoundedExecutor,
```

**File:** consensus/src/epoch_manager.rs (L1509-1509)
```rust
            self.bounded_executor.clone(),
```

**File:** consensus/src/dag/dag_handler.rs (L123-127)
```rust
        let mut futures = FuturesUnordered::new();
        // A separate executor to ensure the message verification sender (above) and receiver (below) are
        // not blocking each other.
        // TODO: make this configurable
        let executor = BoundedExecutor::new(8, Handle::current());
```

**File:** consensus/src/dag/bootstrap.rs (L160-163)
```rust
        let sync_outcome = self
            .handler
            .run(dag_rpc_rx, bootstrapper.executor.clone(), self.buffer)
            .await;
```
