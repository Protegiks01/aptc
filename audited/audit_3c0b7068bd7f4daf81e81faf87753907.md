# Audit Report

## Title
Connection Counter Leak in `/transactions/wait_by_hash` Endpoint Causes Permanent API Service Degradation

## Summary
The `wait_for_hash_active_connections` counter in the API context is incremented when clients connect to the `/transactions/wait_by_hash/:txn_hash` endpoint but fails to decrement when connections are cancelled, dropped, or terminated prematurely. This causes the counter to perpetually increase, eventually reaching the threshold and forcing all future requests into degraded "short poll" mode, effectively disabling the long-polling optimization.

## Finding Description

The vulnerability exists in the `wait_transaction_by_hash` endpoint handler. [1](#0-0) 

The handler increments the counter at the beginning of the request: [2](#0-1) 

If the counter exceeds the threshold, it immediately decrements and returns: [3](#0-2) 

Otherwise, it proceeds to call an async function and only decrements the counter after successful completion: [4](#0-3) 

**The Critical Flaw:**

The decrement at line 276 only executes if the async function completes normally. However, in Poem (the web framework used), when clients disconnect mid-request, **the future is dropped immediately without running cleanup code**. This is explicitly documented in the Aptos codebase: [5](#0-4) 

The proper pattern for handling this requires implementing a Drop trait on a guard struct, as demonstrated in the same file: [6](#0-5) 

The `wait_transaction_by_hash` implementation lacks such a guard, meaning the counter never decrements when:
1. **Client disconnects** during the long poll wait
2. **HTTP request times out** at the framework level  
3. **Future is cancelled** by Tokio runtime
4. **Errors occur** in `wait_transaction_by_hash_inner` (multiple error paths exist) [7](#0-6) 

**Attack Scenario:**

1. Attacker opens multiple connections to `/transactions/wait_by_hash/:txn_hash`
2. Immediately closes connections before they complete (e.g., using `curl` with timeout)
3. Each closed connection leaks +1 to the counter
4. Repeats until counter reaches `wait_by_hash_max_active_connections`
5. All subsequent legitimate requests are forced into "short poll" mode
6. The long-polling optimization is permanently disabled until node restart

This breaks the **Resource Limits** invariant (#9): "All operations must respect gas, storage, and computational limits" - the connection limit enforcement is bypassed through accumulation of leaked counters.

## Impact Explanation

**High Severity** per Aptos bug bounty criteria:

1. **API Service Degradation**: The wait_by_hash endpoint is designed to reduce latency through long-polling. When degraded to short-poll mode, clients must repeatedly poll, increasing API load and latency.

2. **Validator Node Impact**: API nodes serving validators experience increased load from clients forced into polling mode, potentially causing slowdowns during high-traffic periods.

3. **Cascading Effect**: Since the counter never decreases (without restart), the degradation is permanent and worsens over time as more leaks accumulate.

4. **Denial of Service Vector**: Attackers can intentionally trigger this with minimal resources (simple HTTP clients), making it a practical DoS attack vector.

5. **No Self-Recovery**: Unlike transient issues, this requires manual intervention (node restart) to recover, impacting availability.

This qualifies as **"API crashes"** and **"Validator node slowdowns"** under High Severity categories, as the API functionality is effectively broken and can impact validator operations.

## Likelihood Explanation

**HIGH Likelihood:**

1. **Natural Occurrence**: Even without malicious actors, normal network conditions (mobile clients, unstable connections, client crashes) will cause this leak over time. Every dropped connection contributes to counter accumulation.

2. **Easy to Trigger**: Attackers need only basic HTTP client capabilities to intentionally trigger this. No special access, authentication, or complex setup required.

3. **Cumulative Nature**: Small leaks accumulate over days/weeks of operation. A busy API node could naturally reach the threshold through legitimate but interrupted connections.

4. **No Monitoring**: There's no visible indication that the counter has leaked until the threshold is reached and service degrades.

5. **Production Evidence**: The existence of the DropLogger pattern in the codebase suggests the Aptos team is aware that futures get dropped mid-request in production environments.

## Recommendation

Implement an RAII guard using the Drop trait to ensure the counter is always decremented:

```rust
// Add to api/src/transactions.rs

struct ActiveConnectionGuard {
    counter: Arc<AtomicUsize>,
}

impl ActiveConnectionGuard {
    fn new(counter: Arc<AtomicUsize>) -> Self {
        counter.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
        Self { counter }
    }
}

impl Drop for ActiveConnectionGuard {
    fn drop(&mut self) {
        self.counter.fetch_sub(1, std::sync::atomic::Ordering::Relaxed);
    }
}

// Modify wait_transaction_by_hash:
async fn wait_transaction_by_hash(
    &self,
    accept_type: AcceptType,
    txn_hash: Path<HashValue>,
) -> BasicResultWith404<Transaction> {
    fail_point_poem("endpoint_wait_transaction_by_hash")?;
    self.context
        .check_api_output_enabled("Get transactions by hash", &accept_type)?;

    // Check threshold before incrementing
    if self.context.wait_for_hash_active_connections.load(std::sync::atomic::Ordering::Relaxed)
        >= self.context.node_config.api.wait_by_hash_max_active_connections
    {
        metrics::WAIT_TRANSACTION_POLL_TIME
            .with_label_values(&["short"])
            .observe(0.0);
        return self.get_transaction_by_hash_inner(&accept_type, txn_hash.0).await;
    }

    // Create guard - automatically decrements on drop
    let _guard = ActiveConnectionGuard::new(
        self.context.wait_for_hash_active_connections.clone()
    );
    
    let start_time = std::time::Instant::now();
    WAIT_TRANSACTION_GAUGE.inc();

    let result = self
        .wait_transaction_by_hash_inner(
            &accept_type,
            txn_hash.0,
            self.context.node_config.api.wait_by_hash_timeout_ms,
            self.context.node_config.api.wait_by_hash_poll_interval_ms,
        )
        .await;

    WAIT_TRANSACTION_GAUGE.dec();
    metrics::WAIT_TRANSACTION_POLL_TIME
        .with_label_values(&["long"])
        .observe(start_time.elapsed().as_secs_f64());
    
    result
    // Guard automatically decrements counter here when dropped
}
```

## Proof of Concept

```bash
#!/bin/bash
# PoC: Demonstrate connection counter leak

API_ENDPOINT="http://localhost:8080"
HASH="0x0000000000000000000000000000000000000000000000000000000000000001"

echo "Leaking connections by opening and immediately closing..."
for i in {1..100}; do
    # Open connection and close immediately (timeout after 0.1s)
    curl -m 0.1 "$API_ENDPOINT/transactions/wait_by_hash/$HASH" &
    sleep 0.01
done

wait

echo "All connections leaked. Check node metrics:"
echo "- wait_for_hash_active_connections should be elevated"
echo "- All subsequent wait_by_hash requests will use short poll mode"
echo ""
echo "Test that long-poll is now disabled:"
curl -w "\nTime: %{time_total}s\n" "$API_ENDPOINT/transactions/wait_by_hash/$HASH"
echo "^ Should return immediately (short poll) instead of waiting"
```

To verify the leak in a development environment:
1. Add logging to track counter value in `wait_transaction_by_hash`
2. Run the PoC script
3. Observe counter increases but never decreases
4. Observe all subsequent requests forced into short-poll mode
5. Requires node restart to clear the leaked counter

### Citations

**File:** api/src/context.rs (L84-84)
```rust
    pub wait_for_hash_active_connections: Arc<AtomicUsize>,
```

**File:** api/src/transactions.rs (L240-243)
```rust
        if self
            .context
            .wait_for_hash_active_connections
            .fetch_add(1, std::sync::atomic::Ordering::Relaxed)
```

**File:** api/src/transactions.rs (L250-252)
```rust
            self.context
                .wait_for_hash_active_connections
                .fetch_sub(1, std::sync::atomic::Ordering::Relaxed);
```

**File:** api/src/transactions.rs (L264-276)
```rust
        let result = self
            .wait_transaction_by_hash_inner(
                &accept_type,
                txn_hash.0,
                self.context.node_config.api.wait_by_hash_timeout_ms,
                self.context.node_config.api.wait_by_hash_poll_interval_ms,
            )
            .await;

        WAIT_TRANSACTION_GAUGE.dec();
        self.context
            .wait_for_hash_active_connections
            .fetch_sub(1, std::sync::atomic::Ordering::Relaxed);
```

**File:** api/src/transactions.rs (L893-939)
```rust
    async fn wait_transaction_by_hash_inner(
        &self,
        accept_type: &AcceptType,
        hash: HashValue,
        wait_by_hash_timeout_ms: u64,
        wait_by_hash_poll_interval_ms: u64,
    ) -> BasicResultWith404<Transaction> {
        let start_time = std::time::Instant::now();
        loop {
            let context = self.context.clone();
            let accept_type = accept_type.clone();

            let (internal_ledger_info_opt, storage_ledger_info) =
                api_spawn_blocking(move || context.get_latest_internal_and_storage_ledger_info())
                    .await?;
            let storage_version = storage_ledger_info.ledger_version.into();
            let internal_ledger_version = internal_ledger_info_opt
                .as_ref()
                .map(|info| info.ledger_version.into());
            let latest_ledger_info = internal_ledger_info_opt.unwrap_or(storage_ledger_info);
            let txn_data = self
                .get_by_hash(hash.into(), storage_version, internal_ledger_version)
                .await
                .context(format!("Failed to get transaction by hash {}", hash))
                .map_err(|err| {
                    BasicErrorWith404::internal_with_code(
                        err,
                        AptosErrorCode::InternalError,
                        &latest_ledger_info,
                    )
                })?
                .context(format!("Failed to find transaction with hash: {}", hash))
                .map_err(|_| transaction_not_found_by_hash(hash, &latest_ledger_info))?;

            if matches!(txn_data, TransactionData::Pending(_))
                && (start_time.elapsed().as_millis() as u64) < wait_by_hash_timeout_ms
            {
                tokio::time::sleep(Duration::from_millis(wait_by_hash_poll_interval_ms)).await;
                continue;
            }

            let api = self.clone();
            return api_spawn_blocking(move || {
                api.get_transaction_inner(&accept_type, txn_data, &latest_ledger_info)
            })
            .await;
        }
```

**File:** crates/aptos-faucet/core/src/middleware/log.rs (L95-99)
```rust
/// In Poem, if the client hangs up mid request, the future stops getting polled
/// and instead gets dropped. So if we want this middleware logging to happen
/// even if this happens, we have to implement the logging in a Drop impl. If
/// we reach this drop impl and there is no response log attached, we have hit
/// this case and log accordingly.
```

**File:** crates/aptos-faucet/core/src/middleware/log.rs (L118-159)
```rust
impl Drop for DropLogger<'_> {
    fn drop(&mut self) {
        // Get some process info, e.g. the POD_NAME in case we're in a k8s context.
        let process_info = ProcessInfo {
            pod_name: std::env::var("POD_NAME").ok(),
        };

        match &self.response_log {
            Some(response_log) => {
                // Log response statuses generally.
                RESPONSE_STATUS
                    .with_label_values(&[response_log.response_status.to_string().as_str()])
                    .observe(response_log.elapsed.as_secs_f64());

                // Log response status per-endpoint + method.
                HISTOGRAM
                    .with_label_values(&[
                        self.request_log.method.as_str(),
                        response_log.operation_id,
                        response_log.response_status.to_string().as_str(),
                    ])
                    .observe(response_log.elapsed.as_secs_f64());

                // For now log all requests, no sampling, unless it is for `/`.
                if response_log.operation_id == "root" {
                    sample!(
                        SampleRate::Duration(Duration::from_secs(60)),
                        info!(self.request_log, *response_log, process_info)
                    );
                } else if response_log.response_status >= 500 {
                    error!(self.request_log, *response_log, process_info);
                } else {
                    info!(self.request_log, *response_log, process_info);
                }
            },
            None => {
                // If we don't have a response log, it means the client
                // hung up mid-request.
                warn!(self.request_log, process_info, destiny = "hangup");
            },
        }
    }
```
