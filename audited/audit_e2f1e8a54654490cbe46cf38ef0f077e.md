# Audit Report

## Title
Checkpoint Inconsistency Between Event and Transaction Data Can Cause Validator Replay Failure

## Summary
When storage sharding is enabled, the checkpoint creation process for LedgerDb creates snapshots of individual databases (event_db, transaction_db, etc.) sequentially without atomic coordination. This allows a race condition where concurrent transaction commits can result in checkpoints with mismatched event-transaction bindings. While a recovery mechanism exists, specific timing conditions can bypass proper truncation, causing validators to restore into an inconsistent state where execution replay fails.

## Finding Description

The vulnerability exists in the checkpoint creation and recovery flow:

**Checkpoint Creation Race Condition:**

The `LedgerDb::create_checkpoint` function creates checkpoints for each database sequentially without locking: [1](#0-0) 

Between creating individual database checkpoints (event_db at line 346-348, transaction_db at line 359-360), new transaction commits can complete, resulting in:
- event_db checkpoint capturing version N+1 (if commit_events completed)
- transaction_db checkpoint capturing version N (if commit_transactions hasn't completed yet)
- Metadata checkpoint with `OverallCommitProgress` potentially at either version

**Parallel Commit Process:**

Transaction commits happen in parallel across different database components: [2](#0-1) 

The `commit_events` (line 277-283) and `commit_transactions` (line 291-299) execute concurrently. If checkpoint creation interleaves with these operations, different databases can be captured at different versions.

**Recovery Mechanism Weakness:**

The recovery mechanism `sync_commit_progress` attempts to fix inconsistencies during startup: [3](#0-2) 

However, this mechanism can be bypassed when `empty_buffered_state_for_restore` is set: [4](#0-3) 

**Execution Replay Failure:**

When validators attempt to replay transactions, `get_transaction_outputs` fetches data from multiple databases: [5](#0-4) 

If event_db has events for version N (line 398) but transaction_db lacks the transaction for version N (line 400), the operation fails with a NotFound error, causing replay to fail.

**Developer Acknowledgment:**

A TODO comment acknowledges the unhandled data inconsistency issue: [6](#0-5) 

## Impact Explanation

This vulnerability qualifies as **High Severity** under Aptos bug bounty criteria:

1. **Validator Node Failures**: Validators restoring from inconsistent checkpoints cannot complete execution replay, causing node failures and preventing them from participating in consensus.

2. **Network Partition Risk**: If multiple validators restore from the same inconsistent checkpoint (e.g., during coordinated maintenance or disaster recovery), they would all fail to sync, potentially causing a partial network partition.

3. **State Consistency Violation**: The mismatch between events and transactions violates the fundamental invariant that "State transitions must be atomic and verifiable via Merkle proofs."

4. **Deterministic Execution Failure**: Validators cannot produce identical state roots when some have events without corresponding transactions, violating the "Deterministic Execution" invariant.

While not immediately exploitable by external attackers, this impacts validator operations and network reliability, qualifying as a "Significant protocol violation" under High Severity criteria.

## Likelihood Explanation

**Likelihood: Medium-High**

The vulnerability can manifest under realistic conditions:

1. **Common Trigger**: Checkpoint creation during active transaction processing is a standard operational scenario, especially for backup/snapshot operations.

2. **Timing Window**: The sequential checkpoint creation (8 databases) creates a significant time window (potentially milliseconds to seconds) during which commits can interleave.

3. **No Prevention**: There are no locks or atomic checkpoint mechanisms to prevent this race condition.

4. **Bypass Scenarios**: The recovery bypass flag `empty_buffered_state_for_restore` may be used during specific restore operations, preventing automatic recovery from inconsistent checkpoints.

## Recommendation

Implement atomic checkpoint creation for all LedgerDb components:

```rust
pub(crate) fn create_checkpoint(
    db_root_path: impl AsRef<Path>,
    cp_root_path: impl AsRef<Path>,
    sharding: bool,
) -> Result<()> {
    // Acquire a read lock on the database to prevent writes during checkpoint
    let checkpoint_guard = CHECKPOINT_LOCK.read();
    
    // Verify all databases are at consistent versions before checkpointing
    let event_version = ledger_db.event_db().latest_version()?;
    let txn_version = ledger_db.transaction_db().latest_version()?;
    let info_version = ledger_db.transaction_info_db().latest_version()?;
    
    ensure!(
        event_version == txn_version && txn_version == info_version,
        "Database versions must be consistent before checkpoint: event={}, txn={}, info={}",
        event_version.unwrap_or(0),
        txn_version.unwrap_or(0),
        info_version.unwrap_or(0)
    );
    
    // Proceed with checkpoint creation
    // ... existing checkpoint code ...
    
    drop(checkpoint_guard);
    Ok(())
}
```

Additionally, add validation during checkpoint restore:

```rust
// After loading checkpoint, verify consistency
pub fn validate_checkpoint_consistency(&self) -> Result<()> {
    let event_ver = self.event_db().latest_version()?;
    let txn_ver = self.transaction_db().latest_version()?;
    let overall_progress = self.metadata_db().get_synced_version()?;
    
    ensure!(
        event_ver == txn_ver,
        "Checkpoint inconsistency detected: event_db at {:?}, transaction_db at {:?}",
        event_ver,
        txn_ver
    );
    
    ensure!(
        overall_progress.map_or(true, |v| event_ver.map_or(true, |ev| ev == v)),
        "Checkpoint metadata inconsistency: overall_progress={:?}, event_ver={:?}",
        overall_progress,
        event_ver
    );
    
    Ok(())
}
```

## Proof of Concept

```rust
// Reproduction test demonstrating the race condition
#[test]
fn test_checkpoint_race_condition_causes_replay_failure() {
    use std::sync::Arc;
    use std::thread;
    use tempfile::TempDir;
    
    // Setup: Create AptosDB with sharding enabled
    let tmpdir = TempDir::new().unwrap();
    let db = AptosDB::new_for_test_with_sharding(tmpdir.path(), 100);
    
    // Commit initial transactions
    for i in 0..10 {
        commit_test_transaction(&db, i);
    }
    
    // Simulate race: Start checkpoint creation in background
    let checkpoint_dir = TempDir::new().unwrap();
    let db_path = tmpdir.path().to_path_buf();
    let cp_path = checkpoint_dir.path().to_path_buf();
    
    let checkpoint_thread = thread::spawn(move || {
        // Add small delay to increase race window
        thread::sleep(std::time::Duration::from_millis(10));
        LedgerDb::create_checkpoint(db_path, cp_path, true)
    });
    
    // Concurrently commit new transactions while checkpoint is being created
    for i in 10..15 {
        commit_test_transaction(&db, i);
        thread::sleep(std::time::Duration::from_millis(5));
    }
    
    checkpoint_thread.join().unwrap().unwrap();
    
    // Restore from checkpoint with recovery bypassed
    let restored_db = AptosDB::open_from_checkpoint(
        checkpoint_dir.path(),
        /*empty_buffered_state_for_restore=*/ true  // Bypass recovery
    );
    
    // Attempt to get transaction outputs - this should fail
    // if event_db has events for version N but transaction_db doesn't have transaction N
    let result = restored_db.get_transaction_outputs(10, 5, 14);
    
    // Expected: NotFound error due to missing transaction
    assert!(result.is_err(), "Should fail due to inconsistent checkpoint");
}
```

**Notes:**
- This vulnerability breaks the "State Consistency" and "Deterministic Execution" invariants
- The sequential checkpoint creation without atomicity guarantees is the root cause
- While `sync_commit_progress` provides partial mitigation, it can be bypassed and doesn't validate cross-database consistency
- The impact primarily affects validator availability and network reliability rather than direct fund loss

### Citations

**File:** storage/aptosdb/src/ledger_db/mod.rs (L281-281)
```rust
        // TODO(grao): Handle data inconsistency.
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L311-370)
```rust
    pub(crate) fn create_checkpoint(
        db_root_path: impl AsRef<Path>,
        cp_root_path: impl AsRef<Path>,
        sharding: bool,
    ) -> Result<()> {
        let rocksdb_configs = RocksdbConfigs {
            enable_storage_sharding: sharding,
            ..Default::default()
        };
        let env = None;
        let block_cache = None;
        let ledger_db = Self::new(
            db_root_path,
            rocksdb_configs,
            env,
            block_cache,
            /*readonly=*/ false,
        )?;
        let cp_ledger_db_folder = cp_root_path.as_ref().join(LEDGER_DB_FOLDER_NAME);

        info!(
            sharding = sharding,
            "Creating ledger_db checkpoint at: {cp_ledger_db_folder:?}"
        );

        std::fs::remove_dir_all(&cp_ledger_db_folder).unwrap_or(());
        if sharding {
            std::fs::create_dir_all(&cp_ledger_db_folder).unwrap_or(());
        }

        ledger_db
            .metadata_db()
            .create_checkpoint(Self::metadata_db_path(cp_root_path.as_ref(), sharding))?;

        if sharding {
            ledger_db
                .event_db()
                .create_checkpoint(cp_ledger_db_folder.join(EVENT_DB_NAME))?;
            ledger_db
                .persisted_auxiliary_info_db()
                .create_checkpoint(cp_ledger_db_folder.join(PERSISTED_AUXILIARY_INFO_DB_NAME))?;
            ledger_db
                .transaction_accumulator_db()
                .create_checkpoint(cp_ledger_db_folder.join(TRANSACTION_ACCUMULATOR_DB_NAME))?;
            ledger_db
                .transaction_auxiliary_data_db()
                .create_checkpoint(cp_ledger_db_folder.join(TRANSACTION_AUXILIARY_DATA_DB_NAME))?;
            ledger_db
                .transaction_db()
                .create_checkpoint(cp_ledger_db_folder.join(TRANSACTION_DB_NAME))?;
            ledger_db
                .transaction_info_db()
                .create_checkpoint(cp_ledger_db_folder.join(TRANSACTION_INFO_DB_NAME))?;
            ledger_db
                .write_set_db()
                .create_checkpoint(cp_ledger_db_folder.join(WRITE_SET_DB_NAME))?;
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L271-319)
```rust
        THREAD_MANAGER.get_non_exe_cpu_pool().scope(|s| {
            // TODO(grao): Write progress for each of the following databases, and handle the
            // inconsistency at the startup time.
            //
            // TODO(grao): Consider propagating the error instead of panic, if necessary.
            s.spawn(|_| {
                self.commit_events(
                    chunk.first_version,
                    chunk.transaction_outputs,
                    skip_index_and_usage,
                )
                .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .write_set_db()
                    .commit_write_sets(chunk.first_version, chunk.transaction_outputs)
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .transaction_db()
                    .commit_transactions(
                        chunk.first_version,
                        chunk.transactions,
                        skip_index_and_usage,
                    )
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .persisted_auxiliary_info_db()
                    .commit_auxiliary_info(chunk.first_version, chunk.persisted_auxiliary_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_state_kv_and_ledger_metadata(chunk, skip_index_and_usage)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_transaction_infos(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                new_root_hash = self
                    .commit_transaction_accumulator(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
        });
```

**File:** storage/aptosdb/src/state_store/mod.rs (L353-359)
```rust
        if !hack_for_tests && !empty_buffered_state_for_restore {
            Self::sync_commit_progress(
                Arc::clone(&ledger_db),
                Arc::clone(&state_kv_db),
                Arc::clone(&state_merkle_db),
                /*crash_if_difference_is_too_large=*/ true,
            );
```

**File:** storage/aptosdb/src/state_store/mod.rs (L410-502)
```rust
    pub fn sync_commit_progress(
        ledger_db: Arc<LedgerDb>,
        state_kv_db: Arc<StateKvDb>,
        state_merkle_db: Arc<StateMerkleDb>,
        crash_if_difference_is_too_large: bool,
    ) {
        let ledger_metadata_db = ledger_db.metadata_db();
        if let Some(overall_commit_progress) = ledger_metadata_db
            .get_synced_version()
            .expect("DB read failed.")
        {
            info!(
                overall_commit_progress = overall_commit_progress,
                "Start syncing databases..."
            );
            let ledger_commit_progress = ledger_metadata_db
                .get_ledger_commit_progress()
                .expect("Failed to read ledger commit progress.");
            assert_ge!(ledger_commit_progress, overall_commit_progress);

            let state_kv_commit_progress = state_kv_db
                .metadata_db()
                .get::<DbMetadataSchema>(&DbMetadataKey::StateKvCommitProgress)
                .expect("Failed to read state K/V commit progress.")
                .expect("State K/V commit progress cannot be None.")
                .expect_version();
            assert_ge!(state_kv_commit_progress, overall_commit_progress);

            // LedgerCommitProgress was not guaranteed to commit after all ledger changes finish,
            // have to attempt truncating every column family.
            info!(
                ledger_commit_progress = ledger_commit_progress,
                "Attempt ledger truncation...",
            );
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");

            // State K/V commit progress isn't (can't be) written atomically with the data,
            // because there are shards, so we have to attempt truncation anyway.
            info!(
                state_kv_commit_progress = state_kv_commit_progress,
                "Start state KV truncation..."
            );
            let difference = state_kv_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_state_kv_db(
                &state_kv_db,
                state_kv_commit_progress,
                overall_commit_progress,
                std::cmp::max(difference as usize, 1), /* batch_size */
            )
            .expect("Failed to truncate state K/V db.");

            let state_merkle_max_version = get_max_version_in_state_merkle_db(&state_merkle_db)
                .expect("Failed to get state merkle max version.")
                .expect("State merkle max version cannot be None.");
            if state_merkle_max_version > overall_commit_progress {
                let difference = state_merkle_max_version - overall_commit_progress;
                if crash_if_difference_is_too_large {
                    assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
                }
            }
            let state_merkle_target_version = find_tree_root_at_or_before(
                ledger_metadata_db,
                &state_merkle_db,
                overall_commit_progress,
            )
            .expect("DB read failed.")
            .unwrap_or_else(|| {
                panic!(
                    "Could not find a valid root before or at version {}, maybe it was pruned?",
                    overall_commit_progress
                )
            });
            if state_merkle_target_version < state_merkle_max_version {
                info!(
                    state_merkle_max_version = state_merkle_max_version,
                    target_version = state_merkle_target_version,
                    "Start state merkle truncation..."
                );
                truncate_state_merkle_db(&state_merkle_db, state_merkle_target_version)
                    .expect("Failed to truncate state merkle db.");
            }
        } else {
            info!("No overall commit progress was found!");
        }
    }
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L391-420)
```rust
            let (txn_infos, txns_and_outputs, persisted_aux_info) = (start_version
                ..start_version + limit)
                .map(|version| {
                    let txn_info = self
                        .ledger_db
                        .transaction_info_db()
                        .get_transaction_info(version)?;
                    let events = self.ledger_db.event_db().get_events_by_version(version)?;
                    let write_set = self.ledger_db.write_set_db().get_write_set(version)?;
                    let txn = self.ledger_db.transaction_db().get_transaction(version)?;
                    let auxiliary_data = self
                        .ledger_db
                        .transaction_auxiliary_data_db()
                        .get_transaction_auxiliary_data(version)?
                        .unwrap_or_default();
                    let txn_output = TransactionOutput::new(
                        write_set,
                        events,
                        txn_info.gas_used(),
                        txn_info.status().clone().into(),
                        auxiliary_data,
                    );
                    let persisted_aux_info = self
                        .ledger_db
                        .persisted_auxiliary_info_db()
                        .get_persisted_auxiliary_info(version)?
                        .unwrap_or(PersistedAuxiliaryInfo::None);
                    Ok((txn_info, (txn, txn_output), persisted_aux_info))
                })
                .collect::<Result<Vec<_>>>()?
```
