# Audit Report

## Title
Cache Stampede in Optimistic Fetch Processing Enables CPU Exhaustion via Concurrent Proof Generation

## Summary
An attacker can cause CPU exhaustion on storage service nodes by exploiting a race condition in the LRU cache check-and-insert pattern during optimistic fetch processing. When multiple peers send requests with identical stale `known_version` values, the concurrent processing of these requests can bypass the cache protection, causing redundant expensive proof generation operations.

## Finding Description

The storage service handles optimistic fetch requests (`NewTransactionOutputsWithProofRequest`) by storing them in a DashMap indexed by `PeerNetworkId`. [1](#0-0) 

When blockchain data advances, all pending optimistic fetches with stale `known_version` values become "ready" simultaneously. The system spawns concurrent blocking tasks (one per peer) to process these requests. [2](#0-1) 

Each task converts the optimistic fetch into a regular `GetTransactionOutputsWithProof` request and processes it through the handler. [3](#0-2) 

The critical vulnerability lies in the non-atomic cache check-and-insert pattern: [4](#0-3) 

After a cache miss, the system proceeds to generate the proof (CPU-intensive Merkle tree operations): [5](#0-4) 

Finally, the result is inserted into the cache: [6](#0-5) 

**Race Condition Exploitation:**
1. Attacker establishes connections from N peers (especially on Public network)
2. Sends `NewTransactionOutputsWithProofRequest` with same stale `known_version=0` from all peers
3. When blockchain advances to version 1000, all N requests become ready
4. N concurrent tasks spawn and execute `process_cachable_request`
5. Due to race condition: Task1 checks cache → MISS, Task2 checks cache → MISS (Task1 hasn't inserted yet), Task3 checks cache → MISS, etc.
6. All N tasks concurrently call `get_transaction_outputs_with_proof` and generate identical proofs
7. Proof generation involves expensive operations: [7](#0-6) 

The LRU cache size is limited to 500 entries by default, [8](#0-7)  but this doesn't prevent the race condition—it only limits eviction behavior.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program because it enables **validator node slowdowns**. The attack causes:

1. **CPU Exhaustion**: Concurrent proof generation for identical data wastes CPU cycles computing Merkle accumulator range proofs
2. **Storage Service Degradation**: Overloaded storage nodes cannot serve legitimate sync requests efficiently
3. **Network-Wide Impact**: If multiple storage service nodes are targeted, the entire state-sync subsystem degrades
4. **Low Attack Cost**: Attacker only needs to establish multiple peer connections and send minimal request payloads

The attack does not directly cause consensus violations or fund loss, but significantly degrades validator node performance and network availability, meeting the High severity criteria.

## Likelihood Explanation

This vulnerability is **highly likely** to be exploited because:

1. **Low Barrier to Entry**: Any actor can establish multiple peer connections to storage service nodes on the Public network
2. **Simple Attack Vector**: Requires only sending identical optimistic fetch requests with stale versions
3. **Predictable Trigger**: Attacker knows blockchain naturally advances, making requests "ready" automatically
4. **No Authentication Required**: Public network peers do not require special privileges
5. **Request Moderator Bypass**: The validation only checks timestamp freshness, not `known_version` values [9](#0-8) 

The optimistic fetch timeout (5 seconds default) [10](#0-9)  and per-peer invalid request limits do not prevent this attack since the requests are technically valid.

## Recommendation

Implement a request deduplication mechanism or atomic cache operations to prevent concurrent computation of identical proofs. Recommended approaches:

**Option 1: Request Coalescing with Mutex/RwLock per Cache Key**
```rust
// Add to Handler struct
pending_requests: Arc<DashMap<StorageServiceRequest, Arc<Mutex<Option<StorageServiceResponse>>>>>

fn process_cachable_request(...) {
    // Check cache first
    if let Some(response) = self.lru_response_cache.get(request) {
        return Ok(response.clone());
    }
    
    // Get or create a pending request slot
    let pending = self.pending_requests
        .entry(request.clone())
        .or_insert_with(|| Arc::new(Mutex::new(None)));
    
    let mut guard = pending.lock();
    
    // Double-check cache after acquiring lock
    if let Some(response) = self.lru_response_cache.get(request) {
        return Ok(response.clone());
    }
    
    // Check if another thread is computing
    if let Some(response) = &*guard {
        return Ok(response.clone());
    }
    
    // Compute response (only one thread does this)
    let response = /* existing computation logic */;
    
    // Cache and store in pending slot
    self.lru_response_cache.insert(request.clone(), response.clone());
    *guard = Some(response.clone());
    
    Ok(response)
}
```

**Option 2: Limit Concurrent Optimistic Fetches per Network**
Add configuration to limit active optimistic fetches per network type, rejecting new requests when the limit is reached.

**Option 3: Rate Limiting Based on Request Content Hash**
Track recently processed request parameters (hash of `known_version`, `known_epoch`, request type) and reject duplicate requests within a time window.

## Proof of Concept

```rust
// Rust integration test demonstrating the vulnerability
#[tokio::test]
async fn test_optimistic_fetch_cache_stampede() {
    use aptos_storage_service_types::requests::*;
    use std::sync::Arc;
    use std::sync::atomic::{AtomicU64, Ordering};
    
    // Setup storage service with instrumented proof generation counter
    let proof_generation_count = Arc::new(AtomicU64::new(0));
    let storage = MockStorage::new_with_counter(proof_generation_count.clone());
    let handler = Handler::new(/* ... */);
    
    // Create 100 concurrent requests with identical parameters
    let num_peers = 100;
    let mut tasks = vec![];
    
    for i in 0..num_peers {
        let peer_id = PeerNetworkId::new(NetworkId::Public, PeerId::random());
        let request = StorageServiceRequest::new(
            DataRequest::GetNewTransactionOutputsWithProof(
                NewTransactionOutputsWithProofRequest {
                    known_version: 0,  // Stale version
                    known_epoch: 0,
                }
            ),
            false  // no compression
        );
        
        let handler_clone = handler.clone();
        let task = tokio::spawn(async move {
            // Simulate concurrent processing
            handler_clone.process_request(&peer_id, request, true).await
        });
        
        tasks.push(task);
    }
    
    // Wait for all requests to complete
    for task in tasks {
        task.await.unwrap();
    }
    
    // Verify: Due to race condition, proof generation happens multiple times
    // Expected: 1 (with proper deduplication)
    // Actual: >> 1 (demonstrates vulnerability)
    let actual_count = proof_generation_count.load(Ordering::Relaxed);
    println!("Proof generations: {} (expected: 1)", actual_count);
    assert!(actual_count > 1, "Vulnerability: Multiple identical proofs generated");
}
```

## Notes

The vulnerability exists because the `mini_moka::sync::Cache` provides thread-safe individual operations (`get`, `insert`) but does not provide atomic check-and-compute-and-insert semantics. This is a classic "cache stampede" or "thundering herd" problem in distributed systems.

The attack is amplified by the fact that optimistic fetches are processed concurrently via `runtime.spawn_blocking`, with no global limit on concurrent executions for identical requests.

### Citations

**File:** state-sync/storage-service/server/src/handler.rs (L242-280)
```rust
    /// Handles the given optimistic fetch request
    pub fn handle_optimistic_fetch_request(
        &self,
        peer_network_id: PeerNetworkId,
        request: StorageServiceRequest,
        response_sender: ResponseSender,
    ) {
        // Create the optimistic fetch request
        let optimistic_fetch = OptimisticFetchRequest::new(
            request.clone(),
            response_sender,
            self.time_service.clone(),
        );

        // Store the optimistic fetch and check if any existing fetches were found
        if self
            .optimistic_fetches
            .insert(peer_network_id, optimistic_fetch)
            .is_some()
        {
            sample!(
                SampleRate::Duration(Duration::from_secs(ERROR_LOG_FREQUENCY_SECS)),
                trace!(LogSchema::new(LogEntry::OptimisticFetchRequest)
                    .error(&Error::InvalidRequest(
                        "An active optimistic fetch was already found for the peer!".into()
                    ))
                    .peer_network_id(&peer_network_id)
                    .request(&request)
                );
            );
        }

        // Update the optimistic fetch metrics
        increment_counter(
            &metrics::OPTIMISTIC_FETCH_EVENTS,
            peer_network_id.network_id(),
            OPTIMISTIC_FETCH_ADD.into(),
        );
    }
```

**File:** state-sync/storage-service/server/src/handler.rs (L396-404)
```rust
        // Check if the response is already in the cache
        if let Some(response) = self.lru_response_cache.get(request) {
            increment_counter(
                &metrics::LRU_CACHE_EVENT,
                peer_network_id.network_id(),
                LRU_CACHE_HIT.into(),
            );
            return Ok(response.clone());
        }
```

**File:** state-sync/storage-service/server/src/handler.rs (L406-440)
```rust
        // Otherwise, fetch the data from storage and time the operation
        let fetch_data_response = || match &request.data_request {
            DataRequest::GetStateValuesWithProof(request) => {
                self.get_state_value_chunk_with_proof(request)
            },
            DataRequest::GetEpochEndingLedgerInfos(request) => {
                self.get_epoch_ending_ledger_infos(request)
            },
            DataRequest::GetNumberOfStatesAtVersion(version) => {
                self.get_number_of_states_at_version(*version)
            },
            DataRequest::GetTransactionOutputsWithProof(request) => {
                self.get_transaction_outputs_with_proof(request)
            },
            DataRequest::GetTransactionsWithProof(request) => {
                self.get_transactions_with_proof(request)
            },
            DataRequest::GetTransactionsOrOutputsWithProof(request) => {
                self.get_transactions_or_outputs_with_proof(request)
            },
            DataRequest::GetTransactionDataWithProof(request) => {
                self.get_transaction_data_with_proof(request)
            },
            _ => Err(Error::UnexpectedErrorEncountered(format!(
                "Received an unexpected request: {:?}",
                request
            ))),
        };
        let data_response = utils::execute_and_time_duration(
            &metrics::STORAGE_FETCH_PROCESSING_LATENCY,
            Some((peer_network_id, request)),
            None,
            fetch_data_response,
            None,
        )?;
```

**File:** state-sync/storage-service/server/src/handler.rs (L455-458)
```rust
        // Create and cache the storage response
        self.lru_response_cache
            .insert(request.clone(), storage_response.clone());

```

**File:** state-sync/storage-service/server/src/optimistic_fetch.rs (L100-141)
```rust
        // Create the storage request
        let data_request = match &self.request.data_request {
            DataRequest::GetNewTransactionOutputsWithProof(_) => {
                DataRequest::GetTransactionOutputsWithProof(TransactionOutputsWithProofRequest {
                    proof_version: target_version,
                    start_version,
                    end_version,
                })
            },
            DataRequest::GetNewTransactionsWithProof(request) => {
                DataRequest::GetTransactionsWithProof(TransactionsWithProofRequest {
                    proof_version: target_version,
                    start_version,
                    end_version,
                    include_events: request.include_events,
                })
            },
            DataRequest::GetNewTransactionsOrOutputsWithProof(request) => {
                DataRequest::GetTransactionsOrOutputsWithProof(
                    TransactionsOrOutputsWithProofRequest {
                        proof_version: target_version,
                        start_version,
                        end_version,
                        include_events: request.include_events,
                        max_num_output_reductions: request.max_num_output_reductions,
                    },
                )
            },
            DataRequest::GetNewTransactionDataWithProof(request) => {
                DataRequest::GetTransactionDataWithProof(GetTransactionDataWithProofRequest {
                    transaction_data_request_type: request.transaction_data_request_type,
                    proof_version: target_version,
                    start_version,
                    end_version,
                    max_response_bytes: request.max_response_bytes,
                })
            },
            request => unreachable!("Unexpected optimistic fetch request: {:?}", request),
        };
        let storage_request =
            StorageServiceRequest::new(data_request, self.request.use_compression);
        Ok(storage_request)
```

**File:** state-sync/storage-service/server/src/optimistic_fetch.rs (L258-334)
```rust
pub(crate) async fn handle_ready_optimistic_fetches<T: StorageReaderInterface>(
    runtime: Handle,
    cached_storage_server_summary: Arc<ArcSwap<StorageServerSummary>>,
    config: StorageServiceConfig,
    optimistic_fetches: Arc<DashMap<PeerNetworkId, OptimisticFetchRequest>>,
    lru_response_cache: Cache<StorageServiceRequest, StorageServiceResponse>,
    request_moderator: Arc<RequestModerator>,
    storage: T,
    subscriptions: Arc<DashMap<PeerNetworkId, SubscriptionStreamRequests>>,
    time_service: TimeService,
    peers_with_ready_optimistic_fetches: Vec<(PeerNetworkId, LedgerInfoWithSignatures)>,
) {
    for (peer_network_id, target_ledger_info) in peers_with_ready_optimistic_fetches {
        // Remove the optimistic fetch from the active map. Note: we only do this if
        // the known version is lower than the target version. This is because
        // the peer may have updated their highest known version since we last checked.
        let ready_optimistic_fetch =
            optimistic_fetches.remove_if(&peer_network_id, |_, optimistic_fetch| {
                optimistic_fetch.highest_known_version()
                    < target_ledger_info.ledger_info().version()
            });

        // Handle the optimistic fetch request
        if let Some((_, optimistic_fetch)) = ready_optimistic_fetch {
            // Clone all required components for the task
            let cached_storage_server_summary = cached_storage_server_summary.clone();
            let optimistic_fetches = optimistic_fetches.clone();
            let lru_response_cache = lru_response_cache.clone();
            let request_moderator = request_moderator.clone();
            let storage = storage.clone();
            let subscriptions = subscriptions.clone();
            let time_service = time_service.clone();

            // Spawn a blocking task to handle the optimistic fetch
            runtime.spawn_blocking(move || {
                // Get the fetch start time and request
                let optimistic_fetch_start_time = optimistic_fetch.fetch_start_time;
                let optimistic_fetch_request = optimistic_fetch.request.clone();

                // Handle the optimistic fetch request and time the operation
                let handle_request = || {
                    // Get the storage service request for the missing data
                    let missing_data_request = optimistic_fetch
                        .get_storage_request_for_missing_data(config, &target_ledger_info)?;

                    // Notify the peer of the new data
                    utils::notify_peer_of_new_data(
                        cached_storage_server_summary.clone(),
                        optimistic_fetches.clone(),
                        subscriptions.clone(),
                        lru_response_cache.clone(),
                        request_moderator.clone(),
                        storage.clone(),
                        time_service.clone(),
                        &peer_network_id,
                        missing_data_request,
                        target_ledger_info,
                        optimistic_fetch.take_response_sender(),
                    )
                };
                let result = utils::execute_and_time_duration(
                    &metrics::OPTIMISTIC_FETCH_LATENCIES,
                    Some((&peer_network_id, &optimistic_fetch_request)),
                    None,
                    handle_request,
                    Some(optimistic_fetch_start_time),
                );

                // Log an error if the handler failed
                if let Err(error) = result {
                    warn!(LogSchema::new(LogEntry::OptimisticFetchResponse)
                        .error(&Error::UnexpectedErrorEncountered(error.to_string())));
                }
            });
        }
    }
}
```

**File:** state-sync/storage-service/server/src/storage.rs (L703-708)
```rust
            self.storage.get_transaction_accumulator_range_proof(
                start_version,
                num_fetched_outputs as u64,
                proof_version,
            )?
        };
```

**File:** config/src/config/state_sync_config.rs (L202-202)
```rust
            max_lru_cache_size: 500, // At ~0.6MiB per chunk, this should take no more than 0.5GiB
```

**File:** config/src/config/state_sync_config.rs (L207-207)
```rust
            max_optimistic_fetch_period_ms: 5000, // 5 seconds
```

**File:** state-sync/storage-service/types/src/responses.rs (L708-722)
```rust
            GetNewTransactionOutputsWithProof(_) => can_service_optimistic_request(
                aptos_data_client_config,
                time_service,
                self.synced_ledger_info.as_ref(),
            ),
            GetNewTransactionsWithProof(_) => can_service_optimistic_request(
                aptos_data_client_config,
                time_service,
                self.synced_ledger_info.as_ref(),
            ),
            GetNewTransactionsOrOutputsWithProof(_) => can_service_optimistic_request(
                aptos_data_client_config,
                time_service,
                self.synced_ledger_info.as_ref(),
            ),
```
