# Audit Report

## Title
Consensus Safety Violation via Augmented Data Store Initialization Race Condition

## Summary
A Byzantine validator can exploit a race condition during epoch initialization where different validators load different sets of certified augmented data from persistent storage, causing them to reject different randomness shares and produce divergent randomness values for identical blocks.

## Finding Description

The vulnerability exists in the randomness generation initialization flow during epoch transitions. When validators initialize `AugDataStore` for a new epoch, they load certified augmented data from their local databases without any network-wide synchronization to ensure all validators have received the same set of data. [1](#0-0) 

The loaded certified augmented data is used to populate certified augmented public keys (APKs) in the `RandConfig` by calling the `augment()` function, which adds certified deltas: [2](#0-1) 

The critical issue arises because `RandConfig` stores certified APKs in a `Vec<OnceCell<APK>>` structure that can only be populated once per validator: [3](#0-2) 

**Attack Mechanism:**

When a Byzantine validator B broadcasts its certified augmented data strategically around the epoch boundary:
- Some validators (H1) receive and persist B's data before initializing epoch N+1
- Other validators (H2) initialize without B's data in their database

During randomness generation, when validators receive B's randomness share, share verification checks for the certified APK: [4](#0-3) 

**Critical**: Line 74-78 shows that if a certified APK is missing, verification fails with an error. Shares are verified before being accepted: [5](#0-4) 

The verification happens in a background task that silently drops failed verifications: [6](#0-5) 

**Result:**
- H1 validators accept B's share (have B's certified APK)
- H2 validators reject B's share (missing B's certified APK)
- Both groups aggregate different share sets, producing different randomness values via different Lagrange coefficient computations: [7](#0-6) 

The lack of synchronization is evident in the blocking condition that only checks for the validator's own certified data: [8](#0-7) 

## Impact Explanation

**Severity: Critical**

This vulnerability enables a consensus safety violation where different validators compute different randomness values for identical block metadata, breaking the fundamental invariant that all validators must produce identical state transitions.

When validators aggregate different sets of randomness shares, they produce different evaluations through the weighted VUF derivation, which uses Lagrange interpolation over different player subsets. This leads to:

1. **Consensus Split**: Validators commit blocks with different randomness values for the same round
2. **Non-deterministic State**: Execution depending on randomness diverges across validators
3. **Permanent Partition Risk**: Once established, the inconsistent certified APK sets persist for the entire epoch

This qualifies as **Critical Severity** under the Aptos bug bounty program (up to $1,000,000) as it enables consensus/safety violations with < 1/3 Byzantine validators, potentially requiring network intervention or hardfork to resolve.

## Likelihood Explanation

**Likelihood: High**

The attack is highly feasible:

1. **No Special Privileges**: Any validator can broadcast certified augmented data as part of normal protocol operations
2. **Natural Timing Window**: Epoch transitions inherently create a window where validators initialize at different times due to network latency and processing delays
3. **No Synchronization Barrier**: The code explicitly shows no check ensuring all validators have the same certified augmented data before processing blocks
4. **Persistent State**: Once the inconsistent APK sets are established during initialization via `OnceCell`, they cannot be updated for the rest of the epoch

The attacker only needs to control the timing of one reliable broadcast message, which is trivial for a malicious validator with network-level control.

## Recommendation

Add a synchronization barrier before processing blocks to ensure all validators have received and persisted all certified augmented data for the current epoch. Specifically:

1. **Track Expected Certified Data**: Maintain a set of all validators who should have certified augmented data for the epoch
2. **Block Initialization**: Don't process blocks until certified data from all expected validators is received and persisted
3. **Timeout Mechanism**: Implement a timeout where validators wait for complete augmented data sets, with fallback to previous epoch's randomness or liveness degradation if data is missing

Modify the blocking condition in `rand_manager.rs` from checking only `my_certified_aug_data_exists()` to checking that certified augmented data exists for all validators in the validator set.

## Proof of Concept

The vulnerability can be demonstrated by:

1. Setting up a local testnet with 4 validators
2. Having one validator delay broadcasting its certified augmented data until after 2 validators have initialized epoch N+1
3. Observing that the 2 early validators reject the delayed validator's shares while the 2 late validators accept them
4. Both groups reaching threshold with different share sets and producing different randomness values
5. Consensus failing due to disagreement on block randomness

The specific PoC would require modifying the test harness to control network message delivery timing during epoch transitions, which can be achieved through the existing reliable broadcast infrastructure with added delays.

### Citations

**File:** consensus/src/rand/rand_gen/aug_data_store.rs (L57-71)
```rust
        let all_certified_data = db.get_all_certified_aug_data().unwrap_or_default();
        let (to_remove, certified_data) =
            Self::filter_by_epoch(epoch, all_certified_data.into_iter());
        if let Err(e) = db.remove_certified_aug_data(to_remove) {
            error!(
                "[AugDataStore] failed to remove certified aug data: {:?}",
                e
            );
        }

        for (_, certified_data) in &certified_data {
            certified_data
                .data()
                .augment(&config, &fast_config, certified_data.author());
        }
```

**File:** consensus/src/rand/rand_gen/types.rs (L52-81)
```rust
    fn verify(
        &self,
        rand_config: &RandConfig,
        rand_metadata: &RandMetadata,
        author: &Author,
    ) -> anyhow::Result<()> {
        let index = *rand_config
            .validator
            .address_to_validator_index()
            .get(author)
            .ok_or_else(|| anyhow!("Share::verify failed with unknown author"))?;
        let maybe_apk = &rand_config.keys.certified_apks[index];
        if let Some(apk) = maybe_apk.get() {
            WVUF::verify_share(
                &rand_config.vuf_pp,
                apk,
                bcs::to_bytes(&rand_metadata)
                    .map_err(|e| anyhow!("Serialization failed: {}", e))?
                    .as_slice(),
                &self.share,
            )?;
        } else {
            bail!(
                "[RandShare] No augmented public key for validator id {}, {}",
                index,
                author
            );
        }
        Ok(())
    }
```

**File:** consensus/src/rand/rand_gen/types.rs (L178-194)
```rust
    fn augment(
        &self,
        rand_config: &RandConfig,
        fast_rand_config: &Option<RandConfig>,
        author: &Author,
    ) {
        let AugmentedData { delta, fast_delta } = self;
        rand_config
            .add_certified_delta(author, delta.clone())
            .expect("Add delta should succeed");

        if let (Some(config), Some(fast_delta)) = (fast_rand_config, fast_delta) {
            config
                .add_certified_delta(author, fast_delta.clone())
                .expect("Add delta for fast path should succeed");
        }
    }
```

**File:** types/src/randomness.rs (L104-135)
```rust
pub struct RandKeys {
    // augmented secret / public key share of this validator, obtained from the DKG transcript of last epoch
    pub ask: ASK,
    pub apk: APK,
    // certified augmented public key share of all validators,
    // obtained from all validators in the new epoch,
    // which necessary for verifying randomness shares
    pub certified_apks: Vec<OnceCell<APK>>,
    // public key share of all validators, obtained from the DKG transcript of last epoch
    pub pk_shares: Vec<PKShare>,
}

impl RandKeys {
    pub fn new(ask: ASK, apk: APK, pk_shares: Vec<PKShare>, num_validators: usize) -> Self {
        let certified_apks = vec![OnceCell::new(); num_validators];

        Self {
            ask,
            apk,
            certified_apks,
            pk_shares,
        }
    }

    pub fn add_certified_apk(&self, index: usize, apk: APK) -> anyhow::Result<()> {
        assert!(index < self.certified_apks.len());
        if self.certified_apks[index].get().is_some() {
            return Ok(());
        }
        self.certified_apks[index].set(apk).unwrap();
        Ok(())
    }
```

**File:** consensus/src/rand/rand_gen/network_messages.rs (L35-60)
```rust
impl<S: TShare, D: TAugmentedData> RandMessage<S, D> {
    pub fn verify(
        &self,
        epoch_state: &EpochState,
        rand_config: &RandConfig,
        fast_rand_config: &Option<RandConfig>,
        sender: Author,
    ) -> anyhow::Result<()> {
        ensure!(self.epoch() == epoch_state.epoch);
        match self {
            RandMessage::RequestShare(_) => Ok(()),
            RandMessage::Share(share) => share.verify(rand_config),
            RandMessage::AugData(aug_data) => {
                aug_data.verify(rand_config, fast_rand_config, sender)
            },
            RandMessage::CertifiedAugData(certified_aug_data) => {
                certified_aug_data.verify(&epoch_state.verifier)
            },
            RandMessage::FastShare(share) => {
                share.share.verify(fast_rand_config.as_ref().ok_or_else(|| {
                    anyhow::anyhow!("[RandMessage] rand config for fast path not found")
                })?)
            },
            _ => bail!("[RandMessage] unexpected message type"),
        }
    }
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L236-260)
```rust
                    match bcs::from_bytes::<RandMessage<S, D>>(rand_gen_msg.req.data()) {
                        Ok(msg) => {
                            if msg
                                .verify(
                                    &epoch_state_clone,
                                    &config_clone,
                                    &fast_config_clone,
                                    rand_gen_msg.sender,
                                )
                                .is_ok()
                            {
                                let _ = tx.unbounded_send(RpcRequest {
                                    req: msg,
                                    protocol: rand_gen_msg.protocol,
                                    response_sender: rand_gen_msg.response_sender,
                                });
                            }
                        },
                        Err(e) => {
                            warn!("Invalid rand gen message: {}", e);
                        },
                    }
                })
                .await;
        }
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L380-382)
```rust
                Some(blocks) = incoming_blocks.next(), if self.aug_data_store.my_certified_aug_data_exists() => {
                    self.process_incoming_blocks(blocks);
                }
```

**File:** crates/aptos-dkg/src/weighted_vuf/pinkas/mod.rs (L273-314)
```rust
    pub fn collect_lagrange_coeffs_shares_and_rks<'a>(
        wc: &WeightedConfigBlstrs,
        apks: &'a [Option<(RandomizedPKs, Vec<DealtPubKeyShare>)>],
        proof: &'a Vec<(Player, <Self as WeightedVUF>::ProofShare)>,
    ) -> anyhow::Result<(
        Vec<&'a G2Projective>,
        Vec<&'a Vec<G1Projective>>,
        Vec<Scalar>,
        Vec<Range<usize>>,
    )> {
        // Collect all the evaluation points associated with each player's augmented pubkey sub shares.
        let mut sub_player_ids = Vec::with_capacity(wc.get_total_weight());
        // The G2 shares
        let mut shares = Vec::with_capacity(proof.len());
        // The RKs of each player
        let mut rks = Vec::with_capacity(proof.len());
        // The starting & ending index of each player in the `lagr` coefficients vector
        let mut ranges = Vec::with_capacity(proof.len());

        let mut k = 0;
        for (player, share) in proof {
            for j in 0..wc.get_player_weight(player) {
                sub_player_ids.push(wc.get_virtual_player(player, j).id);
            }

            let apk = apks[player.id]
                .as_ref()
                .ok_or_else(|| anyhow!("Missing APK for player {}", player.get_id()))?;

            rks.push(&apk.0.rks);
            shares.push(share);

            let w = wc.get_player_weight(player);
            ranges.push(k..k + w);
            k += w;
        }

        // Compute the Lagrange coefficients associated with those evaluation points
        let batch_dom = wc.get_batch_evaluation_domain();
        let lagr = lagrange_coefficients(batch_dom, &sub_player_ids[..], &Scalar::ZERO);
        Ok((shares, rks, lagr, ranges))
    }
```
