# Audit Report

## Title
Permanent Deadlock After Fallback Sync Failure in Consensus Observer

## Summary
The consensus observer enters a permanent stuck state after a single fallback sync failure, causing total loss of liveness. Contrary to the security question's premise of "repeated calls," the actual vulnerability is worse: after ONE failure, the node can NEVER attempt fallback sync again, resulting in permanent inability to sync.

## Finding Description

The vulnerability exists in the fallback sync failure handling logic across two files: [1](#0-0) [2](#0-1) 

When `sync_for_fallback()` spawns an async task to perform state synchronization, it immediately stores a `DropGuard` handle in `fallback_sync_handle`. If the `sync_for_duration()` call fails, the task logs an error and returns early WITHOUT sending a `FallbackSyncCompleted` notification.

The critical flaw: the `fallback_sync_handle` remains set even after task completion because the only cleanup mechanism requires receiving a successful notification: [3](#0-2) [4](#0-3) 

The `check_progress()` function in the main event loop prevents entering fallback mode when `in_fallback_mode()` returns true: [5](#0-4) 

**Attack Flow:**
1. Progress check detects syncing failure via `check_syncing_progress()`
2. Node enters fallback mode, spawning sync task and setting `fallback_sync_handle`
3. State sync fails (network issues, corrupted data, timeout, etc.)
4. Task returns without sending notification, leaving handle set
5. All subsequent `check_progress()` calls see `in_fallback_mode() == true` and exit early
6. Node permanently stuck - cannot enter fallback mode again, cannot sync, cannot make progress

This violates the **liveness invariant**: consensus observer nodes must be able to recover from temporary sync failures.

## Impact Explanation

**High Severity** - This qualifies under "Validator node slowdowns" and "Significant protocol violations" per the Aptos bug bounty criteria, though the actual impact is closer to Critical ("Total loss of liveness/network availability"):

- **Total Loss of Liveness**: After a single fallback sync failure, the consensus observer node becomes permanently unable to sync with the network
- **No Recovery Mechanism**: No timeout, no cleanup, no automatic retry - the node is stuck forever
- **Silent Failure**: The node continues running but cannot process new blocks
- **Network-Wide Risk**: If multiple observer nodes hit transient sync failures during network issues, they all become permanently stuck
- **Cascade Effect**: Once stuck, these nodes cannot serve as reliable data sources for other nodes

The vulnerability affects all consensus observer nodes in the network, making it a systemic liveness failure risk.

## Likelihood Explanation

**High Likelihood:**
- State sync failures are common during:
  - Network partitions or high latency
  - Database corruption or disk issues
  - Resource exhaustion (memory, CPU)
  - State sync service overload
  - Epoch transitions with timing issues
- The vulnerability triggers automatically on ANY sync failure - no attacker action required
- Once triggered, recovery requires node restart (operational intervention)
- In production networks with hundreds of observer nodes, this will occur regularly

## Recommendation

Add a task completion detection mechanism and automatic handle cleanup:

**Option 1 (Preferred): Use a notification channel for task completion**

Modify `sync_for_fallback()` to handle both success and failure cases:
- Send notification on both success AND failure
- Include error information in failure notifications
- Clear handle in both success and failure notification handlers
- Add exponential backoff before retrying after failures

**Option 2: Add timeout-based handle cleanup**

Implement a background task that:
- Checks if spawned task has completed (using `JoinHandle` instead of `AbortHandle`)
- Clears stale handles after task completion
- Allows retry after configurable timeout

**Option 3: Implement explicit state machine**

Replace boolean `in_fallback_mode()` with explicit states:
- `NotInFallback`
- `FallbackInProgress { started_at: Instant }`
- `FallbackFailed { failed_at: Instant, attempt: u32 }`

Add transition logic with timeout and retry limits.

## Proof of Concept

**Reproduction Steps:**

1. Deploy consensus observer node
2. Inject failure into state sync (using fail_point at line 646 of execution_client.rs): [6](#0-5) 

3. Trigger progress check that detects sync lag
4. Observer enters fallback mode and calls `sync_for_fallback()`
5. Injected failure causes `sync_for_duration()` to return error
6. Task exits without notification, handle remains set
7. All subsequent `check_progress()` calls exit immediately at line 173
8. Node permanently stuck - verify by monitoring metrics: [7](#0-6) 

The `OBSERVER_STATE_SYNC_FALLBACK_COUNTER` increments once, then never again, confirming the node is stuck.

**Note:** The security question asks about "infinite loop of sync_for_fallback() calls" but the actual vulnerability is the OPPOSITE - zero subsequent calls after first failure, creating permanent deadlock instead of infinite loop. This is arguably more severe as it's harder to detect and has no built-in mitigation.

### Citations

**File:** consensus/src/consensus_observer/observer/state_sync_manager.rs (L90-97)
```rust
    pub fn clear_active_fallback_sync(&mut self) {
        // If we're not actively syncing in fallback mode, log an error
        if !self.in_fallback_mode() {
            error!(LogSchema::new(LogEntry::ConsensusObserver)
                .message("Failed to clear fallback sync! No active sync handle found!"));
        }

        self.fallback_sync_handle = None;
```

**File:** consensus/src/consensus_observer/observer/state_sync_manager.rs (L117-187)
```rust
    pub fn sync_for_fallback(&mut self) {
        // Log that we're starting to sync in fallback mode
        info!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "Started syncing in fallback mode! Syncing duration: {:?} ms!",
                self.consensus_observer_config.observer_fallback_duration_ms
            ))
        );

        // Update the state sync fallback counter
        metrics::increment_counter_without_labels(&metrics::OBSERVER_STATE_SYNC_FALLBACK_COUNTER);

        // Clone the required components for the state sync task
        let consensus_observer_config = self.consensus_observer_config;
        let execution_client = self.execution_client.clone();
        let sync_notification_sender = self.state_sync_notification_sender.clone();

        // Spawn a task to sync for the fallback
        let (abort_handle, abort_registration) = AbortHandle::new_pair();
        tokio::spawn(Abortable::new(
            async move {
                // Update the state sync metrics now that we're syncing for the fallback
                metrics::set_gauge_with_label(
                    &metrics::OBSERVER_STATE_SYNC_EXECUTING,
                    metrics::STATE_SYNCING_FOR_FALLBACK,
                    1, // We're syncing for the fallback
                );

                // Get the fallback duration
                let fallback_duration =
                    Duration::from_millis(consensus_observer_config.observer_fallback_duration_ms);

                // Sync for the fallback duration
                let latest_synced_ledger_info = match execution_client
                    .clone()
                    .sync_for_duration(fallback_duration)
                    .await
                {
                    Ok(latest_synced_ledger_info) => latest_synced_ledger_info,
                    Err(error) => {
                        error!(LogSchema::new(LogEntry::ConsensusObserver)
                            .message(&format!("Failed to sync for fallback! Error: {:?}", error)));
                        return;
                    },
                };

                // Notify consensus observer that we've synced for the fallback
                let state_sync_notification =
                    StateSyncNotification::fallback_sync_completed(latest_synced_ledger_info);
                if let Err(error) = sync_notification_sender.send(state_sync_notification) {
                    error!(
                        LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                            "Failed to send state sync notification for fallback! Error: {:?}",
                            error
                        ))
                    );
                }

                // Clear the state sync metrics now that we're done syncing
                metrics::set_gauge_with_label(
                    &metrics::OBSERVER_STATE_SYNC_EXECUTING,
                    metrics::STATE_SYNCING_FOR_FALLBACK,
                    0, // We're no longer syncing for the fallback
                );
            },
            abort_registration,
        ));

        // Save the sync task handle
        self.fallback_sync_handle = Some(DropGuard::new(abort_handle));
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L168-201)
```rust
    async fn check_progress(&mut self) {
        debug!(LogSchema::new(LogEntry::ConsensusObserver)
            .message("Checking consensus observer progress!"));

        // If we've fallen back to state sync, we should wait for it to complete
        if self.state_sync_manager.in_fallback_mode() {
            info!(LogSchema::new(LogEntry::ConsensusObserver)
                .message("Waiting for state sync to complete fallback syncing!",));
            return;
        }

        // If state sync is syncing to a commit decision, we should wait for it to complete
        if self.state_sync_manager.is_syncing_to_commit() {
            info!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Waiting for state sync to reach commit decision: {:?}!",
                    self.observer_block_data.lock().root().commit_info()
                ))
            );
            return;
        }

        // Check if we need to fallback to state sync
        if let Err(error) = self.observer_fallback_manager.check_syncing_progress() {
            // Log the error and enter fallback mode
            warn!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Failed to make syncing progress! Entering fallback mode! Error: {:?}",
                    error
                ))
            );
            self.enter_fallback_mode().await;
            return;
        }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L916-964)
```rust
    /// Processes the state sync notification for the fallback sync
    async fn process_fallback_sync_notification(
        &mut self,
        latest_synced_ledger_info: LedgerInfoWithSignatures,
    ) {
        // Get the epoch and round for the latest synced ledger info
        let ledger_info = latest_synced_ledger_info.ledger_info();
        let epoch = ledger_info.epoch();
        let round = ledger_info.round();

        // Log the state sync notification
        info!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "Received state sync notification for fallback completion! Epoch {}, round: {}!",
                epoch, round
            ))
        );

        // Verify that there is an active fallback sync
        if !self.state_sync_manager.in_fallback_mode() {
            // Log the error and return early
            error!(LogSchema::new(LogEntry::ConsensusObserver).message(
                "Failed to process fallback sync notification! No active fallback sync found!"
            ));
            return;
        }

        // Reset the fallback manager state
        self.observer_fallback_manager
            .reset_syncing_progress(&latest_synced_ledger_info);

        // Update the root with the latest synced ledger info
        self.observer_block_data
            .lock()
            .update_root(latest_synced_ledger_info);

        // If the epoch has changed, end the current epoch and start the latest one
        let current_epoch_state = self.get_epoch_state();
        if epoch > current_epoch_state.epoch {
            // Wait for the latest epoch to start
            self.execution_client.end_epoch().await;
            self.wait_for_epoch_start().await;
        };

        // Reset the pending block state
        self.clear_pending_block_state().await;

        // Reset the state sync manager for the synced fallback
        self.state_sync_manager.clear_active_fallback_sync();
```

**File:** consensus/src/pipeline/execution_client.rs (L646-648)
```rust
        fail_point!("consensus::sync_for_duration", |_| {
            Err(anyhow::anyhow!("Injected error in sync_for_duration").into())
        });
```
