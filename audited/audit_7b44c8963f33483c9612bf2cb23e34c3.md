# Audit Report

## Title
Pipeline Abort Skips Mempool Cleanup Leading to Resource Leak and Transaction Re-execution

## Summary
The `post_ledger_update_fut` future is abortable and excluded from `wait_until_finishes()`, allowing rejected transactions to remain in mempool indefinitely when blocks are pruned. This causes memory leaks, wasted computational resources through re-execution attempts, and potential denial-of-service conditions.

## Finding Description

The consensus pipeline's `post_ledger_update_fut` is responsible for notifying mempool about rejected transactions after block execution completes. However, this future can be aborted during normal operations (block pruning, buffer resets, state sync) before completing its cleanup work.

**The Vulnerability Chain:**

1. **Abortable Design**: The `post_ledger_update_fut` is created with abort handles, making it cancellable: [1](#0-0) 

2. **Missing from Wait Guard**: When pipelines are aborted, `wait_until_finishes()` is called but explicitly excludes `post_ledger_update_fut`: [2](#0-1) 

3. **Cleanup Operation**: The `post_ledger_update()` function notifies mempool about rejected transactions (those with `TransactionStatus::Discard`): [3](#0-2) 

4. **Mempool Notification**: This notification removes rejected transactions from mempool's data structures: [4](#0-3) 

5. **Transaction Removal**: Mempool's `reject_transaction` removes transactions from all indices (hash, priority, timeline, parking lot): [5](#0-4) 

**Abort Trigger Points:**

- **Block Pruning** (routine operation during consensus): [6](#0-5) 

- **Buffer Manager Reset** (uses `wait_until_finishes` which doesn't wait for cleanup): [7](#0-6) 

- **Automatic Drop** (when blocks are deallocated): [8](#0-7) 

**Attack Scenario:**
1. Attacker submits transactions with known rejection conditions (e.g., `SEQUENCE_NUMBER_TOO_OLD`, invalid signatures)
2. Block proposer includes these transactions in a block
3. Block is executed; transactions are rejected during execution
4. `ledger_update_fut` completes successfully
5. Before `post_ledger_update_fut` completes mempool notification, block is pruned (competing fork wins, routine cleanup)
6. Pipeline abort cancels `post_ledger_update_fut`
7. Rejected transactions remain in mempool, occupying memory in multiple indices
8. Transactions may be re-proposed, re-broadcast, and re-executed in future blocks
9. Process repeats, accumulating leaked resources

## Impact Explanation

**Medium Severity** per Aptos bug bounty criteria:

1. **Memory Leak**: Rejected transactions remain in mempool's data structures until expiration (potentially hours). Each transaction occupies space in:
   - Hash index
   - Priority index  
   - Timeline index
   - Parking lot index
   - Main transaction store

2. **Resource Waste**: Rejected transactions that should be removed may be:
   - Re-proposed in subsequent blocks (wasting block space)
   - Re-executed by validators (wasting computational resources)
   - Re-broadcast across the network (wasting bandwidth)

3. **DoS Potential**: Mempool has size limits. Accumulated leaked transactions can:
   - Block valid transactions from entering mempool
   - Slow down mempool operations (searches, sorting)
   - Cause mempool pressure and rejection of legitimate transactions

4. **State Inconsistency**: Different validators may have different mempool states depending on timing of aborts, violating operational consistency expectations.

This meets **Medium Severity** criteria: "State inconsistencies requiring intervention" and can lead to "Limited funds loss" if legitimate transactions are blocked from entering mempool.

## Likelihood Explanation

**High Likelihood**:

1. **Frequent Trigger**: Block pruning occurs continuously during normal consensus operations as the blockchain progresses and old blocks are removed from memory
2. **Timing Window Exists**: The window between `ledger_update_fut` completion and `post_ledger_update_fut` completion is non-zero, creating a race condition
3. **No Attacker Action Required**: This occurs naturally during normal operations, though attackers can amplify by submitting transactions designed to be rejected
4. **Multiple Trigger Points**: Block drops, buffer resets, state sync, and pruning all trigger pipeline aborts
5. **Cumulative Effect**: Each occurrence leaks a small amount of resources, but accumulation over time degrades node performance

## Recommendation

**Option 1: Make post_ledger_update non-abortable** (Recommended)

Remove `post_ledger_update_fut` from abort handles to ensure mempool cleanup always completes:

```rust
let post_ledger_update_fut = spawn_shared_fut(
    Self::post_ledger_update(
        prepare_fut.clone(),
        ledger_update_fut.clone(),
        self.txn_notifier.clone(),
        block.clone(),
    ),
    None,  // Changed from Some(&mut abort_handles) to None
);
```

**Option 2: Include in wait_until_finishes**

Modify `wait_until_finishes()` to wait for cleanup futures:

```rust
pub async fn wait_until_finishes(self) {
    let _ = join_all(vec![
        self.execute_fut.boxed(),
        self.ledger_update_fut.boxed(),
        self.pre_commit_fut.boxed(),
        self.commit_ledger_fut.boxed(),
        self.notify_state_sync_fut.boxed(),
        self.post_ledger_update_fut.boxed(),  // ADD THIS
    ])
    .await;
}
```

**Option 3: Explicit wait before abort**

Add explicit waiting in abort paths:

```rust
pub fn abort_pipeline(&self) -> Option<PipelineFutures> {
    if let Some(futs) = self.pipeline_futs() {
        // Wait for cleanup to complete before aborting
        tokio::spawn(async move {
            let _ = futs.post_ledger_update_fut.await;
        });
    }
    // ... rest of abort logic
}
```

**Recommended approach**: Option 1 is cleanest. Mempool cleanup is essential for resource management and should complete before pipeline termination.

## Proof of Concept

```rust
// Test demonstrating the vulnerability
// File: consensus/src/tests/pipeline_abort_leak_test.rs

#[tokio::test]
async fn test_pipeline_abort_skips_mempool_cleanup() {
    // Setup: Create mock consensus environment with mempool
    let (mempool, txn_notifier) = setup_test_mempool();
    let pipeline_builder = create_test_pipeline_builder(txn_notifier);
    
    // Step 1: Submit transactions to mempool that will be rejected
    let rejected_txns = vec![
        create_txn_with_old_sequence_number(),  // Will be rejected as SEQUENCE_NUMBER_TOO_OLD
        create_txn_with_invalid_signature(),     // Will be rejected as INVALID_SIGNATURE
    ];
    for txn in &rejected_txns {
        mempool.add_txn(txn.clone());
    }
    
    // Verify transactions are in mempool
    assert_eq!(mempool.size(), 2);
    
    // Step 2: Create and execute a block containing these transactions
    let block = create_test_block(rejected_txns.clone());
    let pipelined_block = Arc::new(PipelinedBlock::new_ordered(
        block.clone(),
        OrderedBlockWindow::empty(),
    ));
    
    // Build pipeline futures
    let parent_futs = create_root_pipeline_futures();
    pipeline_builder.build_for_consensus(
        &pipelined_block,
        parent_futs,
        Box::new(|_, _| {}),
    );
    
    // Step 3: Wait for execution to complete (ledger_update_fut)
    let futs = pipelined_block.pipeline_futs().unwrap();
    let _ = futs.ledger_update_fut.await;  // Block execution completes
    
    // Step 4: Abort pipeline BEFORE post_ledger_update completes
    // This simulates block pruning during normal operation
    pipelined_block.abort_pipeline();
    
    // Step 5: Verify vulnerability - transactions still in mempool
    tokio::time::sleep(Duration::from_millis(100)).await;  // Give time for any cleanup
    
    // BUG: Rejected transactions are still in mempool!
    assert_eq!(mempool.size(), 2, "VULNERABILITY: Rejected transactions leaked in mempool");
    
    // Verify transactions were actually rejected in execution
    let compute_result = pipelined_block.compute_result();
    for status in compute_result.compute_status_for_input_txns() {
        assert!(matches!(status, TransactionStatus::Discard(_)), 
                "Transactions should be rejected");
    }
    
    // Expected behavior: mempool should be empty after cleanup
    // Actual behavior: mempool.size() == 2 (resource leak)
    
    // Additional impact: These transactions can be re-proposed
    let batch = mempool.get_batch(10, u64::MAX, true, vec![]);
    assert_eq!(batch.len(), 2, "Leaked transactions available for re-proposal");
}

// Helper to demonstrate cumulative leak over multiple rounds
#[tokio::test]
async fn test_cumulative_resource_leak() {
    let (mempool, txn_notifier) = setup_test_mempool();
    let pipeline_builder = create_test_pipeline_builder(txn_notifier);
    
    let initial_memory = get_mempool_memory_usage(&mempool);
    
    // Simulate 100 rounds of block execution with aborts
    for round in 0..100 {
        let rejected_txns = vec![create_txn_with_old_sequence_number()];
        mempool.add_txn(rejected_txns[0].clone());
        
        let block = create_test_block(rejected_txns);
        let pipelined_block = Arc::new(PipelinedBlock::new_ordered(block, OrderedBlockWindow::empty()));
        
        pipeline_builder.build_for_consensus(&pipelined_block, create_root_pipeline_futures(), Box::new(|_, _| {}));
        
        // Execute and abort before cleanup
        let _ = pipelined_block.pipeline_futs().unwrap().ledger_update_fut.await;
        pipelined_block.abort_pipeline();
    }
    
    let final_memory = get_mempool_memory_usage(&mempool);
    let leaked_memory = final_memory - initial_memory;
    
    // Demonstrate resource leak accumulation
    assert!(leaked_memory > 0, "Memory leaked: {} bytes over 100 rounds", leaked_memory);
    assert!(mempool.size() >= 100, "At least 100 transactions leaked");
}
```

## Notes

This vulnerability violates the **Resource Limits** invariant (#9: "All operations must respect gas, storage, and computational limits"). The mempool cleanup is an essential resource management operation that is being skipped, leading to unbounded resource accumulation.

The issue is particularly insidious because:
1. It occurs during normal operations without attacker intervention
2. The impact is gradual and cumulative
3. No error is logged when cleanup is skipped
4. Different validators may experience different leak rates based on their pruning patterns

While transaction expiration GC eventually cleans up leaked transactions, the delay (potentially hours) allows significant resource waste and potential DoS conditions to develop.

### Citations

**File:** consensus/src/pipeline/pipeline_builder.rs (L558-566)
```rust
        let post_ledger_update_fut = spawn_shared_fut(
            Self::post_ledger_update(
                prepare_fut.clone(),
                ledger_update_fut.clone(),
                self.txn_notifier.clone(),
                block.clone(),
            ),
            Some(&mut abort_handles),
        );
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L926-974)
```rust
    async fn post_ledger_update(
        prepare_fut: TaskFuture<PrepareResult>,
        ledger_update_fut: TaskFuture<LedgerUpdateResult>,
        mempool_notifier: Arc<dyn TxnNotifier>,
        block: Arc<Block>,
    ) -> TaskResult<PostLedgerUpdateResult> {
        let mut tracker = Tracker::start_waiting("post_ledger_update", &block);
        let (user_txns, _) = prepare_fut.await?;
        let (compute_result, _, _) = ledger_update_fut.await?;

        tracker.start_working();
        let compute_status = compute_result.compute_status_for_input_txns();
        // the length of compute_status is user_txns.len() + num_vtxns + 1 due to having blockmetadata
        if user_txns.len() >= compute_status.len() {
            // reconfiguration suffix blocks don't have any transactions
            // otherwise, this is an error
            if !compute_status.is_empty() {
                error!(
                        "Expected compute_status length and actual compute_status length mismatch! user_txns len: {}, compute_status len: {}, has_reconfiguration: {}",
                        user_txns.len(),
                        compute_status.len(),
                        compute_result.has_reconfiguration(),
                    );
            }
        } else {
            let user_txn_status = &compute_status[compute_status.len() - user_txns.len()..];
            // todo: avoid clone
            let txns: Vec<SignedTransaction> = user_txns
                .iter()
                .map(|txn| {
                    txn.borrow_into_inner()
                        .try_as_signed_user_txn()
                        .expect("must be a user txn")
                })
                .cloned()
                .collect();

            // notify mempool about failed transaction
            if let Err(e) = mempool_notifier
                .notify_failed_txn(&txns, user_txn_status)
                .await
            {
                error!(
                    error = ?e, "Failed to notify mempool of rejected txns",
                );
            }
        }
        Ok(())
    }
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L104-113)
```rust
    pub async fn wait_until_finishes(self) {
        let _ = join5(
            self.execute_fut,
            self.ledger_update_fut,
            self.pre_commit_fut,
            self.commit_ledger_fut,
            self.notify_state_sync_fut,
        )
        .await;
    }
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L361-365)
```rust
impl Drop for PipelinedBlock {
    fn drop(&mut self) {
        let _ = self.abort_pipeline();
    }
}
```

**File:** consensus/src/txn_notifier.rs (L48-99)
```rust
    async fn notify_failed_txn(
        &self,
        user_txns: &[SignedTransaction],
        user_txn_statuses: &[TransactionStatus],
    ) -> Result<(), MempoolError> {
        if user_txns.len() != user_txn_statuses.len() {
            return Err(format_err!(
                "[MempoolNotifier] {} != {}",
                user_txns.len(),
                user_txn_statuses.len()
            )
            .into());
        }

        let mut rejected_txns = vec![];
        for (txn, status) in user_txns.iter().zip_eq(user_txn_statuses) {
            if let TransactionStatus::Discard(reason) = status {
                rejected_txns.push(RejectedTransactionSummary {
                    sender: txn.sender(),
                    replay_protector: txn.replay_protector(),
                    hash: txn.committed_hash(),
                    reason: *reason,
                });
            }
        }

        if rejected_txns.is_empty() {
            return Ok(());
        }

        let (callback, callback_rcv) = oneshot::channel();
        let req = QuorumStoreRequest::RejectNotification(rejected_txns, callback);

        // send to shared mempool
        self.consensus_to_mempool_sender
            .clone()
            .try_send(req)
            .map_err(anyhow::Error::from)?;

        if let Err(e) = monitor!(
            "notify_mempool",
            timeout(
                Duration::from_millis(self.mempool_executed_txn_timeout_ms),
                callback_rcv
            )
            .await
        ) {
            Err(format_err!("[consensus] txn notifier did not receive ACK for commit notification sent to mempool on time: {:?}", e).into())
        } else {
            Ok(())
        }
    }
```

**File:** mempool/src/core_mempool/transaction_store.rs (L709-736)
```rust
    pub fn reject_transaction(
        &mut self,
        account: &AccountAddress,
        replay_protector: ReplayProtector,
        hash: &HashValue,
    ) {
        let mut txn_to_remove = None;
        if let Some((indexed_account, indexed_replay_protector)) = self.hash_index.get(hash) {
            if account == indexed_account && replay_protector == *indexed_replay_protector {
                txn_to_remove = self.get_mempool_txn(account, replay_protector).cloned();
            }
        }
        if let Some(txn_to_remove) = txn_to_remove {
            if let Some(txns) = self.transactions.get_mut(account) {
                txns.remove(&replay_protector);
            }
            self.index_remove(&txn_to_remove);

            if aptos_logger::enabled!(Level::Trace) {
                let mut txns_log = TxnsLog::new();
                txns_log.add(
                    txn_to_remove.get_sender(),
                    txn_to_remove.get_replay_protector(),
                );
                trace!(LogSchema::new(LogEntry::CleanRejectedTxn).txns(txns_log));
            }
        }
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L405-434)
```rust
    pub(super) fn find_blocks_to_prune(
        &self,
        next_window_root_id: HashValue,
    ) -> VecDeque<HashValue> {
        // Nothing to do if this is the window root
        if next_window_root_id == self.window_root_id {
            return VecDeque::new();
        }

        let mut blocks_pruned = VecDeque::new();
        let mut blocks_to_be_pruned = vec![self.linkable_window_root()];

        while let Some(block_to_remove) = blocks_to_be_pruned.pop() {
            block_to_remove.executed_block().abort_pipeline();
            // Add the children to the blocks to be pruned (if any), but stop when it reaches the
            // new root
            for child_id in block_to_remove.children() {
                if next_window_root_id == *child_id {
                    continue;
                }
                blocks_to_be_pruned.push(
                    self.get_linkable_block(child_id)
                        .expect("Child must exist in the tree"),
                );
            }
            // Track all the block ids removed
            blocks_pruned.push_back(block_to_remove.id());
        }
        blocks_pruned
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L554-556)
```rust
                if let Some(futs) = b.abort_pipeline() {
                    futs.wait_until_finishes().await;
                }
```
