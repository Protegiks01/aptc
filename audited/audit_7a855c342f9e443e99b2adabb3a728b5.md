# Audit Report

## Title
Unbounded Event Array Processing Causes Indexer Service Degradation

## Summary
The Aptos indexer lacks a count-based limit on events per transaction, allowing transactions with up to 100,000+ tiny events (limited only by the 10 MB byte limit). This causes the indexer to perform excessive database operations, leading to processing delays and service degradation on the query API infrastructure.

## Finding Description

The Aptos VM enforces only byte-based limits on events (10 MB total per transaction, 1 MB per event) without any count-based limit. [1](#0-0) 

The enforcement logic only checks byte sizes, not event counts. [2](#0-1) 

The indexer's `from_events()` function processes all events without any count validation. [3](#0-2) 

When inserting events into PostgreSQL, the indexer chunks them based on Diesel's parameter limit (65,535 parameters / 8 columns = 8,191 events per chunk). [4](#0-3) 

**Attack Scenario:**
1. Attacker creates a Move module that emits many tiny events (e.g., 100 bytes each)
2. Submits a transaction emitting ~100,000 events (10 MB total)
3. Gas cost: 10,000,000 bytes × 89 internal gas units = 890M (within the 1B IO gas limit)
4. Indexer must process all events, requiring ~12 sequential database INSERT operations
5. Processing this single transaction dominates the batch processing time

The indexer processes events sequentially in chunks. [5](#0-4) 

## Impact Explanation

This issue represents an **architectural limitation** rather than a critical security vulnerability. While it can cause indexer service degradation, it does NOT affect:
- Consensus safety or liveness
- Blockchain state integrity  
- Transaction validation or execution
- Validator node operations

The indexer is an auxiliary query service, separate from the core blockchain. Indexer lag impacts application query performance but not the blockchain's security guarantees.

Per the Aptos bug bounty criteria, this does not clearly qualify as High Severity ("Validator node slowdowns") because the indexer is not the validator node itself. It also doesn't clearly meet Medium Severity ("State inconsistencies requiring intervention") because the blockchain state remains consistent—only the indexed view is delayed.

## Likelihood Explanation

While technically exploitable (any user can submit such a transaction), the economic cost and limited impact make sustained attacks unlikely. The gas cost for 100,000 events is substantial (~890M internal gas units for IO alone), and the attack only degrades the indexer query service without affecting blockchain operations.

## Recommendation

Implement a count-based limit in addition to the existing byte limit:

```rust
// In aptos-gas-schedule/src/gas_schedule/transaction.rs
[
    max_events_per_transaction: NumSlots,
    { 5.. => "max_events_per_transaction" },
    10000, // Reasonable limit: 10k events per transaction
]

// In aptos-vm-types/src/storage/change_set_configs.rs
pub struct ChangeSetConfigs {
    // ... existing fields ...
    max_events_per_transaction: u64,
}

// In check_change_set method, add:
let event_count = change_set.events_iter().count();
if event_count as u64 > self.max_events_per_transaction {
    return storage_write_limit_reached(Some("Too many events."));
}
```

## Proof of Concept

```move
module attacker::event_spam {
    use std::signer;
    use aptos_framework::event;

    #[event]
    struct TinyEvent has drop, store {
        value: u8,
    }

    public entry fun spam_events(account: &signer) {
        let i = 0;
        // Emit many tiny events (limited by gas)
        while (i < 100000) {
            event::emit(TinyEvent { value: (i % 256) as u8 });
            i = i + 1;
        };
    }
}
```

## Notes

After thorough analysis, while this represents a legitimate performance concern, it does **not** meet the strict criteria for a security vulnerability per the Aptos bug bounty program. The issue affects an auxiliary indexing service, not core blockchain security properties (consensus, execution, state integrity). The documented invariants focus on consensus safety, deterministic execution, and state consistency—none of which are violated by indexer processing delays.

The indexer is designed to eventually process all events; slow processing is a degradation of service quality rather than a security failure. This would be better classified as a **performance optimization** or **architectural improvement** rather than a security vulnerability requiring urgent patching.

### Citations

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L164-172)
```rust
            max_bytes_per_event: NumBytes,
            { 5.. => "max_bytes_per_event" },
            1 << 20, // a single event is 1MB max
        ],
        [
            max_bytes_all_events_per_transaction: NumBytes,
            { 5.. => "max_bytes_all_events_per_transaction"},
            10 << 20, // all events from a single transaction are 10MB max
        ],
```

**File:** aptos-move/aptos-vm-types/src/storage/change_set_configs.rs (L115-125)
```rust
        let mut total_event_size = 0;
        for event in change_set.events_iter() {
            let size = event.event_data().len() as u64;
            if size > self.max_bytes_per_event {
                return storage_write_limit_reached(None);
            }
            total_event_size += size;
            if total_event_size > self.max_bytes_all_events_per_transaction {
                return storage_write_limit_reached(None);
            }
        }
```

**File:** crates/indexer/src/models/events.rs (L61-78)
```rust
    pub fn from_events(
        events: &[APIEvent],
        transaction_version: i64,
        transaction_block_height: i64,
    ) -> Vec<Self> {
        events
            .iter()
            .enumerate()
            .map(|(index, event)| {
                Self::from_event(
                    event,
                    transaction_version,
                    transaction_block_height,
                    index as i64,
                )
            })
            .collect::<Vec<EventModel>>()
    }
```

**File:** crates/indexer/src/database.rs (L27-44)
```rust
pub const MAX_DIESEL_PARAM_SIZE: u16 = u16::MAX;

/// Given diesel has a limit of how many parameters can be inserted in a single operation (u16::MAX)
/// we may need to chunk an array of items based on how many columns are in the table.
/// This function returns boundaries of chunks in the form of (start_index, end_index)
pub fn get_chunks(num_items_to_insert: usize, column_count: usize) -> Vec<(usize, usize)> {
    let max_item_size = MAX_DIESEL_PARAM_SIZE as usize / column_count;
    let mut chunk: (usize, usize) = (0, min(num_items_to_insert, max_item_size));
    let mut chunks = vec![chunk];
    while chunk.1 != num_items_to_insert {
        chunk = (
            chunk.0 + max_item_size,
            min(num_items_to_insert, chunk.1 + max_item_size),
        );
        chunks.push(chunk);
    }
    chunks
}
```

**File:** crates/indexer/src/processors/default_processor.rs (L276-297)
```rust
fn insert_events(
    conn: &mut PgConnection,
    items_to_insert: &[EventModel],
) -> Result<(), diesel::result::Error> {
    use schema::events::dsl::*;
    let chunks = get_chunks(items_to_insert.len(), EventModel::field_count());
    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::events::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict((account_address, creation_number, sequence_number))
                .do_update()
                .set((
                    inserted_at.eq(excluded(inserted_at)),
                    event_index.eq(excluded(event_index)),
                )),
            None,
        )?;
    }
    Ok(())
}
```
