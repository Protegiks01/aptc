# Audit Report

## Title
Stale `highest_certified_block_id` Reference Causes Validator Panic After Block Pruning

## Summary
The `BlockTree` struct maintains a `highest_certified_block_id` field that is never updated during block pruning operations. When the referenced block is pruned from the tree, subsequent quorum certificate insertions trigger a panic, crashing the validator node. This represents a consensus-layer denial-of-service vulnerability affecting validator availability.

## Finding Description

The `BlockTree` struct tracks the highest certified block using the `highest_certified_block_id` field, which stores the block ID of the certified block with the highest round number. [1](#0-0) 

This field is **only updated** in the `insert_quorum_cert` method when a quorum certificate for a higher-round block is inserted. A grep search of the entire codebase confirms there is exactly one assignment to `highest_certified_block_id` in the entire repository, occurring at line 369 within `insert_quorum_cert`. [2](#0-1) 

The critical vulnerability occurs at line 368, where the code compares rounds by calling `highest_certified_block()`. This method retrieves the block using the stored ID and panics if the block doesn't exist. [3](#0-2) 

However, the `highest_certified_block_id` is **never updated during pruning**. When `commit_callback` is invoked after committing a block, it prunes old blocks from the tree through `find_blocks_to_prune` and `process_pruned_blocks`. [4](#0-3) 

The pruning process identifies blocks to remove based on whether they're on the path to the new window root. [5](#0-4) 

Eventually, pruned blocks are permanently removed from the `id_to_block` HashMap via `remove_block` when the pruned block buffer exceeds `max_pruned_blocks_in_mem`. [6](#0-5) [7](#0-6) 

**Attack Scenario:**

1. Validator receives block B at round R₁ with a quorum certificate, making B the `highest_certified_block`
2. Due to network partition, late message arrival, or validator being temporarily offline, the main chain progresses on a different fork
3. Block C at round R₂ (where R₂ >> R₁) on the alternate fork gets committed
4. During `commit_callback`, pruning removes block B as it's not on the committed chain path
5. As more blocks are committed, the `pruned_block_ids` buffer fills up and block B is permanently removed from `id_to_block`
6. `highest_certified_block_id` still references the pruned block B
7. A new QC arrives for block D with round R₃ > R₁
8. `insert_quorum_cert` is called, which is invoked from production code paths in `round_manager.rs` during consensus message processing
9. Line 368 executes: `if block.round() > self.highest_certified_block().round()`
10. `highest_certified_block()` attempts to retrieve block B from `id_to_block`
11. Block B no longer exists → `expect("Highest cerfified block must exist")` panics
12. Validator node crashes

This vulnerability is triggered through normal consensus operations when QCs are processed, as evidenced by call sites in the round manager. [8](#0-7) [9](#0-8) 

## Impact Explanation

**Severity: High** (Validator node crash/DoS)

This vulnerability causes validator nodes to panic and crash, directly impacting network availability and consensus participation. Per the Aptos bug bounty criteria, this qualifies as **High severity** because it causes validator node crashes requiring manual restart.

The impact includes:
- **Complete validator node failure**: The panic crashes the entire validator process
- **Manual intervention required**: Validator operators must restart the node
- **Consensus participation loss**: During downtime, the validator cannot vote on blocks
- **Potential network liveness degradation**: If multiple validators experience similar network conditions and crash simultaneously, network throughput decreases

This does not reach Critical severity because:
- No fund loss or theft occurs
- No permanent network partition results
- Consensus safety is not violated (no double-spending or chain splits)
- Validators can recover through restart

## Likelihood Explanation

**Likelihood: Medium-High**

This vulnerability can occur through natural network conditions without requiring malicious behavior:

1. **Network Partitions**: Temporary network splits naturally cause validators to receive blocks on different forks
2. **Validator Rejoining**: When a validator restarts or reconnects after being offline, it may have certified a block on a fork that the network has since moved past
3. **Out-of-Order Message Delivery**: P2P networking naturally causes QCs and blocks to arrive in non-sequential order
4. **Fork Resolution**: During normal AptosBFT operation, competing forks can exist until one gets committed, leaving validators with certified blocks on non-committed forks

The likelihood is medium-high because:
- No malicious behavior is required
- The scenario naturally arises during network instabilities
- Byzantine Fault Tolerant consensus inherently involves handling multiple forks
- The buffer mechanism delays but does not prevent the issue (over time, old blocks will be removed)

The vulnerability is amplified in environments with unreliable network connectivity, validators with intermittent connections, or high geographic distribution.

## Recommendation

Update `highest_certified_block_id` during the pruning process to ensure it always points to a valid block. Specifically:

1. In `commit_callback` or `process_pruned_blocks`, check if the `highest_certified_block_id` is being pruned
2. If so, update it to the highest certified block among remaining blocks in the tree
3. Alternatively, modify `highest_certified_block()` to handle the case where the block no longer exists, falling back to the commit root or ordered root

Example fix in `commit_callback`:
```rust
// After pruning blocks
if ids_to_remove.contains(&self.highest_certified_block_id) {
    // Find the highest certified block among remaining blocks
    let new_highest = self.id_to_quorum_cert
        .iter()
        .filter(|(id, _)| self.block_exists(id))
        .max_by_key(|(_, qc)| qc.certified_block().round())
        .map(|(id, qc)| (*id, Arc::clone(qc)));
    
    if let Some((new_id, new_qc)) = new_highest {
        self.highest_certified_block_id = new_id;
        self.highest_quorum_cert = new_qc;
    } else {
        // Fallback to commit root
        self.highest_certified_block_id = self.commit_root_id;
        self.highest_quorum_cert = self.get_quorum_cert_for_block(&self.commit_root_id)
            .expect("Commit root must have QC");
    }
}
```

## Proof of Concept

While a complete executable PoC requires setting up a full consensus test environment, the vulnerability can be demonstrated through the following sequence in a Rust integration test:

```rust
// 1. Create BlockTree with initial root
// 2. Insert block B at round 100 with QC (becomes highest_certified_block)
// 3. Build alternate fork with block C at round 200
// 4. Commit block C, triggering pruning of block B
// 5. Fill pruned_block_ids buffer to force removal of B from id_to_block
// 6. Insert new QC for block D at round 150
// 7. insert_quorum_cert will call highest_certified_block().round()
// 8. Observe panic: "Highest cerfified block must exist"
```

The vulnerability is directly observable by examining the code paths and confirming that no safeguards exist to update `highest_certified_block_id` during pruning operations.

## Notes

This vulnerability represents a **logic bug in state management** where reference integrity is not maintained across pruning operations. It affects validator availability (liveness) rather than consensus safety. The issue is exacerbated by the fact that `highest_certified_block_id` can legitimately reference blocks on non-committed forks in a BFT consensus system, but the pruning logic does not account for this scenario.

### Citations

**File:** consensus/src/block_storage/block_tree.rs (L83-83)
```rust
    highest_certified_block_id: HashValue,
```

**File:** consensus/src/block_storage/block_tree.rs (L174-181)
```rust
    fn remove_block(&mut self, block_id: HashValue) {
        // Remove the block from the store
        if let Some(block) = self.id_to_block.remove(&block_id) {
            let round = block.executed_block().round();
            self.round_to_ids.remove(&round);
        };
        self.id_to_quorum_cert.remove(&block_id);
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L208-211)
```rust
    pub(super) fn highest_certified_block(&self) -> Arc<PipelinedBlock> {
        self.get_block(&self.highest_certified_block_id)
            .expect("Highest cerfified block must exist")
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L368-371)
```rust
                if block.round() > self.highest_certified_block().round() {
                    self.highest_certified_block_id = block.id();
                    self.highest_quorum_cert = Arc::clone(&qc);
                }
```

**File:** consensus/src/block_storage/block_tree.rs (L405-434)
```rust
    pub(super) fn find_blocks_to_prune(
        &self,
        next_window_root_id: HashValue,
    ) -> VecDeque<HashValue> {
        // Nothing to do if this is the window root
        if next_window_root_id == self.window_root_id {
            return VecDeque::new();
        }

        let mut blocks_pruned = VecDeque::new();
        let mut blocks_to_be_pruned = vec![self.linkable_window_root()];

        while let Some(block_to_remove) = blocks_to_be_pruned.pop() {
            block_to_remove.executed_block().abort_pipeline();
            // Add the children to the blocks to be pruned (if any), but stop when it reaches the
            // new root
            for child_id in block_to_remove.children() {
                if next_window_root_id == *child_id {
                    continue;
                }
                blocks_to_be_pruned.push(
                    self.get_linkable_block(child_id)
                        .expect("Child must exist in the tree"),
                );
            }
            // Track all the block ids removed
            blocks_pruned.push_back(block_to_remove.id());
        }
        blocks_pruned
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L496-510)
```rust
    pub(super) fn process_pruned_blocks(&mut self, mut newly_pruned_blocks: VecDeque<HashValue>) {
        counters::NUM_BLOCKS_IN_TREE.sub(newly_pruned_blocks.len() as i64);
        // The newly pruned blocks are pushed back to the deque pruned_block_ids.
        // In case the overall number of the elements is greater than the predefined threshold,
        // the oldest elements (in the front of the deque) are removed from the tree.
        self.pruned_block_ids.append(&mut newly_pruned_blocks);
        if self.pruned_block_ids.len() > self.max_pruned_blocks_in_mem {
            let num_blocks_to_remove = self.pruned_block_ids.len() - self.max_pruned_blocks_in_mem;
            for _ in 0..num_blocks_to_remove {
                if let Some(id) = self.pruned_block_ids.pop_front() {
                    self.remove_block(id);
                }
            }
        }
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L567-600)
```rust
    pub fn commit_callback(
        &mut self,
        storage: Arc<dyn PersistentLivenessStorage>,
        block_id: HashValue,
        block_round: Round,
        finality_proof: WrappedLedgerInfo,
        commit_decision: LedgerInfoWithSignatures,
        window_size: Option<u64>,
    ) {
        let current_round = self.commit_root().round();
        let committed_round = block_round;
        let commit_proof = finality_proof
            .create_merged_with_executed_state(commit_decision)
            .expect("Inconsistent commit proof and evaluation decision, cannot commit block");

        debug!(
            LogSchema::new(LogEvent::CommitViaBlock).round(current_round),
            committed_round = committed_round,
            block_id = block_id,
        );

        let window_root_id = self.find_window_root(block_id, window_size);
        let ids_to_remove = self.find_blocks_to_prune(window_root_id);

        if let Err(e) = storage.prune_tree(ids_to_remove.clone().into_iter().collect()) {
            // it's fine to fail here, as long as the commit succeeds, the next restart will clean
            // up dangling blocks, and we need to prune the tree to keep the root consistent with
            // executor.
            warn!(error = ?e, "fail to delete block");
        }
        self.process_pruned_blocks(ids_to_remove);
        self.update_window_root(window_root_id);
        self.update_highest_commit_cert(commit_proof);
    }
```

**File:** consensus/src/round_manager.rs (L1930-1936)
```rust
        let result = self
            .block_store
            .insert_quorum_cert(&qc, &mut self.create_block_retriever(preferred_peer))
            .await
            .context("[RoundManager] Failed to process a newly aggregated QC");
        self.process_certificates().await?;
        result
```

**File:** consensus/src/round_manager.rs (L1951-1960)
```rust
                let result = self
                    .block_store
                    .insert_quorum_cert(
                        verified_qc.as_ref(),
                        &mut self.create_block_retriever(preferred_peer),
                    )
                    .await
                    .context("[RoundManager] Failed to process the QC from order vote msg");
                self.process_certificates().await?;
                result
```
