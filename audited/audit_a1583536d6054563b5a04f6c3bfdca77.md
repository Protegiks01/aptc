# Audit Report

## Title
Safety Rules Race Condition: Shared SafetyRules Instance Allows Cross-Epoch Signing Without Epoch Validation

## Summary
The Aptos consensus implementation contains a critical race condition where a single `SafetyRules` instance is shared across all epochs through the `SafetyRulesManager`. During epoch transitions, the old epoch's execution pipeline can sign commit votes using the new epoch's updated safety rules, bypassing epoch validation. The `sign_commit_vote()` method lacks epoch validation, allowing epoch N's ledger info to be signed after the shared `SafetyRules` has been updated to epoch N+1.

## Finding Description

The vulnerability exists in the consensus epoch transition logic where safety rules are managed incorrectly across epoch boundaries.

**Shared SafetyRules Architecture:**

The `EpochManager` maintains a single `SafetyRulesManager` as a persistent field that is shared across all epochs: [1](#0-0) 

This `SafetyRulesManager` wraps a single `SafetyRules` instance in `Arc<RwLock<SafetyRules>>`: [2](#0-1) 

When `start_round_manager()` is called for a new epoch, it obtains a client from the shared manager: [3](#0-2) 

The `client()` method returns a new `LocalClient` that clones the `Arc`, sharing the same underlying `SafetyRules`: [4](#0-3) 

Each `LocalClient` operation acquires a write lock on the shared `SafetyRules`: [5](#0-4) 

**Epoch Transition Without Proper Shutdown:**

During epoch transitions via reconfig notifications, the old epoch's processors are NOT shutdown before initializing the new epoch. The `start_new_epoch()` function directly proceeds to start new epoch components without calling `shutdown_current_processor()`: [6](#0-5) 

When the new epoch initializes, it updates the shared `SafetyRules` instance's epoch state: [7](#0-6) 

And updates the validator signer to the new epoch's keys: [8](#0-7) 

**Missing Epoch Validation in sign_commit_vote:**

While regular vote operations validate the epoch: [9](#0-8) 

The `guarded_sign_commit_vote()` method completely lacks epoch validation: [10](#0-9) 

The `LedgerInfo` being signed contains epoch information that could be validated: [11](#0-10) 

**Race Condition Scenario:**

1. Epoch N's execution pipeline has ordered blocks and is preparing commit votes
2. Epoch N+1 starts via reconfig notification
3. `start_round_manager()` calls `perform_initialize()` which updates the shared `SafetyRules` to epoch N+1
4. Old epoch N's signing phase (still running) attempts to sign a commit vote for epoch N's ledger info
5. The signing phase acquires the write lock on the shared `SafetyRules` (now configured for epoch N+1)
6. `guarded_sign_commit_vote()` executes without epoch validation
7. The commit vote for epoch N is signed using epoch N+1's validator signer

This violates the consensus invariant that votes must be signed with the correct epoch's safety rules and validator keys.

## Impact Explanation

This vulnerability qualifies as **Critical Severity** under the Aptos bug bounty program for "Consensus/Safety violations".

The impact includes:

1. **Cryptographic Correctness Violation**: Ledger info from epoch N signed with epoch N+1's validator keys, breaking the invariant that signatures must match the epoch context.

2. **Potential Chain Fork**: If validators use different consensus keys across epochs, the mismatched signature would be invalid and rejected by peers, potentially causing validators to diverge on commit decisions.

3. **Undefined Consensus Behavior**: The consensus protocol assumes votes are always signed with the correct epoch's safety rules. Violating this assumption could lead to unexpected state transitions or safety violations.

4. **Safety Rules Bypass**: The lack of epoch validation in `sign_commit_vote()` means safety checks that should apply to epoch N (using epoch N's safety data) are instead performed using epoch N+1's safety data, potentially bypassing safety rules like `last_voted_round` constraints.

## Likelihood Explanation

**High Likelihood** - This race condition will occur naturally during every epoch transition:

1. Epoch transitions happen regularly on the Aptos network (every ~2 hours typically)
2. The race window exists whenever the old epoch's execution pipeline has pending commit votes during epoch transition
3. The shared `SafetyRules` instance is guaranteed to be updated before old epoch tasks fully terminate
4. No explicit synchronization prevents the old epoch from accessing the updated `SafetyRules`

The race is deterministic given the code structure - it's not dependent on specific timing but rather on the concurrent execution of old and new epoch processes with shared state.

## Recommendation

Implement proper epoch isolation for `SafetyRules`:

1. **Create epoch-specific SafetyRules instances**: Each epoch should have its own isolated `SafetyRules` instance rather than sharing one through `SafetyRulesManager`.

2. **Add epoch validation to sign_commit_vote**: 
```rust
fn guarded_sign_commit_vote(
    &mut self,
    ledger_info: LedgerInfoWithSignatures,
    new_ledger_info: LedgerInfo,
) -> Result<bls12381::Signature, Error> {
    self.signer()?;
    
    // ADD EPOCH VALIDATION
    let safety_data = self.persistent_storage.safety_data()?;
    self.verify_epoch(ledger_info.ledger_info().epoch(), &safety_data)?;
    self.verify_epoch(new_ledger_info.epoch(), &safety_data)?;

    // ... rest of method
}
```

3. **Enforce proper shutdown sequence**: Ensure `shutdown_current_processor()` is called and completes before starting the new epoch:
```rust
async fn start_new_epoch(&mut self, payload: OnChainConfigPayload<P>) {
    // Shutdown old epoch first
    self.shutdown_current_processor().await;
    
    // Then start new epoch
    // ... rest of initialization
}
```

## Proof of Concept

```rust
// Test demonstrating the race condition
#[tokio::test]
async fn test_safety_rules_epoch_race() {
    // Setup: Create epoch N with shared SafetyRules
    let safety_rules_manager = SafetyRulesManager::new(&config);
    let mut safety_rules_n = MetricsSafetyRules::new(
        safety_rules_manager.client(),
        storage.clone()
    );
    
    // Initialize for epoch N
    safety_rules_n.perform_initialize().unwrap();
    
    // Create a commit vote for epoch N
    let ledger_info_n = create_ledger_info_for_epoch(N);
    
    // Simulate epoch N+1 starting - updates shared SafetyRules
    let mut safety_rules_n_plus_1 = MetricsSafetyRules::new(
        safety_rules_manager.client(), // Same shared SafetyRules!
        storage.clone()
    );
    safety_rules_n_plus_1.perform_initialize().unwrap(); // Updates to epoch N+1
    
    // Old epoch attempts to sign commit vote
    // This will use epoch N+1's validator signer on epoch N's ledger info
    let result = safety_rules_n.sign_commit_vote(
        ledger_info_n.clone(),
        ledger_info_n.ledger_info().clone()
    );
    
    // Without epoch validation, this succeeds incorrectly
    assert!(result.is_ok()); // Should fail but doesn't!
}
```

The test demonstrates that the same `SafetyRulesManager` provides clients that share state, allowing cross-epoch signing without proper validation.

## Notes

This vulnerability affects the core consensus safety guarantees. While the exact exploit scenario depends on validator key management practices (whether validators reuse keys across epochs), the architectural flaw of sharing safety-critical state across epochs represents a fundamental violation of the principle that epochs should be isolated security domains. The missing epoch validation in `sign_commit_vote()` combined with the shared `SafetyRules` instance creates a race condition that could manifest as consensus misbehavior during epoch transitions.

### Citations

**File:** consensus/src/epoch_manager.rs (L145-145)
```rust
    safety_rules_manager: SafetyRulesManager,
```

**File:** consensus/src/epoch_manager.rs (L829-829)
```rust
            MetricsSafetyRules::new(self.safety_rules_manager.client(), self.storage.clone());
```

**File:** consensus/src/epoch_manager.rs (L1164-1329)
```rust
    async fn start_new_epoch(&mut self, payload: OnChainConfigPayload<P>) {
        let validator_set: ValidatorSet = payload
            .get()
            .expect("failed to get ValidatorSet from payload");
        let mut verifier: ValidatorVerifier = (&validator_set).into();
        verifier.set_optimistic_sig_verification_flag(self.config.optimistic_sig_verification);

        let epoch_state = Arc::new(EpochState {
            epoch: payload.epoch(),
            verifier: verifier.into(),
        });

        self.epoch_state = Some(epoch_state.clone());

        let onchain_consensus_config: anyhow::Result<OnChainConsensusConfig> = payload.get();
        let onchain_execution_config: anyhow::Result<OnChainExecutionConfig> = payload.get();
        let onchain_randomness_config_seq_num: anyhow::Result<RandomnessConfigSeqNum> =
            payload.get();
        let randomness_config_move_struct: anyhow::Result<RandomnessConfigMoveStruct> =
            payload.get();
        let onchain_jwk_consensus_config: anyhow::Result<OnChainJWKConsensusConfig> = payload.get();
        let dkg_state = payload.get::<DKGState>();

        if let Err(error) = &onchain_consensus_config {
            warn!("Failed to read on-chain consensus config {}", error);
        }

        if let Err(error) = &onchain_execution_config {
            warn!("Failed to read on-chain execution config {}", error);
        }

        if let Err(error) = &randomness_config_move_struct {
            warn!("Failed to read on-chain randomness config {}", error);
        }

        self.epoch_state = Some(epoch_state.clone());

        let consensus_config = onchain_consensus_config.unwrap_or_default();
        let execution_config = onchain_execution_config
            .unwrap_or_else(|_| OnChainExecutionConfig::default_if_missing());
        let onchain_randomness_config_seq_num = onchain_randomness_config_seq_num
            .unwrap_or_else(|_| RandomnessConfigSeqNum::default_if_missing());

        info!(
            epoch = epoch_state.epoch,
            local = self.randomness_override_seq_num,
            onchain = onchain_randomness_config_seq_num.seq_num,
            "Checking randomness config override."
        );
        if self.randomness_override_seq_num > onchain_randomness_config_seq_num.seq_num {
            warn!("Randomness will be force-disabled by local config!");
        }

        let onchain_randomness_config = OnChainRandomnessConfig::from_configs(
            self.randomness_override_seq_num,
            onchain_randomness_config_seq_num.seq_num,
            randomness_config_move_struct.ok(),
        );

        let jwk_consensus_config = onchain_jwk_consensus_config.unwrap_or_else(|_| {
            // `jwk_consensus_config` not yet initialized, falling back to the old configs.
            Self::equivalent_jwk_consensus_config_from_deprecated_resources(&payload)
        });

        let loaded_consensus_key = match self.load_consensus_key(&epoch_state.verifier) {
            Ok(k) => Arc::new(k),
            Err(e) => {
                panic!("load_consensus_key failed: {e}");
            },
        };

        let rand_configs = self.try_get_rand_config_for_new_epoch(
            loaded_consensus_key.clone(),
            &epoch_state,
            &onchain_randomness_config,
            dkg_state,
            &consensus_config,
        );

        let (rand_config, fast_rand_config) = match rand_configs {
            Ok((rand_config, fast_rand_config)) => (Some(rand_config), fast_rand_config),
            Err(reason) => {
                if onchain_randomness_config.randomness_enabled() {
                    if epoch_state.epoch > 2 {
                        error!(
                            "Failed to get randomness config for new epoch [{}]: {:?}",
                            epoch_state.epoch, reason
                        );
                    } else {
                        warn!(
                            "Failed to get randomness config for new epoch [{}]: {:?}",
                            epoch_state.epoch, reason
                        );
                    }
                }
                (None, None)
            },
        };

        info!(
            "[Randomness] start_new_epoch: epoch={}, rand_config={:?}, fast_rand_config={:?}",
            epoch_state.epoch, rand_config, fast_rand_config
        );

        let (network_sender, payload_client, payload_manager) = self
            .initialize_shared_component(
                &epoch_state,
                &consensus_config,
                loaded_consensus_key.clone(),
            )
            .await;

        let (rand_msg_tx, rand_msg_rx) = aptos_channel::new::<AccountAddress, IncomingRandGenRequest>(
            QueueStyle::KLAST,
            self.config.internal_per_key_channel_size,
            None,
        );

        self.rand_manager_msg_tx = Some(rand_msg_tx);

        // TODO(ibalajiarun): setup counters
        let (secret_share_manager_tx, secret_share_manager_rx) =
            aptos_channel::new::<AccountAddress, IncomingSecretShareRequest>(
                QueueStyle::KLAST,
                self.config.internal_per_key_channel_size,
                None,
            );
        self.secret_share_manager_tx = Some(secret_share_manager_tx);

        if consensus_config.is_dag_enabled() {
            warn!("DAG doesn't support secret sharing");
            self.start_new_epoch_with_dag(
                epoch_state,
                loaded_consensus_key.clone(),
                consensus_config,
                execution_config,
                onchain_randomness_config,
                jwk_consensus_config,
                network_sender,
                payload_client,
                payload_manager,
                rand_config,
                fast_rand_config,
                rand_msg_rx,
                secret_share_manager_rx,
            )
            .await
        } else {
            self.start_new_epoch_with_jolteon(
                loaded_consensus_key.clone(),
                epoch_state,
                consensus_config,
                execution_config,
                onchain_randomness_config,
                jwk_consensus_config,
                network_sender,
                payload_client,
                payload_manager,
                rand_config,
                fast_rand_config,
                rand_msg_rx,
                secret_share_manager_rx,
            )
            .await
        }
    }
```

**File:** consensus/safety-rules/src/safety_rules_manager.rs (L134-134)
```rust
            internal_safety_rules: SafetyRulesWrapper::Local(Arc::new(RwLock::new(safety_rules))),
```

**File:** consensus/safety-rules/src/safety_rules_manager.rs (L165-165)
```rust
                Box::new(LocalClient::new(safety_rules.clone()))
```

**File:** consensus/safety-rules/src/local_client.rs (L25-25)
```rust
    internal: Arc<RwLock<SafetyRules>>,
```

**File:** consensus/safety-rules/src/safety_rules.rs (L204-210)
```rust
    pub(crate) fn verify_epoch(&self, epoch: u64, safety_data: &SafetyData) -> Result<(), Error> {
        if epoch != safety_data.epoch {
            return Err(Error::IncorrectEpoch(epoch, safety_data.epoch));
        }

        Ok(())
    }
```

**File:** consensus/safety-rules/src/safety_rules.rs (L310-310)
```rust
        self.epoch_state = Some(epoch_state.clone());
```

**File:** consensus/safety-rules/src/safety_rules.rs (L328-329)
```rust
                            self.validator_signer =
                                Some(ValidatorSigner::new(author, Arc::new(consensus_key)));
```

**File:** consensus/safety-rules/src/safety_rules.rs (L372-418)
```rust
    fn guarded_sign_commit_vote(
        &mut self,
        ledger_info: LedgerInfoWithSignatures,
        new_ledger_info: LedgerInfo,
    ) -> Result<bls12381::Signature, Error> {
        self.signer()?;

        let old_ledger_info = ledger_info.ledger_info();

        if !old_ledger_info.commit_info().is_ordered_only()
            // When doing fast forward sync, we pull the latest blocks and quorum certs from peers
            // and store them in storage. We then compute the root ordered cert and root commit cert
            // from storage and start the consensus from there. But given that we are not storing the
            // ordered cert obtained from order votes in storage, instead of obtaining the root ordered cert
            // from storage, we set root ordered cert to commit certificate.
            // This means, the root ordered cert will not have a dummy executed_state_id in this case.
            // To handle this, we do not raise error if the old_ledger_info.commit_info() matches with
            // new_ledger_info.commit_info().
            && old_ledger_info.commit_info() != new_ledger_info.commit_info()
        {
            return Err(Error::InvalidOrderedLedgerInfo(old_ledger_info.to_string()));
        }

        if !old_ledger_info
            .commit_info()
            .match_ordered_only(new_ledger_info.commit_info())
        {
            return Err(Error::InconsistentExecutionResult(
                old_ledger_info.commit_info().to_string(),
                new_ledger_info.commit_info().to_string(),
            ));
        }

        // Verify that ledger_info contains at least 2f + 1 dostinct signatures
        if !self.skip_sig_verify {
            ledger_info
                .verify_signatures(&self.epoch_state()?.verifier)
                .map_err(|error| Error::InvalidQuorumCertificate(error.to_string()))?;
        }

        // TODO: add guarding rules in unhappy path
        // TODO: add extension check

        let signature = self.sign(&new_ledger_info)?;

        Ok(signature)
    }
```

**File:** types/src/ledger_info.rs (L113-115)
```rust
    pub fn epoch(&self) -> u64 {
        self.commit_info.epoch()
    }
```
