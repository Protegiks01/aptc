# Audit Report

## Title
Failover Bucket Assignment Failure in Single-Peer Mempool Configuration

## Summary
The `update_sender_bucket_for_peers()` function in the mempool priority system fails to assign any Failover bucket priorities when only one peer exists, completely bypassing the configured `default_failovers` redundancy mechanism and reducing transaction propagation reliability.

## Finding Description

The mempool's sender bucket assignment logic divides transactions into buckets and assigns them to peers with either Primary or Failover broadcast priorities. The failover mechanism provides redundancy by broadcasting transactions to backup peers after a configured delay (default 500ms). [1](#0-0) 

When assigning failover buckets, the code attempts to find peers that don't already have a given bucket assigned: [2](#0-1) 

**The vulnerability:** When only one peer exists (`num_prioritized_peers = 1`):

1. **Primary assignment succeeds**: All sender buckets are correctly assigned to the single peer with Primary priority (lines 402-409)
2. **Failover assignment fails**: The inner loop (line 417) runs only ONCE because `num_prioritized_peers = 1`
3. It checks if the bucket entry is vacant for peer_0, but it's NOT vacant (already has Primary)
4. The loop advances `peer_index = (0 + 1) % 1 = 0` (same peer)
5. Since the loop only runs 1 iteration, it exits without assigning Failover
6. **Result**: NO failover buckets are ever assigned

The failover delay mechanism is critical for transaction broadcasting: [3](#0-2) 

With no Failover assignments, transactions are only broadcast once via Primary priority, eliminating the redundancy that `default_failovers` is supposed to provide. [4](#0-3) 

## Impact Explanation

This qualifies as **Medium Severity** per the Aptos Bug Bounty criteria for the following reasons:

1. **State Inconsistency**: The system violates the expected behavior defined by the `default_failovers` configuration parameter, which defaults to 1 for most node types [5](#0-4) 

2. **Reduced Transaction Propagation Reliability**: Without failover broadcasts, if the primary broadcast fails or is delayed due to network issues, there is no backup mechanism to ensure transaction delivery

3. **Configuration Bypass**: User-configured redundancy settings are silently ignored in single-peer scenarios, violating the principle of least surprise

4. **Availability Impact**: During network partitions or when nodes temporarily have only one peer (common during startup or for VFNs connected to a single validator), transaction propagation becomes unreliable

The test validation function expects both Primary and Failover assignments for all buckets: [6](#0-5) 

However, no tests validate the single-peer scenario, allowing this bug to exist undetected.

## Likelihood Explanation

**High likelihood** of occurrence in production environments:

1. **Validator Fullnodes (VFNs)** often connect to only a single validator peer initially
2. **Network partitions** can temporarily reduce peer count to 1
3. **Node startup** periods before full peer discovery
4. **Intentional single-peer configurations** for testing or specific network topologies

The bug triggers automatically whenever `prioritized_peers.len() == 1`, requiring no attacker interaction.

## Recommendation

The failover assignment logic should handle the single-peer case by allowing the same peer to receive both Primary and Failover priorities for the same bucket. This maintains the redundancy contract even with one peer, ensuring delayed rebroadcasts occur as configured.

**Proposed fix** in `update_sender_bucket_for_peers()`:

```rust
// Assign sender buckets with Failover priority. Use Round Robin.
peer_index = 0;
let num_prioritized_peers = self.prioritized_peers.read().len();
for _ in 0..self.mempool_config.default_failovers {
    for bucket_index in 0..self.mempool_config.num_sender_buckets {
        // Find the first peer that already doesn't have the sender bucket, and add the bucket
        let mut assigned = false;
        for _ in 0..num_prioritized_peers {
            let peer = self.prioritized_peers.read()[peer_index];
            let sender_bucket_list =
                self.peer_to_sender_buckets.entry(peer).or_default();
            if let std::collections::hash_map::Entry::Vacant(e) =
                sender_bucket_list.entry(bucket_index)
            {
                e.insert(BroadcastPeerPriority::Failover);
                assigned = true;
                break;
            }
            peer_index = (peer_index + 1) % num_prioritized_peers;
        }
        
        // Special case: if only one peer exists and we couldn't assign failover
        // (because it already has Primary), we need to explicitly allow the same
        // peer to have both Primary and Failover for redundancy
        if !assigned && num_prioritized_peers == 1 {
            let peer = self.prioritized_peers.read()[0];
            let sender_bucket_list =
                self.peer_to_sender_buckets.entry(peer).or_default();
            // Force insert Failover even if Primary exists
            sender_bucket_list.insert(bucket_index, BroadcastPeerPriority::Failover);
        }
    }
}
```

Alternatively, store both priorities in a `Vec<BroadcastPeerPriority>` or `HashSet` instead of a single value per bucket.

## Proof of Concept

```rust
#[test]
fn test_single_peer_failover_assignment() {
    use crate::shared_mempool::priority::PrioritizedPeersState;
    use aptos_config::config::{MempoolConfig, NodeType};
    use aptos_config::network_id::{NetworkId, PeerNetworkId};
    use aptos_types::PeerId;
    use aptos_time_service::TimeService;
    use crate::network::BroadcastPeerPriority;
    
    // Create default mempool config with default_failovers = 1
    let mempool_config = MempoolConfig::default();
    assert_eq!(mempool_config.default_failovers, 1);
    
    let mut prioritized_peers_state = PrioritizedPeersState::new(
        mempool_config.clone(),
        NodeType::PublicFullnode,
        TimeService::mock(),
    );
    
    // Create only ONE peer
    let peer_1 = (
        PeerNetworkId::new(NetworkId::Public, PeerId::random()),
        None,
    );
    
    // Update with single peer
    prioritized_peers_state.update_prioritized_peers(vec![peer_1], 1000, 1000);
    
    // Verify that Primary buckets are assigned
    for bucket in 0..mempool_config.num_sender_buckets {
        let priority = prioritized_peers_state
            .get_sender_bucket_priority_for_peer(&peer_1.0, bucket);
        assert_eq!(priority, Some(BroadcastPeerPriority::Primary), 
            "Bucket {} should have Primary priority", bucket);
    }
    
    // BUG: This will FAIL - no Failover buckets are assigned!
    // Expected: Each bucket should also have a Failover assignment
    // Actual: No Failover assignments exist
    let has_any_failover = (0..mempool_config.num_sender_buckets).any(|bucket| {
        prioritized_peers_state
            .get_sender_bucket_priority_for_peer(&peer_1.0, bucket)
            == Some(BroadcastPeerPriority::Failover)
    });
    
    // This assertion demonstrates the bug
    assert!(has_any_failover, 
        "BUG: No Failover buckets assigned with single peer despite default_failovers = 1");
}
```

This test will fail, demonstrating that the failover assignment is broken when only one peer exists, violating the configured redundancy guarantees.

## Notes

The bug specifically affects scenarios where a node has exactly one peer, which is realistic during:
- Initial node startup before full peer discovery
- Network partitions reducing connectivity  
- VFN configurations with a single validator connection
- Intentional single-peer test or development setups

The vulnerability does not affect consensus safety directly, but it reduces the reliability of transaction propagation across the network, potentially causing delays in transaction inclusion and reducing overall network liveness under adverse conditions.

### Citations

**File:** mempool/src/shared_mempool/priority.rs (L272-277)
```rust
    fn update_sender_bucket_for_peers(
        &mut self,
        peer_monitoring_data: &HashMap<PeerNetworkId, Option<&PeerMonitoringMetadata>>,
        num_mempool_txns_received_since_peers_updated: u64,
        num_committed_txns_received_since_peers_updated: u64,
    ) {
```

**File:** mempool/src/shared_mempool/priority.rs (L411-430)
```rust
            // Assign sender buckets with Failover priority. Use Round Robin.
            peer_index = 0;
            let num_prioritized_peers = self.prioritized_peers.read().len();
            for _ in 0..self.mempool_config.default_failovers {
                for bucket_index in 0..self.mempool_config.num_sender_buckets {
                    // Find the first peer that already doesn't have the sender bucket, and add the bucket
                    for _ in 0..num_prioritized_peers {
                        let peer = self.prioritized_peers.read()[peer_index];
                        let sender_bucket_list =
                            self.peer_to_sender_buckets.entry(peer).or_default();
                        if let std::collections::hash_map::Entry::Vacant(e) =
                            sender_bucket_list.entry(bucket_index)
                        {
                            e.insert(BroadcastPeerPriority::Failover);
                            break;
                        }
                        peer_index = (peer_index + 1) % num_prioritized_peers;
                    }
                }
            }
```

**File:** mempool/src/shared_mempool/priority.rs (L963-986)
```rust
    fn prioritized_peer_state_well_formed(
        prioritized_peers_state: &PrioritizedPeersState,
        num_sender_buckets: u8,
    ) {
        // There is exists a peer with primary priority for each bucket
        for bucket in 0..num_sender_buckets {
            assert!(prioritized_peers_state.peer_to_sender_buckets.iter().any(
                |(_, sender_buckets)| {
                    sender_buckets.contains_key(&bucket)
                        && sender_buckets.get(&bucket).unwrap() == &BroadcastPeerPriority::Primary
                }
            ));
        }

        // There is exists a peer with failover priority for each bucket
        for bucket in 0..num_sender_buckets {
            assert!(prioritized_peers_state.peer_to_sender_buckets.iter().any(
                |(_, sender_buckets)| {
                    sender_buckets.contains_key(&bucket)
                        && sender_buckets.get(&bucket).unwrap() == &BroadcastPeerPriority::Failover
                }
            ));
        }
    }
```

**File:** mempool/src/shared_mempool/network.rs (L528-536)
```rust
                        let before = match peer_priority {
                            BroadcastPeerPriority::Primary => None,
                            BroadcastPeerPriority::Failover => Some(
                                Instant::now()
                                    - Duration::from_millis(
                                        self.mempool_config.shared_mempool_failover_delay_ms,
                                    ),
                            ),
                        };
```

**File:** config/src/config/mempool_config.rs (L48-49)
```rust
    /// Number of failover peers to broadcast to when the primary network is alive
    pub default_failovers: usize,
```

**File:** config/src/config/mempool_config.rs (L124-124)
```rust
            default_failovers: 1,
```
