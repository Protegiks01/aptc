# Audit Report

## Title
State Snapshot Restore Ordering Assumption Violation Leads to Data Loss During Resume

## Summary
The state snapshot restore process in `restore.rs` assumes that chunks in the manifest are sorted by `last_key` when using `skip_while` to resume interrupted restores. If chunks are unsorted (due to malicious manifest or buggy backup software), the resume logic will skip wrong chunks, causing data loss and restore failure.

## Finding Description

The vulnerability exists in the `run_impl()` function where resume logic determines which chunks to skip: [1](#0-0) 

The code retrieves the resume point (hash of the last successfully restored key) and uses `skip_while` to skip chunks where `chunk.last_key <= resume_point`. The `skip_while` iterator adapter skips elements from the start of the iterator **while** the predicate is true, then stops and yields all remaining elements once the predicate becomes false.

**Critical Assumption**: This logic assumes chunks are sorted by `last_key` in ascending order. If chunks are sorted, then once we encounter the first chunk with `last_key > resume_point`, all subsequent chunks will also have `last_key > resume_point`, making `skip_while` safe.

**Violation**: The manifest structure provides no ordering guarantees: [2](#0-1) 

During normal backup operations, chunks are created in order by the `Chunker` and maintained by `try_buffered_x`: [3](#0-2) 

The `try_buffered_x` implementation maintains order through `FuturesOrderedX`: [4](#0-3) 

However, **no validation** exists to verify chunks remain sorted when the manifest is loaded from storage: [5](#0-4) 

**Attack Scenario**:

1. Attacker creates a malicious manifest with unsorted chunks:
   - Chunk 0: last_key_hash = `0x03`
   - Chunk 1: last_key_hash = `0x50` 
   - Chunk 2: last_key_hash = `0x12` (out of order - should be between 0 and 1)

2. Victim starts restore, processes Chunks 0 and 1, progress = `0x50`, then crashes

3. On resume, `skip_while(|chunk| chunk.last_key <= 0x50)` evaluates:
   - Chunk 0: `0x03 <= 0x50` → true, skip
   - Chunk 1: `0x50 <= 0x50` → true, skip  
   - Chunk 2: `0x12 <= 0x50` → true, skip
   - End of list

4. **Result**: Chunk 2 is skipped even though it was never processed, causing permanent data loss

The internal chunk-level overlap detection cannot prevent this: [6](#0-5) 

This only skips keys within a chunk that have already been processed, but cannot recover chunks that were incorrectly skipped by the manifest-level `skip_while` logic.

## Impact Explanation

**Severity: High**

This vulnerability breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs."

Impact:
1. **Data Loss**: Keys from incorrectly-skipped chunks are permanently missing from restored state
2. **Restore Failure**: The final root hash will not match the expected value, causing restore to fail
3. **Validator Availability**: Validators cannot recover from backups, potentially affecting network liveness if multiple validators are impacted simultaneously
4. **No Clear Error**: The failure manifests as a root hash mismatch, obscuring the actual cause (manifest ordering violation)

This falls under **High severity** per Aptos bug bounty criteria: "Significant protocol violations" and "Validator node slowdowns" (inability to restore impacts validator operations). While not directly causing consensus breaks, it undermines the backup/restore infrastructure critical for validator recovery.

## Likelihood Explanation

**Likelihood: Medium**

While normal backup operations produce sorted chunks, the vulnerability can be triggered through:

1. **Malicious Backup Distribution**: Attacker distributes corrupted backups to validators
2. **Buggy Backup Software**: Third-party or modified backup tools that don't maintain ordering
3. **Backup Corruption**: Storage system errors or manual manifest modifications
4. **Supply Chain Attack**: Compromise of backup distribution infrastructure

The attack requires:
- Victim to obtain and use an unsorted manifest (realistic if sourcing backups externally)
- Restore interruption (common during large state snapshots)
- Resume operation (standard recovery procedure)

Validators commonly need backups for:
- Disaster recovery
- Bootstrapping new nodes
- State synchronization issues

The lack of validation means malformed manifests are not rejected, increasing likelihood.

## Recommendation

Add manifest validation to verify chunks are sorted by `last_key` before processing:

```rust
async fn run_impl(self) -> Result<()> {
    // ... existing code ...
    
    let manifest: StateSnapshotBackup =
        self.storage.load_json_file(&self.manifest_handle).await?;
    
    // ADDED: Validate chunks are sorted by last_key
    for window in manifest.chunks.windows(2) {
        ensure!(
            window[0].last_key <= window[1].last_key,
            "Manifest chunks must be sorted by last_key. Found {} > {} at indices",
            window[0].last_key,
            window[1].last_key
        );
    }
    
    // ... rest of existing code ...
}
```

Alternative: Replace `skip_while` with `filter` to skip all chunks with `last_key <= resume_point` regardless of order, though this is less efficient and doesn't address the root cause.

## Proof of Concept

```rust
#[tokio::test]
async fn test_unsorted_chunks_cause_data_loss() {
    use aptos_crypto::HashValue;
    
    // Create manifest with unsorted chunks
    let mut manifest = StateSnapshotBackup {
        version: 1000,
        epoch: 1,
        root_hash: HashValue::random(),
        chunks: vec![
            StateSnapshotChunk {
                first_idx: 0,
                last_idx: 2,
                first_key: HashValue::from_u64(0x01),
                last_key: HashValue::from_u64(0x03), // Small
                blobs: FileHandle::new("chunk0"),
                proof: FileHandle::new("proof0"),
            },
            StateSnapshotChunk {
                first_idx: 3,
                last_idx: 5,
                first_key: HashValue::from_u64(0x50),
                last_key: HashValue::from_u64(0x52), // Large
                blobs: FileHandle::new("chunk1"),
                proof: FileHandle::new("proof1"),
            },
            StateSnapshotChunk {
                first_idx: 6,
                last_idx: 8,
                first_key: HashValue::from_u64(0x10),
                last_key: HashValue::from_u64(0x12), // Medium - OUT OF ORDER
                blobs: FileHandle::new("chunk2"),
                proof: FileHandle::new("proof2"),
            },
        ],
        proof: FileHandle::new("state_proof"),
    };
    
    // Simulate resume after processing first 2 chunks
    let resume_point = HashValue::from_u64(0x52);
    
    // This is what the code does
    let chunks: Vec<_> = manifest.chunks
        .into_iter()
        .skip_while(|chunk| chunk.last_key <= resume_point)
        .collect();
    
    // BUG: Chunk 2 (with last_key 0x12) should be processed but was skipped
    // because it appears after Chunk 1 in the manifest
    assert_eq!(chunks.len(), 0, "All chunks skipped, including chunk 2 which was never processed!");
    
    // Expected behavior: Chunk 2 should be in the list
    // Actual behavior: Empty list, data from chunk 2 is lost
}
```

## Notes

This vulnerability is subtle because:
1. Normal operations produce sorted chunks via `FuturesOrderedX`
2. Cryptographic validations (proof verification) pass regardless of chunk ordering  
3. The failure manifests as a root hash mismatch, not an obvious ordering error
4. Within-chunk overlap detection cannot compensate for whole-chunk skipping

The backup-restore infrastructure is critical for validator operations and disaster recovery. This ordering assumption violation undermines that reliability and could be exploited to prevent validators from recovering, contributing to network availability issues.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L123-136)
```rust
        let manifest: StateSnapshotBackup =
            self.storage.load_json_file(&self.manifest_handle).await?;
        let (txn_info_with_proof, li): (TransactionInfoWithProof, LedgerInfoWithSignatures) =
            self.storage.load_bcs_file(&manifest.proof).await?;
        txn_info_with_proof.verify(li.ledger_info(), manifest.version)?;
        let state_root_hash = txn_info_with_proof
            .transaction_info()
            .ensure_state_checkpoint_hash()?;
        ensure!(
            state_root_hash == manifest.root_hash,
            "Root hash mismatch with that in proof. root hash: {}, expected: {}",
            manifest.root_hash,
            state_root_hash,
        );
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L165-174)
```rust
        let resume_point_opt = receiver.lock().as_mut().unwrap().previous_key_hash()?;
        let chunks = if let Some(resume_point) = resume_point_opt {
            manifest
                .chunks
                .into_iter()
                .skip_while(|chunk| chunk.last_key <= resume_point)
                .collect()
        } else {
            manifest.chunks
        };
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/manifest.rs (L29-51)
```rust
/// State snapshot backup manifest, representing a complete state view at specified version.
#[derive(Deserialize, Serialize)]
pub struct StateSnapshotBackup {
    /// Version at which this state snapshot is taken.
    pub version: Version,
    /// Epoch in which this state snapshot is taken.
    pub epoch: u64,
    /// Hash of the state tree root.
    pub root_hash: HashValue,
    /// All account blobs in chunks.
    pub chunks: Vec<StateSnapshotChunk>,
    /// BCS serialized
    /// `Tuple(TransactionInfoWithProof, LedgerInfoWithSignatures)`.
    ///   - The `TransactionInfoWithProof` is at `Version` above, and carries the same `root_hash`
    /// above; It proves that at specified version the root hash is as specified in a chain
    /// represented by the LedgerInfo below.
    ///   - The signatures on the `LedgerInfoWithSignatures` has a version greater than or equal to
    /// the version of this backup but is within the same epoch, so the signatures on it can be
    /// verified by the validator set in the same epoch, which can be provided by an
    /// `EpochStateBackup` recovered prior to this to the DB; Requiring it to be in the same epoch
    /// limits the requirement on such `EpochStateBackup` to no older than the same epoch.
    pub proof: FileHandle,
}
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/backup.rs (L250-268)
```rust
        let chunk_manifest_fut_stream =
            chunk_stream.map_ok(|chunk| self.write_chunk(&backup_handle, chunk));

        let chunks: Vec<_> = chunk_manifest_fut_stream
            .try_buffered_x(8, 4) // 4 concurrently, at most 8 results in buffer.
            .map_ok(|chunk_manifest| {
                let last_idx = chunk_manifest.last_idx;
                info!(
                    last_idx = last_idx,
                    values_per_second =
                        ((last_idx + 1) as f64 / start.elapsed().as_secs_f64()) as u64,
                    "Chunk written."
                );
                chunk_manifest
            })
            .try_collect()
            .await?;

        self.write_manifest(&backup_handle, chunks).await
```

**File:** storage/backup/backup-cli/src/utils/stream/futures_ordered_x.rs (L121-148)
```rust
impl<Fut: Future> Stream for FuturesOrderedX<Fut> {
    type Item = Fut::Output;

    fn poll_next(mut self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Option<Self::Item>> {
        let this = &mut *self;

        // Check to see if we've already received the next value
        if let Some(next_output) = this.queued_outputs.peek_mut() {
            if next_output.index == this.next_outgoing_index {
                this.next_outgoing_index += 1;
                return Poll::Ready(Some(PeekMut::pop(next_output).data));
            }
        }

        loop {
            match ready!(this.in_progress_queue.poll_next_unpin(cx)) {
                Some(output) => {
                    if output.index == this.next_outgoing_index {
                        this.next_outgoing_index += 1;
                        return Poll::Ready(Some(output.data));
                    } else {
                        this.queued_outputs.push(output)
                    }
                },
                None => return Poll::Ready(None),
            }
        }
    }
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L88-104)
```rust
    pub fn add_chunk(&mut self, mut chunk: Vec<(K, V)>) -> Result<()> {
        // load progress
        let progress_opt = self.db.get_progress(self.version)?;

        // skip overlaps
        if let Some(progress) = progress_opt {
            let idx = chunk
                .iter()
                .position(|(k, _v)| CryptoHash::hash(k) > progress.key_hash)
                .unwrap_or(chunk.len());
            chunk = chunk.split_off(idx);
        }

        // quit if all skipped
        if chunk.is_empty() {
            return Ok(());
        }
```
