# Audit Report

## Title
Cancellation Safety Violation in Execution Schedule Phase Leading to Potential State Inconsistency

## Summary
The `process()` function in `ExecutionSchedulePhase` performs an irreversible synchronous operation (sending randomness and taking `rand_tx`) followed by a cancellable asynchronous operation (waiting for and setting compute results). If the returned future is cancelled mid-execution, blocks are left in an inconsistent state with randomness delivered but compute results not properly set.

## Finding Description

The vulnerability exists in how the execution schedule phase handles block processing: [1](#0-0) 

The function performs two distinct operations:

1. **Synchronous, irreversible operation**: Lines 64-68 iterate through all blocks, calling `rand_tx.take()` (which permanently removes the sender) and sending randomness to each block.

2. **Asynchronous, cancellable operation**: Lines 70-77 create a future that iterates through blocks, waits for compute results, and sets them on each block.

The critical issue is that these operations are **not atomic**. The randomness sending happens immediately and cannot be undone, while the compute result setting happens later in a future that can be cancelled.

### Cancellation Scenarios

**Scenario 1**: Future cancelled before execution
- `rand_tx` removed from all blocks
- Randomness sent to all blocks  
- Async block never executes
- All blocks retain dummy `StateComputeResult` with `ACCUMULATOR_PLACEHOLDER_HASH`

**Scenario 2**: Future cancelled mid-iteration
- `rand_tx` removed from all blocks
- Randomness sent to all blocks
- Blocks 0..N have `set_compute_result()` called
- Blocks N+1..end never get `set_compute_result()` called
- Partial state update with some blocks having real results, others having dummy results

### Why This Breaks Consensus Safety

When `set_compute_result()` is not called, blocks retain their dummy initialization: [2](#0-1) 

The dummy result uses `ACCUMULATOR_PLACEHOLDER_HASH`: [3](#0-2) 

Blocks use this compute result to generate `BlockInfo` for consensus voting: [4](#0-3) 

If validators process blocks with different compute result states due to timing/cancellation differences, they will generate `BlockInfo` with different state roots, violating **Invariant #1: Deterministic Execution**.

## Impact Explanation

This vulnerability can cause **consensus safety violations**:

1. **State Root Divergence**: Validators that successfully complete execution vs. those that experience cancellation will compute different state roots for the same blocks.

2. **Non-Deterministic Consensus**: The same block could produce different `BlockInfo` on different validators depending on cancellation timing.

3. **Potential Chain Splits**: If validators sign votes or commit decisions based on inconsistent state roots, this could lead to consensus disagreement.

This qualifies as **High Severity** under Aptos bug bounty criteria for "Significant protocol violations" and potentially **Critical Severity** for "Consensus/Safety violations" depending on whether it can lead to actual chain splits.

## Likelihood Explanation

Cancellation can occur during:
- Epoch transitions when `reset()` is called: [5](#0-4) 
- Pipeline task aborts during shutdown
- Channel closures in the pipeline phase: [6](#0-5) 
- Error conditions in the execution wait phase: [7](#0-6) 

While not triggered by direct external attack, this can occur during normal operations (epoch changes, error recovery, shutdown) and create consensus inconsistencies **without malicious intent**. The likelihood is **Medium to High** as these conditions occur regularly in validator operation.

## Recommendation

Make the operation atomic by moving the randomness sending inside the async block, or ensure the async block is not cancellable:

```rust
async fn process(&self, req: ExecutionRequest) -> ExecutionWaitRequest {
    let ExecutionRequest { mut ordered_blocks } = req;

    let block_id = match ordered_blocks.last() {
        Some(block) => block.id(),
        None => {
            return ExecutionWaitRequest {
                block_id: HashValue::zero(),
                fut: Box::pin(async { Err(aptos_executor_types::ExecutorError::EmptyBlocks) }),
            }
        },
    };

    // Move randomness sending INSIDE the async block to ensure atomicity
    let fut = async move {
        // Send randomness first
        for b in &ordered_blocks {
            if let Some(tx) = b.pipeline_tx().lock().as_mut() {
                tx.rand_tx.take().map(|tx| tx.send(b.randomness().cloned()));
            }
        }
        
        // Then wait for and set compute results
        for b in ordered_blocks.iter_mut() {
            let (compute_result, execution_time) = b.wait_for_compute_result().await?;
            b.set_compute_result(compute_result, execution_time);
        }
        Ok(ordered_blocks)
    }
    .boxed();

    ExecutionWaitRequest { block_id, fut }
}
```

Alternatively, use a non-cancellable wrapper or ensure proper cleanup/rollback on cancellation.

## Proof of Concept

```rust
#[tokio::test]
async fn test_execution_schedule_cancellation_safety() {
    use std::sync::Arc;
    use futures::FutureExt;
    
    // Create test blocks
    let blocks = create_test_pipelined_blocks(3);
    let req = ExecutionRequest {
        ordered_blocks: blocks.clone(),
    };
    
    // Process through execution schedule phase
    let phase = ExecutionSchedulePhase::new();
    let wait_req = phase.process(req).await;
    
    // Verify randomness was sent and rand_tx taken for all blocks
    for block in &blocks {
        assert!(block.pipeline_tx().lock().as_ref().unwrap().rand_tx.is_none());
    }
    
    // Simulate cancellation by dropping the future mid-execution
    // In a real scenario, this could happen via task abort
    let mut fut = wait_req.fut;
    
    // Poll once to start iteration, then drop (cancel)
    let waker = futures::task::noop_waker();
    let mut cx = std::task::Context::from_waker(&waker);
    let _ = fut.poll_unpin(&mut cx);
    drop(fut); // Cancellation
    
    // Check blocks state - some may have results set, others won't
    // This demonstrates the inconsistent state
    let mut has_real_result = 0;
    let mut has_dummy_result = 0;
    
    for block in &blocks {
        let result = block.compute_result();
        if result.root_hash() == ACCUMULATOR_PLACEHOLDER_HASH {
            has_dummy_result += 1;
        } else {
            has_real_result += 1;
        }
    }
    
    // If cancellation happened mid-iteration, we have inconsistent state
    assert!(has_dummy_result > 0, "Some blocks should have dummy results after cancellation");
    
    // But randomness was already sent to ALL blocks - this is the inconsistency
    // rand_tx is gone from all blocks but only some have compute results
}
```

## Notes

The vulnerability stems from violating Rust's cancellation safety principles by combining an irreversible synchronous operation (`.take()` on `rand_tx`) with a deferred cancellable operation (the async block). While the current implementation may be mitigated by the reset mechanism cleaning up inconsistent state, this is not guaranteed and represents a fundamental design flaw that could manifest as consensus safety violations under specific timing conditions during epoch transitions or error recovery.

### Citations

**File:** consensus/src/pipeline/execution_schedule_phase.rs (L51-80)
```rust
    async fn process(&self, req: ExecutionRequest) -> ExecutionWaitRequest {
        let ExecutionRequest { mut ordered_blocks } = req;

        let block_id = match ordered_blocks.last() {
            Some(block) => block.id(),
            None => {
                return ExecutionWaitRequest {
                    block_id: HashValue::zero(),
                    fut: Box::pin(async { Err(aptos_executor_types::ExecutorError::EmptyBlocks) }),
                }
            },
        };

        for b in &ordered_blocks {
            if let Some(tx) = b.pipeline_tx().lock().as_mut() {
                tx.rand_tx.take().map(|tx| tx.send(b.randomness().cloned()));
            }
        }

        let fut = async move {
            for b in ordered_blocks.iter_mut() {
                let (compute_result, execution_time) = b.wait_for_compute_result().await?;
                b.set_compute_result(compute_result, execution_time);
            }
            Ok(ordered_blocks)
        }
        .boxed();

        ExecutionWaitRequest { block_id, fut }
    }
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L277-330)
```rust
    pub fn set_compute_result(
        &self,
        state_compute_result: StateComputeResult,
        execution_time: Duration,
    ) {
        let mut to_commit = 0;
        let mut to_retry = 0;
        for txn in state_compute_result.compute_status_for_input_txns() {
            match txn {
                TransactionStatus::Keep(_) => to_commit += 1,
                TransactionStatus::Retry => to_retry += 1,
                _ => {},
            }
        }

        let execution_summary = ExecutionSummary {
            payload_len: self
                .block
                .payload()
                .map_or(0, |payload| payload.len_for_execution()),
            to_commit,
            to_retry,
            execution_time,
            root_hash: state_compute_result.root_hash(),
            gas_used: state_compute_result
                .execution_output
                .block_end_info
                .as_ref()
                .map(|info| info.block_effective_gas_units()),
        };
        *self.state_compute_result.lock() = state_compute_result;

        // We might be retrying execution, so it might have already been set.
        // Because we use this for statistics, it's ok that we drop the newer value.
        if let Some(previous) = self.execution_summary.get() {
            if previous.root_hash == execution_summary.root_hash
                || previous.root_hash == *ACCUMULATOR_PLACEHOLDER_HASH
            {
                warn!(
                    "Skipping re-inserting execution result, from {:?} to {:?}",
                    previous, execution_summary
                );
            } else {
                error!(
                    "Re-inserting execution result with different root hash: from {:?} to {:?}",
                    previous, execution_summary
                );
            }
        } else {
            self.execution_summary
                .set(execution_summary)
                .expect("inserting into empty execution summary");
        }
    }
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L394-397)
```rust
    pub fn new_ordered(block: Block, window: OrderedBlockWindow) -> Self {
        let input_transactions = Vec::new();
        let state_compute_result = StateComputeResult::new_dummy();
        Self::new(block, input_transactions, state_compute_result).with_block_window(window)
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L452-459)
```rust
    pub fn block_info(&self) -> BlockInfo {
        let compute_result = self.compute_result();
        self.block().gen_block_info(
            compute_result.root_hash(),
            compute_result.last_version_or_0(),
            compute_result.epoch_state().clone(),
        )
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L578-596)
```rust
    /// It pops everything in the buffer and if reconfig flag is set, it stops the main loop
    async fn process_reset_request(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        info!("Receive reset");

        match signal {
            ResetSignal::Stop => self.stop = true,
            ResetSignal::TargetRound(round) => {
                self.highest_committed_round = round;
                self.latest_round = round;

                let _ = self.drain_pending_commit_proof_till(round);
            },
        }

        self.reset().await;
        let _ = tx.send(ResetAck::default());
        info!("Reset finishes");
    }
```

**File:** consensus/src/pipeline/pipeline_phase.rs (L88-108)
```rust
    pub async fn start(mut self) {
        // main loop
        while let Some(counted_req) = self.rx.next().await {
            let CountedRequest { req, guard: _guard } = counted_req;
            if self.reset_flag.load(Ordering::SeqCst) {
                continue;
            }
            let response = {
                let _timer = BUFFER_MANAGER_PHASE_PROCESS_SECONDS
                    .with_label_values(&[T::NAME])
                    .start_timer();
                self.processor.process(req).await
            };
            if let Some(tx) = &mut self.maybe_tx {
                if tx.send(response).await.is_err() {
                    debug!("Failed to send response, buffer manager probably dropped");
                    break;
                }
            }
        }
    }
```

**File:** consensus/src/pipeline/execution_wait_phase.rs (L49-56)
```rust
    async fn process(&self, req: ExecutionWaitRequest) -> ExecutionResponse {
        let ExecutionWaitRequest { block_id, fut } = req;

        ExecutionResponse {
            block_id,
            inner: fut.await,
        }
    }
```
