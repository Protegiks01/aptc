# Audit Report

## Title
PrunerWorker Blocks Emergency Validator Shutdown Due to Unbounded Join() in Drop Implementation

## Summary
The `PrunerWorker` blocks validator shutdown indefinitely when dropped because its `Drop` implementation calls `.join()` without timeout while the worker thread executes long-running, uninterruptible pruning operations. During emergency scenarios requiring rapid validator shutdown, this design flaw can delay shutdown by minutes or longer if the pruner has accumulated a large backlog. [1](#0-0) 

## Finding Description
When a validator initiates shutdown, the `StateKvPrunerManager` (and other pruner managers) are dropped as part of the cleanup process. The `PrunerWorker` within these managers has a `Drop` implementation that performs two operations:

1. Sets the `quit_worker` flag to signal the worker thread to exit
2. Calls `.join()` on the worker thread, **blocking until it completes** [1](#0-0) 

The critical issue is that the worker thread only checks the `quit_worker` flag **between** calls to `prune()`, not during the pruning operation itself: [2](#0-1) 

Furthermore, each pruner's `prune()` implementation contains a while loop that processes **all outstanding work** until `progress >= target_version`, potentially processing hundreds or thousands of batches in a single call:

**StateKvPruner:** [3](#0-2) 

**LedgerPruner:** [4](#0-3) 

**StateMerklePruner:** [5](#0-4) 

**Exploitation Scenario:**
1. Validator is running normally with pruning enabled (default configuration)
2. Pruner accumulates a backlog of 1,000,000 versions to prune (realistic during catch-up or after configuration changes)
3. Emergency situation requires rapid shutdown (e.g., security incident, critical infrastructure failure, urgent patch deployment)
4. Administrator initiates shutdown
5. `PrunerWorker::drop()` is called, which sets `quit_worker = true` and calls `.join()`
6. Worker thread is inside `prune()` which has a while loop processing batches
7. With batch_size = 5,000 (default), this requires 200 iterations
8. Each iteration performs database operations including RocksDB writes with sync enabled
9. Shutdown is blocked for potentially several minutes until all 200 batches complete
10. Emergency response is delayed, potentially exacerbating the incident [6](#0-5) 

The default batch size is 5,000 versions per batch: [7](#0-6) 

## Impact Explanation
This vulnerability qualifies as **Medium to High severity** under the Aptos Bug Bounty program:

**High Severity Category ($50,000):** "Validator node slowdowns"
- Emergency shutdown is effectively a critical "slowdown" - the validator cannot respond quickly to security incidents
- Delayed shutdown prevents rapid deployment of security patches
- Could force administrators to use SIGKILL, potentially corrupting database state

**Medium Severity Category ($10,000):** "State inconsistencies requiring intervention"
- Forced termination via SIGKILL may leave pruner progress metadata inconsistent
- Could require manual intervention to recover validator state

**Operational Security Impact:**
- Prevents rapid response to active attacks or security vulnerabilities
- Delays infrastructure maintenance requiring restarts
- Increases mean time to recovery (MTTR) for critical incidents
- May force operators to choose between waiting (extended downtime) or killing (potential corruption)

While this is not a direct consensus violation or funds loss, it significantly degrades the security posture of validator operations by preventing timely emergency response.

## Likelihood Explanation
**Likelihood: Medium to High**

This issue will manifest whenever:
1. A validator has pruning enabled (default configuration in production)
2. The pruner has accumulated any significant backlog (common scenarios below)
3. An emergency shutdown is required

**Common Scenarios Creating Large Backlogs:**
- Validator catching up after downtime or state sync
- Configuration changes to prune window or batch size
- Database growth during high transaction volume periods
- Pruner falling behind due to I/O constraints

**Frequency of Emergency Shutdowns:**
- Security incidents requiring immediate patching
- Infrastructure failures requiring failover
- Critical bugs discovered requiring immediate remediation
- Kubernetes/orchestration platform maintenance
- Cloud provider maintenance windows with short notice

Given that production validators operate 24/7 and emergency situations do occur in blockchain operations, this vulnerability will likely impact real validator operators during critical moments.

## Recommendation
Implement a timeout-based shutdown mechanism with early exit support in the pruning loop:

**Option 1: Add Timeout to Join (Immediate Fix)**
```rust
impl Drop for PrunerWorker {
    fn drop(&mut self) {
        self.inner.stop_pruning();
        
        let timeout = Duration::from_secs(10); // Configurable timeout
        let thread = self.worker_thread.take().unwrap_or_else(|| 
            panic!("Pruner worker ({}) thread must exist.", self.worker_name)
        );
        
        match thread.join_timeout(timeout) {
            Ok(_) => {},
            Err(_) => {
                warn!(
                    worker_name = self.worker_name,
                    "Pruner worker did not shutdown within timeout, forcing termination"
                );
                // Thread will be dropped/detached, which is acceptable during shutdown
            }
        }
    }
}
```

**Option 2: Add Cancellation Checks in Prune Loop (Better Long-term Fix)**
Modify `DBPruner` trait to pass a cancellation token:
```rust
pub trait DBPruner: Send + Sync {
    fn prune(&self, max_versions: usize, should_quit: &AtomicBool) -> Result<Version>;
    // ... other methods
}
```

Then in `StateKvPruner::prune()`:
```rust
fn prune(&self, max_versions: usize, should_quit: &AtomicBool) -> Result<Version> {
    let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_pruner__prune"]);
    let mut progress = self.progress();
    let target_version = self.target_version();

    while progress < target_version && !should_quit.load(Ordering::SeqCst) {
        let current_batch_target_version =
            min(progress + max_versions as Version, target_version);
        
        // ... rest of pruning logic ...
        
        // Check cancellation between batches
        if should_quit.load(Ordering::SeqCst) {
            info!("Pruning cancelled, saving progress at version {}", progress);
            break;
        }
    }

    Ok(progress)
}
```

**Option 3: Hybrid Approach (Recommended)**
Combine both approaches: add cancellation checks in the prune loop AND a timeout in the Drop implementation as a safety net.

## Proof of Concept
```rust
#[cfg(test)]
mod pruner_shutdown_test {
    use super::*;
    use std::sync::{Arc, atomic::{AtomicBool, AtomicU64, Ordering}};
    use std::time::{Duration, Instant};
    use std::thread;

    // Mock pruner that simulates long-running operations
    struct MockSlowPruner {
        target_version: AtomicVersion,
        progress: AtomicVersion,
        iterations_completed: Arc<AtomicU64>,
    }

    impl DBPruner for MockSlowPruner {
        fn name(&self) -> &'static str { "mock_slow_pruner" }

        fn prune(&self, max_versions: usize) -> Result<Version> {
            let mut progress = self.progress();
            let target_version = self.target_version();

            // Simulate processing large backlog
            while progress < target_version {
                let batch_target = std::cmp::min(
                    progress + max_versions as Version, 
                    target_version
                );
                
                // Simulate expensive database operation
                thread::sleep(Duration::from_millis(100));
                
                progress = batch_target;
                self.progress.store(progress, Ordering::SeqCst);
                self.iterations_completed.fetch_add(1, Ordering::SeqCst);
            }

            Ok(target_version)
        }

        fn progress(&self) -> Version {
            self.progress.load(Ordering::SeqCst)
        }

        fn set_target_version(&self, target_version: Version) {
            self.target_version.store(target_version, Ordering::SeqCst);
        }

        fn target_version(&self) -> Version {
            self.target_version.load(Ordering::SeqCst)
        }

        fn record_progress(&self, progress: Version) {
            self.progress.store(progress, Ordering::SeqCst);
        }
    }

    #[test]
    fn test_pruner_blocks_shutdown_with_large_backlog() {
        let iterations_completed = Arc::new(AtomicU64::new(0));
        let pruner = Arc::new(MockSlowPruner {
            target_version: AtomicVersion::new(0),
            progress: AtomicVersion::new(0),
            iterations_completed: Arc::clone(&iterations_completed),
        });

        // Create large backlog: 100,000 versions with batch size 5,000 = 20 iterations
        pruner.set_target_version(100_000);
        
        let worker = PrunerWorker::new(pruner, 5_000, "test");
        
        // Give worker time to start processing
        thread::sleep(Duration::from_millis(50));

        // Now try to shutdown - this should block
        let start = Instant::now();
        drop(worker); // Triggers Drop implementation with blocking join()
        let elapsed = start.elapsed();

        // With 20 iterations Ã— 100ms per iteration = 2000ms minimum
        // Demonstrates that shutdown is blocked for extended period
        assert!(
            elapsed >= Duration::from_millis(1500),
            "Shutdown completed too quickly: {:?}, iterations: {}", 
            elapsed,
            iterations_completed.load(Ordering::SeqCst)
        );
        
        println!(
            "Shutdown blocked for {:?}, completed {} iterations",
            elapsed,
            iterations_completed.load(Ordering::SeqCst)
        );
    }

    #[test]
    fn test_emergency_shutdown_delay() {
        // Simulate realistic production scenario
        let iterations_completed = Arc::new(AtomicU64::new(0));
        let pruner = Arc::new(MockSlowPruner {
            target_version: AtomicVersion::new(0),
            progress: AtomicVersion::new(0),
            iterations_completed: Arc::clone(&iterations_completed),
        });

        // Realistic backlog: 1M versions, 5K batch = 200 iterations
        // With 100ms per iteration = 20 seconds blocked shutdown
        pruner.set_target_version(1_000_000);
        
        let worker = PrunerWorker::new(pruner, 5_000, "test");
        thread::sleep(Duration::from_millis(50));

        let start = Instant::now();
        drop(worker);
        let elapsed = start.elapsed();

        // This demonstrates unacceptable shutdown delay in emergency scenarios
        println!(
            "Emergency shutdown delayed by {:?} for {} version backlog",
            elapsed,
            1_000_000
        );
        
        // In production with real database I/O, this would be much longer
        assert!(elapsed.as_secs() >= 15, "Shutdown delay: {:?}", elapsed);
    }
}
```

**Notes**
This vulnerability affects all three pruners in the system (`StateKvPruner`, `LedgerPruner`, `StateMerklePruner`), all of which share the same `PrunerWorker` infrastructure and have while loops that process complete backlogs without interruption. The issue is systemic to the pruner architecture, not specific to any single pruner implementation.

The vulnerability becomes more severe with:
- Larger prune windows (more versions to process)
- Slower storage I/O (increases per-batch duration)
- Sharded configurations (more parallel work per batch)
- High transaction throughput (faster backlog accumulation)

Production validators should implement timeout-based shutdown mechanisms and add cancellation support to pruning loops to ensure responsive emergency shutdowns.

### Citations

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L53-69)
```rust
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
    }
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L105-118)
```rust
impl Drop for PrunerWorker {
    fn drop(&mut self) {
        self.inner.stop_pruning();
        self.worker_thread
            .take()
            .unwrap_or_else(|| panic!("Pruner worker ({}) thread must exist.", self.worker_name))
            .join()
            .unwrap_or_else(|e| {
                panic!(
                    "Pruner worker ({}) thread should join peacefully: {e:?}",
                    self.worker_name
                )
            });
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L49-86)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_pruner__prune"]);

        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning state kv data."
            );
            self.metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state kv shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning state kv data is done.");
        }

        Ok(target_version)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L62-92)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning ledger data."
            );
            self.ledger_metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning ledger data is done.");
        }

        Ok(target_version)
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L59-95)
```rust
    fn prune(&self, batch_size: usize) -> Result<Version> {
        // TODO(grao): Consider separate pruner metrics, and have a label for pruner name.
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_merkle_pruner__prune"]);
        let mut progress = self.progress();
        let target_version = self.target_version();

        if progress >= target_version {
            return Ok(progress);
        }

        info!(
            name = S::name(),
            current_progress = progress,
            target_version = target_version,
            "Start pruning..."
        );

        while progress < target_version {
            if let Some(target_version_for_this_round) = self
                .metadata_pruner
                .maybe_prune_single_version(progress, target_version)?
            {
                self.prune_shards(progress, target_version_for_this_round, batch_size)?;
                progress = target_version_for_this_round;
                info!(name = S::name(), progress = progress);
                self.record_progress(target_version_for_this_round);
            } else {
                self.prune_shards(progress, target_version, batch_size)?;
                self.record_progress(target_version);
                break;
            }
        }

        info!(name = S::name(), progress = target_version, "Done pruning.");

        Ok(target_version)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_pruner_manager.rs (L110-126)
```rust
    fn init_pruner(
        state_kv_db: Arc<StateKvDb>,
        state_kv_pruner_config: LedgerPrunerConfig,
    ) -> PrunerWorker {
        let pruner =
            Arc::new(StateKvPruner::new(state_kv_db).expect("Failed to create state kv pruner."));

        PRUNER_WINDOW
            .with_label_values(&["state_kv_pruner"])
            .set(state_kv_pruner_config.prune_window as i64);

        PRUNER_BATCH_SIZE
            .with_label_values(&["state_kv_pruner"])
            .set(state_kv_pruner_config.batch_size as i64);

        PrunerWorker::new(pruner, state_kv_pruner_config.batch_size, "state_kv")
    }
```

**File:** config/src/config/storage_config.rs (L387-396)
```rust
impl Default for LedgerPrunerConfig {
    fn default() -> Self {
        LedgerPrunerConfig {
            enable: true,
            prune_window: 90_000_000,
            batch_size: 5_000,
            user_pruning_window_offset: 200_000,
        }
    }
}
```
