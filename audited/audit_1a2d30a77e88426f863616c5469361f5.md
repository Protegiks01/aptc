# Audit Report

## Title
Hot State Commit Resource Exhaustion via Large State Delta Processing

## Summary
The `Committer::commit()` function in the hot state subsystem performs an O(n) iteration over all modified state keys in a delta, where n can reach approximately 100,000 unique keys. Under sustained high transaction throughput with many unique state modifications, this can cause the hot state commit thread to fall behind, filling commit queues and eventually blocking transaction commits, leading to validator performance degradation. [1](#0-0) 

## Finding Description

The hot state management system maintains an in-memory cache of frequently accessed state items. When state snapshots are committed, the `Committer::commit()` function iterates over all state keys that changed between the previous committed state and the new state: [2](#0-1) 

**The Attack Vector:**

1. **Delta Size Control**: The delta size is determined by unique state keys modified between commits. Commits are triggered when `estimated_items >= 100,000` or `buffered_versions >= 100,000`: [3](#0-2) [4](#0-3) 

2. **Attacker Capability**: An attacker can submit transactions that modify many unique state keys (creating resources, objects, or modifying different storage locations). The number of modifications is limited only by gas costs. The delta accumulates all unique state key changes since the last snapshot.

3. **Backpressure Propagation**: The hot state commit queue has a limited capacity: [5](#0-4) 

When this queue fills, `enqueue_commit()` blocks: [6](#0-5) 

This blocking propagates through the commit pipeline:
- Blocks `PersistedState::set()`: [7](#0-6) 
- Blocks `StateMerkleBatchCommitter`: [8](#0-7) 
- Blocks `BufferedState::enqueue_commit()`: [9](#0-8) 
- Ultimately blocks `pre_commit_ledger()`: [10](#0-9) 

**Exploitation Scenario:**
1. Attacker submits numerous transactions, each modifying different state keys (e.g., creating unique resources or objects in their account)
2. Between snapshots, ~100,000 unique state keys are modified
3. Each hot state commit must iterate through ~100,000 DashMap operations
4. If commit processing (including delta creation, iteration, and DashMap operations) takes 50-100ms per snapshot
5. Under high throughput (snapshots created every 20-50ms), the commit thread cannot keep pace
6. The 10-item commit queue fills in 500-1000ms of sustained load
7. Further commits block, preventing transaction processing and degrading validator performance

## Impact Explanation

This qualifies as **Medium Severity** under the Aptos bug bounty criteria, potentially approaching High Severity depending on the demonstrated impact:

- **High Severity Category**: "Validator node slowdowns" - If the attack successfully causes measurable performance degradation in transaction commit latency, this would be High Severity.
- **Medium Severity Category**: "State inconsistencies requiring intervention" - The blocking of commit pipelines could require operator intervention to restore normal operation.

The vulnerability affects the **Resource Limits** invariant: the system should handle operations within reasonable performance bounds, but the unbounded delta iteration can cause commit pipeline stalls that affect validator availability.

While this does not directly cause loss of funds or consensus violations, it impacts validator performance and network throughput, which affects the system's ability to process transactions reliably.

## Likelihood Explanation

**Likelihood: Medium**

**Attacker Requirements:**
- Sufficient funds to pay gas for many transactions (each transaction costs gas)
- Ability to sustain high transaction throughput
- Knowledge to craft transactions that modify unique state keys

**Economic Constraints:**
- Gas costs scale with the number of transactions
- To modify 100,000 unique state keys, an attacker might need thousands of transactions
- The attack cost is non-trivial but feasible for a motivated attacker

**System Conditions Required:**
- High transaction throughput where snapshots are created frequently
- Sustained load where multiple large deltas are in the commit pipeline
- The commit thread must be slower than snapshot creation rate

**Mitigating Factors:**
- The 10-item queue provides buffering for temporary spikes
- Most legitimate workloads don't modify 100,000 unique state keys per snapshot
- The DashMap operations are relatively fast, so commit time may be acceptable under normal conditions

## Recommendation

**Recommended Mitigations:**

1. **Implement Batch Size Limits**: Cap the delta size processed in a single commit iteration to prevent excessive blocking time:

```rust
const MAX_DELTA_SIZE_PER_COMMIT: usize = 10_000;

fn commit(&mut self, to_commit: &State) {
    let delta = to_commit.make_delta(&self.committed.lock());
    
    // Process delta in batches to avoid long blocking periods
    for shard_id in 0..NUM_STATE_SHARDS {
        let mut processed = 0;
        for (key, slot) in delta.shards[shard_id].iter() {
            // Process item...
            
            processed += 1;
            if processed >= MAX_DELTA_SIZE_PER_COMMIT {
                // Yield and allow other operations, resume later
                break;
            }
        }
    }
}
```

2. **Increase Queue Capacity**: Increase `MAX_HOT_STATE_COMMIT_BACKLOG` to provide more buffering during high load periods.

3. **Add Monitoring Alerts**: Implement metrics tracking hot state commit duration and queue depth to alert operators when commits are falling behind.

4. **Rate Limiting**: Consider per-account rate limits on unique state key modifications to prevent a single attacker from flooding the system.

5. **Parallel Processing**: Parallelize the shard iteration to reduce overall commit time:

```rust
fn commit(&mut self, to_commit: &State) {
    let delta = to_commit.make_delta(&self.committed.lock());
    
    // Process shards in parallel
    (0..NUM_STATE_SHARDS).into_par_iter().for_each(|shard_id| {
        for (key, slot) in delta.shards[shard_id].iter() {
            // Thread-safe operations on each shard
        }
    });
}
```

## Proof of Concept

The following Rust test demonstrates the vulnerability concept:

```rust
#[test]
fn test_hot_state_commit_slowdown_attack() {
    // Setup: Create hot state with large delta
    let config = HotStateConfig {
        max_items_per_shard: 250_000,
        refresh_interval_versions: 100_000,
        ..Default::default()
    };
    
    let state = State::new_empty(config);
    let hot_state = HotState::new(state, config);
    
    // Simulate attacker: Create transactions modifying many unique state keys
    let num_unique_keys = 100_000;
    let mut state_updates = HashMap::new();
    
    for i in 0..num_unique_keys {
        let key = StateKey::raw(format!("attack_key_{}", i).as_bytes());
        let value = StateValue::new_legacy(vec![0u8; 100]);
        state_updates.insert(key, value);
    }
    
    // Commit multiple large deltas rapidly to fill the queue
    let start = Instant::now();
    for _ in 0..15 {  // More than MAX_HOT_STATE_COMMIT_BACKLOG
        let new_state = create_state_with_updates(&state_updates);
        hot_state.enqueue_commit(new_state);
    }
    let duration = start.elapsed();
    
    // Assertion: The commit should block when queue is full,
    // demonstrating backpressure affecting transaction commits
    assert!(duration.as_millis() > 100, 
        "Commits should block when queue fills, indicating validator slowdown");
}
```

**Note**: A complete PoC would require integration testing with the full commit pipeline to measure actual validator performance degradation under load.

## Notes

- This vulnerability specifically targets the hot state commit subsystem, which is separate from the cold state Merkle tree commit process
- The hot state is an optimization for frequently accessed state items; the core state integrity is maintained by the Jellyfish Merkle Tree regardless of hot state commit delays
- The vulnerability manifests only under sustained high load with adversarial transaction patterns
- Natural transaction patterns typically do not modify tens of thousands of unique state keys per snapshot, making this primarily a targeted DoS vector
- The economic cost (gas fees) provides some natural rate limiting, but a well-funded attacker could sustain this attack

### Citations

**File:** storage/aptosdb/src/state_store/hot_state.rs (L27-27)
```rust
const MAX_HOT_STATE_COMMIT_BACKLOG: usize = 10;
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L138-144)
```rust
    pub fn enqueue_commit(&self, to_commit: State) {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["hot_state_enqueue_commit"]);

        self.commit_tx
            .send(to_commit)
            .expect("Failed to queue for hot state commit.")
    }
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L235-275)
```rust
    fn commit(&mut self, to_commit: &State) {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["hot_state_commit"]);

        let mut n_insert = 0;
        let mut n_update = 0;
        let mut n_evict = 0;

        let delta = to_commit.make_delta(&self.committed.lock());
        for shard_id in 0..NUM_STATE_SHARDS {
            for (key, slot) in delta.shards[shard_id].iter() {
                if slot.is_hot() {
                    let key_size = key.size();
                    self.total_key_bytes += key_size;
                    self.total_value_bytes += slot.size();
                    if let Some(old_slot) = self.base.shards[shard_id].insert(key, slot) {
                        self.total_key_bytes -= key_size;
                        self.total_value_bytes -= old_slot.size();
                        n_update += 1;
                    } else {
                        n_insert += 1;
                    }
                } else if let Some((key, old_slot)) = self.base.shards[shard_id].remove(&key) {
                    self.total_key_bytes -= key.size();
                    self.total_value_bytes -= old_slot.size();
                    n_evict += 1;
                }
            }
            self.heads[shard_id] = to_commit.latest_hot_key(shard_id);
            self.tails[shard_id] = to_commit.oldest_hot_key(shard_id);
            assert_eq!(
                self.base.shards[shard_id].len(),
                to_commit.num_hot_items(shard_id)
            );

            debug_assert!(self.validate_lru(shard_id).is_ok());
        }

        COUNTER.inc_with_by(&["hot_state_insert"], n_insert);
        COUNTER.inc_with_by(&["hot_state_update"], n_update);
        COUNTER.inc_with_by(&["hot_state_evict"], n_evict);
    }
```

**File:** config/src/config/storage_config.rs (L27-28)
```rust
pub const BUFFERED_STATE_TARGET_ITEMS: usize = 100_000;
pub const BUFFERED_STATE_TARGET_ITEMS_FOR_TEST: usize = 10;
```

**File:** storage/aptosdb/src/state_store/buffered_state.rs (L99-108)
```rust
    fn maybe_commit(&mut self, checkpoint: Option<StateWithSummary>, sync_commit: bool) {
        if let Some(checkpoint) = checkpoint {
            if !checkpoint.is_the_same(&self.last_snapshot)
                && (sync_commit
                    || self.estimated_items >= self.target_items
                    || self.buffered_versions() >= TARGET_SNAPSHOT_INTERVAL_IN_VERSION)
            {
                self.enqueue_commit(checkpoint);
            }
        }
```

**File:** storage/aptosdb/src/state_store/buffered_state.rs (L123-134)
```rust
    fn enqueue_commit(&mut self, checkpoint: StateWithSummary) {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["buffered_state___enqueue_commit"]);

        self.state_commit_sender
            .send(CommitMessage::Data(checkpoint.clone()))
            .unwrap();
        // n.b. if the latest state is not a (the latest) checkpoint, the items between them are
        // not counted towards the next commit. If this becomes a concern we can count the items
        // instead of putting it 0 here.
        self.estimated_items = 0;
        self.last_snapshot = checkpoint;
    }
```

**File:** storage/aptosdb/src/state_store/persisted_state.rs (L50-62)
```rust
    pub fn set(&self, persisted: StateWithSummary) {
        let (state, summary) = persisted.into_inner();

        // n.b. Summary must be updated before committing the hot state, otherwise in the execution
        // pipeline we risk having a state generated based on a persisted version (v2) that's newer
        // than that of the summary (v1). That causes issue down the line where we commit the diffs
        // between a later snapshot (v3) and a persisted snapshot (v1) to the JMT, at which point
        // we will not be able to calculate the difference (v1 - v3) because the state links only
        // to as far as v2 (code will panic)
        *self.summary.lock() = summary;

        self.hot_state.enqueue_commit(state);
    }
```

**File:** storage/aptosdb/src/state_store/state_merkle_batch_committer.rs (L106-106)
```rust
                    self.persisted_state.set(snapshot);
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L68-72)
```rust
            self.state_store.buffered_state().lock().update(
                chunk.result_ledger_state_with_summary(),
                chunk.estimated_total_state_updates(),
                sync_commit || chunk.is_reconfig,
            )?;
```
