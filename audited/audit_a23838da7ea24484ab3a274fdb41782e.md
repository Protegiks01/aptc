# Audit Report

## Title
Mainnet/Testnet RLIMIT_NOFILE Bypass via Explicit Low Configuration Values

## Summary
Operators on mainnet/testnet can bypass the intended `ensure_rlimit_nofile` safeguard of 999,999 file descriptors by explicitly setting a non-null but inadequate value (e.g., 100) in their node configuration. The `ConfigOptimizer::optimize()` function only sets the safe default when the value is null, and the `ConfigSanitizer` performs no validation on this field, allowing dangerously low file descriptor limits that can cause node crashes and consensus disruptions.

## Finding Description

The Aptos node configuration system implements a safety mechanism to ensure mainnet and testnet nodes run with adequate file descriptor limits (RLIMIT_NOFILE). However, this mechanism has a critical flaw in its implementation.

**The Vulnerability Chain:**

1. **Default Configuration**: By default, `ensure_rlimit_nofile` is set to 0 (no enforcement). [1](#0-0) 

2. **Optimizer Logic Flaw**: The `ConfigOptimizer::optimize()` function attempts to set a safe value of 999,999 for mainnet/testnet, but only when the configuration value is explicitly null: [2](#0-1) 

   The condition `config_yaml["ensure_rlimit_nofile"].is_null()` returns `false` if ANY value is explicitly set, including inadequate values like 1, 10, or 100.

3. **No Sanitizer Validation**: The `ConfigSanitizer::sanitize()` implementation for `StorageConfig` validates pruning windows and path configurations but completely ignores the `ensure_rlimit_nofile` field: [3](#0-2) 

4. **Blind Enforcement**: At node startup, the `ensure_max_open_files_limit()` function blindly attempts to set whatever value is configured, with no validation of adequacy: [4](#0-3) [5](#0-4) 

**Attack Scenario:**

A mainnet/testnet operator configures their node with:
```yaml
storage:
  ensure_rlimit_nofile: 100  # Inadequate but bypasses the optimizer
  rocksdb_configs:
    enable_storage_sharding: true
```

The optimizer sees the value is not null and skips setting it to 999,999. The node starts with only 100 file descriptors, which is grossly inadequate for RocksDB operations requiring multiple SST files, WAL files, network connections, and other file handles.

## Impact Explanation

**High Severity** - This vulnerability meets the Aptos bug bounty criteria for High severity issues:

1. **Validator Node Slowdowns**: Nodes operating with inadequate file descriptor limits will experience:
   - RocksDB failures when unable to open required SST files
   - Failed database operations causing transaction processing delays
   - State sync failures due to inability to open temporary files

2. **Node Crashes**: When the file descriptor limit is exhausted:
   - RocksDB will fail to open new files, causing panic conditions
   - Network layer cannot accept new connections
   - The node may become unresponsive or crash entirely

3. **Consensus Impact**: If multiple validators run with this misconfiguration:
   - Reduced validator availability affects consensus liveness
   - Intermittent failures during high load can cause round timeouts
   - State sync failures can lead to validators falling behind

4. **Protocol Violations**: Resource exhaustion at the storage layer violates the "Resource Limits" invariant - all operations must respect system resource constraints.

The test suite confirms this is intended behavior for mainnet/testnet but lacks validation: [6](#0-5) 

## Likelihood Explanation

**High Likelihood** - This vulnerability is highly likely to occur because:

1. **Ease of Exploitation**: Operators simply need to add one line to their configuration file. No technical sophistication required.

2. **Legitimate Misunderstanding**: Operators may genuinely believe they are "optimizing" their configuration by setting lower values to reduce resource usage, unaware of the dangers.

3. **No Warning or Error**: The system provides no indication that an inadequate value has been set. The node starts successfully and only fails under load when file descriptors are exhausted.

4. **Testnet Evidence**: The existence of the `assert_rlimit_nofile` flag specifically for testnet suggests the developers are aware of this risk: [7](#0-6) 

5. **Production Impact**: Mainnet nodes lack the `assert_rlimit_nofile` enforcement, meaning they can silently run with inadequate limits until failure occurs.

## Recommendation

Implement mandatory validation in the `ConfigSanitizer` to enforce minimum RLIMIT_NOFILE values for mainnet/testnet:

```rust
impl ConfigSanitizer for StorageConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let config = &node_config.storage;
        
        // NEW: Validate ensure_rlimit_nofile for mainnet/testnet
        if let Some(chain_id) = chain_id {
            if (chain_id.is_testnet() || chain_id.is_mainnet()) 
                && config.ensure_rlimit_nofile > 0 
                && config.ensure_rlimit_nofile < 100_000 {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    format!(
                        "ensure_rlimit_nofile is set to {} which is too low for mainnet/testnet. \
                        Minimum required: 100,000. Recommended: 999,999. \
                        Set to 0 to skip validation or use the recommended value.",
                        config.ensure_rlimit_nofile
                    ),
                ));
            }
        }
        
        // ... existing validation code ...
```

Additionally, improve the optimizer to warn when overriding user-provided inadequate values:

```rust
if (chain_id.is_testnet() || chain_id.is_mainnet()) {
    if config_yaml["ensure_rlimit_nofile"].is_null() {
        config.ensure_rlimit_nofile = 999_999;
        modified_config = true;
    } else if config.ensure_rlimit_nofile > 0 && config.ensure_rlimit_nofile < 100_000 {
        warn!(
            "Configured ensure_rlimit_nofile ({}) is too low for mainnet/testnet. \
            Recommended minimum: 999,999",
            config.ensure_rlimit_nofile
        );
    }
}
```

## Proof of Concept

Create a malicious configuration file `bad_mainnet_config.yaml`:

```yaml
base:
  role: "validator"
  data_dir: "/opt/aptos/data"
  waypoint:
    from_config: "0:0000000000000000000000000000000000000000000000000000000000000000"

storage:
  # BYPASS: Explicitly set inadequate value
  ensure_rlimit_nofile: 100
  rocksdb_configs:
    enable_storage_sharding: true
    
execution:
  genesis_file_location: ""
```

**Reproduction Steps:**

1. Create the malicious config file with `ensure_rlimit_nofile: 100`
2. Start the node: `aptos-node -f bad_mainnet_config.yaml`
3. Observe that:
   - The optimizer skips setting the value to 999,999 (line 655 condition fails)
   - No sanitizer error occurs (no validation exists)
   - The node attempts to set RLIMIT_NOFILE to 100
   - Under load, RocksDB operations fail with "Too many open files" errors

**Expected Behavior:**
- The sanitizer should reject the configuration with an error
- Or the optimizer should override inadequate explicit values with a warning

**Actual Behavior:**
- Configuration is accepted without validation
- Node runs with dangerously low file descriptor limit
- Crashes occur under normal operational load

**Validation Test:**

The existing test only validates the case where the value is NOT explicitly set: [8](#0-7) 

A new test should verify rejection of inadequate explicit values:

```rust
#[test]
fn test_reject_inadequate_rlimit_nofile() {
    let mut node_config = NodeConfig::default();
    
    // Simulate explicit inadequate config
    let yaml = serde_yaml::from_str(
        r#"
        storage:
          ensure_rlimit_nofile: 100
          rocksdb_configs:
            enable_storage_sharding: true
        "#,
    ).unwrap();
    
    // This should either override with warning or fail validation
    let result = StorageConfig::optimize(
        &mut node_config,
        &yaml,
        NodeType::Validator,
        Some(ChainId::mainnet()),
    );
    
    // Current: Bypass succeeds (vulnerability)
    // Expected: Either set to 999_999 with warning, or sanitizer fails
    assert!(node_config.storage.ensure_rlimit_nofile >= 100_000);
}
```

## Notes

This vulnerability specifically targets the production node configuration safety mechanisms. While the enforcement function itself works correctly, the lack of validation at the configuration layer allows operators to inadvertently (or deliberately) run nodes with resource limits that violate operational requirements. The fix requires adding defensive validation at the sanitizer level to catch this misconfiguration before the node starts.

### Citations

**File:** config/src/config/storage_config.rs (L452-452)
```rust
            ensure_rlimit_nofile: 0,
```

**File:** config/src/config/storage_config.rs (L654-659)
```rust
            if (chain_id.is_testnet() || chain_id.is_mainnet())
                && config_yaml["ensure_rlimit_nofile"].is_null()
            {
                config.ensure_rlimit_nofile = 999_999;
                modified_config = true;
            }
```

**File:** config/src/config/storage_config.rs (L660-663)
```rust
            if chain_id.is_testnet() && config_yaml["assert_rlimit_nofile"].is_null() {
                config.assert_rlimit_nofile = true;
                modified_config = true;
            }
```

**File:** config/src/config/storage_config.rs (L682-798)
```rust
impl ConfigSanitizer for StorageConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let config = &node_config.storage;

        let ledger_prune_window = config
            .storage_pruner_config
            .ledger_pruner_config
            .prune_window;
        let state_merkle_prune_window = config
            .storage_pruner_config
            .state_merkle_pruner_config
            .prune_window;
        let epoch_snapshot_prune_window = config
            .storage_pruner_config
            .epoch_snapshot_pruner_config
            .prune_window;
        let user_pruning_window_offset = config
            .storage_pruner_config
            .ledger_pruner_config
            .user_pruning_window_offset;

        if ledger_prune_window < 50_000_000 {
            warn!("Ledger prune_window is too small, harming network data availability.");
        }
        if state_merkle_prune_window < 100_000 {
            warn!("State Merkle prune_window is too small, node might stop functioning.");
        }
        if epoch_snapshot_prune_window < 50_000_000 {
            warn!("Epoch snapshot prune_window is too small, harming network data availability.");
        }
        if user_pruning_window_offset > 1_000_000 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "user_pruning_window_offset too large, so big a buffer is unlikely necessary. Set something < 1 million.".to_string(),
            ));
        }
        if user_pruning_window_offset > ledger_prune_window {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "user_pruning_window_offset is larger than the ledger prune window, the API will refuse to return any data.".to_string(),
            ));
        }

        if let Some(db_path_overrides) = config.db_path_overrides.as_ref() {
            if !config.rocksdb_configs.enable_storage_sharding {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "db_path_overrides is allowed only if sharding is enabled.".to_string(),
                ));
            }

            if let Some(ledger_db_path) = db_path_overrides.ledger_db_path.as_ref() {
                if !ledger_db_path.is_absolute() {
                    return Err(Error::ConfigSanitizerFailed(
                        sanitizer_name,
                        format!(
                            "Path {ledger_db_path:?} in db_path_overrides is not an absolute path."
                        ),
                    ));
                }
            }

            if let Some(state_kv_db_path) = db_path_overrides.state_kv_db_path.as_ref() {
                if let Some(metadata_path) = state_kv_db_path.metadata_path.as_ref() {
                    if !metadata_path.is_absolute() {
                        return Err(Error::ConfigSanitizerFailed(
                            sanitizer_name,
                            format!("Path {metadata_path:?} in db_path_overrides is not an absolute path."),
                        ));
                    }
                }

                if let Err(e) = state_kv_db_path.get_shard_paths() {
                    return Err(Error::ConfigSanitizerFailed(sanitizer_name, e.to_string()));
                }
            }

            if let Some(state_merkle_db_path) = db_path_overrides.state_merkle_db_path.as_ref() {
                if let Some(metadata_path) = state_merkle_db_path.metadata_path.as_ref() {
                    if !metadata_path.is_absolute() {
                        return Err(Error::ConfigSanitizerFailed(
                            sanitizer_name,
                            format!("Path {metadata_path:?} in db_path_overrides is not an absolute path."),
                        ));
                    }
                }

                if let Err(e) = state_merkle_db_path.get_shard_paths() {
                    return Err(Error::ConfigSanitizerFailed(sanitizer_name, e.to_string()));
                }
            }

            if let Some(hot_state_merkle_db_path) =
                db_path_overrides.hot_state_merkle_db_path.as_ref()
            {
                if let Some(metadata_path) = hot_state_merkle_db_path.metadata_path.as_ref() {
                    if !metadata_path.is_absolute() {
                        return Err(Error::ConfigSanitizerFailed(
                            sanitizer_name,
                            format!("Path {metadata_path:?} in db_path_overrides is not an absolute path."),
                        ));
                    }
                }

                if let Err(e) = hot_state_merkle_db_path.get_shard_paths() {
                    return Err(Error::ConfigSanitizerFailed(sanitizer_name, e.to_string()));
                }
            }
        }

        Ok(())
    }
```

**File:** config/src/config/storage_config.rs (L883-919)
```rust
    fn test_optimize_ensure_rlimit_nofile() {
        let mut node_config = NodeConfig::default();
        assert_eq!(node_config.storage.ensure_rlimit_nofile, 0);
        assert!(!node_config.storage.assert_rlimit_nofile);

        let yaml = serde_yaml::from_str(
            r#"
            storage:
              rocksdb_configs:
                enable_storage_sharding: true
            "#,
        )
        .unwrap();
        let modified_config = StorageConfig::optimize(
            &mut node_config,
            &yaml,
            NodeType::Validator,
            Some(ChainId::mainnet()),
        )
        .unwrap();
        assert!(modified_config);

        assert_eq!(node_config.storage.ensure_rlimit_nofile, 999_999);
        assert!(!node_config.storage.assert_rlimit_nofile);

        let modified_config = StorageConfig::optimize(
            &mut node_config,
            &yaml,
            NodeType::Validator,
            Some(ChainId::testnet()),
        )
        .unwrap();
        assert!(modified_config);

        assert_eq!(node_config.storage.ensure_rlimit_nofile, 999_999);
        assert!(node_config.storage.assert_rlimit_nofile);
    }
```

**File:** aptos-node/src/lib.rs (L246-249)
```rust
    ensure_max_open_files_limit(
        config.storage.ensure_rlimit_nofile,
        config.storage.assert_rlimit_nofile,
    );
```

**File:** aptos-node/src/utils.rs (L81-136)
```rust
pub fn ensure_max_open_files_limit(required: u64, assert_success: bool) {
    if required == 0 {
        return;
    }

    // Only works on Unix environments
    #[cfg(unix)]
    {
        if !rlimit::Resource::NOFILE.is_supported() {
            warn!(
                required = required,
                "rlimit setting not supported on this platform. Won't ensure."
            );
            return;
        }

        let (soft, mut hard) = match rlimit::Resource::NOFILE.get() {
            Ok((soft, hard)) => (soft, hard),
            Err(err) => {
                warn!(
                    error = ?err,
                    required = required,
                    "Failed getting RLIMIT_NOFILE. Won't ensure."
                );
                return;
            },
        };

        if soft >= required {
            return;
        }

        if required > hard {
            warn!(
                hard_limit = hard,
                required = required,
                "System RLIMIT_NOFILE hard limit too small."
            );
            // Not panicking right away -- user can be root
            hard = required;
        }

        rlimit::Resource::NOFILE
            .set(required, hard)
            .unwrap_or_else(|err| {
                let msg = format!("RLIMIT_NOFILE soft limit is {soft}, configured requirement is {required}, and \
                    failed to raise to it. Please make sure that `limit -n` shows a number larger than \
                    {required} before starting the node. Error: {err}.");
                if assert_success {
                    panic!("{}", msg)
                } else {
                    error!("{}", msg)
                }
            });
    }
}
```
