# Audit Report

## Title
Infinite Loop DoS in Indexer gRPC Data Service v2 Background Fetch Task

## Summary
The `fetch_transactions()` method in the indexer-grpc-data-service-v2 contains an unbounded retry loop with no timeout, maximum retry count, or backoff delay. A malicious or faulty gRPC manager service can cause the indexer's background data fetch task to hang indefinitely, resulting in a denial of service for the indexer API.

## Finding Description

The vulnerability exists in the `DataClient::fetch_transactions()` method which is called by the background task responsible for continuously fetching new blockchain data into the indexer's in-memory cache. [1](#0-0) 

The infinite loop exits only under two conditions:
1. Server returns an empty transactions vector
2. Server returns transactions starting at the exact requested version

The loop continues indefinitely when:
1. The gRPC call returns an error (`Err`)
2. The server returns transactions but with a mismatched starting version

There is no:
- Timeout on individual gRPC calls
- Maximum retry limit
- Exponential backoff or delay between retries
- Circuit breaker pattern

This method is invoked by the fetch manager's background task: [2](#0-1) [3](#0-2) 

The gRPC client is created without explicit timeout configuration: [4](#0-3) 

**Attack Scenario:**

1. Attacker compromises or controls a gRPC manager service endpoint
2. The malicious service repeatedly returns either:
   - Error responses for every `get_transactions` request, OR
   - Valid transactions with incorrect starting versions (off-by-one attacks)
3. The `fetch_transactions()` loop spins indefinitely with no delay
4. The background task hangs, preventing new data from entering the cache
5. Clients requesting data beyond cached versions receive errors
6. The indexer service becomes effectively unavailable for new data

## Impact Explanation

While this vulnerability causes a **total loss of liveness for the indexer service**, it does NOT affect:
- Blockchain consensus or validator operations
- Transaction submission or execution
- State commitment or storage
- On-chain governance or staking

Per the Aptos bug bounty severity criteria, this qualifies as **High Severity** under "API crashes" rather than Critical severity. The "Total loss of liveness/network availability" criterion under Critical explicitly refers to the blockchain network itself, not auxiliary data services like the indexer.

The indexer-grpc-data-service is an ecosystem component for serving historical blockchain data to external clients. Its failure does not impact the core blockchain protocol or validator operations.

**Affected Systems:**
- Indexer API becomes unavailable for recent data
- Applications relying on the indexer API experience service degradation
- No impact on blockchain validators, consensus, or transaction processing

## Likelihood Explanation

**High Likelihood** - The vulnerability can be triggered through:

1. **Malicious Operator:** An attacker operating a malicious gRPC manager service
2. **Service Compromise:** Compromise of an existing gRPC manager endpoint
3. **Software Bugs:** Even non-malicious bugs in the gRPC manager could trigger this condition
4. **Network Issues:** Transient network failures combined with server bugs could cause repeated errors

The lack of any defensive mechanisms (timeouts, max retries, circuit breakers) makes this trivial to exploit once an attacker controls or influences a gRPC manager endpoint.

## Recommendation

Implement multiple defensive layers:

**1. Add timeout to individual gRPC calls:**
```rust
pub(super) async fn fetch_transactions(&self, starting_version: u64) -> Vec<Transaction> {
    const MAX_RETRIES: usize = 5;
    const RETRY_DELAY_MS: u64 = 1000;
    const REQUEST_TIMEOUT_SECS: u64 = 30;
    
    let request = GetTransactionsRequest {
        starting_version: Some(starting_version),
        transactions_count: None,
        batch_size: None,
        transaction_filter: None,
    };
    
    for attempt in 0..MAX_RETRIES {
        let mut client = self
            .connection_manager
            .get_grpc_manager_client_for_request();
        
        let response = tokio::time::timeout(
            Duration::from_secs(REQUEST_TIMEOUT_SECS),
            client.get_transactions(request.clone())
        ).await;
        
        match response {
            Ok(Ok(response)) => {
                let transactions = response.into_inner().transactions;
                if transactions.is_empty() {
                    return vec![];
                }
                if transactions.first().unwrap().version == starting_version {
                    return transactions;
                }
                warn!("Version mismatch: expected {}, got {}", 
                    starting_version, transactions.first().unwrap().version);
            },
            Ok(Err(e)) => {
                warn!("gRPC error on attempt {}: {:?}", attempt + 1, e);
            },
            Err(_) => {
                warn!("Request timeout on attempt {}", attempt + 1);
            }
        }
        
        if attempt < MAX_RETRIES - 1 {
            tokio::time::sleep(Duration::from_millis(RETRY_DELAY_MS * (attempt as u64 + 1))).await;
        }
    }
    
    error!("Failed to fetch transactions after {} attempts", MAX_RETRIES);
    vec![]
}
```

**2. Configure Channel-level timeout:** [4](#0-3) 

Add `.timeout(Duration::from_secs(30))` to the Channel builder.

**3. Implement circuit breaker pattern:** Track consecutive failures and temporarily stop querying problematic endpoints.

## Proof of Concept

```rust
#[tokio::test]
async fn test_fetch_transactions_dos() {
    use tonic::{transport::Server, Request, Response, Status};
    use aptos_protos::indexer::v1::{
        grpc_manager_server::{GrpcManager, GrpcManagerServer},
        GetTransactionsRequest, GetTransactionsResponse,
    };
    
    // Mock malicious gRPC server that always returns errors
    struct MaliciousGrpcManager;
    
    #[tonic::async_trait]
    impl GrpcManager for MaliciousGrpcManager {
        type GetTransactionsStream = futures::stream::Empty<Result<GetTransactionsResponse, Status>>;
        
        async fn get_transactions(
            &self,
            _request: Request<GetTransactionsRequest>,
        ) -> Result<Response<Self::GetTransactionsStream>, Status> {
            // Always return error
            Err(Status::internal("Malicious error"))
        }
        
        // Implement other required methods...
    }
    
    // Start malicious server
    let addr = "127.0.0.1:50052".parse().unwrap();
    tokio::spawn(async move {
        Server::builder()
            .add_service(GrpcManagerServer::new(MaliciousGrpcManager))
            .serve(addr)
            .await
    });
    
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    // Create DataClient pointing to malicious server
    let connection_manager = Arc::new(
        ConnectionManager::new(1, vec!["http://127.0.0.1:50052".to_string()], 
                              "test".to_string(), true).await
    );
    let data_client = DataClient::new(connection_manager);
    
    // This call will hang indefinitely without the fix
    let start = std::time::Instant::now();
    let result = tokio::time::timeout(
        Duration::from_secs(5),
        data_client.fetch_transactions(0)
    ).await;
    
    // Should timeout, proving the DoS
    assert!(result.is_err(), "fetch_transactions should timeout but it didn't");
    assert!(start.elapsed() >= Duration::from_secs(5), "Should have timed out");
}
```

## Notes

**Severity Classification:** While this causes complete indexer service unavailability, it is classified as **High Severity** ("API crashes") rather than Critical per Aptos bug bounty rules, as it does not affect the blockchain network's consensus, liveness, or validator operations. The indexer-grpc is an auxiliary data service in the `ecosystem/` directory, separate from core blockchain components (consensus, execution, storage, governance, staking).

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/data_client.rs (L27-42)
```rust
        loop {
            let mut client = self
                .connection_manager
                .get_grpc_manager_client_for_request();
            let response = client.get_transactions(request.clone()).await;
            if let Ok(response) = response {
                let transactions = response.into_inner().transactions;
                if transactions.is_empty() {
                    return vec![];
                }
                if transactions.first().unwrap().version == starting_version {
                    return transactions;
                }
            }
            // TODO(grao): Error handling.
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/fetch_manager.rs (L40-46)
```rust
    pub(super) async fn continuously_fetch_latest_data(&'a self) {
        loop {
            let task = self.fetch_latest_data().boxed().shared();
            *self.fetching_latest_data_task.write().await = Some(task.clone());
            let _ = task.await;
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/fetch_manager.rs (L53-53)
```rust
        let transactions = data_client.fetch_transactions(version).await;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/connection_manager.rs (L303-313)
```rust
    fn create_client_from_address(address: &str) -> GrpcManagerClient<Channel> {
        info!("Creating GrpcManagerClient for {address}.");
        let channel = Channel::from_shared(address.to_string())
            .expect("Bad address.")
            .connect_lazy();
        GrpcManagerClient::new(channel)
            .send_compressed(CompressionEncoding::Zstd)
            .accept_compressed(CompressionEncoding::Zstd)
            .max_decoding_message_size(MAX_MESSAGE_SIZE)
            .max_encoding_message_size(MAX_MESSAGE_SIZE)
    }
```
