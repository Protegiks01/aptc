# Audit Report

## Title
Unbounded Channel Overflow in Cross-Shard Execution Leads to Memory Exhaustion and Node Crash

## Summary
The sharded block executor uses unbounded channels for cross-shard message communication, allowing sending shards to produce messages faster than receiving shards can process them. This leads to unbounded memory growth and potential validator node crashes.

## Finding Description

The sharded block executor implements cross-shard communication through message channels to propagate transaction outputs between shards. However, these channels are unbounded, creating a critical resource exhaustion vulnerability.

**Channel Implementation:**

Both `LocalCrossShardClient` and `RemoteCrossShardClient` use unbounded channels created via `crossbeam_channel::unbounded()`: [1](#0-0) [2](#0-1) 

**Message Sending Pattern:**

When transactions commit in parallel execution using BlockSTMv2, multiple worker threads process `PostCommitProcessing` tasks concurrently: [3](#0-2) 

Each task invokes `record_finalized_output()`, which calls the transaction commit hook: [4](#0-3) [5](#0-4) 

For cross-shard execution, this triggers `CrossShardCommitSender::on_transaction_committed()`, which sends messages for each cross-shard dependency: [6](#0-5) [7](#0-6) 

**Message Receiving Pattern:**

Messages are processed by a single thread running `CrossShardCommitReceiver::start()` in a sequential loop: [8](#0-7) 

**The Vulnerability:**

1. Multiple worker threads on the sending shard can commit transactions in parallel
2. Each committed transaction can send multiple cross-shard messages (one per dependent location)
3. All messages are pushed to unbounded channels with no backpressure
4. The receiving shard processes messages sequentially in a single thread
5. If sending rate exceeds receiving rate, the channel buffer grows without limit

**Attack Scenario:**

An attacker crafts a block with many transactions having cross-shard dependencies. When executed:
- Sending shard A commits transactions T1...Tn in parallel across multiple workers
- Each transaction sends k cross-shard messages to shard B
- Total messages sent = n × k potentially in parallel
- Shard B processes messages one-by-one in a single thread
- Channel buffer accumulates (n × k) messages faster than they can be consumed
- Memory grows unbounded until OOM condition triggers node crash

This breaks **Invariant #9 (Resource Limits)**: "All operations must respect gas, storage, and computational limits." The unbounded channel allows unlimited memory allocation regardless of block gas limits or execution constraints.

## Impact Explanation

This is a **High Severity** vulnerability per Aptos bug bounty criteria:

1. **Validator Node Slowdowns**: As channel buffers grow, memory pressure causes garbage collection overhead and performance degradation
2. **Node Crashes**: Eventually leads to Out-Of-Memory crashes, taking validator nodes offline
3. **Network Availability Impact**: If multiple validators process blocks with high cross-shard dependencies simultaneously, coordinated crashes could affect network liveness

While not reaching Critical severity (doesn't cause consensus safety violations or permanent state corruption), it represents a significant protocol violation that can disrupt validator operations and network availability.

## Likelihood Explanation

**High Likelihood:**

1. **No Special Permissions Required**: Any transaction sender can create transactions with cross-shard dependencies by accessing storage locations owned by different shards
2. **Natural Occurrence**: Legitimate workloads with high parallelism and cross-shard dependencies will trigger this without malicious intent
3. **Amplification Factor**: A single transaction can generate multiple cross-shard messages (one per dependent storage location)
4. **No Rate Limiting**: There are no built-in protections or backpressure mechanisms to prevent channel overflow
5. **Production Relevance**: Sharded execution is designed for high-throughput scenarios where this vulnerability is most likely to manifest

## Recommendation

Replace unbounded channels with bounded channels and implement proper backpressure:

**Option 1: Bounded Channels with Backpressure**
```rust
// In LocalCrossShardClient::new()
const MAX_CROSS_SHARD_QUEUE_SIZE: usize = 1000;

let (cross_shard_msg_txs, cross_shard_msg_rxs): (
    Vec<Vec<Sender<CrossShardMsg>>>,
    Vec<Vec<Receiver<CrossShardMsg>>>,
) = (0..num_shards)
    .map(|_| {
        (0..MAX_ALLOWED_PARTITIONING_ROUNDS)
            .map(|_| bounded(MAX_CROSS_SHARD_QUEUE_SIZE))  // Use bounded instead of unbounded
            .unzip()
    })
    .unzip();
```

**Option 2: Asynchronous Processing with Multiple Receivers**

Instead of a single receiver thread, spawn multiple receiver threads per shard to increase processing throughput and reduce the likelihood of queue buildup.

**Option 3: Dynamic Backpressure**

Implement monitoring of channel queue depths and apply backpressure by slowing down transaction commits when queues approach capacity limits.

The fix should include metrics to track channel queue depths and alert operators when approaching capacity.

## Proof of Concept

```rust
// Test demonstrating channel overflow in sharded execution
#[test]
fn test_cross_shard_channel_overflow() {
    use crossbeam_channel::unbounded;
    use std::sync::atomic::{AtomicUsize, Ordering};
    use std::sync::Arc;
    use std::thread;
    use std::time::Duration;
    
    let (tx, rx) = unbounded();
    let message_count = Arc::new(AtomicUsize::new(0));
    let message_count_clone = message_count.clone();
    
    // Simulate receiving shard: single thread processing slowly
    let receiver_handle = thread::spawn(move || {
        while let Ok(_msg) = rx.recv() {
            // Simulate slow processing
            thread::sleep(Duration::from_millis(10));
            message_count_clone.fetch_add(1, Ordering::SeqCst);
        }
    });
    
    // Simulate sending shard: multiple threads sending quickly
    let mut sender_handles = vec![];
    for _ in 0..8 {
        let tx_clone = tx.clone();
        let handle = thread::spawn(move || {
            for _ in 0..1000 {
                // Send succeeds immediately with unbounded channel
                tx_clone.send(vec![0u8; 1024]).unwrap();
            }
        });
        sender_handles.push(handle);
    }
    
    // Wait for all senders to complete
    for handle in sender_handles {
        handle.join().unwrap();
    }
    
    // Total messages sent: 8 threads × 1000 messages = 8000 messages
    // Each message is 1KB, so ~8MB queued
    
    thread::sleep(Duration::from_millis(100));
    let processed = message_count.load(Ordering::SeqCst);
    
    // Receiver processed much fewer messages than sent
    // The difference is sitting in unbounded channel buffer
    assert!(processed < 8000, "Channel buffer accumulated {} messages", 8000 - processed);
    
    drop(tx);
    receiver_handle.join().unwrap();
    
    // This demonstrates unbounded growth: with realistic workloads,
    // channel buffer would grow until OOM occurs
}
```

## Notes

The vulnerability is exacerbated by the parallel post-commit processing model in BlockSTMv2 where commit hooks execute concurrently across multiple worker threads. While BlockSTMv1 has more serialization in the commit path, both versions ultimately rely on unbounded channels for cross-shard communication, making them vulnerable to this issue. The remote execution mode using `NetworkController` is equally affected as it also creates unbounded channels for network communication.

### Citations

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L95-104)
```rust
        let (cross_shard_msg_txs, cross_shard_msg_rxs): (
            Vec<Vec<Sender<CrossShardMsg>>>,
            Vec<Vec<Receiver<CrossShardMsg>>>,
        ) = (0..num_shards)
            .map(|_| {
                (0..MAX_ALLOWED_PARTITIONING_ROUNDS)
                    .map(|_| unbounded())
                    .unzip()
            })
            .unzip();
```

**File:** secure/net/src/network_controller/mod.rs (L115-126)
```rust
    pub fn create_outbound_channel(
        &mut self,
        remote_peer_addr: SocketAddr,
        message_type: String,
    ) -> Sender<Message> {
        let (outbound_sender, outbound_receiver) = unbounded();

        self.outbound_handler
            .register_handler(message_type, remote_peer_addr, outbound_receiver);

        outbound_sender
    }
```

**File:** aptos-move/block-executor/src/executor.rs (L1263-1285)
```rust
    fn record_finalized_output(
        &self,
        txn_idx: TxnIndex,
        output_idx: TxnIndex,
        shared_sync_params: &SharedSyncParams<T, E, S>,
    ) -> Result<(), PanicError> {
        if output_idx < txn_idx {
            return Err(code_invariant_error(format!(
                "Index to record finalized output {} is less than txn index {}",
                output_idx, txn_idx
            )));
        }

        let last_input_output = shared_sync_params.last_input_output;
        if let Some(txn_commit_listener) = &self.transaction_commit_hook {
            last_input_output.notify_listener(txn_idx, txn_commit_listener)?;
        }

        let mut final_results = shared_sync_params.final_results.acquire();

        final_results[output_idx as usize] = last_input_output.take_output(txn_idx)?;
        Ok(())
    }
```

**File:** aptos-move/block-executor/src/executor.rs (L1507-1515)
```rust
                TaskKind::PostCommitProcessing(txn_idx) => {
                    self.materialize_txn_commit(
                        txn_idx,
                        scheduler_wrapper,
                        environment,
                        shared_sync_params,
                    )?;
                    self.record_finalized_output(txn_idx, txn_idx, shared_sync_params)?;
                },
```

**File:** aptos-move/block-executor/src/txn_last_input_output.rs (L409-440)
```rust
    pub(crate) fn notify_listener<L: TransactionCommitHook>(
        &self,
        txn_idx: TxnIndex,
        txn_listener: &L,
    ) -> Result<(), PanicError> {
        let output_wrapper = self.output_wrappers[txn_idx as usize].lock();
        match output_wrapper.output_status_kind {
            OutputStatusKind::Success | OutputStatusKind::SkipRest => {
                txn_listener.on_transaction_committed(
                    txn_idx,
                    output_wrapper
                        .output
                        .as_ref()
                        .expect("Output must be set when status is success or skip rest")
                        .committed_output(),
                );
            },
            OutputStatusKind::Abort(_) => {
                txn_listener.on_execution_aborted(txn_idx);
            },
            OutputStatusKind::SpeculativeExecutionAbortError
            | OutputStatusKind::DelayedFieldsCodeInvariantError
            | OutputStatusKind::None => {
                return Err(code_invariant_error(format!(
                    "Unexpected output status kind {:?}",
                    output_wrapper.output_status_kind
                )));
            },
        }

        Ok(())
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L25-46)
```rust
impl CrossShardCommitReceiver {
    pub fn start<S: StateView + Sync + Send>(
        cross_shard_state_view: Arc<CrossShardStateView<S>>,
        cross_shard_client: Arc<dyn CrossShardClient>,
        round: RoundId,
    ) {
        loop {
            let msg = cross_shard_client.receive_cross_shard_msg(round);
            match msg {
                RemoteTxnWriteMsg(txn_commit_msg) => {
                    let (state_key, write_op) = txn_commit_msg.take();
                    cross_shard_state_view
                        .set_value(&state_key, write_op.and_then(|w| w.as_state_value()));
                },
                CrossShardMsg::StopMsg => {
                    trace!("Cross shard commit receiver stopped for round {}", round);
                    break;
                },
            }
        }
    }
}
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L103-134)
```rust
    fn send_remote_update_for_success(
        &self,
        txn_idx: TxnIndex,
        txn_output: &OnceCell<TransactionOutput>,
    ) {
        let edges = self.dependent_edges.get(&txn_idx).unwrap();
        let write_set = txn_output
            .get()
            .expect("Committed output must be set")
            .write_set();

        for (state_key, write_op) in write_set.expect_write_op_iter() {
            if let Some(dependent_shard_ids) = edges.get(state_key) {
                for (dependent_shard_id, round_id) in dependent_shard_ids.iter() {
                    trace!("Sending remote update for success for shard id {:?} and txn_idx: {:?}, state_key: {:?}, dependent shard id: {:?}", self.shard_id, txn_idx, state_key, dependent_shard_id);
                    let message = RemoteTxnWriteMsg(RemoteTxnWrite::new(
                        state_key.clone(),
                        Some(write_op.clone()),
                    ));
                    if *round_id == GLOBAL_ROUND_ID {
                        self.cross_shard_client.send_global_msg(message);
                    } else {
                        self.cross_shard_client.send_cross_shard_msg(
                            *dependent_shard_id,
                            *round_id,
                            message,
                        );
                    }
                }
            }
        }
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L137-152)
```rust
impl TransactionCommitHook for CrossShardCommitSender {
    fn on_transaction_committed(
        &self,
        txn_idx: TxnIndex,
        txn_output: &OnceCell<TransactionOutput>,
    ) {
        let global_txn_idx = txn_idx + self.index_offset;
        if self.dependent_edges.contains_key(&global_txn_idx) {
            self.send_remote_update_for_success(global_txn_idx, txn_output);
        }
    }

    fn on_execution_aborted(&self, _txn_idx: TxnIndex) {
        todo!("on_transaction_aborted not supported for sharded execution yet")
    }
}
```
