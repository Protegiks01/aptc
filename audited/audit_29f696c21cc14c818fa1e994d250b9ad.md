# Audit Report

## Title
Admin Service Blocking Thread Pool Exhaustion via Unauthenticated Rate Flooding of Database Dump Endpoints

## Summary
The admin service's database dump endpoints (`/debug/consensus/consensusdb`, `/debug/consensus/quorumstoredb`, `/debug/consensus/block`) lack rate limiting and mutex protection, allowing an authenticated attacker to exhaust the blocking thread pool by flooding these expensive operations, causing legitimate admin operations and node monitoring to fail.

## Finding Description

The admin service handles requests through the `serve_requests()` function [1](#0-0) , which validates authentication [2](#0-1)  but implements no rate limiting or concurrent request protection.

While CPU profiling and thread dump endpoints use mutex guards to prevent concurrent execution [3](#0-2)  , the consensus database dump endpoints do not have such protection.

The vulnerable endpoints spawn blocking tasks for expensive database operations:
- `/debug/consensus/consensusdb` dumps the entire consensus database [4](#0-3) 
- `/debug/consensus/quorumstoredb` dumps quorum store batches [5](#0-4) 
- `/debug/consensus/block` extracts and serializes transactions from blocks [6](#0-5) 

The admin service runtime has a hard limit of 64 blocking threads [7](#0-6) . When all blocking threads are occupied, additional tasks queue indefinitely.

**Attack Path:**
1. Attacker obtains valid admin authentication passcode (SHA256 hash)
2. Sends hundreds of concurrent authenticated requests to `/debug/consensus/consensusdb?passcode=<valid>`
3. Each request spawns a blocking task via `tokio::task::spawn_blocking` 
4. After 64 concurrent requests, the blocking thread pool saturates
5. Additional requests queue up, consuming memory for queued tasks
6. Database operations from multiple concurrent tasks create lock contention
7. Legitimate admin operations (profiling, debugging, monitoring) cannot acquire blocking threads and fail

The database dump operations iterate over potentially large datasets without pagination or size limits [8](#0-7) , making each request expensive in both CPU and memory.

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos bug bounty program criteria: "State inconsistencies requiring intervention."

**Specific impacts:**
- **Admin Service Unavailability**: Legitimate operators cannot use admin endpoints for monitoring, debugging, or profiling during an attack
- **Operational Visibility Loss**: Node operators lose critical debugging capabilities when needed most
- **Resource Exhaustion**: Memory consumption grows with queued tasks and large response buffers
- **Cascading Effects**: Database read lock contention from hundreds of concurrent queries could impact consensus/execution performance

This does not rise to High or Critical severity because:
- It does not directly affect consensus safety or validator operations
- It does not cause permanent state corruption or fund loss
- The attack only affects the admin service, not core blockchain functions
- Recovery is possible by restarting the node

However, it exceeds Low severity because:
- It causes operational harm requiring intervention
- It can be sustained to prevent all admin operations
- It affects node reliability and observability

## Likelihood Explanation

**Likelihood: High**

**Attacker Requirements:**
- Valid admin service passcode (SHA256 hash)
- Network access to the admin service port (default 9102)
- Basic HTTP client capability

**Execution Complexity:**
- **Low**: Simple HTTP GET requests with query parameter authentication
- No sophisticated exploit techniques required
- Can be executed with basic scripting tools (curl, Python requests)

**Attack Detection:**
- Difficult to distinguish from legitimate heavy admin usage
- No rate limiting thresholds to trigger alerts
- Authentication is per-request, not session-based

**Passcode Acquisition:**
- Passcodes may be shared among operator teams
- Could be exposed in configuration files, logs, or documentation
- May be transmitted over insecure channels
- Social engineering against operators

On mainnet, authentication is required [9](#0-8) , but on testnet/devnet the service is enabled by default without authentication [10](#0-9) , making exploitation trivial.

## Recommendation

Implement multi-layered protection:

**1. Add Mutex Protection for Database Dump Endpoints**
```rust
// In crates/aptos-admin-service/src/server/consensus/mod.rs
use lazy_static::lazy_static;
use async_mutex::Mutex;

lazy_static! {
    static ref CONSENSUS_DB_DUMP_MUTEX: Mutex<()> = Mutex::new(());
    static ref QUORUM_STORE_DUMP_MUTEX: Mutex<()> = Mutex::new(());
}

pub async fn handle_dump_consensus_db_request(
    _req: Request<Body>,
    consensus_db: Arc<dyn PersistentLivenessStorage>,
) -> hyper::Result<Response<Body>> {
    let lock = CONSENSUS_DB_DUMP_MUTEX.try_lock();
    if lock.is_none() {
        return Ok(reply_with_status(
            StatusCode::TOO_MANY_REQUESTS,
            "A consensus DB dump is already in progress.",
        ));
    }
    
    info!("Dumping consensus db.");
    // ... rest of implementation
}
```

**2. Add Rate Limiting Configuration**
```rust
// In config/src/config/admin_service_config.rs
#[derive(Clone, Debug, Deserialize, PartialEq, Eq, Serialize)]
pub struct AdminServiceConfig {
    // ... existing fields
    pub max_concurrent_db_dumps: usize,  // Default: 1
    pub rate_limit_per_minute: usize,    // Default: 10
}
```

**3. Add Request Timeout**
```rust
// Wrap spawn_blocking with timeout
match tokio::time::timeout(
    Duration::from_secs(30),
    spawn_blocking(move || dump_consensus_db(consensus_db.as_ref()))
).await {
    Ok(Ok(result)) => { /* success */ },
    Ok(Err(e)) => { /* operation error */ },
    Err(_) => { /* timeout */ },
}
```

**4. Implement Request Throttling**
Use a semaphore to limit concurrent database operations across all endpoints.

## Proof of Concept

```rust
#[tokio::test]
async fn test_admin_service_blocking_pool_exhaustion() {
    use hyper::{Client, Uri};
    use std::time::Duration;
    
    // Start admin service with authentication
    let node_config = NodeConfig {
        admin_service: AdminServiceConfig {
            enabled: Some(true),
            address: "127.0.0.1".to_string(),
            port: 9102,
            authentication_configs: vec![
                AuthenticationConfig::PasscodeSha256(
                    "ba7816bf8f01cfea414140de5dae2223b00361a396177a9cb410ff61f20015ad".to_string() // SHA256("abc")
                )
            ],
            ..Default::default()
        },
        ..Default::default()
    };
    
    let admin_service = AdminService::new(&node_config);
    // Set up consensus DBs...
    
    tokio::time::sleep(Duration::from_secs(1)).await;
    
    let client = Client::new();
    let passcode = "abc";
    
    // Spawn 100 concurrent requests to exhaust blocking pool
    let mut handles = vec![];
    for _ in 0..100 {
        let client = client.clone();
        let uri = format!(
            "http://127.0.0.1:9102/debug/consensus/consensusdb?passcode={}", 
            passcode
        ).parse::<Uri>().unwrap();
        
        let handle = tokio::spawn(async move {
            let start = std::time::Instant::now();
            let response = client.get(uri).await;
            let duration = start.elapsed();
            (response.is_ok(), duration)
        });
        handles.push(handle);
    }
    
    // Wait for all requests
    let results = futures::future::join_all(handles).await;
    
    // Verify that later requests experience significant delays
    let mut delays = results.iter()
        .filter_map(|r| r.as_ref().ok())
        .map(|(_, duration)| duration)
        .collect::<Vec<_>>();
    delays.sort();
    
    // After saturating 64 blocking threads, subsequent requests should be severely delayed
    assert!(delays[90].as_secs() > 5, "Requests should be delayed when blocking pool saturated");
    
    // Now try a legitimate admin operation - it should fail or timeout
    let profile_uri = format!(
        "http://127.0.0.1:9102/profilez?passcode={}&seconds=1", 
        passcode
    ).parse::<Uri>().unwrap();
    
    let profile_result = tokio::time::timeout(
        Duration::from_secs(5),
        client.get(profile_uri)
    ).await;
    
    assert!(profile_result.is_err(), "Legitimate admin operation should timeout");
}
```

## Notes

- The vulnerability affects all nodes with admin service enabled (testnet/devnet by default)
- The admin service runs on a dedicated tokio runtime [11](#0-10) , so exhaustion doesn't directly impact consensus threads
- However, database lock contention from concurrent reads could still impact node performance
- The mempool parking lot endpoint [12](#0-11)  uses async messaging and is less vulnerable
- Malloc stats endpoints are synchronous and relatively fast, posing lower risk [13](#0-12)

### Citations

**File:** crates/aptos-admin-service/src/server/mod.rs (L89-90)
```rust
        // Create a runtime for the admin service
        let runtime = aptos_runtimes::spawn_named_runtime("admin".into(), None);
```

**File:** crates/aptos-admin-service/src/server/mod.rs (L142-146)
```rust
    async fn serve_requests(
        context: Arc<Context>,
        req: Request<Body>,
        enabled: bool,
    ) -> hyper::Result<Response<Body>> {
```

**File:** crates/aptos-admin-service/src/server/mod.rs (L154-174)
```rust
        let mut authenticated = false;
        if context.config.authentication_configs.is_empty() {
            authenticated = true;
        } else {
            for authentication_config in &context.config.authentication_configs {
                match authentication_config {
                    AuthenticationConfig::PasscodeSha256(passcode_sha256) => {
                        let query = req.uri().query().unwrap_or("");
                        let query_pairs: HashMap<_, _> =
                            url::form_urlencoded::parse(query.as_bytes()).collect();
                        let passcode: Option<String> =
                            query_pairs.get("passcode").map(|p| p.to_string());
                        if let Some(passcode) = passcode {
                            if sha256::digest(passcode) == *passcode_sha256 {
                                authenticated = true;
                            }
                        }
                    },
                }
            }
        };
```

**File:** crates/aptos-system-utils/src/profiling.rs (L15-16)
```rust
lazy_static! {
    static ref CPU_PROFILE_MUTEX: Mutex<()> = Mutex::new(());
```

**File:** crates/aptos-admin-service/src/server/consensus/mod.rs (L17-23)
```rust
pub async fn handle_dump_consensus_db_request(
    _req: Request<Body>,
    consensus_db: Arc<dyn PersistentLivenessStorage>,
) -> hyper::Result<Response<Body>> {
    info!("Dumping consensus db.");

    match spawn_blocking(move || dump_consensus_db(consensus_db.as_ref())).await {
```

**File:** crates/aptos-admin-service/src/server/consensus/mod.rs (L40-57)
```rust
pub async fn handle_dump_quorum_store_db_request(
    req: Request<Body>,
    quorum_store_db: Arc<dyn QuorumStoreStorage>,
) -> hyper::Result<Response<Body>> {
    let query = req.uri().query().unwrap_or("");
    let query_pairs: HashMap<_, _> = url::form_urlencoded::parse(query.as_bytes()).collect();

    let digest: Option<HashValue> = match query_pairs.get("digest") {
        Some(val) => match val.parse() {
            Ok(val) => Some(val),
            Err(err) => return Ok(reply_with_status(StatusCode::BAD_REQUEST, err.to_string())),
        },
        None => None,
    };

    info!("Dumping quorum store db.");

    match spawn_blocking(move || dump_quorum_store_db(quorum_store_db.as_ref(), digest)).await {
```

**File:** crates/aptos-admin-service/src/server/consensus/mod.rs (L74-114)
```rust
pub async fn handle_dump_block_request(
    req: Request<Body>,
    consensus_db: Arc<dyn PersistentLivenessStorage>,
    quorum_store_db: Arc<dyn QuorumStoreStorage>,
) -> hyper::Result<Response<Body>> {
    let query = req.uri().query().unwrap_or("");
    let query_pairs: HashMap<_, _> = url::form_urlencoded::parse(query.as_bytes()).collect();

    let block_id: Option<HashValue> = match query_pairs.get("block_id") {
        Some(val) => match val.parse() {
            Ok(val) => Some(val),
            Err(err) => return Ok(reply_with_status(StatusCode::BAD_REQUEST, err.to_string())),
        },
        None => None,
    };

    // TODO(grao): I'm lazy, only support this through query parameters, let me know if this need
    // to be done through header.
    let bcs: bool = match query_pairs.get("bcs") {
        Some(val) => match val.parse() {
            Ok(val) => val,
            Err(err) => return Ok(reply_with_status(StatusCode::BAD_REQUEST, err.to_string())),
        },
        None => false,
    };

    if let Some(block_id) = block_id {
        info!("Dumping block ({block_id:?}).");
    } else {
        info!("Dumping all blocks.");
    }

    match spawn_blocking(move || {
        if bcs {
            dump_blocks_bcs(consensus_db.as_ref(), quorum_store_db.as_ref(), block_id)
                .map(Into::<Body>::into)
        } else {
            dump_blocks(consensus_db.as_ref(), quorum_store_db.as_ref(), block_id).map(Into::into)
        }
    })
    .await
```

**File:** crates/aptos-admin-service/src/server/consensus/mod.rs (L133-155)
```rust
    let (last_vote, highest_tc, consensus_blocks, consensus_qcs) =
        consensus_db.consensus_db().get_data()?;

    body.push_str(&format!("Last vote: \n{last_vote:?}\n\n"));
    body.push_str(&format!("Highest tc: \n{highest_tc:?}\n\n"));
    body.push_str("Blocks: \n");
    for block in consensus_blocks {
        body.push_str(&format!(
            "[id: {:?}, author: {:?}, epoch: {}, round: {:02}, parent_id: {:?}, timestamp: {}, payload: {:?}]\n\n",
            block.id(),
            block.author(),
            block.epoch(),
            block.round(),
            block.parent_id(),
            block.timestamp_usecs(),
            block.payload(),
        ));
    }
    body.push_str("QCs: \n");
    for qc in consensus_qcs {
        body.push_str(&format!("{qc:?}\n\n"));
    }
    Ok(body)
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-50)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;

    // Verify the given name has an appropriate length
    if thread_name.len() > MAX_THREAD_NAME_LENGTH {
        panic!(
            "The given runtime thread name is too long! Max length: {}, given name: {}",
            MAX_THREAD_NAME_LENGTH, thread_name
        );
    }

    // Create the runtime builder
    let atomic_id = AtomicUsize::new(0);
    let thread_name_clone = thread_name.clone();
    let mut builder = Builder::new_multi_thread();
    builder
        .thread_name_fn(move || {
            let id = atomic_id.fetch_add(1, Ordering::SeqCst);
            format!("{}-{}", thread_name_clone, id)
        })
        .on_thread_start(on_thread_start)
        .disable_lifo_slot()
        // Limit concurrent blocking tasks from spawn_blocking(), in case, for example, too many
        // Rest API calls overwhelm the node.
        .max_blocking_threads(MAX_BLOCKING_THREADS)
```

**File:** config/src/config/admin_service_config.rs (L67-77)
```rust
        if node_config.admin_service.enabled == Some(true) {
            if let Some(chain_id) = chain_id {
                if chain_id.is_mainnet()
                    && node_config.admin_service.authentication_configs.is_empty()
                {
                    return Err(Error::ConfigSanitizerFailed(
                        sanitizer_name,
                        "Must enable authentication for AdminService on mainnet.".into(),
                    ));
                }
            }
```

**File:** config/src/config/admin_service_config.rs (L93-99)
```rust
        if node_config.admin_service.enabled.is_none() {
            // Only enable the admin service if the chain is not mainnet
            let admin_service_enabled = if let Some(chain_id) = chain_id {
                !chain_id.is_mainnet()
            } else {
                false // We cannot determine the chain ID, so we disable the admin service
            };
```

**File:** crates/aptos-admin-service/src/server/mempool/mod.rs (L12-38)
```rust
pub async fn mempool_handle_parking_lot_address_request(
    _req: Request<Body>,
    mempool_client_sender: MempoolClientSender,
) -> hyper::Result<Response<Body>> {
    match get_parking_lot_addresses(mempool_client_sender).await {
        Ok(addresses) => {
            info!("Finished getting parking lot addresses from mempool.");
            match bcs::to_bytes(&addresses) {
                Ok(addresses) => Ok(reply_with(vec![], addresses)),
                Err(e) => {
                    info!("Failed to bcs serialize parking lot addresses from mempool: {e:?}");
                    Ok(reply_with_status(
                        StatusCode::INTERNAL_SERVER_ERROR,
                        e.to_string(),
                    ))
                },
            }
        },
        Err(e) => {
            info!("Failed to get parking lot addresses from mempool: {e:?}");
            Ok(reply_with_status(
                StatusCode::INTERNAL_SERVER_ERROR,
                e.to_string(),
            ))
        },
    }
}
```

**File:** crates/aptos-admin-service/src/server/malloc.rs (L36-44)
```rust
pub fn handle_malloc_stats_request(max_len: usize) -> hyper::Result<Response<Body>> {
    match get_jemalloc_stats_string(max_len) {
        Ok(stats) => Ok(reply_with(Vec::new(), Body::from(stats))),
        Err(e) => Ok(reply_with_status(
            StatusCode::INTERNAL_SERVER_ERROR,
            format!("Failed to get malloc stats: {e}"),
        )),
    }
}
```
