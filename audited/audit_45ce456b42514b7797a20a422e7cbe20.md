# Audit Report

## Title
Indefinite Backup/Restore Blocking Due to Missing Timeout Handling in Storage Operations

## Summary
The backup storage system lacks timeout handling for async operations, allowing hung storage operations (e.g., stalled S3 uploads, unresponsive NFS mounts) to block backup and restore operations indefinitely. This prevents timely disaster recovery and can degrade validator node availability.

## Finding Description
The `BackupStorage` trait defines async methods for backup operations but does not implement any timeout mechanisms. [1](#0-0) 

Both implementations of this trait (`CommandAdapter` and `LocalFs`) execute storage operations without timeouts:

1. **CommandAdapter Implementation**: Spawns external bash commands (e.g., `aws s3 cp`, `gsutil cp`) using `tokio::process::Command` without any timeout wrapper. [2](#0-1) 

2. **Command Execution Layer**: The underlying `SpawnedCommand::join()` method calls `wait_with_output().await` without timeout protection. [3](#0-2) 

3. **LocalFs Implementation**: Uses tokio async file operations (`create_dir_all`, `OpenOptions`, `read_dir`, etc.) without timeout wrappers. [4](#0-3) 

4. **Backup Controllers**: Call storage methods directly without timeout protection. For example, `StateSnapshotBackupController::write_chunk()` calls `create_for_write()`, `write_all()`, and `shutdown()` without any timeout. [5](#0-4) 

5. **Restore Controllers**: Similarly call `open_for_read()` without timeout protection. [6](#0-5) 

**Attack Scenarios**:
- Network disruption causes `aws s3 cp` command to hang indefinitely
- Storage service degradation (S3, GCS outage) causes upload operations to stall
- NFS mount becomes unresponsive, blocking local filesystem operations
- Misconfigured storage credentials cause authentication loops without timeout

## Impact Explanation
This is **HIGH severity** per the Aptos bug bounty criteria:

1. **Validator Node Slowdowns**: If backups run on validator nodes and get stuck, they can consume resources and prevent subsequent backup operations, degrading node performance.

2. **API/Service Availability**: Stuck backup operations can block the backup service, preventing disaster recovery capabilities for extended periods.

3. **Disaster Recovery Failure**: Without timely backups, the network loses its ability to recover from catastrophic failures. A stuck backup process prevents creation of new recovery points.

4. **Operational Impact**: Operators must manually detect and kill hung processes, increasing operational burden and mean time to recovery (MTTR).

While this doesn't directly break consensus safety or cause fund loss, it significantly impacts the availability and operational reliability of the Aptos network, meeting the "validator node slowdowns" and "significant protocol violations" criteria for HIGH severity.

## Likelihood Explanation
**HIGH likelihood** - This vulnerability can be triggered by common production scenarios:

1. **Network Issues**: Transient network failures, firewall changes, or ISP issues commonly cause storage operations to hang
2. **Cloud Service Degradation**: AWS S3, GCS, and Azure Storage experience periodic outages or degraded performance
3. **Storage Configuration Errors**: Misconfigured credentials, incorrect bucket names, or permission issues can cause indefinite retries
4. **Filesystem Issues**: NFS mounts, network drives, or disk I/O problems frequently cause filesystem operations to hang

No attacker access is required - these are environmental conditions that occur naturally in production systems. The probability increases with:
- Number of backup operations per day
- Network path complexity
- Geographic distribution of storage services
- Scale of data being backed up

## Recommendation
Implement timeout wrappers for all storage operations. Add configurable timeout values to the backup configuration:

```rust
// In BackupStorage trait or implementation
pub const DEFAULT_STORAGE_TIMEOUT_SECS: u64 = 3600; // 1 hour for large uploads

// Wrap storage operations with timeouts
async fn create_for_write_with_timeout(
    &self,
    backup_handle: &BackupHandleRef,
    name: &ShellSafeName,
    timeout: Duration,
) -> Result<(FileHandle, Box<dyn AsyncWrite + Send + Unpin>)> {
    tokio::time::timeout(
        timeout,
        self.create_for_write(backup_handle, name)
    )
    .await
    .map_err(|_| anyhow!("Storage operation timed out after {:?}", timeout))?
}
```

Apply timeouts at multiple levels:
1. Individual storage operation timeouts (for fast-fail on hung operations)
2. Per-chunk timeouts (for data transfer operations)
3. Overall backup job timeouts (for total operation time limits)

Add timeout configuration to `GlobalBackupOpt` and `CommandAdapterConfig`:
```rust
pub struct GlobalBackupOpt {
    // ... existing fields ...
    #[clap(long, default_value = "3600")]
    pub storage_operation_timeout_secs: u64,
}
```

Consider implementing exponential backoff retry logic with maximum retry attempts for transient failures.

## Proof of Concept

```rust
// Test demonstrating indefinite hang with CommandAdapter
use std::time::Duration;
use tokio::time::sleep;

#[tokio::test]
async fn test_hung_storage_operation() {
    // Create a CommandAdapter config that runs a command that hangs
    let config_content = r#"
env_vars: []
commands:
  create_backup: |
    echo "$BACKUP_NAME"
  create_for_write: |
    echo "$BACKUP_HANDLE/$FILE_NAME"
    exec 1>&-
    # Simulate a hung operation - sleep forever
    sleep 999999
  open_for_read: |
    sleep 999999
  save_metadata_line: |
    echo "metadata/$FILE_NAME"
    exec 1>&-
    sleep 999999
  list_metadata_files: |
    echo ""
"#;
    
    let config = CommandAdapterConfig::load_from_str(config_content).unwrap();
    let storage = Arc::new(CommandAdapter::new(config));
    
    // This will hang indefinitely without timeout
    let start = std::time::Instant::now();
    let backup_handle = storage.create_backup(&"test_backup".parse().unwrap()).await.unwrap();
    
    // Attempt to write - this will hang forever
    let result = tokio::time::timeout(
        Duration::from_secs(5),
        storage.create_for_write(&backup_handle, &"test_file".parse().unwrap())
    ).await;
    
    // Without the timeout wrapper above, this would never return
    assert!(result.is_err(), "Storage operation should timeout");
    println!("Hung operation detected after {:?}", start.elapsed());
}
```

To reproduce in production:
1. Configure backup storage with credentials that cause authentication to hang
2. Start a backup operation
3. Observe that the backup process never completes or times out
4. Verify that subsequent backups cannot proceed until the process is manually killed

**Notes**
- The vulnerability exists in both `CommandAdapter` (for cloud storage) and `LocalFs` (for local filesystem) implementations
- The timeout for `BackupServiceClient` (60 seconds) only applies to communication with the backup HTTP service, NOT to storage operations
- Sample configurations show raw `aws s3 cp` and `gsutil cp` commands without timeout flags
- While this doesn't directly compromise consensus safety, it violates the availability and operational reliability requirements critical for disaster recovery

### Citations

**File:** storage/backup/backup-cli/src/storage/mod.rs (L135-188)
```rust
#[async_trait]
pub trait BackupStorage: Send + Sync {
    /// Hint that a bunch of files are gonna be created related to a backup identified by `name`,
    /// which is unique to the content of the backup, i.e. it won't be the same name unless you are
    /// backing up exactly the same thing.
    /// Storage can choose to take actions like create a dedicated folder or do nothing.
    /// Returns a string to identify this operation in potential succeeding file creation requests.
    async fn create_backup(&self, name: &ShellSafeName) -> Result<BackupHandle>;
    /// Ask to create a file for write, `backup_handle` was returned by `create_backup` to identify
    /// the current backup.
    async fn create_for_write(
        &self,
        backup_handle: &BackupHandleRef,
        name: &ShellSafeName,
    ) -> Result<(FileHandle, Box<dyn AsyncWrite + Send + Unpin>)>;
    /// Open file for reading.
    async fn open_for_read(
        &self,
        file_handle: &FileHandleRef,
    ) -> Result<Box<dyn AsyncRead + Send + Unpin>>;
    /// Asks to save a metadata entry and return the File handle of the saved file.
    /// A metadata entry is one line of text.
    /// The backup system doesn't expect a metadata entry to exclusively map to a single file
    /// handle, or the same file handle when accessed later, so there's no need to return one. This
    /// also means a local cache must download each metadata file from remote at least once, to
    /// uncover potential storage glitch sooner.
    /// Behavior on duplicated names is undefined, overwriting the content upon an existing name
    /// is straightforward and acceptable.
    /// See `list_metadata_files`.
    async fn save_metadata_line(
        &self,
        name: &ShellSafeName,
        content: &TextLine,
    ) -> Result<FileHandle> {
        self.save_metadata_lines(name, std::slice::from_ref(content))
            .await
    }
    /// The backup system always asks for all metadata files and cache and build index on top of
    /// the content of them. This means:
    ///   1. The storage is free to reorganise the metadata files, like combining multiple ones to
    /// reduce fragmentation.
    ///   2. But the cache does expect the content stays the same for a file handle, so when
    /// reorganising metadata files, give them new unique names.
    async fn list_metadata_files(&self) -> Result<Vec<FileHandle>>;
    /// Move a metadata file to the metadata file backup folder.
    async fn backup_metadata_file(&self, file_handle: &FileHandleRef) -> Result<()>;
    /// Save a vector of metadata lines to file and return the file handle of saved file.
    /// If the file exists, this will overwrite
    async fn save_metadata_lines(
        &self,
        name: &ShellSafeName,
        lines: &[TextLine],
    ) -> Result<FileHandle>;
}
```

**File:** storage/backup/backup-cli/src/storage/command_adapter/mod.rs (L73-124)
```rust
#[async_trait]
impl BackupStorage for CommandAdapter {
    async fn create_backup(&self, name: &ShellSafeName) -> Result<BackupHandle> {
        let mut child = self
            .cmd(&self.config.commands.create_backup, vec![
                EnvVar::backup_name(name.to_string()),
            ])
            .spawn()?;
        let mut backup_handle = BackupHandle::new();
        child
            .stdout()
            .read_to_string(&mut backup_handle)
            .await
            .err_notes((file!(), line!(), name))?;
        child.join().await?;
        backup_handle.truncate(backup_handle.trim_end().len());

        Ok(backup_handle)
    }

    async fn create_for_write(
        &self,
        backup_handle: &BackupHandleRef,
        name: &ShellSafeName,
    ) -> Result<(FileHandle, Box<dyn AsyncWrite + Send + Unpin>)> {
        let mut child = self
            .cmd(&self.config.commands.create_for_write, vec![
                EnvVar::backup_handle(backup_handle.to_string()),
                EnvVar::file_name(name.as_ref()),
            ])
            .spawn()?;
        let mut file_handle = FileHandle::new();
        child
            .stdout()
            .read_to_string(&mut file_handle)
            .await
            .err_notes(backup_handle)?;
        file_handle.truncate(file_handle.trim_end().len());
        Ok((file_handle, Box::new(child.into_data_sink())))
    }

    async fn open_for_read(
        &self,
        file_handle: &FileHandleRef,
    ) -> Result<Box<dyn AsyncRead + Send + Unpin>> {
        let child = self
            .cmd(&self.config.commands.open_for_read, vec![
                EnvVar::file_handle(file_handle.to_string()),
            ])
            .spawn()?;
        Ok(Box::new(child.into_data_source()))
    }
```

**File:** storage/backup/backup-cli/src/storage/command_adapter/command.rs (L111-126)
```rust
    pub async fn join(self) -> Result<()> {
        match self.child.wait_with_output().await {
            Ok(output) => {
                if output.status.success() {
                    Ok(())
                } else {
                    bail!(
                        "Command {:?} failed with exit status: {}",
                        self.command,
                        output.status
                    )
                }
            },
            Err(e) => bail!("Failed joining command {:?}: {}", self.command, e),
        }
    }
```

**File:** storage/backup/backup-cli/src/storage/local_fs/mod.rs (L71-109)
```rust
#[async_trait]
impl BackupStorage for LocalFs {
    async fn create_backup(&self, name: &ShellSafeName) -> Result<BackupHandle> {
        create_dir_all(self.dir.join(name.as_ref()))
            .await
            .err_notes(self.dir.join(name.as_ref()))?;
        Ok(name.to_string())
    }

    async fn create_for_write(
        &self,
        backup_handle: &BackupHandleRef,
        name: &ShellSafeName,
    ) -> Result<(FileHandle, Box<dyn AsyncWrite + Send + Unpin>)> {
        let file_handle = Path::new(backup_handle)
            .join(name.as_ref())
            .path_to_string()?;
        let abs_path = self.dir.join(&file_handle).path_to_string()?;
        let file = OpenOptions::new()
            .write(true)
            .create_new(true)
            .open(&abs_path)
            .await
            .err_notes(&abs_path)?;
        Ok((file_handle, Box::new(file)))
    }

    async fn open_for_read(
        &self,
        file_handle: &FileHandleRef,
    ) -> Result<Box<dyn AsyncRead + Send + Unpin>> {
        let path = self.dir.join(file_handle);
        let file = OpenOptions::new()
            .read(true)
            .open(&path)
            .await
            .err_notes(&path)?;
        Ok(Box::new(file))
    }
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/backup.rs (L404-447)
```rust
    async fn write_chunk(
        &self,
        backup_handle: &BackupHandleRef,
        chunk: Chunk,
    ) -> Result<StateSnapshotChunk> {
        let _timer = BACKUP_TIMER.timer_with(&["state_snapshot_write_chunk"]);

        let Chunk {
            bytes,
            first_idx,
            last_idx,
            first_key,
            last_key,
        } = chunk;

        let (chunk_handle, mut chunk_file) = self
            .storage
            .create_for_write(backup_handle, &Self::chunk_name(first_idx))
            .await?;
        chunk_file.write_all(&bytes).await?;
        chunk_file.shutdown().await?;
        let (proof_handle, mut proof_file) = self
            .storage
            .create_for_write(backup_handle, &Self::chunk_proof_name(first_idx, last_idx))
            .await?;
        tokio::io::copy(
            &mut self
                .client
                .get_account_range_proof(last_key, self.version())
                .await?,
            &mut proof_file,
        )
        .await?;
        proof_file.shutdown().await?;

        Ok(StateSnapshotChunk {
            first_idx,
            last_idx,
            first_key,
            last_key,
            blobs: chunk_handle,
            proof: proof_handle,
        })
    }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L99-107)
```rust
impl LoadedChunk {
    async fn load(
        manifest: TransactionChunk,
        storage: &Arc<dyn BackupStorage>,
        epoch_history: Option<&Arc<EpochHistory>>,
    ) -> Result<Self> {
        let mut file = BufReader::new(storage.open_for_read(&manifest.transactions).await?);
        let mut txns = Vec::new();
        let mut persisted_aux_info = Vec::new();
```
