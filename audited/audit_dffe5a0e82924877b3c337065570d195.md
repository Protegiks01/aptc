# Audit Report

## Title
Mempool Coordinator Denial of Service via BoundedExecutor Exhaustion on Inbound Transaction Broadcasts

## Summary
The mempool's coordinator can be blocked by malicious authenticated peers flooding inbound transaction broadcasts. The global BoundedExecutor with limited concurrency (4 slots default, 16 for VFNs) combined with the coordinator's blocking behavior creates a resource exhaustion vulnerability that prevents processing of all mempool operations, including legitimate client transactions, consensus requests, and peer updates.

## Finding Description

The exported `network` module in mempool exposes the `MempoolSyncMsg` message types for peer-to-peer synchronization. [1](#0-0) 

When a peer sends a `BroadcastTransactionsRequest`, the coordinator's event loop processes it without pre-validation. [2](#0-1) 

The critical vulnerability occurs in `process_received_txns`, which immediately attempts to spawn a processing task on the BoundedExecutor without validating message size, transaction count, or peer-specific rate limits: [3](#0-2) 

The BoundedExecutor's `spawn()` method blocks via `acquire_permit().await` when at capacity: [4](#0-3) 

The BoundedExecutor capacity is severely limited - only 4 concurrent inbound syncs for regular nodes, or 16 for validator fullnodes: [5](#0-4) 

Each spawned task performs expensive operations:
- Database reads to fetch account sequence numbers [6](#0-5) 
- VM validation of all transactions [7](#0-6) 

**Attack Scenario:**
1. Malicious authenticated peer(s) send `BroadcastTransactionsRequest` messages with maximum transaction counts (up to 300 transactions per batch)
2. Each message spawns a task that performs expensive DB reads and VM validation
3. The 4 BoundedExecutor slots fill with these long-running validation tasks
4. Subsequent broadcasts cause the coordinator to block at `bounded_executor.spawn().await`
5. While blocked, the coordinator cannot process:
   - Client transaction submissions
   - Quorum store requests from consensus
   - Peer connection updates
   - Scheduled rebroadcasts
6. Legitimate operations are delayed or dropped from the KLAST network queue

**Why Existing Protections Are Insufficient:**

While mempool uses KLAST queuing per-peer [8](#0-7) , and network-level IP-based rate limiting exists, the **BoundedExecutor is shared globally across all peers**, creating a centralized bottleneck. Multiple coordinated malicious peers or a single malicious validator (which bypasses rate limits on validator networks) can exhaust this shared resource.

## Impact Explanation

This qualifies as **Medium Severity** per the Aptos bug bounty criteria:

**Validator node slowdowns**: When the coordinator blocks, all mempool operations are delayed, including transaction acceptance from clients and interaction with consensus. This directly impacts node responsiveness and transaction throughput.

**State inconsistencies requiring intervention**: If the coordinator remains blocked for extended periods, the mempool state diverges from other nodes, potentially requiring manual intervention to restore sync.

The attack does not directly cause fund loss or consensus safety violations, but significantly degrades network availability and service quality.

## Likelihood Explanation

**Likelihood: Medium**

**Requirements:**
- Authenticated peer access (requires being a validator, VFN, or having established trusted connection)
- Ability to send multiple transaction broadcasts
- Sufficient message volume to saturate BoundedExecutor

**Feasibility:**
- Malicious validators can attack other validators directly on the validator network without rate limits
- Compromised fullnode operators can attack VFNs they connect to
- Multiple coordinated attackers can amplify the effect
- Each batch can contain up to 300 transactions, maximizing validation cost per message

**Mitigating Factors:**
- Network-level IP rate limiting (100 KiB/s) slows single-peer attacks
- Requires maintaining authenticated peer connection
- KLAST queuing provides some buffering per-peer

## Recommendation

Implement multi-layered protection against inbound broadcast flooding:

**1. Per-Peer Inbound Rate Limiting:**
Add tracking of pending inbound syncs per peer, similar to the existing `max_broadcasts_per_peer` for outbound:

```rust
// In MempoolNetworkInterface
pub struct PeerSyncState {
    // ... existing fields ...
    pending_inbound_syncs: AtomicUsize,
}

// In coordinator.rs process_received_txns()
async fn process_received_txns<NetworkClient, TransactionValidator>(
    bounded_executor: &BoundedExecutor,
    smp: &mut SharedMempool<NetworkClient, TransactionValidator>,
    network_id: NetworkId,
    message_id: MempoolMessageId,
    transactions: Vec<(SignedTransaction, Option<u64>, Option<BroadcastPeerPriority>)>,
    peer_id: PeerId,
) {
    let peer = PeerNetworkId::new(network_id, peer_id);
    
    // NEW: Check per-peer inbound limit
    let max_inbound_per_peer = smp.config.max_inbound_syncs_per_peer; // e.g., 2
    if let Some(sync_state) = smp.network_interface.sync_states.read().get(&peer) {
        if sync_state.pending_inbound_syncs.load(Ordering::Relaxed) >= max_inbound_per_peer {
            // Drop the broadcast, send backoff ACK
            warn!("Dropping broadcast from {} - too many pending inbound syncs", peer);
            return;
        }
        sync_state.pending_inbound_syncs.fetch_add(1, Ordering::Relaxed);
    }
    
    // Spawn task...
    // Ensure pending_inbound_syncs is decremented when task completes
}
```

**2. Lightweight Pre-Validation:**
Validate transaction count and estimated size before spawning expensive validation tasks:

```rust
// NEW: Quick checks before spawning
if transactions.is_empty() || transactions.len() > smp.config.shared_mempool_batch_size {
    warn!("Invalid broadcast size from {}: {} txns", peer, transactions.len());
    return;
}
```

**3. Use try_spawn Instead of spawn:**
Use `bounded_executor.try_spawn()` to avoid blocking the coordinator when at capacity:

```rust
match bounded_executor.try_spawn(tasks::process_transaction_broadcast(...)) {
    Ok(handle) => { /* track handle */ },
    Err(_) => {
        warn!("BoundedExecutor at capacity, dropping broadcast from {}", peer);
        // Send backoff ACK to peer
    }
}
```

**4. Increase BoundedExecutor Capacity:**
Consider increasing `shared_mempool_max_concurrent_inbound_syncs` for better throughput, balanced against memory/CPU constraints.

## Proof of Concept

```rust
// Integration test demonstrating coordinator blocking
#[tokio::test]
async fn test_coordinator_blocking_on_broadcast_flood() {
    use std::sync::Arc;
    use tokio::sync::Barrier;
    
    // Setup test mempool node with small BoundedExecutor capacity
    let mut config = NodeConfig::default();
    config.mempool.shared_mempool_max_concurrent_inbound_syncs = 2; // Small capacity
    
    let (mut coordinator, network_sender, mempool) = setup_mempool(&config);
    
    // Create barrier to hold validation tasks
    let barrier = Arc::new(Barrier::new(3)); // 2 tasks + main thread
    
    // Mock expensive validation by injecting delay
    inject_validation_delay(Arc::clone(&barrier));
    
    // Spawn coordinator in background
    let coord_handle = tokio::spawn(async move {
        coordinator.run().await;
    });
    
    // Simulate malicious peer sending broadcasts rapidly
    for i in 0..10 {
        let txns = generate_test_transactions(300); // Max batch size
        let broadcast = MempoolSyncMsg::BroadcastTransactionsRequest {
            message_id: MempoolMessageId::new(),
            transactions: txns,
        };
        
        network_sender.send((peer_id, broadcast)).await.unwrap();
        
        if i == 2 {
            // After 3 broadcasts, executor should be saturated (2 slots + 1 waiting)
            // Try to submit a client transaction
            let (tx, rx) = oneshot::channel();
            let client_txn = create_test_transaction();
            
            // This should timeout because coordinator is blocked
            let timeout_result = tokio::time::timeout(
                Duration::from_millis(100),
                mempool.submit_transaction(client_txn, tx)
            ).await;
            
            assert!(timeout_result.is_err(), "Coordinator should be blocked!");
            break;
        }
    }
    
    // Release validation tasks
    barrier.wait().await;
    
    // Cleanup
    coord_handle.abort();
}
```

## Notes

This vulnerability stems from the architectural decision to use a global BoundedExecutor with blocking spawn semantics in the coordinator's event loop. While individual protections exist (KLAST queuing, network rate limits), the lack of per-peer inbound rate limiting on broadcast processing creates a centralized resource exhaustion point.

The issue is particularly relevant for validator-to-validator communication where network-level rate limits may be relaxed, and for VFNs which handle higher concurrency (16 slots) but also face more diverse peer connections.

### Citations

**File:** mempool/src/lib.rs (L60-67)
```rust
pub use shared_mempool::{
    bootstrap, network,
    network::MempoolSyncMsg,
    types::{
        MempoolClientRequest, MempoolClientSender, MempoolEventsReceiver, QuorumStoreRequest,
        QuorumStoreResponse, SubmissionStatus,
    },
};
```

**File:** mempool/src/shared_mempool/coordinator.rs (L293-342)
```rust
async fn process_received_txns<NetworkClient, TransactionValidator>(
    bounded_executor: &BoundedExecutor,
    smp: &mut SharedMempool<NetworkClient, TransactionValidator>,
    network_id: NetworkId,
    message_id: MempoolMessageId,
    transactions: Vec<(
        SignedTransaction,
        Option<u64>,
        Option<BroadcastPeerPriority>,
    )>,
    peer_id: PeerId,
) where
    NetworkClient: NetworkClientInterface<MempoolSyncMsg> + 'static,
    TransactionValidator: TransactionValidation + 'static,
{
    smp.network_interface
        .num_mempool_txns_received_since_peers_updated += transactions.len() as u64;
    let smp_clone = smp.clone();
    let peer = PeerNetworkId::new(network_id, peer_id);
    let ineligible_for_broadcast = (smp.network_interface.is_validator()
        && !smp.broadcast_within_validator_network())
        || smp.network_interface.is_upstream_peer(&peer, None);
    let timeline_state = if ineligible_for_broadcast {
        TimelineState::NonQualified
    } else {
        TimelineState::NotReady
    };
    // This timer measures how long it took for the bounded executor to
    // *schedule* the task.
    let _timer = counters::task_spawn_latency_timer(
        counters::PEER_BROADCAST_EVENT_LABEL,
        counters::SPAWN_LABEL,
    );
    // This timer measures how long it took for the task to go from scheduled
    // to started.
    let task_start_timer = counters::task_spawn_latency_timer(
        counters::PEER_BROADCAST_EVENT_LABEL,
        counters::START_LABEL,
    );
    bounded_executor
        .spawn(tasks::process_transaction_broadcast(
            smp_clone,
            transactions,
            message_id,
            timeline_state,
            peer,
            task_start_timer,
        ))
        .await;
}
```

**File:** mempool/src/shared_mempool/coordinator.rs (L356-405)
```rust
    match event {
        Event::Message(peer_id, msg) => {
            counters::shared_mempool_event_inc("message");
            match msg {
                MempoolSyncMsg::BroadcastTransactionsRequest {
                    message_id,
                    transactions,
                } => {
                    process_received_txns(
                        bounded_executor,
                        smp,
                        network_id,
                        message_id,
                        transactions.into_iter().map(|t| (t, None, None)).collect(),
                        peer_id,
                    )
                    .await;
                },
                MempoolSyncMsg::BroadcastTransactionsRequestWithReadyTime {
                    message_id,
                    transactions,
                } => {
                    process_received_txns(
                        bounded_executor,
                        smp,
                        network_id,
                        message_id,
                        transactions
                            .into_iter()
                            .map(|t| (t.0, Some(t.1), Some(t.2)))
                            .collect(),
                        peer_id,
                    )
                    .await;
                },
                MempoolSyncMsg::BroadcastTransactionsResponse {
                    message_id,
                    retry,
                    backoff,
                } => {
                    let ack_timestamp = SystemTime::now();
                    smp.network_interface.process_broadcast_ack(
                        PeerNetworkId::new(network_id, peer_id),
                        message_id,
                        retry,
                        backoff,
                        ack_timestamp,
                    );
                },
            }
```

**File:** crates/bounded-executor/src/executor.rs (L45-52)
```rust
    pub async fn spawn<F>(&self, future: F) -> JoinHandle<F::Output>
    where
        F: Future + Send + 'static,
        F::Output: Send + 'static,
    {
        let permit = self.acquire_permit().await;
        self.executor.spawn(future_with_permit(future, permit))
    }
```

**File:** config/src/config/mempool_config.rs (L116-116)
```rust
            shared_mempool_max_concurrent_inbound_syncs: 4,
```

**File:** mempool/src/shared_mempool/tasks.rs (L328-350)
```rust
    let start_storage_read = Instant::now();
    let state_view = smp
        .db
        .latest_state_checkpoint_view()
        .expect("Failed to get latest state checkpoint view.");

    // Track latency: fetching seq number
    let account_seq_numbers = IO_POOL.install(|| {
        transactions
            .par_iter()
            .map(|(t, _, _)| match t.replay_protector() {
                ReplayProtector::Nonce(_) => Ok(None),
                ReplayProtector::SequenceNumber(_) => {
                    get_account_sequence_number(&state_view, t.sender())
                        .map(Some)
                        .inspect_err(|e| {
                            error!(LogSchema::new(LogEntry::DBError).error(e));
                            counters::DB_ERROR.inc();
                        })
                },
            })
            .collect::<Vec<_>>()
    });
```

**File:** mempool/src/shared_mempool/tasks.rs (L486-503)
```rust
    // Track latency: VM validation
    let vm_validation_timer = counters::PROCESS_TXN_BREAKDOWN_LATENCY
        .with_label_values(&[counters::VM_VALIDATION_LABEL])
        .start_timer();
    let validation_results = VALIDATION_POOL.install(|| {
        transactions
            .par_iter()
            .map(|t| {
                let result = smp.validator.read().validate_transaction(t.0.clone());
                // Pre-compute the hash and length if the transaction is valid, before locking mempool
                if result.is_ok() {
                    t.0.committed_hash();
                    t.0.txn_bytes_len();
                }
                result
            })
            .collect::<Vec<_>>()
    });
```

**File:** aptos-node/src/network.rs (L108-123)
```rust
/// Returns the network application config for the mempool client and service
pub fn mempool_network_configuration(node_config: &NodeConfig) -> NetworkApplicationConfig {
    let direct_send_protocols = vec![ProtocolId::MempoolDirectSend];
    let rpc_protocols = vec![]; // Mempool does not use RPC

    let network_client_config =
        NetworkClientConfig::new(direct_send_protocols.clone(), rpc_protocols.clone());
    let network_service_config = NetworkServiceConfig::new(
        direct_send_protocols,
        rpc_protocols,
        aptos_channel::Config::new(node_config.mempool.max_network_channel_size)
            .queue_style(QueueStyle::KLAST) // TODO: why is this not FIFO?
            .counters(&aptos_mempool::counters::PENDING_MEMPOOL_NETWORK_EVENTS),
    );
    NetworkApplicationConfig::new(network_client_config, network_service_config)
}
```
