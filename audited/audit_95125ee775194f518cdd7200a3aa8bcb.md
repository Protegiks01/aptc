# Audit Report

## Title
Memory Leak in Health Checker Due to Dropped LostPeer Notifications and Failed Disconnect Cleanup

## Summary
The health checker's `health_check_data` HashMap accumulates orphaned peer entries indefinitely when disconnect operations fail and corresponding `LostPeer` notifications are dropped due to channel congestion. This leads to unbounded memory growth that can degrade validator node performance and eventually cause crashes.

## Finding Description

The health checker maintains peer state in the `health_check_data` HashMap, with entries added when peers connect and removed through two mechanisms: successful disconnect operations or `LostPeer` notifications. A critical vulnerability exists when both mechanisms fail simultaneously. [1](#0-0) 

The `disconnect_peer()` method only removes entries from `health_check_data` when the disconnect operation succeeds (returns `Ok`). When disconnect fails, the entry remains. [2](#0-1) 

Disconnect operations fail with `PeerManagerError::NotConnected` when the peer is already removed from `active_peers`. This occurs when another component (connectivity manager, network layer, or remote peer) disconnects the peer before the health checker attempts disconnection. [3](#0-2) 

When a peer disconnects, `LostPeer` notifications are broadcast to subscribers. However, when the notification channel is full (`TrySendError::Full`), the notification is silently dropped with only a logged warning. The channel capacity is 1000 messages. [4](#0-3) 

The health checker relies on `LostPeer` notifications to clean up peer state. When these notifications are dropped, the cleanup never occurs.

**Attack Scenario:**
1. Attacker rapidly connects and disconnects to the validator node, filling the notification channel with connection events
2. Legitimate peer disconnections occur (triggered by network issues or other components)
3. `LostPeer` notifications for these peers are dropped due to full channel
4. Health checker detects ping failures for already-disconnected peers
5. Health checker calls `disconnect_peer()`, which fails with `NotConnected` error
6. Entry remains in `health_check_data` permanently
7. Process repeats, causing unbounded memory growth

The code even acknowledges this limitation: [5](#0-4) 

## Impact Explanation

**High Severity** - This vulnerability causes validator node slowdowns and potential crashes, meeting the High severity criteria per Aptos bug bounty ($50,000):

1. **Resource Exhaustion**: Unbounded memory growth eventually exhausts available RAM
2. **Validator Node Slowdowns**: As the HashMap grows, lookups and iterations become slower, degrading health checking performance
3. **Consensus Impact**: Degraded validator performance affects consensus participation and network health
4. **Node Crashes**: Eventually, memory exhaustion causes OOM kills, forcing validator restarts

The attack requires no privileged access - any network peer can trigger rapid connects/disconnects. The impact is persistent and accumulates over time, potentially affecting all validator nodes on the network.

## Likelihood Explanation

**High Likelihood** - This vulnerability is likely to occur in production:

1. **Natural Occurrence**: Network instability, peer churn, and connectivity issues naturally cause the conditions for this bug without malicious intent
2. **Low Attack Complexity**: An attacker only needs to repeatedly connect/disconnect to fill the notification channel
3. **No Special Privileges**: Any network peer can trigger the vulnerability
4. **Persistent Effect**: Once entries are orphaned, they remain forever until node restart
5. **Production Environment**: High-traffic validator nodes with many peer connections are especially vulnerable

The 1000-message channel capacity can be filled during network events, making dropped notifications realistic in production deployments.

## Recommendation

Implement multiple defense layers:

**1. Always clean up on disconnect attempts:**
```rust
pub async fn disconnect_peer(
    &mut self,
    peer_network_id: PeerNetworkId,
    disconnect_reason: DisconnectReason,
) -> Result<(), Error> {
    let _ = self.update_connection_state(peer_network_id, ConnectionState::Disconnecting);
    let result = self
        .network_client
        .disconnect_from_peer(peer_network_id, disconnect_reason)
        .await;
    let peer_id = peer_network_id.peer_id();
    
    // Always remove from health_check_data, regardless of disconnect result
    // If peer is already disconnected (NotConnected error), we should clean up anyway
    self.health_check_data.write().remove(&peer_id);
    
    result
}
```

**2. Implement periodic garbage collection:**
```rust
// In HealthChecker::start(), add periodic cleanup
if self.round % 100 == 0 {  // Every 100 rounds
    self.network_interface.cleanup_orphaned_peers();
}

// In HealthCheckNetworkInterface
pub fn cleanup_orphaned_peers(&mut self) {
    let connected = self.network_client
        .get_peers_and_metadata()
        .get_connected_peers();
    
    self.health_check_data.write().retain(|peer_id, _| {
        connected.contains(&PeerNetworkId::new(
            self.network_context.network_id(), 
            *peer_id
        ))
    });
}
```

**3. Use bounded channels with backpressure** instead of silently dropping notifications, or increase channel capacity significantly.

## Proof of Concept

```rust
#[tokio::test]
async fn test_orphaned_health_check_entries() {
    use crate::protocols::health_checker::*;
    use crate::application::interface::NetworkClient;
    use aptos_types::PeerId;
    
    // Setup test harness
    let (mut harness, mut health_checker) = TestHarness::new_strict();
    let peer_id = PeerId::random();
    
    // Spawn health checker
    tokio::spawn(async move { health_checker.start().await });
    
    // Step 1: Peer connects - entry added to health_check_data
    harness.send_new_peer_notification(peer_id).await;
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    // Verify entry exists
    let connected = harness.get_connected_peers();
    assert!(connected.contains(&peer_id));
    
    // Step 2: Fill notification channel to simulate congestion
    for _ in 0..1000 {
        let dummy_peer = PeerId::random();
        harness.send_new_peer_notification(dummy_peer).await;
    }
    
    // Step 3: Simulate external disconnect (e.g., from PeerManager)
    // This removes peer from active_peers but LostPeer notification is dropped
    harness.simulate_external_disconnect(peer_id).await;
    
    // Step 4: Health checker detects failure and tries to disconnect
    // This fails with NotConnected because peer already removed
    harness.trigger_ping_failure(peer_id).await;
    
    // Step 5: Verify orphaned entry still exists in health_check_data
    // even though peer is disconnected
    let connected = harness.get_connected_peers();
    assert!(connected.contains(&peer_id), 
        "Orphaned entry persists in health_check_data despite disconnect");
    
    // Step 6: Verify entry is never cleaned up
    for _ in 0..100 {
        harness.trigger_ping().await;
    }
    assert!(connected.contains(&peer_id),
        "Orphaned entry accumulated indefinitely - memory leak confirmed");
}
```

## Notes

The vulnerability is explicitly acknowledged in a code comment stating "until we add GCing" but no garbage collection has been implemented. This represents a complete lack of defensive cleanup mechanisms, making the memory leak inevitable under realistic network conditions. The issue affects all Aptos validator nodes and can be triggered without any special privileges or insider access.

### Citations

**File:** network/framework/src/protocols/health_checker/interface.rs (L64-64)
```rust
    /// Note: This removes the peer outright for now until we add GCing, and historical state management
```

**File:** network/framework/src/protocols/health_checker/interface.rs (L65-81)
```rust
    pub async fn disconnect_peer(
        &mut self,
        peer_network_id: PeerNetworkId,
        disconnect_reason: DisconnectReason,
    ) -> Result<(), Error> {
        // Possibly already disconnected, but try anyways
        let _ = self.update_connection_state(peer_network_id, ConnectionState::Disconnecting);
        let result = self
            .network_client
            .disconnect_from_peer(peer_network_id, disconnect_reason)
            .await;
        let peer_id = peer_network_id.peer_id();
        if result.is_ok() {
            self.health_check_data.write().remove(&peer_id);
        }
        result
    }
```

**File:** network/framework/src/peer_manager/mod.rs (L468-504)
```rust
            ConnectionRequest::DisconnectPeer(peer_id, disconnect_reason, resp_tx) => {
                // Update the connection disconnect metrics
                counters::update_network_connection_operation_metrics(
                    &self.network_context,
                    counters::DISCONNECT_LABEL.into(),
                    disconnect_reason.get_label(),
                );

                // Send a CloseConnection request to Peer and drop the send end of the
                // PeerRequest channel.
                if let Some((conn_metadata, sender)) = self.active_peers.remove(&peer_id) {
                    let connection_id = conn_metadata.connection_id;
                    self.remove_peer_from_metadata(conn_metadata.remote_peer_id, connection_id);

                    // This triggers a disconnect.
                    drop(sender);
                    // Add to outstanding disconnect requests.
                    self.outstanding_disconnect_requests
                        .insert(connection_id, resp_tx);
                } else {
                    info!(
                        NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                        "{} Connection with peer: {} was already closed",
                        self.network_context,
                        peer_id.short_str(),
                    );
                    if let Err(err) = resp_tx.send(Err(PeerManagerError::NotConnected(peer_id))) {
                        info!(
                            NetworkSchema::new(&self.network_context),
                            error = ?err,
                            "{} Failed to notify that connection was already closed for Peer {}: {:?}",
                            self.network_context,
                            peer_id,
                            err
                        );
                    }
                }
```

**File:** network/framework/src/application/storage.rs (L371-395)
```rust
    fn broadcast(&self, event: ConnectionNotification) {
        let mut listeners = self.subscribers.lock();
        let mut to_del = vec![];
        for i in 0..listeners.len() {
            let dest = listeners.get_mut(i).unwrap();
            if let Err(err) = dest.try_send(event.clone()) {
                match err {
                    TrySendError::Full(_) => {
                        // Tried to send to an app, but the app isn't handling its messages fast enough.
                        // Drop message. Maybe increment a metrics counter?
                        sample!(
                            SampleRate::Duration(Duration::from_secs(1)),
                            warn!("PeersAndMetadata.broadcast() failed, some app is slow"),
                        );
                    },
                    TrySendError::Closed(_) => {
                        to_del.push(i);
                    },
                }
            }
        }
        for evict in to_del.into_iter() {
            listeners.swap_remove(evict);
        }
    }
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L209-227)
```rust
                conn_event = connection_events.select_next_some() => {
                    match conn_event {
                        ConnectionNotification::NewPeer(metadata, network_id) => {
                            // PeersAndMetadata is a global singleton across all networks; filter connect/disconnect events to the NetworkId that this HealthChecker instance is watching
                            if network_id == self_network_id {
                                self.network_interface.create_peer_and_health_data(
                                    metadata.remote_peer_id, self.round
                                );
                            }
                        }
                        ConnectionNotification::LostPeer(metadata, network_id) => {
                            // PeersAndMetadata is a global singleton across all networks; filter connect/disconnect events to the NetworkId that this HealthChecker instance is watching
                            if network_id == self_network_id {
                                self.network_interface.remove_peer_and_health_data(
                                    &metadata.remote_peer_id
                                );
                            }
                        }
                    }
```
