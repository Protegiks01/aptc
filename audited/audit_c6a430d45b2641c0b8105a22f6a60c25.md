# Audit Report

## Title
Incorrect LedgerPrunerProgress Initialization Causes Unintended Data Deletion During Sub-Pruner Catch-Up

## Summary
When `LedgerPrunerProgress` is missing from the database, the fallback initialization logic in `LedgerMetadataPruner::new()` can set it to an incorrectly high version by using the first available `VersionData` checkpoint. This causes sub-pruners to incorrectly "catch up" by deleting legitimate ledger data between their stored progress and the incorrectly initialized metadata progress.

## Finding Description

The vulnerability occurs when `LedgerPrunerProgress` is absent from the database metadata. The fallback logic seeks to the first entry in `VersionDataSchema` to initialize the progress counter: [1](#0-0) 

The developer comment acknowledges uncertainty about whether all databases have this key, implementing a fallback path for "super old" databases. [2](#0-1) 

After previous pruning operations have deleted old `VersionData` entries [3](#0-2) , the first remaining entry may be at a version significantly higher than where pruning actually left off. When the `LedgerPruner` initializes, it retrieves this `metadata_progress` and passes it to all sub-pruners during their construction: [4](#0-3) 

Each sub-pruner then attempts to "catch up" by pruning from its stored progress to the metadata progress. The `EventStorePruner` demonstrates this pattern: [5](#0-4) 

The same catch-up pattern is used by all sub-pruners including `TransactionInfoPruner`: [6](#0-5) 

The `get_or_initialize_subpruner_progress` function returns the existing sub-pruner progress if present, but the catch-up call still uses the incorrectly high `metadata_progress` as the target: [7](#0-6) 

**Exploitation Scenario:**
1. Database at version 10,000,000, previously pruned up to version 9,500,000
2. All sub-pruner progress keys correctly at 9,500,000 (EventPrunerProgress, TransactionPrunerProgress, etc.)
3. `LedgerPrunerProgress` is lost due to selective backup/restore, migration bug, or metadata corruption
4. `VersionData` entries below 9,500,000 were already deleted during previous pruning
5. First remaining `VersionData` entry is at version 9,550,000
6. On node restart:
   - `LedgerMetadataPruner` initializes `LedgerPrunerProgress` to 9,550,000 (incorrect)
   - `EventStorePruner` reads its progress as 9,500,000 but receives metadata_progress of 9,550,000
   - Calls `prune(9_500_000, 9_550_000)` to "catch up"
   - Deletes events from versions 9,500,000 to 9,549,999
7. All sub-pruners repeat this deletion for their respective data (transactions, write sets, transaction info, etc.)

This violates the **State Consistency** invariant by creating permanent gaps in the ledger history where data should exist but has been incorrectly deleted.

## Impact Explanation

**Severity: HIGH**

This qualifies as "Significant protocol violations" and "State inconsistencies requiring intervention" under the Aptos bug bounty program:

- **Permanent Data Loss**: Critical ledger data (transactions, events, transaction info, write sets, auxiliary data) is irreversibly deleted from potentially tens of thousands of versions
- **Historical Query Failures**: The node cannot serve queries for the deleted version range, breaking API contracts
- **Consensus Impact**: While this doesn't break consensus agreement on new blocks, it creates ledger inconsistencies between nodes that experienced the bug and those that didn't
- **Recovery Cost**: Affected nodes must re-sync from genesis or restore from backup to recover the deleted data, causing significant operational disruption

The gap between sparse `VersionData` entries can be substantial, potentially causing deletion of tens of thousands of versions in realistic scenarios.

## Likelihood Explanation

**Likelihood: MEDIUM**

This vulnerability can be triggered by legitimate operational scenarios:

1. **Database Migration**: When migrating from old database versions that predate the `LedgerPrunerProgress` feature (as acknowledged in the developer comment)
2. **Selective Restore Operations**: Backup/restore procedures that don't preserve all metadata keys consistently, particularly if different column families are restored independently
3. **Metadata Corruption**: Database corruption affecting specifically the metadata column family where `LedgerPrunerProgress` is stored
4. **Manual Database Operations**: Operators performing maintenance may inadvertently delete or fail to migrate specific metadata keys

While the fast sync path properly saves `LedgerPrunerProgress` [8](#0-7)  via [9](#0-8) , the vulnerability can still be triggered through the other operational scenarios listed above.

## Recommendation

Add validation logic to detect when the fallback initialization produces a suspiciously high value compared to sub-pruner progress keys. The fix should:

1. Check all sub-pruner progress keys before initializing `LedgerPrunerProgress`
2. If any sub-pruner progress exists and is lower than the first `VersionData` entry, use the minimum sub-pruner progress instead
3. Log a warning when this scenario is detected for operator awareness
4. Consider initializing to 0 if the first `VersionData` entry significantly exceeds any existing sub-pruner progress

Example fix:
```rust
// In LedgerMetadataPruner::new(), before initializing from VersionData:
let min_subpruner_progress = check_all_subpruner_progress_keys(ledger_metadata_db);
if let Some(min_progress) = min_subpruner_progress {
    if version > min_progress + REASONABLE_THRESHOLD {
        warn!("First VersionData entry ({}) significantly exceeds sub-pruner progress ({}), using sub-pruner progress to prevent data loss", version, min_progress);
        version = min_progress;
    }
}
```

## Proof of Concept

A proof of concept would require:
1. Creating a database with pruned state up to version 9,500,000
2. Deleting the `LedgerPrunerProgress` metadata key
3. Ensuring sub-pruner progress keys remain at 9,500,000
4. Ensuring no `VersionData` entries exist below 9,550,000
5. Restarting the node and observing the unintended deletion

Due to the complexity of setting up this exact database state, a full PoC would require significant infrastructure. However, the code path is clear from the citations provided, and the vulnerability logic is straightforward to verify through code inspection.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_metadata_pruner.rs (L24-36)
```rust
        } else {
            // NOTE: I **think** all db should have the LedgerPrunerProgress. Have a fallback path
            // here in case the database was super old before we introducing this progress counter.
            let mut iter = ledger_metadata_db.iter::<VersionDataSchema>()?;
            iter.seek_to_first();
            let version = match iter.next().transpose()? {
                Some((version, _)) => version,
                None => 0,
            };
            ledger_metadata_db.put::<DbMetadataSchema>(
                &DbMetadataKey::LedgerPrunerProgress,
                &DbMetadataValue::Version(version),
            )?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_metadata_pruner.rs (L48-49)
```rust
        for version in current_progress..target_version {
            batch.delete::<VersionDataSchema>(&version)?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L129-170)
```rust
        let metadata_progress = ledger_metadata_pruner.progress()?;

        info!(
            metadata_progress = metadata_progress,
            "Created ledger metadata pruner, start catching up all sub pruners."
        );

        let transaction_store = Arc::new(TransactionStore::new(Arc::clone(&ledger_db)));

        let event_store_pruner = Box::new(EventStorePruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
            internal_indexer_db.clone(),
        )?);
        let persisted_auxiliary_info_pruner = Box::new(PersistedAuxiliaryInfoPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);
        let transaction_accumulator_pruner = Box::new(TransactionAccumulatorPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);

        let transaction_auxiliary_data_pruner = Box::new(TransactionAuxiliaryDataPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);

        let transaction_info_pruner = Box::new(TransactionInfoPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);
        let transaction_pruner = Box::new(TransactionPruner::new(
            Arc::clone(&transaction_store),
            Arc::clone(&ledger_db),
            metadata_progress,
            internal_indexer_db,
        )?);
        let write_set_pruner = Box::new(WriteSetPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L90-106)
```rust
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.event_db_raw(),
            &DbMetadataKey::EventPrunerProgress,
            metadata_progress,
        )?;

        let myself = EventStorePruner {
            ledger_db,
            internal_indexer_db,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up EventStorePruner."
        );
        myself.prune(progress, metadata_progress)?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_info_pruner.rs (L41-54)
```rust
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.transaction_info_db_raw(),
            &DbMetadataKey::TransactionInfoPrunerProgress,
            metadata_progress,
        )?;

        let myself = TransactionInfoPruner { ledger_db };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up TransactionInfoPruner."
        );
        myself.prune(progress, metadata_progress)?;
```

**File:** storage/aptosdb/src/pruner/pruner_utils.rs (L44-59)
```rust
pub(crate) fn get_or_initialize_subpruner_progress(
    sub_db: &DB,
    progress_key: &DbMetadataKey,
    metadata_progress: Version,
) -> Result<Version> {
    Ok(
        if let Some(v) = sub_db.get::<DbMetadataSchema>(progress_key)? {
            v.expect_version()
        } else {
            sub_db.put::<DbMetadataSchema>(
                progress_key,
                &DbMetadataValue::Version(metadata_progress),
            )?;
            metadata_progress
        },
    )
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L80-89)
```rust
    fn save_min_readable_version(&self, min_readable_version: Version) -> Result<()> {
        self.min_readable_version
            .store(min_readable_version, Ordering::SeqCst);

        PRUNER_VERSIONS
            .with_label_values(&["ledger_pruner", "min_readable"])
            .set(min_readable_version as i64);

        self.ledger_db.write_pruner_progress(min_readable_version)
    }
```

**File:** storage/aptosdb/src/ledger_db/ledger_metadata_db.rs (L57-62)
```rust
    pub(super) fn write_pruner_progress(&self, version: Version) -> Result<()> {
        self.db.put::<DbMetadataSchema>(
            &DbMetadataKey::LedgerPrunerProgress,
            &DbMetadataValue::Version(version),
        )
    }
```
