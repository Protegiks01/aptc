# Audit Report

## Title
Staging Area Pollution Leading to State Inconsistency and Denial of Service in Large Package Publishing

## Summary
The chunked package publishing mechanism in `large_packages.move` does not enforce cleanup of the staging area before starting a new publish operation. When previous `stage_code_chunk` transactions leave residual data in the `StagingArea` resource, subsequent publish operations incorrectly combine old chunks with new chunks, leading to state inconsistencies that prevent successful package publication until manual cleanup is performed.

## Finding Description

The vulnerability exists in the Move smart contract implementation of the large packages framework. The `stage_code_chunk_internal` function reuses existing `StagingArea` resources without validating or clearing previous data. [1](#0-0) 

When a `StagingArea` already exists, the function directly borrows and mutates it, appending new data to existing data: [2](#0-1) 

For code chunks, the implementation appends new chunks to existing chunks at the same module index: [3](#0-2) 

Critically, `last_module_idx` only increases and never decreases, meaning residual modules from failed publish attempts persist: [4](#0-3) 

When assembling the final package, the function loops from 0 to `last_module_idx`, including all residual modules: [5](#0-4) 

**Attack Scenario:**
1. User/attacker uploads Package A with modules at indices [0, 1, 2] via multiple `stage_code_chunk` calls
2. The final `stage_code_chunk_and_publish_to_account` transaction fails (e.g., incompatible upgrade, gas limit, or intentional abort)
3. `StagingArea` now contains: `metadata_A`, `code[0]=module_A0`, `code[1]=module_A1`, `code[2]=module_A2`, `last_module_idx=2`
4. User attempts to upload Package B with modules at indices [0, 1] only
5. New chunks append to existing: `code[0]=module_A0+module_B0`, `code[1]=module_A1+module_B1`, `code[2]=module_A2` (unchanged)
6. `metadata_serialized` becomes: `metadata_A + metadata_B` (invalid concatenation)
7. When `assemble_module_code` executes, it collects modules 0, 1, and 2 with corrupted bytecode
8. The publish transaction fails with invalid bytecode/metadata, leaving the staging area further polluted
9. User cannot successfully publish until manually calling `cleanup_staging_area`

This breaks the **State Consistency** invariant - the `StagingArea` resource enters an inconsistent state that prevents legitimate operations without manual intervention.

## Impact Explanation

This vulnerability qualifies as **MEDIUM severity** under the Aptos bug bounty criteria:

- **State inconsistencies requiring intervention**: The polluted `StagingArea` resource prevents subsequent publish operations from succeeding until manual cleanup via `cleanup_staging_area` is performed. [6](#0-5) 

- **Denial of Service**: Users whose chunked publish operations fail partway through (due to gas exhaustion, compatibility errors, or other failures) are blocked from publishing new packages until they discover and execute the cleanup procedure.

- **No direct fund loss**: The corrupted bytecode will fail Move bytecode verification, preventing publication of malformed code, so this does not escalate to critical severity.

While the Aptos CLI provides a warning when detecting non-empty staging areas [7](#0-6) , this protection only exists at the CLI layer and can be bypassed by users of the REST API or other SDKs that interact directly with the Move contract.

## Likelihood Explanation

This vulnerability has **HIGH likelihood** of occurrence:

1. **Common trigger conditions**: Any failed chunked publish operation leaves the staging area polluted. Failures can occur due to:
   - Incompatible module upgrades [8](#0-7) 
   - Immutable package upgrade attempts [9](#0-8) 
   - Gas exhaustion during multi-transaction sequences
   - User abandonment of publish operations

2. **No automatic cleanup on failure**: The `cleanup_staging_area` call only executes after successful completion of publish operations [10](#0-9) , leaving residual data on any failure.

3. **Discovery difficulty**: Users may not be aware of the `cleanup_staging_area` command, especially when using non-CLI tools.

4. **Bypass of CLI warnings**: The CLI warning can be dismissed by users, and developers using APIs directly have no such protection.

## Recommendation

Implement one of the following fixes at the Move contract level:

**Option 1: Enforce cleanup before new staging** (Recommended)
Add a check in `stage_code_chunk_internal` that aborts if a `StagingArea` already exists when starting a new publish operation (indicated by the first call with empty previous state):

```move
inline fun stage_code_chunk_internal(
    owner: &signer,
    metadata_chunk: vector<u8>,
    code_indices: vector<u16>,
    code_chunks: vector<vector<u8>>
): &mut StagingArea {
    assert!(
        vector::length(&code_indices) == vector::length(&code_chunks),
        error::invalid_argument(ECODE_MISMATCH)
    );

    let owner_address = signer::address_of(owner);
    
    // If starting fresh metadata and StagingArea exists with data, require cleanup first
    if (exists<StagingArea>(owner_address) && !vector::is_empty(&metadata_chunk)) {
        let staging_area = borrow_global<StagingArea>(owner_address);
        assert!(
            vector::is_empty(&staging_area.metadata_serialized),
            error::invalid_state(ESTAGING_AREA_NOT_CLEAN)
        );
    };
    
    // ... rest of function
}
```

**Option 2: Add explicit reset function**
Provide a `reset_staging_area` function that clears the staging area in place without destroying the resource, and require its call before starting new publishes.

**Option 3: Add transaction sequence tracking**
Include a nonce or session ID in the staging area that must match across all chunk submissions, rejecting chunks from different sessions.

## Proof of Concept

```move
#[test_only]
module aptos_experimental::test_staging_pollution {
    use std::signer;
    use std::vector;
    use aptos_experimental::large_packages;

    #[test(user = @0xcafe)]
    fun test_staging_area_pollution(user: &signer) {
        // Step 1: Upload Package A with 3 modules
        large_packages::stage_code_chunk(
            user,
            b"metadata_A_part1",
            vector[0u16, 1u16],
            vector[b"module_0_chunk1", b"module_1_chunk1"]
        );
        
        large_packages::stage_code_chunk(
            user,
            b"metadata_A_part2",
            vector[2u16],
            vector[b"module_2_complete"]
        );
        
        // Step 2: Simulate failed publish by NOT calling final publish function
        // In practice, this would be a transaction that aborted or failed
        
        // Step 3: Attempt to upload Package B with only 2 modules
        large_packages::stage_code_chunk(
            user,
            b"metadata_B_part1",  // This appends to metadata_A
            vector[0u16, 1u16],
            vector[b"module_0_new", b"module_1_new"]  // These append to existing chunks
        );
        
        // Step 4: Try to publish - this will fail because:
        // - metadata is corrupted (metadata_A + metadata_B)
        // - code[0] = "module_0_chunk1" + "module_0_new" (invalid bytecode)
        // - code[1] = "module_1_chunk1" + "module_1_new" (invalid bytecode)
        // - code[2] = "module_2_complete" (unexpected extra module)
        // The assemble_module_code will collect all 3 modules with corrupted data
        
        // To recover, user must call:
        // large_packages::cleanup_staging_area(user);
    }
}
```

The proof of concept demonstrates that once a staging area is polluted, subsequent chunk submissions combine old and new data, preventing successful package publication until manual cleanup.

**Notes**

The vulnerability exists at the Move smart contract level and affects all users of the large packages framework, regardless of which client tool they use. While the Aptos CLI provides a warning mechanism [7](#0-6) , this is a client-side mitigation that doesn't prevent the underlying contract-level issue. Users calling the Move functions directly through the REST API or other SDKs have no protection. The contract should enforce staging area cleanup requirements to prevent state inconsistencies, rather than relying on client-side validation.

### Citations

**File:** aptos-move/framework/aptos-experimental/sources/large_packages.move (L80-95)
```text
    public entry fun stage_code_chunk_and_publish_to_account(
        owner: &signer,
        metadata_chunk: vector<u8>,
        code_indices: vector<u16>,
        code_chunks: vector<vector<u8>>
    ) acquires StagingArea {
        let staging_area =
            stage_code_chunk_internal(
                owner,
                metadata_chunk,
                code_indices,
                code_chunks
            );
        publish_to_account(owner, staging_area);
        cleanup_staging_area(owner);
    }
```

**File:** aptos-move/framework/aptos-experimental/sources/large_packages.move (L145-154)
```text
        if (!exists<StagingArea>(owner_address)) {
            move_to(
                owner,
                StagingArea {
                    metadata_serialized: vector[],
                    code: smart_table::new(),
                    last_module_idx: 0
                }
            );
        };
```

**File:** aptos-move/framework/aptos-experimental/sources/large_packages.move (L156-160)
```text
        let staging_area = borrow_global_mut<StagingArea>(owner_address);

        if (!vector::is_empty(&metadata_chunk)) {
            vector::append(&mut staging_area.metadata_serialized, metadata_chunk);
        };
```

**File:** aptos-move/framework/aptos-experimental/sources/large_packages.move (L167-176)
```text
            if (smart_table::contains(&staging_area.code, idx)) {
                vector::append(
                    smart_table::borrow_mut(&mut staging_area.code, idx), inner_code
                );
            } else {
                smart_table::add(&mut staging_area.code, idx, inner_code);
                if (idx > staging_area.last_module_idx) {
                    staging_area.last_module_idx = idx;
                }
            };
```

**File:** aptos-move/framework/aptos-experimental/sources/large_packages.move (L213-225)
```text
    inline fun assemble_module_code(staging_area: &mut StagingArea): vector<vector<u8>> {
        let last_module_idx = staging_area.last_module_idx;
        let code = vector[];
        let i = 0;
        while (i <= last_module_idx) {
            vector::push_back(
                &mut code,
                *smart_table::borrow(&staging_area.code, i)
            );
            i = i + 1;
        };
        code
    }
```

**File:** aptos-move/framework/aptos-experimental/sources/large_packages.move (L227-231)
```text
    public entry fun cleanup_staging_area(owner: &signer) acquires StagingArea {
        let StagingArea { metadata_serialized: _, code, last_module_idx: _ } =
            move_from<StagingArea>(signer::address_of(owner));
        smart_table::destroy(code);
    }
```

**File:** crates/aptos/src/move_tool/mod.rs (L1704-1714)
```rust
    if !is_staging_area_empty(txn_options, large_packages_module_address).await? {
        let message = format!(
            "The resource {}::large_packages::StagingArea under account {} is not empty.\
        \nThis may cause package publishing to fail if the data is unexpected. \
        \nUse the `aptos move clear-staging-area` command to clean up the `StagingArea` resource under the account.",
            large_packages_module_address, account_address,
        )
            .bold();
        println!("{}", message);
        prompt_yes_with_override("Do you want to proceed?", txn_options.prompt_options)?;
    }
```

**File:** aptos-move/e2e-move-tests/src/tests/large_package_publishing.rs (L244-261)
```rust
    // Upgrade to incompatible version should fail
    // Staging metadata and code should pass, and the final publishing transaction should fail
    let mut tx_statuses = context.publish_large_package(
        &acc,
        &common::test_dir_path("large_package_publishing.data/large_pack_upgrade_incompat"),
        |_| {},
        PublishType::AccountDeploy,
    );

    let last_tx_status = tx_statuses.pop().unwrap(); // transaction for publishing

    for tx_status in tx_statuses.into_iter() {
        assert_success!(tx_status);
    }
    assert_vm_status!(
        last_tx_status,
        StatusCode::BACKWARD_INCOMPATIBLE_MODULE_UPDATE
    );
```

**File:** aptos-move/e2e-move-tests/src/tests/large_package_publishing.rs (L281-297)
```rust
    // Upgrading immutable package should fail
    // Staging metadata and code should pass, and the final publishing transaction should fail
    let mut tx_statuses = context.publish_large_package(
        &acc,
        &common::test_dir_path("../../../move-examples/large_packages/large_package_example"),
        |_| {},
        PublishType::AccountDeploy,
    );
    let last_tx_status = tx_statuses.pop().unwrap(); // transaction for publishing
    for tx_status in tx_statuses.into_iter() {
        assert_success!(tx_status);
    }
    let abort_info = Some(AbortInfo {
        reason_name: "EUPGRADE_IMMUTABLE".to_string(),
        description: "Cannot upgrade an immutable package".to_string(),
    });
    assert_move_abort!(last_tx_status, abort_info);
```
