# Audit Report

## Title
SSRF via Blacklist Bypass in NFT Metadata Crawler Using Alternative IP Address Representations

## Summary
The NFT metadata crawler's URI blacklist can be bypassed using alternative IP address representations (decimal, hexadecimal, octal, IPv6), allowing Server-Side Request Forgery (SSRF) attacks against internal network resources including localhost services and cloud metadata endpoints (AWS/GCP/Azure).

## Finding Description

The NFT metadata crawler implements a blacklist-based security control to prevent fetching malicious URIs. However, the blacklist check uses simple substring matching on raw URI strings, while the HTTP client (reqwest) accepts multiple valid IP address representations that bypass this check. [1](#0-0) 

The vulnerability exists in this execution flow:

1. **Blacklist check** - Checks raw URI string for blacklisted substrings [2](#0-1) 

2. **URI transformation** - URIParser.parse() processes the URI (transforms IPFS URIs, returns error for others) [3](#0-2) 

3. **HTTP request** - JSONParser.parse() makes HTTP request to the URI [4](#0-3) 

The HTTP client receives the URI and makes requests via reqwest: [5](#0-4) 

**Attack Vectors:**
- Decimal: `http://2852039166/` → resolves to `169.254.169.254` (AWS metadata endpoint)
- Hexadecimal: `http://0xa9fea9fe/` → resolves to `169.254.169.254`
- Octal: `http://0251.0376.0251.0376/` → resolves to `169.254.169.254`
- IPv6: `http://[::1]/` → resolves to localhost
- Short form: `http://127.1/` → resolves to `127.0.0.1`

The blacklist defaults to an empty vector and relies on manual configuration: [6](#0-5) 

Even if configured with common values like `["localhost", "127.0.0.1", "169.254.169.254"]`, the substring matching fails to detect alternative representations:
- `"http://2852039166/".contains("169.254.169.254")` → returns `false`
- But reqwest resolves `2852039166` as the decimal representation of IP `169.254.169.254`

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program criteria:

**Primary Impact - API Crashes:** The crawler service could crash when attempting to fetch from internal services that don't respond appropriately, or when timeouts occur repeatedly on internal network calls.

**Secondary Impacts:**
- **Cloud Metadata Access:** Attackers can access AWS EC2 metadata (`http://169.254.169.254/latest/meta-data/iam/security-credentials/`), GCP metadata, or Azure instance metadata to steal IAM credentials and cloud service tokens
- **Internal Service Enumeration:** Access to localhost services and internal network resources for reconnaissance
- **Potential Credential Theft:** If the crawler runs with cloud IAM roles, stolen credentials could escalate to broader infrastructure compromise
- **Service Disruption:** Repeated SSRF attempts could degrade crawler performance and availability

While the NFT metadata crawler is an ecosystem service rather than a core consensus component, SSRF vulnerabilities against cloud infrastructure represent significant security risks, particularly in production deployments where the crawler may share infrastructure with other critical Aptos services.

## Likelihood Explanation

**Likelihood: High**

**Attacker Requirements:**
- Ability to create NFTs on Aptos (open to any user)
- Knowledge of alternative IP address representations (publicly documented)
- No special permissions or authentication required

**Attack Complexity:**
- Very low - simply mint an NFT with `asset_uri = "http://2852039166/latest/meta-data/iam/security-credentials/"`
- The crawler automatically processes new NFT metadata
- Attack executes immediately when the crawler processes the malicious NFT

**Exploitability:**
The attack is trivial to execute. Alternative IP representations are well-documented SSRF bypass techniques. Any attacker with basic knowledge of SSRF can exploit this by:
1. Creating an NFT with a crafted asset_uri
2. Waiting for the crawler to process it
3. The crawler makes SSRF requests automatically

## Recommendation

**Immediate Fix:** Implement proper URL normalization and IP address validation before the blacklist check.

**Recommended Code Changes:**

```rust
use std::net::IpAddr;
use url::Url;

fn is_blacklisted_uri(&mut self, uri: &str) -> bool {
    // Check raw URI against blacklist
    if self.parser_config.uri_blacklist.iter()
        .any(|blacklist_uri| uri.contains(blacklist_uri)) {
        return true;
    }
    
    // Parse and normalize URL to check for alternative IP representations
    if let Ok(parsed_url) = Url::parse(uri) {
        if let Some(host) = parsed_url.host_str() {
            // Try to parse as IP address
            if let Ok(ip_addr) = host.parse::<IpAddr>() {
                // Check if IP is in blacklisted ranges
                if ip_addr.is_loopback() || 
                   ip_addr.is_link_local() ||
                   is_private_ip(&ip_addr) ||
                   is_metadata_endpoint(&ip_addr) {
                    return true;
                }
            }
            
            // Also check normalized hostname against blacklist
            let normalized_host = host.to_lowercase();
            if self.parser_config.uri_blacklist.iter()
                .any(|blacklist| normalized_host.contains(blacklist)) {
                return true;
            }
        }
    }
    
    false
}

fn is_private_ip(ip: &IpAddr) -> bool {
    match ip {
        IpAddr::V4(ipv4) => {
            ipv4.is_private() || ipv4.is_loopback()
        },
        IpAddr::V6(ipv6) => {
            ipv6.is_loopback() || 
            matches!(ipv6.segments(), [0xfd00..=0xfdff, ..])  // ULA
        }
    }
}

fn is_metadata_endpoint(ip: &IpAddr) -> bool {
    match ip {
        IpAddr::V4(ipv4) => {
            // AWS/GCP/Azure metadata endpoint
            ipv4.octets() == [169, 254, 169, 254]
        },
        _ => false
    }
}
```

**Additional Recommendations:**
1. Implement a whitelist approach instead of blacklist for allowed URI schemes (only allow `https://` for specific trusted domains)
2. Use a dedicated SSRF protection library that handles all edge cases
3. Run the crawler in a sandboxed environment with no access to internal networks
4. Implement network-level controls to prevent outbound connections to private IP ranges
5. Add comprehensive logging and monitoring for URI fetch attempts

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_blacklist_bypass_decimal_ip() {
        // 2852039166 in decimal = 169.254.169.254 (AWS metadata endpoint)
        let malicious_uri = "http://2852039166/latest/meta-data/iam/security-credentials/";
        
        // Simulate blacklist check
        let blacklist = vec![
            "localhost".to_string(),
            "127.0.0.1".to_string(),
            "169.254.169.254".to_string(),
        ];
        
        // Current implementation - substring matching
        let is_blocked = blacklist.iter()
            .any(|blacklist_uri| malicious_uri.contains(blacklist_uri));
        
        // This assertion demonstrates the bypass
        assert_eq!(is_blocked, false, "Decimal IP representation bypasses blacklist");
        
        // Proof that this resolves to the metadata endpoint
        // When reqwest processes this URL, it will connect to 169.254.169.254
        // This can be verified by attempting the request (would succeed if running on EC2)
    }
    
    #[test]
    fn test_blacklist_bypass_hex_ip() {
        // 0xa9fea9fe in hex = 169.254.169.254
        let malicious_uri = "http://0xa9fea9fe/latest/meta-data/";
        
        let blacklist = vec!["169.254.169.254".to_string()];
        let is_blocked = blacklist.iter()
            .any(|blacklist_uri| malicious_uri.contains(blacklist_uri));
        
        assert_eq!(is_blocked, false, "Hex IP representation bypasses blacklist");
    }
    
    #[test]
    fn test_blacklist_bypass_localhost_shorthand() {
        // 127.1 expands to 127.0.0.1
        let malicious_uri = "http://127.1:8080/admin";
        
        let blacklist = vec!["localhost".to_string(), "127.0.0.1".to_string()];
        let is_blocked = blacklist.iter()
            .any(|blacklist_uri| malicious_uri.contains(blacklist_uri));
        
        assert_eq!(is_blocked, false, "Short-form IP bypasses blacklist");
    }
}
```

## Notes

**Clarification on "Before Blacklist Checks":**
The original security question asks if SSRF can occur "before blacklist checks." In the actual code flow, the blacklist check occurs at line 94, and HTTP requests occur later at line 127 and beyond. Therefore, SSRF does not happen *before* the blacklist check in terms of execution order.

However, the critical vulnerability is that the blacklist check can be **bypassed** entirely using alternative IP address representations. The blacklist operates on raw string matching, while HTTP clients interpret multiple valid IP formats, creating a semantic gap that enables SSRF despite the presence of blacklist checks.

The vulnerability affects the NFT metadata crawler ecosystem service. While not directly part of core consensus/execution components, it represents a significant security risk in production deployments where the service may access internal infrastructure or run with cloud IAM credentials.

### Citations

**File:** ecosystem/nft-metadata-crawler/src/parser/worker.rs (L93-100)
```rust
        // Check asset_uri against the URI blacklist
        if self.is_blacklisted_uri(&self.asset_uri.clone()) {
            self.log_info("Found match in URI blacklist, marking as do_not_parse");
            self.model.set_do_not_parse(true);
            self.upsert();
            SKIP_URI_COUNT.with_label_values(&["blacklist"]).inc();
            return Ok(());
        }
```

**File:** ecosystem/nft-metadata-crawler/src/parser/worker.rs (L113-122)
```rust
            let json_uri = URIParser::parse(
                &self.parser_config.ipfs_prefix,
                &self.model.get_asset_uri(),
                self.parser_config.ipfs_auth_key.as_deref(),
            )
            .unwrap_or_else(|_| {
                self.log_warn("Failed to parse asset_uri", None);
                PARSE_URI_TYPE_COUNT.with_label_values(&["other"]).inc();
                self.model.get_asset_uri()
            });
```

**File:** ecosystem/nft-metadata-crawler/src/parser/worker.rs (L126-134)
```rust
            let (raw_image_uri, raw_animation_uri, json) =
                JSONParser::parse(json_uri, self.parser_config.max_file_size_bytes)
                    .await
                    .unwrap_or_else(|e| {
                        // Increment retry count if JSON parsing fails
                        self.log_warn("JSON parsing failed", Some(&e));
                        self.model.increment_json_parser_retry_count();
                        (None, None, Value::Null)
                    });
```

**File:** ecosystem/nft-metadata-crawler/src/parser/worker.rs (L386-391)
```rust
    fn is_blacklisted_uri(&mut self, uri: &str) -> bool {
        self.parser_config
            .uri_blacklist
            .iter()
            .any(|blacklist_uri| uri.contains(blacklist_uri))
    }
```

**File:** ecosystem/nft-metadata-crawler/src/lib.rs (L17-23)
```rust
pub async fn get_uri_metadata(url: &str) -> anyhow::Result<(String, u32)> {
    let client = Client::builder()
        .timeout(Duration::from_secs(MAX_HEAD_REQUEST_RETRY_SECONDS))
        .build()
        .context("Failed to build reqwest client")?;
    let request = client.head(url.trim());
    let response = request.send().await?;
```

**File:** ecosystem/nft-metadata-crawler/src/parser/config.rs (L26-29)
```rust
    #[serde(default)]
    pub ack_parsed_uris: bool,
    #[serde(default)]
    pub uri_blacklist: Vec<String>,
```
