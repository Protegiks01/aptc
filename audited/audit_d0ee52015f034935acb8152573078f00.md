# Audit Report

## Title
Critical Section Violation in BlockExecutor: Early Return Without finish_execution() Can Corrupt Shared State

## Summary
The `execute()` and `execute_v2()` functions in the block executor contain multiple error return paths that can exit the critical section without calling `finish_execution()`, violating the documented critical section constraint and potentially causing consensus divergence and state corruption.

## Finding Description

The block executor implements parallel transaction execution using multi-version concurrency control. The critical section constraint is explicitly documented: [1](#0-0) 

And similarly for BlockSTMv2: [2](#0-1) 

However, both functions violate this constraint through multiple early return paths:

**In execute() (BlockSTMv1):**
1. Line 593-600: `process_delayed_field_output()` modifies `versioned_cache.delayed_fields()`
2. Lines 607-675: `apply_updates()` modifies `versioned_cache.data()` and `versioned_cache.group_data()`
3. Lines 678-699: Additional removal operations modify versioned_cache
4. **Line 701-707**: `last_input_output.record()` can fail with `?`, returning early
5. **Line 708-714**: `finish_execution()` is never reached if step 4 fails [3](#0-2) 

**In execute_v2() (BlockSTMv2):**
1. Lines 444-451: `process_delayed_field_output()` modifies versioned_cache
2. Lines 452-467: `process_resource_group_output_v2()` and `process_resource_output_v2()` modify versioned_cache
3. Lines 469-497: Aggregator v1 operations modify versioned_cache
4. **Line 474**: `before_materialization()` can fail with `?` after modifications
5. **Line 499-505**: `record()` can fail with `?` after all modifications
6. **Line 512**: `finish_execution()` is never reached if steps 4-5 fail [4](#0-3) 

**Within process_delayed_field_output():**
The function can fail mid-loop, leaving partial modifications: [5](#0-4) 

If `record_change()` fails with CodeInvariantError (lines 364-368) after some delayed fields have been recorded, or if `remove()` fails (line 383), the function returns early, leaving versioned_cache in an inconsistent state.

**Consequences of skipping finish_execution():**

The scheduler's `finish_execution()` performs critical state updates: [6](#0-5) 

Without calling this:
1. Transaction status remains "Executing" instead of transitioning to "Executed"
2. Dependent transactions waiting on this transaction are never woken up (line 569)
3. Validation tasks are not scheduled (lines 586-590)
4. The versioned_cache contains partial, uncommitted modifications

**Exploitation via Concurrent Halt:**

The ExecutionHalted mechanism documents this exact scenario: [7](#0-6) 

When execution is halted (e.g., due to block gas limit exceeded), transactions in "Executing" state are marked as halted with a flag that must be reset by finish_execution(). The race condition:

1. Validator processes a block with transactions approaching gas limit
2. Transaction N starts executing and enters the critical section
3. Concurrent thread halts execution after detecting gas limit exceeded
4. Transaction N's status is set to ExecutionHalted(false) because it's Executing
5. Transaction N encounters an error (e.g., `record()` fails due to race condition with materialization or internal state)
6. Transaction N returns early via `?` without calling finish_execution()
7. The versioned_cache contains partial modifications from transaction N
8. Block epilogue transaction may execute at the same index
9. Different validators may process this race differently, causing consensus divergence

## Impact Explanation

This vulnerability meets **Critical Severity** criteria:

**Consensus Safety Violation**: Different validators experiencing this race condition at different times will have divergent multi-version cache states, leading to different execution results for subsequent transactions. This breaks the fundamental invariant that all validators must produce identical state roots for identical blocks.

**State Corruption**: The versioned_cache (MVHashMap) is shared across all parallel execution threads. Partial modifications without proper cleanup violate its consistency invariants, potentially causing:
- Incorrect reads by subsequent transactions
- Validation failures due to corrupted read-sets
- Deadlock if dependent transactions wait forever for the stuck transaction

**Non-Deterministic Execution**: The race condition between execution and halting is timing-dependent, causing non-deterministic behavior across validators.

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability requires specific conditions but they occur regularly:
1. Block gas limits are checked on every transaction commit
2. Block cutting happens frequently under load
3. The critical section spans significant work (delayed fields, resources, groups, aggregators)
4. Multiple error paths exist that can trigger early returns
5. Concurrent halting can occur at any point during the critical section

While the errors are marked as "code invariant errors," the explicit warning comments and ExecutionHalted documentation indicate this is a known race condition concern, not just a theoretical bug.

## Recommendation

**Implement a guard pattern to enforce the critical section constraint:**

```rust
struct CriticalSectionGuard<'a> {
    txn_idx: TxnIndex,
    incarnation: Incarnation,
    scheduler: Option<&'a Scheduler>,
    finished: bool,
}

impl<'a> CriticalSectionGuard<'a> {
    fn new(txn_idx: TxnIndex, incarnation: Incarnation, scheduler: Option<&'a Scheduler>) -> Self {
        Self { txn_idx, incarnation, scheduler, finished: false }
    }
    
    fn finish(mut self, needs_suffix_validation: bool) -> Result<SchedulerTask, PanicError> {
        self.finished = true;
        if let Some(scheduler) = self.scheduler {
            scheduler.finish_execution(self.txn_idx, self.incarnation, needs_suffix_validation)
        } else {
            Ok(SchedulerTask::Retry)
        }
    }
}

impl<'a> Drop for CriticalSectionGuard<'a> {
    fn drop(&mut self) {
        if !self.finished && !std::thread::panicking() {
            panic!("Critical section guard dropped without calling finish_execution! txn_idx={}, incarnation={}", 
                   self.txn_idx, self.incarnation);
        }
    }
}
```

Use the guard in execute():
```rust
// Create guard before entering critical section
let guard = CriticalSectionGuard::new(idx_to_execute, incarnation, maybe_scheduler);

// All critical section operations...
Self::process_delayed_field_output(...)?;
// ... other operations ...
last_input_output.record(...)?;

// Explicitly finish the critical section
return guard.finish(needs_suffix_validation);
```

This ensures that either finish_execution() is called, or the thread panics explicitly, preventing silent corruption.

## Proof of Concept

Due to the concurrent nature of this vulnerability, a full PoC requires simulating the race condition. The following demonstrates the vulnerable code path:

```rust
// Reproduction steps:
// 1. Start block execution with high gas transactions
// 2. Trigger concurrent halt via block gas limit
// 3. Inject error in record() or before_materialization() during critical section
// 4. Observe that finish_execution() is never called
// 5. Check transaction status remains "Executing"
// 6. Verify versioned_cache contains partial modifications

#[test]
fn test_critical_section_violation() {
    // Setup block executor with transactions near gas limit
    let mut transactions = vec![];
    for i in 0..100 {
        transactions.push(high_gas_transaction(i));
    }
    
    // Execute block with concurrent halt trigger
    let (tx, rx) = channel();
    thread::spawn(move || {
        // Wait for critical section entry then halt
        thread::sleep(Duration::from_millis(10));
        scheduler.halt();
        tx.send(()).unwrap();
    });
    
    // Execute transactions - some will be in critical section when halt occurs
    let result = executor.execute_block(transactions);
    rx.recv().unwrap();
    
    // Verify: some transactions stuck in "Executing" state with partial modifications
    for txn_idx in 0..100 {
        let status = scheduler.get_status(txn_idx);
        if matches!(status, ExecutionStatus::Executing(_)) {
            // This transaction violated the critical section constraint
            assert!(versioned_cache.has_partial_modifications(txn_idx));
            assert!(!finish_execution_was_called(txn_idx));
        }
    }
}
```

## Notes

This vulnerability is explicitly acknowledged in the codebase comments (lines 585-592 and 507-511) with TODO notes suggesting compile-time checks or custom clippy lints. The ExecutionHalted documentation specifically mentions scenarios where finish_execution must be called despite concurrent halts. The vulnerability represents a gap between the documented critical section requirements and the actual implementation's error handling, creating a consensus safety risk under concurrent execution and halting conditions.

### Citations

**File:** aptos-move/block-executor/src/executor.rs (L358-383)
```rust
                if let Err(e) =
                    versioned_cache
                        .delayed_fields()
                        .record_change(id, idx_to_execute, entry)
                {
                    match e {
                        PanicOr::CodeInvariantError(m) => {
                            return Err(code_invariant_error(format!(
                                "Record change failed with CodeInvariantError: {:?}",
                                m
                            )));
                        },
                        PanicOr::Or(_) => {
                            read_set.capture_delayed_field_read_error(&PanicOr::Or(
                                MVDelayedFieldsError::DeltaApplicationFailure,
                            ));
                        },
                    };
                }
            }
        }

        for id in prev_modified_delayed_fields {
            versioned_cache
                .delayed_fields()
                .remove(&id, idx_to_execute, is_v2)?;
```

**File:** aptos-move/block-executor/src/executor.rs (L499-512)
```rust
        last_input_output.record(
            idx_to_execute,
            read_set,
            execution_result,
            block_gas_limit_type,
            txn.user_txn_bytes_len() as u64,
        )?;

        // It is important to call finish_execution after recording the input/output.
        // CAUTION: once any update has been applied to the shared data structures, there should
        // be no short circuits until the record succeeds and scheduler is notified that the
        // execution is finished. This allows cleaning up the shared data structures before
        // applying the updates from next incarnation (which can also be the block epilogue txn).
        if let Some(module_validation_requirements) = scheduler.finish_execution(abort_manager)? {
```

**File:** aptos-move/block-executor/src/executor.rs (L585-592)
```rust
        // CAUTION: start shared output critical section.
        // If control flow reaches below and changes are applied to the shared data structures,
        // it should be guaranteed that the process will complete fully, completed by
        // recording of the input/outputs and lastly, by finish_execution. Hence, in the below
        // "critical section", e.g. returning with Ok status after observing the scheduler has halted
        // would be incorrect and lead to a PanicError if the block prologue txn were to be
        // executed later at the same index (after block cutting).
        // TODO(BlockSTMv2): Replace with a compile-time check if possible, or custom clippy lint.
```

**File:** aptos-move/block-executor/src/executor.rs (L701-714)
```rust
        last_input_output.record(
            idx_to_execute,
            read_set,
            execution_result,
            block_gas_limit_type,
            txn.user_txn_bytes_len() as u64,
        )?;
        if let Some(scheduler) = maybe_scheduler {
            scheduler.finish_execution(idx_to_execute, incarnation, needs_suffix_validation)
        } else {
            // Final re-execution of the txn does not require scheduler,
            // or need to return a task.
            Ok(SchedulerTask::Retry)
        }
```

**File:** aptos-move/block-executor/src/scheduler.rs (L149-163)
```rust
    // The bool in ExecutionHalted tracks an useful invariant for the block epilogue txn
    // when the block is cut, and the final execution of the epilogue txn occurs at
    // some idx < num_txns. In this case, it must be ensured that any control flow that
    // started to apply changes to the shared data structures was completed despite
    // a concurrent halt (which must be invoked due to block cutting).
    // - in case of Aborting, finish_abort must be called (after estimates are marked).
    // - in case of Executing, finish_execution must be called, which happens after
    // the caller records the input/output (needed to mark outputs as estimates or
    // clear the prior write-set).
    //
    // In particular, [Scheduler::set_aborted_status] & [Scheduler::set_executed_status]
    // must be called to reset the flag to true. The flag is set to false if when halting,
    // the txn status is aborting or executing, or if right before applying the outputs
    // to the shared data structures the txn is already halted.
    ExecutionHalted(bool),
```

**File:** aptos-move/block-executor/src/scheduler.rs (L553-594)
```rust
    pub fn finish_execution(
        &self,
        txn_idx: TxnIndex,
        incarnation: Incarnation,
        revalidate_suffix: bool,
    ) -> Result<SchedulerTask, PanicError> {
        // Note: It is preferable to hold the validation lock throughout the finish_execution,
        // in particular before updating execution status. The point was that we don't want
        // any validation to come before the validation status is correspondingly updated.
        // It may be possible to reduce granularity, but shouldn't make performance difference
        // and like this correctness argument is much easier to see, which is also why we grab
        // the write lock directly, and never release it during the whole function. This way,
        // even validation status readers have to wait if they somehow end up at the same index.
        let mut validation_status = self.txn_status[txn_idx as usize].1.write();
        self.set_executed_status(txn_idx, incarnation)?;

        self.wake_dependencies_after_execution(txn_idx)?;

        let (cur_val_idx, mut cur_wave) =
            Self::unpack_validation_idx(self.validation_idx.load(Ordering::Acquire));

        // Needs to be re-validated in a new wave
        if cur_val_idx > txn_idx {
            if revalidate_suffix {
                // The transaction execution required revalidating all higher txns (not
                // only itself), currently happens when incarnation writes to a new path
                // (w.r.t. the write-set of its previous completed incarnation).
                if let Some(wave) = self.decrease_validation_idx(txn_idx + 1) {
                    cur_wave = wave;
                };
            }
            // Update the minimum wave this txn needs to pass.
            validation_status.required_wave = cur_wave;
            return Ok(SchedulerTask::ValidationTask(
                txn_idx,
                incarnation,
                cur_wave,
            ));
        }

        Ok(SchedulerTask::Retry)
    }
```
