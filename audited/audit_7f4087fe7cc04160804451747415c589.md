# Audit Report

## Title
Byzantine Peers Can Advertise False State Ranges to Delay Network Consensus Through State Sync Failures

## Summary
Byzantine peers can claim they possess complete state data for version ranges they don't actually have by advertising false ranges in the `AdvertisedData.states` field. The state sync system trusts these unvalidated advertisements when selecting peers for data requests. When honest nodes attempt to sync state from these malicious peers, requests fail or timeout, causing repeated retries and significant delays in state synchronization, which blocks consensus execution and network progress.

## Finding Description

The vulnerability exists in the state sync advertisement and peer selection mechanism. The attack path proceeds as follows:

**Step 1: Byzantine Peer Advertises False State Ranges**

When peers advertise their available data, they send a `StorageServerSummary` containing a `DataSummary` with a `states` field. The data client directly trusts this self-reported data without validation: [1](#0-0) 

The advertised state ranges from all peers are aggregated into a global `AdvertisedData` structure that represents what data the network claims to have available: [2](#0-1) 

**Step 2: Peer Selection Based on False Advertisements**

When the state sync system needs to request state data, it selects peers based on their advertised capabilities. The `can_service_request` method checks if a peer can service a request by examining the peer's self-reported `storage_summary`: [3](#0-2) 

The serviceability check delegates to the peer's advertised data summary, which contains the false state ranges: [4](#0-3) 

**Step 3: Request Failures and Retries**

When a request is sent to a malicious peer that doesn't actually have the advertised data, the peer will either timeout or return an error when trying to fetch data from its storage. The storage layer will fail when attempting to retrieve non-existent state values: [5](#0-4) 

The request failure handling logic retries with other peers, but this causes significant delays: [6](#0-5) 

Failed peers have their scores reduced using a multiplicative penalty: [7](#0-6) 

With a starting score of 50.0, ignore threshold of 25.0, and multiplier of 0.95 per failure, it takes approximately 14 failed requests before a malicious peer is ignored. With typical request timeouts of 30 seconds, this results in 7-14 minutes of delay per malicious peer.

**Step 4: Consensus Blocking**

State synchronization is a critical operation that blocks consensus execution. When consensus needs to sync to a target ledger info, it acquires a write mutex and waits for state sync to complete: [8](#0-7) 

During this synchronization period, consensus cannot process new blocks, effectively halting network progress until state sync completes.

**Broken Invariants:**
1. **State Consistency**: The system assumes advertised data ranges are truthful and can be verified
2. **Liveness**: Consensus progress depends on timely state synchronization, which is delayed by false advertisements

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program for the following reasons:

1. **Validator Node Slowdowns** (High Severity): Malicious peers cause repeated request failures and timeouts, significantly slowing down validator nodes attempting to sync state. Each malicious peer can introduce 7-14 minutes of delay before being ignored.

2. **Significant Protocol Violations** (High Severity): The state sync protocol assumes peers advertise data they actually possess. False advertisements violate this trust assumption and cause the protocol to make incorrect peer selection decisions.

3. **Consensus Delays**: While not a permanent halt, the attack can delay consensus for extended periods (potentially hours with multiple coordinated malicious peers), affecting network liveness and user experience.

The impact does NOT reach Critical severity because:
- No funds are at risk of theft or permanent freezing
- Consensus safety is not violated (no chain splits or double-spending)
- The network eventually recovers as malicious peers are ignored
- No permanent state corruption occurs

## Likelihood Explanation

The likelihood of this vulnerability being exploited is **HIGH**:

1. **Low Attacker Requirements**: Any network peer can participate in the state sync protocol and advertise false data. No validator status, stake, or special privileges are required.

2. **Easy to Execute**: An attacker only needs to:
   - Connect to the network as a regular peer
   - Respond to storage summary requests with false state ranges
   - The attack requires no cryptographic operations or complex logic

3. **No Detection Mechanism**: There is no validation, proof verification, or sanity checking of advertised state ranges before they are added to the global summary and used for peer selection.

4. **Amplification Effect**: Multiple coordinated malicious peers can compound the delay, as the system will try each one sequentially before moving to honest peers.

5. **Persistent Effect**: Even after one malicious peer is ignored, new malicious peers can continue the attack, requiring the scoring system to identify and ignore each one individually.

## Recommendation

Implement validation of advertised state ranges before accepting them into the global data summary. Several mitigation approaches should be combined:

**1. Probabilistic Verification**: Randomly verify a subset of advertised state ranges by requesting sample data from peers and validating the cryptographic proofs. If a peer cannot provide data it claims to have, immediately set its score to 0 and ignore it.

**2. Faster Malicious Peer Detection**: Implement more aggressive scoring penalties for peers that fail to deliver advertised data. For example:
- Use exponential backoff (0.5 multiplier) instead of 0.95 for "not useful" errors
- Classify inability to serve advertised data as "Malicious" (0.8 multiplier) rather than "NotUseful"
- Lower the ignore threshold or increase the initial penalty for advertisement mismatches

**3. Advertisement Consistency Checks**: Validate that advertised state ranges are consistent with synced ledger info:
```rust
// In peer_states.rs, calculate_global_data_summary()
for summary in storage_summaries {
    if let Some(states) = summary.data_summary.states {
        // Validate state range is consistent with synced ledger info
        if let Some(synced_info) = summary.data_summary.synced_ledger_info.as_ref() {
            let synced_version = synced_info.ledger_info().version();
            if states.highest() > synced_version {
                // Peer claims to have states beyond its synced version - suspicious
                warn!("Peer advertises inconsistent state range beyond synced version");
                continue; // Skip this peer's states
            }
        }
        advertised_data.states.push(states);
    }
}
```

**4. Reputation System**: Track historical reliability of peers across multiple data types. Peers with a pattern of advertising unavailable data across multiple categories should be deprioritized more aggressively.

**5. Optimistic Validation**: When a peer is first encountered, place it in a "probationary" state where its advertisements are weighted lower or verified more frequently before being trusted at the same level as established peers.

## Proof of Concept

The following Rust test demonstrates the vulnerability by simulating a Byzantine peer advertising false state ranges:

```rust
#[tokio::test]
async fn test_byzantine_peer_false_state_advertisement() {
    use aptos_config::config::AptosDataClientConfig;
    use aptos_storage_service_types::responses::{
        CompleteDataRange, DataSummary, ProtocolMetadata, StorageServerSummary,
    };
    use aptos_types::ledger_info::LedgerInfoWithSignatures;
    use std::sync::Arc;

    // Create a data client with default config
    let config = Arc::new(AptosDataClientConfig::default());
    let peer_states = PeerStates::new(config.clone());
    
    // Create a Byzantine peer that advertises false state ranges
    let byzantine_peer = PeerNetworkId::random();
    
    // Byzantine peer claims to have states for versions 0-1000000
    // but doesn't actually have this data
    let false_state_range = CompleteDataRange::new(0, 1_000_000).unwrap();
    
    let byzantine_summary = StorageServerSummary {
        protocol_metadata: ProtocolMetadata::default(),
        data_summary: DataSummary {
            synced_ledger_info: Some(LedgerInfoWithSignatures::mock(1_000_000)),
            epoch_ending_ledger_infos: None,
            states: Some(false_state_range), // FALSE ADVERTISEMENT
            transactions: None,
            transaction_outputs: None,
        },
    };
    
    // Update peer states with Byzantine peer's false advertisement
    peer_states.update_summary(byzantine_peer, byzantine_summary);
    
    // Calculate global summary - it will include false state ranges
    let global_summary = peer_states.calculate_global_data_summary();
    
    // Verify that the false state range is now in advertised data
    assert!(!global_summary.advertised_data.states.is_empty());
    assert_eq!(global_summary.advertised_data.states[0].highest(), 1_000_000);
    
    // Now attempt to request state values - peer selection will choose
    // the Byzantine peer because it claims to have the data
    let state_request = StorageServiceRequest {
        data_request: DataRequest::GetStateValuesWithProof(
            StateValuesWithProofRequest {
                version: 500_000, // Within the false advertised range
                start_index: 0,
                end_index: 100,
            }
        ),
        use_compression: false,
    };
    
    // The Byzantine peer will be identified as serviceable
    let can_service = peer_states.can_service_request(
        &byzantine_peer,
        TimeService::mock(),
        &state_request,
    );
    assert!(can_service); // Peer claims it can service, but actually cannot
    
    // When the actual request is made, it will fail/timeout
    // This would be repeated ~14 times (7-14 minutes of delay)
    // before the peer score drops below ignore threshold
}
```

**To reproduce the full attack:**
1. Deploy a modified storage service node that advertises false state ranges
2. Connect it to a testnet or local network
3. Monitor an honest node attempting to sync state
4. Observe repeated request failures and timeouts to the malicious peer
5. Measure the total delay before the malicious peer is ignored
6. Demonstrate that consensus is blocked during this period by examining the write_mutex acquisition in sync_to_target

**Notes**

This vulnerability represents a fundamental trust assumption in the state sync protocol: that peers honestly advertise their available data. The lack of validation or proof requirements allows Byzantine peers to manipulate the peer selection process, causing significant delays in state synchronization and consensus progress. While the network eventually recovers through the peer scoring mechanism, the delay can be substantial, especially with multiple coordinated malicious peers.

The recommended mitigations focus on adding validation layers, faster detection of malicious behavior, and consistency checks that make false advertisements detectable and penalized more quickly. The most robust solution would be probabilistic verification of advertised data combined with aggressive scoring penalties for advertisement mismatches.

### Citations

**File:** state-sync/aptos-data-client/src/peer_states.rs (L32-43)
```rust
/// Scores for peer rankings based on preferences and behavior.
const MAX_SCORE: f64 = 100.0;
const MIN_SCORE: f64 = 0.0;
const STARTING_SCORE: f64 = 50.0;
/// Add this score on a successful response.
const SUCCESSFUL_RESPONSE_DELTA: f64 = 1.0;
/// Not necessarily a malicious response, but not super useful.
const NOT_USEFUL_MULTIPLIER: f64 = 0.95;
/// Likely to be a malicious response.
const MALICIOUS_MULTIPLIER: f64 = 0.8;
/// Ignore a peer when their score dips below this threshold.
const IGNORE_PEER_THRESHOLD: f64 = 25.0;
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L200-227)
```rust
    pub fn can_service_request(
        &self,
        peer: &PeerNetworkId,
        time_service: TimeService,
        request: &StorageServiceRequest,
    ) -> bool {
        // Storage services can always respond to data advertisement requests.
        // We need this outer check, since we need to be able to send data summary
        // requests to new peers (who don't have a peer state yet).
        if request.data_request.is_storage_summary_request()
            || request.data_request.is_protocol_version_request()
        {
            return true;
        }

        // Check if the peer can service the request
        if let Some(peer_state) = self.peer_to_state.get(peer) {
            return match peer_state.get_storage_summary_if_not_ignored() {
                Some(storage_summary) => {
                    storage_summary.can_service(&self.data_client_config, time_service, request)
                },
                None => false, // The peer is temporarily ignored
            };
        }

        // Otherwise, the request cannot be serviced
        false
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L371-372)
```rust
            if let Some(states) = summary.data_summary.states {
                advertised_data.states.push(states);
```

**File:** state-sync/aptos-data-client/src/global_summary.rs (L64-89)
```rust
#[derive(Clone, Eq, PartialEq)]
pub struct AdvertisedData {
    /// The ranges of epoch ending ledger infos advertised, e.g., if a range
    /// is (X,Y), it means all epoch ending ledger infos for epochs X->Y
    /// (inclusive) are available.
    pub epoch_ending_ledger_infos: Vec<CompleteDataRange<Epoch>>,

    /// The ranges of states advertised, e.g., if a range is
    /// (X,Y), it means all states are held for every version X->Y
    /// (inclusive).
    pub states: Vec<CompleteDataRange<Version>>,

    /// The ledger infos corresponding to the highest synced versions
    /// currently advertised.
    pub synced_ledger_infos: Vec<LedgerInfoWithSignatures>,

    /// The ranges of transactions advertised, e.g., if a range is
    /// (X,Y), it means all transactions for versions X->Y (inclusive)
    /// are available.
    pub transactions: Vec<CompleteDataRange<Version>>,

    /// The ranges of transaction outputs advertised, e.g., if a range
    /// is (X,Y), it means all transaction outputs for versions X->Y
    /// (inclusive) are available.
    pub transaction_outputs: Vec<CompleteDataRange<Version>>,
}
```

**File:** state-sync/storage-service/types/src/responses.rs (L727-742)
```rust
            GetStateValuesWithProof(request) => {
                let proof_version = request.version;

                let can_serve_states = self
                    .states
                    .map(|range| range.contains(request.version))
                    .unwrap_or(false);

                let can_create_proof = self
                    .synced_ledger_info
                    .as_ref()
                    .map(|li| li.ledger_info().version() >= proof_version)
                    .unwrap_or(false);

                can_serve_states && can_create_proof
            },
```

**File:** state-sync/storage-service/server/src/storage.rs (L900-987)
```rust
    fn get_state_value_chunk_with_proof_by_size(
        &self,
        version: u64,
        start_index: u64,
        end_index: u64,
        max_response_size: u64,
        use_size_and_time_aware_chunking: bool,
    ) -> Result<StateValueChunkWithProof, Error> {
        // Calculate the number of state values to fetch
        let expected_num_state_values = inclusive_range_len(start_index, end_index)?;
        let max_num_state_values = self.config.max_state_chunk_size;
        let num_state_values_to_fetch = min(expected_num_state_values, max_num_state_values);

        // If size and time-aware chunking are disabled, use the legacy implementation
        if !use_size_and_time_aware_chunking {
            return self.get_state_value_chunk_with_proof_by_size_legacy(
                version,
                start_index,
                end_index,
                num_state_values_to_fetch,
                max_response_size,
            );
        }

        // Get the state value chunk iterator
        let mut state_value_iterator = self.storage.get_state_value_chunk_iter(
            version,
            start_index as usize,
            num_state_values_to_fetch as usize,
        )?;

        // Initialize the fetched state values
        let mut state_values = vec![];

        // Create a response progress tracker
        let mut response_progress_tracker = ResponseDataProgressTracker::new(
            num_state_values_to_fetch,
            max_response_size,
            self.config.max_storage_read_wait_time_ms,
            self.time_service.clone(),
        );

        // Fetch as many state values as possible
        while !response_progress_tracker.is_response_complete() {
            match state_value_iterator.next() {
                Some(Ok(state_value)) => {
                    // Calculate the number of serialized bytes for the state value
                    let num_serialized_bytes = get_num_serialized_bytes(&state_value)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;

                    // Add the state value to the list
                    if response_progress_tracker
                        .data_items_fits_in_response(true, num_serialized_bytes)
                    {
                        state_values.push(state_value);
                        response_progress_tracker.add_data_item(num_serialized_bytes);
                    } else {
                        break; // Cannot add any more data items
                    }
                },
                Some(Err(error)) => {
                    return Err(Error::StorageErrorEncountered(error.to_string()));
                },
                None => {
                    // Log a warning that the iterator did not contain all the expected data
                    warn!(
                        "The state value iterator is missing data! Version: {:?}, \
                        start index: {:?}, end index: {:?}, num state values to fetch: {:?}",
                        version, start_index, end_index, num_state_values_to_fetch
                    );
                    break;
                },
            }
        }

        // Create the state value chunk with proof
        let state_value_chunk_with_proof = self.storage.get_state_value_chunk_proof(
            version,
            start_index as usize,
            state_values,
        )?;

        // Update the data truncation metrics
        response_progress_tracker
            .update_data_truncation_metrics(DataResponse::get_state_value_chunk_with_proof_label());

        Ok(state_value_chunk_with_proof)
    }
```

**File:** state-sync/aptos-data-client/src/client.rs (L675-701)
```rust
        // Wait for the first successful response and abort all other tasks.
        // If all requests fail, gather the errors and return them.
        let num_sent_requests = sent_requests.len();
        let mut sent_request_errors = vec![];
        for _ in 0..num_sent_requests {
            if let Ok(response_result) = sent_requests.select_next_some().await {
                match response_result {
                    Ok(response) => {
                        // We received a valid response. Abort all pending tasks.
                        for abort_handle in abort_handles {
                            abort_handle.abort();
                        }
                        return Ok(response); // Return the response
                    },
                    Err(error) => {
                        // Gather the error and continue waiting for a response
                        sent_request_errors.push(error)
                    },
                }
            }
        }

        // Otherwise, all requests failed and we should return an error
        Err(Error::DataIsUnavailable(format!(
            "All {} attempts failed for the given request: {:?}. Errors: {:?}",
            num_sent_requests, request, sent_request_errors
        )))
```

**File:** consensus/src/state_computer.rs (L177-233)
```rust
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        // Grab the logical time lock and calculate the target logical time
        let mut latest_logical_time = self.write_mutex.lock().await;
        let target_logical_time =
            LogicalTime::new(target.ledger_info().epoch(), target.ledger_info().round());

        // Before state synchronization, we have to call finish() to free the
        // in-memory SMT held by BlockExecutor to prevent a memory leak.
        self.executor.finish();

        // The pipeline phase already committed beyond the target block timestamp, just return.
        if *latest_logical_time >= target_logical_time {
            warn!(
                "State sync target {:?} is lower than already committed logical time {:?}",
                target_logical_time, *latest_logical_time
            );
            return Ok(());
        }

        // This is to update QuorumStore with the latest known commit in the system,
        // so it can set batches expiration accordingly.
        // Might be none if called in the recovery path, or between epoch stop and start.
        if let Some(inner) = self.state.read().as_ref() {
            let block_timestamp = target.commit_info().timestamp_usecs();
            inner
                .payload_manager
                .notify_commit(block_timestamp, Vec::new());
        }

        // Inject an error for fail point testing
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });

        // Invoke state sync to synchronize to the specified target. Here, the
        // ChunkExecutor will process chunks and commit to storage. However, after
        // block execution and commits, the internal state of the ChunkExecutor may
        // not be up to date. So, it is required to reset the cache of the
        // ChunkExecutor in state sync when requested to sync.
        let result = monitor!(
            "sync_to_target",
            self.state_sync_notifier.sync_to_target(target).await
        );

        // Update the latest logical time
        *latest_logical_time = target_logical_time;

        // Similarly, after state synchronization, we have to reset the cache of
        // the BlockExecutor to guarantee the latest committed state is up to date.
        self.executor.reset()?;

        // Return the result
        result.map_err(|error| {
            let anyhow_error: anyhow::Error = error.into();
            anyhow_error.into()
        })
    }
```
