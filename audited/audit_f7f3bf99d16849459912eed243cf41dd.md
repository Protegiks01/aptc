# Audit Report

## Title
State KV Shard Pruner Permanently Skips Entries Below Initial Progress, Causing Unbounded Database Growth

## Summary
The `StateKvShardPruner::prune()` function uses RocksDB iterator seek with `current_progress` to find entries to prune. When a shard is first initialized, its progress is set to `metadata_progress` without actually pruning entries below that version. The subsequent seek operation positions the iterator at the first entry where `stale_since_version >= current_progress`, permanently skipping all entries with `stale_since_version < current_progress`. These orphaned entries are never pruned, leading to unbounded database growth and eventual storage exhaustion.

## Finding Description

The vulnerability exists in the interaction between shard initialization and the pruning logic: [1](#0-0) 

When `get_or_initialize_subpruner_progress()` is called for a shard without existing progress metadata, it **immediately writes** `metadata_progress` to the database and returns it. This happens during shard pruner initialization: [2](#0-1) 

The initialization then calls `prune(progress, metadata_progress)`. Since `progress == metadata_progress`, the prune function executes but performs no actual pruning: [3](#0-2) 

The critical flaw is at line 57: `iter.seek(&current_progress)` positions the iterator using RocksDB's seek semantics, which finds the first key where `stale_since_version >= current_progress`. The `SeekKeyCodec` implementation encodes only the version as 8 bytes: [4](#0-3) 

When the iterator seeks to a version that doesn't have an exact entry, it positions at the next higher version, skipping all entries below `current_progress`.

**Attack Scenario:**
1. A validator node operates with sharding enabled and has accumulated stale state entries at versions 400000, 450000, 500000, etc.
2. Metadata pruner has progressed to version 1000000
3. Shard 5's database is corrupted and restored from a backup taken at version 600000, containing entries at `stale_since_version = 400000, 450000, 500000`
4. Node restarts and initializes shard pruner:
   - `get_or_initialize_subpruner_progress()` finds no progress metadata (corrupted)
   - Writes `progress = 1000000` to shard database
   - Calls `prune(1000000, 1000000)`
   - `seek(1000000)` positions at first entry >= 1000000
   - Loop immediately breaks (no entries to process)
5. All entries at 400000, 450000, 500000 remain in database forever
6. Future pruning runs continue from version 1000000+, never revisiting lower versions

This also occurs during:
- Initial sharding enablement migration (if old entries exist in shards)
- Manual database restoration procedures
- Any scenario where shard progress metadata is lost or reset

## Impact Explanation

**High Severity** - This vulnerability causes:

1. **Unbounded Database Growth**: Stale state values that should be pruned accumulate indefinitely, consuming disk space without bound.

2. **Storage Exhaustion**: Over time, unpruned entries cause the database to grow until disk space is exhausted, leading to node crashes and network availability issues.

3. **Performance Degradation**: Large databases with unpruned entries slow down query performance, affecting validator responsiveness and potentially causing consensus timeouts.

4. **Operational Disruption**: Nodes experiencing storage exhaustion require manual intervention, database cleanup, or hardware upgrades to continue operating.

While this doesn't directly cause consensus violations or fund loss, it leads to **significant protocol violations** and **validator node failures**, qualifying as **High Severity** per the Aptos bug bounty criteria ("Validator node slowdowns", "Significant protocol violations").

The issue breaks the **State Consistency** invariant (state transitions must be atomic and verifiable) and **Resource Limits** invariant (all operations must respect storage limits).

## Likelihood Explanation

**High Likelihood** in production environments:

1. **Database Restoration**: Common operational procedure when recovering from hardware failures or migrating nodes
2. **Backup/Restore Operations**: Regular maintenance activity for validator operators
3. **Sharding Migration**: Occurs when enabling sharding feature (AIP-97) on existing nodes
4. **Database Corruption Recovery**: Real-world scenario requiring database repair

The vulnerability triggers automatically during these operations without requiring attacker interaction. Once triggered, the unpruned entries persist permanently until manual intervention.

## Recommendation

Fix the initialization logic to actually prune entries below `metadata_progress` instead of just setting the progress marker:

```rust
pub(in crate::pruner) fn new(
    shard_id: usize,
    db_shard: Arc<DB>,
    metadata_progress: Version,
) -> Result<Self> {
    let progress = get_or_initialize_subpruner_progress(
        &db_shard,
        &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
        metadata_progress,
    )?;
    let myself = Self { shard_id, db_shard };

    info!(
        progress = progress,
        metadata_progress = metadata_progress,
        "Catching up state kv shard {shard_id}."
    );
    
    // FIX: Prune from 0 instead of from progress
    // This ensures all entries below metadata_progress are pruned
    if progress < metadata_progress {
        myself.prune(0, metadata_progress)?;
    }

    Ok(myself)
}
```

Alternative fix in `get_or_initialize_subpruner_progress`:

```rust
pub(crate) fn get_or_initialize_subpruner_progress(
    sub_db: &DB,
    progress_key: &DbMetadataKey,
    metadata_progress: Version,
) -> Result<Version> {
    Ok(
        if let Some(v) = sub_db.get::<DbMetadataSchema>(progress_key)? {
            v.expect_version()
        } else {
            // FIX: Don't write progress yet, return 0 to force full pruning
            // The caller will write the correct progress after pruning
            0
        },
    )
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use crate::schema::{
        stale_state_value_index_by_key_hash::StaleStateValueIndexByKeyHashSchema,
        state_value_by_key_hash::StateValueByKeyHashSchema,
    };
    use aptos_crypto::HashValue;
    use aptos_schemadb::DB;
    use aptos_types::state_store::state_value::StaleStateValueByKeyHashIndex;
    use tempfile::TempDir;

    #[test]
    fn test_pruner_skips_old_entries_on_initialization() {
        // Setup: Create a shard database with old entries
        let tmpdir = TempDir::new().unwrap();
        let db = Arc::new(DB::open(
            tmpdir.path(),
            "test_shard",
            vec!["default", "stale_state_value_index_by_key_hash", "state_value_by_key_hash"],
            &rocksdb::Options::default(),
        ).unwrap());

        // Insert old stale entries that should be pruned
        let old_entries = vec![
            (100u64, HashValue::random()),
            (200u64, HashValue::random()),
            (300u64, HashValue::random()),
        ];
        
        for (version, hash) in &old_entries {
            let index = StaleStateValueByKeyHashIndex {
                stale_since_version: *version,
                version: *version - 50,
                state_key_hash: *hash,
            };
            db.put::<StaleStateValueIndexByKeyHashSchema>(&index, &()).unwrap();
            db.put::<StateValueByKeyHashSchema>(&(*hash, *version - 50), &vec![1, 2, 3]).unwrap();
        }

        // Simulate high metadata progress (normal in production after long operation)
        let metadata_progress = 1000u64;

        // Initialize shard pruner (this triggers the vulnerability)
        let pruner = StateKvShardPruner::new(0, db.clone(), metadata_progress).unwrap();

        // Verify: Old entries still exist (BUG!)
        let mut iter = db.iter::<StaleStateValueIndexByKeyHashSchema>().unwrap();
        iter.seek_to_first();
        let mut found_entries = Vec::new();
        for item in iter {
            let (index, _) = item.unwrap();
            found_entries.push(index.stale_since_version);
        }

        // VULNERABILITY: Entries at 100, 200, 300 should have been pruned
        // but still exist in the database
        assert_eq!(found_entries, vec![100, 200, 300], 
            "Old entries were not pruned during initialization!");

        // These entries will NEVER be pruned in future runs
        // because progress is now at 1000, and seek(1000) will skip them
        pruner.prune(1000, 1500).unwrap();
        
        // Verify they STILL exist after a pruning run
        let mut iter = db.iter::<StaleStateValueIndexByKeyHashSchema>().unwrap();
        iter.seek_to_first();
        let mut still_exist = Vec::new();
        for item in iter {
            let (index, _) = item.unwrap();
            still_exist.push(index.stale_since_version);
        }
        
        assert_eq!(still_exist, vec![100, 200, 300],
            "Old entries persist forever - UNBOUNDED GROWTH!");
    }
}
```

**Notes:**
- This vulnerability is particularly critical for long-running validators that accumulate significant stale state data
- The impact compounds over time as more entries are skipped during subsequent database operations
- Manual database cleanup or node reinitialization from genesis would be required to resolve affected nodes
- The issue affects all 16 shards when sharding is enabled, multiplying the storage growth impact

### Citations

**File:** storage/aptosdb/src/pruner/pruner_utils.rs (L44-60)
```rust
pub(crate) fn get_or_initialize_subpruner_progress(
    sub_db: &DB,
    progress_key: &DbMetadataKey,
    metadata_progress: Version,
) -> Result<Version> {
    Ok(
        if let Some(v) = sub_db.get::<DbMetadataSchema>(progress_key)? {
            v.expect_version()
        } else {
            sub_db.put::<DbMetadataSchema>(
                progress_key,
                &DbMetadataValue::Version(metadata_progress),
            )?;
            metadata_progress
        },
    )
}
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L25-45)
```rust
    pub(in crate::pruner) fn new(
        shard_id: usize,
        db_shard: Arc<DB>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
            metadata_progress,
        )?;
        let myself = Self { shard_id, db_shard };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up state kv shard {shard_id}."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L47-72)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();

        let mut iter = self
            .db_shard
            .iter::<StaleStateValueIndexByKeyHashSchema>()?;
        iter.seek(&current_progress)?;
        for item in iter {
            let (index, _) = item?;
            if index.stale_since_version > target_version {
                break;
            }
            batch.delete::<StaleStateValueIndexByKeyHashSchema>(&index)?;
            batch.delete::<StateValueByKeyHashSchema>(&(index.state_key_hash, index.version))?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
            &DbMetadataValue::Version(target_version),
        )?;

        self.db_shard.write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/schema/stale_state_value_index_by_key_hash/mod.rs (L76-80)
```rust
impl SeekKeyCodec<StaleStateValueIndexByKeyHashSchema> for Version {
    fn encode_seek_key(&self) -> Result<Vec<u8>> {
        Ok(self.to_be_bytes().to_vec())
    }
}
```
