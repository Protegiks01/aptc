# Audit Report

## Title
ConsensusObserverMessage Deserialization Memory Exhaustion via Unbounded Vector Allocation

## Summary
The `ConsensusObserverMessage` deserialization process is vulnerable to memory exhaustion attacks through crafted messages containing unbounded vector sizes. While BCS recursion depth is limited to 64 levels, there are no limits on vector breadth during deserialization, allowing an attacker to trigger massive memory allocations before any validation occurs.

## Finding Description

The ConsensusObserver protocol deserializes incoming network messages using BCS (Binary Canonical Serialization) with a recursion limit of 64. However, this limit only protects against **stack overflow from deeply nested structures**, not **memory exhaustion from wide structures** with large vectors. [1](#0-0) 

The `ConsensusObserver` protocol uses `Encoding::CompressedBcs(RECURSION_LIMIT)` where `RECURSION_LIMIT = 64`: [2](#0-1) [3](#0-2) 

The deserialization flow is:
1. Compressed bytes received over network (limited by `MAX_APPLICATION_MESSAGE_SIZE` ≈ 62 MB after decompression)
2. BCS deserialization with `bcs::from_bytes_with_limit(bytes, 64)` 
3. Network handler forwards already-deserialized message to observer [4](#0-3) [5](#0-4) 

The critical flaw is that deserialization happens **before validation**. The `OrderedBlock` structure contains unbounded vectors: [6](#0-5) 

**Attack Scenario:**
1. Malicious validator crafts `ConsensusObserverMessage::DirectSend(OrderedBlock)` with `Vec<Arc<PipelinedBlock>>` containing 100,000 entries
2. Each `PipelinedBlock` contains `Vec<SignedTransaction>` with 1,000 transactions
3. Each `SignedTransaction` has `Vec<Vec<u8>>` arguments with 100 entries
4. Compressed size: ~5 MB (highly compressible repetitive data)
5. Decompressed BCS bytes: ~60 MB (within `MAX_APPLICATION_MESSAGE_SIZE`)
6. **In-memory allocation during deserialization: 10+ GB** (100K × 1K × 100 allocations)
7. Recursion depth: only ~10-15 levels (well within 64 limit)

The decompression size limit provides no protection against allocation bombs: [7](#0-6) 

Validation like `verify_ordered_blocks()` only checks logical correctness **after** allocation: [8](#0-7) 

## Impact Explanation

**HIGH Severity** - This meets the "Validator node slowdowns" and "Significant protocol violations" criteria for High severity bugs:

1. **Memory Exhaustion**: Observer nodes can be crashed via OOM killer
2. **CPU Exhaustion**: Deserializing millions of objects causes severe slowdown
3. **Availability Impact**: Consensus observer functionality becomes unavailable
4. **Cascading Failure**: Multiple observer nodes can be targeted simultaneously

While not a Critical severity (no fund loss or consensus safety violation), this enables DoS attacks on the consensus observer infrastructure, degrading network monitoring and potentially impacting validators relying on observer data.

## Likelihood Explanation

**MEDIUM-HIGH Likelihood**:

**Attack Requirements:**
- Attacker needs to send messages to consensus observers
- No authentication/authorization on `ConsensusObserver` subscriptions beyond network connectivity
- Any connected peer can subscribe and receive messages from publishers [9](#0-8) 

**Realistic Attack Paths:**
1. **Malicious Validator**: Byzantine validator runs malicious publisher
2. **Network Attack**: MITM injection if attacker controls network path
3. **Compromised VFN**: Compromised validator fullnode with publisher enabled

The lack of message size validation before deserialization makes exploitation straightforward.

## Recommendation

Implement multi-layered protection:

**1. Pre-deserialization Size Checks:**
Add explicit length checks during BCS deserialization using custom deserializers for critical vectors. Example:

```rust
// In observer_message.rs
const MAX_BLOCKS_PER_MESSAGE: usize = 100;
const MAX_TRANSACTIONS_PER_BLOCK: usize = 10000;

impl<'de> Deserialize<'de> for OrderedBlock {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: Deserializer<'de>,
    {
        #[derive(Deserialize)]
        struct OrderedBlockHelper {
            blocks: Vec<Arc<PipelinedBlock>>,
            ordered_proof: LedgerInfoWithSignatures,
        }
        
        let helper = OrderedBlockHelper::deserialize(deserializer)?;
        
        if helper.blocks.len() > MAX_BLOCKS_PER_MESSAGE {
            return Err(serde::de::Error::custom(format!(
                "Too many blocks: {} exceeds limit {}",
                helper.blocks.len(),
                MAX_BLOCKS_PER_MESSAGE
            )));
        }
        
        Ok(OrderedBlock {
            blocks: helper.blocks,
            ordered_proof: helper.ordered_proof,
        })
    }
}
```

**2. Message Authentication:**
Add authentication for consensus observer publishers to prevent unauthorized message injection.

**3. Memory Budget Tracking:**
Implement allocation tracking during deserialization with configurable limits.

**4. Rate Limiting:**
Add per-peer rate limits for consensus observer messages.

## Proof of Concept

```rust
#[cfg(test)]
mod deserialization_bomb_test {
    use super::*;
    use aptos_consensus_types::{
        block::Block,
        block_data::{BlockData, BlockType},
        quorum_cert::QuorumCert,
    };
    use aptos_types::{
        aggregate_signature::AggregateSignature,
        block_info::BlockInfo,
        ledger_info::{LedgerInfo, LedgerInfoWithSignatures},
    };
    
    #[test]
    #[should_panic(expected = "out of memory")]
    fn test_ordered_block_deserialization_bomb() {
        // Create a legitimate-looking but oversized OrderedBlock
        let mut blocks = Vec::new();
        
        // Create 100,000 blocks (memory bomb via breadth, not depth)
        for i in 0..100_000 {
            let block_info = BlockInfo::new(
                0,  // epoch
                i,  // round
                HashValue::random(),
                HashValue::random(),
                0,  // version
                0,  // timestamp
                None,
            );
            
            let block_data = BlockData::new_for_testing(
                0,
                i,
                0,
                QuorumCert::dummy(),
                BlockType::Genesis,
            );
            
            let block = Block::new_for_testing(block_info.id(), block_data, None);
            let pipelined_block = Arc::new(PipelinedBlock::new_ordered(
                block,
                OrderedBlockWindow::empty(),
            ));
            
            blocks.push(pipelined_block);
        }
        
        let ordered_proof = LedgerInfoWithSignatures::new(
            LedgerInfo::new(
                blocks.last().unwrap().block_info().clone(),
                HashValue::random(),
            ),
            AggregateSignature::empty(),
        );
        
        let ordered_block = OrderedBlock::new(blocks, ordered_proof);
        
        // Serialize to BCS
        let message = ConsensusObserverMessage::DirectSend(
            ConsensusObserverDirectSend::OrderedBlock(ordered_block)
        );
        
        let serialized = bcs::to_bytes(&message).unwrap();
        
        // Compress
        let compressed = aptos_compression::compress(
            serialized,
            aptos_compression::CompressionClient::ConsensusObserver,
            MAX_APPLICATION_MESSAGE_SIZE,
        ).unwrap();
        
        println!("Compressed size: {} bytes", compressed.len());
        
        // This deserialization will allocate gigabytes of memory
        // causing OOM or severe slowdown
        let decompressed = aptos_compression::decompress(
            &compressed,
            aptos_compression::CompressionClient::ConsensusObserver,
            MAX_APPLICATION_MESSAGE_SIZE,
        ).unwrap();
        
        // Deserialization triggers massive allocation
        let _decoded: ConsensusObserverMessage = 
            bcs::from_bytes_with_limit(&decompressed, 64).unwrap();
        
        // Node crashes or becomes unresponsive here
    }
}
```

## Notes

The vulnerability stems from a fundamental limitation of BCS recursion limits: they protect against **depth-based** attacks (stack overflow) but not **breadth-based** attacks (memory exhaustion). The `RECURSION_LIMIT = 64` constant protects call stack depth, but `Vec<T>` deserialization can allocate arbitrary amounts of memory for element storage.

Additional context:
- TypeTag structures have separate `MAX_TYPE_TAG_NESTING = 8` protection: [10](#0-9) 
- VM values have `DEFAULT_MAX_VM_VALUE_NESTED_DEPTH = 128` protection, but this doesn't apply to network deserialization
- The issue affects all `ConsensusObserverDirectSend` variants that contain unbounded vectors

### Citations

**File:** network/framework/src/protocols/wire/handshake/v1/mod.rs (L38-39)
```rust
pub const USER_INPUT_RECURSION_LIMIT: usize = 32;
pub const RECURSION_LIMIT: usize = 64;
```

**File:** network/framework/src/protocols/wire/handshake/v1/mod.rs (L156-172)
```rust
    fn encoding(self) -> Encoding {
        match self {
            ProtocolId::ConsensusDirectSendJson | ProtocolId::ConsensusRpcJson => Encoding::Json,
            ProtocolId::ConsensusDirectSendCompressed | ProtocolId::ConsensusRpcCompressed => {
                Encoding::CompressedBcs(RECURSION_LIMIT)
            },
            ProtocolId::ConsensusObserver => Encoding::CompressedBcs(RECURSION_LIMIT),
            ProtocolId::DKGDirectSendCompressed | ProtocolId::DKGRpcCompressed => {
                Encoding::CompressedBcs(RECURSION_LIMIT)
            },
            ProtocolId::JWKConsensusDirectSendCompressed
            | ProtocolId::JWKConsensusRpcCompressed => Encoding::CompressedBcs(RECURSION_LIMIT),
            ProtocolId::MempoolDirectSend => Encoding::CompressedBcs(USER_INPUT_RECURSION_LIMIT),
            ProtocolId::MempoolRpc => Encoding::Bcs(USER_INPUT_RECURSION_LIMIT),
            _ => Encoding::Bcs(RECURSION_LIMIT),
        }
    }
```

**File:** network/framework/src/protocols/wire/handshake/v1/mod.rs (L259-262)
```rust
    /// Deserializes the value using BCS encoding (with a specified limit)
    fn bcs_decode<T: DeserializeOwned>(&self, bytes: &[u8], limit: usize) -> anyhow::Result<T> {
        bcs::from_bytes_with_limit(bytes, limit).map_err(|e| anyhow!("{:?}", e))
    }
```

**File:** consensus/src/consensus_observer/network/network_handler.rs (L130-143)
```rust
                Some(network_message) = self.network_service_events.next() => {
                    // Unpack the network message
                    let NetworkMessage {
                        peer_network_id,
                        protocol_id: _,
                        consensus_observer_message,
                        response_sender,
                    } = network_message;

                    // Process the consensus observer message
                    match consensus_observer_message {
                        ConsensusObserverMessage::DirectSend(message) => {
                            self.handle_observer_message(peer_network_id, message);
                        },
```

**File:** consensus/src/consensus_observer/network/observer_message.rs (L180-184)
```rust
#[derive(Clone, Debug, Deserialize, Eq, PartialEq, Serialize)]
pub struct OrderedBlock {
    blocks: Vec<Arc<PipelinedBlock>>,
    ordered_proof: LedgerInfoWithSignatures,
}
```

**File:** consensus/src/consensus_observer/network/observer_message.rs (L225-266)
```rust
    /// Verifies the ordered blocks and returns an error if the data is invalid.
    /// Note: this does not check the ordered proof.
    pub fn verify_ordered_blocks(&self) -> Result<(), Error> {
        // Verify that we have at least one ordered block
        if self.blocks.is_empty() {
            return Err(Error::InvalidMessageError(
                "Received empty ordered block!".to_string(),
            ));
        }

        // Verify the last block ID matches the ordered proof block ID
        if self.last_block().id() != self.proof_block_info().id() {
            return Err(Error::InvalidMessageError(
                format!(
                    "Last ordered block ID does not match the ordered proof ID! Number of blocks: {:?}, Last ordered block ID: {:?}, Ordered proof ID: {:?}",
                    self.blocks.len(),
                    self.last_block().id(),
                    self.proof_block_info().id()
                )
            ));
        }

        // Verify the blocks are correctly chained together (from the last block to the first)
        let mut expected_parent_id = None;
        for block in self.blocks.iter().rev() {
            if let Some(expected_parent_id) = expected_parent_id {
                if block.id() != expected_parent_id {
                    return Err(Error::InvalidMessageError(
                        format!(
                            "Block parent ID does not match the expected parent ID! Block ID: {:?}, Expected parent ID: {:?}",
                            block.id(),
                            expected_parent_id
                        )
                    ));
                }
            }

            expected_parent_id = Some(block.parent_id());
        }

        Ok(())
    }
```

**File:** config/src/config/network_config.rs (L47-50)
```rust
pub const MAX_APPLICATION_MESSAGE_SIZE: usize =
    (MAX_MESSAGE_SIZE - MAX_MESSAGE_METADATA_SIZE) - MESSAGE_PADDING_SIZE; /* The message size that applications should check against */
pub const MAX_FRAME_SIZE: usize = 4 * 1024 * 1024; /* 4 MiB large messages will be chunked into multiple frames and streamed */
pub const MAX_MESSAGE_SIZE: usize = 64 * 1024 * 1024; /* 64 MiB */
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L167-208)
```rust
    /// Processes a network message received by the consensus publisher
    fn process_network_message(&self, network_message: ConsensusPublisherNetworkMessage) {
        // Unpack the network message
        let (peer_network_id, message, response_sender) = network_message.into_parts();

        // Update the RPC request counter
        metrics::increment_counter(
            &metrics::PUBLISHER_RECEIVED_REQUESTS,
            message.get_label(),
            &peer_network_id,
        );

        // Handle the message
        match message {
            ConsensusObserverRequest::Subscribe => {
                // Add the peer to the set of active subscribers
                self.add_active_subscriber(peer_network_id);
                info!(LogSchema::new(LogEntry::ConsensusPublisher)
                    .event(LogEvent::Subscription)
                    .message(&format!(
                        "New peer subscribed to consensus updates! Peer: {:?}",
                        peer_network_id
                    )));

                // Send a simple subscription ACK
                response_sender.send(ConsensusObserverResponse::SubscribeAck);
            },
            ConsensusObserverRequest::Unsubscribe => {
                // Remove the peer from the set of active subscribers
                self.remove_active_subscriber(&peer_network_id);
                info!(LogSchema::new(LogEntry::ConsensusPublisher)
                    .event(LogEvent::Subscription)
                    .message(&format!(
                        "Peer unsubscribed from consensus updates! Peer: {:?}",
                        peer_network_id
                    )));

                // Send a simple unsubscription ACK
                response_sender.send(ConsensusObserverResponse::UnsubscribeAck);
            },
        }
    }
```

**File:** third_party/move/move-core/types/src/safe_serialize.rs (L11-11)
```rust
pub(crate) const MAX_TYPE_TAG_NESTING: u8 = 8;
```
