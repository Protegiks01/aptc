# Audit Report

## Title
Storage Service Request Moderator Threshold Bypass via Strategic Peer Disconnection

## Summary
A malicious public network peer can indefinitely bypass the storage service's invalid request rate limiting by sending exactly `max_invalid_requests - 1` (default: 499) invalid requests, disconnecting before reaching the threshold, then reconnecting to reset their state. This allows unlimited invalid requests over time without ever being ignored.

## Finding Description

The `RequestModerator` in the storage service tracks invalid requests per peer using `UnhealthyPeerState` to prevent resource exhaustion. When a peer exceeds `max_invalid_requests_per_peer`, they should be temporarily ignored. However, a critical flaw exists in how this state is managed across disconnections.

**The Vulnerability:**

The `increment_invalid_request_count()` function only marks a peer as ignored when the count reaches or exceeds the threshold: [1](#0-0) 

The `refresh_peer_state()` function only resets the invalid request count when a peer is ALREADY ignored and sufficient time has elapsed: [2](#0-1) 

However, `refresh_unhealthy_peer_states()` removes disconnected peers entirely from tracking: [3](#0-2) 

When a peer reconnects, a completely new `UnhealthyPeerState` is created with `invalid_request_count = 0`: [4](#0-3) 

**Attack Path:**

1. Malicious peer connects to storage service on public network
2. Peer sends exactly 499 invalid storage requests (one below the default threshold of 500)
3. `invalid_request_count` reaches 499, but peer is NOT ignored since `499 < 500`
4. Peer voluntarily disconnects
5. Within 1 second (default `request_moderator_refresh_interval_ms`), the periodic refresh runs
6. Since peer is disconnected, their entry is completely removed from `unhealthy_peer_states`
7. Peer reconnects (with same or new PeerId)
8. First invalid request triggers creation of new `UnhealthyPeerState` with count = 0
9. Peer repeats steps 2-8 indefinitely, sending unlimited invalid requests

This breaks the **Resource Limits** invariant by allowing unlimited operations that should be rate-limited.

## Impact Explanation

**Severity: Medium** (per Aptos bug bounty categories)

While this vulnerability enables resource exhaustion, the impact is limited:

- Each invalid request consumes CPU cycles for validation checking, but these are lightweight operations (simple range comparisons in `DataSummary.can_service()`)
- A sophisticated attacker could run multiple malicious peers simultaneously to amplify the effect
- Storage service performance degradation could slow down state synchronization for legitimate peers
- Does NOT compromise consensus safety, state integrity, or cause fund loss
- Does NOT cause total network unavailability

The impact fits the **Medium Severity** category: "State inconsistencies requiring intervention" - while not directly causing state inconsistencies, it can degrade service availability requiring operator intervention to identify and block malicious peers at the network level.

If successfully exploited at scale, this could potentially escalate to **High Severity** ("Validator node slowdowns"), but the lightweight nature of request validation limits the practical impact.

## Likelihood Explanation

**Likelihood: High**

This vulnerability is highly likely to be exploitable because:

1. **Low Complexity**: The attack requires only basic network operations (connect, send requests, disconnect)
2. **No Special Privileges**: Any entity can run a public network peer
3. **Automated Exploitation**: The attack can be easily scripted to run continuously
4. **No Cryptographic Barriers**: PeerId can be reused or regenerated per connection
5. **Default Configuration Vulnerable**: Default values make this immediately exploitable without configuration changes

The only limiting factors are:
- Connection limits enforced by the peer manager (but multiple IPs can be used)
- Network bandwidth for establishing connections
- The validation overhead is small, reducing attacker motivation

## Recommendation

**Option 1: Persist State Across Disconnections (Recommended)**

Track invalid request counts in a time-windowed manner that persists across disconnections. Use a decay mechanism to gradually reduce counts over time rather than immediate reset:

```rust
pub struct UnhealthyPeerState {
    ignore_start_time: Option<Instant>,
    invalid_request_count: u64,
    last_invalid_request_time: Option<Instant>, // NEW
    invalid_request_window_secs: u64, // NEW: e.g., 3600 for 1-hour window
    max_invalid_requests: u64,
    min_time_to_ignore_secs: u64,
    time_service: TimeService,
}

pub fn increment_invalid_request_count(&mut self, peer_network_id: &PeerNetworkId) {
    // Decay old counts based on time window
    if let Some(last_time) = self.last_invalid_request_time {
        let elapsed = self.time_service.now().duration_since(last_time);
        if elapsed > Duration::from_secs(self.invalid_request_window_secs) {
            // Reset count after time window
            self.invalid_request_count = 0;
        }
    }
    
    self.invalid_request_count += 1;
    self.last_invalid_request_time = Some(self.time_service.now());
    
    // Rest of existing logic...
}
```

Modify `refresh_unhealthy_peer_states()` to NOT remove disconnected peers immediately, only after the time window expires:

```rust
self.unhealthy_peer_states.retain(|peer_network_id, unhealthy_peer_state| {
    if connected_peers_and_metadata.contains_key(peer_network_id) {
        // Peer is connected - refresh state
        unhealthy_peer_state.refresh_peer_state(peer_network_id);
        // ... existing logic
        true
    } else {
        // Peer is disconnected - only remove after time window expires
        if let Some(last_time) = unhealthy_peer_state.last_invalid_request_time {
            let elapsed = time_service.now().duration_since(last_time);
            elapsed > Duration::from_secs(unhealthy_peer_state.invalid_request_window_secs)
        } else {
            true // No requests yet, can keep or remove
        }
    }
});
```

**Option 2: Partial Reset on Disconnect**

When a peer disconnects, reduce their count by a factor (e.g., 50%) instead of complete removal, retaining some memory of misbehavior.

**Option 3: Connection Rate Limiting**

Add connection frequency tracking per PeerId and reject rapid reconnections from the same peer.

## Proof of Concept

```rust
#[tokio::test]
async fn test_threshold_bypass_via_disconnect() {
    use crate::tests::{mock::MockClient, utils};
    use aptos_config::config::StorageServiceConfig;
    use aptos_network::application::metadata::ConnectionState;
    
    // Create test data
    let highest_synced_version = 1000;
    let highest_synced_epoch = 10;
    
    // Configure low threshold for easier testing
    let max_invalid_requests_per_peer = 5;
    let storage_service_config = StorageServiceConfig {
        max_invalid_requests_per_peer,
        ..Default::default()
    };
    
    // Create storage client and server
    let (mut mock_client, mut service, _, time_service, peers_and_metadata) =
        MockClient::new(None, Some(storage_service_config));
    utils::update_storage_server_summary(
        &mut service,
        highest_synced_version,
        highest_synced_epoch,
    );
    
    let request_moderator = service.get_request_moderator();
    let unhealthy_peer_states = request_moderator.get_unhealthy_peer_states();
    
    // Connect a public network peer
    let peer_network_id = PeerNetworkId::new(NetworkId::Public, PeerId::random());
    peers_and_metadata
        .insert_connection_metadata(
            peer_network_id,
            create_connection_metadata(peer_network_id.peer_id(), 0),
        )
        .unwrap();
    
    tokio::spawn(service.start());
    
    // Exploitation loop - repeat 3 times to demonstrate unlimited bypass
    for round in 0..3 {
        // Send (max_invalid_requests - 1) invalid requests
        for _ in 0..(max_invalid_requests_per_peer - 1) {
            let response = send_invalid_transaction_request(
                highest_synced_version,
                &mut mock_client,
                peer_network_id,
            ).await;
            // Verify we get InvalidRequest error, NOT TooManyInvalidRequests
            assert_matches!(response.unwrap_err(), StorageServiceError::InvalidRequest(_));
        }
        
        // Verify peer is NOT ignored yet
        let state = unhealthy_peer_states.get(&peer_network_id).unwrap();
        assert!(!state.is_ignored());
        assert_eq!(state.invalid_request_count, max_invalid_requests_per_peer - 1);
        
        // Disconnect the peer
        peers_and_metadata
            .update_connection_state(peer_network_id, ConnectionState::Disconnecting)
            .unwrap();
        
        // Wait for garbage collection
        time_service.advance(Duration::from_secs(2));
        tokio::time::sleep(Duration::from_millis(100)).await;
        
        // Verify peer state was removed
        assert!(!unhealthy_peer_states.contains_key(&peer_network_id));
        
        // Reconnect the peer
        peers_and_metadata
            .update_connection_state(peer_network_id, ConnectionState::Connected)
            .unwrap();
        
        println!("Round {}: Successfully sent {} invalid requests without being blocked", 
                 round + 1, max_invalid_requests_per_peer - 1);
    }
    
    // Total: 12 invalid requests sent (3 rounds Ã— 4 requests) without ever being blocked
    // This demonstrates the threshold bypass vulnerability
}
```

**Notes**

The existing test `test_request_moderator_peer_garbage_collect` in the test file inadvertently demonstrates part of this vulnerability by showing that disconnected peer state is completely removed and recreated on reconnection: [5](#0-4) 

The vulnerability is exacerbated by two factors:

1. **PeerId Derivation**: PeerId is cryptographically derived from x25519 public keys, but an attacker can generate unlimited key pairs to appear as different peers entirely

2. **Refresh Frequency**: The default 1-second refresh interval means disconnected state is quickly garbage collected, enabling rapid exploitation cycles

The combination of complete state removal on disconnection and the ability to stay just below the threshold creates a practical bypass of the intended rate limiting protection.

### Citations

**File:** state-sync/storage-service/server/src/moderator.rs (L50-69)
```rust
    pub fn increment_invalid_request_count(&mut self, peer_network_id: &PeerNetworkId) {
        // Increment the invalid request count
        self.invalid_request_count += 1;

        // If the peer is a PFN and has sent too many invalid requests, start ignoring it
        if self.ignore_start_time.is_none()
            && peer_network_id.network_id().is_public_network()
            && self.invalid_request_count >= self.max_invalid_requests
        {
            // TODO: at some point we'll want to terminate the connection entirely

            // Start ignoring the peer
            self.ignore_start_time = Some(self.time_service.now());

            // Log the fact that we're now ignoring the peer
            warn!(LogSchema::new(LogEntry::RequestModeratorIgnoredPeer)
                .peer_network_id(peer_network_id)
                .message("Ignoring peer due to too many invalid requests!"));
        }
    }
```

**File:** state-sync/storage-service/server/src/moderator.rs (L79-98)
```rust
    pub fn refresh_peer_state(&mut self, peer_network_id: &PeerNetworkId) {
        if let Some(ignore_start_time) = self.ignore_start_time {
            let ignored_duration = self.time_service.now().duration_since(ignore_start_time);
            if ignored_duration >= Duration::from_secs(self.min_time_to_ignore_secs) {
                // Reset the invalid request count
                self.invalid_request_count = 0;

                // Reset the ignore start time
                self.ignore_start_time = None;

                // Double the min time to ignore the peer
                self.min_time_to_ignore_secs *= 2;

                // Log the fact that we're no longer ignoring the peer
                warn!(LogSchema::new(LogEntry::RequestModeratorIgnoredPeer)
                    .peer_network_id(peer_network_id)
                    .message("No longer ignoring peer! Enough time has elapsed."));
            }
        }
    }
```

**File:** state-sync/storage-service/server/src/moderator.rs (L161-178)
```rust
                let mut unhealthy_peer_state = self
                    .unhealthy_peer_states
                    .entry(*peer_network_id)
                    .or_insert_with(|| {
                        // Create a new unhealthy peer state (this is the first invalid request)
                        let max_invalid_requests =
                            self.storage_service_config.max_invalid_requests_per_peer;
                        let min_time_to_ignore_peers_secs =
                            self.storage_service_config.min_time_to_ignore_peers_secs;
                        let time_service = self.time_service.clone();

                        UnhealthyPeerState::new(
                            max_invalid_requests,
                            min_time_to_ignore_peers_secs,
                            time_service,
                        )
                    });
                unhealthy_peer_state.increment_invalid_request_count(peer_network_id);
```

**File:** state-sync/storage-service/server/src/moderator.rs (L213-228)
```rust
        self.unhealthy_peer_states
            .retain(|peer_network_id, unhealthy_peer_state| {
                if connected_peers_and_metadata.contains_key(peer_network_id) {
                    // Refresh the ignored peer state
                    unhealthy_peer_state.refresh_peer_state(peer_network_id);

                    // If the peer is ignored, increment the ignored peer count
                    if unhealthy_peer_state.is_ignored() {
                        num_ignored_peers += 1;
                    }

                    true // The peer is still connected, so we should keep it
                } else {
                    false // The peer is no longer connected, so we should remove it
                }
            });
```

**File:** state-sync/storage-service/server/src/tests/request_moderator.rs (L303-318)
```rust
    // Reconnect the first peer
    peers_and_metadata
        .update_connection_state(peer_network_ids[0], ConnectionState::Connected)
        .unwrap();

    // Send an invalid request from the first peer
    send_invalid_transaction_request(
        highest_synced_version,
        &mut mock_client,
        peer_network_ids[0],
    )
    .await
    .unwrap_err();

    // Verify the peer is now tracked as unhealthy
    assert!(unhealthy_peer_states.contains_key(&peer_network_ids[0]));
```
