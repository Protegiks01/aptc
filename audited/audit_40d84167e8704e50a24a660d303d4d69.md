# Audit Report

## Title
Storage-Memory Inconsistency Due to Concurrent QuorumCert Insertion Race Condition

## Summary
The `insert_single_quorum_cert()` function in the consensus BlockStore contains a write-after-write hazard where concurrent insertions of different QuorumCerts (QCs) for the same block can cause storage-memory inconsistencies. The in-memory BlockTree uses `or_insert_with` to preserve the first QC, while the persistent storage unconditionally overwrites with the last QC, creating a three-way inconsistency across PipelinedBlock, BlockTree, and persistent storage.

## Finding Description

The vulnerability occurs in the QuorumCert insertion logic: [1](#0-0) 

When two threads concurrently insert different QCs for the same block, the following race condition occurs:

**Thread A**: `insert_single_quorum_cert(qc1)` for block B
**Thread B**: `insert_single_quorum_cert(qc2)` for block B

The execution sequence:
1. Both threads validate their QCs against the block (lines 527-536) - both pass
2. Both threads call `pipelined_block.set_qc()` (line 547) - QC2 overwrites QC1 in the PipelinedBlock's mutex
3. Both threads call `storage.save_tree()` (lines 552-554) - QC2 overwrites QC1 in persistent storage
4. Both threads call `inner.write().insert_quorum_cert()` (line 555) - but the BlockTree uses `or_insert_with`: [2](#0-1) 

This causes QC1 to remain in the BlockTree while QC2 is written to storage.

**Storage Key Collision**: QCs are keyed by block hash in persistent storage: [3](#0-2) [4](#0-3) 

Multiple QCs for the same block share the same storage key, causing overwrites.

**Three-Way Inconsistency Result:**
- `PipelinedBlock.block_qc`: Contains QC2 (last set)
- `BlockTree.id_to_quorum_cert`: Contains QC1 (first inserted)  
- `Persistent Storage`: Contains QC2 (last written)

**Consensus Safety Violation**: After a crash and recovery, the node loads QC2 from storage but had been using QC1 during operation. If QC1 and QC2 have different `commit_info` fields (which block they commit), this violates the deterministic execution invariant - the node makes different consensus decisions before and after restart.

## Impact Explanation

This is a **High Severity** vulnerability per Aptos bug bounty criteria:

1. **Significant Protocol Violation**: Violates the "State Consistency" and "Deterministic Execution" invariants - nodes may behave differently after restart if they recover different QCs than they were using.

2. **Storage Corruption**: Lost updates in persistent storage mean QCs that validators received and processed can be silently discarded, potentially losing critical consensus proofs.

3. **Recovery Inconsistency**: Nodes that crash will recover with different state than they had during operation, potentially causing consensus divergence across the network as different nodes restart at different times.

While this requires specific conditions (concurrent QC delivery for the same block), it affects the critical consensus path and could be triggered during network instability or by malicious validators delivering multiple valid QCs.

## Likelihood Explanation

**Medium Likelihood**: 

This can occur when:
- Multiple validator sets form different QCs for the same block during network partitions
- A Byzantine validator crafts and distributes different QCs to different nodes
- High transaction throughput causes natural timing overlaps in QC delivery

The race window exists between storage write and in-memory tree update. While BFT protocols typically converge on a single QC per block, edge cases during epoch transitions, network partitions, or Byzantine behavior can produce multiple valid QCs with different signer sets but the same certified block.

## Recommendation

Introduce atomicity between storage persistence and in-memory state updates by holding the write lock during the entire operation:

```rust
pub fn insert_single_quorum_cert(&self, qc: QuorumCert) -> anyhow::Result<()> {
    match self.get_block(qc.certified_block().id()) {
        Some(pipelined_block) => {
            ensure!(
                pipelined_block
                    .block_info()
                    .match_ordered_only(qc.certified_block()),
                "QC for block {} has different {:?} than local {:?}",
                qc.certified_block().id(),
                qc.certified_block(),
                pipelined_block.block_info()
            );
            
            // Acquire write lock BEFORE storage operations
            let mut tree = self.inner.write();
            
            // Check if QC already exists - if so, return early
            if tree.get_quorum_cert_for_block(&qc.certified_block().id()).is_some() {
                return Ok(());
            }
            
            // Persist to storage while holding lock
            self.storage
                .save_tree(vec![], vec![qc.clone()])
                .context("Insert block failed when saving quorum")?;
            
            // Update in-memory state atomically
            pipelined_block.set_qc(Arc::new(qc.clone()));
            tree.insert_quorum_cert(qc)?;
            
            observe_block(pipelined_block.block().timestamp_usecs(), BlockStage::QC_ADDED);
        },
        None => bail!("Insert {} without having the block in store first", qc),
    };
    
    Ok(())
}
```

Alternative: Use compare-and-swap semantics in storage to detect and prevent overwrites.

## Proof of Concept

```rust
// Rust test demonstrating the race condition
#[tokio::test]
async fn test_concurrent_qc_insertion_race() {
    let (block_store, block, qc1, qc2) = setup_test_environment();
    
    // Insert block first
    block_store.insert_block(block.clone()).await.unwrap();
    
    // Spawn two concurrent QC insertions
    let store1 = block_store.clone();
    let store2 = block_store.clone();
    
    let handle1 = tokio::spawn(async move {
        store1.insert_single_quorum_cert(qc1.clone())
    });
    
    let handle2 = tokio::spawn(async move {
        store2.insert_single_quorum_cert(qc2.clone())
    });
    
    // Wait for both to complete
    let _ = tokio::join!(handle1, handle2);
    
    // Verify inconsistency
    let tree_qc = block_store.get_quorum_cert_for_block(block.id()).unwrap();
    let storage_qcs = block_store.storage.get_all_quorum_certs().unwrap();
    let storage_qc = storage_qcs.iter()
        .find(|qc| qc.certified_block().id() == block.id())
        .unwrap();
    
    // If tree_qc != storage_qc, the race condition occurred
    assert_ne!(
        tree_qc.ledger_info().signatures(),
        storage_qc.ledger_info().signatures(),
        "Storage-memory inconsistency detected"
    );
}
```

## Notes

The vulnerability requires two distinct valid QuorumCerts for the same block to be delivered concurrently. While the AptosBFT protocol should ideally produce identical QCs from all honest validators, Byzantine scenarios, network partitions, or implementation bugs in vote aggregation could produce multiple valid QCs with different signature sets. The storage schema design using block hash as the sole key exacerbates this issue by making overwrites inevitable when multiple QCs exist for the same block.

### Citations

**File:** consensus/src/block_storage/block_store.rs (L519-556)
```rust
    pub fn insert_single_quorum_cert(&self, qc: QuorumCert) -> anyhow::Result<()> {
        // If the parent block is not the root block (i.e not None), ensure the executed state
        // of a block is consistent with its QuorumCert, otherwise persist the QuorumCert's
        // state and on restart, a new execution will agree with it.  A new execution will match
        // the QuorumCert's state on the next restart will work if there is a memory
        // corruption, for example.
        match self.get_block(qc.certified_block().id()) {
            Some(pipelined_block) => {
                ensure!(
                    // decoupled execution allows dummy block infos
                    pipelined_block
                        .block_info()
                        .match_ordered_only(qc.certified_block()),
                    "QC for block {} has different {:?} than local {:?}",
                    qc.certified_block().id(),
                    qc.certified_block(),
                    pipelined_block.block_info()
                );
                observe_block(
                    pipelined_block.block().timestamp_usecs(),
                    BlockStage::QC_ADDED,
                );
                if pipelined_block.block().is_opt_block() {
                    observe_block(
                        pipelined_block.block().timestamp_usecs(),
                        BlockStage::QC_ADDED_OPT_BLOCK,
                    );
                }
                pipelined_block.set_qc(Arc::new(qc.clone()));
            },
            None => bail!("Insert {} without having the block in store first", qc),
        };

        self.storage
            .save_tree(vec![], vec![qc.clone()])
            .context("Insert block failed when saving quorum")?;
        self.inner.write().insert_quorum_cert(qc)
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L376-378)
```rust
        self.id_to_quorum_cert
            .entry(block_id)
            .or_insert_with(|| Arc::clone(&qc));
```

**File:** consensus/src/consensusdb/schema/quorum_certificate/mod.rs (L7-10)
```rust
//! ```text
//! |<---key---->|<----value--->|
//! | block_hash |  QuorumCert  |
//! ```
```

**File:** consensus/src/consensusdb/mod.rs (L133-135)
```rust
        qc_data
            .iter()
            .try_for_each(|qc| batch.put::<QCSchema>(&qc.certified_block().id(), qc))?;
```
