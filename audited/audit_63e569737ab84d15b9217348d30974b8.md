# Audit Report

## Title
EventStorePruner and TransactionAccumulatorPruner Desynchronization Leading to Unprovable Event Data

## Summary
The `EventStorePruner` and `TransactionAccumulatorPruner` can prune at different rates due to lack of atomic coordination between their independent progress tracking, causing events to persist while their corresponding transaction accumulator nodes are deleted. This makes affected events unprovable and breaks state sync operations.

## Finding Description

The `LedgerPruner` coordinates multiple sub-pruners using parallel execution, but each sub-pruner maintains independent progress metadata and writes its own database batch atomically without cross-pruner coordination. [1](#0-0) 

Both `TransactionAccumulatorPruner` and `EventStorePruner` write their own progress metadata independently within their respective `prune()` methods: [2](#0-1) [3](#0-2) 

Each pruner uses its own metadata key for progress tracking: [4](#0-3) 

**Attack Scenario:**

1. Both pruners are called in parallel with `prune(1000, 2000)`
2. `TransactionAccumulatorPruner` successfully completes and writes its batch, updating its progress to 2000 and deleting transaction accumulator nodes for versions 1000-2000
3. System crashes or `EventStorePruner` fails before writing its batch
4. On restart, `TransactionAccumulatorPruner` progress is 2000, but `EventStorePruner` progress is still 1000
5. During initialization, each pruner catches up individually using `get_or_initialize_subpruner_progress`: [5](#0-4) 

6. Since `TransactionAccumulatorPruner` is ahead of `metadata_progress`, its catch-up call `prune(2000, 1000)` is a no-op (begin >= end in the pruning loop)
7. Events for versions 1000-2000 now exist without their transaction accumulator nodes

**Impact on Proof Generation:**

When attempting to retrieve events with proofs, the system calls `get_transaction_info_with_proof()` which requires transaction accumulator nodes: [6](#0-5) 

The proof generation fails because `get_transaction_proof()` cannot read the deleted accumulator nodes: [7](#0-6) 

This breaks the **State Consistency** invariant (#4) which requires that state transitions be atomic and verifiable via Merkle proofs. Events become unverifiable orphaned data.

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos bug bounty criteria as it causes "State inconsistencies requiring intervention." Specifically:

- Events for affected version ranges exist but cannot be proven against the ledger
- State sync operations fail when attempting to verify events in the affected range
- Nodes attempting to sync from peers cannot validate event data, causing sync failures
- The inconsistency persists until manual database intervention or full resync
- Does not directly cause consensus failure or fund loss, but severely impacts network availability for syncing nodes

The issue affects all nodes that experience the failure scenario during pruning operations.

## Likelihood Explanation

This vulnerability has **moderate likelihood** of occurrence:

**Triggering Conditions:**
- System crash during pruning operations (hardware failure, OOM killer, power loss)
- Disk I/O errors during batch writes
- Database write failures under resource exhaustion
- Any failure that occurs after one pruner completes but before another finishes

**Frequency:**
- Pruning runs continuously on production validators with significant ledger history
- Each pruning batch processes thousands of versions, creating multiple failure windows
- No checkpointing or rollback mechanism exists between parallel pruner operations

**Real-World Scenarios:**
- Validator node crashes during maintenance windows
- Disk failures on high-throughput validators
- Resource exhaustion during peak network load

## Recommendation

Implement atomic coordination between sub-pruners by ensuring they cannot desynchronize. Two approaches:

**Option 1: Shared Progress Tracking (Preferred)**

Modify the LedgerPruner to only update sub-pruner progress after ALL sub-pruners succeed, using a shared transaction:

```rust
// In LedgerPruner::prune()
let mut shared_batch = SchemaBatch::new();

// Run pruning operations without updating progress
THREAD_MANAGER.get_background_pool().install(|| {
    self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
        let mut pruner_batch = SchemaBatch::new();
        sub_pruner.prune_without_progress_update(progress, current_batch_target_version, &mut pruner_batch)?;
        // Merge into shared batch (requires synchronization)
        Ok(())
    })
})?;

// Only update all progress metadata AFTER all pruners succeed
for sub_pruner in &self.sub_pruners {
    shared_batch.put::<DbMetadataSchema>(
        sub_pruner.progress_key(),
        &DbMetadataValue::Version(current_batch_target_version),
    )?;
}

// Atomic write of all changes
self.ledger_db.write_schemas(shared_batch)?;
```

**Option 2: Progress Validation on Startup**

Add validation during initialization to detect and correct desynchronization:

```rust
// In LedgerPruner::new()
let mut min_progress = Version::MAX;
let mut max_progress = Version::MIN;

// Detect desynchronization
for sub_pruner in &sub_pruners {
    let progress = sub_pruner.get_progress()?;
    min_progress = min_progress.min(progress);
    max_progress = max_progress.max(progress);
}

if max_progress > min_progress {
    warn!("Detected sub-pruner desynchronization: min={}, max={}", min_progress, max_progress);
    // Force all pruners back to minimum safe progress
    for sub_pruner in &sub_pruners {
        sub_pruner.reset_progress(min_progress)?;
    }
}
```

The first option provides stronger guarantees but requires architectural changes. The second option is a defensive measure that prevents inconsistent states from persisting.

## Proof of Concept

To reproduce this vulnerability, create a test that simulates partial pruner failure:

```rust
#[test]
fn test_pruner_desynchronization() {
    // Setup: Create AptosDB with events and transaction accumulator
    let tmpdir = TempPath::new();
    let db = AptosDB::new_for_test(&tmpdir);
    
    // Commit 3000 transactions with events
    for i in 0..3000 {
        let txn = create_test_transaction_with_events(i);
        db.save_transactions(&[txn], i, i, None, true, None).unwrap();
    }
    
    // Simulate TransactionAccumulatorPruner completing successfully
    {
        let mut batch = SchemaBatch::new();
        TransactionAccumulatorDb::prune(0, 2000, &mut batch).unwrap();
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::TransactionAccumulatorPrunerProgress,
            &DbMetadataValue::Version(2000),
        ).unwrap();
        db.ledger_db().transaction_accumulator_db().write_schemas(batch).unwrap();
    }
    
    // Simulate EventStorePruner NOT updating (crash before write)
    // EventPrunerProgress remains at 0
    
    // Restart and attempt to get events with proof for version 1500
    let result = db.get_transaction_with_proof(1500, 3000, true);
    
    // This should fail because accumulator nodes are deleted
    match result {
        Err(e) if e.to_string().contains("does not exist") => {
            // Expected: accumulator nodes are missing
            println!("Vulnerability confirmed: Events exist but accumulator nodes deleted");
        },
        Ok(_) => panic!("Expected failure due to missing accumulator nodes"),
        Err(e) => panic!("Unexpected error: {}", e),
    }
}
```

This test demonstrates that when `TransactionAccumulatorPruner` advances beyond `EventStorePruner`, events become unprovable due to missing accumulator nodes, confirming the vulnerability.

## Notes

The vulnerability is exacerbated by the fact that `error_if_ledger_pruned()` only checks against the global `min_readable_version` maintained by `LedgerPrunerManager`: [8](#0-7) 

This check is based on the overall ledger pruner progress, not individual sub-pruner progress, so it cannot detect the desynchronization between event data and accumulator nodes. The inconsistency only manifests when attempting to generate proofs for the affected version range.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L78-84)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_accumulator_pruner.rs (L25-35)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        TransactionAccumulatorDb::prune(current_progress, target_version, &mut batch)?;
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::TransactionAccumulatorPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
        self.ledger_db
            .transaction_accumulator_db()
            .write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L43-81)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        let mut indexer_batch = None;

        let indices_batch = if let Some(indexer_db) = self.indexer_db() {
            if indexer_db.event_enabled() {
                indexer_batch = Some(SchemaBatch::new());
            }
            indexer_batch.as_mut()
        } else {
            Some(&mut batch)
        };
        let num_events_per_version = self.ledger_db.event_db().prune_event_indices(
            current_progress,
            target_version,
            indices_batch,
        )?;
        self.ledger_db.event_db().prune_events(
            num_events_per_version,
            current_progress,
            target_version,
            &mut batch,
        )?;
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::EventPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        if let Some(mut indexer_batch) = indexer_batch {
            indexer_batch.put::<InternalIndexerMetadataSchema>(
                &IndexerMetadataKey::EventPrunerProgress,
                &IndexerMetadataValue::Version(target_version),
            )?;
            self.expect_indexer_db()
                .get_inner_db_ref()
                .write_schemas(indexer_batch)?;
        }
        self.ledger_db.event_db().write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/schema/db_metadata/mod.rs (L61-62)
```rust
    EventPrunerProgress,
    TransactionAccumulatorPrunerProgress,
```

**File:** storage/aptosdb/src/pruner/pruner_utils.rs (L44-60)
```rust
pub(crate) fn get_or_initialize_subpruner_progress(
    sub_db: &DB,
    progress_key: &DbMetadataKey,
    metadata_progress: Version,
) -> Result<Version> {
    Ok(
        if let Some(v) = sub_db.get::<DbMetadataSchema>(progress_key)? {
            v.expect_version()
        } else {
            sub_db.put::<DbMetadataSchema>(
                progress_key,
                &DbMetadataValue::Version(metadata_progress),
            )?;
            metadata_progress
        },
    )
}
```

**File:** storage/aptosdb/src/ledger_db/transaction_info_db.rs (L73-83)
```rust
    pub(crate) fn get_transaction_info_with_proof(
        &self,
        version: Version,
        ledger_version: Version,
        transaction_accumulator_db: &TransactionAccumulatorDb,
    ) -> Result<TransactionInfoWithProof> {
        Ok(TransactionInfoWithProof::new(
            transaction_accumulator_db.get_transaction_proof(version, ledger_version)?,
            self.get_transaction_info(version)?,
        ))
    }
```

**File:** storage/aptosdb/src/ledger_db/transaction_accumulator_db.rs (L66-73)
```rust
    pub fn get_transaction_proof(
        &self,
        version: Version,
        ledger_version: Version,
    ) -> Result<TransactionAccumulatorProof> {
        Accumulator::get_proof(self, ledger_version + 1 /* num_leaves */, version)
            .map_err(Into::into)
    }
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L261-271)
```rust
    pub(super) fn error_if_ledger_pruned(&self, data_type: &str, version: Version) -> Result<()> {
        let min_readable_version = self.ledger_pruner.get_min_readable_version();
        ensure!(
            version >= min_readable_version,
            "{} at version {} is pruned, min available version is {}.",
            data_type,
            version,
            min_readable_version
        );
        Ok(())
    }
```
