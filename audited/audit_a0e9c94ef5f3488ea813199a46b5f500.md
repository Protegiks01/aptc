# Audit Report

## Title
Silent Network Stream Termination Causes Partial State Sync Availability Degradation

## Summary
The storage service's use of `select_all()` to merge multiple network event streams lacks monitoring for individual stream termination. When one network's event stream closes due to infrastructure failure, the storage service continues serving only the remaining networks without any error indication, metrics update, or alert, creating a silent partial network partition that degrades state sync availability.

## Finding Description

The storage service is designed to serve state sync requests from multiple networks simultaneously (Validator, VFN, Public networks). [1](#0-0) 

The implementation uses `select_all()` to merge network event streams without any validation or monitoring. [2](#0-1) 

When the storage service starts, it simply polls the merged stream without checking if all expected networks remain active. [3](#0-2) 

The Rust futures `select_all()` function has specific behavior: when any underlying stream ends (returns `Poll::Ready(None)`), it removes that stream from the merge set and continues yielding items from remaining streams. The merged stream only terminates when ALL underlying streams have ended.

This creates a silent failure mode where:
1. A network event stream closes (e.g., due to peer manager crash, network runtime failure)
2. `select_all()` silently removes the failed stream
3. Storage service continues serving only remaining networks
4. Peers on the failed network cannot reach this storage service
5. No error is logged, no metric indicates the failure, no alert is triggered

Unlike consensus, which explicitly validates it has exactly one network and panics otherwise [4](#0-3) , the storage service has no such validation.

The metrics system tracks various storage service events [5](#0-4)  but does not track the number of active network streams or detect when streams terminate.

## Impact Explanation

**Severity Assessment: High**

This issue causes **validator node slowdowns** and **significant protocol violations** as defined in the Aptos bug bounty program's High severity category ($50,000):

1. **State Sync Degradation**: Nodes experiencing this issue can only serve state sync requests from a subset of networks, forcing peers on failed networks to find alternative sync sources, increasing sync latency network-wide.

2. **Cascading Availability Failure**: If multiple nodes experience the same network failure pattern, entire network segments may be unable to sync state efficiently, particularly affecting:
   - New validators joining the network
   - Nodes recovering from downtime
   - Full nodes catching up with the chain

3. **Silent Degradation**: The lack of monitoring means operators cannot detect or diagnose the issue quickly, prolonging the availability impact.

4. **No Automatic Recovery**: Once a network stream closes, the storage service never attempts to reconnect or restart the failed stream.

However, this does NOT qualify as Critical severity because:
- It does not cause consensus safety violations (existing synced validators continue normally)
- It does not cause permanent network partition (requires hardfork)
- It does not affect funds or cause total liveness loss
- The network can recover through manual intervention or node restarts

## Likelihood Explanation

**Likelihood: Medium to Low**

This issue requires network infrastructure failure as a trigger:

**Triggering Conditions:**
1. Peer manager crash for a specific network
2. Network runtime panic or termination
3. Channel closure due to memory pressure or bugs
4. Graceful shutdown of one network while others continue

**Factors Increasing Likelihood:**
- Multi-network deployments are standard (Validator + VFN networks)
- Network infrastructure bugs could affect one network independently
- Resource exhaustion could selectively impact specific networks
- Long-running nodes accumulate state that might trigger edge cases

**Factors Decreasing Likelihood:**
- Network infrastructure is generally stable
- Most failures affect all networks simultaneously (node restart)
- Peer managers rarely crash independently
- Active monitoring usually detects node issues before partial failures

**Exploitability:** An unprivileged attacker CANNOT directly trigger this condition without pre-existing infrastructure vulnerabilities or access. This is a robustness/monitoring gap, not an exploitable security vulnerability.

## Recommendation

Implement active monitoring and validation for network stream health:

```rust
// In StorageServiceNetworkEvents::new()
pub fn new(network_service_events: NetworkServiceEvents<StorageServiceMessage>) -> Self {
    let network_and_events = network_service_events.into_network_and_events();
    
    // Track expected networks
    let expected_networks: HashSet<NetworkId> = network_and_events.keys().cloned().collect();
    info!(
        "Storage service initialized with networks: {:?}",
        expected_networks
    );
    
    // Add monitoring for each network stream
    let network_events: Vec<_> = network_and_events
        .into_iter()
        .map(|(network_id, events)| {
            events
                .map(move |event| (network_id, event))
                .chain(futures::stream::once(async move {
                    // Log when stream ends
                    error!(
                        "Storage service network stream terminated: {:?}",
                        network_id
                    );
                    counters::STORAGE_NETWORK_STREAM_TERMINATED
                        .with_label_values(&[network_id.as_str()])
                        .inc();
                    // Return None to signal termination while logging
                    None
                }))
                .filter_map(|x| futures::future::ready(x))
        })
        .collect();
    
    let network_events = select_all(network_events).fuse();
    // ... rest of implementation
}
```

Additionally:
1. Add a metric `STORAGE_NETWORK_STREAM_TERMINATED` to track stream closures
2. Add a periodic health check that validates all expected networks are still active
3. Implement automatic reconnection logic when a stream terminates unexpectedly
4. Add alerting when network count drops below expected value

## Proof of Concept

This PoC demonstrates the silent stream termination behavior:

```rust
// test_network_stream_termination.rs
#[tokio::test]
async fn test_silent_network_stream_termination() {
    use futures::stream::{select_all, StreamExt};
    use aptos_channels::aptos_channel;
    use std::collections::HashMap;
    
    // Create two network streams
    let (mut sender1, receiver1) = aptos_channel::new(QueueStyle::FIFO, 10, None);
    let (mut sender2, receiver2) = aptos_channel::new(QueueStyle::FIFO, 10, None);
    
    let stream1 = receiver1.map(|msg| ("Network1", msg));
    let stream2 = receiver2.map(|msg| ("Network2", msg));
    
    let mut merged = select_all(vec![stream1, stream2]);
    
    // Send messages from both networks
    sender1.push((), "msg1_from_net1").unwrap();
    sender2.push((), "msg1_from_net2").unwrap();
    
    // Receive both messages
    assert_eq!(merged.next().await, Some(("Network1", "msg1_from_net1")));
    assert_eq!(merged.next().await, Some(("Network2", "msg1_from_net2")));
    
    // Drop sender1 to simulate network1 failure
    drop(sender1);
    
    // Send message from network2
    sender2.push((), "msg2_from_net2").unwrap();
    
    // Merged stream continues without any error indication
    // This demonstrates the silent failure mode
    assert_eq!(merged.next().await, Some(("Network2", "msg2_from_net2")));
    
    // Network1 is now silently unavailable
    // No error, no metric, no log
    println!("Network1 stream terminated silently!");
}
```

To reproduce in production:
1. Deploy a validator with multiple networks (Validator + VFN)
2. Simulate network manager crash for one network (inject fault into peer manager)
3. Observe that storage service continues serving only the remaining network
4. Check metrics - no indication of network stream termination
5. Attempt state sync from failed network - requests timeout without clear error

**Notes:**
- This is a monitoring and robustness issue rather than a directly exploitable vulnerability
- Requires infrastructure failure as trigger, not attacker action
- Impact is availability degradation, not security compromise
- Recommended fixes improve operational visibility and resilience

### Citations

**File:** state-sync/storage-service/server/src/network.rs (L40-59)
```rust
    pub fn new(network_service_events: NetworkServiceEvents<StorageServiceMessage>) -> Self {
        // Transform the event streams to also include the network ID
        let network_events: Vec<_> = network_service_events
            .into_network_and_events()
            .into_iter()
            .map(|(network_id, events)| events.map(move |event| (network_id, event)))
            .collect();
        let network_events = select_all(network_events).fuse();

        // Transform each event to a network request
        let network_request_stream = network_events
            .filter_map(|(network_id, event)| {
                future::ready(Self::event_to_request(network_id, event))
            })
            .boxed();

        Self {
            network_request_stream,
        }
    }
```

**File:** state-sync/storage-service/server/src/lib.rs (L388-390)
```rust
        // Handle the storage requests as they arrive
        while let Some(network_request) = self.network_requests.next().await {
            // All handler methods are currently CPU-bound and synchronous
```

**File:** consensus/src/network.rs (L771-777)
```rust
        // Verify the network events have been constructed correctly
        let network_and_events = network_service_events.into_network_and_events();
        if (network_and_events.values().len() != 1)
            || !network_and_events.contains_key(&NetworkId::Validator)
        {
            panic!("The network has not been setup correctly for consensus!");
        }
```

**File:** state-sync/storage-service/server/src/metrics.rs (L94-102)
```rust
/// Counter for pending network events to the storage service (server-side)
pub static PENDING_STORAGE_SERVER_NETWORK_EVENTS: Lazy<IntCounterVec> = Lazy::new(|| {
    register_int_counter_vec!(
        "aptos_storage_service_server_pending_network_events",
        "Counters for pending network events for the storage server",
        &["state"]
    )
    .unwrap()
});
```
