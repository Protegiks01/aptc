# Audit Report

## Title
Out-of-Memory Crash in TransactionPruner During Initialization Catch-Up Phase

## Summary
The `get_pruning_candidate_transactions()` function in `TransactionPruner` pre-allocates a `Vec` with capacity `(end - start)` and loads all transactions in that range into memory. During the initialization catch-up phase, this function is called without batching constraints, allowing `end - start` to be arbitrarily large (potentially 50-100+ million versions). This causes out-of-memory (OOM) errors and crashes validator nodes when pruning has been delayed or disabled.

## Finding Description

The vulnerability exists in the initialization flow of `TransactionPruner`: [1](#0-0) 

During initialization, `TransactionPruner::new()` calls `myself.prune(progress, metadata_progress)` without any batching mechanism. When pruning has been delayed (node offline, pruning temporarily disabled, or first startup after long sync), `metadata_progress - progress` can be tens or hundreds of millions of versions.

This directly invokes the `prune()` method which calls `get_pruning_candidate_transactions()`: [2](#0-1) 

The `get_pruning_candidate_transactions()` function contains the critical flaw: [3](#0-2) 

On line 121, `Vec::with_capacity((end - start) as usize)` pre-allocates memory for potentially millions of `(Version, Transaction)` tuples. The loop then loads ALL transactions from disk into memory. The comment on lines 119-120 incorrectly claims this is "capped by the max number of txns we prune in a single batch," but this is FALSE during initialization—the batch size constraint only applies during normal operation through `LedgerPruner::prune()`.

**Normal Operation (Batched):** [4](#0-3) 

Line 68 ensures `current_batch_target_version = min(progress + max_versions, target_version)`, capping the range. However, this batching logic is NOT applied during the sub-pruner initialization catch-up.

**Configuration:** [5](#0-4) 

The default `batch_size` is 5,000, but this is bypassed during initialization.

**Memory Impact Calculation:**
- If `metadata_progress - progress = 50,000,000` (realistic for delayed pruning)
- `Transaction` enum size: ~200-400 bytes (including discriminant and largest variant)
- Memory allocation: 50M × 300 bytes ≈ **15GB**
- Plus actual transaction data loaded from disk
- Result: **OOM crash on validator nodes with limited RAM**

This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits."

## Impact Explanation

**Severity: HIGH**

Per Aptos Bug Bounty criteria, this qualifies as **High Severity** because:
1. "Validator node slowdowns" is explicitly listed as High severity (up to $50,000)
2. This vulnerability causes complete **validator node crashes** (worse than slowdowns)
3. Affects **network availability** when multiple validators crash during restart

**Affected Scenarios:**
- Validator restarts after extended downtime (hours/days)
- Pruning temporarily disabled then re-enabled
- Nodes catching up after network partition
- New validators syncing with pruning enabled

**Impact Quantification:**
- Single validator crash: Reduced consensus participation
- Multiple validators affected: Network liveness degradation
- Persistent crashes: Validators unable to restart, requiring manual intervention

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

This vulnerability will trigger in realistic operational scenarios:

1. **Extended Node Downtime:** Validators experiencing hardware failures, maintenance, or software bugs that keep them offline while the chain progresses 50M+ versions (approximately 1-2 days at 5,000 TPS)

2. **Pruning Configuration Changes:** Operators temporarily disabling pruning to conserve I/O resources during high load, then re-enabling it

3. **Database Migration/Recovery:** Validators restoring from backups that don't include recent pruning progress

4. **Chain Growth:** As Aptos processes billions of transactions, the gap between pruner progress and chain tip grows larger during any outage

The default `prune_window` of 90 million versions means even brief outages can create dangerous catch-up scenarios.

**Attacker Requirements:** None—this is a bug exploitable through normal operations, not requiring malicious intent. However, an adversary with knowledge of this vulnerability could indirectly exploit it by causing validator downtime (separate attack), knowing the restart will trigger OOM.

## Recommendation

Implement batched catch-up during sub-pruner initialization. The fix should apply the same batching logic used in normal operation:

```rust
impl TransactionPruner {
    pub(in crate::pruner) fn new(
        transaction_store: Arc<TransactionStore>,
        ledger_db: Arc<LedgerDb>,
        metadata_progress: Version,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.transaction_db_raw(),
            &DbMetadataKey::TransactionPrunerProgress,
            metadata_progress,
        )?;

        let myself = TransactionPruner {
            transaction_store,
            ledger_db,
            internal_indexer_db,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up TransactionPruner."
        );
        
        // FIX: Apply batching during catch-up
        const CATCHUP_BATCH_SIZE: u64 = 5_000; // Match default batch_size
        let mut current_progress = progress;
        while current_progress < metadata_progress {
            let batch_target = std::cmp::min(
                current_progress + CATCHUP_BATCH_SIZE,
                metadata_progress
            );
            myself.prune(current_progress, batch_target)?;
            current_progress = batch_target;
        }

        Ok(myself)
    }
}
```

**Alternative:** Modify `get_pruning_candidate_transactions()` to enforce a maximum capacity:

```rust
fn get_pruning_candidate_transactions(
    &self,
    start: Version,
    end: Version,
) -> Result<Vec<(Version, Transaction)>> {
    ensure!(end >= start, "{} must be >= {}", end, start);
    
    // FIX: Cap the range to prevent OOM
    const MAX_BATCH_SIZE: u64 = 10_000;
    if end - start > MAX_BATCH_SIZE {
        return Err(AptosDbError::Other(format!(
            "Transaction pruning range too large: {} (max: {}). Use batched pruning.",
            end - start, MAX_BATCH_SIZE
        )));
    }

    // ... rest of function
}
```

The batched initialization approach is preferred as it maintains the catch-up functionality while preventing OOM.

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use aptos_temppath::TempPath;
    
    #[test]
    #[should_panic(expected = "memory allocation")]
    fn test_transaction_pruner_oom_on_large_catchup() {
        // Setup: Create a test database with 50M versions
        let tmpdir = TempPath::new();
        let db = AptosDB::new_for_test(&tmpdir);
        
        // Simulate 50M transaction versions
        let metadata_progress: Version = 50_000_000;
        let pruner_progress: Version = 0;
        
        // Manually set pruner progress to 0 while metadata is at 50M
        // This simulates delayed pruning scenario
        db.ledger_db()
            .transaction_db_raw()
            .put::<DbMetadataSchema>(
                &DbMetadataKey::TransactionPrunerProgress,
                &DbMetadataValue::Version(pruner_progress),
            )
            .unwrap();
            
        // Attempt to create TransactionPruner
        // This will call prune(0, 50_000_000) which attempts to allocate
        // Vec::with_capacity(50_000_000) causing OOM
        let transaction_store = Arc::new(TransactionStore::new(db.ledger_db()));
        let _pruner = TransactionPruner::new(
            transaction_store,
            db.ledger_db(),
            metadata_progress,
            None,
        );
        
        // If not enough memory, this panics with OOM
        // On systems with sufficient RAM, this will hang loading 50M transactions
    }
}
```

**Real-World Reproduction Steps:**
1. Start a validator node with pruning enabled
2. Stop the node and disable pruning in config (`ledger_pruner_config.enable: false`)
3. Let the network progress by 50M+ versions (~1-2 days at 5K TPS)
4. Re-enable pruning in config (`ledger_pruner_config.enable: true`)
5. Restart the validator node
6. Observe OOM crash during `TransactionPruner::new()` initialization

**Notes:**

This vulnerability specifically affects `TransactionPruner` due to its pattern of loading all candidate transactions into memory. Other sub-pruners like `TransactionInfoPruner` and `EventStorePruner` follow the same initialization pattern but don't exhibit the same memory issue as they prune directly without loading data into a large Vec first.

The misleading comment in the code (lines 119-120) suggests developers believed the batch size constraint was enforced, but architectural oversight allowed the initialization path to bypass this protection.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L37-40)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        let candidate_transactions =
            self.get_pruning_candidate_transactions(current_progress, target_version)?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L78-104)
```rust
    pub(in crate::pruner) fn new(
        transaction_store: Arc<TransactionStore>,
        ledger_db: Arc<LedgerDb>,
        metadata_progress: Version,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.transaction_db_raw(),
            &DbMetadataKey::TransactionPrunerProgress,
            metadata_progress,
        )?;

        let myself = TransactionPruner {
            transaction_store,
            ledger_db,
            internal_indexer_db,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up TransactionPruner."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L106-131)
```rust
    fn get_pruning_candidate_transactions(
        &self,
        start: Version,
        end: Version,
    ) -> Result<Vec<(Version, Transaction)>> {
        ensure!(end >= start, "{} must be >= {}", end, start);

        let mut iter = self
            .ledger_db
            .transaction_db_raw()
            .iter::<TransactionSchema>()?;
        iter.seek(&start)?;

        // The capacity is capped by the max number of txns we prune in a single batch. It's a
        // relatively small number set in the config, so it won't cause high memory usage here.
        let mut txns = Vec::with_capacity((end - start) as usize);
        for item in iter {
            let (version, txn) = item?;
            if version >= end {
                break;
            }
            txns.push((version, txn));
        }

        Ok(txns)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L62-92)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning ledger data."
            );
            self.ledger_metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning ledger data is done.");
        }

        Ok(target_version)
    }
```

**File:** config/src/config/storage_config.rs (L387-396)
```rust
impl Default for LedgerPrunerConfig {
    fn default() -> Self {
        LedgerPrunerConfig {
            enable: true,
            prune_window: 90_000_000,
            batch_size: 5_000,
            user_pruning_window_offset: 200_000,
        }
    }
}
```
