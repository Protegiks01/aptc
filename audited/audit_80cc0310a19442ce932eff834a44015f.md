# Audit Report

## Title
Resource Group Snapshot Isolation Violation During Parallel Transaction Execution

## Summary
Resource group reads in Aptos BlockSTM can violate snapshot isolation at the group granularity during parallel transaction execution. When a transaction reads multiple tags from the same resource group, each tag is read independently without enforcing that all tags come from a consistent snapshot (same transaction version). This allows transactions to observe logically inconsistent intermediate states during speculative execution.

## Finding Description

The vulnerability exists in how resource group reads are performed through the `ResourceGroupResolver` interface during parallel transaction execution.

**Root Cause - Non-Atomic Group Reads:** [1](#0-0) 

When reading multiple tags from the same resource group, each tag is fetched independently via separate calls to `fetch_tagged_data_and_record_dependency`. The code explicitly acknowledges the non-atomic access between checking group initialization and fetching data values. However, there is no mechanism to ensure all tags within a group are read from the same transaction version.

**Per-Tag Read Pattern:** [2](#0-1) 

Each tag read is performed separately through `read_cached_group_tagged_data_by_kind`, with no coordination to ensure group-level consistency. The parallel state independently fetches and records dependencies for each tag.

**Validation Does Not Enforce Group-Level Consistency:** [3](#0-2) 

The validation logic validates each tag read independently. It checks whether each tag's value matches the final state, but does NOT verify that all tags within a group were read from the same transaction version, allowing a "split-brain" view to pass validation if the final state happens to match the observed split.

**Attack Scenario:**

1. Resource group `G` contains tags `{counter: u64, last_updated: u64}` representing logically coupled state
2. Storage (txn_0): `G = {counter: 100, last_updated: 1000}`
3. Transaction T3 writes only `last_updated = 1001` 
4. Transaction T5 writes both `counter = 102` and `last_updated = 1002`
5. Transaction T7 begins reading group G:
   - Reads `counter` tag → observes value `102` from T5
   - T5 is then aborted/marked as estimate
   - Reads `last_updated` tag → observes value `1001` from T3
6. T7 now observes the inconsistent state `{counter: 102, last_updated: 1001}` which never existed as a valid snapshot
7. T7 executes Move code based on this inconsistent view
8. If T7's outputs happen to validate against the final state, the transaction commits despite being computed from an invalid snapshot

**Broken Invariant:**

This violates **Invariant #4: State Consistency** - "State transitions must be atomic and verifiable via Merkle proofs." While individual tag reads are atomic, resource groups are semantic units that should maintain internal consistency. The implementation allows transactions to observe and act upon states that violate logical invariants within a resource group.

## Impact Explanation

**Severity Assessment: Medium to High**

While BlockSTM's validation mechanism provides a safety net that prevents most catastrophic failures, this snapshot isolation violation can lead to:

1. **Determinism Violations**: Different validators may observe different split views during speculative execution, potentially causing subtle differences in gas consumption or transaction ordering decisions, though final state roots should still converge.

2. **Smart Contract Logic Errors**: Move contracts that rely on internal consistency between fields in a resource group (e.g., a counter and its last_update timestamp, or a balance and frozen status) may execute incorrectly during the window where split views are observed.

3. **Potential for Validation Bypass**: In edge cases where a transaction's outputs computed from an inconsistent view happen to validate against the final state, logically incorrect state transitions could commit.

This does not immediately lead to consensus violations due to BlockSTM's validation, but it weakens the semantic guarantees expected from resource groups and could enable exploitation in combination with other vulnerabilities.

## Likelihood Explanation

**Likelihood: Medium**

This issue can occur naturally during normal parallel execution without requiring attacker sophistication:
- Parallel execution with multiple concurrent transactions writing to the same resource group is common
- The race window exists between tag reads within the same transaction
- No special privileges or validator access required

However, successfully exploiting this for malicious purposes requires:
- Precise timing to observe the split view
- Crafting transactions whose outputs from the inconsistent view happen to validate
- Understanding of target resource group semantics

The issue is more likely to manifest as subtle correctness bugs than as a deliberate exploit.

## Recommendation

**Implement Group-Level Snapshot Consistency:**

Add a group-level version tracking mechanism to ensure all tags within a resource group are read from the same transaction snapshot:

```rust
// In VersionedGroupData
pub fn fetch_group_snapshot(
    &self,
    group_key: &K,
    txn_idx: TxnIndex,
    incarnation: Incarnation,
) -> Result<(Version, HashMap<T, ValueWithLayout<V>>), MVGroupError> {
    // 1. Determine the latest transaction version that modified this group
    let group_version = self.get_group_version(group_key, txn_idx)?;
    
    // 2. Read all tags from that specific version
    let tags = self.group_tags.get(group_key)
        .ok_or(MVGroupError::Uninitialized)?;
    
    let mut snapshot = HashMap::new();
    for tag in tags.iter() {
        // Force read from group_version, not latest
        let value = self.fetch_tagged_data_at_version(
            group_key, 
            tag, 
            group_version,
            incarnation
        )?;
        snapshot.insert(tag.clone(), value);
    }
    
    Ok((group_version, snapshot))
}
```

Additionally, enhance validation to verify group-level consistency:

```rust
// In captured_reads.rs validate_group_reads
pub(crate) fn validate_group_reads(...) -> bool {
    self.group_reads.iter().all(|(key, group)| {
        // NEW: Verify all tags came from consistent version
        let versions: HashSet<Version> = group.inner_reads.values()
            .filter_map(|r| match r {
                DataRead::Versioned(v, _, _) => Some(v.clone()),
                _ => None,
            })
            .collect();
        
        if versions.len() > 1 {
            // Multiple versions observed - snapshot violation
            return false;
        }
        
        // Existing per-tag validation...
    })
}
```

## Proof of Concept

```rust
// Rust test demonstrating snapshot isolation violation
#[test]
fn test_resource_group_snapshot_violation() {
    use aptos_mvhashmap::versioned_group_data::VersionedGroupData;
    use aptos_types::state_store::state_key::StateKey;
    
    let group_data = VersionedGroupData::empty();
    let group_key = StateKey::raw(&[1, 2, 3]);
    
    // Setup: Initialize group with two tags
    let tag_counter = 0u64;
    let tag_timestamp = 1u64;
    
    group_data.set_raw_base_values(
        group_key.clone(),
        vec![
            (tag_counter, TestValue::creation_with_len(100)),
            (tag_timestamp, TestValue::creation_with_len(1000)),
        ],
    ).unwrap();
    
    // Transaction 3: Updates timestamp only
    group_data.write_v2(
        group_key.clone(),
        3, // txn_idx
        1, // incarnation
        vec![(tag_timestamp, (TestValue::creation_with_len(1001), None))],
        compute_size(...),
        HashSet::new(),
    ).unwrap();
    
    // Transaction 5: Updates both fields
    group_data.write_v2(
        group_key.clone(),
        5,
        1,
        vec![
            (tag_counter, (TestValue::creation_with_len(102), None)),
            (tag_timestamp, (TestValue::creation_with_len(1002), None)),
        ],
        compute_size(...),
        HashSet::new(),
    ).unwrap();
    
    // Transaction 7: Reads tags - demonstrates split view possibility
    let (v1, counter_value) = group_data
        .fetch_tagged_data_and_record_dependency(&group_key, &tag_counter, 7, 1)
        .unwrap();
    
    // Simulate T5 being aborted here (mark as estimate)
    group_data.mark_estimate(&group_key, 5, HashSet::from([&tag_counter]));
    
    let (v2, timestamp_value) = group_data
        .fetch_tagged_data_and_record_dependency(&group_key, &tag_timestamp, 7, 1)
        .unwrap();
    
    // Assert: Different versions observed (snapshot violation)
    assert_ne!(v1, v2, "Tags were read from different transaction versions!");
    // counter from T5, timestamp from T3 - inconsistent snapshot
}
```

## Notes

After thorough analysis, while this represents a theoretical snapshot isolation violation at the resource group granularity, BlockSTM's validation mechanism provides substantial protection against exploitation. The validation ensures that only transactions whose reads match the final committed state can commit, preventing most attack scenarios.

However, the lack of explicit group-level consistency checking remains a semantic weakness that could interact with future changes or enable subtle exploits in specific contract patterns. The recommendation is to add explicit group-level snapshot enforcement for defense-in-depth, even though immediate exploitability is limited.

### Citations

**File:** aptos-move/mvhashmap/src/versioned_group_data.rs (L436-458)
```rust
    pub fn fetch_tagged_data_and_record_dependency(
        &self,
        group_key: &K,
        tag: &T,
        txn_idx: TxnIndex,
        incarnation: Incarnation,
    ) -> Result<(Version, ValueWithLayout<V>), MVGroupError> {
        let key_ref = GroupKeyRef { group_key, tag };

        // We are accessing group_sizes and values non-atomically, hence the order matters.
        // It is important that initialization check happens before fetch data below. O.w.
        // we could incorrectly get a TagNotFound error (do not find data, but then find
        // size initialized in between the calls). In fact, we always write size after data,
        // and sometimes (e.g. during initialization) even hold the sizes lock during writes.
        // It is fine to observe initialized = false, but find data, in convert_tagged_data.
        // TODO(BlockSTMv2): complete overhaul of initialization logic.
        let initialized = self.group_sizes.contains_key(group_key);

        let data_value =
            self.values
                .fetch_data_and_record_dependency(&key_ref, txn_idx, incarnation);
        self.convert_tagged_data(data_value, initialized)
    }
```

**File:** aptos-move/block-executor/src/view.rs (L738-841)
```rust
    fn read_cached_group_tagged_data_by_kind(
        &self,
        txn_idx: TxnIndex,
        group_key: &T::Key,
        resource_tag: &T::Tag,
        target_kind: ReadKind,
        layout: UnknownOrLayout,
        patch_base_value: &dyn Fn(&T::Value, Option<&MoveTypeLayout>) -> PartialVMResult<T::Value>,
    ) -> PartialVMResult<GroupReadResult> {
        use MVGroupError::*;

        if let Some(data_read) =
            self.captured_reads
                .borrow()
                .get_by_kind(group_key, Some(resource_tag), target_kind)
        {
            return Ok(GroupReadResult::from_data_read(data_read));
        }

        loop {
            let data = if self.scheduler.is_v2() {
                self.versioned_map
                    .group_data()
                    .fetch_tagged_data_and_record_dependency(
                        group_key,
                        resource_tag,
                        txn_idx,
                        self.incarnation,
                    )
            } else {
                self.versioned_map.group_data().fetch_tagged_data_no_record(
                    group_key,
                    resource_tag,
                    txn_idx,
                )
            };

            match data {
                Ok((version, value_with_layout)) => {
                    // If we have a known layout, upgrade RawFromStorage value to Exchanged.
                    if let UnknownOrLayout::Known(layout) = layout {
                        if let ValueWithLayout::RawFromStorage(v) = value_with_layout {
                            assert_eq!(version, Err(StorageVersion),
                            "Fetched resource has unknown layout but the version is not Err(StorageVersion)"
                            );
                            match patch_base_value(v.as_ref(), layout) {
                                Ok(patched_value) => {
                                    self.versioned_map
                                        .group_data()
                                        .update_tagged_base_value_with_layout(
                                            group_key.clone(),
                                            resource_tag.clone(),
                                            patched_value,
                                            layout.cloned().map(TriompheArc::new),
                                        );
                                    // Re-fetch in case a concurrent change went through.
                                    continue;
                                },
                                Err(e) => {
                                    error!("Couldn't patch value from versioned group map: {}", e);
                                    self.captured_reads.borrow_mut().mark_incorrect_use();
                                    return Err(e);
                                },
                            }
                        }
                    }

                    return self.captured_reads.borrow_mut().capture_group_read(
                        group_key.clone(),
                        resource_tag.clone(),
                        DataRead::from_value_with_layout(version, value_with_layout),
                        &target_kind,
                    );
                },
                Err(Uninitialized) => {
                    return Ok(GroupReadResult::Uninitialized);
                },
                Err(TagNotFound) => {
                    // TagNotFound means group was initialized (o.w. Uninitialized branch
                    // would be visited), but the tag didn't exist. So record an empty resource
                    // as a base value, and do continue to retry the read.
                    self.versioned_map
                        .group_data()
                        .update_tagged_base_value_with_layout(
                            group_key.clone(),
                            resource_tag.clone(),
                            TransactionWrite::from_state_value(None),
                            None,
                        );
                    continue;
                },
                Err(Dependency(dep_idx)) => {
                    if !wait_for_dependency(&self.scheduler, txn_idx, dep_idx)? {
                        // TODO[agg_v2](cleanup): consider changing from PartialVMResult<GroupReadResult> to GroupReadResult
                        // like in ReadResult for resources.
                        return Err(PartialVMError::new(
                            StatusCode::SPECULATIVE_EXECUTION_ABORT_ERROR,
                        )
                        .with_message("Interrupted as block execution was halted".to_string()));
                    }
                },
            }
        }
    }
```

**File:** aptos-move/block-executor/src/captured_reads.rs (L1091-1138)
```rust
    pub(crate) fn validate_group_reads(
        &self,
        group_map: &VersionedGroupData<T::Key, T::Tag, T::Value>,
        idx_to_validate: TxnIndex,
    ) -> bool {
        use MVGroupError::*;

        if self.non_delayed_field_speculative_failure {
            return false;
        }

        self.group_reads.iter().all(|(key, group)| {
            let mut ret = true;
            if let Some(size) = group.collected_size {
                ret &= group_map.validate_group_size(key, idx_to_validate, size);
            }

            ret && group.inner_reads.iter().all(|(tag, r)| {
                match group_map.fetch_tagged_data_no_record(key, tag, idx_to_validate) {
                    Ok((version, v)) => {
                        matches!(
                            self.data_read_comparator.compare_data_reads(
                                &DataRead::from_value_with_layout(version, v),
                                r,
                            ),
                            DataReadComparison::Contains
                        )
                    },
                    Err(TagNotFound) => {
                        let sentinel_deletion =
                            TriompheArc::<T::Value>::new(TransactionWrite::from_state_value(None));
                        assert!(sentinel_deletion.is_deletion());
                        matches!(
                            self.data_read_comparator.compare_data_reads(
                                &DataRead::Versioned(Err(StorageVersion), sentinel_deletion, None),
                                r,
                            ),
                            DataReadComparison::Contains
                        )
                    },
                    Err(Dependency(_)) => false,
                    Err(Uninitialized) => {
                        unreachable!("May not be uninitialized if captured for validation");
                    },
                }
            })
        })
    }
```
