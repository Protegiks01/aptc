# Audit Report

## Title
Missing Authentication in Remote Executor gRPC Service Enables Unauthorized Block Execution Commands

## Summary
The remote executor service lacks any authentication mechanism in its gRPC communication layer, allowing any client with network access to send arbitrary execution commands to executor shards, potentially causing consensus divergence and validator state manipulation.

## Finding Description

The remote executor system uses a gRPC-based communication protocol to distribute block execution work across multiple shards. The security issue exists at multiple layers:

**1. Auto-generated Client Code (Misleading Focus)** [1](#0-0) 

The `connect()` method mentioned in the security question is auto-generated tonic boilerplate that simply establishes a transport channel. Authentication checks would not occur here in any properly designed system.

**2. Actual Vulnerability: Unauthenticated gRPC Server** [2](#0-1) 

The gRPC server starts without any TLS configuration, authentication interceptors, or access control mechanisms. It accepts connections from any client that can reach the listen address.

**3. Unauthenticated Message Handler** [3](#0-2) 

The `simple_msg_exchange` handler processes all incoming messages without verifying the sender's identity or authorization. It directly forwards messages to registered handlers based solely on message type.

**4. Plain HTTP Client Connection** [4](#0-3) 

The client connects using plain HTTP without TLS or credentials, confirming no transport-layer security exists.

**5. Production Usage in Block Execution** [5](#0-4) 

The remote executor is used in production code for sharded block execution when remote addresses are configured, making this a critical path in validator operation.

**Attack Flow:**
1. Attacker gains network access to executor shard listening address (e.g., through network misconfiguration, compromised pod in same Kubernetes cluster, or same VPC)
2. Attacker crafts malicious `ExecuteBlockCommand` messages containing arbitrary transactions [6](#0-5) 
3. Attacker sends commands to executor shard via unauthenticated gRPC connection
4. Shard accepts and processes the command without authentication [7](#0-6) 
5. Shard executes malicious block, producing different state than legitimate coordinator [8](#0-7) 

This breaks the **Deterministic Execution** invariant: validators executing the same block with different shard inputs will produce different state roots, causing consensus failure.

## Impact Explanation

**Critical Severity** - This vulnerability enables:

1. **Consensus Safety Violations**: An attacker can cause different validators to execute different blocks by sending divergent commands to different shards, breaking the fundamental consensus safety guarantee.

2. **Validator State Manipulation**: Unauthorized execution commands can manipulate validator state calculations, potentially affecting rewards, penalties, or validator set composition.

3. **Denial of Service**: Malformed or resource-intensive execution requests can cause shard crashes or performance degradation.

4. **Non-Recoverable State Divergence**: If different validators execute different blocks due to shard poisoning, the network may require manual intervention or hard fork to recover.

This meets the "Consensus/Safety violations" category for Critical Severity ($1,000,000) in the Aptos bug bounty program.

## Likelihood Explanation

**Likelihood: Medium to High (deployment-dependent)**

The exploitability depends critically on network deployment configuration:

**Exploitable scenarios:**
- Executor shards listening on public IP addresses (misconfiguration)
- Compromised workload in same Kubernetes cluster (common attack vector)
- Compromised VM in same VPC/private network
- Insider with network access but not validator key material

**Mitigating factors:**
- Service may be deployed behind firewall rules
- Kubernetes NetworkPolicy may restrict access [9](#0-8) 
- Service appears intended for trusted internal networks

However, **defense-in-depth principles mandate authentication even on internal networks**. Network isolation alone is insufficient security, as demonstrated by countless real-world breaches where internal networks are compromised.

**Critical gap:** The codebase provides NO enforcement that these services must be deployed privately. The main binary accepts arbitrary socket addresses: [10](#0-9) 

## Recommendation

Implement mutual TLS (mTLS) authentication for all remote executor gRPC connections:

```rust
// In GRPCNetworkMessageServiceClientWrapper::get_channel
async fn get_channel(remote_addr: String) -> NetworkMessageServiceClient<Channel> {
    let tls_config = ClientTlsConfig::new()
        .ca_certificate(Certificate::from_pem(CA_CERT))
        .identity(Identity::from_pem(CLIENT_CERT, CLIENT_KEY));
    
    let conn = tonic::transport::Endpoint::new(remote_addr)
        .unwrap()
        .tls_config(tls_config)
        .unwrap()
        .connect_lazy();
    NetworkMessageServiceClient::new(conn).max_decoding_message_size(MAX_MESSAGE_SIZE)
}

// In GRPCNetworkMessageServiceServerWrapper::start_async
async fn start_async(self, server_addr: SocketAddr, ...) {
    let tls_config = ServerTlsConfig::new()
        .ca_certificate(Certificate::from_pem(CA_CERT))
        .identity(Identity::from_pem(SERVER_CERT, SERVER_KEY))
        .client_auth_required(true);
    
    Server::builder()
        .tls_config(tls_config)
        .unwrap()
        .add_service(NetworkMessageServiceServer::new(self)...)
        ...
}
```

Add configuration options for certificate paths similar to other Aptos gRPC services: [11](#0-10) 

## Proof of Concept

```rust
// Malicious client that sends unauthorized execution commands
use aptos_protos::remote_executor::v1::network_message_service_client::NetworkMessageServiceClient;
use aptos_executor_service::{ExecuteBlockCommand, RemoteExecutionRequest};
use tonic::Request;

#[tokio::main]
async fn main() {
    // Connect to victim shard (no authentication required)
    let victim_shard = "http://10.0.1.100:50051"; // Example internal IP
    let mut client = NetworkMessageServiceClient::connect(victim_shard)
        .await
        .expect("Failed to connect");
    
    // Craft malicious execution command
    let malicious_command = RemoteExecutionRequest::ExecuteBlock(ExecuteBlockCommand {
        sub_blocks: /* crafted malicious sub-blocks */,
        concurrency_level: 1,
        onchain_config: /* manipulated config */,
    });
    
    // Serialize and send without authentication
    let message = NetworkMessage {
        message: bcs::to_bytes(&malicious_command).unwrap(),
        message_type: "execute_command_0".to_string(),
    };
    
    // Server accepts and processes - NO AUTHENTICATION CHECK
    client.simple_msg_exchange(Request::new(message))
        .await
        .expect("Command accepted by unauthenticated server");
    
    println!("Successfully sent unauthorized execution command to shard!");
}
```

**Expected Result**: The executor shard accepts and processes the malicious command without any authentication, demonstrating the vulnerability.

**To reproduce**:
1. Deploy executor service with `--remote-executor-addresses` pointing to network-accessible addresses
2. Run malicious client from different machine/pod with network access
3. Observe unauthenticated command execution

---

## Notes

While the vulnerability technically exists in the codebase, its **exploitability is highly deployment-dependent**. The remote executor appears designed for trusted internal networks, though this is neither documented nor enforced in code. 

**Critical considerations:**
- If deployed with proper network isolation (private VPC, Kubernetes NetworkPolicy, firewall rules), the attack surface is significantly reduced
- However, assuming network isolation as the sole security control violates defense-in-depth principles
- The service's naming ("aptos-secure-net") is misleading given the absence of cryptographic security
- Comparison: Aptos's validator P2P network uses Noise protocol with mutual authentication, showing the team understands the need for authenticated communication in distributed systems

**Strict Validation Concern**: The vulnerability requires the attacker to have network access to internal infrastructure services. Under the strictest interpretation of "exploitable by unprivileged attacker," this may not qualify if network access to internal services is considered a privileged position. However, network misconfigurations and lateral movement attacks are common enough that defense-in-depth mandates authentication regardless of network topology.

### Citations

**File:** protos/rust/src/pb/aptos.remote_executor.v1.tonic.rs (L17-24)
```rust
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
```

**File:** secure/net/src/grpc_network_service/mod.rs (L75-86)
```rust
        Server::builder()
            .timeout(std::time::Duration::from_millis(rpc_timeout_ms))
            .add_service(
                NetworkMessageServiceServer::new(self).max_decoding_message_size(MAX_MESSAGE_SIZE),
            )
            .add_service(reflection_service)
            .serve_with_shutdown(server_addr, async {
                server_shutdown_rx.await.ok();
                info!("Received signal to shutdown server at {:?}", server_addr);
            })
            .await
            .unwrap();
```

**File:** secure/net/src/grpc_network_service/mod.rs (L93-115)
```rust
    async fn simple_msg_exchange(
        &self,
        request: Request<NetworkMessage>,
    ) -> Result<Response<Empty>, Status> {
        let _timer = NETWORK_HANDLER_TIMER
            .with_label_values(&[&self.self_addr.to_string(), "inbound_msgs"])
            .start_timer();
        let remote_addr = request.remote_addr();
        let network_message = request.into_inner();
        let msg = Message::new(network_message.message);
        let message_type = MessageType::new(network_message.message_type);

        if let Some(handler) = self.inbound_handlers.lock().unwrap().get(&message_type) {
            // Send the message to the registered handler
            handler.send(msg).unwrap();
        } else {
            error!(
                "No handler registered for sender: {:?} and msg type {:?}",
                remote_addr, message_type
            );
        }
        Ok(Response::new(Empty {}))
    }
```

**File:** secure/net/src/grpc_network_service/mod.rs (L132-138)
```rust
    async fn get_channel(remote_addr: String) -> NetworkMessageServiceClient<Channel> {
        info!("Trying to connect to remote server at {:?}", remote_addr);
        let conn = tonic::transport::Endpoint::new(remote_addr)
            .unwrap()
            .connect_lazy();
        NetworkMessageServiceClient::new(conn).max_decoding_message_size(MAX_MESSAGE_SIZE)
    }
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L261-267)
```rust
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
```

**File:** execution/executor-service/src/lib.rs (L44-53)
```rust
pub enum RemoteExecutionRequest {
    ExecuteBlock(ExecuteBlockCommand),
}

#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct ExecuteBlockCommand {
    pub(crate) sub_blocks: SubBlocksForShard<AnalyzedTransaction>,
    pub(crate) concurrency_level: usize,
    pub(crate) onchain_config: BlockExecutorConfigFromOnchain,
}
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L80-109)
```rust
    fn receive_execute_command(&self) -> ExecutorShardCommand<RemoteStateViewClient> {
        match self.command_rx.recv() {
            Ok(message) => {
                let _rx_timer = REMOTE_EXECUTOR_TIMER
                    .with_label_values(&[&self.shard_id.to_string(), "cmd_rx"])
                    .start_timer();
                let bcs_deser_timer = REMOTE_EXECUTOR_TIMER
                    .with_label_values(&[&self.shard_id.to_string(), "cmd_rx_bcs_deser"])
                    .start_timer();
                let request: RemoteExecutionRequest = bcs::from_bytes(&message.data).unwrap();
                drop(bcs_deser_timer);

                match request {
                    RemoteExecutionRequest::ExecuteBlock(command) => {
                        let init_prefetch_timer = REMOTE_EXECUTOR_TIMER
                            .with_label_values(&[&self.shard_id.to_string(), "init_prefetch"])
                            .start_timer();
                        let state_keys = Self::extract_state_keys(&command);
                        self.state_view_client.init_for_block(state_keys);
                        drop(init_prefetch_timer);

                        let (sub_blocks, concurrency, onchain_config) = command.into();
                        ExecutorShardCommand::ExecuteSubBlocks(
                            self.state_view_client.clone(),
                            sub_blocks,
                            concurrency,
                            onchain_config,
                        )
                    },
                }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L215-254)
```rust
    pub fn start(&self) {
        trace!(
            "Shard starting, shard_id={}, num_shards={}.",
            self.shard_id,
            self.num_shards
        );
        let mut num_txns = 0;
        loop {
            let command = self.coordinator_client.receive_execute_command();
            match command {
                ExecutorShardCommand::ExecuteSubBlocks(
                    state_view,
                    transactions,
                    concurrency_level_per_shard,
                    onchain_config,
                ) => {
                    num_txns += transactions.num_txns();
                    trace!(
                        "Shard {} received ExecuteBlock command of block size {} ",
                        self.shard_id,
                        num_txns
                    );
                    let exe_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "execute_block"]);
                    let ret = self.execute_block(
                        transactions,
                        state_view.as_ref(),
                        BlockExecutorConfig {
                            local: BlockExecutorLocalConfig::default_with_concurrency_level(
                                concurrency_level_per_shard,
                            ),
                            onchain: onchain_config,
                        },
                    );
                    drop(state_view);
                    drop(exe_timer);

                    let _result_tx_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "result_tx"]);
                    self.coordinator_client.send_execution_result(ret);
```

**File:** terraform/helm/aptos-node/values.yaml (L1-50)
```yaml
chain:
  # -- Internal: name of the testnet to connect to
  name: testnet
  # -- Bump this number to wipe the underlying storage
  era: 1
  # -- Chain ID
  chain_id: 4

# -- Default image tag to use for all validator and fullnode images
imageTag: devnet

# -- Number of validators to deploy
numValidators: 1
# -- Total number of fullnode groups to deploy
numFullnodeGroups: 1

# -- Options for multicluster mode. This is *experimental only*.
multicluster:
  enabled: false
  targetClusters: ["forge-multiregion-1", "forge-multiregion-2", "forge-multiregion-3"]

# -- Specify validator and fullnode NodeConfigs via named ConfigMaps, rather than the generated ones from this chart.
overrideNodeConfig: false

# -- If true, helm will always override the deployed image with what is configured in the helm values. If not, helm will take the latest image from the currently running workloads, which is useful if you have a separate procedure to update images (e.g. rollout)
manageImages: true

haproxy:
  # -- Enable HAProxy deployment in front of validator and fullnodes
  enabled: true
  # -- Number of HAProxy replicas
  replicas: 1
  image:
    # -- Image repo to use for HAProxy images
    repo: haproxy
    # -- Image tag to use for HAProxy images
    tag: 3.0.2@sha256:3fa2e323a2f422239a39eff345b41ab20a7a91aa4ad8c3c82b9ae85dd241214b
    # -- Image pull policy to use for HAProxy images
    pullPolicy: IfNotPresent
  resources:
    limits:
      cpu: 7
      memory: 16Gi
    requests:
      cpu: 7
      memory: 16Gi
  nodeSelector: {}
  tolerations: []
  affinity: {}

```

**File:** execution/executor-service/src/main.rs (L10-25)
```rust
struct Args {
    #[clap(long, default_value_t = 8)]
    pub num_executor_threads: usize,

    #[clap(long)]
    pub shard_id: usize,

    #[clap(long)]
    pub num_shards: usize,

    #[clap(long, num_args = 1..)]
    pub remote_executor_addresses: Vec<SocketAddr>,

    #[clap(long)]
    pub coordinator_address: SocketAddr,
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/config.rs (L1-50)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use crate::service::RawDataServerWrapper;
use anyhow::{bail, Result};
use aptos_indexer_grpc_server_framework::RunnableConfig;
use aptos_indexer_grpc_utils::{
    compression_util::StorageFormat, config::IndexerGrpcFileStoreConfig,
    in_memory_cache::InMemoryCacheConfig, types::RedisUrl,
};
use aptos_protos::{
    indexer::v1::FILE_DESCRIPTOR_SET as INDEXER_V1_FILE_DESCRIPTOR_SET,
    transaction::v1::FILE_DESCRIPTOR_SET as TRANSACTION_V1_TESTING_FILE_DESCRIPTOR_SET,
    util::timestamp::FILE_DESCRIPTOR_SET as UTIL_TIMESTAMP_FILE_DESCRIPTOR_SET,
};
use aptos_transaction_filter::BooleanTransactionFilter;
use serde::{Deserialize, Serialize};
use std::{net::SocketAddr, sync::Arc};
use tonic::{codec::CompressionEncoding, transport::Server};

pub const SERVER_NAME: &str = "idxdatasvc";

// Default max response channel size.
const DEFAULT_MAX_RESPONSE_CHANNEL_SIZE: usize = 3;

// HTTP2 ping interval and timeout.
// This can help server to garbage collect dead connections.
// tonic server: https://docs.rs/tonic/latest/tonic/transport/server/struct.Server.html#method.http2_keepalive_interval
const HTTP2_PING_INTERVAL_DURATION: std::time::Duration = std::time::Duration::from_secs(60);
const HTTP2_PING_TIMEOUT_DURATION: std::time::Duration = std::time::Duration::from_secs(10);

#[derive(Clone, Debug, Deserialize, Serialize)]
#[serde(deny_unknown_fields)]
pub struct TlsConfig {
    /// The address for the TLS GRPC server to listen on.
    pub data_service_grpc_listen_address: SocketAddr,
    pub cert_path: String,
    pub key_path: String,
}

#[derive(Clone, Debug, Deserialize, Serialize)]
#[serde(deny_unknown_fields)]
pub struct NonTlsConfig {
    /// The address for the TLS GRPC server to listen on.
    pub data_service_grpc_listen_address: SocketAddr,
}

#[derive(Clone, Debug, Deserialize, Serialize)]
#[serde(deny_unknown_fields)]
pub struct IndexerGrpcDataServiceConfig {
```
