# Audit Report

## Title
State Inconsistency Between ConsensusDB and Execution Layer Due to Missing Rollback and Premature State Update in sync_to_target()

## Summary
The `fast_forward_sync()` function lacks atomicity between ConsensusDB persistence and execution state synchronization. A bug in `ExecutionProxy::sync_to_target()` that updates internal state before validating sync success causes nodes to become temporarily unable to self-recover when state sync fails, requiring either node restart or waiting for network progression to higher rounds.

## Finding Description

The vulnerability exists due to two compounding issues in the consensus fast-forward synchronization path:

**Issue 1: Missing Rollback in fast_forward_sync()**

In `fast_forward_sync()`, blocks are persisted to ConsensusDB before execution state synchronization occurs, with no rollback mechanism if the latter fails: [1](#0-0) 

When `storage.save_tree()` succeeds at line 503, blocks and quorum certificates are permanently written to ConsensusDB via RocksDB. If `execution_client.sync_to_target()` subsequently fails at line 512-514, there is no code path to rollback the ConsensusDB changes.

**Issue 2: Premature State Update in sync_to_target()**

The critical bug exists in `ExecutionProxy::sync_to_target()` where `latest_logical_time` is unconditionally updated BEFORE checking whether state synchronization succeeded: [2](#0-1) 

At line 222, `latest_logical_time` is set to `target_logical_time` immediately after invoking the state sync operation (line 216-219), but the result is only checked and returned at lines 229-232. This means even when state sync fails, the ExecutionProxy incorrectly records that it has synced to the target.

**Exploitation Sequence:**

On subsequent retry attempts within the same node session, this guard clause prevents actual synchronization: [3](#0-2) 

Since `latest_logical_time` was incorrectly updated during the failed sync attempt, when the node retries with the same or similar target, this check returns `Ok()` without performing any state synchronization.

**Attack Scenario:**
1. Node receives sync_info requiring fast-forward sync to round N
2. Pre-validation at lines 476-501 confirms blocks form valid chain: [4](#0-3) 

3. `storage.save_tree()` persists blocks to ConsensusDB successfully
4. `sync_to_target()` attempts execution state sync but fails (network issues, timeout, malicious peer withholding state sync data)
5. Despite failure, `latest_logical_time` is set to target round (bug)
6. Error propagates, `fast_forward_sync()` returns error
7. On retry with same target, `sync_to_target()` returns Ok() without syncing
8. `storage.start()` is called: [5](#0-4) 

9. Blocks saved to ConsensusDB that don't form a continuous chain with execution state get pruned by `find_blocks_to_prune()`: [6](#0-5) 

10. Node ends up stuck at old state, unable to sync to the same target again due to the guard clause

## Impact Explanation

**Severity: Medium**

This vulnerability qualifies as **Medium Severity** under the Aptos Bug Bounty criteria:

1. **State Inconsistencies Requiring Intervention**: The node enters a temporarily inconsistent state where ConsensusDB and AptosDB are desynchronized. Recovery requires either:
   - Node restart (resets in-memory `latest_logical_time`)
   - Waiting for network to progress beyond the target round
   - Manual operator intervention

2. **Temporary Liveness Impact**: While the affected node cannot sync to the same target, it can recover when:
   - Network progresses to higher rounds (new sync targets bypass the guard clause)
   - Node is restarted (write_mutex is reinitialized to LogicalTime(0,0)): [7](#0-6) 

3. **Limited Scope**: This affects individual validator nodes, not the entire network. However, if multiple validators experience this simultaneously during network partitions or epoch transitions, it could contribute to temporary liveness degradation.

4. **No Direct Fund Loss**: This does not enable theft, minting, or permanent fund freezing.

The impact matches the Medium severity criteria: "State inconsistencies requiring manual intervention" and "Temporary liveness issues."

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability can be triggered by realistic conditions:

1. **Natural Occurrence**: Transient network failures, state sync service timeouts, or database errors during the critical window between lines 503-514 are common in distributed systems.

2. **Malicious Amplification**: A malicious network peer (not requiring validator privileges) can:
   - Send valid sync_info messages to trigger fast-forward sync
   - Provide blocks successfully for retrieval
   - Refuse to provide state sync data or cause timeouts

3. **Common Trigger**: Fast-forward sync is frequently invoked when nodes fall behind during network partitions, high load, or epoch transitions.

4. **Recovery Available**: Unlike truly critical vulnerabilities, this has multiple recovery paths (restart, network progression), reducing the overall severity.

## Recommendation

**Fix 1: Move latest_logical_time update after result validation**

In `consensus/src/state_computer.rs`, move line 222 to after the result check:

```rust
async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
    let mut latest_logical_time = self.write_mutex.lock().await;
    let target_logical_time = LogicalTime::new(target.ledger_info().epoch(), target.ledger_info().round());
    
    self.executor.finish();
    
    if *latest_logical_time >= target_logical_time {
        return Ok(());
    }
    
    // State sync notification
    if let Some(inner) = self.state.read().as_ref() {
        inner.payload_manager.notify_commit(target.commit_info().timestamp_usecs(), Vec::new());
    }
    
    let result = self.state_sync_notifier.sync_to_target(target).await;
    
    // Only update if successful
    if result.is_ok() {
        *latest_logical_time = target_logical_time;
        self.executor.reset()?;
    }
    
    result.map_err(|error| error.into())
}
```

**Fix 2: Add rollback mechanism in fast_forward_sync()**

Add cleanup logic to remove persisted blocks if sync_to_target fails:

```rust
storage.save_tree(blocks.clone(), quorum_certs.clone())?;

let sync_result = execution_client
    .sync_to_target(highest_commit_cert.ledger_info().clone())
    .await;

if let Err(e) = sync_result {
    // Rollback ConsensusDB changes
    let block_ids: Vec<_> = blocks.iter().map(|b| b.id()).collect();
    storage.prune_tree(block_ids)?;
    return Err(e);
}
```

## Proof of Concept

The vulnerability can be demonstrated by:

1. Setting up a test validator node
2. Triggering fast-forward sync via network sync_info messages
3. Injecting a state sync failure using the fail point: [8](#0-7) 

4. Observing that `latest_logical_time` is updated despite the failure
5. Retrying sync with the same target and observing the guard clause prevents recovery
6. Verifying that only node restart or higher round sync resolves the issue

A complete Rust integration test would involve consensus test infrastructure with state sync failure injection.

## Notes

- The vulnerability affects nodes during active sync operations, not normal consensus participation
- Recovery is possible via restart (write_mutex is in-memory only), making this less severe than initially claimed
- The impact is primarily on individual node availability rather than network-wide consensus safety
- Multiple validators hitting this simultaneously during network stress could compound liveness issues

### Citations

**File:** consensus/src/block_storage/sync_manager.rs (L476-501)
```rust
        // Check early that recovery will succeed, and return before corrupting our state in case it will not.
        LedgerRecoveryData::new(highest_commit_cert.ledger_info().clone())
            .find_root(
                &mut blocks.clone(),
                &mut quorum_certs.clone(),
                order_vote_enabled,
                window_size,
            )
            .with_context(|| {
                // for better readability
                quorum_certs.sort_by_key(|qc| qc.certified_block().round());
                format!(
                    "\nRoot: {:?}\nBlocks in db: {}\nQuorum Certs in db: {}\n",
                    highest_commit_cert.commit_info(),
                    blocks
                        .iter()
                        .map(|b| format!("\n\t{}", b))
                        .collect::<Vec<String>>()
                        .concat(),
                    quorum_certs
                        .iter()
                        .map(|qc| format!("\n\t{}", qc))
                        .collect::<Vec<String>>()
                        .concat(),
                )
            })?;
```

**File:** consensus/src/block_storage/sync_manager.rs (L503-514)
```rust
        storage.save_tree(blocks.clone(), quorum_certs.clone())?;
        // abort any pending executor tasks before entering state sync
        // with zaptos, things can run before hitting buffer manager
        if let Some(block_store) = maybe_block_store {
            monitor!(
                "abort_pipeline_for_state_sync",
                block_store.abort_pipeline_for_state_sync().await
            );
        }
        execution_client
            .sync_to_target(highest_commit_cert.ledger_info().clone())
            .await?;
```

**File:** consensus/src/block_storage/sync_manager.rs (L519-522)
```rust
        let recovery_data = match storage.start(order_vote_enabled, window_size) {
            LivenessStorageData::FullRecoveryData(recovery_data) => recovery_data,
            _ => panic!("Failed to construct recovery data after fast forward sync"),
        };
```

**File:** consensus/src/state_computer.rs (L66-84)
```rust
    pub fn new(
        executor: Arc<dyn BlockExecutorTrait>,
        txn_notifier: Arc<dyn TxnNotifier>,
        state_sync_notifier: Arc<dyn ConsensusNotificationSender>,
        txn_filter_config: BlockTransactionFilterConfig,
        enable_pre_commit: bool,
        secret_share_config: Option<SecretShareConfig>,
    ) -> Self {
        Self {
            executor,
            txn_notifier,
            state_sync_notifier,
            write_mutex: AsyncMutex::new(LogicalTime::new(0, 0)),
            txn_filter_config: Arc::new(txn_filter_config),
            state: RwLock::new(None),
            enable_pre_commit,
            secret_share_config,
        }
    }
```

**File:** consensus/src/state_computer.rs (L188-193)
```rust
        if *latest_logical_time >= target_logical_time {
            warn!(
                "State sync target {:?} is lower than already committed logical time {:?}",
                target_logical_time, *latest_logical_time
            );
            return Ok(());
```

**File:** consensus/src/state_computer.rs (L206-209)
```rust
        // Inject an error for fail point testing
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });
```

**File:** consensus/src/state_computer.rs (L216-232)
```rust
        let result = monitor!(
            "sync_to_target",
            self.state_sync_notifier.sync_to_target(target).await
        );

        // Update the latest logical time
        *latest_logical_time = target_logical_time;

        // Similarly, after state synchronization, we have to reset the cache of
        // the BlockExecutor to guarantee the latest committed state is up to date.
        self.executor.reset()?;

        // Return the result
        result.map_err(|error| {
            let anyhow_error: anyhow::Error = error.into();
            anyhow_error.into()
        })
```

**File:** consensus/src/persistent_liveness_storage.rs (L448-475)
```rust
    fn find_blocks_to_prune(
        root_id: HashValue,
        blocks: &mut Vec<Block>,
        quorum_certs: &mut Vec<QuorumCert>,
    ) -> Vec<HashValue> {
        // prune all the blocks that don't have root as ancestor
        let mut tree = HashSet::new();
        let mut to_remove = HashSet::new();
        tree.insert(root_id);
        // assume blocks are sorted by round already
        blocks.retain(|block| {
            if tree.contains(&block.parent_id()) {
                tree.insert(block.id());
                true
            } else {
                to_remove.insert(block.id());
                false
            }
        });
        quorum_certs.retain(|qc| {
            if tree.contains(&qc.certified_block().id()) {
                true
            } else {
                to_remove.insert(qc.certified_block().id());
                false
            }
        });
        to_remove.into_iter().collect()
```
