# Audit Report

## Title
Data Stream Permanent Zombie State Due to Partial Initialization Failure

## Summary
A critical error recovery flaw in the data streaming service allows streams to become permanently stuck in a "zombie" state when `initialize_data_requests()` fails after marking the stream as initialized. The stream remains in memory indefinitely, consuming resources and continuously failing with the same error on every progress check cycle, causing validator node slowdowns and resource exhaustion.

## Finding Description

The vulnerability exists in the initialization flow between two functions: [1](#0-0) [2](#0-1) 

The critical flaw is in the ordering of operations within `initialize_data_requests()`. The function first sets `sent_data_requests = Some(VecDeque::new())` (marking the stream as initialized), then attempts to create and send requests via `create_and_send_client_requests()`. If this second call fails, the error propagates up, but the stream remains marked as initialized. [3](#0-2) 

The `data_requests_initialized()` check only verifies if `sent_data_requests` is `Some`, not whether initialization actually succeeded. This creates a state inconsistency.

When `update_progress_of_data_stream()` is called again, it sees the stream as initialized and enters the `else` branch to call `process_data_responses()`: [4](#0-3) 

Since the request queue is empty (no requests were ever sent), `process_data_responses()` immediately calls `create_and_send_client_requests()` again at the end, which can fail with the same error. The errors that can trigger this include: [5](#0-4) 

The stream remains in the `data_streams` HashMap because error handling only logs the error without removing the stream: [6](#0-5) 

Critically, the `request_failure_count` is never incremented for initialization failures (only for request retry scenarios): [7](#0-6) 

This means the stream never reaches the maximum retry limit and is never automatically terminated. The stream becomes a permanent "zombie" - perpetually consuming memory, CPU cycles for repeated failed processing attempts, and spamming error logs.

**Attack Scenario:**
1. Attacker or network conditions cause a state sync stream request where `next_request_index > number_of_states` (e.g., requesting states starting at index 1000 when only 500 states exist)
2. Stream is created and initialization begins
3. `initialize_data_requests()` marks stream as initialized, then `create_data_client_requests()` returns `Error::NoDataToFetch`
4. Stream remains in `data_streams` HashMap forever
5. Every progress check cycle (configured milliseconds), the stream attempts reinitialization, fails, logs error
6. Over time, multiple zombie streams accumulate if the condition repeats
7. Validator performance degrades due to accumulated zombie streams

## Impact Explanation

**High Severity** per Aptos Bug Bounty criteria:

1. **Validator Node Slowdowns**: Each zombie stream consumes CPU cycles on every progress check interval (default: configurable milliseconds). With multiple zombie streams, this creates measurable performance degradation.

2. **Resource Exhaustion**: Memory leaks from streams that are never garbage collected, plus ongoing task spawning for failed requests.

3. **Log Spam**: Continuous error logging can fill disk space and obscure legitimate errors, hindering debugging and incident response.

4. **Liveness Impact**: In extreme cases with many zombie streams, the cumulative overhead could impact state sync performance enough to affect validator liveness, preventing nodes from staying synced with the network.

This does not reach Critical severity because:
- No direct consensus safety violation
- No fund loss or theft
- Not a complete loss of liveness (degradation only)
- Recoverable via node restart (though temporary)

However, it meets the High severity threshold for "validator node slowdowns" and represents a significant protocol violation where the streaming service fails to maintain proper resource lifecycle management.

## Likelihood Explanation

**Medium-High Likelihood:**

This vulnerability can be triggered through multiple realistic scenarios:

1. **Race Conditions**: During network partitions or high load, global data summaries may be stale when streams are created
2. **Invalid Client Requests**: Misconfigured state sync clients requesting non-existent data ranges
3. **Epoch Transitions**: During epoch changes, temporary inconsistencies in advertised data vs actual available data
4. **Network Churn**: Peers advertising data they no longer have, followed by stream initialization attempts

The vulnerability requires no attacker privileges and can occur naturally under adverse network conditions. Once triggered, the impact persists indefinitely until node restart. The lack of any timeout or cleanup mechanism makes this a guaranteed resource leak whenever the triggering condition occurs.

## Recommendation

**Fix Option 1: Atomic Initialization (Recommended)**

Move the initialization marker to AFTER successful request creation:

```rust
pub fn initialize_data_requests(
    &mut self,
    global_data_summary: GlobalDataSummary,
) -> Result<(), Error> {
    // Create and send the data client requests FIRST
    let mut temp_queue = VecDeque::new();
    
    // Use a temporary queue during initialization
    let original_sent_requests = self.sent_data_requests.take();
    self.sent_data_requests = Some(VecDeque::new());
    
    // Try to create and send requests
    let result = self.create_and_send_client_requests(&global_data_summary);
    
    if result.is_err() {
        // Restore original state on failure
        self.sent_data_requests = original_sent_requests;
        return result;
    }
    
    // Only mark as initialized if successful
    Ok(())
}
```

**Fix Option 2: Add Initialization Failure Tracking**

Track initialization failures and terminate streams that fail repeatedly:

```rust
pub struct DataStream<T> {
    // ... existing fields ...
    initialization_failure_count: u64,
}

// In update_progress_of_data_stream():
if !data_stream.data_requests_initialized() {
    match data_stream.initialize_data_requests(global_data_summary) {
        Ok(_) => {
            data_stream.initialization_failure_count = 0;
            // ... log success ...
        }
        Err(error) => {
            data_stream.initialization_failure_count += 1;
            if data_stream.initialization_failure_count >= max_initialization_retries {
                // Terminate the zombie stream
                self.data_streams.remove(data_stream_id);
                return Err(Error::UnexpectedErrorEncountered(
                    "Stream failed to initialize after max retries".into()
                ));
            }
            return Err(error);
        }
    }
}
```

**Fix Option 3: Stream Timeout Mechanism**

Add timestamp tracking and terminate streams stuck in uninitialized state beyond a timeout threshold.

## Proof of Concept

```rust
#[cfg(test)]
mod zombie_stream_test {
    use super::*;
    use aptos_config::config::DataStreamingServiceConfig;
    use aptos_data_client::global_summary::GlobalDataSummary;
    use aptos_time_service::MockTimeService;
    
    #[tokio::test]
    async fn test_zombie_stream_on_initialization_failure() {
        // Create a streaming service with a mock data client
        let data_client_config = AptosDataClientConfig::default();
        let streaming_service_config = DataStreamingServiceConfig::default();
        let (stream_requests_tx, stream_requests_rx) = 
            aptos_channel::new(QueueStyle::LIFO, 10, None);
        
        // Create a mock data client that will return NoDataToFetch error
        let mock_data_client = create_mock_client_with_invalid_state_range();
        let time_service = TimeService::mock();
        
        let mut streaming_service = DataStreamingService::new(
            data_client_config,
            streaming_service_config,
            mock_data_client,
            stream_requests_rx,
            time_service,
        );
        
        // Create a state stream request with invalid range (next_index > total_states)
        let stream_request = StreamRequest::GetAllStates(GetAllStatesRequest {
            version: 100,
            start_index: 1000, // Request starts beyond available states
        });
        
        // Process the stream request
        let (tx, rx) = oneshot::channel();
        let request_message = StreamRequestMessage {
            stream_request,
            response_sender: tx,
        };
        
        streaming_service.handle_stream_request_message(
            request_message,
            streaming_service.stream_update_notifier.clone(),
        );
        
        let data_stream_listener = rx.await.unwrap().unwrap();
        let stream_id = data_stream_listener.data_stream_id;
        
        // Verify stream was created
        assert_eq!(streaming_service.data_streams.len(), 1);
        
        // First progress check - initialization fails
        let result = streaming_service
            .update_progress_of_data_stream(&stream_id)
            .await;
        assert!(result.is_err()); // Error on initialization
        
        // Verify stream is marked as initialized (zombie state)
        let data_stream = streaming_service.data_streams.get(&stream_id).unwrap();
        assert!(data_stream.data_requests_initialized()); // BUG: marked initialized
        assert_eq!(data_stream.get_num_pending_data_requests().unwrap(), 0); // But queue is empty!
        
        // Second progress check - should reinitialize, but instead processes empty queue
        let result = streaming_service
            .update_progress_of_data_stream(&stream_id)
            .await;
        assert!(result.is_err()); // Still errors
        
        // Stream is never removed - it's a zombie!
        assert_eq!(streaming_service.data_streams.len(), 1);
        
        // Simulate many progress checks - stream remains forever
        for _ in 0..100 {
            let _ = streaming_service
                .update_progress_of_data_stream(&stream_id)
                .await;
            assert_eq!(streaming_service.data_streams.len(), 1); // Never cleaned up!
        }
        
        // Verify the stream never increments request_failure_count
        let data_stream = streaming_service.data_streams.get(&stream_id).unwrap();
        assert_eq!(data_stream.request_failure_count, 0); // Never incremented!
    }
}
```

## Notes

This vulnerability demonstrates a classic atomicity violation in error handling. The separation of state mutation (`sent_data_requests = Some(...)`) from the operation that can fail (`create_and_send_client_requests()`) creates an inconsistent state on failure. The lack of rollback logic or failure tracking allows these zombie streams to persist indefinitely.

The impact is amplified by the periodic nature of `check_progress_of_all_data_streams()`, which continuously retries failed streams without bound. In a production environment with unstable network conditions or misconfigured clients, this could lead to accumulation of dozens or hundreds of zombie streams over time, significantly degrading validator performance.

The recommended fix ensures atomicity by deferring the initialization marker until after successful request creation, preventing the inconsistent state entirely.

### Citations

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L308-337)
```rust
    /// Ensures that all existing data streams are making progress
    async fn check_progress_of_all_data_streams(&mut self) {
        // Drive the progress of each stream
        let data_stream_ids = self.get_all_data_stream_ids();
        for data_stream_id in &data_stream_ids {
            if let Err(error) = self.update_progress_of_data_stream(data_stream_id).await {
                if matches!(error, Error::NoDataToFetch(_)) {
                    sample!(
                        SampleRate::Duration(Duration::from_secs(NO_DATA_TO_FETCH_LOG_FREQ_SECS)),
                        info!(LogSchema::new(LogEntry::CheckStreamProgress)
                            .stream_id(*data_stream_id)
                            .event(LogEvent::Pending)
                            .error(&error))
                    );
                } else {
                    metrics::increment_counter(
                        &metrics::CHECK_STREAM_PROGRESS_ERROR,
                        error.get_label(),
                    );
                    warn!(LogSchema::new(LogEntry::CheckStreamProgress)
                        .stream_id(*data_stream_id)
                        .event(LogEvent::Error)
                        .error(&error));
                }
            }
        }

        // Update the metrics
        metrics::set_active_data_streams(data_stream_ids.len());
    }
```

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L341-384)
```rust
    async fn update_progress_of_data_stream(
        &mut self,
        data_stream_id: &DataStreamId,
    ) -> Result<(), Error> {
        let global_data_summary = self.get_global_data_summary();

        // If there was a send failure, terminate the stream
        let data_stream = self.get_data_stream(data_stream_id)?;
        if data_stream.send_failure() {
            info!(
                (LogSchema::new(LogEntry::TerminateStream)
                    .stream_id(*data_stream_id)
                    .event(LogEvent::Success)
                    .message("There was a send failure, terminating the stream."))
            );
            metrics::DATA_STREAM_SEND_FAILURE.inc();
            if self.data_streams.remove(data_stream_id).is_none() {
                return Err(Error::UnexpectedErrorEncountered(format!(
                    "Failed to terminate stream id {:?} for send failure! Stream not found.",
                    data_stream_id
                )));
            }
            return Ok(());
        }

        // Drive data stream progress
        if !data_stream.data_requests_initialized() {
            // Initialize the request batch by sending out data client requests
            data_stream.initialize_data_requests(global_data_summary)?;
            info!(
                (LogSchema::new(LogEntry::InitializeStream)
                    .stream_id(*data_stream_id)
                    .event(LogEvent::Success)
                    .message("Data stream initialized."))
            );
        } else {
            // Process any data client requests that have received responses
            data_stream
                .process_data_responses(global_data_summary)
                .await?;
        }

        Ok(())
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L187-189)
```rust
    pub fn data_requests_initialized(&self) -> bool {
        self.sent_data_requests.is_some()
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L210-219)
```rust
    pub fn initialize_data_requests(
        &mut self,
        global_data_summary: GlobalDataSummary,
    ) -> Result<(), Error> {
        // Initialize the data client requests queue
        self.sent_data_requests = Some(VecDeque::new());

        // Create and send the data client requests to the network
        self.create_and_send_client_requests(&global_data_summary)
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L442-545)
```rust
    pub async fn process_data_responses(
        &mut self,
        global_data_summary: GlobalDataSummary,
    ) -> Result<(), Error> {
        if self.stream_engine.is_stream_complete()
            || self.request_failure_count >= self.streaming_service_config.max_request_retry
            || self.send_failure
        {
            if !self.send_failure && self.stream_end_notification_id.is_none() {
                self.send_end_of_stream_notification().await?;
            }
            return Ok(()); // There's nothing left to do
        }

        // Continuously process any ready data responses
        while let Some(pending_response) = self.pop_pending_response_queue()? {
            // Get the client request and response information
            let maybe_client_response = pending_response.lock().client_response.take();
            let client_response = maybe_client_response.ok_or_else(|| {
                Error::UnexpectedErrorEncountered("The client response should be ready!".into())
            })?;
            let client_request = &pending_response.lock().client_request.clone();

            // Process the client response
            match client_response {
                Ok(client_response) => {
                    // Sanity check and process the response
                    if sanity_check_client_response_type(client_request, &client_response) {
                        // If the response wasn't enough to satisfy the original request (e.g.,
                        // it was truncated), missing data should be requested.
                        let mut head_of_line_blocked = false;
                        match self.request_missing_data(client_request, &client_response.payload) {
                            Ok(missing_data_requested) => {
                                if missing_data_requested {
                                    head_of_line_blocked = true; // We're now head of line blocked on the missing data
                                }
                            },
                            Err(error) => {
                                warn!(LogSchema::new(LogEntry::ReceivedDataResponse)
                                    .stream_id(self.data_stream_id)
                                    .event(LogEvent::Error)
                                    .error(&error)
                                    .message("Failed to determine if missing data was requested!"));
                            },
                        }

                        // If the request was a subscription request and the subscription
                        // stream is lagging behind the data advertisements, the stream
                        // engine should be notified (e.g., so that it can catch up).
                        if client_request.is_subscription_request() {
                            if let Err(error) = self.check_subscription_stream_lag(
                                &global_data_summary,
                                &client_response.payload,
                            ) {
                                self.notify_new_data_request_error(client_request, error)?;
                                head_of_line_blocked = true; // We're now head of line blocked on the failed stream
                            }
                        }

                        // The response is valid, send the data notification to the client
                        self.send_data_notification_to_client(client_request, client_response)
                            .await?;

                        // If the request is for specific data, increase the prefetching limit.
                        // Note: we don't increase the limit for new data requests because
                        // those don't invoke the prefetcher (as we're already up-to-date).
                        if !client_request.is_new_data_request() {
                            self.dynamic_prefetching_state
                                .increase_max_concurrent_requests();
                        }

                        // If we're head of line blocked, we should return early
                        if head_of_line_blocked {
                            break;
                        }
                    } else {
                        // The sanity check failed
                        self.handle_sanity_check_failure(client_request, &client_response.context)?;
                        break; // We're now head of line blocked on the failed request
                    }
                },
                Err(error) => {
                    // Handle the error depending on the request type
                    if client_request.is_new_data_request() {
                        // The request was for new data. We should notify the
                        // stream engine and clear the requests queue.
                        self.notify_new_data_request_error(client_request, error)?;
                    } else {
                        // Decrease the prefetching limit on an error
                        self.dynamic_prefetching_state
                            .decrease_max_concurrent_requests();

                        // Handle the error and simply retry
                        self.handle_data_client_error(client_request, &error)?;
                    }
                    break; // We're now head of line blocked on the failed request
                },
            }
        }

        // Create and send further client requests to the network
        // to ensure we're maximizing the number of concurrent requests.
        self.create_and_send_client_requests(&global_data_summary)
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L733-734)
```rust
        // Increment the number of client failures for this request
        self.request_failure_count += 1;
```

**File:** state-sync/data-streaming-service/src/stream_engine.rs (L373-378)
```rust
                    if number_of_states < self.next_request_index {
                        return Err(Error::NoDataToFetch(format!(
                            "The next state index to fetch is higher than the \
                            total number of states. Next index: {:?}, total states: {:?}",
                            self.next_request_index, number_of_states
                        )));
```
