# Audit Report

## Title
Division by Zero Panics in Peer Selection Due to Missing Configuration Validation

## Summary
The `AptosDataClientConfig` and its nested configuration structures lack input validation, allowing critical parameters to be set to zero. This causes division by zero panics during peer selection, resulting in complete node crashes and denial of state synchronization services.

## Finding Description

Two critical division by zero vulnerabilities exist in the peer selection logic due to missing configuration parameter validation:

**Vulnerability 1: Multi-Fetch Bucket Size Division by Zero**

The `multi_fetch_peer_bucket_size` parameter can be set to zero in the configuration, causing a panic when calculating peer ratios for multi-fetch requests. [1](#0-0) 

When `multi_fetch_peer_bucket_size` is zero, the division operation `num_serviceable_peers / multi_fetch_config.multi_fetch_peer_bucket_size` triggers a panic.

**Vulnerability 2: Latency Filtering Reduction Factor Division by Zero**

The `latency_filtering_reduction_factor` parameter can be set to zero, causing a panic when filtering peers by latency. [2](#0-1) 

When `latency_filtering_reduction_factor` is zero and the latency filtering conditions are met (lines 102-104), the division operation causes a panic.

**Root Cause: Missing Configuration Validation**

Neither `AptosDataClientConfig`, `AptosDataMultiFetchConfig`, nor `AptosLatencyFilteringConfig` implement the `ConfigSanitizer` trait to validate parameter values. [3](#0-2) [4](#0-3) 

The configuration deserialization directly accepts any values from YAML without validation: [5](#0-4) 

**Execution Path for Vulnerability 1:**

1. Node loads configuration with `multi_fetch_peer_bucket_size: 0`
2. State sync client attempts to select peers for a data request
3. Multi-fetch is enabled (default: true)
4. Code reaches line 306 in `client.rs` calculating peer ratio
5. Division by zero causes panic
6. Node crashes [6](#0-5) 

**Execution Path for Vulnerability 2:**

1. Node loads configuration with `latency_filtering_reduction_factor: 0`
2. Peer selection with `ignore_high_latency_peers=true` is invoked
3. At least 10 peers are available (default threshold)
4. Peer ratio meets the threshold (default: 5)
5. Code reaches line 107 in `utils.rs` attempting to divide
6. Division by zero causes panic
7. Node crashes [7](#0-6) 

This function calls the vulnerable `choose_peers_by_latency` with `ignore_high_latency_peers=true`.

## Impact Explanation

**Critical Severity** - This meets the Critical impact criteria for "Total loss of liveness/network availability":

1. **Complete Node Failure**: The division by zero causes an unrecoverable panic, crashing the entire node process
2. **State Synchronization DoS**: The node cannot sync state from the network, becoming permanently out of sync
3. **No Graceful Degradation**: There is no error handling - the panic is unrecoverable
4. **Widespread Impact Potential**: If malicious configurations are distributed via deployment automation, multiple nodes could be affected simultaneously

This breaks the **availability invariant** - nodes must remain operational to maintain network liveness. A coordinated attack distributing malicious configurations could cause widespread network degradation.

## Likelihood Explanation

**Likelihood: Low to Medium**

The attack requires one of the following scenarios:

1. **Compromised Deployment Pipeline**: Attacker injects malicious configuration values into deployment scripts or configuration management systems
2. **Supply Chain Attack**: Malicious configuration templates distributed to node operators  
3. **Accidental Misconfiguration**: Node operator accidentally sets invalid values (defense-in-depth failure)
4. **Compromised Node**: Attacker with filesystem access modifies configuration

While direct exploitation requires privileged access, the **complete absence of validation** represents a critical defense-in-depth failure. The Aptos codebase demonstrates configuration validation best practices elsewhere (e.g., `StateSyncDriverConfig::sanitize`), making this omission particularly concerning. [8](#0-7) 

## Recommendation

Implement `ConfigSanitizer` for all three configuration structures to validate critical parameters:

```rust
impl ConfigSanitizer for AptosDataMultiFetchConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let config = &node_config.state_sync.aptos_data_client.data_multi_fetch_config;
        
        if config.multi_fetch_peer_bucket_size == 0 {
            return Err(Error::ConfigSanitizerFailed(
                "AptosDataMultiFetchConfig".into(),
                "multi_fetch_peer_bucket_size must be greater than 0".into(),
            ));
        }
        
        if config.min_peers_for_multi_fetch > config.max_peers_for_multi_fetch {
            return Err(Error::ConfigSanitizerFailed(
                "AptosDataMultiFetchConfig".into(),
                "min_peers_for_multi_fetch cannot exceed max_peers_for_multi_fetch".into(),
            ));
        }
        
        Ok(())
    }
}

impl ConfigSanitizer for AptosLatencyFilteringConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let config = &node_config.state_sync.aptos_data_client.latency_filtering_config;
        
        if config.latency_filtering_reduction_factor == 0 {
            return Err(Error::ConfigSanitizerFailed(
                "AptosLatencyFilteringConfig".into(),
                "latency_filtering_reduction_factor must be greater than 0".into(),
            ));
        }
        
        Ok(())
    }
}
```

Register these sanitizers in the `StateSyncConfig::sanitize` implementation to ensure validation occurs during configuration loading.

## Proof of Concept

**PoC for Vulnerability 1 (Multi-Fetch Division by Zero):**

```rust
// File: state-sync/aptos-data-client/src/tests/config_validation.rs
#[tokio::test]
#[should_panic(expected = "attempt to divide by zero")]
async fn test_multi_fetch_zero_bucket_size_panic() {
    use crate::{client::AptosDataClient, tests::mock::MockNetwork};
    use aptos_config::config::{AptosDataClientConfig, AptosDataMultiFetchConfig};
    
    // Create malicious config with zero bucket size
    let malicious_config = AptosDataClientConfig {
        data_multi_fetch_config: AptosDataMultiFetchConfig {
            enable_multi_fetch: true,
            multi_fetch_peer_bucket_size: 0, // MALICIOUS VALUE
            min_peers_for_multi_fetch: 2,
            max_peers_for_multi_fetch: 3,
            additional_requests_per_peer_bucket: 1,
        },
        ..Default::default()
    };
    
    // Create mock network with malicious config
    let (mut mock_network, _, client, _) = MockNetwork::new(
        None,
        Some(malicious_config),
        None,
    );
    
    // Add sufficient peers to trigger multi-fetch logic
    let peer_priority = crate::priority::PeerPriority::HighPriority;
    for _ in 0..20 {
        mock_network.add_peer(peer_priority);
    }
    
    // Create a non-subscription request that will trigger peer selection
    use aptos_storage_service_types::requests::{DataRequest, StorageServiceRequest};
    let request = StorageServiceRequest::new(
        DataRequest::GetServerProtocolVersion,
        true,
    );
    
    // This will panic due to division by zero at client.rs:306
    let _ = client.choose_peers_for_request(&request).await;
}
```

**PoC for Vulnerability 2 (Latency Filtering Division by Zero):**

```rust
#[tokio::test]  
#[should_panic(expected = "attempt to divide by zero")]
async fn test_latency_filtering_zero_reduction_factor_panic() {
    use crate::{client::AptosDataClient, tests::mock::MockNetwork};
    use aptos_config::config::{AptosDataClientConfig, AptosLatencyFilteringConfig};
    
    // Create malicious config with zero reduction factor
    let malicious_config = AptosDataClientConfig {
        latency_filtering_config: AptosLatencyFilteringConfig {
            latency_filtering_reduction_factor: 0, // MALICIOUS VALUE
            min_peer_ratio_for_latency_filtering: 5,
            min_peers_for_latency_filtering: 10,
        },
        ..Default::default()
    };
    
    // Create mock network with malicious config
    let (mut mock_network, _, client, _) = MockNetwork::new(
        None,
        Some(malicious_config),
        None,
    );
    
    // Add sufficient peers (>10) with latency metadata to trigger filtering
    let peer_priority = crate::priority::PeerPriority::HighPriority;
    for i in 0..50 {
        let peer = mock_network.add_peer(peer_priority);
        // Set latency for each peer
        mock_network.update_peer_latency(peer, 0.1 + (i as f64 * 0.01));
    }
    
    // Trigger peer selection with latency filtering
    // This will panic due to division by zero at utils.rs:107
    let serviceable_peers = client.get_serviceable_peers().await;
    let _ = client.choose_random_peers_by_latency(serviceable_peers, 5);
}
```

## Notes

This vulnerability demonstrates a **defense-in-depth failure**. While exploitation requires privileged access to modify configuration files, the complete absence of validation for parameters used in division operations represents a critical oversight. The Aptos codebase implements configuration sanitizers elsewhere, establishing a pattern that should be followed for all configurations that affect critical operations.

The impact is particularly severe because:
1. The panics are unrecoverable (no error handling)
2. State synchronization completely fails
3. No graceful degradation or fallback mechanism exists
4. Automated deployment systems could propagate malicious configurations to multiple nodes

This finding specifically addresses the security question: **Yes, malicious modification of `AptosDataClientConfig` parameters can cause all peer selection functions to fail** through panic-induced node crashes.

### Citations

**File:** state-sync/aptos-data-client/src/client.rs (L290-320)
```rust
        let multi_fetch_config = self.data_client_config.data_multi_fetch_config;
        let num_peers_for_request = if multi_fetch_config.enable_multi_fetch {
            // Calculate the total number of priority serviceable peers
            let mut num_serviceable_peers = 0;
            for (index, peers) in serviceable_peers_by_priorities.iter().enumerate() {
                // Only include the lowest priority peers if no other peers are
                // available (the lowest priority peers are generally unreliable).
                if (num_serviceable_peers == 0)
                    || (index < serviceable_peers_by_priorities.len() - 1)
                {
                    num_serviceable_peers += peers.len();
                }
            }

            // Calculate the number of peers to select for the request
            let peer_ratio_for_request =
                num_serviceable_peers / multi_fetch_config.multi_fetch_peer_bucket_size;
            let mut num_peers_for_request = multi_fetch_config.min_peers_for_multi_fetch
                + (peer_ratio_for_request * multi_fetch_config.additional_requests_per_peer_bucket);

            // Bound the number of peers by the number of serviceable peers
            num_peers_for_request = min(num_peers_for_request, num_serviceable_peers);

            // Ensure the number of peers is no larger than the maximum
            min(
                num_peers_for_request,
                multi_fetch_config.max_peers_for_multi_fetch,
            )
        } else {
            1 // Multi-fetch is disabled (only select a single peer)
        };
```

**File:** state-sync/aptos-data-client/src/client.rs (L521-537)
```rust
    fn choose_random_peers_by_latency(
        &self,
        serviceable_peers: HashSet<PeerNetworkId>,
        num_peers_to_choose: usize,
    ) -> HashSet<PeerNetworkId> {
        // Choose peers weighted by latency
        let selected_peers = utils::choose_peers_by_latency(
            self.data_client_config.clone(),
            num_peers_to_choose as u64,
            serviceable_peers.clone(),
            self.get_peers_and_metadata(),
            true,
        );

        // Extend the selected peers with random peers (if necessary)
        utils::extend_with_random_peers(selected_peers, serviceable_peers, num_peers_to_choose)
    }
```

**File:** state-sync/aptos-data-client/src/utils.rs (L107-107)
```rust
            num_peers_to_consider /= latency_filtering_config.latency_filtering_reduction_factor
```

**File:** config/src/config/state_sync_config.rs (L362-388)
```rust
pub struct AptosDataMultiFetchConfig {
    /// Whether or not to enable multi-fetch for data client requests
    pub enable_multi_fetch: bool,
    /// The number of additional requests to send per peer bucket
    pub additional_requests_per_peer_bucket: usize,
    /// The minimum number of peers for each multi-fetch request
    pub min_peers_for_multi_fetch: usize,
    /// The maximum number of peers for each multi-fetch request
    pub max_peers_for_multi_fetch: usize,
    /// The number of peers per multi-fetch bucket. We use buckets
    /// to track the number of peers that can service a multi-fetch
    /// request and determine the number of requests to send based on
    /// the configured min, max and additional requests per bucket.
    pub multi_fetch_peer_bucket_size: usize,
}

impl Default for AptosDataMultiFetchConfig {
    fn default() -> Self {
        Self {
            enable_multi_fetch: true,
            additional_requests_per_peer_bucket: 1,
            min_peers_for_multi_fetch: 2,
            max_peers_for_multi_fetch: 3,
            multi_fetch_peer_bucket_size: 10,
        }
    }
}
```

**File:** config/src/config/state_sync_config.rs (L392-409)
```rust
pub struct AptosLatencyFilteringConfig {
    /// The reduction factor for latency filtering when selecting peers
    pub latency_filtering_reduction_factor: u64,
    /// Minimum peer ratio for latency filtering
    pub min_peer_ratio_for_latency_filtering: u64,
    /// Minimum number of peers before latency filtering can occur
    pub min_peers_for_latency_filtering: u64,
}

impl Default for AptosLatencyFilteringConfig {
    fn default() -> Self {
        Self {
            latency_filtering_reduction_factor: 2, // Only consider the best 50% of peers
            min_peer_ratio_for_latency_filtering: 5, // Only filter if we have at least 5 potential peers per request
            min_peers_for_latency_filtering: 10, // Only filter if we have at least 10 total peers
        }
    }
}
```

**File:** config/src/config/state_sync_config.rs (L413-458)
```rust
pub struct AptosDataClientConfig {
    /// Whether transaction data v2 is enabled
    pub enable_transaction_data_v2: bool,
    /// The aptos data poller config for the data client
    pub data_poller_config: AptosDataPollerConfig,
    /// The aptos data multi-fetch config for the data client
    pub data_multi_fetch_config: AptosDataMultiFetchConfig,
    /// Whether or not to ignore peers with low peer scores
    pub ignore_low_score_peers: bool,
    /// The aptos latency filtering config for the data client
    pub latency_filtering_config: AptosLatencyFilteringConfig,
    /// The interval (milliseconds) at which to refresh the latency monitor
    pub latency_monitor_loop_interval_ms: u64,
    /// Maximum number of epoch ending ledger infos per chunk
    pub max_epoch_chunk_size: u64,
    /// Maximum number of output reductions (division by 2) before transactions are returned,
    /// e.g., if 1000 outputs are requested in a single data chunk, and this is set to 1, then
    /// we'll accept anywhere between 1000 and 500 outputs. Any less, and the server should
    /// return transactions instead of outputs.
    // TODO: migrate away from this, and use cleaner chunk packing configs and logic.
    pub max_num_output_reductions: u64,
    /// Maximum lag (in seconds) we'll tolerate when sending optimistic fetch requests
    pub max_optimistic_fetch_lag_secs: u64,
    /// Maximum number of bytes to send in a single response
    pub max_response_bytes: u64,
    /// Maximum timeout (in ms) when waiting for a response (after exponential increases)
    pub max_response_timeout_ms: u64,
    /// Maximum number of state keys and values per chunk
    pub max_state_chunk_size: u64,
    /// Maximum lag (in seconds) we'll tolerate when sending subscription requests
    pub max_subscription_lag_secs: u64,
    /// Maximum number of transactions per chunk
    pub max_transaction_chunk_size: u64,
    /// Maximum number of transaction outputs per chunk
    pub max_transaction_output_chunk_size: u64,
    /// Timeout (in ms) when waiting for an optimistic fetch response
    pub optimistic_fetch_timeout_ms: u64,
    /// The duration (in seconds) after which to panic if no progress has been made
    pub progress_check_max_stall_time_secs: u64,
    /// First timeout (in ms) when waiting for a response
    pub response_timeout_ms: u64,
    /// Timeout (in ms) when waiting for a subscription response
    pub subscription_response_timeout_ms: u64,
    /// Whether or not to request compression for incoming data
    pub use_compression: bool,
}
```

**File:** config/src/config/state_sync_config.rs (L498-520)
```rust
impl ConfigSanitizer for StateSyncDriverConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let state_sync_driver_config = &node_config.state_sync.state_sync_driver;

        // Verify that auto-bootstrapping is not enabled for
        // nodes that are fast syncing.
        let fast_sync_enabled = state_sync_driver_config.bootstrapping_mode.is_fast_sync();
        if state_sync_driver_config.enable_auto_bootstrapping && fast_sync_enabled {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "Auto-bootstrapping should not be enabled for nodes that are fast syncing!"
                    .to_string(),
            ));
        }

        Ok(())
    }
}
```
