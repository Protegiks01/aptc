# Audit Report

## Title
StateComputer Trait Race Condition: Unsynchronized Epoch State Transitions Can Cause Node Crashes and State Inconsistencies

## Summary
The `StateComputer` trait implementation in `ExecutionProxy` uses two separate, uncoordinated locks (`write_mutex` and `state` RwLock) to protect related state. This allows synchronous methods (`new_epoch`, `end_epoch`) to race with async methods (`sync_to_target`, `sync_for_duration`), leading to node panics, executor state inconsistencies, and potential consensus violations during epoch transitions.

## Finding Description

The `StateComputer` trait declares thread safety via `Send + Sync` bounds [1](#0-0) , suggesting thread-safe concurrent access. However, the `ExecutionProxy` implementation uses two separate synchronization primitives without coordination:

1. **`write_mutex: AsyncMutex<LogicalTime>`** - protects executor operations in async methods [2](#0-1) 

2. **`state: RwLock<Option<MutableState>>`** - protects epoch configuration in synchronous methods [3](#0-2) 

The async methods (`sync_to_target`, `sync_for_duration`) acquire `write_mutex` and interact with the executor [4](#0-3) , while synchronous methods (`new_epoch`, `end_epoch`) only acquire the `state` lock [5](#0-4) .

**Critical Race Scenario 1: Node Panic During Epoch Transition**

Task A (background state sync): `sync_to_target()`
- Acquires `write_mutex` [6](#0-5) 
- Calls `executor.finish()` to clear SMT memory [7](#0-6) 
- Performing state synchronization...

Task B (epoch transition): `end_epoch()`
- Acquires `state.write()` lock (NO coordination with `write_mutex`)
- Sets state to `None` [8](#0-7) 

Task C (consensus operation): `pipeline_builder()`
- Attempts to read state [9](#0-8) 
- **Panics with "must be set within an epoch"** because state is `None`

**Critical Race Scenario 2: Executor/Epoch State Desynchronization**

Task A: `sync_to_target(old_epoch_target)`
- Acquires `write_mutex`
- Calls `executor.finish()` [7](#0-6) 
- Reads epoch state for old epoch [10](#0-9) 
- Performing state sync for old epoch target...

Task B: `new_epoch(new_epoch_state)`
- Writes new epoch configuration to `state` (NO coordination with Task A) [11](#0-10) 

Task A continues:
- Updates `logical_time` to old epoch target [12](#0-11) 
- Calls `executor.reset()` [13](#0-12) 

**Result**: The `state` field contains new epoch configuration, but `logical_time` and executor are synchronized to the old epoch. This breaks the fundamental invariant that executor state must match epoch state, potentially causing validators to process blocks with incorrect epoch configurations.

This race can occur during epoch transitions when:
- Background state sync operations (e.g., consensus observer fallback) overlap with epoch changes [14](#0-13) 
- Network delays cause concurrent execution of epoch transition and sync operations
- Multiple execution paths call StateComputer methods without global coordination

## Impact Explanation

**High Severity** (up to $50,000):

1. **Validator Node Crashes**: The panic in `pipeline_builder` causes immediate node termination during epoch transitions, leading to validator unavailability and network liveness degradation.

2. **State Inconsistency**: Desynchronization between epoch configuration and executor state can cause validators to:
   - Process blocks with wrong validator set
   - Apply incorrect consensus parameters
   - Mistrack logical time across epochs

3. **Consensus Degradation**: Multiple validators crashing simultaneously during epoch transitions (a critical period) can push the network toward the 2/3 liveness threshold, potentially halting block production.

While this doesn't directly enable fund theft or consensus safety violations under normal Byzantine assumptions, it represents a significant protocol vulnerability that degrades network availability during critical epoch transitions—a key attack surface for adversarial exploitation.

## Likelihood Explanation

**Medium-High Likelihood**:

- Epoch transitions occur regularly in the Aptos network
- State sync operations can be triggered by various events (falling behind, network partitions, observer fallback)
- The race window is non-trivial: state sync operations can take seconds during high load
- No attacker-controlled input required—occurs during normal protocol operation under adversarial network conditions
- Can be exacerbated by network delays, high load, or coordinated timing attacks during epoch boundaries

An adversary could increase likelihood by:
- Causing network partitions during epoch transitions
- Triggering state sync via targeted denial of blocks
- Timing attacks at epoch boundaries

## Recommendation

Introduce a unified synchronization mechanism that coordinates ALL StateComputer operations:

```rust
pub struct ExecutionProxy {
    executor: Arc<dyn BlockExecutorTrait>,
    txn_notifier: Arc<dyn TxnNotifier>,
    state_sync_notifier: Arc<dyn ConsensusNotificationSender>,
    // Replace separate locks with single async RwLock
    epoch_state: AsyncRwLock<EpochState>,
    txn_filter_config: Arc<BlockTransactionFilterConfig>,
    enable_pre_commit: bool,
    secret_share_config: Option<SecretShareConfig>,
}

// All methods must acquire the same lock:
async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
    let mut epoch_guard = self.epoch_state.write().await;  // Acquire write lock
    epoch_guard.logical_time = ...; 
    self.executor.finish();
    // ... state sync operations
    self.executor.reset()?;
    Ok(())
}

async fn new_epoch(&self, epoch_state: &EpochState, ...) {
    let mut epoch_guard = self.epoch_state.write().await;  // Coordinate with sync methods
    epoch_guard.state = Some(MutableState { ... });
}
```

**Alternative**: Make `new_epoch` and `end_epoch` async and have them acquire `write_mutex` to coordinate with state sync operations.

**Key Principle**: All methods that access or modify epoch-related state (executor operations, logical time, epoch configuration) must be protected by the SAME synchronization primitive.

## Proof of Concept

```rust
#[cfg(test)]
mod race_condition_test {
    use super::*;
    use std::sync::Arc;
    use tokio::time::{sleep, Duration};
    
    #[tokio::test(flavor = "multi_thread", worker_threads = 4)]
    async fn test_concurrent_epoch_transition_and_sync() {
        // Setup ExecutionProxy with real executor (or mock)
        let execution_proxy = Arc::new(setup_execution_proxy());
        
        // Simulate epoch N state
        let epoch_n_state = create_test_epoch_state(10);
        execution_proxy.new_epoch(&epoch_n_state, ...);
        
        let proxy_clone = execution_proxy.clone();
        
        // Task 1: Long-running state sync (simulating slow sync)
        let sync_task = tokio::spawn(async move {
            let target = create_target_ledger_info(10, 100);
            // This holds write_mutex for the duration
            proxy_clone.sync_to_target(target).await
        });
        
        // Small delay to ensure sync_task acquires write_mutex
        sleep(Duration::from_millis(10)).await;
        
        // Task 2: Epoch transition (runs concurrently!)
        let proxy_clone2 = execution_proxy.clone();
        let epoch_task = tokio::spawn(async move {
            // This modifies state WITHOUT acquiring write_mutex
            proxy_clone2.end_epoch();
            let epoch_n_plus_1_state = create_test_epoch_state(11);
            proxy_clone2.new_epoch(&epoch_n_plus_1_state, ...);
        });
        
        // Task 3: Attempt to create pipeline (will panic if state is None)
        let proxy_clone3 = execution_proxy.clone();
        let pipeline_task = tokio::spawn(async move {
            sleep(Duration::from_millis(15)).await;
            // This should panic if end_epoch() cleared state
            proxy_clone3.pipeline_builder(create_test_signer());
        });
        
        // Wait for tasks
        let results = tokio::try_join!(sync_task, epoch_task, pipeline_task);
        
        // Expected: pipeline_task panics with "must be set within an epoch"
        // or state is inconsistent between executor and epoch config
        assert!(results.is_err() || state_is_inconsistent(&execution_proxy));
    }
}
```

**Expected Result**: The test demonstrates either:
1. A panic from `pipeline_builder` when state is `None`
2. State inconsistency where `execution_proxy.state` epoch differs from `write_mutex` logical time

This confirms the lack of thread-safe coordination between synchronous and asynchronous StateComputer methods.

### Citations

**File:** consensus/src/state_replication.rs (L23-23)
```rust
pub trait StateComputer: Send + Sync {
```

**File:** consensus/src/state_computer.rs (L58-58)
```rust
    write_mutex: AsyncMutex<LogicalTime>,
```

**File:** consensus/src/state_computer.rs (L60-60)
```rust
    state: RwLock<Option<MutableState>>,
```

**File:** consensus/src/state_computer.rs (L98-102)
```rust
            .state
            .read()
            .as_ref()
            .cloned()
            .expect("must be set within an epoch");
```

**File:** consensus/src/state_computer.rs (L177-233)
```rust
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        // Grab the logical time lock and calculate the target logical time
        let mut latest_logical_time = self.write_mutex.lock().await;
        let target_logical_time =
            LogicalTime::new(target.ledger_info().epoch(), target.ledger_info().round());

        // Before state synchronization, we have to call finish() to free the
        // in-memory SMT held by BlockExecutor to prevent a memory leak.
        self.executor.finish();

        // The pipeline phase already committed beyond the target block timestamp, just return.
        if *latest_logical_time >= target_logical_time {
            warn!(
                "State sync target {:?} is lower than already committed logical time {:?}",
                target_logical_time, *latest_logical_time
            );
            return Ok(());
        }

        // This is to update QuorumStore with the latest known commit in the system,
        // so it can set batches expiration accordingly.
        // Might be none if called in the recovery path, or between epoch stop and start.
        if let Some(inner) = self.state.read().as_ref() {
            let block_timestamp = target.commit_info().timestamp_usecs();
            inner
                .payload_manager
                .notify_commit(block_timestamp, Vec::new());
        }

        // Inject an error for fail point testing
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });

        // Invoke state sync to synchronize to the specified target. Here, the
        // ChunkExecutor will process chunks and commit to storage. However, after
        // block execution and commits, the internal state of the ChunkExecutor may
        // not be up to date. So, it is required to reset the cache of the
        // ChunkExecutor in state sync when requested to sync.
        let result = monitor!(
            "sync_to_target",
            self.state_sync_notifier.sync_to_target(target).await
        );

        // Update the latest logical time
        *latest_logical_time = target_logical_time;

        // Similarly, after state synchronization, we have to reset the cache of
        // the BlockExecutor to guarantee the latest committed state is up to date.
        self.executor.reset()?;

        // Return the result
        result.map_err(|error| {
            let anyhow_error: anyhow::Error = error.into();
            anyhow_error.into()
        })
    }
```

**File:** consensus/src/state_computer.rs (L235-268)
```rust
    fn new_epoch(
        &self,
        epoch_state: &EpochState,
        payload_manager: Arc<dyn TPayloadManager>,
        transaction_shuffler: Arc<dyn TransactionShuffler>,
        block_executor_onchain_config: BlockExecutorConfigFromOnchain,
        transaction_deduper: Arc<dyn TransactionDeduper>,
        randomness_enabled: bool,
        consensus_onchain_config: OnChainConsensusConfig,
        persisted_auxiliary_info_version: u8,
        network_sender: Arc<NetworkSender>,
    ) {
        *self.state.write() = Some(MutableState {
            validators: epoch_state
                .verifier
                .get_ordered_account_addresses_iter()
                .collect::<Vec<_>>()
                .into(),
            payload_manager,
            transaction_shuffler,
            block_executor_onchain_config,
            transaction_deduper,
            is_randomness_enabled: randomness_enabled,
            consensus_onchain_config,
            persisted_auxiliary_info_version,
            network_sender,
        });
    }

    // Clears the epoch-specific state. Only a sync_to call is expected before calling new_epoch
    // on the next epoch.
    fn end_epoch(&self) {
        self.state.write().take();
    }
```

**File:** consensus/src/consensus_observer/observer/state_sync_manager.rs (L150-153)
```rust
                let latest_synced_ledger_info = match execution_client
                    .clone()
                    .sync_for_duration(fallback_duration)
                    .await
```
