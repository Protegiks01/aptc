# Audit Report

## Title
Consensus Safety Violation via Non-Durable Safety Data Writes Allowing Double-Voting After Crash

## Summary
The OnDiskStorage backend used for persisting consensus safety data lacks proper `fsync()` calls after writing critical voting state to disk. When combined with the enabled-by-default safety data caching (`enable_cached_safety_data: true`), validators can double-vote on the same round after unexpected crashes, violating BFT consensus safety guarantees.

## Finding Description

The vulnerability stems from a critical durability gap in the storage layer that persists consensus safety data. The SafetyData structure contains essential voting state including `last_voted_round`, `last_vote`, `preferred_round`, and `one_chain_round` that prevent double-voting—a fundamental requirement for BFT consensus safety. [1](#0-0) 

The vulnerability occurs through the following chain:

**1. Storage Write Without fsync:**
The OnDiskStorage backend's `write()` method performs file operations but never calls `sync_all()` or `fsync()`: [2](#0-1) 

This means writes return `Ok(())` even though data may only be in OS buffer cache, not on persistent storage.

**2. Cached Safety Data Enabled by Default:**
The safety rules configuration enables caching by default: [3](#0-2) 

**3. Cache Update on Non-Durable Write:**
When safety data is updated (e.g., after voting), the cache is updated immediately upon storage write "success": [4](#0-3) 

**4. Voting Logic Relies on Safety Data:**
The voting function reads safety data, updates `last_voted_round` to prevent double-voting, then persists it: [5](#0-4) 

The double-voting prevention check: [6](#0-5) 

**Attack Scenario:**

1. Validator is at round 9 (persisted and cached)
2. Vote request arrives for round 10
3. `guarded_construct_and_sign_vote_two_chain()` executes:
   - Reads safety_data (round 9 from cache or storage)
   - Updates local `last_voted_round` to 10
   - Calls `set_safety_data()` which writes to OnDiskStorage (returns Ok without fsync) and updates cache to round 10
   - Creates and broadcasts vote for round 10
4. **Node crashes** (power failure, kernel panic, OOM killer) before OS flushes buffers
5. On restart:
   - Cache is empty (it was in-memory)
   - Storage still contains round 9 (write wasn't fsynced)
6. Another vote request for round 10 arrives
7. Safety check passes (last_voted_round = 9 < 10)
8. **Validator double-votes on round 10** → Consensus safety violation

**Production Applicability:**
The OnDiskStorage backend is production-viable. The config sanitizer only blocks InMemoryStorage for mainnet: [7](#0-6) 

## Impact Explanation

**Severity: CRITICAL ($1,000,000 tier)**

This vulnerability directly violates the **Consensus Safety** invariant: "AptosBFT must prevent double-spending and chain splits under < 1/3 Byzantine nodes."

Double-voting allows a single validator to sign conflicting blocks at the same round, which can lead to:

1. **Chain Forks**: If multiple validators crash and restart, different validators may commit different blocks at the same round
2. **Safety Proof Violation**: BFT consensus guarantees break when validators equivocate (vote for conflicting blocks)
3. **Network Partition Risk**: Divergent commit histories may require manual intervention or hard fork to resolve

Under the Aptos bug bounty criteria, this qualifies as:
- **Consensus/Safety violations** (Critical tier)
- Potential for **non-recoverable network partition requiring hardfork** (Critical tier)

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability triggers on any unexpected node crash that occurs after a storage write returns but before OS buffer flush:

1. **Power failures** - Common in datacenter environments
2. **Kernel panics** - OS-level crashes
3. **OOM killer** - Process termination under memory pressure
4. **Hardware failures** - Disk controller errors, memory corruption

The time window between `write_all()` returning and actual disk persistence can be **several seconds** depending on OS write-back settings. Modern filesystems typically have 5-30 second writeback intervals.

Given that validators may vote multiple times per second during normal operation, the probability of crash during the vulnerability window is non-trivial, especially over extended operational periods.

## Recommendation

Add explicit durability guarantees to OnDiskStorage by calling `sync_all()` after writing data:

```rust
fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
    let contents = serde_json::to_vec(data)?;
    let mut file = File::create(self.temp_path.path())?;
    file.write_all(&contents)?;
    file.sync_all()?;  // ADD THIS LINE - Force fsync before rename
    fs::rename(&self.temp_path, &self.file_path)?;
    Ok(())
}
```

**Alternative/Additional Mitigations:**

1. **Disable caching for safety data**: Set `enable_cached_safety_data: false` to always read from storage, though this still doesn't fix the root cause
2. **Add write verification**: After writing, re-read the value to confirm persistence
3. **Document durability requirements**: Clearly document that all storage backends MUST provide durability guarantees
4. **Storage backend validation**: Add runtime checks that storage backends properly fsync

## Proof of Concept

```rust
// Rust integration test demonstrating the vulnerability
// File: consensus/safety-rules/tests/crash_recovery_test.rs

#[test]
fn test_double_vote_after_crash_without_fsync() {
    use aptos_secure_storage::{OnDiskStorage, KVStorage, Storage};
    use aptos_consensus_types::safety_data::SafetyData;
    use aptos_temppath::TempPath;
    use std::process;
    
    // Setup: Create OnDiskStorage
    let temp_path = TempPath::new();
    temp_path.create_as_file().unwrap();
    let storage_path = temp_path.path().to_path_buf();
    
    // Phase 1: Vote on round 10
    {
        let mut storage = OnDiskStorage::new(storage_path.clone());
        
        // Initial state: round 9
        let safety_data_r9 = SafetyData::new(1, 9, 9, 9, None, 0);
        storage.set("safety_data", safety_data_r9).unwrap();
        
        // Update to round 10 (simulating a vote)
        let safety_data_r10 = SafetyData::new(1, 10, 10, 10, None, 0);
        storage.set("safety_data", safety_data_r10).unwrap();
        
        // Write returns Ok, but data may not be on disk yet
        // Simulate crash by NOT dropping storage cleanly
        std::mem::forget(storage); // Simulates abrupt termination
    }
    
    // Phase 2: After "crash" - read storage again
    {
        let storage = OnDiskStorage::new(storage_path.clone());
        let recovered_data: SafetyData = storage.get("safety_data")
            .unwrap()
            .value;
        
        // BUG: Without fsync, this may still be round 9!
        // This allows double-voting on round 10
        println!("Recovered last_voted_round: {}", recovered_data.last_voted_round);
        
        // In a real scenario, if this is still 9, the validator 
        // would accept another vote request for round 10
        assert_eq!(recovered_data.last_voted_round, 10, 
            "VULNERABILITY: Safety data not durable - allows double voting!");
    }
}
```

**Note:** This test may pass or fail depending on OS write-back timing. To reliably reproduce, add a `sync::sync()` call or kill the process forcefully between phases.

### Citations

**File:** consensus/consensus-types/src/safety_data.rs (L8-21)
```rust
/// Data structure for safety rules to ensure consensus safety.
#[derive(Debug, Deserialize, Eq, PartialEq, Serialize, Clone, Default)]
pub struct SafetyData {
    pub epoch: u64,
    pub last_voted_round: u64,
    // highest 2-chain round, used for 3-chain
    pub preferred_round: u64,
    // highest 1-chain round, used for 2-chain
    #[serde(default)]
    pub one_chain_round: u64,
    pub last_vote: Option<Vote>,
    #[serde(default)]
    pub highest_timeout_round: u64,
}
```

**File:** secure/storage/src/on_disk.rs (L64-70)
```rust
    fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
        let contents = serde_json::to_vec(data)?;
        let mut file = File::create(self.temp_path.path())?;
        file.write_all(&contents)?;
        fs::rename(&self.temp_path, &self.file_path)?;
        Ok(())
    }
```

**File:** config/src/config/safety_rules_config.rs (L45-45)
```rust
            enable_cached_safety_data: true,
```

**File:** config/src/config/safety_rules_config.rs (L86-96)
```rust
            // Verify that the secure backend is appropriate for mainnet validators
            if chain_id.is_mainnet()
                && node_type.is_validator()
                && safety_rules_config.backend.is_in_memory()
            {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "The secure backend should not be set to in memory storage in mainnet!"
                        .to_string(),
                ));
            }
```

**File:** consensus/safety-rules/src/persistent_safety_storage.rs (L150-170)
```rust
    pub fn set_safety_data(&mut self, data: SafetyData) -> Result<(), Error> {
        let _timer = counters::start_timer("set", SAFETY_DATA);
        counters::set_state(counters::EPOCH, data.epoch as i64);
        counters::set_state(counters::LAST_VOTED_ROUND, data.last_voted_round as i64);
        counters::set_state(
            counters::HIGHEST_TIMEOUT_ROUND,
            data.highest_timeout_round as i64,
        );
        counters::set_state(counters::PREFERRED_ROUND, data.preferred_round as i64);

        match self.internal_store.set(SAFETY_DATA, data.clone()) {
            Ok(_) => {
                self.cached_safety_data = Some(data);
                Ok(())
            },
            Err(error) => {
                self.cached_safety_data = None;
                Err(Error::SecureStorageUnexpectedError(error.to_string()))
            },
        }
    }
```

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L53-95)
```rust
    pub(crate) fn guarded_construct_and_sign_vote_two_chain(
        &mut self,
        vote_proposal: &VoteProposal,
        timeout_cert: Option<&TwoChainTimeoutCertificate>,
    ) -> Result<Vote, Error> {
        // Exit early if we cannot sign
        self.signer()?;

        let vote_data = self.verify_proposal(vote_proposal)?;
        if let Some(tc) = timeout_cert {
            self.verify_tc(tc)?;
        }
        let proposed_block = vote_proposal.block();
        let mut safety_data = self.persistent_storage.safety_data()?;

        // if already voted on this round, send back the previous vote
        // note: this needs to happen after verifying the epoch as we just check the round here
        if let Some(vote) = safety_data.last_vote.clone() {
            if vote.vote_data().proposed().round() == proposed_block.round() {
                return Ok(vote);
            }
        }

        // Two voting rules
        self.verify_and_update_last_vote_round(
            proposed_block.block_data().round(),
            &mut safety_data,
        )?;
        self.safe_to_vote(proposed_block, timeout_cert)?;

        // Record 1-chain data
        self.observe_qc(proposed_block.quorum_cert(), &mut safety_data);
        // Construct and sign vote
        let author = self.signer()?.author();
        let ledger_info = self.construct_ledger_info_2chain(proposed_block, vote_data.hash())?;
        let signature = self.sign(&ledger_info)?;
        let vote = Vote::new_with_signature(vote_data, author, ledger_info, signature);

        safety_data.last_vote = Some(vote.clone());
        self.persistent_storage.set_safety_data(safety_data)?;

        Ok(vote)
    }
```

**File:** consensus/safety-rules/src/safety_rules.rs (L213-232)
```rust
    pub(crate) fn verify_and_update_last_vote_round(
        &self,
        round: Round,
        safety_data: &mut SafetyData,
    ) -> Result<(), Error> {
        if round <= safety_data.last_voted_round {
            return Err(Error::IncorrectLastVotedRound(
                round,
                safety_data.last_voted_round,
            ));
        }

        safety_data.last_voted_round = round;
        trace!(
            SafetyLogSchema::new(LogEntry::LastVotedRound, LogEvent::Update)
                .last_voted_round(safety_data.last_voted_round)
        );

        Ok(())
    }
```
