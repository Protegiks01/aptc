# Audit Report

## Title
Memory Exhaustion DoS via Unbounded DKG Transcript Deserialization in `ark_de()`

## Summary
The `ark_de()` function in the DKG transcript deserialization path lacks size limits before allocating and validating arkworks elliptic curve points. A malicious validator can craft a DKG transcript with oversized multi-dimensional vectors (e.g., `Cs: Vec<Vec<Vec<E::G1>>>`), causing victim validators to exhaust memory or CPU during deserialization, leading to node crashes, hangs, or consensus disruption.

## Finding Description
The vulnerability exists in the DKG (Distributed Key Generation) transcript processing pipeline. When validators submit or receive DKG transcripts, the deserialization path calls `bcs::from_bytes()` without size limits, which internally invokes `ark_de()` for arkworks cryptographic types. [1](#0-0) 

This function deserializes elliptic curve points with validation enabled, but does not check or limit the size of nested vector structures before allocation. The DKG transcript structure contains deeply nested vectors: [2](#0-1) 

The critical field `Cs: Vec<Vec<Vec<E::G1>>>` is a 3-dimensional vector. A malicious validator can craft a transcript where each dimension has 1000+ elements, resulting in billions of G1 points requiring deserialization and validation.

**Attack Path:**

1. Malicious validator crafts a DKG transcript with `Cs` having dimensions [1000][1000][1000] = 1 billion G1 points
2. Transcript is submitted as a `ValidatorTransaction::DKGResult` or broadcast to peers
3. Victim validators deserialize the transcript at multiple locations WITHOUT size checks: [3](#0-2) [4](#0-3) [5](#0-4) 

4. BCS attempts to allocate memory for 1 billion points (~48 GB for compressed points)
5. Arkworks validation checks each point for curve membership (expensive cryptographic operations)
6. Victim validator either:
   - **Crashes** (OOM killer terminates process)
   - **Hangs** (CPU exhaustion from validating millions of points)
   - **Becomes unresponsive** (affecting consensus participation)

This breaks the **Resource Limits** invariant (all operations must respect computational limits) and the **Consensus Safety** invariant (liveness requires validators to remain operational).

## Impact Explanation
**Severity: High** (up to $50,000 per Aptos Bug Bounty)

This vulnerability enables:
- **Validator node slowdowns**: CPU exhaustion from excessive validation operations
- **Validator node crashes**: OOM conditions from unbounded memory allocation
- **Consensus disruption**: If multiple validators process the malicious transcript, consensus could stall

While this requires a malicious validator to exploit, Aptos consensus is designed to tolerate up to 1/3 Byzantine validators. A single validator causing multiple other validators to crash could approach or exceed this threshold, especially if the attack is timed during epoch transitions when DKG is actively running.

This meets the **High Severity** criteria: "Validator node slowdowns" and "Significant protocol violations."

## Likelihood Explanation
**Likelihood: High**

- **Attack Complexity**: Low - requires only crafting a malformed BCS-serialized structure with large length prefixes
- **Attacker Requirements**: Must be an active validator (or compromise validator keys)
- **Detection Difficulty**: High - appears as legitimate DKG traffic until deserialization begins
- **Blast Radius**: Network-wide if transcript propagates through reliable broadcast

The attack is highly feasible because:
1. DKG runs during every epoch transition (regular occurrence)
2. No size validation exists in the deserialization path
3. BCS format allows arbitrary vector lengths (only limited by `usize`)
4. Arkworks validation is computationally expensive by design

## Recommendation
Implement strict size limits before deserialization in all DKG transcript processing paths:

**Fix 1**: Add size limit check before BCS deserialization:

```rust
// In dkg/src/transcript_aggregation/mod.rs:88
const MAX_TRANSCRIPT_SIZE: usize = 10_000_000; // 10 MB limit
ensure!(
    transcript_bytes.len() <= MAX_TRANSCRIPT_SIZE,
    "[DKG] transcript size {} exceeds maximum {}",
    transcript_bytes.len(),
    MAX_TRANSCRIPT_SIZE
);
let transcript = bcs::from_bytes(transcript_bytes.as_slice()).map_err(|e| {
    anyhow!("[DKG] adding peer transcript failed with trx deserialization error: {e}")
})?;
```

**Fix 2**: Use `bcs::from_bytes_with_limit()` instead of `bcs::from_bytes()`:

```rust
let transcript = bcs::from_bytes_with_limit(
    transcript_bytes.as_slice(),
    MAX_TRANSCRIPT_SIZE
).map_err(|e| {
    anyhow!("[DKG] adding peer transcript failed with trx deserialization error: {e}")
})?;
```

Apply similar fixes to: [4](#0-3) [5](#0-4) 

**Fix 3**: Add structural validation in `Subtranscript` deserialization to check vector dimensions:

```rust
const MAX_VECTOR_LEN: usize = 1000;
ensure!(
    subtranscript.Cs.len() <= MAX_VECTOR_LEN &&
    subtranscript.Cs.iter().all(|v| v.len() <= MAX_VECTOR_LEN) &&
    subtranscript.Cs.iter().all(|v| v.iter().all(|w| w.len() <= MAX_VECTOR_LEN)),
    "Transcript vector dimensions exceed limits"
);
```

## Proof of Concept

```rust
// PoC demonstrating memory exhaustion attack
// File: crates/aptos-dkg/tests/dos_transcript_test.rs

#[cfg(test)]
mod dos_transcript_attack {
    use aptos_dkg::pvss::das::WeightedTranscript;
    use ark_bn254::{G1Projective, Fr};
    use ark_ec::CurveGroup;
    use std::time::Instant;
    
    #[test]
    #[ignore] // Ignore by default as it will OOM/hang
    fn test_malicious_oversized_transcript() {
        // Craft a malicious Subtranscript with huge Cs vector
        let malicious_cs_size = 1000; // 1000^3 = 1 billion points would be needed
        
        // Even 100^3 = 1 million points will cause issues
        let test_size = 100;
        
        println!("Creating malicious transcript with {}^3 = {} G1 points", 
                 test_size, test_size.pow(3));
        
        let mut malicious_cs = Vec::new();
        for i in 0..test_size {
            let mut middle_vec = Vec::new();
            for j in 0..test_size {
                let mut inner_vec = Vec::new();
                for k in 0..test_size {
                    // Even creating this many points is expensive
                    inner_vec.push(G1Projective::generator());
                }
                middle_vec.push(inner_vec);
            }
            malicious_cs.push(middle_vec);
            if i % 10 == 0 {
                println!("Progress: {}/{}", i, test_size);
            }
        }
        
        println!("Created malicious structure, attempting serialization...");
        let start = Instant::now();
        
        // Serialize the malicious structure
        let serialized = bcs::to_bytes(&malicious_cs).expect("Serialization failed");
        println!("Serialized {} MB in {:?}", 
                 serialized.len() / 1_000_000, 
                 start.elapsed());
        
        // Now attempt deserialization (this will hang/crash)
        println!("Attempting deserialization (this will hang/crash)...");
        let deser_start = Instant::now();
        
        // This will cause OOM or extreme CPU usage
        let _result: Vec<Vec<Vec<G1Projective>>> = bcs::from_bytes(&serialized)
            .expect("Deserialization failed");
            
        println!("Deserialization took {:?}", deser_start.elapsed());
        
        // If we reach here, the system survived, but at what cost?
        println!("System survived but may have experienced severe slowdown");
    }
    
    #[test]
    fn test_transcript_size_check_missing() {
        // Demonstrate that no size check exists
        use aptos_types::dkg::DKGTranscript;
        use move_core_types::account_address::AccountAddress;
        
        // Create a large transcript
        let huge_payload = vec![0u8; 100_000_000]; // 100 MB
        let transcript = DKGTranscript::new(0, AccountAddress::ZERO, huge_payload);
        
        // No size validation happens at construction time
        assert_eq!(transcript.transcript_bytes.len(), 100_000_000);
        println!("Successfully created 100MB transcript without size checks");
    }
}
```

## Notes
The vulnerability affects all DKG transcript processing paths during epoch transitions. The fix requires adding size limits at multiple layers: BCS deserialization, structure validation, and potentially rate limiting of DKG submissions. The codebase uses `bcs::from_bytes_with_limit` in other security-critical paths (e.g., network handshakes), demonstrating awareness of this attack vector, but DKG processing was overlooked.

### Citations

**File:** crates/aptos-batch-encryption/src/shared/ark_serialize.rs (L18-25)
```rust
pub fn ark_de<'de, D, A: CanonicalDeserialize>(data: D) -> Result<A, D::Error>
where
    D: serde::de::Deserializer<'de>,
{
    let s: Bytes = serde::de::Deserialize::deserialize(data)?;
    let a = A::deserialize_with_mode(s.reader(), Compress::Yes, Validate::Yes);
    a.map_err(serde::de::Error::custom)
}
```

**File:** crates/aptos-dkg/src/pvss/chunky/weighted_transcript.rs (L78-91)
```rust
pub struct Subtranscript<E: Pairing> {
    // The dealt public key
    #[serde(serialize_with = "ark_se", deserialize_with = "ark_de")]
    pub V0: E::G2,
    // The dealt public key shares
    #[serde(serialize_with = "ark_se", deserialize_with = "ark_de")]
    pub Vs: Vec<Vec<E::G2>>,
    /// First chunked ElGamal component: C[i][j] = s_{i,j} * G + r_j * ek_i. Here s_i = \sum_j s_{i,j} * B^j // TODO: change notation because B is not a group element?
    #[serde(serialize_with = "ark_se", deserialize_with = "ark_de")]
    pub Cs: Vec<Vec<Vec<E::G1>>>, // TODO: maybe make this and the other fields affine? The verifier will have to do it anyway... and we are trying to speed that up
    /// Second chunked ElGamal component: R[j] = r_j * H
    #[serde(serialize_with = "ark_se", deserialize_with = "ark_de")]
    pub Rs: Vec<Vec<E::G1>>,
}
```

**File:** dkg/src/transcript_aggregation/mod.rs (L88-90)
```rust
        let transcript = bcs::from_bytes(transcript_bytes.as_slice()).map_err(|e| {
            anyhow!("[DKG] adding peer transcript failed with trx deserialization error: {e}")
        })?;
```

**File:** aptos-move/aptos-vm/src/validator_txns/dkg.rs (L106-109)
```rust
        let transcript = bcs::from_bytes::<<DefaultDKG as DKGTrait>::Transcript>(
            dkg_node.transcript_bytes.as_slice(),
        )
        .map_err(|_| Expected(TranscriptDeserializationFailed))?;
```

**File:** types/src/dkg/mod.rs (L84-85)
```rust
        let transcripts: Transcripts = bcs::from_bytes(&self.transcript_bytes)
            .context("Transcripts deserialization failed")?;
```
