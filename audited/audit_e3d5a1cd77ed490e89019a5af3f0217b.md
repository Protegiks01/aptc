# Audit Report

## Title
Lack of Flow Control Between Network Sender and Receiver Channels Enables Message Drop-Based Validator Degradation

## Summary
The `ApplicationNetworkHandle` struct contains independent `network_sender` and `network_events` channels that operate without synchronization or flow control. When inbound message queues fill up due to processing rate mismatches, critical consensus messages are silently dropped using FIFO eviction policy, potentially causing validator unavailability. [1](#0-0) 

## Finding Description
The network architecture uses separate, unsynchronized channels for sending and receiving messages. The `network_sender` uses `pm_reqs_tx` for outbound messages, while `network_events` receives from an independently configured inbound queue. [2](#0-1) 

These channels use `aptos_channel`, which implements a non-blocking design that **drops messages** when queues are full rather than applying backpressure. [3](#0-2) 

For consensus, the network uses FIFO queue style with a default capacity of 1024 messages. When this queue fills, **new incoming messages are dropped** (FIFO drops newest messages). [4](#0-3) [5](#0-4) 

**Attack Scenario:**
1. Attacker floods validator with quorum store messages (which have larger queue capacity of 50)
2. Deserialization tasks become saturated (NetworkEvents uses `spawn_blocking` for parallel deserialization) [6](#0-5) 
3. Network inbound queue (capacity 1024) fills up
4. Critical consensus messages (votes, proposals) arrive but are dropped
5. NetworkTask's internal consensus queue (capacity 10) also drops messages on push failures [7](#0-6) 
6. Node misses votes/proposals, fails to participate in consensus rounds
7. Validator becomes unavailable or falls behind

**Key Code Paths:**
- Inbound: Network → Peer → `upstream_handlers.push()` → NetworkEvents → NetworkTask → internal queues
- Message drops occur silently at multiple layers with only warning logs
- No backpressure signal to rate-limit senders [8](#0-7) 

## Impact Explanation
This qualifies as **High Severity** per Aptos bug bounty criteria:
- **Validator node slowdowns**: Targeted message flooding can degrade specific validators
- **Significant protocol violations**: Silent message drops break the assumption that messages are reliably delivered or explicitly fail

While consensus has timeout and recovery mechanisms, these assume **complete message loss** (connectivity issues), not **selective message drops** under load. A validator under sustained attack would repeatedly fall behind, request sync, and experience degraded availability.

## Likelihood Explanation
**High Likelihood:**
- Any network peer can send messages to a validator
- No per-peer rate limiting at the channel level
- Queue sizes are relatively small (10-1024 messages)
- Deserialization can be CPU-intensive for large batches
- Attack requires minimal resources (message flooding)
- Multiple drop points amplify vulnerability

The consensus configuration shows awareness of this issue with comments like "TODO: tune this value based on quorum store messages with backpressure" [9](#0-8) , but no actual backpressure mechanism exists.

## Recommendation
Implement flow control between `network_sender` and `network_events`:

1. **Add backpressure signals**: When inbound queues reach threshold (e.g., 80% full), signal to consensus layer to slow proposal generation
2. **Implement per-peer rate limiting**: Limit messages per peer per protocol in `upstream_handlers` before the main queue
3. **Priority queuing**: Replace FIFO with priority-based queueing where consensus messages take precedence over batch messages
4. **Increase critical path queue sizes**: Consensus messages queue of 10 is too small; increase to match network queue size
5. **Application-level flow control**: Have NetworkTask provide backpressure to NetworkEvents when internal queues are full

Example fix for priority-based dropping:
```rust
// In message_queues.rs, add priority-aware eviction
pub enum MessagePriority { Critical, Normal, Low }

// When queue is full, drop lowest priority messages first
// rather than FIFO/LIFO which is priority-blind
```

## Proof of Concept
```rust
#[tokio::test]
async fn test_message_drop_under_load() {
    // Setup: Create network with small queue size
    let config = NetworkApplicationConfig::new(
        NetworkClientConfig::new(vec![ProtocolId::ConsensusDirectSend], vec![]),
        NetworkServiceConfig::new(
            vec![ProtocolId::ConsensusDirectSend], 
            vec![],
            aptos_channel::Config::new(10) // Small queue
                .queue_style(QueueStyle::FIFO)
        )
    );
    
    // Flood with 100 messages rapidly
    for i in 0..100 {
        sender.send_to(peer, ProtocolId::ConsensusDirectSend, 
                      ConsensusMsg::VoteMsg(create_vote(i))).unwrap();
    }
    
    // Verify: Only ~10 messages received, rest dropped
    let mut received = 0;
    while let Ok(Some(_)) = timeout(Duration::from_millis(100), 
                                    network_events.next()).await {
        received += 1;
    }
    
    assert!(received < 20); // Most messages dropped
    // Check metrics show drops: counters::DROPPED should be ~90
}
```

## Notes
While consensus has recovery mechanisms (timeouts, sync), this issue represents a **design flaw** where the network layer provides no guarantees about message delivery under load. The lack of synchronization between sender and receiver enables targeted degradation of validator availability through selective message dropping.

The default configuration values (consensus queue: 1024, internal queues: 10-50) are insufficient for burst traffic scenarios, and the FIFO dropping policy is suboptimal for consensus-critical messages that should take priority over batch/quorum store messages.

### Citations

**File:** aptos-node/src/network.rs (L49-53)
```rust
struct ApplicationNetworkHandle<T> {
    pub network_id: NetworkId,
    pub network_sender: NetworkSender<T>,
    pub network_events: NetworkEvents<T>,
}
```

**File:** aptos-node/src/network.rs (L64-70)
```rust
    let network_service_config = NetworkServiceConfig::new(
        direct_send_protocols,
        rpc_protocols,
        aptos_channel::Config::new(node_config.consensus.max_network_channel_size)
            .queue_style(QueueStyle::FIFO)
            .counters(&aptos_consensus::counters::PENDING_CONSENSUS_NETWORK_EVENTS),
    );
```

**File:** network/builder/src/builder.rs (L431-445)
```rust
    pub fn add_client_and_service<SenderT: NewNetworkSender, EventsT: NewNetworkEvents>(
        &mut self,
        config: &NetworkApplicationConfig,
        max_parallel_deserialization_tasks: Option<usize>,
        allow_out_of_order_delivery: bool,
    ) -> (SenderT, EventsT) {
        (
            self.add_client(&config.network_client_config),
            self.add_service(
                &config.network_service_config,
                max_parallel_deserialization_tasks,
                allow_out_of_order_delivery,
            ),
        )
    }
```

**File:** crates/channel/src/aptos_channel.rs (L85-112)
```rust
    pub fn push(&self, key: K, message: M) -> Result<()> {
        self.push_with_feedback(key, message, None)
    }

    /// Same as `push`, but this function also accepts a oneshot::Sender over which the sender can
    /// be notified when the message eventually gets delivered or dropped.
    pub fn push_with_feedback(
        &self,
        key: K,
        message: M,
        status_ch: Option<oneshot::Sender<ElementStatus<M>>>,
    ) -> Result<()> {
        let mut shared_state = self.shared_state.lock();
        ensure!(!shared_state.receiver_dropped, "Channel is closed");
        debug_assert!(shared_state.num_senders > 0);

        let dropped = shared_state.internal_queue.push(key, (message, status_ch));
        // If this or an existing message had to be dropped because of the queue being full, we
        // notify the corresponding status channel if it was registered.
        if let Some((dropped_val, Some(dropped_status_ch))) = dropped {
            // Ignore errors.
            let _err = dropped_status_ch.send(ElementStatus::Dropped(dropped_val));
        }
        if let Some(w) = shared_state.waker.take() {
            w.wake();
        }
        Ok(())
    }
```

**File:** crates/channel/src/message_queues.rs (L134-147)
```rust
        if key_message_queue.len() >= self.max_queue_size.get() {
            if let Some(c) = self.counters.as_ref() {
                c.with_label_values(&["dropped"]).inc();
            }
            match self.queue_style {
                // Drop the newest message for FIFO
                QueueStyle::FIFO => Some(message),
                // Drop the oldest message for LIFO
                QueueStyle::LIFO | QueueStyle::KLAST => {
                    let oldest = key_message_queue.pop_front();
                    key_message_queue.push_back(message);
                    oldest
                },
            }
```

**File:** network/framework/src/protocols/network/mod.rs (L214-235)
```rust
        // Determine the number of parallel deserialization tasks to use
        let max_parallel_deserialization_tasks = max_parallel_deserialization_tasks.unwrap_or(1);

        let data_event_stream = peer_mgr_notifs_rx.map(|notification| {
            tokio::task::spawn_blocking(move || received_message_to_event(notification))
        });

        let data_event_stream: Pin<
            Box<dyn Stream<Item = Event<TMessage>> + Send + Sync + 'static>,
        > = if allow_out_of_order_delivery {
            Box::pin(
                data_event_stream
                    .buffer_unordered(max_parallel_deserialization_tasks)
                    .filter_map(|res| future::ready(res.expect("JoinError from spawn blocking"))),
            )
        } else {
            Box::pin(
                data_event_stream
                    .buffered(max_parallel_deserialization_tasks)
                    .filter_map(|res| future::ready(res.expect("JoinError from spawn blocking"))),
            )
        };
```

**File:** consensus/src/network.rs (L762-766)
```rust
        let (quorum_store_messages_tx, quorum_store_messages) = aptos_channel::new(
            QueueStyle::FIFO,
            // TODO: tune this value based on quorum store messages with backpressure
            50,
            Some(&counters::QUORUM_STORE_CHANNEL_MSGS),
```

**File:** consensus/src/network.rs (L807-812)
```rust
        if let Err(e) = tx.push((peer_id, discriminant(&msg)), (peer_id, msg)) {
            warn!(
                remote_peer = peer_id,
                error = ?e, "Error pushing consensus msg",
            );
        }
```

**File:** network/framework/src/peer/mod.rs (L459-491)
```rust
                match self.upstream_handlers.get(&direct.protocol_id) {
                    None => {
                        counters::direct_send_messages(&self.network_context, UNKNOWN_LABEL).inc();
                        counters::direct_send_bytes(&self.network_context, UNKNOWN_LABEL)
                            .inc_by(data_len as u64);
                    },
                    Some(handler) => {
                        let key = (self.connection_metadata.remote_peer_id, direct.protocol_id);
                        let sender = self.connection_metadata.remote_peer_id;
                        let network_id = self.network_context.network_id();
                        let sender = PeerNetworkId::new(network_id, sender);
                        match handler.push(key, ReceivedMessage::new(message, sender)) {
                            Err(_err) => {
                                // NOTE: aptos_channel never returns other than Ok(()), but we might switch to tokio::sync::mpsc and then this would work
                                counters::direct_send_messages(
                                    &self.network_context,
                                    DECLINED_LABEL,
                                )
                                .inc();
                                counters::direct_send_bytes(&self.network_context, DECLINED_LABEL)
                                    .inc_by(data_len as u64);
                            },
                            Ok(_) => {
                                counters::direct_send_messages(
                                    &self.network_context,
                                    RECEIVED_LABEL,
                                )
                                .inc();
                                counters::direct_send_bytes(&self.network_context, RECEIVED_LABEL)
                                    .inc_by(data_len as u64);
                            },
                        }
                    },
```
