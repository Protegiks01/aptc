# Audit Report

## Title
Missing Input Validation on `peer_monitor_interval_usec` Allows CPU Exhaustion via Misconfiguration

## Summary
The `peer_monitor_interval_usec` configuration parameter in `PeerMonitoringServiceConfig` lacks minimum value validation, allowing node operators to set extremely small values (e.g., 1 microsecond) that cause the peer monitoring loop to execute millions of times per second, leading to 100% CPU consumption and severe validator performance degradation.

## Finding Description

The peer monitoring service client uses the `peer_monitor_interval_usec` configuration value directly to create a loop interval ticker without any validation. [1](#0-0) 

The configuration struct defines this field as a raw `u64` with a default of 1,000,000 microseconds (1 second), but provides no validation or bounds checking: [2](#0-1) 

Critically, `PeerMonitoringServiceConfig` is **not** included in the config sanitizer framework that validates other node configurations: [3](#0-2) 

When the monitoring loop executes, it performs substantial work on each iteration for all connected peers: [4](#0-3) 

For each of 3 peer state keys (LatencyInfo, NetworkInfo, NodeInfo), and for each connected peer, the loop:
- Acquires read locks on peer state data structures
- Checks request tracker state (additional locks)
- Evaluates whether new requests are needed
- Updates in-flight request metrics
- Periodically updates additional metrics and logs

**Attack Scenario:**
1. Node operator sets `peer_monitor_interval_usec: 1` in their `node.yaml` config (either by typo, misunderstanding units, or if credentials are compromised)
2. Node starts and spawns the peer monitoring service: [5](#0-4) 
3. The monitoring loop attempts to execute 1,000,000 times per second
4. With even 10 connected peers, this results in:
   - 30,000,000 lock acquisitions per second
   - Millions of request tracker checks per second
   - Continuous metric updates
   - 100% CPU consumption on the monitoring thread
5. Validator performance degrades, potentially missing consensus rounds, failing to produce blocks, or being unable to vote in time

This breaks **Invariant #9** ("All operations must respect gas, storage, and computational limits") as the monitoring loop consumes unlimited CPU resources without bounds checking.

## Impact Explanation

This qualifies as **High Severity** per the Aptos bug bounty program criteria:
- **"Validator node slowdowns"** is explicitly listed as High severity (up to $50,000)
- CPU exhaustion at 100% usage directly causes validator performance degradation
- Validators may fail to participate effectively in consensus, impacting:
  - Block production capabilities
  - Vote participation in AptosBFT rounds
  - Overall network liveness if multiple validators are affected
  - Validator rewards due to poor performance metrics

While actual network requests are rate-limited by the `RequestTracker`, the monitoring loop itself still executes continuously, performing:
- Lock contention across peer state data structures
- Repeated function calls and time checks
- Metric update operations

Even without sending network requests, the tight loop causes severe CPU starvation for other critical validator operations.

## Likelihood Explanation

**Medium Likelihood:**
- **Operator Misconfiguration:** Realistic scenario where an operator:
  - Makes a typo (enters `1` instead of `1000000`)
  - Misunderstands units (thinks it's milliseconds, sets to `1000` meaning 1 second, but actually gets 1ms = 1000 iterations/sec)
  - Copies a test configuration into production (tests use values as low as 100 microseconds): [6](#0-5) 
  
- **No Validation Feedback:** Unlike other critical configs, there's no sanitizer to catch obviously invalid values at startup
- **Silent Failure:** The node doesn't crash or log errors; it just silently consumes CPU, making diagnosis difficult

## Recommendation

Implement a `ConfigSanitizer` for `PeerMonitoringServiceConfig` with minimum value validation:

```rust
// In config/src/config/peer_monitoring_config.rs
use crate::config::{config_sanitizer::ConfigSanitizer, Error, NodeConfig};
use crate::config::node_config_loader::NodeType;
use aptos_types::chain_id::ChainId;

// Define minimum acceptable interval (e.g., 10ms = 10,000 microseconds)
const MIN_PEER_MONITOR_INTERVAL_USEC: u64 = 10_000; 

impl ConfigSanitizer for PeerMonitoringServiceConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let config = &node_config.peer_monitoring_service;
        
        if config.peer_monitor_interval_usec < MIN_PEER_MONITOR_INTERVAL_USEC {
            return Err(Error::ConfigSanitizerFailed(
                Self::get_sanitizer_name(),
                format!(
                    "peer_monitor_interval_usec ({}) is below minimum acceptable value ({}). \
                    Setting this too low causes excessive CPU consumption.",
                    config.peer_monitor_interval_usec,
                    MIN_PEER_MONITOR_INTERVAL_USEC
                ),
            ));
        }
        
        Ok(())
    }
}
```

Then add to the sanitization chain in `config_sanitizer.rs`:

```rust
impl ConfigSanitizer for NodeConfig {
    fn sanitize(
        node_config: &NodeConfig,
        node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        // ... existing sanitizers ...
        PeerMonitoringServiceConfig::sanitize(node_config, node_type, chain_id)?;
        // ... rest of sanitizers ...
        Ok(())
    }
}
```

## Proof of Concept

**Setup:**
1. Create a malicious node configuration file `malicious_node.yaml`:
```yaml
base:
  role: "validator"
  
peer_monitoring_service:
  enable_peer_monitoring_client: true
  peer_monitor_interval_usec: 1  # Malicious value: 1 microsecond
  
# ... rest of standard validator config ...
```

**Execution Steps:**
1. Start an Aptos validator node with the malicious config:
```bash
aptos-node --config malicious_node.yaml
```

2. Monitor CPU usage:
```bash
top -p $(pgrep aptos-node) -H
# Observe the "peer-mon" thread consuming 100% CPU
```

3. Check monitoring loop execution frequency:
```bash
# Add logging to start_peer_monitor_with_state() before line 116
# Count log entries over 1 second - should see ~1,000,000 iterations
```

**Expected Result:**
- The peer monitoring thread consumes 100% of one CPU core
- The monitoring loop executes approximately 1,000,000 times per second
- Validator performance metrics degrade (slower block proposals, missed votes)
- No error or warning is logged about the invalid configuration

**Validation:**
The vulnerability is confirmed by the absence of any bounds checking between the config definition and its usage in the monitoring loop ticker creation.

## Notes

- This issue affects both validators and fullnodes running the peer monitoring service client
- The vulnerability is in the **missing validation layer**, not the monitoring loop itself
- Similar interval configurations in other services (latency monitoring, network monitoring) use millisecond granularity with their own interval controls, but they also lack top-level validation
- The fix should be applied before the node starts critical services to fail fast on invalid configurations

### Citations

**File:** peer-monitoring-service/client/src/lib.rs (L104-107)
```rust
    let monitoring_service_config = node_config.peer_monitoring_service;
    let peer_monitor_duration =
        Duration::from_micros(monitoring_service_config.peer_monitor_interval_usec);
    let peer_monitor_ticker = time_service.interval(peer_monitor_duration);
```

**File:** config/src/config/peer_monitoring_config.rs (L18-34)
```rust
    pub peer_monitor_interval_usec: u64, // The interval (usec) between peer monitor executions
}

impl Default for PeerMonitoringServiceConfig {
    fn default() -> Self {
        Self {
            enable_peer_monitoring_client: true,
            latency_monitoring: LatencyMonitoringConfig::default(),
            max_concurrent_requests: 1000,
            max_network_channel_size: 1000,
            max_num_response_bytes: 100 * 1024, // 100 KB
            max_request_jitter_ms: 1000,        // Monitoring requests are very infrequent
            metadata_update_interval_ms: 5000,  // 5 seconds
            network_monitoring: NetworkMonitoringConfig::default(),
            node_monitoring: NodeMonitoringConfig::default(),
            peer_monitor_interval_usec: 1_000_000, // 1 second
        }
```

**File:** config/src/config/config_sanitizer.rs (L39-70)
```rust
impl ConfigSanitizer for NodeConfig {
    fn sanitize(
        node_config: &NodeConfig,
        node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        // If config sanitization is disabled, don't do anything!
        if node_config.node_startup.skip_config_sanitizer {
            return Ok(());
        }

        // Sanitize all of the sub-configs
        AdminServiceConfig::sanitize(node_config, node_type, chain_id)?;
        ApiConfig::sanitize(node_config, node_type, chain_id)?;
        BaseConfig::sanitize(node_config, node_type, chain_id)?;
        ConsensusConfig::sanitize(node_config, node_type, chain_id)?;
        DagConsensusConfig::sanitize(node_config, node_type, chain_id)?;
        ExecutionConfig::sanitize(node_config, node_type, chain_id)?;
        sanitize_failpoints_config(node_config, node_type, chain_id)?;
        sanitize_fullnode_network_configs(node_config, node_type, chain_id)?;
        IndexerGrpcConfig::sanitize(node_config, node_type, chain_id)?;
        InspectionServiceConfig::sanitize(node_config, node_type, chain_id)?;
        LoggerConfig::sanitize(node_config, node_type, chain_id)?;
        MempoolConfig::sanitize(node_config, node_type, chain_id)?;
        NetbenchConfig::sanitize(node_config, node_type, chain_id)?;
        StateSyncConfig::sanitize(node_config, node_type, chain_id)?;
        StorageConfig::sanitize(node_config, node_type, chain_id)?;
        InternalIndexerDBConfig::sanitize(node_config, node_type, chain_id)?;
        sanitize_validator_network_config(node_config, node_type, chain_id)?;

        Ok(()) // All configs passed validation
    }
```

**File:** peer-monitoring-service/client/src/peer_states/mod.rs (L29-88)
```rust
pub fn refresh_peer_states(
    monitoring_service_config: &PeerMonitoringServiceConfig,
    peer_monitor_state: PeerMonitorState,
    peer_monitoring_client: PeerMonitoringServiceClient<
        NetworkClient<PeerMonitoringServiceMessage>,
    >,
    connected_peers_and_metadata: HashMap<PeerNetworkId, PeerMetadata>,
    time_service: TimeService,
    runtime: Option<Handle>,
) -> Result<(), Error> {
    // Process all state entries (in order) and update the ones that
    // need to be refreshed for each peer.
    for peer_state_key in PeerStateKey::get_all_keys() {
        let mut num_in_flight_requests = 0;

        // Go through all connected peers and see if we should refresh the state
        for (peer_network_id, peer_metadata) in &connected_peers_and_metadata {
            // Get the peer state
            let peer_state = get_peer_state(&peer_monitor_state, peer_network_id)?;

            // If there's an-flight request, update the metrics counter
            let request_tracker = peer_state.get_request_tracker(&peer_state_key)?;
            if request_tracker.read().in_flight_request() {
                num_in_flight_requests += 1;
            }

            // Update the state if it needs to be refreshed
            let should_refresh_peer_state_key = request_tracker.read().new_request_required();
            if should_refresh_peer_state_key {
                peer_state.refresh_peer_state_key(
                    monitoring_service_config,
                    &peer_state_key,
                    peer_monitoring_client.clone(),
                    *peer_network_id,
                    peer_metadata.clone(),
                    peer_monitor_state.request_id_generator.clone(),
                    time_service.clone(),
                    runtime.clone(),
                )?;
            }
        }

        // Update the in-flight request metrics
        update_in_flight_metrics(peer_state_key, num_in_flight_requests);
    }

    // Periodically update the metrics
    sample!(
        SampleRate::Duration(Duration::from_secs(METRICS_FREQUENCY_SECS)),
        update_peer_state_metrics(&peer_monitor_state, &connected_peers_and_metadata)?;
    );

    // Periodically update the logs
    sample!(
        SampleRate::Duration(Duration::from_secs(LOGS_FREQUENCY_SECS)),
        update_peer_state_logs(&peer_monitor_state, &connected_peers_and_metadata)?;
    );

    Ok(())
}
```

**File:** aptos-node/src/services.rs (L252-262)
```rust
    if node_config
        .peer_monitoring_service
        .enable_peer_monitoring_client
    {
        peer_monitoring_service_runtime.spawn(
            aptos_peer_monitoring_service_client::start_peer_monitor(
                node_config.clone(),
                network_client,
                Some(peer_monitoring_service_runtime.handle().clone()),
            ),
        );
```

**File:** peer-monitoring-service/client/src/tests/multiple_peers.rs (L149-154)
```rust
    let node_config = NodeConfig {
        peer_monitoring_service: PeerMonitoringServiceConfig {
            peer_monitor_interval_usec: 100,
            ..Default::default()
        },
        ..Default::default()
```
