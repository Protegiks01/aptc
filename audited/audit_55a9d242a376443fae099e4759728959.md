# Audit Report

## Title
Unbounded Memory Growth in Remote Executor Network Channels Leading to OOM Crash

## Summary
The `create_outbound_channel` function in the secure networking layer creates unbounded channels for remote executor communication. When combined with slow or unresponsive remote peers, this causes unbounded memory growth that leads to Out-of-Memory (OOM) crashes of validator nodes during consensus-critical block execution, resulting in network liveness failures.

## Finding Description

The vulnerability exists in the interaction between three components:

1. **Unbounded Channel Creation**: The `create_outbound_channel` method creates channels without size limits. [1](#0-0) 

2. **Serial Message Processing**: The `OutboundHandler` processes all outbound messages serially (one at a time) across all channels using a single async task with blocking GRPC calls. [2](#0-1) 

3. **Blocking Network Calls**: Each message requires a synchronous GRPC call that can be arbitrarily slow. [3](#0-2) 

**Attack Scenario:**

When the remote sharded block executor is enabled (used in consensus block execution), the system creates multiple unbounded channels for:
- Cross-shard communication (8 rounds × N shards) [4](#0-3) 

- Execution commands to shards [5](#0-4) 

- State value requests [6](#0-5) 

During block execution in consensus, if a remote peer (executor shard) is slow or malicious:
1. The coordinator sends execution commands via unbounded channels [7](#0-6) 

2. The OutboundHandler processes messages one at a time, waiting for slow GRPC responses
3. Messages accumulate in memory without bound because senders never block [8](#0-7) 

4. Memory grows until OOM, crashing the validator node
5. The crashed node cannot participate in consensus, affecting network liveness

This is used in the consensus-critical execution path: [9](#0-8) 

**Invariant Violations:**
- **Resource Limits (Invariant #9)**: Memory usage is not bounded, violating the requirement that all operations respect computational limits
- **Network Availability**: Validator nodes can crash due to OOM, causing liveness failures

## Impact Explanation

**Critical Severity** - This meets multiple criteria from the Aptos Bug Bounty program:

1. **Total loss of liveness/network availability**: When validator nodes crash due to OOM, they cannot participate in consensus. If multiple validators are affected (which is likely in a coordinated attack), the network can lose liveness.

2. **Validator node crash**: OOM causes immediate termination of the validator process, requiring manual restart and potential state recovery.

3. **Consensus participation failure**: Crashed nodes cannot vote, propose blocks, or execute transactions, directly impacting consensus operation.

The vulnerability is exploitable during normal block execution when remote executor sharding is enabled, affecting the core consensus-to-execution pipeline. Unlike network-level DoS (which is out of scope), this is a memory exhaustion vulnerability in the application layer caused by architectural design flaws.

## Likelihood Explanation

**High Likelihood** for the following reasons:

1. **Low Attacker Requirements**: An attacker only needs to:
   - Set up a remote executor shard (or compromise one)
   - Accept connections but respond slowly or not at all
   - Wait for the victim's memory to exhaust

2. **No Special Privileges Needed**: Any configured remote peer can trigger this vulnerability. No validator consensus participation or insider access is required.

3. **Natural Occurrence**: This can happen without malicious intent:
   - Network congestion or partition
   - Legitimate shard experiencing high load
   - Configuration errors leading to slow peers

4. **Amplification Factor**: With `MAX_ALLOWED_PARTITIONING_ROUNDS = 8` and multiple shards, many channels exist simultaneously, accelerating memory growth. [10](#0-9) 

5. **Consensus-Critical Path**: This affects block execution, which is invoked on every consensus round for transaction processing.

## Recommendation

Implement bounded channels with backpressure to prevent unbounded memory growth:

```rust
pub fn create_outbound_channel(
    &mut self,
    remote_peer_addr: SocketAddr,
    message_type: String,
) -> Sender<Message> {
    // Use bounded channel with reasonable capacity
    const CHANNEL_CAPACITY: usize = 1000;
    let (outbound_sender, outbound_receiver) = bounded(CHANNEL_CAPACITY);

    self.outbound_handler
        .register_handler(message_type, remote_peer_addr, outbound_receiver);

    outbound_sender
}
```

Additional mitigations:

1. **Timeout Enforcement**: Add per-message timeouts to prevent indefinite blocking
2. **Circuit Breaker Pattern**: Detect slow peers and temporarily skip sending to them
3. **Monitoring**: Add metrics for channel depth to detect memory growth early
4. **Multiple Worker Tasks**: Process multiple channels concurrently instead of serially to reduce message accumulation

The same fix should be applied to `create_inbound_channel`: [11](#0-10) 

## Proof of Concept

```rust
// Test that demonstrates unbounded memory growth
// Add to secure/net/src/network_controller/mod.rs tests

#[test]
fn test_unbounded_channel_oom_vulnerability() {
    use std::sync::Arc;
    use std::sync::atomic::{AtomicBool, Ordering};
    use std::time::Duration;

    let server_port = utils::get_available_port();
    let server_addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), server_port);
    
    // Create a slow/unresponsive server that accepts connections but doesn't process
    let mut slow_server = NetworkController::new("slow".to_string(), server_addr, 100000);
    let _rx = slow_server.create_inbound_channel("test".to_string());
    slow_server.start();
    
    // Give server time to start
    thread::sleep(Duration::from_millis(100));
    
    // Create client that will send messages rapidly
    let client_port = utils::get_available_port();
    let client_addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), client_port);
    let mut client = NetworkController::new("client".to_string(), client_addr, 1000);
    let sender = client.create_outbound_channel(server_addr, "test".to_string());
    client.start();
    
    // Simulate rapid message sending (like during block execution)
    // In real scenario, this would be cross-shard messages or execution commands
    let should_stop = Arc::new(AtomicBool::new(false));
    let should_stop_clone = should_stop.clone();
    
    let send_thread = thread::spawn(move || {
        let mut count = 0;
        while !should_stop_clone.load(Ordering::Relaxed) {
            // Send large messages rapidly (simulating execution payloads)
            let large_data = vec![0u8; 1024 * 100]; // 100KB per message
            if sender.send(Message::new(large_data)).is_err() {
                break;
            }
            count += 1;
            if count % 100 == 0 {
                println!("Sent {} messages, channel has unbounded depth", count);
            }
        }
        count
    });
    
    // Let messages accumulate for a few seconds
    // In production, memory would grow until OOM
    thread::sleep(Duration::from_secs(5));
    should_stop.store(true, Ordering::Relaxed);
    
    let message_count = send_thread.join().unwrap();
    println!("Sent {} messages without blocking - memory would exhaust", message_count);
    
    // This test demonstrates that:
    // 1. Sender never blocks (can send indefinitely)
    // 2. Messages accumulate in channel buffer
    // 3. Memory grows proportional to send rate × message size
    // 4. With slow consumer, this leads to OOM
    
    client.shutdown();
    slow_server.shutdown();
    
    // Assert that we could send many messages without blocking
    assert!(message_count > 1000, "Should be able to send many messages without backpressure");
}
```

**To reproduce the actual OOM:**
1. Configure remote executor with slow/unresponsive shards
2. Enable remote sharded execution in validator configuration
3. Submit blocks for execution during consensus
4. Monitor memory growth as messages accumulate in unbounded channels
5. Observe OOM crash when system memory is exhausted

## Notes

This vulnerability is particularly dangerous because:

1. **Silent Accumulation**: Memory grows gradually without obvious symptoms until sudden crash
2. **Consensus Integration**: Affects the critical execution path used by consensus for block processing
3. **Cascading Failures**: One slow peer can impact multiple validators attempting to communicate with it
4. **No Recovery Mechanism**: Once OOM occurs, manual intervention is required to restart the node

The fix requires careful consideration of capacity values and timeout mechanisms to balance performance with resource protection.

### Citations

**File:** secure/net/src/network_controller/mod.rs (L115-126)
```rust
    pub fn create_outbound_channel(
        &mut self,
        remote_peer_addr: SocketAddr,
        message_type: String,
    ) -> Sender<Message> {
        let (outbound_sender, outbound_receiver) = unbounded();

        self.outbound_handler
            .register_handler(message_type, remote_peer_addr, outbound_receiver);

        outbound_sender
    }
```

**File:** secure/net/src/network_controller/mod.rs (L128-137)
```rust
    pub fn create_inbound_channel(&mut self, message_type: String) -> Receiver<Message> {
        let (inbound_sender, inbound_receiver) = unbounded();

        self.inbound_handler
            .lock()
            .unwrap()
            .register_handler(message_type, inbound_sender);

        inbound_receiver
    }
```

**File:** secure/net/src/network_controller/outbound_handler.rs (L103-161)
```rust
    async fn process_one_outgoing_message(
        outbound_handlers: Vec<(Receiver<Message>, SocketAddr, MessageType)>,
        socket_addr: &SocketAddr,
        inbound_handler: Arc<Mutex<InboundHandler>>,
        grpc_clients: &mut HashMap<SocketAddr, GRPCNetworkMessageServiceClientWrapper>,
    ) {
        loop {
            let mut select = Select::new();
            for (receiver, _, _) in outbound_handlers.iter() {
                select.recv(receiver);
            }

            let index;
            let msg;
            let _timer;
            {
                let oper = select.select();
                _timer = NETWORK_HANDLER_TIMER
                    .with_label_values(&[&socket_addr.to_string(), "outbound_msgs"])
                    .start_timer();
                index = oper.index();
                match oper.recv(&outbound_handlers[index].0) {
                    Ok(m) => {
                        msg = m;
                    },
                    Err(e) => {
                        warn!(
                            "{:?} for outbound handler on {:?}. This can happen in shutdown,\
                             but should not happen otherwise",
                            e.to_string(),
                            socket_addr
                        );
                        return;
                    },
                }
            }

            let remote_addr = &outbound_handlers[index].1;
            let message_type = &outbound_handlers[index].2;

            if message_type.get_type() == "stop_task" {
                return;
            }

            if remote_addr == socket_addr {
                // If the remote address is the same as the local address, then we are sending a message to ourselves
                // so we should just pass it to the inbound handler
                inbound_handler
                    .lock()
                    .unwrap()
                    .send_incoming_message_to_handler(message_type, msg);
            } else {
                grpc_clients
                    .get_mut(remote_addr)
                    .unwrap()
                    .send_message(*socket_addr, msg, message_type)
                    .await;
            }
        }
```

**File:** secure/net/src/grpc_network_service/mod.rs (L140-160)
```rust
    pub async fn send_message(
        &mut self,
        sender_addr: SocketAddr,
        message: Message,
        mt: &MessageType,
    ) {
        let request = tonic::Request::new(NetworkMessage {
            message: message.data,
            message_type: mt.get_type(),
        });
        // TODO: Retry with exponential backoff on failures
        match self.remote_channel.simple_msg_exchange(request).await {
            Ok(_) => {},
            Err(e) => {
                panic!(
                    "Error '{}' sending message to {} on node {:?}",
                    e, self.remote_addr, sender_addr
                );
            },
        }
    }
```

**File:** execution/executor-service/src/remote_cross_shard_client.rs (L22-34)
```rust
    pub fn new(controller: &mut NetworkController, shard_addresses: Vec<SocketAddr>) -> Self {
        let mut message_txs = vec![];
        let mut message_rxs = vec![];
        // Create outbound channels for each shard per round.
        for remote_address in shard_addresses.iter() {
            let mut txs = vec![];
            for round in 0..MAX_ALLOWED_PARTITIONING_ROUNDS {
                let message_type = format!("cross_shard_{}", round);
                let tx = controller.create_outbound_channel(*remote_address, message_type);
                txs.push(Mutex::new(tx));
            }
            message_txs.push(txs);
        }
```

**File:** execution/executor-service/src/remote_cross_shard_client.rs (L55-59)
```rust
    fn send_cross_shard_msg(&self, shard_id: ShardId, round: RoundId, msg: CrossShardMsg) {
        let input_message = bcs::to_bytes(&msg).unwrap();
        let tx = self.message_txs[shard_id][round].lock().unwrap();
        tx.send(Message::new(input_message)).unwrap();
    }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L107-119)
```rust
        let (command_txs, result_rxs) = remote_shard_addresses
            .iter()
            .enumerate()
            .map(|(shard_id, address)| {
                let execute_command_type = format!("execute_command_{}", shard_id);
                let execute_result_type = format!("execute_result_{}", shard_id);
                let command_tx = Mutex::new(
                    controller_mut_ref.create_outbound_channel(*address, execute_command_type),
                );
                let result_rx = controller_mut_ref.create_inbound_channel(execute_result_type);
                (command_tx, result_rx)
            })
            .unzip();
```

**File:** execution/executor-service/src/remote_executor_client.rs (L193-206)
```rust
        for (shard_id, sub_blocks) in sub_blocks.into_iter().enumerate() {
            let senders = self.command_txs.clone();
            let execution_request = RemoteExecutionRequest::ExecuteBlock(ExecuteBlockCommand {
                sub_blocks,
                concurrency_level: concurrency_level_per_shard,
                onchain_config: onchain_config.clone(),
            });

            senders[shard_id]
                .lock()
                .unwrap()
                .send(Message::new(bcs::to_bytes(&execution_request).unwrap()))
                .unwrap();
        }
```

**File:** execution/executor-service/src/remote_state_view.rs (L91-95)
```rust
        let kv_request_type = "remote_kv_request";
        let kv_response_type = "remote_kv_response";
        let result_rx = controller.create_inbound_channel(kv_response_type.to_string());
        let command_tx =
            controller.create_outbound_channel(coordinator_address, kv_request_type.to_string());
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L261-267)
```rust
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
```

**File:** types/src/block_executor/partitioner.rs (L20-21)
```rust
pub static MAX_ALLOWED_PARTITIONING_ROUNDS: usize = 8;
pub static GLOBAL_ROUND_ID: usize = MAX_ALLOWED_PARTITIONING_ROUNDS + 1;
```
