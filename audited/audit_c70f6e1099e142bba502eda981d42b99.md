# Audit Report

## Title
Silent Error Propagation in Randomness Generation Storage Cleanup Leads to Unbounded Database Growth

## Summary
The `remove_certified_aug_data()` function's deletion errors are caught and only logged in `AugDataStore::new()`, allowing epoch transition cleanup to silently fail. This leads to unbounded accumulation of stale randomness data across epochs, potentially causing storage exhaustion and node unavailability.

## Finding Description

The randomness generation subsystem stores certified augmented data (cryptographic key shares) per epoch in persistent storage. During epoch transitions, `AugDataStore::new()` is responsible for cleaning up data from previous epochs. [1](#0-0) 

The deletion operation calls `db.remove_certified_aug_data()` which returns `anyhow::Result<()>`. However, errors are caught and only logged, not propagated. The constructor continues execution and returns successfully regardless of whether cleanup succeeded.

The underlying implementation uses RocksDB batch writes: [2](#0-1) [3](#0-2) [4](#0-3) 

RocksDB write operations can fail due to:
- Disk full / storage exhaustion
- I/O errors (hardware failures, network filesystem issues)  
- Database corruption
- Permission issues

**Failure Scenario:**
1. Epoch N→N+1 transition: `AugDataStore::new()` loads all certified aug data from database
2. Filters out epoch N data for deletion (validator_count items)
3. `remove_certified_aug_data()` fails due to disk I/O error
4. Error is logged but constructor completes successfully
5. Epoch N data remains in database
6. Epoch N+1→N+2 transition: Now must load and filter epoch N + epoch N+1 data
7. Cleanup fails again - accumulation continues
8. After M failed epochs: database contains M × validator_count stale items

The in-memory state remains correct (only current epoch data), but the persistent storage grows unbounded.

## Impact Explanation

This constitutes a **Medium severity** issue under the "State inconsistencies requiring intervention" category:

1. **Storage Exhaustion**: Unbounded growth of stale cryptographic data eventually fills disk
2. **Performance Degradation**: Each epoch transition must load increasingly large datasets from disk
3. **Cascading Failures**: Once disk is near capacity, failure to cleanup accelerates exhaustion
4. **Operational Blindness**: Only error logging means operators lack visibility until manifestation as node failure
5. **Node Unavailability**: Disk exhaustion prevents consensus participation, affecting network liveness

While not directly exploitable by external attackers (requires pre-existing system stress), this violates the **Resource Limits** invariant - operations must respect storage constraints. Under degraded conditions (disk stress from other sources), this failure mode makes recovery harder and accelerates node failure.

## Likelihood Explanation

**Likelihood: Medium**

- Disk I/O failures occur in production environments (hardware degradation, network storage issues)
- Cloud storage can experience transient failures
- Disk capacity planning errors are common
- Once triggered, the issue compounds over multiple epochs
- Randomness generation runs on all validator nodes, affecting multiple validators simultaneously

The gradual nature means it won't manifest immediately but becomes increasingly likely as:
- Node uptime increases
- Storage approaches capacity limits  
- I/O subsystem degrades

## Recommendation

Propagate deletion errors to callers instead of silent logging. The epoch transition should fail-fast if cleanup cannot complete:

```rust
pub fn new(
    epoch: u64,
    signer: Arc<ValidatorSigner>,
    config: RandConfig,
    fast_config: Option<RandConfig>,
    db: Arc<dyn RandStorage<D>>,
) -> anyhow::Result<Self> {
    let all_data = db.get_all_aug_data()?;
    let (to_remove, aug_data) = Self::filter_by_epoch(epoch, all_data.into_iter());
    db.remove_aug_data(to_remove)?;  // Propagate error

    let all_certified_data = db.get_all_certified_aug_data()?;
    let (to_remove, certified_data) =
        Self::filter_by_epoch(epoch, all_certified_data.into_iter());
    db.remove_certified_aug_data(to_remove)?;  // Propagate error

    for (_, certified_data) in &certified_data {
        certified_data
            .data()
            .augment(&config, &fast_config, certified_data.author());
    }

    Ok(Self {
        epoch,
        signer,
        config,
        fast_config,
        data: aug_data
            .into_iter()
            .map(|(id, data)| (id.author(), data))
            .collect(),
        certified_data: certified_data
            .into_iter()
            .map(|(id, data)| (id.author(), data))
            .collect(),
        db,
    })
}
```

Callers should handle the error appropriately - either retry, alert operators, or halt epoch transition until storage issues are resolved. [5](#0-4) 

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use aptos_types::validator_signer::ValidatorSigner;
    
    // Mock RandStorage that fails on deletion
    struct FailingRandDb<D> {
        inner: InMemRandDb<D>,
        fail_deletes: bool,
    }
    
    impl<D: TAugmentedData> RandStorage<D> for FailingRandDb<D> {
        fn remove_certified_aug_data(
            &self,
            _certified_aug_data: Vec<CertifiedAugData<D>>,
        ) -> anyhow::Result<()> {
            if self.fail_deletes {
                anyhow::bail!("Simulated disk I/O error")
            }
            Ok(())
        }
        // ... other methods delegate to inner
    }
    
    #[test]
    fn test_silent_deletion_failure() {
        // Setup: Create certified aug data for epoch 1
        let db = Arc::new(FailingRandDb { 
            inner: InMemRandDb::new(),
            fail_deletes: true 
        });
        
        let cert_data = create_certified_aug_data(epoch=1, author);
        db.save_certified_aug_data(&cert_data).unwrap();
        
        // Epoch transition to epoch 2 with failing deletions
        let store = AugDataStore::new(
            epoch=2, signer, config, fast_config, db.clone()
        );
        // Should succeed despite deletion failure
        
        // Verify: Old epoch data still in database
        let all_data = db.get_all_certified_aug_data().unwrap();
        assert_eq!(all_data.len(), 1); // Stale data remains
        assert_eq!(all_data[0].0.epoch(), 1); // From epoch 1
        
        // In-memory store is correct (only epoch 2 data)
        assert!(store.certified_data.is_empty());
        
        // Repeat for multiple epochs - accumulation continues
        for epoch in 3..10 {
            AugDataStore::new(epoch, signer, config, fast_config, db.clone());
            let all = db.get_all_certified_aug_data().unwrap();
            // Database grows with each epoch
            assert_eq!(all.len(), epoch - 1);
        }
    }
}
```

## Notes

This vulnerability requires pre-existing system stress (storage issues) to trigger, and is not directly exploitable by external attackers. However, it represents a critical error handling defect in consensus-critical code that violates fail-fast principles and creates an unsafe degradation path. The Medium severity classification reflects the potential for node unavailability and the operational impact of accumulated storage debt.

### Citations

**File:** consensus/src/rand/rand_gen/aug_data_store.rs (L57-65)
```rust
        let all_certified_data = db.get_all_certified_aug_data().unwrap_or_default();
        let (to_remove, certified_data) =
            Self::filter_by_epoch(epoch, all_certified_data.into_iter());
        if let Err(e) = db.remove_certified_aug_data(to_remove) {
            error!(
                "[AugDataStore] failed to remove certified aug data: {:?}",
                e
            );
        }
```

**File:** consensus/src/rand/rand_gen/storage/db.rs (L67-71)
```rust
    fn delete<S: Schema>(&self, mut keys: impl Iterator<Item = S::Key>) -> Result<(), DbError> {
        let mut batch = SchemaBatch::new();
        keys.try_for_each(|key| batch.delete::<S>(&key))?;
        self.commit(batch)
    }
```

**File:** consensus/src/rand/rand_gen/storage/db.rs (L114-120)
```rust
    fn remove_certified_aug_data(
        &self,
        certified_aug_data: Vec<CertifiedAugData<D>>,
    ) -> Result<()> {
        Ok(self
            .delete::<CertifiedAugDataSchema<D>>(certified_aug_data.into_iter().map(|d| d.id()))?)
    }
```

**File:** storage/schemadb/src/lib.rs (L289-303)
```rust
    fn write_schemas_inner(&self, batch: impl IntoRawBatch, option: &WriteOptions) -> DbResult<()> {
        let labels = [self.name.as_str()];
        let _timer = APTOS_SCHEMADB_BATCH_COMMIT_LATENCY_SECONDS.timer_with(&labels);

        let raw_batch = batch.into_raw_batch(self)?;

        let serialized_size = raw_batch.inner.size_in_bytes();
        self.inner
            .write_opt(raw_batch.inner, option)
            .into_db_res()?;

        raw_batch.stats.commit();
        APTOS_SCHEMADB_BATCH_COMMIT_BYTES.observe_with(&[&self.name], serialized_size as f64);

        Ok(())
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L105-111)
```rust
        let aug_data_store = AugDataStore::new(
            epoch_state.epoch,
            signer,
            config.clone(),
            fast_config.clone(),
            db,
        );
```
