# Audit Report

## Title
Unhandled Initialization Failures in Ledger Pruner Cause Validator Panic During Startup

## Summary
The `LedgerPrunerManager::new()` constructor uses multiple `.expect()` calls that cause validator nodes to panic (crash) if pruner initialization fails. When `ledger_pruner_config.enable` is true and database operations fail during initialization, the node crashes immediately during startup. This creates a risk of coordinated network-wide availability failure if all validators encounter the same initialization conditions.

## Finding Description
The vulnerability exists in the initialization code path of the ledger pruner. When a validator node starts up with ledger pruning enabled, it must initialize the pruner subsystem. This initialization involves multiple database operations that can fail due to various reasons (corrupted metadata, missing keys, disk errors, etc.).

The critical panic points are: [1](#0-0) [2](#0-1) [3](#0-2) 

During validator startup, the initialization flow is: [4](#0-3) [5](#0-4) 

Each sub-pruner also performs actual pruning operations during initialization, which can fail: [6](#0-5) 

The database operations that can fail include:
- Reading pruner progress from metadata: [7](#0-6) 
- Initializing sub-pruner progress: [8](#0-7) 

**Attack Scenario:**
In a coordinated validator network deployment:
1. All validators use identical configuration with `ledger_pruner_config.enable = true`
2. All validators restore from the same state snapshot or share database state (common in coordinated deployments)
3. That shared database state contains corrupted metadata, missing pruner progress keys, or other database anomalies
4. When all validators start simultaneously (e.g., after a coordinated upgrade), they ALL attempt to initialize the ledger pruner
5. All validators encounter the same database initialization failure
6. All validators panic and crash via `.expect()` calls
7. **Network experiences total liveness failure** - no validators can start

## Impact Explanation
This qualifies as **Critical Severity** per the Aptos bug bounty criteria: "Total loss of liveness/network availability".

When all validators crash simultaneously during startup and cannot recover (because the issue persists on every restart attempt), the network cannot process transactions or reach consensus. This requires:
- Manual intervention to disable the pruner or fix the database state
- Potentially a coordinated configuration change across all validators
- In worst case, a network-wide hardfork to recover

The impact is network-wide availability failure, which is one of the most severe issues in a blockchain system.

## Likelihood Explanation
The likelihood is **Medium to Low** because it requires specific conditions:

**Conditions Required:**
1. All validators must have `enable: true` in their pruner config (common in production)
2. Database state must cause initialization failures (uncommon but possible from corrupted backups, bad state sync, disk issues)
3. The issue must affect all validators simultaneously (requires shared database state or common corruption)

**Why it's not Low:**
- Validators often use identical configurations managed by orchestration systems
- State snapshots and backups are commonly shared across validator operators
- Coordinated deployments mean validators start simultaneously
- Database corruption can occur from bugs in state sync, backup/restore processes, or storage layer issues

**Why it's not High:**
- Requires database-level issues, not just configuration mistakes
- Well-maintained validator operations typically have database integrity checks
- The issue would be detected in testnet before production deployment

## Recommendation
The initialization code must handle errors gracefully instead of panicking:

```rust
pub fn new(
    ledger_db: Arc<LedgerDb>,
    ledger_pruner_config: LedgerPrunerConfig,
    internal_indexer_db: Option<InternalIndexerDB>,
) -> Result<Self> {  // Changed from -> Self
    let pruner_worker = if ledger_pruner_config.enable {
        match Self::init_pruner(
            Arc::clone(&ledger_db),
            ledger_pruner_config,
            internal_indexer_db,
        ) {
            Ok(worker) => Some(worker),
            Err(err) => {
                warn!("Failed to initialize ledger pruner: {}, continuing without pruning", err);
                None  // Gracefully degrade to no pruning
            }
        }
    } else {
        None
    };

    let min_readable_version = pruner_utils::get_ledger_pruner_progress(&ledger_db)
        .unwrap_or_else(|err| {
            warn!("Failed to get ledger pruner progress: {}, using default 0", err);
            0
        });

    // ... rest of initialization
    Ok(Self { /* ... */ })
}
```

Similarly, `init_pruner()` should return `Result<PrunerWorker>`:

```rust
fn init_pruner(
    ledger_db: Arc<LedgerDb>,
    ledger_pruner_config: LedgerPrunerConfig,
    internal_indexer_db: Option<InternalIndexerDB>,
) -> Result<PrunerWorker> {  // Changed return type
    let pruner = Arc::new(LedgerPruner::new(ledger_db, internal_indexer_db)?);  // Use ?
    
    PRUNER_WINDOW
        .with_label_values(&["ledger_pruner"])
        .set(ledger_pruner_config.prune_window as i64);

    PRUNER_BATCH_SIZE
        .with_label_values(&["ledger_pruner"])
        .set(ledger_pruner_config.batch_size as i64);

    Ok(PrunerWorker::new(pruner, ledger_pruner_config.batch_size, "ledger"))
}
```

And in `LedgerPruner::new()`, remove the `.expect()`:

```rust
let ledger_metadata_pruner = Box::new(LedgerMetadataPruner::new(
    ledger_db.metadata_db_arc()
)?);  // Use ? instead of .expect()
```

## Proof of Concept

```rust
// Rust test demonstrating the panic behavior
#[test]
#[should_panic(expected = "Failed to create ledger pruner")]
fn test_ledger_pruner_initialization_panic() {
    // Setup: Create a database with corrupted metadata
    let temp_dir = TempPath::new();
    let db_path = temp_dir.path().to_path_buf();
    
    // Create AptosDB
    let mut db = AptosDB::open(
        StorageDirPaths::from_path(&db_path),
        false,
        NO_OP_STORAGE_PRUNER_CONFIG,
        RocksdbConfigs::default(),
        false,
        1000,
        1000,
        None,
        HotStateConfig::default(),
    ).unwrap();
    
    // Corrupt the metadata by writing invalid data
    db.ledger_db.metadata_db().put::<DbMetadataSchema>(
        &DbMetadataKey::LedgerPrunerProgress,
        &DbMetadataValue::Version(u64::MAX), // Invalid version
    ).unwrap();
    
    drop(db);
    
    // Try to reopen with pruner enabled
    let pruner_config = LedgerPrunerConfig {
        enable: true,
        prune_window: 1000,
        batch_size: 100,
        user_pruning_window_offset: 0,
    };
    
    // This will panic with "Failed to create ledger pruner"
    let _db = AptosDB::open(
        StorageDirPaths::from_path(&db_path),
        false,
        PrunerConfig {
            ledger_pruner_config: pruner_config,
            ..NO_OP_STORAGE_PRUNER_CONFIG
        },
        RocksdbConfigs::default(),
        false,
        1000,
        1000,
        None,
        HotStateConfig::default(),
    ).unwrap(); // The panic happens here during initialization
}
```

## Notes
While the immediate trigger is database state issues rather than "malformed config values" themselves, the configuration setting `enable: true` is what activates the vulnerable code path. The lack of input validation on config values (e.g., `batch_size: 0` or `prune_window: u64::MAX`) could also potentially cause issues in edge cases, though the primary vulnerability is the unhandled error panics during initialization.

The recommended fix enables graceful degradation: if pruner initialization fails, the node can continue operating without pruning rather than crashing. This maintains network availability while logging the issue for operator investigation.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L123-124)
```rust
        let min_readable_version =
            pruner_utils::get_ledger_pruner_progress(&ledger_db).expect("Must succeed.");
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L146-149)
```rust
        let pruner = Arc::new(
            LedgerPruner::new(ledger_db, internal_indexer_db)
                .expect("Failed to create ledger pruner."),
        );
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L124-127)
```rust
        let ledger_metadata_pruner = Box::new(
            LedgerMetadataPruner::new(ledger_db.metadata_db_arc())
                .expect("Failed to initialize ledger_metadata_pruner."),
        );
```

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L48-59)
```rust
        let mut db_main = AptosDB::open(
            config.storage.get_dir_paths(),
            /*readonly=*/ false,
            config.storage.storage_pruner_config,
            config.storage.rocksdb_configs,
            config.storage.enable_indexer,
            config.storage.buffered_state_target_items,
            config.storage.max_num_nodes_per_lru_cache_shard,
            internal_indexer_db,
            config.storage.hot_state_config,
        )
        .map_err(|err| anyhow!("fast sync DB failed to open {}", err))?;
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L86-90)
```rust
        let ledger_pruner = LedgerPrunerManager::new(
            Arc::clone(&ledger_db),
            pruner_config.ledger_pruner_config,
            internal_indexer_db,
        );
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L90-106)
```rust
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.event_db_raw(),
            &DbMetadataKey::EventPrunerProgress,
            metadata_progress,
        )?;

        let myself = EventStorePruner {
            ledger_db,
            internal_indexer_db,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up EventStorePruner."
        );
        myself.prune(progress, metadata_progress)?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_metadata_pruner.rs (L20-37)
```rust
        if let Some(v) =
            ledger_metadata_db.get::<DbMetadataSchema>(&DbMetadataKey::LedgerPrunerProgress)?
        {
            v.expect_version();
        } else {
            // NOTE: I **think** all db should have the LedgerPrunerProgress. Have a fallback path
            // here in case the database was super old before we introducing this progress counter.
            let mut iter = ledger_metadata_db.iter::<VersionDataSchema>()?;
            iter.seek_to_first();
            let version = match iter.next().transpose()? {
                Some((version, _)) => version,
                None => 0,
            };
            ledger_metadata_db.put::<DbMetadataSchema>(
                &DbMetadataKey::LedgerPrunerProgress,
                &DbMetadataValue::Version(version),
            )?;
        }
```

**File:** storage/aptosdb/src/pruner/pruner_utils.rs (L44-60)
```rust
pub(crate) fn get_or_initialize_subpruner_progress(
    sub_db: &DB,
    progress_key: &DbMetadataKey,
    metadata_progress: Version,
) -> Result<Version> {
    Ok(
        if let Some(v) = sub_db.get::<DbMetadataSchema>(progress_key)? {
            v.expect_version()
        } else {
            sub_db.put::<DbMetadataSchema>(
                progress_key,
                &DbMetadataValue::Version(metadata_progress),
            )?;
            metadata_progress
        },
    )
}
```
