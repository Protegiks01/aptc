# Audit Report

## Title
State Sync Accepts Repeated Consensus Sync Target Requests Without Validation, Causing Callback Orphaning and Resource Churn

## Summary
The `initialize_sync_target_request()` function in state-sync unconditionally accepts and overwrites active sync requests from consensus without checking if a sync is already in progress. This allows repeated sync target submissions to orphan previous callbacks and cause resource allocation churn, leading to a Medium severity DoS vulnerability.

## Finding Description

The vulnerability exists in the state synchronization driver's handling of consensus sync target notifications. When consensus submits a sync target request, the state sync driver stores it and responds with a callback once the target is reached. [1](#0-0) 

The critical flaw is that `initialize_sync_target_request()` performs **no validation** to check whether an active sync request is already being processed. At line 315, it unconditionally overwrites `self.consensus_sync_request` with a new Arc: [2](#0-1) 

Each `ConsensusSyncTargetNotification` contains a oneshot callback channel that consensus awaits for confirmation: [3](#0-2) 

When consensus sends a sync target, it blocks waiting for the response: [4](#0-3) 

**Attack Scenario:**

1. **Initial state**: Node is at version 1000
2. **T=0ms**: Consensus sends sync target for version 1010 (creates callback₁, stored in Arc₁)
3. **T=100ms**: Before reaching version 1010, consensus sends sync target for version 1020 (creates callback₂, stored in Arc₂)
   - Arc₁ is **overwritten** by Arc₂
   - The `ConsensusSyncTargetNotification` in Arc₁ (containing callback₁) is **dropped**
   - When callback₁'s oneshot sender is dropped, consensus receives `RecvError` on the receiver
4. **T=200ms**: Consensus sends sync target for version 1030 (creates callback₃, stored in Arc₃)
   - Arc₂ is overwritten, callback₂ is orphaned
   - Consensus receives another `RecvError`

This creates a cascading failure where:
- **Orphaned callbacks**: Previous sync requests never receive responses, causing consensus to treat them as failures
- **Resource churn**: Repeated Arc allocations, logging, and metric updates
- **State confusion**: The active sync target keeps changing, potentially causing the continuous syncer to adjust its synchronization goals

## Impact Explanation

This vulnerability qualifies as **Medium Severity** ($10,000 range) under the Aptos Bug Bounty Program criteria for the following reasons:

1. **State Inconsistency**: The state sync driver maintains inconsistent internal state where multiple sync requests exist but only the latest is tracked, violating state consistency guarantees.

2. **Consensus Communication Failure**: Consensus receives spurious sync failures (RecvError) when callbacks are orphaned, potentially causing it to retry or enter degraded states. This affects the reliable operation of the consensus-to-state-sync interface.

3. **Resource Exhaustion**: Rapid sync target submissions cause:
   - Memory allocation churn (creating/dropping Arc objects)
   - Log spam (each request logs at INFO level)
   - Metric counter inflation
   - Potential for memory growth if Arc references accumulate

4. **Limited DoS Impact**: While this doesn't cause total liveness failure, it degrades validator node performance through wasted CPU cycles, memory pressure, and confused consensus state. This aligns with "State inconsistencies requiring intervention" per the Medium severity criteria.

The vulnerability does **not** reach High/Critical severity because:
- It doesn't directly cause fund loss or consensus safety violations
- It doesn't create non-recoverable network partitions
- It requires consensus to misbehave (either through a bug or malicious behavior)

## Likelihood Explanation

**Likelihood: Medium**

This vulnerability is moderately likely to occur in the following scenarios:

1. **Consensus Bugs**: If consensus has a bug that causes it to send sync targets more frequently than intended (e.g., in response to network messages, during epoch transitions, or during catch-up scenarios), this vulnerability will be triggered automatically.

2. **Network Conditions**: During network partitions or high-latency scenarios, consensus might issue new sync targets before previous ones complete if it receives newer commit certificates from peers.

3. **Edge Cases**: During validator set changes, epoch boundaries, or fast-forward sync scenarios, consensus may legitimately need to update sync targets rapidly if the network state advances quickly.

The vulnerability is NOT highly likely because:
- Consensus typically uses locks to serialize sync operations [5](#0-4) 
- Normal operation shouldn't trigger rapid sync target changes
- It requires specific timing conditions

However, the lack of defensive programming makes this exploitable whenever consensus behavior deviates from the happy path, which can occur due to bugs, network issues, or adversarial conditions.

## Recommendation

Add validation to prevent accepting new sync requests while one is already active:

```rust
/// Initializes the sync target request received from consensus
pub async fn initialize_sync_target_request(
    &mut self,
    sync_target_notification: ConsensusSyncTargetNotification,
    latest_pre_committed_version: Version,
    latest_synced_ledger_info: LedgerInfoWithSignatures,
) -> Result<(), Error> {
    // Get the target sync version and latest committed version
    let sync_target_version = sync_target_notification
        .get_target()
        .ledger_info()
        .version();
    let latest_committed_version = latest_synced_ledger_info.ledger_info().version();

    // Check if there's already an active sync request
    if let Some(existing_request) = self.consensus_sync_request.lock().as_ref() {
        if let Some(existing_target) = existing_request.get_sync_target() {
            let existing_version = existing_target.ledger_info().version();
            
            // Only accept the new request if it's significantly different
            // or if the existing target has been reached
            if existing_version != latest_committed_version {
                let error = Err(Error::UnexpectedErrorEncountered(format!(
                    "Cannot accept new sync target {} while actively syncing to target {}",
                    sync_target_version, existing_version
                )));
                self.respond_to_sync_target_notification(
                    sync_target_notification, 
                    error.clone()
                )?;
                return error;
            }
        }
    }

    // Existing validation logic...
    if sync_target_version < latest_committed_version
        || sync_target_version < latest_pre_committed_version
    {
        let error = Err(Error::OldSyncRequest(
            sync_target_version,
            latest_pre_committed_version,
            latest_committed_version,
        ));
        self.respond_to_sync_target_notification(sync_target_notification, error.clone())?;
        return error;
    }

    // ... rest of function
}
```

Additionally, consider implementing:
1. **Rate limiting**: Limit how frequently sync target requests can be accepted (e.g., max 1 per second)
2. **Minimum version delta**: Only accept new targets if they're at least N versions ahead of the current target
3. **Proper cleanup**: Ensure callbacks from overwritten requests receive error responses before being dropped

## Proof of Concept

```rust
#[tokio::test]
async fn test_rapid_sync_target_churn() {
    // Create a validator driver at genesis
    let (mut validator_driver, _, consensus_notifier, _, _, _, _, time_service) =
        create_validator_driver(None).await;
    
    // Wait for auto-bootstrapping
    wait_for_auto_bootstrapping(validator_driver, time_service).await;
    
    // Rapidly submit multiple sync targets slightly ahead
    let base_version = 1000;
    let mut callbacks = vec![];
    
    for i in 1..=10 {
        let target_version = base_version + (i * 10);
        let ledger_info = create_ledger_info_at_version(target_version);
        
        // Send sync target asynchronously
        let notifier = consensus_notifier.clone();
        let callback = tokio::spawn(async move {
            notifier.sync_to_target(ledger_info).await
        });
        
        callbacks.push(callback);
        
        // Small delay between requests
        tokio::time::sleep(Duration::from_millis(50)).await;
    }
    
    // Check results - earlier callbacks should fail with RecvError
    let mut failed_count = 0;
    for (idx, callback) in callbacks.into_iter().enumerate() {
        match callback.await.unwrap() {
            Err(e) => {
                // Earlier callbacks should be orphaned
                if idx < 9 {
                    failed_count += 1;
                    println!("Callback {} failed as expected: {:?}", idx, e);
                }
            },
            Ok(_) => {
                // Only the last callback should succeed
                assert_eq!(idx, 9, "Only the last sync target should succeed");
            }
        }
    }
    
    // Verify that multiple callbacks were orphaned
    assert!(failed_count >= 8, "Expected at least 8 orphaned callbacks, got {}", failed_count);
}
```

This PoC demonstrates that rapidly submitting sync targets causes earlier requests to be orphaned, with their callbacks dropped without responses. The consensus side receives errors for these orphaned requests, demonstrating the vulnerability.

## Notes

This vulnerability represents a failure in defensive programming rather than a direct consensus safety violation. While consensus is a trusted component, state sync should handle edge cases and misbehavior gracefully rather than silently orphaning callbacks and causing confusion. The issue becomes more severe if there are upstream bugs in consensus that could trigger rapid sync target submissions, creating a cascading failure scenario.

### Citations

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L261-318)
```rust
    /// Initializes the sync target request received from consensus
    pub async fn initialize_sync_target_request(
        &mut self,
        sync_target_notification: ConsensusSyncTargetNotification,
        latest_pre_committed_version: Version,
        latest_synced_ledger_info: LedgerInfoWithSignatures,
    ) -> Result<(), Error> {
        // Get the target sync version and latest committed version
        let sync_target_version = sync_target_notification
            .get_target()
            .ledger_info()
            .version();
        let latest_committed_version = latest_synced_ledger_info.ledger_info().version();

        // If the target version is old, return an error to consensus (something is wrong!)
        if sync_target_version < latest_committed_version
            || sync_target_version < latest_pre_committed_version
        {
            let error = Err(Error::OldSyncRequest(
                sync_target_version,
                latest_pre_committed_version,
                latest_committed_version,
            ));
            self.respond_to_sync_target_notification(sync_target_notification, error.clone())?;
            return error;
        }

        // If the committed version is at the target, return successfully
        if sync_target_version == latest_committed_version {
            info!(
                LogSchema::new(LogEntry::NotificationHandler).message(&format!(
                    "We're already at the requested sync target version: {} \
                (pre-committed version: {}, committed version: {})!",
                    sync_target_version, latest_pre_committed_version, latest_committed_version
                ))
            );
            let result = Ok(());
            self.respond_to_sync_target_notification(sync_target_notification, result.clone())?;
            return result;
        }

        // If the pre-committed version is already at the target, something has else gone wrong
        if sync_target_version == latest_pre_committed_version {
            let error = Err(Error::InvalidSyncRequest(
                sync_target_version,
                latest_pre_committed_version,
            ));
            self.respond_to_sync_target_notification(sync_target_notification, error.clone())?;
            return error;
        }

        // Save the request so we can notify consensus once we've hit the target
        let consensus_sync_request =
            ConsensusSyncRequest::new_with_target(sync_target_notification);
        self.consensus_sync_request = Arc::new(Mutex::new(Some(consensus_sync_request)));

        Ok(())
    }
```

**File:** state-sync/inter-component/consensus-notifications/src/lib.rs (L181-207)
```rust
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), Error> {
        // Create a consensus sync target notification
        let (notification, callback_receiver) = ConsensusSyncTargetNotification::new(target);
        let sync_target_notification = ConsensusNotification::SyncToTarget(notification);

        // Send the notification to state sync
        if let Err(error) = self
            .notification_sender
            .clone()
            .send(sync_target_notification)
            .await
        {
            return Err(Error::NotificationError(format!(
                "Failed to notify state sync of sync target! Error: {:?}",
                error
            )));
        }

        // Process the response
        match callback_receiver.await {
            Ok(response) => response.get_result(),
            Err(error) => Err(Error::UnexpectedErrorEncountered(format!(
                "Sync to target failure: {:?}",
                error
            ))),
        }
    }
```

**File:** state-sync/inter-component/consensus-notifications/src/lib.rs (L383-396)
```rust
pub struct ConsensusSyncTargetNotification {
    target: LedgerInfoWithSignatures,
    callback: oneshot::Sender<ConsensusNotificationResponse>,
}

impl ConsensusSyncTargetNotification {
    pub fn new(
        target: LedgerInfoWithSignatures,
    ) -> (Self, oneshot::Receiver<ConsensusNotificationResponse>) {
        let (callback, callback_receiver) = oneshot::channel();
        let notification = ConsensusSyncTargetNotification { target, callback };

        (notification, callback_receiver)
    }
```

**File:** consensus/src/state_computer.rs (L178-194)
```rust
        // Grab the logical time lock and calculate the target logical time
        let mut latest_logical_time = self.write_mutex.lock().await;
        let target_logical_time =
            LogicalTime::new(target.ledger_info().epoch(), target.ledger_info().round());

        // Before state synchronization, we have to call finish() to free the
        // in-memory SMT held by BlockExecutor to prevent a memory leak.
        self.executor.finish();

        // The pipeline phase already committed beyond the target block timestamp, just return.
        if *latest_logical_time >= target_logical_time {
            warn!(
                "State sync target {:?} is lower than already committed logical time {:?}",
                target_logical_time, *latest_logical_time
            );
            return Ok(());
        }
```
