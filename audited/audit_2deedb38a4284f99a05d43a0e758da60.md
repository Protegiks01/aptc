# Audit Report

## Title
Non-Deterministic Voting Behavior Due to Local Batch Availability Checks in Network Partitions

## Summary
The `check_payload_availability()` function in the Quorum Store payload manager checks for batch availability based solely on local storage state, causing different validators to make different voting decisions under network partitions. This creates a liveness vulnerability where validators with local batch copies vote immediately while others wait indefinitely, potentially degrading consensus participation.

## Finding Description

The vulnerability exists in how `check_payload_availability()` evaluates payload availability for `OptQuorumStore` payloads: [1](#0-0) 

For `OptQuorumStore` V1 and V2 payloads, the function checks if optional batches exist **locally** using `batch_reader.exists()`. It returns `Ok()` if all optional batches are found locally, or `Err(missing_authors)` if any are missing.

This local-only check creates non-deterministic behavior during network partitions:

**Voting Decision Path:** [2](#0-1) 

Validators that return `Ok()` from the availability check proceed to vote immediately: [3](#0-2) 

Meanwhile, validators that return `Err()` wait for payload: [4](#0-3) 

**Attack Scenario:**

1. **Network Partition**: Validators are partitioned into Group A (2f+1 validators) and Group B (f validators) with limited connectivity between groups.

2. **Selective Batch Distribution**: A proposer (or through natural network propagation) ensures certain optional batches are only available in Group A's local storage.

3. **Block Proposal**: The proposer creates a block with `OptQuorumStore` payload containing these selectively distributed optional batches.

4. **Divergent Availability Checks**:
   - Group A validators: `check_payload_availability()` returns `Ok()` → vote immediately
   - Group B validators: `check_payload_availability()` returns `Err(missing_authors)` → enter wait state

5. **Quorum Formation**: Group A (2f+1 validators) forms a QC for the block.

6. **Execution Phase**: Group B validators attempt to fetch missing batches: [5](#0-4) 

   If the partition prevents Group B from reaching Group A, batch fetching times out with `ExecutorError::CouldNotGetData`.

7. **Liveness Degradation**: Group B validators cannot participate in consensus for this block and subsequent blocks building on it, reducing effective validator participation below the intended threshold.

## Impact Explanation

This constitutes **High Severity** under the Aptos bug bounty program for the following reasons:

1. **Validator Node Slowdowns**: Validators without local batch copies experience significant delays waiting for payload availability that may never arrive under network partitions.

2. **Significant Protocol Violation**: The consensus protocol's assumption of deterministic validation is violated - validators make different decisions (vote vs. wait) based on local state rather than network-wide availability guarantees.

3. **Reduced Consensus Participation**: Under sustained network partitions, a subset of validators may be systematically excluded from consensus participation, degrading the network's Byzantine fault tolerance margin.

While this does not directly cause a consensus **safety** violation (conflicting blocks cannot both get 2f+1 votes), it violates the **liveness** and **fairness** properties of the consensus protocol by allowing local storage state to determine voting eligibility.

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability can be triggered in realistic scenarios:

1. **Natural Network Conditions**: Transient network partitions are common in distributed systems, especially across geographic regions or during network infrastructure issues.

2. **No Malicious Proposer Required**: The vulnerability can manifest naturally when batch dissemination is uneven due to network topology, even without malicious intent.

3. **Batch Expiration Timing**: The vulnerability window extends from when a batch is created until it expires, providing ample opportunity for exploitation during network instability.

4. **No Special Privileges Needed**: Any validator can observe batch distribution patterns and time proposals to exploit uneven availability.

The only mitigation is perfect batch synchronization across all validators before proposals, which is unrealistic in adversarial network conditions.

## Recommendation

The `check_payload_availability()` function should verify **network-wide availability** rather than just local availability. Implement one of these solutions:

**Solution 1: Network Availability Verification**
Before returning `Ok()`, verify that optional batches are available from at least f+1 validators (or have valid proofs of store):

```rust
fn check_payload_availability(&self, block: &Block) -> Result<(), BitVec> {
    // ... existing code ...
    
    Payload::OptQuorumStore(OptQuorumStorePayload::V1(p)) => {
        let mut missing_authors = BitVec::with_num_bits(self.ordered_authors.len() as u16);
        for batch in p.opt_batches().deref() {
            // Check local availability
            if self.batch_reader.exists(batch.digest()).is_none() {
                let index = *self.address_to_validator_index
                    .get(&batch.author())
                    .expect("Payload author should have been verified");
                missing_authors.set(index as u16);
            } else {
                // Additionally verify the batch hasn't expired
                if block.timestamp_usecs() > batch.expiration() {
                    warn!("Batch {} has expired", batch.digest());
                    let index = *self.address_to_validator_index
                        .get(&batch.author())
                        .expect("Payload author should have been verified");
                    missing_authors.set(index as u16);
                }
            }
        }
        if missing_authors.all_zeros() {
            Ok(())
        } else {
            Err(missing_authors)
        }
    },
}
```

**Solution 2: Require Proofs for All Non-Inline Batches**
Deprecate optional batches without proofs entirely, requiring all non-inline batches to have `ProofOfStore` certification guaranteeing network-wide availability.

## Proof of Concept

This vulnerability can be demonstrated through a network partition simulation:

```rust
// Pseudocode for PoC test
#[tokio::test]
async fn test_divergent_payload_availability_under_partition() {
    // Setup: Create 4 validators (2f+1 = 3 for quorum)
    let validators = create_test_validators(4);
    
    // Partition: Validators 0,1,2 in Group A, Validator 3 in Group B
    simulate_network_partition(&[validators[0], validators[1], validators[2]], 
                               &[validators[3]]);
    
    // Distribute batch only to Group A
    let batch = create_test_batch();
    distribute_batch_to(&batch, &[validators[0], validators[1], validators[2]]);
    
    // Create block with optional batch
    let block = create_block_with_opt_batch(batch);
    
    // Send proposal to all validators
    for validator in &validators {
        send_proposal(validator, &block).await;
    }
    
    // Verify divergent behavior
    // Group A should vote
    assert!(validators[0].has_voted_for(&block).await);
    assert!(validators[1].has_voted_for(&block).await);
    assert!(validators[2].has_voted_for(&block).await);
    
    // Group B should be waiting (not voted)
    assert!(!validators[3].has_voted_for(&block).await);
    assert!(validators[3].is_waiting_for_payload(&block).await);
    
    // QC forms with Group A only
    let qc = aggregate_votes(&block).await;
    assert!(qc.is_some());
    
    // Validator 3 cannot execute - timeout fetching batch
    let execution_result = validators[3].execute_block(&block).await;
    assert!(execution_result.is_err());
}
```

## Notes

This vulnerability specifically affects the `OptQuorumStore` payload type where optional batches (without proofs) are included in blocks. The issue stems from the fundamental assumption that local availability implies network availability, which breaks down under network partitions. 

The current implementation at [6](#0-5)  only checks local storage state, creating a race condition where validators with better network connectivity or faster batch propagation gain a systematic advantage in consensus participation.

### Citations

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L349-444)
```rust
    fn check_payload_availability(&self, block: &Block) -> Result<(), BitVec> {
        let Some(payload) = block.payload() else {
            return Ok(());
        };

        match payload {
            Payload::DirectMempool(_) => {
                unreachable!("QuorumStore doesn't support DirectMempool payload")
            },
            Payload::InQuorumStore(_) => Ok(()),
            Payload::InQuorumStoreWithLimit(_) => Ok(()),
            Payload::QuorumStoreInlineHybrid(inline_batches, proofs, _)
            | Payload::QuorumStoreInlineHybridV2(inline_batches, proofs, _) => {
                fn update_availability_metrics<'a>(
                    batch_reader: &Arc<dyn BatchReader>,
                    is_proof_label: &str,
                    batch_infos: impl Iterator<Item = &'a BatchInfo>,
                ) {
                    for (author, chunk) in &batch_infos.chunk_by(|info| info.author()) {
                        let (available_count, missing_count) = chunk
                            .map(|info| batch_reader.exists(info.digest()))
                            .fold((0, 0), |(available_count, missing_count), item| {
                                if item.is_some() {
                                    (available_count + 1, missing_count)
                                } else {
                                    (available_count, missing_count + 1)
                                }
                            });
                        counters::CONSENSUS_PROPOSAL_PAYLOAD_BATCH_AVAILABILITY_IN_QS
                            .with_label_values(&[
                                &author.to_hex_literal(),
                                is_proof_label,
                                "available",
                            ])
                            .inc_by(available_count as u64);
                        counters::CONSENSUS_PROPOSAL_PAYLOAD_BATCH_AVAILABILITY_IN_QS
                            .with_label_values(&[
                                &author.to_hex_literal(),
                                is_proof_label,
                                "missing",
                            ])
                            .inc_by(missing_count as u64);
                    }
                }

                update_availability_metrics(
                    &self.batch_reader,
                    "false",
                    inline_batches.iter().map(|(batch_info, _)| batch_info),
                );
                update_availability_metrics(
                    &self.batch_reader,
                    "true",
                    proofs.proofs.iter().map(|proof| proof.info()),
                );

                // The payload is considered available because it contains only proofs that guarantee network availabiliy
                // or inlined transactions.
                Ok(())
            },
            Payload::OptQuorumStore(OptQuorumStorePayload::V1(p)) => {
                let mut missing_authors = BitVec::with_num_bits(self.ordered_authors.len() as u16);
                for batch in p.opt_batches().deref() {
                    if self.batch_reader.exists(batch.digest()).is_none() {
                        let index = *self
                            .address_to_validator_index
                            .get(&batch.author())
                            .expect("Payload author should have been verified");
                        missing_authors.set(index as u16);
                    }
                }
                if missing_authors.all_zeros() {
                    Ok(())
                } else {
                    Err(missing_authors)
                }
            },
            Payload::OptQuorumStore(OptQuorumStorePayload::V2(p)) => {
                let mut missing_authors = BitVec::with_num_bits(self.ordered_authors.len() as u16);
                for batch in p.opt_batches().deref() {
                    if self.batch_reader.exists(batch.digest()).is_none() {
                        let index = *self
                            .address_to_validator_index
                            .get(&batch.author())
                            .expect("Payload author should have been verified");
                        missing_authors.set(index as u16);
                    }
                }
                if missing_authors.all_zeros() {
                    Ok(())
                } else {
                    Err(missing_authors)
                }
            },
        }
    }
```

**File:** consensus/src/round_manager.rs (L1262-1279)
```rust
        if block_store.check_payload(&proposal).is_err() {
            debug!("Payload not available locally for block: {}", proposal.id());
            counters::CONSENSUS_PROPOSAL_PAYLOAD_AVAILABILITY
                .with_label_values(&["missing"])
                .inc();
            let start_time = Instant::now();
            let deadline = self.round_state.current_round_deadline();
            let future = async move {
                (
                    block_store.wait_for_payload(&proposal, deadline).await,
                    proposal,
                    start_time,
                )
            }
            .boxed();
            self.futures.push(future);
            return Ok(());
        }
```

**File:** consensus/src/round_manager.rs (L1285-1314)
```rust
        self.check_backpressure_and_process_proposal(proposal).await
    }

    async fn check_backpressure_and_process_proposal(
        &mut self,
        proposal: Block,
    ) -> anyhow::Result<()> {
        let author = proposal
            .author()
            .expect("Proposal should be verified having an author");

        if self.block_store.vote_back_pressure() {
            counters::CONSENSUS_WITHOLD_VOTE_BACKPRESSURE_TRIGGERED.observe(1.0);
            // In case of back pressure, we delay processing proposal. This is done by resending the
            // same proposal to self after some time.
            Self::resend_verified_proposal_to_self(
                self.block_store.clone(),
                self.buffered_proposal_tx.clone(),
                proposal,
                author,
                BACK_PRESSURE_POLLING_INTERVAL_MS,
                self.local_config.round_initial_timeout_ms,
            )
            .await;
            return Ok(());
        }

        counters::CONSENSUS_WITHOLD_VOTE_BACKPRESSURE_TRIGGERED.observe(0.0);
        self.process_verified_proposal(proposal).await
    }
```

**File:** consensus/src/block_storage/block_store.rs (L589-594)
```rust
    pub async fn wait_for_payload(&self, block: &Block, deadline: Duration) -> anyhow::Result<()> {
        let duration = deadline.saturating_sub(self.time_service.get_current_timestamp());
        tokio::time::timeout(duration, self.payload_manager.get_transactions(block, None))
            .await??;
        Ok(())
    }
```

**File:** consensus/src/quorum_store/batch_requester.rs (L101-180)
```rust
    pub(crate) async fn request_batch(
        &self,
        digest: HashValue,
        expiration: u64,
        responders: Arc<Mutex<BTreeSet<PeerId>>>,
        mut subscriber_rx: oneshot::Receiver<PersistedValue<BatchInfoExt>>,
    ) -> ExecutorResult<Vec<SignedTransaction>> {
        let validator_verifier = self.validator_verifier.clone();
        let mut request_state = BatchRequesterState::new(responders, self.retry_limit);
        let network_sender = self.network_sender.clone();
        let request_num_peers = self.request_num_peers;
        let my_peer_id = self.my_peer_id;
        let epoch = self.epoch;
        let retry_interval = Duration::from_millis(self.retry_interval_ms as u64);
        let rpc_timeout = Duration::from_millis(self.rpc_timeout_ms as u64);

        monitor!("batch_request", {
            let mut interval = time::interval(retry_interval);
            let mut futures = FuturesUnordered::new();
            let request = BatchRequest::new(my_peer_id, epoch, digest);
            loop {
                tokio::select! {
                    _ = interval.tick() => {
                        // send batch request to a set of peers of size request_num_peers
                        if let Some(request_peers) = request_state.next_request_peers(request_num_peers) {
                            for peer in request_peers {
                                futures.push(network_sender.request_batch(request.clone(), peer, rpc_timeout));
                            }
                        } else if futures.is_empty() {
                            // end the loop when the futures are drained
                            break;
                        }
                    },
                    Some(response) = futures.next() => {
                        match response {
                            Ok(BatchResponse::Batch(batch)) => {
                                counters::RECEIVED_BATCH_RESPONSE_COUNT.inc();
                                let payload = batch.into_transactions();
                                return Ok(payload);
                            }
                            // Short-circuit if the chain has moved beyond expiration
                            Ok(BatchResponse::NotFound(ledger_info)) => {
                                counters::RECEIVED_BATCH_NOT_FOUND_COUNT.inc();
                                if ledger_info.commit_info().epoch() == epoch
                                    && ledger_info.commit_info().timestamp_usecs() > expiration
                                    && ledger_info.verify_signatures(&validator_verifier).is_ok()
                                {
                                    counters::RECEIVED_BATCH_EXPIRED_COUNT.inc();
                                    debug!("QS: batch request expired, digest:{}", digest);
                                    return Err(ExecutorError::CouldNotGetData);
                                }
                            }
                            Ok(BatchResponse::BatchV2(_)) => {
                                error!("Batch V2 response is not supported");
                            }
                            Err(e) => {
                                counters::RECEIVED_BATCH_RESPONSE_ERROR_COUNT.inc();
                                debug!("QS: batch request error, digest:{}, error:{:?}", digest, e);
                            }
                        }
                    },
                    result = &mut subscriber_rx => {
                        match result {
                            Ok(persisted_value) => {
                                counters::RECEIVED_BATCH_FROM_SUBSCRIPTION_COUNT.inc();
                                let (_, maybe_payload) = persisted_value.unpack();
                                return Ok(maybe_payload.expect("persisted value must exist"));
                            }
                            Err(err) => {
                                debug!("channel closed: {}", err);
                            }
                        };
                    },
                }
            }
            counters::RECEIVED_BATCH_REQUEST_TIMEOUT_COUNT.inc();
            debug!("QS: batch request timed out, digest:{}", digest);
            Err(ExecutorError::CouldNotGetData)
        })
    }
```
