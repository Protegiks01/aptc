# Audit Report

## Title
State Snapshot Restore Completes Successfully with Incomplete Merkle Tree Due to Missing Root Hash Verification

## Summary
The `finish_impl()` method in `JellyfishMerkleRestore` fails to verify that the final computed root hash matches the expected root hash after state snapshot restoration. This security gap allows incomplete state trees to be written to storage when backup manifests are missing chunks, potentially causing state inconsistency across nodes.

## Finding Description

During state snapshot restoration, the system processes chunks from a backup manifest and reconstructs a Jellyfish Merkle tree. The critical security guarantee is that the final restored tree must match the `expected_root_hash` verified against the ledger info.

**The Vulnerability:**

The `expected_root_hash` field is stored in the `JellyfishMerkleRestore` struct [1](#0-0)  but is only used during per-chunk verification [2](#0-1) , not during finalization.

When `finish_impl()` is called, it freezes remaining partial nodes and writes them to storage [3](#0-2)  without verifying the final root hash matches `expected_root_hash`. The method simply returns `Ok(())` after writing nodes to storage.

**Attack Scenario:**

1. State snapshot chunks are loaded from a backup manifest [4](#0-3) 

2. A malicious backup provider crafts a manifest listing only a subset of required chunks [5](#0-4) 

3. Each chunk passes verification individually because sparse Merkle range proofs include right siblings covering unprocessed tree portions [6](#0-5) 

4. When all manifest chunks complete, `finish()` is called [7](#0-6) 

5. The incomplete tree is written to storage without final root hash verification

6. Different nodes restoring from different incomplete manifests produce different state trees with different root hashes

**Security Guarantee Violated:**

This breaks the state consistency invariant that all validators must produce identical state roots for identical transaction histories. The missing verification allows state divergence between nodes participating in consensus.

## Impact Explanation

**HIGH Severity** - This vulnerability causes state consistency violations:

1. **State Consistency Violation**: Nodes restoring from incomplete manifests produce different state trees than nodes with complete state. When these nodes attempt to participate in consensus, they produce different state roots for the same transactions, causing consensus failures.

2. **Validator Onboarding Issues**: New validators using compromised backup sources would have incorrect state from the start, requiring manual intervention and re-synchronization from trusted sources.

3. **Recovery Complexity**: While not permanently unrecoverable, affected nodes require manual detection and re-synchronization. The vulnerability could delay network recovery during disaster scenarios when multiple nodes need rapid state restoration.

4. **Limited Scope**: The impact is limited to nodes using the malicious backup source and would be detected through consensus validation failures. The check in `JellyfishMerkleRestore::new()` [8](#0-7)  would catch mismatches on subsequent restarts.

This does not qualify as CRITICAL severity because it does not cause permanent fund loss, network-wide partitions, or non-recoverable state corruption. Affected nodes can re-sync from correct backup sources.

## Likelihood Explanation

**HIGH Likelihood** - The vulnerability is easily exploitable:

1. **No Privileged Access Required**: Backup storage providers are external entities not listed as trusted roles in the Aptos threat model. Any entity hosting backup data could provide incomplete manifests.

2. **Simple Attack Vector**: The exploit requires only providing a backup manifest with missing chunks - no complex timing, race conditions, or cryptographic breaks needed.

3. **No Manifest Completeness Validation**: The system does not validate that manifest chunks cover all required key ranges continuously. It simply processes whatever chunks are listed [9](#0-8) 

4. **No Defense in Depth**: There is no subsequent validation after `finish()` in the restore coordinator or in `finalize_state_snapshot()` [10](#0-9) 

5. **Real-World Scenarios**: Beyond malicious actors, legitimate backup corruption during transmission or storage could trigger this vulnerability, causing silent failures.

## Recommendation

Add final root hash verification in `finish_impl()`:

```rust
pub fn finish_impl(mut self) -> Result<()> {
    self.wait_for_async_commit()?;
    
    // ... existing special case handling ...
    
    self.freeze(0);
    
    // ADDED: Verify final root hash before writing to storage
    let root_node = self.frozen_nodes
        .get(&NodeKey::new_empty_path(self.version))
        .ok_or_else(|| anyhow!("Root node not found after freezing"))?;
    
    ensure!(
        root_node.hash() == self.expected_root_hash,
        "Final root hash mismatch. Expected: {}, Got: {}",
        self.expected_root_hash,
        root_node.hash()
    );
    
    self.store.write_node_batch(&self.frozen_nodes)?;
    Ok(())
}
```

Additionally, consider adding manifest completeness validation to verify chunks cover all required key ranges continuously without gaps.

## Proof of Concept

The vulnerability can be demonstrated by:

1. Creating a valid state snapshot backup with complete manifest
2. Modifying the manifest to remove chunks in the middle key range
3. Attempting restoration with the incomplete manifest
4. Observing that `finish()` completes successfully despite missing chunks
5. Verifying the final state tree has different root hash than expected

A complete PoC would require setting up the backup infrastructure and state snapshot creation, but the code analysis confirms the vulnerability exists as described - `finish_impl()` at lines 750-789 contains no reference to `self.expected_root_hash` for final verification.

## Notes

While this is a valid security vulnerability requiring a fix, the practical impact is somewhat mitigated by:

- Consensus validation would detect state inconsistencies relatively quickly
- The `JellyfishMerkleRestore::new()` check catches mismatches on restart
- Affected nodes can recover by re-syncing from correct sources
- Limited to nodes using the compromised backup source

The fix is straightforward - adding a single verification check before writing the final tree to storage ensures the restored state matches the cryptographically verified expected root hash from the ledger info.

### Citations

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L176-176)
```rust
    expected_root_hash: HashValue,
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L196-206)
```rust
        let (finished, partial_nodes, previous_leaf) = if let Some(root_node) =
            tree_reader.get_node_option(&NodeKey::new_empty_path(version), "restore")?
        {
            info!("Previous restore is complete, checking root hash.");
            ensure!(
                root_node.hash() == expected_root_hash,
                "Previous completed restore has root hash {}, expecting {}",
                root_node.hash(),
                expected_root_hash,
            );
            (true, vec![], None)
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L624-697)
```rust
    /// Verifies that all states that have been added so far (from the leftmost one to
    /// `self.previous_leaf`) are correct, i.e., we are able to construct `self.expected_root_hash`
    /// by combining all existing states and `proof`.
    #[allow(clippy::collapsible_if)]
    fn verify(&self, proof: SparseMerkleRangeProof) -> Result<()> {
        let previous_leaf = self
            .previous_leaf
            .as_ref()
            .expect("The previous leaf must exist.");

        let previous_key = previous_leaf.account_key();
        // If we have all siblings on the path from root to `previous_key`, we should be able to
        // compute the root hash. The siblings on the right are already in the proof. Now we
        // compute the siblings on the left side, which represent all the states that have ever
        // been added.
        let mut left_siblings = vec![];

        // The following process might add some extra placeholder siblings on the left, but it is
        // nontrivial to determine when the loop should stop. So instead we just add these
        // siblings for now and get rid of them in the next step.
        let mut num_visited_right_siblings = 0;
        for (i, bit) in previous_key.iter_bits().enumerate() {
            if bit {
                // This node is a right child and there should be a sibling on the left.
                let sibling = if i >= self.partial_nodes.len() * 4 {
                    *SPARSE_MERKLE_PLACEHOLDER_HASH
                } else {
                    Self::compute_left_sibling(
                        &self.partial_nodes[i / 4],
                        previous_key.get_nibble(i / 4),
                        (3 - i % 4) as u8,
                    )
                };
                left_siblings.push(sibling);
            } else {
                // This node is a left child and there should be a sibling on the right.
                num_visited_right_siblings += 1;
            }
        }
        ensure!(
            num_visited_right_siblings >= proof.right_siblings().len(),
            "Too many right siblings in the proof.",
        );

        // Now we remove any extra placeholder siblings at the bottom. We keep removing the last
        // sibling if 1) it's a placeholder 2) it's a sibling on the left.
        for bit in previous_key.iter_bits().rev() {
            if bit {
                if *left_siblings.last().expect("This sibling must exist.")
                    == *SPARSE_MERKLE_PLACEHOLDER_HASH
                {
                    left_siblings.pop();
                } else {
                    break;
                }
            } else if num_visited_right_siblings > proof.right_siblings().len() {
                num_visited_right_siblings -= 1;
            } else {
                break;
            }
        }

        // Left siblings must use the same ordering as the right siblings in the proof
        left_siblings.reverse();

        // Verify the proof now that we have all the siblings
        proof
            .verify(
                self.expected_root_hash,
                SparseMerkleLeafNode::new(*previous_key, previous_leaf.value_hash()),
                left_siblings,
            )
            .map_err(Into::into)
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L786-788)
```rust
        self.freeze(0);
        self.store.write_node_batch(&self.frozen_nodes)?;
        Ok(())
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L167-174)
```rust
            manifest
                .chunks
                .into_iter()
                .skip_while(|chunk| chunk.last_key <= resume_point)
                .collect()
        } else {
            manifest.chunks
        };
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L187-226)
```rust
        let futs_iter = chunks.into_iter().enumerate().map(|(chunk_idx, chunk)| {
            let storage = storage.clone();
            async move {
                tokio::spawn(async move {
                    let blobs = Self::read_state_value(&storage, chunk.blobs.clone()).await?;
                    let proof = storage.load_bcs_file(&chunk.proof).await?;
                    Result::<_>::Ok((chunk_idx, chunk, blobs, proof))
                })
                .await?
            }
        });
        let con = self.concurrent_downloads;
        let mut futs_stream = stream::iter(futs_iter).buffered_x(con * 2, con);
        let mut start = None;
        while let Some((chunk_idx, chunk, mut blobs, proof)) = futs_stream.try_next().await? {
            start = start.or_else(|| Some(Instant::now()));
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["add_state_chunk"]);
            let receiver = receiver.clone();
            if self.validate_modules {
                blobs = tokio::task::spawn_blocking(move || {
                    Self::validate_modules(&blobs);
                    blobs
                })
                .await?;
            }
            tokio::task::spawn_blocking(move || {
                receiver.lock().as_mut().unwrap().add_chunk(blobs, proof)
            })
            .await??;
            leaf_idx.set(chunk.last_idx as i64);
            info!(
                chunk = chunk_idx,
                chunks_to_add = chunks_to_add,
                last_idx = chunk.last_idx,
                values_per_second = ((chunk.last_idx + 1 - start_idx) as f64
                    / start.as_ref().unwrap().elapsed().as_secs_f64())
                    as u64,
                "State chunk added.",
            );
        }
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L228-228)
```rust
        tokio::task::spawn_blocking(move || receiver.lock().take().unwrap().finish()).await??;
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/manifest.rs (L39-39)
```rust
    pub chunks: Vec<StateSnapshotChunk>,
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L125-200)
```rust
    fn finalize_state_snapshot(
        &self,
        version: Version,
        output_with_proof: TransactionOutputListWithProofV2,
        ledger_infos: &[LedgerInfoWithSignatures],
    ) -> Result<()> {
        let (output_with_proof, persisted_aux_info) = output_with_proof.into_parts();
        gauged_api("finalize_state_snapshot", || {
            // Ensure the output with proof only contains a single transaction output and info
            let num_transaction_outputs = output_with_proof.get_num_outputs();
            let num_transaction_infos = output_with_proof.proof.transaction_infos.len();
            ensure!(
                num_transaction_outputs == 1,
                "Number of transaction outputs should == 1, but got: {}",
                num_transaction_outputs
            );
            ensure!(
                num_transaction_infos == 1,
                "Number of transaction infos should == 1, but got: {}",
                num_transaction_infos
            );

            // TODO(joshlind): include confirm_or_save_frozen_subtrees in the change set
            // bundle below.

            // Update the merkle accumulator using the given proof
            let frozen_subtrees = output_with_proof
                .proof
                .ledger_info_to_transaction_infos_proof
                .left_siblings();
            restore_utils::confirm_or_save_frozen_subtrees(
                self.ledger_db.transaction_accumulator_db_raw(),
                version,
                frozen_subtrees,
                None,
            )?;

            // Create a single change set for all further write operations
            let mut ledger_db_batch = LedgerDbSchemaBatches::new();
            let mut sharded_kv_batch = self.state_kv_db.new_sharded_native_batches();
            let mut state_kv_metadata_batch = SchemaBatch::new();
            // Save the target transactions, outputs, infos and events
            let (transactions, outputs): (Vec<Transaction>, Vec<TransactionOutput>) =
                output_with_proof
                    .transactions_and_outputs
                    .into_iter()
                    .unzip();
            let events = outputs
                .clone()
                .into_iter()
                .map(|output| output.events().to_vec())
                .collect::<Vec<_>>();
            let wsets: Vec<WriteSet> = outputs
                .into_iter()
                .map(|output| output.write_set().clone())
                .collect();
            let transaction_infos = output_with_proof.proof.transaction_infos;
            // We should not save the key value since the value is already recovered for this version
            restore_utils::save_transactions(
                self.state_store.clone(),
                self.ledger_db.clone(),
                version,
                &transactions,
                &persisted_aux_info,
                &transaction_infos,
                &events,
                wsets,
                Some((
                    &mut ledger_db_batch,
                    &mut sharded_kv_batch,
                    &mut state_kv_metadata_batch,
                )),
                false,
            )?;

            // Save the epoch ending ledger infos
```
