# Audit Report

## Title
Unverified Database-Loaded Certified Augmented Data Enables Consensus Divergence Under Storage Corruption

## Summary
The `AugDataStore::new()` function loads certified augmented data from persistent storage and applies it to the randomness generation configuration without verifying aggregate signatures. This asymmetry with network-received data, combined with silent deserialization error handling, can cause honest validators to load different augmented data sets, resulting in divergent APK configurations and consensus failure.

## Finding Description

The vulnerability exists in the randomness generation system's initialization path. When `AugDataStore::new()` is called during epoch transitions, it loads all certified augmented data from the database and directly applies it to the `RandConfig` without signature verification. [1](#0-0) 

The loaded certified data is applied through the `augment()` method, which modifies the `RandConfig` by adding certified deltas that derive and store Augmented Public Keys (APKs). [2](#0-1) [3](#0-2) 

In stark contrast, network-received certified augmented data undergoes strict verification before being processed: [4](#0-3) [5](#0-4) 

Additionally, the database implementation silently ignores deserialization errors using a `filter_map` pattern: [6](#0-5) 

This means validators experiencing different deserialization failures will load different subsets of certified data, leading to divergent APK configurations.

**Consensus Divergence Mechanism:**

The APKs stored in `RandConfig` are consensus-critical because they're used to verify randomness shares: [7](#0-6) 

When validators have different APKs due to loading corrupted or partially deserialized data:
1. They accept different sets of randomness shares based on their loaded APKs
2. Share aggregation produces different randomness values for the same round [8](#0-7) 

3. Different randomness values are included in block metadata transactions during execution [9](#0-8) 

4. Different metadata transactions produce different state roots
5. Validators vote on different state roots and cannot reach the 2f+1 quorum required for consensus

This breaks the fundamental consensus safety invariant that all honest validators must agree on the same state root for a given block.

## Impact Explanation

This vulnerability qualifies as **High Severity** under Aptos bug bounty criteria:

**Consensus Safety Violation**: The vulnerability enables honest validators to produce different state roots for the same block due to divergent randomness values. This directly violates AptosBFT's safety guarantees that require state agreement among honest validators.

**Non-Recoverable Network Partition Risk**: If multiple validators load different corrupted data during epoch transitions, they will permanently disagree on randomness values, preventing quorum formation. This could require manual intervention or coordinated database recovery across validators.

**Protocol Violation with Byzantine Fault Tolerance Impact**: The randomness protocol assumes all certified augmented data has been cryptographically verified. This defensive programming failure means storage corruption can cause honest validators to diverge, effectively reducing the network's Byzantine fault tolerance below the theoretical 1/3 threshold.

While this doesn't enable direct fund theft or require malicious validators, it represents a critical consensus layer vulnerability that could halt the network or cause chain splits under storage failure conditions.

## Likelihood Explanation

**Moderate Likelihood:**

1. **Silent Deserialization Failures**: The confirmed use of `Err(_) => None` in the database iterator means any deserialization errors are silently ignored, causing different validators to load different data sets without error logging.

2. **Storage Corruption Statistics**: While modern storage systems have ECC and checksums, bit flips and corruption do occur over time, especially across large validator sets running continuously. The larger the validator set and the longer the network operates, the higher the cumulative probability.

3. **Epoch Transition Critical Window**: During epoch boundaries, all validators simultaneously read from their databases to load certified augmented data, creating a critical window where any storage inconsistencies manifest as consensus divergence.

4. **No Additional Integrity Checks**: The database implementation relies solely on RocksDB's consistency guarantees without additional application-level signature verification or checksums for persisted certified data.

The likelihood increases with validator set size, network age, and hardware diversity. While individual corruption events are rare, the silent error handling and lack of verification create a persistent vulnerability that accumulates risk over time.

## Recommendation

Add aggregate signature verification for database-loaded certified augmented data to match the verification performed on network-received data:

```rust
// In AugDataStore::new(), after line 59:
for (_, certified_data) in &certified_data {
    // Verify aggregate signature before applying
    if let Err(e) = certified_data.verify(&config.validator) {
        error!(
            "[AugDataStore] Invalid certified data from database: {:?}",
            e
        );
        continue; // Skip invalid data instead of panic
    }
    
    certified_data
        .data()
        .augment(&config, &fast_config, certified_data.author());
}
```

Additionally, improve error handling in the database layer to log deserialization failures instead of silently ignoring them:

```rust
// In db.rs get_all():
Ok(iter
    .filter_map(|e| match e {
        Ok((k, v)) => Some((k, v)),
        Err(e) => {
            error!("Database deserialization error: {:?}", e);
            None
        }
    })
    .collect::<Vec<(S::Key, S::Value)>>())
```

## Proof of Concept

A PoC would require:
1. Modifying a validator's RocksDB database to inject corrupted certified augmented data
2. Starting the validator node and observing it load the corrupted data without verification
3. Demonstrating that this validator computes different randomness than other validators
4. Showing that consensus fails due to state root disagreement

While this requires filesystem access to validator databases (a trusted role scenario), the core vulnerability is the defensive programming failure that allows unverified data to affect consensus-critical computations.

## Notes

**Threat Model Clarification**: This vulnerability affects honest validators experiencing storage failures, not malicious validators. While validator operators are trusted roles, the Byzantine fault tolerance model assumes honest validators can experience hardware failures. The lack of verification represents a defensive programming failure that reduces the network's resilience to non-malicious storage corruption.

**Asymmetry is Unjustified**: The code verifies network-received certified augmented data but not database-loaded data. Both paths feed into the same consensus-critical APK configuration, so both should have equivalent verification requirements.

**Silent Failures Amplify Risk**: The silent deserialization error handling means validators won't detect or report when they've loaded different data sets, making diagnosis of consensus failures more difficult.

### Citations

**File:** consensus/src/rand/rand_gen/aug_data_store.rs (L57-71)
```rust
        let all_certified_data = db.get_all_certified_aug_data().unwrap_or_default();
        let (to_remove, certified_data) =
            Self::filter_by_epoch(epoch, all_certified_data.into_iter());
        if let Err(e) = db.remove_certified_aug_data(to_remove) {
            error!(
                "[AugDataStore] failed to remove certified aug data: {:?}",
                e
            );
        }

        for (_, certified_data) in &certified_data {
            certified_data
                .data()
                .augment(&config, &fast_config, certified_data.author());
        }
```

**File:** consensus/src/rand/rand_gen/types.rs (L52-81)
```rust
    fn verify(
        &self,
        rand_config: &RandConfig,
        rand_metadata: &RandMetadata,
        author: &Author,
    ) -> anyhow::Result<()> {
        let index = *rand_config
            .validator
            .address_to_validator_index()
            .get(author)
            .ok_or_else(|| anyhow!("Share::verify failed with unknown author"))?;
        let maybe_apk = &rand_config.keys.certified_apks[index];
        if let Some(apk) = maybe_apk.get() {
            WVUF::verify_share(
                &rand_config.vuf_pp,
                apk,
                bcs::to_bytes(&rand_metadata)
                    .map_err(|e| anyhow!("Serialization failed: {}", e))?
                    .as_slice(),
                &self.share,
            )?;
        } else {
            bail!(
                "[RandShare] No augmented public key for validator id {}, {}",
                index,
                author
            );
        }
        Ok(())
    }
```

**File:** consensus/src/rand/rand_gen/types.rs (L97-148)
```rust
    fn aggregate<'a>(
        shares: impl Iterator<Item = &'a RandShare<Self>>,
        rand_config: &RandConfig,
        rand_metadata: RandMetadata,
    ) -> anyhow::Result<Randomness>
    where
        Self: Sized,
    {
        let timer = std::time::Instant::now();
        let mut apks_and_proofs = vec![];
        for share in shares {
            let id = rand_config
                .validator
                .address_to_validator_index()
                .get(share.author())
                .copied()
                .ok_or_else(|| {
                    anyhow!(
                        "Share::aggregate failed with invalid share author: {}",
                        share.author
                    )
                })?;
            let apk = rand_config
                .get_certified_apk(share.author())
                .ok_or_else(|| {
                    anyhow!(
                        "Share::aggregate failed with missing apk for share from {}",
                        share.author
                    )
                })?;
            apks_and_proofs.push((Player { id }, apk.clone(), share.share().share));
        }

        let proof = WVUF::aggregate_shares(&rand_config.wconfig, &apks_and_proofs);
        let metadata_serialized = bcs::to_bytes(&rand_metadata).map_err(|e| {
            anyhow!("Share::aggregate failed with metadata serialization error: {e}")
        })?;
        let eval = WVUF::derive_eval(
            &rand_config.wconfig,
            &rand_config.vuf_pp,
            metadata_serialized.as_slice(),
            &rand_config.get_all_certified_apk(),
            &proof,
            THREAD_MANAGER.get_exe_cpu_pool(),
        )
        .map_err(|e| anyhow!("Share::aggregate failed with WVUF derive_eval error: {e}"))?;
        debug!("WVUF derivation time: {} ms", timer.elapsed().as_millis());
        let eval_bytes = bcs::to_bytes(&eval)
            .map_err(|e| anyhow!("Share::aggregate failed with eval serialization error: {e}"))?;
        let rand_bytes = Sha3_256::digest(eval_bytes.as_slice()).to_vec();
        Ok(Randomness::new(rand_metadata, rand_bytes))
    }
```

**File:** consensus/src/rand/rand_gen/types.rs (L178-194)
```rust
    fn augment(
        &self,
        rand_config: &RandConfig,
        fast_rand_config: &Option<RandConfig>,
        author: &Author,
    ) {
        let AugmentedData { delta, fast_delta } = self;
        rand_config
            .add_certified_delta(author, delta.clone())
            .expect("Add delta should succeed");

        if let (Some(config), Some(fast_delta)) = (fast_rand_config, fast_delta) {
            config
                .add_certified_delta(author, fast_delta.clone())
                .expect("Add delta for fast path should succeed");
        }
    }
```

**File:** consensus/src/rand/rand_gen/types.rs (L555-558)
```rust
    pub fn verify(&self, verifier: &ValidatorVerifier) -> anyhow::Result<()> {
        verifier.verify_multi_signatures(&self.aug_data, &self.signatures)?;
        Ok(())
    }
```

**File:** consensus/src/rand/rand_gen/types.rs (L661-665)
```rust
    pub fn add_certified_delta(&self, peer: &Author, delta: Delta) -> anyhow::Result<()> {
        let apk = self.derive_apk(peer, delta)?;
        self.add_certified_apk(peer, apk)?;
        Ok(())
    }
```

**File:** consensus/src/rand/rand_gen/network_messages.rs (L50-52)
```rust
            RandMessage::CertifiedAugData(certified_aug_data) => {
                certified_aug_data.verify(&epoch_state.verifier)
            },
```

**File:** consensus/src/rand/rand_gen/storage/db.rs (L73-82)
```rust
    fn get_all<S: Schema>(&self) -> Result<Vec<(S::Key, S::Value)>, DbError> {
        let mut iter = self.db.iter::<S>()?;
        iter.seek_to_first();
        Ok(iter
            .filter_map(|e| match e {
                Ok((k, v)) => Some((k, v)),
                Err(_) => None,
            })
            .collect::<Vec<(S::Key, S::Value)>>())
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L807-811)
```rust
        let metadata_txn = if let Some(maybe_rand) = rand_result {
            block.new_metadata_with_randomness(&validator, maybe_rand)
        } else {
            block.new_block_metadata(&validator).into()
        };
```
