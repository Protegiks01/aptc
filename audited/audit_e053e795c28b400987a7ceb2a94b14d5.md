# Audit Report

## Title
Message Replay Attack via Unbounded Channel Flooding in Quorum Store NetworkListener

## Summary
The NetworkListener in Aptos consensus does not track processed message IDs or sequence numbers to prevent replay attacks. A Byzantine validator can repeatedly send the same valid SignedBatchInfo, BatchMsg, or ProofOfStore messages to exhaust channel buffers and cause blocking, resulting in resource exhaustion and consensus delays.

## Finding Description

The vulnerability exists in the message processing flow between the NetworkListener and downstream coordinators/managers. The NetworkListener receives verified consensus messages and forwards them to internal channels using blocking send operations, but there is no replay protection mechanism to prevent duplicate messages from consuming resources.

**Attack Flow:**

1. A Byzantine validator crafts valid SignedBatchInfo/BatchMsg/ProofOfStore messages that pass cryptographic verification.

2. The validator sends the same message thousands of times in rapid succession.

3. Each message passes signature verification in the EpochManager: [1](#0-0) 

The verification only checks sender/signer match, expiration time, and signature validity—no sequence numbers or message IDs.

4. Verified messages are forwarded to the NetworkListener via non-blocking push to the quorum_store_msg_tx channel: [2](#0-1) 

5. The NetworkListener receives messages and forwards them using **blocking send operations** to coordinator/manager channels: [3](#0-2) [4](#0-3) [5](#0-4) 

6. These channels have a default buffer size of 1000 messages: [6](#0-5) 

7. When the buffer fills up, `tokio::sync::mpsc::Sender::send().await` blocks the NetworkListener, preventing it from processing other valid messages.

8. While deduplication exists at the coordinator/manager level (ProofCoordinator uses SignatureAggregator with BTreeMap, BatchStore checks digest duplicates, ProofManager checks for existing proofs), this deduplication happens **AFTER** the messages have already consumed channel buffer space and triggered processing overhead: [7](#0-6) [8](#0-7) 

**Broken Invariant:**
This violates the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." The lack of replay protection allows unbounded resource consumption through duplicate message processing.

## Impact Explanation

**Severity: HIGH** per Aptos Bug Bounty Program

This vulnerability qualifies as High severity under two categories:

1. **Validator node slowdowns**: A Byzantine validator can cause significant performance degradation by flooding channels with duplicate messages, blocking the NetworkListener and delaying processing of legitimate consensus messages.

2. **Significant protocol violations**: The attack disrupts the quorum store protocol's normal operation, potentially delaying batch certification, proof formation, and consensus progress.

**Specific Impacts:**
- NetworkListener becomes unresponsive when channels fill and block on send operations
- Legitimate SignedBatchInfo/BatchMsg/ProofOfStore messages experience processing delays
- Batch certification and proof formation are delayed, slowing consensus
- The attack can be sustained continuously and selectively target different message types
- Multiple Byzantine validators can coordinate to amplify the effect

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability is highly likely to be exploited because:

1. **Low Attack Complexity**: The attacker only needs to send the same valid message repeatedly—no cryptographic bypass, signature forgery, or complex setup required.

2. **Single Malicious Actor**: Only one Byzantine validator is needed to execute the attack (Aptos tolerates up to f Byzantine validators out of 3f+1 total).

3. **No Detection/Rate Limiting**: There is no message ID tracking, sequence number checking, or rate limiting at the NetworkListener level to prevent this attack.

4. **Persistent Effect**: The attack can be sustained indefinitely as long as the Byzantine validator continues sending duplicate messages.

5. **Valid Signatures**: Since the messages pass cryptographic verification, they appear legitimate to the system until deduplication occurs downstream (after consuming resources).

## Recommendation

Implement message replay protection at the NetworkListener level before forwarding to coordinators/managers. Three approaches:

**Option 1: Message ID Tracking (Recommended)**
```rust
// Add to NetworkListener struct
recently_processed: LruCache<MessageId, Instant>,

// Before sending to coordinators, check:
let msg_id = compute_message_id(&msg);
if self.recently_processed.contains(&msg_id) {
    counters::DUPLICATE_MESSAGE_REJECTED.inc();
    continue; // Skip duplicate
}
self.recently_processed.put(msg_id, Instant::now());
```

**Option 2: Per-Sender Sequence Numbers**
Add sequence numbers to SignedBatchInfo/BatchMsg/ProofOfStore messages and track the highest sequence number received from each sender.

**Option 3: Short-Term Deduplication Cache**
Maintain a time-bounded cache of message digests (e.g., last 30 seconds) and reject duplicates within that window.

**Critical Fix Points:** [9](#0-8) 

Additionally, consider using non-blocking try_send instead of blocking send to prevent the NetworkListener from becoming completely unresponsive:
```rust
match self.proof_coordinator_tx.try_send(cmd) {
    Ok(_) => {},
    Err(TrySendError::Full(_)) => {
        counters::CHANNEL_FULL_DROPPED_MSGS.inc();
        warn!("Proof coordinator channel full, dropping message");
    },
    Err(TrySendError::Closed(_)) => {
        error!("Proof coordinator channel closed");
    }
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_replay_attack_resource_exhaustion() {
    use consensus::quorum_store::network_listener::NetworkListener;
    use consensus::quorum_store::proof_coordinator::ProofCoordinatorCommand;
    use aptos_channels::aptos_channel;
    use tokio::sync::mpsc;
    
    // Setup channels with default size (1000)
    let (proof_coordinator_tx, mut proof_coordinator_rx) = 
        mpsc::channel::<ProofCoordinatorCommand>(1000);
    let (batch_coordinator_tx, _) = mpsc::channel(1000);
    let (proof_manager_tx, _) = mpsc::channel(1000);
    
    let (network_msg_tx, network_msg_rx) = 
        aptos_channel::new(aptos_channels::message_queues::QueueStyle::FIFO, 100, None);
    
    // Create NetworkListener
    let listener = NetworkListener::new(
        network_msg_rx,
        proof_coordinator_tx,
        vec![batch_coordinator_tx],
        proof_manager_tx,
    );
    
    // Start NetworkListener in background
    tokio::spawn(async move {
        listener.start().await;
    });
    
    // Create a valid SignedBatchInfo message
    let signed_batch_info = create_valid_signed_batch_info(); // helper function
    let verified_event = VerifiedEvent::SignedBatchInfo(Box::new(signed_batch_info));
    let sender = AccountAddress::random();
    
    // Send the SAME message 2000 times (exceeds channel buffer of 1000)
    for i in 0..2000 {
        network_msg_tx.push(sender, (sender, verified_event.clone())).unwrap();
        
        if i == 1500 {
            // After 1500 duplicates, try to send a different valid message
            let different_batch = create_different_valid_batch();
            network_msg_tx.push(sender, (sender, different_batch)).unwrap();
        }
    }
    
    // Slow receiver - simulates coordinator processing delay
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    // Verify that the proof_coordinator channel is full and NetworkListener is blocked
    let mut received_count = 0;
    while let Ok(_) = proof_coordinator_rx.try_recv() {
        received_count += 1;
        if received_count >= 1000 {
            break; // Channel buffer is full
        }
    }
    
    assert_eq!(received_count, 1000, "Channel should be full with duplicate messages");
    
    // The NetworkListener is now blocked on send().await, unable to process
    // the different valid message sent at iteration 1500
    
    println!("Attack successful: NetworkListener blocked, legitimate messages delayed");
}
```

**Notes:**
- This PoC demonstrates how flooding with duplicate messages fills the channel buffer
- The blocking `send().await` prevents the NetworkListener from processing subsequent messages
- In production, this would delay legitimate consensus operations and batch processing
- The attack requires only valid message crafting, no cryptographic breaks

### Citations

**File:** consensus/consensus-types/src/proof_of_store.rs (L459-482)
```rust
    pub fn verify(
        &self,
        sender: PeerId,
        max_batch_expiry_gap_usecs: u64,
        validator: &ValidatorVerifier,
    ) -> anyhow::Result<()> {
        if sender != self.signer {
            bail!("Sender {} mismatch signer {}", sender, self.signer);
        }

        if self.expiration()
            > aptos_infallible::duration_since_epoch().as_micros() as u64
                + max_batch_expiry_gap_usecs
        {
            bail!(
                "Batch expiration too far in future: {} > {}",
                self.expiration(),
                aptos_infallible::duration_since_epoch().as_micros() as u64
                    + max_batch_expiry_gap_usecs
            );
        }

        Ok(validator.optimistic_verify(self.signer, &self.info, &self.signature)?)
    }
```

**File:** consensus/src/epoch_manager.rs (L1718-1728)
```rust
    fn forward_event_to<K: Eq + Hash + Clone, V>(
        mut maybe_tx: Option<aptos_channel::Sender<K, V>>,
        key: K,
        value: V,
    ) -> anyhow::Result<()> {
        if let Some(tx) = &mut maybe_tx {
            tx.push(key, value)
        } else {
            bail!("channel not initialized");
        }
    }
```

**File:** consensus/src/quorum_store/network_listener.rs (L40-111)
```rust
    pub async fn start(mut self) {
        info!("QS: starting networking");
        let mut next_batch_coordinator_idx = 0;
        while let Some((sender, msg)) = self.network_msg_rx.next().await {
            monitor!("qs_network_listener_main_loop", {
                match msg {
                    // TODO: does the assumption have to be that network listener is shutdown first?
                    VerifiedEvent::Shutdown(ack_tx) => {
                        counters::QUORUM_STORE_MSG_COUNT
                            .with_label_values(&["NetworkListener::shutdown"])
                            .inc();
                        info!("QS: shutdown network listener received");
                        ack_tx
                            .send(())
                            .expect("Failed to send shutdown ack to QuorumStore");
                        break;
                    },
                    VerifiedEvent::SignedBatchInfo(signed_batch_infos) => {
                        counters::QUORUM_STORE_MSG_COUNT
                            .with_label_values(&["NetworkListener::signedbatchinfo"])
                            .inc();
                        let cmd =
                            ProofCoordinatorCommand::AppendSignature(sender, *signed_batch_infos);
                        self.proof_coordinator_tx
                            .send(cmd)
                            .await
                            .expect("Could not send signed_batch_info to proof_coordinator");
                    },
                    VerifiedEvent::BatchMsg(batch_msg) => {
                        counters::QUORUM_STORE_MSG_COUNT
                            .with_label_values(&["NetworkListener::batchmsg"])
                            .inc();
                        // Batch msg verify function alreay ensures that the batch_msg is not empty.
                        let author = batch_msg.author().expect("Empty batch message");
                        let batches = batch_msg.take();
                        counters::RECEIVED_BATCH_MSG_COUNT.inc();

                        // Round-robin assignment to batch coordinator.
                        let idx = next_batch_coordinator_idx;
                        next_batch_coordinator_idx = (next_batch_coordinator_idx + 1)
                            % self.remote_batch_coordinator_tx.len();
                        trace!(
                            "QS: peer_id {:?},  # network_worker {}, hashed to idx {}",
                            author,
                            self.remote_batch_coordinator_tx.len(),
                            idx
                        );
                        counters::BATCH_COORDINATOR_NUM_BATCH_REQS
                            .with_label_values(&[&idx.to_string()])
                            .inc();
                        self.remote_batch_coordinator_tx[idx]
                            .send(BatchCoordinatorCommand::NewBatches(author, batches))
                            .await
                            .expect("Could not send remote batch");
                    },
                    VerifiedEvent::ProofOfStoreMsg(proofs) => {
                        counters::QUORUM_STORE_MSG_COUNT
                            .with_label_values(&["NetworkListener::proofofstore"])
                            .inc();
                        let cmd = ProofManagerCommand::ReceiveProofs(*proofs);
                        self.proof_manager_tx
                            .send(cmd)
                            .await
                            .expect("could not push Proof proof_of_store");
                    },
                    _ => {
                        unreachable!()
                    },
                };
            });
        }
    }
```

**File:** consensus/src/quorum_store/quorum_store_builder.rs (L178-184)
```rust
        let (coordinator_tx, coordinator_rx) = futures_channel::mpsc::channel(config.channel_size);
        let (batch_generator_cmd_tx, batch_generator_cmd_rx) =
            tokio::sync::mpsc::channel(config.channel_size);
        let (proof_coordinator_cmd_tx, proof_coordinator_cmd_rx) =
            tokio::sync::mpsc::channel(config.channel_size);
        let (proof_manager_cmd_tx, proof_manager_cmd_rx) =
            tokio::sync::mpsc::channel(config.channel_size);
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L181-188)
```rust
        if self
            .items
            .get(&batch_key)
            .is_some_and(|item| item.proof.is_some() || item.is_committed())
        {
            counters::inc_rejected_pos_count(counters::POS_DUPLICATE_LABEL);
            return;
        }
```

**File:** consensus/src/quorum_store/batch_store.rs (L368-382)
```rust
            let cache_entry = self.db_cache.entry(digest);

            if let Occupied(entry) = &cache_entry {
                match entry.get().expiration().cmp(&expiration_time) {
                    std::cmp::Ordering::Equal => return Ok(false),
                    std::cmp::Ordering::Greater => {
                        debug!(
                            "QS: already have the digest with higher expiration {}",
                            digest
                        );
                        return Ok(false);
                    },
                    std::cmp::Ordering::Less => {},
                }
            };
```
