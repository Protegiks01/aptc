Based on my thorough validation of the code, I can confirm this is a **valid vulnerability**. Let me provide the complete audit report:

# Audit Report

## Title
Race Condition in Pruner Reset During State Snapshot Finalization Causes Data Loss

## Summary
A race condition exists in `finalize_state_snapshot()` where updating `min_readable_version` for all pruners does not synchronize with ongoing pruner worker threads. This allows pruner workers to continue deleting historical data beyond the newly set minimum readable version, causing premature data deletion and state inconsistency.

## Finding Description

The vulnerability occurs when `finalize_state_snapshot()` calls `save_min_readable_version()` sequentially for all four pruners (ledger, state_merkle, epoch_snapshot, state_kv). [1](#0-0) 

The `save_min_readable_version()` method only updates the atomic `min_readable_version` field and persists it to disk, but crucially **does NOT update the pruner worker's `target_version`**. [2](#0-1) 

In contrast, the normal pruning path uses `set_pruner_target_db_version()` which updates BOTH the `min_readable_version` AND the pruner worker's `target_version` by calling `worker.set_target_db_version()`. [3](#0-2) 

The pruner worker thread continuously executes in a loop, calling the pruner's `prune()` method which uses only the `target_version` to determine what to prune, never checking `min_readable_version`. [4](#0-3) [5](#0-4) 

**Race Condition Scenario:**

1. Node operates with pruning enabled (default configuration)
2. Normal pruning sets both `min_readable_version` and worker's `target_version` to 9000
3. Pruner worker is actively deleting versions (progress = 8500, target = 9000)
4. State sync restore calls `finalize_state_snapshot(8700)` [6](#0-5) 
5. All pruners' `save_min_readable_version(8700)` executes, setting atomic `min_readable_version` = 8700
6. **But pruner worker's `target_version` remains 9000** (never updated by `save_min_readable_version`)
7. Pruner continues executing, deleting versions 8700-8999
8. Subsequent read requests for versions in [8700, 9000) pass the validation check but fail to retrieve data

The validation in `error_if_ledger_pruned()` reads the updated `min_readable_version` and incorrectly assumes data exists. [7](#0-6) 

This breaks the **State Consistency Invariant** - the system guarantees that data >= `min_readable_version` should be readable, but the pruner has deleted it due to the unsynchronized worker target.

## Impact Explanation

This is a **Medium Severity** vulnerability per the Aptos bug bounty criteria:

1. **Significant Protocol Violation**: The `min_readable_version` represents a critical storage invariant that data at or above this version should be available. Breaking this causes state inconsistency and violates the storage layer's fundamental guarantee.

2. **Data Loss**: Historical ledger data, state merkle tree nodes, epoch snapshots, and state KV pairs can be prematurely deleted despite the system claiming they should exist.

3. **Node Failures**: When queries request data in the deleted range, the `error_if_ledger_pruned()` check passes but data retrieval fails, causing query failures and potentially incorrect responses to state sync requests.

4. **State Sync Corruption**: Nodes restoring from state snapshots may have their freshly restored data immediately pruned, making the restore operation ineffective and requiring manual intervention or node restarts.

While this doesn't directly cause loss of funds or consensus violations, it represents a **state inconsistency requiring manual intervention**, which aligns with Medium severity in the bug bounty program.

## Likelihood Explanation

**Likelihood: Medium**

This vulnerability can occur in realistic operational scenarios:

1. **State Sync During Active Operations**: When a node performs state sync restore while pruning is enabled (default configuration)
2. **Node Recovery**: During node restarts or failover scenarios where snapshot finalization overlaps with pruner worker initialization
3. **Fast Sync Mode**: During bootstrap operations that use `finalize_state_snapshot()`

The vulnerability requires:
- Pruners to be enabled (default production configuration)
- `finalize_state_snapshot()` to be called with a version lower than the current pruner target (occurs during snapshot restore)
- Timing overlap where pruner worker is actively running (no synchronization mechanism exists)

This is a natural race condition that can occur during normal state sync and node recovery operations common in production deployments. The lack of synchronization between `save_min_readable_version()` and the pruner worker's `target_version` makes this a realistic scenario.

## Recommendation

Modify `save_min_readable_version()` to also update the pruner worker's target version, ensuring synchronization:

```rust
fn save_min_readable_version(&self, min_readable_version: Version) -> Result<()> {
    self.min_readable_version
        .store(min_readable_version, Ordering::SeqCst);

    PRUNER_VERSIONS
        .with_label_values(&["ledger_pruner", "min_readable"])
        .set(min_readable_version as i64);

    // NEW: Also update the worker's target to prevent over-pruning
    if let Some(worker) = &self.pruner_worker {
        worker.set_target_db_version(min_readable_version);
    }

    self.ledger_db.write_pruner_progress(min_readable_version)
}
```

Alternatively, add a mechanism to pause/stop pruner workers before calling `finalize_state_snapshot()` and restart them afterward with the correct target version.

## Proof of Concept

This vulnerability manifests during state sync operations when:
1. A node has pruning enabled with active worker threads
2. `finalize_state_snapshot()` is called during state restore
3. The pruner worker continues using its old `target_version` while `min_readable_version` is updated to a lower value
4. Subsequent queries for versions between the new `min_readable_version` and old `target_version` pass validation but fail to retrieve data

The race window exists because there is no synchronization mechanism between the atomic `min_readable_version` update and the pruner worker's `target_version` stored in the `DBPruner` trait implementation.

## Notes

This vulnerability affects all four pruner types (ledger, state_merkle, epoch_snapshot, state_kv) as they all follow the same pattern where `save_min_readable_version()` is called without updating the worker's target. The issue is particularly critical because the storage layer's correctness depends on the invariant that data >= `min_readable_version` is always available, and this race condition breaks that guarantee.

### Citations

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L225-234)
```rust
            self.ledger_pruner.save_min_readable_version(version)?;
            self.state_store
                .state_merkle_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .epoch_snapshot_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .state_kv_pruner
                .save_min_readable_version(version)?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L80-89)
```rust
    fn save_min_readable_version(&self, min_readable_version: Version) -> Result<()> {
        self.min_readable_version
            .store(min_readable_version, Ordering::SeqCst);

        PRUNER_VERSIONS
            .with_label_values(&["ledger_pruner", "min_readable"])
            .set(min_readable_version as i64);

        self.ledger_db.write_pruner_progress(min_readable_version)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L162-176)
```rust
    fn set_pruner_target_db_version(&self, latest_version: Version) {
        assert!(self.pruner_worker.is_some());
        let min_readable_version = latest_version.saturating_sub(self.prune_window);
        self.min_readable_version
            .store(min_readable_version, Ordering::SeqCst);

        PRUNER_VERSIONS
            .with_label_values(&["ledger_pruner", "min_readable"])
            .set(min_readable_version as i64);

        self.pruner_worker
            .as_ref()
            .unwrap()
            .set_target_db_version(min_readable_version);
    }
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L53-69)
```rust
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L62-92)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning ledger data."
            );
            self.ledger_metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning ledger data is done.");
        }

        Ok(target_version)
    }
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L1129-1136)
```rust
    storage
        .writer
        .finalize_state_snapshot(
            version,
            target_output_with_proof.clone(),
            epoch_change_proofs,
        )
        .map_err(|error| format!("Failed to finalize the state snapshot! Error: {:?}", error))?;
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L261-271)
```rust
    pub(super) fn error_if_ledger_pruned(&self, data_type: &str, version: Version) -> Result<()> {
        let min_readable_version = self.ledger_pruner.get_min_readable_version();
        ensure!(
            version >= min_readable_version,
            "{} at version {} is pruned, min available version is {}.",
            data_type,
            version,
            min_readable_version
        );
        Ok(())
    }
```
