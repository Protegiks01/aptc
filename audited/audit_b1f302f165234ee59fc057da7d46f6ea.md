# Audit Report

## Title
Race Condition Between Pipeline Abort and Executor finish()/reset() Causing Validator Node Panic

## Summary
A critical race condition exists between the consensus pipeline's `spawn_blocking` executor operations and the state synchronization's `finish()/reset()` sequence. When state sync aborts pipeline tasks and immediately calls `finish()` to free executor memory, background threads executing `pre_commit_block()` or `commit_ledger()` may still be running and access the destroyed executor state, causing a validator node panic.

## Finding Description

The vulnerability occurs in the interaction between three components:

1. **State Computer** calls `finish()` then `reset()` during state synchronization [1](#0-0) 

2. **Block Executor** implements `finish()` to destroy inner state and `pre_commit_block()`/`commit_ledger()` with `.expect()` that panic when inner is None [2](#0-1) 

3. **Pipeline Builder** spawns blocking threads for pre-commit and commit operations that access the executor [3](#0-2) 

The race condition occurs when:

**Step 1**: Consensus pipeline spawns a `pre_commit_block()` or `commit_ledger()` operation in a `spawn_blocking` thread, moving the executor Arc into the closure.

**Step 2**: State sync manager calls `abort_pipeline_for_state_sync()` which aborts pipeline tasks using `AbortHandle` [4](#0-3) 

**Step 3**: `wait_until_finishes()` completes immediately when tasks are aborted, but **tokio's abort only cancels the awaiting of spawn_blocking results - the background blocking threads continue running** [5](#0-4) 

**Step 4**: State sync proceeds to call `executor.finish()` which sets `*self.inner.write() = None`, destroying the executor's internal state [6](#0-5) 

**Step 5**: The still-running `spawn_blocking` thread executes `self.inner.read().as_ref().expect("BlockExecutor is not reset")` and encounters None, triggering a **panic** that crashes the validator node [7](#0-6) 

The root cause is that `spawn_blocking` tasks cannot be truly aborted mid-execution - they run synchronous code on blocking threads that continue until completion or panic, even after the tokio task is aborted.

## Impact Explanation

**Critical Severity** - This vulnerability causes:

1. **Validator Node Crash**: The panic in pre_commit or commit operations crashes the entire consensus node
2. **Consensus Availability Impact**: Crashed validators cannot participate in consensus, reducing the available validator set
3. **Network Liveness Risk**: If multiple validators crash simultaneously during state sync, the network could lose liveness if Byzantine threshold is crossed
4. **Non-recoverable State**: The panic occurs during critical commit operations, potentially leaving the node in an inconsistent state requiring manual intervention

This meets the **Critical Severity** criteria: "Total loss of liveness/network availability" and "Remote Code Execution on validator node" (panic = uncontrolled termination).

## Likelihood Explanation

**High Likelihood** - This race condition can occur during normal operations:

1. **Frequent Trigger**: State sync happens regularly when nodes fall behind (network delays, restarts, catching up)
2. **No Malicious Actor Required**: Natural timing of concurrent pipeline execution and state sync operations
3. **Wide Attack Window**: The race window spans from task abort until the blocking thread completes, potentially hundreds of milliseconds
4. **Multiple Vulnerable Paths**: Both `pre_commit_block()` and `commit_ledger()` use `.expect()` and are vulnerable
5. **Observable in Production**: Network conditions, load variations, and hardware differences increase timing variability

The vulnerability is deterministic given the right timing - it's not a probabilistic bug but a guaranteed panic when the race condition occurs.

## Recommendation

**Immediate Fix**: Ensure `spawn_blocking` operations complete before calling `finish()`:

```rust
// In consensus/consensus-types/src/pipelined_block.rs
impl PipelineFutures {
    pub async fn wait_until_finishes(self) {
        // Wait for all futures INCLUDING their spawn_blocking threads
        let (execute, ledger, pre_commit, commit, notify) = join5(
            self.execute_fut,
            self.ledger_update_fut,
            self.pre_commit_fut,
            self.commit_ledger_fut,
            self.notify_state_sync_fut,
        ).await;
        
        // Give time for spawn_blocking threads to observe abort and exit
        tokio::time::sleep(Duration::from_millis(100)).await;
    }
}
```

**Better Fix**: Replace `.expect()` with proper error handling in BlockExecutor:

```rust
// In execution/executor/src/block_executor/mod.rs
fn pre_commit_block(&self, block_id: HashValue) -> ExecutorResult<()> {
    let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "pre_commit_block"]);
    
    self.inner
        .read()
        .as_ref()
        .ok_or_else(|| ExecutorError::InternalError {
            error: "BlockExecutor was reset during operation".into(),
        })?
        .pre_commit_block(block_id)
}
```

**Comprehensive Fix**: Use a proper synchronization mechanism with reference counting to track active operations and block `finish()` until all operations complete.

## Proof of Concept

```rust
#[tokio::test]
async fn test_race_condition_finish_reset() {
    // Setup executor and state computer
    let executor = Arc::new(BlockExecutor::new(db));
    let state_computer = ExecutionProxy::new(executor.clone(), ...);
    
    // Start pre-commit in background (simulating pipeline)
    let executor_clone = executor.clone();
    let handle = tokio::spawn(async move {
        tokio::task::spawn_blocking(move || {
            // Simulate slow pre-commit operation
            std::thread::sleep(Duration::from_millis(200));
            executor_clone.pre_commit_block(block_id) // Will panic if finish() was called
        }).await
    });
    
    // Give spawn_blocking time to start
    tokio::time::sleep(Duration::from_millis(50)).await;
    
    // Simulate state sync aborting and calling finish
    handle.abort();
    executor.finish(); // Destroys inner state
    
    // The spawn_blocking thread is still running and will panic
    // Expected: Node crash with "BlockExecutor is not reset" panic
    // Actual: Panic occurs in production during state sync
}
```

To reproduce in production:
1. Trigger state sync while blocks are in pre-commit/commit phase
2. Monitor for panics with message "BlockExecutor is not reset"
3. Observe validator node crashes during high load or network instability

## Notes

This is a time-of-check-time-of-use (TOCTOU) race condition specific to tokio's `spawn_blocking` behavior where abort doesn't stop the blocking thread. The vulnerability is latent but deterministic - it will occur whenever the timing aligns during concurrent state sync and pipeline execution.

### Citations

**File:** consensus/src/state_computer.rs (L141-167)
```rust
        self.executor.finish();

        // Inject an error for fail point testing
        fail_point!("consensus::sync_for_duration", |_| {
            Err(anyhow::anyhow!("Injected error in sync_for_duration").into())
        });

        // Invoke state sync to synchronize for the specified duration. Here, the
        // ChunkExecutor will process chunks and commit to storage. However, after
        // block execution and commits, the internal state of the ChunkExecutor may
        // not be up to date. So, it is required to reset the cache of the
        // ChunkExecutor in state sync when requested to sync.
        let result = monitor!(
            "sync_for_duration",
            self.state_sync_notifier.sync_for_duration(duration).await
        );

        // Update the latest logical time
        if let Ok(latest_synced_ledger_info) = &result {
            let ledger_info = latest_synced_ledger_info.ledger_info();
            let synced_logical_time = LogicalTime::new(ledger_info.epoch(), ledger_info.round());
            *latest_logical_time = synced_logical_time;
        }

        // Similarly, after state synchronization, we have to reset the cache of
        // the BlockExecutor to guarantee the latest committed state is up to date.
        self.executor.reset()?;
```

**File:** execution/executor/src/block_executor/mod.rs (L131-155)
```rust
    fn pre_commit_block(&self, block_id: HashValue) -> ExecutorResult<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "pre_commit_block"]);

        self.inner
            .read()
            .as_ref()
            .expect("BlockExecutor is not reset")
            .pre_commit_block(block_id)
    }

    fn commit_ledger(&self, ledger_info_with_sigs: LedgerInfoWithSignatures) -> ExecutorResult<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "commit_ledger"]);

        self.inner
            .read()
            .as_ref()
            .expect("BlockExecutor is not reset")
            .commit_ledger(ledger_info_with_sigs)
    }

    fn finish(&self) {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "finish"]);

        *self.inner.write() = None;
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L1067-1073)
```rust
        tokio::task::spawn_blocking(move || {
            executor
                .pre_commit_block(block.id())
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
```

**File:** consensus/src/block_storage/sync_manager.rs (L507-510)
```rust
            monitor!(
                "abort_pipeline_for_state_sync",
                block_store.abort_pipeline_for_state_sync().await
            );
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L104-113)
```rust
    pub async fn wait_until_finishes(self) {
        let _ = join5(
            self.execute_fut,
            self.ledger_update_fut,
            self.pre_commit_fut,
            self.commit_ledger_fut,
            self.notify_state_sync_fut,
        )
        .await;
    }
```
