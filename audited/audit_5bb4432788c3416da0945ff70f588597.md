# Audit Report

## Title
Non-Exhaustive Match Statement Causes Silent Dropping of Quorum Store V2 Messages

## Summary
The `ConsensusMsg` enum includes V2 variants (`BatchMsgV2`, `SignedBatchInfoMsgV2`, `ProofOfStoreMsgV2`) that are sent when the `enable_batch_v2` configuration flag is enabled. However, the message routing logic in the network layer uses a non-exhaustive match statement with a catch-all pattern that silently drops these V2 messages instead of processing them, breaking quorum store functionality and causing consensus performance degradation.

## Finding Description

The `ConsensusMsg` enum defines V2 variants for quorum store messages: [1](#0-0) 

When `enable_batch_v2` is enabled in the configuration, validators send these V2 messages via broadcast: [2](#0-1) [3](#0-2) [4](#0-3) 

However, the receiving side in the network layer has a non-exhaustive match statement that only handles V1 quorum store messages: [5](#0-4) 

The match statement explicitly handles only the V1 variants (`BatchMsg`, `SignedBatchInfo`, `ProofOfStoreMsg`) at lines 823-830. The V2 variants (`BatchMsgV2`, `SignedBatchInfoMsgV2`, `ProofOfStoreMsgV2`) fall through to the catch-all pattern `_` at line 937, which logs a warning and drops the message without processing it.

**Attack Scenario:**
1. A validator operator enables `enable_batch_v2` in their quorum store configuration
2. The validator begins broadcasting `BatchMsgV2`, `SignedBatchInfoMsgV2`, and `ProofOfStoreMsgV2` messages
3. Peer validators receive these V2 messages over the network
4. The receiving validators' network layer matches against the message type
5. V2 messages don't match any explicit pattern and hit the catch-all `_` branch
6. Messages are logged as "Unexpected direct send msg" and silently dropped
7. Quorum store protocol fails because batch messages, signed batch info, and proof of store messages are not propagated
8. Consensus performance degrades as the quorum store cannot function properly
9. If multiple validators enable V2 while others don't, network behavior becomes inconsistent

## Impact Explanation

This vulnerability has **High Severity** impact according to Aptos bug bounty criteria:

1. **Validator Node Slowdowns**: The quorum store is critical for batching transactions efficiently. When V2 messages are dropped, validators cannot properly coordinate batch processing, leading to performance degradation.

2. **Significant Protocol Violations**: The quorum store protocol requires reliable message delivery for:
   - Batch messages (transaction batches being shared)
   - Signed batch info (votes/acknowledgments on batches)
   - Proof of store messages (aggregated signatures proving batch availability)
   
   Dropping these messages breaks the protocol's correctness guarantees.

3. **Consensus Liveness Impact**: While not a complete liveness failure, the inability to process batches efficiently can significantly slow down block production and transaction throughput.

4. **Inconsistent Network Behavior**: Validators with V2 enabled will behave differently than those without it, creating a two-tier network where some validators' messages are ignored by others. This violates the deterministic execution invariant.

The configuration flag defaults to `false`: [6](#0-5) 

However, operators could enable it expecting improved functionality, not realizing their messages will be dropped by peers.

## Likelihood Explanation

**Likelihood: Medium to High**

1. **Ease of Trigger**: Any validator operator can enable the `enable_batch_v2` flag in their configuration file, making this trivially exploitable.

2. **Silent Failure**: The bug manifests as silent message drops with only a warning log, making it difficult to detect and diagnose.

3. **Latent Bug**: The code was clearly intended to support V2 messages (the From trait handles them, and sending code exists), but the critical routing logic was not updated, suggesting this is an incomplete migration.

4. **Production Impact**: If this flag is enabled in production (e.g., during a rollout of the V2 feature), it will immediately break quorum store functionality for those validators.

5. **No Attacker Required**: This is not an attack per se, but a logic bug that causes protocol violations whenever the feature flag is enabled.

## Recommendation

Update the network message routing logic to handle V2 quorum store messages. The fix should add the V2 variants to the quorum store message pattern:

```rust
// In consensus/src/network.rs, line 823-830
quorum_store_msg @ (ConsensusMsg::SignedBatchInfo(_)
| ConsensusMsg::BatchMsg(_)
| ConsensusMsg::ProofOfStoreMsg(_)
| ConsensusMsg::SignedBatchInfoMsgV2(_)  // ADD THIS
| ConsensusMsg::BatchMsgV2(_)  // ADD THIS
| ConsensusMsg::ProofOfStoreMsgV2(_)) => {  // ADD THIS
    Self::push_msg(
        peer_id,
        quorum_store_msg,
        &self.quorum_store_messages_tx,
    );
},
```

Additionally, to prevent similar issues in the future:

1. **Remove catch-all patterns**: Replace `_` patterns with explicit exhaustive matching to force compiler errors when new variants are added.

2. **Add compile-time checks**: Consider using `#[deny(non_exhaustive_omitted_patterns)]` or similar lints to enforce exhaustive matching.

3. **Add integration tests**: Test message routing with V2 messages enabled to catch this type of regression.

## Proof of Concept

```rust
// Reproduction steps:
// 1. Enable enable_batch_v2 in QuorumStoreConfig on validator A
// 2. Validator A sends BatchMsgV2/SignedBatchInfoMsgV2/ProofOfStoreMsgV2
// 3. Validator B receives the message
// 4. Observe in Validator B logs: "Unexpected direct send msg" warning
// 5. Verify message is not forwarded to quorum_store_messages_tx channel
// 6. Observe quorum store coordination failures and performance degradation

// To verify the bug exists:
// grep "Unexpected direct send msg" in validator logs when V2 is enabled
// Check metrics: CONSENSUS_RECEIVED_MSGS counter increments but messages not processed
// Monitor quorum store batch formation - it will fail or be severely delayed
```

**Notes:**
- This is a type safety violation where non-exhaustive match statements allow new enum variants to be silently ignored
- The vulnerability exists in the gap between the sending code (which supports V2) and receiving code (which doesn't)
- The fix is straightforward but critical for V2 feature deployment
- Similar patterns should be audited throughout the codebase to prevent recurrence

### Citations

**File:** consensus/src/network_interface.rs (L97-102)
```rust
    BatchMsgV2(Box<BatchMsg<BatchInfoExt>>),
    /// Quorum Store: Send a signed batch digest with BatchInfoExt. This is a vote for the batch and a promise that
    /// the batch of transactions was received and will be persisted until batch expiration.
    SignedBatchInfoMsgV2(Box<SignedBatchInfoMsg<BatchInfoExt>>),
    /// Quorum Store: Broadcast a certified proof of store (a digest that received 2f+1 votes) with BatchInfoExt.
    ProofOfStoreMsgV2(Box<ProofOfStoreMsg<BatchInfoExt>>),
```

**File:** consensus/src/quorum_store/batch_generator.rs (L494-501)
```rust
                            if self.config.enable_batch_v2 {
                                network_sender.broadcast_batch_msg_v2(batches).await;
                            } else {
                                let batches = batches.into_iter().map(|batch| {
                                    batch.try_into().expect("Cannot send V2 batch with flag disabled")
                                }).collect();
                                network_sender.broadcast_batch_msg(batches).await;
                            }
```

**File:** consensus/src/quorum_store/batch_coordinator.rs (L102-110)
```rust
            if persist_requests[0].batch_info().is_v2() {
                let signed_batch_infos = batch_store.persist(persist_requests);
                if !signed_batch_infos.is_empty() {
                    if approx_created_ts_usecs > 0 {
                        observe_batch(approx_created_ts_usecs, peer_id, BatchStage::SIGNED);
                    }
                    network_sender
                        .send_signed_batch_info_msg_v2(signed_batch_infos, vec![peer_id])
                        .await;
```

**File:** consensus/src/quorum_store/proof_coordinator.rs (L485-494)
```rust
                                    if proofs_iter.peek().is_some_and(|p| p.info().is_v2()) {
                                        let proofs: Vec<_> = proofs_iter.collect();
                                        network_sender.broadcast_proof_of_store_msg_v2(proofs).await;
                                    } else {
                                        let proofs: Vec<_> = proofs_iter.map(|proof| {
                                            let (info, sig) = proof.unpack();
                                            ProofOfStore::new(info.info().clone(), sig)
                                        }).collect();
                                        network_sender.broadcast_proof_of_store_msg(proofs).await;
                                    }
```

**File:** consensus/src/network.rs (L822-941)
```rust
                    match msg {
                        quorum_store_msg @ (ConsensusMsg::SignedBatchInfo(_)
                        | ConsensusMsg::BatchMsg(_)
                        | ConsensusMsg::ProofOfStoreMsg(_)) => {
                            Self::push_msg(
                                peer_id,
                                quorum_store_msg,
                                &self.quorum_store_messages_tx,
                            );
                        },
                        // Remove after migration to use rpc.
                        ConsensusMsg::CommitVoteMsg(commit_vote) => {
                            let (tx, _rx) = oneshot::channel();
                            let req_with_callback =
                                IncomingRpcRequest::CommitRequest(IncomingCommitRequest {
                                    req: CommitMessage::Vote(*commit_vote),
                                    protocol: RPC[0],
                                    response_sender: tx,
                                });
                            if let Err(e) = self.rpc_tx.push(
                                (peer_id, discriminant(&req_with_callback)),
                                (peer_id, req_with_callback),
                            ) {
                                warn!(error = ?e, "aptos channel closed");
                            };
                        },
                        ConsensusMsg::CommitDecisionMsg(commit_decision) => {
                            let (tx, _rx) = oneshot::channel();
                            let req_with_callback =
                                IncomingRpcRequest::CommitRequest(IncomingCommitRequest {
                                    req: CommitMessage::Decision(*commit_decision),
                                    protocol: RPC[0],
                                    response_sender: tx,
                                });
                            if let Err(e) = self.rpc_tx.push(
                                (peer_id, discriminant(&req_with_callback)),
                                (peer_id, req_with_callback),
                            ) {
                                warn!(error = ?e, "aptos channel closed");
                            };
                        },
                        consensus_msg @ (ConsensusMsg::ProposalMsg(_)
                        | ConsensusMsg::OptProposalMsg(_)
                        | ConsensusMsg::VoteMsg(_)
                        | ConsensusMsg::RoundTimeoutMsg(_)
                        | ConsensusMsg::OrderVoteMsg(_)
                        | ConsensusMsg::SyncInfo(_)
                        | ConsensusMsg::EpochRetrievalRequest(_)
                        | ConsensusMsg::EpochChangeProof(_)) => {
                            if let ConsensusMsg::ProposalMsg(proposal) = &consensus_msg {
                                observe_block(
                                    proposal.proposal().timestamp_usecs(),
                                    BlockStage::NETWORK_RECEIVED,
                                );
                                info!(
                                    LogSchema::new(LogEvent::NetworkReceiveProposal)
                                        .remote_peer(peer_id),
                                    block_round = proposal.proposal().round(),
                                    block_hash = proposal.proposal().id(),
                                );
                            }
                            if let ConsensusMsg::OptProposalMsg(proposal) = &consensus_msg {
                                observe_block(
                                    proposal.timestamp_usecs(),
                                    BlockStage::NETWORK_RECEIVED,
                                );
                                observe_block(
                                    proposal.timestamp_usecs(),
                                    BlockStage::NETWORK_RECEIVED_OPT_PROPOSAL,
                                );
                                info!(
                                    LogSchema::new(LogEvent::NetworkReceiveOptProposal)
                                        .remote_peer(peer_id),
                                    block_author = proposal.proposer(),
                                    block_epoch = proposal.epoch(),
                                    block_round = proposal.round(),
                                );
                            }
                            Self::push_msg(peer_id, consensus_msg, &self.consensus_messages_tx);
                        },
                        // TODO: get rid of the rpc dummy value
                        ConsensusMsg::RandGenMessage(req) => {
                            let (tx, _rx) = oneshot::channel();
                            let req_with_callback =
                                IncomingRpcRequest::RandGenRequest(IncomingRandGenRequest {
                                    req,
                                    sender: peer_id,
                                    protocol: RPC[0],
                                    response_sender: tx,
                                });
                            if let Err(e) = self.rpc_tx.push(
                                (peer_id, discriminant(&req_with_callback)),
                                (peer_id, req_with_callback),
                            ) {
                                warn!(error = ?e, "aptos channel closed");
                            };
                        },
                        // TODO: get rid of the rpc dummy value
                        ConsensusMsg::SecretShareMsg(req) => {
                            let (tx, _rx) = oneshot::channel();
                            let req_with_callback = IncomingRpcRequest::SecretShareRequest(
                                IncomingSecretShareRequest {
                                    req,
                                    sender: peer_id,
                                    protocol: RPC[0],
                                    response_sender: tx,
                                },
                            );
                            if let Err(e) = self.rpc_tx.push(
                                (peer_id, discriminant(&req_with_callback)),
                                (peer_id, req_with_callback),
                            ) {
                                warn!(error = ?e, "aptos channel closed");
                            };
                        },
                        _ => {
                            warn!(remote_peer = peer_id, "Unexpected direct send msg");
                            continue;
                        },
                    }
```

**File:** config/src/config/quorum_store_config.rs (L144-144)
```rust
            enable_batch_v2: false,
```
