# Audit Report

## Title
SignedBatchInfo Replay Attack: Missing Batch Expiration and Commitment State Validation

## Summary
The `SignedBatchInfo::verify()` function fails to validate whether a batch has expired or been previously committed, allowing attackers to replay old signed batch messages indefinitely within a ~60-second window. This enables creation of duplicate `ProofOfStore` messages for already-committed batches, wasting network bandwidth, computational resources, and block space.

## Finding Description

The quorum store system has a critical replay vulnerability in its batch signature aggregation flow. When validators sign batch digests, the `SignedBatchInfo::verify()` function performs insufficient validation: [1](#0-0) 

The verification only checks:
1. Sender matches signer
2. Expiration is not too far in the **future** (line 469-479)
3. Signature is cryptographically valid

**Critical Omission:** It does NOT check:
- Whether the batch has already expired (expiration in the **past**)
- Whether the batch has been committed to the blockchain
- Whether a ProofOfStore was already created for this batch_id

**Attack Flow:**

1. **Normal Operation:** Batch X created at T0 with expiration T0+10s, gets signed, ProofOfStore P1 created, committed in block at T1.

2. **BatchProofQueue Cleanup:** At T0+10s, batch X expires and is removed from `BatchProofQueue.items`: [2](#0-1) 

3. **ProofCoordinator Timeout:** After proof_timeout_ms, the completed proof state is removed from `batch_info_to_proof`: [3](#0-2) 

4. **Batch Still in Storage:** However, batch X remains in `BatchStore.db_cache` for an additional `expiration_buffer_usecs` (~60 seconds): [4](#0-3) 

5. **Replay Attack Window:** Between T0+10s and T0+70s, attacker replays old `SignedBatchInfo` messages for batch X.

6. **ProofCoordinator Accepts Replay:** Since `batch_info_to_proof` no longer contains batch X (timed out), `init_proof` is called: [5](#0-4) 

The `batch_reader.exists()` check passes because batch X is still in `db_cache`: [6](#0-5) 

7. **Duplicate ProofOfStore Created:** A NEW `IncrementalProofState` is created for the already-committed batch X. When quorum signatures are collected, a duplicate ProofOfStore P2 is broadcast.

8. **BatchProofQueue Accepts Duplicate:** When P2 arrives at `BatchProofQueue::insert_proof`, the duplicate detection fails because batch X was already removed from `items`: [7](#0-6) 

The check `items.get(&batch_key)` returns `None` (batch expired), so the duplicate is not detected and P2 is inserted as a new proof.

## Impact Explanation

This is **High Severity** per Aptos bug bounty criteria:

**Validator Node Slowdowns:** Validators waste computational resources:
- Verifying replayed signatures multiple times
- Aggregating duplicate proofs
- Broadcasting duplicate ProofOfStore messages across the network
- Processing duplicate proofs in block proposals

**Network Bandwidth Waste:** Duplicate ProofOfStore messages (containing multi-signatures from 2f+1 validators) are broadcast repeatedly, consuming significant bandwidth.

**Block Space Waste:** Duplicate proofs may be included in block proposals, reducing space available for legitimate new transactions.

**Potential DoS Vector:** An attacker can continuously replay signed batch messages from multiple expired batches simultaneously, flooding the network with duplicate proofs and degrading consensus performance.

While transaction-level deduplication prevents double-execution: [8](#0-7) 

The proofs themselves still waste resources throughout the consensus pipeline before reaching execution.

## Likelihood Explanation

**High Likelihood:**

1. **Wide Attack Window:** ~60 second window (from batch expiration until db_cache cleanup) provides ample opportunity for replay attacks.

2. **No Authentication Required:** Any network peer can replay previously observed `SignedBatchInfo` messages - no cryptographic material needed as signatures are already valid.

3. **Batch Lifecycle Guarantees Window Exists:** The system intentionally keeps batches in storage after expiration to help slow nodes catch up, creating a structural attack window.

4. **Continuous Attack Vector:** Attacker can replay messages for every batch that enters the vulnerable window, creating sustained resource exhaustion.

5. **Network Observability:** SignedBatchInfo messages are broadcast over the network and easily observable/stored by any peer for later replay.

## Recommendation

**Primary Fix:** Add expiration validation to `SignedBatchInfo::verify()`:

```rust
pub fn verify(
    &self,
    sender: PeerId,
    max_batch_expiry_gap_usecs: u64,
    validator: &ValidatorVerifier,
) -> anyhow::Result<()> {
    if sender != self.signer {
        bail!("Sender {} mismatch signer {}", sender, self.signer);
    }

    let current_time = aptos_infallible::duration_since_epoch().as_micros() as u64;
    
    // NEW: Check if batch has already expired
    if self.expiration() <= current_time {
        bail!(
            "Batch already expired: {} <= {}",
            self.expiration(),
            current_time
        );
    }

    if self.expiration() > current_time + max_batch_expiry_gap_usecs {
        bail!(
            "Batch expiration too far in future: {} > {}",
            self.expiration(),
            current_time + max_batch_expiry_gap_usecs
        );
    }

    Ok(validator.optimistic_verify(self.signer, &self.info, &self.signature)?)
}
```

**Secondary Fix:** Add commitment state tracking in `ProofCoordinator::init_proof()`:

```rust
fn init_proof(
    &mut self,
    signed_batch_info: &SignedBatchInfo<BatchInfoExt>,
) -> Result<(), SignedBatchInfoError> {
    if signed_batch_info.author() != self.peer_id {
        return Err(SignedBatchInfoError::WrongAuthor);
    }
    
    // NEW: Check if batch was already committed
    if self.committed_batches.contains(signed_batch_info.digest()) {
        return Err(SignedBatchInfoError::AlreadyCommitted);
    }
    
    let batch_author = self
        .batch_reader
        .exists(signed_batch_info.digest())
        .ok_or(SignedBatchInfoError::NotFound)?;
    if batch_author != signed_batch_info.author() {
        return Err(SignedBatchInfoError::WrongAuthor);
    }

    // ... rest of init_proof
}
```

And maintain `committed_batches` set via `CommitNotification` handler, with periodic cleanup based on expiration times.

## Proof of Concept

```rust
// Integration test demonstrating the replay attack
#[tokio::test]
async fn test_signed_batch_info_replay_attack() {
    // Setup: Create validator set and batch store
    let validators = ValidatorSet::new(vec![validator1, validator2, validator3, validator4]);
    let batch_store = Arc::new(BatchStore::new(...));
    let mut proof_coordinator = ProofCoordinator::new(...);
    
    // Step 1: Create and commit a batch normally
    let batch_info = BatchInfo::new(
        author,
        BatchId::new(1),
        epoch,
        expiration_time,
        digest,
        100,
        1000,
        0,
    );
    
    // Step 2: Collect signatures and create ProofOfStore
    let signed_infos: Vec<_> = validators.iter()
        .map(|v| SignedBatchInfo::new(batch_info.clone(), v.signer()))
        .collect();
    
    for signed_info in &signed_infos {
        proof_coordinator.add_signature(signed_info.clone(), &validators.verifier())
            .expect("First signature aggregation succeeds");
    }
    
    // Verify ProofOfStore was created
    assert!(proof_created);
    
    // Step 3: Simulate batch commitment and expiration
    proof_coordinator.handle_commit_notification(vec![batch_info.clone()]);
    tokio::time::advance(Duration::from_secs(11)).await; // Batch expires
    proof_coordinator.expire().await; // Proof times out
    
    // Step 4: Verify batch is removed from proof coordinator but still in batch store
    assert!(!proof_coordinator.batch_info_to_proof.contains_key(&batch_info));
    assert!(batch_store.db_cache.contains_key(batch_info.digest()));
    
    // Step 5: REPLAY ATTACK - Send same SignedBatchInfo messages again
    for signed_info in &signed_infos {
        // Verify accepts expired batch (vulnerability!)
        signed_info.verify(author, max_expiry_gap, &validators.verifier())
            .expect("VULNERABILITY: Expired batch signature accepted");
        
        // Proof coordinator accepts replay
        proof_coordinator.add_signature(signed_info.clone(), &validators.verifier())
            .expect("VULNERABILITY: Replay creates duplicate proof state");
    }
    
    // Step 6: Verify duplicate ProofOfStore was created
    assert!(duplicate_proof_created);
    
    // Impact: Duplicate proof broadcasts, wastes resources, consumes block space
}
```

**Expected Behavior:** Expired batch replays should be rejected.

**Actual Behavior:** System accepts replayed signatures for expired, already-committed batches, creating duplicate proofs and wasting resources.

### Citations

**File:** consensus/consensus-types/src/proof_of_store.rs (L459-482)
```rust
    pub fn verify(
        &self,
        sender: PeerId,
        max_batch_expiry_gap_usecs: u64,
        validator: &ValidatorVerifier,
    ) -> anyhow::Result<()> {
        if sender != self.signer {
            bail!("Sender {} mismatch signer {}", sender, self.signer);
        }

        if self.expiration()
            > aptos_infallible::duration_since_epoch().as_micros() as u64
                + max_batch_expiry_gap_usecs
        {
            bail!(
                "Batch expiration too far in future: {} > {}",
                self.expiration(),
                aptos_infallible::duration_since_epoch().as_micros() as u64
                    + max_batch_expiry_gap_usecs
            );
        }

        Ok(validator.optimistic_verify(self.signer, &self.info, &self.signature)?)
    }
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L175-188)
```rust
    pub(crate) fn insert_proof(&mut self, proof: ProofOfStore<BatchInfoExt>) {
        if proof.expiration() <= self.latest_block_timestamp {
            counters::inc_rejected_pos_count(counters::POS_EXPIRED_LABEL);
            return;
        }
        let batch_key = BatchKey::from_info(proof.info());
        if self
            .items
            .get(&batch_key)
            .is_some_and(|item| item.proof.is_some() || item.is_committed())
        {
            counters::inc_rejected_pos_count(counters::POS_DUPLICATE_LABEL);
            return;
        }
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L730-760)
```rust
        let mut num_expired_but_not_committed = 0;
        for key in &expired {
            if let Some(mut queue) = self.author_to_batches.remove(&key.author()) {
                if let Some(batch) = queue.remove(key) {
                    let item = self
                        .items
                        .get(&key.batch_key)
                        .expect("Entry for unexpired batch must exist");
                    if item.proof.is_some() {
                        // not committed proof that is expired
                        num_expired_but_not_committed += 1;
                        counters::GAP_BETWEEN_BATCH_EXPIRATION_AND_CURRENT_TIME_WHEN_COMMIT
                            .observe((block_timestamp - batch.expiration()) as f64);
                        if let Some(ref txn_summaries) = item.txn_summaries {
                            for txn_summary in txn_summaries {
                                if let Some(count) =
                                    self.txn_summary_num_occurrences.get_mut(txn_summary)
                                {
                                    *count -= 1;
                                    if *count == 0 {
                                        self.txn_summary_num_occurrences.remove(txn_summary);
                                    }
                                };
                            }
                        }
                        self.dec_remaining_proofs(&batch.author(), batch.num_txns());
                        counters::GARBAGE_COLLECTED_IN_PROOF_QUEUE_COUNTER
                            .with_label_values(&["expired_proof"])
                            .inc();
                    }
                    claims::assert_some!(self.items.remove(&key.batch_key));
```

**File:** consensus/src/quorum_store/proof_coordinator.rs (L269-311)
```rust
    fn init_proof(
        &mut self,
        signed_batch_info: &SignedBatchInfo<BatchInfoExt>,
    ) -> Result<(), SignedBatchInfoError> {
        // Check if the signed digest corresponding to our batch
        if signed_batch_info.author() != self.peer_id {
            return Err(SignedBatchInfoError::WrongAuthor);
        }
        let batch_author = self
            .batch_reader
            .exists(signed_batch_info.digest())
            .ok_or(SignedBatchInfoError::NotFound)?;
        if batch_author != signed_batch_info.author() {
            return Err(SignedBatchInfoError::WrongAuthor);
        }

        self.timeouts.add(
            signed_batch_info.batch_info().clone(),
            self.proof_timeout_ms,
        );
        if signed_batch_info.batch_info().is_v2() {
            self.batch_info_to_proof.insert(
                signed_batch_info.batch_info().clone(),
                IncrementalProofState::new_batch_info_ext(signed_batch_info.batch_info().clone()),
            );
        } else {
            self.batch_info_to_proof.insert(
                signed_batch_info.batch_info().clone(),
                IncrementalProofState::new_batch_info(
                    signed_batch_info.batch_info().info().clone(),
                ),
            );
        }
        self.batch_info_to_time
            .entry(signed_batch_info.batch_info().clone())
            .or_insert(Instant::now());
        debug!(
            LogSchema::new(LogEvent::ProofOfStoreInit),
            digest = signed_batch_info.digest(),
            batch_id = signed_batch_info.batch_id().id,
        );
        Ok(())
    }
```

**File:** consensus/src/quorum_store/proof_coordinator.rs (L369-402)
```rust
    async fn expire(&mut self) {
        let mut batch_ids = vec![];
        for signed_batch_info_info in self.timeouts.expire() {
            if let Some(state) = self.batch_info_to_proof.remove(&signed_batch_info_info) {
                if !state.completed {
                    batch_ids.push(signed_batch_info_info.batch_id());
                }
                Self::update_counters_on_expire(&state);

                // We skip metrics if the proof did not complete and did not get a self vote, as it
                // is considered a proof that was re-inited due to a very late vote.
                if !state.completed && !state.self_voted {
                    continue;
                }

                if !state.completed {
                    counters::TIMEOUT_BATCHES_COUNT.inc();
                    info!(
                        LogSchema::new(LogEvent::IncrementalProofExpired),
                        digest = signed_batch_info_info.digest(),
                        self_voted = state.self_voted,
                    );
                }
            }
        }
        if self
            .batch_generator_cmd_tx
            .send(BatchGeneratorCommand::ProofExpiration(batch_ids))
            .await
            .is_err()
        {
            warn!("Failed to send proof expiration to batch generator");
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L443-472)
```rust
    pub(crate) fn clear_expired_payload(&self, certified_time: u64) -> Vec<HashValue> {
        // To help slow nodes catch up via execution without going to state sync we keep the blocks for 60 extra seconds
        // after the expiration time. This will help remote peers fetch batches that just expired but are within their
        // execution window.
        let expiration_time = certified_time.saturating_sub(self.expiration_buffer_usecs);
        let expired_digests = self.expirations.lock().expire(expiration_time);
        let mut ret = Vec::new();
        for h in expired_digests {
            let removed_value = match self.db_cache.entry(h) {
                Occupied(entry) => {
                    // We need to check up-to-date expiration again because receiving the same
                    // digest with a higher expiration would update the persisted value and
                    // effectively extend the expiration.
                    if entry.get().expiration() <= expiration_time {
                        self.persist_subscribers.remove(entry.get().digest());
                        Some(entry.remove())
                    } else {
                        None
                    }
                },
                Vacant(_) => unreachable!("Expired entry not in cache"),
            };
            // No longer holding the lock on db_cache entry.
            if let Some(value) = removed_value {
                self.free_quota(value);
                ret.push(h);
            }
        }
        ret
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L727-732)
```rust
    fn exists(&self, digest: &HashValue) -> Option<PeerId> {
        self.batch_store
            .get_batch_from_local(digest)
            .map(|v| v.author())
            .ok()
    }
```

**File:** consensus/src/txn_hash_and_authenticator_deduper.rs (L38-94)
```rust
impl TransactionDeduper for TxnHashAndAuthenticatorDeduper {
    fn dedup(&self, transactions: Vec<SignedTransaction>) -> Vec<SignedTransaction> {
        let _timer = TXN_DEDUP_SECONDS.start_timer();
        let mut seen = HashMap::new();
        let mut is_possible_duplicate = false;
        let mut possible_duplicates = vec![false; transactions.len()];
        for (i, txn) in transactions.iter().enumerate() {
            match seen.get(&(txn.sender(), txn.replay_protector())) {
                None => {
                    seen.insert((txn.sender(), txn.replay_protector()), i);
                },
                Some(first_index) => {
                    is_possible_duplicate = true;
                    possible_duplicates[*first_index] = true;
                    possible_duplicates[i] = true;
                },
            }
        }
        if !is_possible_duplicate {
            TXN_DEDUP_FILTERED.observe(0 as f64);
            return transactions;
        }

        let num_txns = transactions.len();

        let hash_and_authenticators: Vec<_> = possible_duplicates
            .into_par_iter()
            .zip(&transactions)
            .with_min_len(optimal_min_len(num_txns, 48))
            .map(|(need_hash, txn)| match need_hash {
                true => Some((txn.committed_hash(), txn.authenticator())),
                false => None,
            })
            .collect();

        // TODO: Possibly parallelize. See struct comment.
        let mut seen_hashes = HashSet::new();
        let mut num_duplicates: usize = 0;
        let filtered: Vec<_> = hash_and_authenticators
            .into_iter()
            .zip(transactions)
            .filter_map(|(maybe_hash, txn)| match maybe_hash {
                None => Some(txn),
                Some(hash_and_authenticator) => {
                    if seen_hashes.insert(hash_and_authenticator) {
                        Some(txn)
                    } else {
                        num_duplicates += 1;
                        None
                    }
                },
            })
            .collect();

        TXN_DEDUP_FILTERED.observe(num_duplicates as f64);
        filtered
    }
```
