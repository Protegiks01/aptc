# Audit Report

## Title
Unbounded Queue Growth in Randomness Generation Allows Resource Exhaustion Attack

## Summary
The `BlockQueue` in the randomness generation system has no maximum size limit, allowing Byzantine validators to cause memory exhaustion on honest nodes by delaying randomness share propagation while consensus continues ordering blocks at normal rate.

## Finding Description

The randomness generation system maintains a queue of blocks waiting for randomness decisions. This queue is implemented as an unbounded `BTreeMap` with no maximum size enforcement. [1](#0-0) 

Blocks enter this queue when consensus orders them and sends them to the RandManager: [2](#0-1) 

The `push_back` method adds items without any size validation: [3](#0-2) 

Blocks are only removed from the queue when randomness has been successfully generated for all blocks in a `QueueItem`: [4](#0-3) 

Randomness generation requires collecting shares from validators with at least 2/3+ total weight: [5](#0-4) 

The incoming blocks flow through an unbounded channel from consensus: [6](#0-5) 

**Attack Path:**
1. Byzantine validators (up to 1/3 of total stake weight) strategically delay sending their randomness shares
2. While honest validators generate shares immediately, the 2/3+ threshold takes longer to reach
3. Consensus continues ordering blocks at normal rate (~1 block per second)
4. Each ordered block batch enters the queue via `push_back()`
5. Blocks accumulate faster than randomness can be generated and blocks dequeued
6. The unbounded `BTreeMap` grows continuously, consuming increasing memory
7. Eventually, the node runs out of memory and crashes or becomes severely degraded

The same vulnerability exists in the secret sharing queue: [7](#0-6) 

## Impact Explanation

This vulnerability qualifies as **Medium severity** per the Aptos bug bounty criteria:

- **State inconsistencies requiring intervention**: When nodes crash due to memory exhaustion, the network experiences validator unavailability requiring manual restart/intervention
- **Validator node slowdowns**: As memory pressure increases before crash, node performance degrades significantly
- **Potential consensus impact**: If enough validators crash simultaneously, network liveness could be affected

The impact is limited to Medium (not High/Critical) because:
- It doesn't directly cause fund loss or consensus safety violations
- Byzantine validators cannot exceed their 1/3 threshold limitation
- The attack causes availability issues rather than safety breaks

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability can manifest in two scenarios:

1. **Byzantine Attack (Medium likelihood)**:
   - Requires 1/3 Byzantine validators coordinating to delay shares
   - They remain within protocol rules (not sending invalid shares, just delaying)
   - Detection is difficult as delays could appear as normal network latency
   - Attack can be sustained indefinitely

2. **Natural Occurrence (High likelihood)**:
   - Network partitions or latency spikes naturally delay share propagation
   - High block production rate during peak activity
   - Cryptographic computation overhead for share aggregation
   - No maximum queue size means any sustained delay causes accumulation

The attack doesn't require:
- More than 1/3 Byzantine validators
- Complex cryptographic attacks
- Compromising node software
- Violating consensus rules

## Recommendation

**Immediate Fix**: Implement a maximum queue size limit with proper backpressure signaling:

```rust
pub struct BlockQueue {
    queue: BTreeMap<Round, QueueItem>,
    max_queue_size: usize, // Add configuration parameter
}

impl BlockQueue {
    pub fn new(max_queue_size: usize) -> Self {
        Self {
            queue: BTreeMap::new(),
            max_queue_size,
        }
    }
    
    pub fn push_back(&mut self, item: QueueItem) -> Result<(), QueueFullError> {
        if self.queue.len() >= self.max_queue_size {
            return Err(QueueFullError::new(self.queue.len(), self.max_queue_size));
        }
        
        for block in item.blocks() {
            observe_block(block.timestamp_usecs(), BlockStage::RAND_ENTER);
        }
        assert!(self.queue.insert(item.first_round(), item).is_none());
        Ok(())
    }
}
```

**Additional Recommendations**:

1. Add backpressure mechanism: When queue reaches threshold (e.g., 80% of max), signal consensus to slow down block production
2. Implement queue monitoring alerts: Track `RAND_QUEUE_SIZE` metric and alert operators when approaching limits
3. Add timeout-based cleanup: Remove blocks older than a threshold if randomness generation fails
4. Consider dynamic queue sizing based on available memory
5. Apply same fix to `consensus/src/rand/secret_sharing/block_queue.rs`

**Suggested Configuration**:
- `max_randomness_queue_size`: 1000 blocks (configurable in ConsensusConfig)
- Backpressure threshold: 800 blocks (80%)
- Alert threshold: 700 blocks (70%)

## Proof of Concept

```rust
#[cfg(test)]
mod resource_exhaustion_test {
    use super::*;
    use crate::rand::rand_gen::test_utils::create_ordered_blocks;
    use std::time::Instant;
    
    #[test]
    fn test_unbounded_queue_growth() {
        // This test demonstrates unbounded queue growth
        let mut queue = BlockQueue::new();
        let start_memory = get_current_memory_usage();
        
        // Simulate rapid block ordering without randomness generation
        let num_blocks = 10000; // Simulate many blocks
        for i in 0..num_blocks {
            let rounds = vec![i * 100]; // Each batch at different round
            let item = QueueItem::new(
                create_ordered_blocks(rounds),
                None
            );
            queue.push_back(item);
        }
        
        // Verify queue has grown unbounded
        assert_eq!(queue.queue().len(), num_blocks);
        
        // Measure memory growth
        let end_memory = get_current_memory_usage();
        let memory_growth_mb = (end_memory - start_memory) / (1024 * 1024);
        
        println!("Queue size: {}", queue.queue().len());
        println!("Memory growth: {} MB", memory_growth_mb);
        
        // In a real attack scenario with full block data,
        // this would grow to multiple GB causing OOM
        assert!(memory_growth_mb > 10, "Significant memory growth observed");
    }
    
    #[test]
    fn test_queue_should_have_max_size() {
        // This test documents the expected behavior (currently fails)
        let max_size = 1000;
        let mut queue = BlockQueue::new(); // Should accept max_size parameter
        
        // Try to add more than max_size blocks
        for i in 0..max_size + 100 {
            let rounds = vec![i * 100];
            let item = QueueItem::new(create_ordered_blocks(rounds), None);
            let result = queue.push_back(item); // Should return Result
            
            if i >= max_size {
                // Should fail after reaching max size
                assert!(result.is_err(), "Queue should reject items beyond max size");
            }
        }
    }
    
    fn get_current_memory_usage() -> usize {
        // Platform-specific memory measurement
        #[cfg(target_os = "linux")]
        {
            std::fs::read_to_string("/proc/self/statm")
                .ok()
                .and_then(|s| s.split_whitespace().next().map(|v| v.parse().ok()))
                .flatten()
                .unwrap_or(0) * 4096 // Page size
        }
        #[cfg(not(target_os = "linux"))]
        0
    }
}
```

**Notes**

The vulnerability affects consensus availability and node resource management. The queue monitoring metric `RAND_QUEUE_SIZE` exists for observability but provides no enforcement. [8](#0-7) 

Byzantine validators can exploit this without exceeding their protocol-defined capabilities, making it a realistic attack vector. The same pattern exists in both the randomness generation and secret sharing systems, indicating a systemic design issue that should be addressed comprehensively.

### Citations

**File:** consensus/src/rand/rand_gen/block_queue.rs (L94-102)
```rust
pub struct BlockQueue {
    queue: BTreeMap<Round, QueueItem>,
}
impl BlockQueue {
    pub fn new() -> Self {
        Self {
            queue: BTreeMap::new(),
        }
    }
```

**File:** consensus/src/rand/rand_gen/block_queue.rs (L108-113)
```rust
    pub fn push_back(&mut self, item: QueueItem) {
        for block in item.blocks() {
            observe_block(block.timestamp_usecs(), BlockStage::RAND_ENTER);
        }
        assert!(self.queue.insert(item.first_round(), item).is_none());
    }
```

**File:** consensus/src/rand/rand_gen/block_queue.rs (L118-137)
```rust
    pub fn dequeue_rand_ready_prefix(&mut self) -> Vec<OrderedBlocks> {
        let mut rand_ready_prefix = vec![];
        while let Some((_starting_round, item)) = self.queue.first_key_value() {
            if item.num_undecided() == 0 {
                let (_, item) = self.queue.pop_first().unwrap();
                for block in item.blocks() {
                    observe_block(block.timestamp_usecs(), BlockStage::RAND_READY);
                }
                let QueueItem { ordered_blocks, .. } = item;
                debug_assert!(ordered_blocks
                    .ordered_blocks
                    .iter()
                    .all(|block| block.has_randomness()));
                rand_ready_prefix.push(ordered_blocks);
            } else {
                break;
            }
        }
        rand_ready_prefix
    }
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L132-143)
```rust
    fn process_incoming_blocks(&mut self, blocks: OrderedBlocks) {
        let rounds: Vec<u64> = blocks.ordered_blocks.iter().map(|b| b.round()).collect();
        info!(rounds = rounds, "Processing incoming blocks.");
        let broadcast_handles: Vec<_> = blocks
            .ordered_blocks
            .iter()
            .map(|block| FullRandMetadata::from(block.block()))
            .map(|metadata| self.process_incoming_metadata(metadata))
            .collect();
        let queue_item = QueueItem::new(blocks, Some(broadcast_handles));
        self.block_queue.push_back(queue_item);
    }
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L477-480)
```rust
    pub fn observe_queue(&self) {
        let queue = &self.block_queue.queue();
        RAND_QUEUE_SIZE.set(queue.len() as i64);
    }
```

**File:** consensus/src/rand/rand_gen/rand_store.rs (L41-49)
```rust
    pub fn try_aggregate(
        self,
        rand_config: &RandConfig,
        rand_metadata: FullRandMetadata,
        decision_tx: Sender<Randomness>,
    ) -> Either<Self, RandShare<S>> {
        if self.total_weight < rand_config.threshold() {
            return Either::Left(self);
        }
```

**File:** consensus/src/pipeline/execution_client.rs (L233-234)
```rust
        let (ordered_block_tx, ordered_block_rx) = unbounded::<OrderedBlocks>();
        let (rand_ready_block_tx, rand_ready_block_rx) = unbounded::<OrderedBlocks>();
```

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L89-109)
```rust
pub struct BlockQueue {
    queue: BTreeMap<Round, QueueItem>,
}

impl BlockQueue {
    pub fn new() -> Self {
        Self {
            queue: BTreeMap::new(),
        }
    }

    pub fn queue(&self) -> &BTreeMap<Round, QueueItem> {
        &self.queue
    }

    pub fn push_back(&mut self, item: QueueItem) {
        for block in item.blocks() {
            observe_block(block.timestamp_usecs(), BlockStage::SECRET_SHARING_ENTER);
        }
        assert!(self.queue.insert(item.first_round(), item).is_none());
    }
```
