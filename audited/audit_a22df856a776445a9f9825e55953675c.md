# Audit Report

## Title
BufferedX Fails to Clean Up Partial Backup Files on Cancellation

## Summary
When a backup operation using `BufferedX` is cancelled via CLI (e.g., Ctrl+C), in-flight futures are dropped without cleanup, leaving partial chunk files on storage. There is no Drop implementation or cancellation-aware cleanup mechanism to remove these incomplete files.

## Finding Description

The `BufferedX` stream implementation manages concurrent backup futures through nested structures (`FuturesOrderedX` → `FuturesUnorderedX` → `FuturesUnordered`). [1](#0-0) 

During state snapshot backups, these futures execute `write_chunk` operations that create files, write data, and finalize with shutdown. [2](#0-1) 

The backup operation uses `try_buffered_x` to process 4 chunks concurrently with up to 8 results buffered. [3](#0-2) 

When cancellation occurs, the `Stream::poll_next` implementation simply drops in-progress futures without cleanup. [4](#0-3) 

For LocalFs storage, files are created with `create_new(true)`, ensuring they persist even if the write operation is interrupted. [5](#0-4) 

There is no Drop implementation for BufferedX or cleanup tracking mechanism to identify and remove partial files.

## Impact Explanation

**This is a Low Severity operational issue, NOT a blockchain security vulnerability.**

This issue does NOT meet High severity criteria because:
- It does not cause validator node slowdowns, API crashes, or protocol violations
- It affects the backup/restore operational tooling, not the core blockchain
- No blockchain state inconsistency, consensus impact, or fund loss occurs
- Partial backup files do not interfere with blockchain operations

The impact is limited to:
- Wasted storage space accumulation from repeated cancellations
- Manual cleanup required to remove orphaned chunk files  
- Potential confusion if partial files are mistaken for valid backups
- Increased cloud storage costs in production environments

## Likelihood Explanation

**Likelihood: Medium** - Cancellations occur regularly in operational scenarios:
- Operators interrupting long-running backups due to time constraints
- System shutdowns during scheduled maintenance
- Resource exhaustion causing process termination
- Network issues triggering timeout-based cancellations

However, this requires legitimate operator access to the backup CLI tool, not external attacker exploitation.

## Recommendation

Implement cancellation-aware cleanup using a Drop guard pattern:

1. Track created file handles in a cleanup registry during `write_chunk`
2. Implement Drop for a guard struct that deletes partial files
3. Clear the registry only after successful `shutdown()` completion
4. Use tokio's CancellationToken to propagate cancellation signals
5. Add a cleanup pass at backup start to remove orphaned files from previous runs

Example approach: Wrap file creation in a guard that tracks the handle and deletes on drop unless explicitly disarmed after successful completion.

## Proof of Concept

```rust
// Reproduction steps:
// 1. Start a state snapshot backup: 
//    $ aptos-db-tool backup oneoff state-snapshot --state-snapshot-epoch 100
// 2. Wait until chunks start writing (monitor logs)
// 3. Send SIGINT (Ctrl+C) to cancel
// 4. Check backup directory for partial .chunk files
// 5. Repeat cancellation multiple times
// 6. Observe accumulation of orphaned chunk files with no cleanup
//
// Expected: Partial files should be deleted on cancellation
// Actual: Partial files remain indefinitely, consuming storage space
```

---

**Notes:** While this is a legitimate quality-of-life improvement for the backup system, it does not constitute a blockchain security vulnerability per the strict validation criteria. The issue affects operational tooling reliability rather than blockchain safety, consensus, or state integrity. It should be addressed in routine maintenance rather than as an urgent security fix.

### Citations

**File:** storage/backup/backup-cli/src/utils/stream/buffered_x.rs (L17-28)
```rust
#[pin_project]
#[must_use = "streams do nothing unless polled"]
pub struct BufferedX<St>
where
    St: Stream,
    St::Item: Future,
{
    #[pin]
    stream: Fuse<St>,
    in_progress_queue: FuturesOrderedX<St::Item>,
    max: usize,
}
```

**File:** storage/backup/backup-cli/src/utils/stream/buffered_x.rs (L67-91)
```rust
    fn poll_next(self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Option<Self::Item>> {
        let mut this = self.project();

        // First up, try to spawn off as many futures as possible by filling up
        // our queue of futures.
        while this.in_progress_queue.len() < *this.max {
            match this.stream.as_mut().poll_next(cx) {
                Poll::Ready(Some(fut)) => this.in_progress_queue.push(fut),
                Poll::Ready(None) | Poll::Pending => break,
            }
        }

        // Attempt to pull the next value from the in_progress_queue
        let res = this.in_progress_queue.poll_next_unpin(cx);
        if let Some(val) = ready!(res) {
            return Poll::Ready(Some(val));
        }

        // If more values are still coming from the stream, we're not done yet
        if this.stream.is_done() {
            Poll::Ready(None)
        } else {
            Poll::Pending
        }
    }
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/backup.rs (L253-266)
```rust
        let chunks: Vec<_> = chunk_manifest_fut_stream
            .try_buffered_x(8, 4) // 4 concurrently, at most 8 results in buffer.
            .map_ok(|chunk_manifest| {
                let last_idx = chunk_manifest.last_idx;
                info!(
                    last_idx = last_idx,
                    values_per_second =
                        ((last_idx + 1) as f64 / start.elapsed().as_secs_f64()) as u64,
                    "Chunk written."
                );
                chunk_manifest
            })
            .try_collect()
            .await?;
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/backup.rs (L404-424)
```rust
    async fn write_chunk(
        &self,
        backup_handle: &BackupHandleRef,
        chunk: Chunk,
    ) -> Result<StateSnapshotChunk> {
        let _timer = BACKUP_TIMER.timer_with(&["state_snapshot_write_chunk"]);

        let Chunk {
            bytes,
            first_idx,
            last_idx,
            first_key,
            last_key,
        } = chunk;

        let (chunk_handle, mut chunk_file) = self
            .storage
            .create_for_write(backup_handle, &Self::chunk_name(first_idx))
            .await?;
        chunk_file.write_all(&bytes).await?;
        chunk_file.shutdown().await?;
```

**File:** storage/backup/backup-cli/src/storage/local_fs/mod.rs (L80-96)
```rust
    async fn create_for_write(
        &self,
        backup_handle: &BackupHandleRef,
        name: &ShellSafeName,
    ) -> Result<(FileHandle, Box<dyn AsyncWrite + Send + Unpin>)> {
        let file_handle = Path::new(backup_handle)
            .join(name.as_ref())
            .path_to_string()?;
        let abs_path = self.dir.join(&file_handle).path_to_string()?;
        let file = OpenOptions::new()
            .write(true)
            .create_new(true)
            .open(&abs_path)
            .await
            .err_notes(&abs_path)?;
        Ok((file_handle, Box::new(file)))
    }
```
