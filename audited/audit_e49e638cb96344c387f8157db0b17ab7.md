# Audit Report

## Title
Missing Version Validation in IndexerAsyncV2 Allows Out-of-Order Indexing and Data Integrity Issues

## Summary
The `index_with_annotator()` function in `IndexerAsyncV2` lacks version continuity validation that exists in the original `Indexer` implementation. This allows out-of-order transaction processing, enabling gaps or duplicate entries in the table info index when combined with crash/restart scenarios or parallel processing race conditions.

## Finding Description

The `IndexerAsyncV2.index_with_annotator()` function processes write sets and indexes table information, but critically fails to validate that the `first_version` parameter matches the expected `self.next_version`. [1](#0-0) 

In contrast, the original `Indexer` implementation contains explicit validation to ensure continuous version processing: [2](#0-1) 

This missing validation breaks the **State Consistency** invariant by allowing:

1. **Gap Scenario**: If versions 0-99 are indexed, then versions 200-299 are indexed (skipping 100-199), the system accepts both without error. After `update_next_version(300)` is called, versions 100-199 are permanently lost from the index.

2. **Duplicate Scenario**: When `index_with_annotator()` writes table info to the database but the process crashes before `update_next_version()` can update the metadata, on restart the same versions are reprocessed: [3](#0-2) 

The table info is committed to the database (line 113) before the version tracking metadata is updated via `update_next_version()`: [4](#0-3) 

The parallel processing architecture in `TableInfoService` exacerbates this issue by spawning multiple concurrent tasks that each call `index_with_annotator()` with different version ranges: [5](#0-4) 

Without validation, if these complete out-of-order or if the process crashes between completing indexing and updating the version tracker, the system cannot detect or prevent inconsistencies.

## Impact Explanation

This vulnerability falls under **Medium to High Severity**:

**Medium Severity Impact** ($10,000 category): "State inconsistencies requiring intervention"
- Gaps in table info cause queries to fail, breaking indexer-dependent APIs and services
- Duplicate processing wastes resources and may cause transient inconsistencies
- Manual intervention required to re-index missing ranges or resolve duplicates

**High Severity Impact** ($50,000 category): "Significant protocol violations"
- The indexer is a critical component for table metadata resolution
- Violations of the continuous version processing invariant affect system reliability
- API crashes and service degradation when table info queries fail

The impact is significant because the table info indexer is used by ecosystem services to resolve table handles to their key/value type information, which is essential for properly displaying and parsing on-chain table data.

## Likelihood Explanation

**HIGH likelihood** of occurrence due to:

1. **Crash/Restart Scenarios**: Node crashes between writing table info (line 113 in db_v2.rs) and updating version metadata (line 302-304 in table_info_service.rs) will cause duplicate processing on restart.

2. **Parallel Processing Race Conditions**: Multiple threads processing transaction chunks can complete in non-sequential order, though this is partially mitigated by sequential retry logic.

3. **Error Handling Gaps**: If `update_next_version()` fails after table info has been written, the version tracker and actual indexed data become desynchronized.

4. **No Detection Mechanism**: Without validation, these issues occur silently and are difficult to detect until queries fail.

This is not a malicious exploitation scenario but rather a **data integrity bug** that will naturally occur during normal operations, especially in production environments with frequent restarts, upgrades, or error conditions.

## Recommendation

Add the same version validation that exists in the original `Indexer` implementation:

```rust
pub fn index_with_annotator<R: StateView>(
    &self,
    annotator: &AptosValueAnnotator<R>,
    first_version: Version,
    write_sets: &[&WriteSet],
) -> Result<()> {
    // Add validation
    let next_version = self.next_version.load(Ordering::Relaxed);
    db_ensure!(
        first_version <= next_version,
        "Indexer expects to see continuous transaction versions. Expecting: {}, got: {}",
        next_version,
        first_version,
    );
    
    let end_version = first_version + write_sets.len() as Version;
    if end_version <= next_version {
        warn!(
            "Seeing old transactions. Expecting version: {}, got {} transactions starting from version {}.",
            next_version,
            write_sets.len(),
            first_version,
        );
        return Ok(());
    }
    
    // Rest of the function remains unchanged
    let mut table_info_parser = TableInfoParser::new(self, annotator, &self.pending_on);
    // ...
}
```

Additionally, consider making the write to database and version metadata update atomic, or add transaction-level protection to ensure consistency.

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;
    
    #[test]
    fn test_out_of_order_indexing_not_detected() {
        // Create a temporary database
        let tmpdir = TempDir::new().unwrap();
        let db = DB::open(tmpdir.path(), "test_db", &RocksdbConfigs::default()).unwrap();
        let indexer = IndexerAsyncV2::new(db).unwrap();
        
        // Initial state: next_version = 0
        assert_eq!(indexer.next_version(), 0);
        
        // Create mock write sets for versions 100-199 (skipping 0-99)
        let mock_state_view = create_mock_state_view();
        let annotator = AptosValueAnnotator::new(&mock_state_view);
        let write_sets = create_mock_write_sets(100);
        let write_sets_ref: Vec<_> = write_sets.iter().collect();
        
        // This should FAIL but doesn't due to missing validation
        // Expected error: "Indexer expects to see continuous transaction versions. Expecting: 0, got: 100"
        let result = indexer.index_with_annotator(&annotator, 100, &write_sets_ref);
        
        // BUG: This succeeds when it should fail
        assert!(result.is_ok()); // <-- This passes, demonstrating the vulnerability
        
        // After update_next_version(200), we have indexed 100-199 but next_version claims 200
        indexer.update_next_version(200).unwrap();
        assert_eq!(indexer.next_version(), 200);
        
        // Versions 0-99 are now permanently missing - a GAP has been created
    }
    
    #[test]
    fn test_duplicate_indexing_not_detected() {
        let tmpdir = TempDir::new().unwrap();
        let db = DB::open(tmpdir.path(), "test_db", &RocksdbConfigs::default()).unwrap();
        let indexer = IndexerAsyncV2::new(db).unwrap();
        
        let mock_state_view = create_mock_state_view();
        let annotator = AptosValueAnnotator::new(&mock_state_view);
        let write_sets = create_mock_write_sets(100);
        let write_sets_ref: Vec<_> = write_sets.iter().collect();
        
        // First processing: versions 0-99
        indexer.index_with_annotator(&annotator, 0, &write_sets_ref).unwrap();
        indexer.update_next_version(100).unwrap();
        
        // Simulate crash before version update or buggy retry
        // Processing same versions again - should FAIL but doesn't
        let result = indexer.index_with_annotator(&annotator, 0, &write_sets_ref);
        
        // BUG: This succeeds when it should be rejected as "old transactions"
        assert!(result.is_ok()); // <-- This passes, demonstrating the vulnerability
        
        // Table info has been written twice for the same versions - DUPLICATE
    }
}
```

**Notes:**
- This vulnerability is a regression from the original `Indexer` implementation which had proper validation
- The issue is particularly critical given that `IndexerAsyncV2` is intended to replace the original implementation
- The parallel processing architecture in `TableInfoService` increases the risk of version tracking desynchronization
- While not a malicious exploitation vector, this represents a serious data integrity issue that violates the State Consistency invariant and requires manual intervention to resolve

### Citations

**File:** storage/indexer/src/db_v2.rs (L87-93)
```rust
    pub fn index_with_annotator<R: StateView>(
        &self,
        annotator: &AptosValueAnnotator<R>,
        first_version: Version,
        write_sets: &[&WriteSet],
    ) -> Result<()> {
        let end_version = first_version + write_sets.len() as Version;
```

**File:** storage/indexer/src/db_v2.rs (L113-114)
```rust
        self.db.write_schemas(batch)?;
        Ok(())
```

**File:** storage/indexer/src/lib.rs (L101-107)
```rust
        let next_version = self.next_version();
        db_ensure!(
            first_version <= next_version,
            "Indexer expects to see continuous transaction versions. Expecting: {}, got: {}",
            next_version,
            first_version,
        );
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L269-272)
```rust
            let task = tokio::spawn(async move {
                Self::process_transactions(context, indexer_async_v2, &transactions[start..end])
                    .await
            });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L302-304)
```rust
                self.indexer_async_v2
                    .update_next_version(end_version + 1)
                    .unwrap();
```
