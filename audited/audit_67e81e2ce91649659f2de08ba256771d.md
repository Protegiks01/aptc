# Audit Report

## Title
Race Condition in Pruner Progress Updates Causes Progress to Move Backward

## Summary
The `write_pruner_progress()` function performs unsynchronized direct database writes to pruner progress metadata. When concurrent writes occur from both the background pruner worker thread and the fast sync finalization path, the last write wins regardless of version value, causing pruner progress to move backward and breaking critical pruner invariants.

## Finding Description

The vulnerability exists in two concurrent code paths that write to the same database key (`DbMetadataKey::PersistedAuxiliaryInfoPrunerProgress`) without synchronization:

**Path 1: Background Pruner Worker Thread**

The pruner worker runs in a dedicated background thread that continuously prunes old data. [1](#0-0) 

When pruning, each sub-pruner writes its progress atomically within a batch operation. [2](#0-1) 

**Path 2: Fast Sync Finalization Path**

During fast sync completion, `finalize_state_snapshot()` is called which updates pruner progress for all sub-databases. [3](#0-2) [4](#0-3) 

This calls `save_min_readable_version()` which invokes `write_pruner_progress()`. [5](#0-4) [6](#0-5) 

The `write_pruner_progress()` function performs a direct, unsynchronized database write. [7](#0-6) 

**The Race Condition:**

Both paths write to the same RocksDB key using separate write operations. The `put()` method internally creates a batch and calls `write_schemas()`. [8](#0-7) 

Since RocksDB write operations are not coordinated between these two code paths, the last write wins regardless of the version value. The pruner worker is started immediately when the database opens if pruning is enabled. [9](#0-8) 

There is no synchronization mechanism (no locks, no atomic compare-and-swap) between these code paths. The sub-pruners execute in parallel without coordination. [10](#0-9) 

## Impact Explanation

**Severity: Medium** - This vulnerability causes state inconsistencies requiring manual intervention:

1. **Pruner State Corruption**: The pruner's progress metadata becomes inconsistent with actually pruned data, violating the invariant that progress should only move forward monotonically.

2. **Data Integrity Issues**: When progress moves backward, the pruner believes it needs to prune versions again that may already be deleted, creating confusion about what data is available versus what has been pruned.

3. **Operational Impact**: Nodes relying on pruner progress metadata for operational decisions may make incorrect assumptions about data availability, potentially requiring node restarts or manual intervention to correct the metadata.

This qualifies as **Medium Severity** per Aptos bug bounty criteria: "State inconsistencies requiring manual intervention." While this does not directly cause fund loss or consensus violations (which would be Critical), it represents a significant operational issue that can affect node reliability and require administrator intervention to resolve.

## Likelihood Explanation

**Likelihood: Medium**

This race condition will occur whenever:
1. A node has pruning enabled (common for production validators)
2. The node performs fast sync (common during initial bootstrap or recovery)
3. The timing of I/O operations causes writes to complete out of order

The vulnerability is moderately likely because:
- The pruner worker runs continuously in the background with no coordination with fast sync operations
- RocksDB write operation timing is non-deterministic and depends on I/O load
- There is NO synchronization mechanism to prevent this race
- The comment explicitly states the function is used by fast sync [11](#0-10) 

## Recommendation

Implement proper synchronization between the pruner worker and fast sync operations:

1. **Add a pruner progress lock**: Protect all writes to pruner progress metadata with a mutex in the pruner manager.

2. **Use atomic compare-and-swap**: Instead of direct writes, use atomic operations that only update progress if the new value is greater than the current value.

3. **Coordinate with fast sync**: Consider pausing the pruner worker during fast sync operations, or implement a protocol where fast sync signals the pruner to update its state rather than directly writing progress.

4. **Validate progress monotonicity**: Add assertions to detect and log cases where progress would move backward, preventing the corruption from persisting.

## Proof of Concept

A full PoC would require setting up a scenario where:
1. A node is performing fast sync to restore to a specific version
2. The pruner worker is actively running and completing pruning cycles
3. The I/O timing causes the writes to complete out of order

This is timing-dependent and difficult to reproduce deterministically, but the concurrent code paths are clearly present in the codebase as demonstrated by the citations above.

## Notes

The core vulnerability is the lack of synchronization between two concurrent write paths to the same database key. While RocksDB provides atomicity for individual write operations, it cannot prevent race conditions when separate, uncoordinated operations write to the same key. The pruner progress metadata should be treated as a shared resource requiring proper synchronization.

### Citations

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L81-84)
```rust
        let worker_thread = std::thread::Builder::new()
            .name(format!("{name}_pruner"))
            .spawn(move || inner_cloned.work())
            .expect("Creating pruner thread should succeed.");
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/persisted_auxiliary_info_pruner.rs (L28-30)
```rust
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::PersistedAuxiliaryInfoPrunerProgress,
            &DbMetadataValue::Version(target_version),
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L1131-1131)
```rust
        .finalize_state_snapshot(
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L225-225)
```rust
            self.ledger_pruner.save_min_readable_version(version)?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L88-88)
```rust
        self.ledger_db.write_pruner_progress(min_readable_version)
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L113-119)
```rust
        let pruner_worker = if ledger_pruner_config.enable {
            Some(Self::init_pruner(
                Arc::clone(&ledger_db),
                ledger_pruner_config,
                internal_indexer_db,
            ))
        } else {
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L372-374)
```rust
    // Only expect to be used by fast sync when it is finished.
    pub(crate) fn write_pruner_progress(&self, version: Version) -> Result<()> {
        info!("Fast sync is done, writing pruner progress {version} for all ledger sub pruners.");
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L376-377)
```rust
        self.persisted_auxiliary_info_db
            .write_pruner_progress(version)?;
```

**File:** storage/aptosdb/src/ledger_db/persisted_auxiliary_info_db.rs (L32-37)
```rust
    pub(super) fn write_pruner_progress(&self, version: Version) -> Result<()> {
        self.db.put::<DbMetadataSchema>(
            &DbMetadataKey::PersistedAuxiliaryInfoPrunerProgress,
            &DbMetadataValue::Version(version),
        )
    }
```

**File:** storage/schemadb/src/lib.rs (L239-243)
```rust
    pub fn put<S: Schema>(&self, key: &S::Key, value: &S::Value) -> DbResult<()> {
        // Not necessary to use a batch, but we'd like a central place to bump counters.
        let mut batch = self.new_native_batch();
        batch.put::<S>(key, value)?;
        self.write_schemas(batch)
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L78-84)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;
```
