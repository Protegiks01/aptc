# Audit Report

## Title
Partial QuorumStore Shutdown Leaves Components Running During Epoch Transition Failure

## Summary
When the QuorumStore coordinator shutdown sequence fails at any intermediate step, components positioned after the failure point remain running while the epoch manager panics and terminates the node. This creates a critical window where partially shutdown QuorumStore components are in an inconsistent state, potentially corrupting data or violating consensus invariants before the process exits.

## Finding Description

The vulnerability exists in the shutdown sequence orchestrated between `epoch_manager.rs::shutdown_current_processor()` and `quorum_store_coordinator.rs::start()`. [1](#0-0) 

The epoch manager sends a shutdown command and expects an acknowledgment. However, the coordinator performs a sequential shutdown of five components: [2](#0-1) 

The shutdown sequence is:
1. NetworkListener (lines 93-107)
2. BatchGenerator (lines 109-117)  
3. RemoteBatchCoordinators (lines 119-134)
4. ProofCoordinator (lines 136-146)
5. ProofManager (lines 148-156)

**Critical Flaw:** Each shutdown step uses `.expect()` which panics on failure. If any intermediate component fails to acknowledge its shutdown (e.g., BatchGenerator at line 117), the coordinator panics immediately.

When the coordinator panics:
- Components **before** the failure point are shutdown (e.g., NetworkListener)
- Components **after** the failure point are **still running** (e.g., RemoteBatchCoordinators, ProofCoordinator, ProofManager)
- The final acknowledgment (line 158-160) is never sent
- The `ack_tx` sender is dropped
- In epoch_manager, `ack_rx.await` fails and panics at line 681

**The Invariant Violation:**

During the critical window between coordinator panic and process termination (via crash handler), partially shutdown QuorumStore components are running in an inconsistent state: [3](#0-2) [4](#0-3) [5](#0-4) 

If NetworkListener is shutdown but ProofManager is still running:
- ProofManager may process messages and update state
- ProofManager may interact with storage (quorum_store_storage)
- These operations occur while the epoch transition is failing
- Storage writes may be inconsistent with the failed epoch transition
- The node crashes via the panic handler, but state corruption may have already occurred [6](#0-5) 

## Impact Explanation

**HIGH Severity** per Aptos bug bounty criteria:

1. **Node Crash During Epoch Transition**: The node terminates during a critical epoch transition, causing unavailability
2. **Consensus Liveness Impact**: If multiple nodes crash during the same epoch transition (e.g., due to a common bug in a QuorumStore component), the network may lose liveness
3. **State Inconsistency Risk**: Running components may write inconsistent state to storage before the process exits, potentially requiring manual intervention or state recovery
4. **Epoch Transition Failure**: Violates the invariant that epoch transitions must be atomic and clean

The vulnerability qualifies as "Validator node slowdowns" and "Significant protocol violations" from the HIGH severity category.

## Likelihood Explanation

**MEDIUM-HIGH Likelihood:**

This vulnerability is triggered whenever:
1. A QuorumStore component crashes/panics before receiving shutdown (component already dead)
2. A QuorumStore component has a bug that prevents sending shutdown acknowledgment
3. Channel communication failures between coordinator and components
4. Race conditions during concurrent operations and shutdown

These scenarios are realistic because:
- QuorumStore components are complex asynchronous tasks with multiple failure points
- Epoch transitions happen regularly (every epoch change)
- Component bugs or resource exhaustion could prevent proper shutdown
- The sequential shutdown design has no error recovery or rollback mechanism

## Recommendation

Implement defensive shutdown with timeout and error recovery:

```rust
async fn shutdown_current_processor(&mut self) {
    // ... existing shutdown code for other components ...
    
    if let Some(mut quorum_store_coordinator_tx) = self.quorum_store_coordinator_tx.take() {
        let (ack_tx, ack_rx) = oneshot::channel();
        
        // Send shutdown command
        if let Err(e) = quorum_store_coordinator_tx
            .send(CoordinatorCommand::Shutdown(ack_tx))
            .await 
        {
            error!("Failed to send shutdown to QuorumStore coordinator: {:?}", e);
            // Continue with epoch transition - coordinator is already dead
            return;
        }
        
        // Wait for acknowledgment with timeout
        match tokio::time::timeout(Duration::from_secs(10), ack_rx).await {
            Ok(Ok(())) => {
                info!("QuorumStore shutdown completed successfully");
            }
            Ok(Err(e)) => {
                error!("Failed to receive QuorumStore shutdown ack: {:?}", e);
                // Log but don't panic - allow epoch transition to continue
            }
            Err(_) => {
                error!("QuorumStore shutdown timed out after 10s");
                // Components may still be running but we must proceed
            }
        }
    }
}
```

Additionally, modify the coordinator to implement graceful degradation:

```rust
// In quorum_store_coordinator.rs, wrap each shutdown in a timeout
let shutdown_timeout = Duration::from_secs(5);

if let Err(e) = tokio::time::timeout(
    shutdown_timeout,
    network_listener_shutdown_rx
).await {
    error!("NetworkListener shutdown timed out, forcing cleanup");
    // Continue with next component
} else if let Err(e) = ... {
    error!("NetworkListener shutdown failed: {:?}", e);
    // Continue with next component
}
```

## Proof of Concept

```rust
// Reproduction test demonstrating the vulnerability
#[tokio::test]
async fn test_partial_quorum_store_shutdown() {
    // 1. Setup QuorumStore components
    let (coordinator_tx, coordinator_rx) = futures_channel::mpsc::channel(10);
    let (batch_gen_tx, mut batch_gen_rx) = tokio::sync::mpsc::channel(10);
    
    // 2. Spawn coordinator
    let coordinator = QuorumStoreCoordinator::new(
        /* ... params ... */
        batch_gen_tx,
        /* ... other component channels ... */
    );
    tokio::spawn(coordinator.start(coordinator_rx));
    
    // 3. Simulate batch generator that crashes before acknowledging shutdown
    tokio::spawn(async move {
        while let Some(cmd) = batch_gen_rx.recv().await {
            match cmd {
                BatchGeneratorCommand::Shutdown(_ack_tx) => {
                    // Simulate crash: drop ack_tx without sending
                    panic!("Simulated batch generator crash");
                }
                _ => {}
            }
        }
    });
    
    // 4. Trigger shutdown from epoch manager
    let (ack_tx, ack_rx) = oneshot::channel();
    coordinator_tx.send(CoordinatorCommand::Shutdown(ack_tx))
        .await
        .expect("Send should succeed");
    
    // 5. Observe failure
    let result = ack_rx.await;
    assert!(result.is_err(), "Should fail when coordinator panics");
    
    // At this point:
    // - NetworkListener is shutdown
    // - BatchGenerator crashed  
    // - ProofCoordinator is STILL RUNNING
    // - ProofManager is STILL RUNNING
    // This demonstrates the partial shutdown vulnerability
}
```

**Notes:**

The vulnerability is a **design flaw** in the shutdown orchestration. The sequential shutdown with panic-on-error provides no resilience against component failures. During epoch transitions—critical moments requiring atomicity—this creates a window for state corruption and node crashes that could impact consensus liveness if affecting multiple validators simultaneously.

### Citations

**File:** consensus/src/epoch_manager.rs (L675-682)
```rust
        if let Some(mut quorum_store_coordinator_tx) = self.quorum_store_coordinator_tx.take() {
            let (ack_tx, ack_rx) = oneshot::channel();
            quorum_store_coordinator_tx
                .send(CoordinatorCommand::Shutdown(ack_tx))
                .await
                .expect("Could not send shutdown indicator to QuorumStore");
            ack_rx.await.expect("Failed to stop QuorumStore");
        }
```

**File:** consensus/src/quorum_store/quorum_store_coordinator.rs (L82-162)
```rust
                    CoordinatorCommand::Shutdown(ack_tx) => {
                        counters::QUORUM_STORE_MSG_COUNT
                            .with_label_values(&["QSCoordinator::shutdown"])
                            .inc();
                        // Note: Shutdown is done from the back of the quorum store pipeline to the
                        // front, so senders are always shutdown before receivers. This avoids sending
                        // messages through closed channels during shutdown.
                        // Oneshots that send data in the reverse order of the pipeline must assume that
                        // the receiver could be unavailable during shutdown, and resolve this without
                        // panicking.

                        let (network_listener_shutdown_tx, network_listener_shutdown_rx) =
                            oneshot::channel();
                        match self.quorum_store_msg_tx.push(
                            self.my_peer_id,
                            (
                                self.my_peer_id,
                                VerifiedEvent::Shutdown(network_listener_shutdown_tx),
                            ),
                        ) {
                            Ok(()) => info!("QS: shutdown network listener sent"),
                            Err(err) => panic!("Failed to send to NetworkListener, Err {:?}", err),
                        };
                        network_listener_shutdown_rx
                            .await
                            .expect("Failed to stop NetworkListener");

                        let (batch_generator_shutdown_tx, batch_generator_shutdown_rx) =
                            oneshot::channel();
                        self.batch_generator_cmd_tx
                            .send(BatchGeneratorCommand::Shutdown(batch_generator_shutdown_tx))
                            .await
                            .expect("Failed to send to BatchGenerator");
                        batch_generator_shutdown_rx
                            .await
                            .expect("Failed to stop BatchGenerator");

                        for remote_batch_coordinator_cmd_tx in self.remote_batch_coordinator_cmd_tx
                        {
                            let (
                                remote_batch_coordinator_shutdown_tx,
                                remote_batch_coordinator_shutdown_rx,
                            ) = oneshot::channel();
                            remote_batch_coordinator_cmd_tx
                                .send(BatchCoordinatorCommand::Shutdown(
                                    remote_batch_coordinator_shutdown_tx,
                                ))
                                .await
                                .expect("Failed to send to Remote BatchCoordinator");
                            remote_batch_coordinator_shutdown_rx
                                .await
                                .expect("Failed to stop Remote BatchCoordinator");
                        }

                        let (proof_coordinator_shutdown_tx, proof_coordinator_shutdown_rx) =
                            oneshot::channel();
                        self.proof_coordinator_cmd_tx
                            .send(ProofCoordinatorCommand::Shutdown(
                                proof_coordinator_shutdown_tx,
                            ))
                            .await
                            .expect("Failed to send to ProofCoordinator");
                        proof_coordinator_shutdown_rx
                            .await
                            .expect("Failed to stop ProofCoordinator");

                        let (proof_manager_shutdown_tx, proof_manager_shutdown_rx) =
                            oneshot::channel();
                        self.proof_manager_cmd_tx
                            .send(ProofManagerCommand::Shutdown(proof_manager_shutdown_tx))
                            .await
                            .expect("Failed to send to ProofManager");
                        proof_manager_shutdown_rx
                            .await
                            .expect("Failed to stop ProofManager");

                        ack_tx
                            .send(())
                            .expect("Failed to send shutdown ack from QuorumStore");
                        break;
                    },
```

**File:** consensus/src/quorum_store/network_listener.rs (L47-55)
```rust
                    VerifiedEvent::Shutdown(ack_tx) => {
                        counters::QUORUM_STORE_MSG_COUNT
                            .with_label_values(&["NetworkListener::shutdown"])
                            .inc();
                        info!("QS: shutdown network listener received");
                        ack_tx
                            .send(())
                            .expect("Failed to send shutdown ack to QuorumStore");
                        break;
```

**File:** consensus/src/quorum_store/proof_coordinator.rs (L415-420)
```rust
                        ProofCoordinatorCommand::Shutdown(ack_tx) => {
                            counters::QUORUM_STORE_MSG_COUNT.with_label_values(&["ProofCoordinator::shutdown"]).inc();
                            ack_tx
                                .send(())
                                .expect("Failed to send shutdown ack to QuorumStore");
                            break;
```

**File:** consensus/src/quorum_store/proof_manager.rs (L296-301)
```rust
                            ProofManagerCommand::Shutdown(ack_tx) => {
                                counters::QUORUM_STORE_MSG_COUNT.with_label_values(&["ProofManager::shutdown"]).inc();
                                ack_tx
                                    .send(())
                                    .expect("Failed to send shutdown ack to QuorumStore");
                                break;
```

**File:** crates/crash-handler/src/lib.rs (L48-57)
```rust
    // Do not kill the process if the panics happened at move-bytecode-verifier.
    // This is safe because the `state::get_state()` uses a thread_local for storing states. Thus the state can only be mutated to VERIFIER by the thread that's running the bytecode verifier.
    //
    // TODO: once `can_unwind` is stable, we should assert it. See https://github.com/rust-lang/rust/issues/92988.
    if state::get_state() == VMState::VERIFIER || state::get_state() == VMState::DESERIALIZER {
        return;
    }

    // Kill the process
    process::exit(12);
```
