# Audit Report

## Title
State Store Liveness Failure After Incomplete KV Replay Restore

## Summary
The `save_transactions_and_replay_kv()` function sets state summaries to `CORRUPTION_SENTINEL` after replaying key-value changes, creating a critical liveness vulnerability if the subsequent tree restore phase doesn't complete. A validator that completes Phase 1 of a two-phase restore but fails to complete Phase 2 will crash when attempting to process new blocks.

## Finding Description

The vulnerability exists in the two-phase database restore workflow. When `kv_replay=true` is used: [1](#0-0) 

This code recalculates state from write sets and calls `set_state_ignoring_summary()`, which intentionally sets state summaries to `CORRUPTION_SENTINEL`: [2](#0-1) 

The `StateSummary::update()` function explicitly prevents updates when the root hash is `CORRUPTION_SENTINEL`: [3](#0-2) 

**Critical Gap**: The `replay_kv()` function used in Phase 1 does NOT call `reset_state_store()`: [4](#0-3) 

In contrast, `replay_transactions()` in Phase 2 DOES call reset: [5](#0-4) 

**Attack Scenario:**
1. Validator starts two-phase restore (Phase 1: KV replay with `kv_replay=true`)
2. Phase 1 completes, leaving state summaries with `CORRUPTION_SENTINEL`
3. Restore is interrupted (crash, network failure, operator error) before Phase 2
4. Validator restarts and attempts to sync/execute new blocks
5. `StateSummary::update()` is called and triggers assertion failure
6. Validator crashes, causing liveness failure

## Impact Explanation

**High Severity** - This meets the criteria for validator node operational issues:

- **Liveness Failure**: Affected validators cannot process new blocks and will repeatedly crash
- **Consensus Participation**: Crashed validators cannot participate in consensus, reducing network security if multiple validators are affected
- **Operational Impact**: Requires manual intervention to complete the restore or restart from scratch

While not a consensus safety violation (doesn't cause different validators to commit different blocks), it causes **total loss of liveness** for affected validators, which per Aptos bug bounty criteria could qualify for **Critical Severity** if it affects multiple validators or network availability.

## Likelihood Explanation

**Medium-High Likelihood:**
- Restore operations are common during validator setup, disaster recovery, and state sync
- Network interruptions, hardware failures, or operator errors during multi-hour restore processes are realistic
- The two-phase restore workflow is complex with multiple steps where failures can occur
- No explicit validation prevents the node from attempting to operate in the CORRUPTION_SENTINEL state

## Recommendation

Add state validation before allowing block execution and ensure proper cleanup on restore interruption:

```rust
// In restore_handler.rs or state_store/mod.rs
pub fn validate_state_ready_for_execution(&self) -> Result<()> {
    let current_state = self.current_state_locked();
    ensure!(
        current_state.summary().root_hash() != *CORRUPTION_SENTINEL,
        "State store not initialized - restore incomplete. Root hash is CORRUPTION_SENTINEL."
    );
    ensure!(
        current_state.last_checkpoint().summary().root_hash() != *CORRUPTION_SENTINEL,
        "State store checkpoint not initialized - restore incomplete."
    );
    Ok(())
}

// Call this before starting consensus/execution
// In replay_kv(), add cleanup on completion:
async fn replay_kv(...) -> Result<()> {
    // ... existing code ...
    
    // After KV replay completes but BEFORE Phase 2, document the state
    warn!("KV replay completed. State summaries are PLACEHOLDER values. \
           Tree restore (Phase 2) MUST complete before node can process blocks.");
    
    Ok(())
}
```

Additionally, add `reset_state_store()` call in `replay_kv()` after completion, or add explicit validation in the node startup path to detect and handle incomplete restore states.

## Proof of Concept

```rust
// Simulated restore interruption
#[test]
fn test_incomplete_kv_replay_causes_crash() {
    let db = setup_test_db();
    let restore_handler = RestoreHandler::new(db.clone());
    
    // Phase 1: Simulate KV replay
    restore_handler.force_state_version_for_kv_restore(Some(100)).unwrap();
    
    let txns = vec![/* test transactions */];
    let write_sets = vec![/* corresponding write sets */];
    
    restore_handler.save_transactions_and_replay_kv(
        101,
        &txns,
        &[],
        &[],
        &[],
        write_sets,
    ).unwrap();
    
    // Phase 2 does NOT happen (simulate interruption)
    
    // Attempt to execute new block
    let state_store = db.state_store();
    let current = state_store.current_state_locked();
    
    // This should fail or require special handling
    let updates = StateUpdateRefs::index_write_sets(102, &new_write_sets, 1, vec![0]);
    
    // This will panic with assertion failure
    let result = current.ledger_state_summary().update(
        &persisted_summary,
        &hot_updates,
        &updates,
    );
    
    assert!(result.is_err() || /* panics */);
}
```

**Notes:**
- This vulnerability specifically affects the restoration workflow integrity
- The CORRUPTION_SENTINEL design is intentional but lacks proper safeguards against incomplete restore states
- Validators should validate state integrity before participating in consensus
- The restore coordinator should implement transactional semantics or explicit rollback on Phase 1 completion without Phase 2

### Citations

**File:** storage/aptosdb/src/backup/restore_utils.rs (L269-277)
```rust
    if kv_replay && first_version > 0 && state_store.get_usage(Some(first_version - 1)).is_ok() {
        let (ledger_state, _hot_state_updates) = state_store.calculate_state_and_put_updates(
            &StateUpdateRefs::index_write_sets(first_version, write_sets, write_sets.len(), vec![]),
            &mut ledger_db_batch.ledger_metadata_db_batches, // used for storing the storage usage
            state_kv_batches,
        )?;
        // n.b. ideally this is set after the batches are committed
        state_store.set_state_ignoring_summary(ledger_state);
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1208-1222)
```rust
    pub fn set_state_ignoring_summary(&self, ledger_state: LedgerState) {
        let hot_smt = SparseMerkleTree::new(*CORRUPTION_SENTINEL);
        let smt = SparseMerkleTree::new(*CORRUPTION_SENTINEL);
        let last_checkpoint_summary = StateSummary::new_at_version(
            ledger_state.last_checkpoint().version(),
            hot_smt.clone(),
            smt.clone(),
            HotStateConfig::default(),
        );
        let summary = StateSummary::new_at_version(
            ledger_state.version(),
            hot_smt,
            smt,
            HotStateConfig::default(),
        );
```

**File:** storage/storage-interface/src/state_store/state_summary.rs (L92-93)
```rust
        assert_ne!(self.hot_state_summary.root_hash(), *CORRUPTION_SENTINEL);
        assert_ne!(self.global_state_summary.root_hash(), *CORRUPTION_SENTINEL);
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L554-568)
```rust
    async fn replay_kv(
        &self,
        restore_handler: &RestoreHandler,
        txns_to_execute_stream: impl Stream<
            Item = Result<(
                Transaction,
                PersistedAuxiliaryInfo,
                TransactionInfo,
                WriteSet,
                Vec<ContractEvent>,
            )>,
        >,
    ) -> Result<()> {
        let (first_version, _) = self.replay_from_version.unwrap();
        restore_handler.force_state_version_for_kv_restore(first_version.checked_sub(1))?;
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L640-654)
```rust
    async fn replay_transactions(
        &self,
        restore_handler: &RestoreHandler,
        txns_to_execute_stream: impl Stream<
            Item = Result<(
                Transaction,
                PersistedAuxiliaryInfo,
                TransactionInfo,
                WriteSet,
                Vec<ContractEvent>,
            )>,
        >,
    ) -> Result<()> {
        let (first_version, _) = self.replay_from_version.unwrap();
        restore_handler.reset_state_store();
```
