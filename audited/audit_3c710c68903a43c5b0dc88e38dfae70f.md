# Audit Report

## Title
Lack of Exponential Backoff in Stream Creation Allows Sybil-Based Resource Exhaustion During State Sync Bootstrapping

## Summary
The state sync bootstrapper lacks exponential backoff or rate limiting when creating new data streams after version mismatch failures. An attacker controlling multiple peer identities can exploit this to cause sustained resource consumption through repeated stream resets, delaying node bootstrapping.

## Finding Description

The vulnerability exists in the state sync driver's retry mechanism when handling version mismatches. When `verify_payload_start_version()` detects that `payload_start_version` doesn't match `expected_start_version`, it resets the active stream with `InvalidPayloadData` feedback: [1](#0-0) 

This error propagates to the driver's main loop, which logs it but continues execution: [2](#0-1) 

The driver retries after `progress_check_interval_ms` (default 100ms): [3](#0-2) 

When retrying, `initialize_active_data_stream()` is called, which always resets the chunk executor, performing database reads: [4](#0-3) [5](#0-4) 

**Attack Path:**

1. Attacker controls multiple peer identities (Sybil attack)
2. Each malicious peer sends transaction/output proofs with deliberately mismatched `first_transaction_version`
3. The syncing node detects mismatch, resets stream, and provides `InvalidPayloadData` feedback
4. Malicious peer's score is reduced by 0.95 multiplier per error
5. After ~14 errors, peer's score drops below 25.0 threshold and is ignored
6. Node selects next available peer (potentially another Sybil identity)
7. Process repeats for each Sybil peer

**Peer Scoring Calculation:** [6](#0-5) 

New peers start at score 50.0, and InvalidPayloadData applies 0.95 multiplier (NotUseful error type): [7](#0-6) 

While peer scoring eventually filters out malicious peers, there's no immediate protection through backoff or circuit breakers. With N Sybil peers, an attacker can cause ~14N stream resets before all peers are ignored.

## Impact Explanation

**Severity: Medium**

This vulnerability allows temporary resource exhaustion and delayed bootstrapping but doesn't cause permanent damage:

- **Resource Consumption:** Each stream reset involves database reads (ChunkExecutorInner creation), network bandwidth for fetching invalid data, CPU for verification, and stream creation/termination overhead
- **Bootstrapping Delay:** New nodes or nodes recovering from failures experience delayed sync completion
- **Limited Duration:** Attack is self-limiting as malicious peers get ignored (14 failures Ã— 100ms = 1.4 seconds per peer)
- **Sybil Attack Required:** Attacker needs multiple peer identities to sustain the attack

This fits **Medium severity** per Aptos bounty criteria as it can cause temporary validator node slowdowns during bootstrapping, though it doesn't result in permanent state inconsistencies.

## Likelihood Explanation

**Likelihood: Medium**

The attack requires:
- Multiple peer identities (Sybil attack capability)
- Target node must be bootstrapping (not already synced)
- Some honest peers must exist (otherwise legitimate sync failure)

The attack is straightforward to execute once attacker has peer identities, but the impact is limited by the peer scoring system which eventually filters out all malicious peers. The 100ms retry interval without backoff makes the attack more effective than it would be with exponential backoff.

## Recommendation

Implement exponential backoff when stream creation repeatedly fails due to verification errors:

```rust
// In bootstrapper.rs, add backoff state
struct StreamResetBackoff {
    consecutive_failures: u32,
    last_reset_time: Instant,
}

impl StreamResetBackoff {
    fn calculate_backoff_ms(&self) -> u64 {
        // Exponential backoff: 100ms, 200ms, 400ms, 800ms, capped at 5000ms
        let base_interval = 100;
        let max_backoff = 5000;
        min(base_interval * (2_u64.pow(self.consecutive_failures)), max_backoff)
    }
    
    fn should_retry(&self, progress_check_interval_ms: u64) -> bool {
        let backoff_ms = self.calculate_backoff_ms();
        self.last_reset_time.elapsed().as_millis() >= backoff_ms as u128
    }
}
```

Additionally, implement a circuit breaker that pauses stream creation after excessive consecutive failures (e.g., 50 failures within 1 minute), forcing a longer delay before retrying.

## Proof of Concept

```rust
// Test demonstrating resource consumption through repeated version mismatches
#[tokio::test]
async fn test_version_mismatch_resource_exhaustion() {
    // Setup: Create bootstrapper with mock peers
    let mut bootstrapper = create_test_bootstrapper().await;
    
    // Create multiple mock peers that send mismatched versions
    let num_sybil_peers = 10;
    let mut sybil_peers = Vec::new();
    
    for i in 0..num_sybil_peers {
        let peer = create_mock_peer_with_wrong_version(
            expected_version = 100,
            wrong_version = 200 + i, // Each peer sends different wrong version
        );
        sybil_peers.push(peer);
    }
    
    // Track resource consumption
    let start_time = Instant::now();
    let mut reset_count = 0;
    let mut db_read_count = 0;
    
    // Attempt to bootstrap - should fail repeatedly
    for _ in 0..(num_sybil_peers * 14) {
        match bootstrapper.drive_progress(&global_summary).await {
            Err(Error::VerificationError(_)) => {
                reset_count += 1;
                db_read_count += 1; // Each reset triggers DB read
            },
            _ => break,
        }
        tokio::time::sleep(Duration::from_millis(100)).await;
    }
    
    let elapsed = start_time.elapsed();
    
    // Verify resource exhaustion occurred
    assert!(reset_count >= num_sybil_peers * 10, 
        "Should have many resets before peers are ignored");
    assert!(db_read_count >= num_sybil_peers * 10,
        "Should have triggered many DB reads");
    assert!(elapsed.as_secs() >= ((num_sybil_peers * 14 * 100) / 1000) as u64,
        "Attack should have delayed bootstrapping significantly");
}
```

## Notes

The peer scoring system provides eventual mitigation by ignoring peers with scores below 25.0: [8](#0-7) 

However, the lack of immediate protection through backoff mechanisms means the attack can still cause meaningful resource consumption before all Sybil peers are exhausted. The fix should complement the existing peer scoring with exponential backoff to provide defense-in-depth.

### Citations

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L456-458)
```rust
    ) -> Result<(), Error> {
        // Reset the chunk executor to flush any invalid state currently held in-memory
        self.storage_synchronizer.reset_chunk_executor()?;
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L1346-1355)
```rust
            if payload_start_version != expected_start_version {
                self.reset_active_stream(Some(NotificationAndFeedback::new(
                    notification_id,
                    NotificationFeedback::InvalidPayloadData,
                )))
                .await?;
                Err(Error::VerificationError(format!(
                    "The payload start version does not match the expected version! Start: {:?}, expected: {:?}",
                    payload_start_version, expected_start_version
                )))
```

**File:** state-sync/state-sync-driver/src/driver.rs (L711-719)
```rust
        } else if let Err(error) = self.bootstrapper.drive_progress(&global_data_summary).await {
            sample!(
                    SampleRate::Duration(Duration::from_secs(DRIVER_ERROR_LOG_FREQ_SECS)),
                    warn!(LogSchema::new(LogEntry::Driver)
                        .error(&error)
                        .message("Error found when checking the bootstrapper progress!"));
            );
            metrics::increment_counter(&metrics::BOOTSTRAPPER_ERRORS, error.get_label());
        };
```

**File:** config/src/config/state_sync_config.rs (L142-142)
```rust
            progress_check_interval_ms: 100,
```

**File:** execution/executor/src/chunk_executor/mod.rs (L214-218)
```rust
    fn reset(&self) -> Result<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["chunk", "reset"]);

        *self.inner.write() = Some(ChunkExecutorInner::new(self.db.clone())?);
        Ok(())
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L35-43)
```rust
const STARTING_SCORE: f64 = 50.0;
/// Add this score on a successful response.
const SUCCESSFUL_RESPONSE_DELTA: f64 = 1.0;
/// Not necessarily a malicious response, but not super useful.
const NOT_USEFUL_MULTIPLIER: f64 = 0.95;
/// Likely to be a malicious response.
const MALICIOUS_MULTIPLIER: f64 = 0.8;
/// Ignore a peer when their score dips below this threshold.
const IGNORE_PEER_THRESHOLD: f64 = 25.0;
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L54-62)
```rust
impl From<ResponseError> for ErrorType {
    fn from(error: ResponseError) -> Self {
        match error {
            ResponseError::InvalidData | ResponseError::InvalidPayloadDataType => {
                ErrorType::NotUseful
            },
            ResponseError::ProofVerificationError => ErrorType::Malicious,
        }
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L143-160)
```rust
    pub fn get_storage_summary_if_not_ignored(&self) -> Option<&StorageServerSummary> {
        if self.is_ignored() {
            None
        } else {
            self.storage_summary.as_ref()
        }
    }

    /// Returns true iff the peer is currently ignored
    fn is_ignored(&self) -> bool {
        // Only ignore peers if the config allows it
        if !self.data_client_config.ignore_low_score_peers {
            return false;
        }

        // Otherwise, ignore peers with a low score
        self.score <= IGNORE_PEER_THRESHOLD
    }
```
