# Audit Report

## Title
Index Invalidation Bug in broadcast() Causes Out-of-Bounds Panic During Subscriber Cleanup

## Summary
The `broadcast()` function in `PeersAndMetadata` contains a critical logic error when removing multiple closed subscribers. The function collects indices of closed channels and then calls `swap_remove()` on each index, but `swap_remove()` invalidates subsequent indices by changing the vector length and element positions. This causes out-of-bounds panics when multiple subscribers close simultaneously, disrupting the node's networking layer.

## Finding Description
The vulnerability exists in the `broadcast()` function [1](#0-0) 

The function iterates through all subscribers and collects the indices of closed channels in a `to_del` vector. After iteration, it attempts to remove these subscribers using `swap_remove()` for each collected index. However, `swap_remove(i)` removes the element at index `i` and replaces it with the last element in the vector, changing both the vector's length and the positions of elements.

**Exploitation scenario:**
1. An attacker (or even normal operations) causes multiple subscribers to close their channels
2. During the next `broadcast()` call, these closed channels are detected
3. For example, if subscribers at indices `[1, 3, 4]` have closed channels in a vector of length 5:
   - First `swap_remove(1)`: Vector becomes length 4, element at index 4 moves to index 1
   - Then `swap_remove(3)`: Vector becomes length 3
   - Finally `swap_remove(4)`: **PANIC** - trying to access index 4 in a vector of length 3

The function is called during critical network events: when peers connect [2](#0-1)  and when peers disconnect [3](#0-2) 

These operations are invoked by the PeerManager during connection lifecycle events [4](#0-3)  and [5](#0-4) 

## Impact Explanation
This qualifies as **High Severity** according to Aptos bug bounty criteria:
- **API crashes**: The panic terminates the PeerManager task, disrupting the node's networking layer
- **Significant protocol violations**: Loss of peer connection management affects the node's ability to participate in consensus

When the PeerManager task panics and terminates [6](#0-5) , the node:
- Loses ability to track peer connections and disconnections
- Cannot properly handle new peer connections
- May become isolated from the network
- For validator nodes, this impacts consensus participation and network liveness

While the panic doesn't directly cause fund loss, it severely degrades node availability and could contribute to consensus disruption if multiple nodes are affected.

## Likelihood Explanation
**Likelihood: Medium to High**

The bug can be triggered through:
1. **Intentional attack**: An application can call `subscribe()` multiple times [7](#0-6) , then drop all receivers simultaneously, causing multiple closed channels to be detected in the next `broadcast()` call
2. **Accidental triggers**: Multiple applications crashing or exiting simultaneously during network events
3. **Natural conditions**: High churn in subscriber applications during periods of network instability

The bug triggers whenever:
- 3+ subscribers close their channels before the next broadcast
- The closed subscribers have indices such that `swap_remove` operations cause index invalidation (e.g., consecutive indices near the end)

## Recommendation
Fix the removal logic to account for index changes during `swap_remove()`. Remove elements in reverse order or use a different approach:

**Option 1: Remove in reverse order**
```rust
fn broadcast(&self, event: ConnectionNotification) {
    let mut listeners = self.subscribers.lock();
    let mut to_del = vec![];
    for i in 0..listeners.len() {
        let dest = listeners.get_mut(i).unwrap();
        if let Err(err) = dest.try_send(event.clone()) {
            match err {
                TrySendError::Full(_) => {
                    sample!(
                        SampleRate::Duration(Duration::from_secs(1)),
                        warn!("PeersAndMetadata.broadcast() failed, some app is slow"),
                    );
                },
                TrySendError::Closed(_) => {
                    to_del.push(i);
                },
            }
        }
    }
    // Remove in reverse order to maintain index validity
    for evict in to_del.into_iter().rev() {
        listeners.swap_remove(evict);
    }
}
```

**Option 2: Use retain**
```rust
fn broadcast(&self, event: ConnectionNotification) {
    let mut listeners = self.subscribers.lock();
    listeners.retain_mut(|dest| {
        match dest.try_send(event.clone()) {
            Ok(_) => true,
            Err(TrySendError::Full(_)) => {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    warn!("PeersAndMetadata.broadcast() failed, some app is slow"),
                );
                true
            },
            Err(TrySendError::Closed(_)) => false,
        }
    });
}
```

## Proof of Concept
```rust
#[cfg(test)]
mod test_broadcast_panic {
    use super::*;
    use tokio::sync::mpsc;

    #[tokio::test]
    async fn test_multiple_closed_subscribers_panic() {
        let peers_and_metadata = PeersAndMetadata::new(&[NetworkId::Validator]);
        
        // Subscribe 5 times
        let mut receivers = vec![];
        for _ in 0..5 {
            receivers.push(peers_and_metadata.subscribe());
        }
        
        // Drop receivers at indices 1, 3, 4 to trigger the bug
        drop(receivers.remove(4)); // Index 4
        drop(receivers.remove(3)); // Index 3  
        drop(receivers.remove(1)); // Index 1
        
        // Allow channel close to propagate
        tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
        
        // Trigger broadcast - this should panic with the bug
        let conn_meta = ConnectionMetadata::mock(PeerId::random());
        let _ = peers_and_metadata.insert_connection_metadata(
            PeerNetworkId::new(NetworkId::Validator, PeerId::random()),
            conn_meta,
        );
        // If the bug exists, the above call will panic with "index out of bounds"
    }
}
```

## Notes
The mutex synchronization between `broadcast()` and `subscribe()` [8](#0-7)  prevents true data races, but does not protect against the logic error in the removal algorithm. The issue is purely in the index invalidation caused by the sequential `swap_remove()` operations on stale indices.

### Citations

**File:** network/framework/src/application/storage.rs (L53-53)
```rust
    subscribers: Mutex<Vec<tokio::sync::mpsc::Sender<ConnectionNotification>>>,
```

**File:** network/framework/src/application/storage.rs (L186-214)
```rust
    pub fn insert_connection_metadata(
        &self,
        peer_network_id: PeerNetworkId,
        connection_metadata: ConnectionMetadata,
    ) -> Result<(), Error> {
        // Grab the write lock for the peer metadata
        let mut peers_and_metadata = self.peers_and_metadata.write();

        // Fetch the peer metadata for the given network
        let peer_metadata_for_network =
            get_peer_metadata_for_network(&peer_network_id, &mut peers_and_metadata)?;

        // Update the metadata for the peer or insert a new entry
        peer_metadata_for_network
            .entry(peer_network_id.peer_id())
            .and_modify(|peer_metadata| {
                peer_metadata.connection_metadata = connection_metadata.clone()
            })
            .or_insert_with(|| PeerMetadata::new(connection_metadata.clone()));

        // Update the cached peers and metadata
        self.set_cached_peers_and_metadata(peers_and_metadata.clone());

        let event =
            ConnectionNotification::NewPeer(connection_metadata, peer_network_id.network_id());
        self.broadcast(event);

        Ok(())
    }
```

**File:** network/framework/src/application/storage.rs (L219-262)
```rust
    pub fn remove_peer_metadata(
        &self,
        peer_network_id: PeerNetworkId,
        connection_id: ConnectionId,
    ) -> Result<PeerMetadata, Error> {
        // Grab the write lock for the peer metadata
        let mut peers_and_metadata = self.peers_and_metadata.write();

        // Fetch the peer metadata for the given network
        let peer_metadata_for_network =
            get_peer_metadata_for_network(&peer_network_id, &mut peers_and_metadata)?;

        // Remove the peer metadata for the peer
        let peer_metadata = if let Entry::Occupied(entry) =
            peer_metadata_for_network.entry(peer_network_id.peer_id())
        {
            // Don't remove the peer if the connection doesn't match!
            // For now, remove the peer entirely, we could in the future
            // have multiple connections for a peer
            let active_connection_id = entry.get().connection_metadata.connection_id;
            if active_connection_id == connection_id {
                let peer_metadata = entry.remove();
                let event = ConnectionNotification::LostPeer(
                    peer_metadata.connection_metadata.clone(),
                    peer_network_id.network_id(),
                );
                self.broadcast(event);
                peer_metadata
            } else {
                return Err(Error::UnexpectedError(format!(
                    "The peer connection id did not match! Given: {:?}, found: {:?}.",
                    connection_id, active_connection_id
                )));
            }
        } else {
            // Unable to find the peer metadata for the given peer
            return Err(missing_peer_metadata_error(&peer_network_id));
        };

        // Update the cached peers and metadata
        self.set_cached_peers_and_metadata(peers_and_metadata.clone());

        Ok(peer_metadata)
    }
```

**File:** network/framework/src/application/storage.rs (L371-395)
```rust
    fn broadcast(&self, event: ConnectionNotification) {
        let mut listeners = self.subscribers.lock();
        let mut to_del = vec![];
        for i in 0..listeners.len() {
            let dest = listeners.get_mut(i).unwrap();
            if let Err(err) = dest.try_send(event.clone()) {
                match err {
                    TrySendError::Full(_) => {
                        // Tried to send to an app, but the app isn't handling its messages fast enough.
                        // Drop message. Maybe increment a metrics counter?
                        sample!(
                            SampleRate::Duration(Duration::from_secs(1)),
                            warn!("PeersAndMetadata.broadcast() failed, some app is slow"),
                        );
                    },
                    TrySendError::Closed(_) => {
                        to_del.push(i);
                    },
                }
            }
        }
        for evict in to_del.into_iter() {
            listeners.swap_remove(evict);
        }
    }
```

**File:** network/framework/src/application/storage.rs (L400-419)
```rust
    pub fn subscribe(&self) -> tokio::sync::mpsc::Receiver<ConnectionNotification> {
        let (sender, receiver) = tokio::sync::mpsc::channel(NOTIFICATION_BACKLOG);
        let peers_and_metadata = self.peers_and_metadata.read();
        'outer: for (network_id, network_peers_and_metadata) in peers_and_metadata.iter() {
            for (_addr, peer_metadata) in network_peers_and_metadata.iter() {
                let event = ConnectionNotification::NewPeer(
                    peer_metadata.connection_metadata.clone(),
                    *network_id,
                );
                if let Err(err) = sender.try_send(event) {
                    warn!("could not send initial NewPeer on subscribe(): {:?}", err);
                    break 'outer;
                }
            }
        }
        // I expect the peers_and_metadata read lock to still be in effect until after listeners.push() below
        let mut listeners = self.subscribers.lock();
        listeners.push(sender);
        receiver
    }
```

**File:** network/framework/src/peer_manager/mod.rs (L407-420)
```rust
    fn remove_peer_from_metadata(&mut self, peer_id: AccountAddress, connection_id: ConnectionId) {
        let peer_network_id = PeerNetworkId::new(self.network_context.network_id(), peer_id);
        if let Err(error) = self
            .peers_and_metadata
            .remove_peer_metadata(peer_network_id, connection_id)
        {
            warn!(
                NetworkSchema::new(&self.network_context),
                "Failed to remove peer from peers and metadata. Peer: {:?}, error: {:?}",
                peer_network_id,
                error
            );
        }
    }
```

**File:** network/framework/src/peer_manager/mod.rs (L684-687)
```rust
        self.peers_and_metadata.insert_connection_metadata(
            PeerNetworkId::new(self.network_context.network_id(), peer_id),
            conn_meta.clone(),
        )?;
```

**File:** network/framework/src/peer_manager/builder.rs (L358-359)
```rust
        executor.spawn(peer_manager.start());
        debug!("{} Started peer manager", self.network_context);
```
