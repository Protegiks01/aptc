# Audit Report

## Title
Deadlock in Executor-Benchmark Pipeline When skip_commit=true AND split_stages=true Due to Insufficient Buffer Sizing

## Summary
When both `skip_commit=true` and `split_stages=true` configuration flags are enabled, the `ledger_update_sender` channel buffer is undersized, creating a deadlock condition where the execution thread blocks on sending while the ledger update thread waits for a start signal that never arrives.

## Finding Description

The vulnerability exists in the channel buffer sizing logic combined with the thread synchronization pattern: [1](#0-0) 

When `split_stages=true`, the ledger update thread uses a delayed consumption pattern where it waits for a signal before processing messages: [2](#0-1) [3](#0-2) 

The ledger update thread waits at line 279 for `start_ledger_update_rx` before consuming any messages. This signal is only sent by the execution thread after processing all blocks: [4](#0-3) 

Meanwhile, the execution thread sends messages to the bounded `ledger_update_sender` channel for each block: [5](#0-4) 

**The Deadlock Scenario:**

1. Buffer is sized to `(num_blocks + 1).max(3)` slots
2. Execution thread processes blocks, sending one message per block to the bounded channel
3. If the number of messages exceeds buffer capacity, execution thread blocks on `send()`
4. Ledger update thread is waiting for the completion signal from execution thread
5. Execution thread cannot send the signal because it's blocked on `send()`
6. **DEADLOCK**: Both threads wait forever

**Critical Asymmetry:**

The `executable_block_sender` uses `.max(50)` for its buffer when `split_stages=true`: [6](#0-5) 

But `ledger_update_sender` only uses `.max(3)`, creating the vulnerability when `num_blocks` is small or underestimated.

## Impact Explanation

This issue affects the executor-benchmark tool used for performance testing and CI/CD pipelines. While not directly affecting production blockchain nodes, it represents a **Medium Severity** issue because:

1. **Development Infrastructure Disruption**: Hangs in CI/CD pipelines prevent performance regression testing
2. **Resource Waste**: Deadlocked processes consume system resources indefinitely
3. **Misdiagnosis Risk**: Deadlocks may be misinterpreted as performance issues rather than configuration bugs

Under the Aptos bug bounty criteria, this falls under **Medium Severity** as it affects critical development infrastructure and could cause "state inconsistencies requiring intervention" in benchmarking workflows.

## Likelihood Explanation

**Likelihood: Medium to High**

The deadlock is easily triggered when:
- Running benchmarks with `--split_stages --skip_commit` flags (both are valid CLI options)
- Using small `num_blocks` values (≤3) where buffer = 3
- Processing ≥4 blocks in the workload
- Any mismatch between expected and actual block count [7](#0-6) 

Both flags are legitimate configuration options that users may combine for specific testing scenarios.

## Recommendation

Change the buffer sizing logic to use a larger minimum buffer size when `split_stages=true`, matching the pattern used for `executable_block_sender`:

```rust
let (ledger_update_sender, ledger_update_receiver) =
    mpsc::sync_channel::<LedgerUpdateMessage>(
        if config.split_stages {
            (num_blocks.unwrap() + 1).max(50)  // Match executable_block_sender
        } else if config.skip_commit {
            (num_blocks.unwrap() + 1).max(3)
        } else {
            3
        },
    );
```

This ensures the buffer is large enough to hold all messages before the consumer starts when using the delayed consumption pattern.

**Alternative Fix**: Remove the delayed consumption pattern for ledger_update_thread when only `skip_commit=true` (without `split_stages=true`), but this would change benchmarking behavior.

## Proof of Concept

**Reproduction Steps:**

1. Run executor-benchmark with the following configuration:
   ```bash
   cargo run --release -p aptos-executor-benchmark -- \
     --block-size 100 \
     --num-blocks 2 \
     --split_stages \
     --skip_commit \
     <other required params>
   ```

2. Expected behavior: Buffer size = max(3, 3) = 3 slots

3. If the workload generates 4 or more blocks (or if block processing creates more messages than expected):
   - Message 1-3: Successfully buffered
   - Message 4: Execution thread blocks on `send()`
   - Ledger update thread waits for signal that never arrives
   - **Process hangs indefinitely**

**Verification:**
Monitor thread states - execution thread will be blocked in `mpsc::SyncSender::send()` while ledger update thread is blocked in `mpsc::Receiver::recv()` on the start signal channel.

## Notes

This vulnerability specifically manifests when the `split_stages` flag creates a delayed consumption pattern while the buffer sizing doesn't account for the need to buffer all messages before consumption begins. The asymmetry between `executable_block_sender` (`.max(50)`) and `ledger_update_sender` (`.max(3)`) suggests this was an oversight in the buffer sizing logic.

While this affects benchmarking infrastructure rather than production blockchain nodes, it represents a real liveness failure in critical testing tools.

### Citations

**File:** execution/executor-benchmark/src/pipeline.rs (L93-100)
```rust
        let (executable_block_sender, executable_block_receiver) =
            mpsc::sync_channel::<ExecuteBlockMessage>(
                if config.split_stages {
                    (num_blocks.unwrap() + 1).max(50)
                } else {
                    10
                }, /* bound */
            );
```

**File:** execution/executor-benchmark/src/pipeline.rs (L102-109)
```rust
        let (ledger_update_sender, ledger_update_receiver) =
            mpsc::sync_channel::<LedgerUpdateMessage>(
                if config.split_stages || config.skip_commit {
                    (num_blocks.unwrap() + 1).max(3)
                } else {
                    3
                }, /* bound */
            );
```

**File:** execution/executor-benchmark/src/pipeline.rs (L122-123)
```rust
        let (start_ledger_update_tx, start_ledger_update_rx) =
            create_start_tx_rx(config.split_stages);
```

**File:** execution/executor-benchmark/src/pipeline.rs (L270-270)
```rust
                start_ledger_update_tx.map(|tx| tx.send(()));
```

**File:** execution/executor-benchmark/src/pipeline.rs (L276-291)
```rust
        let ledger_update_thread = std::thread::Builder::new()
            .name("ledger_update".to_string())
            .spawn(move || {
                start_ledger_update_rx.map(|rx| rx.recv());

                while let Ok(ledger_update_msg) = ledger_update_receiver.recv() {
                    NUM_TXNS
                        .inc_with_by(&["ledger_update"], ledger_update_msg.num_input_txns as u64);
                    ledger_update_stage.ledger_update(ledger_update_msg);
                }
                start_commit_tx.map(|tx| tx.send(()));

                0
            })
            .expect("Failed to spawn ledger update thread.");
        join_handles.push(ledger_update_thread);
```

**File:** execution/executor-benchmark/src/transaction_executor.rs (L86-86)
```rust
        self.ledger_update_sender.send(msg).unwrap();
```

**File:** execution/executor-benchmark/src/main.rs (L153-157)
```rust
    split_stages: bool,
    /// Skip commit stage - i.e. create executed blocks in memory, but never commit them.
    /// Useful when commit is the bottleneck, to see throughput of the rest of the pipeline.
    #[clap(long)]
    skip_commit: bool,
```
