# Audit Report

## Title
Memory Exhaustion via Gzip Decompression Bomb in Telemetry Service Endpoints

## Summary
The telemetry service contains multiple endpoints that accept gzip-compressed payloads without enforcing limits on decompressed data size. An authenticated attacker can send a small compressed payload that expands to gigabytes during decompression, causing memory exhaustion and service crashes.

## Finding Description

The telemetry service implements several data ingestion endpoints that accept gzip-compressed payloads. These endpoints decompress the data without limiting the decompressed size, creating a classic decompression bomb vulnerability.

**Affected Endpoints:**

1. **Custom Contract Log Ingestion** - The endpoint accepts gzip-compressed log data without any size limits on either the compressed or decompressed payload. [1](#0-0) 

The handler decompresses the entire payload into memory without bounds: [2](#0-1) 

2. **Custom Contract Metrics Ingestion** - Accepts arbitrary-sized payloads without limits: [3](#0-2) 

3. **Regular Log Ingestion** - While this endpoint enforces a 1MB limit on compressed data, it still doesn't limit decompressed size: [4](#0-3) 

The decompression happens without size validation: [5](#0-4) 

The `MAX_CONTENT_LENGTH` constant is only 1MB: [6](#0-5) 

**Attack Flow:**

1. Attacker authenticates to custom contract endpoints (via on-chain allowlist and JWT) or uses validator credentials for regular endpoints
2. Creates a gzip bomb: highly compressible data (e.g., repeated zeros) that compresses to <1MB but expands to gigabytes
3. Sends POST request with `Content-Encoding: gzip` header
4. Service decompresses payload using `GzDecoder` with no memory limits
5. Service exhausts available memory and crashes

**Root Cause:**

The endpoints use `warp::body::bytes()` without `warp::body::content_length_limit()` for custom contract endpoints, or with limits only on compressed size for regular endpoints. The `GzDecoder::read_to_end()` and `serde_json::from_reader()` calls consume the entire decompressed stream into memory without intermediate size checks.

## Impact Explanation

**Severity: High**

This vulnerability causes **API crashes** and **service unavailability**, which qualifies as High severity under the Aptos bug bounty program.

**Specific Impacts:**

1. **Telemetry Service Denial of Service**: The telemetry collection infrastructure can be crashed, blinding operators from monitoring validator health and network metrics
2. **Resource Exhaustion**: Memory exhaustion can cause cascading failures if the service shares resources with other components
3. **Operational Impact**: Loss of telemetry data during attacks disrupts incident response and troubleshooting capabilities

**Note on Scope**: This vulnerability affects the telemetry service infrastructure, NOT the core consensus nodes. Validator nodes that send telemetry data are unaffected by telemetry service crashes. The blockchain continues operating normally even if telemetry collection fails.

## Likelihood Explanation

**Likelihood: Medium-High**

**For Custom Contract Endpoints:**
- Requires authentication via on-chain allowlist and JWT token
- Any allowlisted entity (storage providers, monitoring services) can exploit this
- Attack is trivial to execute (standard gzip bomb techniques)

**For Regular Endpoints:**
- Requires validator or node credentials
- Compromised or malicious node operators can exploit this
- Attack is also trivial to execute

The attack requires some level of privileged access (authentication), but once authenticated, exploitation is straightforward with no additional barriers.

## Recommendation

**Immediate Fix**: Enforce strict limits on decompressed payload sizes across all endpoints.

**Implementation:**

1. **Add decompressed size limit constant:**
```rust
// In constants.rs
pub const MAX_DECOMPRESSED_CONTENT_LENGTH: u64 = 10 * 1024 * 1024; // 10MB
```

2. **Use a size-limited reader wrapper:**
```rust
use std::io::{self, Read};

struct LimitedReader<R> {
    inner: R,
    limit: u64,
    read: u64,
}

impl<R: Read> Read for LimitedReader<R> {
    fn read(&mut self, buf: &mut [u8]) -> io::Result<usize> {
        if self.read >= self.limit {
            return Err(io::Error::new(
                io::ErrorKind::Other,
                "decompressed size limit exceeded"
            ));
        }
        let max_read = std::cmp::min(buf.len() as u64, self.limit - self.read) as usize;
        let n = self.inner.read(&mut buf[..max_read])?;
        self.read += n as u64;
        Ok(n)
    }
}
```

3. **Apply to decompression in log_ingest.rs:**
```rust
let log_messages: Vec<String> = if let Some(encoding) = encoding {
    if encoding.eq_ignore_ascii_case("gzip") {
        let decoder = GzDecoder::new(body.reader());
        let limited = LimitedReader { 
            inner: decoder, 
            limit: MAX_DECOMPRESSED_CONTENT_LENGTH,
            read: 0 
        };
        serde_json::from_reader(limited).map_err(|e| {
            debug!("unable to decode and deserialize body: {}", e);
            ServiceError::bad_request(LogIngestError::UnexpectedPayloadBody.into())
        })?
    } else {
        return Err(reject::custom(ServiceError::bad_request(
            LogIngestError::UnexpectedContentEncoding.into(),
        )));
    }
} else {
    serde_json::from_reader(body.reader()).map_err(|e| {
        error!("unable to deserialize body: {}", e);
        ServiceError::bad_request(LogIngestError::UnexpectedPayloadBody.into())
    })?
};
```

4. **Add content_length_limit to custom contract endpoints:**
```rust
// In custom_contract_ingest.rs
pub fn log_ingest(context: Context) -> BoxedFilter<(impl Reply,)> {
    warp::path!("custom-contract" / String / "ingest" / "logs")
        .and(warp::post())
        .and(context.clone().filter())
        .and(with_custom_contract_auth(context.clone()))
        .and(warp::header::optional("content-encoding"))
        .and(warp::body::content_length_limit(MAX_CONTENT_LENGTH))  // ADD THIS
        .and(warp::body::bytes())
        .and_then(handle_log_ingest)
        .boxed()
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod decompression_bomb_test {
    use flate2::write::GzEncoder;
    use flate2::Compression;
    use std::io::Write;

    #[test]
    fn test_gzip_bomb_creation() {
        // Create a payload that compresses well
        // 10MB of zeros compresses to ~10KB
        let uncompressed_size = 10 * 1024 * 1024; // 10MB
        let zeros = vec![0u8; uncompressed_size];
        
        // Compress the data
        let mut encoder = GzEncoder::new(Vec::new(), Compression::best());
        encoder.write_all(&zeros).unwrap();
        let compressed = encoder.finish().unwrap();
        
        println!("Uncompressed size: {} bytes", uncompressed_size);
        println!("Compressed size: {} bytes", compressed.len());
        println!("Compression ratio: {}x", uncompressed_size / compressed.len());
        
        // Verify this is under 1MB compressed
        assert!(compressed.len() < 1024 * 1024);
        
        // An attacker could send this to custom contract endpoints with no limits,
        // or to regular endpoints (compressed size is under 1MB limit)
        // Upon decompression, this expands to 10MB
        // With larger or more compressible data, could expand to gigabytes
    }

    #[test]
    fn test_extreme_gzip_bomb() {
        // Create a more extreme example: 100MB of zeros
        let uncompressed_size = 100 * 1024 * 1024; // 100MB
        let zeros = vec![0u8; uncompressed_size];
        
        let mut encoder = GzEncoder::new(Vec::new(), Compression::best());
        encoder.write_all(&zeros).unwrap();
        let compressed = encoder.finish().unwrap();
        
        println!("Extreme bomb - Uncompressed: {} MB", uncompressed_size / 1024 / 1024);
        println!("Extreme bomb - Compressed: {} KB", compressed.len() / 1024);
        println!("Compression ratio: {}x", uncompressed_size / compressed.len());
        
        // This compresses to ~100KB but expands to 100MB
        // Could easily exhaust service memory
        assert!(compressed.len() < 200 * 1024); // Under 200KB compressed
    }
}
```

## Notes

- This vulnerability is in the **telemetry service infrastructure**, not the core blockchain consensus/execution layer. Exploiting it does not affect validator operation, consensus safety, or blockchain liveness.

- The telemetry service collects monitoring data FROM nodes but runs as separate infrastructure. A crash only impacts observability, not chain security.

- Both custom contract endpoints and regular validator endpoints are affected, though custom contract endpoints are worse due to the complete absence of size limits.

- The `TelemetryDump` struct itself in `telemetry.rs` is not directly vulnerable - the issue lies in how the endpoint handlers process compressed data before deserialization.

### Citations

**File:** crates/aptos-telemetry-service/src/custom_contract_ingest.rs (L29-37)
```rust
pub fn metrics_ingest(context: Context) -> BoxedFilter<(impl Reply,)> {
    warp::path!("custom-contract" / String / "ingest" / "metrics")
        .and(warp::post())
        .and(context.clone().filter())
        .and(with_custom_contract_auth(context.clone()))
        .and(warp::header::optional("content-encoding"))
        .and(warp::body::bytes())
        .and_then(handle_metrics_ingest)
        .boxed()
```

**File:** crates/aptos-telemetry-service/src/custom_contract_ingest.rs (L143-151)
```rust
pub fn log_ingest(context: Context) -> BoxedFilter<(impl Reply,)> {
    warp::path!("custom-contract" / String / "ingest" / "logs")
        .and(warp::post())
        .and(context.clone().filter())
        .and(with_custom_contract_auth(context.clone()))
        .and(warp::header::optional("content-encoding"))
        .and(warp::body::bytes())
        .and_then(handle_log_ingest)
        .boxed()
```

**File:** crates/aptos-telemetry-service/src/custom_contract_ingest.rs (L193-206)
```rust
    let log_data = if content_encoding.as_deref() == Some("gzip") {
        let mut decoder = GzDecoder::new(&body[..]);
        let mut decompressed = Vec::new();
        decoder.read_to_end(&mut decompressed).map_err(|_| {
            record_custom_contract_error(
                &contract_name,
                CustomContractEndpoint::LogsIngest,
                CustomContractErrorType::InvalidPayload,
            );
            reject::custom(ServiceError::bad_request(
                LogIngestError::UnexpectedContentEncoding.into(),
            ))
        })?;
        decompressed
```

**File:** crates/aptos-telemetry-service/src/log_ingest.rs (L34-36)
```rust
        .and(warp::header::optional(CONTENT_ENCODING.as_str()))
        .and(warp::body::content_length_limit(MAX_CONTENT_LENGTH))
        .and(warp::body::aggregate())
```

**File:** crates/aptos-telemetry-service/src/log_ingest.rs (L64-70)
```rust
    let log_messages: Vec<String> = if let Some(encoding) = encoding {
        if encoding.eq_ignore_ascii_case("gzip") {
            let decoder = GzDecoder::new(body.reader());
            serde_json::from_reader(decoder).map_err(|e| {
                debug!("unable to decode and deserialize body: {}", e);
                ServiceError::bad_request(LogIngestError::UnexpectedPayloadBody.into())
            })?
```

**File:** crates/aptos-telemetry-service/src/constants.rs (L4-5)
```rust
/// The maximum content length to accept in the http body.
pub const MAX_CONTENT_LENGTH: u64 = 1024 * 1024;
```
