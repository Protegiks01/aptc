# Audit Report

## Title
State Synchronization Stall: EpochEndingStreamEngine Initialization with Stale Advertised Data Causes Incomplete Epoch Sync

## Summary
The `EpochEndingStreamEngine` captures and permanently stores the highest available epoch from `advertised_data` at stream initialization time. If the blockchain progresses past this epoch after initialization but before stream completion, the stream prematurely marks itself as complete, leaving nodes with missing epoch ending ledger infos and incomplete validator set knowledge.

## Finding Description

**Clarification on Security Question**: The security question incorrectly states that both `EpochEndingStreamEngine` and `GetAllStatesRequest` use `advertised_data`. In reality, only `EpochEndingStreamEngine` uses it. [1](#0-0) 

**Vulnerability Root Cause**:

In `EpochEndingStreamEngine::new()`, the engine captures `end_epoch` from the advertised data's highest epoch ending ledger info and stores it immutably: [2](#0-1) 

This `end_epoch` value is then used to bound all data client requests throughout the stream's lifetime: [3](#0-2) 

The stream marks itself complete when it reaches this stored `end_epoch`, even if newer epochs have been committed to the blockchain: [4](#0-3) 

**Attack Path**:

1. At time T0, the global data summary poller collects advertised data from peers showing highest_epoch = N
2. At time T1, a node requests to sync all epoch ending ledger infos via `GetAllEpochEndingLedgerInfos`
3. The streaming service refreshes the global data summary and extracts advertised_data: [5](#0-4) 
4. `EpochEndingStreamEngine` is initialized with `end_epoch = N`
5. At time T2 (during stream operation), the blockchain progresses to epoch N+5
6. The stream successfully syncs epochs up to N, then marks itself complete
7. The node believes it has all epoch ending ledger infos but is actually missing epochs N+1 through N+5
8. Missing epoch ending ledger infos means the node lacks critical information about validator set changes and epoch boundaries

**Broken Invariant**: This violates the **State Consistency** invariant - nodes must have complete and accurate state information to participate correctly in consensus and validate transactions across epoch boundaries.

## Impact Explanation

This vulnerability qualifies as **HIGH severity** under the Aptos bug bounty criteria for the following reasons:

1. **Significant Protocol Violations**: Nodes with incomplete epoch ending ledger infos cannot properly validate epoch transitions or track validator set changes, affecting their ability to participate in consensus correctly.

2. **State Inconsistencies**: Different nodes may sync to different "final" epochs depending on when they initialized their streams relative to blockchain progression, creating state divergence across the network.

3. **Validator Node Slowdowns**: Validators missing epoch ending ledger infos may experience operational issues when attempting to participate in consensus for newer epochs, potentially causing performance degradation.

4. **Consensus Participation Issues**: Epoch ending ledger infos contain critical validator set information. Missing these can prevent nodes from properly validating blocks and participating in AptosBFT consensus.

The global data summary aggregation mechanism attempts to mitigate malicious peer influence by taking the maximum epoch across all peers: [6](#0-5) 

However, this does not protect against staleness caused by timing delays between polling and stream initialization.

## Likelihood Explanation

This vulnerability has **MEDIUM-HIGH likelihood** of occurrence:

1. **Natural Race Condition**: This is not an attack requiring manipulation - it happens naturally during normal network operation whenever the blockchain progresses between stream initialization and data fetching.

2. **Polling Intervals**: The global data summary is refreshed at configurable intervals. If the interval is long (e.g., several seconds) and epochs are progressing rapidly, staleness is highly likely.

3. **Epoch Transition Timing**: Most likely to occur during periods of rapid epoch progression or immediately after network partitions heal.

4. **No Mitigation**: Unlike `StateStreamEngine` which dynamically fetches the number of states at runtime, `EpochEndingStreamEngine` locks in the target epoch at initialization with no mechanism to detect or recover from staleness.

5. **No Detection**: The stream completes successfully and reports no errors, so nodes have no indication they're missing data.

## Recommendation

**Fix Strategy**: Implement dynamic epoch boundary detection similar to how `StateStreamEngine` handles state counts. Instead of capturing `end_epoch` once at initialization, the engine should periodically check for new epochs and extend the sync range.

**Recommended Code Changes**:

1. Remove the immutable `end_epoch` field from initialization
2. Add a method to dynamically query the latest available epoch from fresh advertised data
3. Update `create_data_client_requests()` to check for new epochs before declaring the stream complete
4. Add a completion check that verifies no newer epochs are available before marking `stream_is_complete = true`

**Alternative Fix**: If the above is too complex, at minimum add a validation step in `transform_client_response_into_notification()` that compares the current `end_epoch` against the latest `global_data_summary.advertised_data` and logs a warning or extends the stream if newer epochs are detected.

## Proof of Concept

**Rust Integration Test** (to be placed in `state-sync/data-streaming-service/src/tests/`):

```rust
#[tokio::test]
async fn test_epoch_ending_stream_stale_advertised_data() {
    // Setup: Create a mock data client with epoch ending ledger infos for epochs 0-100
    let (mut mock_client, mut mock_stream) = create_mock_data_client();
    
    // Simulate initial advertised data showing highest epoch = 100
    let initial_advertised_data = create_advertised_data_with_epochs(0, 100);
    
    // Step 1: Initialize EpochEndingStreamEngine with stale advertised data
    let request = GetAllEpochEndingLedgerInfosRequest {
        start_epoch: 90,
    };
    let engine = EpochEndingStreamEngine::new(&request, &initial_advertised_data).unwrap();
    
    // Verify end_epoch is set to 100
    assert_eq!(engine.end_epoch, 100);
    
    // Step 2: Simulate blockchain progression - epochs 101-105 are now available
    mock_client.add_epoch_ending_ledger_infos(101, 105);
    
    // Step 3: Stream requests and receives data up to epoch 100
    // (simulate normal stream operation here)
    
    // Step 4: Verify the stream marks itself complete after epoch 100
    // even though epochs 101-105 are now available
    assert!(engine.is_stream_complete());
    
    // Step 5: Verify epochs 101-105 were never requested
    let requested_epochs = mock_stream.get_requested_epochs();
    assert!(!requested_epochs.contains(&101));
    assert!(!requested_epochs.contains(&105));
    
    // Result: Node is missing critical epoch ending ledger infos
}
```

**Expected Behavior**: The stream should detect that new epochs are available and continue syncing rather than prematurely marking itself complete.

**Actual Behavior**: The stream completes after syncing to epoch 100, leaving the node with incomplete epoch information and potential consensus participation issues.

### Citations

**File:** state-sync/data-streaming-service/src/stream_engine.rs (L145-147)
```rust
            StreamRequest::GetAllStates(request) => Ok(StateStreamEngine::new(request)?.into()),
            StreamRequest::GetAllEpochEndingLedgerInfos(request) => {
                Ok(EpochEndingStreamEngine::new(request, advertised_data)?.into())
```

**File:** state-sync/data-streaming-service/src/stream_engine.rs (L1483-1517)
```rust
    fn new(
        request: &GetAllEpochEndingLedgerInfosRequest,
        advertised_data: &AdvertisedData,
    ) -> Result<Self, Error> {
        let end_epoch = advertised_data
            .highest_epoch_ending_ledger_info()
            .ok_or_else(|| {
                Error::DataIsUnavailable(format!(
                    "Unable to find any epoch ending ledger info in the network: {:?}",
                    advertised_data
                ))
            })?;

        if end_epoch < request.start_epoch {
            return Err(Error::DataIsUnavailable(format!(
                "The epoch to start syncing from is higher than the highest epoch ending ledger info! Highest: {:?}, start: {:?}",
                end_epoch, request.start_epoch
            )));
        }
        info!(
            (LogSchema::new(LogEntry::ReceivedDataResponse)
                .event(LogEvent::Success)
                .message(&format!(
                    "Setting the highest epoch ending ledger info for the stream at: {:?}",
                    end_epoch
                )))
        );

        Ok(EpochEndingStreamEngine {
            request: request.clone(),
            end_epoch,
            next_stream_epoch: request.start_epoch,
            next_request_epoch: request.start_epoch,
            stream_is_complete: false,
        })
```

**File:** state-sync/data-streaming-service/src/stream_engine.rs (L1541-1568)
```rust
    fn create_data_client_requests(
        &mut self,
        max_number_of_requests: u64,
        max_in_flight_requests: u64,
        num_in_flight_requests: u64,
        global_data_summary: &GlobalDataSummary,
        _unique_id_generator: Arc<U64IdGenerator>,
    ) -> Result<Vec<DataClientRequest>, Error> {
        // Calculate the number of requests to send
        let num_requests_to_send = calculate_num_requests_to_send(
            max_number_of_requests,
            max_in_flight_requests,
            num_in_flight_requests,
        );

        // Create the client requests
        let client_requests = create_data_client_request_batch(
            self.next_request_epoch,
            self.end_epoch,
            num_requests_to_send,
            global_data_summary.optimal_chunk_sizes.epoch_chunk_size,
            self.clone().into(),
        )?;

        // Return the requests
        self.update_request_tracking(&client_requests)?;
        Ok(client_requests)
    }
```

**File:** state-sync/data-streaming-service/src/stream_engine.rs (L1630-1633)
```rust
                // Check if the stream is complete
                if last_received_epoch >= self.end_epoch {
                    self.stream_is_complete = true;
                }
```

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L265-283)
```rust
        // Refresh the cached global data summary
        refresh_global_data_summary(
            self.aptos_data_client.clone(),
            self.global_data_summary.clone(),
        );

        // Create a new data stream
        let stream_id = self.stream_id_generator.next();
        let advertised_data = self.get_global_data_summary().advertised_data.clone();
        let (data_stream, stream_listener) = DataStream::new(
            self.data_client_config,
            self.streaming_service_config,
            stream_id,
            &request_message.stream_request,
            stream_update_notifier,
            self.aptos_data_client.clone(),
            self.notification_id_generator.clone(),
            &advertised_data,
            self.time_service.clone(),
```

**File:** state-sync/data-streaming-service/src/peer_states.rs (L339-408)
```rust

```
