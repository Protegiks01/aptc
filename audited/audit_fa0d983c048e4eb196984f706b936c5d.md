# Audit Report

## Title
Panic Vulnerability in Shamir Secret Sharing Due to Missing Duplicate Index Validation

## Summary
The `lagrange_for_subset()` function in the Shamir secret sharing implementation fails to validate that input indices are unique. When duplicate indices are provided, the vanishing polynomial derivative evaluates to zero at repeated roots, causing `batch_inversion` to panic and crash the node.

## Finding Description

The `lagrange_for_subset()` function computes Lagrange coefficients for Shamir secret reconstruction without validating that the input `indices` array contains unique values. [1](#0-0) 

When duplicate indices exist (e.g., `[1, 2, 2, 3]`), the mathematical computation fails:

1. **Vanishing Polynomial Construction**: The function creates a vanishing polynomial from the roots corresponding to the indices. [2](#0-1) [3](#0-2) 

If `xs_vec` contains duplicates, the vanishing polynomial will have repeated roots: `(X - ω¹)(X - ω²)²(X - ω³)` instead of `(X - ω¹)(X - ω²)(X - ω³)(X - ω⁴)`.

2. **Derivative Evaluation at Repeated Root**: For a polynomial with a repeated root at `ω²`, the derivative evaluates to zero at that point due to the factor `(X - ω²)` remaining after differentiation. [4](#0-3) 

3. **Batch Inversion Panic**: The `batch_inversion` operation uses Montgomery's trick, which requires all elements to be non-zero. When a zero value is encountered, the accumulated product becomes zero, and the inversion operation panics. [5](#0-4) 

The evidence shows similar validation expectations exist elsewhere in the codebase: [6](#0-5) 

**Attack Vector**: The `reconstruct_secret_from_shares` function in the DKG implementation creates Player objects from untrusted input without deduplication: [7](#0-6) 

If `input_player_share_pairs` contains duplicate u64 player IDs, they will be converted to duplicate Player objects and passed to the reconstruction pipeline, eventually triggering the panic.

## Impact Explanation

**Severity: High** (Validator Node Crash / DoS)

This vulnerability can cause validator nodes to crash during secret reconstruction operations, impacting:
- **DKG transcript reconstruction** during epoch transitions
- **Consensus randomness aggregation** via secret sharing
- **Availability**: Affected validators would need restart/recovery

However, exploitation requires specific conditions that limit direct external attack:
- Production code uses `HashMap<Author, SecretShare>` for share aggregation, preventing author-level duplicates
- ValidatorVerifier should map unique Authors to unique Player IDs
- The vulnerable code path appears primarily used in testing/verification contexts

This does NOT meet **Critical** severity because:
- No evidence of direct external exploit path bypassing existing protections
- No consensus safety violation (only availability impact)
- Existing safeguards (HashMap deduplication) mitigate normal operation

## Likelihood Explanation

**Likelihood: Low-to-Medium**

The vulnerability requires specific preconditions:
1. Duplicate player indices reaching `lagrange_for_subset()`
2. Bypassing or bugs in higher-level protections (SecretShareAggregator, ValidatorVerifier)
3. Malformed input to `reconstruct_secret_from_shares`

**Mitigating factors:**
- SecretShareAggregator uses `HashMap<Author, SecretShare>` preventing author duplicates [8](#0-7) 

- Normal operation paths appear protected

**Elevating factors:**
- No explicit validation in the reconstruction pipeline
- Virtual player expansion in weighted configs could theoretically create collisions if misconfigured
- Internal bugs (e.g., validator index assignment errors) could trigger this

## Recommendation

**Add explicit validation for duplicate indices in `lagrange_for_subset()`:**

```rust
pub fn lagrange_for_subset(&self, indices: &[usize]) -> Vec<F> {
    // Step 0: check that subset is large enough
    assert!(
        indices.len() >= self.t,
        "subset size {} is smaller than threshold t={}",
        indices.len(),
        self.t
    );
    
    // NEW: Check for duplicate indices
    let unique_indices: std::collections::HashSet<_> = indices.iter().collect();
    assert_eq!(
        unique_indices.len(),
        indices.len(),
        "indices contain duplicates, which will cause incorrect Lagrange coefficient computation"
    );
    
    // ... rest of function
}
```

**Additionally, add validation in `reconstruct_secret_from_shares`:**

```rust
fn reconstruct_secret_from_shares(
    pub_params: &Self::PublicParams,
    input_player_share_pairs: Vec<(u64, Self::DealtSecretShare)>,
) -> anyhow::Result<Self::DealtSecret> {
    // NEW: Check for duplicate player IDs
    let player_ids: Vec<u64> = input_player_share_pairs.iter().map(|(id, _)| *id).collect();
    let unique_ids: std::collections::HashSet<_> = player_ids.iter().collect();
    ensure!(
        unique_ids.len() == player_ids.len(),
        "input_player_share_pairs contains duplicate player IDs"
    );
    
    // ... rest of function
}
```

## Proof of Concept

```rust
#[test]
#[should_panic(expected = "attempt to invert zero")]
fn test_lagrange_duplicate_indices_panic() {
    use ark_bn254::Fr;
    use crate::arkworks::shamir::ShamirThresholdConfig;
    
    let t = 3;
    let n = 5;
    let config = ShamirThresholdConfig::<Fr>::new(t, n);
    
    // Create indices with a duplicate (index 2 appears twice)
    let indices_with_duplicate = vec![1, 2, 2, 3];
    
    // This should panic due to zero in batch_inversion
    let _lagrange_coeffs = config.lagrange_for_subset(&indices_with_duplicate);
}
```

**Notes:**

While this is a legitimate defensive programming issue that should be fixed, the **practical exploitability is limited** due to existing safeguards in production code paths. The vulnerability primarily represents a **robustness concern** rather than a direct external attack vector. However, it should still be addressed to prevent potential crashes from internal bugs or edge cases in validator index management.

### Citations

**File:** crates/aptos-crypto/src/arkworks/shamir.rs (L253-260)
```rust
    pub fn lagrange_for_subset(&self, indices: &[usize]) -> Vec<F> {
        // Step 0: check that subset is large enough
        assert!(
            indices.len() >= self.t,
            "subset size {} is smaller than threshold t={}",
            indices.len(),
            self.t
        );
```

**File:** crates/aptos-crypto/src/arkworks/shamir.rs (L262-266)
```rust
        let xs_vec: Vec<F> = indices.iter().map(|i| self.domain.element(*i)).collect();

        // Step 1: compute poly w/ roots at all x in xs, compute eval at 0
        let vanishing_poly = vanishing_poly::from_roots(&xs_vec);
        let vanishing_poly_at_0 = vanishing_poly.coeffs[0]; // vanishing_poly(0) = const term
```

**File:** crates/aptos-crypto/src/arkworks/shamir.rs (L276-282)
```rust
        // Step 3a (denominators): Compute derivative of poly from step 1, and its evaluations
        let derivative = vanishing_poly.differentiate();
        let derivative_evals = derivative.evaluate_over_domain(self.domain).evals; // TODO: with a filter perhaps we don't have to store all evals, but then batch inversion becomes a bit more tedious

        // Step 3b: Only keep the relevant evaluations, then perform a batch inversion
        let mut denominators: Vec<F> = indices.iter().map(|i| derivative_evals[*i]).collect();
        batch_inversion(&mut denominators);
```

**File:** crates/aptos-crypto/src/arkworks/vanishing_poly.rs (L20-52)
```rust
pub fn from_roots<F: FftField>(roots: &[F]) -> DensePolynomial<F> {
    match roots.len() {
        0 => DensePolynomial::from_coefficients_vec(vec![F::one()]), // Is this correct? F::one() or empty vec?
        1 => DensePolynomial::from_coefficients_vec(vec![-roots[0], F::one()]),
        2 => {
            let (a, b) = (roots[0], roots[1]);
            DensePolynomial::from_coefficients_vec(vec![a * b, -(a + b), F::one()])
        },
        3 => {
            let (a, b, c) = (roots[0], roots[1], roots[2]);
            DensePolynomial::from_coefficients_vec(vec![
                -(a * b * c),
                a * b + a * c + b * c,
                -(a + b + c),
                F::one(),
            ])
        }, // Not sure 2 and 3 are really useful
        _ => {
            let mid = roots.len() / 2;
            let (left, right) =
                rayon::join(|| from_roots(&roots[..mid]), || from_roots(&roots[mid..]));

            let result_len = left.coeffs.len() + right.coeffs.len() - 1;
            let dom_size = result_len.next_power_of_two();

            if dom_size < FFT_THRESH {
                naive_poly_mul(&left, &right)
            } else {
                &left * &right
            }
        },
    }
}
```

**File:** crates/aptos-dkg/benches/serialization.rs (L87-103)
```rust
fn batch_inversion<F: Field>(v: &mut [F]) {
    let mut acc = F::ONE;
    // prefix products
    let mut prod = Vec::with_capacity(v.len());
    for x in v.iter() {
        prod.push(acc);
        acc *= x;
    }
    // invert the total product
    acc = acc.invert().unwrap(); // shouldn't happen, the only element with zero z-coordinate in the Weierstrass model is the identity (0 : 1 : 0)
                                 // propagate inverses backwards
    for (x, p) in v.iter_mut().rev().zip(prod.into_iter().rev()) {
        let tmp = acc * *x;
        *x = acc * p;
        acc = tmp;
    }
}
```

**File:** crates/aptos-crypto/src/blstrs/lagrange.rs (L177-182)
```rust
    let mut denominators = Vec::with_capacity(T.len());
    for i in 0..T.len() {
        debug_assert_ne!(Z[T[i]], Scalar::ZERO);
        denominators.push(Z[T[i]]);
    }
    denominators.batch_invert();
```

**File:** types/src/dkg/real_dkg/mod.rs (L470-483)
```rust
    fn reconstruct_secret_from_shares(
        pub_params: &Self::PublicParams,
        input_player_share_pairs: Vec<(u64, Self::DealtSecretShare)>,
    ) -> anyhow::Result<Self::DealtSecret> {
        let player_share_pairs: Vec<_> = input_player_share_pairs
            .clone()
            .into_iter()
            .map(|(x, y)| (Player { id: x as usize }, y.main))
            .collect();
        let reconstructed_secret = <WTrx as Transcript>::DealtSecretKey::reconstruct(
            &pub_params.pvss_config.wconfig,
            &player_share_pairs,
        )
        .unwrap();
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L17-35)
```rust
pub struct SecretShareAggregator {
    self_author: Author,
    shares: HashMap<Author, SecretShare>,
    total_weight: u64,
}

impl SecretShareAggregator {
    pub fn new(self_author: Author) -> Self {
        Self {
            self_author,
            shares: HashMap::new(),
            total_weight: 0,
        }
    }

    pub fn add_share(&mut self, share: SecretShare, weight: u64) {
        if self.shares.insert(share.author, share).is_none() {
            self.total_weight += weight;
        }
```
