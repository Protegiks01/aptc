# Audit Report

## Title
NoiseStream Incomplete Frame DoS: Missing Read Timeout Allows Connection Slot Exhaustion

## Summary
The NoiseStream implementation in `network/framework/src/noise/stream.rs` lacks timeout protection when reading frame payloads. An attacker can send a valid frame length header without the corresponding payload data, causing the connection to hang indefinitely in `poll_read_exact`. While the health checker eventually detects and disconnects hung connections after ~70 seconds, an attacker can repeatedly exploit this to exhaust all inbound connection slots (`MAX_INBOUND_CONNECTIONS = 100`), preventing legitimate peers from connecting. [1](#0-0) 

## Finding Description

The NoiseStream protocol prefixes each encrypted message with a 2-byte big-endian frame length. The reading process follows this state machine:

1. **ReadFrameLen state**: Read 2 bytes for frame length
2. **ReadFrame state**: Read exactly `frame_len` bytes of encrypted payload
3. **CopyDecryptedFrame state**: Decrypt and return data to caller

The vulnerability exists in the ReadFrame state where `poll_read_exact` is called without any timeout protection. The comment at line 491 explicitly acknowledges: *"It is possible that this function never completes, so a timeout needs to be set on the caller side."* [2](#0-1) 

However, the caller (Peer event loop) does NOT set a timeout on reads: [3](#0-2) 

**Attack Execution:**

1. Attacker completes the Noise handshake (within the 30-second `TRANSPORT_TIMEOUT`)
2. Connection is established and added to the Peer's event loop
3. Attacker sends 2 bytes: `0x00 0x01` (frame_len = 1)
4. Attacker never sends the 1 byte of payload
5. NoiseStream enters `ReadFrame` state and calls `poll_read_exact` to read 1 byte
6. `poll_read_exact` returns `Poll::Pending` indefinitely as the socket has no data
7. The Peer event loop is blocked at `reader.next()` with no timeout
8. The connection cannot process ANY messages (including health check pings)
9. Health checker eventually times out after 3 failed pings (~70 seconds total) [4](#0-3) 

**DoS Impact:**

An attacker can open up to `MAX_INBOUND_CONNECTIONS` (100) connections and hang all of them simultaneously. During the ~70-second detection window, legitimate peers cannot establish new connections. The attacker can then repeatedly reconnect to maintain the DoS. [5](#0-4) 

## Impact Explanation

This vulnerability meets **Medium Severity** criteria per the Aptos bug bounty program:
- **State inconsistencies requiring intervention**: Exhausted connection slots prevent normal peer communication
- **Validator node slowdowns**: Inability to accept legitimate connections degrades network participation
- **Significant protocol violations**: Violates Resource Limits invariant #9 (operations must respect resource limits)

While not a Critical severity issue because:
- Hung connections are eventually cleaned up by health checker (~70 seconds)
- Does not cause permanent network partition or fund loss
- Limited to connection slot exhaustion, not total node compromise

The impact is amplified because:
- Attack is repeatable (attacker can reconnect immediately after disconnection)
- Low resource cost for attacker (only 2 bytes per connection)
- Affects node's ability to participate in consensus if validator cannot maintain peer connections

## Likelihood Explanation

**Likelihood: High**

**Attacker Requirements:**
- Ability to establish TCP connections to the target node
- Valid x25519 key pair (trivial to generate)
- Ability to complete Noise handshake (no special authentication in `MaybeMutual` mode for public-facing nodes)

**Execution Complexity:**
- Very simple: send 2 bytes then stop sending data
- No specialized knowledge required beyond basic TCP socket programming
- Can be scripted and automated

**Detection Difficulty:**
- Attack appears as normal connections that "slow down" rather than obvious malicious patterns
- Distinguishing from legitimate network issues is difficult
- By the time health checker detects and disconnects (70s), attacker can already reconnect

## Recommendation

**Primary Fix: Add read timeout to Peer event loop**

Add a timeout wrapper around `reader.next()` in the Peer event loop:

```rust
// In peer/mod.rs, around line 252
use tokio::time::timeout;
use crate::transport::TRANSPORT_TIMEOUT;

// Inside the futures::select! block:
maybe_message = timeout(TRANSPORT_TIMEOUT, reader.next()) => {
    match maybe_message {
        Ok(Some(message)) => {
            if let Err(err) = self.handle_inbound_message(message, &mut write_reqs_tx) {
                // existing error handling
            }
        },
        Ok(None) => self.shutdown(DisconnectReason::ConnectionClosed),
        Err(_) => {
            warn!(
                NetworkSchema::new(&self.network_context)
                    .connection_metadata(&self.connection_metadata),
                "{} Read timeout for peer: {}", 
                self.network_context,
                remote_peer_id.short_str()
            );
            self.shutdown(DisconnectReason::InputOutputError)
        }
    }
},
```

**Alternative Fix: Add timeout at NoiseStream level**

Wrap the `poll_read_exact` calls with tokio's timeout mechanism, though this is less preferred as it adds overhead to every read operation.

**Defense in Depth:**
- Consider lowering `PING_INTERVAL_MS` and `PING_TIMEOUT_MS` for faster detection
- Implement connection rate limiting per IP address
- Add metrics to track connections stuck in specific read states

## Proof of Concept

```rust
#[cfg(test)]
mod exploit_test {
    use super::*;
    use crate::testutils::fake_socket::ReadOnlyTestSocket;
    use futures::executor::block_on;
    use futures::io::AsyncReadExt;
    use aptos_crypto::noise;
    use std::time::Duration;
    use tokio::time::timeout;

    #[tokio::test]
    async fn test_incomplete_frame_hangs_forever() {
        // Create a fake socket that sends frame_len=1 but no payload
        let frame_header = vec![0x00, 0x01]; // frame_len = 1 in big-endian
        let mut fake_socket = ReadOnlyTestSocket::new(&frame_header);
        
        // Socket will return pending after reading the 2-byte header
        // simulating attacker that stops sending after frame length
        
        // Setup NoiseStream
        let noise_session = noise::NoiseSession::new_for_testing();
        let mut peer = NoiseStream::new(fake_socket, noise_session);
        
        // Try to read with a timeout - should timeout, not complete
        let mut buffer = [0u8; 128];
        let result = timeout(
            Duration::from_secs(5),
            peer.read(&mut buffer)
        ).await;
        
        // Assert that read timed out (proving the hang)
        assert!(result.is_err(), "Read should timeout waiting for frame data");
        
        // In production without timeout, this would hang indefinitely
    }
    
    #[test]
    fn test_connection_slot_exhaustion() {
        // Simulate attacker opening MAX_INBOUND_CONNECTIONS (100)
        // and sending partial frames on each
        // Legitimate peers cannot connect until health checker 
        // times out all hung connections (~70 seconds)
        
        const MAX_INBOUND_CONNECTIONS: usize = 100;
        const PING_INTERVAL_MS: u64 = 10_000;
        const PING_TIMEOUT_MS: u64 = 20_000;
        const PING_FAILURES_TOLERATED: u64 = 3;
        
        let dos_window_ms = PING_INTERVAL_MS + 
            (PING_FAILURES_TOLERATED * PING_TIMEOUT_MS);
        
        println!("Attacker can exhaust {} connection slots", MAX_INBOUND_CONNECTIONS);
        println!("Each connection hangs for ~{} seconds", dos_window_ms / 1000);
        println!("During this window, legitimate peers cannot connect");
        
        assert_eq!(dos_window_ms, 70_000); // 70 seconds per connection
    }
}
```

## Notes

This vulnerability is particularly concerning because:

1. **The code explicitly documents the issue** via the comment "It is possible that this function never completes, so a timeout needs to be set on the caller side" but the caller does not implement this timeout.

2. **Health checker provides mitigation but not prevention**: The 70-second window is long enough for an attacker to cause sustained service degradation through repeated reconnections.

3. **Affects all network types**: Whether in `Mutual` or `MaybeMutual` authentication mode, once handshake completes, this attack is viable.

4. **Test gap identified**: The existing `dont_read_forever()` test only covers `frame_len=0` but does not test the more realistic attack of `frame_len > 0` with incomplete payload.

The fix should be implemented at the Peer event loop level rather than NoiseStream level to maintain separation of concerns and avoid adding timeout overhead to every read operation.

### Citations

**File:** network/framework/src/noise/stream.rs (L134-167)
```rust
                ReadState::ReadFrame {
                    frame_len,
                    ref mut offset,
                } => {
                    match ready!(poll_read_exact(
                        context,
                        Pin::new(&mut self.socket),
                        &mut self.buffers.read_buffer[..(frame_len as usize)],
                        offset
                    )) {
                        Ok(()) => {
                            match self.session.read_message_in_place(
                                &mut self.buffers.read_buffer[..(frame_len as usize)],
                            ) {
                                Ok(decrypted) => {
                                    self.read_state = ReadState::CopyDecryptedFrame {
                                        decrypted_len: decrypted.len(),
                                        offset: 0,
                                    };
                                },
                                Err(e) => {
                                    error!(error = %e, "Decryption Error: {}", e);
                                    self.read_state = ReadState::DecryptionError(e);
                                },
                            }
                        },
                        Err(e) => {
                            if e.kind() == io::ErrorKind::UnexpectedEof {
                                self.read_state = ReadState::Eof(Err(()));
                            }
                            return Poll::Ready(Err(e));
                        },
                    }
                },
```

**File:** network/framework/src/noise/stream.rs (L488-514)
```rust
/// As data can be fragmented over multiple TCP packets, poll_read_exact
/// continuously calls poll_read on the socket until enough data is read.
/// It is possible that this function never completes,
/// so a timeout needs to be set on the caller side.
fn poll_read_exact<TSocket>(
    context: &mut Context,
    mut socket: Pin<&mut TSocket>,
    buf: &mut [u8],
    offset: &mut usize,
) -> Poll<io::Result<()>>
where
    TSocket: AsyncRead,
{
    assert!(*offset <= buf.len());
    loop {
        let n = ready!(socket.as_mut().poll_read(context, &mut buf[*offset..]))?;
        trace!("poll_read_exact: read {}/{} bytes", *offset + n, buf.len());
        if n == 0 {
            return Poll::Ready(Err(io::ErrorKind::UnexpectedEof.into()));
        }
        assert!(n <= buf.len() - *offset);
        *offset += n;
        if *offset == buf.len() {
            return Poll::Ready(Ok(()));
        }
    }
}
```

**File:** network/framework/src/peer/mod.rs (L250-270)
```rust
                // Handle a new inbound MultiplexMessage that we've just read off
                // the wire from the remote peer.
                maybe_message = reader.next() => {
                    match maybe_message {
                        Some(message) =>  {
                            if let Err(err) = self.handle_inbound_message(message, &mut write_reqs_tx) {
                                warn!(
                                    NetworkSchema::new(&self.network_context)
                                        .connection_metadata(&self.connection_metadata),
                                    error = %err,
                                    "{} Error in handling inbound message from peer: {}, error: {}",
                                    self.network_context,
                                    remote_peer_id.short_str(),
                                    err
                                );
                            }
                        },
                        // The socket was gracefully closed by the remote peer.
                        None => self.shutdown(DisconnectReason::ConnectionClosed),
                    }
                },
```

**File:** config/src/config/network_config.rs (L38-44)
```rust
pub const PING_INTERVAL_MS: u64 = 10_000;
pub const PING_TIMEOUT_MS: u64 = 20_000;
pub const PING_FAILURES_TOLERATED: u64 = 3;
pub const CONNECTIVITY_CHECK_INTERVAL_MS: u64 = 5000;
pub const MAX_CONNECTION_DELAY_MS: u64 = 60_000; /* 1 minute */
pub const MAX_FULLNODE_OUTBOUND_CONNECTIONS: usize = 6;
pub const MAX_INBOUND_CONNECTIONS: usize = 100;
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L343-395)
```rust
            Err(err) => {
                warn!(
                    NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                    round = round,
                    "{} Ping failed for peer: {} round: {} with error: {:#}",
                    self.network_context,
                    peer_id.short_str(),
                    round,
                    err
                );
                self.network_interface
                    .increment_peer_round_failure(peer_id, round);

                // If the ping failures are now more than
                // `self.ping_failures_tolerated`, we disconnect from the node.
                // The HealthChecker only performs the disconnect. It relies on
                // ConnectivityManager or the remote peer to re-establish the connection.
                let failures = self
                    .network_interface
                    .get_peer_failures(peer_id)
                    .unwrap_or(0);
                if failures > self.ping_failures_tolerated {
                    info!(
                        NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                        "{} Disconnecting from peer: {}",
                        self.network_context,
                        peer_id.short_str()
                    );
                    let peer_network_id =
                        PeerNetworkId::new(self.network_context.network_id(), peer_id);
                    if let Err(err) = timeout(
                        Duration::from_millis(50),
                        self.network_interface.disconnect_peer(
                            peer_network_id,
                            DisconnectReason::NetworkHealthCheckFailure,
                        ),
                    )
                    .await
                    {
                        warn!(
                            NetworkSchema::new(&self.network_context)
                                .remote_peer(&peer_id),
                            error = ?err,
                            "{} Failed to disconnect from peer: {} with error: {:?}",
                            self.network_context,
                            peer_id.short_str(),
                            err
                        );
                    }
                }
            },
        }
    }
```
