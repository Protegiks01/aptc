# Audit Report

## Title
Unbounded Memory Exhaustion via Excessive Batch Count in Payload Prefetching

## Summary
The `prefetch_payload_data` method in the consensus payload manager creates in-memory futures for every batch in a received block payload without validating the number of batches. An attacker can craft a malicious proposal containing thousands of small batches (each within individual limits) to exhaust validator node memory, causing performance degradation or crashes.

## Finding Description

The vulnerability exists in the payload prefetch mechanism that runs when a block proposal is received. The attack flow is:

**Step 1: Missing Validation**
When a proposal is received, the validation in `process_proposal` only checks total transaction count and total byte size, but NOT the number of batches/proofs: [1](#0-0) 

Similarly, the `Payload::verify()` method verifies cryptographic signatures and epochs, but does not limit the number of proofs: [2](#0-1) 

**Step 2: Premature Prefetch Trigger**
In the epoch manager, `prefetch_payload_data` is called immediately when a proposal is received, BEFORE full validation: [3](#0-2) 

**Step 3: Unbounded Future Creation**
The `prefetch_payload_data` method iterates through all batches and creates a future for each via `request_transactions`: [4](#0-3) 

**Step 4: Memory Allocation**
Each call to `get_or_fetch_batch` creates a `BatchFetchUnit` with associated state and spawns a tokio task: [5](#0-4) 

**Attack Scenario:**
1. Attacker is a valid proposer for a round
2. Crafts a payload with 5,000 batches, each containing 2 transactions
3. Total: 10,000 transactions (within `max_receiving_block_txns` default of 10,000)
4. Total bytes: within `max_receiving_block_bytes` (6MB default)
5. Each batch: 2 txns (within `receiver_max_batch_txns` of 100)
6. When `prefetch_payload_data` is called, 5,000 `BatchFetchUnit` structures are allocated
7. Each includes: Arc pointers, Mutex, BTreeSet, boxed futures, spawned tasks
8. Memory consumption: several megabytes minimum, potentially causing OOM on resource-constrained nodes [6](#0-5) [7](#0-6) 

## Impact Explanation

This is a **High Severity** vulnerability under the Aptos bug bounty program criteria:

- **Validator node slowdowns**: Excessive memory allocation degrades performance
- **Potential crashes**: Memory exhaustion can crash validator nodes
- **Consensus disruption**: Affected validators may miss rounds or fail to vote

While not a direct consensus safety violation, this can cause temporary liveness issues if multiple validators are targeted simultaneously. It exploits a missing resource bound that violates the documented invariant: "Resource Limits: All operations must respect gas, storage, and computational limits."

The attack is particularly concerning because:
1. It bypasses all existing validation (transaction count, byte size, per-batch limits)
2. The memory allocation happens BEFORE the payload is fully validated
3. An attacker only needs to be a valid proposer (1 in N validators each round)

## Likelihood Explanation

**Likelihood: High**

- **Attacker Requirements**: Must be a valid proposer for at least one round (routinely achieved in round-robin or stake-weighted leader election)
- **Complexity**: Low - simply construct a payload with many small batches
- **Detection Difficulty**: The attack appears as legitimate traffic with valid batch sizes
- **Frequency**: Can be repeated every time the attacker is elected as proposer

The vulnerability is highly likely to be exploited because:
1. No special privileges required beyond being in the validator set
2. Attack construction is straightforward
3. Difficult to distinguish from legitimate high-batch-count proposals
4. Can be used strategically to degrade competitor validators

## Recommendation

**Fix 1: Add Batch Count Validation**
Add a check in `process_proposal` to limit the total number of batches in a payload:

```rust
// In consensus/src/round_manager.rs, process_proposal function
// After existing payload validation around line 1193:

// Validate maximum number of batches in payload
let num_batches = match payload {
    Payload::InQuorumStore(p) => p.proofs.len(),
    Payload::InQuorumStoreWithLimit(p) => p.proof_with_data.proofs.len(),
    Payload::QuorumStoreInlineHybrid(inline, proofs, _) => inline.len() + proofs.proofs.len(),
    Payload::QuorumStoreInlineHybridV2(inline, proofs, _) => inline.len() + proofs.proofs.len(),
    Payload::OptQuorumStore(OptQuorumStorePayload::V1(p)) => {
        p.inline_batches().len() + p.opt_batches().len() + p.proof_with_data().len()
    },
    _ => 0,
};

const MAX_BATCHES_PER_BLOCK: usize = 200; // Configurable parameter
ensure!(
    num_batches <= MAX_BATCHES_PER_BLOCK,
    "Payload contains too many batches: {} > {}",
    num_batches,
    MAX_BATCHES_PER_BLOCK
);
```

**Fix 2: Add Configuration Parameter**
Add `max_batches_per_block` to `ConsensusConfig`: [8](#0-7) 

**Fix 3: Rate Limit Prefetch Operations**
Add a semaphore or counter to limit concurrent prefetch operations in `BatchReaderImpl`.

## Proof of Concept

```rust
// Test demonstrating unbounded batch creation
// Add to consensus/src/payload_manager/quorum_store_payload_manager.rs tests

#[tokio::test]
async fn test_excessive_batch_count_memory_exhaustion() {
    use aptos_consensus_types::{
        common::{Payload, ProofWithData},
        proof_of_store::{BatchInfo, ProofOfStore},
    };
    
    // Create a payload with 5000 small batches
    let num_batches = 5000;
    let mut proofs = Vec::new();
    
    for i in 0..num_batches {
        // Each batch has 2 transactions, within receiver_max_batch_txns
        let batch_info = create_test_batch_info(2, 1000); // 2 txns, 1000 bytes
        let proof = create_test_proof_of_store(batch_info);
        proofs.push(proof);
    }
    
    let payload = Payload::InQuorumStore(ProofWithData::new(proofs));
    
    // Total: 10,000 transactions (within max_receiving_block_txns of 10,000)
    assert_eq!(payload.len(), 10000);
    
    // But 5000 batches will trigger 5000 future creations
    // Memory measurement would show significant allocation
    
    // Simulate prefetch_payload_data call
    let batch_reader = create_test_batch_reader();
    let payload_manager = QuorumStorePayloadManager::new(
        batch_reader,
        /* ... */
    );
    
    // This will create 5000 BatchFetchUnit allocations
    payload_manager.prefetch_payload_data(&payload, author, timestamp);
    
    // Measure memory consumption here
    // Expected: Several MB of heap allocations
    // Actual limit: None enforced
}
```

**Notes:**
- The vulnerability is confirmed through code analysis of the validation, prefetch, and batch fetching logic
- The fix requires adding a new validation dimension (batch count) alongside existing transaction count and byte size checks
- The maximum batch count should be calibrated based on expected use cases while preventing abuse (suggested: 100-500 batches per block)

### Citations

**File:** consensus/src/round_manager.rs (L1178-1193)
```rust
        let payload_len = proposal.payload().map_or(0, |payload| payload.len());
        let payload_size = proposal.payload().map_or(0, |payload| payload.size());
        ensure!(
            num_validator_txns + payload_len as u64 <= self.local_config.max_receiving_block_txns,
            "Payload len {} exceeds the limit {}",
            payload_len,
            self.local_config.max_receiving_block_txns,
        );

        ensure!(
            validator_txns_total_bytes + payload_size as u64
                <= self.local_config.max_receiving_block_bytes,
            "Payload size {} exceeds the limit {}",
            payload_size,
            self.local_config.max_receiving_block_bytes,
        );
```

**File:** consensus/consensus-types/src/common.rs (L574-632)
```rust
    pub fn verify(
        &self,
        verifier: &ValidatorVerifier,
        proof_cache: &ProofCache,
        quorum_store_enabled: bool,
    ) -> anyhow::Result<()> {
        match (quorum_store_enabled, self) {
            (false, Payload::DirectMempool(_)) => Ok(()),
            (true, Payload::InQuorumStore(proof_with_status)) => {
                Self::verify_with_cache(&proof_with_status.proofs, verifier, proof_cache)
            },
            (true, Payload::InQuorumStoreWithLimit(proof_with_status)) => Self::verify_with_cache(
                &proof_with_status.proof_with_data.proofs,
                verifier,
                proof_cache,
            ),
            (true, Payload::QuorumStoreInlineHybrid(inline_batches, proof_with_data, _))
            | (true, Payload::QuorumStoreInlineHybridV2(inline_batches, proof_with_data, _)) => {
                Self::verify_with_cache(&proof_with_data.proofs, verifier, proof_cache)?;
                Self::verify_inline_batches(
                    inline_batches.iter().map(|(info, txns)| (info, txns)),
                )?;
                Ok(())
            },
            (true, Payload::OptQuorumStore(OptQuorumStorePayload::V1(p))) => {
                let proof_with_data = p.proof_with_data();
                Self::verify_with_cache(&proof_with_data.batch_summary, verifier, proof_cache)?;
                Self::verify_inline_batches(
                    p.inline_batches()
                        .iter()
                        .map(|batch| (batch.info(), batch.transactions())),
                )?;
                Self::verify_opt_batches(verifier, p.opt_batches())?;
                Ok(())
            },
            (true, Payload::OptQuorumStore(OptQuorumStorePayload::V2(p))) => {
                if true {
                    bail!("OptQuorumStorePayload::V2 cannot be accepted yet");
                }
                #[allow(unreachable_code)]
                {
                    let proof_with_data = p.proof_with_data();
                    Self::verify_with_cache(&proof_with_data.batch_summary, verifier, proof_cache)?;
                    Self::verify_inline_batches(
                        p.inline_batches()
                            .iter()
                            .map(|batch| (batch.info(), batch.transactions())),
                    )?;
                    Self::verify_opt_batches(verifier, p.opt_batches())?;
                    Ok(())
                }
            },
            (_, _) => Err(anyhow::anyhow!(
                "Wrong payload type. Expected Payload::InQuorumStore {} got {} ",
                quorum_store_enabled,
                self
            )),
        }
    }
```

**File:** consensus/src/epoch_manager.rs (L1764-1773)
```rust
            proposal_event @ VerifiedEvent::ProposalMsg(_) => {
                if let VerifiedEvent::ProposalMsg(p) = &proposal_event {
                    if let Some(payload) = p.proposal().payload() {
                        payload_manager.prefetch_payload_data(
                            payload,
                            p.proposer(),
                            p.proposal().timestamp_usecs(),
                        );
                    }
                    pending_blocks.lock().insert_block(p.proposal().clone());
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L210-306)
```rust
    fn prefetch_payload_data(&self, payload: &Payload, author: Author, timestamp: u64) {
        // This is deprecated.
        // TODO(ibalajiarun): Remove this after migrating to OptQuorumStore type
        let request_txns_and_update_status =
            move |proof_with_status: &ProofWithData, batch_reader: Arc<dyn BatchReader>| {
                Self::request_transactions(
                    proof_with_status
                        .proofs
                        .iter()
                        .map(|proof| {
                            (
                                proof.info().clone(),
                                proof.shuffled_signers(&self.ordered_authors),
                            )
                        })
                        .collect(),
                    timestamp,
                    batch_reader,
                );
            };

        fn prefetch_helper<T: TDataInfo>(
            data_pointer: &BatchPointer<T>,
            batch_reader: Arc<dyn BatchReader>,
            author: Option<Author>,
            timestamp: u64,
            ordered_authors: &[PeerId],
        ) {
            let batches_and_responders = data_pointer
                .batch_summary
                .iter()
                .map(|data_info| {
                    let mut signers = data_info.signers(ordered_authors);
                    if let Some(author) = author {
                        signers.push(author);
                    }
                    (data_info.info().clone(), signers)
                })
                .collect();
            QuorumStorePayloadManager::request_transactions(
                batches_and_responders,
                timestamp,
                batch_reader,
            );
        }

        match payload {
            Payload::InQuorumStore(proof_with_status) => {
                request_txns_and_update_status(proof_with_status, self.batch_reader.clone());
            },
            Payload::InQuorumStoreWithLimit(proof_with_data) => {
                request_txns_and_update_status(
                    &proof_with_data.proof_with_data,
                    self.batch_reader.clone(),
                );
            },
            Payload::QuorumStoreInlineHybrid(_, proof_with_data, _)
            | Payload::QuorumStoreInlineHybridV2(_, proof_with_data, _) => {
                request_txns_and_update_status(proof_with_data, self.batch_reader.clone());
            },
            Payload::DirectMempool(_) => {
                unreachable!()
            },
            Payload::OptQuorumStore(OptQuorumStorePayload::V1(p)) => {
                prefetch_helper(
                    p.opt_batches(),
                    self.batch_reader.clone(),
                    Some(author),
                    timestamp,
                    &self.ordered_authors,
                );
                prefetch_helper(
                    p.proof_with_data(),
                    self.batch_reader.clone(),
                    None,
                    timestamp,
                    &self.ordered_authors,
                )
            },
            Payload::OptQuorumStore(OptQuorumStorePayload::V2(p)) => {
                prefetch_helper(
                    p.opt_batches(),
                    self.batch_reader.clone(),
                    Some(author),
                    timestamp,
                    &self.ordered_authors,
                );
                prefetch_helper(
                    p.proof_with_data(),
                    self.batch_reader.clone(),
                    None,
                    timestamp,
                    &self.ordered_authors,
                )
            },
        };
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L663-723)
```rust
    fn get_or_fetch_batch(
        &self,
        batch_info: BatchInfo,
        responders: Vec<PeerId>,
    ) -> Shared<Pin<Box<dyn Future<Output = ExecutorResult<Vec<SignedTransaction>>> + Send>>> {
        let mut responders = responders.into_iter().collect();

        self.inflight_fetch_requests
            .lock()
            .entry(*batch_info.digest())
            .and_modify(|fetch_unit| {
                fetch_unit.responders.lock().append(&mut responders);
            })
            .or_insert_with(|| {
                let responders = Arc::new(Mutex::new(responders));
                let responders_clone = responders.clone();

                let inflight_requests_clone = self.inflight_fetch_requests.clone();
                let batch_store = self.batch_store.clone();
                let requester = self.batch_requester.clone();

                let fut = async move {
                    let batch_digest = *batch_info.digest();
                    defer!({
                        inflight_requests_clone.lock().remove(&batch_digest);
                    });
                    // TODO(ibalajiarun): Support V2 batch
                    if let Ok(mut value) = batch_store.get_batch_from_local(&batch_digest) {
                        Ok(value.take_payload().expect("Must have payload"))
                    } else {
                        // Quorum store metrics
                        counters::MISSED_BATCHES_COUNT.inc();
                        let subscriber_rx = batch_store.subscribe(*batch_info.digest());
                        let payload = requester
                            .request_batch(
                                batch_digest,
                                batch_info.expiration(),
                                responders,
                                subscriber_rx,
                            )
                            .await?;
                        batch_store.persist(vec![PersistedValue::new(
                            batch_info.into(),
                            Some(payload.clone()),
                        )]);
                        Ok(payload)
                    }
                }
                .boxed()
                .shared();

                tokio::spawn(fut.clone());

                BatchFetchUnit {
                    responders: responders_clone,
                    fut,
                }
            })
            .fut
            .clone()
    }
```

**File:** config/src/config/consensus_config.rs (L30-104)
```rust
#[derive(Clone, Debug, Deserialize, PartialEq, Serialize)]
#[serde(default, deny_unknown_fields)]
pub struct ConsensusConfig {
    // length of inbound queue of messages
    pub max_network_channel_size: usize,
    pub max_sending_block_txns: u64,
    pub max_sending_block_txns_after_filtering: u64,
    pub max_sending_opt_block_txns_after_filtering: u64,
    pub max_sending_block_bytes: u64,
    pub max_sending_inline_txns: u64,
    pub max_sending_inline_bytes: u64,
    pub max_receiving_block_txns: u64,
    pub max_receiving_block_bytes: u64,
    pub max_pruned_blocks_in_mem: usize,
    // Timeout for consensus to get an ack from mempool for executed transactions (in milliseconds)
    pub mempool_executed_txn_timeout_ms: u64,
    // Timeout for consensus to pull transactions from mempool and get a response (in milliseconds)
    pub mempool_txn_pull_timeout_ms: u64,
    pub round_initial_timeout_ms: u64,
    pub round_timeout_backoff_exponent_base: f64,
    pub round_timeout_backoff_max_exponent: usize,
    pub safety_rules: SafetyRulesConfig,
    // Only sync committed transactions but not vote for any pending blocks. This is useful when
    // validators coordinate on the latest version to apply a manual transaction.
    pub sync_only: bool,
    // The size of the round/recovery manager and proposal buffer channels.
    pub internal_per_key_channel_size: usize,
    pub quorum_store_pull_timeout_ms: u64,
    // Decides how long the leader waits before proposing empty block if there's no txns in mempool
    pub quorum_store_poll_time_ms: u64,
    // Whether to create partial blocks when few transactions exist, or empty blocks when there is
    // pending ordering, or to wait for quorum_store_poll_count * 30ms to collect transactions for a block
    //
    // It is more efficient to execute larger blocks, as it creates less overhead. On the other hand
    // waiting increases latency (unless we are under high load that added waiting latency
    // is compensated by faster execution time). So we want to balance the two, by waiting only
    // when we are saturating the execution pipeline:
    // - if there are more pending blocks then usual in the execution pipeline,
    //   block is going to wait there anyways, so we can wait to create a bigger/more efificent block
    // - in case our node is faster than others, and we don't have many pending blocks,
    //   but we still see very large recent (pending) blocks, we know that there is demand
    //   and others are creating large blocks, so we can wait as well.
    pub wait_for_full_blocks_above_pending_blocks: usize,
    pub wait_for_full_blocks_above_recent_fill_threshold: f32,
    pub intra_consensus_channel_buffer_size: usize,
    pub quorum_store: QuorumStoreConfig,
    pub vote_back_pressure_limit: u64,
    /// If backpressure target block size is below it, update `max_txns_to_execute` instead.
    /// Applied to execution, pipeline and chain health backpressure.
    /// Needed as we cannot subsplit QS batches.
    pub min_max_txns_in_block_after_filtering_from_backpressure: u64,
    pub execution_backpressure: Option<ExecutionBackpressureConfig>,
    pub pipeline_backpressure: Vec<PipelineBackpressureValues>,
    // Used to decide if backoff is needed.
    // must match one of the CHAIN_HEALTH_WINDOW_SIZES values.
    pub window_for_chain_health: usize,
    pub chain_health_backoff: Vec<ChainHealthBackoffValues>,
    // Deprecated
    pub qc_aggregator_type: QcAggregatorType,
    // Max blocks allowed for block retrieval requests
    pub max_blocks_per_sending_request: u64,
    pub max_blocks_per_sending_request_quorum_store_override: u64,
    pub max_blocks_per_receiving_request: u64,
    pub max_blocks_per_receiving_request_quorum_store_override: u64,
    pub broadcast_vote: bool,
    pub proof_cache_capacity: u64,
    pub rand_rb_config: ReliableBroadcastConfig,
    pub num_bounded_executor_tasks: u64,
    pub enable_pre_commit: bool,
    pub max_pending_rounds_in_commit_vote_cache: u64,
    pub optimistic_sig_verification: bool,
    pub enable_round_timeout_msg: bool,
    pub enable_optimistic_proposal_rx: bool,
    pub enable_optimistic_proposal_tx: bool,
}
```

**File:** config/src/config/consensus_config.rs (L220-231)
```rust
impl Default for ConsensusConfig {
    fn default() -> ConsensusConfig {
        ConsensusConfig {
            max_network_channel_size: 1024,
            max_sending_block_txns: MAX_SENDING_BLOCK_TXNS,
            max_sending_block_txns_after_filtering: MAX_SENDING_BLOCK_TXNS_AFTER_FILTERING,
            max_sending_opt_block_txns_after_filtering: MAX_SENDING_OPT_BLOCK_TXNS_AFTER_FILTERING,
            max_sending_block_bytes: 3 * 1024 * 1024, // 3MB
            max_receiving_block_txns: *MAX_RECEIVING_BLOCK_TXNS,
            max_sending_inline_txns: 100,
            max_sending_inline_bytes: 200 * 1024,       // 200 KB
            max_receiving_block_bytes: 6 * 1024 * 1024, // 6MB
```

**File:** config/src/config/quorum_store_config.rs (L120-126)
```rust
            receiver_max_batch_txns: 100,
            receiver_max_batch_bytes: 1024 * 1024 + BATCH_PADDING_BYTES,
            receiver_max_num_batches: 20,
            receiver_max_total_txns: 2000,
            receiver_max_total_bytes: 4 * 1024 * 1024
                + DEFAULT_MAX_NUM_BATCHES
                + BATCH_PADDING_BYTES,
```
