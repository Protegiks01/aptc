# Audit Report

## Title
Irrecoverable Node Liveness Failure Due to Partial State KV Pruner Failure and Aggressive Recovery Mechanism

## Summary
When `StateKvPruner::prune()` partially fails (metadata pruner succeeds but shard pruner fails), the system creates an inconsistent state between the metadata database and shard databases. On node restart, the mandatory catch-up mechanism can cause permanent node failure if the underlying issue persists, violating the liveness invariant.

## Finding Description

The `StateKvPruner::prune()` function executes pruning in two phases: metadata pruning followed by parallel shard pruning. [1](#0-0) 

The metadata pruner atomically commits both deletions and progress updates in a single batch. [2](#0-1) 

If the metadata pruner succeeds but any shard pruner fails, the function returns an error without updating the in-memory progress. [3](#0-2) 

During runtime, the pruner worker catches the error and retries using the in-memory progress. [4](#0-3) 

**The Critical Issue:** On node restart, each shard pruner attempts to catch up from its persisted progress to the metadata progress. [5](#0-4) 

If this catch-up fails, the initialization panics with `.expect()`, preventing the node from starting. [6](#0-5) 

Meanwhile, the `StateKvPrunerManager` proactively sets `min_readable_version` based on the latest committed version, independent of actual pruning success. [7](#0-6) 

This creates a scenario where:
1. Metadata DB shows progress = N (advanced)
2. Failed shard still has unpruned data from [M, N) where M < N
3. Manager claims min_readable_version = N
4. On restart, shard catch-up from M to N fails persistently
5. Node cannot start, becoming permanently unavailable

## Impact Explanation

This qualifies as **High Severity** under "Significant protocol violations" because:

1. **Validator Unavailability**: A validator node experiencing persistent pruning failures (due to disk corruption, I/O errors, or shard-specific issues) becomes permanently unable to restart, removing it from consensus participation.

2. **Cascading Failures**: If multiple validators encounter similar issues simultaneously (e.g., during a wave of upgrades or after a problematic block), the network could lose >1/3 validators, threatening liveness.

3. **No Automatic Recovery**: The `.expect()` panic provides no fallback mechanism, requiring manual database intervention or rollback.

4. **False Availability Claims**: The manager advertises `min_readable_version` optimistically, causing queries to be rejected for data that still exists in failed shards, reducing data availability.

While this doesn't meet Critical severity (requires hardfork or total network loss), it represents a significant operational vulnerability affecting validator reliability.

## Likelihood Explanation

**Moderate to High Likelihood** in production environments:

1. **Trigger Conditions**: Any transient I/O error, disk space issue, permission problem, or database corruption during the critical window between metadata and shard pruning can create this state.

2. **Persistence**: If the underlying cause (e.g., corrupted data in a specific version range, hardware failure on one shard) persists across restarts, recovery becomes impossible without manual intervention.

3. **Operational Reality**: Large-scale deployments commonly experience:
   - Disk failures
   - I/O timeouts under load  
   - Filesystem permission issues
   - Database corruption from unclean shutdowns

4. **No Degradation**: The system has no graceful degradation pathâ€”it's binary (works or panics).

## Recommendation

**Immediate Fix**: Replace the panic-on-failure catch-up mechanism with a bounded retry strategy with fallback:

```rust
// In StateKvShardPruner::new()
const MAX_CATCHUP_RETRIES: usize = 3;
let mut retries = 0;
loop {
    match myself.prune(progress, metadata_progress) {
        Ok(_) => break,
        Err(e) if retries < MAX_CATCHUP_RETRIES => {
            warn!("Shard {shard_id} catch-up failed (attempt {}/{MAX_CATCHUP_RETRIES}): {e}", 
                  retries + 1);
            retries += 1;
            std::thread::sleep(Duration::from_secs(1 << retries)); // Exponential backoff
        }
        Err(e) => {
            error!("Shard {shard_id} catch-up failed after {MAX_CATCHUP_RETRIES} retries");
            // Option 1: Initialize with metadata_progress but mark degraded
            // Option 2: Rollback metadata progress to minimum shard progress
            // Option 3: Disable pruner and allow node to start (manual recovery needed)
            return Err(e); // Graceful error propagation instead of panic
        }
    }
}
```

**Longer-term Solutions**:
1. Implement two-phase commit for pruner progress updates (all shards commit or none)
2. Add health checks that detect shard progress lag before restart
3. Store per-shard min_readable_version and use the minimum across all shards
4. Implement repair tools for recovering from progress inconsistencies

## Proof of Concept

```rust
// Reproduction steps (requires test infrastructure):
#[test]
fn test_pruner_restart_failure_after_partial_prune() {
    // 1. Setup: Create StateKvDb with sharding enabled, 2 shards
    let db = create_test_state_kv_db_with_shards(2);
    
    // 2. Populate with state versions 0-1000
    populate_state_kv_data(&db, 0, 1000);
    
    // 3. Initialize pruner
    let pruner = StateKvPruner::new(Arc::new(db)).unwrap();
    
    // 4. Set target to prune up to version 500
    pruner.set_target_version(500);
    
    // 5. Simulate partial failure: metadata succeeds, shard 1 fails
    // (This would require injecting a failure into the shard 1 DB)
    // After this, metadata DB shows progress=500, shard 1 shows progress=0
    
    // 6. Attempt to restart (reinitialize) pruner
    // Expected: Panic during StateKvShardPruner::new() catch-up
    let result = std::panic::catch_unwind(|| {
        StateKvPruner::new(Arc::new(db)).unwrap()
    });
    
    assert!(result.is_err(), "Node should panic on restart with inconsistent progress");
}
```

**Note**: Full reproduction requires the ability to inject I/O failures into specific shards during pruning, which is infrastructure-dependent.

## Notes

To directly answer the security question: **The system correctly retries the same batch and does NOT skip ahead leaving unpruned versions**. [8](#0-7)  The in-memory progress is not advanced on failure, ensuring the next prune call processes the same range.

However, the restart recovery mechanism creates a **liveness vulnerability** where persistent failures lead to permanent node unavailability, violating the system's availability guarantees.

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L52-52)
```rust
        let mut progress = self.progress();
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L64-78)
```rust
            self.metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state kv shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L80-81)
```rust
            progress = current_batch_target_version;
            self.record_progress(progress);
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L114-115)
```rust

        let metadata_pruner = StateKvMetadataPruner::new(Arc::clone(&state_kv_db));
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs (L67-72)
```rust
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        self.state_kv_db.metadata_db().write_schemas(batch)
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L55-64)
```rust
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L30-42)
```rust
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
            metadata_progress,
        )?;
        let myself = Self { shard_id, db_shard };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up state kv shard {shard_id}."
        );
        myself.prune(progress, metadata_progress)?;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_pruner_manager.rs (L128-142)
```rust
    fn set_pruner_target_db_version(&self, latest_version: Version) {
        assert!(self.pruner_worker.is_some());
        let min_readable_version = latest_version.saturating_sub(self.prune_window);
        self.min_readable_version
            .store(min_readable_version, Ordering::SeqCst);

        PRUNER_VERSIONS
            .with_label_values(&["state_kv_pruner", "min_readable"])
            .set(min_readable_version as i64);

        self.pruner_worker
            .as_ref()
            .unwrap()
            .set_target_db_version(min_readable_version);
    }
```
