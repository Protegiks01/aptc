# Audit Report

## Title
Non-Deterministic Payload Availability Check Enables Targeted Validator Liveness Attack in OptQuorumStore

## Summary
The `check_payload_availability()` function in `QuorumStorePayloadManager` makes voting decisions based on local batch cache state rather than the block's intrinsic properties, allowing network adversaries to selectively degrade validator participation through strategic batch dissemination during network partitions.

## Finding Description

The vulnerability exists in how `check_payload_availability()` determines whether a validator should vote on a block with `OptQuorumStore` payload. [1](#0-0) 

The function checks if opt batches exist in the **local** batch reader cache via `batch_reader.exists(batch.digest())`. [2](#0-1)  This local lookup returns `Some(author)` if the batch is in the validator's local cache, or `None` if it's not present.

During proposal processing, this creates a critical decision branch: [3](#0-2) 

- If `check_payload()` returns `Ok()` (all batches locally available), the validator immediately proceeds to vote
- If `check_payload()` returns `Err(missing_authors)` (some batches missing), the validator waits for payload with a deadline

When the wait times out, the validator simply logs a warning and does not vote: [4](#0-3) 

**Attack Scenario:**

An adversary controlling network topology during a partition can:

1. Proposer creates a block with `OptQuorumStore` payload containing opt batches B1, B2, B3
2. Adversary ensures batches are delivered to validators V1-V10 (≥2f+1) but NOT to validators V11-V13 (minority)
3. V1-V10 check availability → all batches exist locally → vote immediately  
4. V11-V13 check availability → batches missing → wait for payload → timeout after deadline → no vote
5. Block is committed with votes from V1-V10
6. V11-V13 must use state sync to recover, degrading their participation

This can be repeated to systematically exclude specific validators from consensus participation without Byzantine behavior from validators themselves.

## Impact Explanation

This meets **High Severity** criteria per the Aptos bug bounty program as it enables "Validator node slowdowns" and "Significant protocol violations":

1. **Reduced Fault Tolerance**: By forcing minority validators offline repeatedly, the effective Byzantine fault tolerance is reduced from f to less than f
2. **Targeted Validator DoS**: Specific validators can be systematically excluded from consensus participation
3. **Liveness Degradation**: Affected validators repeatedly enter state sync mode, slowing overall network throughput
4. **Non-deterministic Consensus Participation**: Validators make different decisions about the same block based on local network conditions, violating the deterministic execution invariant

While this doesn't directly cause fund loss or double-spending (hence not Critical), it significantly degrades network resilience and enables targeted attacks on specific validator operators.

## Likelihood Explanation

**High Likelihood:**

1. **Network partitions occur naturally** in distributed systems, making this exploitable without requiring active attack
2. **OptQuorumStore is production code** actively used in Aptos mainnet
3. **No authentication required** - any network-level adversary with ability to partition traffic can exploit this
4. **Repeatable attack** - can be executed continuously to maintain degraded state
5. **Affects all validators** using OptQuorumStore payloads (V1 and V2)

The attack requires only:
- Network partition capability (common in geo-distributed deployments)
- OR malicious proposer who selectively disseminates batches
- No validator key compromise needed
- No special transaction crafting required

## Recommendation

Decouple payload availability checks from local cache state by implementing one of these solutions:

**Option 1: Batch availability proof requirement**
- Require opt batches to include availability proofs (quorum signatures from batch store)
- Reject blocks with opt batches that lack sufficient availability guarantees
- This ensures all validators can fetch batches from the network

**Option 2: Remove local availability check for opt batches**
- Always return `Ok()` for OptQuorumStore payloads, similar to InQuorumStore
- Let payload fetching during execution handle missing batches
- Add timeout and retry logic in the execution pipeline

**Option 3: Consensus-level batch availability verification**
- During block validation, verify that opt batches were seen by ≥2f+1 validators
- Include batch availability attestations in the block proposal
- Reject blocks where opt batches lack sufficient witness coverage

**Recommended Fix (Option 2 - simplest):**

```rust
Payload::OptQuorumStore(OptQuorumStorePayload::V1(p)) 
| Payload::OptQuorumStore(OptQuorumStorePayload::V2(p)) => {
    // Don't check opt batch availability here - let execution handle fetching
    // This ensures all validators make the same decision about the block
    Ok(())
},
```

This aligns OptQuorumStore with the behavior of other payload types and ensures deterministic decision-making across all validators.

## Proof of Concept

Due to the distributed nature of this vulnerability, a full PoC requires network simulation. However, the divergent behavior can be demonstrated with this Rust test outline:

```rust
#[tokio::test]
async fn test_divergent_availability_check() {
    // Setup two validators with same block
    let block = create_test_block_with_opt_batches();
    
    // Validator 1: has all batches in local cache
    let batch_reader_1 = MockBatchReader::new();
    batch_reader_1.insert_batch(batch1);
    batch_reader_1.insert_batch(batch2);
    let manager_1 = QuorumStorePayloadManager::new(batch_reader_1, ...);
    
    // Validator 2: missing batches in local cache
    let batch_reader_2 = MockBatchReader::new();
    // Don't insert batches - simulates network partition
    let manager_2 = QuorumStorePayloadManager::new(batch_reader_2, ...);
    
    // Check availability - should diverge
    let result_1 = manager_1.check_payload_availability(&block);
    let result_2 = manager_2.check_payload_availability(&block);
    
    assert!(result_1.is_ok());  // Validator 1 proceeds to vote
    assert!(result_2.is_err()); // Validator 2 waits/times out
    
    // Same block, different decisions - violates determinism
}
```

## Notes

This vulnerability is inherent to OptQuorumStore's design of using local batch availability to make consensus decisions. The issue affects both V1 and V2 implementations of OptQuorumStore payloads. The root cause is checking local state (`batch_reader.exists()`) rather than verifying network-wide batch availability before making voting decisions.

### Citations

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L409-442)
```rust
            Payload::OptQuorumStore(OptQuorumStorePayload::V1(p)) => {
                let mut missing_authors = BitVec::with_num_bits(self.ordered_authors.len() as u16);
                for batch in p.opt_batches().deref() {
                    if self.batch_reader.exists(batch.digest()).is_none() {
                        let index = *self
                            .address_to_validator_index
                            .get(&batch.author())
                            .expect("Payload author should have been verified");
                        missing_authors.set(index as u16);
                    }
                }
                if missing_authors.all_zeros() {
                    Ok(())
                } else {
                    Err(missing_authors)
                }
            },
            Payload::OptQuorumStore(OptQuorumStorePayload::V2(p)) => {
                let mut missing_authors = BitVec::with_num_bits(self.ordered_authors.len() as u16);
                for batch in p.opt_batches().deref() {
                    if self.batch_reader.exists(batch.digest()).is_none() {
                        let index = *self
                            .address_to_validator_index
                            .get(&batch.author())
                            .expect("Payload author should have been verified");
                        missing_authors.set(index as u16);
                    }
                }
                if missing_authors.all_zeros() {
                    Ok(())
                } else {
                    Err(missing_authors)
                }
            },
```

**File:** consensus/src/quorum_store/batch_store.rs (L727-732)
```rust
    fn exists(&self, digest: &HashValue) -> Option<PeerId> {
        self.batch_store
            .get_batch_from_local(digest)
            .map(|v| v.author())
            .ok()
    }
```

**File:** consensus/src/round_manager.rs (L1262-1279)
```rust
        if block_store.check_payload(&proposal).is_err() {
            debug!("Payload not available locally for block: {}", proposal.id());
            counters::CONSENSUS_PROPOSAL_PAYLOAD_AVAILABILITY
                .with_label_values(&["missing"])
                .inc();
            let start_time = Instant::now();
            let deadline = self.round_state.current_round_deadline();
            let future = async move {
                (
                    block_store.wait_for_payload(&proposal, deadline).await,
                    proposal,
                    start_time,
                )
            }
            .boxed();
            self.futures.push(future);
            return Ok(());
        }
```

**File:** consensus/src/round_manager.rs (L2155-2158)
```rust
                        Err(err) => {
                            counters::CONSENSUS_PROPOSAL_PAYLOAD_FETCH_DURATION.with_label_values(&["error"]).observe(elapsed);
                            warn!("unable to fetch payload for block {}: {}", id, err);
                        },
```
