# Audit Report

## Title
State Snapshot Restore: Permanent Indexer-MainDB Desynchronization on Commit Failure

## Summary
The `write_kv_batch()` function in the state snapshot restoration flow lacks atomic transaction coordination between the internal indexer DB and main state KV DB. While the question asks about indexer write failure (which cannot cause desync due to early return), the **reverse scenario is the actual vulnerability**: if indexer write succeeds but main DB commit fails, the indexer DB retains committed data while main DB has nothing, causing permanent desynchronization with no automatic recovery mechanism.

## Finding Description

The vulnerability exists in the transaction ordering within `write_kv_batch()`: [1](#0-0) 

The function performs two separate database commits in sequence:

1. **Indexer DB Write (if enabled)**: Calls `write_keys_to_indexer_db()` which internally commits to the indexer database [2](#0-1) 

2. **Main DB Write**: Commits to state KV database via `state_kv_db.commit()` [3](#0-2) 

**Critical Issue**: These are **two independent database transactions** with no distributed transaction protocol (2PC) or rollback mechanism between them. 

**To clarify the question's scenario**: If `write_keys_to_indexer_db()` fails, the `?` operator causes immediate return, preventing main DB write. This scenario cannot cause desync.

**The actual vulnerability**: If `write_keys_to_indexer_db()` succeeds (committing to indexer DB) but `state_kv_db.commit()` subsequently fails, the system enters an inconsistent state:

- Indexer DB contains state key metadata and progress markers
- Main DB lacks the corresponding state values
- No rollback mechanism exists for the indexer DB

The progress validation in `get_progress()` explicitly **allows** indexer to be ahead of main DB: [4](#0-3) 

This means the inconsistent state is accepted by the system. The recovery mechanism `sync_commit_progress()` only handles main database components (ledger_db, state_kv_db, state_merkle_db) but **not the indexer DB**: [5](#0-4) 

**Attack Scenario** (during state snapshot restoration):

1. Node begins state snapshot restoration at version V
2. Chunk N restoration: `write_kv_batch()` is called
3. Indexer write succeeds → Indexer DB records keys K1...Kn and progress P
4. Main DB write fails (disk full, I/O error, corruption, crash) → No commit
5. On restart/retry:
   - `get_progress()` returns main DB progress (behind or None)
   - Indexer DB has orphaned keys K1...Kn with no corresponding values in main DB
   - System attempts re-restoration but indexer already has stale metadata
6. Result: Permanent desynchronization where queries using indexer to locate keys will fail to find values in main DB

## Impact Explanation

**Severity: High** (potentially Medium)

This vulnerability breaks the **State Consistency** invariant (#4): "State transitions must be atomic and verifiable via Merkle proofs."

**Impacts:**
- **State Inconsistencies Requiring Intervention**: Indexer DB contains key metadata that doesn't exist in main DB, requiring manual database repair or re-synchronization
- **Query Failures**: Operations that rely on the internal indexer to locate state keys will retrieve keys that have no corresponding values in the state KV database
- **Restore Process Corruption**: State snapshot restoration may enter an unrecoverable state where progress tracking is inconsistent across databases
- **No Automatic Recovery**: Unlike main DB components, the indexer DB has no truncation/rollback mechanism in `sync_commit_progress()`

This qualifies as **Medium Severity** per bug bounty criteria: "State inconsistencies requiring intervention." It could potentially escalate to **High Severity** if it causes "Validator node slowdowns" or "API crashes" when queries encounter the desynchronization.

## Likelihood Explanation

**Likelihood: Medium**

This vulnerability can occur in several realistic scenarios:

1. **Disk Space Exhaustion**: Main DB commit fails due to insufficient disk space while indexer DB (often on same disk) had space for smaller metadata write
2. **I/O Errors**: Transient or permanent disk errors during main DB commit after successful indexer write
3. **Process Crashes**: Node crashes between the two commits
4. **Database Corruption**: Main DB corruption prevents commit while indexer DB remains healthy

The vulnerability only manifests during state snapshot restoration operations, which occur:
- During initial node sync
- When nodes fall behind and need to catch up
- During state sync from snapshots

While not every restoration encounters failures, the lack of atomicity guarantees means production deployments will eventually hit this edge case, especially under adverse conditions (resource pressure, hardware failures).

## Recommendation

Implement atomic transaction coordination between indexer DB and main DB writes. Several approaches:

**Option 1: Write-Ahead Logging (Preferred)**
- Record intended indexer operations to a WAL before committing to indexer DB
- Commit main DB first
- Then commit indexer DB
- On failure, WAL allows rollback/replay

**Option 2: Reverse Order**
- Commit main DB first (source of truth)
- Then commit indexer DB
- If indexer commit fails, log warning but continue (indexer can catch up later)
- Add recovery mechanism to rebuild indexer from main DB

**Option 3: Two-Phase Commit**
- Implement distributed transaction protocol between databases
- Prepare both, commit both, or rollback both atomically

**Option 4: Indexer Rebuild on Mismatch**
Enhance `get_progress()` to detect and repair desync:

```rust
fn get_progress(&self, version: Version) -> Result<Option<StateSnapshotProgress>> {
    let main_db_progress = self.state_kv_db.metadata_db()
        .get::<DbMetadataSchema>(&DbMetadataKey::StateSnapshotKvRestoreProgress(version))?
        .map(|v| v.expect_state_snapshot_progress());

    if self.internal_indexer_db.is_some() 
        && self.internal_indexer_db.as_ref().unwrap().statekeys_enabled() 
    {
        let indexer_progress = self.internal_indexer_db.as_ref().unwrap()
            .get_restore_progress(version)?;

        match (main_db_progress, indexer_progress) {
            (None, Some(_)) => {
                // Indexer ahead - TRUNCATE indexer DB to sync with main DB
                warn!("Indexer DB ahead of main DB, truncating indexer progress");
                self.truncate_indexer_progress(version)?;
                return Ok(None);
            },
            (Some(main_p), Some(idx_p)) if main_p.key_hash < idx_p.key_hash => {
                // Main DB behind - truncate indexer to match
                warn!("Main DB behind indexer, truncating indexer to match");
                self.truncate_indexer_to_progress(version, main_p)?;
            },
            // ... rest of validation
        }
    }

    Ok(main_db_progress)
}
```

## Proof of Concept

**Reproduction Steps** (requires modification to simulate failure):

```rust
// In storage/aptosdb/src/state_store/mod.rs tests

#[test]
fn test_indexer_main_db_desync_on_commit_failure() {
    // Setup: Create state store with indexer enabled
    let (mut store, indexer_db) = create_test_store_with_indexer();
    
    // Create test state snapshot data
    let version = 100;
    let test_keys: Vec<StateKey> = (0..10)
        .map(|i| StateKey::raw(format!("key_{}", i)))
        .collect();
    let test_batch: StateValueBatch = test_keys
        .iter()
        .map(|k| ((k.clone(), version), Some(StateValue::new_legacy(vec![1,2,3]))))
        .collect();
    
    // Inject failure: Modify state_kv_db to fail on commit
    // (In real implementation, this could be disk full, I/O error, etc.)
    store.state_kv_db.inject_commit_failure(true);
    
    // Attempt write_kv_batch - indexer succeeds, main DB fails
    let result = store.write_kv_batch(
        version,
        &test_batch,
        StateSnapshotProgress::new(test_keys.last().unwrap().hash(), StateStorageUsage::zero()),
    );
    
    // Assert: Operation failed
    assert!(result.is_err());
    
    // Verify desync:
    // 1. Indexer DB has the keys
    let indexer_progress = indexer_db.get_restore_progress(version).unwrap();
    assert!(indexer_progress.is_some(), "Indexer DB should have progress");
    
    // 2. Main DB has NO keys
    let main_progress = store.state_kv_db.metadata_db()
        .get::<DbMetadataSchema>(&DbMetadataKey::StateSnapshotKvRestoreProgress(version))
        .unwrap();
    assert!(main_progress.is_none(), "Main DB should have no progress");
    
    // 3. get_progress() allows this inconsistency
    let reported_progress = store.get_progress(version).unwrap();
    // No error thrown despite desync
    
    // 4. Query fails: Indexer reports keys, main DB has no values
    for key in &test_keys {
        let value = store.state_kv_db.get_state_value_by_version(&key, version).unwrap();
        assert!(value.is_none(), "Main DB should not have values");
    }
    
    println!("VULNERABILITY CONFIRMED: Indexer DB ahead of main DB with no recovery");
}
```

**Notes**
- The scenario described in the question ("indexer fails, main succeeds") is prevented by the `?` operator early return mechanism and cannot cause desynchronization
- The actual vulnerability is the reverse: indexer succeeds, main DB fails, causing permanent desync
- This affects the state snapshot restoration critical path during node synchronization
- No automatic recovery mechanism exists for indexer DB inconsistencies
- The `get_progress()` function explicitly allows indexer to be ahead of main DB, accepting the inconsistent state
- Production systems under resource pressure or hardware failures will encounter this edge case

### Citations

**File:** storage/aptosdb/src/state_store/mod.rs (L408-502)
```rust
    // We commit the overall commit progress at the last, and use it as the source of truth of the
    // commit progress.
    pub fn sync_commit_progress(
        ledger_db: Arc<LedgerDb>,
        state_kv_db: Arc<StateKvDb>,
        state_merkle_db: Arc<StateMerkleDb>,
        crash_if_difference_is_too_large: bool,
    ) {
        let ledger_metadata_db = ledger_db.metadata_db();
        if let Some(overall_commit_progress) = ledger_metadata_db
            .get_synced_version()
            .expect("DB read failed.")
        {
            info!(
                overall_commit_progress = overall_commit_progress,
                "Start syncing databases..."
            );
            let ledger_commit_progress = ledger_metadata_db
                .get_ledger_commit_progress()
                .expect("Failed to read ledger commit progress.");
            assert_ge!(ledger_commit_progress, overall_commit_progress);

            let state_kv_commit_progress = state_kv_db
                .metadata_db()
                .get::<DbMetadataSchema>(&DbMetadataKey::StateKvCommitProgress)
                .expect("Failed to read state K/V commit progress.")
                .expect("State K/V commit progress cannot be None.")
                .expect_version();
            assert_ge!(state_kv_commit_progress, overall_commit_progress);

            // LedgerCommitProgress was not guaranteed to commit after all ledger changes finish,
            // have to attempt truncating every column family.
            info!(
                ledger_commit_progress = ledger_commit_progress,
                "Attempt ledger truncation...",
            );
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");

            // State K/V commit progress isn't (can't be) written atomically with the data,
            // because there are shards, so we have to attempt truncation anyway.
            info!(
                state_kv_commit_progress = state_kv_commit_progress,
                "Start state KV truncation..."
            );
            let difference = state_kv_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_state_kv_db(
                &state_kv_db,
                state_kv_commit_progress,
                overall_commit_progress,
                std::cmp::max(difference as usize, 1), /* batch_size */
            )
            .expect("Failed to truncate state K/V db.");

            let state_merkle_max_version = get_max_version_in_state_merkle_db(&state_merkle_db)
                .expect("Failed to get state merkle max version.")
                .expect("State merkle max version cannot be None.");
            if state_merkle_max_version > overall_commit_progress {
                let difference = state_merkle_max_version - overall_commit_progress;
                if crash_if_difference_is_too_large {
                    assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
                }
            }
            let state_merkle_target_version = find_tree_root_at_or_before(
                ledger_metadata_db,
                &state_merkle_db,
                overall_commit_progress,
            )
            .expect("DB read failed.")
            .unwrap_or_else(|| {
                panic!(
                    "Could not find a valid root before or at version {}, maybe it was pruned?",
                    overall_commit_progress
                )
            });
            if state_merkle_target_version < state_merkle_max_version {
                info!(
                    state_merkle_max_version = state_merkle_max_version,
                    target_version = state_merkle_target_version,
                    "Start state merkle truncation..."
                );
                truncate_state_merkle_db(&state_merkle_db, state_merkle_target_version)
                    .expect("Failed to truncate state merkle db.");
            }
        } else {
            info!("No overall commit progress was found!");
        }
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1244-1279)
```rust
    fn write_kv_batch(
        &self,
        version: Version,
        node_batch: &StateValueBatch,
        progress: StateSnapshotProgress,
    ) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_value_writer_write_chunk"]);
        let mut batch = SchemaBatch::new();
        let mut sharded_schema_batch = self.state_kv_db.new_sharded_native_batches();

        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateSnapshotKvRestoreProgress(version),
            &DbMetadataValue::StateSnapshotProgress(progress),
        )?;

        if self.internal_indexer_db.is_some()
            && self
                .internal_indexer_db
                .as_ref()
                .unwrap()
                .statekeys_enabled()
        {
            let keys = node_batch.keys().map(|key| key.0.clone()).collect();
            self.internal_indexer_db
                .as_ref()
                .unwrap()
                .write_keys_to_indexer_db(&keys, version, progress)?;
        }
        self.shard_state_value_batch(
            &mut sharded_schema_batch,
            node_batch,
            self.state_kv_db.enabled_sharding(),
        )?;
        self.state_kv_db
            .commit(version, Some(batch), sharded_schema_batch)
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1338-1340)
```rust
            match (main_db_progress, progress_opt) {
                (None, None) => (),
                (None, Some(_)) => (),
```

**File:** storage/indexer/src/db_indexer.rs (L90-108)
```rust
    pub fn write_keys_to_indexer_db(
        &self,
        keys: &Vec<StateKey>,
        snapshot_version: Version,
        progress: StateSnapshotProgress,
    ) -> Result<()> {
        // add state value to internal indexer
        let mut batch = SchemaBatch::new();
        for state_key in keys {
            batch.put::<StateKeysSchema>(state_key, &())?;
        }

        batch.put::<InternalIndexerMetadataSchema>(
            &MetadataKey::StateSnapshotRestoreProgress(snapshot_version),
            &MetadataValue::StateSnapshotProgress(progress),
        )?;
        self.db.write_schemas(batch)?;
        Ok(())
    }
```

**File:** storage/aptosdb/src/state_kv_db.rs (L177-208)
```rust
    pub(crate) fn commit(
        &self,
        version: Version,
        state_kv_metadata_batch: Option<SchemaBatch>,
        sharded_state_kv_batches: ShardedStateKvSchemaBatch,
    ) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_db__commit"]);
        {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_db__commit_shards"]);
            THREAD_MANAGER.get_io_pool().scope(|s| {
                let mut batches = sharded_state_kv_batches.into_iter();
                for shard_id in 0..NUM_STATE_SHARDS {
                    let state_kv_batch = batches
                        .next()
                        .expect("Not sufficient number of sharded state kv batches");
                    s.spawn(move |_| {
                        // TODO(grao): Consider propagating the error instead of panic, if necessary.
                        self.commit_single_shard(version, shard_id, state_kv_batch)
                            .unwrap_or_else(|err| {
                                panic!("Failed to commit shard {shard_id}: {err}.")
                            });
                    });
                }
            });
        }
        if let Some(batch) = state_kv_metadata_batch {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_db__commit_metadata"]);
            self.state_kv_metadata_db.write_schemas(batch)?;
        }

        self.write_progress(version)
    }
```
