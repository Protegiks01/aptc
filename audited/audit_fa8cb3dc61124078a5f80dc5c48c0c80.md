# Audit Report

## Title
Hot State Memory Exhaustion via Large StateValue Exploitation

## Summary
The hot state LRU cache in AptosDB enforces only item count limits (`max_items_per_shard = 250,000`) but not total byte size limits. An attacker can craft transactions that write state values at or near the maximum allowed size (1MB per value via `max_bytes_per_write_op`), causing the hot state cache to consume orders of magnitude more memory than designed (up to 4TB worst case vs 2GB expected), leading to validator OOM crashes and denial of service.

## Finding Description

The hot state cache is an in-memory LRU cache designed to accelerate access to frequently-used blockchain state. The configuration comment explicitly states the design intention: [1](#0-0) 

With the default configuration of 250,000 items per shard across 16 shards (4 million items total), this implies an expected average of approximately 500 bytes per item: [2](#0-1) 

However, the system has a critical architectural mismatch between transaction-level validation and hot state memory management:

**1. Transaction-level validation allows large values:**
The `max_bytes_per_write_op` parameter limits each state value to 1MB (1,048,576 bytes): [3](#0-2) 

This validation is enforced during transaction execution: [4](#0-3) 

**2. Hot state eviction only checks item count, not byte size:**
The `Committer::commit()` function tracks `total_value_bytes` by calling `slot.size()` on each StateSlot: [5](#0-4) 

The `StateSlot::size()` method returns the byte length of the StateValue: [6](#0-5) 

Which in turn calls `StateValue::size()`: [7](#0-6) 

However, `total_value_bytes` is only used for metrics reporting, not for eviction decisions: [8](#0-7) 

**3. LRU eviction is purely count-based:**
The `HotStateLRU::maybe_evict()` method only checks item count against capacity: [9](#0-8) 

**Attack Scenario:**
1. Attacker submits transactions that create/update state values at ~1MB each across different StateKeys
2. Each transaction passes `check_change_set()` validation since values are within the 1MB limit
3. These values are committed to the database and loaded into the hot state cache
4. The hot state cache accepts up to 4 million items (250K per shard × 16 shards)
5. With 1MB values, worst case memory consumption: 4M items × 1MB = **4TB**
6. This is 2000× the designed 2GB, causing memory exhaustion and OOM

The system violates the invariant "Resource Limits: All operations must respect gas, storage, and computational limits" because the hot state memory consumption is unbounded beyond item count.

## Impact Explanation

**HIGH Severity** per Aptos Bug Bounty criteria: "Validator node slowdowns" and "Significant protocol violations"

The vulnerability enables a Denial of Service attack against validator nodes:

- **Direct Impact**: Validator nodes will experience extreme memory pressure leading to:
  - Severe performance degradation as the system swaps to disk
  - OOM kills requiring node restarts
  - Potential consensus participation failures during memory exhaustion
  
- **Attack Cost vs Damage**: While the attacker must pay storage fees for creating large state values, the fees are bounded by gas limits. The attacker can cause disproportionate harm (validator crashes) for moderate cost (storage fees for 1MB values).

- **Persistence**: Once large values are written, they remain in hot state until evicted by the LRU policy (based on item count), meaning the attack effect persists across multiple blocks.

- **Network-wide Impact**: If an attacker targets multiple validator nodes simultaneously or if large values become frequently accessed across the network, multiple validators could experience memory exhaustion, potentially affecting network liveness.

This does not reach Critical severity because it does not directly cause loss of funds, consensus safety violations, or permanent network damage. However, it clearly meets High severity as a significant availability attack.

## Likelihood Explanation

**MEDIUM-HIGH Likelihood**

**Attacker Requirements:**
- APT tokens to pay for transaction gas and storage fees (moderate barrier)
- Ability to craft transactions with large state values (trivial)
- No special privileges required (any user can execute)

**Exploitation Complexity:**
- Low technical complexity: simply write large Move resources or modify large state values
- Attack can be sustained over time by creating new large values or refreshing existing ones
- The hot state naturally tracks recently-accessed items, so creating values that are accessed by other transactions makes the attack more effective

**Mitigating Factors:**
- Storage fees provide economic cost (though bounded)
- Hot state evicts old items based on LRU, so inactive large values eventually get evicted
- Requires sustained effort to maintain pressure on memory

**Amplifying Factors:**
- Default configuration is vulnerable (250K items/shard with no byte limit)
- No monitoring or alerting specifically for hot state byte size approaching dangerous levels
- Multiple attackers could coordinate to amplify impact

A motivated attacker with moderate resources (sufficient to pay storage fees) can execute this attack with high probability of causing at least temporary validator degradation.

## Recommendation

**Immediate Fix:** Add total byte size limits to the hot state cache alongside item count limits.

**Configuration Update:** [10](#0-9) 

Add a `max_bytes_per_shard` field to enforce the designed 2GB total (approximately 128MB per shard with 16 shards).

**Hot State LRU Update:**

Modify the `HotStateLRU` structure to track total bytes and enforce byte limits during eviction: [11](#0-10) 

Update the eviction logic to check both item count AND total bytes.

**Long-term Improvements:**
1. Consider tiered limits: allocate more slots for small values than large values
2. Add monitoring and alerting when hot state memory usage exceeds thresholds
3. Evaluate whether the 1MB `max_bytes_per_write_op` limit can be reduced to 256KB or 512KB
4. Document the relationship between transaction-level limits and storage-level constraints

## Proof of Concept

Due to the complexity of the full Aptos environment, the following is a conceptual demonstration:

**Move Module (Conceptual):**
An attacker would deploy and execute a Move module that creates resources with large vector data approaching the 1MB limit. The module would iterate across multiple accounts to create many such resources, each passing validation but collectively consuming excessive hot state memory.

**Rust Test (Conceptual):**
A test would instantiate a hot state cache with default configuration, then simulate committing 100,000 state values of ~1MB each. The test would measure memory consumption and demonstrate that eviction only occurs after 250,000 items (not at 2GB byte limit), allowing up to 100GB memory consumption from just 100K items. In practice, this would cause OOM before reaching the item limit.

The vulnerability is confirmed by examining the code flow:
1. Large values pass `check_change_set()` validation
2. They are tracked by `total_value_bytes` in `Committer::commit()`
3. But `HotStateLRU::maybe_evict()` never checks `total_value_bytes`
4. Only `num_items` is checked against `capacity`

This architectural gap enables memory exhaustion attacks.

### Citations

**File:** config/src/config/storage_config.rs (L24-24)
```rust
// Lru cache will consume about 2G RAM based on this default value.
```

**File:** config/src/config/storage_config.rs (L241-254)
```rust
#[derive(Clone, Copy, Debug, Eq, PartialEq, Serialize, Deserialize)]
#[serde(default, deny_unknown_fields)]
pub struct HotStateConfig {
    /// Max number of items in each shard.
    pub max_items_per_shard: usize,
    /// Every now and then refresh `hot_since_version` for hot items to prevent them from being
    /// evicted.
    pub refresh_interval_versions: u64,
    /// Whether to delete persisted data on disk on restart. Used during development.
    pub delete_on_restart: bool,
    /// Whether we compute root hashes for hot state in executor and commit the resulting JMT to
    /// db.
    pub compute_root_hash: bool,
}
```

**File:** config/src/config/storage_config.rs (L259-259)
```rust
            max_items_per_shard: 250_000,
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L154-156)
```rust
            max_bytes_per_write_op: NumBytes,
            { 5.. => "max_bytes_per_write_op" },
            1 << 20, // a single state item is 1MB max
```

**File:** aptos-move/aptos-vm-types/src/storage/change_set_configs.rs (L102-108)
```rust
        for (key, op_size) in change_set.write_set_size_iter() {
            if let Some(len) = op_size.write_len() {
                let write_op_size = len + (key.size() as u64);
                if write_op_size > self.max_bytes_per_write_op {
                    return storage_write_limit_reached(None);
                }
                write_set_size += write_op_size;
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L199-201)
```rust
            GAUGE.set_with(&["hot_state_items"], self.base.len() as i64);
            GAUGE.set_with(&["hot_state_key_bytes"], self.total_key_bytes as i64);
            GAUGE.set_with(&["hot_state_value_bytes"], self.total_value_bytes as i64);
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L244-261)
```rust
            for (key, slot) in delta.shards[shard_id].iter() {
                if slot.is_hot() {
                    let key_size = key.size();
                    self.total_key_bytes += key_size;
                    self.total_value_bytes += slot.size();
                    if let Some(old_slot) = self.base.shards[shard_id].insert(key, slot) {
                        self.total_key_bytes -= key_size;
                        self.total_value_bytes -= old_slot.size();
                        n_update += 1;
                    } else {
                        n_insert += 1;
                    }
                } else if let Some((key, old_slot)) = self.base.shards[shard_id].remove(&key) {
                    self.total_key_bytes -= key.size();
                    self.total_value_bytes -= old_slot.size();
                    n_evict += 1;
                }
            }
```

**File:** types/src/state_store/state_slot.rs (L153-158)
```rust
    pub fn size(&self) -> usize {
        match self {
            ColdVacant | HotVacant { .. } => 0,
            ColdOccupied { value, .. } | HotOccupied { value, .. } => value.size(),
        }
    }
```

**File:** types/src/state_store/state_value.rs (L272-274)
```rust
    pub fn size(&self) -> usize {
        self.bytes().len()
    }
```

**File:** storage/storage-interface/src/state_store/hot_state.rs (L11-27)
```rust
pub(crate) struct HotStateLRU<'a> {
    /// Max total number of items in the cache.
    capacity: NonZeroUsize,
    /// The entire committed hot state. While this contains all the shards, this struct is supposed
    /// to handle a single shard.
    committed: Arc<dyn HotStateView>,
    /// Additional entries resulted from previous speculative execution.
    overlay: &'a LayeredMap<StateKey, StateSlot>,
    /// The new entries from current execution.
    pending: HashMap<StateKey, StateSlot>,
    /// Points to the latest entry. `None` if empty.
    head: Option<StateKey>,
    /// Points to the oldest entry. `None` if empty.
    tail: Option<StateKey>,
    /// Total number of items.
    num_items: usize,
}
```

**File:** storage/storage-interface/src/state_store/hot_state.rs (L82-106)
```rust
    pub fn maybe_evict(&mut self) -> Vec<(StateKey, StateSlot)> {
        let mut current = match &self.tail {
            Some(tail) => tail.clone(),
            None => {
                assert_eq!(self.num_items, 0);
                return Vec::new();
            },
        };

        let mut evicted = Vec::new();
        while self.num_items > self.capacity.get() {
            let slot = self
                .delete(&current)
                .expect("There must be entries to evict when current size is above capacity.");
            let prev_key = slot
                .prev()
                .cloned()
                .expect("There must be at least one newer entry (num_items > capacity >= 1).");
            evicted.push((current.clone(), slot.clone()));
            self.pending.insert(current, slot.to_cold());
            current = prev_key;
            self.num_items -= 1;
        }
        evicted
    }
```
