# Audit Report

## Title
Memory Exhaustion via Unbounded Resource Deserialization in Read Path

## Summary
The Move VM's resource loading mechanism lacks size validation on the read path, allowing deserialization of arbitrarily large resources before gas limits are enforced. While write-side limits prevent creation of new oversized resources, legacy resources from early blockchain versions (pre-gas feature version 3) could bypass these protections, enabling memory exhaustion attacks against validator nodes.

## Finding Description

The vulnerability exists in the resource loading flow where size validation occurs **after** deserialization rather than before: [1](#0-0) 

The `get_resource_bytes_with_metadata_and_layout` trait method returns `(Option<Bytes>, usize)` with no size constraints. The implementation in `StorageAdapter` retrieves bytes from storage without validation: [2](#0-1) 

The critical flaw occurs in `create_data_cache_entry` where raw bytes are deserialized without prior size checks: [3](#0-2) 

Gas is only charged **after** the resource is fully loaded and deserialized: [4](#0-3) 

**Write-side protection** exists but only applies to new writes: [5](#0-4) [6](#0-5) 

Before gas feature version 3, limits were unlimited. Version 3-4 had 1MB limits, but these only applied to **writes**, not reads: [7](#0-6) 

**Invariant Violation:** This breaks the critical invariant that "All operations must respect gas, storage, and computational limits" and "Bytecode execution must respect gas limits and memory constraints."

## Impact Explanation

**Severity: High** (per Aptos Bug Bounty criteria for "Validator node slowdowns")

If exploitable, an attacker could:
1. Identify or reference large legacy resources in storage (from pre-version-3 era)
2. Craft transactions that load these resources via `BorrowGlobal`, `MoveFrom`, or similar operations
3. Force validators to deserialize multi-megabyte or larger resources into memory **before** gas validation
4. Even if the transaction fails due to insufficient gas, memory consumption has already occurred
5. Repeat with multiple transactions to exhaust validator memory, causing:
   - Node crashes or slowdowns
   - Degraded block processing performance
   - Potential consensus liveness issues if enough validators are affected

The storage layer reads directly from RocksDB without size checks: [8](#0-7) 

## Likelihood Explanation

**Likelihood: Low to Medium**

Factors reducing likelihood:
- Write limits have been enforced since gas feature version 3 (though not version 0-2)
- Requires existence of oversized resources in storage from early blockchain versions
- Aptos mainnet may have never run with version < 3

Factors increasing likelihood:
- No defense-in-depth validation on read path
- System transactions also go through same vulnerable code path
- Historical data from testnet migrations or genesis could contain oversized resources
- Table items and resource groups added after version 3 may have had different enforcement history

**Critical question:** Without access to historical Aptos blockchain state, I cannot verify whether oversized resources exist. However, the architectural flaw exists regardless.

## Recommendation

**Add size validation before deserialization:**

1. Check resource byte size against `max_bytes_per_write_op` limit **before** calling deserialize
2. Return `STORAGE_ERROR` or `RESOURCE_TOO_LARGE` if size exceeds limit
3. Charge gas for the size check before attempting deserialization

Proposed fix in `create_data_cache_entry`:

```rust
// After line 295 in data_cache.rs
let (data, bytes_loaded) = { /* ... */ };

// ADD SIZE VALIDATION HERE
if bytes_loaded > MAX_RESOURCE_SIZE_BYTES {
    return Err(PartialVMError::new(StatusCode::STORAGE_ERROR)
        .with_message(format!(
            "Resource at {} exceeds maximum size: {} bytes (limit: {} bytes)",
            addr, bytes_loaded, MAX_RESOURCE_SIZE_BYTES
        )));
}

// Then proceed with deserialization
let value = match data { /* ... */ };
```

Add constant:
```rust
const MAX_RESOURCE_SIZE_BYTES: usize = 1 << 20; // 1MB, matching write limit
```

This implements defense-in-depth by validating size at read time, not just write time.

## Proof of Concept

**Conceptual PoC** (requires historical oversized resource):

```rust
// Hypothetical scenario - would require historical state
#[test]
fn test_oversized_resource_dos() {
    // Setup: Assume resource at address with >1MB data exists from pre-v3 era
    let oversized_address = AccountAddress::from_hex_literal("0xBAD").unwrap();
    
    // Attacker crafts transaction to load the resource
    let txn = create_transaction_with_borrow_global(oversized_address);
    
    // Execute transaction
    let result = vm.execute_transaction(txn);
    
    // Even if transaction fails due to gas:
    // - Memory was consumed during deserialization
    // - Validator performance degraded
    // - Multiple such transactions could exhaust memory
}
```

Without access to verify historical oversized resources exist in Aptos mainnet/testnet state, this remains a theoretical vulnerability with high severity **if** exploitable.

## Notes

This vulnerability represents a **defense-in-depth failure** where the read path trusts that all stored resources respect current size limits. While write-side enforcement prevents new oversized resources, the lack of read-side validation means:

1. Any historical oversized resources remain exploitable
2. Future bugs in write-side enforcement could be amplified
3. The system violates the principle of "check before use"

The actual exploitability depends on whether Aptos blockchain state contains resources exceeding current limits, which cannot be verified from code analysis alone. This requires historical data analysis of the actual blockchain state.

### Citations

**File:** third_party/move/move-vm/types/src/resolver.rs (L26-33)
```rust
    fn get_resource_bytes_with_metadata_and_layout(
        &self,
        address: &AccountAddress,
        struct_tag: &StructTag,
        metadata: &[Metadata],
        layout: Option<&MoveTypeLayout>,
    ) -> PartialVMResult<(Option<Bytes>, usize)>;
}
```

**File:** aptos-move/aptos-vm/src/data_cache.rs (L165-174)
```rust
    fn get_resource_bytes_with_metadata_and_layout(
        &self,
        address: &AccountAddress,
        struct_tag: &StructTag,
        metadata: &[Metadata],
        maybe_layout: Option<&MoveTypeLayout>,
    ) -> PartialVMResult<(Option<Bytes>, usize)> {
        self.get_any_resource_with_layout(address, struct_tag, metadata, maybe_layout)
    }
}
```

**File:** third_party/move/move-vm/runtime/src/data_cache.rs (L289-315)
```rust
            resource_resolver.get_resource_bytes_with_metadata_and_layout(
                addr,
                &struct_tag,
                &module.metadata,
                layout_with_delayed_fields.layout_when_contains_delayed_fields(),
            )?
        };

        let function_value_extension = FunctionValueExtensionAdapter { module_storage };
        let (layout, contains_delayed_fields) = layout_with_delayed_fields.unpack();
        let value = match data {
            Some(blob) => {
                let max_value_nest_depth = function_value_extension.max_value_nest_depth();
                let val = ValueSerDeContext::new(max_value_nest_depth)
                    .with_func_args_deserialization(&function_value_extension)
                    .with_delayed_fields_serde()
                    .deserialize(&blob, &layout)
                    .ok_or_else(|| {
                        let msg = format!(
                            "Failed to deserialize resource {} at {}!",
                            struct_tag.to_canonical_string(),
                            addr
                        );
                        PartialVMError::new(StatusCode::FAILED_TO_DESERIALIZE_RESOURCE)
                            .with_message(msg)
                    })?;
                GlobalValue::cached(val)?
```

**File:** third_party/move/move-vm/runtime/src/interpreter.rs (L1318-1332)
```rust
        let (gv, bytes_loaded) =
            data_cache.load_resource_mut(gas_meter, traversal_context, &addr, ty)?;
        if let Some(bytes_loaded) = bytes_loaded {
            gas_meter.charge_load_resource(
                addr,
                TypeWithRuntimeEnvironment {
                    ty,
                    runtime_environment: self.loader.runtime_environment(),
                },
                gv.view(),
                bytes_loaded,
            )?;
        }

        Ok(gv)
```

**File:** aptos-move/aptos-vm-types/src/storage/change_set_configs.rs (L31-39)
```rust
    pub fn new(feature_version: u64, gas_params: &AptosGasParameters) -> Self {
        if feature_version >= 5 {
            Self::from_gas_params(feature_version, gas_params)
        } else if feature_version >= 3 {
            Self::for_feature_version_3()
        } else {
            Self::unlimited_at_gas_feature_version(feature_version)
        }
    }
```

**File:** aptos-move/aptos-vm-types/src/storage/change_set_configs.rs (L68-72)
```rust
    fn for_feature_version_3() -> Self {
        const MB: u64 = 1 << 20;

        Self::new_impl(3, MB, u64::MAX, MB, 10 * MB, u64::MAX)
    }
```

**File:** aptos-move/aptos-vm-types/src/storage/change_set_configs.rs (L86-128)
```rust
    pub fn check_change_set(&self, change_set: &impl ChangeSetInterface) -> Result<(), VMStatus> {
        let storage_write_limit_reached = |maybe_message: Option<&str>| {
            let mut err = PartialVMError::new(StatusCode::STORAGE_WRITE_LIMIT_REACHED);
            if let Some(message) = maybe_message {
                err = err.with_message(message.to_string())
            }
            Err(err.finish(Location::Undefined).into_vm_status())
        };

        if self.max_write_ops_per_transaction != 0
            && change_set.num_write_ops() as u64 > self.max_write_ops_per_transaction
        {
            return storage_write_limit_reached(Some("Too many write ops."));
        }

        let mut write_set_size = 0;
        for (key, op_size) in change_set.write_set_size_iter() {
            if let Some(len) = op_size.write_len() {
                let write_op_size = len + (key.size() as u64);
                if write_op_size > self.max_bytes_per_write_op {
                    return storage_write_limit_reached(None);
                }
                write_set_size += write_op_size;
            }
            if write_set_size > self.max_bytes_all_write_ops_per_transaction {
                return storage_write_limit_reached(None);
            }
        }

        let mut total_event_size = 0;
        for event in change_set.events_iter() {
            let size = event.event_data().len() as u64;
            if size > self.max_bytes_per_event {
                return storage_write_limit_reached(None);
            }
            total_event_size += size;
            if total_event_size > self.max_bytes_all_events_per_transaction {
                return storage_write_limit_reached(None);
            }
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/state_kv_db.rs (L374-402)
```rust
    pub(crate) fn get_state_value_with_version_by_version(
        &self,
        state_key: &StateKey,
        version: Version,
    ) -> Result<Option<(Version, StateValue)>> {
        let mut read_opts = ReadOptions::default();

        // We want `None` if the state_key changes in iteration.
        read_opts.set_prefix_same_as_start(true);
        if !self.enabled_sharding() {
            let mut iter = self
                .db_shard(state_key.get_shard_id())
                .iter_with_opts::<StateValueSchema>(read_opts)?;
            iter.seek(&(state_key.clone(), version))?;
            Ok(iter
                .next()
                .transpose()?
                .and_then(|((_, version), value_opt)| value_opt.map(|value| (version, value))))
        } else {
            let mut iter = self
                .db_shard(state_key.get_shard_id())
                .iter_with_opts::<StateValueByKeyHashSchema>(read_opts)?;
            iter.seek(&(state_key.hash(), version))?;
            Ok(iter
                .next()
                .transpose()?
                .and_then(|((_, version), value_opt)| value_opt.map(|value| (version, value))))
        }
    }
```
