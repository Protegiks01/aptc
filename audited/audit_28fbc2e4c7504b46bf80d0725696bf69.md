# Audit Report

## Title
Race Condition in Network Connection Event Broadcasting Causes Out-of-Order NewPeer/LostPeer Notifications

## Summary
A critical race condition exists in the `PeersAndMetadata::broadcast()` mechanism due to asymmetric lock handling between `insert_connection_metadata()` and `remove_peer_metadata()`. This allows `NewPeer` and `LostPeer` events for the same peer to arrive out of order to subscribers, causing incorrect connection state tracking in critical network components like the HealthChecker.

## Finding Description

The vulnerability stems from inconsistent lock management in two key functions: [1](#0-0) 

In `insert_connection_metadata()`, the write lock on `peers_and_metadata` is acquired, the peer metadata is inserted, the cached data is updated, **then the lock is released** (when `peers_and_metadata` goes out of scope after line 207), and finally `broadcast(NewPeer)` is called at line 211 **without lock protection**. [2](#0-1) 

In contrast, `remove_peer_metadata()` calls `broadcast(LostPeer)` at line 245 **while still holding the write lock**, before updating the cached data at line 259.

This asymmetry creates a race condition window:

**Race Condition Timeline:**
1. **Thread A** calls `insert_connection_metadata(peer_X)`: acquires lock → inserts peer → releases lock (line 207) → **[WINDOW]** → broadcasts NewPeer (line 211)
2. **Thread B** calls `remove_peer_metadata(peer_X)` during the window: acquires lock → removes peer → broadcasts LostPeer (line 245) → releases lock

**Result:** Subscribers receive events in order: `LostPeer` → `NewPeer`, even though the actual final state is that peer_X is **disconnected**.

The HealthChecker, which subscribes to these events, maintains connection state based on these notifications: [3](#0-2) 

When out-of-order events arrive:
1. `LostPeer` received → HealthChecker calls `remove_peer_and_health_data()` → removes peer from tracking
2. `NewPeer` received → HealthChecker calls `create_peer_and_health_data()` → **adds peer back** to tracking [4](#0-3) 

The HealthChecker now believes the peer is connected and will attempt to send health check pings to a non-existent connection, causing protocol failures.

## Impact Explanation

This vulnerability qualifies as **HIGH severity** under the Aptos bug bounty criteria:

1. **Validator node slowdowns**: The HealthChecker will continuously attempt to ping disconnected peers, wasting resources and causing timeouts that slow down network operations.

2. **Significant protocol violations**: Connection state tracking is a fundamental network protocol invariant. Breaking this causes cascading failures in message routing, peer selection, and network reliability.

3. **API crashes**: Components relying on accurate connection state may crash or enter error states when attempting operations on peers they believe are connected but are actually disconnected.

The impact extends beyond HealthChecker to any component that subscribes to connection events via `PeersAndMetadata::subscribe()`, including potential future consensus and state sync observers.

## Likelihood Explanation

The likelihood is **MEDIUM to HIGH**:

**Triggering Conditions:**
- Peer connects (Thread A: `insert_connection_metadata`)
- Peer immediately disconnects due to network issues, authentication failure, or deliberate disconnect (Thread B: `remove_peer_metadata`)
- Timing window between Thread A releasing the lock (line 207) and calling broadcast (line 211)

**Realistic Scenarios:**
1. **Rapid peer churn**: In production networks with unstable connections, peers frequently connect/disconnect
2. **Network partitions**: During network splits or recovery, multiple peers may reconnect/disconnect rapidly
3. **Connection replacement**: When a peer reconnects with a new connection while the old one is being cleaned up
4. **Deliberate exploitation**: A malicious peer can repeatedly connect/disconnect to trigger the race condition

The race window is small but deterministic under concurrent load, making it exploitable through repeated attempts.

## Recommendation

**Fix: Ensure consistent lock scope for both functions**

Move the `broadcast()` call inside the lock scope for `insert_connection_metadata()`:

```rust
pub fn insert_connection_metadata(
    &self,
    peer_network_id: PeerNetworkId,
    connection_metadata: ConnectionMetadata,
) -> Result<(), Error> {
    let mut peers_and_metadata = self.peers_and_metadata.write();
    
    let peer_metadata_for_network =
        get_peer_metadata_for_network(&peer_network_id, &mut peers_and_metadata)?;
    
    peer_metadata_for_network
        .entry(peer_network_id.peer_id())
        .and_modify(|peer_metadata| {
            peer_metadata.connection_metadata = connection_metadata.clone()
        })
        .or_insert_with(|| PeerMetadata::new(connection_metadata.clone()));
    
    // Update cached data and broadcast BEFORE releasing the lock
    self.set_cached_peers_and_metadata(peers_and_metadata.clone());
    
    let event = ConnectionNotification::NewPeer(
        connection_metadata, 
        peer_network_id.network_id()
    );
    self.broadcast(event);
    
    // Lock released here when peers_and_metadata drops
    Ok(())
}
```

This ensures both `insert_connection_metadata()` and `remove_peer_metadata()` call `broadcast()` with lock protection, preventing out-of-order delivery.

## Proof of Concept

```rust
#[tokio::test]
async fn test_concurrent_peer_events_race_condition() {
    use std::sync::Arc;
    use tokio::sync::Barrier;
    use aptos_config::network_id::NetworkId;
    
    // Create peers and metadata
    let peers_and_metadata = PeersAndMetadata::new(&[NetworkId::Validator]);
    let subscriber_events = peers_and_metadata.subscribe();
    
    // Create test peer and connection
    let peer_id = PeerId::random();
    let peer_network_id = PeerNetworkId::new(NetworkId::Validator, peer_id);
    let connection_metadata = create_test_connection_metadata(peer_id);
    let connection_id = connection_metadata.connection_id;
    
    // Barrier to synchronize threads for maximum race condition likelihood
    let barrier = Arc::new(Barrier::new(2));
    let barrier_clone = barrier.clone();
    let peers_and_metadata_clone = peers_and_metadata.clone();
    let peer_network_id_clone = peer_network_id.clone();
    let connection_metadata_clone = connection_metadata.clone();
    
    // Thread A: insert peer
    let insert_handle = tokio::spawn(async move {
        barrier_clone.wait().await;
        peers_and_metadata_clone
            .insert_connection_metadata(peer_network_id_clone, connection_metadata_clone)
            .unwrap();
    });
    
    // Thread B: remove peer
    let remove_handle = tokio::spawn(async move {
        barrier.wait().await;
        // Small delay to hit the race window
        tokio::time::sleep(Duration::from_micros(10)).await;
        let _ = peers_and_metadata
            .remove_peer_metadata(peer_network_id, connection_id);
    });
    
    insert_handle.await.unwrap();
    remove_handle.await.unwrap();
    
    // Collect all events
    let mut events = vec![];
    while let Ok(event) = tokio::time::timeout(
        Duration::from_millis(100), 
        subscriber_events.recv()
    ).await {
        if let Some(e) = event {
            events.push(e);
        }
    }
    
    // Check if we received out-of-order events
    if events.len() >= 2 {
        match (&events[events.len()-2], &events[events.len()-1]) {
            (ConnectionNotification::LostPeer(_, _), 
             ConnectionNotification::NewPeer(_, _)) => {
                panic!("OUT OF ORDER EVENTS DETECTED: LostPeer followed by NewPeer!");
            }
            _ => {}
        }
    }
}
```

Run this test repeatedly (e.g., 1000 iterations) to observe the race condition manifest as out-of-order event delivery.

## Notes

The vulnerability affects all subscribers to connection events through `PeersAndMetadata::subscribe()`. While the HealthChecker is the primary documented consumer, any future consensus observer, state sync component, or network monitor using this subscription mechanism would be affected. [5](#0-4) 

The `broadcast()` function itself uses proper locking on `self.subscribers`, but this only protects the sending operation, not the ordering guarantees across multiple callers. The root cause is the inconsistent lock scope between the two calling functions.

### Citations

**File:** network/framework/src/application/storage.rs (L186-214)
```rust
    pub fn insert_connection_metadata(
        &self,
        peer_network_id: PeerNetworkId,
        connection_metadata: ConnectionMetadata,
    ) -> Result<(), Error> {
        // Grab the write lock for the peer metadata
        let mut peers_and_metadata = self.peers_and_metadata.write();

        // Fetch the peer metadata for the given network
        let peer_metadata_for_network =
            get_peer_metadata_for_network(&peer_network_id, &mut peers_and_metadata)?;

        // Update the metadata for the peer or insert a new entry
        peer_metadata_for_network
            .entry(peer_network_id.peer_id())
            .and_modify(|peer_metadata| {
                peer_metadata.connection_metadata = connection_metadata.clone()
            })
            .or_insert_with(|| PeerMetadata::new(connection_metadata.clone()));

        // Update the cached peers and metadata
        self.set_cached_peers_and_metadata(peers_and_metadata.clone());

        let event =
            ConnectionNotification::NewPeer(connection_metadata, peer_network_id.network_id());
        self.broadcast(event);

        Ok(())
    }
```

**File:** network/framework/src/application/storage.rs (L219-262)
```rust
    pub fn remove_peer_metadata(
        &self,
        peer_network_id: PeerNetworkId,
        connection_id: ConnectionId,
    ) -> Result<PeerMetadata, Error> {
        // Grab the write lock for the peer metadata
        let mut peers_and_metadata = self.peers_and_metadata.write();

        // Fetch the peer metadata for the given network
        let peer_metadata_for_network =
            get_peer_metadata_for_network(&peer_network_id, &mut peers_and_metadata)?;

        // Remove the peer metadata for the peer
        let peer_metadata = if let Entry::Occupied(entry) =
            peer_metadata_for_network.entry(peer_network_id.peer_id())
        {
            // Don't remove the peer if the connection doesn't match!
            // For now, remove the peer entirely, we could in the future
            // have multiple connections for a peer
            let active_connection_id = entry.get().connection_metadata.connection_id;
            if active_connection_id == connection_id {
                let peer_metadata = entry.remove();
                let event = ConnectionNotification::LostPeer(
                    peer_metadata.connection_metadata.clone(),
                    peer_network_id.network_id(),
                );
                self.broadcast(event);
                peer_metadata
            } else {
                return Err(Error::UnexpectedError(format!(
                    "The peer connection id did not match! Given: {:?}, found: {:?}.",
                    connection_id, active_connection_id
                )));
            }
        } else {
            // Unable to find the peer metadata for the given peer
            return Err(missing_peer_metadata_error(&peer_network_id));
        };

        // Update the cached peers and metadata
        self.set_cached_peers_and_metadata(peers_and_metadata.clone());

        Ok(peer_metadata)
    }
```

**File:** network/framework/src/application/storage.rs (L371-395)
```rust
    fn broadcast(&self, event: ConnectionNotification) {
        let mut listeners = self.subscribers.lock();
        let mut to_del = vec![];
        for i in 0..listeners.len() {
            let dest = listeners.get_mut(i).unwrap();
            if let Err(err) = dest.try_send(event.clone()) {
                match err {
                    TrySendError::Full(_) => {
                        // Tried to send to an app, but the app isn't handling its messages fast enough.
                        // Drop message. Maybe increment a metrics counter?
                        sample!(
                            SampleRate::Duration(Duration::from_secs(1)),
                            warn!("PeersAndMetadata.broadcast() failed, some app is slow"),
                        );
                    },
                    TrySendError::Closed(_) => {
                        to_del.push(i);
                    },
                }
            }
        }
        for evict in to_del.into_iter() {
            listeners.swap_remove(evict);
        }
    }
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L209-227)
```rust
                conn_event = connection_events.select_next_some() => {
                    match conn_event {
                        ConnectionNotification::NewPeer(metadata, network_id) => {
                            // PeersAndMetadata is a global singleton across all networks; filter connect/disconnect events to the NetworkId that this HealthChecker instance is watching
                            if network_id == self_network_id {
                                self.network_interface.create_peer_and_health_data(
                                    metadata.remote_peer_id, self.round
                                );
                            }
                        }
                        ConnectionNotification::LostPeer(metadata, network_id) => {
                            // PeersAndMetadata is a global singleton across all networks; filter connect/disconnect events to the NetworkId that this HealthChecker instance is watching
                            if network_id == self_network_id {
                                self.network_interface.remove_peer_and_health_data(
                                    &metadata.remote_peer_id
                                );
                            }
                        }
                    }
```

**File:** network/framework/src/protocols/health_checker/interface.rs (L95-106)
```rust
    pub fn create_peer_and_health_data(&mut self, peer_id: PeerId, round: u64) {
        self.health_check_data
            .write()
            .entry(peer_id)
            .and_modify(|health_check_data| health_check_data.round = round)
            .or_insert_with(|| HealthCheckData::new(round));
    }

    /// Removes the peer and any associated health data
    pub fn remove_peer_and_health_data(&mut self, peer_id: &PeerId) {
        self.health_check_data.write().remove(peer_id);
    }
```
