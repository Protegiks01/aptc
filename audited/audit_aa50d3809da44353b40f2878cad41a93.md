# Audit Report

## Title
Race Condition in Parallel State Restore Causing KV-Tree Inconsistency with Async Commits

## Summary
A race condition exists in `StateSnapshotRestore::add_chunk()` where asynchronous tree writes can fail after synchronous KV writes have completed for subsequent chunks, resulting in state inconsistency between the StateKV database and Jellyfish Merkle Tree during backup restoration operations.

## Finding Description

The vulnerability exists in the parallel execution model used during state snapshot restoration when `async_commit=true`. The production restore handler explicitly enables asynchronous commits: [1](#0-0) 

During chunk processing, two operations execute in parallel via `IO_POOL.join()`: [2](#0-1) 

The KV write function (`kv_fn`) performs synchronous commits that block until completion: [3](#0-2) 

The synchronous nature is enforced by `StateKvDb::commit()` which uses `scope()` to wait for all shard writes: [4](#0-3) 

In contrast, the tree write function (`tree_fn`) spawns asynchronous writes that return immediately when `async_commit=true`: [5](#0-4) 

The asynchronous write failure is only detected during the NEXT chunk's processing: [6](#0-5) 

**Race Condition Scenario:**
1. Chunk N: Tree spawns async write and returns Ok immediately; KV completes synchronous write
2. Chunk N+1: Both functions start in parallel
   - KV function writes chunk N+1 synchronously to database
   - Tree function waits for chunk N's async write which fails
3. Result: KV database contains chunks {N, N+1} with progress updated, but tree only has chunks {0...N-1}

The progress tracking uses the minimum of both: [7](#0-6) 

However, there is no validation at finish to detect this inconsistency: [8](#0-7) 

## Impact Explanation

**Severity: Medium** per Aptos Bug Bounty criteria - "State inconsistencies requiring manual intervention"

The vulnerability causes state desynchronization where:
- State KV database contains more recent chunks than Jellyfish Merkle Tree
- State root hash no longer corresponds to the actual KV data
- Node cannot serve valid Merkle proofs for the stored state
- Requires either completing the restore (if possible) or manual database cleanup
- If the node attempts consensus participation with inconsistent state, it will produce incorrect state roots causing validator disagreement

While the progress tracking mechanism allows automatic recovery by resuming the restore, this requires the restore to be completable. If the restore cannot be completed due to persistent failures or unavailable backup sources, manual intervention is required.

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability occurs during:
- Backup restoration operations (confirmed by `async_commit=true` in production)
- I/O failures during async tree writes (disk errors, storage exhaustion, network issues)
- Resource constraint scenarios during large-scale state restores

The race window exists between every chunk processing during multi-GB state restores. With hundreds of thousands of chunks processed, the probability of encountering an I/O failure during the async write window becomes non-negligible, especially under:
- Resource exhaustion conditions
- Unreliable storage infrastructure
- Network disruptions during remote backup fetching

## Recommendation

Implement one of the following solutions:

**Option 1: Synchronous validation before KV commit**
Wait for the previous tree async write to complete before starting the KV write for the next chunk:
```rust
// In add_chunk(), before running kv_fn
self.wait_for_async_commit()?;
let (r1, r2) = IO_POOL.join(kv_fn, tree_fn);
```

**Option 2: Transactional commit coordination**
Ensure both KV and tree commits complete successfully before updating progress, with rollback capability if either fails.

**Option 3: Add consistency validation at finish()**
Verify that KV progress and tree progress match before completing the restore operation.

## Proof of Concept

The vulnerability can be demonstrated by:
1. Starting a state snapshot restore with async_commit=true
2. Simulating an I/O failure for the tree async write after chunk N completes
3. Allowing chunk N+1's KV write to complete
4. Observing that KV database has chunk N+1 but tree only has up to chunk N-1
5. Verifying that `previous_key_hash()` returns min(kv_progress, tree_progress) which allows resume but leaves KV database ahead of tree until manual intervention

The race condition is inherent in the design where asynchronous tree writes can fail after synchronous KV writes have already committed subsequent chunks.

### Citations

**File:** storage/aptosdb/src/backup/restore_handler.rs (L47-54)
```rust
        StateSnapshotRestore::new(
            &self.state_store.state_merkle_db,
            &self.state_store,
            version,
            expected_root_hash,
            true, /* async_commit */
            restore_mode,
        )
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L196-214)
```rust
    pub fn previous_key_hash(&self) -> Result<Option<HashValue>> {
        let hash_opt = match (
            self.kv_restore
                .lock()
                .as_ref()
                .unwrap()
                .previous_key_hash()?,
            self.tree_restore
                .lock()
                .as_ref()
                .unwrap()
                .previous_key_hash(),
        ) {
            (None, hash_opt) => hash_opt,
            (hash_opt, None) => hash_opt,
            (Some(hash1), Some(hash2)) => Some(std::cmp::min(hash1, hash2)),
        };
        Ok(hash_opt)
    }
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L249-254)
```rust
            StateSnapshotRestoreMode::Default => {
                // We run kv_fn with TreeOnly to restore the usage of DB
                let (r1, r2) = IO_POOL.join(kv_fn, tree_fn);
                r1?;
                r2?;
            },
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L260-273)
```rust
    fn finish(self) -> Result<()> {
        match self.restore_mode {
            StateSnapshotRestoreMode::KvOnly => self.kv_restore.lock().take().unwrap().finish()?,
            StateSnapshotRestoreMode::TreeOnly => {
                self.tree_restore.lock().take().unwrap().finish_impl()?
            },
            StateSnapshotRestoreMode::Default => {
                // for tree only mode, we also need to write the usage to DB
                self.kv_restore.lock().take().unwrap().finish()?;
                self.tree_restore.lock().take().unwrap().finish_impl()?
            },
        }
        Ok(())
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1244-1279)
```rust
    fn write_kv_batch(
        &self,
        version: Version,
        node_batch: &StateValueBatch,
        progress: StateSnapshotProgress,
    ) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_value_writer_write_chunk"]);
        let mut batch = SchemaBatch::new();
        let mut sharded_schema_batch = self.state_kv_db.new_sharded_native_batches();

        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateSnapshotKvRestoreProgress(version),
            &DbMetadataValue::StateSnapshotProgress(progress),
        )?;

        if self.internal_indexer_db.is_some()
            && self
                .internal_indexer_db
                .as_ref()
                .unwrap()
                .statekeys_enabled()
        {
            let keys = node_batch.keys().map(|key| key.0.clone()).collect();
            self.internal_indexer_db
                .as_ref()
                .unwrap()
                .write_keys_to_indexer_db(&keys, version, progress)?;
        }
        self.shard_state_value_batch(
            &mut sharded_schema_batch,
            node_batch,
            self.state_kv_db.enabled_sharding(),
        )?;
        self.state_kv_db
            .commit(version, Some(batch), sharded_schema_batch)
    }
```

**File:** storage/aptosdb/src/state_kv_db.rs (L177-208)
```rust
    pub(crate) fn commit(
        &self,
        version: Version,
        state_kv_metadata_batch: Option<SchemaBatch>,
        sharded_state_kv_batches: ShardedStateKvSchemaBatch,
    ) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_db__commit"]);
        {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_db__commit_shards"]);
            THREAD_MANAGER.get_io_pool().scope(|s| {
                let mut batches = sharded_state_kv_batches.into_iter();
                for shard_id in 0..NUM_STATE_SHARDS {
                    let state_kv_batch = batches
                        .next()
                        .expect("Not sufficient number of sharded state kv batches");
                    s.spawn(move |_| {
                        // TODO(grao): Consider propagating the error instead of panic, if necessary.
                        self.commit_single_shard(version, shard_id, state_kv_batch)
                            .unwrap_or_else(|err| {
                                panic!("Failed to commit shard {shard_id}: {err}.")
                            });
                    });
                }
            });
        }
        if let Some(batch) = state_kv_metadata_batch {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_db__commit_metadata"]);
            self.state_kv_metadata_db.write_schemas(batch)?;
        }

        self.write_progress(version)
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L394-410)
```rust
        if self.async_commit {
            self.wait_for_async_commit()?;
            let (tx, rx) = channel();
            self.async_commit_result = Some(rx);

            let mut frozen_nodes = HashMap::new();
            std::mem::swap(&mut frozen_nodes, &mut self.frozen_nodes);
            let store = self.store.clone();

            IO_POOL.spawn(move || {
                let res = store.write_node_batch(&frozen_nodes);
                tx.send(res).unwrap();
            });
        } else {
            self.store.write_node_batch(&self.frozen_nodes)?;
            self.frozen_nodes.clear();
        }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L741-746)
```rust
    pub fn wait_for_async_commit(&mut self) -> Result<()> {
        if let Some(rx) = self.async_commit_result.take() {
            rx.recv()??;
        }
        Ok(())
    }
```
