# Audit Report

## Title
Indexer-GRPC-Manager Metadata Corruption on Ungraceful Shutdown Due to Missing Signal Handling

## Summary
The indexer-grpc-manager service lacks signal handling for SIGTERM/SIGINT, causing immediate process termination without graceful shutdown. This results in corrupted metadata files during in-progress write operations, preventing service restart and causing permanent availability loss.

## Finding Description

The `main()` function in the indexer-grpc-manager performs no signal handling, relying solely on the default `#[tokio::main]` runtime behavior. [1](#0-0) 

When SIGTERM or SIGINT is sent to the process, the tokio runtime terminates immediately without cleanup. The server framework spawns multiple background tasks using `tokio_scoped::scope()`: [2](#0-1) 

The `FileStoreUploader` performs critical metadata writes using non-atomic operations: [3](#0-2) [4](#0-3) 

For local storage, file writes use `tokio::fs::write()` which is non-atomic: [5](#0-4) 

**Attack Path:**
1. Operator sends SIGTERM to restart the indexer-grpc-manager service
2. Process terminates immediately without signal handler
3. If `FileStoreUploader::do_upload()` is mid-execution writing metadata files, writes are interrupted
4. Metadata files (JSON format) are left partially written
5. On restart, `recover()` attempts to read metadata via `get_file_store_metadata()`
6. JSON deserialization fails with panic: [6](#0-5) 

7. Service cannot restart, requiring manual metadata restoration

The vulnerability breaks the implicit state consistency requirement that metadata files remain valid across service restarts.

## Impact Explanation

This vulnerability qualifies as **HIGH severity** under the Aptos bug bounty program category "API crashes" because:

1. **Availability Loss**: The indexer-grpc API becomes permanently unavailable after crash, as the service panics on startup when attempting to read corrupted metadata
2. **No Automatic Recovery**: Manual intervention is required to restore corrupted metadata files from backups or rebuild the indexer state
3. **Operational Impact**: Any routine service restart (updates, maintenance, resource limits) can trigger data corruption
4. **Data Integrity**: Corrupted version tracking can cause transaction data loss or duplication if metadata is manually corrected incorrectly

## Likelihood Explanation

**VERY HIGH likelihood**:
- Triggered by normal operations (service restarts, deployments, container orchestration)
- No attacker sophistication required - standard process signals cause the issue
- Write operations occur continuously during normal indexer operation
- Race condition window is significant during high transaction throughput
- Affects all deployment environments (production, staging, development)

## Recommendation

Implement graceful shutdown with proper signal handling:

```rust
// In main.rs
#[tokio::main]
async fn main() -> Result<()> {
    let args = ServerArgs::parse();
    
    // Register signal handlers
    let shutdown_signal = async {
        tokio::signal::ctrl_c()
            .await
            .expect("Failed to install CTRL+C handler");
    };
    
    tokio::select! {
        result = args.run::<IndexerGrpcManagerConfig>() => result,
        _ = shutdown_signal => {
            tracing::info!("Shutdown signal received, cleaning up...");
            // Implement cleanup logic
            Ok(())
        }
    }
}
```

Additionally, implement atomic metadata writes using write-rename pattern:

```rust
// In local.rs save_raw_file()
async fn save_raw_file(&self, file_path: PathBuf, data: Vec<u8>) -> Result<()> {
    let file_path = self.path.join(file_path);
    if let Some(parent) = file_path.parent() {
        tokio::fs::create_dir_all(parent).await?;
    }
    
    // Write to temporary file first
    let temp_path = file_path.with_extension("tmp");
    tokio::fs::write(&temp_path, data).await?;
    
    // Atomic rename
    tokio::fs::rename(&temp_path, &file_path).await?;
    
    Ok(())
}
```

## Proof of Concept

```rust
// Rust test demonstrating the vulnerability
#[tokio::test]
async fn test_sigterm_metadata_corruption() {
    use std::process::{Command, Stdio};
    use std::time::Duration;
    use tokio::time::sleep;
    
    // Start indexer-grpc-manager process
    let mut child = Command::new("indexer-grpc-manager")
        .arg("--config-path")
        .arg("test_config.yaml")
        .stdout(Stdio::piped())
        .spawn()
        .expect("Failed to start indexer");
    
    // Wait for metadata writes to begin
    sleep(Duration::from_secs(2)).await;
    
    // Send SIGTERM during operation
    unsafe {
        libc::kill(child.id() as i32, libc::SIGTERM);
    }
    
    let _ = child.wait();
    
    // Attempt restart
    let restart = Command::new("indexer-grpc-manager")
        .arg("--config-path")
        .arg("test_config.yaml")
        .output()
        .expect("Failed to restart");
    
    // Verify panic on corrupted metadata
    assert!(
        String::from_utf8_lossy(&restart.stderr)
            .contains("Metadata JSON is invalid"),
        "Expected panic on corrupted metadata"
    );
}
```

## Notes

This vulnerability affects all indexer-grpc services using the `ServerArgs` framework pattern, including:
- indexer-grpc-cache-worker
- indexer-grpc-data-service-v2  
- indexer-grpc-file-store
- indexer-grpc-gateway

All these services lack signal handling and are vulnerable to metadata corruption on ungraceful shutdown.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/main.rs (L13-17)
```rust
#[tokio::main]
async fn main() -> Result<()> {
    let args = ServerArgs::parse();
    args.run::<IndexerGrpcManagerConfig>().await
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/grpc_manager.rs (L106-127)
```rust
        let (tx, rx) = channel();
        tokio_scoped::scope(|s| {
            s.spawn(async move {
                self.metadata_manager.start().await.unwrap();
            });
            s.spawn(async move { self.data_manager.start(self.is_master, rx).await });
            if self.is_master {
                s.spawn(async move {
                    self.file_store_uploader
                        .lock()
                        .await
                        .start(self.data_manager.clone(), tx)
                        .await
                        .unwrap();
                });
            }
            s.spawn(async move {
                info!("Starting GrpcManager at {}.", service_config.listen_address);
                server.serve(service_config.listen_address).await.unwrap();
            });
        });

```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/file_store_uploader.rs (L236-242)
```rust
            self.writer
                .save_raw_file(
                    batch_metadata_path,
                    serde_json::to_vec(&batch_metadata).map_err(anyhow::Error::msg)?,
                )
                .await?;
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/file_store_uploader.rs (L271-273)
```rust
        self.writer
            .save_raw_file(PathBuf::from(METADATA_FILE_NAME), raw_data)
            .await
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/local.rs (L61-69)
```rust
    async fn save_raw_file(&self, file_path: PathBuf, data: Vec<u8>) -> Result<()> {
        let file_path = self.path.join(file_path);
        if let Some(parent) = file_path.parent() {
            tokio::fs::create_dir_all(parent).await?;
        }
        tokio::fs::write(file_path, data)
            .await
            .map_err(anyhow::Error::msg)
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_reader.rs (L160-166)
```rust
    pub async fn get_file_store_metadata(&self) -> Option<FileStoreMetadata> {
        self.reader
            .get_raw_file(PathBuf::from(METADATA_FILE_NAME))
            .await
            .expect("Failed to get file store metadata.")
            .map(|data| serde_json::from_slice(&data).expect("Metadata JSON is invalid."))
    }
```
