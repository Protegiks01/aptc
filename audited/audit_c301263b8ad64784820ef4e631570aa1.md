# Audit Report

## Title
Missing LedgerInfo Signature Verification in Transaction Backup Restoration Enables Fake Transaction Injection

## Summary
The transaction backup restoration process fails to verify LedgerInfo signatures when `epoch_history` is not provided or when the epoch is beyond the historical range. This allows an attacker with database write access to create malicious backups containing fabricated transactions that would be accepted as valid during restoration, corrupting the blockchain state of any node restored from the compromised backup.

## Finding Description

The vulnerability exists in the backup restoration flow where cryptographic verification of LedgerInfo signatures is conditionally skipped, breaking the fundamental security guarantee that only validator-signed state transitions are valid.

**Attack Flow:**

1. **Backup Creation from Compromised Database**: When `BackupHandler::get_transaction_range_proof()` is called, it reads directly from the database without validating authenticity: [1](#0-0) 

2. **Database Manipulation**: An attacker with database write access can:
   - Insert arbitrary `Transaction` records
   - Insert corresponding fake `TransactionInfo` records  
   - Construct accumulator nodes that form a valid Merkle tree for the fake transaction infos
   - Insert a `LedgerInfoWithSignatures` with **invalid or missing validator signatures**

3. **Backup Contains Unverified Data**: The backup file will contain the fake proof and fake LedgerInfo because `get_transaction_range_proof()` simply reads what's in the database.

4. **Restoration Without Signature Verification**: During restoration, the critical vulnerability occurs in `LoadedChunk::load()`: [2](#0-1) 

   The signature verification is **conditional** - it only happens if `epoch_history` is `Some`: [3](#0-2) 

5. **Bypassing Verification**: The signature check is bypassed in multiple scenarios:

   **Scenario A**: One-off transaction restore command explicitly passes `None` for epoch_history: [4](#0-3) 

   **Scenario B**: Full restore with `--skip-epoch-endings` flag: [5](#0-4) 

   **Scenario C**: Even when epoch_history exists, if the epoch is beyond the historical range, verification returns `Ok()` with only a warning: [6](#0-5) 

6. **Proof Verification Without Authority Check**: The `AccumulatorRangeProof::verify()` method only validates Merkle tree cryptography: [7](#0-6) 

   It verifies that the transaction info hashes reconstruct to the `expected_root_hash`, but **never verifies that this root hash is authenticated by validator signatures**. The verification assumes the `expected_root_hash` parameter is trustworthy, but when `epoch_history` is absent, this assumption is violated.

**Broken Invariants:**
- **Consensus Safety**: Fake transactions are accepted without validator consensus
- **State Consistency**: Fabricated state transitions bypass cryptographic verification  
- **Cryptographic Correctness**: BLS signature verification is skipped entirely

## Impact Explanation

**Severity: CRITICAL** (per Aptos Bug Bounty criteria)

This vulnerability enables multiple critical security breaches:

1. **Consensus Safety Violation**: An attacker can inject arbitrary transactions that were never agreed upon by validators, fundamentally breaking the consensus protocol's safety guarantee.

2. **State Corruption**: Any node restored from a malicious backup will have a corrupted state tree containing fabricated transactions. This could include:
   - Fake token transfers creating unauthorized balances
   - Fraudulent smart contract executions
   - Manipulated governance proposal outcomes
   - Altered validator set configurations

3. **Chain Fork Potential**: If multiple nodes restore from malicious backups, they will disagree with honest nodes on the canonical chain state, potentially requiring a network-wide hardfork to resolve.

4. **Loss of Funds**: Attackers could craft transactions that transfer assets to their control or mint new tokens, directly causing financial loss.

5. **Trust Destruction**: The ability to inject fake historical transactions undermines trust in the blockchain's immutability guarantee.

The impact qualifies as Critical because it:
- Violates consensus safety (explicitly listed as Critical severity)
- Enables loss of funds (explicitly listed as Critical severity)  
- Could require a hardfork to fix (non-recoverable network partition)

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

The attack is realistic because:

1. **Common Operational Pattern**: Database restoration from backups is a standard operational procedure for:
   - Spinning up new archive nodes
   - Recovering from hardware failures
   - Setting up development/testing environments
   - Migrating nodes to new infrastructure

2. **Attacker Prerequisites Are Achievable**:
   - Archive nodes (non-validators) run backup processes and could be targeted
   - API nodes with database access are common attack surfaces
   - A single compromised node can generate malicious backups that affect many others
   - The `--skip-epoch-endings` flag exists as a convenience option that users might enable

3. **Silent Failure Mode**: The vulnerability operates silentlyâ€”there's no error message when signature verification is skipped, only a warning log for the "epoch too new" case that administrators might overlook.

4. **Widespread Impact From Single Compromise**: One malicious backup can infect multiple nodes during restoration, amplifying the impact of a single database compromise.

The likelihood is not "High" only because:
- Requires initial database-level compromise OR social engineering to provide malicious backup
- Administrators must choose to use the compromised backup for restoration

However, given the operational necessity of backups and the subtlety of the vulnerability, exploitation is reasonably probable in real-world deployments.

## Recommendation

**Mandatory Signature Verification**: Make LedgerInfo signature verification mandatory during all restoration operations.

**Implementation Changes:**

1. **Remove Optional Epoch History**: Change `LoadedChunk::load()` signature to require epoch_history:
   ```rust
   async fn load(
       manifest: TransactionChunk,
       storage: &Arc<dyn BackupStorage>,
       epoch_history: &Arc<EpochHistory>,  // Remove Option<>
   ) -> Result<Self>
   ```

2. **Enforce Verification**: Make the signature check unconditional:
   ```rust
   // In LoadedChunk::load(), replace lines 152-154:
   epoch_history.verify_ledger_info(&ledger_info)?;  // Remove if let Some
   ```

3. **Handle "Epoch Too New" Properly**: In `EpochHistory::verify_ledger_info()`, replace the warning with an error:
   ```rust
   // Replace lines 279-287:
   if epoch > self.epoch_endings.len() as u64 {
       bail!(
           "Cannot verify LedgerInfo for epoch {} - epoch history only extends to epoch {}. \
           Refusing to restore unverified data.",
           epoch,
           self.epoch_endings.len()
       );
   }
   ```

4. **Remove Bypass Options**: Eliminate the ability to skip epoch history:
   - Remove `None` option for `epoch_history` parameter throughout restore code
   - Remove or deprecate `--skip-epoch-endings` flag
   - For one-off restore commands, require users to provide epoch ending backups

5. **Add Explicit Verification Logging**: Log successful signature verification:
   ```rust
   epoch_history.verify_ledger_info(&ledger_info)?;
   info!(
       "LedgerInfo signature verified for epoch {} version {}",
       ledger_info.ledger_info().epoch(),
       ledger_info.ledger_info().version()
   );
   ```

## Proof of Concept

**Conceptual PoC Steps** (cannot fully implement without access to a running node):

```rust
// Step 1: Simulate compromised database with fake data
// On a node with DB write access, inject fake records:

// 1. Create fake Transaction
let fake_txn = Transaction::UserTransaction(/* craft arbitrary signed txn */);

// 2. Create fake TransactionInfo  
let fake_txn_info = TransactionInfo::new(
    fake_txn.hash(),
    HashValue::zero(), // fake state root
    HashValue::zero(), // fake event root  
    None,              // no checkpoint
    0,                 // fake gas used
    ExecutionStatus::Success,
);

// 3. Insert into database
db.transaction_db().put_transaction(version, &fake_txn)?;
db.transaction_info_db().put_transaction_info(version, &fake_txn_info)?;

// 4. Build fake accumulator nodes for the fake txn_info
let fake_accumulator_nodes = build_fake_accumulator(vec![fake_txn_info.hash()]);
for (pos, hash) in fake_accumulator_nodes {
    db.transaction_accumulator_db().put(pos, hash)?;
}

// 5. Insert fake LedgerInfo with invalid signatures
let fake_ledger_info = LedgerInfoWithSignatures::new(
    LedgerInfo::new(/* set accumulator root to match fake tree */),
    AggregateSignature::empty(), // No valid signatures!
);
db.metadata_db().put_ledger_info(&fake_ledger_info)?;

// Step 2: Create backup from compromised DB
let backup_handler = BackupHandler::new(/* ... */);
let (proof, ledger_info) = backup_handler.get_transaction_range_proof(version, version)?;
// proof and ledger_info now contain fake data with invalid signatures

// Step 3: Restore on clean node without epoch_history
// Use one-off restore command:
// $ aptos-db-tool restore oneoff transaction \
//     --target-db-dir /path/to/clean/db \
//     --transaction-manifest /path/to/fake/backup/manifest.json
// 
// This calls TransactionRestoreController::new with epoch_history=None,
// causing signature verification to be skipped (line 106 in restore.rs)

// Step 4: Verify fake transaction is now in clean node's database
let restored_txn = clean_db.get_transaction(version)?;
assert_eq!(restored_txn, fake_txn); // Fake transaction accepted!
```

**Verification Command to Detect Vulnerability:**
```bash
# Create a backup with fake data, then restore with:
aptos-db-tool restore oneoff transaction \
  --target-db-dir /tmp/test_restore \
  --transaction-manifest /path/to/malicious/backup.json

# If restoration succeeds without signature verification errors,
# the vulnerability exists.
```

## Notes

This vulnerability represents a critical gap in the defense-in-depth model. While database compromise is serious, the blockchain's cryptographic guarantees should prevent compromised data from propagating to other nodes. The LedgerInfo signature scheme exists precisely to provide this protection, but it's being bypassed in the restoration path.

The fix is straightforward: make signature verification mandatory and fail loudly when it cannot be performed. This aligns with the principle that **data should never be trusted without cryptographic proof**, even if it comes from the node's own database.

### Citations

**File:** storage/aptosdb/src/backup/backup_handler.rs (L113-137)
```rust
    pub fn get_transaction_range_proof(
        &self,
        first_version: Version,
        last_version: Version,
    ) -> Result<(TransactionAccumulatorRangeProof, LedgerInfoWithSignatures)> {
        ensure!(
            last_version >= first_version,
            "Bad transaction range: [{}, {}]",
            first_version,
            last_version
        );
        let num_transactions = last_version - first_version + 1;
        let ledger_metadata_db = self.ledger_db.metadata_db();
        let epoch = ledger_metadata_db.get_epoch(last_version)?;
        let ledger_info = ledger_metadata_db.get_latest_ledger_info_in_epoch(epoch)?;
        let accumulator_proof = self
            .ledger_db
            .transaction_accumulator_db()
            .get_transaction_range_proof(
                Some(first_version),
                num_transactions,
                ledger_info.ledger_info().version(),
            )?;
        Ok((accumulator_proof, ledger_info))
    }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L147-167)
```rust
        let (range_proof, ledger_info) = storage
            .load_bcs_file::<(TransactionAccumulatorRangeProof, LedgerInfoWithSignatures)>(
                &manifest.proof,
            )
            .await?;
        if let Some(epoch_history) = epoch_history {
            epoch_history.verify_ledger_info(&ledger_info)?;
        }

        // make a `TransactionListWithProof` to reuse its verification code.
        let txn_list_with_proof =
            TransactionListWithProofV2::new(TransactionListWithAuxiliaryInfos::new(
                TransactionListWithProof::new(
                    txns,
                    Some(event_vecs),
                    Some(manifest.first_version),
                    TransactionInfoListWithProof::new(range_proof, txn_infos),
                ),
                persisted_aux_info,
            ));
        txn_list_with_proof.verify(ledger_info.ledger_info(), Some(manifest.first_version))?;
```

**File:** storage/db-tool/src/restore.rs (L102-110)
```rust
                        TransactionRestoreController::new(
                            opt,
                            global.try_into()?,
                            storage.init_storage().await?,
                            None, /* epoch_history */
                            VerifyExecutionMode::NoVerify,
                        )
                        .run()
                        .await?;
```

**File:** storage/backup/backup-cli/src/coordinators/restore.rs (L219-231)
```rust
        let epoch_history = if !self.skip_epoch_endings {
            Some(Arc::new(
                EpochHistoryRestoreController::new(
                    epoch_handles,
                    self.global_opt.clone(),
                    self.storage.clone(),
                )
                .run()
                .await?,
            ))
        } else {
            None
        };
```

**File:** storage/backup/backup-cli/src/backup_types/epoch_ending/restore.rs (L279-287)
```rust
        if epoch > self.epoch_endings.len() as u64 {
            // TODO(aldenhu): fix this from upper level
            warn!(
                epoch = epoch,
                epoch_history_until = self.epoch_endings.len(),
                "Epoch is too new and can't be verified. Previous chunks are verified and node \
                won't be able to start if this data is malicious."
            );
            return Ok(());
```

**File:** types/src/proof/definition.rs (L619-714)
```rust
        &self,
        expected_root_hash: HashValue,
        first_leaf_index: Option<u64>,
        leaf_hashes: &[HashValue],
    ) -> Result<()> {
        if first_leaf_index.is_none() {
            ensure!(
                leaf_hashes.is_empty(),
                "first_leaf_index indicated empty list while leaf_hashes is not empty.",
            );
            ensure!(
                self.left_siblings.is_empty() && self.right_siblings.is_empty(),
                "No siblings are needed.",
            );
            return Ok(());
        }

        ensure!(
            self.left_siblings.len() <= MAX_ACCUMULATOR_PROOF_DEPTH,
            "Proof has more than {} ({}) left siblings.",
            MAX_ACCUMULATOR_PROOF_DEPTH,
            self.left_siblings.len(),
        );
        ensure!(
            self.right_siblings.len() <= MAX_ACCUMULATOR_PROOF_DEPTH,
            "Proof has more than {} ({}) right siblings.",
            MAX_ACCUMULATOR_PROOF_DEPTH,
            self.right_siblings.len(),
        );
        ensure!(
            !leaf_hashes.is_empty(),
            "leaf_hashes is empty while first_leaf_index indicated non-empty list.",
        );

        let mut left_sibling_iter = self.left_siblings.iter().peekable();
        let mut right_sibling_iter = self.right_siblings.iter().peekable();

        let mut first_pos = Position::from_leaf_index(
            first_leaf_index.expect("first_leaf_index should not be None."),
        );
        let mut current_hashes = leaf_hashes.to_vec();
        let mut parent_hashes = vec![];

        // Keep reducing the list of hashes by combining all the children pairs, until there is
        // only one hash left.
        while current_hashes.len() > 1
            || left_sibling_iter.peek().is_some()
            || right_sibling_iter.peek().is_some()
        {
            let mut children_iter = current_hashes.iter();

            // If the first position on the current level is a right child, it needs to be combined
            // with a sibling on the left.
            if first_pos.is_right_child() {
                let left_hash = *left_sibling_iter.next().ok_or_else(|| {
                    format_err!("First child is a right child, but missing sibling on the left.")
                })?;
                let right_hash = *children_iter.next().expect("The first leaf must exist.");
                parent_hashes.push(MerkleTreeInternalNode::<H>::new(left_hash, right_hash).hash());
            }

            // Next we take two children at a time and compute their parents.
            let mut children_iter = children_iter.as_slice().chunks_exact(2);
            for chunk in children_iter.by_ref() {
                let left_hash = chunk[0];
                let right_hash = chunk[1];
                parent_hashes.push(MerkleTreeInternalNode::<H>::new(left_hash, right_hash).hash());
            }

            // Similarly, if the last position is a left child, it needs to be combined with a
            // sibling on the right.
            let remainder = children_iter.remainder();
            assert!(remainder.len() <= 1);
            if !remainder.is_empty() {
                let left_hash = remainder[0];
                let right_hash = *right_sibling_iter.next().ok_or_else(|| {
                    format_err!("Last child is a left child, but missing sibling on the right.")
                })?;
                parent_hashes.push(MerkleTreeInternalNode::<H>::new(left_hash, right_hash).hash());
            }

            first_pos = first_pos.parent();
            current_hashes.clear();
            std::mem::swap(&mut current_hashes, &mut parent_hashes);
        }

        ensure!(
            current_hashes[0] == expected_root_hash,
            "{}: Root hashes do not match. Actual root hash: {:x}. Expected root hash: {:x}.",
            type_name::<Self>(),
            current_hashes[0],
            expected_root_hash,
        );

        Ok(())
    }
```
