# Audit Report

## Title
DKG Transcript Decompression Bomb Vulnerability During Deserialization

## Summary
A decompression bomb vulnerability exists in the DKG (Distributed Key Generation) transcript processing where maliciously crafted transcripts cause excessive CPU consumption during BCS deserialization before size validation occurs, enabling validator node slowdowns or DoS attacks by Byzantine validators.

## Finding Description

The vulnerability exists in the DKG transcript processing flow where BCS deserialization of elliptic curve points happens BEFORE transcript size validation. The attack exploits multiple deserialization points in the validator transaction processing pipeline.

**First Deserialization Point**: When a `ValidatorTransaction::DKGResult` is validated during consensus, the `DKGTranscript::verify()` method deserializes the transcript bytes without any prior size checking: [1](#0-0) 

**Second Deserialization Point**: During VM execution, `process_dkg_result_inner()` deserializes the transcript again: [2](#0-1) 

**Critical Issue**: The code documentation explicitly confirms that point decompression and validation occur during deserialization: [3](#0-2) 

The `Transcript` struct contains vectors of G1 and G2 elliptic curve points that undergo expensive decompression operations (including square root computations in finite fields and subgroup membership checks) during BCS deserialization: [4](#0-3) 

**Size Validation Occurs Too Late**: The `check_sizes()` method only executes AFTER deserialization completes, inside the `verify()` method: [5](#0-4) [6](#0-5) 

**ValidatorTransaction Size Limit**: The system allows validator transactions up to 2MB in total bytes: [7](#0-6) 

**Attack Path**:
1. Byzantine validator crafts a DKG transcript with excessive elliptic curve points (e.g., ~21,845 G2 points â‰ˆ 1.9MB within the 2MB limit)
2. Validator submits `ValidatorTransaction::DKGResult` through consensus
3. During consensus validation, `vtxn.verify()` is called which triggers deserialization: [8](#0-7) 

4. BCS deserialization decompresses all points (expensive CPU operations)
5. Size validation in `check_sizes()` detects the mismatch and rejects the transcript
6. The computational work is already done, causing validator slowdown
7. Size check occurs only after verification: [9](#0-8) 

## Impact Explanation

This is a **High Severity** vulnerability per the Aptos bug bounty criteria as it causes "Validator node slowdowns" through resource exhaustion at the protocol level.

A Byzantine validator (< 1/3 of stake, within the threat model) can craft transcripts that:
- Stay within the 2MB validator transaction size limit
- Contain ~21,845 G2 points (96 bytes compressed each) or ~43,690 G1 points (48 bytes compressed each)
- Require seconds of CPU time to decompress (each G2 point decompression involves modular square root computations in the Fp2 finite field)
- Are ultimately rejected after expensive processing

Multiple such transactions can significantly degrade validator performance during critical DKG epochs, affecting consensus liveness and epoch transitions. Unlike regular transaction gas limits, there is no cost accounting or rate limiting for failed validator transaction processing.

## Likelihood Explanation

**Medium to High likelihood**:
- **Attacker Profile**: Requires a Byzantine validator, which is explicitly within the Aptos threat model (< 1/3 Byzantine tolerance)
- **Execution Complexity**: Simple - craft a serialized transcript with excessive points and submit through validator transaction pool
- **Preconditions**: Normal network operation during DKG epoch (predictable, occurs at every epoch transition)
- **Cost**: No cost for failed validation - the expensive work is done before rejection
- **Rate Limiting**: No evidence of rate limiting on failed validator transaction processing
- **Repeatability**: A single Byzantine validator can repeatedly submit malicious transcripts throughout the DKG phase

## Recommendation

Implement size validation BEFORE deserialization by checking the raw byte length of vector fields:

1. **Add pre-deserialization size checks**: Before calling `bcs::from_bytes()`, validate that the serialized data doesn't contain suspiciously large vector lengths by inspecting the BCS encoding structure

2. **Implement early bounds checking**: Add a lightweight check in `DKGTranscript::verify()` that validates the transcript byte length against expected bounds before full deserialization

3. **Add rate limiting**: Implement rate limiting or cost accounting for failed validator transaction processing to prevent repeated attacks

4. **Use bounded deserialization**: Consider using `bcs::from_bytes_with_limit()` with appropriate depth and size limits

Example fix structure:
```rust
// In types/src/dkg/mod.rs, before deserialization:
pub(crate) fn verify(&self, verifier: &ValidatorVerifier) -> Result<()> {
    // Add early size check before deserialization
    ensure!(
        self.transcript_bytes.len() <= MAX_REASONABLE_TRANSCRIPT_SIZE,
        "Transcript bytes exceed reasonable size"
    );
    
    let transcripts: Transcripts = bcs::from_bytes(&self.transcript_bytes)
        .context("Transcripts deserialization failed")?;
    RealDKG::verify_transcript_extra(&transcripts, verifier, true, None)
}
```

## Proof of Concept

A Byzantine validator can trigger this vulnerability by:

1. Creating a `Transcript` with vectors containing excessive points (staying under 2MB)
2. Serializing it using BCS
3. Submitting as `ValidatorTransaction::DKGResult(DKGTranscript { ... })`
4. All validators processing this transaction will deserialize and decompress all points before `check_sizes()` rejects it

The expensive work occurs in the `blstrs` library's `GroupEncoding::from_bytes()` implementation called during BCS deserialization, performing field arithmetic operations for each point before size validation can reject the transcript.

## Notes

This vulnerability is distinct from network-layer DoS attacks (which are out of scope). It exploits a protocol-level resource exhaustion issue where validation ordering allows expensive cryptographic operations to occur before lightweight size checks. The threat model explicitly considers Byzantine validators as untrusted actors capable of submitting malicious transactions, making this a valid security concern within the Aptos consensus layer.

### Citations

**File:** types/src/dkg/mod.rs (L83-87)
```rust
    pub(crate) fn verify(&self, verifier: &ValidatorVerifier) -> Result<()> {
        let transcripts: Transcripts = bcs::from_bytes(&self.transcript_bytes)
            .context("Transcripts deserialization failed")?;
        RealDKG::verify_transcript_extra(&transcripts, verifier, true, None)
    }
```

**File:** aptos-move/aptos-vm/src/validator_txns/dkg.rs (L106-109)
```rust
        let transcript = bcs::from_bytes::<<DefaultDKG as DKGTrait>::Transcript>(
            dkg_node.transcript_bytes.as_slice(),
        )
        .map_err(|_| Expected(TranscriptDeserializationFailed))?;
```

**File:** crates/aptos-dkg/src/pvss/das/weighted_protocol.rs (L48-72)
```rust
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq, Eq, BCSCryptoHash, CryptoHasher)]
#[allow(non_snake_case)]
pub struct Transcript {
    /// Proofs-of-knowledge (PoKs) for the dealt secret committed in $c = g_2^{p(0)}$.
    /// Since the transcript could have been aggregated from other transcripts with their own
    /// committed secrets in $c_i = g_2^{p_i(0)}$, this is a vector of PoKs for all these $c_i$'s
    /// such that $\prod_i c_i = c$.
    ///
    /// Also contains BLS signatures from each player $i$ on that player's contribution $c_i$, the
    /// player ID $i$ and auxiliary information `aux[i]` provided during dealing.
    soks: Vec<SoK<G1Projective>>,
    /// Commitment to encryption randomness $g_1^{r_j} \in G_1, \forall j \in [W]$
    R: Vec<G1Projective>,
    /// Same as $R$ except uses $g_2$.
    R_hat: Vec<G2Projective>,
    /// First $W$ elements are commitments to the evaluations of $p(X)$: $g_1^{p(\omega^i)}$,
    /// where $i \in [W]$. Last element is $g_1^{p(0)}$ (i.e., the dealt public key).
    V: Vec<G1Projective>,
    /// Same as $V$ except uses $g_2$.
    V_hat: Vec<G2Projective>,
    /// ElGamal encryption of the $j$th share of player $i$:
    /// i.e., $C[s_i+j-1] = h_1^{p(\omega^{s_i + j - 1})} ek_i^{r_j}, \forall i \in [n], j \in [w_i]$.
    /// We sometimes denote $C[s_i+j-1]$ by C_{i, j}.
    C: Vec<G1Projective>,
}
```

**File:** crates/aptos-dkg/src/pvss/das/weighted_protocol.rs (L85-89)
```rust
    fn try_from(bytes: &[u8]) -> Result<Self, Self::Error> {
        // NOTE: The `serde` implementation in `blstrs` already performs the necessary point validation
        // by ultimately calling `GroupEncoding::from_bytes`.
        bcs::from_bytes::<Transcript>(bytes).map_err(|_| CryptoMaterialError::DeserializationError)
    }
```

**File:** crates/aptos-dkg/src/pvss/das/weighted_protocol.rs (L288-289)
```rust
        self.check_sizes(sc)?;
        let n = sc.get_total_num_players();
```

**File:** crates/aptos-dkg/src/pvss/das/weighted_protocol.rs (L415-455)
```rust
    fn check_sizes(&self, sc: &WeightedConfigBlstrs) -> anyhow::Result<()> {
        let W = sc.get_total_weight();

        if self.V.len() != W + 1 {
            bail!(
                "Expected {} G_2 (polynomial) commitment elements, but got {}",
                W + 1,
                self.V.len()
            );
        }

        if self.V_hat.len() != W + 1 {
            bail!(
                "Expected {} G_2 (polynomial) commitment elements, but got {}",
                W + 1,
                self.V_hat.len()
            );
        }

        if self.R.len() != W {
            bail!(
                "Expected {} G_1 commitment(s) to ElGamal randomness, but got {}",
                W,
                self.R.len()
            );
        }

        if self.R_hat.len() != W {
            bail!(
                "Expected {} G_2 commitment(s) to ElGamal randomness, but got {}",
                W,
                self.R_hat.len()
            );
        }

        if self.C.len() != W {
            bail!("Expected C of length {}, but got {}", W, self.C.len());
        }

        Ok(())
    }
```

**File:** types/src/on_chain_config/consensus_config.rs (L125-126)
```rust
const VTXN_CONFIG_PER_BLOCK_LIMIT_TXN_COUNT_DEFAULT: u64 = 2;
const VTXN_CONFIG_PER_BLOCK_LIMIT_TOTAL_BYTES_DEFAULT: u64 = 2097152; //2MB
```

**File:** consensus/src/dag/rb_handler.rs (L129-130)
```rust
            vtxn.verify(self.epoch_state.verifier.as_ref())
                .context(format!("{} verification failed", vtxn_type_name))?;
```

**File:** consensus/src/dag/rb_handler.rs (L132-137)
```rust
        let vtxn_total_bytes = node
            .validator_txns()
            .iter()
            .map(ValidatorTransaction::size_in_bytes)
            .sum::<usize>() as u64;
        ensure!(vtxn_total_bytes <= self.vtxn_config.per_block_limit_total_bytes());
```
