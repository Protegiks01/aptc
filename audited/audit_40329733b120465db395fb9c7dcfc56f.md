# Audit Report

## Title
State Sync Metadata Checkpoint Creation Lacks Integrity Verification Leading to Node Availability Failure

## Summary
The `create_checkpoint()` function in `PersistentMetadataStorage` creates database checkpoints without performing any integrity verification. This allows corrupted or incomplete checkpoints to be created undetected. When nodes attempt to open corrupted checkpoints, they panic and fail to start, causing denial of service.

## Finding Description

The `create_checkpoint()` function creates a physical RocksDB checkpoint but performs **zero integrity verification** after creation. [1](#0-0) 

The function simply:
1. Removes any existing checkpoint directory
2. Calls the underlying RocksDB checkpoint creation
3. Logs success
4. Returns Ok()

There is no verification that:
- All required files were successfully copied to the checkpoint
- The checkpoint can be successfully opened
- Critical metadata is readable from the checkpoint

The underlying RocksDB checkpoint implementation also provides no post-creation validation: [2](#0-1) 

**Failure Scenarios:**

Checkpoint corruption can occur through:
1. **Process interruption**: SIGKILL during checkpoint creation leaves incomplete file sets
2. **Disk I/O errors**: Failed writes during file copying
3. **Filesystem errors**: Out of disk space during checkpoint creation
4. **Post-creation corruption**: Bit rot or hardware failures after successful creation

When a node attempts to use a corrupted checkpoint, the `PersistentMetadataStorage::new()` constructor opens the database: [3](#0-2) 

If the database is corrupted and RocksDB's `open()` call fails, the code **panics**, immediately terminating the node process.

**Current Usage Context:**

The checkpoint functionality is currently invoked in node initialization: [4](#0-3) 

This is part of `create_rocksdb_checkpoint_and_change_working_dir`, which is marked as test-only: [5](#0-4) 

However, the vulnerability exists in the production checkpoint implementation itself, not just test code.

## Impact Explanation

**Severity: High** (meets "Validator node slowdowns" and "API crashes" criteria)

While currently limited to test scenarios, the impact if expanded to production use would be:

1. **Node Availability Failure**: Nodes attempting to restore from corrupted checkpoints panic and cannot start
2. **Cascading Failures**: If corrupted checkpoints are distributed (e.g., via backup systems), multiple nodes could fail simultaneously
3. **Recovery Complexity**: Operators must manually identify and remove corrupted checkpoints
4. **State Sync Disruption**: The metadata storage tracks snapshot sync progress; corruption prevents nodes from properly resuming state synchronization after restarts

The panic behavior guarantees node failure rather than graceful degradation, maximizing availability impact.

## Likelihood Explanation

**Current Likelihood: Low** (test-only usage)
**Future Likelihood: Medium** (if used in production)

Checkpoint corruption can occur through:
- Environmental failures (disk errors, out of space) - naturally occurring
- Process management issues (restart during checkpoint) - operationally common
- Hardware degradation - increases over time

The lack of integrity verification means **any** corruption goes undetected until restoration, when it causes catastrophic failure.

## Recommendation

Implement post-creation checkpoint integrity verification:

```rust
pub fn create_checkpoint<P: AsRef<Path>>(&self, path: P) -> Result<()> {
    let start = Instant::now();
    let state_sync_db_path = path.as_ref().join(STATE_SYNC_DB_NAME);
    std::fs::remove_dir_all(&state_sync_db_path).unwrap_or(());
    
    // Create the checkpoint
    self.database.create_checkpoint(&state_sync_db_path)?;
    
    // ADDED: Verify checkpoint integrity by attempting to open it
    let verify_result = DB::open(
        state_sync_db_path.clone(),
        "state_sync_verify",
        vec![METADATA_CF_NAME],
        &Options::default(),
    );
    
    if let Err(e) = verify_result {
        // Checkpoint verification failed - clean up and return error
        std::fs::remove_dir_all(&state_sync_db_path).ok();
        return Err(anyhow::anyhow!(
            "Checkpoint verification failed: {:?}", e
        ));
    }
    
    // ADDED: Verify critical data is readable (optional but recommended)
    // Try to read snapshot progress metadata to ensure data integrity
    // [Implementation of basic read tests]
    
    info!(
        path = state_sync_db_path,
        time_ms = %start.elapsed().as_millis(),
        "Made StateSyncDB checkpoint and verified integrity."
    );
    Ok(())
}
```

Additionally:
1. Add checksum validation for checkpoint files
2. Implement graceful degradation instead of panic on corruption detection
3. Add automated cleanup of failed checkpoints
4. Log checkpoint creation failures for monitoring

## Proof of Concept

```rust
#[test]
fn test_corrupted_checkpoint_detection() {
    use tempfile::TempDir;
    use std::fs;
    
    // Create a metadata storage with data
    let source_dir = TempDir::new().unwrap();
    let storage = PersistentMetadataStorage::new(source_dir.path());
    let target = create_ledger_info_at_version(100);
    storage.update_last_persisted_state_value_index(&target, 50, false).unwrap();
    
    // Create checkpoint
    let checkpoint_dir = TempDir::new().unwrap();
    storage.create_checkpoint(checkpoint_dir.path()).unwrap();
    
    // Simulate corruption: delete critical database files
    let checkpoint_db_path = checkpoint_dir.path().join("state_sync_db");
    if let Ok(entries) = fs::read_dir(&checkpoint_db_path) {
        for entry in entries.take(2) { // Delete first 2 files
            if let Ok(entry) = entry {
                fs::remove_file(entry.path()).ok();
            }
        }
    }
    
    // Attempt to open corrupted checkpoint - this should panic
    // demonstrating the vulnerability
    let result = std::panic::catch_unwind(|| {
        PersistentMetadataStorage::new(checkpoint_dir.path())
    });
    
    // Without integrity verification, corrupted checkpoint is not detected
    // during creation, only during opening (which panics)
    assert!(result.is_err(), "Opening corrupted checkpoint should fail");
}
```

**Notes:**

This vulnerability demonstrates a violation of the **State Consistency** invariant: "State transitions must be atomic and verifiable." Checkpoints are not verified after creation, breaking the verifiability requirement. While currently limited to test scenarios, the fundamental design flaw exists in production code and could cause High severity availability issues if the feature were expanded to production use cases such as disaster recovery, node bootstrap, or backup/restore operations.

### Citations

**File:** state-sync/state-sync-driver/src/metadata_storage.rs (L73-86)
```rust
        let state_sync_db_path = db_root_path.as_ref().join(STATE_SYNC_DB_NAME);
        let instant = Instant::now();
        let database = DB::open(
            state_sync_db_path.clone(),
            "state_sync",
            vec![METADATA_CF_NAME],
            &options,
        )
        .unwrap_or_else(|error| {
            panic!(
                "Failed to open/create the state sync database at: {:?}. Error: {:?}",
                state_sync_db_path, error
            )
        });
```

**File:** state-sync/state-sync-driver/src/metadata_storage.rs (L167-178)
```rust
    pub fn create_checkpoint<P: AsRef<Path>>(&self, path: P) -> Result<()> {
        let start = Instant::now();
        let state_sync_db_path = path.as_ref().join(STATE_SYNC_DB_NAME);
        std::fs::remove_dir_all(&state_sync_db_path).unwrap_or(());
        self.database.create_checkpoint(&state_sync_db_path)?;
        info!(
            path = state_sync_db_path,
            time_ms = %start.elapsed().as_millis(),
            "Made StateSyncDB checkpoint."
        );
        Ok(())
    }
```

**File:** storage/schemadb/src/lib.rs (L356-362)
```rust
    pub fn create_checkpoint<P: AsRef<Path>>(&self, path: P) -> DbResult<()> {
        rocksdb::checkpoint::Checkpoint::new(&self.inner)
            .into_db_res()?
            .create_checkpoint(path)
            .into_db_res()?;
        Ok(())
    }
```

**File:** aptos-node/src/storage.rs (L162-166)
```rust
    let state_sync_db =
        aptos_state_sync_driver::metadata_storage::PersistentMetadataStorage::new(&source_dir);
    state_sync_db
        .create_checkpoint(&checkpoint_dir)
        .expect("StateSyncDB checkpoint creation failed.");
```

**File:** aptos-node/src/storage.rs (L181-185)
```rust
    // If required, create RocksDB checkpoints and change the working directory.
    // This is test-only.
    if let Some(working_dir) = node_config.base.working_dir.clone() {
        create_rocksdb_checkpoint_and_change_working_dir(node_config, working_dir);
    }
```
