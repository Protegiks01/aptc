# Audit Report

## Title
Non-Atomic Database Operations in `finalize_state_snapshot` Can Lead to Unrecoverable Database Corruption

## Summary
The `finalize_state_snapshot` function executes multiple database write operations non-atomically, creating a window where a process crash can leave the database in an inconsistent state. If the automatic recovery mechanism fails, the node's database becomes permanently corrupted, requiring manual intervention or a hardfork to recover.

## Finding Description

The `finalize_state_snapshot` function is responsible for finalizing a state snapshot after all state values have been synced. It performs several critical database operations that should execute atomically but do not. [1](#0-0) 

The function first commits frozen subtrees to the transaction accumulator database immediately by passing `None` to `confirm_or_save_frozen_subtrees`, which causes it to create its own batch and commit synchronously. [2](#0-1) 

Then, much later, the main ledger data (transactions, transaction infos, events, and progress markers) is committed in a separate transaction. [3](#0-2) 

**Attack Scenario:**

1. Node is performing state sync and calls `finalize_state_snapshot`
2. Frozen subtrees for version V are committed to transaction_accumulator_db (line 155-160)
3. **Process crashes here** (crash, power failure, or OOM kill)
4. Main ledger data never commits
5. `OverallCommitProgress` remains at version V-1
6. Database now has frozen subtrees for V but no corresponding transaction data

**Recovery Attempt:**

The system has a recovery mechanism that runs on startup: [4](#0-3) 

This calls `truncate_ledger_db` to remove data beyond `OverallCommitProgress`: [5](#0-4) 

**Vulnerability:** If the truncation process fails (due to disk errors, file system corruption, insufficient disk space, or other I/O failures), the database remains in an inconsistent state:
- Frozen subtrees for version V exist
- No transaction data for version V
- `OverallCommitProgress` indicates version V-1
- Node cannot proceed forward (missing transaction data) or backward (inconsistent accumulator)

This breaks the **State Consistency** invariant that "state transitions must be atomic and verifiable via Merkle proofs."

## Impact Explanation

This vulnerability qualifies as **Critical Severity** under the Aptos bug bounty program for the following reasons:

1. **Non-recoverable network partition (requires hardfork)**: If truncation fails and the database becomes corrupted, the node cannot automatically recover. The validator operator would need to either:
   - Manually repair the database (complex and error-prone)
   - Re-sync from genesis (extremely time-consuming)
   - Restore from a backup (if available)
   - In worst case, require a coordinated hardfork if multiple validators are affected

2. **Total loss of liveness**: An affected validator cannot participate in consensus until the database is repaired, reducing network capacity and potentially threatening liveness if enough validators are affected simultaneously.

3. **State inconsistency requiring intervention**: The database is in a state where the transaction accumulator has data for a version that doesn't exist according to the commit progress markers, violating fundamental database consistency guarantees.

The TODO comment at line 147-148 explicitly acknowledges this should be fixed, confirming developers are aware of the atomicity issue. [6](#0-5) 

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability requires two conditions:
1. **Process crash during state sync**: This can occur naturally (power failure, OOM, hardware failure) or be induced. State sync operations can take significant time, increasing the crash window.
2. **Truncation failure on recovery**: While database operations are generally reliable, failures can occur due to:
   - Disk full conditions (the deleted data needs temporary space)
   - File system corruption
   - I/O errors on failing hardware
   - Bugs in the truncation logic itself

The vulnerability is particularly concerning because:
- State sync is a frequent operation for new or recovering nodes
- The non-atomic window exists for every state sync operation
- Multiple nodes syncing simultaneously (e.g., after a network partition) increases the probability
- An attacker could potentially trigger crashes (resource exhaustion, targeted DoS) during known state sync periods

## Recommendation

**Fix: Make all operations in `finalize_state_snapshot` atomic**

The frozen subtree commit should be included in the same atomic batch as the ledger data commit. Modify the code as follows:

1. Create the ledger_db_batch BEFORE calling `confirm_or_save_frozen_subtrees`
2. Pass the batch to `confirm_or_save_frozen_subtrees` instead of `None`
3. Commit everything atomically in a single write operation

Pseudo-code for the fix:

```rust
// Create a single change set for ALL write operations
let mut ledger_db_batch = LedgerDbSchemaBatches::new();
let mut sharded_kv_batch = self.state_kv_db.new_sharded_native_batches();
let mut state_kv_metadata_batch = SchemaBatch::new();

// Update the merkle accumulator using the given proof - ADD TO BATCH
let frozen_subtrees = output_with_proof
    .proof
    .ledger_info_to_transaction_infos_proof
    .left_siblings();
restore_utils::confirm_or_save_frozen_subtrees(
    self.ledger_db.transaction_accumulator_db_raw(),
    version,
    frozen_subtrees,
    Some(&mut ledger_db_batch.transaction_accumulator_db_batches), // Changed from None
)?;

// ... rest of the operations ...

// Commit everything atomically in ONE write
self.ledger_db.write_schemas(ledger_db_batch)?;
```

The pruner progress updates (lines 225-234) should also ideally be included in the atomic batch or made idempotent to handle partial updates gracefully.

## Proof of Concept

**Reproduction Steps:**

1. Set up a test node performing state sync
2. Modify the code to inject a controlled crash after line 160 (after frozen subtrees commit)
3. Restart the node
4. Simulate truncation failure by:
   - Setting disk to read-only mode, OR
   - Injecting an error in `truncate_transaction_accumulator`, OR
   - Filling disk to prevent truncation writes
5. Observe that:
   - `get_synced_version()` returns V-1
   - Transaction accumulator has frozen subtrees for V
   - `get_transaction_info(V)` fails (no data)
   - Node cannot proceed with consensus or state sync

**Validation Script (Rust test outline):**

```rust
#[test]
fn test_finalize_snapshot_crash_recovery_failure() {
    // 1. Setup: Create DB with state at version 100
    let db = setup_test_db_at_version(100);
    
    // 2. Start state sync to version 101
    let snapshot_receiver = db.get_state_snapshot_receiver(101, expected_root);
    // ... receive all state values ...
    snapshot_receiver.finish();
    
    // 3. Call finalize_state_snapshot but crash after frozen subtrees
    // (inject panic or use test framework to simulate crash)
    inject_crash_after_frozen_subtrees();
    db.finalize_state_snapshot(101, output_with_proof, ledger_infos);
    
    // 4. Simulate restart with truncation failure
    let recovered_db = reopen_db_with_failed_truncation();
    
    // 5. Verify inconsistent state
    assert_eq!(recovered_db.get_synced_version(), Some(100)); // Still at old version
    assert!(has_frozen_subtrees_for_version(&recovered_db, 101)); // But has new frozen subtrees
    assert!(recovered_db.get_transaction_info(101).is_err()); // No transaction data
    
    // 6. Verify node cannot proceed
    assert!(recovered_db.save_transactions(...).is_err()); // Cannot commit new data
}
```

This test would demonstrate that the database can be left in an unrecoverable state where automatic recovery mechanisms fail, requiring manual intervention.

### Citations

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L147-160)
```rust
            // TODO(joshlind): include confirm_or_save_frozen_subtrees in the change set
            // bundle below.

            // Update the merkle accumulator using the given proof
            let frozen_subtrees = output_with_proof
                .proof
                .ledger_info_to_transaction_infos_proof
                .left_siblings();
            restore_utils::confirm_or_save_frozen_subtrees(
                self.ledger_db.transaction_accumulator_db_raw(),
                version,
                frozen_subtrees,
                None,
            )?;
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L207-223)
```rust
            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::LedgerCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;
            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::OverallCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;

            // Apply the change set writes to the database (atomically) and update in-memory state
            //
            // state kv and SMT should use shared way of committing.
            self.ledger_db.write_schemas(ledger_db_batch)?;
```

**File:** storage/aptosdb/src/backup/restore_utils.rs (L92-108)
```rust
    if let Some(existing_batch) = existing_batch {
        confirm_or_save_frozen_subtrees_impl(
            transaction_accumulator_db,
            frozen_subtrees,
            positions,
            existing_batch,
        )?;
    } else {
        let mut batch = SchemaBatch::new();
        confirm_or_save_frozen_subtrees_impl(
            transaction_accumulator_db,
            frozen_subtrees,
            positions,
            &mut batch,
        )?;
        transaction_accumulator_db.write_schemas(batch)?;
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L410-450)
```rust
    pub fn sync_commit_progress(
        ledger_db: Arc<LedgerDb>,
        state_kv_db: Arc<StateKvDb>,
        state_merkle_db: Arc<StateMerkleDb>,
        crash_if_difference_is_too_large: bool,
    ) {
        let ledger_metadata_db = ledger_db.metadata_db();
        if let Some(overall_commit_progress) = ledger_metadata_db
            .get_synced_version()
            .expect("DB read failed.")
        {
            info!(
                overall_commit_progress = overall_commit_progress,
                "Start syncing databases..."
            );
            let ledger_commit_progress = ledger_metadata_db
                .get_ledger_commit_progress()
                .expect("Failed to read ledger commit progress.");
            assert_ge!(ledger_commit_progress, overall_commit_progress);

            let state_kv_commit_progress = state_kv_db
                .metadata_db()
                .get::<DbMetadataSchema>(&DbMetadataKey::StateKvCommitProgress)
                .expect("Failed to read state K/V commit progress.")
                .expect("State K/V commit progress cannot be None.")
                .expect_version();
            assert_ge!(state_kv_commit_progress, overall_commit_progress);

            // LedgerCommitProgress was not guaranteed to commit after all ledger changes finish,
            // have to attempt truncating every column family.
            info!(
                ledger_commit_progress = ledger_commit_progress,
                "Attempt ledger truncation...",
            );
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");

```

**File:** storage/aptosdb/src/utils/truncation_helper.rs (L299-323)
```rust
fn truncate_transaction_accumulator(
    transaction_accumulator_db: &DB,
    start_version: Version,
    batch: &mut SchemaBatch,
) -> Result<()> {
    let mut iter = transaction_accumulator_db.iter::<TransactionAccumulatorSchema>()?;
    iter.seek_to_last();
    let (position, _) = iter.next().transpose()?.unwrap();
    let num_frozen_nodes = position.to_postorder_index() + 1;
    let num_frozen_nodes_after = num_frozen_nodes_in_accumulator(start_version);
    let mut num_nodes_to_delete = num_frozen_nodes - num_frozen_nodes_after;

    let start_position = Position::from_postorder_index(num_frozen_nodes_after)?;
    iter.seek(&start_position)?;

    for item in iter {
        let (position, _) = item?;
        batch.delete::<TransactionAccumulatorSchema>(&position)?;
        num_nodes_to_delete -= 1;
    }

    assert_eq!(num_nodes_to_delete, 0);

    Ok(())
}
```
