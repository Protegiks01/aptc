# Audit Report

## Title
Non-Atomic File Writes in Backup Creation Allow Partial File Corruption

## Summary
The `create_for_write()` function in the LocalFs backup storage backend creates files directly at their final destination without atomic write guarantees or durability synchronization. Power failures or system crashes during backup operations can leave partial, corrupted files that will cause backup restoration to fail, potentially rendering disaster recovery impossible.

## Finding Description

The `create_for_write()` function creates backup files directly at their final path using a non-atomic write pattern: [1](#0-0) 

This implementation has three critical durability flaws:

**1. No Atomic Write Pattern**: The function opens files directly at the final destination rather than using the standard write-to-temporary-then-rename pattern. This means partial writes are immediately visible.

**2. No fsync/sync Guarantees**: The entire backup CLI codebase contains zero calls to `fsync`, `sync_all`, or `sync_data`. The `shutdown()` method from `AsyncWriteExt` only closes the file descriptor—it does NOT flush OS buffers to disk.

**3. Multi-File Backup Consistency**: During backup creation, multiple interdependent files are written sequentially (chunks, proofs, manifests). A crash mid-backup leaves an inconsistent set of files: [2](#0-1) [3](#0-2) 

The backup manifest references chunk files by path only—no checksums, sizes, or integrity metadata: [4](#0-3) 

**Attack Scenario (Disaster Recovery Failure)**:

1. Validator operator initiates state snapshot backup
2. Backup process writes chunk files sequentially (lines 421-424 in backup.rs)
3. Power failure occurs after chunk 50 is partially written
4. System restarts, backup process completes manifest file
5. Manifest references all chunks including the corrupted chunk 50
6. Weeks later, disaster strikes and restore is needed
7. Restore process attempts to load chunk 50: [5](#0-4) 

8. BCS deserialization fails or loads corrupted data
9. **Disaster recovery fails—blockchain state cannot be restored**

Notably, the codebase demonstrates awareness of atomic file operations. The metadata cache download explicitly uses temp-file-then-rename: [6](#0-5) 

But this pattern is **not** applied to the primary backup data path in `create_for_write()`.

## Impact Explanation

This qualifies as **Medium Severity** under the Aptos bug bounty criteria for the following reasons:

1. **State Inconsistencies Requiring Intervention**: Corrupted backups represent critical state inconsistencies in the disaster recovery infrastructure. While not affecting live blockchain state, they compromise the ability to recover from catastrophic failures.

2. **Operational Impact**: In a true disaster scenario (database corruption, hardware failure, catastrophic consensus failure), discovering that backups are corrupted eliminates the primary recovery mechanism. This could lead to permanent data loss and network downtime.

3. **Scope**: The LocalFs backend is used for testing and potentially production local backups. While cloud storage via CommandAdapter may have its own durability guarantees, the core backup API lacks durability requirements.

This does NOT reach High severity because:
- It doesn't affect live validator operation
- It doesn't cause immediate consensus violations
- It requires environmental failure (power loss/crash), not exploitation

This does NOT reach Critical severity because:
- No direct funds loss or theft
- No live consensus safety violation
- Recovery may be possible from other nodes' backups

## Likelihood Explanation

**Likelihood: Medium to High**

1. **Trigger Conditions**: Power failures and system crashes are common operational realities, especially for:
   - Hardware failures in data centers
   - Kernel panics or OOM kills
   - Infrastructure maintenance windows
   - Cloud provider outages

2. **Time Window**: State snapshot backups can take hours for large databases, creating extended vulnerability windows.

3. **Silent Failure**: Backup corruption is only discovered during restore attempts, potentially weeks or months after creation.

4. **Production Impact**: Multiple Aptos validators run continuous backup processes. Statistical likelihood of encountering this issue increases with fleet size and operational time.

## Recommendation

Implement atomic writes with durability guarantees in `create_for_write()`:

```rust
async fn create_for_write(
    &self,
    backup_handle: &BackupHandleRef,
    name: &ShellSafeName,
) -> Result<(FileHandle, Box<dyn AsyncWrite + Send + Unpin>)> {
    let file_handle = Path::new(backup_handle)
        .join(name.as_ref())
        .path_to_string()?;
    let abs_path = self.dir.join(&file_handle);
    
    // Write to temporary file with unique suffix
    let temp_path = abs_path.with_extension("tmp");
    
    let file = OpenOptions::new()
        .write(true)
        .create_new(true)
        .open(&temp_path)
        .await
        .err_notes(&temp_path)?;
    
    // Return wrapper that performs atomic rename on successful close
    Ok((file_handle, Box::new(AtomicFileWriter::new(
        file,
        temp_path,
        abs_path
    ))))
}
```

Where `AtomicFileWriter` is a wrapper that:
1. Writes to the temporary file
2. On `shutdown()`, calls `sync_all()` to flush to disk
3. Performs atomic `rename()` from temp to final path
4. On drop without shutdown, removes the temp file

Additionally, consider adding checksums to manifest files to enable corruption detection during restore validation.

## Proof of Concept

```rust
// Test: Simulate crash during backup by killing process with SIGKILL
// File: storage/backup/backup-cli/tests/corruption_test.rs

use std::process::{Command, Stdio};
use std::time::Duration;
use tempfile::TempDir;

#[tokio::test]
async fn test_crash_during_backup_leaves_partial_files() {
    let temp_dir = TempDir::new().unwrap();
    
    // Start backup process
    let mut child = Command::new("aptos-backup-cli")
        .arg("state-snapshot")
        .arg("--state-snapshot-epoch=100")
        .arg("--dir")
        .arg(temp_dir.path())
        .stdout(Stdio::null())
        .stderr(Stdio::null())
        .spawn()
        .unwrap();
    
    // Allow backup to start writing files
    tokio::time::sleep(Duration::from_secs(5)).await;
    
    // Simulate crash with SIGKILL (no graceful cleanup)
    child.kill().unwrap();
    
    // Verify partial files exist
    let entries = std::fs::read_dir(temp_dir.path()).unwrap();
    let mut found_partial = false;
    
    for entry in entries {
        let entry = entry.unwrap();
        let metadata = entry.metadata().unwrap();
        
        // Check for files that should contain data but are suspiciously small
        if entry.path().extension().map(|s| s == "chunk").unwrap_or(false) {
            if metadata.len() > 0 && metadata.len() < 1024 {
                found_partial = true;
                
                // Attempt to read and deserialize - should fail or be incomplete
                let content = std::fs::read(entry.path()).unwrap();
                let result: Result<Vec<(StateKey, StateValue)>, _> = 
                    bcs::from_bytes(&content);
                
                assert!(result.is_err() || {
                    // File is valid BCS but incomplete
                    let items = result.unwrap();
                    items.len() < expected_chunk_size()
                });
            }
        }
    }
    
    assert!(found_partial, "No partial files detected after simulated crash");
}
```

## Notes

This vulnerability demonstrates a gap between the backup system's reliability requirements and its implementation. While the codebase shows awareness of atomic file patterns (as evidenced in metadata cache operations), these patterns are not consistently applied to the critical backup data path. Given Aptos's emphasis on validator availability and disaster recovery, ensuring backup integrity is essential to the network's operational resilience.

### Citations

**File:** storage/backup/backup-cli/src/storage/local_fs/mod.rs (L80-96)
```rust
    async fn create_for_write(
        &self,
        backup_handle: &BackupHandleRef,
        name: &ShellSafeName,
    ) -> Result<(FileHandle, Box<dyn AsyncWrite + Send + Unpin>)> {
        let file_handle = Path::new(backup_handle)
            .join(name.as_ref())
            .path_to_string()?;
        let abs_path = self.dir.join(&file_handle).path_to_string()?;
        let file = OpenOptions::new()
            .write(true)
            .create_new(true)
            .open(&abs_path)
            .await
            .err_notes(&abs_path)?;
        Ok((file_handle, Box::new(file)))
    }
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/backup.rs (L404-447)
```rust
    async fn write_chunk(
        &self,
        backup_handle: &BackupHandleRef,
        chunk: Chunk,
    ) -> Result<StateSnapshotChunk> {
        let _timer = BACKUP_TIMER.timer_with(&["state_snapshot_write_chunk"]);

        let Chunk {
            bytes,
            first_idx,
            last_idx,
            first_key,
            last_key,
        } = chunk;

        let (chunk_handle, mut chunk_file) = self
            .storage
            .create_for_write(backup_handle, &Self::chunk_name(first_idx))
            .await?;
        chunk_file.write_all(&bytes).await?;
        chunk_file.shutdown().await?;
        let (proof_handle, mut proof_file) = self
            .storage
            .create_for_write(backup_handle, &Self::chunk_proof_name(first_idx, last_idx))
            .await?;
        tokio::io::copy(
            &mut self
                .client
                .get_account_range_proof(last_key, self.version())
                .await?,
            &mut proof_file,
        )
        .await?;
        proof_file.shutdown().await?;

        Ok(StateSnapshotChunk {
            first_idx,
            last_idx,
            first_key,
            last_key,
            blobs: chunk_handle,
            proof: proof_handle,
        })
    }
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/backup.rs (L449-492)
```rust
    async fn write_manifest(
        &self,
        backup_handle: &BackupHandleRef,
        chunks: Vec<StateSnapshotChunk>,
    ) -> Result<FileHandle> {
        let proof_bytes = self.client.get_state_root_proof(self.version()).await?;
        let (txn_info, _): (TransactionInfoWithProof, LedgerInfoWithSignatures) =
            bcs::from_bytes(&proof_bytes)?;

        let (proof_handle, mut proof_file) = self
            .storage
            .create_for_write(backup_handle, Self::proof_name())
            .await?;
        proof_file.write_all(&proof_bytes).await?;
        proof_file.shutdown().await?;

        let manifest = StateSnapshotBackup {
            epoch: self.epoch,
            version: self.version(),
            root_hash: txn_info.transaction_info().ensure_state_checkpoint_hash()?,
            chunks,
            proof: proof_handle,
        };

        let (manifest_handle, mut manifest_file) = self
            .storage
            .create_for_write(backup_handle, Self::manifest_name())
            .await?;
        manifest_file
            .write_all(&serde_json::to_vec(&manifest)?)
            .await?;
        manifest_file.shutdown().await?;

        let metadata = Metadata::new_state_snapshot_backup(
            self.epoch,
            self.version(),
            manifest_handle.clone(),
        );
        self.storage
            .save_metadata_line(&metadata.name(), &metadata.to_text_line()?)
            .await?;

        Ok(manifest_handle)
    }
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/manifest.rs (L9-27)
```rust
/// A chunk of a state snapshot manifest, representing accounts in the key range
/// [`first_key`, `last_key`] (right side inclusive).
#[derive(Deserialize, Serialize)]
pub struct StateSnapshotChunk {
    /// index of the first account in this chunk over all accounts.
    pub first_idx: usize,
    /// index of the last account in this chunk over all accounts.
    pub last_idx: usize,
    /// key of the first account in this chunk.
    pub first_key: HashValue,
    /// key of the last account in this chunk.
    pub last_key: HashValue,
    /// Repeated `len(record) + record` where `record` is BCS serialized tuple
    /// `(key, state_value)`
    pub blobs: FileHandle,
    /// BCS serialized `SparseMerkleRangeProof` that proves this chunk adds up to the root hash
    /// indicated in the backup (`StateSnapshotBackup::root_hash`).
    pub proof: FileHandle,
}
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L253-266)
```rust
    async fn read_state_value(
        storage: &Arc<dyn BackupStorage>,
        file_handle: FileHandle,
    ) -> Result<Vec<(StateKey, StateValue)>> {
        let mut file = storage.open_for_read(&file_handle).await?;

        let mut chunk = vec![];

        while let Some(record_bytes) = file.read_record_bytes().await? {
            chunk.push(bcs::from_bytes(&record_bytes)?);
        }

        Ok(chunk)
    }
```

**File:** storage/backup/backup-cli/src/metadata/cache.rs (L156-162)
```rust
            match download_file(storage_ref, file_handle, &local_tmp_file).await {
                Ok(_) => {
                    // rename to target file only if successful; stale tmp file caused by failure will be
                    // reclaimed on next run
                    tokio::fs::rename(local_tmp_file.clone(), local_file)
                        .await
                        .err_notes(local_tmp_file)?;
```
