# Audit Report

## Title
Network Runtime Thread Starvation via Blocking REST Call in Discovery Stream

## Summary
The `RestStream` implementation in the network discovery module uses `futures::executor::block_on()` inside its `poll_next()` method, which blocks tokio runtime worker threads for up to 10 seconds per REST API call. This anti-pattern can starve the network runtime, delaying critical message processing including consensus messages, leading to validator node slowdowns and potential consensus liveness issues.

## Finding Description

The vulnerability exists in [1](#0-0) 

The `RestStream::poll_next()` method is called by the tokio async executor when polling the discovery stream. Instead of properly integrating with the async runtime, it uses `block_on()` to synchronously wait for a REST API call. This blocks the entire worker thread for the duration of the HTTP request, which has a 10-second timeout configured in [2](#0-1) 

**Critical Architecture Issue:**

Both the `DiscoveryChangeListener` (containing `RestStream`) and the `PeerManager` (handling all network I/O including consensus messages) are spawned on the same network runtime as shown in [3](#0-2)  and [4](#0-3) 

The network runtime is created with a configurable number of threads in [5](#0-4) , defaulting to the number of CPU cores when `runtime_threads` is `None` as seen in [6](#0-5) 

**Attack/Failure Scenarios:**

1. **Network Congestion**: Slow or unresponsive REST endpoints cause the 10-second timeout to be hit regularly
2. **Thread Pool Exhaustion**: With discovery intervals (e.g., 1 second as shown in [7](#0-6) ), multiple REST calls can overlap if responses are slow, blocking multiple threads simultaneously
3. **Message Processing Delays**: The `PeerManager` event loop [8](#0-7)  runs on the same runtime and competes for threads with the blocking discovery task
4. **Consensus Impact**: Delayed network message processing can cause consensus messages to be delayed beyond round timeout thresholds, leading to consensus liveness degradation

**Broken Invariants:**
- Violates async runtime contract (blocking in poll methods)
- Can impact consensus liveness (CRITICAL INVARIANT #2 mentions preventing liveness failures)
- Degrades validator node performance under network stress

## Impact Explanation

This qualifies as **High Severity** per the Aptos Bug Bounty program criteria: "Validator node slowdowns."

The blocking behavior directly causes validator nodes to experience performance degradation when:
- REST discovery is enabled (production deployments may use this)
- Network conditions are poor (latency, packet loss)
- Runtime thread pools are limited (test configurations use 1 thread, production uses CPU cores)

In worst-case scenarios with single-threaded network runtimes or severe network issues, this could escalate toward **Critical Severity** by causing "Total loss of liveness" if consensus message processing is delayed beyond recovery thresholds.

**Quantified Impact:**
- **Thread blocking duration**: Up to 10 seconds per REST call
- **Frequency**: Every `interval_secs` (typically 1 second in tests)
- **Affected components**: All network tasks on the same runtime (peer management, message routing, consensus message handling)
- **Blast radius**: All validators using REST-based discovery under network stress

## Likelihood Explanation

**Likelihood: Medium to High**

This issue occurs naturally without attacker involvement:
- Network congestion or slow REST API servers trigger it automatically
- Test environments with single-threaded runtimes are particularly vulnerable
- Production systems with limited CPU cores or high network load are affected
- The developers acknowledged this is problematic (see TODO comment at line 47)

**Triggering Conditions:**
1. REST discovery is configured in network settings
2. REST endpoint becomes slow or experiences network latency
3. Discovery interval is shorter than REST call response time
4. Network runtime has limited threads

**Attacker Requirements:**
- No privileged access needed
- Could be amplified by causing network delays (though pure network DoS is out of scope)
- Exploitation is opportunistic based on legitimate network conditions

## Recommendation

Replace the blocking `block_on()` call with a proper async integration. The REST client already supports async operations, so the fix is straightforward:

**Recommended Fix:**

```rust
impl Stream for RestStream {
    type Item = Result<PeerSet, DiscoveryError>;

    fn poll_next(mut self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Option<Self::Item>> {
        // Wait for delay, or add the delay for next call
        futures::ready!(self.interval.as_mut().poll_next(cx));

        // Store the future in RestStream as an Option<Pin<Box<dyn Future>>>
        // and poll it properly instead of blocking
        let fut = self.rest_client.get_account_resource_bcs::<ValidatorSet>(
            AccountAddress::ONE,
            "0x1::stake::ValidatorSet",
        );
        
        // Properly integrate with async runtime by storing and polling the future
        // This requires restructuring RestStream to hold a state machine
        // for the in-progress REST call
    }
}
```

**Better approach**: Restructure `RestStream` to use a state machine pattern that properly stores and polls the async future without blocking, similar to how other async streams are implemented in the codebase.

## Proof of Concept

```rust
// Reproduction steps (conceptual - would need full test harness):

use futures::executor::block_on;
use tokio::runtime::Runtime;
use std::time::Duration;

#[test]
fn test_blocking_starves_runtime() {
    // Create a single-threaded runtime (simulating test config)
    let runtime = Runtime::new().unwrap();
    
    runtime.block_on(async {
        let handle = tokio::spawn(async {
            // Simulate RestStream behavior
            loop {
                // This blocks the worker thread for 10 seconds
                let _ = block_on(async {
                    tokio::time::sleep(Duration::from_secs(10)).await;
                    "REST response"
                });
                tokio::time::sleep(Duration::from_secs(1)).await;
            }
        });
        
        // This task should run concurrently but will be starved
        let message_task = tokio::spawn(async {
            // Simulate consensus message processing
            tokio::time::sleep(Duration::from_millis(100)).await;
            "Message processed"
        });
        
        // In a single-threaded runtime, message_task cannot progress
        // while the blocking task holds the thread
        tokio::time::timeout(Duration::from_secs(5), message_task)
            .await
            .expect_err("Task should timeout due to thread starvation");
    });
}
```

**Real-world demonstration**: Configure a node with REST discovery pointing to a slow REST endpoint, set `runtime_threads` to 1, and observe consensus message delays in logs during discovery polling intervals.

## Notes

The developers were aware this is problematic, as evidenced by the TODO comment in [9](#0-8) . This vulnerability affects any deployment using REST-based peer discovery, with severity increasing in proportion to network latency and inverse to available runtime threads. While the immediate impact is performance degradation (High severity), under adverse conditions this could contribute to consensus liveness issues (Critical severity).

### Citations

**File:** network/discovery/src/rest.rs (L42-51)
```rust
    fn poll_next(mut self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Option<Self::Item>> {
        // Wait for delay, or add the delay for next call
        futures::ready!(self.interval.as_mut().poll_next(cx));

        // Retrieve the onchain resource at the interval
        // TODO there should be a better way than converting this to a blocking call
        let response = block_on(self.rest_client.get_account_resource_bcs::<ValidatorSet>(
            AccountAddress::ONE,
            "0x1::stake::ValidatorSet",
        ));
```

**File:** crates/aptos-rest-client/src/client_builder.rs (L54-54)
```rust
            timeout: Duration::from_secs(10), // Default to 10 seconds
```

**File:** aptos-node/src/network.rs (L276-282)
```rust
        // Create a network runtime for the config
        let runtime = create_network_runtime(&network_config);

        // Entering gives us a runtime to instantiate all the pieces of the builder
        let _enter = runtime.enter();

        // Create a new network builder
```

**File:** aptos-node/src/network.rs (L458-469)
```rust
/// Creates a network runtime for the given network config
fn create_network_runtime(network_config: &NetworkConfig) -> Runtime {
    let network_id = network_config.network_id;
    debug!("Creating runtime for network ID: {}", network_id);

    // Create the runtime
    let thread_name = format!(
        "network-{}",
        network_id.as_str().chars().take(3).collect::<String>()
    );
    aptos_runtimes::spawn_named_runtime(thread_name, network_config.runtime_threads)
}
```

**File:** network/builder/src/builder.rs (L278-282)
```rust
        if let Some(discovery_listeners) = self.discovery_listeners.take() {
            discovery_listeners
                .into_iter()
                .for_each(|listener| listener.start(executor))
        }
```

**File:** config/src/config/network_config.rs (L82-83)
```rust
    /// Number of threads to run for networking
    pub runtime_threads: Option<usize>,
```

**File:** testsuite/smoke-test/src/network.rs (L156-159)
```rust
    network_config.discovery_method = DiscoveryMethod::Rest(RestDiscovery {
        url: rest_endpoint,
        interval_secs: 1,
    });
```

**File:** network/framework/src/peer_manager/mod.rs (L232-260)
```rust
    pub async fn start(mut self) {
        // Start listening for connections.
        info!(
            NetworkSchema::new(&self.network_context),
            "Start listening for incoming connections on {}", self.listen_addr
        );
        self.start_connection_listener();
        loop {
            ::futures::select! {
                connection_event = self.transport_notifs_rx.select_next_some() => {
                    self.handle_connection_event(connection_event);
                }
                connection_request = self.connection_reqs_rx.select_next_some() => {
                    self.handle_outbound_connection_request(connection_request).await;
                }
                request = self.requests_rx.select_next_some() => {
                    self.handle_outbound_request(request).await;
                }
                complete => {
                    break;
                }
            }
        }

        warn!(
            NetworkSchema::new(&self.network_context),
            "PeerManager actor terminated"
        );
    }
```
