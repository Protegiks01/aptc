# Audit Report

## Title
Memory Double-Counting in Table Native Operations Causes Premature Memory Quota Exhaustion

## Summary
The memory tracking system double-counts heap memory when values are passed to table native functions, causing transactions to hit MEMORY_LIMIT_EXCEEDED errors prematurely. This occurs due to inconsistencies between `abstract_heap_size`, `abstract_packed_size`, and `abstract_stack_size` calculations combined with special handling for table operations.

## Finding Description
The memory tracker uses three different size calculation methods that operate on different principles: [1](#0-0) [2](#0-1) [3](#0-2) 

The vulnerability arises from the interaction between the memory tracker and table native functions:

1. **Memory Leak Flag for Table Operations**: When calling table native functions, a special flag prevents argument memory from being released: [4](#0-3) [5](#0-4) 

2. **Double Charging in Table Natives**: The table native functions then charge memory again for the same values: [6](#0-5) 

3. **Feature Flag Fix Not Yet Enabled**: The fix exists but is scheduled for October 2025: [7](#0-6) [8](#0-7) 

**Exploitation Flow**:
1. User creates a large value (e.g., vector with 1000 u64 elements) â†’ charges heap_size H
2. User calls `table::add(table, key, value)` 
3. Memory tracker does NOT release H (due to `should_leak_memory_for_native = true`)
4. Native function charges H again via `context.use_heap_memory()`
5. Total charged: 2*H (double-counted)
6. Repeated operations exhaust memory quota at 2x the expected rate

## Impact Explanation
**Medium Severity** - This breaks the Move VM Safety invariant that "bytecode execution must respect gas limits and memory constraints." Specifically:

- **Resource Exhaustion**: Legitimate users hit MEMORY_LIMIT_EXCEEDED prematurely, preventing valid operations
- **Deterministic but Incorrect**: All validators apply the same incorrect accounting, so no consensus split occurs
- **Temporary Impact**: The issue is scheduled to be fixed in October 2025 via the timed feature flag
- **Limited Scope**: Only affects transactions using table operations (add, borrow, contains, remove)

This does not qualify for Critical or High severity because:
- No funds loss or theft
- No consensus safety violations
- No network partition or liveness issues
- State remains consistent across validators

## Likelihood Explanation
**High Likelihood** - This bug is currently active on mainnet and testnet until the feature flag activates:

- Any transaction using table operations is affected
- The bug is deterministic and occurs on every table operation
- Affects common patterns like storing collections in tables
- Impact scales with value sizes (larger values = more severe double-counting)
- Currently exploitable until October 2025

## Recommendation
The fix is already implemented via the `FixTableNativesMemoryDoubleCounting` feature flag. Once enabled, the table natives only charge memory for values loaded from storage, not for arguments: [9](#0-8) 

**Immediate mitigation**: Enable the `FixTableNativesMemoryDoubleCounting` feature flag earlier than the currently scheduled October 2025 date.

**Long-term fix**: Remove the `should_leak_memory_for_native` special handling once all table operations correctly track memory without double-counting.

## Proof of Concept

```move
#[test_only]
module test_addr::memory_double_count_poc {
    use std::vector;
    use aptos_std::table::{Self, Table};
    
    struct LargeValue has store, drop {
        data: vector<u64>
    }
    
    #[test(account = @test_addr)]
    fun test_table_memory_double_counting(account: &signer) {
        // Create a table
        let table = table::new<u64, LargeValue>();
        
        // Create a large value (1000 u64s = ~8KB)
        let large_vec = vector::empty<u64>();
        let i = 0;
        while (i < 1000) {
            vector::push_back(&mut large_vec, i);
            i = i + 1;
        };
        let large_value = LargeValue { data: large_vec };
        
        // Add to table - this double-counts the memory
        // Expected memory: ~8KB for the vector
        // Actual charged: ~16KB (8KB from initial creation + 8KB from table::add)
        table::add(&mut table, 1, large_value);
        
        // With repeated additions, memory quota exhausts at 2x expected rate
        // This will hit MEMORY_LIMIT_EXCEEDED earlier than it should
        
        table::drop_unchecked_box(table);
    }
}
```

## Notes

This vulnerability demonstrates a clear memory tracking gap created by inconsistencies between the three size calculation methods (`abstract_heap_size`, `abstract_packed_size`, `abstract_stack_size`). The special handling for table operations (`should_leak_memory_for_native`) combined with the natives' own memory charging creates systematic double-counting that prematurely exhausts memory quotas.

While the impact is limited to resource exhaustion rather than consensus or funds loss, it breaks the Move VM's resource limit invariants and affects all users of table operations until the scheduled fix in October 2025.

### Citations

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/misc.rs (L495-730)
```rust
    pub fn abstract_stack_size(
        &self,
        val: impl ValueView,
        feature_version: u64,
    ) -> PartialVMResult<AbstractValueSize> {
        struct Visitor<'a> {
            feature_version: u64,
            params: &'a AbstractValueSizeGasParameters,
            res: Option<AbstractValueSize>,
            max_value_nest_depth: Option<u64>,
        }

        impl Visitor<'_> {
            check_depth_impl!();
        }

        impl ValueVisitor for Visitor<'_> {
            #[inline]
            fn visit_delayed(&mut self, depth: u64, _val: DelayedFieldID) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.u64);
                Ok(())
            }

            #[inline]
            fn visit_u8(&mut self, depth: u64, _val: u8) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.u8);
                Ok(())
            }

            #[inline]
            fn visit_u16(&mut self, depth: u64, _val: u16) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.u16);
                Ok(())
            }

            #[inline]
            fn visit_u32(&mut self, depth: u64, _val: u32) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.u32);
                Ok(())
            }

            #[inline]
            fn visit_u64(&mut self, depth: u64, _val: u64) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.u64);
                Ok(())
            }

            #[inline]
            fn visit_u128(&mut self, depth: u64, _val: u128) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.u128);
                Ok(())
            }

            #[inline]
            fn visit_u256(&mut self, depth: u64, _val: &U256) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.u256);
                Ok(())
            }

            #[inline]
            fn visit_i8(&mut self, depth: u64, _val: i8) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.i8);
                Ok(())
            }

            #[inline]
            fn visit_i16(&mut self, depth: u64, _val: i16) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.i16);
                Ok(())
            }

            #[inline]
            fn visit_i32(&mut self, depth: u64, _val: i32) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.i32);
                Ok(())
            }

            #[inline]
            fn visit_i64(&mut self, depth: u64, _val: i64) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.i64);
                Ok(())
            }

            #[inline]
            fn visit_i128(&mut self, depth: u64, _val: i128) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.i128);
                Ok(())
            }

            #[inline]
            fn visit_i256(&mut self, depth: u64, _val: &I256) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.i256);
                Ok(())
            }

            #[inline]
            fn visit_bool(&mut self, depth: u64, _val: bool) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.bool);
                Ok(())
            }

            #[inline]
            fn visit_address(&mut self, depth: u64, _val: &AccountAddress) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.address);
                Ok(())
            }

            #[inline]
            fn visit_struct(&mut self, depth: u64, _len: usize) -> PartialVMResult<bool> {
                self.check_depth(depth)?;
                self.res = Some(self.params.struct_);
                Ok(false)
            }

            #[inline]
            fn visit_closure(&mut self, depth: u64, _len: usize) -> PartialVMResult<bool> {
                self.check_depth(depth)?;
                self.res = Some(self.params.closure);
                Ok(false)
            }

            #[inline]
            fn visit_vec(&mut self, depth: u64, _len: usize) -> PartialVMResult<bool> {
                self.check_depth(depth)?;
                self.res = Some(self.params.vector);
                Ok(false)
            }

            #[inline]
            fn visit_ref(&mut self, depth: u64, _is_global: bool) -> PartialVMResult<bool> {
                self.check_depth(depth)?;
                self.res = Some(self.params.reference);
                Ok(false)
            }

            // TODO(Gas): The following function impls are necessary due to a bug upstream.
            //            Remove them once the bug is fixed.
            #[inline]
            fn visit_vec_u8(&mut self, depth: u64, vals: &[u8]) -> PartialVMResult<()> {
                if self.feature_version < 3 {
                    self.res = Some(0.into());
                } else {
                    self.visit_vec(depth, vals.len())?;
                }
                Ok(())
            }

            #[inline]
            fn visit_vec_u16(&mut self, depth: u64, vals: &[u16]) -> PartialVMResult<()> {
                self.visit_vec(depth, vals.len())?;
                Ok(())
            }

            #[inline]
            fn visit_vec_u32(&mut self, depth: u64, vals: &[u32]) -> PartialVMResult<()> {
                self.visit_vec(depth, vals.len())?;
                Ok(())
            }

            #[inline]
            fn visit_vec_u64(&mut self, depth: u64, vals: &[u64]) -> PartialVMResult<()> {
                if self.feature_version < 3 {
                    self.res = Some(0.into());
                } else {
                    self.visit_vec(depth, vals.len())?;
                }
                Ok(())
            }

            #[inline]
            fn visit_vec_u128(&mut self, depth: u64, vals: &[u128]) -> PartialVMResult<()> {
                if self.feature_version < 3 {
                    self.res = Some(0.into());
                } else {
                    self.visit_vec(depth, vals.len())?;
                }
                Ok(())
            }

            #[inline]
            fn visit_vec_u256(&mut self, depth: u64, vals: &[U256]) -> PartialVMResult<()> {
                self.visit_vec(depth, vals.len())?;
                Ok(())
            }

            #[inline]
            fn visit_vec_bool(&mut self, depth: u64, vals: &[bool]) -> PartialVMResult<()> {
                if self.feature_version < 3 {
                    self.res = Some(0.into());
                } else {
                    self.visit_vec(depth, vals.len())?;
                }
                Ok(())
            }

            #[inline]
            fn visit_vec_address(
                &mut self,
                depth: u64,
                vals: &[AccountAddress],
            ) -> PartialVMResult<()> {
                if self.feature_version < 3 {
                    self.res = Some(0.into());
                } else {
                    self.visit_vec(depth, vals.len())?;
                }
                Ok(())
            }
        }

        let mut visitor = Visitor {
            feature_version,
            params: self,
            res: None,
            max_value_nest_depth: Some(DEFAULT_MAX_VM_VALUE_NESTED_DEPTH),
        };
        val.visit(&mut visitor)?;
        visitor.res.ok_or_else(|| {
            PartialVMError::new_invariant_violation("Visitor should have set the `res` value")
        })
    }
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/misc.rs (L732-942)
```rust
    pub fn abstract_packed_size(&self, val: impl ValueView) -> PartialVMResult<AbstractValueSize> {
        struct Visitor<'a> {
            params: &'a AbstractValueSizeGasParameters,
            res: Option<AbstractValueSize>,
            max_value_nest_depth: Option<u64>,
        }

        impl Visitor<'_> {
            check_depth_impl!();
        }

        impl ValueVisitor for Visitor<'_> {
            #[inline]
            fn visit_delayed(&mut self, depth: u64, _val: DelayedFieldID) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.per_u64_packed * NumArgs::from(1));
                Ok(())
            }

            #[inline]
            fn visit_u8(&mut self, depth: u64, _val: u8) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.per_u8_packed * NumArgs::from(1));
                Ok(())
            }

            #[inline]
            fn visit_u16(&mut self, depth: u64, _val: u16) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.per_u16_packed * NumArgs::from(1));
                Ok(())
            }

            #[inline]
            fn visit_u32(&mut self, depth: u64, _val: u32) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.per_u32_packed * NumArgs::from(1));
                Ok(())
            }

            #[inline]
            fn visit_u64(&mut self, depth: u64, _val: u64) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.per_u64_packed * NumArgs::from(1));
                Ok(())
            }

            #[inline]
            fn visit_u128(&mut self, depth: u64, _val: u128) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.per_u128_packed * NumArgs::from(1));
                Ok(())
            }

            #[inline]
            fn visit_u256(&mut self, depth: u64, _val: &U256) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.per_u256_packed * NumArgs::from(1));
                Ok(())
            }

            #[inline]
            fn visit_i8(&mut self, depth: u64, _val: i8) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.per_i8_packed * NumArgs::from(1));
                Ok(())
            }

            #[inline]
            fn visit_i16(&mut self, depth: u64, _val: i16) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.per_i16_packed * NumArgs::from(1));
                Ok(())
            }

            #[inline]
            fn visit_i32(&mut self, depth: u64, _val: i32) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.per_i32_packed * NumArgs::from(1));
                Ok(())
            }

            #[inline]
            fn visit_i64(&mut self, depth: u64, _val: i64) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.per_i64_packed * NumArgs::from(1));
                Ok(())
            }

            #[inline]
            fn visit_i128(&mut self, depth: u64, _val: i128) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.per_i128_packed * NumArgs::from(1));
                Ok(())
            }

            #[inline]
            fn visit_i256(&mut self, depth: u64, _val: &I256) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.per_i256_packed * NumArgs::from(1));
                Ok(())
            }

            #[inline]
            fn visit_bool(&mut self, depth: u64, _val: bool) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.per_bool_packed * NumArgs::from(1));
                Ok(())
            }

            #[inline]
            fn visit_address(&mut self, depth: u64, _val: &AccountAddress) -> PartialVMResult<()> {
                self.check_depth(depth)?;
                self.res = Some(self.params.per_address_packed * NumArgs::from(1));
                Ok(())
            }

            #[inline]
            fn visit_struct(&mut self, depth: u64, _len: usize) -> PartialVMResult<bool> {
                self.check_depth(depth)?;
                self.res = Some(self.params.struct_);
                Ok(false)
            }

            #[inline]
            fn visit_closure(&mut self, depth: u64, _len: usize) -> PartialVMResult<bool> {
                self.check_depth(depth)?;
                self.res = Some(self.params.closure);
                Ok(false)
            }

            #[inline]
            fn visit_vec(&mut self, depth: u64, _len: usize) -> PartialVMResult<bool> {
                self.check_depth(depth)?;
                self.res = Some(self.params.vector);
                Ok(false)
            }

            #[inline]
            fn visit_ref(&mut self, depth: u64, _is_global: bool) -> PartialVMResult<bool> {
                // TODO(Gas): This should be unreachable...
                //            See if we can handle this in a more graceful way.
                self.check_depth(depth)?;
                self.res = Some(self.params.reference);
                Ok(false)
            }

            // TODO(Gas): The following function impls are necessary due to a bug upstream.
            //            Remove them once the bug is fixed.
            #[inline]
            fn visit_vec_u8(&mut self, depth: u64, vals: &[u8]) -> PartialVMResult<()> {
                self.visit_vec(depth, vals.len())?;
                Ok(())
            }

            #[inline]
            fn visit_vec_u16(&mut self, depth: u64, vals: &[u16]) -> PartialVMResult<()> {
                self.visit_vec(depth, vals.len())?;
                Ok(())
            }

            #[inline]
            fn visit_vec_u32(&mut self, depth: u64, vals: &[u32]) -> PartialVMResult<()> {
                self.visit_vec(depth, vals.len())?;
                Ok(())
            }

            #[inline]
            fn visit_vec_u64(&mut self, depth: u64, vals: &[u64]) -> PartialVMResult<()> {
                self.visit_vec(depth, vals.len())?;
                Ok(())
            }

            #[inline]
            fn visit_vec_u128(&mut self, depth: u64, vals: &[u128]) -> PartialVMResult<()> {
                self.visit_vec(depth, vals.len())?;
                Ok(())
            }

            fn visit_vec_u256(&mut self, depth: u64, vals: &[U256]) -> PartialVMResult<()> {
                self.visit_vec(depth, vals.len())?;
                Ok(())
            }

            #[inline]
            fn visit_vec_bool(&mut self, depth: u64, vals: &[bool]) -> PartialVMResult<()> {
                self.visit_vec(depth, vals.len())?;
                Ok(())
            }

            #[inline]
            fn visit_vec_address(
                &mut self,
                depth: u64,
                vals: &[AccountAddress],
            ) -> PartialVMResult<()> {
                self.visit_vec(depth, vals.len())?;
                Ok(())
            }
        }

        let mut visitor = Visitor {
            params: self,
            res: None,
            max_value_nest_depth: Some(DEFAULT_MAX_VM_VALUE_NESTED_DEPTH),
        };
        val.visit(&mut visitor)?;
        visitor.res.ok_or_else(|| {
            PartialVMError::new_invariant_violation("Visitor should have set the `res` value")
        })
    }
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/misc.rs (L956-965)
```rust
    pub fn abstract_heap_size(
        &self,
        val: impl ValueView,
        feature_version: u64,
    ) -> PartialVMResult<AbstractValueSize> {
        let stack_size = self.abstract_stack_size(&val, feature_version)?;
        let abs_size = self.abstract_value_size(val, feature_version)?;

        Ok(abs_size.checked_sub(stack_size).unwrap_or_else(|| 0.into()))
    }
```

**File:** aptos-move/aptos-memory-usage-tracker/src/lib.rs (L310-319)
```rust
        // Save the info for charge_native_function_before_execution.
        self.should_leak_memory_for_native = (*module_id.address() == CORE_CODE_ADDRESS
            && module_id.name().as_str() == "table")
            || (self.feature_version() >= 4
                && *module_id.address() == CORE_CODE_ADDRESS
                && module_id.name().as_str() == "event");

        self.base
            .charge_call_generic(module_id, func_name, ty_args, args, num_locals)
    }
```

**File:** aptos-move/aptos-memory-usage-tracker/src/lib.rs (L322-344)
```rust
    fn charge_native_function_before_execution(
        &mut self,
        ty_args: impl ExactSizeIterator<Item = impl TypeView> + Clone,
        args: impl ExactSizeIterator<Item = impl ValueView> + Clone,
    ) -> PartialVMResult<()> {
        // TODO(Gas): https://github.com/aptos-labs/aptos-core/issues/5485
        if !self.should_leak_memory_for_native {
            self.release_heap_memory(args.clone().try_fold(
                AbstractValueSize::zero(),
                |acc, val| {
                    let heap_size = self
                        .vm_gas_params()
                        .misc
                        .abs_val
                        .abstract_heap_size(val, self.feature_version())?;
                    Ok::<_, PartialVMError>(acc + heap_size)
                },
            )?);
        }

        self.base
            .charge_native_function_before_execution(ty_args, args)
    }
```

**File:** aptos-move/framework/table-natives/src/lib.rs (L395-441)
```rust
    let fix_memory_double_counting =
        context.timed_feature_enabled(TimedFeatureFlag::FixTableNativesMemoryDoubleCounting);

    let (extensions, mut loader_context, abs_val_gas_params, gas_feature_version) =
        context.extensions_with_loader_context_and_gas_params();
    let table_context = extensions.get::<NativeTableContext>();
    let mut table_data = table_context.table_data.borrow_mut();

    let val = args.pop_back().unwrap();
    let key = args.pop_back().unwrap();
    let handle = get_table_handle(&safely_pop_arg!(args, StructRef))?;

    let table =
        table_data.get_or_create_table(&mut loader_context, handle, &ty_args[0], &ty_args[2])?;

    let function_value_extension = loader_context.function_value_extension();
    let key_bytes = serialize_key(&function_value_extension, &table.key_layout, &key)?;
    let key_cost = ADD_BOX_PER_BYTE_SERIALIZED * NumBytes::new(key_bytes.len() as u64);

    let (gv, loaded) =
        table.get_or_create_global_value(&function_value_extension, table_context, key_bytes)?;
    let mem_usage = if !fix_memory_double_counting || loaded.is_some() {
        gv.view()
            .map(|val| {
                abs_val_gas_params
                    .abstract_heap_size(&val, gas_feature_version)
                    .map(u64::from)
            })
            .transpose()?
    } else {
        None
    };

    let res = match gv.move_to(val) {
        Ok(_) => Ok(smallvec![]),
        Err(_) => Err(SafeNativeError::Abort {
            abort_code: ALREADY_EXISTS,
        }),
    };

    drop(table_data);

    // TODO(Gas): Figure out a way to charge this earlier.
    context.charge(key_cost)?;
    if let Some(amount) = mem_usage {
        context.use_heap_memory(amount)?;
    }
```

**File:** types/src/on_chain_config/timed_features.rs (L26-27)
```rust
    /// Fixes the bug that table natives double count the memory usage of the global values.
    FixTableNativesMemoryDoubleCounting,
```

**File:** types/src/on_chain_config/timed_features.rs (L128-135)
```rust
            (FixTableNativesMemoryDoubleCounting, TESTNET) => Los_Angeles
                .with_ymd_and_hms(2025, 10, 16, 17, 0, 0)
                .unwrap()
                .with_timezone(&Utc),
            (FixTableNativesMemoryDoubleCounting, MAINNET) => Los_Angeles
                .with_ymd_and_hms(2025, 10, 21, 10, 0, 0)
                .unwrap()
                .with_timezone(&Utc),
```
