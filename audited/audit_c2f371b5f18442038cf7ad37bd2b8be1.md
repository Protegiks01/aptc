# Audit Report

## Title
Race Condition Between State Sync and Persisting Phase Causes Consensus State Corruption

## Summary
The persisting phase unconditionally returns success even when block commits fail due to pipeline abortion, allowing the buffer manager to incorrectly update consensus state tracking. Combined with state sync's `abort_pipeline_for_state_sync`, this creates a race condition that can corrupt the `highest_committed_round` tracking.

## Finding Description

The persisting phase assumes that `PipelinedBlock` maintains valid pipeline state throughout its execution, but this assumption is violated when `abort_pipeline_for_state_sync` is called during state synchronization. [1](#0-0) 

The code attempts to send the commit proof via `commit_proof_tx` but ignores send failures. The `.take().map()` pattern returns `Option<Result<(), T>>` but the error is never checked. [2](#0-1) 

When `wait_for_commit_ledger()` is called, any errors from the aborted pipeline are silently ignored.

The race condition occurs when state sync aborts block pipelines: [3](#0-2) 

This `abort_pipeline_for_state_sync` is called during fast forward sync: [4](#0-3) 

**Attack Sequence:**
1. Blocks at round R are aggregated and sent to persisting phase
2. State sync detects node is behind and triggers fast forward sync  
3. `abort_pipeline_for_state_sync` aborts all block pipelines including those in persisting phase
4. The `commit_proof_tx` oneshot receiver is dropped when pipeline is aborted
5. Persisting phase sends on `commit_proof_tx` - send fails silently
6. `wait_for_commit_ledger` returns immediately (pipeline aborted, error ignored)
7. Persisting phase returns `Ok(round)` despite commit never executing
8. Buffer manager receives persisting response and unconditionally updates state: [5](#0-4) 

The critical flaw: if this stale persisting response arrives AFTER the reset completes, it overwrites the corrected `highest_committed_round` value with an incorrect one, corrupting consensus state tracking.

## Impact Explanation

**Severity: Medium to High**

This violates **Consensus Safety** and **State Consistency** invariants:

1. **State Tracking Corruption**: The buffer manager's `highest_committed_round` can be set to an uncommitted round, causing the node to believe blocks are committed when they are not in storage.

2. **Consensus Divergence**: With incorrect `highest_committed_round`, the node may:
   - Reject valid blocks as "already committed"
   - Process blocks in wrong order
   - Fail to sync correctly with network

3. **Recovery Issues**: After restart, storage doesn't match consensus tracking, requiring manual intervention.

The impact is limited because state sync eventually corrects storage, but the window of inconsistency can cause operational issues and temporary divergence from network consensus.

## Likelihood Explanation

**Likelihood: Medium**

The race condition requires:
1. Blocks in persisting phase when state sync triggers
2. Persisting phase response arriving after reset completes
3. Specific timing window in the tokio select! loop

This can occur naturally during:
- Epoch transitions with state sync
- Node catching up after temporary disconnection
- Fast forward sync operations

The race is timing-dependent but not rare - state sync operations are common in a production network, and the decoupled execution architecture increases the probability of blocks being in persisting phase during sync events.

## Recommendation

**Fix 1: Check errors in persisting phase** [6](#0-5) 

Replace the error-ignoring logic with proper error propagation:

```rust
async fn process(&self, req: PersistingRequest) -> PersistingResponse {
    let PersistingRequest { blocks, commit_ledger_info } = req;
    
    for b in &blocks {
        if let Some(tx) = b.pipeline_tx().lock().as_mut() {
            if let Some(sender) = tx.commit_proof_tx.take() {
                // Check send result
                if sender.send(commit_ledger_info.clone()).is_err() {
                    return Err(ExecutorError::InternalError { 
                        error: "Commit proof channel closed".to_string() 
                    });
                }
            }
        }
        // Check wait result
        match b.wait_for_commit_ledger_with_result().await {
            Ok(Some(_)) => {}, // Commit succeeded
            Ok(None) => {}, // Block was prefix, ok
            Err(e) => return Err(e), // Propagate error
        }
    }
    
    let response = Ok(blocks.last().expect("Blocks can't be empty").round());
    if commit_ledger_info.ledger_info().ends_epoch() {
        self.commit_msg_tx.send_epoch_change(...).await;
    }
    response
}
```

**Fix 2: Validate persisting responses in buffer manager** [5](#0-4) 

Add validation before updating state:

```rust
Some(Ok(round)) = self.persisting_phase_rx.next() => {
    // Ignore stale responses after reset
    if round > self.highest_committed_round {
        self.pending_commit_votes = self.pending_commit_votes.split_off(&(round + 1));
        self.highest_committed_round = round;
        self.pending_commit_blocks = self.pending_commit_blocks.split_off(&(round + 1));
    } else {
        warn!("Ignoring stale persisting response for round {} (current: {})", 
              round, self.highest_committed_round);
    }
},
```

## Proof of Concept

This race condition can be demonstrated with a Rust test:

```rust
#[tokio::test]
async fn test_persisting_phase_abort_race() {
    // Setup: Create a block with pipeline
    let block = create_test_block(100);
    let (tx, _rx) = setup_pipeline_channels();
    block.set_pipeline_tx(tx);
    block.set_pipeline_futs(create_test_futures());
    
    // Send block to persisting phase
    let persisting_phase = PersistingPhase::new(network_sender);
    let commit_info = create_test_commit_ledger_info(100);
    
    // Race: Abort pipeline while persisting phase is processing
    tokio::spawn(async move {
        tokio::time::sleep(Duration::from_millis(10)).await;
        block.abort_pipeline(); // Simulate state sync abort
    });
    
    let request = PersistingRequest {
        blocks: vec![Arc::new(block)],
        commit_ledger_info: commit_info,
    };
    
    // Persisting phase should fail but returns Ok
    let result = persisting_phase.process(request).await;
    
    // BUG: This returns Ok even though commit never happened
    assert!(result.is_ok());
    
    // Verify: Storage does NOT have the block committed
    let storage_has_block = executor.get_committed_block(100).is_ok();
    assert!(!storage_has_block); // Block not in storage
    
    // But buffer manager would update highest_committed_round to 100
    // causing state corruption
}
```

---

**Notes**

This vulnerability specifically targets the assumption that `PipelinedBlock.pipeline_tx` and pipeline futures remain valid throughout persisting phase execution. The abort mechanism violates this assumption without proper error handling, creating a window for consensus state corruption during state synchronization operations.

### Citations

**File:** consensus/src/pipeline/persisting_phase.rs (L59-81)
```rust
    async fn process(&self, req: PersistingRequest) -> PersistingResponse {
        let PersistingRequest {
            blocks,
            commit_ledger_info,
        } = req;

        for b in &blocks {
            if let Some(tx) = b.pipeline_tx().lock().as_mut() {
                tx.commit_proof_tx
                    .take()
                    .map(|tx| tx.send(commit_ledger_info.clone()));
            }
            b.wait_for_commit_ledger().await;
        }

        let response = Ok(blocks.last().expect("Blocks can't be empty").round());
        if commit_ledger_info.ledger_info().ends_epoch() {
            self.commit_msg_tx
                .send_epoch_change(EpochChangeProof::new(vec![commit_ledger_info], false))
                .await;
        }
        response
    }
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L562-568)
```rust
    pub async fn wait_for_commit_ledger(&self) {
        // may be aborted (e.g. by reset)
        if let Some(fut) = self.pipeline_futs() {
            // this may be cancelled
            let _ = fut.commit_ledger_fut.await;
        }
    }
```

**File:** consensus/src/block_storage/block_store.rs (L617-627)
```rust
    pub async fn abort_pipeline_for_state_sync(&self) {
        let blocks = self.inner.read().get_all_blocks();
        // the blocks are not ordered by round here, so we need to abort all then wait
        let futs: Vec<_> = blocks
            .into_iter()
            .filter_map(|b| b.abort_pipeline())
            .collect();
        for f in futs {
            f.wait_until_finishes().await;
        }
    }
```

**File:** consensus/src/block_storage/sync_manager.rs (L504-514)
```rust
        // abort any pending executor tasks before entering state sync
        // with zaptos, things can run before hitting buffer manager
        if let Some(block_store) = maybe_block_store {
            monitor!(
                "abort_pipeline_for_state_sync",
                block_store.abort_pipeline_for_state_sync().await
            );
        }
        execution_client
            .sync_to_target(highest_commit_cert.ledger_info().clone())
            .await?;
```

**File:** consensus/src/pipeline/buffer_manager.rs (L968-973)
```rust
                Some(Ok(round)) = self.persisting_phase_rx.next() => {
                    // see where `need_backpressure()` is called.
                    self.pending_commit_votes = self.pending_commit_votes.split_off(&(round + 1));
                    self.highest_committed_round = round;
                    self.pending_commit_blocks = self.pending_commit_blocks.split_off(&(round + 1));
                },
```
