# Audit Report

## Title
Tokio Task Leak in ScheduledBroadcast When Dropped During Sleep

## Summary
The `ScheduledBroadcast` struct spawns a tokio task that continues running even after the `ScheduledBroadcast` future is dropped, causing temporary resource leaks during mempool coordinator shutdown or future cancellation.

## Finding Description

The `ScheduledBroadcast::new()` constructor spawns an independent tokio task that sleeps until a deadline, then attempts to wake a `Waker`. The issue arises because there is no cancellation mechanism when the `ScheduledBroadcast` future is dropped. [1](#0-0) 

The spawned task captures `waker_clone`, an `Arc<Mutex<Option<Waker>>>`, which keeps the task alive even after the `ScheduledBroadcast` is dropped. The `ScheduledBroadcast` is used in `FuturesUnordered` collection: [2](#0-1) 

When the coordinator shuts down or when peers disconnect unexpectedly, pending `ScheduledBroadcast` futures in the `FuturesUnordered` are dropped, but their spawned tasks continue sleeping until their deadline. [3](#0-2) 

The default intervals are 10ms for normal broadcasts and 30 seconds for backoff broadcasts: [4](#0-3) 

## Impact Explanation

This issue qualifies as **Medium severity** under the resource exhaustion category. Each leaked task consumes:
- One tokio runtime task slot
- Memory for `Arc<Mutex<Option<Waker>>>`
- CPU time for sleep timer management

During normal operation, the impact is minimal as broadcasts complete naturally. However, during coordinator shutdown or rapid peer churn scenarios, multiple tasks may leak simultaneously. While each leak is temporary (tasks complete after their deadline), in a node restart scenario with many pending broadcasts, this could briefly exhaust tokio runtime capacity.

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability triggers automatically during:
1. **Coordinator shutdown**: When the mempool coordinator stops, all pending broadcasts in `FuturesUnordered` are dropped
2. **Peer disconnection edge cases**: When peers disconnect while broadcasts are scheduled

Given that node restarts occur during upgrades or failures, and peer churn is normal in P2P networks, this issue will occur regularly in production environments. However, the temporary nature of the leak limits its severity.

## Recommendation

Implement a `Drop` handler for `ScheduledBroadcast` that cancels the spawned tokio task using `JoinHandle`:

```rust
pub(crate) struct ScheduledBroadcast {
    deadline: Instant,
    peer: PeerNetworkId,
    backoff: bool,
    waker: Arc<Mutex<Option<Waker>>>,
    task_handle: Option<tokio::task::JoinHandle<()>>,
}

impl ScheduledBroadcast {
    pub fn new(deadline: Instant, peer: PeerNetworkId, backoff: bool, executor: Handle) -> Self {
        let waker: Arc<Mutex<Option<Waker>>> = Arc::new(Mutex::new(None));
        let waker_clone = waker.clone();

        let task_handle = if deadline > Instant::now() {
            let tokio_instant = tokio::time::Instant::from_std(deadline);
            Some(executor.spawn(async move {
                tokio::time::sleep_until(tokio_instant).await;
                let mut waker = waker_clone.lock();
                if let Some(waker) = waker.take() {
                    waker.wake()
                }
            }))
        } else {
            None
        };

        Self {
            deadline,
            peer,
            backoff,
            waker,
            task_handle,
        }
    }
}

impl Drop for ScheduledBroadcast {
    fn drop(&mut self) {
        if let Some(handle) = self.task_handle.take() {
            handle.abort();
        }
    }
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_scheduled_broadcast_task_leak() {
    use std::sync::Arc;
    use std::sync::atomic::{AtomicUsize, Ordering};
    use std::time::{Duration, Instant};
    use tokio::runtime::Handle;
    use aptos_config::network_id::{NetworkId, PeerNetworkId};
    use aptos_types::PeerId;

    let executor = Handle::current();
    let peer = PeerNetworkId::new(NetworkId::Validator, PeerId::random());
    let task_counter = Arc::new(AtomicUsize::new(0));
    let counter_clone = task_counter.clone();

    // Spawn a monitoring task to track active tasks
    tokio::spawn(async move {
        loop {
            counter_clone.fetch_add(1, Ordering::SeqCst);
            tokio::time::sleep(Duration::from_millis(100)).await;
        }
    });

    // Create and immediately drop ScheduledBroadcast
    let deadline = Instant::now() + Duration::from_secs(30);
    {
        let _broadcast = ScheduledBroadcast::new(deadline, peer, false, executor.clone());
        // broadcast is dropped here
    }

    // Wait and verify the spawned task is still running
    tokio::time::sleep(Duration::from_millis(500)).await;
    
    // The leaked task should still be sleeping
    // After 30 seconds, it will complete, but until then it consumes resources
    assert!(task_counter.load(Ordering::SeqCst) > 0, "Task leaked and still running");
}
```

## Notes

While this is a genuine resource management issue, its security impact is limited:
- Leaks are temporary (tasks complete after deadline)
- Primarily affects shutdown scenarios
- Each leaked task is small
- No direct attack vector for unprivileged actors

However, proper resource cleanup is a best practice that should be implemented to prevent accumulation of leaked tasks during node restarts or high peer churn.

### Citations

**File:** mempool/src/shared_mempool/types.rs (L133-154)
```rust
    pub fn new(deadline: Instant, peer: PeerNetworkId, backoff: bool, executor: Handle) -> Self {
        let waker: Arc<Mutex<Option<Waker>>> = Arc::new(Mutex::new(None));
        let waker_clone = waker.clone();

        if deadline > Instant::now() {
            let tokio_instant = tokio::time::Instant::from_std(deadline);
            executor.spawn(async move {
                tokio::time::sleep_until(tokio_instant).await;
                let mut waker = waker_clone.lock();
                if let Some(waker) = waker.take() {
                    waker.wake()
                }
            });
        }

        Self {
            deadline,
            peer,
            backoff,
            waker,
        }
    }
```

**File:** mempool/src/shared_mempool/coordinator.rs (L83-83)
```rust
    let mut scheduled_broadcasts = FuturesUnordered::new();
```

**File:** mempool/src/shared_mempool/tasks.rs (L116-121)
```rust
    scheduled_broadcasts.push(ScheduledBroadcast::new(
        Instant::now() + Duration::from_millis(interval_ms),
        peer,
        schedule_backoff,
        executor,
    ))
```

**File:** config/src/config/mempool_config.rs (L111-112)
```rust
            shared_mempool_tick_interval_ms: 10,
            shared_mempool_backoff_interval_ms: 30_000,
```
