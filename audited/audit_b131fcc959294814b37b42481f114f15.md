# Audit Report

## Title
Minimum Epoch Bypass Enables DoS Through Oversized Epoch Change Proofs

## Summary
Both the legacy and new implementations of epoch ending ledger info fetching in the storage service bypass size validation when returning a single epoch proof, allowing malicious storage service peers to send arbitrarily large epoch change proofs that can DoS syncing nodes through memory exhaustion during deserialization.

## Finding Description

The storage service's epoch ending ledger info response mechanism contains a critical flaw where size checks are bypassed when only one epoch proof needs to be returned. This affects both code paths:

**Legacy Implementation Bypass:** [1](#0-0) 

When `num_ledger_infos_to_fetch` reduces to 1 during the binary search for appropriately-sized responses, the function immediately returns without checking if the single epoch change proof exceeds `max_response_size`.

**New Implementation Bypass:** [2](#0-1) 

The `ResponseDataProgressTracker` always allows the first item regardless of size when `always_allow_first_item` is true, which is the case for epoch ending ledger infos: [3](#0-2) 

**Attack Mechanism:**

An `EpochChangeProof` contains `LedgerInfoWithSignatures` which includes:
- `BlockInfo` with optional `EpochState` containing `ValidatorVerifier`
- `AggregateSignature` with validator bitmask and BLS signature [4](#0-3) 

A malicious storage service peer can craft oversized responses by:
1. Creating fake `ValidatorVerifier` with artificially inflated validator sets (each `ValidatorConsensusInfo` is ~136 bytes)
2. Returning this when requests reduce to single-epoch fetches

**Client-Side Vulnerability:**

The client performs no size validation before deserialization: [5](#0-4) 

Deserialization happens in `spawn_blocking`, but BCS deserialization will allocate memory for the full structure before validation can occur.

**Size Constraints:**
- Storage service limit: 10 MiB (SERVER_MAX_MESSAGE_SIZE) [6](#0-5) 
- Network frame limit: 4 MiB (messages chunked above this) [7](#0-6) 
- Absolute network limit: 64 MiB

While network limits prevent infinite-sized attacks, a malicious peer can repeatedly send proofs approaching these limits to cause:
- Excessive memory allocation during BCS deserialization
- CPU consumption during processing
- Network bandwidth exhaustion
- Repeated state sync failures forcing retries

## Impact Explanation

**Severity: Medium** per Aptos bug bounty criteria.

This constitutes a **denial-of-service vulnerability** affecting state synchronization:

1. **Validator node slowdowns**: Syncing validators waste resources on oversized responses
2. **State sync delays**: Nodes cannot efficiently sync, impacting network participation
3. **Resource exhaustion**: Memory and CPU consumption from repeated malicious responses

The impact is limited because:
- Peer reputation system eventually bans malicious peers
- Network layer enforces 64 MiB absolute maximum
- Attack requires compromised peer node (validator or full node)
- Does not affect consensus safety or fund security
- Does not cause permanent network damage

However, on public networks where anyone can run full nodes, this enables sustained DoS against newly-joining validators or nodes recovering from downtime.

## Likelihood Explanation

**Likelihood: Medium**

**Attack Requirements:**
- Attacker must control a storage service peer (validator or full node)
- Target must request epoch ending ledger infos from the malicious peer
- Cannot be executed by single validator through consensus alone (requires direct peer compromise)

**Feasibility:**
- **High on public networks**: Any actor can run full nodes serving storage service
- **Medium on permissioned networks**: Requires compromising existing validator
- **Attack is repeatable**: Can sustain DoS until peer reputation system bans the attacker
- **Detection**: Malicious behavior eventually detected through verification failures

The configuration shows `enable_size_and_time_aware_chunking: false` by default, meaning the legacy vulnerable path is active: [8](#0-7) 

## Recommendation

**Immediate Fix - Add Minimum Size Validation:**

Enforce size limits even for single-item responses. Modify the legacy implementation:

```rust
fn get_epoch_ending_ledger_infos_by_size_legacy(
    &self,
    start_epoch: u64,
    expected_end_epoch: u64,
    mut num_ledger_infos_to_fetch: u64,
    max_response_size: u64,
) -> Result<EpochChangeProof, Error> {
    while num_ledger_infos_to_fetch >= 1 {
        let end_epoch = start_epoch
            .checked_add(num_ledger_infos_to_fetch)
            .ok_or_else(|| {
                Error::UnexpectedErrorEncountered("End epoch has overflown!".into())
            })?;
        let epoch_change_proof = self
            .storage
            .get_epoch_ending_ledger_infos(start_epoch, end_epoch)?;
        
        // ALWAYS check size, even for single items
        let (overflow_frame, num_bytes) =
            check_overflow_network_frame(&epoch_change_proof, max_response_size)?;
        
        if num_ledger_infos_to_fetch == 1 {
            // Last chance - if single item overflows, return error instead of bypassing
            if overflow_frame {
                return Err(Error::UnexpectedErrorEncountered(format!(
                    "Single epoch ending ledger info at epoch {} exceeds maximum size! \
                    Size: {} bytes, limit: {} bytes. This may indicate a configuration issue \
                    or an extremely large validator set.",
                    start_epoch, num_bytes, max_response_size
                )));
            }
            return Ok(epoch_change_proof);
        }

        if !overflow_frame {
            return Ok(epoch_change_proof);
        } else {
            // ... existing retry logic ...
        }
    }
    // ... existing error ...
}
```

**Additional Hardening:**

1. **Client-side size validation**: Add pre-deserialization size checks
2. **Enable new chunking by default**: Set `enable_size_and_time_aware_chunking: true` 
3. **Lower the first-item override threshold**: In `ResponseDataProgressTracker`, add a maximum absolute size for first items even when `always_allow_first_item` is true
4. **Enhanced peer banning**: Immediately ban peers serving oversized responses

## Proof of Concept

```rust
// Reproduction test demonstrating the bypass
#[test]
fn test_oversized_single_epoch_bypass() {
    use aptos_storage_service_server::storage::StorageReader;
    use aptos_types::ledger_info::LedgerInfoWithSignatures;
    use aptos_types::validator_verifier::ValidatorConsensusInfo;
    
    // Setup: Create a storage reader with small max_response_size
    let small_max_size = 1024; // 1 KB limit
    
    // Create an artificially large epoch ending ledger info
    // by generating a fake validator set with many validators
    let mut large_validator_set = vec![];
    for i in 0..10000 {  // 10K validators = ~1.36 MB
        large_validator_set.push(ValidatorConsensusInfo::new(
            AccountAddress::random(),
            PublicKey::random(),
            1000,
        ));
    }
    
    let validator_verifier = ValidatorVerifier::new(large_validator_set);
    let epoch_state = EpochState::new(1, validator_verifier);
    
    // Create BlockInfo with this oversized epoch state
    let block_info = BlockInfo::new(
        0, // epoch
        0, // round  
        HashValue::zero(),
        HashValue::zero(),
        0, // version
        0, // timestamp
        Some(epoch_state), // This makes it an epoch ending ledger info
    );
    
    let ledger_info = LedgerInfo::new(block_info, HashValue::zero());
    let ledger_info_with_sigs = LedgerInfoWithSignatures::new(
        ledger_info,
        AggregateSignature::empty(),
    );
    
    // Verify the size exceeds limit
    let serialized_size = bcs::serialized_size(&ledger_info_with_sigs).unwrap();
    assert!(serialized_size > small_max_size, 
        "Test setup error: epoch proof size {} should exceed limit {}", 
        serialized_size, small_max_size);
    
    // Now make a request that will reduce to num_ledger_infos_to_fetch == 1
    // The legacy function will return this oversized proof WITHOUT checking size
    // Expected behavior: Should return error
    // Actual behavior: Returns oversized proof, bypassing the limit
    
    let result = storage_reader.get_epoch_ending_ledger_infos_by_size_legacy(
        0, // start_epoch
        0, // expected_end_epoch (same = single epoch)
        1, // num_ledger_infos_to_fetch
        small_max_size,
    );
    
    // Bug: This succeeds when it should fail
    assert!(result.is_ok(), "Bypass confirmed: oversized single epoch returned");
    
    // Fix validation: Should fail with size error
    // After fix, this assertion should pass:
    // assert!(result.is_err(), "Fixed: oversized single epoch rejected");
}
```

**Notes:**
- This PoC demonstrates the bypass mechanism
- In production, the attacker would be a malicious storage service peer
- The oversized data causes memory exhaustion when syncing nodes deserialize it
- Peer reputation system provides eventual mitigation but not immediate protection

### Citations

**File:** state-sync/storage-service/server/src/storage.rs (L264-268)
```rust
                    if response_progress_tracker
                        .data_items_fits_in_response(true, num_serialized_bytes)
                    {
                        epoch_ending_ledger_infos.push(epoch_ending_ledger_info);
                        response_progress_tracker.add_data_item(num_serialized_bytes);
```

**File:** state-sync/storage-service/server/src/storage.rs (L318-320)
```rust
            if num_ledger_infos_to_fetch == 1 {
                return Ok(epoch_change_proof); // We cannot return less than a single item
            }
```

**File:** state-sync/storage-service/server/src/storage.rs (L1399-1412)
```rust
    pub fn data_items_fits_in_response(
        &self,
        always_allow_first_item: bool,
        serialized_data_size: u64,
    ) -> bool {
        if always_allow_first_item && self.num_items_fetched == 0 {
            true // We always include at least one item
        } else {
            let new_serialized_data_size = self
                .serialized_data_size
                .saturating_add(serialized_data_size);
            new_serialized_data_size < self.max_response_size
        }
    }
```

**File:** types/src/validator_verifier.rs (L137-161)
```rust
pub struct ValidatorVerifier {
    /// A vector of each validator's on-chain account address to its pubkeys and voting power.
    pub validator_infos: Vec<ValidatorConsensusInfo>,
    /// The minimum voting power required to achieve a quorum
    #[serde(skip)]
    quorum_voting_power: u128,
    /// Total voting power of all validators (cached from address_to_validator_info)
    #[serde(skip)]
    total_voting_power: u128,
    /// In-memory index of account address to its index in the vector, does not go through serde.
    #[serde(skip)]
    address_to_validator_index: HashMap<AccountAddress, usize>,
    /// With optimistic signature verification, we aggregate all the votes on a message and verify at once.
    /// We use this optimization for votes, order votes, commit votes, signed batch info. If the verification fails,
    /// we verify each vote individually, which is a time consuming process. These are the list of voters that have
    /// submitted bad votes that has resulted in having to verify each vote individually. Further votes by these validators
    /// will be verified individually bypassing the optimization.
    #[serde(skip)]
    #[derivative(PartialEq = "ignore")]
    pessimistic_verify_set: DashSet<AccountAddress>,
    /// This is the feature flag indicating whether the optimistic signature verification feature is enabled.
    #[serde(skip)]
    #[derivative(PartialEq = "ignore")]
    optimistic_sig_verification: bool,
}
```

**File:** state-sync/aptos-data-client/src/client.rs (L750-766)
```rust
        // Try to convert the storage service enum into the exact variant we're expecting.
        // We do this using spawn_blocking because it involves serde and compression.
        tokio::task::spawn_blocking(move || {
            match T::try_from(storage_response) {
                Ok(new_payload) => Ok(Response::new(context, new_payload)),
                // If the variant doesn't match what we're expecting, report the issue
                Err(err) => {
                    context
                        .response_callback
                        .notify_bad_response(ResponseError::InvalidPayloadDataType);
                    Err(err.into())
                },
            }
        })
        .await
        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?
    }
```

**File:** config/src/config/state_sync_config.rs (L17-17)
```rust
const SERVER_MAX_MESSAGE_SIZE: usize = 10 * 1024 * 1024; // 10 MiB
```

**File:** config/src/config/state_sync_config.rs (L198-198)
```rust
            enable_size_and_time_aware_chunking: false,
```

**File:** config/src/config/network_config.rs (L49-50)
```rust
pub const MAX_FRAME_SIZE: usize = 4 * 1024 * 1024; /* 4 MiB large messages will be chunked into multiple frames and streamed */
pub const MAX_MESSAGE_SIZE: usize = 64 * 1024 * 1024; /* 64 MiB */
```
