# Audit Report

## Title
Unvalidated `batch_size` Parameter in Internal Indexer Configuration Allows Node Denial of Service

## Summary
The `InternalIndexerDBConfig::new()` constructor and its `ConfigSanitizer` implementation fail to validate the `batch_size` parameter, allowing values of 0 (causing indexer liveness failure) or extremely large values (causing memory exhaustion and node crashes).

## Finding Description

The `InternalIndexerDBConfig` struct contains a `batch_size` field that controls how many transactions are processed per batch in the internal indexer. This parameter has no validation in two critical locations:

**1. Constructor Missing Validation:** [1](#0-0) 

The constructor accepts any `usize` value without bounds checking.

**2. ConfigSanitizer Missing Validation:** [2](#0-1) 

The sanitizer only validates storage sharding requirements but ignores `batch_size` entirely, despite being called during config loading: [3](#0-2) 

**3. Critical Usage in Batch Processing:** [4](#0-3) 

When `batch_size = 0`, the function returns 0 transactions to process, causing the indexer to never make progress: [5](#0-4) 

When the processing loop receives 0 transactions, it breaks immediately at line 401-404 without indexing anything.

**4. Memory Exhaustion with Large Values:** [6](#0-5) 

With extremely large `batch_size` values (e.g., 1,000,000+), the code attempts to load all transactions into a single `SchemaBatch`, which stores data in a `HashMap<ColumnFamilyName, Vec<WriteOp>>`: [7](#0-6) 

Each transaction can generate multiple write operations containing transaction data, events, and state keys, leading to gigabytes of memory consumption.

## Impact Explanation

**High Severity** - This issue qualifies under the "Validator node slowdowns" and "API crashes" categories:

1. **batch_size = 0**: Complete internal indexer liveness failure. The indexer cannot process any transactions, breaking API query functionality that depends on indexed data. While consensus continues normally, this severely degrades node utility.

2. **batch_size > 1,000,000**: Memory exhaustion leading to Out-Of-Memory (OOM) crashes. A node attempting to index millions of transactions in a single batch will exhaust available memory, causing the node process to crash.

The issue breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits."

## Likelihood Explanation

**Medium Likelihood** - While the configuration is operator-controlled (not remotely exploitable by unprivileged attackers), this issue can manifest through:

1. Honest operator mistakes when manually editing `node.yaml` configuration files
2. Automated deployment scripts with incorrect default values
3. Configuration inheritance from corrupted or outdated templates

The default value (10,000) is safe, but nothing prevents dangerous overrides during node deployment or upgrades.

## Recommendation

Add validation in the `ConfigSanitizer::sanitize()` implementation:

```rust
impl ConfigSanitizer for InternalIndexerDBConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let config = node_config.indexer_db_config;

        // Validate batch_size bounds
        const MIN_BATCH_SIZE: usize = 1;
        const MAX_BATCH_SIZE: usize = 100_000;
        
        if config.batch_size < MIN_BATCH_SIZE {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name.clone(),
                format!(
                    "batch_size must be at least {}. Got: {}",
                    MIN_BATCH_SIZE, config.batch_size
                ),
            ));
        }
        
        if config.batch_size > MAX_BATCH_SIZE {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name.clone(),
                format!(
                    "batch_size must not exceed {} to prevent memory exhaustion. Got: {}",
                    MAX_BATCH_SIZE, config.batch_size
                ),
            ));
        }

        // Existing validation
        if !node_config.storage.rocksdb_configs.enable_storage_sharding
            && config.is_internal_indexer_db_enabled()
        {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "Don't turn on internal indexer db if DB sharding is off".into(),
            ));
        }

        Ok(())
    }
}
```

## Proof of Concept

**Test Case 1: Zero batch_size causes indexer to stall**

Create a config file `node_zero_batch.yaml`:
```yaml
indexer_db_config:
  enable_transaction: true
  enable_event: true
  enable_statekeys: true
  batch_size: 0  # Invalid value
```

Start node with this config - the internal indexer will never process any transactions.

**Test Case 2: Excessive batch_size causes OOM**

Create a config file `node_huge_batch.yaml`:
```yaml
indexer_db_config:
  enable_transaction: true
  enable_event: true
  enable_statekeys: true
  batch_size: 10000000  # 10 million - will exhaust memory
```

When the indexer attempts to process 10 million transactions in one batch, memory usage will spike to multiple gigabytes, likely crashing the node process.

**Rust Test to Add:**

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use crate::config::{ConfigSanitizer, NodeConfig};
    
    #[test]
    fn test_batch_size_zero_rejected() {
        let mut node_config = NodeConfig::default();
        node_config.indexer_db_config.batch_size = 0;
        
        let result = InternalIndexerDBConfig::sanitize(
            &node_config,
            NodeType::Validator,
            None
        );
        
        assert!(result.is_err());
        assert!(matches!(result.unwrap_err(), Error::ConfigSanitizerFailed(_, _)));
    }
    
    #[test]
    fn test_batch_size_excessive_rejected() {
        let mut node_config = NodeConfig::default();
        node_config.indexer_db_config.batch_size = 10_000_000;
        
        let result = InternalIndexerDBConfig::sanitize(
            &node_config,
            NodeType::Validator,
            None
        );
        
        assert!(result.is_err());
        assert!(matches!(result.unwrap_err(), Error::ConfigSanitizerFailed(_, _)));
    }
    
    #[test]
    fn test_batch_size_valid_accepted() {
        let mut node_config = NodeConfig::default();
        node_config.indexer_db_config.batch_size = 10_000;
        
        let result = InternalIndexerDBConfig::sanitize(
            &node_config,
            NodeType::Validator,
            None
        );
        
        assert!(result.is_ok());
    }
}
```

**Notes**

While this vulnerability requires operator-level access to the configuration files (not remotely exploitable by unprivileged attackers), it represents a significant defensive programming failure. The lack of input validation violates the principle of defense-in-depth and can lead to operational failures during deployment, upgrades, or configuration management at scale. The issue is particularly relevant given that configuration files may be generated programmatically or inherited from templates, increasing the risk of accidental misconfigurations propagating across multiple nodes.

### Citations

**File:** config/src/config/internal_indexer_db_config.rs (L22-38)
```rust
    pub fn new(
        enable_transaction: bool,
        enable_event: bool,
        enable_event_v2_translation: bool,
        event_v2_translation_ignores_below_version: u64,
        enable_statekeys: bool,
        batch_size: usize,
    ) -> Self {
        Self {
            enable_transaction,
            enable_event,
            enable_event_v2_translation,
            event_v2_translation_ignores_below_version,
            enable_statekeys,
            batch_size,
        }
    }
```

**File:** config/src/config/internal_indexer_db_config.rs (L82-103)
```rust
impl ConfigSanitizer for InternalIndexerDBConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let config = node_config.indexer_db_config;

        // Shouldn't turn on internal indexer for db without sharding
        if !node_config.storage.rocksdb_configs.enable_storage_sharding
            && config.is_internal_indexer_db_enabled()
        {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "Don't turn on internal indexer db if DB sharding is off".into(),
            ));
        }

        Ok(())
    }
}
```

**File:** config/src/config/config_sanitizer.rs (L66-66)
```rust
        InternalIndexerDBConfig::sanitize(node_config, node_type, chain_id)?;
```

**File:** storage/indexer/src/db_indexer.rs (L382-394)
```rust
    fn get_num_of_transactions(&self, version: Version, end_version: Version) -> Result<u64> {
        let highest_version = min(self.main_db_reader.ensure_synced_version()?, end_version);
        if version > highest_version {
            // In case main db is not synced yet or recreated
            return Ok(0);
        }
        // we want to include the last transaction since the iterator interface will is right exclusive.
        let num_of_transaction = min(
            self.indexer_db.config.batch_size as u64,
            highest_version + 1 - version,
        );
        Ok(num_of_transaction)
    }
```

**File:** storage/indexer/src/db_indexer.rs (L397-407)
```rust
    pub fn process(&self, start_version: Version, end_version: Version) -> Result<Version> {
        let mut version = start_version;
        while version < end_version {
            let next_version = self.process_a_batch(version, end_version)?;
            if next_version == version {
                break;
            }
            version = next_version;
        }
        Ok(version)
    }
```

**File:** storage/indexer/src/db_indexer.rs (L410-550)
```rust
    pub fn process_a_batch(&self, start_version: Version, end_version: Version) -> Result<Version> {
        let _timer: aptos_metrics_core::HistogramTimer = TIMER.timer_with(&["process_a_batch"]);
        let mut version = start_version;
        let num_transactions = self.get_num_of_transactions(version, end_version)?;
        // This promises num_transactions should be readable from main db
        let mut db_iter = self.get_main_db_iter(version, num_transactions)?;
        let mut batch = SchemaBatch::new();
        let mut event_keys: HashSet<EventKey> = HashSet::new();
        db_iter.try_for_each(|res| {
            let (txn, events, writeset) = res?;
            if let Some(signed_txn) = txn.try_as_signed_user_txn() {
                if self.indexer_db.transaction_enabled() {
                    if let ReplayProtector::SequenceNumber(seq_num) = signed_txn.replay_protector()
                    {
                        batch.put::<OrderedTransactionByAccountSchema>(
                            &(signed_txn.sender(), seq_num),
                            &version,
                        )?;
                    }
                }
            }

            if self.indexer_db.event_enabled() {
                events.iter().enumerate().try_for_each(|(idx, event)| {
                    if let ContractEvent::V1(v1) = event {
                        batch
                            .put::<EventByKeySchema>(
                                &(*v1.key(), v1.sequence_number()),
                                &(version, idx as u64),
                            )
                            .expect("Failed to put events by key to a batch");
                        batch
                            .put::<EventByVersionSchema>(
                                &(*v1.key(), version, v1.sequence_number()),
                                &(idx as u64),
                            )
                            .expect("Failed to put events by version to a batch");
                    }
                    if self.indexer_db.event_v2_translation_enabled() {
                        if let ContractEvent::V2(v2) = event {
                            if let Some(translated_v1_event) =
                                self.translate_event_v2_to_v1(v2).map_err(|e| {
                                    anyhow::anyhow!(
                                        "Failed to translate event: {:?}. Error: {}",
                                        v2,
                                        e
                                    )
                                })?
                            {
                                let key = *translated_v1_event.key();
                                let sequence_number = translated_v1_event.sequence_number();
                                self.event_v2_translation_engine
                                    .cache_sequence_number(&key, sequence_number);
                                event_keys.insert(key);
                                batch
                                    .put::<EventByKeySchema>(
                                        &(key, sequence_number),
                                        &(version, idx as u64),
                                    )
                                    .expect("Failed to put events by key to a batch");
                                batch
                                    .put::<EventByVersionSchema>(
                                        &(key, version, sequence_number),
                                        &(idx as u64),
                                    )
                                    .expect("Failed to put events by version to a batch");
                                batch
                                    .put::<TranslatedV1EventSchema>(
                                        &(version, idx as u64),
                                        &translated_v1_event,
                                    )
                                    .expect("Failed to put translated v1 events to a batch");
                            }
                        }
                    }
                    Ok::<(), AptosDbError>(())
                })?;
            }

            if self.indexer_db.statekeys_enabled() {
                writeset.write_op_iter().for_each(|(state_key, write_op)| {
                    if write_op.is_creation() || write_op.is_modification() {
                        batch
                            .put::<StateKeysSchema>(state_key, &())
                            .expect("Failed to put state keys to a batch");
                    }
                });
            }
            version += 1;
            Ok::<(), AptosDbError>(())
        })?;
        assert!(version > 0, "batch number should be greater than 0");

        assert_eq!(num_transactions, version - start_version);

        if self.indexer_db.event_v2_translation_enabled() {
            batch.put::<InternalIndexerMetadataSchema>(
                &MetadataKey::EventV2TranslationVersion,
                &MetadataValue::Version(version - 1),
            )?;

            for event_key in event_keys {
                batch
                    .put::<EventSequenceNumberSchema>(
                        &event_key,
                        &self
                            .event_v2_translation_engine
                            .get_cached_sequence_number(&event_key)
                            .unwrap_or(0),
                    )
                    .expect("Failed to put events by key to a batch");
            }
        }

        if self.indexer_db.transaction_enabled() {
            batch.put::<InternalIndexerMetadataSchema>(
                &MetadataKey::TransactionVersion,
                &MetadataValue::Version(version - 1),
            )?;
        }
        if self.indexer_db.event_enabled() {
            batch.put::<InternalIndexerMetadataSchema>(
                &MetadataKey::EventVersion,
                &MetadataValue::Version(version - 1),
            )?;
        }
        if self.indexer_db.statekeys_enabled() {
            batch.put::<InternalIndexerMetadataSchema>(
                &MetadataKey::StateVersion,
                &MetadataValue::Version(version - 1),
            )?;
        }
        batch.put::<InternalIndexerMetadataSchema>(
            &MetadataKey::LatestVersion,
            &MetadataValue::Version(version - 1),
        )?;
        self.sender
            .send(Some(batch))
            .map_err(|e| AptosDbError::Other(e.to_string()))?;
        Ok(version)
    }
```

**File:** storage/schemadb/src/batch.rs (L127-133)
```rust
/// `SchemaBatch` holds a collection of updates that can be applied to a DB atomically. The updates
/// will be applied in the order in which they are added to the `SchemaBatch`.
#[derive(Debug, Default)]
pub struct SchemaBatch {
    rows: DropHelper<HashMap<ColumnFamilyName, Vec<WriteOp>>>,
    stats: SampledBatchStats,
}
```
