# Audit Report

## Title
Epoch Isolation Violation in JWK Consensus: Stale Observations Signed with Old Epoch Keys Can Be Executed Across Epoch Boundaries

## Summary
The `process_new_observation()` function in the JWK consensus manager does not validate that its `epoch_state` matches the current epoch before signing observations and starting consensus. This allows observations signed with old epoch validator keys to be executed in subsequent epochs, violating the critical epoch isolation security invariant.

## Finding Description

The vulnerability exists in the JWK consensus system's handling of epoch boundaries. The `IssuerLevelConsensusManager::process_new_observation()` function accepts observations from JWKObserver threads and signs them using `self.consensus_key` without validating that `self.epoch_state` represents the current active epoch. [1](#0-0) 

During epoch transitions, the following sequence creates a vulnerability:

1. **Epoch N** is active with validator set V_N and consensus keys K_N
2. JWKManager_N processes observations and signs them with keys from K_N
3. **Epoch N+1 transition initiates** - EpochManager updates to new epoch state
4. **Race window exists**: Old JWKManager_N continues running until shutdown completes
5. JWKManager_N can still receive observations from JWKObserver threads and sign them with old epoch N keys
6. These observations get aggregated into `QuorumCertifiedUpdate` objects using epoch N signatures
7. Updates are placed into the validator transaction pool [2](#0-1) 

The critical issue is that `QuorumCertifiedUpdate` contains **no epoch field**: [3](#0-2) 

When these updates are executed on-chain, the verification in `process_jwk_update_inner` uses the **current** validator set, not the validator set from when the signatures were created: [4](#0-3) 

The verification fetches the current ValidatorSet (line 109-110) and uses it to verify signatures that were created in a previous epoch (line 140-142). There is **no epoch validation check** - the system cannot distinguish between observations signed in epoch N versus epoch N+1 versus epoch N+K.

Furthermore, the `ValidatorTransaction::verify()` method for `ObservedJWKUpdate` performs **no verification**: [5](#0-4) 

The validator transaction pool also performs no epoch validation when transactions are added: [6](#0-5) 

**Attack Scenario:**

1. During epoch N, validators observe and sign JWK updates using epoch N keys
2. Epoch N+1 begins - some validators' keys are rotated or they are removed from the validator set
3. The old `QuorumCertifiedUpdate` (signed in epoch N) remains in the transaction pool or is about to be included in a block
4. The update gets executed in epoch N+1
5. Verification uses the epoch N+1 validator set
6. If sufficient validators from epoch N remain in epoch N+1 with the same keys, the old signatures validate successfully
7. The system accepts an observation that was signed with old, potentially compromised epoch keys

This violates the **epoch isolation invariant**: Each epoch should have independent consensus processes, and observations from epoch N should not be executable in epoch N+1. This is critical because:
- Compromised validator keys should be invalidated at epoch boundaries
- Malicious validators can be removed at epoch boundaries  
- The security model assumes epoch boundaries provide a clean break for consensus state

## Impact Explanation

**Severity: Critical** (potential for consensus safety violations)

This vulnerability breaks the fundamental **epoch isolation** security invariant of AptosBFT consensus. Per the Aptos critical invariants:
- **Invariant #2 (Consensus Safety)**: This allows consensus decisions from one epoch to affect another epoch using stale cryptographic material
- **Invariant #10 (Cryptographic Correctness)**: Signatures from old epochs with potentially rotated/compromised keys can be accepted

Potential impacts:
- **Consensus Safety Violation**: If validator keys are compromised in epoch N and rotated in epoch N+1, observations signed with compromised keys could still be executed
- **Malicious Validator Persistence**: Validators removed from the set in epoch N+1 due to malicious behavior can still have their epoch N signatures accepted
- **Temporal Confusion**: No way to determine when an observation was actually made, enabling replay-like attacks across epoch boundaries
- **Key Rotation Bypass**: The security benefit of key rotation at epoch boundaries is undermined

While the JWKs themselves represent legitimate OIDC provider state, the lack of temporal binding and epoch validation violates the security model's assumption that epoch boundaries provide isolation.

## Likelihood Explanation

**Likelihood: High during every epoch transition**

This vulnerability manifests during **every epoch transition** due to the race condition window:
1. JWKObserver threads run independently and continue fetching JWKs
2. The shutdown sequence is asynchronous - observations can arrive during shutdown
3. The `tokio::select!` in the main loop is unbiased and may process observations even after epoch transition begins

The likelihood that stale observations are signed and submitted:
- **Guaranteed to occur** if JWKObserver fetches new data during the epoch transition window (typically seconds)
- **High probability** in production where JWKObservers poll every 10 seconds [7](#0-6) 

The likelihood these stale observations get executed successfully:
- **High** if validator set composition remains similar between epochs (common case)
- **Medium-High** even with moderate validator set changes, as only quorum is needed
- **Guaranteed exploitable** by validators who intentionally hold observations to submit across epoch boundaries

## Recommendation

**Add explicit epoch validation throughout the JWK consensus pipeline:**

1. **Add epoch field to `QuorumCertifiedUpdate`:**
```rust
pub struct QuorumCertifiedUpdate {
    pub epoch: u64,  // Add this field
    pub update: ProviderJWKs,
    pub multi_sig: AggregateSignature,
}
```

2. **Validate epoch in `process_new_observation()`:**
```rust
pub fn process_new_observation(
    &mut self,
    issuer: Issuer,
    jwks: Vec<JWKMoveStruct>,
) -> Result<()> {
    // Add epoch validation
    if self.stopped {
        bail!("Manager is stopped, rejecting observation");
    }
    
    // ... rest of the function
}
```

3. **Add epoch check in `process_jwk_update_inner`:**
```rust
fn process_jwk_update_inner(
    &self,
    resolver: &impl AptosMoveResolver,
    module_storage: &impl AptosModuleStorage,
    log_context: &AdapterLogSchema,
    session_id: SessionId,
    update: jwks::QuorumCertifiedUpdate,
) -> Result<(VMStatus, VMOutput), ExecutionFailure> {
    // Load resources
    let validator_set = ValidatorSet::fetch_config(resolver)
        .ok_or(Expected(MissingResourceValidatorSet))?;
    
    // Add epoch validation
    let current_epoch = validator_set.epoch();
    if update.epoch != current_epoch {
        return Err(Expected(IncorrectEpoch));
    }
    
    // ... rest of verification
}
```

4. **Implement `ValidatorTransaction::verify()` properly:**
```rust
pub fn verify(&self, verifier: &ValidatorVerifier, current_epoch: u64) -> anyhow::Result<()> {
    match self {
        ValidatorTransaction::DKGResult(dkg_result) => dkg_result
            .verify(verifier)
            .context("DKGResult verification failed"),
        ValidatorTransaction::ObservedJWKUpdate(update) => {
            ensure!(
                update.epoch == current_epoch,
                "ObservedJWKUpdate epoch mismatch"
            );
            // Additional validation can be added here
            Ok(())
        },
    }
}
```

5. **Clear transaction pool on epoch transitions:**
Ensure the validator transaction pool is explicitly cleared when epoch changes, not just relying on TxnGuard drops.

## Proof of Concept

The following scenario demonstrates the vulnerability:

**Setup:**
1. Epoch N with validators V1, V2, V3 (quorum = 2)
2. V1's consensus key K1_N
3. JWKObserver observes JWKs = [JWK_A]

**Exploitation Steps:**

```rust
// Step 1: In Epoch N - Observer fetches JWKs
// JWKObserver periodically fetches and sends observation
// (issuer="provider", jwks=[JWK_A]) sent to local_observation_rx

// Step 2: JWKManager processes observation in epoch N
// process_new_observation() is called
// Signs with epoch N consensus key (K1_N)
let signature_epoch_N = consensus_key_epoch_N.sign(&observed);
// Creates ObservedUpdate with epoch N signature

// Step 3: Starts consensus with epoch N state
update_certifier.start_produce(
    epoch_state_N.clone(),  // Uses epoch N state
    observed.clone(),
    qc_update_tx.clone(),
);

// Step 4: Epoch transition to N+1
// - V1's key rotates: K1_N -> K1_N+1
// - EpochManager updates to epoch N+1
// - But old JWKManager still running with epoch N state

// Step 5: Quorum certification completes using epoch N signatures
// QuorumCertifiedUpdate created with signatures from epoch N
// NO EPOCH FIELD in QuorumCertifiedUpdate!

// Step 6: Update put into validator transaction pool
vtxn_pool.put(Topic::JWK_CONSENSUS(issuer), Arc::new(txn), None);

// Step 7: Consensus proposes block in epoch N+1 containing this update
// Step 8: Execution in epoch N+1
process_jwk_update_inner(resolver, module_storage, log_context, session_id, update);

// Step 9: Verification uses epoch N+1 validator set
let validator_set = ValidatorSet::fetch_config(resolver).unwrap(); // Epoch N+1 set!
let verifier = ValidatorVerifier::from(&validator_set);

// Step 10: Signature verification with epoch N+1 public keys
// If V1 is still in validator set (even with rotated key), and enough 
// other validators from epoch N remain, the multi-sig validates!
verifier.verify_multi_signatures(&observed, &multi_sig)?;

// Result: Observation signed with epoch N keys is accepted in epoch N+1
// Epoch isolation violated!
```

**Verification:**
1. Run an Aptos testnet with epoch duration of 1 minute
2. Deploy OIDC provider that updates JWKs every 30 seconds
3. Observe JWK consensus logs during epoch transition
4. Verify that observations created in epoch N get executed in epoch N+1
5. Confirm no epoch validation errors despite cross-epoch execution

**Notes**

The vulnerability exists because the JWK consensus system lacks proper epoch tracking and validation. While individual components (EpochManager, observation aggregation) perform some epoch checks on RPC messages, the core data structure (`QuorumCertifiedUpdate`) and execution path have no epoch validation. This creates a fundamental violation of epoch isolation that is exploitable during every epoch transition.

The root cause is architectural: treating JWK observations as "timeless" data when they should be bound to specific epochs for consensus safety. The fix requires adding epoch awareness throughout the pipeline and enforcing strict epoch validation at transaction execution time.

### Citations

**File:** crates/aptos-jwk-consensus/src/jwk_manager/mod.rs (L118-124)
```rust
                        this.epoch_state.epoch,
                        this.my_addr,
                        issuer,
                        config_url,
                        Duration::from_secs(10),
                        local_observation_tx.clone(),
                    )),
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager/mod.rs (L184-228)
```rust
    pub fn process_new_observation(
        &mut self,
        issuer: Issuer,
        jwks: Vec<JWKMoveStruct>,
    ) -> Result<()> {
        debug!(
            epoch = self.epoch_state.epoch,
            issuer = String::from_utf8(issuer.clone()).ok(),
            "Processing new observation."
        );
        let state = self.states_by_issuer.entry(issuer.clone()).or_default();
        state.observed = Some(jwks.clone());
        if state.observed.as_ref() != state.on_chain.as_ref().map(ProviderJWKs::jwks) {
            let observed = ProviderJWKs {
                issuer: issuer.clone(),
                version: state.on_chain_version() + 1,
                jwks,
            };
            let signature = self
                .consensus_key
                .sign(&observed)
                .context("process_new_observation failed with signing error")?;
            let abort_handle = self
                .update_certifier
                .start_produce(
                    self.epoch_state.clone(),
                    observed.clone(),
                    self.qc_update_tx.clone(),
                )
                .context(
                    "process_new_observation failed with update_certifier.start_produce failure",
                )?;
            state.consensus_state = ConsensusState::InProgress {
                my_proposal: ObservedUpdate {
                    author: self.my_addr,
                    observed: observed.clone(),
                    signature,
                },
                abort_handle_wrapper: QuorumCertProcessGuard::new(abort_handle),
            };
            info!("[JWK] update observed, update={:?}", observed);
        }

        Ok(())
    }
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager/mod.rs (L322-359)
```rust
    /// Triggered once the `update_certifier` produced a quorum-certified update.
    pub fn process_quorum_certified_update(&mut self, update: QuorumCertifiedUpdate) -> Result<()> {
        let issuer = update.update.issuer.clone();
        info!(
            epoch = self.epoch_state.epoch,
            issuer = String::from_utf8(issuer.clone()).ok(),
            version = update.update.version,
            "JWKManager processing certified update."
        );
        let state = self.states_by_issuer.entry(issuer.clone()).or_default();
        match &state.consensus_state {
            ConsensusState::InProgress { my_proposal, .. } => {
                //TODO: counters
                let txn = ValidatorTransaction::ObservedJWKUpdate(update.clone());
                let vtxn_guard =
                    self.vtxn_pool
                        .put(Topic::JWK_CONSENSUS(issuer.clone()), Arc::new(txn), None);
                state.consensus_state = ConsensusState::Finished {
                    vtxn_guard,
                    my_proposal: my_proposal.clone(),
                    quorum_certified: update.clone(),
                };
                info!(
                    epoch = self.epoch_state.epoch,
                    issuer = String::from_utf8(issuer).ok(),
                    version = update.update.version,
                    "certified update accepted."
                );
                Ok(())
            },
            _ => Err(anyhow!(
                "qc update not expected for issuer {:?} in state {}",
                String::from_utf8(issuer.clone()),
                state.consensus_state.name()
            )),
        }
    }
}
```

**File:** types/src/jwks/mod.rs (L302-307)
```rust
/// A JWK update in format of `ProviderJWKs` and a multi-signature of it as a quorum certificate.
#[derive(Clone, Debug, Eq, PartialEq, Serialize, Deserialize, CryptoHasher, BCSCryptoHash)]
pub struct QuorumCertifiedUpdate {
    pub update: ProviderJWKs,
    pub multi_sig: AggregateSignature,
}
```

**File:** aptos-move/aptos-vm/src/validator_txns/jwk.rs (L100-143)
```rust
    fn process_jwk_update_inner(
        &self,
        resolver: &impl AptosMoveResolver,
        module_storage: &impl AptosModuleStorage,
        log_context: &AdapterLogSchema,
        session_id: SessionId,
        update: jwks::QuorumCertifiedUpdate,
    ) -> Result<(VMStatus, VMOutput), ExecutionFailure> {
        // Load resources.
        let validator_set =
            ValidatorSet::fetch_config(resolver).ok_or(Expected(MissingResourceValidatorSet))?;
        let observed_jwks =
            ObservedJWKs::fetch_config(resolver).ok_or(Expected(MissingResourceObservedJWKs))?;

        let mut jwks_by_issuer: HashMap<Issuer, ProviderJWKs> =
            observed_jwks.into_providers_jwks().into();
        let issuer = update.update.issuer.clone();
        let on_chain = jwks_by_issuer
            .entry(issuer.clone())
            .or_insert_with(|| ProviderJWKs::new(issuer));
        let verifier = ValidatorVerifier::from(&validator_set);

        let QuorumCertifiedUpdate {
            update: observed,
            multi_sig,
        } = update;

        // Check version.
        if on_chain.version + 1 != observed.version {
            return Err(Expected(IncorrectVersion));
        }

        let authors = multi_sig.get_signers_addresses(&verifier.get_ordered_account_addresses());

        // Check voting power.
        verifier
            .check_voting_power(authors.iter(), true)
            .map_err(|_| Expected(NotEnoughVotingPower))?;

        // Verify multi-sig.
        verifier
            .verify_multi_signatures(&observed, &multi_sig)
            .map_err(|_| Expected(MultiSigVerificationFailed))?;

```

**File:** types/src/validator_txn.rs (L45-52)
```rust
    pub fn verify(&self, verifier: &ValidatorVerifier) -> anyhow::Result<()> {
        match self {
            ValidatorTransaction::DKGResult(dkg_result) => dkg_result
                .verify(verifier)
                .context("DKGResult verification failed"),
            ValidatorTransaction::ObservedJWKUpdate(_) => Ok(()),
        }
    }
```
