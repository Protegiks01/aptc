# Audit Report

## Title
Memory Exhaustion in LedgerPruner Initialization During Catchup Phase

## Summary
The `TransactionAccumulatorPruner` and other ledger sub-pruners can attempt to prune billions of versions in a single database batch during initialization, causing memory exhaustion and node crashes when pruning is enabled on nodes with large version gaps.

## Finding Description

The vulnerability exists in the initialization catchup mechanism of all ledger sub-pruners. When a node enables pruning after running for an extended period without it, the sub-pruners attempt to "catch up" by pruning the entire version gap in a single call, bypassing the batch size limits enforced during normal operation.

**Normal Operation (Safe):**
The `PrunerWorker` calls `LedgerPruner::prune(max_versions)` with a configurable batch size (default 5,000 versions). [1](#0-0) 

The `LedgerPruner` enforces this limit by calculating `current_batch_target_version = min(progress + max_versions as Version, target_version)`, ensuring each iteration processes at most `max_versions` versions. [2](#0-1) 

**Vulnerable Initialization (Unsafe):**
During initialization, `TransactionAccumulatorPruner::new()` directly calls `myself.prune(progress, metadata_progress)` without any batch size limit. [3](#0-2) 

This calls `TransactionAccumulatorDb::prune(begin, end, db_batch)` which iterates through the entire range `begin..end`, accumulating all delete operations in a single `SchemaBatch`. [4](#0-3) 

The `SchemaBatch` stores all operations in memory as `HashMap<ColumnFamilyName, Vec<WriteOp>>`. [5](#0-4) 

Each delete operation pushes a `WriteOp::Deletion` into the vector. [6](#0-5) 

**All seven sub-pruners exhibit this pattern:**
1. EventStorePruner [7](#0-6) 
2. PersistedAuxiliaryInfoPruner
3. TransactionAccumulatorPruner
4. TransactionAuxiliaryDataPruner  
5. TransactionInfoPruner
6. TransactionPruner
7. WriteSetPruner

**Attack Scenario:**
1. Node operator runs a validator/fullnode with pruning disabled for months/years
2. The ledger accumulates 100M-1B+ versions
3. Operator enables pruning by setting `enable: true` in `LedgerPrunerConfig` [8](#0-7) 
4. On node restart, `LedgerPruner::new()` creates sub-pruners with catchup calls
5. Each sub-pruner attempts to prune the entire gap (potentially 1B versions) in one batch
6. The `SchemaBatch` accumulates billions of `WriteOp` entries in memory (40+ bytes each)
7. Memory exhaustion: 100M versions = ~4GB+, 1B versions = ~40-60GB+ just for the batch
8. Node crashes with OOM error

This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits."

## Impact Explanation

This is a **Medium Severity** vulnerability per Aptos bug bounty criteria because it causes "State inconsistencies requiring intervention" - specifically:

- Nodes cannot enable pruning without manual intervention after accumulating large version counts
- Affected nodes crash during initialization, causing unavailability
- Recovery requires either: (a) dramatically increasing memory, (b) manual database surgery, or (c) full resync from scratch
- Does NOT affect consensus safety, fund security, or network-wide liveness
- Requires administrative action (enabling pruning) to trigger
- Impact limited to individual node operators, not systemic

The severity is appropriately Medium because while it causes node unavailability, it:
- Does not compromise consensus or blockchain integrity
- Does not enable fund theft or minting
- Requires operator configuration change to trigger
- Affects only nodes enabling pruning, not existing pruned nodes

## Likelihood Explanation

**Moderate to High Likelihood:**

**Common Triggering Scenarios:**
1. **Archive nodes transitioning to pruned**: Operators running archive nodes for historical data may decide to enable pruning to reduce storage costs
2. **Misconfigured new nodes**: New nodes deployed with pruning accidentally disabled initially
3. **Testnet/devnet experimentation**: Developers testing pruning configurations
4. **Disaster recovery**: Nodes recovering from backups with stale configurations

**Realistic Version Gaps:**
- Mainnet at 5,000 TPS: ~432M versions/day, ~13B versions/month
- Default prune window: 90M versions (line 391 in storage_config.rs)
- A node running 2-3 months without pruning: 200M-400M versions
- Long-running archive nodes: 1B+ versions

**Memory Impact Calculations:**
- 100M versions: ~4-6 GB batch memory
- 500M versions: ~20-30 GB batch memory  
- 1B versions: ~40-60 GB batch memory

Many validator nodes run with 32-64 GB total RAM, making this easily exploitable.

## Recommendation

Implement batch size limiting during the initialization catchup phase. The sub-pruners should apply the same batching logic used during normal operation:

**Fix for TransactionAccumulatorPruner::new():**

```rust
pub(in crate::pruner) fn new(
    ledger_db: Arc<LedgerDb>,
    metadata_progress: Version,
) -> Result<Self> {
    let progress = get_or_initialize_subpruner_progress(
        ledger_db.transaction_accumulator_db_raw(),
        &DbMetadataKey::TransactionAccumulatorPrunerProgress,
        metadata_progress,
    )?;

    let myself = TransactionAccumulatorPruner { ledger_db };

    info!(
        progress = progress,
        metadata_progress = metadata_progress,
        "Catching up TransactionAccumulatorPruner."
    );
    
    // FIX: Apply batching during catchup
    const CATCHUP_BATCH_SIZE: u64 = 10_000; // Use larger batch for catchup
    let mut current_progress = progress;
    while current_progress < metadata_progress {
        let batch_target = std::cmp::min(
            current_progress + CATCHUP_BATCH_SIZE,
            metadata_progress
        );
        myself.prune(current_progress, batch_target)?;
        current_progress = batch_target;
        
        info!(
            progress = current_progress,
            target = metadata_progress,
            "TransactionAccumulatorPruner catchup progress"
        );
    }

    Ok(myself)
}
```

**Apply the same fix to all sub-pruners:** EventStorePruner, PersistedAuxiliaryInfoPruner, TransactionAuxiliaryDataPruner, TransactionInfoPruner, TransactionPruner, and WriteSetPruner.

**Alternative: Share batching logic from LedgerPruner:**
Extract the batching logic from `LedgerPruner::prune()` into a reusable helper function that sub-pruners can use during initialization.

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use aptos_temppath::TempPath;
    use aptos_schemadb::SchemaBatch;
    
    #[test]
    #[should_panic(expected = "out of memory")]
    fn test_unbounded_catchup_oom() {
        // Simulate a large version gap
        let huge_version_gap = 100_000_000u64; // 100M versions
        
        let mut batch = SchemaBatch::new();
        
        // This simulates what TransactionAccumulatorDb::prune does
        // In reality, this would run out of memory before completing
        for version in 0..huge_version_gap {
            // Each iteration adds operations to the in-memory batch
            batch.delete::<TransactionAccumulatorRootHashSchema>(&version)
                .expect("delete failed");
            
            if version % 2 == 1 {
                // Additional deletes for tree nodes (simplified)
                batch.delete::<TransactionAccumulatorSchema>(&Position::from_leaf_index(version))
                    .expect("delete failed");
            }
            
            // Monitor memory growth
            if version % 10_000_000 == 0 {
                eprintln!("Processed {} versions, batch growing unbounded...", version);
            }
        }
        
        // This line would never be reached due to OOM
        eprintln!("Batch accumulated {} million operations", huge_version_gap / 1_000_000);
    }
    
    #[test]
    fn test_batched_catchup_succeeds() {
        // With batching, the same operation succeeds
        let huge_version_gap = 100_000_000u64;
        const BATCH_SIZE: u64 = 10_000;
        
        let mut current = 0u64;
        let mut batches_processed = 0;
        
        while current < huge_version_gap {
            let batch_end = std::cmp::min(current + BATCH_SIZE, huge_version_gap);
            let mut batch = SchemaBatch::new();
            
            // Process one batch
            for version in current..batch_end {
                batch.delete::<TransactionAccumulatorRootHashSchema>(&version)
                    .expect("delete failed");
            }
            
            // Batch gets written and released, memory stays bounded
            current = batch_end;
            batches_processed += 1;
        }
        
        assert_eq!(batches_processed, huge_version_gap / BATCH_SIZE);
        eprintln!("Successfully processed {} batches with bounded memory", batches_processed);
    }
}
```

**Notes**

The vulnerability affects all seven ledger sub-pruners during their initialization catchup phase. While the normal pruning operation correctly enforces batch size limits of 5,000 versions (configurable), the initialization bypass this protection entirely. On production mainnet nodes that have accumulated hundreds of millions or billions of versions, enabling pruning would trigger immediate memory exhaustion. The fix requires applying the same batching discipline to the catchup phase that already exists in the normal operation flow.

### Citations

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L55-55)
```rust
            let pruner_result = self.pruner.prune(self.batch_size);
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L67-68)
```rust
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_accumulator_pruner.rs (L56-56)
```rust
        myself.prune(progress, metadata_progress)?;
```

**File:** storage/aptosdb/src/ledger_db/transaction_accumulator_db.rs (L149-172)
```rust
    pub(crate) fn prune(begin: Version, end: Version, db_batch: &mut SchemaBatch) -> Result<()> {
        for version_to_delete in begin..end {
            db_batch.delete::<TransactionAccumulatorRootHashSchema>(&version_to_delete)?;
            // The even version will be pruned in the iteration of version + 1.
            if version_to_delete % 2 == 0 {
                continue;
            }

            let first_ancestor_that_is_a_left_child =
                Self::find_first_ancestor_that_is_a_left_child(version_to_delete);

            // This assertion is true because we skip the leaf nodes with address which is a
            // a multiple of 2.
            assert!(!first_ancestor_that_is_a_left_child.is_leaf());

            let mut current = first_ancestor_that_is_a_left_child;
            while !current.is_leaf() {
                db_batch.delete::<TransactionAccumulatorSchema>(&current.left_child())?;
                db_batch.delete::<TransactionAccumulatorSchema>(&current.right_child())?;
                current = current.right_child();
            }
        }
        Ok(())
    }
```

**File:** storage/schemadb/src/batch.rs (L130-133)
```rust
pub struct SchemaBatch {
    rows: DropHelper<HashMap<ColumnFamilyName, Vec<WriteOp>>>,
    stats: SampledBatchStats,
}
```

**File:** storage/schemadb/src/batch.rs (L165-172)
```rust
    fn raw_delete(&mut self, cf_name: ColumnFamilyName, key: Vec<u8>) -> DbResult<()> {
        self.rows
            .entry(cf_name)
            .or_default()
            .push(WriteOp::Deletion { key });

        Ok(())
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L106-106)
```rust
        myself.prune(progress, metadata_progress)?;
```

**File:** config/src/config/storage_config.rs (L327-341)
```rust
pub struct LedgerPrunerConfig {
    /// Boolean to enable/disable the ledger pruner. The ledger pruner is responsible for pruning
    /// everything else except for states (e.g. transactions, events etc.)
    pub enable: bool,
    /// This is the default pruning window for any other store except for state store. State store
    /// being big in size, we might want to configure a smaller window for state store vs other
    /// store.
    pub prune_window: u64,
    /// Batch size of the versions to be sent to the ledger pruner - this is to avoid slowdown due to
    /// issuing too many DB calls and batch prune instead. For ledger pruner, this means the number
    /// of versions to prune a time.
    pub batch_size: usize,
    /// The offset for user pruning window to adjust
    pub user_pruning_window_offset: u64,
}
```
