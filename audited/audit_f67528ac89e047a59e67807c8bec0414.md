# Audit Report

## Title
TOCTOU Race Condition Allows Malicious Peer Data in Global State Sync Summary

## Summary
A Time-Of-Check-Time-Of-Use (TOCTOU) race condition exists in `calculate_global_data_summary()` where malicious peer data can be included in the global summary even after the peer has been marked as ignored due to malicious behavior. This occurs because peer scores can change between the filtering check and data aggregation steps, allowing newly-ignored peers' advertised data to influence state synchronization decisions.

## Finding Description

The vulnerability exists in the `calculate_global_data_summary()` function in `peer_states.rs`. The function aggregates storage summaries from all non-ignored peers to build a global view of available data in the network. [1](#0-0) 

The issue occurs during the collection phase where the function iterates over peers using DashMap's `.iter()` and filters based on each peer's ignore status. The check for whether a peer is ignored happens at: [2](#0-1) 

Which calls: [3](#0-2) 

The race condition occurs because:

1. **Thread A** (global summary calculation): Iterates peers, finds peer X with score 26.0 (above `IGNORE_PEER_THRESHOLD` of 25.0), includes their storage summary
2. **Thread B** (response processing): Detects malicious behavior from peer X (e.g., invalid proof), calls `update_score_error()` with `ErrorType::Malicious` [4](#0-3) 

3. **Thread B** reduces peer X's score: `score *= MALICIOUS_MULTIPLIER (0.8)`, resulting in score 20.8 (now below threshold)
4. **Thread A** has already collected peer X's (potentially malicious) advertised data in the Vec
5. **Result**: Malicious peer's data is included in global summary despite being marked as ignored [5](#0-4) 

The global summary is used by state sync components to determine:
- Available data ranges in the network
- Target versions for synchronization
- Waypoint satisfiability during bootstrapping [6](#0-5) [7](#0-6) 

## Impact Explanation

**Severity: Medium**

While this race condition exists, its impact is limited by several factors:

1. **Temporal Limitation**: The global summary is recalculated every 100ms (poll_loop_interval_ms), meaning malicious data persists for at most one polling cycle. [8](#0-7) 

2. **Defense in Depth**: Actual data requests still require cryptographic proof verification, preventing acceptance of invalid blockchain data even if advertised.

3. **No Consensus Impact**: This affects state synchronization metadata, not consensus or execution. The blockchain's safety and liveness guarantees remain intact.

However, the vulnerability can cause:
- **Temporary State Sync Delays**: Malicious peers advertising false high versions can cause the node to attempt syncing to non-existent data
- **Resource Waste**: Failed sync attempts consume network bandwidth and CPU cycles
- **Coordination Issues**: Multiple nodes experiencing this simultaneously could reduce network sync efficiency

This qualifies as **Medium Severity** under the "State inconsistencies requiring intervention" category, as operators may need to manually identify and ban persistently malicious peers exploiting this race window.

## Likelihood Explanation

**Likelihood: Medium to High**

The attack is feasible because:

1. **Frequent Execution**: Global summary calculation runs every 100ms, providing repeated opportunities
2. **Natural Timing**: The race window occurs naturally when peers send malicious responses during summary calculation
3. **No Special Privileges**: Any network peer can attempt this attack without validator access
4. **Concurrent Operations**: Score updates from response processing happen concurrently with summary calculations by design

However, exploitation requires:
- Network timing to coincide malicious responses with summary calculations
- The peer maintaining a score just above the ignore threshold (25.0-26.0 range)
- Repeated attempts to maintain presence across multiple summary cycles

## Recommendation

Implement atomic snapshot semantics for peer score checks during global summary calculation. Two approaches:

**Option 1: Lock-Based Snapshot**
```rust
pub fn calculate_global_data_summary(&self) -> GlobalDataSummary {
    // Collect peer data with consistent score checks
    let storage_summaries: Vec<StorageServerSummary> = self
        .peer_to_state
        .iter()
        .filter_map(|peer_state_ref| {
            let peer_state = peer_state_ref.value();
            // Atomically check score and clone summary under the same guard
            if peer_state.is_ignored() {
                None
            } else {
                peer_state.storage_summary.clone()
            }
        })
        .collect();
    // ... rest of function
}
```

**Option 2: Score Snapshot with Validation**
```rust
pub fn calculate_global_data_summary(&self) -> GlobalDataSummary {
    // First pass: collect peer IDs and scores
    let peer_snapshots: Vec<(PeerNetworkId, f64)> = self
        .peer_to_state
        .iter()
        .map(|entry| (*entry.key(), entry.value().get_score()))
        .collect();
    
    // Second pass: collect summaries only from peers still above threshold
    let storage_summaries: Vec<StorageServerSummary> = peer_snapshots
        .iter()
        .filter(|(_, score)| *score > IGNORE_PEER_THRESHOLD)
        .filter_map(|(peer, _)| {
            self.peer_to_state
                .get(peer)
                .and_then(|entry| entry.get_storage_summary_if_not_ignored().cloned())
        })
        .collect();
    // ... rest of function
}
```

**Option 3: Reduce Summary Update Frequency for Newly-Ignored Peers**
Add a grace period where peers marked as malicious within the last summary cycle are excluded:
```rust
struct PeerState {
    // ... existing fields
    last_marked_malicious: Option<Instant>,
}

fn is_ignored(&self) -> bool {
    if !self.data_client_config.ignore_low_score_peers {
        return false;
    }
    
    // Ignore if score is low OR recently marked malicious
    if let Some(marked_time) = self.last_marked_malicious {
        if marked_time.elapsed() < Duration::from_millis(200) {
            return true;
        }
    }
    
    self.score <= IGNORE_PEER_THRESHOLD
}
```

## Proof of Concept

Due to the concurrent nature of this vulnerability, a full PoC requires multi-threaded Rust code. Here's a conceptual reproduction:

```rust
#[tokio::test]
async fn test_race_condition_malicious_peer_in_summary() {
    // Setup: Create data client with one peer at score 26.0
    let config = Arc::new(AptosDataClientConfig::default());
    let peer_states = PeerStates::new(config.clone());
    let malicious_peer = create_test_peer();
    
    // Peer advertises false high version
    let malicious_summary = create_storage_summary(1_000_000);
    peer_states.update_summary(malicious_peer, malicious_summary);
    peer_states.update_score_success(malicious_peer); // Score = 51
    
    // Spawn two concurrent operations
    let peer_states_clone = peer_states.clone();
    let handle1 = tokio::spawn(async move {
        // Continuously calculate global summary
        loop {
            let summary = peer_states_clone.calculate_global_data_summary();
            // Check if malicious peer's high version is present
            if let Some(highest) = summary.advertised_data.highest_synced_ledger_info() {
                if highest.ledger_info().version() == 1_000_000 {
                    return true; // Malicious data was included
                }
            }
            tokio::time::sleep(Duration::from_micros(50)).await;
        }
    });
    
    let peer_states_clone = peer_states.clone();
    let handle2 = tokio::spawn(async move {
        // Continuously mark peer as malicious
        loop {
            peer_states_clone.update_score_error(malicious_peer, ErrorType::Malicious);
            // Wait for score to potentially recover
            tokio::time::sleep(Duration::from_micros(100)).await;
        }
    });
    
    // Run for 1 second and check if race occurred
    tokio::time::sleep(Duration::from_secs(1)).await;
    // If handle1 returns true, malicious data was included despite being ignored
}
```

This demonstrates that malicious peer advertisements can appear in the global summary even after the peer has been marked as ignored, violating the intended filtering invariant.

## Notes

While this vulnerability exists at a technical level, its practical impact is mitigated by:
- Short-lived effect (100ms window)
- Defense-in-depth verification of actual data
- No direct consensus or funds impact

The vulnerability is most concerning in scenarios where:
- Multiple malicious peers coordinate to persistently pollute summaries
- Network conditions allow precise timing of attacks
- Nodes are bootstrapping and rely heavily on advertised data ranges

The fix should prioritize maintaining snapshot consistency during peer iteration while minimizing performance overhead on the critical state sync path.

### Citations

**File:** state-sync/aptos-data-client/src/peer_states.rs (L143-149)
```rust
    pub fn get_storage_summary_if_not_ignored(&self) -> Option<&StorageServerSummary> {
        if self.is_ignored() {
            None
        } else {
            self.storage_summary.as_ref()
        }
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L152-160)
```rust
    fn is_ignored(&self) -> bool {
        // Only ignore peers if the config allows it
        if !self.data_client_config.ignore_low_score_peers {
            return false;
        }

        // Otherwise, ignore peers with a low score
        self.score <= IGNORE_PEER_THRESHOLD
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L303-322)
```rust
    pub fn update_score_error(&self, peer: PeerNetworkId, error: ErrorType) {
        if let Some(mut entry) = self.peer_to_state.get_mut(&peer) {
            // Get the peer's old score
            let old_score = entry.score;

            // Update the peer's score with an error
            entry.update_score_error(error);

            // Log if the peer is now ignored
            let new_score = entry.score;
            if old_score > IGNORE_PEER_THRESHOLD && new_score <= IGNORE_PEER_THRESHOLD {
                info!(
                    (LogSchema::new(LogEntry::PeerStates)
                        .event(LogEvent::PeerIgnored)
                        .message("Peer will be ignored")
                        .peer(&peer))
                );
            }
        }
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L339-350)
```rust
    pub fn calculate_global_data_summary(&self) -> GlobalDataSummary {
        // Gather all storage summaries, but exclude peers that are ignored
        let storage_summaries: Vec<StorageServerSummary> = self
            .peer_to_state
            .iter()
            .filter_map(|peer_state| {
                peer_state
                    .value()
                    .get_storage_summary_if_not_ignored()
                    .cloned()
            })
            .collect();
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L363-394)
```rust
        for summary in storage_summaries {
            // Collect aggregate data advertisements
            if let Some(epoch_ending_ledger_infos) = summary.data_summary.epoch_ending_ledger_infos
            {
                advertised_data
                    .epoch_ending_ledger_infos
                    .push(epoch_ending_ledger_infos);
            }
            if let Some(states) = summary.data_summary.states {
                advertised_data.states.push(states);
            }
            if let Some(synced_ledger_info) = summary.data_summary.synced_ledger_info.as_ref() {
                advertised_data
                    .synced_ledger_infos
                    .push(synced_ledger_info.clone());
            }
            if let Some(transactions) = summary.data_summary.transactions {
                advertised_data.transactions.push(transactions);
            }
            if let Some(transaction_outputs) = summary.data_summary.transaction_outputs {
                advertised_data
                    .transaction_outputs
                    .push(transaction_outputs);
            }

            // Collect preferred max chunk sizes
            max_epoch_chunk_sizes.push(summary.protocol_metadata.max_epoch_chunk_size);
            max_state_chunk_sizes.push(summary.protocol_metadata.max_state_chunk_size);
            max_transaction_chunk_sizes.push(summary.protocol_metadata.max_transaction_chunk_size);
            max_transaction_output_chunk_sizes
                .push(summary.protocol_metadata.max_transaction_output_chunk_size);
        }
```

**File:** state-sync/data-streaming-service/src/stream_engine.rs (L523-529)
```rust
        if let Some(highest_synced_ledger_info) = advertised_data.highest_synced_ledger_info() {
            let (next_request_version, _) = self.next_request_version_and_epoch;
            if next_request_version > highest_synced_ledger_info.ledger_info().version() {
                Ok(None) // We're already at the highest synced ledger info. There's no known target.
            } else {
                Ok(Some(highest_synced_ledger_info))
            }
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L894-910)
```rust
        let highest_advertised_ledger_info = global_data_summary
            .advertised_data
            .highest_synced_ledger_info()
            .ok_or_else(|| {
                Error::UnsatisfiableWaypoint(
                    "Unable to check waypoint satisfiability! No highest advertised ledger info found in the network!".into(),
                )
            })?;
        let highest_advertised_version = highest_advertised_ledger_info.ledger_info().version();

        // Compare the highest advertised version with our waypoint
        if highest_advertised_version < waypoint_version {
            Err(Error::UnsatisfiableWaypoint(
                format!(
                    "The waypoint is not satisfiable! No advertised version higher than our waypoint! Highest version: {:?}, waypoint version: {:?}.",
                    highest_advertised_version, waypoint_version
                )
```

**File:** config/src/config/state_sync_config.rs (L343-357)
```rust
    pub poll_loop_interval_ms: u64,
}

impl Default for AptosDataPollerConfig {
    fn default() -> Self {
        Self {
            additional_polls_per_peer_bucket: 1,
            min_polls_per_second: 5,
            max_num_in_flight_priority_polls: 30,
            max_num_in_flight_regular_polls: 30,
            max_polls_per_second: 20,
            peer_bucket_size: 10,
            poll_loop_interval_ms: 100,
        }
    }
```
