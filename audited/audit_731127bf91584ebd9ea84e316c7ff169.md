# Audit Report

## Title
Time-of-Check-Time-of-Use Race in Pending Block Insertion Causes Permanent Map Inconsistency and Memory Exhaustion

## Summary
A TOCTOU race condition between checking for existing pending blocks and inserting new blocks allows the two internal maps (`blocks_without_payloads` and `blocks_without_payloads_by_hash`) to become permanently inconsistent, leading to memory exhaustion attacks against consensus observer nodes.

## Finding Description

The `PendingBlockStore` maintains two synchronized maps keyed differently: one by `(epoch, round)` and another by block hash. [1](#0-0) 

When inserting blocks, the code performs a check-then-act pattern across separate mutex acquisitions:

1. **Check phase**: The code checks if a block already exists [2](#0-1) 

2. **Race window**: The lock is released between check and insert

3. **Insert phase**: If the check passes, insertion proceeds [3](#0-2) 

The insertion logic handles collisions independently for each map: [4](#0-3) 

**Attack Scenario:**
- Thread A checks if block (epoch=1, round=10) exists → NOT FOUND
- Thread B checks if block (epoch=1, round=10) exists → NOT FOUND (race window)
- Thread A inserts block with hash H1 into both maps successfully
- Thread B attempts to insert block with hash H2:
  - First map keyed by (1, 10): `Entry::Occupied` → **skip insert** (logs warning)
  - Second map keyed by H2: `Entry::Vacant` → **inserts successfully**
- Result: Map 1 has 1 entry, Map 2 has 2 entries → **permanent inconsistency**

The `garbage_collect_pending_blocks()` function detects this mismatch but continues operating: [5](#0-4) 

Critically, garbage collection calculates removal count based solely on the first map's size: [6](#0-5) 

This means orphaned entries in the second map are **never garbage collected**, allowing unbounded memory growth through repeated exploitation.

While `remove_ready_block()` does rebuild both maps, this only occurs when blocks become ready for processing. [7](#0-6) 

If an attacker sends blocks without payloads (or where payloads never arrive), orphaned entries accumulate indefinitely in the hash map.

## Impact Explanation

**Severity: Medium** 

This breaks the **State Consistency** invariant (#4) - the two internal maps that should remain synchronized become permanently desynchronized, with one map growing without bound.

Impact categories:
1. **Memory Exhaustion**: An attacker can cause unbounded memory growth in consensus observer nodes by repeatedly exploiting this race to create orphaned hash map entries
2. **Resource Exhaustion**: Qualifies as "State inconsistencies requiring intervention" (Medium severity per bug bounty criteria)
3. **Node Degradation**: Consensus observer nodes will experience memory pressure, potentially requiring restart

The impact is limited to consensus observer nodes (not validators directly), preventing this from reaching Critical severity. However, if validators run consensus observers for monitoring, they could be affected.

## Likelihood Explanation

**Likelihood: High**

- **Attack Requirements**: Any network peer can send ordered block messages; no validator access needed
- **Exploitation Complexity**: Low - attacker simply sends duplicate block messages rapidly to trigger race conditions
- **Natural Occurrence**: Can also occur organically through network delays causing duplicate message processing
- **Persistent Damage**: Once orphaned entries exist, they persist until `remove_ready_block()` is called, which may never happen for blocks without payloads

Byzantine validators or malicious peers can deliberately send conflicting blocks for the same `(epoch, round)` to maximize orphaned entries.

## Recommendation

**Fix the TOCTOU race by performing the existence check and insertion atomically:**

```rust
// In consensus_observer.rs, replace separate lock acquisitions:
pub async fn process_ordered_block_message(...) {
    // ... existing validation code ...
    
    // ATOMIC check-and-insert within single lock acquisition
    let should_process = {
        let mut block_data = self.observer_block_data.lock();
        
        // Check within same critical section as insert
        if block_data.existing_pending_block(&ordered_block) {
            false // Block already exists, skip
        } else {
            // Insert immediately while holding lock
            let pending_block_with_metadata = PendingBlockWithMetadata::new_with_arc(
                peer_network_id,
                message_received_time,
                observed_ordered_block,
            );
            
            if block_data.all_payloads_exist(pending_block_with_metadata.ordered_block().blocks()) {
                true // Will process outside lock
            } else {
                block_data.insert_pending_block(pending_block_with_metadata);
                false // Inserted, no further processing
            }
        }
    }; // Lock released here
    
    if should_process {
        self.process_ordered_block(pending_block_with_metadata).await;
    }
}
```

**Additional hardening**: Make garbage collection aware of hash map size to prevent unbounded growth:

```rust
fn garbage_collect_pending_blocks(&mut self) {
    let num_pending_blocks = self.blocks_without_payloads.len() as u64;
    let num_pending_blocks_by_hash = self.blocks_without_payloads_by_hash.len() as u64;
    
    // Use MAXIMUM of both sizes for removal calculation
    let max_size = num_pending_blocks.max(num_pending_blocks_by_hash);
    let num_blocks_to_remove = max_size.saturating_sub(max_pending_blocks);
    
    // ... existing removal logic ...
    
    // CLEANUP: Remove any orphaned entries from hash map
    self.blocks_without_payloads_by_hash.retain(|hash, block| {
        let epoch_round = (block.ordered_block().first_block().epoch(), 
                          block.ordered_block().first_block().round());
        self.blocks_without_payloads.contains_key(&epoch_round)
    });
}
```

## Proof of Concept

```rust
#[test]
fn test_toctou_race_causes_map_inconsistency() {
    use std::sync::Arc;
    use std::thread;
    use aptos_infallible::Mutex;
    
    // Create pending block store
    let config = ConsensusObserverConfig {
        max_num_pending_blocks: 100,
        ..Default::default()
    };
    let store = Arc::new(Mutex::new(PendingBlockStore::new(config)));
    
    // Create two identical (epoch, round) blocks with different hashes
    let epoch = 1;
    let round = 10;
    let block1 = create_ordered_block(epoch, round, 1, HashValue::random());
    let block2 = create_ordered_block(epoch, round, 1, HashValue::random());
    
    let pending1 = PendingBlockWithMetadata::new_with_arc(
        PeerNetworkId::random(),
        Instant::now(),
        ObservedOrderedBlock::new(block1),
    );
    let pending2 = PendingBlockWithMetadata::new_with_arc(
        PeerNetworkId::random(),
        Instant::now(),
        ObservedOrderedBlock::new(block2),
    );
    
    // Simulate TOCTOU race with concurrent inserts
    let store1 = store.clone();
    let store2 = store.clone();
    
    let handle1 = thread::spawn(move || {
        store1.lock().insert_pending_block(pending1);
    });
    
    let handle2 = thread::spawn(move || {
        // Small delay to increase race probability
        thread::sleep(Duration::from_micros(1));
        store2.lock().insert_pending_block(pending2);
    });
    
    handle1.join().unwrap();
    handle2.join().unwrap();
    
    // Verify inconsistency: maps have different sizes
    let locked_store = store.lock();
    let map1_size = locked_store.blocks_without_payloads.len();
    let map2_size = locked_store.blocks_without_payloads_by_hash.len();
    
    assert_ne!(map1_size, map2_size, 
        "TOCTOU race should cause map size mismatch: map1={}, map2={}", 
        map1_size, map2_size);
}
```

## Notes

The vulnerability is NOT within `garbage_collect_pending_blocks()` itself—that function is atomic due to Rust's `&mut self` borrow and the mutex held by callers. The actual vulnerability is a **higher-level TOCTOU race** in the insertion logic that allows the stores to become inconsistent before garbage collection runs. The question correctly identified that operations on the two maps are non-atomic, but the atomicity break occurs during insertion, not during garbage collection.

### Citations

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L65-72)
```rust
    // A map of ordered blocks that are without payloads. The key is
    // the (epoch, round) of the first block in the ordered block.
    blocks_without_payloads: BTreeMap<(u64, Round), Arc<PendingBlockWithMetadata>>,

    // A map of ordered blocks that are without payloads. The key is
    // the hash of the first block in the ordered block.
    // Note: this is the same as blocks_without_payloads, but with a different key.
    blocks_without_payloads_by_hash: BTreeMap<HashValue, Arc<PendingBlockWithMetadata>>,
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L118-150)
```rust
        match self.blocks_without_payloads.entry(first_block_epoch_round) {
            Entry::Occupied(_) => {
                // The block is already in the store
                warn!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "A pending block was already found for the given epoch and round: {:?}",
                        first_block_epoch_round
                    ))
                );
            },
            Entry::Vacant(entry) => {
                // Insert the block into the store
                entry.insert(pending_block.clone());
            },
        }

        // Insert the block into the hash store using the hash of the first block
        let first_block_hash = first_block.id();
        match self.blocks_without_payloads_by_hash.entry(first_block_hash) {
            Entry::Occupied(_) => {
                // The block is already in the hash store
                warn!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "A pending block was already found for the given block hash: {:?}",
                        first_block_hash
                    ))
                );
            },
            Entry::Vacant(entry) => {
                // Insert the block into the hash store
                entry.insert(pending_block);
            },
        }
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L159-170)
```rust
        // Verify that both stores have the same number of entries.
        // If not, log an error as this should never happen.
        let num_pending_blocks = self.blocks_without_payloads.len() as u64;
        let num_pending_blocks_by_hash = self.blocks_without_payloads_by_hash.len() as u64;
        if num_pending_blocks != num_pending_blocks_by_hash {
            error!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "The pending block stores have different numbers of entries: {} and {} (by hash)",
                    num_pending_blocks, num_pending_blocks_by_hash
                ))
            );
        }
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L172-174)
```rust
        // Calculate the number of blocks to remove
        let max_pending_blocks = self.consensus_observer_config.max_num_pending_blocks;
        let num_blocks_to_remove = num_pending_blocks.saturating_sub(max_pending_blocks);
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L243-252)
```rust
        // Clear all blocks from the pending block stores
        self.clear_missing_blocks();

        // Update the pending block stores to only include the blocks at higher rounds
        self.blocks_without_payloads = blocks_at_higher_rounds;
        for pending_block in self.blocks_without_payloads.values() {
            let first_block = pending_block.ordered_block().first_block();
            self.blocks_without_payloads_by_hash
                .insert(first_block.id(), pending_block.clone());
        }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L681-684)
```rust
        let block_pending = self
            .observer_block_data
            .lock()
            .existing_pending_block(&ordered_block);
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L710-712)
```rust
            self.observer_block_data
                .lock()
                .insert_pending_block(pending_block_with_metadata);
```
