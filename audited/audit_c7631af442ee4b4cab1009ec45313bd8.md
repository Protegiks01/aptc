# Audit Report

## Title
Catastrophic Data Loss During DAG Store Recovery Due to Silent Error Swallowing in Certified Node Deserialization

## Summary
The DAG consensus implementation silently swallows all errors when loading certified nodes from storage during bootstrap, converting any corruption or deserialization failure into complete loss of ALL certified nodes, not just corrupted entries. This creates an improper recovery mechanism that violates state consistency invariants.

## Finding Description

The vulnerability exists in the DAG store initialization logic where certified nodes are loaded from the database: [1](#0-0) 

The code uses `unwrap_or_default()` which silently converts any error into an empty vector. When `get_certified_nodes()` is called, it performs a full iteration over the certified_node column family: [2](#0-1) 

This uses the generic `get_all()` method which creates an iterator: [3](#0-2) 

The iterator attempts to deserialize each entry using BCS: [4](#0-3) 

The critical flaw is in the iterator implementation where ANY deserialization failure causes the entire iteration to fail: [5](#0-4) 

**The cascading failure occurs as follows:**

1. A single corrupted certified node entry causes `bcs::from_bytes()` to return an error
2. The iterator propagates this error through the `collect()` operation
3. `get_certified_nodes()` returns an error to the caller
4. `unwrap_or_default()` silently converts the error to an empty vector
5. ALL certified nodes are lost, including valid ones
6. The DAG initializes empty and logs only a generic warning: [6](#0-5) 

**Does corruption cascade to other consensus data?**

Analysis of the storage architecture shows that column families are physically isolated: [7](#0-6) 

Blocks, QCs, votes, and certified nodes are stored in separate RocksDB column families. Individual writes to certified_node CF do not batch with other CFs: [8](#0-7) 

Therefore, **physical corruption does not cascade** to other column families. However, **logical state inconsistency occurs** because the node continues operating with empty certified nodes while other consensus data remains present.

## Impact Explanation

This qualifies as **Medium Severity** under the Aptos bug bounty program: "State inconsistencies requiring intervention."

**Specific impacts:**

1. **Complete data loss**: All certified nodes are lost, not just corrupted entries
2. **No visibility**: Operators receive only a generic "need state sync" warning with no indication of the underlying corruption
3. **Availability degradation**: Node cannot participate in DAG consensus until state sync completes
4. **Recovery complexity**: No mechanism for partial recovery or manual intervention

**However, consensus safety is preserved** because:
- The affected node enters state sync mode automatically
- It will not produce invalid blocks or votes
- Other validators are unaffected
- Eventually recovers through state synchronization [9](#0-8) 

## Likelihood Explanation

**Likelihood: Medium to High** in production environments.

Corruption scenarios include:
- Hardware failures (disk sector corruption, memory errors)
- Power loss during database writes
- File system corruption
- Software bugs in serialization/deserialization
- Incompatible schema changes during upgrades

While not directly exploitable by remote attackers, these scenarios occur naturally in distributed systems. The severity is amplified because a single corrupted entry loses ALL data rather than just the affected entry.

## Recommendation

Implement proper error handling with partial recovery capability:

```rust
pub fn new(
    epoch_state: Arc<EpochState>,
    storage: Arc<dyn DAGStorage>,
    payload_manager: Arc<dyn TPayloadManager>,
    start_round: Round,
    window_size: u64,
) -> Self {
    let all_nodes = match storage.get_certified_nodes() {
        Ok(nodes) => nodes,
        Err(e) => {
            error!(
                error = ?e,
                "CRITICAL: Failed to load certified nodes from storage. \
                 This may indicate database corruption. All certified nodes lost. \
                 State sync required."
            );
            // Emit metric for monitoring
            counters::CERTIFIED_NODE_LOAD_FAILURES.inc();
            vec![]
        }
    };
    
    // Rest of initialization...
}
```

Additionally, implement an alternative iterator that skips corrupted entries:

```rust
pub fn get_certified_nodes_with_partial_recovery(&self) -> anyhow::Result<(Vec<(HashValue, CertifiedNode)>, Vec<HashValue>)> {
    let mut valid_nodes = vec![];
    let mut corrupted_keys = vec![];
    
    let mut iter = self.consensus_db.iter::<CertifiedNodeSchema>()?;
    iter.seek_to_first();
    
    while let Some(result) = iter.next() {
        match result {
            Ok((key, value)) => valid_nodes.push((key, value)),
            Err(e) => {
                warn!("Skipping corrupted certified node entry: {:?}", e);
                corrupted_keys.push(/* extract key from error if possible */);
            }
        }
    }
    
    Ok((valid_nodes, corrupted_keys))
}
```

## Proof of Concept

This vulnerability requires simulating database corruption, which is complex in a PoC. A conceptual test would:

```rust
#[test]
fn test_corrupted_certified_node_causes_complete_data_loss() {
    // Setup: Create DagStore with 10 valid certified nodes
    let storage = create_storage_with_nodes(10);
    
    // Inject corruption: Corrupt bytes of one certified node entry
    inject_corruption_to_certified_node(&storage, node_index: 5);
    
    // Attempt to bootstrap DagStore
    let dag_store = DagStore::new(
        epoch_state,
        storage.clone(),
        payload_manager,
        start_round,
        window_size,
    );
    
    // Verify: ALL 10 nodes are lost, not just the corrupted one
    assert!(dag_store.read().is_empty());
    assert_eq!(dag_store.read().highest_round(), start_round);
    
    // Verify: No explicit error was logged about corruption
    // Only generic "need state sync" warning appears
}
```

The test demonstrates that corruption of a single entry results in total loss of all certified nodes due to the error swallowing pattern.

---

**Notes:**

The corruption does NOT physically cascade to other column families (blocks, QCs, votes) as they are stored separately in RocksDB. However, the recovery mechanism is inadequate because:

1. Silent error swallowing masks the root cause
2. All-or-nothing failure loses valid data
3. No partial recovery capability exists
4. Operators lack visibility into the corruption

This represents a state consistency violation requiring manual intervention to diagnose and resolve, meeting Medium severity criteria.

### Citations

**File:** consensus/src/dag/dag_store.rs (L461-461)
```rust
        let mut all_nodes = storage.get_certified_nodes().unwrap_or_default();
```

**File:** consensus/src/dag/dag_store.rs (L482-487)
```rust
        if dag.read().is_empty() {
            warn!(
                "[DAG] Start with empty DAG store at {}, need state sync",
                start_round
            );
        }
```

**File:** consensus/src/dag/adapter.rs (L367-371)
```rust
    fn save_certified_node(&self, node: &CertifiedNode) -> anyhow::Result<()> {
        Ok(self
            .consensus_db
            .put::<CertifiedNodeSchema>(&node.digest(), node)?)
    }
```

**File:** consensus/src/dag/adapter.rs (L373-375)
```rust
    fn get_certified_nodes(&self) -> anyhow::Result<Vec<(HashValue, CertifiedNode)>> {
        Ok(self.consensus_db.get_all::<CertifiedNodeSchema>()?)
    }
```

**File:** consensus/src/consensusdb/mod.rs (L52-61)
```rust
        let column_families = vec![
            /* UNUSED CF = */ DEFAULT_COLUMN_FAMILY_NAME,
            BLOCK_CF_NAME,
            QC_CF_NAME,
            SINGLE_ENTRY_CF_NAME,
            NODE_CF_NAME,
            CERTIFIED_NODE_CF_NAME,
            DAG_VOTE_CF_NAME,
            "ordered_anchor_id", // deprecated CF
        ];
```

**File:** consensus/src/consensusdb/mod.rs (L201-205)
```rust
    pub fn get_all<S: Schema>(&self) -> Result<Vec<(S::Key, S::Value)>, DbError> {
        let mut iter = self.db.iter::<S>()?;
        iter.seek_to_first();
        Ok(iter.collect::<Result<Vec<(S::Key, S::Value)>, AptosDbError>>()?)
    }
```

**File:** consensus/src/consensusdb/schema/dag/mod.rs (L88-96)
```rust
impl ValueCodec<CertifiedNodeSchema> for CertifiedNode {
    fn encode_value(&self) -> Result<Vec<u8>> {
        Ok(bcs::to_bytes(&self)?)
    }

    fn decode_value(data: &[u8]) -> Result<Self> {
        Ok(bcs::from_bytes(data)?)
    }
}
```

**File:** storage/schemadb/src/iterator.rs (L118-121)
```rust
        let key = <S::Key as KeyCodec<S>>::decode_key(raw_key);
        let value = <S::Value as ValueCodec<S>>::decode_value(raw_value);

        Ok(Some((key?, value?)))
```

**File:** consensus/src/dag/dag_state_sync.rs (L147-155)
```rust
        dag_reader.is_empty()
            || dag_reader.highest_round() + 1 + self.dag_window_size_config
                < li.commit_info().round()
            || self
                .ledger_info_provider
                .get_highest_committed_anchor_round()
                + 2 * self.dag_window_size_config
                < li.commit_info().round()
    }
```
