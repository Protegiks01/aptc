# Audit Report

## Title
Validator Liveness Failure During Epoch Transitions Due to Unavailable Epoch-Ending Ledger Infos

## Summary
During epoch transitions, validators that fall behind may become permanently unable to catch up if the network has advanced beyond the epochs they need. The `DataIsUnavailable` error in epoch-ending ledger info fetching can cause validators to be stuck in an infinite retry loop, preventing them from learning about new validator sets and participating in consensus.

## Finding Description

When a validator needs to bootstrap or catch up after falling behind, it must fetch epoch-ending ledger infos to verify epoch transitions and learn about new validator sets. [1](#0-0) 

The bootstrapper attempts to fetch these infos by calling the streaming service, which creates an `EpochEndingStreamEngine`. However, this engine has a critical validation check: [2](#0-1) 

If no peers advertise any epoch-ending ledger info (`highest_epoch_ending_ledger_info()` returns `None`), or if the highest advertised epoch is lower than the start epoch the validator needs, a `DataIsUnavailable` error is returned.

When this error occurs:
1. The bootstrapper's stream creation fails and returns the error [3](#0-2) 
2. The error propagates to the driver's main loop [4](#0-3) 
3. The driver logs the error and continues the loop, retrying on the next iteration
4. If the underlying condition is persistent (no peers serve the old epoch-ending ledger infos), the validator remains stuck indefinitely

The critical issue is that validators cannot progress past bootstrapping without these epoch-ending ledger infos. Since the bootstrapper must complete before transactions can be processed and committed, the validator never reaches the point where it can: [5](#0-4) 

Without committing transactions, no reconfiguration notifications are sent to consensus. Consensus waits indefinitely for these notifications: [6](#0-5) 

## Impact Explanation

This represents a **High Severity** validator liveness issue. While it doesn't allow theft of funds or consensus safety violations, it causes:
- **Validator node unable to participate**: Affected validators cannot join consensus for new epochs
- **Reduced network resilience**: If multiple validators fall behind simultaneously, the network's fault tolerance is compromised
- **Potential permanent validator exclusion**: Without manual intervention (full resync from genesis or snapshot), the validator cannot recover

This qualifies as "Validator node slowdowns" or potentially "Total loss of liveness" for the affected validator per the Aptos bug bounty criteria.

## Likelihood Explanation

This issue has **MEDIUM likelihood** in practice:

**Triggering conditions:**
- A validator falls behind by multiple epochs due to extended downtime or network partition
- The network advances beyond the epochs the validator needs
- Peers prune or stop serving old epoch-ending ledger infos (e.g., due to storage limits or configuration)

**Mitigating factors:**
- Most peers retain recent epoch-ending ledger infos
- Archive nodes typically serve historical data
- The network would need to advance significantly before peers stop serving the required epochs

However, as the network matures and epochs accumulate, this becomes more likely for validators experiencing extended outages.

## Recommendation

Implement a fallback mechanism for validators that cannot fetch required epoch-ending ledger infos:

1. **Add peer priority/retry logic**: Try archive nodes or full nodes that advertise historical data before failing
2. **Implement checkpoint/snapshot bootstrapping**: Allow validators to bootstrap from trusted snapshots that include verified epoch-ending ledger infos
3. **Add explicit error handling with recovery guidance**: Instead of infinite retry, detect the permanent failure condition and provide clear recovery instructions
4. **Consider increasing epoch-ending ledger info retention requirements**: Require validators to serve a minimum number of recent epoch-ending ledger infos

Example fix for detection and guidance:

```rust
// In bootstrapper.rs, add persistent error detection
const MAX_CONSECUTIVE_DATA_UNAVAILABLE_ERRORS: u64 = 10;
let mut consecutive_data_unavailable_errors = 0;

// In drive_progress error handling
if let Err(error) = self.initialize_active_data_stream(global_data_summary).await {
    if matches!(error, Error::DataIsUnavailable(_)) {
        consecutive_data_unavailable_errors += 1;
        if consecutive_data_unavailable_errors >= MAX_CONSECUTIVE_DATA_UNAVAILABLE_ERRORS {
            panic!(
                "Unable to fetch required epoch-ending ledger infos after {} attempts. \
                The network may have advanced beyond available historical data. \
                Recovery options: 1) Restore from trusted snapshot, 2) Wait for peers \
                to advertise historical data, 3) Resync from genesis with archive node",
                consecutive_data_unavailable_errors
            );
        }
    } else {
        consecutive_data_unavailable_errors = 0;
    }
    return Err(error);
}
```

## Proof of Concept

This issue cannot be easily demonstrated without a full network simulation, but the vulnerability path can be validated:

1. **Setup**: Deploy a validator node and let it participate in epoch N
2. **Trigger**: Shut down the validator for multiple epochs (N+1, N+2, N+3...)
3. **Network advancement**: Let the network advance to epoch N+10 with epoch transitions
4. **Peer configuration**: Configure all active peers to only advertise epoch-ending ledger infos for epochs >= N+5
5. **Restart**: Restart the validator node
6. **Observe**: The validator will attempt to bootstrap from epoch N, requesting epoch-ending ledger infos starting at N+1
7. **Failure**: The `EpochEndingStreamEngine::new()` will fail with `DataIsUnavailable` because highest advertised epoch (N+5) doesn't cover the required range starting at N+1
8. **Result**: The validator enters an infinite retry loop, unable to progress

**Validation steps in code:**
- Set breakpoint in `stream_engine.rs` line 1496-1500
- Verify that `end_epoch < request.start_epoch` condition is triggered
- Observe error propagation through the stack
- Confirm driver retry behavior without recovery

---

**Notes:**
This is a protocol-level liveness issue rather than an exploitable attack vector. It represents a design limitation where validators that fall significantly behind cannot automatically recover if historical epoch data is unavailable. While not directly exploitable by malicious actors, it creates operational risk and reduces network resilience.

### Citations

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L814-876)
```rust
    async fn fetch_epoch_ending_ledger_infos(
        &mut self,
        global_data_summary: &GlobalDataSummary,
    ) -> Result<(), Error> {
        // Verify the waypoint can be satisfied
        self.verify_waypoint_is_satisfiable(global_data_summary)?;

        // Get the highest advertised epoch that has ended
        let highest_advertised_epoch_end = global_data_summary
            .advertised_data
            .highest_epoch_ending_ledger_info()
            .ok_or_else(|| {
                Error::AdvertisedDataError(
                    "No highest advertised epoch end found in the network!".into(),
                )
            })?;

        // Fetch the highest epoch end known locally
        let highest_known_ledger_info = self.get_highest_known_ledger_info()?;
        let highest_known_ledger_info = highest_known_ledger_info.ledger_info();
        let highest_local_epoch_end = if highest_known_ledger_info.ends_epoch() {
            highest_known_ledger_info.epoch()
        } else if highest_known_ledger_info.epoch() > 0 {
            highest_known_ledger_info
                .epoch()
                .checked_sub(1)
                .ok_or_else(|| {
                    Error::IntegerOverflow("The highest local epoch end has overflown!".into())
                })?
        } else {
            unreachable!("Genesis should always end the first epoch!");
        };

        // Compare the highest local epoch end to the highest advertised epoch end
        if highest_local_epoch_end < highest_advertised_epoch_end {
            info!(LogSchema::new(LogEntry::Bootstrapper).message(&format!(
                "Found higher epoch ending ledger infos in the network! Local: {:?}, advertised: {:?}",
                   highest_local_epoch_end, highest_advertised_epoch_end
            )));
            let next_epoch_end = highest_local_epoch_end.checked_add(1).ok_or_else(|| {
                Error::IntegerOverflow("The next epoch end has overflown!".into())
            })?;
            let epoch_ending_stream = self
                .streaming_client
                .get_all_epoch_ending_ledger_infos(next_epoch_end)
                .await?;
            self.active_data_stream = Some(epoch_ending_stream);
        } else if self.verified_epoch_states.verified_waypoint() {
            info!(LogSchema::new(LogEntry::Bootstrapper).message(
                "No new epoch ending ledger infos to fetch! All peers are in the same epoch!"
            ));
            self.verified_epoch_states
                .set_fetched_epoch_ending_ledger_infos();
        } else {
            return Err(Error::AdvertisedDataError(format!(
                "Our waypoint is unverified, but there's no higher epoch ending ledger infos \
                advertised! Highest local epoch end: {:?}, highest advertised epoch end: {:?}",
                highest_local_epoch_end, highest_advertised_epoch_end
            )));
        };

        Ok(())
    }
```

**File:** state-sync/data-streaming-service/src/stream_engine.rs (L1487-1501)
```rust
        let end_epoch = advertised_data
            .highest_epoch_ending_ledger_info()
            .ok_or_else(|| {
                Error::DataIsUnavailable(format!(
                    "Unable to find any epoch ending ledger info in the network: {:?}",
                    advertised_data
                ))
            })?;

        if end_epoch < request.start_epoch {
            return Err(Error::DataIsUnavailable(format!(
                "The epoch to start syncing from is higher than the highest epoch ending ledger info! Highest: {:?}, start: {:?}",
                end_epoch, request.start_epoch
            )));
        }
```

**File:** state-sync/data-streaming-service/src/streaming_client.rs (L350-359)
```rust
    async fn get_all_epoch_ending_ledger_infos(
        &self,
        start_epoch: u64,
    ) -> Result<DataStreamListener, Error> {
        let client_request =
            StreamRequest::GetAllEpochEndingLedgerInfos(GetAllEpochEndingLedgerInfosRequest {
                start_epoch,
            });
        self.send_request_and_await_response(client_request).await
    }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L711-719)
```rust
        } else if let Err(error) = self.bootstrapper.drive_progress(&global_data_summary).await {
            sample!(
                    SampleRate::Duration(Duration::from_secs(DRIVER_ERROR_LOG_FREQ_SECS)),
                    warn!(LogSchema::new(LogEntry::Driver)
                        .error(&error)
                        .message("Error found when checking the bootstrapper progress!"));
            );
            metrics::increment_counter(&metrics::BOOTSTRAPPER_ERRORS, error.get_label());
        };
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L75-112)
```rust
    pub async fn handle_transaction_notification<
        M: MempoolNotificationSender,
        S: StorageServiceNotificationSender,
    >(
        events: Vec<ContractEvent>,
        transactions: Vec<Transaction>,
        latest_synced_version: Version,
        latest_synced_ledger_info: LedgerInfoWithSignatures,
        mut mempool_notification_handler: MempoolNotificationHandler<M>,
        event_subscription_service: Arc<Mutex<EventSubscriptionService>>,
        mut storage_service_notification_handler: StorageServiceNotificationHandler<S>,
    ) -> Result<(), Error> {
        // Log the highest synced version and timestamp
        let blockchain_timestamp_usecs = latest_synced_ledger_info.ledger_info().timestamp_usecs();
        debug!(
            LogSchema::new(LogEntry::NotificationHandler).message(&format!(
                "Notifying the storage service, mempool and the event subscription service of version: {:?} and timestamp: {:?}.",
                latest_synced_version, blockchain_timestamp_usecs
            ))
        );

        // Notify the storage service of the committed transactions
        storage_service_notification_handler
            .notify_storage_service_of_committed_transactions(latest_synced_version)
            .await?;

        // Notify mempool of the committed transactions
        mempool_notification_handler
            .notify_mempool_of_committed_transactions(transactions, blockchain_timestamp_usecs)
            .await?;

        // Notify the event subscription service of the events
        event_subscription_service
            .lock()
            .notify_events(latest_synced_version, events)?;

        Ok(())
    }
```

**File:** consensus/src/epoch_manager.rs (L1912-1920)
```rust
    async fn await_reconfig_notification(&mut self) {
        let reconfig_notification = self
            .reconfig_events
            .next()
            .await
            .expect("Reconfig sender dropped, unable to start new epoch");
        self.start_new_epoch(reconfig_notification.on_chain_configs)
            .await;
    }
```
