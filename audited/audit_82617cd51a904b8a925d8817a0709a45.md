# Audit Report

## Title
Silent Transaction Loss in Indexer-GRPC V2 File Store Backfiller Due to Missing Stream Completion Validation

## Summary
The indexer-grpc-v2-file-store-backfiller fails to validate that all requested transactions were received before updating its progress file. A malicious or faulty fullnode can terminate the gRPC stream early without error, causing the backfiller to silently skip transactions and create permanent gaps in the file store.

## Finding Description

The vulnerability exists in the `backfill()` function where transaction streaming and progress tracking are handled without proper validation. [1](#0-0) 

The stream processing loop exits when `stream.next().await` returns `None`, which occurs when the stream terminates normally. However, there is **no validation** that the expected number of transactions (`num_transactions_per_folder`) were actually received before the stream ended.

After the stream loop completes, the code simply logs that backfilling is "finished" and the spawned task completes successfully. The main loop then unconditionally increments the version counter and updates the progress file: [2](#0-1) 

The `FileStoreOperatorV2` validates transaction sequence continuity within received transactions, but has no mechanism to enforce that all expected transactions must be received: [3](#0-2) 

**Attack Path:**
1. Backfiller requests transactions for versions [V, V+N) where N = `num_transactions_per_folder`
2. Malicious fullnode sends only M transactions (M < N)
3. Fullnode closes the gRPC stream normally (not with an error)
4. The `while let Some(response_item) = stream.next().await` loop exits when `None` is received
5. FileStoreOperatorV2 has only processed M transactions (version is now V+M)
6. If buffered transactions weren't flushed (buffer not full AND not at batch boundary), they are lost when the operator is dropped
7. Task completes, progress file is updated to V+N
8. Transactions [V+M, V+N) are permanently skipped

This breaks the **Data Integrity Invariant**: The file store must contain a complete, gap-free transaction history for downstream indexing consumers.

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos Bug Bounty criteria: "State inconsistencies requiring intervention."

**Concrete Impact:**
- **Permanent Data Gaps**: Transactions that fall in the skipped range are never backfilled because the progress file has moved past them
- **Silent Failure**: No error is logged, making detection difficult without external monitoring
- **Downstream Corruption**: Indexers, analytics tools, and applications relying on this file store receive incomplete transaction history
- **Manual Recovery Required**: Operators must manually identify gaps, reset progress files, and re-run backfills

The vulnerability does not directly affect consensus, validator operations, or on-chain state, but creates persistent inconsistencies in critical off-chain infrastructure used throughout the Aptos ecosystem.

## Likelihood Explanation

**Likelihood: Medium to High**

The attack requires:
1. **Malicious or Faulty Fullnode**: The backfiller must connect to a compromised fullnode OR a fullnode with buggy gRPC stream implementation
2. **No Special Privileges**: Any fullnode the backfiller connects to can execute this attack
3. **Simple Execution**: The fullnode simply sends partial data and closes the stream

**Realistic Scenarios:**
- Misconfigured or buggy fullnode implementations that don't properly handle `transactions_count` limits
- Network interruptions that terminate streams prematurely without proper error signaling
- Compromised fullnodes in the indexer infrastructure
- Malicious operators running fullnodes specifically to corrupt indexer data

The lack of any safeguards makes this highly exploitable once the preconditions are met.

## Recommendation

Add validation after stream completion to ensure all expected transactions were received:

```rust
// After the stream loop (line 199), before logging success:
let expected_version = task_version + num_transactions_per_folder;
ensure!(
    file_store_operator.version() == expected_version,
    "Stream terminated early: expected version {}, got {}. Missing {} transactions.",
    expected_version,
    file_store_operator.version(),
    expected_version - file_store_operator.version()
);

// Also add a forced flush of any remaining buffered transactions
if !file_store_operator.buffer_is_empty() {
    file_store_operator
        .force_flush_buffer(tx.clone())
        .await?;
}
```

Additionally, implement batch-level validation before progress file updates:

```rust
// Before updating progress file (before line 212):
// Verify all spawned tasks completed successfully and processed expected counts
// Track actual processed versions per task and validate against expected ranges
```

The v1 backfiller provides a better pattern with explicit `finished_starting_versions` tracking that should be adopted. [4](#0-3) 

## Proof of Concept

```rust
// Minimal PoC demonstrating the vulnerability
// This would be added to the backfiller integration tests

#[tokio::test]
async fn test_malicious_fullnode_early_termination() {
    use futures::stream;
    use tonic::Status;
    
    // Mock a malicious fullnode that sends only partial transactions
    let malicious_stream = stream::iter(vec![
        // Send init status
        Ok(TransactionsFromNodeResponse {
            chain_id: 1,
            response: Some(Response::Status(StreamStatus {
                r#type: StatusType::Init as i32,
                start_version: 0,
                end_version: None,
            })),
        }),
        // Send only 500 out of expected 1000 transactions
        Ok(TransactionsFromNodeResponse {
            chain_id: 1,
            response: Some(Response::Data(TransactionsOutput {
                transactions: generate_mock_transactions(0, 500),
            })),
        }),
        // Stream ends here - no more items (simulates early termination)
    ]);
    
    // Create backfiller configured to expect 1000 transactions per batch
    let mut processor = Processor::new(
        /* ... config ... */
        num_transactions_per_folder: 1000,
        /* ... */
    ).await.unwrap();
    
    // Run backfill with malicious stream
    processor.backfill().await.unwrap();
    
    // BUG: Progress file shows version 1000 (full batch complete)
    let progress = read_progress_file(&progress_file_path).unwrap();
    assert_eq!(progress.version, 1000);
    
    // BUG: Only 500 transactions actually exist in file store
    let actual_count = count_transactions_in_store(&file_store, 0, 1000).await;
    assert_eq!(actual_count, 500); // Should be 1000!
    
    // Transactions 500-999 are permanently lost - gap created
}
```

**Notes**

This vulnerability is specific to the v2 file store backfiller implementation. The v1 backfiller has more robust validation with batch completion tracking and explicit `BatchEnd` status signals that mitigate this issue. The indexer infrastructure should standardize on the safer v1 pattern or add equivalent validation to v2.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-v2-file-store-backfiller/src/processor.rs (L173-199)
```rust
                        while let Some(response_item) = stream.next().await {
                            match response_item {
                                Ok(r) => {
                                    assert!(r.chain_id == chain_id);
                                    match r.response.unwrap() {
                                        Response::Data(data) => {
                                            let transactions = data.transactions;
                                            for transaction in transactions {
                                                file_store_operator
                                                    .buffer_and_maybe_dump_transactions_to_file(
                                                        transaction,
                                                        tx.clone(),
                                                    )
                                                    .await
                                                    .unwrap();
                                            }
                                        },
                                        Response::Status(_) => {
                                            continue;
                                        },
                                    }
                                },
                                Err(e) => {
                                    panic!("Error when getting transactions from fullnode: {e}.")
                                },
                            }
                        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-v2-file-store-backfiller/src/processor.rs (L207-220)
```rust
                    version += self.num_transactions_per_folder;
                }
            });

            // Update the progress file.
            let progress_file = ProgressFile {
                version,
                backfill_id: self.backfill_id,
            };
            let bytes =
                serde_json::to_vec(&progress_file).context("Failed to serialize progress file.")?;
            std::fs::write(&self.progress_file_path, &bytes)
                .context("Failed to write progress file.")?;
            info!("Progress file updated to version {}.", version,);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_operator.rs (L43-64)
```rust
    pub async fn buffer_and_maybe_dump_transactions_to_file(
        &mut self,
        transaction: Transaction,
        tx: Sender<(Vec<Transaction>, BatchMetadata, bool)>,
    ) -> Result<()> {
        let end_batch = (transaction.version + 1) % self.num_txns_per_folder == 0;
        let size_bytes = transaction.encoded_len();
        ensure!(
            self.version == transaction.version,
            "Gap is found when buffering transaction, expected: {}, actual: {}",
            self.version,
            transaction.version,
        );
        self.buffer.push(transaction);
        self.buffer_size_in_bytes += size_bytes;
        self.version += 1;
        if self.buffer_size_in_bytes >= self.max_size_per_file || end_batch {
            self.dump_transactions_to_file(end_batch, tx).await?;
        }

        Ok(())
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store-backfiller/src/processor.rs (L149-213)
```rust
        let finished_starting_versions = Arc::new(Mutex::new(BTreeSet::new()));
        let chain_id = self.chain_id;
        let ending_version = self.ending_version;

        let mut grpc_stream = self.grpc_stream.take().expect("Stream is not initialized.");
        let init_frame = grpc_stream
            .next()
            .await
            .expect("Failed to get the first frame")?
            .response
            .unwrap();
        match init_frame {
            Response::Status(signal) => {
                if signal.r#type() != StatusType::Init {
                    anyhow::bail!("Unexpected status signal type");
                }
            },
            _ => {
                anyhow::bail!("Unexpected response type");
            },
        }
        let mut tasks = Vec::new();
        let receiver_ref = std::sync::Arc::new(Mutex::new(receiver));
        let file_store_operator = self.file_store_operator.clone_box();
        for _ in 0..self.backfill_processing_task_count {
            tracing::info!("Creating a new task");
            let mut current_file_store_operator = file_store_operator.clone_box();
            let current_finished_starting_versions = finished_starting_versions.clone();
            let receiver_ref = receiver_ref.clone();
            let task = tokio::spawn(async move {
                tracing::info!("Task started");
                loop {
                    let transactions = {
                        let mut receiver = receiver_ref.lock().await;
                        // Connection may end.
                        let transactions = match receiver.recv().await {
                            Some(transactions) => transactions,
                            None => return Ok(()),
                        };
                        // Data quality check.
                        ensure!(transactions.len() == 1000, "Unexpected transaction count");
                        ensure!(
                            transactions[0].version % 1000 == 0,
                            "Unexpected starting version"
                        );
                        for (ide, t) in transactions.iter().enumerate() {
                            ensure!(
                                t.version == transactions[0].version + ide as u64,
                                "Unexpected version"
                            );
                        }
                        transactions
                    };
                    let starting_version = transactions[0].version;
                    // If uploading failure, crash the process and let k8s restart it.
                    current_file_store_operator
                        .upload_transaction_batch(chain_id, transactions)
                        .await
                        .unwrap();
                    {
                        let mut finished_starting_versions =
                            current_finished_starting_versions.lock().await;
                        finished_starting_versions.insert(starting_version);
                    }
                }
```
