# Audit Report

## Title
QuorumStoreDB Lacks Global Quota Enforcement Leading to Unbounded Database Growth with Validator Count

## Summary
The QuorumStore batch storage system implements per-validator quotas but lacks global database size limits. Each validator receives a full quota allocation (default: 300 MB), allowing total database growth to scale linearly with validator count. With the maximum validator set size of 65,536, this enables up to 19.66 TB of batch data storage, risking disk exhaustion and validator node degradation.

## Finding Description

The QuorumStore consensus component stores transaction batches in QuorumStoreDB. The quota enforcement mechanism is implemented in `BatchStore` using per-validator `QuotaManager` instances, not at the database layer. [1](#0-0) 

When batches are inserted, the system creates or retrieves a `QuotaManager` for each validator with the FULL quota allocation: [2](#0-1) 

The default quota values are configured as: [3](#0-2) 

The maximum validator set size is capped at 65,536: [4](#0-3) 

The `QuorumStoreDB` layer itself performs no quota checks - it only provides read/write operations: [5](#0-4) 

**Attack Path:**
1. Malicious validators join the network (or existing validators behave maliciously)
2. Each validator creates batches up to their 300 MB quota limit
3. Batches are persisted to disk via `save_batch_v2()` without global size checks
4. With N validators, total database grows to N × 300 MB
5. All validator nodes experience disk pressure and I/O degradation

**Broken Invariant:** "Resource Limits: All operations must respect gas, storage, and computational limits" - The system fails to enforce a global storage limit, allowing unbounded growth proportional to validator count.

## Impact Explanation

This qualifies as **High Severity** per the Aptos bug bounty criteria:

**"Validator node slowdowns"** - As disk space fills and I/O pressure increases, all validator nodes experience performance degradation. The impact scales with:

- **100 validators:** 30 GB total database size
- **1,000 validators:** 300 GB total database size  
- **10,000 validators:** 3 TB total database size
- **65,536 validators (max):** 19.66 TB total database size

Real-world impact includes:
- Disk exhaustion causing node crashes
- Increased I/O latency affecting consensus participation
- Cascading failures as nodes struggle with storage
- Need for emergency intervention to clear batches

While batches expire and are garbage collected, within a single epoch, coordinated or organic growth can exhaust available disk space before cleanup occurs.

## Likelihood Explanation

**Likelihood: Medium to High**

The attack requires:
- Being a validator (or multiple validators coordinating)
- Creating batches up to quota limits (trivial operation)
- No special cryptographic material or byzantine consensus control needed

Factors increasing likelihood:
- Natural network growth increases validator count
- No alerting when approaching global capacity limits
- Each validator independently makes quota decisions
- Attack can be disguised as legitimate high transaction volume
- Organic growth (non-malicious) can trigger the same issue

The vulnerability manifests both through:
1. **Malicious attack:** Coordinated validators intentionally filling quotas
2. **Organic scaling:** Normal network growth with many active validators

## Recommendation

Implement a global database quota that limits total batch storage across all validators:

```rust
pub struct BatchStore {
    // ... existing fields ...
    global_db_quota: usize,
    global_db_used: Arc<AtomicUsize>, // Track total usage across all peers
    // ... rest of fields ...
}

impl BatchStore {
    pub(crate) fn insert_to_cache(
        &self,
        value: &PersistedValue<BatchInfoExt>,
    ) -> anyhow::Result<bool> {
        // ... existing logic ...
        
        // Check global quota BEFORE per-peer quota
        let current_global_used = self.global_db_used.load(Ordering::Acquire);
        if current_global_used + value.num_bytes() as usize > self.global_db_quota {
            counters::EXCEEDED_GLOBAL_STORAGE_QUOTA_COUNT.inc();
            bail!("Global storage quota exceeded");
        }
        
        // ... continue with existing per-peer quota logic ...
        
        // Update global counter on success
        self.global_db_used.fetch_add(value.num_bytes() as usize, Ordering::Release);
        
        // ... rest of logic ...
    }
    
    fn free_quota(&self, value: PersistedValue<BatchInfoExt>) {
        // ... existing per-peer quota freeing ...
        
        // Free global quota
        self.global_db_used.fetch_sub(value.num_bytes() as usize, Ordering::Release);
    }
}
```

Add configuration:
```rust
pub struct QuorumStoreConfig {
    // ... existing fields ...
    pub global_db_quota: usize, // e.g., 10 GB total across all validators
    // ... rest of fields ...
}
```

This ensures bounded total storage regardless of validator count while maintaining per-validator fairness through existing per-peer quotas.

## Proof of Concept

```rust
#[test]
fn test_unbounded_database_growth_with_multiple_validators() {
    use crate::quorum_store::{
        batch_store::BatchStore,
        quorum_store_db::QuorumStoreDB,
        types::PersistedValue,
    };
    use aptos_consensus_types::proof_of_store::BatchInfo;
    use aptos_crypto::HashValue;
    use aptos_temppath::TempPath;
    use aptos_types::{
        account_address::AccountAddress,
        quorum_store::BatchId,
        validator_verifier::random_validator_verifier,
    };
    use std::sync::Arc;

    let tmp_dir = TempPath::new();
    let db = Arc::new(QuorumStoreDB::new(&tmp_dir));
    let (signers, _) = random_validator_verifier(4, None, false);
    
    let batch_store = Arc::new(BatchStore::new(
        1,      // epoch
        false,  // is_new_epoch
        10,     // last_certified_time
        db,
        120_000_000,  // memory_quota: 120 MB
        300_000_000,  // db_quota: 300 MB per validator
        300_000,      // batch_quota
        signers[0].clone(),
        0,
    ));

    // Simulate 100 validators each filling their 300 MB quota
    // Total: 30 GB of database growth with no global limit
    let num_validators = 100;
    let quota_per_validator = 300_000_000; // 300 MB
    
    for validator_idx in 0..num_validators {
        let author = AccountAddress::random();
        let mut total_bytes_for_validator = 0;
        let mut batch_count = 0;
        
        // Fill this validator's quota
        while total_bytes_for_validator < quota_per_validator {
            let batch_size = 1_000_000; // 1 MB batches
            let digest = HashValue::random();
            
            let batch_info = BatchInfo::new(
                author,
                BatchId::new_for_test(batch_count),
                1,  // epoch
                1000000,  // expiration (far future)
                digest,
                100,  // num_txns
                batch_size,
                0,
            );
            
            let persisted_value = PersistedValue::new(batch_info.into(), None);
            
            // This succeeds because each validator gets their own 300 MB quota
            match batch_store.insert_to_cache(&persisted_value) {
                Ok(_) => {
                    total_bytes_for_validator += batch_size as usize;
                    batch_count += 1;
                }
                Err(_) => break, // Quota exceeded for this validator
            }
        }
        
        println!("Validator {} stored {} MB", validator_idx, total_bytes_for_validator / 1_000_000);
    }
    
    // At this point, ~30 GB has been committed to database
    // With 65,536 validators (max), this would be 19.66 TB
    // No global quota prevents this unbounded growth
    
    println!("Total potential database size with max validators: {} TB", 
             (65536 * 300_000_000) / 1_000_000_000_000);
}
```

**Notes**

The vulnerability stems from architectural design where quota management is delegated per-validator without aggregate enforcement. While individual validator quotas provide fairness, the absence of a global cap creates a resource exhaustion vector that scales with network size. The issue is particularly severe because:

1. **QuorumStoreDB has no quota awareness** - It blindly persists whatever BatchStore sends
2. **Per-validator quotas accumulate** - N validators = N × quota total capacity  
3. **No global monitoring** - System lacks visibility into aggregate storage consumption
4. **Epoch-scoped lifetime** - While batches expire, long epochs allow sustained high usage

This represents a critical oversight in the resource limits invariant enforcement where per-entity limits exist but system-wide bounds are absent.

### Citations

**File:** consensus/src/quorum_store/batch_store.rs (L52-62)
```rust
    pub(crate) fn new(db_quota: usize, memory_quota: usize, batch_quota: usize) -> Self {
        assert!(db_quota >= memory_quota);
        Self {
            memory_balance: memory_quota,
            db_balance: db_quota,
            batch_balance: batch_quota,
            memory_quota,
            db_quota,
            batch_quota,
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L383-391)
```rust
            let value_to_be_stored = if self
                .peer_quota
                .entry(author)
                .or_insert(QuotaManager::new(
                    self.db_quota,
                    self.memory_quota,
                    self.batch_quota,
                ))
                .update_quota(value.num_bytes() as usize)?
```

**File:** config/src/config/quorum_store_config.rs (L133-135)
```rust
            memory_quota: 120_000_000,
            db_quota: 300_000_000,
            batch_quota: 300_000,
```

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L98-100)
```text
    /// Limit the maximum size to u16::max, it's the current limit of the bitvec
    /// https://github.com/aptos-labs/aptos-core/blob/main/crates/aptos-bitvec/src/lib.rs#L20
    const MAX_VALIDATOR_SET_SIZE: u64 = 65536;
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L110-117)
```rust
    fn save_batch(&self, batch: PersistedValue<BatchInfo>) -> Result<(), DbError> {
        trace!(
            "QS: db persists digest {} expiration {:?}",
            batch.digest(),
            batch.expiration()
        );
        self.put::<BatchSchema>(batch.digest(), &batch)
    }
```
