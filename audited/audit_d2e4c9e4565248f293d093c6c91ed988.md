# Audit Report

## Title
Unhandled Network Failures in Remote Cross-Shard Message Passing Lead to Node Crashes and State Synchronization Hangs

## Summary
The remote cross-shard communication system lacks error handling for network failures, causing validator nodes to panic and crash when GRPC operations fail, and causing indefinite hangs when messages are lost. This breaks the availability guarantees required for consensus participation.

## Finding Description

The sharded block executor can operate in remote mode where different shards execute on separate processes/machines and communicate via GRPC. The cross-shard message system transmits critical state updates (`RemoteTxnWrite` containing `StateKey` and `WriteOp`) that dependent shards need to maintain consistent state views.

**Critical Code Paths with Missing Error Handling:**

1. **GRPC Send Failure - Immediate Panic** [1](#0-0) 

When network failures occur (connection refused, timeout, remote server down), the `send_message()` function panics instead of handling the error gracefully, immediately crashing the sender node.

2. **Channel Receive Failure - Panic on Disconnection** [2](#0-1) 

The `receive_cross_shard_msg()` function blocks waiting for messages using `.recv().unwrap()`. If the sender crashes or the channel disconnects before all messages arrive, this causes a panic on the receiver side.

3. **Channel Send Failure - Cascading Panics** [3](#0-2) 

If the outbound handler crashes or channel is closed, the send operation panics.

4. **Inbound Handler Silent Drops with Successful Response** [4](#0-3) 

When no handler is registered for a message type, the message is silently dropped but GRPC returns success to the sender, creating a silent message loss scenario.

**Attack Scenario - Operational Failure:**

While this vulnerability requires network-level conditions rather than direct attacker exploitation, the operational failure mode creates availability issues:

1. Validator runs sharded execution across multiple processes/machines
2. Shard A executes transaction writing to state key needed by Shard B
3. `CrossShardCommitSender` sends `RemoteTxnWriteMsg` via GRPC
4. Network partition/timeout occurs between machines
5. **GRPC send fails → Shard A process panics and crashes**
6. Partial messages (1-4) already delivered to Shard B
7. Remaining messages (5-10) never sent due to crash
8. **Shard B's `CrossShardCommitReceiver` hangs forever waiting for message 5**
9. Block execution never completes
10. Validator cannot vote on block → liveness failure [5](#0-4) 

The `CrossShardCommitReceiver::start()` function runs in an infinite loop calling `receive_cross_shard_msg()`, which will block indefinitely if a message is lost.

## Impact Explanation

**Severity: High** (Validator node slowdowns / API crashes / Significant protocol violations)

This issue causes:
- **Total loss of liveness for affected validators**: Node crashes or hangs prevent block execution completion
- **Validator set degradation**: Multiple validators experiencing network issues cannot participate in consensus
- **Non-deterministic execution failures**: Validators with reliable networks succeed while others fail

This does NOT qualify as **Critical** severity because:
- It does not cause consensus safety violations (different state roots) - failed validators crash rather than producing incorrect results
- It does not allow unprivileged attackers to exploit the system without network-level access (which is out of scope)
- It is an operational robustness issue rather than a cryptographic or consensus protocol vulnerability

The bug bounty program explicitly excludes "Network-level DoS attacks" which would be required for an unprivileged attacker to exploit this.

## Likelihood Explanation

**Likelihood: Medium in distributed deployments**

- Network failures are common in distributed systems (timeouts, partitions, packet loss)
- The code uses `.unwrap()` universally with no error handling or retry logic
- Developers acknowledge the issue with a TODO comment: "TODO: Retry with exponential backoff on failures" [6](#0-5) 

However:
- Remote sharded execution may not be used in production (unclear from codebase)
- Local sharded execution (same process) is not affected
- Requires operational deployment across multiple machines

## Recommendation

**Implement comprehensive error handling with retry logic:**

1. **Change trait signatures to return Results:**
```rust
pub trait CrossShardClient: Send + Sync {
    fn send_global_msg(&self, msg: CrossShardMsg) -> Result<(), Error>;
    fn send_cross_shard_msg(&self, shard_id: ShardId, round: RoundId, msg: CrossShardMsg) -> Result<(), Error>;
    fn receive_cross_shard_msg(&self, current_round: RoundId) -> Result<CrossShardMsg, Error>;
}
```

2. **Add retry logic with exponential backoff in GRPC client:**
```rust
pub async fn send_message(&mut self, sender_addr: SocketAddr, message: Message, mt: &MessageType) -> Result<(), Error> {
    let request = tonic::Request::new(NetworkMessage {
        message: message.data,
        message_type: mt.get_type(),
    });
    
    let mut retry_count = 0;
    let max_retries = 5;
    
    loop {
        match self.remote_channel.simple_msg_exchange(request.clone()).await {
            Ok(_) => return Ok(()),
            Err(e) if retry_count < max_retries => {
                warn!("Retry {}/{} after error: {}", retry_count, max_retries, e);
                tokio::time::sleep(Duration::from_millis(100 * 2_u64.pow(retry_count))).await;
                retry_count += 1;
            },
            Err(e) => return Err(Error::NetworkFailure(e.to_string())),
        }
    }
}
```

3. **Add timeout and error propagation in receive:**
```rust
fn receive_cross_shard_msg(&self, current_round: RoundId) -> Result<CrossShardMsg, Error> {
    let rx = self.message_rxs[current_round].lock().unwrap();
    let message = rx.recv_timeout(Duration::from_secs(60))
        .map_err(|e| Error::ChannelRecvTimeout(e))?;
    let msg: CrossShardMsg = bcs::from_bytes(&message.to_bytes())
        .map_err(|e| Error::DeserializationError(e))?;
    Ok(msg)
}
```

4. **Add message acknowledgment system to detect lost messages**
5. **Implement graceful degradation instead of panics**

## Proof of Concept

```rust
#[cfg(test)]
mod test_network_failure {
    use super::*;
    use std::thread;
    use std::time::Duration;

    #[test]
    fn test_network_failure_causes_panic() {
        // Setup: Create two remote executor services
        let shard0_addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 8000);
        let shard1_addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 8001);
        
        let mut controller0 = NetworkController::new("shard0".to_string(), shard0_addr, 1000);
        let cross_shard_client = RemoteCrossShardClient::new(&mut controller0, vec![shard1_addr]);
        
        // Shard 1 is NOT started - simulating network failure
        
        // Attempt to send cross-shard message
        let msg = CrossShardMsg::RemoteTxnWriteMsg(RemoteTxnWrite::new(
            StateKey::raw(b"test_key"),
            Some(WriteOp::Deletion),
        ));
        
        // This should panic with "Error sending message to 127.0.0.1:8001"
        // Instead of returning an error that can be handled
        let result = std::panic::catch_unwind(|| {
            cross_shard_client.send_cross_shard_msg(0, 0, msg);
        });
        
        assert!(result.is_err(), "Expected panic on network failure");
    }

    #[test]
    fn test_channel_disconnect_causes_hang() {
        // Setup receiver
        let mut controller = NetworkController::new("test".to_string(), 
            SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 8002), 1000);
        let cross_shard_client = RemoteCrossShardClient::new(&mut controller, vec![]);
        controller.start();
        
        // Spawn thread that will hang forever
        let handle = thread::spawn(move || {
            // This will block indefinitely if no message arrives
            cross_shard_client.receive_cross_shard_msg(0)
        });
        
        // Wait to demonstrate hang (in real test, would timeout)
        thread::sleep(Duration::from_secs(2));
        
        // Thread is still blocked, cannot be joined without timeout
        assert!(!handle.is_finished(), "Thread should be blocked waiting for message");
    }
}
```

## Notes

While this represents a legitimate reliability and operational robustness issue in the remote sharded execution system, it does not fully meet the strictest criteria for bug bounty eligibility because:

1. **Network-level attacks are explicitly out of scope** - Causing network failures between validator infrastructure requires network-level access
2. **Validator infrastructure is considered trusted** - The remote execution service is part of internal validator operations  
3. **Impact is availability/liveness, not safety** - Failed validators crash rather than producing incorrect results
4. **Developers appear aware** - TODO comment indicates known limitation

However, this still represents a **High severity operational security issue** that should be addressed to ensure validator reliability and network availability, even if it may not qualify for the maximum bug bounty rewards.

### Citations

**File:** secure/net/src/grpc_network_service/mod.rs (L105-114)
```rust
        if let Some(handler) = self.inbound_handlers.lock().unwrap().get(&message_type) {
            // Send the message to the registered handler
            handler.send(msg).unwrap();
        } else {
            error!(
                "No handler registered for sender: {:?} and msg type {:?}",
                remote_addr, message_type
            );
        }
        Ok(Response::new(Empty {}))
```

**File:** secure/net/src/grpc_network_service/mod.rs (L140-160)
```rust
    pub async fn send_message(
        &mut self,
        sender_addr: SocketAddr,
        message: Message,
        mt: &MessageType,
    ) {
        let request = tonic::Request::new(NetworkMessage {
            message: message.data,
            message_type: mt.get_type(),
        });
        // TODO: Retry with exponential backoff on failures
        match self.remote_channel.simple_msg_exchange(request).await {
            Ok(_) => {},
            Err(e) => {
                panic!(
                    "Error '{}' sending message to {} on node {:?}",
                    e, self.remote_addr, sender_addr
                );
            },
        }
    }
```

**File:** execution/executor-service/src/remote_cross_shard_client.rs (L55-59)
```rust
    fn send_cross_shard_msg(&self, shard_id: ShardId, round: RoundId, msg: CrossShardMsg) {
        let input_message = bcs::to_bytes(&msg).unwrap();
        let tx = self.message_txs[shard_id][round].lock().unwrap();
        tx.send(Message::new(input_message)).unwrap();
    }
```

**File:** execution/executor-service/src/remote_cross_shard_client.rs (L61-66)
```rust
    fn receive_cross_shard_msg(&self, current_round: RoundId) -> CrossShardMsg {
        let rx = self.message_rxs[current_round].lock().unwrap();
        let message = rx.recv().unwrap();
        let msg: CrossShardMsg = bcs::from_bytes(&message.to_bytes()).unwrap();
        msg
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L26-45)
```rust
    pub fn start<S: StateView + Sync + Send>(
        cross_shard_state_view: Arc<CrossShardStateView<S>>,
        cross_shard_client: Arc<dyn CrossShardClient>,
        round: RoundId,
    ) {
        loop {
            let msg = cross_shard_client.receive_cross_shard_msg(round);
            match msg {
                RemoteTxnWriteMsg(txn_commit_msg) => {
                    let (state_key, write_op) = txn_commit_msg.take();
                    cross_shard_state_view
                        .set_value(&state_key, write_op.and_then(|w| w.as_state_value()));
                },
                CrossShardMsg::StopMsg => {
                    trace!("Cross shard commit receiver stopped for round {}", round);
                    break;
                },
            }
        }
    }
```
