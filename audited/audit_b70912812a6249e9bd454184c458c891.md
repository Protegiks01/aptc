# Audit Report

## Title
Missing fsync() in OnDiskStorage Causes Safety State Loss and Consensus Equivocation

## Summary
The `OnDiskStorage` backend for `PersistentSafetyStorage` does not call `fsync()` after writing safety-critical consensus state to disk. This creates a vulnerability window where validator crashes can cause loss of `last_voted_round` and other safety data, enabling double-voting (equivocation) that breaks consensus safety guarantees.

## Finding Description

The vulnerability exists in the persistence layer used by SafetyRules for storing consensus safety state. When a validator votes on a block proposal, the following sequence occurs:

1. **Vote Processing**: The validator receives a vote proposal through the remote service interface [1](#0-0) 

2. **Safety Data Update**: The voting logic updates critical safety state including `last_voted_round`, which enforces the fundamental rule that a validator cannot vote twice for the same round [2](#0-1) 

3. **Persistence Attempt**: The updated `SafetyData` is written to storage [3](#0-2) 

4. **Critical Bug - Missing fsync()**: When using `OnDiskStorage`, the write operation creates a temporary file, writes data, and renames it to the target file **without calling fsync()** [4](#0-3) 

5. **Response Sent**: The vote response is sent over the network before the OS flushes write buffers to persistent storage.

6. **Vulnerability Window**: If the validator process crashes or experiences power loss after sending the response but before the OS flushes the file write buffers, the safety state update is lost.

7. **Equivocation on Restart**: Upon restart, the validator loads stale safety data showing a lower `last_voted_round` value, allowing it to vote again for a round it already voted on with potentially different vote content.

**Configuration Evidence**: The `OnDiskStorage` backend is actively used in production configurations [5](#0-4)  and [6](#0-5) 

**Safety Data Structure**: The `SafetyData` struct contains all consensus safety-critical fields that must never be lost [7](#0-6) 

## Impact Explanation

**Severity: CRITICAL** (Consensus/Safety Violations - up to $1,000,000 per Aptos Bug Bounty)

This vulnerability breaks the **Consensus Safety** invariant by enabling equivocation:

1. **Double-Voting**: A validator can produce two different votes for the same round with different block hashes
2. **Chain Forks**: Equivocation can lead to conflicting quorum certificates and blockchain forks
3. **BFT Safety Violation**: The AptosBFT consensus protocol's safety guarantee assumes validators cannot equivocate; this bug violates that assumption
4. **Network Split Risk**: Different validators may commit different blocks, requiring manual intervention or hard fork to recover

The bug affects any validator configured with `OnDiskStorage` backend, which includes default Docker Compose and Helm chart deployments. Even a single equivocating validator can compromise consensus safety if it causes conflicting quorum certificates.

## Likelihood Explanation

**Likelihood: MEDIUM to HIGH**

The vulnerability requires specific timing but is inevitable over time:

**Triggering Conditions:**
- Validator configured with `OnDiskStorage` backend (common in deployments)
- Process crash, system panic, or power loss during the vulnerability window (microseconds to seconds)
- Window exists between safety data write returning and OS buffer flush completing

**Probability Factors:**
- Operating systems typically flush dirty buffers within 30 seconds by default
- Critical validators may experience crashes due to OOM, panics, hardware failures, or power issues
- With hundreds of validators and continuous operation, crashes during the window become statistically likely
- The vulnerability is **deterministic** - any crash during the window guarantees state loss

**Real-World Scenarios:**
- Kubernetes pod eviction/restart
- Out-of-memory kills
- Hardware failures (power supply, disk controller)
- Kernel panics
- Forced shutdowns during maintenance

## Recommendation

**Immediate Fix**: Add `fsync()` or `sync_all()` call after writing safety data to ensure durability before acknowledging success.

**Code Fix for `on_disk.rs`:**

Modify the `write` method to include synchronous flush:

```rust
fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
    let contents = serde_json::to_vec(data)?;
    let mut file = File::create(self.temp_path.path())?;
    file.write_all(&contents)?;
    // CRITICAL FIX: Ensure data is persisted to disk before rename
    file.sync_all()?;  // <-- ADD THIS LINE
    fs::rename(&self.temp_path, &self.file_path)?;
    Ok(())
}
```

The `sync_all()` method ensures both file data and metadata are flushed to persistent storage before the function returns, guaranteeing durability.

**Additional Recommendations:**
1. Add integration tests that simulate crashes during safety data persistence
2. Document the durability requirements for storage backends
3. Consider adding a `DurableStorage` trait to enforce fsync semantics
4. Add monitoring alerts for storage backend type to warn about `OnDiskStorage` usage in production
5. Recommend `VaultStorage` or other durable backends for production validators

## Proof of Concept

**Reproduction Steps:**

1. **Setup**: Deploy a validator with OnDiskStorage configuration
2. **Trigger Vote**: Submit a block proposal to trigger a vote in round N
3. **Capture State**: Before the vote completes, verify safety_data shows last_voted_round < N
4. **Crash Timing**: Kill the validator process with `SIGKILL` immediately after the vote response is sent (within ~100ms)
5. **Verify Loss**: Restart the validator and check the `secure-data.json` file
6. **Expected**: The file still shows last_voted_round < N (stale data)
7. **Exploit**: Submit a different block proposal for round N
8. **Equivocation**: The validator produces a second vote for round N with different content

**Rust Test Skeleton:**

```rust
#[test]
fn test_ondisk_storage_durability_violation() {
    use std::process;
    use std::sync::Arc;
    use std::sync::atomic::{AtomicBool, Ordering};
    
    let storage_path = TempPath::new();
    let mut storage = OnDiskStorage::new(storage_path.path().to_path_buf());
    
    // Simulate vote: update safety data
    let safety_data = SafetyData::new(1, 100, 90, 90, None, 0);
    storage.set(SAFETY_DATA, safety_data.clone()).unwrap();
    
    // Simulate crash BEFORE fsync completes (aggressive timing)
    // In real scenario, this would be process crash during OS buffer flush
    process::abort(); // Immediate termination without cleanup
    
    // On restart (separate test run):
    // Verify that data is lost if no fsync occurred
    let recovered_storage = OnDiskStorage::new(storage_path.path().to_path_buf());
    let recovered_data: SafetyData = recovered_storage.get(SAFETY_DATA).unwrap().value;
    
    // BUG: recovered_data.last_voted_round may be < 100 if crash happened before flush
    assert!(recovered_data.last_voted_round < safety_data.last_voted_round);
}
```

This vulnerability represents a fundamental violation of the Write-Ahead Logging (WAL) principle required for consensus safety in distributed systems.

### Citations

**File:** consensus/safety-rules/src/remote_service.rs (L47-55)
```rust
fn process_one_message(
    network_server: &mut NetworkServer,
    serializer_service: &mut SerializerService,
) -> Result<(), Error> {
    let request = network_server.read()?;
    let response = serializer_service.handle_message(request)?;
    network_server.write(&response)?;
    Ok(())
}
```

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L77-92)
```rust
        self.verify_and_update_last_vote_round(
            proposed_block.block_data().round(),
            &mut safety_data,
        )?;
        self.safe_to_vote(proposed_block, timeout_cert)?;

        // Record 1-chain data
        self.observe_qc(proposed_block.quorum_cert(), &mut safety_data);
        // Construct and sign vote
        let author = self.signer()?.author();
        let ledger_info = self.construct_ledger_info_2chain(proposed_block, vote_data.hash())?;
        let signature = self.sign(&ledger_info)?;
        let vote = Vote::new_with_signature(vote_data, author, ledger_info, signature);

        safety_data.last_vote = Some(vote.clone());
        self.persistent_storage.set_safety_data(safety_data)?;
```

**File:** consensus/safety-rules/src/persistent_safety_storage.rs (L150-170)
```rust
    pub fn set_safety_data(&mut self, data: SafetyData) -> Result<(), Error> {
        let _timer = counters::start_timer("set", SAFETY_DATA);
        counters::set_state(counters::EPOCH, data.epoch as i64);
        counters::set_state(counters::LAST_VOTED_ROUND, data.last_voted_round as i64);
        counters::set_state(
            counters::HIGHEST_TIMEOUT_ROUND,
            data.highest_timeout_round as i64,
        );
        counters::set_state(counters::PREFERRED_ROUND, data.preferred_round as i64);

        match self.internal_store.set(SAFETY_DATA, data.clone()) {
            Ok(_) => {
                self.cached_safety_data = Some(data);
                Ok(())
            },
            Err(error) => {
                self.cached_safety_data = None;
                Err(Error::SecureStorageUnexpectedError(error.to_string()))
            },
        }
    }
```

**File:** secure/storage/src/on_disk.rs (L64-70)
```rust
    fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
        let contents = serde_json::to_vec(data)?;
        let mut file = File::create(self.temp_path.path())?;
        file.write_all(&contents)?;
        fs::rename(&self.temp_path, &self.file_path)?;
        Ok(())
    }
```

**File:** terraform/helm/aptos-node/files/configs/validator-base.yaml (L14-17)
```yaml
    backend:
      type: "on_disk_storage"
      path: secure-data.json
      namespace: ~
```

**File:** docker/compose/aptos-node/validator.yaml (L11-14)
```yaml
    backend:
      type: "on_disk_storage"
      path: secure-data.json
      namespace: ~
```

**File:** consensus/consensus-types/src/safety_data.rs (L8-21)
```rust
/// Data structure for safety rules to ensure consensus safety.
#[derive(Debug, Deserialize, Eq, PartialEq, Serialize, Clone, Default)]
pub struct SafetyData {
    pub epoch: u64,
    pub last_voted_round: u64,
    // highest 2-chain round, used for 3-chain
    pub preferred_round: u64,
    // highest 1-chain round, used for 2-chain
    #[serde(default)]
    pub one_chain_round: u64,
    pub last_vote: Option<Vote>,
    #[serde(default)]
    pub highest_timeout_round: u64,
}
```
