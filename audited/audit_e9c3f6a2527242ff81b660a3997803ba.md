# Audit Report

## Title
Secret Sharing Logic Error: Duplicate Rounds Cause Silent Block Skipping and Pipeline Deadlock

## Summary
The `QueueItem::new()` function constructs an `offsets_by_round` HashMap by collecting round-to-index mappings. When `OrderedBlocks` contains blocks with duplicate rounds, the HashMap silently overwrites earlier occurrences, retaining only the last index for each round. Consequently, `set_secret_shared_key()` delivers secret keys exclusively to the last block with duplicate rounds, while earlier blocks never receive their keys, causing their decryption pipelines to deadlock indefinitely. [1](#0-0) 

## Finding Description

The vulnerability manifests through a three-stage failure chain:

**Stage 1: Silent Data Loss in Offset Mapping Construction**

In `QueueItem::new()`, the offset mapping is built using HashMap collection. When duplicate rounds exist, the `.collect()` operation silently discards earlier occurrences, keeping only the final index for each round. [1](#0-0) 

**Stage 2: Inconsistent Secret Key Distribution**

The secret share manager tracks pending rounds using a `HashSet`, which only stores unique rounds. When processing incoming blocks with duplicate rounds, the HashSet contains each round once, but multiple blocks exist. [2](#0-1) 

When `set_secret_shared_key()` is invoked, it retrieves the offset for a round and sends the secret key only to the block at that offset (the last occurrence). The round is then removed from `pending_secret_key_rounds`, marking the round as complete despite earlier blocks never receiving their keys. [3](#0-2) 

**Stage 3: Pipeline Deadlock**

Each block's decryption pipeline awaits on `secret_shared_key_rx` with `.expect()`. Blocks that never receive their secret keys will deadlock at this await point, causing the entire execution pipeline to stall. [4](#0-3) 

**How Duplicate Rounds Can Occur:**

While blockchain invariants suggest rounds should be unique along any chain path, the BlockTree implementation permits multiple blocks with the same round. It logs a warning but does not prevent insertion: [5](#0-4) 

This creates scenarios where duplicate rounds could propagate through:
- Byzantine validators crafting malformed blocks with duplicate rounds
- Implementation bugs in consensus block ordering during epoch transitions
- Race conditions in concurrent block processing
- Edge cases in DAG ordering or consensus observer synchronization

## Impact Explanation

**HIGH Severity** per Aptos bug bounty criteria:

1. **Validator Node Slowdowns/Stalls**: Affected blocks cause decryption pipeline tasks to deadlock indefinitely, stalling the execution pipeline
2. **Consensus Liveness Failure**: The execution pipeline cannot proceed, blocking consensus progress
3. **Encrypted Transaction Processing Failure**: Blocks containing encrypted transactions cannot be decrypted without secret keys

The vulnerability affects consensus liveness and validator availability, meeting the High severity threshold for "Validator node slowdowns" and "Significant protocol violations."

## Likelihood Explanation

**MEDIUM-LOW Likelihood**: The vulnerability requires OrderedBlocks to contain duplicate rounds, which violates blockchain invariants. However, multiple factors increase exploitability:

1. **No Defensive Validation**: The code lacks validation that rounds are unique within OrderedBlocks
2. **BlockTree Permits Duplicates**: The tree structure allows multiple blocks with identical rounds, logging warnings but not rejecting them
3. **No Round Uniqueness Checks**: OrderedBlocks creation paths lack round uniqueness validation
4. **Byzantine Attack Surface**: Malicious validators could craft blocks with duplicate rounds
5. **Edge Case Exposure**: Epoch transitions, reconfiguration events, and consensus observer synchronization present edge case opportunities

The lack of defensive programming transforms this from a theoretical impossibility into a latent bug triggerable by consensus implementation bugs, Byzantine behavior, or edge cases.

## Recommendation

Add explicit validation in `QueueItem::new()` to detect and reject duplicate rounds: [6](#0-5) 

Implement the following defensive checks:

```rust
pub fn new(
    ordered_blocks: OrderedBlocks,
    share_requester_handles: Option<Vec<DropGuard>>,
    pending_secret_key_rounds: HashSet<Round>,
) -> Self {
    assert!(!ordered_blocks.ordered_blocks.is_empty());
    
    // Validate no duplicate rounds
    let mut seen_rounds = HashSet::new();
    for block in &ordered_blocks.ordered_blocks {
        let round = block.round();
        assert!(
            seen_rounds.insert(round),
            "Duplicate round {} detected in OrderedBlocks - consensus invariant violated", 
            round
        );
    }
    
    let offsets_by_round: HashMap<Round, usize> = ordered_blocks
        .ordered_blocks
        .iter()
        .enumerate()
        .map(|(idx, b)| (b.round(), idx))
        .collect();
    
    // Verify mapping completeness (detect HashMap data loss)
    assert_eq!(
        offsets_by_round.len(),
        ordered_blocks.ordered_blocks.len(),
        "Internal error: offset mapping inconsistency detected"
    );
    
    Self {
        ordered_blocks,
        offsets_by_round,
        share_requester_handles,
        pending_secret_key_rounds,
    }
}
```

Additionally, add validation in `process_incoming_blocks` to ensure consistency: [7](#0-6) 

## Proof of Concept

```rust
#[cfg(test)]
mod test_duplicate_rounds_detection {
    use super::*;
    use std::collections::HashSet;
    use std::sync::Arc;
    
    // Helper to create test blocks with specified rounds
    fn create_test_block(round: u64) -> Arc<PipelinedBlock> {
        // Implementation would create a PipelinedBlock with the specified round
        // Details omitted for brevity
    }
    
    #[test]
    #[should_panic(expected = "Duplicate round")]
    fn test_duplicate_rounds_cause_panic() {
        // Create OrderedBlocks with duplicate rounds
        let block1 = create_test_block(100);
        let block2 = create_test_block(100);
        
        let ordered_blocks = OrderedBlocks {
            ordered_blocks: vec![block1, block2],
            ordered_proof: create_test_ledger_info(),
        };
        
        // With the fix, this should panic detecting duplicates
        let _queue_item = QueueItem::new(
            ordered_blocks,
            None,
            HashSet::from([100]),
        );
    }
    
    #[test]
    fn test_offset_mapping_data_loss() {
        let block1 = create_test_block(100);
        let block2 = create_test_block(100);
        
        let ordered_blocks = OrderedBlocks {
            ordered_blocks: vec![block1, block2],
            ordered_proof: create_test_ledger_info(),
        };
        
        // Without validation, this silently succeeds
        let queue_item = QueueItem::new(
            ordered_blocks,
            None,
            HashSet::from([100]),
        );
        
        // But the offset mapping only contains one entry
        assert_eq!(queue_item.offsets_by_round.len(), 1);
        // And offset(100) always returns index 1 (last occurrence)
        assert_eq!(queue_item.offset(100), 1);
        // Block at index 0 is now unreachable via offset()
    }
}
```

## Notes

This vulnerability represents a defensive programming failure where the code assumes round uniqueness without validation. While normal consensus operations should prevent duplicate rounds, the absence of validation creates a latent bug exploitable through consensus implementation bugs, Byzantine behavior, or edge cases. The fix is straightforward: add explicit validation to fail fast when invariants are violated, preventing silent corruption of the offset mapping and subsequent pipeline deadlocks.

### Citations

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L25-43)
```rust
    pub fn new(
        ordered_blocks: OrderedBlocks,
        share_requester_handles: Option<Vec<DropGuard>>,
        pending_secret_key_rounds: HashSet<Round>,
    ) -> Self {
        assert!(!ordered_blocks.ordered_blocks.is_empty());
        let offsets_by_round: HashMap<Round, usize> = ordered_blocks
            .ordered_blocks
            .iter()
            .enumerate()
            .map(|(idx, b)| (b.round(), idx))
            .collect();
        Self {
            ordered_blocks,
            offsets_by_round,
            share_requester_handles,
            pending_secret_key_rounds,
        }
    }
```

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L64-77)
```rust
    pub fn set_secret_shared_key(&mut self, round: Round, key: SecretSharedKey) {
        let offset = self.offset(round);
        if self.pending_secret_key_rounds.contains(&round) {
            observe_block(
                self.blocks()[offset].timestamp_usecs(),
                BlockStage::SECRET_SHARING_ADD_DECISION,
            );
            let block = &self.blocks_mut()[offset];
            if let Some(tx) = block.pipeline_tx().lock().as_mut() {
                tx.secret_shared_key_tx.take().map(|tx| tx.send(Some(key)));
            }
            self.pending_secret_key_rounds.remove(&round);
        }
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L112-130)
```rust
    async fn process_incoming_blocks(&mut self, blocks: OrderedBlocks) {
        let rounds: Vec<u64> = blocks.ordered_blocks.iter().map(|b| b.round()).collect();
        info!(rounds = rounds, "Processing incoming blocks.");

        let mut share_requester_handles = Vec::new();
        let mut pending_secret_key_rounds = HashSet::new();
        for block in blocks.ordered_blocks.iter() {
            let handle = self.process_incoming_block(block).await;
            share_requester_handles.push(handle);
            pending_secret_key_rounds.insert(block.round());
        }

        let queue_item = QueueItem::new(
            blocks,
            Some(share_requester_handles),
            pending_secret_key_rounds,
        );
        self.block_queue.push_back(queue_item);
    }
```

**File:** consensus/src/pipeline/decryption_pipeline_builder.rs (L115-119)
```rust
        let maybe_decryption_key = secret_shared_key_rx
            .await
            .expect("decryption key should be available");
        // TODO(ibalajiarun): account for the case where decryption key is not available
        let decryption_key = maybe_decryption_key.expect("decryption key should be available");
```

**File:** consensus/src/block_storage/block_tree.rs (L327-335)
```rust
            if let Some(old_block_id) = self.round_to_ids.get(&arc_block.round()) {
                warn!(
                    "Multiple blocks received for round {}. Previous block id: {}",
                    arc_block.round(),
                    old_block_id
                );
            } else {
                self.round_to_ids.insert(arc_block.round(), block_id);
            }
```
