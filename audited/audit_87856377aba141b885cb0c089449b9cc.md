# Audit Report

## Title
Race Condition in Parallel Token Claim Indexing Causes State Inconsistency

## Summary
The token indexer processes transaction batches in parallel, creating a race condition where later batches can complete before earlier ones. This causes the optimistic concurrency control mechanism (version-based WHERE clause) to silently reject updates from earlier transactions, resulting in the indexer displaying incorrect claim states that diverge from on-chain reality.

## Finding Description

The Aptos indexer processes token claims through a parallel batch processing architecture that violates the invariant of sequential state consistency. The vulnerability arises from the interaction between three components:

1. **Parallel Batch Processing**: The indexer runtime spawns multiple concurrent tasks to process transaction batches [1](#0-0) 

2. **Version-Based Optimistic Concurrency**: The database upsert for token claims uses a WHERE clause to prevent stale data from overwriting fresh data [2](#0-1) 

3. **Sequential Transaction Version Assignment**: Transaction versions are monotonically increasing and extracted from committed blockchain transactions [3](#0-2) 

**Attack Scenario:**

1. Transaction at version 150: User A creates a token offer to User B (stored in indexer as `amount=100, last_transaction_version=150`)
2. Transaction at version 250: User B claims the token (should update indexer to `amount=0, last_transaction_version=250`)  
3. Indexer fetches batches: Batch 1 (versions 100-199), Batch 2 (versions 200-299)
4. Both batches are processed in parallel by different tasks [4](#0-3) 
5. **Race condition**: Batch 2 completes first, writing version 250 to database
6. Batch 1 completes second, attempts to write version 150
7. The WHERE clause evaluates `250 <= 150` which is FALSE, so the update is **silently rejected**
8. **Result**: Database retains stale data from an intermediate state, missing critical state transitions

The root cause is that the WHERE clause prevents older data from overwriting newer data, but when batches process out-of-order, "older" refers to transaction order, not processing order. This causes legitimate state transitions to be dropped.

**Additional Bug**: The sorting logic for claims contains a copy-paste error [5](#0-4)  where `&a.to_address` is compared with `&a.to_address` instead of `&b.to_address`, causing incorrect sorting that can exacerbate database deadlock issues.

## Impact Explanation

**Severity: HIGH** (per Aptos Bug Bounty criteria for "Significant protocol violations")

This vulnerability causes **state inconsistency** between the blockchain and the indexer:

- **Indexer Data Integrity Violation**: The indexer will show claims as unclaimed when they were actually claimed on-chain, or vice versa
- **User Confusion**: Applications relying on indexer data will display incorrect claim states, leading to failed transaction attempts when users try to claim already-claimed tokens
- **Trust Erosion**: The indexer becomes unreliable as a source of truth for token claim states
- **No Automatic Recovery**: Once the race condition occurs, the incorrect state persists indefinitely since future updates also check the version

While this doesn't directly cause loss of funds (the on-chain state remains correct), it represents a **significant protocol violation** where the indexer - a critical infrastructure component that applications depend on - provides systematically incorrect data. This matches the HIGH severity category for protocol violations.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

The vulnerability will occur probabilistically whenever:
- Multiple batches containing claim operations are processed concurrently (controlled by `processor_tasks` configuration parameter)
- A later batch completes processing before an earlier batch (dependent on batch size, database performance, and system load)

**Factors increasing likelihood:**
- Higher `processor_tasks` values increase parallelism [6](#0-5) 
- Variable batch processing times due to database contention or complex token operations
- High transaction volume periods where multiple batches are queued
- No special attacker privileges required - normal token offer/claim operations trigger the condition

**Attacker amplification:**
An attacker can deliberately increase the probability by:
1. Creating multiple token offers in rapid succession across different transaction versions
2. Having a confederate claim the offers, creating predictable claim deletion operations
3. Timing these operations to span multiple indexer batch boundaries
4. The probabilistic nature of parallel processing ensures eventual race condition occurrence

## Recommendation

**Fix 1 (Primary): Enforce Sequential Batch Completion**

Replace parallel batch processing with sequential processing, or implement a coordination mechanism to ensure database writes occur in transaction version order:

```rust
// In runtime.rs, replace the parallel spawn pattern
// OLD CODE (lines 210-219): Multiple concurrent tasks
for _ in 0..processor_tasks {
    let other_tailer = tailer.clone();
    let task = tokio::spawn(async move { other_tailer.process_next_batch().await });
    tasks.push(task);
}

// NEW CODE: Process batches sequentially
for _ in 0..processor_tasks {
    let (num_txn, res) = tailer.process_next_batch().await;
    // Process result immediately before fetching next batch
    if let Some(result) = res {
        // Handle result synchronously
    }
}
```

**Fix 2 (Secondary): Fix Sorting Bug**

Correct the comparison in claim sorting:

```rust
// In token_processor.rs line 966
// OLD: &a.to_address
// NEW: &b.to_address
all_current_token_claims.sort_by(|a, b| {
    (
        &a.token_data_id_hash,
        &a.property_version,
        &a.from_address,
        &a.to_address,
    )
        .cmp(&(
            &b.token_data_id_hash,
            &b.property_version,
            &b.from_address,
            &b.to_address,  // FIXED: was &a.to_address
        ))
});
```

**Fix 3 (Alternative): Change Concurrency Control**

If parallel processing is required for performance, change the WHERE clause logic to use a different conflict resolution strategy, or implement a transaction-level lock mechanism to serialize writes to the same claim record.

## Proof of Concept

```rust
// Rust test demonstrating the race condition
// File: crates/indexer/src/processors/token_processor_test.rs

#[tokio::test]
async fn test_parallel_batch_race_condition() {
    let pool = setup_test_db_pool();
    let processor = TokenTransactionProcessor::new(pool.clone(), None, None);
    
    // Create mock transactions
    let txn_150 = create_mock_token_offer_txn(
        150, 
        "0xofferer",
        "0xrecipient", 
        "token_id_1",
        100 // amount
    );
    
    let txn_250 = create_mock_token_claim_txn(
        250,
        "0xofferer", 
        "0xrecipient",
        "token_id_1",
        0 // amount = 0 indicates claim
    );
    
    // Simulate parallel processing with deliberate ordering
    let handle1 = tokio::spawn({
        let processor = processor.clone();
        async move {
            // Simulate slower processing of earlier batch
            tokio::time::sleep(Duration::from_millis(100)).await;
            processor.process_transactions(vec![txn_150], 150, 150).await
        }
    });
    
    let handle2 = tokio::spawn({
        let processor = processor.clone();
        async move {
            // Process later batch first (faster)
            processor.process_transactions(vec![txn_250], 250, 250).await
        }
    });
    
    // Wait for both to complete
    let _ = tokio::join!(handle1, handle2);
    
    // Verify database state
    let claim = query_current_claim(&pool, "token_id_1", "0xofferer", "0xrecipient").await;
    
    // BUG: Expected amount=0, version=250 (claim completed)
    // ACTUAL: amount=100, version=250 or missing version 150 update
    // The earlier transaction's update was silently dropped
    assert_eq!(claim.amount, 0, "Claim should show as completed");
    assert_eq!(claim.last_transaction_version, 250, "Should reflect latest version");
}
```

The test demonstrates that when batch 2 (version 250) completes before batch 1 (version 150), the database update from version 150 is silently rejected by the WHERE clause, causing the indexer to miss critical state transitions and diverge from blockchain reality.

### Citations

**File:** crates/indexer/src/runtime.rs (L112-112)
```rust
    let processor_tasks = config.processor_tasks.unwrap();
```

**File:** crates/indexer/src/runtime.rs (L210-215)
```rust
        let mut tasks = vec![];
        for _ in 0..processor_tasks {
            let other_tailer = tailer.clone();
            let task = tokio::spawn(async move { other_tailer.process_next_batch().await });
            tasks.push(task);
        }
```

**File:** crates/indexer/src/runtime.rs (L216-219)
```rust
        let batches = match futures::future::try_join_all(tasks).await {
            Ok(res) => res,
            Err(err) => panic!("Error processing transaction batches: {:?}", err),
        };
```

**File:** crates/indexer/src/processors/token_processor.rs (L551-551)
```rust
            Some(" WHERE current_token_pending_claims.last_transaction_version <= excluded.last_transaction_version "),
```

**File:** crates/indexer/src/processors/token_processor.rs (L966-966)
```rust
                    &a.to_address,
```

**File:** crates/indexer/src/models/token_models/tokens.rs (L103-103)
```rust
            let txn_version = user_txn.info.version.0 as i64;
```
