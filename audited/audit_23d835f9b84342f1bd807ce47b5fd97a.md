# Audit Report

## Title
Network Topology Eclipse Attack via Stale Distance Metrics in Peer Monitoring Service

## Summary
The peer monitoring service client accepts network topology information (`distance_from_validators`) from peers without timestamp validation or TTL enforcement. Malicious peers can exploit this to manipulate their perceived proximity to validators, enabling eclipse attacks that compromise mempool transaction propagation and consensus observer subscriptions.

## Finding Description

The vulnerability exists in the `record_network_info_response()` function, which stores network topology information without any freshness validation: [1](#0-0) 

The function blindly accepts and stores the `distance_from_validators` metric from peer responses. While basic validation checks exist for role consistency, there is no mechanism to:
1. Timestamp the received topology information
2. Expire stale topology data via TTL
3. Cross-validate distance claims between peers
4. Detect when network topology has changed

The `NetworkInformationResponse` structure itself lacks timestamp fields: [2](#0-1) 

This stale topology information is then consumed by critical network components:

**Mempool Peer Prioritization:** The mempool uses `distance_from_validators` to prioritize peers for transaction broadcasting: [3](#0-2) [4](#0-3) 

**Consensus Observer Subscription:** The consensus observer uses the same metric to prioritize peers for consensus data subscriptions: [5](#0-4) [6](#0-5) 

### Attack Scenario:

1. **Initial Connection (T=0s):** A malicious VFN peer connects to a victim node and responds to network info requests claiming `distance_from_validators = 1` (which passes validation since the peer's role is VFN).

2. **Topology Change (T=60s):** The malicious peer loses its connection to validators, increasing its real distance to `MAX_DISTANCE_FROM_VALIDATORS` (100).

3. **Stale Information Persistence (T=120s+):** When the victim re-queries the malicious peer (every 60 seconds per default config), the attacker continues claiming `distance = 1`. The victim accepts this with only basic role validation: [7](#0-6) 

The validation only checks that distance=1 comes from a VFN on the correct network, but doesn't verify the VFN is actually connected to validators.

4. **Eclipse Effect:** The victim now preferentially:
   - Broadcasts transactions to the malicious peer (mempool)
   - Subscribes to the malicious peer for consensus data (consensus observer)
   - Ignores or deprioritizes honest peers with accurate distance metrics

5. **Network Isolation:** With multiple colluding malicious peers all claiming low distances, the victim becomes eclipsed from the honest network.

## Impact Explanation

This vulnerability qualifies as **High Severity** per Aptos bug bounty criteria:

1. **Validator Node Slowdowns:** Validators subscribing to malicious consensus observers receive delayed or censored consensus data, degrading network performance.

2. **Significant Protocol Violations:** 
   - Transaction propagation is compromised when mempool preferentially sends to malicious peers
   - Consensus observer functionality is subverted by prioritizing dishonest peers
   - Network partitioning can occur if enough nodes are eclipsed

3. **Attack Consequences:**
   - **Transaction Censorship:** Malicious peers can selectively drop transactions
   - **MEV Extraction:** Attackers can reorder or delay transaction propagation
   - **Network Partition:** Eclipsed nodes become isolated from honest validators
   - **Consensus Observer Manipulation:** Validators may receive stale or manipulated consensus data

While this doesn't directly steal funds or break consensus safety (not Critical severity), it significantly degrades network reliability and enables various griefing attacks.

## Likelihood Explanation

**High Likelihood:**

1. **Low Attack Complexity:** Attackers need only:
   - Establish peer connections (standard network operation)
   - Respond to periodic queries (every 60s by default)
   - Claim false but role-consistent distance values

2. **No Special Privileges Required:** Any node can connect as a VFN or PFN and provide false topology data.

3. **Persistent Effect:** Once stale data is cached, it persists indefinitely as long as the peer connection remains active.

4. **Validation Gaps:** The existing validation at lines 118-141 only checks role consistency, not topology correctness:
   - VFNs can claim distance=1 regardless of validator connectivity
   - PFNs can claim any distance â‰¥2 up to MAX_DISTANCE_FROM_VALIDATORS
   - No cross-validation between peers

5. **Wide Attack Surface:** The vulnerability affects:
   - All fullnodes running mempool
   - All validators using consensus observer
   - Both public and private network deployments

## Recommendation

Implement timestamp-based freshness validation and TTL enforcement:

```rust
// In peer-monitoring-service/types/src/response.rs
#[derive(Clone, Debug, Deserialize, Eq, PartialEq, Serialize)]
pub struct NetworkInformationResponse {
    pub connected_peers: BTreeMap<PeerNetworkId, ConnectionMetadata>,
    pub distance_from_validators: u64,
    pub topology_timestamp_usecs: u64, // Add timestamp when topology was observed
}

// In peer-monitoring-service/client/src/peer_states/network_info.rs
#[derive(Clone, Debug)]
pub struct NetworkInfoState {
    base_config: BaseConfig,
    network_monitoring_config: NetworkMonitoringConfig,
    recorded_network_info_response: Option<NetworkInformationResponse>,
    response_timestamp: Option<Instant>, // Add local timestamp when received
    request_tracker: Arc<RwLock<RequestTracker>>,
}

impl NetworkInfoState {
    pub fn record_network_info_response(
        &mut self,
        network_info_response: NetworkInformationResponse,
    ) {
        // Update the request tracker with a successful response
        self.request_tracker.write().record_response_success();
        
        // Record the timestamp when this response was received
        self.response_timestamp = Some(Instant::now());
        
        // Save the network info
        self.recorded_network_info_response = Some(network_info_response);
    }
    
    pub fn get_latest_network_info_response(&self) -> Option<NetworkInformationResponse> {
        // Check if the cached response is still fresh (e.g., within 5 minutes)
        const MAX_TOPOLOGY_AGE_SECS: u64 = 300;
        
        if let Some(timestamp) = self.response_timestamp {
            if timestamp.elapsed().as_secs() > MAX_TOPOLOGY_AGE_SECS {
                // Topology information is too old, treat as missing
                return None;
            }
        }
        
        self.recorded_network_info_response.clone()
    }
}
```

Additional mitigations:
1. **Cross-validation:** Compare distance claims from multiple peers to detect outliers
2. **Connectivity verification:** Periodically verify claimed validator connections
3. **Reputation scoring:** Track peer accuracy over time and penalize false claims
4. **Rate limiting:** Limit how quickly peers can update their distance values

## Proof of Concept

```rust
// Integration test demonstrating the vulnerability
#[tokio::test]
async fn test_stale_topology_eclipse_attack() {
    use aptos_peer_monitoring_service_types::{
        response::{NetworkInformationResponse, PeerMonitoringServiceResponse},
    };
    use std::collections::BTreeMap;
    
    // Setup: Create a victim node and malicious peer
    let victim_node = create_test_node(RoleType::FullNode);
    let malicious_peer = create_test_peer(PeerRole::ValidatorFullNode);
    
    // Step 1: Malicious peer initially claims distance=1 (appears close to validators)
    let malicious_response_t0 = NetworkInformationResponse {
        connected_peers: BTreeMap::new(),
        distance_from_validators: 1, // Falsely claims to be one hop from validators
    };
    
    // Victim accepts and stores this information
    victim_node.handle_peer_response(
        malicious_peer.id(),
        PeerMonitoringServiceResponse::NetworkInformation(malicious_response_t0),
    );
    
    // Step 2: Time passes, malicious peer loses validator connection
    // but continues claiming distance=1
    tokio::time::sleep(Duration::from_secs(120)).await;
    
    let malicious_response_t120 = NetworkInformationResponse {
        connected_peers: BTreeMap::new(),
        distance_from_validators: 1, // Still falsely claiming distance=1
    };
    
    victim_node.handle_peer_response(
        malicious_peer.id(),
        PeerMonitoringServiceResponse::NetworkInformation(malicious_response_t120),
    );
    
    // Step 3: Setup honest peer with correct distance
    let honest_peer = create_test_peer(PeerRole::ValidatorFullNode);
    let honest_response = NetworkInformationResponse {
        connected_peers: BTreeMap::new(),
        distance_from_validators: 2, // Truthful distance
    };
    
    victim_node.handle_peer_response(
        honest_peer.id(),
        PeerMonitoringServiceResponse::NetworkInformation(honest_response),
    );
    
    // Step 4: Verify that victim incorrectly prioritizes malicious peer
    let mempool_priorities = victim_node.get_mempool_peer_priorities();
    
    // VULNERABILITY: Malicious peer is prioritized due to falsely claimed low distance
    assert!(mempool_priorities.get_peer_priority(&malicious_peer.id()) 
            < mempool_priorities.get_peer_priority(&honest_peer.id()),
            "Malicious peer with stale/false distance=1 is prioritized over honest peer with distance=2");
    
    // VULNERABILITY: Victim would broadcast transactions preferentially to malicious peer
    let broadcast_targets = victim_node.select_mempool_broadcast_targets(10);
    assert!(broadcast_targets[0] == malicious_peer.id(),
            "Victim selects malicious peer as primary broadcast target");
}
```

## Notes

The vulnerability is particularly concerning because:

1. **Default Configuration:** With `network_info_request_interval_ms: 60_000` (1 minute), stale data can persist between refresh cycles while remaining "recent" enough to appear valid. [8](#0-7) 

2. **Metadata Propagation:** The stale distance metric propagates through `PeerMonitoringMetadata` to multiple critical subsystems: [9](#0-8) 

3. **No Disconnection Protection:** Even when peers disconnect and reconnect, there's no mechanism to invalidate their previously reported topology information if the connection ID changes.

This vulnerability demonstrates a critical gap in the peer monitoring service's trust model, where self-reported topology metrics are accepted without temporal or consistency validation.

### Citations

**File:** peer-monitoring-service/client/src/peer_states/network_info.rs (L55-64)
```rust
    pub fn record_network_info_response(
        &mut self,
        network_info_response: NetworkInformationResponse,
    ) {
        // Update the request tracker with a successful response
        self.request_tracker.write().record_response_success();

        // Save the network info
        self.recorded_network_info_response = Some(network_info_response);
    }
```

**File:** peer-monitoring-service/client/src/peer_states/network_info.rs (L116-158)
```rust
        // Sanity check the response depth from the peer metadata
        let network_id = peer_network_id.network_id();
        let is_valid_depth = match network_info_response.distance_from_validators {
            0 => {
                // Verify the peer is a validator and has the correct network id
                let peer_is_validator = peer_metadata.get_connection_metadata().role.is_validator();
                let peer_has_correct_network = match self.base_config.role {
                    RoleType::Validator => network_id.is_validator_network(), // We're a validator
                    RoleType::FullNode => network_id.is_vfn_network(),        // We're a VFN
                };
                peer_is_validator && peer_has_correct_network
            },
            1 => {
                // Verify the peer is a VFN and has the correct network id
                let peer_is_vfn = peer_metadata.get_connection_metadata().role.is_vfn();
                let peer_has_correct_network = match self.base_config.role {
                    RoleType::Validator => network_id.is_vfn_network(), // We're a validator
                    RoleType::FullNode => network_id.is_public_network(), // We're a VFN or PFN
                };
                peer_is_vfn && peer_has_correct_network
            },
            distance_from_validators => {
                // The distance must be less than or equal to the max
                distance_from_validators <= MAX_DISTANCE_FROM_VALIDATORS
            },
        };

        // If the depth did not pass our sanity checks, handle a failure
        if !is_valid_depth {
            warn!(LogSchema::new(LogEntry::NetworkInfoRequest)
                .event(LogEvent::InvalidResponse)
                .peer(peer_network_id)
                .message(&format!(
                    "Peer returned invalid depth from validators: {}",
                    network_info_response.distance_from_validators
                )));
            self.handle_request_failure();
            return;
        }

        // Store the new latency ping result
        self.record_network_info_response(network_info_response);
    }
```

**File:** peer-monitoring-service/types/src/response.rs (L51-55)
```rust
#[derive(Clone, Debug, Deserialize, Eq, PartialEq, Serialize)]
pub struct NetworkInformationResponse {
    pub connected_peers: BTreeMap<PeerNetworkId, ConnectionMetadata>, // Connected peers
    pub distance_from_validators: u64, // The distance of the peer from the validator set
}
```

**File:** mempool/src/shared_mempool/priority.rs (L507-516)
```rust
fn get_distance_from_validators(
    monitoring_metadata: &Option<&PeerMonitoringMetadata>,
) -> Option<u64> {
    monitoring_metadata.and_then(|metadata| {
        metadata
            .latest_network_info_response
            .as_ref()
            .map(|network_info_response| network_info_response.distance_from_validators)
    })
}
```

**File:** mempool/src/shared_mempool/priority.rs (L615-639)
```rust
fn compare_validator_distance(
    monitoring_metadata_a: &Option<&PeerMonitoringMetadata>,
    monitoring_metadata_b: &Option<&PeerMonitoringMetadata>,
) -> Ordering {
    // Get the validator distance from the monitoring metadata
    let validator_distance_a = get_distance_from_validators(monitoring_metadata_a);
    let validator_distance_b = get_distance_from_validators(monitoring_metadata_b);

    // Compare the distances
    match (validator_distance_a, validator_distance_b) {
        (Some(validator_distance_a), Some(validator_distance_b)) => {
            // Prioritize the peer with the lowest validator distance
            validator_distance_a.cmp(&validator_distance_b).reverse()
        },
        (Some(_), None) => {
            Ordering::Greater // Prioritize the peer with a validator distance
        },
        (None, Some(_)) => {
            Ordering::Less // Prioritize the peer with a validator distance
        },
        (None, None) => {
            Ordering::Equal // Neither peer has a validator distance
        },
    }
}
```

**File:** consensus/src/consensus_observer/observer/subscription_utils.rs (L196-218)
```rust
fn get_distance_for_peer(
    peer_network_id: &PeerNetworkId,
    peer_metadata: &PeerMetadata,
) -> Option<u64> {
    // Get the distance for the peer
    let peer_monitoring_metadata = peer_metadata.get_peer_monitoring_metadata();
    let distance = peer_monitoring_metadata
        .latest_network_info_response
        .as_ref()
        .map(|response| response.distance_from_validators);

    // If the distance is missing, log a warning
    if distance.is_none() {
        warn!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "Unable to get distance for peer! Peer: {:?}",
                peer_network_id
            ))
        );
    }

    distance
}
```

**File:** consensus/src/consensus_observer/observer/subscription_utils.rs (L283-312)
```rust
pub fn sort_peers_by_subscription_optimality(
    peers_and_metadata: &HashMap<PeerNetworkId, PeerMetadata>,
) -> Vec<PeerNetworkId> {
    // Group peers and latencies by validator distance, i.e., distance -> [(peer, latency)]
    let mut unsupported_peers = Vec::new();
    let mut peers_and_latencies_by_distance = BTreeMap::new();
    for (peer_network_id, peer_metadata) in peers_and_metadata {
        // Verify that the peer supports consensus observer
        if !supports_consensus_observer(peer_metadata) {
            unsupported_peers.push(*peer_network_id);
            continue; // Skip the peer
        }

        // Get the distance and latency for the peer
        let distance = get_distance_for_peer(peer_network_id, peer_metadata);
        let latency = get_latency_for_peer(peer_network_id, peer_metadata);

        // If the distance is not found, use the maximum distance
        let distance =
            distance.unwrap_or(aptos_peer_monitoring_service_types::MAX_DISTANCE_FROM_VALIDATORS);

        // If the latency is not found, use a large latency
        let latency = latency.unwrap_or(MAX_PING_LATENCY_SECS);

        // Add the peer and latency to the distance group
        peers_and_latencies_by_distance
            .entry(distance)
            .or_insert_with(Vec::new)
            .push((*peer_network_id, OrderedFloat(latency)));
    }
```

**File:** config/src/config/peer_monitoring_config.rs (L65-72)
```rust
impl Default for NetworkMonitoringConfig {
    fn default() -> Self {
        Self {
            network_info_request_interval_ms: 60_000, // 1 minute
            network_info_request_timeout_ms: 10_000,  // 10 seconds
        }
    }
}
```

**File:** peer-monitoring-service/types/src/lib.rs (L44-50)
```rust
#[derive(Clone, Default, Deserialize, PartialEq, Serialize)]
pub struct PeerMonitoringMetadata {
    pub average_ping_latency_secs: Option<f64>, // The average latency ping for the peer
    pub latest_ping_latency_secs: Option<f64>,  // The latest latency ping for the peer
    pub latest_network_info_response: Option<NetworkInformationResponse>, // The latest network info response
    pub latest_node_info_response: Option<NodeInformationResponse>, // The latest node info response
    pub internal_client_state: Option<String>, // A detailed client state string for debugging and logging
```
