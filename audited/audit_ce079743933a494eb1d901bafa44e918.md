# Audit Report

## Title
Consensus Message Loss Due to Untracked Concurrent Peer Dial Operations

## Summary
The PeerManager's `handle_outbound_connection_request` function does not track in-progress dial operations, allowing concurrent dial requests for the same peer to both proceed simultaneously. When both dials complete, the second connection replaces the first via tie-breaking logic, causing any consensus messages sent on the first connection to be silently lost without retry or notification.

## Finding Description

The vulnerability exists in the interaction between `ConnectionRequestSender::dial_peer()` and `PeerManager::handle_outbound_connection_request()`. [1](#0-0) 

When `dial_peer()` is called, it creates a `ConnectionRequest::DialPeer` and sends it through a channel to the PeerManager. The PeerManager processes these requests in its event loop: [2](#0-1) 

The critical flaw is at the check on line 434: it only verifies if a peer exists in `active_peers` (completed connections), not whether there's an in-progress dial to that peer. If two dial requests arrive before either completes, both pass this check and both are forwarded to the TransportHandler.

When both dials complete: [3](#0-2) 

The second connection triggers simultaneous dial tie-breaking. For two outbound connections (same origin), line 575 returns `true`, causing the first connection to be dropped at line 634-636. This closes the channel to the first Peer actor, triggering shutdown: [4](#0-3) 

**Attack Scenario**: During network churn or epoch transitions, if connection establishment logic from multiple sources (ConnectivityManager periodic checks + explicit reconnection logic) triggers concurrent dials to the same validator peer, consensus messages sent on the first established connection during the replacement window are lost. This includes critical BFT messages like votes, proposals, and quorum certificates.

## Impact Explanation

This qualifies as **High Severity** under the Aptos bug bounty criteria ("Significant protocol violations" / "Validator node slowdowns"):

1. **Consensus Message Loss**: Votes cast on the first connection are lost when it's replaced, potentially preventing quorum formation
2. **Block Proposal Delays**: Proposals sent during the window are dropped, delaying block production  
3. **Validator Performance Degradation**: Lost votes count against validator performance metrics
4. **Liveness Impact**: In tight consensus rounds, lost timeout messages could extend round duration

While ConnectivityManager has protection via `dial_queue`, this protection is at a higher level. The PeerManager API itself lacks this protection, making it vulnerable to:
- Bugs in caller state management
- Edge cases during network state transitions
- Future code that uses the API incorrectly

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability requires specific timing conditions:
- Two dial requests for the same peer must arrive at PeerManager before either completes
- The time window is typically hundreds of milliseconds (network RTT + handshake)
- Most likely during: network partitions healing, validator restarts, epoch transitions

While ConnectivityManager has protection, the fundamental lack of tracking at the PeerManager level means the vulnerability persists as a latent bug that could be triggered by future code changes or edge cases in existing code.

## Recommendation

Add tracking of in-progress dials at the PeerManager level:

```rust
// In PeerManager struct, add:
pending_dials: HashMap<PeerId, Vec<oneshot::Sender<Result<(), PeerManagerError>>>>,

// In handle_outbound_connection_request:
ConnectionRequest::DialPeer(requested_peer_id, addr, response_tx) => {
    if let Some((curr_connection, _)) = self.active_peers.get(&requested_peer_id) {
        // Already connected - return error
        let error = PeerManagerError::AlreadyConnected(curr_connection.addr.clone());
        let _ = response_tx.send(Err(error));
    } else if let Some(pending) = self.pending_dials.get_mut(&requested_peer_id) {
        // Dial already in progress - queue the response channel
        pending.push(response_tx);
    } else {
        // Start new dial
        self.pending_dials.insert(requested_peer_id, vec![response_tx]);
        let request = TransportRequest::DialPeer(requested_peer_id, addr, /* ... */);
        self.transport_reqs_tx.send(request).await.unwrap();
    }
}

// When dial completes, respond to all queued response channels
```

This ensures only one dial proceeds per peer, with subsequent requests either returning an error or waiting for the in-progress dial to complete.

## Proof of Concept

```rust
#[tokio::test]
async fn test_concurrent_dial_message_loss() {
    // Setup: Create PeerManager with two ConnectionRequestSenders
    let (conn_req_tx1, _) = create_peer_manager_and_senders();
    let conn_req_tx2 = conn_req_tx1.clone();
    
    let peer_id = PeerId::random();
    let addr = NetworkAddress::mock();
    
    // Trigger concurrent dials
    let dial1 = tokio::spawn(async move {
        conn_req_tx1.dial_peer(peer_id, addr.clone()).await
    });
    let dial2 = tokio::spawn(async move {
        conn_req_tx2.dial_peer(peer_id, addr.clone()).await
    });
    
    // Both succeed
    dial1.await.unwrap().unwrap();
    dial2.await.unwrap().unwrap();
    
    // Send consensus message during the race window
    // Observe: Message sent on first connection is lost when second replaces it
    // Expected: Both dials should be deduplicated or messages should be preserved
}
```

## Notes

While the tie-breaking logic is intentional for handling simultaneous dials from both sides (inbound + outbound), the lack of in-progress dial tracking means the PeerManager API surface is unsafe for concurrent use, even if higher-level callers have their own protection. This violates the principle of defense-in-depth and creates a maintenance hazard for future development.

### Citations

**File:** network/framework/src/peer_manager/senders.rs (L117-126)
```rust
    pub async fn dial_peer(
        &self,
        peer: PeerId,
        addr: NetworkAddress,
    ) -> Result<(), PeerManagerError> {
        let (oneshot_tx, oneshot_rx) = oneshot::channel();
        self.inner
            .push(peer, ConnectionRequest::DialPeer(peer, addr, oneshot_tx))?;
        oneshot_rx.await?
    }
```

**File:** network/framework/src/peer_manager/mod.rs (L432-466)
```rust
            ConnectionRequest::DialPeer(requested_peer_id, addr, response_tx) => {
                // Only dial peers which we aren't already connected with
                if let Some((curr_connection, _)) = self.active_peers.get(&requested_peer_id) {
                    let error = PeerManagerError::AlreadyConnected(curr_connection.addr.clone());
                    debug!(
                        NetworkSchema::new(&self.network_context)
                            .connection_metadata_with_address(curr_connection),
                        "{} Already connected to Peer {} with connection {:?}. Not dialing address {}",
                        self.network_context,
                        requested_peer_id.short_str(),
                        curr_connection,
                        addr
                    );
                    if let Err(send_err) = response_tx.send(Err(error)) {
                        info!(
                            NetworkSchema::new(&self.network_context)
                                .remote_peer(&requested_peer_id),
                            "{} Failed to notify that peer is already connected for Peer {}: {:?}",
                            self.network_context,
                            requested_peer_id.short_str(),
                            send_err
                        );
                    }
                } else {
                    // Update the connection dial metrics
                    counters::update_network_connection_operation_metrics(
                        &self.network_context,
                        counters::DIAL_LABEL.into(),
                        counters::DIAL_PEER_LABEL.into(),
                    );

                    // Send a transport request to dial the peer
                    let request = TransportRequest::DialPeer(requested_peer_id, addr, response_tx);
                    self.transport_reqs_tx.send(request).await.unwrap();
                };
```

**File:** network/framework/src/peer_manager/mod.rs (L626-655)
```rust
        if let Entry::Occupied(active_entry) = self.active_peers.entry(peer_id) {
            let (curr_conn_metadata, _) = active_entry.get();
            if Self::simultaneous_dial_tie_breaking(
                self.network_context.peer_id(),
                peer_id,
                curr_conn_metadata.origin,
                conn_meta.origin,
            ) {
                let (_, peer_handle) = active_entry.remove();
                // Drop the existing connection and replace it with the new connection
                drop(peer_handle);
                info!(
                    NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                    "{} Closing existing connection with Peer {} to mitigate simultaneous dial",
                    self.network_context,
                    peer_id.short_str()
                );
                send_new_peer_notification = false;
            } else {
                info!(
                    NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                    "{} Closing incoming connection with Peer {} to mitigate simultaneous dial",
                    self.network_context,
                    peer_id.short_str()
                );
                // Drop the new connection and keep the one already stored in active_peers
                self.disconnect(connection);
                return Ok(());
            }
        }
```

**File:** network/framework/src/peer/mod.rs (L242-248)
```rust
                maybe_request = self.peer_reqs_rx.next() => {
                    match maybe_request {
                        Some(request) => self.handle_outbound_request(request, &mut write_reqs_tx),
                        // The PeerManager is requesting this connection to close
                        // by dropping the corresponding peer_reqs_tx handle.
                        None => self.shutdown(DisconnectReason::RequestedByPeerManager),
                    }
```
