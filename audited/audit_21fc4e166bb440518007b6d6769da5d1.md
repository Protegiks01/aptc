# Audit Report

## Title
Non-Broadcast Proofs Create Consensus Fairness Violation and Potential Liveness Degradation

## Summary
When the `broadcast_proofs` flag is set to false in the QuorumStore configuration, proofs are sent only to the originating validator rather than being gossiped to all validators. This creates a fairness violation where only the proof creator can include those batches in block proposals, potentially leading to transaction censorship and reduced network throughput.

## Finding Description

In the QuorumStore consensus mechanism, the normal flow is:
1. A validator creates a batch and broadcasts it to all validators
2. Validators receive the batch, store it, and send back signatures
3. The originating validator aggregates signatures to form a ProofOfStore
4. **Critical divergence**: With `broadcast_proofs = false`, the proof is sent only to the originating validator [1](#0-0) 

When `broadcast_proofs` is false, `send_proof_of_store_msg_to_self` is called instead of `broadcast_proof_of_store_msg_v2`: [2](#0-1) 

This only sends the proof to `self.author`, not to all validators. In contrast, the broadcast version sends to all: [3](#0-2) 

The proof is passed to the ProofCoordinator with this flag: [4](#0-3) 

**Impact**: Other validators don't receive the proof in their ProofManagers. When they need to propose blocks, they pull proofs from their BatchProofQueue: [5](#0-4) 

Without the proof, other validators cannot include that batch in their proposals, even though they have the underlying batch data. This violates the fairness property of the consensus protocol.

## Impact Explanation

This does **not** cause a network partition because:
- Validators can still validate blocks containing non-broadcast proofs (proofs are included in the block payload)
- Validators have the batch transactions locally (received when the batch was initially broadcast)
- Consensus can still proceed

However, it **does** cause:
- **Fairness violation**: Only the validator with `broadcast_proofs = false` can propose blocks containing their batches
- **Potential censorship vector**: A malicious validator can monopolize certain transactions
- **Reduced throughput**: The network cannot efficiently utilize all available batches
- **Degraded liveness**: If the non-broadcasting validator becomes unresponsive, transactions in their batches may be delayed

This falls under **Medium Severity** as it creates state inconsistencies in the proof distribution layer that require operational intervention, though it doesn't directly cause fund loss or total consensus failure.

## Likelihood Explanation

**Likelihood: Medium to Low**
- Requires a validator to explicitly set or be misconfigured with `broadcast_proofs = false`
- The flag exists in the codebase as a configurable parameter
- Could occur through malicious intent or accidental misconfiguration
- Not easily detectable without monitoring proof gossip patterns

## Recommendation

Remove the `broadcast_proofs` flag entirely or enforce that it must always be true. The flag provides no legitimate use case and only creates attack vectors:

```rust
// In ProofCoordinator::start, remove the conditional:
if proofs_iter.peek().is_some() {
    observe_batch(approx_created_ts_usecs, self_peer_id, BatchStage::POS_FORMED);
    // Always broadcast - remove the if enable_broadcast_proofs check
    if proofs_iter.peek().is_some_and(|p| p.info().is_v2()) {
        let proofs: Vec<_> = proofs_iter.collect();
        network_sender.broadcast_proof_of_store_msg_v2(proofs).await;
    } else {
        let proofs: Vec<_> = proofs_iter.map(|proof| {
            let (info, sig) = proof.unpack();
            ProofOfStore::new(info.info().clone(), sig)
        }).collect();
        network_sender.broadcast_proof_of_store_msg(proofs).await;
    }
}
```

Additionally, remove the `broadcast_proofs` field from InnerBuilder and related structs.

## Proof of Concept

```rust
// This is a conceptual PoC - actual implementation would require running validators

// Setup: Configure Validator A with broadcast_proofs = false
// 1. Validator A creates batch B1 with transactions T1, T2, T3
// 2. Batch B1 is broadcast to all validators (Validator B, C, D receive it)
// 3. Validators B, C, D sign B1 and send signatures to A
// 4. Validator A forms ProofOfStore P1 by aggregating signatures
// 5. Due to broadcast_proofs = false, P1 is sent only to Validator A

// Verify the attack:
// - Validator B tries to propose a block
// - Validator B's ProofManager.pull_proofs() is called
// - B's BatchProofQueue does not contain P1 (was never gossiped)
// - B cannot include batch B1 in their proposal
// - Only Validator A can propose blocks containing B1

// Validators B, C, D can still validate blocks from A containing P1
// because P1 is in the block payload and they have the batch data
// But fairness is violated - A monopolizes those transactions
```

**Notes**: 
- This is not a critical network partition vulnerability as initially suggested by the question
- Validators can still execute blocks even without receiving proofs via gossip because the batch data was broadcast separately and the proof is included in the block payload
- The primary impact is fairness violation and reduced network efficiency, not consensus safety or total liveness failure
- The vulnerability requires validator-level configuration access, limiting its exploitability

### Citations

**File:** consensus/src/quorum_store/proof_coordinator.rs (L484-498)
```rust
                                if enable_broadcast_proofs {
                                    if proofs_iter.peek().is_some_and(|p| p.info().is_v2()) {
                                        let proofs: Vec<_> = proofs_iter.collect();
                                        network_sender.broadcast_proof_of_store_msg_v2(proofs).await;
                                    } else {
                                        let proofs: Vec<_> = proofs_iter.map(|proof| {
                                            let (info, sig) = proof.unpack();
                                            ProofOfStore::new(info.info().clone(), sig)
                                        }).collect();
                                        network_sender.broadcast_proof_of_store_msg(proofs).await;
                                    }
                                } else {
                                    let proofs: Vec<_> = proofs_iter.collect();
                                    network_sender.send_proof_of_store_msg_to_self(proofs).await;
                                }
```

**File:** consensus/src/network.rs (L629-633)
```rust
    async fn broadcast_proof_of_store_msg_v2(&mut self, proofs: Vec<ProofOfStore<BatchInfoExt>>) {
        fail_point!("consensus::send::proof_of_store", |_| ());
        let msg = ConsensusMsg::ProofOfStoreMsgV2(Box::new(ProofOfStoreMsg::new(proofs)));
        self.broadcast(msg).await
    }
```

**File:** consensus/src/network.rs (L635-639)
```rust
    async fn send_proof_of_store_msg_to_self(&mut self, proofs: Vec<ProofOfStore<BatchInfoExt>>) {
        fail_point!("consensus::send::proof_of_store", |_| ());
        let msg = ConsensusMsg::ProofOfStoreMsgV2(Box::new(ProofOfStoreMsg::new(proofs)));
        self.send(msg, vec![self.author]).await
    }
```

**File:** consensus/src/quorum_store/quorum_store_builder.rs (L345-362)
```rust
        let proof_coordinator_cmd_rx = self.proof_coordinator_cmd_rx.take().unwrap();
        let proof_coordinator = ProofCoordinator::new(
            self.config.proof_timeout_ms,
            self.author,
            self.batch_reader.clone().unwrap(),
            self.batch_generator_cmd_tx.clone(),
            self.proof_cache,
            self.broadcast_proofs,
            self.config.batch_expiry_gap_when_init_usecs,
        );
        spawn_named!(
            "proof_coordinator",
            proof_coordinator.start(
                proof_coordinator_cmd_rx,
                self.network_sender.clone(),
                self.verifier.clone(),
            )
        );
```

**File:** consensus/src/quorum_store/proof_manager.rs (L114-122)
```rust
        let (proof_block, txns_with_proof_size, cur_unique_txns, proof_queue_fully_utilized) =
            self.batch_proof_queue.pull_proofs(
                &excluded_batches,
                request.max_txns,
                request.max_txns_after_filtering,
                request.soft_max_txns_after_filtering,
                request.return_non_full,
                request.block_timestamp,
            );
```
