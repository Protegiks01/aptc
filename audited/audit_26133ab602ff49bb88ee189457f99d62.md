# Audit Report

## Title
TOCTOU Race Condition in ChunkExecutor::with_inner() Leading to False Positive Node Panics

## Summary
The `with_inner()` function in ChunkExecutor contains a Time-of-Check to Time-of-Use (TOCTOU) race condition where the `has_pending_pre_commit` atomic flag is read before executing the inner closure, but the flag's value can change during execution by concurrent threads. This leads to false positive panics that crash the storage synchronizer pipeline, causing nodes to stop syncing state.

## Finding Description
The vulnerability exists in the error handling logic of `with_inner()`. [1](#0-0) 

The function reads `has_pending_pre_commit` at line 96 using `Ordering::Acquire`, then executes the closure `f(inner)`. If an error occurs, it checks the **stale** value captured at line 96 to decide whether to panic (lines 98-103). This value can become stale because concurrent threads can modify the atomic flag during the execution of `f(inner)`.

The ChunkExecutor is designed to be accessed concurrently by three separate async tasks in the state synchronization pipeline: [2](#0-1) 

These three tasks share the same `Arc<ChunkExecutor>`:
1. **Executor task** (`spawn_executor`) - calls `enqueue_chunk_by_execution()` or `enqueue_chunk_by_transaction_outputs()` [3](#0-2) 
2. **Ledger updater task** (`spawn_ledger_updater`) - calls `update_ledger()` [4](#0-3) 
3. **Committer task** (`spawn_committer`) - calls `commit_chunk()` [5](#0-4) 

All these methods use `with_inner()` which acquires only a **read lock** (line 93), allowing concurrent execution. [6](#0-5) 

The race condition occurs when:
1. Thread A (executor) calls `with_inner()` and reads `has_pending_pre_commit = true`
2. Thread A begins executing the closure (e.g., `enqueue_chunk`)
3. Thread B (committer) concurrently calls `with_inner()` (both can hold read locks)
4. Thread B executes `commit_chunk()` successfully and sets `has_pending_pre_commit = false` [7](#0-6) 
5. Thread A's operation encounters an error (e.g., version mismatch) [8](#0-7) 
6. Thread A panics based on the stale value from step 1, even though the pre-committed ledger has been successfully committed

This results in a **false positive panic** - the node crashes even though the state is safe.

The panic behavior is intentional when there genuinely is pending pre-committed data, as verified by the test suite. [9](#0-8)  However, the TOCTOU race causes panics even after the pre-committed data has been successfully committed.

## Impact Explanation
This vulnerability causes **unnecessary node crashes** during state synchronization operations, qualifying as **High Severity** per Aptos bug bounty criteria: "API crashes" and "Validator node slowdowns".

When the panic occurs inside the `spawn_blocking` task, it propagates through the async executor task, causing the entire storage synchronizer pipeline to break. The node can no longer sync state and becomes operationally degraded until restart and resync.

The impact includes:
- **Availability degradation**: Nodes panic unnecessarily during state sync, reducing network stability
- **Service disruption**: The crashed node must restart and resync, causing temporary unavailability
- **Operational reliability**: False positive crashes during normal operations undermine node reliability

While this does not cause consensus violations or loss of funds (not Critical severity), it significantly impacts node reliability and availability, meeting the criteria for High severity under the Aptos bug bounty program.

## Likelihood Explanation
This race condition has a **moderate to high likelihood** of occurring in production:

1. **Concurrent design by intent**: The storage synchronizer explicitly spawns three concurrent tasks that share the same ChunkExecutor, making concurrent access the normal operation mode

2. **Substantial timing window**: The race window extends for the entire duration of closure execution, which can be significant during transaction execution or chunk validation

3. **Read lock architecture**: Both operations only require read locks, providing no mutual exclusion to prevent the race

4. **Pipeline architecture**: The three-stage pipeline (execute → update → commit) is designed for concurrent processing of multiple chunks simultaneously

The race requires specific conditions: (1) pre-committed chunks must exist, (2) an error must occur during chunk execution, and (3) the committer must complete during the executor's operation. While not guaranteed to occur on every sync, these conditions can arise during normal state sync operations, particularly during network disruptions or chunk validation failures.

## Recommendation
Fix the TOCTOU race by re-reading the `has_pending_pre_commit` flag **after** the closure executes but **before** making the panic decision:

```rust
fn with_inner<F, T>(&self, f: F) -> Result<T>
where
    F: FnOnce(&ChunkExecutorInner<V>) -> Result<T>,
{
    let locked = self.inner.read();
    let inner = locked.as_ref().expect("not reset");

    // Remove the early read - only read after execution
    f(inner).map_err(|error| {
        // Read the CURRENT value, not a stale cached value
        let has_pending_pre_commit = inner.has_pending_pre_commit.load(Ordering::Acquire);
        if has_pending_pre_commit {
            panic!(
                "Hit error with pending pre-committed ledger, panicking. {:?}",
                error,
            );
        }
        error
    })
}
```

This ensures the panic decision is based on the current state of `has_pending_pre_commit` at the time of error handling, eliminating the TOCTOU race.

## Proof of Concept
While a complete PoC would require complex orchestration of the state sync pipeline, the vulnerability can be understood through code analysis:

1. The three concurrent tasks are spawned and share `Arc<ChunkExecutor>`: [2](#0-1) 

2. All call `with_inner()` with read locks allowing concurrent access: [10](#0-9) 

3. The flag is read early (line 96) but used later (lines 98-103) after potentially concurrent modifications

4. The committer clears the flag after successful commit: [11](#0-10) 

The race is inherent in the design: early read + delayed use + concurrent modification = TOCTOU vulnerability.

## Notes
- This vulnerability affects the storage synchronization pipeline specifically, not the consensus layer
- The panic is intentional when there genuinely is pending pre-committed data (as designed), but the race causes false positives
- The vulnerability can occur during normal operations without attacker involvement, though an attacker cannot reliably trigger it through crafted chunks (chunks must have valid ledger info signatures)
- Impact is limited to individual node availability, not network-wide consensus or funds

### Citations

**File:** execution/executor/src/chunk_executor/mod.rs (L89-106)
```rust
    fn with_inner<F, T>(&self, f: F) -> Result<T>
    where
        F: FnOnce(&ChunkExecutorInner<V>) -> Result<T>,
    {
        let locked = self.inner.read();
        let inner = locked.as_ref().expect("not reset");

        let has_pending_pre_commit = inner.has_pending_pre_commit.load(Ordering::Acquire);
        f(inner).map_err(|error| {
            if has_pending_pre_commit {
                panic!(
                    "Hit error with pending pre-committed ledger, panicking. {:?}",
                    error,
                );
            }
            error
        })
    }
```

**File:** execution/executor/src/chunk_executor/mod.rs (L304-309)
```rust
        ensure!(
            chunk.first_version() == parent_state.next_version(),
            "Chunk carries unexpected first version. Expected: {}, got: {}",
            parent_state.next_version(),
            chunk.first_version(),
        );
```

**File:** execution/executor/src/chunk_executor/mod.rs (L394-397)
```rust
    fn commit_chunk(&self) -> Result<ChunkCommitNotification> {
        let _timer = COMMIT_CHUNK.start_timer();
        let executed_chunk = self.commit_chunk_impl()?;
        self.has_pending_pre_commit.store(false, Ordering::Release);
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L234-262)
```rust
        let executor_handle = spawn_executor(
            chunk_executor.clone(),
            error_notification_sender.clone(),
            executor_listener,
            ledger_updater_notifier,
            pending_data_chunks.clone(),
            runtime.clone(),
        );

        // Spawn the ledger updater that updates the ledger in storage
        let ledger_updater_handle = spawn_ledger_updater(
            chunk_executor.clone(),
            error_notification_sender.clone(),
            ledger_updater_listener,
            committer_notifier,
            pending_data_chunks.clone(),
            runtime.clone(),
        );

        // Spawn the committer that commits executed (but pending) chunks
        let committer_handle = spawn_committer(
            chunk_executor.clone(),
            error_notification_sender.clone(),
            committer_listener,
            commit_post_processor_notifier,
            pending_data_chunks.clone(),
            runtime.clone(),
            storage.reader.clone(),
        );
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L1026-1064)
```rust
async fn execute_transaction_chunk<ChunkExecutor: ChunkExecutorTrait + 'static>(
    chunk_executor: Arc<ChunkExecutor>,
    transactions_with_proof: TransactionListWithProofV2,
    target_ledger_info: LedgerInfoWithSignatures,
    end_of_epoch_ledger_info: Option<LedgerInfoWithSignatures>,
) -> anyhow::Result<()> {
    // Execute the transaction chunk
    let num_transactions = transactions_with_proof
        .get_transaction_list_with_proof()
        .transactions
        .len();
    let result = tokio::task::spawn_blocking(move || {
        chunk_executor.enqueue_chunk_by_execution(
            transactions_with_proof,
            &target_ledger_info,
            end_of_epoch_ledger_info.as_ref(),
        )
    })
    .await
    .expect("Spawn_blocking(execute_transaction_chunk) failed!");

    // Update the logs and metrics if the chunk was executed successfully
    if result.is_ok() {
        // Log the execution event
        info!(
            LogSchema::new(LogEntry::StorageSynchronizer).message(&format!(
                "Executed a new transaction chunk! Transaction total: {:?}.",
                num_transactions
            ))
        );

        // Update the chunk metrics
        let operation_label =
            metrics::StorageSynchronizerOperations::ExecutedTransactions.get_label();
        update_synchronizer_chunk_metrics(num_transactions, operation_label);
    }

    result
}
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L1083-1089)
```rust
async fn update_ledger<ChunkExecutor: ChunkExecutorTrait + 'static>(
    chunk_executor: Arc<ChunkExecutor>,
) -> anyhow::Result<()> {
    tokio::task::spawn_blocking(move || chunk_executor.update_ledger())
        .await
        .expect("Spawn_blocking(update_ledger) failed!")
}
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L1094-1100)
```rust
async fn commit_chunk<ChunkExecutor: ChunkExecutorTrait + 'static>(
    chunk_executor: Arc<ChunkExecutor>,
) -> anyhow::Result<ChunkCommitNotification> {
    tokio::task::spawn_blocking(move || chunk_executor.commit_chunk())
        .await
        .expect("Spawn_blocking(commit_chunk) failed!")
}
```

**File:** execution/executor/src/tests/chunk_executor_tests.rs (L364-400)
```rust
#[test]
#[should_panic(expected = "Hit error with pending pre-committed ledger, panicking.")]
fn test_panic_on_mismatch_with_pre_committed() {
    // See comments on `commit_1_pre_commit_2_return_3()`
    let (db, _chunk3, _ledger_info2, _ledger_info3) = commit_1_pre_commit_2_return_3();

    let (bad_chunks, bad_ledger_info) = create_transaction_chunks(vec![1..=7, 8..=12]);
    // bad chunk has txn 8-12
    let bad_chunk = bad_chunks[1].clone();

    let chunk_executor = ChunkExecutor::<MockVM>::new(db);
    // chunk executor knows there's pre-committed txns in the DB and when a verified chunk
    // doesn't match the pre-committed root hash it panics in hope that pre-committed versions
    // get truncated on reboot
    let _res = chunk_executor.execute_chunk(bad_chunk, &bad_ledger_info, None);
}

#[test]
fn test_continue_from_pre_committed() {
    // See comments on `commit_1_pre_commit_2_return_3()`
    let (db, chunk3, _ledger_info2, ledger_info3) = commit_1_pre_commit_2_return_3();

    let (bad_chunks, bad_ledger_info) = create_transaction_chunks(vec![1..=10, 11..=15]);
    // bad chunk has txn 11-15
    let bad_chunk = bad_chunks[1].clone();

    // continue from pre-committed version
    let chunk_executor = ChunkExecutor::<MockVM>::new(db);
    chunk_executor
        .execute_chunk(chunk3, &ledger_info3, None)
        .unwrap();
    chunk_executor.commit_chunk().unwrap();
    // once pre-committed range is committed, don't panic on errors
    assert!(chunk_executor
        .execute_chunk(bad_chunk, &bad_ledger_info, None)
        .is_err());
}
```
