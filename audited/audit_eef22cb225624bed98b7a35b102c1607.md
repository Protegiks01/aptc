# Audit Report

## Title
Counter Leak in wait_transaction_by_hash Causes Permanent DoS of Long-Polling Feature

## Summary
The `wait_transaction_by_hash` endpoint in `api/src/transactions.rs` implements connection counting using atomic operations (`fetch_add`/`fetch_sub`) without proper RAII guards. When HTTP clients disconnect mid-request, the async Future is dropped by the Poem framework, causing the decrement operation to never execute. This leads to permanent counter leaks that eventually disable the long-polling feature for all users, forcing degraded service via short polling.

## Finding Description

The vulnerability exists in the `wait_transaction_by_hash` function's connection counting mechanism: [1](#0-0) 

The function increments `wait_for_hash_active_connections` at the start and decrements it at the end. However, the decrement operation at line 276 is not protected by an RAII guard. When a client disconnects mid-request, the Poem HTTP framework drops the Future, as documented in the codebase: [2](#0-1) 

The execution flow is:
1. Line 243: `fetch_add(1)` increments counter
2. Line 265-271: `.await` on `wait_transaction_by_hash_inner()`
3. Client disconnects → Future is dropped
4. Lines 273-276: **NEVER EXECUTE** → `fetch_sub(1)` never called
5. Counter permanently leaks +1

An attacker can exploit this by:
1. Sending multiple requests to `/transactions/wait_by_hash/:txn_hash`
2. Immediately cancelling each request (closing HTTP connection)
3. Each cancelled request permanently leaks +1 to the counter
4. After 100 cancelled requests (default `wait_by_hash_max_active_connections`), the counter permanently exceeds the limit
5. All subsequent requests fail the check at line 244 and fall back to short polling
6. Long-polling feature is disabled for all users until node restart

The counter is defined as: [3](#0-2) 

And initialized to zero: [4](#0-3) 

The limit defaults to 100: [5](#0-4) 

The codebase contains proper RAII guard patterns for similar scenarios (e.g., `TaskGuard`), but they are not used here: [6](#0-5) 

## Impact Explanation

**Severity: MEDIUM** (up to $10,000 per Aptos Bug Bounty)

This vulnerability causes service degradation but not complete failure:

1. **Service Degradation**: Long-polling is permanently disabled, forcing all clients to use inefficient short polling
2. **Increased Server Load**: Short polling generates 50x more requests (polling every 20ms vs waiting 1000ms)
3. **User Experience Impact**: Higher latency for transaction confirmation queries
4. **Persistent Until Restart**: Counter leaks are permanent until node operator restarts the API service

The issue does NOT cause:
- Loss of funds or consensus violations (API-only issue)
- Complete API unavailability (short polling still works)
- Validator node compromise or blockchain state corruption

This fits the **Medium Severity** category: "State inconsistencies requiring intervention" - the counter state becomes inconsistent and requires node restart to fix.

## Likelihood Explanation

**Likelihood: HIGH**

The attack is:
- **Trivial to Execute**: Simple HTTP client that opens and closes connections
- **No Authentication Required**: Public API endpoint
- **Automatable**: Can be scripted in any language
- **Persistent Impact**: Each successful attack permanently leaks the counter
- **No Rate Limiting Protection**: Not protected by typical rate limiting (different transaction hashes can be queried)

Example attack script:
```bash
for i in {1..100}; do
  curl http://node:8080/transactions/wait_by_hash/0x$RANDOM &
  sleep 0.1
  kill $!  # Cancel the request
done
```

## Recommendation

Implement an RAII guard pattern to ensure the counter is always decremented, even on early returns or Future drops:

```rust
// Add to api/src/transactions.rs

struct ConnectionGuard {
    counter: Arc<AtomicUsize>,
}

impl ConnectionGuard {
    fn new(counter: Arc<AtomicUsize>) -> Option<Self> {
        let old_count = counter.fetch_add(1, Ordering::Relaxed);
        Some(Self { counter })
    }
}

impl Drop for ConnectionGuard {
    fn drop(&mut self) {
        self.counter.fetch_sub(1, Ordering::Relaxed);
    }
}

// Modify wait_transaction_by_hash:
async fn wait_transaction_by_hash(
    &self,
    accept_type: AcceptType,
    txn_hash: Path<HashValue>,
) -> BasicResultWith404<Transaction> {
    fail_point_poem("endpoint_wait_transaction_by_hash")?;
    self.context
        .check_api_output_enabled("Get transactions by hash", &accept_type)?;

    // Check if we can accept more connections
    let current = self.context.wait_for_hash_active_connections.load(Ordering::Relaxed);
    if current >= self.context.node_config.api.wait_by_hash_max_active_connections {
        metrics::WAIT_TRANSACTION_POLL_TIME
            .with_label_values(&["short"])
            .observe(0.0);
        return self
            .get_transaction_by_hash_inner(&accept_type, txn_hash.0)
            .await;
    }

    // Create guard that auto-decrements on drop
    let _guard = ConnectionGuard::new(
        self.context.wait_for_hash_active_connections.clone()
    );

    let start_time = std::time::Instant::now();
    WAIT_TRANSACTION_GAUGE.inc();

    let result = self
        .wait_transaction_by_hash_inner(
            &accept_type,
            txn_hash.0,
            self.context.node_config.api.wait_by_hash_timeout_ms,
            self.context.node_config.api.wait_by_hash_poll_interval_ms,
        )
        .await;

    WAIT_TRANSACTION_GAUGE.dec();
    metrics::WAIT_TRANSACTION_POLL_TIME
        .with_label_values(&["long"])
        .observe(start_time.elapsed().as_secs_f64());
    
    result
    // Guard automatically decrements counter when it goes out of scope
}
```

## Proof of Concept

**Rust reproduction steps:**

1. Deploy an Aptos node with default API configuration
2. Create a test script that sends 100 requests and cancels them:

```rust
use tokio::time::{timeout, Duration};

#[tokio::main]
async fn main() {
    let client = reqwest::Client::new();
    let base_url = "http://localhost:8080";
    
    // Leak the counter by cancelling 100 requests
    for i in 0..100 {
        let hash = format!("{:064x}", i);
        let url = format!("{}/transactions/wait_by_hash/{}", base_url, hash);
        
        // Start request and immediately cancel it via timeout
        let request = client.get(&url).send();
        let _ = timeout(Duration::from_millis(1), request).await;
        
        println!("Cancelled request {}", i);
    }
    
    println!("Counter should now be at 100 (leaked)");
    
    // Try a legitimate request - it should be forced to short poll
    let hash = format!("{:064x}", 999);
    let url = format!("{}/transactions/wait_by_hash/{}", base_url, hash);
    let start = std::time::Instant::now();
    let response = client.get(&url).send().await.unwrap();
    let elapsed = start.elapsed();
    
    // Short poll should return almost immediately (< 100ms)
    // Long poll would wait ~1000ms
    assert!(elapsed.as_millis() < 100, "Should use short poll, took {:?}", elapsed);
    println!("Confirmed: Long-polling is disabled due to leaked counter");
}
```

3. Monitor the node logs - all subsequent requests will use short polling even though no actual long-polling connections are active.

### Citations

**File:** api/src/transactions.rs (L240-280)
```rust
        if self
            .context
            .wait_for_hash_active_connections
            .fetch_add(1, std::sync::atomic::Ordering::Relaxed)
            >= self
                .context
                .node_config
                .api
                .wait_by_hash_max_active_connections
        {
            self.context
                .wait_for_hash_active_connections
                .fetch_sub(1, std::sync::atomic::Ordering::Relaxed);
            metrics::WAIT_TRANSACTION_POLL_TIME
                .with_label_values(&["short"])
                .observe(0.0);
            return self
                .get_transaction_by_hash_inner(&accept_type, txn_hash.0)
                .await;
        }

        let start_time = std::time::Instant::now();
        WAIT_TRANSACTION_GAUGE.inc();

        let result = self
            .wait_transaction_by_hash_inner(
                &accept_type,
                txn_hash.0,
                self.context.node_config.api.wait_by_hash_timeout_ms,
                self.context.node_config.api.wait_by_hash_poll_interval_ms,
            )
            .await;

        WAIT_TRANSACTION_GAUGE.dec();
        self.context
            .wait_for_hash_active_connections
            .fetch_sub(1, std::sync::atomic::Ordering::Relaxed);
        metrics::WAIT_TRANSACTION_POLL_TIME
            .with_label_values(&["long"])
            .observe(start_time.elapsed().as_secs_f64());
        result
```

**File:** crates/aptos-faucet/core/src/middleware/log.rs (L95-99)
```rust
/// In Poem, if the client hangs up mid request, the future stops getting polled
/// and instead gets dropped. So if we want this middleware logging to happen
/// even if this happens, we have to implement the logging in a Drop impl. If
/// we reach this drop impl and there is no response log attached, we have hit
/// this case and log accordingly.
```

**File:** api/src/context.rs (L84-84)
```rust
    pub wait_for_hash_active_connections: Arc<AtomicUsize>,
```

**File:** api/src/context.rs (L136-136)
```rust
            wait_for_hash_active_connections: Arc::new(AtomicUsize::new(0)),
```

**File:** config/src/config/api_config.rs (L144-144)
```rust
            wait_by_hash_max_active_connections: 100,
```

**File:** consensus/src/pipeline/pipeline_phase.rs (L26-45)
```rust
struct TaskGuard {
    counter: Arc<AtomicU64>,
}

impl TaskGuard {
    fn new(counter: Arc<AtomicU64>) -> Self {
        counter.fetch_add(1, Ordering::SeqCst);
        Self { counter }
    }

    fn spawn(&self) -> Self {
        Self::new(self.counter.clone())
    }
}

impl Drop for TaskGuard {
    fn drop(&mut self) {
        self.counter.fetch_sub(1, Ordering::SeqCst);
    }
}
```
