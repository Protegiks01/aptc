# Audit Report

## Title
Sequential Stream Processing Enables Subscription Starvation in Data Streaming Service

## Summary
The data streaming service processes multiple subscriptions sequentially without fairness guarantees or batching limits. A single subscription with many pending responses can monopolize CPU and bandwidth resources, preventing other subscriptions from making progress and degrading node synchronization performance.

## Finding Description

The data streaming service manages multiple concurrent data streams (subscriptions) through a HashMap structure. [1](#0-0) 

When checking progress across all active streams, the service iterates sequentially through each stream ID and awaits completion of each stream's processing before moving to the next. [2](#0-1) 

Within each individual stream's processing, an unbounded while loop continuously processes all pending responses until the queue is empty or a head-of-line blocking condition is hit. [3](#0-2) 

The configuration allows up to 50 pending requests per stream by default. [4](#0-3) 

**Attack Scenario:**
1. An attacker creates Stream A requesting a large amount of historical data (e.g., state values from genesis)
2. The attacker also creates Stream B for legitimate synchronization
3. Stream A accumulates 50 pending responses (max_pending_requests=50)
4. When `check_progress_of_all_data_streams()` executes, Stream A processes all 50 responses before yielding
5. Stream B is starved and cannot make progress until Stream A completes its batch
6. This cycle repeats every progress_check_interval_ms (50ms default)

This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." There is no per-stream CPU time quota or fairness scheduling mechanism.

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos bug bounty program:

- **Validator node slowdowns**: A subscription monopolizing resources can delay block propagation and state synchronization for validators
- **State inconsistencies requiring intervention**: Starved subscriptions may timeout or fail, requiring manual intervention to restart sync processes
- **Limited availability impact**: While not causing total network failure, it degrades synchronization performance for affected nodes

The impact is constrained to the state sync subsystem and does not directly compromise consensus safety, funds, or cause network partition. However, it can significantly impair node performance and availability.

## Likelihood Explanation

**Likelihood: Medium to High**

- **Low barrier to entry**: Any network participant can create subscriptions without special privileges
- **Default configuration vulnerable**: The default max_pending_requests (50) and sequential processing enable the attack
- **Natural occurrence possible**: Even without malicious intent, legitimate heavy sync operations (e.g., fast-sync from genesis) can inadvertently starve other subscriptions
- **No detection or mitigation**: There are no built-in mechanisms to detect or prevent subscription starvation

The attack is straightforward to execute and requires no validator access or special network positioning.

## Recommendation

Implement fair scheduling and batching mechanisms for stream processing:

**Option 1: Round-Robin Processing with Batch Limits**
```rust
async fn check_progress_of_all_data_streams(&mut self) {
    let data_stream_ids = self.get_all_data_stream_ids();
    let max_responses_per_stream_per_cycle = 10; // Process max 10 responses per stream per cycle
    
    for data_stream_id in &data_stream_ids {
        if let Err(error) = self.update_progress_of_data_stream_with_limit(
            data_stream_id, 
            max_responses_per_stream_per_cycle
        ).await {
            // Error handling...
        }
    }
}
```

**Option 2: Time-based Fairness**
```rust
async fn process_data_responses(&mut self, global_data_summary: GlobalDataSummary) -> Result<(), Error> {
    let max_processing_duration = Duration::from_millis(10); // Max 10ms per cycle
    let start_time = Instant::now();
    
    while let Some(pending_response) = self.pop_pending_response_queue()? {
        // Process response...
        
        // Yield after max duration to allow other streams to progress
        if start_time.elapsed() >= max_processing_duration {
            break;
        }
    }
}
```

**Option 3: Weighted Fair Queueing**
Assign priority weights to streams based on their type (subscription vs. optimistic fetch vs. state sync) and use weighted fair queueing to ensure all streams make proportional progress.

## Proof of Concept

```rust
#[tokio::test]
async fn test_subscription_starvation() {
    use std::time::{Duration, Instant};
    use state_sync::streaming_service::*;
    use state_sync::streaming_client::*;
    
    // Create streaming service with default config
    let (streaming_client, mut streaming_service) = 
        create_streaming_client_and_server(None, false, false, true, true);
    
    // Create Stream A: Heavy subscription with many pending responses
    let stream_a = streaming_client
        .get_all_states(0, 100000) // Request large state range
        .await
        .unwrap();
    
    // Create Stream B: Normal subscription
    let stream_b = streaming_client
        .continuously_stream_transactions(0, 0, None)
        .await
        .unwrap();
    
    // Allow Stream A to accumulate pending responses
    tokio::time::sleep(Duration::from_secs(5)).await;
    
    // Measure time for Stream B to receive first notification
    let start = Instant::now();
    let mut received_b = false;
    
    for _ in 0..100 {
        streaming_service.check_progress_of_all_data_streams().await;
        
        // Check if Stream B received any data
        if stream_b.select_next_some().now_or_never().is_some() {
            received_b = true;
            break;
        }
    }
    
    let elapsed = start.elapsed();
    
    // Stream B should be significantly delayed due to Stream A monopolizing resources
    assert!(elapsed > Duration::from_secs(1), 
        "Stream B should be starved by Stream A, but received data in {:?}", elapsed);
    assert!(received_b, "Stream B should eventually receive data");
}
```

## Notes

This vulnerability is an architectural issue in the data streaming service's scheduler. While the default configuration limits (max_pending_requests=50, progress_check_interval_ms=50) provide some bounds, they are insufficient to prevent resource monopolization by a single subscription. The lack of fairness guarantees can lead to degraded performance for critical synchronization operations, especially in scenarios where multiple subscriptions compete for resources.

### Citations

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L67-68)
```rust
    // All requested data streams from clients
    data_streams: HashMap<DataStreamId, DataStream<T>>,
```

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L309-337)
```rust
    async fn check_progress_of_all_data_streams(&mut self) {
        // Drive the progress of each stream
        let data_stream_ids = self.get_all_data_stream_ids();
        for data_stream_id in &data_stream_ids {
            if let Err(error) = self.update_progress_of_data_stream(data_stream_id).await {
                if matches!(error, Error::NoDataToFetch(_)) {
                    sample!(
                        SampleRate::Duration(Duration::from_secs(NO_DATA_TO_FETCH_LOG_FREQ_SECS)),
                        info!(LogSchema::new(LogEntry::CheckStreamProgress)
                            .stream_id(*data_stream_id)
                            .event(LogEvent::Pending)
                            .error(&error))
                    );
                } else {
                    metrics::increment_counter(
                        &metrics::CHECK_STREAM_PROGRESS_ERROR,
                        error.get_label(),
                    );
                    warn!(LogSchema::new(LogEntry::CheckStreamProgress)
                        .stream_id(*data_stream_id)
                        .event(LogEvent::Error)
                        .error(&error));
                }
            }
        }

        // Update the metrics
        metrics::set_active_data_streams(data_stream_ids.len());
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L456-540)
```rust
        // Continuously process any ready data responses
        while let Some(pending_response) = self.pop_pending_response_queue()? {
            // Get the client request and response information
            let maybe_client_response = pending_response.lock().client_response.take();
            let client_response = maybe_client_response.ok_or_else(|| {
                Error::UnexpectedErrorEncountered("The client response should be ready!".into())
            })?;
            let client_request = &pending_response.lock().client_request.clone();

            // Process the client response
            match client_response {
                Ok(client_response) => {
                    // Sanity check and process the response
                    if sanity_check_client_response_type(client_request, &client_response) {
                        // If the response wasn't enough to satisfy the original request (e.g.,
                        // it was truncated), missing data should be requested.
                        let mut head_of_line_blocked = false;
                        match self.request_missing_data(client_request, &client_response.payload) {
                            Ok(missing_data_requested) => {
                                if missing_data_requested {
                                    head_of_line_blocked = true; // We're now head of line blocked on the missing data
                                }
                            },
                            Err(error) => {
                                warn!(LogSchema::new(LogEntry::ReceivedDataResponse)
                                    .stream_id(self.data_stream_id)
                                    .event(LogEvent::Error)
                                    .error(&error)
                                    .message("Failed to determine if missing data was requested!"));
                            },
                        }

                        // If the request was a subscription request and the subscription
                        // stream is lagging behind the data advertisements, the stream
                        // engine should be notified (e.g., so that it can catch up).
                        if client_request.is_subscription_request() {
                            if let Err(error) = self.check_subscription_stream_lag(
                                &global_data_summary,
                                &client_response.payload,
                            ) {
                                self.notify_new_data_request_error(client_request, error)?;
                                head_of_line_blocked = true; // We're now head of line blocked on the failed stream
                            }
                        }

                        // The response is valid, send the data notification to the client
                        self.send_data_notification_to_client(client_request, client_response)
                            .await?;

                        // If the request is for specific data, increase the prefetching limit.
                        // Note: we don't increase the limit for new data requests because
                        // those don't invoke the prefetcher (as we're already up-to-date).
                        if !client_request.is_new_data_request() {
                            self.dynamic_prefetching_state
                                .increase_max_concurrent_requests();
                        }

                        // If we're head of line blocked, we should return early
                        if head_of_line_blocked {
                            break;
                        }
                    } else {
                        // The sanity check failed
                        self.handle_sanity_check_failure(client_request, &client_response.context)?;
                        break; // We're now head of line blocked on the failed request
                    }
                },
                Err(error) => {
                    // Handle the error depending on the request type
                    if client_request.is_new_data_request() {
                        // The request was for new data. We should notify the
                        // stream engine and clear the requests queue.
                        self.notify_new_data_request_error(client_request, error)?;
                    } else {
                        // Decrease the prefetching limit on an error
                        self.dynamic_prefetching_state
                            .decrease_max_concurrent_requests();

                        // Handle the error and simply retry
                        self.handle_data_client_error(client_request, &error)?;
                    }
                    break; // We're now head of line blocked on the failed request
                },
            }
        }
```

**File:** config/src/config/state_sync_config.rs (L249-252)
```rust
    /// Maximum number of pending requests per data stream. This includes the
    /// requests that have already succeeded but have not yet been consumed
    /// because they're head-of-line blocked by other requests.
    pub max_pending_requests: u64,
```
