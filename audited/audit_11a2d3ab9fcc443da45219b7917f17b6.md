# Audit Report

## Title
Critical Validator Crash Due to Empty Weight HashMap in Secret Share State Machine

## Summary
A validator node will panic and crash when processing secret shares if peer shares arrive before the node's own self-share computation completes. This occurs due to an inconsistency between `get_peer_weight()` (returns hardcoded 1) and `get_peer_weights()` (returns empty HashMap) in `SecretShareConfig`, causing a `.expect()` panic in the `retain()` function during state transition from `PendingMetadata` to `PendingDecision`.

## Finding Description

The `SecretShareItem` state machine manages three states: `PendingMetadata`, `PendingDecision`, and `Decided`. The vulnerability exists in the transition from `PendingMetadata` to `PendingDecision` when the `add_self_share()` method is invoked. [1](#0-0) 

The `SecretShareConfig` initializes an empty `weights` HashMap, but provides inconsistent methods to access weights:
- `get_peer_weight()` returns hardcoded value `1`
- `get_peer_weights()` returns the empty HashMap [2](#0-1) 

When shares from other validators arrive via `add_share()`, they use `get_peer_weight()` and are successfully added to the aggregator in `PendingMetadata` state: [3](#0-2) [4](#0-3) 

However, when `add_self_share()` is called later, it retrieves the empty weights HashMap via `get_peer_weights()`: [5](#0-4) 

This empty HashMap is passed to `add_share_with_metadata()`, which calls `retain()` to filter shares by metadata: [6](#0-5) 

The `retain()` function attempts to recalculate weights by looking up each share's author in the empty HashMap, resulting in a panic: [7](#0-6) 

**Attack Scenario:**
1. Validator A, B, and C all receive block at round R at approximately the same time
2. Validator B computes its secret share fastest and broadcasts it
3. Validator A receives Validator B's share while still computing its own share
4. Validator A's `add_share()` is called, adding Validator B's share to aggregator in `PendingMetadata` state
5. Validator A finishes computing its own share and calls `add_self_share()`
6. `retain()` is invoked with the empty weights HashMap
7. `.expect("Author must exist for weight")` panics because Validator B's author is not in the empty HashMap
8. Validator A crashes

This is not a malicious attack but a natural race condition in distributed systems where validators have different computational speeds and network latencies.

## Impact Explanation

**Severity: Critical** (potentially up to $1,000,000)

This vulnerability causes validator nodes to panic and crash, directly impacting consensus availability. The impact escalates based on the number of affected validators:

- **Single Validator Impact**: Loss of one validator reduces network redundancy and increases risk of liveness failures
- **Multiple Validator Impact**: If enough validators crash (>1/3 of voting power), the network loses liveness and cannot make progress
- **Non-recoverable without intervention**: Crashed validators must be manually restarted, and the race condition can recur

This maps to the Critical severity category: "Total loss of liveness/network availability" if the race condition affects multiple validators simultaneously, which is highly likely given that all validators process blocks at similar times.

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability will trigger frequently in production environments because:

1. **Natural Network Conditions**: Validators have different hardware capabilities, network latencies, and computational loads. Some validators naturally compute shares faster than others.

2. **No Special Privileges Required**: This is not an attack requiring malicious actors - normal network operation triggers it.

3. **Timing Window**: The window between receiving peer shares and computing own share is significant (involves cryptographic operations), making race conditions common.

4. **Guaranteed Trigger**: In any scenario where peer shares arrive before self-share computation completes (which will happen regularly), the validator will crash.

5. **Network-Wide Impact**: Since all validators process blocks at approximately the same time, multiple validators can crash simultaneously.

The only reason this might not be observed in current deployments is if:
- The feature is not yet activated in production
- Network conditions happen to process self-shares before peer shares consistently
- The bug has already been encountered but not diagnosed

## Recommendation

**Immediate Fix**: Populate the `weights` HashMap in `SecretShareConfig` initialization based on validator information.

**Option 1 - Use ValidatorVerifier weights:**
```rust
pub fn new(
    author: Author,
    epoch: u64,
    validator: Arc<ValidatorVerifier>,
    digest_key: DigestKey,
    msk_share: MasterSecretKeyShare,
    verification_keys: Vec<VerificationKey>,
    config: <FPTXWeighted as BatchThresholdEncryption>::ThresholdConfig,
    encryption_key: EncryptionKey,
) -> Self {
    // Populate weights from ValidatorVerifier
    let weights: HashMap<Author, u64> = validator
        .get_ordered_account_addresses_iter()
        .map(|addr| (addr, validator.get_voting_power(&addr).unwrap_or(0)))
        .collect();
    
    Self {
        _author: author,
        _epoch: epoch,
        validator,
        digest_key,
        msk_share,
        verification_keys,
        config,
        encryption_key,
        weights,
    }
}
```

**Option 2 - Make get_peer_weights() consistent with get_peer_weight():**
```rust
pub fn get_peer_weights(&self) -> HashMap<Author, u64> {
    // If weights HashMap is empty, fall back to uniform weight of 1
    if self.weights.is_empty() {
        self.validator
            .get_ordered_account_addresses_iter()
            .map(|addr| (addr, 1u64))
            .collect()
    } else {
        self.weights.clone()
    }
}
```

**Option 3 - Remove the panic and handle gracefully:**
```rust
fn retain(&mut self, metadata: &SecretShareMetadata, weights: &HashMap<Author, u64>) {
    self.shares.retain(|_, share| share.metadata == *metadata);
    self.total_weight = self
        .shares
        .keys()
        .filter_map(|author| weights.get(author).copied())
        .sum();
    
    // If weights weren't found, use default weight of 1
    if self.total_weight == 0 && !self.shares.is_empty() {
        self.total_weight = self.shares.len() as u64;
    }
}
```

**Recommended approach**: Option 2 is safest as it maintains backward compatibility and provides a clear fallback strategy consistent with the TODO comment in the codebase.

## Proof of Concept

```rust
#[tokio::test]
async fn test_secret_share_empty_weights_panic() {
    use consensus::rand::secret_sharing::secret_share_store::SecretShareStore;
    use aptos_types::secret_sharing::{SecretShare, SecretShareConfig, SecretShareMetadata};
    use aptos_consensus_types::common::Author;
    
    // Create SecretShareConfig with empty weights (current implementation)
    let epoch = 1;
    let self_author = Author::random();
    let peer_author = Author::random();
    let validator = Arc::new(ValidatorVerifier::new(/* ... */));
    let config = SecretShareConfig::new(
        self_author,
        epoch,
        validator.clone(),
        // ... other parameters
    );
    
    let (decision_tx, _decision_rx) = unbounded();
    let mut store = SecretShareStore::new(epoch, self_author, config, decision_tx);
    
    // Simulate block arrival
    store.update_highest_known_round(10);
    
    // Create metadata for round 10
    let metadata = SecretShareMetadata::new(epoch, 10, 1000, HashValue::random(), Digest::default());
    
    // Peer share arrives FIRST (simulating faster validator)
    let peer_share = SecretShare::new(peer_author, metadata.clone(), /* cryptographic share */);
    
    // This succeeds - adds share in PendingMetadata state
    store.add_share(peer_share).expect("Should succeed");
    
    // Self share computation completes LATER
    let self_share = SecretShare::new(self_author, metadata.clone(), /* cryptographic share */);
    
    // This will PANIC at retain() when trying to look up peer_author in empty weights HashMap
    // Expected panic: "Author must exist for weight"
    store.add_self_share(self_share).expect("This will panic!");
}
```

**Reproduction Steps:**
1. Set up a local testnet with 4 validators
2. Submit a block that requires secret sharing
3. Introduce artificial delay on one validator's secret share computation (e.g., add sleep)
4. Observe that the delayed validator receives peer shares before completing its own computation
5. Validator will panic with "Author must exist for weight" message
6. Check validator logs for panic in `secret_share_store.rs` at the `retain()` function

**Notes**

The vulnerability stems from an incomplete implementation where weighted validator support was planned (evidenced by the TODO comment) but never completed. The empty `weights` HashMap is a vestigial data structure that creates a critical inconsistency with the hardcoded weight return value. This affects the core consensus availability invariant, as crashed validators cannot participate in block finalization.

### Citations

**File:** types/src/secret_sharing.rs (L145-169)
```rust
    weights: HashMap<Author, u64>,
}

impl SecretShareConfig {
    pub fn new(
        author: Author,
        epoch: u64,
        validator: Arc<ValidatorVerifier>,
        digest_key: DigestKey,
        msk_share: MasterSecretKeyShare,
        verification_keys: Vec<VerificationKey>,
        config: <FPTXWeighted as BatchThresholdEncryption>::ThresholdConfig,
        encryption_key: EncryptionKey,
    ) -> Self {
        Self {
            _author: author,
            _epoch: epoch,
            validator,
            digest_key,
            msk_share,
            verification_keys,
            config,
            encryption_key,
            weights: HashMap::new(),
        }
```

**File:** types/src/secret_sharing.rs (L196-202)
```rust
    pub fn get_peer_weight(&self, _peer: &Author) -> u64 {
        1
    }

    pub fn get_peer_weights(&self) -> &HashMap<Author, u64> {
        &self.weights
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L74-81)
```rust
    fn retain(&mut self, metadata: &SecretShareMetadata, weights: &HashMap<Author, u64>) {
        self.shares.retain(|_, share| share.metadata == *metadata);
        self.total_weight = self
            .shares
            .keys()
            .map(|author| weights.get(author).expect("Author must exist for weight"))
            .sum();
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L108-113)
```rust
    fn add_share(&mut self, share: SecretShare, share_weight: u64) -> anyhow::Result<()> {
        match self {
            SecretShareItem::PendingMetadata(aggr) => {
                aggr.add_share(share, share_weight);
                Ok(())
            },
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L156-172)
```rust
    fn add_share_with_metadata(
        &mut self,
        share: SecretShare,
        share_weights: &HashMap<Author, u64>,
    ) -> anyhow::Result<()> {
        let item = std::mem::replace(self, Self::new(Author::ONE));
        let share_weight = *share_weights
            .get(share.author())
            .expect("Author must exist in weights");
        let new_item = match item {
            SecretShareItem::PendingMetadata(mut share_aggregator) => {
                let metadata = share.metadata.clone();
                share_aggregator.retain(share.metadata(), share_weights);
                share_aggregator.add_share(share, share_weight);
                SecretShareItem::PendingDecision {
                    metadata,
                    share_aggregator,
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L237-254)
```rust
    pub fn add_self_share(&mut self, share: SecretShare) -> anyhow::Result<()> {
        assert!(
            self.self_author == share.author,
            "Only self shares can be added with metadata"
        );
        let peer_weights = self.secret_share_config.get_peer_weights();
        let metadata = share.metadata();
        ensure!(metadata.epoch == self.epoch, "Share from different epoch");
        ensure!(
            metadata.round <= self.highest_known_round + FUTURE_ROUNDS_TO_ACCEPT,
            "Share from future round"
        );

        let item = self
            .secret_share_map
            .entry(metadata.round)
            .or_insert_with(|| SecretShareItem::new(self.self_author));
        item.add_share_with_metadata(share, peer_weights)?;
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L259-275)
```rust
    pub fn add_share(&mut self, share: SecretShare) -> anyhow::Result<bool> {
        let weight = self.secret_share_config.get_peer_weight(share.author());
        let metadata = share.metadata();
        ensure!(metadata.epoch == self.epoch, "Share from different epoch");
        ensure!(
            metadata.round <= self.highest_known_round + FUTURE_ROUNDS_TO_ACCEPT,
            "Share from future round"
        );

        let item = self
            .secret_share_map
            .entry(metadata.round)
            .or_insert_with(|| SecretShareItem::new(self.self_author));
        item.add_share(share, weight)?;
        item.try_aggregate(&self.secret_share_config, self.decision_tx.clone());
        Ok(item.has_decision())
    }
```
