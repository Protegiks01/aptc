# Audit Report

## Title
Epoch-Transaction Version Misalignment in Concurrent Backup Streams Bypasses Cryptographic Verification

## Summary
The concurrent execution of `backup_epoch_endings`, `backup_state_snapshots`, and `backup_transactions` streams can create backup sets where transaction data from epoch N is backed up before epoch N's ending ledger info is available. During restore/verification, this triggers a verification bypass that accepts unverified transaction data with only a warning, violating cryptographic integrity guarantees. [1](#0-0) 

## Finding Description

The backup coordinator uses watch channels to coordinate three concurrent backup streams. The critical flow is:

1. **Epoch Ending Backup** backs up completed epochs only (current epoch hasn't ended yet) [2](#0-1) 

2. **Transaction Backup** backs up transactions up to `committed_version` (includes current open epoch) [3](#0-2) 

3. Both streams receive the same `DbState` and run concurrently, but back up different epoch ranges

**Race Condition Scenario:**
- Current state: epoch=101, committed_version=15000 (epoch 101 is ongoing)
- `backup_epoch_endings` backs up epoch 100 ending only (epoch 101 hasn't ended)
- `backup_transactions` backs up transactions 10001-15000 (which are in epoch 101)
- Transaction backup includes a proof with `LedgerInfoWithSignatures` from epoch 101 [4](#0-3) 

**Verification Bypass:**
During restore, the transaction proof's LedgerInfo must be verified against epoch history. However, if the epoch history only contains epochs 0-100 but the transaction is from epoch 101+: [5](#0-4) 

The condition at line 279 triggers when `epoch > epoch_endings.len()`, causing verification to be skipped with only a warning. This explicitly states: "Previous chunks are verified and node won't be able to start if this data is malicious" - but this assumption is false when backup sets are inconsistent. [6](#0-5) 

The TODO comment acknowledges this is a known issue requiring an upstream fix.

## Impact Explanation

**High Severity** - This meets "Significant protocol violations" criteria:

1. **Cryptographic Verification Bypass**: Transaction proofs cannot be cryptographically verified without proper epoch history, yet the system accepts them
2. **State Consistency Violations**: Restored nodes may have incorrect state if malicious/corrupted transaction backups are accepted
3. **Backup System Integrity**: The entire backup/restore infrastructure's security guarantee is compromised
4. **Potential Consensus Impact**: If multiple nodes restore from inconsistent backups, they may have divergent state leading to consensus failures

While not directly a consensus attack, this breaks the invariant: "State transitions must be atomic and verifiable via Merkle proofs" - the proofs cannot be verified but are accepted anyway.

## Likelihood Explanation

**High Likelihood** during normal operations:

1. Continuous backup runs automatically on validator nodes
2. The race window exists whenever transactions are being committed in the current epoch
3. No attacker action required - this is a systemic design flaw
4. Backup sets created at different times naturally have this misalignment
5. The TODO comment indicates developers are aware but haven't implemented a fix

The issue occurs whenever:
- Backups are taken continuously (recommended practice)
- The blockchain is actively processing transactions
- Transaction backup catches up to current `committed_version` before the epoch ends

## Recommendation

**Immediate Fix**: Enforce epoch alignment before signaling downstream workers:

```rust
async fn backup_epoch_endings(
    &self,
    mut last_epoch_ending_epoch_in_backup: Option<u64>,
    db_state: DbState,
    downstream_db_state_broadcaster: &watch::Sender<Option<DbState>>,
) -> Result<Option<u64>> {
    loop {
        // ... existing backup logic ...
        
        if db_state.epoch <= last {
            break;
        }
        
        // Backup epoch ending
        EpochEndingBackupController::new(...).run().await?;
        last_epoch_ending_epoch_in_backup = Some(last)
    }
    
    // NEW: Only signal downstream if we've caught up to at least db_state.epoch - 1
    // This ensures transaction backups won't include data from epochs without endings
    let safe_db_state = if let Some(last_backed_up) = last_epoch_ending_epoch_in_backup {
        if db_state.epoch > last_backed_up + 1 {
            // Limit committed_version to not exceed last backed up epoch's range
            let max_version = self.ledger_db.metadata_db()
                .get_latest_ledger_info_in_epoch(last_backed_up)?
                .ledger_info().version();
            DbState {
                epoch: db_state.epoch,
                committed_version: min(db_state.committed_version, max_version),
            }
        } else {
            db_state
        }
    } else {
        db_state
    };
    
    downstream_db_state_broadcaster
        .send(Some(safe_db_state))
        .unwrap();
    Ok(last_epoch_ending_epoch_in_backup)
}
```

**Verification Fix**: Make verification failure fatal instead of warning:

```rust
pub fn verify_ledger_info(&self, li_with_sigs: &LedgerInfoWithSignatures) -> Result<()> {
    let epoch = li_with_sigs.ledger_info().epoch();
    ensure!(!self.epoch_endings.is_empty(), "Empty epoch history.");
    
    // REMOVE the bypass, make it an error
    ensure!(
        epoch <= self.epoch_endings.len() as u64,
        "Cannot verify LedgerInfo from epoch {} - epoch history only contains {} epochs. \
        Epoch ending backups must be complete before transaction backups can be verified.",
        epoch,
        self.epoch_endings.len()
    );
    
    // ... rest of verification logic
}
```

## Proof of Concept

```rust
// Reproduction steps for continuous backup scenario
#[tokio::test]
async fn test_epoch_transaction_version_misalignment() {
    // Setup: Mock blockchain at epoch 101, committed_version 15000
    let db_state = DbState {
        epoch: 101,
        committed_version: 15000,
    };
    
    // Simulate backup state: epoch endings only up to 100
    let backup_state = BackupStorageState {
        latest_epoch_ending_epoch: Some(100),
        latest_state_snapshot_epoch: Some(100),
        latest_state_snapshot_version: Some(10000),
        latest_transaction_version: Some(10000),
    };
    
    // Concurrent execution of backup streams with same DbState
    let (tx2, rx2) = watch::channel::<Option<DbState>>(Some(db_state));
    
    // Transaction backup will back up to version 15000 (epoch 101)
    let txn_backup_handle = tokio::spawn(async move {
        // Simulates backup_transactions worker
        let last_version = 10000;
        let (first, last) = get_batch_range(Some(last_version), 5000);
        // first=10001, last=15000 (in epoch 101)
        assert_eq!(first, 10001);
        assert_eq!(last, 15000);
        
        // Transaction proof will contain LedgerInfo from epoch 101
        // But epoch ending for 101 hasn't been backed up
    });
    
    // Verify the issue during restore
    let epoch_history = EpochHistory {
        epoch_endings: vec![/* epochs 0-100 */],
        trusted_waypoints: Arc::new(HashMap::new()),
    };
    
    let ledger_info_from_epoch_101 = /* LedgerInfo from transaction backup */;
    
    // This should fail but returns Ok() with warning
    let result = epoch_history.verify_ledger_info(&ledger_info_from_epoch_101);
    assert!(result.is_ok()); // VULNERABILITY: Verification bypassed!
}
```

## Notes

This vulnerability demonstrates how the concurrent backup architecture creates timing windows where cryptographic verification can be bypassed. The comment "TODO(aldenhu): fix this from upper level" confirms this is a known architectural issue requiring systematic fixes to the backup coordination logic, not just the verification layer.

The fix must ensure that transaction backups never include data from epochs whose endings haven't been backed up yet, maintaining the invariant that all backed-up data can be cryptographically verified during restore.

### Citations

**File:** storage/backup/backup-cli/src/coordinators/backup.rs (L124-159)
```rust
        // On new DbState retrieved:
        // `watch_db_state` informs `backup_epoch_endings` via channel 1,
        // and the latter informs the other backup type workers via channel 2, after epoch
        // ending is properly backed up, if necessary. This way, the epoch ending LedgerInfo needed
        // for proof verification is always available in the same backup storage.
        let (tx1, rx1) = watch::channel::<Option<DbState>>(None);
        let (tx2, rx2) = watch::channel::<Option<DbState>>(None);

        // Schedule work streams.
        let watch_db_state = IntervalStream::new(interval(Duration::from_secs(1)))
            .then(|_| self.try_refresh_db_state(&tx1))
            .boxed_local();

        let backup_epoch_endings = self
            .backup_work_stream(
                backup_state.latest_epoch_ending_epoch,
                &rx1,
                |slf, last_epoch, db_state| {
                    Self::backup_epoch_endings(slf, last_epoch, db_state, &tx2)
                },
            )
            .boxed_local();
        let backup_state_snapshots = self
            .backup_work_stream(
                backup_state.latest_state_snapshot_epoch,
                &rx2,
                Self::backup_state_snapshot,
            )
            .boxed_local();
        let backup_transactions = self
            .backup_work_stream(
                backup_state.latest_transaction_version,
                &rx2,
                Self::backup_transactions,
            )
            .boxed_local();
```

**File:** storage/backup/backup-cli/src/coordinators/backup.rs (L199-235)
```rust
    async fn backup_epoch_endings(
        &self,
        mut last_epoch_ending_epoch_in_backup: Option<u64>,
        db_state: DbState,
        downstream_db_state_broadcaster: &watch::Sender<Option<DbState>>,
    ) -> Result<Option<u64>> {
        loop {
            if let Some(epoch) = last_epoch_ending_epoch_in_backup {
                EPOCH_ENDING_EPOCH.set(epoch as i64);
            }
            let (first, last) = get_batch_range(last_epoch_ending_epoch_in_backup, 1);

            if db_state.epoch <= last {
                // "<=" because `db_state.epoch` hasn't ended yet, wait for the next db_state update
                break;
            }

            EpochEndingBackupController::new(
                EpochEndingBackupOpt {
                    start_epoch: first,
                    end_epoch: last + 1,
                },
                self.global_opt.clone(),
                Arc::clone(&self.client),
                Arc::clone(&self.storage),
            )
            .run()
            .await?;
            last_epoch_ending_epoch_in_backup = Some(last)
        }

        downstream_db_state_broadcaster
            .send(Some(db_state))
            .map_err(|e| anyhow!("Receivers should not be cancelled: {}", e))
            .unwrap();
        Ok(last_epoch_ending_epoch_in_backup)
    }
```

**File:** storage/backup/backup-cli/src/coordinators/backup.rs (L269-302)
```rust
    async fn backup_transactions(
        &self,
        mut last_transaction_version_in_backup: Option<Version>,
        db_state: DbState,
    ) -> Result<Option<u64>> {
        loop {
            if let Some(version) = last_transaction_version_in_backup {
                TRANSACTION_VERSION.set(version as i64);
            }
            let (first, last) = get_batch_range(
                last_transaction_version_in_backup,
                self.transaction_batch_size,
            );

            if db_state.committed_version < last {
                // wait for the next db_state update
                return Ok(last_transaction_version_in_backup);
            }

            TransactionBackupController::new(
                TransactionBackupOpt {
                    start_version: first,
                    num_transactions: (last + 1 - first) as usize,
                },
                self.global_opt.clone(),
                Arc::clone(&self.client),
                Arc::clone(&self.storage),
            )
            .run()
            .await?;

            last_transaction_version_in_backup = Some(last);
        }
    }
```

**File:** storage/aptosdb/src/backup/backup_handler.rs (L111-137)
```rust
    /// Gets the proof for a transaction chunk.
    /// N.B. the `LedgerInfo` returned will always be in the same epoch of the `last_version`.
    pub fn get_transaction_range_proof(
        &self,
        first_version: Version,
        last_version: Version,
    ) -> Result<(TransactionAccumulatorRangeProof, LedgerInfoWithSignatures)> {
        ensure!(
            last_version >= first_version,
            "Bad transaction range: [{}, {}]",
            first_version,
            last_version
        );
        let num_transactions = last_version - first_version + 1;
        let ledger_metadata_db = self.ledger_db.metadata_db();
        let epoch = ledger_metadata_db.get_epoch(last_version)?;
        let ledger_info = ledger_metadata_db.get_latest_ledger_info_in_epoch(epoch)?;
        let accumulator_proof = self
            .ledger_db
            .transaction_accumulator_db()
            .get_transaction_range_proof(
                Some(first_version),
                num_transactions,
                ledger_info.ledger_info().version(),
            )?;
        Ok((accumulator_proof, ledger_info))
    }
```

**File:** storage/backup/backup-cli/src/backup_types/epoch_ending/restore.rs (L276-312)
```rust
    pub fn verify_ledger_info(&self, li_with_sigs: &LedgerInfoWithSignatures) -> Result<()> {
        let epoch = li_with_sigs.ledger_info().epoch();
        ensure!(!self.epoch_endings.is_empty(), "Empty epoch history.",);
        if epoch > self.epoch_endings.len() as u64 {
            // TODO(aldenhu): fix this from upper level
            warn!(
                epoch = epoch,
                epoch_history_until = self.epoch_endings.len(),
                "Epoch is too new and can't be verified. Previous chunks are verified and node \
                won't be able to start if this data is malicious."
            );
            return Ok(());
        }
        if epoch == 0 {
            ensure!(
                li_with_sigs.ledger_info() == &self.epoch_endings[0],
                "Genesis epoch LedgerInfo info doesn't match.",
            );
        } else if let Some(wp_trusted) = self
            .trusted_waypoints
            .get(&li_with_sigs.ledger_info().version())
        {
            let wp_li = Waypoint::new_any(li_with_sigs.ledger_info());
            ensure!(
                *wp_trusted == wp_li,
                "Waypoints don't match. In backup: {}, trusted: {}",
                wp_li,
                wp_trusted,
            );
        } else {
            self.epoch_endings[epoch as usize - 1]
                .next_epoch_state()
                .ok_or_else(|| anyhow!("Shouldn't contain non- epoch bumping LIs."))?
                .verify(li_with_sigs)?;
        };
        Ok(())
    }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L147-167)
```rust
        let (range_proof, ledger_info) = storage
            .load_bcs_file::<(TransactionAccumulatorRangeProof, LedgerInfoWithSignatures)>(
                &manifest.proof,
            )
            .await?;
        if let Some(epoch_history) = epoch_history {
            epoch_history.verify_ledger_info(&ledger_info)?;
        }

        // make a `TransactionListWithProof` to reuse its verification code.
        let txn_list_with_proof =
            TransactionListWithProofV2::new(TransactionListWithAuxiliaryInfos::new(
                TransactionListWithProof::new(
                    txns,
                    Some(event_vecs),
                    Some(manifest.first_version),
                    TransactionInfoListWithProof::new(range_proof, txn_infos),
                ),
                persisted_aux_info,
            ));
        txn_list_with_proof.verify(ledger_info.ledger_info(), Some(manifest.first_version))?;
```
