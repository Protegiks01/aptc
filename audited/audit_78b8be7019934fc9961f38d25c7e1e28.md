# Audit Report

## Title
Primary Key Collision in Token V2 Metadata Indexer Due to Resource Type Truncation

## Summary
The token v2 metadata indexer truncates Move resource type strings to 128 characters before using them as part of the composite primary key `(object_address, resource_type)`. In the blockchain's Move VM, resource types are compared using their full string representation with arbitrarily long generic parameters. This allows two distinct resources with different full type names to collide in the indexer database if they share the same first 128 characters, causing data corruption where one resource's metadata overwrites another's. [1](#0-0) 

## Finding Description

The vulnerability exists in the token v2 metadata indexing logic. When processing blockchain write operations, the indexer extracts resource types and truncates them: [2](#0-1) 

The truncation occurs at line 57, where `NAME_LENGTH` is defined as 128 characters: [3](#0-2) [4](#0-3) 

The database schema enforces this truncated type as part of the primary key: [5](#0-4) 

However, Move's global storage model allows multiple resources of different types at the same address, where types are distinguished by their **full** type string including generic parameters: [6](#0-5) 

The Move VM stores the complete type string without truncation: [7](#0-6) 

**Attack Scenario:**
1. Attacker deploys a Move module with resource types using long generic parameters
2. Creates two resource types that differ only after character 128:
   - `0xAddr::module::Resource<0xOther::verylongmodule::VeryLongTypeNameAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA>`
   - `0xAddr::module::Resource<0xOther::verylongmodule::VeryLongTypeNameBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB>`
3. Attaches both resources to the same token object address
4. Blockchain state correctly stores both as separate resources (different full types)
5. Indexer treats them as the same due to truncation collision
6. Database upsert logic overwrites the first resource's data with the second's: [8](#0-7) 

The upsert's WHERE clause only checks transaction version ordering, not resource uniqueness. Within a transaction batch, the HashMap deduplication also causes data loss: [9](#0-8) [10](#0-9) 

## Impact Explanation

This vulnerability causes **data corruption in the indexer database**, classifying as **Medium Severity** under the Aptos bug bounty program's "State inconsistencies requiring intervention" category.

**Affected Systems:**
- Indexer database serving external queries
- Applications and users relying on token metadata accuracy
- Off-chain services making decisions based on corrupted metadata

**Not Affected:**
- Blockchain consensus or state integrity (canonical state remains correct)
- Move VM execution or validator operations
- On-chain token functionality or ownership

While the blockchain itself operates correctly, the indexer serves as the primary API for most applications to query token data. Corrupted metadata can lead to:
- Incorrect token properties displayed to users
- Potential financial harm if metadata affects valuations
- Loss of trust in indexer data accuracy
- Manual intervention required to identify and fix corrupted records

## Likelihood Explanation

**Likelihood: Medium-Low**

**Feasibility:**
- Requires crafting Move resource types with specific collision patterns
- Needs ability to deploy Move modules and attach resources to token objects
- Technically achievable within Move's type system constraints
- Generic type parameters can be arbitrarily nested

**Barriers:**
- Attacker must control or influence token object resource attachments
- Requires understanding of both Move type system and indexer implementation
- Impact limited to indexer queries, not blockchain integrity
- Collision must be crafted at exactly the 128-character boundary

**Real-world Occurrence:**
- Accidental collisions unlikely due to 128-character space
- Deliberate exploitation requires malicious intent and technical knowledge
- May occur more frequently as DeFi protocols use complex generic types

## Recommendation

**Fix 1: Use Full Type Hash Instead of Truncation**

Replace the truncated `resource_type` string with a hash of the full type string to guarantee uniqueness:

```rust
// In v2_token_metadata.rs, CurrentTokenV2Metadata::from_write_resource()
let resource_type = hash_str(&resource.type_); // Use full hash instead of truncation

// Update database schema to:
// resource_type VARCHAR(66) NOT NULL  -- Store full hash (0x + 64 hex chars)
```

**Fix 2: Include State Key Hash in Primary Key**

The `state_key_hash` already uniquely identifies the resource in blockchain storage. Use it as part of the primary key:

```sql
-- Update schema to:
PRIMARY KEY (object_address, state_key_hash)
-- Or if resource_type is needed for queries:
PRIMARY KEY (object_address, resource_type, state_key_hash)
```

**Fix 3: Increase Truncation Limit**

While not a complete solution, increasing the limit reduces collision probability:

```rust
pub const RESOURCE_TYPE_LENGTH: usize = 512; // Increased from 128
```

**Recommended Approach:** Implement Fix 1 (full hash) for guaranteed uniqueness while maintaining efficient indexing. This aligns with how other identifiers (addresses, hashes) are stored as fixed-length hex strings.

## Proof of Concept

```move
// File: sources/collision_poc.move
module attacker::collision_poc {
    use std::signer;
    use aptos_framework::object::{Self, Object};
    
    // Generic wrapper allowing arbitrary type parameters
    struct CustomMetadata<phantom T> has key {
        value: u64
    }
    
    // Helper types with long names to create collision
    struct VeryLongTypeNameForCollisionTesting_AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA {}
    struct VeryLongTypeNameForCollisionTesting_BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB {}
    
    // Create token with first metadata type
    public entry fun create_with_type_a(creator: &signer) {
        let constructor_ref = object::create_object(@attacker);
        let obj_signer = object::generate_signer(&constructor_ref);
        
        // Attach first custom metadata
        move_to(&obj_signer, CustomMetadata<VeryLongTypeNameForCollisionTesting_AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA> {
            value: 111
        });
    }
    
    // Attach second metadata type to same object (different Move type, same truncated indexer key)
    public entry fun add_type_b(creator: &signer, obj: Object<CustomMetadata<VeryLongTypeNameForCollisionTesting_AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA>>) {
        let obj_addr = object::object_address(&obj);
        let obj_signer = /* obtain signer through transfer ref or other mechanism */;
        
        // Attach second custom metadata (different type, colliding truncation)
        move_to(&obj_signer, CustomMetadata<VeryLongTypeNameForCollisionTesting_BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB> {
            value: 222
        });
        
        // Blockchain state now has TWO distinct resources at same address
        // Indexer will only show one due to primary key collision
    }
}
```

**Verification Steps:**
1. Deploy the Move module with two long generic type names differing after 128 chars
2. Execute `create_with_type_a()` to create object with first metadata type
3. Execute `add_type_b()` to attach second metadata type to same object
4. Query blockchain state: both resources exist with distinct full type names
5. Query indexer database: only one entry exists in `current_token_v2_metadata` table
6. The `data` field shows value 222 (second resource), not 111 (first resource)
7. Data for the first resource type is lost in the indexer despite existing on-chain

**Notes**

This vulnerability is specific to the **indexer component**, not the core blockchain protocol. The canonical blockchain state maintained by validators remains correct and uncorrupted. However, since most applications query the indexer API rather than running full nodes, corrupted indexer data can have widespread impact on the ecosystem. The fix should be prioritized for indexer deployments to ensure data integrity for downstream consumers.

### Citations

**File:** crates/indexer/src/models/token_models/v2_token_metadata.rs (L20-32)
```rust
// PK of current_objects, i.e. object_address, resource_type
pub type CurrentTokenV2MetadataPK = (String, String);

#[derive(Debug, Deserialize, FieldCount, Identifiable, Insertable, Serialize)]
#[diesel(primary_key(object_address, resource_type))]
#[diesel(table_name = current_token_v2_metadata)]
pub struct CurrentTokenV2Metadata {
    pub object_address: String,
    pub resource_type: String,
    pub data: Value,
    pub state_key_hash: String,
    pub last_transaction_version: i64,
}
```

**File:** crates/indexer/src/models/token_models/v2_token_metadata.rs (L36-66)
```rust
    pub fn from_write_resource(
        write_resource: &WriteResource,
        txn_version: i64,
        token_v2_metadata: &TokenV2AggregatedDataMapping,
    ) -> anyhow::Result<Option<Self>> {
        let object_address = standardize_address(&write_resource.address.to_string());
        if let Some(metadata) = token_v2_metadata.get(&object_address) {
            // checking if token_v2
            if metadata.token.is_some() {
                let resource_type_addr = write_resource.data.typ.address.to_string();
                if matches!(resource_type_addr.as_str(), "0x1" | "0x3" | "0x4") {
                    return Ok(None);
                }

                let resource = MoveResource::from_write_resource(write_resource, 0, txn_version, 0);

                let state_key_hash = metadata.object.get_state_key_hash();
                if state_key_hash != resource.state_key_hash {
                    return Ok(None);
                }

                let resource_type = truncate_str(&resource.type_, NAME_LENGTH);
                return Ok(Some(CurrentTokenV2Metadata {
                    object_address,
                    resource_type,
                    data: resource
                        .data
                        .context("data must be present in write resource")?,
                    state_key_hash: resource.state_key_hash,
                    last_transaction_version: txn_version,
                }));
```

**File:** crates/indexer/src/models/token_models/token_utils.rs (L17-17)
```rust
pub const NAME_LENGTH: usize = 128;
```

**File:** crates/indexer/src/util.rs (L23-27)
```rust
pub fn truncate_str(val: &str, max_chars: usize) -> String {
    let mut trunc = val.to_string();
    trunc.truncate(max_chars);
    trunc
}
```

**File:** crates/indexer/migrations/2023-05-24-052435_token_properties_v2/up.sql (L14-23)
```sql
CREATE TABLE IF NOT EXISTS current_token_v2_metadata (
  object_address VARCHAR(66) NOT NULL,
  resource_type VARCHAR(128) NOT NULL,
  data jsonb NOT NULL,
  state_key_hash VARCHAR(66) NOT NULL,
  last_transaction_version BIGINT NOT NULL,
  inserted_at TIMESTAMP NOT NULL DEFAULT NOW(),
  -- constraints
  PRIMARY KEY (object_address, resource_type)
);
```

**File:** third_party/move/documentation/book/src/global-storage-operators.md (L8-8)
```markdown
|`move_to<T>(&signer,T)`                 | Publish `T` under `signer.address`                              | If `signer.address` already holds a `T` |
```

**File:** crates/indexer/src/models/move_resources.rs (L36-56)
```rust
    pub fn from_write_resource(
        write_resource: &WriteResource,
        write_set_change_index: i64,
        transaction_version: i64,
        transaction_block_height: i64,
    ) -> Self {
        let parsed_data = Self::convert_move_struct_tag(&write_resource.data.typ);
        Self {
            transaction_version,
            transaction_block_height,
            write_set_change_index,
            type_: write_resource.data.typ.to_string(),
            name: parsed_data.name.clone(),
            address: standardize_address(&write_resource.address.to_string()),
            module: parsed_data.module.clone(),
            generic_type_params: parsed_data.generic_type_params,
            data: Some(serde_json::to_value(&write_resource.data.data).unwrap()),
            is_deleted: false,
            state_key_hash: standardize_address(write_resource.state_key_hash.as_str()),
        }
    }
```

**File:** crates/indexer/src/processors/token_processor.rs (L819-843)
```rust
fn insert_current_token_v2_metadatas(
    conn: &mut PgConnection,
    items_to_insert: &[CurrentTokenV2Metadata],
) -> Result<(), diesel::result::Error> {
    use schema::current_token_v2_metadata::dsl::*;

    let chunks = get_chunks(items_to_insert.len(), CurrentTokenV2Metadata::field_count());

    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::current_token_v2_metadata::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict((object_address, resource_type))
                .do_update()
                .set((
                    data.eq(excluded(data)),
                    state_key_hash.eq(excluded(state_key_hash)),
                    last_transaction_version.eq(excluded(last_transaction_version)),
                    inserted_at.eq(excluded(inserted_at)),
                )),
            Some(" WHERE current_token_v2_metadata.last_transaction_version <= excluded.last_transaction_version "),
        )?;
    }
    Ok(())
```

**File:** crates/indexer/src/processors/token_processor.rs (L1074-1075)
```rust
    let mut current_token_v2_metadata: HashMap<CurrentTokenV2MetadataPK, CurrentTokenV2Metadata> =
        HashMap::new();
```

**File:** crates/indexer/src/processors/token_processor.rs (L1465-1478)
```rust
                        if let Some(token_metadata) = CurrentTokenV2Metadata::from_write_resource(
                            resource,
                            txn_version,
                            &token_v2_metadata_helper,
                        )
                        .unwrap()
                        {
                            current_token_v2_metadata.insert(
                                (
                                    token_metadata.object_address.clone(),
                                    token_metadata.resource_type.clone(),
                                ),
                                token_metadata,
                            );
```
