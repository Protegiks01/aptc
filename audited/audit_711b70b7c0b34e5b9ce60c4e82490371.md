# Audit Report

## Title
Silent Pruning Failure Vulnerability Leading to Unbounded Database Growth and Node Crashes

## Summary
The `PrunerWorker` in the AptosDB storage layer silently catches and suppresses all pruning errors, logging them with sampling but never propagating them upward. This allows pruning operations to fail indefinitely without alerting operators, causing unbounded database growth that leads to disk exhaustion and validator node crashes.

## Finding Description

The vulnerability exists in the pruner worker loop that manages all three main pruners (LedgerPruner, StateKvPruner, and StateMerklePruner). While individual sub-pruners correctly propagate errors through `Result<()>` return types as specified in the `DBSubPruner` trait, the worker thread that invokes these pruners suppresses all errors. [1](#0-0) 

The trait properly defines error propagation with `Result<()>`, and all implementations correctly use the `?` operator to propagate errors: [2](#0-1) 

However, the `PrunerWorker` that calls these pruners has a critical flaw in its main loop: [3](#0-2) 

When `pruner.prune()` returns an error, the code:
1. Logs the error with sampling (may not log every occurrence)
2. Sleeps briefly
3. Continues the loop
4. **Never propagates the error or stops the pruner**
5. **Never updates progress**, so the same pruning operation is retried indefinitely

This breaks the **Resource Limits** invariant - the system must respect storage constraints through proper pruning. It also violates **State Consistency** as different nodes may have different pruning states if some fail silently while others succeed.

**Attack Scenario:**
1. A validator node experiences disk I/O errors, filesystem corruption, or disk space pressure
2. Pruning operations begin failing (write_schemas returns errors)
3. Errors are only logged with 1-second sampling rate
4. The pruner worker continues running, appearing healthy via `is_pruning_pending()`
5. Database grows unbounded as old data is never removed
6. Eventually disk fills completely
7. Node crashes due to inability to write new blocks
8. Network may experience validator outages or reduced capacity

All three pruner managers instantiate PrunerWorker with this vulnerability: [4](#0-3) 

## Impact Explanation

This is a **High Severity** vulnerability per Aptos bug bounty criteria:

- **Validator node slowdowns**: As database grows unbounded, query performance degrades significantly
- **API crashes**: Eventually disk exhaustion causes node crashes and API unavailability  
- **Significant protocol violations**: Pruning is a critical maintenance operation; silent failures violate operational guarantees

The impact escalates based on how many validators are affected:
- Single validator: Loss of one validator's availability
- Multiple validators: If common conditions (e.g., specific RocksDB version bug, filesystem issues) affect multiple nodes, could cause significant network degradation
- In extreme cases with correlated failures across many validators, could approach **Critical Severity** if it causes "Total loss of liveness/network availability"

## Likelihood Explanation

**HIGH likelihood** - This will occur in production environments because:

1. **Disk I/O errors** are common in production (hardware failures, filesystem corruption, I/O timeouts)
2. **Disk space pressure** is a realistic operational scenario
3. **RocksDB write failures** can occur due to various reasons (corruption, permissions, resource limits)
4. **No alerting mechanism** exists - operators won't know pruning has failed unless they specifically monitor disk growth or check logs
5. The `is_pruning_pending()` check will return `true` forever (since progress never advances), giving false impression work is ongoing

The sampling rate means errors may not even appear in logs frequently, making detection even harder: [5](#0-4) 

## Recommendation

The pruner worker should halt operation and raise alerts when pruning fails persistently. Here's the recommended fix:

```rust
fn work(&self) {
    let mut consecutive_failures = 0;
    const MAX_CONSECUTIVE_FAILURES: u32 = 3;
    
    while !self.quit_worker.load(Ordering::SeqCst) {
        let pruner_result = self.pruner.prune(self.batch_size);
        if pruner_result.is_err() {
            consecutive_failures += 1;
            let err = pruner_result.err().unwrap();
            
            // Always log pruning errors without sampling
            error!(
                error = ?err,
                consecutive_failures = consecutive_failures,
                "Pruner encountered error."
            );
            
            // Halt pruner after repeated failures to prevent silent data accumulation
            if consecutive_failures >= MAX_CONSECUTIVE_FAILURES {
                error!(
                    "Pruner {} has failed {} consecutive times. Halting pruner to prevent unbounded growth.",
                    self.pruner.name(),
                    consecutive_failures
                );
                // Set a metric to alert operators
                PRUNER_ERRORS
                    .with_label_values(&[self.pruner.name(), "halted"])
                    .inc();
                break;
            }
            
            sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            continue;
        }
        
        // Reset failure counter on success
        consecutive_failures = 0;
        
        if !self.pruner.is_pruning_pending() {
            sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
        }
    }
}
```

Additionally:
1. Add metrics for pruning failures and consecutive failure counts
2. Expose pruner health status via API endpoints for monitoring
3. Consider implementing exponential backoff for retries
4. Add alerts when pruner is halted or failing persistently

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::{Arc, Mutex};
    use aptos_storage_interface::AptosDbError;
    
    // Mock pruner that fails on every call
    struct FailingPruner {
        target_version: AtomicVersion,
        progress: AtomicVersion,
        call_count: Arc<Mutex<u32>>,
    }
    
    impl DBPruner for FailingPruner {
        fn name(&self) -> &'static str {
            "FailingPruner"
        }
        
        fn prune(&self, _batch_size: usize) -> Result<Version> {
            let mut count = self.call_count.lock().unwrap();
            *count += 1;
            // Simulate disk I/O error
            Err(AptosDbError::Other(format!("Disk I/O error on call {}", *count)))
        }
        
        fn progress(&self) -> Version {
            self.progress.load(Ordering::SeqCst)
        }
        
        fn set_target_version(&self, target_version: Version) {
            self.target_version.store(target_version, Ordering::SeqCst);
        }
        
        fn target_version(&self) -> Version {
            self.target_version.load(Ordering::SeqCst)
        }
        
        fn record_progress(&self, version: Version) {
            self.progress.store(version, Ordering::SeqCst);
        }
    }
    
    #[test]
    fn test_silent_pruning_failure() {
        let call_count = Arc::new(Mutex::new(0u32));
        let pruner = Arc::new(FailingPruner {
            target_version: AtomicVersion::new(1000),
            progress: AtomicVersion::new(0),
            call_count: Arc::clone(&call_count),
        });
        
        let worker = PrunerWorker::new(pruner.clone(), 100, "test");
        
        // Set target to trigger pruning
        worker.set_target_db_version(1000);
        
        // Wait for multiple pruning attempts
        std::thread::sleep(Duration::from_millis(500));
        
        let final_count = *call_count.lock().unwrap();
        
        // Verify pruner was called multiple times despite failures
        assert!(final_count > 1, "Pruner should retry after failures");
        
        // Verify progress never advanced (the vulnerability)
        assert_eq!(pruner.progress(), 0, "Progress should not advance on failure");
        
        // Verify pruning appears "pending" forever
        assert!(pruner.is_pruning_pending(), "Pruner appears to have pending work");
        
        // In production, this would continue indefinitely, accumulating data
        // until disk is full, causing node crash
    }
}
```

## Notes

This vulnerability affects all validator nodes running AptosDB. The silent nature of the failure makes it particularly dangerous because:

1. Operators have no automated way to detect pruning failures
2. Metrics show pruning as "pending" (which looks normal)
3. Database growth is gradual, making the issue hard to spot until critical
4. Different nodes may fail at different times due to different hardware conditions
5. Recovery requires manual intervention and potentially complex database repairs

The vulnerability demonstrates why error handling in critical infrastructure components must include proper alerting, circuit breakers, and fail-fast mechanisms rather than silent retry loops.

### Citations

**File:** storage/aptosdb/src/pruner/db_sub_pruner.rs (L7-14)
```rust
pub trait DBSubPruner {
    /// Returns the name of the sub pruner.
    fn name(&self) -> &str;

    /// Performs the actual pruning, a target version is passed, which is the target the pruner
    /// tries to prune.
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()>;
}
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L78-84)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L53-69)
```rust
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L159-159)
```rust
        PrunerWorker::new(pruner, ledger_pruner_config.batch_size, "ledger")
```
