# Audit Report

## Title
Configuration Validation Bypass in ConsensusObserverConfig Allows Invalid Runtime Values Causing Node Failures

## Summary
The `ConsensusObserverConfig::optimize()` function fails to validate configuration values and always returns `Ok()`, allowing semantically invalid configurations (such as zero values for critical parameters) to be silently accepted. Additionally, `ConsensusObserverConfig` does not implement the `ConfigSanitizer` trait, bypassing the validation layer entirely. This enables invalid configurations to reach runtime, causing consensus observer failures and node synchronization issues. [1](#0-0) 

## Finding Description
The configuration loading system uses two validation mechanisms:

1. **ConfigOptimizer** - Sets optimized values based on node type/chain ID
2. **ConfigSanitizer** - Validates configuration correctness

The `ConsensusObserverConfig` implements `ConfigOptimizer` but **not** `ConfigSanitizer`. The `optimize()` function always returns `Ok(modified_config)` without performing any validation: [2](#0-1) 

During node startup, the config loader calls both optimization and sanitization: [3](#0-2) 

However, `ConsensusObserverConfig::sanitize()` is never invoked because it's not in the sanitization chain: [4](#0-3) 

This allows invalid values to reach runtime. Critical runtime impacts include:

**1. Zero `max_num_pending_blocks` breaks block storage:**

In `PendingBlockStore`, when `max_num_pending_blocks` is 0, all blocks are immediately garbage collected: [5](#0-4) 

In `OrderedBlockStore`, the check prevents any blocks from being inserted: [6](#0-5) 

In `BlockPayloadStore`, payloads are immediately dropped: [7](#0-6) 

**2. Zero `max_concurrent_subscriptions` prevents consensus observation:**

The subscription manager cannot create any subscriptions: [8](#0-7) 

## Impact Explanation
This is a **High Severity** vulnerability:

- **Validator node slowdowns**: Nodes with invalid configs cannot observe consensus properly, falling behind the network
- **Significant protocol violations**: Consensus observer system fails to function, breaking state synchronization guarantees
- **Node availability issues**: Affected nodes cannot maintain sync with the chain, potentially requiring manual intervention

While this doesn't directly cause fund loss or consensus safety violations, it degrades network reliability and can cause operational failures across multiple nodes if configuration templates are distributed with invalid values.

## Likelihood Explanation
**Likelihood: Medium-High**

This can occur through:
1. **Operator error**: Manual config errors setting fields to 0
2. **Template propagation**: Invalid config templates distributed to multiple nodes
3. **Automated tooling bugs**: Config generation scripts producing invalid values
4. **Malicious insider**: Node operator intentionally degrading their node

The vulnerability is triggered at node startup, making it immediately exploitable. The lack of validation means the error is only discovered at runtime when the consensus observer attempts to use the invalid values, not during config loading where it should be caught.

## Recommendation

**1. Implement ConfigSanitizer for ConsensusObserverConfig:**

Add validation logic to ensure all critical fields have valid ranges:

```rust
impl ConfigSanitizer for ConsensusObserverConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let config = &node_config.consensus_observer;
        
        // Validate max_num_pending_blocks
        if config.max_num_pending_blocks == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "max_num_pending_blocks must be greater than 0".into(),
            ));
        }
        
        // Validate max_concurrent_subscriptions
        if config.max_concurrent_subscriptions == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "max_concurrent_subscriptions must be greater than 0".into(),
            ));
        }
        
        // Validate timeout values
        if config.network_request_timeout_ms == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "network_request_timeout_ms must be greater than 0".into(),
            ));
        }
        
        // Add similar checks for other critical fields
        
        Ok(())
    }
}
```

**2. Add sanitization call to NodeConfig::sanitize():** [9](#0-8) 

Insert `ConsensusObserverConfig::sanitize()` call after line 54.

**3. Optional: Add validation in optimize() as defense-in-depth:**

Even though sanitization is the proper place for validation, the `optimize()` function could check if it's modifying values to invalid ranges and return an error.

## Proof of Concept

```rust
#[test]
fn test_invalid_max_num_pending_blocks_not_caught() {
    use aptos_config::config::{NodeConfig, ConsensusObserverConfig};
    use aptos_config::config::node_config_loader::{NodeConfigLoader, NodeType};
    use aptos_types::chain_id::ChainId;
    use std::fs;
    use tempfile::NamedTempFile;
    
    // Create a config file with invalid max_num_pending_blocks
    let invalid_config_yaml = r#"
base:
  role: "full_node"
consensus_observer:
  observer_enabled: true
  publisher_enabled: true
  max_num_pending_blocks: 0
  max_concurrent_subscriptions: 0
full_node_networks:
  - network_id: "public"
"#;
    
    // Write to temporary file
    let mut temp_file = NamedTempFile::new().unwrap();
    fs::write(temp_file.path(), invalid_config_yaml).unwrap();
    
    // Load the config - this should fail but currently succeeds
    let result = NodeConfigLoader::new(temp_file.path()).load_and_sanitize_config();
    
    // Currently this passes (BUG), but should fail with validation error
    match result {
        Ok(config) => {
            // Invalid config was accepted!
            assert_eq!(config.consensus_observer.max_num_pending_blocks, 0);
            assert_eq!(config.consensus_observer.max_concurrent_subscriptions, 0);
            println!("BUG: Invalid configuration was accepted without validation!");
        },
        Err(e) => {
            println!("Config properly rejected: {:?}", e);
        }
    }
}
```

This test demonstrates that a configuration with `max_num_pending_blocks: 0` loads successfully without errors, even though this will cause runtime failures in the consensus observer system.

## Notes

The vulnerability stems from an architectural gap: `ConsensusObserverConfig` was added with `ConfigOptimizer` support but without corresponding `ConfigSanitizer` implementation. The `optimize()` function's `Result<bool, Error>` return type suggests error propagation was intended but never implemented. Similar validation exists for other config types (e.g., validator network mutual authentication checks), establishing the pattern that critical configs should be validated. [10](#0-9)

### Citations

**File:** config/src/config/consensus_observer_config.rs (L95-154)
```rust
    fn optimize(
        node_config: &mut NodeConfig,
        local_config_yaml: &Value,
        node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<bool, Error> {
        let consensus_observer_config = &mut node_config.consensus_observer;
        let local_observer_config_yaml = &local_config_yaml["consensus_observer"];

        // Check if the observer configs are manually set in the local config.
        // If they are, we don't want to override them.
        let observer_manually_set = !local_observer_config_yaml["observer_enabled"].is_null();
        let publisher_manually_set = !local_observer_config_yaml["publisher_enabled"].is_null();

        // Enable the consensus observer and publisher based on the node type
        let mut modified_config = false;
        match node_type {
            NodeType::Validator => {
                if ENABLE_ON_VALIDATORS && !publisher_manually_set {
                    // Only enable the publisher for validators
                    consensus_observer_config.publisher_enabled = true;
                    modified_config = true;
                }
            },
            NodeType::ValidatorFullnode => {
                if ENABLE_ON_VALIDATOR_FULLNODES
                    && !observer_manually_set
                    && !publisher_manually_set
                {
                    // Enable both the observer and the publisher for VFNs
                    consensus_observer_config.observer_enabled = true;
                    consensus_observer_config.publisher_enabled = true;
                    modified_config = true;
                }
            },
            NodeType::PublicFullnode => {
                if ENABLE_ON_PUBLIC_FULLNODES && !observer_manually_set && !publisher_manually_set {
                    // Enable both the observer and the publisher for PFNs
                    consensus_observer_config.observer_enabled = true;
                    consensus_observer_config.publisher_enabled = true;
                    modified_config = true;
                }
            },
        }

        // Optimize the max number of pending blocks to accommodate increased block rates.
        // Note: we currently only do this for test networks (e.g., devnet).
        if let Some(chain_id) = chain_id {
            if local_observer_config_yaml["max_num_pending_blocks"].is_null()
                && !chain_id.is_testnet()
                && !chain_id.is_mainnet()
            {
                consensus_observer_config.max_num_pending_blocks =
                    MAX_NUM_PENDING_BLOCKS_FOR_TEST_NETWORKS;
                modified_config = true;
            }
        }

        Ok(modified_config)
    }
```

**File:** config/src/config/node_config_loader.rs (L141-144)
```rust
    NodeConfig::optimize(node_config, &local_config_yaml, node_type, chain_id)?;

    // Sanitize the node config
    NodeConfig::sanitize(node_config, node_type, chain_id)
```

**File:** config/src/config/config_sanitizer.rs (L50-68)
```rust
        // Sanitize all of the sub-configs
        AdminServiceConfig::sanitize(node_config, node_type, chain_id)?;
        ApiConfig::sanitize(node_config, node_type, chain_id)?;
        BaseConfig::sanitize(node_config, node_type, chain_id)?;
        ConsensusConfig::sanitize(node_config, node_type, chain_id)?;
        DagConsensusConfig::sanitize(node_config, node_type, chain_id)?;
        ExecutionConfig::sanitize(node_config, node_type, chain_id)?;
        sanitize_failpoints_config(node_config, node_type, chain_id)?;
        sanitize_fullnode_network_configs(node_config, node_type, chain_id)?;
        IndexerGrpcConfig::sanitize(node_config, node_type, chain_id)?;
        InspectionServiceConfig::sanitize(node_config, node_type, chain_id)?;
        LoggerConfig::sanitize(node_config, node_type, chain_id)?;
        MempoolConfig::sanitize(node_config, node_type, chain_id)?;
        NetbenchConfig::sanitize(node_config, node_type, chain_id)?;
        StateSyncConfig::sanitize(node_config, node_type, chain_id)?;
        StorageConfig::sanitize(node_config, node_type, chain_id)?;
        InternalIndexerDBConfig::sanitize(node_config, node_type, chain_id)?;
        sanitize_validator_network_config(node_config, node_type, chain_id)?;

```

**File:** config/src/config/config_sanitizer.rs (L191-197)
```rust
        // Ensure that mutual authentication is enabled
        if !validator_network_config.mutual_authentication {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "Mutual authentication must be enabled for the validator network!".into(),
            ));
        }
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L172-174)
```rust
        // Calculate the number of blocks to remove
        let max_pending_blocks = self.consensus_observer_config.max_num_pending_blocks;
        let num_blocks_to_remove = num_pending_blocks.saturating_sub(max_pending_blocks);
```

**File:** consensus/src/consensus_observer/observer/ordered_blocks.rs (L78-87)
```rust
        let max_num_ordered_blocks = self.consensus_observer_config.max_num_pending_blocks as usize;
        if self.ordered_blocks.len() >= max_num_ordered_blocks {
            warn!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Exceeded the maximum number of ordered blocks: {:?}. Dropping block: {:?}.",
                    max_num_ordered_blocks,
                    observed_ordered_block.ordered_block().proof_block_info()
                ))
            );
            return; // Drop the block if we've exceeded the maximum
```

**File:** consensus/src/consensus_observer/observer/payload_store.rs (L85-94)
```rust
        let max_num_pending_blocks = self.consensus_observer_config.max_num_pending_blocks as usize;
        if self.block_payloads.lock().len() >= max_num_pending_blocks {
            warn!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Exceeded the maximum number of payloads: {:?}. Dropping block: {:?}!",
                    max_num_pending_blocks,
                    block_payload.block(),
                ))
            );
            return; // Drop the block if we've exceeded the maximum
```

**File:** consensus/src/consensus_observer/observer/subscription_manager.rs (L124-127)
```rust
        let max_concurrent_subscriptions =
            self.consensus_observer_config.max_concurrent_subscriptions as usize;
        let num_subscriptions_to_create =
            max_concurrent_subscriptions.saturating_sub(remaining_subscription_peers.len());
```
