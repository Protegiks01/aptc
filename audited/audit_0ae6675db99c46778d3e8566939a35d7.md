# Audit Report

## Title
Missing Signal Handler in NFT Metadata Crawler Causes Database State Corruption on Abrupt Termination

## Summary
The NFT metadata crawler lacks proper SIGTERM/SIGINT signal handling in its server framework, allowing abrupt process termination to leave the PostgreSQL database in an inconsistent state with partially completed NFT processing operations.

## Finding Description

The NFT metadata crawler service uses the `ServerArgs` framework which does not implement signal handling. [1](#0-0) 

The framework's `run_server_with_config` function spawns tasks without registering any signal handlers. [2](#0-1) 

During NFT processing, the worker performs multiple sequential database upserts throughout the parsing pipeline - after JSON parsing, image optimization, and animation optimization. [3](#0-2) [4](#0-3) [5](#0-4) 

When SIGTERM or SIGINT arrives during multi-step processing, the service terminates immediately without completing the current operation. This can leave the database with:
- Assets uploaded to GCS but not recorded in the database
- Incomplete retry counts leading to premature abandonment or excessive retries  
- Assets not marked as `do_not_parse` despite exceeding retry limits
- Orphaned CDN references in partial states

The database upsert operations are atomic at the SQL level, but the multi-step workflow is not transactionally protected across multiple upsert calls. [6](#0-5) 

Other Aptos services properly implement signal handling using `tokio::signal::ctrl_c()` or the `ctrlc` crate for graceful shutdown. [7](#0-6) 

## Impact Explanation

This qualifies as **Medium Severity** under the Aptos bug bounty criteria: "State inconsistencies requiring intervention."

When the service is restarted (common in Kubernetes, Docker, systemd deployments), in-flight processing operations are interrupted, leaving:
- Inconsistent database records requiring manual cleanup
- Orphaned GCS objects consuming storage
- Processing loops on problematic assets due to incorrect retry counts
- Operational overhead to identify and correct corrupted records

## Likelihood Explanation

**High likelihood** - This occurs during every service deployment or restart:
- Kubernetes uses SIGTERM for pod termination (default 30s grace period)
- Docker uses SIGTERM for container stops
- Systemd uses SIGTERM for service management
- CI/CD pipelines trigger frequent deployments

Without signal handling, every restart has a probability of leaving active parsing operations in an incomplete state.

## Recommendation

Implement graceful shutdown with signal handling in the `ServerArgs` framework:

```rust
pub async fn run_server_with_config<C>(config: GenericConfig<C>) -> Result<()>
where
    C: RunnableConfig,
{
    let health_port = config.health_check_port;
    let config_clone = config.clone();
    let task_handler = tokio::spawn(async move {
        register_probes_and_metrics_handler(config_clone, health_port).await;
        anyhow::Ok(())
    });
    let main_task_handler =
        tokio::spawn(async move { config.run().await.expect("task should exit with Ok.") });
    
    // Add signal handler
    let shutdown_signal = async {
        tokio::signal::ctrl_c()
            .await
            .expect("Failed to install CTRL+C signal handler");
    };
    
    tokio::select! {
        res = task_handler => {
            if let Err(e) = res {
                error!("Probes and metrics handler panicked: {:?}", e);
                process::exit(1);
            }
        },
        res = main_task_handler => {
            if let Err(e) = res {
                error!("Main task panicked: {:?}", e);
                process::exit(1);
            }
        },
        _ = shutdown_signal => {
            info!("Shutdown signal received, gracefully terminating...");
            // Allow tasks to complete current operations
            tokio::time::sleep(tokio::time::Duration::from_secs(5)).await;
        }
    }
    Ok(())
}
```

Additionally, modify the NFT parser worker to use database transactions for multi-step operations to ensure atomicity across the entire processing pipeline.

## Proof of Concept

```rust
// Reproduction steps:
// 1. Start the NFT metadata crawler service
// 2. Trigger NFT metadata processing for a batch of assets
// 3. While processing is active, send SIGTERM to the process:
//    kill -TERM <pid>
// 4. Check the database state:

// Expected: Partial records in parsed_asset_uris table
// - cdn_json_uri populated but cdn_image_uri NULL
// - retry_count incremented but do_not_parse not set
// - GCS objects exist but not referenced in database

// Query to detect inconsistent state:
// SELECT * FROM nft_metadata_crawler.parsed_asset_uris 
// WHERE cdn_json_uri IS NOT NULL 
//   AND cdn_image_uri IS NULL 
//   AND json_parser_retry_count = 0;
```

**Notes:**

While this is a valid operational reliability issue, it's important to note that the NFT metadata crawler is an off-chain indexing service and does not affect blockchain consensus or on-chain state. The impact is limited to the indexer's PostgreSQL database consistency and requires operational intervention rather than representing a direct security threat to the blockchain itself.

### Citations

**File:** ecosystem/nft-metadata-crawler/src/main.rs (L7-11)
```rust
#[tokio::main]
async fn main() -> anyhow::Result<()> {
    let args = <ServerArgs as clap::Parser>::parse();
    args.run::<NFTMetadataCrawlerConfig>().await
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-server-framework/src/lib.rs (L46-77)
```rust
pub async fn run_server_with_config<C>(config: GenericConfig<C>) -> Result<()>
where
    C: RunnableConfig,
{
    let health_port = config.health_check_port;
    // Start liveness and readiness probes.
    let config_clone = config.clone();
    let task_handler = tokio::spawn(async move {
        register_probes_and_metrics_handler(config_clone, health_port).await;
        anyhow::Ok(())
    });
    let main_task_handler =
        tokio::spawn(async move { config.run().await.expect("task should exit with Ok.") });
    tokio::select! {
        res = task_handler => {
            if let Err(e) = res {
                error!("Probes and metrics handler panicked or was shutdown: {:?}", e);
                process::exit(1);
            } else {
                panic!("Probes and metrics handler exited unexpectedly");
            }
        },
        res = main_task_handler => {
            if let Err(e) = res {
                error!("Main task panicked or was shutdown: {:?}", e);
                process::exit(1);
            } else {
                panic!("Main task exited unexpectedly");
            }
        },
    }
}
```

**File:** ecosystem/nft-metadata-crawler/src/parser/worker.rs (L78-165)
```rust
    pub async fn parse(&mut self) -> anyhow::Result<()> {
        // Deduplicate asset_uri
        // Exit if not force or if asset_uri has already been parsed
        let prev_model = ParsedAssetUrisQuery::get_by_asset_uri(&mut self.conn, &self.asset_uri);
        if let Some(pm) = prev_model {
            DUPLICATE_ASSET_URI_COUNT.inc();
            self.model = pm.into();
            if !self.force && self.model.get_do_not_parse() {
                self.log_info("asset_uri has been marked as do_not_parse, skipping parse");
                SKIP_URI_COUNT.with_label_values(&["do_not_parse"]).inc();
                self.upsert();
                return Ok(());
            }
        }

        // Check asset_uri against the URI blacklist
        if self.is_blacklisted_uri(&self.asset_uri.clone()) {
            self.log_info("Found match in URI blacklist, marking as do_not_parse");
            self.model.set_do_not_parse(true);
            self.upsert();
            SKIP_URI_COUNT.with_label_values(&["blacklist"]).inc();
            return Ok(());
        }

        // Skip if asset_uri is not a valid URI, do not write invalid URI to Postgres
        if Url::parse(&self.asset_uri).is_err() {
            self.log_info("URI is invalid, skipping parse, marking as do_not_parse");
            self.model.set_do_not_parse(true);
            SKIP_URI_COUNT.with_label_values(&["invalid"]).inc();
            return Ok(());
        }

        if self.force || self.model.get_cdn_json_uri().is_none() {
            // Parse asset_uri
            self.log_info("Parsing asset_uri");
            let json_uri = URIParser::parse(
                &self.parser_config.ipfs_prefix,
                &self.model.get_asset_uri(),
                self.parser_config.ipfs_auth_key.as_deref(),
            )
            .unwrap_or_else(|_| {
                self.log_warn("Failed to parse asset_uri", None);
                PARSE_URI_TYPE_COUNT.with_label_values(&["other"]).inc();
                self.model.get_asset_uri()
            });

            // Parse JSON for raw_image_uri and raw_animation_uri
            self.log_info("Starting JSON parsing");
            let (raw_image_uri, raw_animation_uri, json) =
                JSONParser::parse(json_uri, self.parser_config.max_file_size_bytes)
                    .await
                    .unwrap_or_else(|e| {
                        // Increment retry count if JSON parsing fails
                        self.log_warn("JSON parsing failed", Some(&e));
                        self.model.increment_json_parser_retry_count();
                        (None, None, Value::Null)
                    });

            self.model.set_raw_image_uri(raw_image_uri);
            self.model.set_raw_animation_uri(raw_animation_uri);

            // Save parsed JSON to GCS
            if json != Value::Null {
                self.log_info("Writing JSON to GCS");
                let cdn_json_uri_result = write_json_to_gcs(
                    &self.parser_config.bucket,
                    &self.asset_uri,
                    &json,
                    &self.gcs_client,
                )
                .await;

                if let Err(e) = cdn_json_uri_result.as_ref() {
                    self.log_warn(
                        "Failed to write JSON to GCS, maybe upload timed out?",
                        Some(e),
                    );
                }

                let cdn_json_uri = cdn_json_uri_result
                    .map(|value| format!("{}{}", self.parser_config.cdn_prefix, value))
                    .ok();
                self.model.set_cdn_json_uri(cdn_json_uri);
            }

            // Commit model to Postgres
            self.log_info("Committing JSON to Postgres");
            self.upsert();
```

**File:** ecosystem/nft-metadata-crawler/src/parser/worker.rs (L270-273)
```rust
            // Commit model to Postgres
            self.log_info("Committing image to Postgres");
            self.upsert();
        }
```

**File:** ecosystem/nft-metadata-crawler/src/parser/worker.rs (L359-362)
```rust
            // Commit model to Postgres
            self.log_info("Committing animation to Postgres");
            self.upsert();
        }
```

**File:** ecosystem/nft-metadata-crawler/src/utils/database.rs (L36-64)
```rust
pub fn upsert_uris(
    conn: &mut PooledConnection<ConnectionManager<PgConnection>>,
    entry: &ParsedAssetUris,
    ltv: i64,
) -> anyhow::Result<usize> {
    use schema::nft_metadata_crawler::parsed_asset_uris::dsl::*;

    let query = diesel::insert_into(schema::nft_metadata_crawler::parsed_asset_uris::table)
        .values(entry)
        .on_conflict(asset_uri)
        .do_update()
        .set((
            raw_image_uri.eq(excluded(raw_image_uri)),
            raw_animation_uri.eq(excluded(raw_animation_uri)),
            cdn_json_uri.eq(excluded(cdn_json_uri)),
            cdn_image_uri.eq(excluded(cdn_image_uri)),
            cdn_animation_uri.eq(excluded(cdn_animation_uri)),
            image_optimizer_retry_count.eq(excluded(image_optimizer_retry_count)),
            json_parser_retry_count.eq(excluded(json_parser_retry_count)),
            animation_optimizer_retry_count.eq(excluded(animation_optimizer_retry_count)),
            inserted_at.eq(excluded(inserted_at)),
            do_not_parse.eq(excluded(do_not_parse)),
            last_transaction_version.eq(ltv),
        ));

    let debug_query = diesel::debug_query::<diesel::pg::Pg, _>(&query).to_string();
    debug!("Executing Query: {}", debug_query);
    query.execute(conn).context(debug_query)
}
```

**File:** crates/aptos/src/node/local_testnet/mod.rs (L438-443)
```rust
        let abort_handle = join_set.spawn(async move {
            tokio::signal::ctrl_c()
                .await
                .expect("Failed to register ctrl-c hook");
            Ok(())
        });
```
