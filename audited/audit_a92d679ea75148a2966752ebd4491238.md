# Audit Report

## Title
Race Condition in Batch Expiration Allows Quota Bypass and Resource Exhaustion

## Summary
The `update_certified_timestamp()` function in `batch_store.rs` contains a race condition between timestamp updates and expiration cleanup that allows expired batches to remain in the cache indefinitely. While `last_certified_time` is correctly updated atomically using `fetch_max`, the subsequent `clear_expired_payload()` call uses the local `certified_time` parameter instead of reading the updated atomic value, creating a window where batches can bypass expiration checks and accumulate in memory.

## Finding Description

The vulnerability exists in the non-atomic composition of timestamp update and expiration cleanup operations: [1](#0-0) 

The atomic `fetch_max` correctly prevents the timestamp from regressing, but `clear_expired_payload()` is called with the local `certified_time` parameter. This creates a race with the `save()` operation: [2](#0-1) 

The `insert_to_cache()` operation is non-atomic across the cache and expirations heap: [3](#0-2) 

**Attack Scenario with Realistic Timestamps:**

1. Initial state: `last_certified_time = 1,000,000,000` microseconds (1000 seconds)
2. Attacker sends batch with `expiration = 1,080,000,000` microseconds (1080 seconds)
3. Thread A executes `save()`, reads `last_certified_time = 1,000,000,000`
4. Thread A validates: `1,080,000,000 > 1,000,000,000` ✓ (passes)
5. Thread A inserts batch into `db_cache` (line 405-408)
6. Thread B calls `update_certified_timestamp(1,150,000,000)`
7. Thread B: `fetch_max(1,150,000,000)` → `last_certified_time = 1,150,000,000`
8. Thread B: `clear_expired_payload(1,150,000,000)` calculates:
   - `expiration_time = 1,150,000,000 - 60,000,000 = 1,090,000,000`
   - `expire(1,090,000,000)` should expire batch (1,080,000,000 < 1,090,000,000)
   - But batch is NOT in `expirations` heap yet, so it's not expired
9. Thread A adds batch to `expirations` heap (line 414)
10. Result: Batch with expiration 1,080,000,000 remains in cache even though `last_certified_time = 1,150,000,000`

The batch persists until the next `update_certified_timestamp()` call, which could be arbitrarily delayed.

**Concurrent Access is Guaranteed:**

The `BatchStore` is wrapped in `Arc` and shared across multiple components: [4](#0-3) 

The `notify_commit()` method is called from concurrent execution pipelines: [5](#0-4) [6](#0-5) 

Multiple blocks can be processed concurrently in the pipeline, resulting in concurrent calls to `update_certified_timestamp()`.

## Impact Explanation

This vulnerability qualifies as **High Severity** per Aptos bug bounty criteria:

1. **Resource Exhaustion (Validator Node Slowdowns)**: Expired batches accumulate in memory, consuming the `memory_quota` and preventing legitimate batches from being stored. The quota system is designed to limit resource usage per peer: [7](#0-6) 

2. **Quota Bypass**: Attackers can repeatedly send batches that expire within the race window, exhausting the quota without proper cleanup. The quota is only freed when batches are expired: [8](#0-7) 

3. **Protocol Violation**: The system invariant that "batches with `expiration < last_certified_time + buffer` are removed from cache" is broken. This violates the documented expiration logic and the "Resource Limits" invariant.

4. **Potential Validator Inconsistency**: Different validators may have different sets of cached batches depending on race timing, as the `exists()` method doesn't validate expiration: [9](#0-8) 

## Likelihood Explanation

This vulnerability has **HIGH likelihood** of exploitation:

1. **Natural Occurrence**: The race condition occurs naturally in normal operation when blocks are committed concurrently, requiring no special attacker setup
2. **Low Complexity**: Attackers only need to send batches with expiration times near the current certified timestamp
3. **Repeatable**: The attack can be repeated continuously to amplify resource exhaustion
4. **No Privileges Required**: Any network peer can send batches to trigger this race

## Recommendation

The fix requires making `clear_expired_payload()` use the UPDATED `last_certified_time` value instead of the local parameter:

```rust
pub fn update_certified_timestamp(&self, certified_time: u64) {
    trace!("QS: batch reader updating time {:?}", certified_time);
    self.last_certified_time
        .fetch_max(certified_time, Ordering::SeqCst);
    
    // FIXED: Read the updated atomic value instead of using the parameter
    let current_certified_time = self.last_certified_time.load(Ordering::SeqCst);
    let expired_keys = self.clear_expired_payload(current_certified_time);
    
    if let Err(e) = self.db.delete_batches(expired_keys) {
        debug!("Error deleting batches: {:?}", e)
    }
}
```

Alternatively, restructure to ensure atomicity:

```rust
pub fn update_certified_timestamp(&self, certified_time: u64) {
    trace!("QS: batch reader updating time {:?}", certified_time);
    
    // Acquire expiration lock first to prevent races
    let mut expirations = self.expirations.lock();
    let old_time = self.last_certified_time.fetch_max(certified_time, Ordering::SeqCst);
    let new_time = old_time.max(certified_time);
    
    // Perform expiration with lock held
    let expiration_time = new_time.saturating_sub(self.expiration_buffer_usecs);
    let expired_digests = expirations.expire(expiration_time);
    drop(expirations);
    
    // Clean up cache entries
    let mut expired_keys = Vec::new();
    for h in expired_digests {
        // ... existing cleanup logic ...
    }
    
    if let Err(e) = self.db.delete_batches(expired_keys) {
        debug!("Error deleting batches: {:?}", e)
    }
}
```

## Proof of Concept

```rust
#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_race_condition_expired_batch_bypass() {
    use std::sync::Arc;
    use tokio::task;
    
    // Setup BatchStore with initial certified time
    let batch_store = Arc::new(create_batch_store(1_000_000_000));
    
    // Create batch with expiration that will be within expiration window
    let batch_expiration = 1_080_000_000; // 80 seconds
    let batch = create_test_batch(batch_expiration);
    
    // Spawn concurrent tasks
    let store_clone = batch_store.clone();
    let save_handle = task::spawn(async move {
        // This will check against old certified time and proceed
        tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
        store_clone.save(&batch).unwrap();
    });
    
    let update_handle = task::spawn(async move {
        // This advances certified time and tries to expire batches
        batch_store.update_certified_timestamp(1_150_000_000);
    });
    
    // Wait for both operations
    let _ = tokio::join!(save_handle, update_handle);
    
    // Verify vulnerability: batch with expiration 1_080_000_000 should be expired
    // (1_080_000_000 < 1_150_000_000 - 60_000_000 = 1_090_000_000)
    // But due to race, it remains in cache
    let result = batch_store.get_batch_from_local(&batch.digest());
    
    // Expected: Err (batch should be expired)
    // Actual: Ok (batch is still in cache due to race)
    assert!(result.is_ok(), "Race condition allowed expired batch to remain in cache");
    
    // Verify the batch is expired according to current certified time
    let current_time = batch_store.last_certified_time.load(Ordering::Relaxed);
    assert!(batch_expiration < current_time - 60_000_000, 
            "Batch should be expired but is still accessible");
}
```

## Notes

The vulnerability stems from the separation between atomic timestamp updates and non-atomic cache cleanup. While `fetch_max` with `SeqCst` ordering correctly ensures `last_certified_time` never regresses, the expiration cleanup operates on stale data. The 60-second expiration buffer amplifies the impact by creating a wide window for the race condition to manifest. This breaks the system's resource management guarantees and could be exploited to degrade validator performance through memory exhaustion.

### Citations

**File:** consensus/src/quorum_store/batch_store.rs (L41-84)
```rust
pub(crate) struct QuotaManager {
    memory_balance: usize,
    db_balance: usize,
    batch_balance: usize,
    // Recording the provided quotas for asserts.
    memory_quota: usize,
    db_quota: usize,
    batch_quota: usize,
}

impl QuotaManager {
    pub(crate) fn new(db_quota: usize, memory_quota: usize, batch_quota: usize) -> Self {
        assert!(db_quota >= memory_quota);
        Self {
            memory_balance: memory_quota,
            db_balance: db_quota,
            batch_balance: batch_quota,
            memory_quota,
            db_quota,
            batch_quota,
        }
    }

    pub(crate) fn update_quota(&mut self, num_bytes: usize) -> anyhow::Result<StorageMode> {
        if self.batch_balance == 0 {
            counters::EXCEEDED_BATCH_QUOTA_COUNT.inc();
            bail!("Batch quota exceeded ");
        }

        if self.db_balance >= num_bytes {
            self.batch_balance -= 1;
            self.db_balance -= num_bytes;

            if self.memory_balance >= num_bytes {
                self.memory_balance -= num_bytes;
                Ok(StorageMode::MemoryAndPersisted)
            } else {
                Ok(StorageMode::PersistedOnly)
            }
        } else {
            counters::EXCEEDED_STORAGE_QUOTA_COUNT.inc();
            bail!("Storage quota exceeded ");
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L342-348)
```rust
    fn free_quota(&self, value: PersistedValue<BatchInfoExt>) {
        let mut quota_manager = self
            .peer_quota
            .get_mut(&value.author())
            .expect("No QuotaManager for batch author");
        quota_manager.free_quota(value.num_bytes() as usize, value.payload_storage_mode());
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L358-417)
```rust
    pub(crate) fn insert_to_cache(
        &self,
        value: &PersistedValue<BatchInfoExt>,
    ) -> anyhow::Result<bool> {
        let digest = *value.digest();
        let author = value.author();
        let expiration_time = value.expiration();

        {
            // Acquire dashmap internal lock on the entry corresponding to the digest.
            let cache_entry = self.db_cache.entry(digest);

            if let Occupied(entry) = &cache_entry {
                match entry.get().expiration().cmp(&expiration_time) {
                    std::cmp::Ordering::Equal => return Ok(false),
                    std::cmp::Ordering::Greater => {
                        debug!(
                            "QS: already have the digest with higher expiration {}",
                            digest
                        );
                        return Ok(false);
                    },
                    std::cmp::Ordering::Less => {},
                }
            };
            let value_to_be_stored = if self
                .peer_quota
                .entry(author)
                .or_insert(QuotaManager::new(
                    self.db_quota,
                    self.memory_quota,
                    self.batch_quota,
                ))
                .update_quota(value.num_bytes() as usize)?
                == StorageMode::PersistedOnly
            {
                PersistedValue::new(value.batch_info().clone(), None)
            } else {
                value.clone()
            };

            match cache_entry {
                Occupied(entry) => {
                    let (k, prev_value) = entry.replace_entry(value_to_be_stored);
                    debug_assert!(k == digest);
                    self.free_quota(prev_value);
                },
                Vacant(slot) => {
                    slot.insert(value_to_be_stored);
                },
            }
        }

        // Add expiration for the inserted entry, no need to be atomic w. insertion.
        #[allow(clippy::unwrap_used)]
        {
            self.expirations.lock().add_item(digest, expiration_time);
        }
        Ok(true)
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L419-439)
```rust
    pub(crate) fn save(&self, value: &PersistedValue<BatchInfoExt>) -> anyhow::Result<bool> {
        let last_certified_time = self.last_certified_time();
        if value.expiration() > last_certified_time {
            fail_point!("quorum_store::save", |_| {
                // Skip caching and storing value to the db
                Ok(false)
            });
            counters::GAP_BETWEEN_BATCH_EXPIRATION_AND_CURRENT_TIME_WHEN_SAVE.observe(
                Duration::from_micros(value.expiration() - last_certified_time).as_secs_f64(),
            );

            return self.insert_to_cache(value);
        }
        counters::NUM_BATCH_EXPIRED_WHEN_SAVE.inc();
        bail!(
            "Incorrect expiration {} in epoch {}, last committed timestamp {}",
            value.expiration(),
            self.epoch(),
            last_certified_time,
        );
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L530-539)
```rust
    pub fn update_certified_timestamp(&self, certified_time: u64) {
        trace!("QS: batch reader updating time {:?}", certified_time);
        self.last_certified_time
            .fetch_max(certified_time, Ordering::SeqCst);

        let expired_keys = self.clear_expired_payload(certified_time);
        if let Err(e) = self.db.delete_batches(expired_keys) {
            debug!("Error deleting batches: {:?}", e)
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L649-655)
```rust
    batch_store: Arc<BatchStore>,
    batch_requester: Arc<BatchRequester<T>>,
    inflight_fetch_requests: Arc<Mutex<HashMap<HashValue, BatchFetchUnit>>>,
}

impl<T: QuorumStoreSender + Clone + Send + Sync + 'static> BatchReaderImpl<T> {
    pub(crate) fn new(batch_store: Arc<BatchStore>, batch_requester: BatchRequester<T>) -> Self {
```

**File:** consensus/src/quorum_store/batch_store.rs (L727-732)
```rust
    fn exists(&self, digest: &HashValue) -> Option<PeerId> {
        self.batch_store
            .get_batch_from_local(digest)
            .map(|v| v.author())
            .ok()
    }
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L166-171)
```rust
#[async_trait]
impl TPayloadManager for QuorumStorePayloadManager {
    fn notify_commit(&self, block_timestamp: u64, payloads: Vec<Payload>) {
        self.batch_reader
            .update_certified_timestamp(block_timestamp);

```

**File:** consensus/src/pipeline/pipeline_builder.rs (L1132-1135)
```rust
        let payload = block.payload().cloned();
        let timestamp = block.timestamp_usecs();
        let payload_vec = payload.into_iter().collect();
        payload_manager.notify_commit(timestamp, payload_vec);
```
