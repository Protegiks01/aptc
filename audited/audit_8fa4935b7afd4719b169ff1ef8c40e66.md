# Audit Report

## Title
Consensus Liveness Failure Due to LeaderReputation Proposer Election Desynchronization During Round Advancement

## Summary
When using LeaderReputation-based proposer election with `use_root_hash` enabled, validators can elect different proposers for the same round due to asynchronous execution not being synchronized with round advancement. This occurs because block execution happens asynchronously via a channel, but the round manager immediately advances to the next round before database commits complete, causing validators with different execution speeds to query different database states and compute different root hashes for proposer selection.

## Finding Description

The vulnerability exists in the interaction between three components:

1. **Asynchronous Execution**: [1](#0-0) 
   The `finalize_order()` method sends blocks to an execution channel and returns immediately without waiting for database commit.

2. **Immediate Round Advancement**: [2](#0-1) 
   After calling `add_certs()` which triggers `finalize_order()`, the `sync_up()` method immediately calls `process_certificates()` to advance rounds.

3. **Root Hash Dependency**: [3](#0-2) 
   The `get_valid_proposer()` method queries the database for block metadata and uses the `root_hash` as a seed for deterministic proposer selection when `use_root_hash` is true.

**Attack Scenario:**

When a validator syncs up (e.g., receives a QC for round 100 while at round 50):

1. `add_certs()` sends blocks for execution via channel
2. `finalize_order()` returns immediately (line 623 of execution_client.rs)
3. Round advances to 101 via `process_certificates()` 
4. `get_valid_proposer(101)` is called

At this point, if the validator's execution pipeline hasn't completed:

- [4](#0-3) 
  The `get_block_metadata()` query finds no recent blocks in the database
  
- [5](#0-4) 
  Returns empty result with `root_hash = HashValue::zero()` (line 151)

- [6](#0-5) 
  Different `root_hash` values in the state array cause `choose_index()` to select different proposers

The warning message at lines 119-122 explicitly acknowledges: "Elected proposers are unlikely to match!!" but only logs it as a warning instead of preventing the issue.

**Configuration**: [7](#0-6) 
The `use_root_hash` is enabled for all LeaderReputation versions except V1 (ProposerAndVoter), meaning this affects the default/newer configurations.

## Impact Explanation

This is a **High Severity** vulnerability per Aptos bug bounty criteria:

**Liveness Impact**: When validators disagree on the valid proposer:
- Validator subset A expects Alice to propose for round R
- Validator subset B expects Bob to propose for round R  
- Proposals from Alice are rejected by subset B (wrong proposer check at [8](#0-7) )
- Proposals from Bob are rejected by subset A
- Neither proposal can reach quorum, causing the round to timeout
- This can cascade across multiple rounds during network synchronization periods

**Deterministic Failure**: The issue manifests predictably when:
- Validators sync from behind (common during catch-up or after restarts)
- Validators have different execution pipeline speeds (varying load, hardware)
- Large round jumps occur (network partitions, extended downtimes)

This breaks the **Consensus Liveness** invariant - the network must be able to make progress and commit blocks.

## Likelihood Explanation

**Highly Likely** to occur in production:

1. **Common Trigger Conditions**: Validators regularly sync when joining, restarting, or recovering from transient network issues

2. **Natural Variability**: Different validators inherently have varying:
   - Hardware performance (CPU, disk I/O)
   - Network latency to peers
   - Current system load
   - Execution pipeline queue depth

3. **Existing Evidence**: The explicit warning message in the code indicates developers are aware of this issue but treated it as non-critical

4. **Attack Surface**: No malicious behavior required - this is a protocol race condition that happens naturally under normal network operations

5. **Amplification**: Once one round fails due to proposer disagreement, validators remain out of sync for subsequent rounds until execution catches up

## Recommendation

**Short-term Fix**: Ensure execution completes before advancing rounds by making `finalize_order()` wait for commit acknowledgment.

**Implementation Approach**:

1. Add a callback channel to `finalize_order()` that signals when blocks are committed to database
2. In `send_for_execution()`, wait for this callback before returning
3. Add timeout handling to prevent indefinite blocking

**Alternative**: If synchronous execution is too slow, add a check in `get_valid_proposer()` to ensure database state is fresh enough:

```rust
// In LeaderReputation::get_valid_proposer_and_voting_power_participation_ratio
let (sliding_window, root_hash) = self.backend.get_block_metadata(self.epoch, target_round);

// Verify we have sufficient recent history
if sliding_window.is_empty() || 
   (sliding_window.first().map_or(0, |e| e.round()) < target_round && target_round > 0) {
    // Return error instead of continuing with potentially wrong proposer
    panic!("Cannot elect proposer: local database not synced to target_round {}", target_round);
}
```

**Long-term Fix**: Redesign proposer election to not depend on database state that may lag consensus state, or ensure atomic updates of both consensus round and execution state.

## Proof of Concept

The vulnerability can be demonstrated with this Rust integration test scenario:

```rust
// Pseudocode for reproduction test
#[tokio::test]
async fn test_proposer_election_desync() {
    // Setup two validators with LeaderReputation (ProposerAndVoterV2)
    let (validator_a, validator_b) = setup_validators_with_leader_reputation();
    
    // Validator A: normal execution speed
    // Validator B: inject 500ms delay in execution pipeline
    validator_b.inject_execution_delay(Duration::from_millis(500));
    
    // Both validators at round 50
    // Send sync_info advancing them to round 100 (big jump)
    let sync_info = create_sync_info_for_round(100);
    
    // Both validators process sync_info
    tokio::join!(
        validator_a.process_sync_info(sync_info.clone()),
        validator_b.process_sync_info(sync_info.clone())
    );
    
    // Query proposer for round 101 from both validators
    let proposer_a = validator_a.get_valid_proposer(101);
    let proposer_b = validator_b.get_valid_proposer(101);
    
    // Assertion: proposers should match but don't due to race condition
    // Validator A has executed blocks and has proper root_hash
    // Validator B hasn't finished execution, has HashValue::zero()
    assert_ne!(proposer_a, proposer_b, "Proposer election desynchronized!");
    
    // Demonstrate liveness failure: neither proposal reaches quorum
    let proposal_from_a = create_proposal(proposer_a, 101);
    let proposal_from_b = create_proposal(proposer_b, 101);
    
    // Validator A accepts from proposer_a, rejects from proposer_b
    assert!(validator_a.validate_proposal(&proposal_from_a).is_ok());
    assert!(validator_a.validate_proposal(&proposal_from_b).is_err());
    
    // Validator B accepts from proposer_b, rejects from proposer_a  
    assert!(validator_b.validate_proposal(&proposal_from_b).is_ok());
    assert!(validator_b.validate_proposal(&proposal_from_a).is_err());
    
    // Neither proposal can get quorum -> round timeout
}
```

**Notes**

The caching layer ( [9](#0-8) ) does not mitigate this issue because the cache is computed lazily on first access, which still queries the potentially-stale database state.

This vulnerability only affects deployments using LeaderReputation with `use_root_hash = true` (all versions except V1). Networks using simple RotatingProposer are not affected since it's stateless ( [10](#0-9) ).

### Citations

**File:** consensus/src/pipeline/execution_client.rs (L590-624)
```rust
    async fn finalize_order(
        &self,
        blocks: Vec<Arc<PipelinedBlock>>,
        ordered_proof: WrappedLedgerInfo,
    ) -> ExecutorResult<()> {
        assert!(!blocks.is_empty());
        let mut execute_tx = match self.handle.read().execute_tx.clone() {
            Some(tx) => tx,
            None => {
                debug!("Failed to send to buffer manager, maybe epoch ends");
                return Ok(());
            },
        };

        for block in &blocks {
            block.set_insertion_time();
            if let Some(tx) = block.pipeline_tx().lock().as_mut() {
                tx.order_proof_tx
                    .take()
                    .map(|tx| tx.send(ordered_proof.clone()));
            }
        }

        if execute_tx
            .send(OrderedBlocks {
                ordered_blocks: blocks,
                ordered_proof: ordered_proof.ledger_info().clone(),
            })
            .await
            .is_err()
        {
            debug!("Failed to send to buffer manager, maybe epoch ends");
        }
        Ok(())
    }
```

**File:** consensus/src/round_manager.rs (L877-907)
```rust
    /// Sync to the sync info sending from peer if it has newer certificates.
    async fn sync_up(&mut self, sync_info: &SyncInfo, author: Author) -> anyhow::Result<()> {
        let local_sync_info = self.block_store.sync_info();
        if sync_info.has_newer_certificates(&local_sync_info) {
            info!(
                self.new_log(LogEvent::ReceiveNewCertificate)
                    .remote_peer(author),
                "Local state {},\n remote state {}", local_sync_info, sync_info
            );
            // Some information in SyncInfo is ahead of what we have locally.
            // First verify the SyncInfo (didn't verify it in the yet).
            sync_info.verify(&self.epoch_state.verifier).map_err(|e| {
                error!(
                    SecurityEvent::InvalidSyncInfoMsg,
                    sync_info = sync_info,
                    remote_peer = author,
                    error = ?e,
                );
                VerifyError::from(e)
            })?;
            SYNC_INFO_RECEIVED_WITH_NEWER_CERT.inc();
            let result = self
                .block_store
                .add_certs(sync_info, self.create_block_retriever(author))
                .await;
            self.process_certificates().await?;
            result
        } else {
            Ok(())
        }
    }
```

**File:** consensus/src/round_manager.rs (L1195-1200)
```rust
        ensure!(
            self.proposer_election.is_valid_proposal(&proposal),
            "[RoundManager] Proposer {} for block {} is not a valid proposer for this round or created duplicate proposal",
            author,
            proposal,
        );
```

**File:** consensus/src/liveness/leader_reputation.rs (L109-164)
```rust
    ) -> (Vec<NewBlockEvent>, HashValue) {
        // Do not warn when round==0, because check will always be unsure of whether we have
        // all events from the previous epoch. If there is an actual issue, next round will log it.
        if target_round != 0 {
            let has_larger = events.first().is_some_and(|e| {
                (e.event.epoch(), e.event.round()) >= (target_epoch, target_round)
            });
            if !has_larger {
                // error, and not a fatal, in an unlikely scenario that we have many failed consecutive rounds,
                // and nobody has any newer successful blocks.
                warn!(
                    "Local history is too old, asking for {} epoch and {} round, and latest from db is {} epoch and {} round! Elected proposers are unlikely to match!!",
                    target_epoch, target_round, events.first().map_or(0, |e| e.event.epoch()), events.first().map_or(0, |e| e.event.round()))
            }
        }

        let mut max_version = 0;
        let mut result = vec![];
        for event in events {
            if (event.event.epoch(), event.event.round()) <= (target_epoch, target_round)
                && result.len() < self.window_size
            {
                max_version = std::cmp::max(max_version, event.version);
                result.push(event.event.clone());
            }
        }

        if result.len() < self.window_size && !hit_end {
            error!(
                "We are not fetching far enough in history, we filtered from {} to {}, but asked for {}. Target ({}, {}), received from {:?} to {:?}.",
                events.len(),
                result.len(),
                self.window_size,
                target_epoch,
                target_round,
                events.last().map_or((0, 0), |e| (e.event.epoch(), e.event.round())),
                events.first().map_or((0, 0), |e| (e.event.epoch(), e.event.round())),
            );
        }

        if result.is_empty() {
            warn!("No events in the requested window could be found");
            (result, HashValue::zero())
        } else {
            let root_hash = self
                .aptos_db
                .get_accumulator_root_hash(max_version)
                .unwrap_or_else(|_| {
                    error!(
                        "We couldn't fetch accumulator hash for the {} version, for {} epoch, {} round",
                        max_version, target_epoch, target_round,
                    );
                    HashValue::zero()
                });
            (result, root_hash)
        }
```

**File:** consensus/src/liveness/leader_reputation.rs (L168-214)
```rust
impl MetadataBackend for AptosDBBackend {
    // assume the target_round only increases
    fn get_block_metadata(
        &self,
        target_epoch: u64,
        target_round: Round,
    ) -> (Vec<NewBlockEvent>, HashValue) {
        let mut locked = self.db_result.lock();
        let latest_db_version = self.aptos_db.get_latest_ledger_info_version().unwrap_or(0);
        // lazy init db_result
        if locked.is_none() {
            if let Err(e) = self.refresh_db_result(&mut locked, latest_db_version) {
                warn!(
                    error = ?e, "[leader reputation] Fail to initialize db result",
                );
                return (vec![], HashValue::zero());
            }
        }
        let (events, version, hit_end) = {
            // locked is somenthing
            #[allow(clippy::unwrap_used)]
            let result = locked.as_ref().unwrap();
            (&result.0, result.1, result.2)
        };

        let has_larger = events
            .first()
            .is_some_and(|e| (e.event.epoch(), e.event.round()) >= (target_epoch, target_round));
        // check if fresher data has potential to give us different result
        if !has_larger && version < latest_db_version {
            let fresh_db_result = self.refresh_db_result(&mut locked, latest_db_version);
            match fresh_db_result {
                Ok((events, _version, hit_end)) => {
                    self.get_from_db_result(target_epoch, target_round, &events, hit_end)
                },
                Err(e) => {
                    // fails if requested events were pruned / or we never backfil them.
                    warn!(
                        error = ?e, "[leader reputation] Fail to refresh window",
                    );
                    (vec![], HashValue::zero())
                },
            }
        } else {
            self.get_from_db_result(target_epoch, target_round, events, hit_end)
        }
    }
```

**File:** consensus/src/liveness/leader_reputation.rs (L695-744)
```rust
impl ProposerElection for LeaderReputation {
    fn get_valid_proposer_and_voting_power_participation_ratio(
        &self,
        round: Round,
    ) -> (Author, VotingPowerRatio) {
        let target_round = round.saturating_sub(self.exclude_round);
        let (sliding_window, root_hash) = self.backend.get_block_metadata(self.epoch, target_round);
        let voting_power_participation_ratio =
            self.compute_chain_health_and_add_metrics(&sliding_window, round);
        let mut weights =
            self.heuristic
                .get_weights(self.epoch, &self.epoch_to_proposers, &sliding_window);
        let proposers = &self.epoch_to_proposers[&self.epoch];
        assert_eq!(weights.len(), proposers.len());

        // Multiply weights by voting power:
        let stake_weights: Vec<u128> = weights
            .iter_mut()
            .enumerate()
            .map(|(i, w)| *w as u128 * self.voting_powers[i] as u128)
            .collect();

        let state = if self.use_root_hash {
            [
                root_hash.to_vec(),
                self.epoch.to_le_bytes().to_vec(),
                round.to_le_bytes().to_vec(),
            ]
            .concat()
        } else {
            [
                self.epoch.to_le_bytes().to_vec(),
                round.to_le_bytes().to_vec(),
            ]
            .concat()
        };

        let chosen_index = choose_index(stake_weights, state);
        (proposers[chosen_index], voting_power_participation_ratio)
    }

    fn get_valid_proposer(&self, round: Round) -> Author {
        self.get_valid_proposer_and_voting_power_participation_ratio(round)
            .0
    }

    fn get_voting_power_participation_ratio(&self, round: Round) -> VotingPowerRatio {
        self.get_valid_proposer_and_voting_power_participation_ratio(round)
            .1
    }
```

**File:** types/src/on_chain_config/consensus_config.rs (L540-544)
```rust
impl LeaderReputationType {
    pub fn use_root_hash_for_seed(&self) -> bool {
        // all versions after V1 should use root hash
        !matches!(self, Self::ProposerAndVoter(_))
    }
```

**File:** consensus/src/liveness/cached_proposer_election.rs (L1-69)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use super::proposer_election::ProposerElection;
use crate::counters::PROPOSER_ELECTION_DURATION;
use aptos_consensus_types::common::{Author, Round};
use aptos_infallible::Mutex;
use aptos_logger::prelude::info;
use std::collections::BTreeMap;

// Wrapper around ProposerElection.
//
// Function get_valid_proposer can be expensive, and we want to make sure
// it is computed only once for a given round.
pub struct CachedProposerElection {
    epoch: u64,
    proposer_election: Box<dyn ProposerElection + Send + Sync>,
    // We use BTreeMap since we want a fixed window of cached elements
    // to look back (and caller knows how big of a window it needs).
    // LRU cache wouldn't work as well, as access order of the elements
    // would define eviction, and could lead to evicting still needed elements.
    recent_elections: Mutex<BTreeMap<Round, (Author, f64)>>,
    window: usize,
}

impl CachedProposerElection {
    pub fn new(
        epoch: u64,
        proposer_election: Box<dyn ProposerElection + Send + Sync>,
        window: usize,
    ) -> Self {
        Self {
            epoch,
            proposer_election,
            recent_elections: Mutex::new(BTreeMap::new()),
            window,
        }
    }

    pub fn get_or_compute_entry(&self, round: Round) -> (Author, f64) {
        let mut recent_elections = self.recent_elections.lock();

        if round > self.window as u64 {
            *recent_elections = recent_elections.split_off(&(round - self.window as u64));
        }

        *recent_elections.entry(round).or_insert_with(|| {
            let _timer = PROPOSER_ELECTION_DURATION.start_timer();
            let result = self
                .proposer_election
                .get_valid_proposer_and_voting_power_participation_ratio(round);
            info!(
                "ProposerElection for epoch {} and round {}: {:?}",
                self.epoch, round, result
            );
            result
        })
    }
}

impl ProposerElection for CachedProposerElection {
    fn get_valid_proposer(&self, round: Round) -> Author {
        self.get_or_compute_entry(round).0
    }

    fn get_voting_power_participation_ratio(&self, round: Round) -> f64 {
        self.get_or_compute_entry(round).1
    }
}
```

**File:** consensus/src/liveness/rotating_proposer_election.rs (L35-40)
```rust
impl ProposerElection for RotatingProposer {
    fn get_valid_proposer(&self, round: Round) -> Author {
        self.proposers
            [((round / u64::from(self.contiguous_rounds)) % self.proposers.len() as u64) as usize]
    }
}
```
