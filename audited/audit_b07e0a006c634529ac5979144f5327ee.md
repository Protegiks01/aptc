# Audit Report

## Title
Time-of-Check to Time-of-Use Race Condition in `get_start_version()` Causes Internal Indexer Service Crash

## Summary
The `get_start_version()` function performs multiple sequential, non-atomic database reads to verify version consistency across different indexer types. A concurrent batch write by the DBCommitter thread between these reads can cause the function to observe inconsistent versions and panic, crashing the Internal Indexer DB Service and breaking account-based API endpoints.

## Finding Description

The Internal Indexer DB Service provides critical indexing functionality for account-based API queries (events, transactions, and resources). The `get_start_version()` function is called during service initialization to determine the starting version for indexing operations. [1](#0-0) 

This function performs multiple separate database read operations without snapshot isolation:

1. Reads `get_persisted_version()` to establish base `start_version`
2. Conditionally reads `get_state_version()`, `get_transaction_version()`, `get_event_version()`, and `get_event_v2_translation_version()`
3. Compares each version against `start_version` and panics if they don't match

Each version getter is a separate RocksDB read operation: [2](#0-1) 

The underlying `db.get()` method performs individual, non-atomic reads: [3](#0-2) 

Meanwhile, the DBCommitter runs in a separate thread, continuously processing batches and atomically writing all version metadata together: [4](#0-3) 

**Race Condition Scenario:**

1. Service starts, calls `get_start_version()`
2. Reads `get_persisted_version()` → returns version 1000
3. Calculates `start_version = 1001` (1000 + 1)
4. **DBCommitter thread writes batch, atomically updating all versions to 1001**
5. Reads `get_state_version()` → returns version 1001
6. Calculates `state_start_version = 1002` (1001 + 1)
7. Comparison: `start_version (1001) != state_start_version (1002)` 
8. **Panic: "Cannot start state indexer because the progress doesn't match."**

The service crashes and must be restarted. During downtime, all account-based API queries fail. [5](#0-4) 

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program criteria for "API crashes". When the race condition triggers:

1. **Service Availability**: The Internal Indexer DB Service crashes with a panic, requiring manual restart
2. **API Failures**: All account-based REST API endpoints become unavailable:
   - `/accounts/{address}/events/*`
   - `/accounts/{address}/transactions`
   - `/accounts/{address}/modules`
   - `/accounts/{address}/resources`
3. **User Impact**: Applications relying on these APIs experience service disruption

While the vulnerability doesn't directly compromise funds, consensus, or validator operations, it breaks the availability guarantee of critical node APIs, meeting the High Severity threshold.

## Likelihood Explanation

**Likelihood: LOW to MEDIUM**

The race window is small (microseconds between database reads), but several factors increase probability:

1. **Timing**: Most likely during service startup/restart when `get_start_version()` is called
2. **Load Dependency**: Higher transaction throughput increases batch write frequency, expanding the attack surface
3. **Natural Occurrence**: The race can trigger without attacker involvement during normal operation
4. **Attacker Influence**: While attackers cannot precisely time the race, they can increase probability by:
   - Flooding the network with transactions to trigger frequent batch writes
   - Attempting to cause service restarts (if other vulnerabilities exist)

The issue is not easily exploitable on-demand but can occur naturally in production environments, especially under heavy load.

## Recommendation

Implement snapshot-based consistent reads in `get_start_version()` to ensure all version queries observe the same database state:

**Solution**: Create a RocksDB snapshot at the start of `get_start_version()` and use it for all version reads. This ensures atomic, consistent reads across all indexer types.

```rust
pub async fn get_start_version(&self, node_config: &NodeConfig) -> Result<Version> {
    let fast_sync_enabled = node_config
        .state_sync
        .state_sync_driver
        .bootstrapping_mode
        .is_fast_sync();
    let mut main_db_synced_version = self.db_indexer.main_db_reader.ensure_synced_version()?;

    // Wait till fast sync is done
    while fast_sync_enabled && main_db_synced_version == 0 {
        tokio::time::sleep(std::time::Duration::from_secs(1)).await;
        main_db_synced_version = self.db_indexer.main_db_reader.ensure_synced_version()?;
    }

    // Create a snapshot for consistent reads across all version queries
    let snapshot = self.db_indexer.indexer_db.db.snapshot();
    
    let start_version = self
        .db_indexer
        .indexer_db
        .get_persisted_version_with_snapshot(&snapshot)?
        .map_or(0, |v| v + 1);

    if node_config.indexer_db_config.enable_statekeys() {
        let state_start_version = self
            .db_indexer
            .indexer_db
            .get_state_version_with_snapshot(&snapshot)?
            .map_or(0, |v| v + 1);
        if start_version != state_start_version {
            panic!("Cannot start state indexer because the progress doesn't match.");
        }
    }
    
    // Continue with other version checks using the same snapshot...
    
    Ok(start_version)
}
```

Additionally, add corresponding `*_with_snapshot()` methods in `InternalIndexerDB` that accept a snapshot parameter.

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::{Arc, atomic::{AtomicBool, Ordering}};
    use std::thread;
    use std::time::Duration;

    #[test]
    fn test_get_start_version_race_condition() {
        // Setup: Create an InternalIndexerDBService with initial version 1000
        // (initialization code omitted for brevity)
        
        let service = Arc::new(/* initialized service */);
        let race_detected = Arc::new(AtomicBool::new(false));
        
        // Thread 1: Continuously call get_start_version()
        let service_clone = Arc::clone(&service);
        let race_clone = Arc::clone(&race_detected);
        let reader_thread = thread::spawn(move || {
            for _ in 0..100 {
                match service_clone.get_start_version(&node_config).await {
                    Ok(_) => {},
                    Err(_) => {
                        // Panic occurred due to race condition
                        race_clone.store(true, Ordering::SeqCst);
                        break;
                    }
                }
                thread::sleep(Duration::from_micros(100));
            }
        });
        
        // Thread 2: Continuously write batches
        let service_clone = Arc::clone(&service);
        let writer_thread = thread::spawn(move || {
            for version in 1000..1100 {
                // Simulate batch write that updates all version metadata
                let mut batch = SchemaBatch::new();
                batch.put::<InternalIndexerMetadataSchema>(
                    &MetadataKey::LatestVersion,
                    &MetadataValue::Version(version),
                )?;
                batch.put::<InternalIndexerMetadataSchema>(
                    &MetadataKey::StateVersion,
                    &MetadataValue::Version(version),
                )?;
                service_clone.db_indexer.indexer_db.db.write_schemas(batch)?;
                thread::sleep(Duration::from_micros(50));
            }
        });
        
        reader_thread.join().unwrap();
        writer_thread.join().unwrap();
        
        assert!(race_detected.load(Ordering::SeqCst), 
                "Race condition should have been detected");
    }
}
```

## Notes

This is a classic Time-of-Check to Time-of-Use (TOCTOU) race condition where non-atomic reads of related data can observe inconsistent state due to concurrent modifications. While the DBCommitter atomically writes all version metadata in a single batch, the reader observes these updates through separate, uncoordinated read operations.

The fix requires using RocksDB's snapshot feature to ensure all reads within `get_start_version()` observe the same consistent point-in-time view of the database, eliminating the race window entirely.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/internal_indexer_db_service.rs (L88-165)
```rust
    pub async fn get_start_version(&self, node_config: &NodeConfig) -> Result<Version> {
        let fast_sync_enabled = node_config
            .state_sync
            .state_sync_driver
            .bootstrapping_mode
            .is_fast_sync();
        let mut main_db_synced_version = self.db_indexer.main_db_reader.ensure_synced_version()?;

        // Wait till fast sync is done
        while fast_sync_enabled && main_db_synced_version == 0 {
            tokio::time::sleep(std::time::Duration::from_secs(1)).await;
            main_db_synced_version = self.db_indexer.main_db_reader.ensure_synced_version()?;
        }

        let start_version = self
            .db_indexer
            .indexer_db
            .get_persisted_version()?
            .map_or(0, |v| v + 1);

        if node_config.indexer_db_config.enable_statekeys() {
            let state_start_version = self
                .db_indexer
                .indexer_db
                .get_state_version()?
                .map_or(0, |v| v + 1);
            if start_version != state_start_version {
                panic!("Cannot start state indexer because the progress doesn't match.");
            }
        }

        if node_config.indexer_db_config.enable_transaction() {
            let transaction_start_version = self
                .db_indexer
                .indexer_db
                .get_transaction_version()?
                .map_or(0, |v| v + 1);
            if start_version != transaction_start_version {
                panic!("Cannot start transaction indexer because the progress doesn't match.");
            }
        }

        if node_config.indexer_db_config.enable_event() {
            let event_start_version = self
                .db_indexer
                .indexer_db
                .get_event_version()?
                .map_or(0, |v| v + 1);
            if start_version != event_start_version {
                panic!("Cannot start event indexer because the progress doesn't match.");
            }
        }

        if node_config.indexer_db_config.enable_event_v2_translation() {
            let event_v2_translation_start_version = self
                .db_indexer
                .indexer_db
                .get_event_v2_translation_version()?
                .map_or(0, |v| v + 1);
            if node_config
                .indexer_db_config
                .event_v2_translation_ignores_below_version()
                < start_version
                && start_version != event_v2_translation_start_version
            {
                panic!(
                    "Cannot start event v2 translation indexer because the progress doesn't match. \
                    start_version: {}, event_v2_translation_start_version: {}",
                    start_version, event_v2_translation_start_version
                );
            }
            if !node_config.indexer_db_config.enable_event() {
                panic!("Cannot start event v2 translation indexer because event indexer is not enabled.");
            }
        }

        Ok(start_version)
    }
```

**File:** storage/indexer/src/db_indexer.rs (L287-292)
```rust
    fn get_version(&self, key: &MetadataKey) -> Result<Option<Version>> {
        Ok(self
            .db
            .get::<InternalIndexerMetadataSchema>(key)?
            .map(|v| v.expect_version()))
    }
```

**File:** storage/indexer/src/db_indexer.rs (L505-549)
```rust
        if self.indexer_db.event_v2_translation_enabled() {
            batch.put::<InternalIndexerMetadataSchema>(
                &MetadataKey::EventV2TranslationVersion,
                &MetadataValue::Version(version - 1),
            )?;

            for event_key in event_keys {
                batch
                    .put::<EventSequenceNumberSchema>(
                        &event_key,
                        &self
                            .event_v2_translation_engine
                            .get_cached_sequence_number(&event_key)
                            .unwrap_or(0),
                    )
                    .expect("Failed to put events by key to a batch");
            }
        }

        if self.indexer_db.transaction_enabled() {
            batch.put::<InternalIndexerMetadataSchema>(
                &MetadataKey::TransactionVersion,
                &MetadataValue::Version(version - 1),
            )?;
        }
        if self.indexer_db.event_enabled() {
            batch.put::<InternalIndexerMetadataSchema>(
                &MetadataKey::EventVersion,
                &MetadataValue::Version(version - 1),
            )?;
        }
        if self.indexer_db.statekeys_enabled() {
            batch.put::<InternalIndexerMetadataSchema>(
                &MetadataKey::StateVersion,
                &MetadataValue::Version(version - 1),
            )?;
        }
        batch.put::<InternalIndexerMetadataSchema>(
            &MetadataKey::LatestVersion,
            &MetadataValue::Version(version - 1),
        )?;
        self.sender
            .send(Some(batch))
            .map_err(|e| AptosDbError::Other(e.to_string()))?;
        Ok(version)
```

**File:** storage/schemadb/src/lib.rs (L216-232)
```rust
    pub fn get<S: Schema>(&self, schema_key: &S::Key) -> DbResult<Option<S::Value>> {
        let _timer = APTOS_SCHEMADB_GET_LATENCY_SECONDS.timer_with(&[S::COLUMN_FAMILY_NAME]);

        let k = <S::Key as KeyCodec<S>>::encode_key(schema_key)?;
        let cf_handle = self.get_cf_handle(S::COLUMN_FAMILY_NAME)?;

        let result = self.inner.get_cf(cf_handle, k).into_db_res()?;
        APTOS_SCHEMADB_GET_BYTES.observe_with(
            &[S::COLUMN_FAMILY_NAME],
            result.as_ref().map_or(0.0, |v| v.len() as f64),
        );

        result
            .map(|raw_value| <S::Value as ValueCodec<S>>::decode_value(&raw_value))
            .transpose()
            .map_err(Into::into)
    }
```

**File:** storage/README.md (L155-167)
```markdown
Internal indexer is used to provide data for the following node APIs after DB sharding.

Account based event APIs
* /accounts/{address}/events/{event_handle}/{field_name}
* /accounts/{address}/events/{creation_number}

Account based transaction API
* /accounts/{address}/transactions

Account based resource APIs
* /accounts/{address}/modules
* /accounts/{address}/resources

```
