# Audit Report

## Title
Tokio Blocking Thread Pool Exhaustion via Parallel API Requests Leading to Denial of Service

## Summary
The Aptos REST API endpoints that use `api_spawn_blocking()` are vulnerable to resource exhaustion attacks. While the security question mentions "database connection pools," the actual vulnerability involves exhaustion of the tokio blocking thread pool. An attacker can send parallel requests to `get_events_by_creation_number()` and other API endpoints to saturate the fixed pool of 64 blocking threads, causing the entire API to become unresponsive and potentially affecting validator node operations.

## Finding Description

The vulnerability exists in how the API handles blocking database operations. The `get_events_by_creation_number()` function uses `api_spawn_blocking()` to offload database reads to a blocking thread pool. [1](#0-0) 

The `api_spawn_blocking()` function is a thin wrapper around tokio's `spawn_blocking()`: [2](#0-1) 

The tokio runtime used by the API is created with a hard limit of **64 blocking threads**: [3](#0-2) 

**Attack Scenario:**

1. Attacker sends 100+ parallel GET requests to `/v1/accounts/{address}/events/{creation_number}` (or other endpoints using `api_spawn_blocking()`)
2. Each request spawns a blocking task that performs database reads via `context.get_events()`
3. The first 64 requests occupy all available blocking threads
4. Subsequent requests queue, waiting for blocking threads to become available
5. When `spawn_blocking` queues tasks, it can block the calling async task
6. This blocks async runtime workers, causing cascading failures
7. The API becomes unresponsive to all requests, including health checks

**Breaking Invariant:** The Resource Limits invariant is violated: "All operations must respect gas, storage, and computational limits." The API has no rate limiting or per-endpoint concurrency controls despite the developers acknowledging this risk in comments.

**Note on "Database Connection Pools":** AptosDB uses RocksDB (an embedded database), which does not use connection pools like traditional client-server databases. However, the underlying vulnerability of blocking thread pool exhaustion is real and exploitable. [4](#0-3) 

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos bug bounty program:

- **API crashes**: The API becomes completely unresponsive, unable to serve any requests
- **Validator node slowdowns**: If the API shares resources with consensus or other critical components on a validator node, this could impact consensus participation
- **Significant protocol violations**: Loss of API availability violates availability guarantees for the network

Multiple API endpoints are affected beyond just events: [5](#0-4) 

The API runtime configuration shows no rate limiting middleware is applied - only CORS, compression, size limits, and panic handling are present. This leaves the API exposed to resource exhaustion attacks.

## Likelihood Explanation

**Likelihood: HIGH**

- **Attack Complexity**: Very low - requires only basic HTTP client tools
- **Authentication**: None required - endpoints are publicly accessible
- **Resource Requirements**: Minimal - attacker needs bandwidth for ~100 parallel connections
- **Detection Difficulty**: Easy to execute without detection until impact occurs
- **Existing Awareness**: Developers left a comment acknowledging this exact risk but only implemented partial mitigation (64 thread limit)

The API configuration shows no explicit concurrency or rate limiting protections: [6](#0-5) 

## Recommendation

Implement multi-layered protection against resource exhaustion:

1. **Add Per-Endpoint Concurrency Limits** using tokio semaphores:
```rust
pub struct Context {
    // ... existing fields ...
    blocking_semaphore: Arc<Semaphore>,
}

// In Context::new():
blocking_semaphore: Arc::new(Semaphore::new(32)), // Limit concurrent blocking ops

// Modify api_spawn_blocking:
pub async fn api_spawn_blocking<F, T, E>(
    semaphore: Arc<Semaphore>,
    func: F
) -> Result<T, E>
where
    F: FnOnce() -> Result<T, E> + Send + 'static,
    T: Send + 'static,
    E: InternalError + Send + 'static,
{
    let _permit = semaphore.acquire().await
        .map_err(|_| E::internal_with_code_no_info(
            anyhow!("Semaphore closed"), 
            AptosErrorCode::InternalError
        ))?;
    tokio::task::spawn_blocking(func)
        .await
        .map_err(|err| E::internal_with_code_no_info(err, AptosErrorCode::InternalError))?
}
```

2. **Add Rate Limiting Middleware** at the infrastructure level (HAProxy) or application level using the existing `aptos-rate-limiter` crate

3. **Increase MAX_BLOCKING_THREADS** based on expected load, or make it configurable

4. **Add Metrics and Monitoring** for blocking thread pool utilization and queue depth

5. **Implement Request Timeouts** to prevent long-running blocking tasks from holding threads indefinitely

## Proof of Concept

```rust
// Test to reproduce the vulnerability
#[tokio::test]
async fn test_blocking_pool_exhaustion() {
    use reqwest::Client;
    use futures::future::join_all;
    
    // Start API server (assuming running on localhost:8080)
    let client = Client::new();
    let base_url = "http://localhost:8080/v1";
    
    // Send 100 parallel requests to events endpoint
    let mut handles = vec![];
    for i in 0..100 {
        let client = client.clone();
        let url = format!(
            "{}/accounts/0x1/events/{}",
            base_url,
            i % 10  // Cycle through creation numbers
        );
        handles.push(tokio::spawn(async move {
            let start = std::time::Instant::now();
            let result = client.get(&url).send().await;
            let elapsed = start.elapsed();
            (result.is_ok(), elapsed)
        }));
    }
    
    // Wait for all requests
    let results = join_all(handles).await;
    
    // Analyze results - later requests should show significantly increased latency
    // or timeouts, demonstrating thread pool saturation
    for (i, result) in results.iter().enumerate() {
        if let Ok((success, elapsed)) = result {
            println!("Request {}: success={}, latency={:?}", i, success, elapsed);
        }
    }
    
    // Verify API is still responsive after attack
    let health_check = client
        .get(format!("{}/-/healthy", base_url))
        .timeout(std::time::Duration::from_secs(5))
        .send()
        .await;
    
    assert!(health_check.is_ok(), "API should remain responsive");
}
```

**Alternative command-line PoC:**
```bash
# Using Apache Bench to send 100 concurrent requests
ab -n 1000 -c 100 http://localhost:8080/v1/accounts/0x1/events/0

# Monitor thread pool saturation via metrics endpoint
watch -n 1 'curl -s http://localhost:9101/metrics | grep tokio'
```

### Citations

**File:** api/src/events.rs (L78-87)
```rust
        api_spawn_blocking(move || {
            let account = Account::new(api.context.clone(), address.0, None, None, None)?;
            api.list(
                account.latest_ledger_info,
                accept_type,
                page,
                EventKey::new(creation_number.0 .0, address.0.into()),
            )
        })
        .await
```

**File:** api/src/context.rs (L1645-1654)
```rust
pub async fn api_spawn_blocking<F, T, E>(func: F) -> Result<T, E>
where
    F: FnOnce() -> Result<T, E> + Send + 'static,
    T: Send + 'static,
    E: InternalError + Send + 'static,
{
    tokio::task::spawn_blocking(func)
        .await
        .map_err(|err| E::internal_with_code_no_info(err, AptosErrorCode::InternalError))?
}
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-50)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;

    // Verify the given name has an appropriate length
    if thread_name.len() > MAX_THREAD_NAME_LENGTH {
        panic!(
            "The given runtime thread name is too long! Max length: {}, given name: {}",
            MAX_THREAD_NAME_LENGTH, thread_name
        );
    }

    // Create the runtime builder
    let atomic_id = AtomicUsize::new(0);
    let thread_name_clone = thread_name.clone();
    let mut builder = Builder::new_multi_thread();
    builder
        .thread_name_fn(move || {
            let id = atomic_id.fetch_add(1, Ordering::SeqCst);
            format!("{}-{}", thread_name_clone, id)
        })
        .on_thread_start(on_thread_start)
        .disable_lifo_slot()
        // Limit concurrent blocking tasks from spawn_blocking(), in case, for example, too many
        // Rest API calls overwhelm the node.
        .max_blocking_threads(MAX_BLOCKING_THREADS)
```

**File:** storage/aptosdb/src/event_store/mod.rs (L32-40)
```rust
#[derive(Debug)]
pub struct EventStore {
    event_db: Arc<DB>,
}

impl EventStore {
    pub fn new(event_db: Arc<DB>) -> Self {
        Self { event_db }
    }
```

**File:** api/src/runtime.rs (L230-259)
```rust
        let cors = Cors::new()
            // To allow browsers to use cookies (for cookie-based sticky
            // routing in the LB) we must enable this:
            // https://stackoverflow.com/a/24689738/3846032
            .allow_credentials(true)
            .allow_methods(vec![Method::GET, Method::POST]);

        // Build routes for the API
        let route = Route::new()
            .at("/", poem::get(root_handler))
            .nest(
                "/v1",
                Route::new()
                    .nest("/", api_service)
                    .at("/spec.json", poem::get(spec_json))
                    .at("/spec.yaml", poem::get(spec_yaml))
                    // TODO: We add this manually outside of the OpenAPI spec for now.
                    // https://github.com/poem-web/poem/issues/364
                    .at(
                        "/set_failpoint",
                        poem::get(set_failpoints::set_failpoint_poem).data(context.clone()),
                    ),
            )
            .with(cors)
            .with_if(config.api.compression_enabled, Compression::new())
            .with(PostSizeLimit::new(size_limit))
            .with(CatchPanic::new().with_handler(panic_handler))
            // NOTE: Make sure to keep this after all the `with` middleware.
            .catch_all_error(convert_error)
            .around(middleware_log);
```

**File:** config/src/config/api_config.rs (L15-93)
```rust
#[derive(Clone, Debug, Deserialize, PartialEq, Eq, Serialize)]
#[serde(default, deny_unknown_fields)]
pub struct ApiConfig {
    /// Enables the REST API endpoint
    #[serde(default = "default_enabled")]
    pub enabled: bool,
    /// Address for the REST API to listen on. Set to 0.0.0.0:port to allow all inbound connections.
    pub address: SocketAddr,
    /// Path to a local TLS certificate to enable HTTPS
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls_cert_path: Option<String>,
    /// Path to a local TLS key to enable HTTPS
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub tls_key_path: Option<String>,
    /// A maximum limit to the body of a POST request in bytes
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub content_length_limit: Option<u64>,
    /// Enables failpoints for error testing
    #[serde(default = "default_disabled")]
    pub failpoints_enabled: bool,
    /// Enables JSON output of APIs that support it
    #[serde(default = "default_enabled")]
    pub json_output_enabled: bool,
    /// Enables BCS output of APIs that support it
    #[serde(default = "default_enabled")]
    pub bcs_output_enabled: bool,
    /// Enables compression middleware for API responses
    #[serde(default = "default_enabled")]
    pub compression_enabled: bool,
    /// Enables encode submission API
    #[serde(default = "default_enabled")]
    pub encode_submission_enabled: bool,
    /// Enables transaction submission APIs
    #[serde(default = "default_enabled")]
    pub transaction_submission_enabled: bool,
    /// Enables transaction simulation
    #[serde(default = "default_enabled")]
    pub transaction_simulation_enabled: bool,
    /// Maximum number of transactions that can be sent with the Batch submit API
    pub max_submit_transaction_batch_size: usize,
    /// Maximum page size for transaction paginated APIs
    pub max_transactions_page_size: u16,
    /// Maximum page size for block transaction APIs
    pub max_block_transactions_page_size: u16,
    /// Maximum page size for event paginated APIs
    pub max_events_page_size: u16,
    /// Maximum page size for resource paginated APIs
    pub max_account_resources_page_size: u16,
    /// Maximum page size for module paginated APIs
    pub max_account_modules_page_size: u16,
    /// Maximum gas unit limit for view functions
    ///
    /// This limits the execution length of a view function to the given gas used.
    pub max_gas_view_function: u64,
    /// Optional: Maximum number of worker threads for the API.
    ///
    /// If not set, `runtime_worker_multiplier` will multiply times the number of CPU cores on the machine
    pub max_runtime_workers: Option<usize>,
    /// Multiplier for number of worker threads with number of CPU cores
    ///
    /// If `max_runtime_workers` is set, this is ignored
    pub runtime_worker_multiplier: usize,
    /// Configs for computing unit gas price estimation
    pub gas_estimation: GasEstimationConfig,
    /// Periodically call gas estimation
    pub periodic_gas_estimation_ms: Option<u64>,
    /// Configuration to filter view function requests.
    pub view_filter: ViewFilter,
    /// Periodically log stats for view function and simulate transaction usage
    pub periodic_function_stats_sec: Option<u64>,
    /// The time wait_by_hash will wait before returning 404.
    pub wait_by_hash_timeout_ms: u64,
    /// The interval at which wait_by_hash will poll the storage for the transaction.
    pub wait_by_hash_poll_interval_ms: u64,
    /// The number of active wait_by_hash requests that can be active at any given time.
    pub wait_by_hash_max_active_connections: usize,
    /// Allow submission of encrypted transactions via the API
    pub allow_encrypted_txns_submission: bool,
}
```
