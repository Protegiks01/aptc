# Audit Report

## Title
Unhandled GRPC Server Binding Failures Cause Silent Validator Node Crashes During Remote Block Execution

## Summary
The remote executor service's GRPC server binding failure is not properly handled, causing spawned tasks to panic silently. When remote sharded block execution is enabled, binding failures (e.g., port already in use) lead to delayed validator node crashes during block execution, as the coordinator attempts to communicate with non-existent servers. This vulnerability affects validator availability and can cause unpredictable node failures.

## Finding Description

When `NetworkController` initializes the `InboundHandler` to start a GRPC server for remote executor communication, binding failures are handled improperly through a chain of `.unwrap()` calls in asynchronously spawned tasks. [1](#0-0) 

The `InboundHandler::start()` method spawns an async task that calls `GRPCNetworkMessageServiceServerWrapper::start_async()`: [2](#0-1) 

The critical vulnerability occurs in `start_async()` where `serve_with_shutdown()` is called with `.unwrap()`: [3](#0-2) 

**The code even acknowledges this limitation** in the comments: "There is no easy way to know if/when the server has started successfully."

**Attack Path:**

1. **Binding Failure**: When the GRPC server attempts to bind to `listen_addr`, if the port is already in use or unavailable, `serve_with_shutdown().await` returns an `Err`.

2. **Silent Panic**: The `.unwrap()` on line 86 causes the spawned async task to panic. Since this happens in a spawned task, the panic is not propagated to the caller, and `NetworkController::start()` returns successfully without indication of failure.

3. **Execution Attempt**: When remote sharded block execution occurs, the coordinator sends messages to remote executor shards: [4](#0-3) [5](#0-4) 

4. **Cascade Failure**: The outbound handler attempts to send messages to the failed server: [6](#0-5) 

The `send_message()` method panics on any error (line 154), causing the outbound handler task to terminate.

5. **Coordinator Crash**: The coordinator waits for responses using `.unwrap()`: [7](#0-6) 

When the channel disconnects due to the panicked outbound handler, `rx.recv().unwrap()` panics in the main execution thread, **crashing the validator node during block execution**.

## Impact Explanation

This vulnerability qualifies as **High Severity** per Aptos Bug Bounty criteria:

**Primary Impact: Validator Node Crashes**
- When remote executor sharding is enabled (via `--remote-executor-addresses` configuration), binding failures cause delayed validator crashes during block execution
- The node appears to start successfully but crashes during the first block execution attempt
- This directly causes "validator node slowdowns" and crashes, fitting the High Severity category

**Secondary Impacts:**
- **Reduced Network Availability**: Multiple affected validators reduce overall network liveness
- **Difficult Diagnosis**: Silent failures make debugging extremely challenging
- **Unpredictable Behavior**: Timing-dependent panics can manifest differently across runs

The impact is not Critical because:
- It requires remote executor sharding to be explicitly enabled (optional feature, primarily for benchmarking)
- Does not directly violate consensus safety invariants
- Does not cause loss of funds or state corruption
- Affected nodes can be restarted with corrected configuration

However, if this feature is used in production environments, the impact escalates significantly.

## Likelihood Explanation

**Likelihood: Medium-High** in environments using remote executor sharding.

**Factors Increasing Likelihood:**

1. **Configuration Errors**: Easy to misconfigure port numbers, especially with multiple remote shards requiring unique ports
2. **Port Conflicts**: Common in containerized or multi-tenant environments where port allocation may conflict
3. **No Validation**: No pre-flight checks to verify ports are available before starting
4. **Silent Failures**: Operators cannot detect the issue until execution attempts occur

**Factors Decreasing Likelihood:**

1. **Feature Opt-In**: Remote executor sharding must be explicitly enabled
2. **Testing Context**: Primarily used in benchmark and testing scenarios currently
3. **Port Management**: Most production deployments use dedicated port ranges with proper isolation

**Real-World Scenarios:**

- Operator accidentally starts multiple executor shards on same port
- Previous process didn't cleanly release port before restart
- Firewall or OS restrictions prevent binding
- Container orchestration assigns conflicting ports

## Recommendation

**Fix 1: Propagate Binding Errors to Caller**

Replace the async spawn pattern with a mechanism that allows the caller to verify server startup. Use a oneshot channel to signal success/failure:

```rust
// In GRPCNetworkMessageServiceServerWrapper::start_async
async fn start_async(
    self,
    server_addr: SocketAddr,
    rpc_timeout_ms: u64,
    server_shutdown_rx: oneshot::Receiver<()>,
    startup_tx: oneshot::Sender<Result<(), String>>,
) {
    let reflection_service = tonic_reflection::server::Builder::configure()
        .register_encoded_file_descriptor_set(FILE_DESCRIPTOR_SET)
        .build_v1()
        .unwrap();

    info!("Starting Server async at {:?}", server_addr);
    
    let server = Server::builder()
        .timeout(std::time::Duration::from_millis(rpc_timeout_ms))
        .add_service(
            NetworkMessageServiceServer::new(self).max_decoding_message_size(MAX_MESSAGE_SIZE),
        )
        .add_service(reflection_service);

    // Attempt to bind first
    match server.serve_with_shutdown(server_addr, async {
        server_shutdown_rx.await.ok();
        info!("Received signal to shutdown server at {:?}", server_addr);
    }).await {
        Ok(_) => {
            info!("Server shutdown at {:?}", server_addr);
        },
        Err(e) => {
            let err_msg = format!("Failed to bind server at {:?}: {}", server_addr, e);
            error!("{}", err_msg);
            startup_tx.send(Err(err_msg)).ok();
            return;
        }
    }
}
```

**Fix 2: Add Healthcheck Verification**

Implement a healthcheck mechanism as suggested in the existing comments:

```rust
pub fn start(&self, rt: &Runtime) -> Result<Option<oneshot::Sender<()>>, String> {
    if self.inbound_handlers.lock().unwrap().is_empty() {
        return Ok(None);
    }

    let (server_shutdown_tx, server_shutdown_rx) = oneshot::channel();
    let (startup_tx, startup_rx) = oneshot::channel();
    
    GRPCNetworkMessageServiceServerWrapper::new(
        self.inbound_handlers.clone(),
        self.listen_addr,
    )
    .start(rt, self.service.clone(), self.listen_addr, self.rpc_timeout_ms, 
           server_shutdown_rx, startup_tx);
    
    // Wait for server to confirm startup or timeout
    match startup_rx.recv_timeout(Duration::from_secs(5)) {
        Ok(Ok(())) => Ok(Some(server_shutdown_tx)),
        Ok(Err(e)) => Err(format!("Server startup failed: {}", e)),
        Err(_) => Err("Server startup timed out".to_string()),
    }
}
```

**Fix 3: Graceful Error Handling in Message Send**

Replace panics with proper error propagation:

```rust
pub async fn send_message(
    &mut self,
    sender_addr: SocketAddr,
    message: Message,
    mt: &MessageType,
) -> Result<(), String> {
    let request = tonic::Request::new(NetworkMessage {
        message: message.data,
        message_type: mt.get_type(),
    });
    
    self.remote_channel.simple_msg_exchange(request).await
        .map(|_| ())
        .map_err(|e| format!("Error sending message to {} on node {:?}: {}", 
                             self.remote_addr, sender_addr, e))
}
```

## Proof of Concept

```rust
// test_port_binding_failure.rs
#[cfg(test)]
mod port_binding_tests {
    use aptos_secure_net::network_controller::NetworkController;
    use std::net::{IpAddr, Ipv4Addr, SocketAddr, TcpListener};
    use std::thread;
    use std::time::Duration;

    #[test]
    #[should_panic(expected = "Failed to bind")]
    fn test_port_already_in_use_causes_panic() {
        let port = 12345;
        let addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), port);
        
        // Occupy the port first
        let _listener = TcpListener::bind(addr).expect("Failed to bind test listener");
        
        // Try to start NetworkController on the same port
        let mut controller = NetworkController::new(
            "test_service".to_string(),
            addr,
            1000,
        );
        
        // Register a handler to trigger server startup
        let _rx = controller.create_inbound_channel("test_type".to_string());
        
        // Start the controller - this will silently fail to bind
        controller.start();
        
        // Give the async task time to attempt binding and panic
        thread::sleep(Duration::from_millis(100));
        
        // Try to use the controller - this should reveal the binding failure
        // by causing a panic when messages cannot be sent/received
        let test_addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 12346);
        let tx = controller.create_outbound_channel(test_addr, "test_type".to_string());
        
        // This send will eventually cause a panic when the outbound handler
        // tries to send to a non-existent server
        tx.send(Message::new(vec![1, 2, 3])).unwrap();
        
        // Wait for the cascade of panics
        thread::sleep(Duration::from_secs(2));
    }
    
    #[test]
    fn test_binding_error_detection() {
        let port = 12347;
        let addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), port);
        
        // Occupy the port
        let _listener = TcpListener::bind(addr).expect("Failed to bind test listener");
        
        // With proper error handling (after fix), this should return an error
        // instead of panicking later
        let result = std::panic::catch_unwind(|| {
            let mut controller = NetworkController::new(
                "test_service".to_string(),
                addr,
                1000,
            );
            controller.create_inbound_channel("test".to_string());
            controller.start();
            thread::sleep(Duration::from_millis(500));
        });
        
        // Currently this will panic, but after fix should handle gracefully
        assert!(result.is_err(), "Expected panic due to port conflict");
    }
}
```

**Notes:**

- This vulnerability is currently exploitable in any deployment using remote executor sharding
- The issue is exacerbated by the lack of pre-flight validation and post-startup health checks
- The cascade of `.unwrap()` calls creates multiple panic points depending on timing
- Production deployments should avoid using remote executor sharding until this is fixed, or implement external port conflict detection

### Citations

**File:** secure/net/src/network_controller/mod.rs (L95-100)
```rust
    pub fn new(service: String, listen_addr: SocketAddr, timeout_ms: u64) -> Self {
        let inbound_handler = Arc::new(Mutex::new(InboundHandler::new(
            service.clone(),
            listen_addr,
            timeout_ms,
        )));
```

**File:** secure/net/src/network_controller/inbound_handler.rs (L44-63)
```rust
    pub fn start(&self, rt: &Runtime) -> Option<oneshot::Sender<()>> {
        if self.inbound_handlers.lock().unwrap().is_empty() {
            return None;
        }

        let (server_shutdown_tx, server_shutdown_rx) = oneshot::channel();
        // The server is started in a separate task
        GRPCNetworkMessageServiceServerWrapper::new(
            self.inbound_handlers.clone(),
            self.listen_addr,
        )
        .start(
            rt,
            self.service.clone(),
            self.listen_addr,
            self.rpc_timeout_ms,
            server_shutdown_rx,
        );
        Some(server_shutdown_tx)
    }
```

**File:** secure/net/src/grpc_network_service/mod.rs (L57-88)
```rust
    async fn start_async(
        self,
        server_addr: SocketAddr,
        rpc_timeout_ms: u64,
        server_shutdown_rx: oneshot::Receiver<()>,
    ) {
        let reflection_service = tonic_reflection::server::Builder::configure()
            .register_encoded_file_descriptor_set(FILE_DESCRIPTOR_SET)
            .build_v1()
            .unwrap();

        info!("Starting Server async at {:?}", server_addr);
        // NOTE: (1) serve_with_shutdown() starts the server, if successful the task does not return
        //           till the server is shutdown. Hence this should be called as a separate
        //           non-blocking task. Signal handler 'server_shutdown_rx' is needed to shutdown
        //           the server
        //       (2) There is no easy way to know if/when the server has started successfully. Hence
        //           we may need to implement a healthcheck service to check if the server is up
        Server::builder()
            .timeout(std::time::Duration::from_millis(rpc_timeout_ms))
            .add_service(
                NetworkMessageServiceServer::new(self).max_decoding_message_size(MAX_MESSAGE_SIZE),
            )
            .add_service(reflection_service)
            .serve_with_shutdown(server_addr, async {
                server_shutdown_rx.await.ok();
                info!("Received signal to shutdown server at {:?}", server_addr);
            })
            .await
            .unwrap();
        info!("Server shutdown at {:?}", server_addr);
    }
```

**File:** secure/net/src/grpc_network_service/mod.rs (L140-160)
```rust
    pub async fn send_message(
        &mut self,
        sender_addr: SocketAddr,
        message: Message,
        mt: &MessageType,
    ) {
        let request = tonic::Request::new(NetworkMessage {
            message: message.data,
            message_type: mt.get_type(),
        });
        // TODO: Retry with exponential backoff on failures
        match self.remote_channel.simple_msg_exchange(request).await {
            Ok(_) => {},
            Err(e) => {
                panic!(
                    "Error '{}' sending message to {} on node {:?}",
                    e, self.remote_addr, sender_addr
                );
            },
        }
    }
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L256-276)
```rust
    fn execute_block_sharded<V: VMBlockExecutor>(
        partitioned_txns: PartitionedTransactions,
        state_view: Arc<CachedStateView>,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<Vec<TransactionOutput>> {
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        } else {
            Ok(V::execute_block_sharded(
                &SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        }
    }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L163-172)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        trace!("RemoteExecutorClient Waiting for results");
        let mut results = vec![];
        for rx in self.result_rxs.iter() {
            let received_bytes = rx.recv().unwrap().to_bytes();
            let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
            results.push(result.inner?);
        }
        Ok(results)
    }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L180-212)
```rust
    fn execute_block(
        &self,
        state_view: Arc<S>,
        transactions: PartitionedTransactions,
        concurrency_level_per_shard: usize,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<ShardedExecutionOutput, VMStatus> {
        trace!("RemoteExecutorClient Sending block to shards");
        self.state_view_service.set_state_view(state_view);
        let (sub_blocks, global_txns) = transactions.into();
        if !global_txns.is_empty() {
            panic!("Global transactions are not supported yet");
        }
        for (shard_id, sub_blocks) in sub_blocks.into_iter().enumerate() {
            let senders = self.command_txs.clone();
            let execution_request = RemoteExecutionRequest::ExecuteBlock(ExecuteBlockCommand {
                sub_blocks,
                concurrency_level: concurrency_level_per_shard,
                onchain_config: onchain_config.clone(),
            });

            senders[shard_id]
                .lock()
                .unwrap()
                .send(Message::new(bcs::to_bytes(&execution_request).unwrap()))
                .unwrap();
        }

        let execution_results = self.get_output_from_shards()?;

        self.state_view_service.drop_state_view();
        Ok(ShardedExecutionOutput::new(execution_results, vec![]))
    }
```
