# Audit Report

## Title
StateValueSyncer Non-Atomic Field Updates Cause State Inconsistency During Stream Resets

## Summary
The `StateValueSyncer` struct in the bootstrapper maintains critical state for fast sync operations, but its fields (`initialized_state_snapshot_receiver`, `ledger_info_to_sync`, `next_state_index_to_process`, `transaction_output_to_sync`) are updated separately at different times and are never reset during stream resets. This allows a stale receiver thread to persist across stream resets, leading to silent data skipping and incomplete state synchronization. [1](#0-0) 

## Finding Description
The vulnerability occurs through the following mechanism:

**Step 1: Initial State Sync**
When fast sync begins, the bootstrapper initializes the `StateValueSyncer` by setting fields separately:
- `ledger_info_to_sync` is set first [2](#0-1) 
- `transaction_output_to_sync` is set later [3](#0-2) 
- `next_state_index_to_process` is updated when streams restart [4](#0-3) 
- `initialized_state_snapshot_receiver` is set to true after spawning the receiver thread [5](#0-4) 

**Step 2: Receiver Thread Spawning**
The `initialize_state_synchronizer` method spawns a dedicated receiver thread that maintains internal state, including `previous_leaf` tracking the last processed state key: [6](#0-5) 

This receiver thread processes state value chunks and updates its internal `previous_leaf` marker: [7](#0-6) 

**Step 3: Stream Reset Without Cleanup**
When a stream times out or encounters an error, `reset_active_stream` is called: [8](#0-7) 

**CRITICAL**: This only clears `active_data_stream` and `speculative_stream_state`, but:
- Does NOT reset `StateValueSyncer` fields
- Does NOT terminate the receiver thread
- Does NOT reset `initialized_state_snapshot_receiver` to false
- The receiver thread continues running in the background

**Step 4: Re-initialization Skipped**
When a new stream is initialized after the reset, the code checks the flag: [9](#0-8) 

Because `initialized_state_snapshot_receiver` is still `true`, it **skips** calling `initialize_state_synchronizer()` again. The new stream reuses the old receiver thread via the existing channel.

**Step 5: State Inconsistency**
The old receiver thread has internal state (`previous_leaf`) that may be ahead of what metadata storage indicates. When new stream fetches the starting index: [10](#0-9) 

If metadata storage returns an earlier index (due to failed persistence in the previous attempt), the overlap-skipping logic in the receiver silently skips these state values: [11](#0-10) 

This creates a gap in state synchronization where indices that should be written are silently skipped, leading to incomplete state and potential Merkle tree corruption.

## Impact Explanation
This vulnerability breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs."

**Impact Severity: High (up to $50,000)**

The consequences include:

1. **Incomplete State Synchronization**: A node may complete fast sync with missing state values, believing it has a complete snapshot when critical state entries are absent.

2. **Merkle Tree Corruption**: The Jellyfish Merkle tree may have gaps or inconsistent internal nodes, causing verification failures when other nodes try to sync from this node or when transactions attempt to read the missing state.

3. **Node Divergence**: Different nodes that experienced different stream reset patterns may end up with different state, violating consensus determinism.

4. **Undetected Corruption**: The silent skipping means no error is raised, so the node continues operating with corrupted state until a transaction accesses the missing data or another node detects the inconsistency.

This qualifies as a "Significant protocol violation" under High Severity criteria, as it can cause state inconsistencies requiring manual intervention and potentially affect network availability if multiple nodes are affected.

## Likelihood Explanation
**Likelihood: Medium to High**

This vulnerability can be triggered by:

1. **Network Timeouts**: Stream timeouts are handled at line 592-597, automatically triggering reset when `max_num_stream_timeouts` is exceeded. In unstable network conditions, this is common.

2. **Invalid Payloads**: Any invalid data from malicious or buggy peers triggers stream resets throughout the notification processing (e.g., lines 926-930, 947-950, 1022-1026).

3. **Natural Occurrence**: The overlap logic at lines 699-715 is explicitly designed for stream resets ("on each stream reset, we overlap every chunk by a single item"), indicating resets are expected during normal operation.

4. **No Privilege Required**: Any network peer can cause stream resets by sending invalid data or by network-level delays, making this exploitable without special access.

The combination of expected stream resets during normal operation and the lack of StateValueSyncer cleanup makes this vulnerability highly likely to manifest in production environments, especially during initial node bootstrapping or when syncing nodes experience network instability.

## Recommendation

**Fix: Reset StateValueSyncer fields during stream reset**

Add a method to reset the StateValueSyncer state:

```rust
impl StateValueSyncer {
    /// Resets all fields to their initial state
    pub fn reset(&mut self) {
        self.initialized_state_snapshot_receiver = false;
        self.ledger_info_to_sync = None;
        self.next_state_index_to_process = 0;
        self.transaction_output_to_sync = None;
    }
}
```

Then modify `reset_active_stream` to call this method:

```rust
pub async fn reset_active_stream(
    &mut self,
    notification_and_feedback: Option<NotificationAndFeedback>,
) -> Result<(), Error> {
    if let Some(active_data_stream) = &self.active_data_stream {
        let data_stream_id = active_data_stream.data_stream_id;
        utils::terminate_stream_with_feedback(
            &mut self.streaming_client,
            data_stream_id,
            notification_and_feedback,
        )
        .await?;
    }

    self.active_data_stream = None;
    self.speculative_stream_state = None;
    
    // ADD THIS: Reset StateValueSyncer to clear stale state
    self.state_value_syncer.reset();
    
    Ok(())
}
```

Additionally, add cleanup for the storage synchronizer's state_snapshot_notifier to ensure the old receiver thread is properly terminated when resetting.

## Proof of Concept

```rust
#[tokio::test]
async fn test_state_value_syncer_inconsistency_on_reset() {
    // Setup: Create a bootstrapper with mock components
    let (mut bootstrapper, _) = create_mock_bootstrapper().await;
    
    // Step 1: Initialize state sync with target version 1000
    let target_ledger_info = create_mock_ledger_info(1000);
    bootstrapper.state_value_syncer
        .set_ledger_info_to_sync(target_ledger_info.clone());
    
    // Step 2: Fetch transaction output (first stream)
    let transaction_output = create_mock_transaction_output(1000);
    bootstrapper.state_value_syncer
        .set_transaction_output_to_sync(transaction_output);
    
    // Step 3: Initialize receiver and process some chunks
    // This sets initialized_state_snapshot_receiver = true
    // and receiver processes chunks 0-499
    process_state_value_chunks(&mut bootstrapper, 0, 499).await;
    
    // Step 4: Verify flag is set
    assert!(bootstrapper.state_value_syncer.initialized_state_snapshot_receiver);
    assert_eq!(bootstrapper.state_value_syncer.next_state_index_to_process, 500);
    
    // Step 5: Simulate stream reset (timeout or error)
    bootstrapper.reset_active_stream(None).await.unwrap();
    
    // BUG: StateValueSyncer fields are NOT reset!
    assert!(bootstrapper.state_value_syncer.initialized_state_snapshot_receiver); // Still true!
    assert_eq!(bootstrapper.state_value_syncer.next_state_index_to_process, 500); // Still 500!
    
    // Step 6: Simulate metadata returning earlier index (300)
    // This happens if some chunks failed to persist in step 3
    set_mock_metadata_last_persisted_index(&mut bootstrapper, 300);
    
    // Step 7: Reinitialize stream - it will start from index 300
    initialize_stream_from_metadata(&mut bootstrapper).await;
    assert_eq!(bootstrapper.state_value_syncer.next_state_index_to_process, 300);
    
    // Step 8: Send chunks starting from 300
    // BUG: initialize_state_synchronizer is NOT called (flag still true)
    // The OLD receiver thread (with previous_leaf at key_499) receives this data
    // The overlap logic SKIPS indices 300-499 because they're < previous_leaf
    // But metadata expects these to be written!
    let result = process_state_value_chunks(&mut bootstrapper, 300, 500).await;
    
    // Step 9: Verify state inconsistency
    // The node thinks it synced indices 300-500, but the receiver skipped 300-499
    // State is incomplete and corrupted
    let persisted_indices = get_persisted_state_indices(&bootstrapper).await;
    assert!(persisted_indices.contains(&300)); // This will FAIL - data was skipped!
    assert!(persisted_indices.contains(&450)); // This will FAIL - data was skipped!
    
    // Only indices 500+ were actually written
    assert!(persisted_indices.contains(&500)); // This passes
}
```

This demonstrates how the stale `initialized_state_snapshot_receiver` flag prevents proper receiver reinitialization, causing silent data loss and state inconsistency during fast sync after stream resets.

### Citations

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L244-287)
```rust
pub(crate) struct StateValueSyncer {
    // Whether or not a state snapshot receiver has been initialized
    initialized_state_snapshot_receiver: bool,

    // The epoch ending ledger info for the version we're syncing
    ledger_info_to_sync: Option<LedgerInfoWithSignatures>,

    // The next state value index to process (all state values before this have been
    // processed -- i.e., sent to the storage synchronizer).
    next_state_index_to_process: u64,

    // The transaction output (inc. info and proof) for the version we're syncing
    transaction_output_to_sync: Option<TransactionOutputListWithProofV2>,
}

impl StateValueSyncer {
    pub fn new() -> Self {
        Self {
            initialized_state_snapshot_receiver: false,
            ledger_info_to_sync: None,
            next_state_index_to_process: 0,
            transaction_output_to_sync: None,
        }
    }

    /// Sets the ledger info to sync
    pub fn set_ledger_info_to_sync(&mut self, ledger_info_to_sync: LedgerInfoWithSignatures) {
        self.ledger_info_to_sync = Some(ledger_info_to_sync);
    }

    /// Sets the transaction output to sync
    pub fn set_transaction_output_to_sync(
        &mut self,
        transaction_output_to_sync: TransactionOutputListWithProofV2,
    ) {
        self.transaction_output_to_sync = Some(transaction_output_to_sync);
    }

    /// Updates the next state index to process
    pub fn update_next_state_index_to_process(&mut self, next_state_index_to_process: u64) {
        self.next_state_index_to_process = next_state_index_to_process;
    }
}

```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L682-683)
```rust
            self.state_value_syncer
                .set_ledger_info_to_sync(target_ledger_info.clone());
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L699-719)
```rust
            let next_state_index_to_process = if existing_snapshot_progress {
                // The state snapshot receiver requires that after each reboot we
                // rewrite the last persisted index (again!). This is a limitation
                // of how the snapshot is persisted (i.e., in-memory sibling freezing).
                // Thus, on each stream reset, we overlap every chunk by a single item.
                self
                    .metadata_storage
                    .get_last_persisted_state_value_index(&target_ledger_info)
                    .map_err(|error| {
                        Error::StorageError(format!(
                            "Failed to get the last persisted state value index at version {:?}! Error: {:?}",
                            target_ledger_info_version, error
                        ))
                    })?
            } else {
                0 // We need to start the snapshot sync from index 0
            };

            // Fetch the missing state values
            self.state_value_syncer
                .update_next_state_index_to_process(next_state_index_to_process);
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L985-1001)
```rust
        if !self.state_value_syncer.initialized_state_snapshot_receiver {
            // Fetch all verified epoch change proofs
            let version_to_sync = ledger_info_to_sync.ledger_info().version();
            let epoch_change_proofs = if version_to_sync == GENESIS_TRANSACTION_VERSION {
                vec![ledger_info_to_sync.clone()] // Sync to genesis
            } else {
                self.verified_epoch_states.all_epoch_ending_ledger_infos() // Sync beyond genesis
            };

            // Initialize the state value synchronizer
            let _join_handle = self.storage_synchronizer.initialize_state_synchronizer(
                epoch_change_proofs,
                ledger_info_to_sync,
                transaction_output_to_sync.clone(),
            )?;
            self.state_value_syncer.initialized_state_snapshot_receiver = true;
        }
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L1299-1300)
```rust
                        self.state_value_syncer
                            .set_transaction_output_to_sync(transaction_outputs_with_proof);
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L1539-1556)
```rust
    pub async fn reset_active_stream(
        &mut self,
        notification_and_feedback: Option<NotificationAndFeedback>,
    ) -> Result<(), Error> {
        if let Some(active_data_stream) = &self.active_data_stream {
            let data_stream_id = active_data_stream.data_stream_id;
            utils::terminate_stream_with_feedback(
                &mut self.streaming_client,
                data_stream_id,
                notification_and_feedback,
            )
            .await?;
        }

        self.active_data_stream = None;
        self.speculative_stream_state = None;
        Ok(())
    }
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L378-406)
```rust
    fn initialize_state_synchronizer(
        &mut self,
        epoch_change_proofs: Vec<LedgerInfoWithSignatures>,
        target_ledger_info: LedgerInfoWithSignatures,
        target_output_with_proof: TransactionOutputListWithProofV2,
    ) -> Result<JoinHandle<()>, Error> {
        // Create a channel to notify the state snapshot receiver when data chunks are ready
        let max_pending_data_chunks = self.driver_config.max_pending_data_chunks as usize;
        let (state_snapshot_notifier, state_snapshot_listener) =
            mpsc::channel(max_pending_data_chunks);

        // Spawn the state snapshot receiver that commits state values
        let receiver_handle = spawn_state_snapshot_receiver(
            self.chunk_executor.clone(),
            state_snapshot_listener,
            self.commit_notification_sender.clone(),
            self.error_notification_sender.clone(),
            self.pending_data_chunks.clone(),
            self.metadata_storage.clone(),
            self.storage.clone(),
            epoch_change_proofs,
            target_ledger_info,
            target_output_with_proof,
            self.runtime.clone(),
        );
        self.state_snapshot_notifier = Some(state_snapshot_notifier);

        Ok(receiver_handle)
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L349-368)
```rust
        if let Some(prev_leaf) = &self.previous_leaf {
            let skip_until = chunk
                .iter()
                .find_position(|(key, _hash)| key.hash() > *prev_leaf.account_key());
            chunk = match skip_until {
                None => {
                    info!("Skipping entire chunk.");
                    return Ok(());
                },
                Some((0, _)) => chunk,
                Some((num_to_skip, next_leaf)) => {
                    info!(
                        num_to_skip = num_to_skip,
                        next_leaf = next_leaf,
                        "Skipping leaves."
                    );
                    chunk.split_off(num_to_skip)
                },
            }
        };
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L376-386)
```rust
                ensure!(
                    &hashed_key > prev_leaf.account_key(),
                    "State keys must come in increasing order.",
                )
            }
            self.previous_leaf.replace(LeafNode::new(
                hashed_key,
                value_hash,
                (key.clone(), self.version),
            ));
            self.add_one(key, value_hash);
```
