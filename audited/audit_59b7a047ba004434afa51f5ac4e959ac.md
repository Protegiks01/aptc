# Audit Report

## Title
Unbounded Resource Exhaustion in Metadata File Downloads Leading to Validator Node Crash and Consensus Liveness Failure

## Summary
The `download_file()` function in the backup-cli metadata cache system uses `tokio::io::copy()` without any size limits to download metadata files from external backup storage. A malicious or compromised backup storage can return arbitrarily large files, exhausting disk space or memory, causing validator nodes to crash during restore operations and potentially leading to consensus liveness failures.

## Finding Description

The vulnerability exists in the metadata caching system used by the Aptos backup-cli tool, which validator nodes use to restore their state from backups. The critical code path is: [1](#0-0) 

The `download_file()` function uses `tokio::io::copy()` to transfer data from a remote backup storage source to a local temporary file. The Rust standard library's `tokio::io::copy()` function **does not enforce any size limits** - it will continue copying data until EOF is reached or an error occurs.

After downloading, the entire file is loaded into memory: [2](#0-1) 

The attack vector leverages the fact that backup storage is controlled by external commands configured via the CommandAdapter: [3](#0-2) 

These commands execute shell scripts that download from cloud storage (S3, GCS, Azure): [4](#0-3) 

The metadata structure `CompactionTimestampsMeta` contains an unbounded HashMap that can grow arbitrarily large: [5](#0-4) 

This function is called during critical restore operations: [6](#0-5) 

**Attack Scenario:**

1. Attacker compromises backup storage credentials or provides a malicious backup endpoint
2. Attacker uploads metadata files with gigabytes/terabytes of data (e.g., CompactionTimestampsMeta with millions of entries)
3. Validator operator initiates restore operation (common after node failures or when bootstrapping new validators)
4. `sync_and_load()` downloads metadata files via `download_file()`
5. `tokio::io::copy()` continues copying until disk is exhausted OR
6. If disk has sufficient space, `read_to_string()` attempts to load the entire file into memory, exhausting RAM
7. Validator node crashes due to out-of-disk or out-of-memory error
8. If multiple validators restore simultaneously from compromised backup, consensus cannot reach quorum

This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits."

## Impact Explanation

**Severity: HIGH (up to $50,000 per Aptos Bug Bounty Program)**

This vulnerability qualifies as HIGH severity under the following categories:
- **Validator node slowdowns**: Before crashing, the node will experience severe performance degradation while attempting to process gigabyte-scale files
- **API crashes**: The restore process will crash, preventing validator node recovery

**Potential Escalation to CRITICAL:**
If multiple validators attempt to restore from the same compromised backup storage simultaneously (realistic during network-wide incidents or coordinated validator bootstrapping), this could result in:
- **Total loss of liveness/network availability**: If enough validators crash and cannot rejoin consensus, the network cannot reach the required 2/3+ quorum for block finalization

The impact is amplified because:
1. Validator operators commonly share backup infrastructure or use the same cloud storage providers
2. During network incidents, multiple validators may restore simultaneously
3. The attack persists until backup storage is cleaned, affecting all future restore attempts
4. No timeout or size limit protections exist in the current implementation

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

The attack is highly feasible because:

1. **Common Operation**: Validator operators regularly perform backup restores:
   - After hardware failures
   - When bootstrapping new validator nodes  
   - During disaster recovery scenarios
   - For testing and validation purposes

2. **Realistic Attacker Capabilities**:
   - Backup storage credentials could be leaked through misconfiguration, phishing, or supply chain attacks
   - Cloud storage providers have been compromised in past incidents
   - Insider threats (disgruntled operators with backup access)
   - Man-in-the-middle attacks if backup commands don't enforce HTTPS/authentication

3. **No Defense Mechanisms**:
   - No size limits on downloads (confirmed in code analysis)
   - No timeout configuration available
   - No validation of metadata file sizes before downloading
   - Error handling is minimal and doesn't prevent the attack [7](#0-6) 

The concurrent_downloads setting only controls parallelism, not individual file sizes.

## Recommendation

Implement multiple layers of protection:

### 1. Add Maximum File Size Limit
Add a configurable maximum metadata file size (e.g., 100MB default):

```rust
// In MetadataCacheOpt
pub struct MetadataCacheOpt {
    // ... existing fields ...
    
    #[clap(
        long = "metadata-max-file-size",
        default_value_t = 104857600, // 100MB default
        help = "Maximum size in bytes for metadata files during download."
    )]
    max_file_size: u64,
}

// Modified download_file function
async fn download_file(
    storage_ref: &dyn BackupStorage,
    file_handle: &FileHandle,
    local_tmp_file: &Path,
    max_size: u64,
) -> Result<()> {
    let mut reader = storage_ref
        .open_for_read(file_handle)
        .await
        .err_notes(file_handle)?;
    
    let mut writer = OpenOptions::new()
        .write(true)
        .create_new(true)
        .open(local_tmp_file)
        .await
        .err_notes(local_tmp_file)?;
    
    // Use limited copy with size tracking
    let mut total_bytes = 0u64;
    let mut buffer = [0u8; 8192];
    
    loop {
        let n = reader.read(&mut buffer).await?;
        if n == 0 {
            break;
        }
        
        total_bytes += n as u64;
        if total_bytes > max_size {
            return Err(anyhow!(
                "Metadata file exceeds maximum size limit of {} bytes (downloaded {})",
                max_size,
                total_bytes
            ));
        }
        
        writer.write_all(&buffer[..n]).await?;
    }
    
    Ok(())
}
```

### 2. Add Timeout Protection
Implement download timeouts to prevent indefinite hangs:

```rust
use tokio::time::{timeout, Duration};

const DOWNLOAD_TIMEOUT_SECS: u64 = 300; // 5 minutes

async fn download_file_with_timeout(...) -> Result<()> {
    timeout(
        Duration::from_secs(DOWNLOAD_TIMEOUT_SECS),
        download_file(...)
    )
    .await
    .map_err(|_| anyhow!("Metadata file download timed out after {} seconds", DOWNLOAD_TIMEOUT_SECS))?
}
```

### 3. Validate Metadata Content Size
After loading, validate that metadata structures are within reasonable bounds:

```rust
fn validate_metadata_size(metadata: &Metadata) -> Result<()> {
    match metadata {
        Metadata::CompactionTimestamps(meta) => {
            const MAX_ENTRIES: usize = 100_000;
            ensure!(
                meta.compaction_timestamps.len() <= MAX_ENTRIES,
                "CompactionTimestamps contains {} entries, exceeding maximum of {}",
                meta.compaction_timestamps.len(),
                MAX_ENTRIES
            );
        },
        // Validate other metadata types...
        _ => {}
    }
    Ok(())
}
```

### 4. Add Pre-Download Size Check
If the backup storage protocol supports it, check file size before downloading:

```rust
// Add to BackupStorage trait
async fn get_file_size(&self, file_handle: &FileHandleRef) -> Result<Option<u64>>;
```

## Proof of Concept

```rust
#[cfg(test)]
mod resource_exhaustion_tests {
    use super::*;
    use async_trait::async_trait;
    use tokio::io::AsyncRead;
    use std::pin::Pin;
    use std::task::{Context, Poll};
    
    // Mock reader that returns infinite data
    struct InfiniteReader {
        bytes_sent: u64,
    }
    
    impl InfiniteReader {
        fn new() -> Self {
            Self { bytes_sent: 0 }
        }
    }
    
    impl AsyncRead for InfiniteReader {
        fn poll_read(
            mut self: Pin<&mut Self>,
            _cx: &mut Context<'_>,
            buf: &mut tokio::io::ReadBuf<'_>,
        ) -> Poll<std::io::Result<()>> {
            // Fill buffer with data indefinitely
            let remaining = buf.remaining();
            buf.put_slice(&vec![0xAA; remaining]);
            self.bytes_sent += remaining as u64;
            
            // Log progress every 100MB
            if self.bytes_sent % (100 * 1024 * 1024) == 0 {
                println!("Malicious storage sent: {} MB", self.bytes_sent / (1024 * 1024));
            }
            
            Poll::Ready(Ok(()))
        }
    }
    
    struct MaliciousBackupStorage;
    
    #[async_trait]
    impl BackupStorage for MaliciousBackupStorage {
        async fn open_for_read(
            &self,
            _file_handle: &FileHandleRef,
        ) -> Result<Box<dyn AsyncRead + Send + Unpin>> {
            Ok(Box::new(InfiniteReader::new()))
        }
        
        // Implement other required trait methods with minimal stubs...
        async fn create_backup(&self, _name: &ShellSafeName) -> Result<BackupHandle> {
            Ok("malicious".to_string())
        }
        
        async fn create_for_write(
            &self,
            _backup_handle: &BackupHandleRef,
            _name: &ShellSafeName,
        ) -> Result<(FileHandle, Box<dyn AsyncWrite + Send + Unpin>)> {
            unimplemented!()
        }
        
        async fn list_metadata_files(&self) -> Result<Vec<FileHandle>> {
            Ok(vec!["malicious_metadata.meta".to_string()])
        }
        
        async fn backup_metadata_file(&self, _file_handle: &FileHandleRef) -> Result<()> {
            Ok(())
        }
        
        async fn save_metadata_lines(
            &self,
            _name: &ShellSafeName,
            _lines: &[TextLine],
        ) -> Result<FileHandle> {
            Ok("saved".to_string())
        }
    }
    
    #[tokio::test]
    async fn test_unbounded_metadata_download() {
        let temp_dir = tempfile::tempdir().unwrap();
        let temp_file = temp_dir.path().join("test_download");
        
        let malicious_storage = MaliciousBackupStorage;
        
        // This will attempt to download infinite data
        // In a real scenario, this would exhaust disk space
        let result = tokio::time::timeout(
            std::time::Duration::from_secs(5),
            download_file(
                &malicious_storage,
                &"malicious_file",
                &temp_file,
            )
        ).await;
        
        // The download should timeout, proving no size limit exists
        assert!(result.is_err(), "Download should timeout without size limits");
        
        // Check how much was downloaded in 5 seconds
        if temp_file.exists() {
            let metadata = std::fs::metadata(&temp_file).unwrap();
            println!("Downloaded {} MB in 5 seconds without any size validation", 
                     metadata.len() / (1024 * 1024));
            
            // This demonstrates the vulnerability - potentially hundreds of MB 
            // downloaded without limits
            assert!(metadata.len() > 10 * 1024 * 1024, 
                   "Should have downloaded >10MB, proving no size limit");
        }
    }
}
```

**To run the PoC:**
1. Add the test to `storage/backup/backup-cli/src/metadata/cache.rs`
2. Run: `cargo test test_unbounded_metadata_download --package backup-cli`
3. Observe that the download continues indefinitely without size limits
4. The test will timeout after 5 seconds, demonstrating that gigabytes could be downloaded without protection

## Notes

This vulnerability affects all validator operators who use the backup-cli tool for disaster recovery. The risk is particularly high during:
- Network-wide incidents requiring multiple validators to restore simultaneously
- Initial validator bootstrapping when joining the network
- Testing and validation procedures

The recommended fixes should be implemented with backward compatibility in mind, using sensible defaults (e.g., 100MB max file size) that can be overridden via CLI flags for legitimate large metadata files.

### Citations

**File:** storage/backup/backup-cli/src/metadata/cache.rs (L67-87)
```rust
async fn download_file(
    storage_ref: &dyn BackupStorage,
    file_handle: &FileHandle,
    local_tmp_file: &Path,
) -> Result<()> {
    tokio::io::copy(
        &mut storage_ref
            .open_for_read(file_handle)
            .await
            .err_notes(file_handle)?,
        &mut OpenOptions::new()
            .write(true)
            .create_new(true)
            .open(local_tmp_file)
            .await
            .err_notes(local_tmp_file)?,
    )
    .await
    .map_err(|e| anyhow!("Failed to download file: {}", e))?;
    Ok(())
}
```

**File:** storage/backup/backup-cli/src/metadata/cache.rs (L236-247)
```rust
impl<R: AsyncRead + Send + Unpin> LoadMetadataLines for R {
    async fn load_metadata_lines(&mut self) -> Result<Vec<Metadata>> {
        let mut buf = String::new();
        self.read_to_string(&mut buf)
            .await
            .err_notes((file!(), line!(), &buf))?;
        Ok(buf
            .lines()
            .map(serde_json::from_str::<Metadata>)
            .collect::<Result<_, serde_json::error::Error>>()?)
    }
}
```

**File:** storage/backup/backup-cli/src/storage/command_adapter/mod.rs (L114-124)
```rust
    async fn open_for_read(
        &self,
        file_handle: &FileHandleRef,
    ) -> Result<Box<dyn AsyncRead + Send + Unpin>> {
        let child = self
            .cmd(&self.config.commands.open_for_read, vec![
                EnvVar::file_handle(file_handle.to_string()),
            ])
            .spawn()?;
        Ok(Box::new(child.into_data_source()))
    }
```

**File:** storage/backup/backup-cli/src/storage/command_adapter/sample_configs/s3.sample.yaml (L19-21)
```yaml
  open_for_read: |
    # route file handle content to stdout
    aws s3 cp "s3://$BUCKET/$SUB_DIR/$FILE_HANDLE" - | gzip -cd
```

**File:** storage/backup/backup-cli/src/metadata/mod.rs (L203-207)
```rust
#[derive(Clone, Debug, Deserialize, Serialize, Eq)]
pub struct CompactionTimestampsMeta {
    pub file_compacted_at: u64,
    pub compaction_timestamps: HashMap<FileHandle, Option<u64>>,
}
```

**File:** storage/backup/backup-cli/src/coordinators/restore.rs (L117-122)
```rust
        let metadata_view = metadata::cache::sync_and_load(
            &self.metadata_cache_opt,
            Arc::clone(&self.storage),
            self.global_opt.concurrent_downloads,
        )
        .await?;
```

**File:** storage/backup/backup-cli/src/utils/mod.rs (L365-383)
```rust
#[derive(Clone, Copy, Default, Parser)]
pub struct ConcurrentDownloadsOpt {
    #[clap(
        long,
        help = "Number of concurrent downloads from the backup storage. This covers the initial \
        metadata downloads as well. Speeds up remote backup access. [Defaults to number of CPUs]"
    )]
    concurrent_downloads: Option<usize>,
}

impl ConcurrentDownloadsOpt {
    pub fn get(&self) -> usize {
        let ret = self.concurrent_downloads.unwrap_or_else(num_cpus::get);
        info!(
            concurrent_downloads = ret,
            "Determined concurrency level for downloading."
        );
        ret
    }
```
