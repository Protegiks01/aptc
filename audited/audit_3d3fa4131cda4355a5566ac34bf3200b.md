# Audit Report

## Title
Connection Resource Leak in PeerManager Error Path

## Summary
The `handle_new_connection_event` method in `PeerManager` fails to properly clean up connections when `get_trusted_peers` returns an error. This causes the `Connection<NoiseStream<T>>` to be dropped without calling its cleanup method, leading to resource leaks including unclosed sockets, leaked file descriptors, and orphaned TCP connections.

## Finding Description

When a new connection is established, `PeerManager::handle_new_connection_event` validates it before accepting. However, there is an error path where the connection is improperly cleaned up. [1](#0-0) 

When `get_trusted_peers` fails at line 334-338, the function logs an error and returns early at line 347 **without calling `self.disconnect(conn)`**. This directly drops the `Connection` object.

In contrast, when connection limits are exceeded, the code properly calls disconnect: [2](#0-1) 

The proper `disconnect` method spawns an async task that explicitly closes the socket with a timeout before dropping: [3](#0-2) 

Without this explicit close, the connection cleanup chain is broken:

1. **Connection struct** has no Drop implementation: [4](#0-3) 

2. **NoiseStream** has no Drop implementation: [5](#0-4) 

3. **TcpSocket** has no Drop implementation and explicitly requires manual close: [6](#0-5) 

The comment explains that TcpStream's close is a no-op, requiring explicit shutdown. When dropped without calling `poll_close`, the TCP write half is not properly shut down, leaving file descriptors open and connections in improper states.

**Trigger Conditions:**
The `get_trusted_peers` call fails when the network_id doesn't exist in the trusted peers HashMap: [7](#0-6) 

This can occur during:
- Node initialization race conditions where network contexts are registered but trusted peers aren't loaded yet
- Configuration errors where a NetworkId is used but not properly initialized
- State transitions during network reconfiguration or shutdown

## Impact Explanation

This issue constitutes **Medium severity** per the Aptos bug bounty criteria:
- **Resource exhaustion**: Leaked file descriptors and memory accumulate over time
- **State inconsistencies**: Remote peers left with dangling connections in CLOSE_WAIT state  
- **Availability impact**: Repeated failures can exhaust system resources, degrading validator performance

Each leaked connection consumes:
- File descriptor on the validator node
- Kernel memory for TCP state
- Application memory in NoiseStream buffers (2x MAX_NOISE_MSG_SIZE â‰ˆ 128KB per connection)
- Remote peer resources holding the connection open

If triggered repeatedly (e.g., during initialization storms or misconfigurations affecting multiple connections), this can lead to:
- File descriptor exhaustion (typical limit: 1024-65536 per process)
- Memory pressure from leaked buffers
- TCP connection table exhaustion
- Validator node performance degradation or failure

## Likelihood Explanation

**Likelihood: Low to Medium**

While not directly exploitable by remote attackers, this bug can trigger in realistic scenarios:

1. **Initialization races**: During node startup, if connections arrive before `PeersAndMetadata` is fully initialized with all network contexts
2. **Configuration errors**: Misconfigured nodes attempting connections on unregistered NetworkIds
3. **Dynamic reconfiguration**: Network topology changes or validator set updates that create timing windows
4. **Repeated failures**: Once triggered, each failed connection leaks resources, compounding the problem

The bug is deterministic - whenever `get_trusted_peers` fails, the leak occurs. The main uncertainty is how often this precondition is met in production deployments.

## Recommendation

**Fix**: Add explicit disconnect call in the error path:

```rust
fn handle_new_connection_event(&mut self, conn: Connection<TSocket>) {
    // Get the trusted peers
    let trusted_peers = match self
        .peers_and_metadata
        .get_trusted_peers(&self.network_context.network_id())
    {
        Ok(trusted_peers) => trusted_peers,
        Err(error) => {
            error!(
                NetworkSchema::new(&self.network_context)
                    .connection_metadata_with_address(&conn.metadata),
                "Failed to get trusted peers for network context: {:?}, error: {:?}",
                self.network_context,
                error
            );
            // FIX: Properly disconnect before returning
            self.disconnect(conn);
            return;
        },
    };
    // ... rest of function
}
```

**Alternative**: Implement Drop for Connection to ensure cleanup:

```rust
impl<TSocket: AsyncWrite + Unpin> Drop for Connection<TSocket> {
    fn drop(&mut self) {
        // Log warning about ungraceful drop
        warn!("Connection dropped without explicit disconnect");
        // Note: Cannot perform async close in sync Drop
        // This serves as a safety check and logging mechanism
    }
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_connection_leak_on_get_trusted_peers_failure() {
    // Setup PeerManager with invalid/missing network configuration
    let mut peer_manager = /* initialize PeerManager */;
    
    // Create a connection that will be leaked
    let (socket, _remote) = create_test_connection().await;
    let connection = Connection {
        socket,
        metadata: ConnectionMetadata::mock(PeerId::random()),
    };
    
    // Track file descriptors before
    let fds_before = get_open_fd_count();
    
    // Trigger the bug: this should fail get_trusted_peers and leak the connection
    peer_manager.handle_new_connection_event(connection);
    
    // Verify file descriptor leaked (not properly closed)
    let fds_after = get_open_fd_count();
    assert!(fds_after > fds_before, "File descriptor leaked");
    
    // Proper cleanup would have closed the socket
}
```

## Notes

This vulnerability directly answers the security question: "When Connection<NoiseStream<T>> is dropped, are socket and metadata properly cleaned up?" The answer is **no** - in the error path at line 347, connections are dropped without proper cleanup, causing resource leaks. While metadata is properly deallocated (it's stack-allocated), the socket resources (file descriptors, TCP state, buffers) leak.

The fix is straightforward and should be applied to ensure all code paths properly call `disconnect()` before dropping connections.

### Citations

**File:** network/framework/src/peer_manager/mod.rs (L331-349)
```rust
    /// Handles a new connection event
    fn handle_new_connection_event(&mut self, conn: Connection<TSocket>) {
        // Get the trusted peers
        let trusted_peers = match self
            .peers_and_metadata
            .get_trusted_peers(&self.network_context.network_id())
        {
            Ok(trusted_peers) => trusted_peers,
            Err(error) => {
                error!(
                    NetworkSchema::new(&self.network_context)
                        .connection_metadata_with_address(&conn.metadata),
                    "Failed to get trusted peers for network context: {:?}, error: {:?}",
                    self.network_context,
                    error
                );
                return;
            },
        };
```

**File:** network/framework/src/peer_manager/mod.rs (L376-387)
```rust
                {
                    info!(
                        NetworkSchema::new(&self.network_context)
                            .connection_metadata_with_address(&conn.metadata),
                        "{} Connection rejected due to connection limit: {}",
                        self.network_context,
                        conn.metadata
                    );
                    counters::connections_rejected(&self.network_context, conn.metadata.origin)
                        .inc();
                    self.disconnect(conn);
                    return;
```

**File:** network/framework/src/peer_manager/mod.rs (L581-605)
```rust
    fn disconnect(&mut self, connection: Connection<TSocket>) {
        let network_context = self.network_context;
        let time_service = self.time_service.clone();

        // Close connection, and drop it
        let drop_fut = async move {
            let mut connection = connection;
            let peer_id = connection.metadata.remote_peer_id;
            if let Err(e) = time_service
                .timeout(TRANSPORT_TIMEOUT, connection.socket.close())
                .await
            {
                warn!(
                    NetworkSchema::new(&network_context)
                        .remote_peer(&peer_id),
                    error = %e,
                    "{} Closing connection with Peer {} failed with error: {}",
                    network_context,
                    peer_id.short_str(),
                    e
                );
            };
        };
        self.executor.spawn(drop_fut);
    }
```

**File:** network/framework/src/transport/mod.rs (L185-191)
```rust
/// The `Connection` struct consists of connection metadata and the actual socket for
/// communication.
#[derive(Debug)]
pub struct Connection<TSocket> {
    pub socket: TSocket,
    pub metadata: ConnectionMetadata,
}
```

**File:** network/framework/src/noise/stream.rs (L33-45)
```rust
#[derive(Debug)]
pub struct NoiseStream<TSocket> {
    /// the socket we write to and read from
    socket: TSocket,
    /// the noise session used to encrypt and decrypt messages
    session: noise::NoiseSession,
    /// handy buffers to write/read
    buffers: Box<NoiseBuffers>,
    /// an enum used for progressively reading a noise payload
    read_state: ReadState,
    /// an enum used for progressively writing a noise payload
    write_state: WriteState,
}
```

**File:** network/netcore/src/transport/tcp.rs (L353-373)
```rust
/// A wrapper around a tokio TcpStream
///
/// In order to properly implement the AsyncRead/AsyncWrite traits we need to wrap a TcpStream to
/// ensure that the "close" method actually closes the write half of the TcpStream.  This is
/// because the "close" method on a TcpStream just performs a no-op instead of actually shutting
/// down the write side of the TcpStream.
//TODO Probably should add some tests for this
#[derive(Debug)]
pub struct TcpSocket {
    inner: Compat<TcpStream>,
}

impl TcpSocket {
    pub fn new(socket: TcpStream) -> Self {
        use tokio_util::compat::TokioAsyncReadCompatExt;

        Self {
            inner: socket.compat(),
        }
    }
}
```

**File:** network/framework/src/application/storage.rs (L329-345)
```rust
    pub fn get_trusted_peers(&self, network_id: &NetworkId) -> Result<PeerSet, Error> {
        let trusted_peers = self.get_trusted_peer_set_for_network(network_id)?;
        Ok(trusted_peers.load().clone().deref().clone())
    }

    /// Returns the trusted peer set for the given network ID
    fn get_trusted_peer_set_for_network(
        &self,
        network_id: &NetworkId,
    ) -> Result<Arc<ArcSwap<PeerSet>>, Error> {
        self.trusted_peers.get(network_id).cloned().ok_or_else(|| {
            Error::UnexpectedError(format!(
                "No trusted peers were found for the given network id: {:?}",
                network_id
            ))
        })
    }
```
