# Audit Report

## Title
SafetyRules Client Reference Leak Across Epochs Leading to Potential Consensus Safety Violation

## Summary
The `execution_client.end_epoch()` function does not properly ensure that SigningPhase and its SafetyRules client reference are fully released before returning. This creates a race condition where SafetyRules clients from the previous epoch can leak into the new epoch, potentially causing state corruption and consensus safety violations when the shared underlying SafetyRules is re-initialized.

## Finding Description

During epoch transitions in `EpochManager::shutdown_current_processor()`, the system calls `execution_client.end_epoch()` to shutdown the execution pipeline and release SafetyRules clients. However, the implementation has a critical synchronization flaw: [1](#0-0) 

The BufferManager's reset acknowledgment is sent **before** the BufferManager task actually exits: [2](#0-1) 

The acknowledgment at line 594 is sent while `process_reset_request` is still executing. After returning from this method, the main loop must still check the stop flag and exit: [3](#0-2) 

Only when this loop exits does the BufferManager get dropped, which then drops the channel senders to pipeline phases, allowing SigningPhase to exit and release the SafetyRules client reference.

The SigningPhase holds the SafetyRules client through an `Arc<dyn CommitSignerProvider>`: [4](#0-3) 

The critical issue is that SafetyRulesManager is created once and persists across epochs: [5](#0-4) 

Each epoch creates new clients via `safety_rules_manager.client()`, but for the Local variant, these clients share the same underlying `Arc<RwLock<SafetyRules>>`: [6](#0-5) 

When a new epoch starts, `perform_initialize()` re-initializes this shared SafetyRules with new epoch state: [7](#0-6) 

**The Race Condition:**
1. Epoch N ends, `end_epoch()` receives acknowledgment from buffer manager
2. `end_epoch()` returns, but SigningPhase task is still running
3. Epoch N+1 starts, calls `perform_initialize()` on the shared SafetyRules
4. Old SigningPhase from epoch N processes in-flight signing request
5. **Result:** Epoch N block signatures may be generated using epoch N+1 safety state, or state corruption if initialization happens during signing [8](#0-7) 

## Impact Explanation

This vulnerability has **HIGH severity** potential impact:

1. **Consensus Safety Violation**: If old SigningPhase signs blocks after SafetyRules is re-initialized for a new epoch, it could produce invalid signatures or violate safety rules designed to prevent equivocation
2. **State Corruption**: Concurrent access to SafetyRules during re-initialization (even with RwLock) could lead to inconsistent safety state across validators
3. **Reference Leak**: Multiple SafetyRules client references exist simultaneously across epochs, causing delayed resource cleanup

While this doesn't directly cause fund loss, it violates the critical "Consensus Safety" invariant that AptosBFT must prevent chain splits under < 1/3 Byzantine failures.

## Likelihood Explanation

**Likelihood: Medium-High**

This race condition occurs during every epoch transition where:
- The buffer manager has in-flight blocks in the signing pipeline
- The new epoch starts before the old pipeline phases finish shutting down
- Given that epoch transitions happen regularly and the timing window exists between acknowledgment and actual task termination, this is likely to occur in production environments

The vulnerability doesn't require any attacker action - it's a protocol-level timing bug that manifests during normal operation.

## Recommendation

Ensure that `end_epoch()` waits for the pipeline phase tasks to actually complete before returning, not just for the acknowledgment. Modify the shutdown sequence to:

1. Send Stop signal to buffer manager
2. Wait for buffer manager task to **complete** (not just acknowledge)
3. Verify all pipeline phase tasks have terminated
4. Only then return from `end_epoch()`

**Suggested fix approach:**

```rust
// In BufferManager, send acknowledgment AFTER the main loop exits
// by restructuring to use a channel that notifies when start() completes

// In ExecutionProxyClient::end_epoch(), add additional synchronization
// to ensure spawned tasks complete before proceeding to the next epoch
```

Alternatively, use separate SafetyRules instances per epoch instead of sharing a single instance, preventing state corruption from re-initialization.

## Proof of Concept

```rust
// Rust test demonstrating the race condition
#[tokio::test]
async fn test_safety_rules_leak_across_epochs() {
    // Setup: Create epoch N with signing phase holding safety rules client
    let safety_rules_manager = SafetyRulesManager::new(&config);
    let safety_rules_n = Arc::new(Mutex::new(
        MetricsSafetyRules::new(safety_rules_manager.client(), storage.clone())
    ));
    
    // Start epoch N with execution client
    execution_client.start_epoch(
        consensus_key.clone(),
        epoch_state_n.clone(),
        safety_rules_n.clone(),
        // ... other params
    ).await;
    
    // Trigger epoch transition
    execution_client.end_epoch().await;
    
    // At this point, end_epoch has returned but SigningPhase may still be running
    // Start epoch N+1 immediately
    let safety_rules_n1 = Arc::new(Mutex::new(
        MetricsSafetyRules::new(safety_rules_manager.client(), storage.clone())
    ));
    
    safety_rules_n1.lock().perform_initialize().unwrap();
    
    // VULNERABILITY: If old SigningPhase signs now, it uses re-initialized SafetyRules
    // Expected: Old phase dropped before new epoch starts
    // Actual: Old phase may still be active, causing state inconsistency
}
```

## Notes

This vulnerability exists in the synchronization logic between epoch transitions and pipeline shutdown. The fix requires ensuring complete cleanup before proceeding to the next epoch, as indicated by the comment at line 669 of epoch_manager.rs which explicitly states the intention to "release the SafetyRule client." [9](#0-8)

### Citations

**File:** consensus/src/pipeline/execution_client.rs (L711-760)
```rust
    async fn end_epoch(&self) {
        let (
            reset_tx_to_rand_manager,
            reset_tx_to_buffer_manager,
            reset_tx_to_secret_share_manager,
        ) = {
            let mut handle = self.handle.write();
            handle.reset()
        };

        if let Some(mut tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop rand manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop rand manager");
        }

        if let Some(mut tx) = reset_tx_to_secret_share_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop secret share manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop secret share manager");
        }

        if let Some(mut tx) = reset_tx_to_buffer_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop buffer manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop buffer manager");
        }
        self.execution_proxy.end_epoch();
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L579-596)
```rust
    async fn process_reset_request(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        info!("Receive reset");

        match signal {
            ResetSignal::Stop => self.stop = true,
            ResetSignal::TargetRound(round) => {
                self.highest_committed_round = round;
                self.latest_round = round;

                let _ = self.drain_pending_commit_proof_till(round);
            },
        }

        self.reset().await;
        let _ = tx.send(ResetAck::default());
        info!("Reset finishes");
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L935-996)
```rust
        while !self.stop {
            // advancing the root will trigger sending requests to the pipeline
            ::tokio::select! {
                Some(blocks) = self.block_rx.next(), if !self.need_back_pressure() => {
                    self.latest_round = blocks.latest_round();
                    monitor!("buffer_manager_process_ordered", {
                    self.process_ordered_blocks(blocks).await;
                    if self.execution_root.is_none() {
                        self.advance_execution_root();
                    }});
                },
                Some(reset_event) = self.reset_rx.next() => {
                    monitor!("buffer_manager_process_reset",
                    self.process_reset_request(reset_event).await);
                },
                Some(response) = self.execution_schedule_phase_rx.next() => {
                    monitor!("buffer_manager_process_execution_schedule_response", {
                    self.process_execution_schedule_response(response).await;
                })},
                Some(response) = self.execution_wait_phase_rx.next() => {
                    monitor!("buffer_manager_process_execution_wait_response", {
                    self.process_execution_response(response).await;
                    self.advance_execution_root();
                    if self.signing_root.is_none() {
                        self.advance_signing_root().await;
                    }});
                },
                Some(response) = self.signing_phase_rx.next() => {
                    monitor!("buffer_manager_process_signing_response", {
                    self.process_signing_response(response).await;
                    self.advance_signing_root().await
                    })
                },
                Some(Ok(round)) = self.persisting_phase_rx.next() => {
                    // see where `need_backpressure()` is called.
                    self.pending_commit_votes = self.pending_commit_votes.split_off(&(round + 1));
                    self.highest_committed_round = round;
                    self.pending_commit_blocks = self.pending_commit_blocks.split_off(&(round + 1));
                },
                Some(rpc_request) = verified_commit_msg_rx.next() => {
                    monitor!("buffer_manager_process_commit_message",
                    if let Some(aggregated_block_id) = self.process_commit_message(rpc_request) {
                        self.advance_head(aggregated_block_id).await;
                        if self.execution_root.is_none() {
                            self.advance_execution_root();
                        }
                        if self.signing_root.is_none() {
                            self.advance_signing_root().await;
                        }
                    });
                }
                _ = interval.tick().fuse() => {
                    monitor!("buffer_manager_process_interval_tick", {
                    self.update_buffer_manager_metrics();
                    self.rebroadcast_commit_votes_if_needed().await
                    });
                },
                // no else branch here because interval.tick will always be available
            }
        }
        info!("Buffer manager stops.");
    }
```

**File:** consensus/src/pipeline/signing_phase.rs (L55-62)
```rust
pub struct SigningPhase {
    safety_rule_handle: Arc<dyn CommitSignerProvider>,
}

impl SigningPhase {
    pub fn new(safety_rule_handle: Arc<dyn CommitSignerProvider>) -> Self {
        Self { safety_rule_handle }
    }
```

**File:** consensus/src/epoch_manager.rs (L209-210)
```rust
        let safety_rules_manager = SafetyRulesManager::new(sr_config);
        let key_storage = safety_rules_manager::storage(sr_config);
```

**File:** consensus/src/epoch_manager.rs (L668-669)
```rust
        // Shutdown the previous buffer manager, to release the SafetyRule client
        self.execution_client.end_epoch().await;
```

**File:** consensus/src/epoch_manager.rs (L826-846)
```rust
        info!(epoch = epoch, "Update SafetyRules");

        let mut safety_rules =
            MetricsSafetyRules::new(self.safety_rules_manager.client(), self.storage.clone());
        match safety_rules.perform_initialize() {
            Err(e) if matches!(e, Error::ValidatorNotInSet(_)) => {
                warn!(
                    epoch = epoch,
                    error = e,
                    "Unable to initialize safety rules.",
                );
            },
            Err(e) => {
                error!(
                    epoch = epoch,
                    error = e,
                    "Unable to initialize safety rules.",
                );
            },
            Ok(()) => (),
        }
```

**File:** consensus/safety-rules/src/safety_rules_manager.rs (L162-173)
```rust
    pub fn client(&self) -> Box<dyn TSafetyRules + Send + Sync> {
        match &self.internal_safety_rules {
            SafetyRulesWrapper::Local(safety_rules) => {
                Box::new(LocalClient::new(safety_rules.clone()))
            },
            SafetyRulesWrapper::Process(process) => Box::new(process.client()),
            SafetyRulesWrapper::Serializer(serializer_service) => {
                Box::new(SerializerClient::new(serializer_service.clone()))
            },
            SafetyRulesWrapper::Thread(thread) => Box::new(thread.client()),
        }
    }
```

**File:** consensus/src/metrics_safety_rules.rs (L40-69)
```rust
    pub fn perform_initialize(&mut self) -> Result<(), Error> {
        let consensus_state = self.consensus_state()?;
        let mut waypoint_version = consensus_state.waypoint().version();
        loop {
            let proofs = self
                .storage
                .retrieve_epoch_change_proof(waypoint_version)
                .map_err(|e| {
                    Error::InternalError(format!(
                        "Unable to retrieve Waypoint state from storage, encountered Error:{}",
                        e
                    ))
                })?;
            // We keep initializing safety rules as long as the waypoint continues to increase.
            // This is due to limits in the number of epoch change proofs that storage can provide.
            match self.initialize(&proofs) {
                Err(Error::WaypointOutOfDate(
                    prev_version,
                    curr_version,
                    current_epoch,
                    provided_epoch,
                )) if prev_version < curr_version => {
                    waypoint_version = curr_version;
                    info!("Previous waypoint version {}, updated version {}, current epoch {}, provided epoch {}", prev_version, curr_version, current_epoch, provided_epoch);
                    continue;
                },
                result => return result,
            }
        }
    }
```
