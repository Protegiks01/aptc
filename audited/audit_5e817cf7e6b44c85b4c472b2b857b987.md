# Audit Report

## Title
Mutex Poisoning in Sharded Block Executor Causes Cascading Failures Across All Shards

## Summary
The sharded block executor uses mutexes in `RemoteStateValue` for cross-shard coordination without proper poison handling. When a panic occurs in any shard thread while holding these mutexes, the mutex becomes poisoned and causes cascading panics in other threads within the same shard. This leads to complete shard failure, which propagates to the coordinator and causes all shards to fail, resulting in total block execution failure and loss of liveness.

## Finding Description
The `RemoteStateValue` struct uses a `Mutex` with `Condvar` to synchronize cross-shard state values between executor threads and the `CrossShardCommitReceiver` thread within each shard. [1](#0-0) 

The critical vulnerability lies in the use of `.unwrap()` on all mutex lock operations without handling `PoisonError`. [2](#0-1) [3](#0-2) 

**Attack Scenario:**

1. **Initial Panic**: During block execution in Shard A, an executor thread calls `get_value()` to read a cross-shard dependency, acquiring the mutex. If ANY panic occurs while the mutex is held (e.g., during `value.clone()`, or due to VM bugs, assertion failures, stack overflow, or OOM), the thread unwinds and the mutex becomes poisoned.

2. **Cascading Failure Within Shard**: The `CrossShardCommitReceiver` thread (running concurrently in the same shard) receives a cross-shard message and attempts to call `set_value()` to update the state. [4](#0-3)  When it tries to acquire the poisoned mutex, `lock().unwrap()` panics because the lock returns `Err(PoisonError)`.

3. **Shard-Level Failure**: Both threads spawned in the rayon scope panic. [5](#0-4)  The scope exits with a panic, causing `execute_transactions_with_dependencies` to fail. The shard thread dies without sending results to the coordinator.

4. **Cross-Shard Propagation**: The coordinator blocks waiting for results from all shards. [6](#0-5)  When Shard A fails to respond (channel closed), the coordinator panics with the message "Did not receive output from shard". This causes complete block execution failure across ALL shards.

This breaks the **State Consistency** invariant (state transitions must be atomic) and the **liveness** guarantee (blocks must be processable).

## Impact Explanation
**High Severity** per Aptos bug bounty criteria:

- **Validator node slowdowns / API crashes**: Complete block execution failure causes validators to stall and be unable to process blocks, directly impacting network liveness.
- **Significant protocol violations**: The failure to handle mutex poisoning violates fault tolerance principles. A single panic in one shard cascades to bring down all shards, making the system extremely brittle.
- **Availability Impact**: Any bug, edge case, or malicious transaction that triggers a panic during cross-shard state access can cause complete execution failure. This affects all validators running sharded execution.

While not achieving Critical severity (no fund loss or permanent chain halt), this represents a significant availability and reliability vulnerability that can be triggered by various execution bugs or potentially crafted malicious inputs.

## Likelihood Explanation
**High Likelihood:**

1. **Multiple Trigger Points**: Panics can occur from VM bugs, assertion failures, edge cases, resource exhaustion (OOM, stack overflow), or malicious transaction inputs that trigger unhandled error paths.

2. **No Defensive Handling**: The code has zero defensive handling for mutex poisoning. Every mutex operation uses `.unwrap()`, guaranteeing that any poison will propagate as a panic. [7](#0-6) 

3. **Concurrent Execution**: The architecture spawns multiple concurrent threads (executor threads + CrossShardCommitReceiver) that share RemoteStateValue mutexes, increasing the window for race conditions and poison scenarios.

4. **Production Impact**: This affects any validator using sharded block execution, which is the high-performance execution mode for Aptos.

## Recommendation
Replace all `.unwrap()` calls on mutex operations with proper poison handling using `into_inner()` or explicit error handling:

```rust
pub fn set_value(&self, value: Option<StateValue>) {
    let (lock, cvar) = &*self.value_condition;
    let mut status = lock.lock().unwrap_or_else(|poisoned| {
        // Recover from poisoning by extracting the inner guard
        warn!("Mutex poisoned in set_value, recovering");
        poisoned.into_inner()
    });
    *status = RemoteValueStatus::Ready(value);
    cvar.notify_all();
}

pub fn get_value(&self) -> Option<StateValue> {
    let (lock, cvar) = &*self.value_condition;
    let mut status = lock.lock().unwrap_or_else(|poisoned| {
        warn!("Mutex poisoned in get_value, recovering");
        poisoned.into_inner()
    });
    while let RemoteValueStatus::Waiting = *status {
        status = cvar.wait(status).unwrap_or_else(|poisoned| {
            warn!("Mutex poisoned during wait, recovering");
            poisoned.into_inner()
        });
    }
    match &*status {
        RemoteValueStatus::Ready(value) => value.clone(),
        RemoteValueStatus::Waiting => unreachable!(),
    }
}

pub fn is_ready(&self) -> bool {
    let (lock, _cvar) = &*self.value_condition;
    let status = lock.lock().unwrap_or_else(|poisoned| poisoned.into_inner());
    matches!(&*status, RemoteValueStatus::Ready(_))
}
```

Additionally, add catch_unwind boundaries in critical execution paths to prevent panics from propagating beyond shard boundaries, similar to how verifier and deserializer are protected in other parts of the codebase.

## Proof of Concept
```rust
#[cfg(test)]
mod mutex_poison_poc {
    use super::*;
    use std::thread;
    use std::panic;
    use std::sync::Arc;
    
    #[test]
    fn test_mutex_poisoning_cascade() {
        let remote_value = Arc::new(RemoteStateValue::waiting());
        let remote_value_clone = remote_value.clone();
        
        // Thread 1: Simulate executor thread that panics while holding mutex
        let handle1 = thread::spawn(move || {
            // This will panic after acquiring the mutex, poisoning it
            let _ = panic::catch_unwind(|| {
                let (lock, _) = &*remote_value_clone.value_condition;
                let _guard = lock.lock().unwrap();
                // Simulate panic during state value clone or other operation
                panic!("Simulated executor panic while holding mutex");
            });
        });
        
        handle1.join().unwrap();
        
        // Thread 2: Simulate CrossShardCommitReceiver trying to set value
        let remote_value_clone2 = remote_value.clone();
        let handle2 = thread::spawn(move || {
            // This should panic due to poisoned mutex
            let result = panic::catch_unwind(|| {
                remote_value_clone2.set_value(Some(StateValue::new_legacy(vec![1,2,3].into())));
            });
            assert!(result.is_err(), "Expected panic due to poisoned mutex");
        });
        
        handle2.join().unwrap();
    }
}
```

This PoC demonstrates that once a panic occurs while holding the RemoteStateValue mutex, all subsequent operations on that mutex will panic, causing cascading failures that propagate through the shard and ultimately to all shards via the coordinator.

### Citations

**File:** aptos-move/aptos-vm/src/sharded_block_executor/remote_state_value.rs (L11-13)
```rust
pub struct RemoteStateValue {
    value_condition: Arc<(Mutex<RemoteValueStatus>, Condvar)>,
}
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/remote_state_value.rs (L22-27)
```rust
    pub fn set_value(&self, value: Option<StateValue>) {
        let (lock, cvar) = &*self.value_condition;
        let mut status = lock.lock().unwrap();
        *status = RemoteValueStatus::Ready(value);
        cvar.notify_all();
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/remote_state_value.rs (L29-39)
```rust
    pub fn get_value(&self) -> Option<StateValue> {
        let (lock, cvar) = &*self.value_condition;
        let mut status = lock.lock().unwrap();
        while let RemoteValueStatus::Waiting = *status {
            status = cvar.wait(status).unwrap();
        }
        match &*status {
            RemoteValueStatus::Ready(value) => value.clone(),
            RemoteValueStatus::Waiting => unreachable!(),
        }
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/remote_state_value.rs (L42-45)
```rust
        let (lock, _cvar) = &*self.value_condition;
        let status = lock.lock().unwrap();
        matches!(&*status, RemoteValueStatus::Ready(_))
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_state_view.rs (L49-56)
```rust
    pub fn set_value(&self, state_key: &StateKey, state_value: Option<StateValue>) {
        self.cross_shard_data
            .get(state_key)
            .unwrap()
            .set_value(state_value);
        // uncomment the following line to debug waiting count
        // trace!("waiting count for shard id {} is {}", self.shard_id, self.waiting_count());
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L134-180)
```rust
        executor_thread_pool.clone().scope(|s| {
            s.spawn(move |_| {
                CrossShardCommitReceiver::start(
                    cross_shard_state_view_clone,
                    cross_shard_client,
                    round,
                );
            });
            s.spawn(move |_| {
                let txn_provider =
                    DefaultTxnProvider::new_without_info(signature_verified_transactions);
                let ret = AptosVMBlockExecutorWrapper::execute_block_on_thread_pool(
                    executor_thread_pool,
                    &txn_provider,
                    aggr_overridden_state_view.as_ref(),
                    // Since we execute blocks in parallel, we cannot share module caches, so each
                    // thread has its own caches.
                    &AptosModuleCacheManager::new(),
                    config,
                    TransactionSliceMetadata::unknown(),
                    cross_shard_commit_sender,
                )
                .map(BlockOutput::into_transaction_outputs_forced);
                if let Some(shard_id) = shard_id {
                    trace!(
                        "executed sub block for shard {} and round {}",
                        shard_id,
                        round
                    );
                    // Send a self message to stop the cross-shard commit receiver.
                    cross_shard_client_clone.send_cross_shard_msg(
                        shard_id,
                        round,
                        CrossShardMsg::StopMsg,
                    );
                } else {
                    trace!("executed block for global shard and round {}", round);
                    // Send a self message to stop the cross-shard commit receiver.
                    cross_shard_client_clone.send_global_msg(CrossShardMsg::StopMsg);
                }
                callback.send(ret).unwrap();
                executor_thread_pool_clone.spawn(move || {
                    // Explicit async drop
                    drop(txn_provider);
                });
            });
        });
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L164-175)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        let _timer = WAIT_FOR_SHARDED_OUTPUT_SECONDS.start_timer();
        trace!("LocalExecutorClient Waiting for results");
        let mut results = vec![];
        for (i, rx) in self.result_rxs.iter().enumerate() {
            results.push(
                rx.recv()
                    .unwrap_or_else(|_| panic!("Did not receive output from shard {}", i))?,
            );
        }
        Ok(results)
    }
```
