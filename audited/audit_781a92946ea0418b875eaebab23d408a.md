# Audit Report

## Title
Consensus Observer Subscription Timeouts Not Reflected in Node Health Checker Metric

## Summary
The node health checker monitors `aptos_consensus_timeout_count` to detect consensus timeouts, but this metric is only incremented by validators participating in consensus rounds. Consensus observer nodes (VFNs and PFNs) use a completely different timeout mechanism for subscription management that increments a separate metric (`consensus_observer_terminated_subscriptions`), creating a monitoring blind spot where observer nodes experiencing severe timeout issues appear healthy to the node checker. [1](#0-0) 

## Finding Description

The node checker hardcodes the metric name `aptos_consensus_timeout_count` and monitors it to detect consensus health issues: [2](#0-1) 

This metric corresponds to the `TIMEOUT_COUNT` counter defined in the consensus layer: [3](#0-2) 

The metric is incremented in exactly one location - when a validator's round state processes a local timeout: [4](#0-3) 

**However, consensus observer nodes follow a completely different architecture:**

Consensus observers (used by VFNs and PFNs) do NOT create a `RoundManager` or `RoundState`. Instead, they use a `ConsensusObserver` that receives ordered blocks via subscriptions: [5](#0-4) 

When consensus observer subscriptions timeout, the timeout is detected via `check_subscription_timeout()`: [6](#0-5) 

These subscription timeouts increment a **different metric** entirely: [7](#0-6) 

**The Result:** When a consensus observer node experiences subscription timeouts (meaning it's not receiving consensus updates from publishers), the `aptos_consensus_timeout_count` metric remains at 0, and the node checker reports the node as healthy, even though the node may be falling behind or serving stale data.

## Impact Explanation

This creates a **monitoring blind spot** affecting all VFNs and PFNs running in consensus observer mode. Per the Aptos bug bounty criteria, this qualifies as **Medium severity** under "State inconsistencies requiring intervention":

1. **Undetected Service Degradation**: Node operators receive false healthy status while their observer nodes are experiencing timeout issues
2. **Stale Data Exposure**: Observer nodes may serve outdated blockchain state to users/applications without operators being alerted
3. **Operational Risk**: Manual intervention required to discover and remediate issues that should be automatically detected

The node checker's TODO comment acknowledges the risk of metric name changes but not this architectural mismatch: [8](#0-7) 

## Likelihood Explanation

This issue affects **all consensus observer deployments** in normal operations:

- **Affected Nodes**: All VFNs (Validator Full Nodes) and PFNs (Public Full Nodes) running with `consensus_observer.observer_enabled = true`
- **Trigger Condition**: Any subscription timeout due to network issues, publisher problems, or synchronization delays
- **Detection**: Current monitoring systems will NOT alert operators to these timeout conditions

The architectural split between validator consensus (using `RoundState` + `TIMEOUT_COUNT`) and observer subscriptions (using `ConsensusObserver` + `OBSERVER_TERMINATED_SUBSCRIPTIONS`) is by design, but the node checker was not updated to account for this.

## Recommendation

**Option 1 - Update Node Checker to Monitor Both Metrics:**

Modify the checker to distinguish between validator and observer nodes, and monitor the appropriate metric for each:

```rust
// For validators: monitor aptos_consensus_timeout_count
// For observers: monitor consensus_observer_terminated_subscriptions with label "subscription_timeout"
```

**Option 2 - Create Unified Timeout Metric:**

Have consensus observers also increment `TIMEOUT_COUNT` when subscription timeouts occur, ensuring the node checker works for all node types without modification.

**Option 3 - Deprecate Metric-Based Checking:**

Move to a type-safe metrics API that would catch these discrepancies at compile time, as suggested by the TODO comment.

## Proof of Concept

The following demonstrates the architectural mismatch:

**Step 1:** Deploy a consensus observer node (VFN configuration with `observer_enabled: true`)

**Step 2:** Trigger a subscription timeout by:
- Disconnecting the observer from publisher nodes for longer than `max_subscription_timeout_ms` (default 15 seconds)
- Or advancing mock time in a test environment: [9](#0-8) 

**Step 3:** Query metrics and observe:
- `aptos_consensus_timeout_count` = 0 (node checker sees this)
- `consensus_observer_terminated_subscriptions{termination_label="subscription_timeout"}` > 0 (actual timeout counter)

**Step 4:** Run node checker, observe it reports healthy status despite subscription timeouts.

## Notes

This architectural mismatch exists because:
1. Validators use `RoundState::process_local_timeout()` which is part of the consensus protocol
2. Observers don't participate in consensus rounds and use subscription-based synchronization
3. The node checker was designed when only the validator model existed

The consensus observer metrics module contains no timeout-specific counters: [10](#0-9) 

This confirms that observer timeout monitoring was never integrated with the existing node health checking infrastructure.

### Citations

**File:** ecosystem/node-checker/src/checker/consensus_timeouts.rs (L22-25)
```rust
// TODO: When we have it, switch to using a crate that unifies metric names.
// As it is now, this metric name could change and we'd never catch it here
// at compile time.
const METRIC: &str = "aptos_consensus_timeout_count";
```

**File:** consensus/src/counters.rs (L666-672)
```rust
/// Count the number of timeouts a node experienced since last restart (close to 0 in happy path).
/// This count is different from `TIMEOUT_ROUNDS_COUNT`, because not every time a node has
/// a timeout there is an ultimate decision to move to the next round (it might take multiple
/// timeouts to get the timeout certificate).
pub static TIMEOUT_COUNT: Lazy<IntCounter> = Lazy::new(|| {
    register_int_counter!("aptos_consensus_timeout_count", "Count the number of timeouts a node experienced since last restart (close to 0 in happy path).").unwrap()
});
```

**File:** consensus/src/liveness/round_state.rs (L233-241)
```rust
    pub fn process_local_timeout(&mut self, round: Round) -> bool {
        if round != self.current_round {
            return false;
        }
        warn!(round = round, "Local timeout");
        counters::TIMEOUT_COUNT.inc();
        self.setup_timeout(1);
        true
    }
```

**File:** consensus/src/consensus_provider.rs (L150-185)
```rust
    // If the consensus observer is enabled, create the execution client.
    // If not, stub it out with a dummy client.
    let execution_client = if node_config.consensus_observer.observer_enabled {
        // Create the execution proxy
        let txn_notifier = Arc::new(MempoolNotifier::new(
            consensus_to_mempool_sender.clone(),
            node_config.consensus.mempool_executed_txn_timeout_ms,
        ));
        let execution_proxy = ExecutionProxy::new(
            Arc::new(BlockExecutor::<AptosVMBlockExecutor>::new(aptos_db.clone())),
            txn_notifier,
            state_sync_notifier,
            node_config.transaction_filters.execution_filter.clone(),
            node_config.consensus.enable_pre_commit,
            None,
        );

        // Create the execution proxy client
        let bounded_executor =
            BoundedExecutor::new(32, consensus_observer_runtime.handle().clone());
        let rand_storage = Arc::new(RandDb::new(node_config.storage.dir()));
        let execution_proxy_client = Arc::new(ExecutionProxyClient::new(
            node_config.consensus.clone(),
            Arc::new(execution_proxy),
            AccountAddress::ONE,
            self_sender.clone(),
            consensus_network_client,
            bounded_executor,
            rand_storage.clone(),
            node_config.consensus_observer,
            consensus_publisher.clone(),
        ));
        execution_proxy_client as Arc<dyn TExecutionClient>
    } else {
        Arc::new(DummyExecutionClient) as Arc<dyn TExecutionClient>
    };
```

**File:** consensus/src/consensus_observer/observer/subscription.rs (L164-179)
```rust
    /// Verifies that the subscription has not timed out based
    /// on the last received message time.
    fn check_subscription_timeout(&self) -> Result<(), Error> {
        // Calculate the duration since the last message
        let time_now = self.time_service.now();
        let duration_since_last_message = time_now.duration_since(self.last_message_receive_time);

        // Check if the subscription has timed out
        if duration_since_last_message
            > Duration::from_millis(self.consensus_observer_config.max_subscription_timeout_ms)
        {
            return Err(Error::SubscriptionTimeout(format!(
                "Subscription to peer: {} has timed out! No message received for: {:?}",
                self.peer_network_id, duration_since_last_message
            )));
        }
```

**File:** consensus/src/consensus_observer/common/metrics.rs (L1-40)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

#![allow(clippy::unwrap_used)]

use aptos_config::network_id::{NetworkId, PeerNetworkId};
use aptos_metrics_core::{
    exponential_buckets, register_histogram_vec, register_int_counter, register_int_counter_vec,
    register_int_gauge_vec, HistogramVec, IntCounter, IntCounterVec, IntGaugeVec,
};
use once_cell::sync::Lazy;

// Useful observer metric labels
pub const BLOCK_PAYLOAD_LABEL: &str = "block_payload";
pub const COMMIT_DECISION_LABEL: &str = "commit_decision";
pub const COMMITTED_BLOCKS_LABEL: &str = "committed_blocks";
pub const CREATED_SUBSCRIPTION_LABEL: &str = "created_subscription";
pub const ORDERED_BLOCK_ENTRIES_LABEL: &str = "ordered_block_entries";
pub const ORDERED_BLOCK_LABEL: &str = "ordered_block";
pub const ORDERED_BLOCK_WITH_WINDOW_LABEL: &str = "ordered_block_with_window";
pub const PENDING_BLOCK_ENTRIES_BY_HASH_LABEL: &str = "pending_block_by_hash_entries";
pub const PENDING_BLOCK_ENTRIES_LABEL: &str = "pending_block_entries";
pub const PENDING_BLOCKS_BY_HASH_LABEL: &str = "pending_blocks_by_hash";
pub const PENDING_BLOCKS_LABEL: &str = "pending_blocks";
pub const STORED_PAYLOADS_LABEL: &str = "stored_payloads";

// Useful state sync metric labels
pub const STATE_SYNCING_FOR_FALLBACK: &str = "sync_for_fallback";
pub const STATE_SYNCING_TO_COMMIT: &str = "sync_to_commit";

/// Counter for tracking created subscriptions for the consensus observer
pub static OBSERVER_CREATED_SUBSCRIPTIONS: Lazy<IntCounterVec> = Lazy::new(|| {
    register_int_counter_vec!(
        "consensus_observer_created_subscriptions",
        "Counters for created subscriptions for consensus observer",
        &["creation_label", "network_id"]
    )
    .unwrap()
});

```

**File:** consensus/src/consensus_observer/common/metrics.rs (L199-207)
```rust
/// Counter for tracking terminated subscriptions for the consensus observer
pub static OBSERVER_TERMINATED_SUBSCRIPTIONS: Lazy<IntCounterVec> = Lazy::new(|| {
    register_int_counter_vec!(
        "consensus_observer_terminated_subscriptions",
        "Counters for terminated subscriptions for consensus observer",
        &["termination_label", "network_id"]
    )
    .unwrap()
});
```

**File:** consensus/src/consensus_observer/observer/subscription_manager.rs (L522-535)
```rust
        // Elapse time to simulate a timeout for peer 1
        let mock_time_service = time_service.into_mock();
        mock_time_service.advance(Duration::from_millis(
            consensus_observer_config.max_subscription_timeout_ms + 1,
        ));

        // Check and manage the subscriptions
        subscription_manager
            .check_and_manage_subscriptions()
            .await
            .unwrap();

        // Verify that the first subscription was terminated
        verify_active_subscription_peers(&subscription_manager, vec![connected_peer_2]);
```
