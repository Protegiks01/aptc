# Audit Report

## Title
Database Iterator Exhaustion Masks Corruption Leading to State Sync Availability Degradation

## Summary
When database iterators return None prematurely due to corruption or inconsistencies, the storage service logs a warning but continues processing, returning incomplete transaction proofs without error. This masks database corruption and can cause state sync liveness failures if corruption is widespread across the network.

## Finding Description

In `get_transactions_with_proof_by_size()`, when the multizip iterator combining transaction, transaction_info, events, and persisted_auxiliary_info iterators returns None early, the code logs a warning but continues to create a response with partial data: [1](#0-0) 

The function then creates a valid cryptographic proof for whatever partial data was fetched and returns it successfully: [2](#0-1) 

**Root Cause - Database Pruning Architecture:**

The database uses parallel sub-pruners that commit independently, creating windows where tables can be inconsistent: [3](#0-2) 

Each sub-pruner (TransactionPruner, TransactionInfoPruner, EventStorePruner, etc.) commits atomically but independently: [4](#0-3) 

**How Iterator Exhaustion Occurs:**

The `ContinuousVersionIter` returns None when the underlying database iterator returns None, without error: [5](#0-4) 

**Attack Scenarios:**

1. **Natural Database Corruption**: Disk failures, crashes during writes, or bugs in pruning can create missing data ranges where transaction_info exists but transactions don't (or vice versa)

2. **Malicious Full Node**: A node operator could intentionally corrupt specific version ranges to degrade state sync performance for connected clients

3. **Pruning Race Conditions**: If TransactionInfoPruner succeeds but TransactionPruner fails and isn't properly retried, permanent inconsistencies arise

**Client-Side Handling:**

While clients detect incomplete responses and create follow-up requests: [6](#0-5) 

The problem is that corrupted nodes continue serving incomplete data repeatedly, and clients cannot distinguish corruption from legitimate size-based truncation.

## Impact Explanation

**Severity: HIGH** per Aptos bug bounty criteria:
- **Validator node slowdowns**: Repeated retries due to corrupted nodes degrade sync performance
- **Significant protocol violations**: Database corruption is silently tolerated instead of failing fast

**Specific Impacts:**
1. **State Sync Liveness Degradation**: Clients repeatedly retry corrupted nodes, slowing sync
2. **Operational Blindness**: Corrupted nodes continue operating without alerting operators
3. **Network-Wide Risk**: If corruption affects multiple nodes (e.g., from a common bug), state sync can fail broadly
4. **Poor User Experience**: New nodes experience slow/failing sync without clear error messages

This breaks the **State Consistency** invariant: state transitions must be atomic and verifiable. While the proof is technically valid for the partial data returned, serving incomplete ranges without error violates the integrity of the state sync protocol.

## Likelihood Explanation

**Medium-to-High Likelihood:**

1. **Natural Occurrence**: Database corruption from disk failures, system crashes, and out-of-memory conditions happens in production systems
2. **Pruning Complexity**: The parallel independent sub-pruner architecture creates opportunities for inconsistencies
3. **No Detection Mechanism**: The current warning-only approach means corruption can persist undetected
4. **Cascading Effect**: Once one node is corrupted, it can spread degradation to clients connecting to it

The sub-pruner implementation shows each commits independently, and if one fails after another succeeds, the error handling retries but the successful pruner's changes persist, creating potential for permanent inconsistencies.

## Recommendation

**Return an error instead of logging a warning when iterators exhaust prematurely:**

```rust
None => {
    // Iterator exhausted prematurely - database corruption or inconsistency
    return Err(Error::UnexpectedErrorEncountered(format!(
        "Database inconsistency detected: iterators exhausted at version {:?}. \
         Expected {:?} transactions but only fetched {:?}. \
         This indicates corruption in transaction storage tables.",
        start_version + transactions.len() as u64,
        num_transactions_to_fetch,
        transactions.len()
    )));
},
```

**Additional Improvements:**

1. **Add database consistency checks** between related tables during startup and periodically
2. **Implement cross-table transaction** or two-phase commit for pruning operations
3. **Add health check endpoint** that validates database consistency
4. **Track and alert** on repeated premature iterator exhaustion
5. **Fail-stop behavior** for corrupted nodes to prevent serving bad data

## Proof of Concept

**Rust Test Demonstrating the Vulnerability:**

```rust
#[test]
fn test_premature_iterator_exhaustion_returns_success() {
    // Setup: Create a mock database with missing transaction_info entries
    // at versions 1050-1100 while transactions exist for 1000-1200
    
    let mock_db = create_corrupted_mock_db();
    let storage_reader = StorageReader::new(
        StorageServiceConfig::default(),
        Arc::new(mock_db),
        TimeService::mock(),
    );
    
    // Request transactions 1000-1200 (should span the corrupted range)
    let result = storage_reader.get_transactions_with_proof(
        1200, // proof_version
        1000, // start_version  
        1200, // end_version
        false, // include_events
    );
    
    // BUG: This should return an error but instead succeeds
    assert!(result.is_ok(), "Should succeed despite corruption");
    
    let response = result.unwrap();
    let txn_list = response.transaction_list_with_proof.unwrap();
    let num_txns = txn_list.get_num_transactions();
    
    // Only 50 transactions returned instead of 201 due to corruption
    assert_eq!(num_txns, 50);
    
    // The warning is logged but no error surfaced:
    // "The iterators for transactions, transaction infos, events and 
    //  persisted auxiliary infos are missing data! Start version: 1000,
    //  end version: 1200, num transactions to fetch: 201, num fetched: 50."
    
    // Client will retry for versions 1050-1200 and get stuck in loop
}

fn create_corrupted_mock_db() -> MockDatabase {
    // Implementation: Create DB with transactions for 1000-1200
    // but transaction_infos only for 1000-1049
    // This simulates corruption where one table is missing data
}
```

**Notes:**
- The vulnerability requires database corruption which can occur from disk failures, crashes, or pruning bugs
- Client-side detection exists but cannot distinguish corruption from legitimate truncation
- The key issue is silent failure allowing corrupted nodes to continue serving
- Fix requires returning errors instead of warnings to enable fail-fast behavior

### Citations

**File:** state-sync/storage-service/server/src/storage.rs (L457-469)
```rust
                None => {
                    // Log a warning that the iterators did not contain all the expected data
                    warn!(
                        "The iterators for transactions, transaction infos, events and \
                        persisted auxiliary infos are missing data! Start version: {:?}, \
                        end version: {:?}, num transactions to fetch: {:?}, num fetched: {:?}.",
                        start_version,
                        end_version,
                        num_transactions_to_fetch,
                        transactions.len()
                    );
                    break;
                },
```

**File:** state-sync/storage-service/server/src/storage.rs (L473-510)
```rust
        // Create the transaction info list with proof
        let accumulator_range_proof = self.storage.get_transaction_accumulator_range_proof(
            start_version,
            transactions.len() as u64,
            proof_version,
        )?;
        let info_list_with_proof =
            TransactionInfoListWithProof::new(accumulator_range_proof, transaction_infos);

        // Create the transaction list with proof
        let transaction_events = if include_events {
            Some(transaction_events)
        } else {
            None
        };
        let transaction_list_with_proof = TransactionListWithProof::new(
            transactions,
            transaction_events,
            Some(start_version),
            info_list_with_proof,
        );

        // Update the data truncation metrics
        response_progress_tracker
            .update_data_truncation_metrics(DataResponse::get_transactions_with_proof_v2_label());

        // Create the transaction data with proof response
        let transaction_list_with_proof_v2 =
            TransactionListWithProofV2::new(TransactionListWithAuxiliaryInfos::new(
                transaction_list_with_proof,
                persisted_auxiliary_infos,
            ));
        let response = TransactionDataWithProofResponse {
            transaction_data_response_type: TransactionDataResponseType::TransactionData,
            transaction_list_with_proof: Some(transaction_list_with_proof_v2),
            transaction_output_list_with_proof: None,
        };
        Ok(response)
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L78-84)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_info_pruner.rs (L25-33)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        TransactionInfoDb::prune(current_progress, target_version, &mut batch)?;
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::TransactionInfoPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
        self.ledger_db.transaction_info_db().write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/utils/iterators.rs (L40-62)
```rust
    fn next_impl(&mut self) -> Result<Option<T>> {
        if self.expected_next_version >= self.end_version {
            return Ok(None);
        }

        let ret = match self.inner.next().transpose()? {
            Some((version, transaction)) => {
                ensure!(
                    version == self.expected_next_version,
                    "{} iterator: first version {}, expecting version {}, got {} from underlying iterator.",
                    std::any::type_name::<T>(),
                    self.first_version,
                    self.expected_next_version,
                    version,
                );
                self.expected_next_version += 1;
                Some(transaction)
            },
            None => None,
        };

        Ok(ret)
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L1151-1184)
```rust
fn create_missing_transactions_request(
    request: &TransactionsWithProofRequest,
    response_payload: &ResponsePayload,
) -> Result<Option<DataClientRequest>, Error> {
    // Determine the number of requested transactions
    let num_requested_transactions = request
        .end_version
        .checked_sub(request.start_version)
        .and_then(|v| v.checked_add(1))
        .ok_or_else(|| {
            Error::IntegerOverflow("Number of requested transactions has overflown!".into())
        })?;

    // Identify the missing data if the request was not satisfied
    match response_payload {
        ResponsePayload::TransactionsWithProof(transactions_with_proof) => {
            // Check if the request was satisfied
            let num_received_transactions = transactions_with_proof.get_num_transactions() as u64;
            if num_received_transactions < num_requested_transactions {
                let start_version = request
                    .start_version
                    .checked_add(num_received_transactions)
                    .ok_or_else(|| Error::IntegerOverflow("Start version has overflown!".into()))?;
                Ok(Some(DataClientRequest::TransactionsWithProof(
                    TransactionsWithProofRequest {
                        start_version,
                        end_version: request.end_version,
                        proof_version: request.proof_version,
                        include_events: request.include_events,
                    },
                )))
            } else {
                Ok(None) // The request was satisfied!
            }
```
