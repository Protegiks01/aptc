# Audit Report

## Title
Missing Proof Size Validation in State Snapshot Backup Enables Storage Exhaustion

## Summary
The `write_chunk()` function in the state snapshot backup process lacks validation of proof data size when copying from the backup service to storage. This allows unexpectedly large or malformed proofs to exhaust backup storage and cause memory exhaustion during restore operations.

## Finding Description

The vulnerability exists in the proof data copying operation within `StateSnapshotBackupController::write_chunk()`: [1](#0-0) 

This code uses `tokio::io::copy` to stream proof data directly from the backup service to a file without any size validation. The expected proof structure is `SparseMerkleRangeProof`, which should contain at most 256 `HashValue` siblings: [2](#0-1) 

The legitimate maximum size for a serialized `SparseMerkleRangeProof` is approximately 8,196 bytes (256 siblings Ã— 32 bytes + BCS overhead). However, the current implementation places no upper bound on the data written to the proof file.

**Attack Propagation Path:**

1. **Proof Generation:** The backup service calls `get_account_state_range_proof()`: [3](#0-2) 

2. **BCS Serialization:** The proof is serialized via BCS without inherent size limits on vector lengths.

3. **Unchecked Streaming:** The backup client streams this data without validation: [4](#0-3) 

4. **Storage Exhaustion:** With thousands of state chunks, each potentially writing oversized proof files, backup storage can be exhausted.

5. **Memory Exhaustion on Restore:** During restoration, proof files are loaded entirely into memory before validation: [5](#0-4) 

**Vulnerable Scenarios:**

- **Database Corruption:** Corrupted Jellyfish Merkle Tree state could cause proof generation to produce malformed proofs with excessive siblings
- **Backup Service Bug:** A bug in `JellyfishMerkleTree::get_range_proof()` could generate invalid proofs
- **Malicious Operator:** Compromised node operator could modify backup service to return arbitrary data

While proof verification eventually occurs during restore, it happens AFTER loading the entire file into memory: [6](#0-5) 

## Impact Explanation

This vulnerability enables **Medium Severity** impacts per Aptos bug bounty criteria:

1. **Storage Exhaustion (Backup Phase):** A state snapshot backup with thousands of chunks could write gigabytes of invalid proof data, exhausting backup storage and preventing successful backups.

2. **Memory Exhaustion (Restore Phase):** During restoration, concurrent loading of oversized proof files into memory via `read_all()` could trigger OOM conditions, causing restore failures and requiring manual intervention.

3. **Operational Disruption:** Failed backups compromise disaster recovery capabilities; failed restores prevent node bootstrapping or state synchronization.

This qualifies as "State inconsistencies requiring intervention" under Medium severity criteria, though it doesn't directly impact live consensus operations or validator availability.

## Likelihood Explanation

**Likelihood: Low-Medium**

This vulnerability requires one of the following preconditions:
- Database corruption in the Jellyfish Merkle Tree implementation
- A bug in proof generation code paths
- Compromise of the backup service endpoint

The backup service typically runs on localhost-only access (`http://localhost:6186` by default), limiting direct external exploitation. However, the lack of defensive validation means that ANY bug in the proof generation pipeline would be amplified across potentially thousands of chunks, turning a localized error into a system-wide backup failure.

## Recommendation

Add size validation before copying proof data:

```rust
async fn write_chunk(
    &self,
    backup_handle: &BackupHandleRef,
    chunk: Chunk,
) -> Result<StateSnapshotChunk> {
    // ... existing code ...
    
    let (proof_handle, mut proof_file) = self
        .storage
        .create_for_write(backup_handle, &Self::chunk_proof_name(first_idx, last_idx))
        .await?;
    
    // Add size validation
    const MAX_PROOF_SIZE: u64 = 10_000; // ~10KB allows overhead beyond theoretical max
    let mut proof_stream = self
        .client
        .get_account_range_proof(last_key, self.version())
        .await?;
    
    let bytes_copied = tokio::io::copy(&mut proof_stream.take(MAX_PROOF_SIZE), &mut proof_file).await?;
    ensure!(
        bytes_copied < MAX_PROOF_SIZE,
        "Proof size {} exceeds maximum expected size {}",
        bytes_copied,
        MAX_PROOF_SIZE
    );
    
    proof_file.shutdown().await?;
    // ... rest of function ...
}
```

Additionally, add early size validation during restore before loading into memory:

```rust
async fn load_bcs_file<T: DeserializeOwned>(&self, file_handle: &FileHandleRef) -> Result<T> {
    let mut file = self.open_for_read(file_handle).await?;
    let metadata = file.metadata().await?;
    ensure!(
        metadata.len() <= MAX_PROOF_SIZE,
        "Proof file size {} exceeds maximum {}",
        metadata.len(),
        MAX_PROOF_SIZE
    );
    let mut bytes = Vec::new();
    file.read_to_end(&mut bytes).await?;
    Ok(bcs::from_bytes(&bytes)?)
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tokio::io::AsyncWriteExt;
    
    #[tokio::test]
    async fn test_oversized_proof_detection() {
        // Simulate backup service returning oversized proof
        let oversized_proof_data = vec![0u8; 1_000_000]; // 1MB instead of ~8KB
        
        // Current implementation would write this without validation
        let mut file = tokio::fs::File::create("/tmp/test_proof").await.unwrap();
        tokio::io::copy(
            &mut oversized_proof_data.as_slice(),
            &mut file
        ).await.unwrap();
        
        // Verify the issue: file is much larger than expected
        let metadata = tokio::fs::metadata("/tmp/test_proof").await.unwrap();
        assert!(metadata.len() > 100_000); // Demonstrates the problem
        
        // With fix: should fail
        let result = tokio::io::copy(
            &mut oversized_proof_data.as_slice().take(10_000),
            &mut file
        ).await;
        assert!(result.is_err() || result.unwrap() == 10_000);
    }
}
```

## Notes

This vulnerability represents a defense-in-depth failure where the backup system trusts data from the backup service without validation. While the backup service is typically trusted infrastructure, defensive programming requires validating data sizes to prevent amplification of bugs or corruption in upstream components. The fix is straightforward and adds minimal performance overhead while preventing potential operational failures.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/backup.rs (L429-437)
```rust
        tokio::io::copy(
            &mut self
                .client
                .get_account_range_proof(last_key, self.version())
                .await?,
            &mut proof_file,
        )
        .await?;
        proof_file.shutdown().await?;
```

**File:** types/src/proof/definition.rs (L335-341)
```rust
        ensure!(
            self.siblings.len() + root_depth <= HashValue::LENGTH_IN_BITS,
            "Sparse Merkle Tree proof has more than {} ({} + {}) siblings.",
            HashValue::LENGTH_IN_BITS,
            root_depth,
            self.siblings.len(),
        );
```

**File:** types/src/proof/definition.rs (L763-767)
```rust
pub struct SparseMerkleRangeProof {
    /// The vector of siblings on the right of the path from root to last leaf. The ones near the
    /// bottom are at the beginning of the vector. In the above example, it's `[X, h]`.
    right_siblings: Vec<HashValue>,
}
```

**File:** storage/aptosdb/src/backup/backup_handler.rs (L165-172)
```rust
    pub fn get_account_state_range_proof(
        &self,
        rightmost_key: HashValue,
        version: Version,
    ) -> Result<SparseMerkleRangeProof> {
        self.state_store
            .get_value_range_proof(rightmost_key, version)
    }
```

**File:** storage/backup/backup-cli/src/utils/backup_service_client.rs (L95-102)
```rust
    pub async fn get_account_range_proof(
        &self,
        key: HashValue,
        version: Version,
    ) -> Result<impl AsyncRead + use<>> {
        self.get("state_range_proof", &format!("{}/{:x}", version, key))
            .await
    }
```

**File:** storage/backup/backup-cli/src/utils/storage_ext.rs (L24-29)
```rust
    async fn read_all(&self, file_handle: &FileHandleRef) -> Result<Vec<u8>> {
        let mut file = self.open_for_read(file_handle).await?;
        let mut bytes = Vec::new();
        file.read_to_end(&mut bytes).await?;
        Ok(bytes)
    }
```
