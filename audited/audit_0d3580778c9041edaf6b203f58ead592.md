# Audit Report

## Title
Infinite Loop in Error Handler Prevents Graceful Indexer Shutdown and Causes Resource Exhaustion

## Summary
The `TransactionProcessingError` error handling mechanism contains a critical design flaw where error recovery attempts infinitely retry database connection acquisition, causing the indexer to hang indefinitely during shutdown or database outages and preventing proper resource cleanup.

## Finding Description

When a `TransactionProcessingError` occurs during transaction processing, the error handling pathway attempts to log the error status to the database. However, this error logging mechanism itself can fail catastrophically during shutdown or database unavailability scenarios.

The vulnerability exists in the error handling flow:

1. When `process_transactions()` fails, it returns a `TransactionProcessingError` [1](#0-0) 

2. The `process_transactions_with_status()` method catches this error and calls `update_status_err()` [2](#0-1) 

3. `update_status_err()` attempts to write the error to the database via `apply_processor_status()` [3](#0-2) 

4. **Critical flaw**: `apply_processor_status()` calls `get_conn()` which contains an **infinite retry loop** with no timeout or cancellation mechanism [4](#0-3) 

5. Additionally, if the connection succeeds but the database write fails, the code panics during error handling [5](#0-4) 

During shutdown or database connection pool exhaustion:
- The `get_conn()` method loops forever waiting for a connection that will never come
- The indexer thread becomes blocked indefinitely
- The panic in `runtime.rs` that should handle the error is never reached [6](#0-5) 
- Database connections remain unreleased
- No graceful shutdown occurs

The indexer runtime has no cancellation token or shutdown signal mechanism: [7](#0-6) 

## Impact Explanation

This qualifies as **High Severity** under the Aptos bug bounty program criteria for "Validator node slowdowns" and "API crashes":

**Operational Impact:**
- Indexer nodes hang indefinitely during shutdown attempts
- Database connection pool exhaustion leads to cascading failures
- Recovery requires forceful process termination (SIGKILL)
- Incomplete cleanup may corrupt indexer state requiring full re-indexing

**Resource Exhaustion:**
- Database connections leak and remain held indefinitely
- Thread pool resources are consumed by blocked tasks
- Multiple concurrent errors can exhaust all processor task slots [8](#0-7) 

**Availability Impact:**
- Indexer services become unresponsive
- API endpoints depending on indexed data fail
- Monitoring and alerting systems cannot detect clean vs. dirty shutdown

## Likelihood Explanation

**High Likelihood** during operational scenarios:

1. **Database maintenance windows**: Routine PostgreSQL maintenance causes temporary unavailability
2. **Connection pool saturation**: High load can exhaust the connection pool
3. **Network partitions**: Database connectivity issues trigger the vulnerability
4. **Graceful shutdown attempts**: Normal shutdown procedures trigger the hang
5. **Error cascades**: Initial database errors compound as error handlers fail

The infinite loop in `get_conn()` guarantees this will occur whenever database connectivity is lost during error handling. No attacker action is requiredâ€”this is a latent operational bug.

## Recommendation

Implement bounded retry with timeout and graceful degradation:

**Solution 1: Add timeout to get_conn()**
```rust
fn get_conn(&self) -> Result<PgPoolConnection, PoolError> {
    let pool = self.connection_pool();
    let max_retries = 3;
    let mut retry_count = 0;
    
    loop {
        match pool.get() {
            Ok(conn) => {
                GOT_CONNECTION.inc();
                return Ok(conn);
            },
            Err(err) => {
                UNABLE_TO_GET_CONNECTION.inc();
                retry_count += 1;
                if retry_count >= max_retries {
                    aptos_logger::error!(
                        "Failed to get DB connection after {} retries: {:?}",
                        max_retries, err
                    );
                    return Err(err);
                }
                aptos_logger::warn!(
                    "Could not get DB connection, retry {}/{}: {:?}",
                    retry_count, max_retries, err
                );
                std::thread::sleep(std::time::Duration::from_secs(1));
            },
        };
    }
}
```

**Solution 2: Make error logging best-effort**
```rust
fn apply_processor_status(&self, psms: &[ProcessorStatusModel]) {
    match self.get_conn() {
        Ok(mut conn) => {
            let chunks = get_chunks(psms.len(), ProcessorStatusModel::field_count());
            for (start_ind, end_ind) in chunks {
                if let Err(e) = execute_with_better_error(/* ... */) {
                    aptos_logger::error!(
                        "Failed to update processor status (non-fatal): {:?}", e
                    );
                    // Don't panic during error handling
                }
            }
        },
        Err(e) => {
            aptos_logger::error!(
                "Could not get connection to log error status (non-fatal): {:?}", e
            );
            // Fail gracefully - error logging is best-effort
        }
    }
}
```

**Solution 3: Add shutdown signal**
Add a `CancellationToken` to allow graceful shutdown: [9](#0-8) 

## Proof of Concept

**Reproduction Steps:**

1. Start an Aptos indexer with PostgreSQL backend
2. While indexer is processing transactions, cause a database error (e.g., shutdown PostgreSQL or exhaust connection pool)
3. Observe indexer thread hangs in `get_conn()` infinite loop
4. Attempt graceful shutdown (SIGTERM) - indexer does not exit
5. Database connections remain held, preventing cleanup

**Rust Test Scenario:**
```rust
#[tokio::test]
async fn test_error_handler_hang_during_shutdown() {
    // Setup indexer with connection pool
    let (pool, tailer) = setup_indexer().await.unwrap();
    
    // Simulate connection pool exhaustion by acquiring all connections
    let mut held_connections = vec![];
    for _ in 0..pool.max_size() {
        held_connections.push(pool.get().unwrap());
    }
    
    // Trigger a transaction processing error
    // This will hang in update_status_err() -> get_conn() forever
    let result = tailer.processor
        .process_transactions_with_status(vec![invalid_txn])
        .await;
    
    // Test will timeout - demonstrating infinite hang
    // Expected: Should return error or timeout gracefully
    // Actual: Hangs indefinitely in get_conn() loop
}
```

**Notes**

This vulnerability represents a violation of the error handling principle that **error recovery mechanisms must not themselves fail catastrophically**. The infinite retry loop in the error handler creates a deadlock scenario during database unavailability, preventing proper resource cleanup and graceful shutdown.

The root cause is architectural: error logging is treated as a critical operation that must succeed, when it should be best-effort. During shutdown or outage scenarios, the error handler becomes the problem rather than the solution, blocking the main error handling pathway in `runtime.rs` from executing its panic and cleanup logic.

While the indexer is not part of consensus and cannot directly impact blockchain safety, this bug significantly impacts operational reliability and availability of indexer services, which are critical infrastructure for applications and users querying blockchain state.

### Citations

**File:** crates/indexer/src/processors/default_processor.rs (L617-622)
```rust
            Err(err) => Err(TransactionProcessingError::TransactionCommitError((
                anyhow::Error::from(err),
                start_version,
                end_version,
                self.name(),
            ))),
```

**File:** crates/indexer/src/indexer/transaction_processor.rs (L45-63)
```rust
    fn get_conn(&self) -> PgPoolConnection {
        let pool = self.connection_pool();
        loop {
            match pool.get() {
                Ok(conn) => {
                    GOT_CONNECTION.inc();
                    return conn;
                },
                Err(err) => {
                    UNABLE_TO_GET_CONNECTION.inc();
                    aptos_logger::error!(
                        "Could not get DB connection from pool, will retry in {:?}. Err: {:?}",
                        pool.connection_timeout(),
                        err
                    );
                },
            };
        }
    }
```

**File:** crates/indexer/src/indexer/transaction_processor.rs (L86-90)
```rust
        match res.as_ref() {
            Ok(processing_result) => self.update_status_success(processing_result),
            Err(tpe) => self.update_status_err(tpe),
        };
        res
```

**File:** crates/indexer/src/indexer/transaction_processor.rs (L134-143)
```rust
    fn update_status_err(&self, tpe: &TransactionProcessingError) {
        aptos_logger::debug!(
            "[{}] Marking processing version Err: {:?}",
            self.name(),
            tpe
        );
        PROCESSOR_ERRORS.with_label_values(&[self.name()]).inc();
        let psm = ProcessorStatusModel::from_transaction_processing_err(tpe);
        self.apply_processor_status(&psm);
    }
```

**File:** crates/indexer/src/indexer/transaction_processor.rs (L150-164)
```rust
            execute_with_better_error(
                &mut conn,
                diesel::insert_into(processor_statuses::table)
                    .values(&psms[start_ind..end_ind])
                    .on_conflict((dsl::name, dsl::version))
                    .do_update()
                    .set((
                        dsl::success.eq(excluded(dsl::success)),
                        dsl::details.eq(excluded(dsl::details)),
                        dsl::last_updated.eq(excluded(dsl::last_updated)),
                    )),
                None,
            )
            .expect("Error updating Processor Status!");
        }
```

**File:** crates/indexer/src/runtime.rs (L92-101)
```rust
    runtime.spawn(async move {
        let context = Arc::new(Context::new(
            chain_id,
            db,
            mp_sender,
            node_config,
            None, /* table info reader */
        ));
        run_forever(indexer_config, context).await;
    });
```

**File:** crates/indexer/src/runtime.rs (L210-219)
```rust
        let mut tasks = vec![];
        for _ in 0..processor_tasks {
            let other_tailer = tailer.clone();
            let task = tokio::spawn(async move { other_tailer.process_next_batch().await });
            tasks.push(task);
        }
        let batches = match futures::future::try_join_all(tasks).await {
            Ok(res) => res,
            Err(err) => panic!("Error processing transaction batches: {:?}", err),
        };
```

**File:** crates/indexer/src/runtime.rs (L230-243)
```rust
                Some(Err(tpe)) => {
                    let (err, start_version, end_version, _) = tpe.inner();
                    error!(
                        processor_name = processor_name,
                        start_version = start_version,
                        end_version = end_version,
                        error =? err,
                        "Error processing batch!"
                    );
                    panic!(
                        "Error in '{}' while processing batch: {:?}",
                        processor_name, err
                    );
                },
```

**File:** aptos-move/aptos-workspace-server/src/lib.rs (L1-50)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

//! This library runs and manages a set of services that makes up a local Aptos network.
//! - node
//!     - node API
//!     - indexer grpc
//! - faucet
//! - indexer
//!     - postgres db
//!     - processors
//!     - indexer API
//!
//! The services are bound to unique OS-assigned ports to allow for multiple local networks
//! to operate simultaneously, enabling testing and development in isolated environments.
//!
//! ## Key Features:
//! - Shared Futures
//!     - The code makes extensive use of shared futures across multiple services,
//!       ensuring orderly startup while maximizing parallel execution.
//! - Graceful Shutdown
//!     - When a `Ctrl-C` signal is received or if any of the services fail to start
//!       or exit unexpectedly, the system attempts to gracefully shut down all services,
//!       cleaning up resources like Docker containers, volumes and networks.

mod common;
mod services;

use anyhow::{anyhow, Context, Result};
use aptos_localnet::docker::get_docker;
use clap::Parser;
use common::make_shared;
use futures::TryFutureExt;
use services::{
    docker_common::create_docker_network_permanent, indexer_api::start_indexer_api,
    processors::start_all_processors,
};
use std::time::Duration;
use tokio::io::{AsyncBufReadExt, BufReader};
use tokio_util::sync::CancellationToken;
use uuid::Uuid;

#[derive(Parser)]
enum ControlCommand {
    Stop,
}

/// Starts an async task that reads and processes commands from stdin.
///
/// Currently there is only one command:
```
