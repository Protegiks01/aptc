# Audit Report

## Title
Resource Exhaustion via Unbounded Blocking Task Queue in Consensus Observer Publisher

## Summary
The `max_parallel_serialization_tasks` configuration parameter in `ConsensusObserverConfig` lacks validation, allowing it to be set to extremely large values (e.g., 10,000). This causes unbounded memory growth in Tokio's blocking thread pool queue, leading to memory exhaustion and validator node unresponsiveness, breaking consensus participation.

## Finding Description

The consensus observer publisher uses `max_parallel_serialization_tasks` to control parallel message serialization [1](#0-0) . This parameter is used with the `buffered()` stream combinator to allow concurrent serialization futures [2](#0-1) .

Each serialization task spawns a blocking task using `tokio::task::spawn_blocking` [3](#0-2) . The Aptos runtime limits the blocking thread pool to only 64 threads [4](#0-3) .

Critically, `ConsensusObserverConfig` does NOT implement the `ConfigSanitizer` trait, and there is no validation for `max_parallel_serialization_tasks` [5](#0-4) . The parameter can be set to any `usize` value through the YAML configuration.

**Attack Flow:**
1. A validator has many subscribers (e.g., 100+ VFNs/PFNs)
2. Consensus frequently publishes messages via `publish_message()` [6](#0-5) 
3. Each publish creates N channel entries (one per subscriber) [7](#0-6) 
4. With `max_parallel_serialization_tasks=10000`, up to 10,000 `spawn_blocking` tasks are submitted
5. Only 64 threads available; remaining 9,936 tasks queue in Tokio's **unbounded** blocking pool queue
6. Each queued task holds consensus message data (blocks, transactions, proofs)
7. Memory exhaustion occurs as the queue grows unboundedly
8. Serialization delays increase from milliseconds to minutes
9. Validator cannot send consensus messages timely, becoming unresponsive

This breaks **Invariant #9 (Resource Limits)**: All operations must respect computational limits.

## Impact Explanation

**High Severity** per Aptos Bug Bounty criteria: **Validator node slowdowns**

When a validator becomes unresponsive due to memory exhaustion and message delays:
- Cannot participate effectively in consensus rounds
- Misses voting deadlines, reducing network liveness
- If multiple validators are affected, could impact consensus participation threshold
- Network performance degrades significantly

The impact is amplified by:
- High message frequency in Aptos consensus (~10 messages/second)
- Large message sizes (blocks with transactions can be hundreds of KB)
- Multiple subscribers multiplying the effect
- Unbounded queue growth in Tokio's blocking pool

## Likelihood Explanation

**Medium-to-High Likelihood:**

**Prerequisites:**
- Validator operator access to modify configuration (trusted role)
- OR system compromise to modify config files

**However**, this vulnerability has two concerning aspects:

1. **Configuration Mistake:** A validator operator might accidentally set an unreasonably high value, believing higher parallelism improves performance, without understanding the resource implications.

2. **Compromised Validator:** If a validator's system is compromised through any other vulnerability, an attacker could modify this configuration to cause a subtle DoS that appears as performance degradation rather than an obvious attack.

The lack of validation means there's no safety net preventing this misconfiguration, even by accident.

## Recommendation

Implement `ConfigSanitizer` for `ConsensusObserverConfig` with reasonable bounds:

```rust
impl ConfigSanitizer for ConsensusObserverConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let config = &node_config.consensus_observer;
        
        // Validate max_parallel_serialization_tasks
        const MAX_REASONABLE_PARALLELISM: usize = 128;
        if config.max_parallel_serialization_tasks > MAX_REASONABLE_PARALLELISM {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                format!(
                    "max_parallel_serialization_tasks ({}) exceeds maximum ({})",
                    config.max_parallel_serialization_tasks,
                    MAX_REASONABLE_PARALLELISM
                ),
            ));
        }
        
        // Validate max_network_channel_size
        const MAX_REASONABLE_CHANNEL_SIZE: u64 = 5000;
        if config.max_network_channel_size > MAX_REASONABLE_CHANNEL_SIZE {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                format!(
                    "max_network_channel_size ({}) exceeds maximum ({})",
                    config.max_network_channel_size,
                    MAX_REASONABLE_CHANNEL_SIZE
                ),
            ));
        }
        
        Ok(())
    }
}
```

Then add to `NodeConfig::sanitize()`:
```rust
ConsensusObserverConfig::sanitize(node_config, node_type, chain_id)?;
```

## Proof of Concept

**Configuration File (`validator.yaml`):**
```yaml
consensus_observer:
  publisher_enabled: true
  max_parallel_serialization_tasks: 10000
  max_network_channel_size: 10000
```

**Reproduction Steps:**

1. Deploy a validator node with the above malicious configuration
2. Set up 100+ subscriber nodes (VFNs/PFNs) that subscribe to this validator
3. Let consensus run normally for a few minutes
4. Monitor validator memory usage and message latency metrics
5. Observe:
   - Memory usage growing continuously (unbounded queue)
   - `PUBLISHER_SENT_MESSAGES` latency increasing from <10ms to >1000ms
   - `spawn_blocking` queue depth growing via tokio-console
   - Validator missing consensus votes due to delayed message sends

**Expected Behavior:** Memory exhaustion within 10-30 minutes depending on available RAM, followed by OOM kill or severe degradation making the validator unresponsive.

**Notes:**
- This vulnerability exists because `ConsensusObserverConfig` lacks input validation [8](#0-7) 
- The blocking thread pool limit provides partial protection but cannot prevent unbounded queue growth [9](#0-8) 
- The bounded channel upstream provides limited protection but is also configurable without validation [10](#0-9)

### Citations

**File:** config/src/config/consensus_observer_config.rs (L19-61)
```rust
#[derive(Clone, Copy, Debug, Deserialize, PartialEq, Serialize)]
#[serde(default, deny_unknown_fields)]
pub struct ConsensusObserverConfig {
    /// Whether the consensus observer is enabled
    pub observer_enabled: bool,
    /// Whether the consensus publisher is enabled
    pub publisher_enabled: bool,

    /// Maximum number of pending network messages
    pub max_network_channel_size: u64,
    /// Maximum number of parallel serialization tasks for message sends
    pub max_parallel_serialization_tasks: usize,
    /// Timeout (in milliseconds) for network RPC requests
    pub network_request_timeout_ms: u64,

    /// Interval (in milliseconds) to garbage collect peer state
    pub garbage_collection_interval_ms: u64,
    /// Maximum number of blocks to keep in memory (e.g., pending blocks, ordered blocks, etc.)
    pub max_num_pending_blocks: u64,
    /// Interval (in milliseconds) to check progress of the consensus observer
    pub progress_check_interval_ms: u64,

    /// The maximum number of concurrent subscriptions
    pub max_concurrent_subscriptions: u64,
    /// Maximum timeout (in milliseconds) we'll wait for the synced version to
    /// increase before terminating the active subscription.
    pub max_subscription_sync_timeout_ms: u64,
    /// Maximum message timeout (in milliseconds) for active subscriptions
    pub max_subscription_timeout_ms: u64,
    /// Interval (in milliseconds) to check for subscription related peer changes
    pub subscription_peer_change_interval_ms: u64,
    /// Interval (in milliseconds) to refresh the subscription
    pub subscription_refresh_interval_ms: u64,

    /// Duration (in milliseconds) to require state sync to synchronize when in fallback mode
    pub observer_fallback_duration_ms: u64,
    /// Duration (in milliseconds) we'll wait on startup before considering fallback mode
    pub observer_fallback_startup_period_ms: u64,
    /// Duration (in milliseconds) we'll wait for syncing progress before entering fallback mode
    pub observer_fallback_progress_threshold_ms: u64,
    /// Duration (in milliseconds) of acceptable sync lag before entering fallback mode
    pub observer_fallback_sync_lag_threshold_ms: u64,
}
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L57-59)
```rust
        let max_network_channel_size = consensus_observer_config.max_network_channel_size as usize;
        let (outbound_message_sender, outbound_message_receiver) =
            mpsc::channel(max_network_channel_size);
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L217-221)
```rust
        for peer_network_id in &active_subscribers {
            // Send the message to the outbound receiver for publishing
            let mut outbound_message_sender = self.outbound_message_sender.clone();
            if let Err(error) =
                outbound_message_sender.try_send((*peer_network_id, message.clone()))
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L293-298)
```rust
                tokio::task::spawn_blocking(move || {
                    let message_label = message.get_label();
                    let serialized_message = consensus_observer_client_clone
                        .serialize_message_for_peer(&peer_network_id, message);
                    (peer_network_id, serialized_message, message_label)
                })
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L303-304)
```rust
        serialization_task
            .buffered(consensus_observer_config.max_parallel_serialization_tasks)
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-50)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;

    // Verify the given name has an appropriate length
    if thread_name.len() > MAX_THREAD_NAME_LENGTH {
        panic!(
            "The given runtime thread name is too long! Max length: {}, given name: {}",
            MAX_THREAD_NAME_LENGTH, thread_name
        );
    }

    // Create the runtime builder
    let atomic_id = AtomicUsize::new(0);
    let thread_name_clone = thread_name.clone();
    let mut builder = Builder::new_multi_thread();
    builder
        .thread_name_fn(move || {
            let id = atomic_id.fetch_add(1, Ordering::SeqCst);
            format!("{}-{}", thread_name_clone, id)
        })
        .on_thread_start(on_thread_start)
        .disable_lifo_slot()
        // Limit concurrent blocking tasks from spawn_blocking(), in case, for example, too many
        // Rest API calls overwhelm the node.
        .max_blocking_threads(MAX_BLOCKING_THREADS)
```

**File:** config/src/config/config_sanitizer.rs (L39-70)
```rust
impl ConfigSanitizer for NodeConfig {
    fn sanitize(
        node_config: &NodeConfig,
        node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        // If config sanitization is disabled, don't do anything!
        if node_config.node_startup.skip_config_sanitizer {
            return Ok(());
        }

        // Sanitize all of the sub-configs
        AdminServiceConfig::sanitize(node_config, node_type, chain_id)?;
        ApiConfig::sanitize(node_config, node_type, chain_id)?;
        BaseConfig::sanitize(node_config, node_type, chain_id)?;
        ConsensusConfig::sanitize(node_config, node_type, chain_id)?;
        DagConsensusConfig::sanitize(node_config, node_type, chain_id)?;
        ExecutionConfig::sanitize(node_config, node_type, chain_id)?;
        sanitize_failpoints_config(node_config, node_type, chain_id)?;
        sanitize_fullnode_network_configs(node_config, node_type, chain_id)?;
        IndexerGrpcConfig::sanitize(node_config, node_type, chain_id)?;
        InspectionServiceConfig::sanitize(node_config, node_type, chain_id)?;
        LoggerConfig::sanitize(node_config, node_type, chain_id)?;
        MempoolConfig::sanitize(node_config, node_type, chain_id)?;
        NetbenchConfig::sanitize(node_config, node_type, chain_id)?;
        StateSyncConfig::sanitize(node_config, node_type, chain_id)?;
        StorageConfig::sanitize(node_config, node_type, chain_id)?;
        InternalIndexerDBConfig::sanitize(node_config, node_type, chain_id)?;
        sanitize_validator_network_config(node_config, node_type, chain_id)?;

        Ok(()) // All configs passed validation
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L405-405)
```rust
            consensus_publisher.publish_message(message);
```
