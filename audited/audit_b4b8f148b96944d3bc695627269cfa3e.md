# Audit Report

## Title
State KV Pruner Progress Desynchronization Leading to Unbounded Stale Data Accumulation

## Summary
The StateKvPruner's metadata pruner commits its progress to the database before shard pruners execute their deletions. If shard pruners fail after the metadata pruner succeeds, the metadata progress remains committed while stale data is left unpruned. The pruner has no recovery mechanism to retry failed ranges, causing indefinite accumulation of stale data and eventual storage exhaustion.

## Finding Description

The vulnerability exists in the coordination between `StateKvMetadataPruner` and `StateKvShardPruner` components when sharding is enabled.

**Execution Flow:** [1](#0-0) 

The main pruning loop first calls the metadata pruner, then the shard pruners in parallel. [2](#0-1) 

In the sharding path (lines 35-50), the metadata pruner iterates through shards but performs **no deletions**. It then unconditionally updates the progress marker to `target_version` (lines 67-70) and commits this to the metadata database (line 72). [3](#0-2) 

The actual deletion work is performed by individual shard pruners, which run AFTER the metadata pruner has already committed its progress.

**The Critical Flaw:**

The metadata DB and shard DBs are separate RocksDB instances with no transactional coordination: [4](#0-3) 

When a shard pruner fails after the metadata pruner succeeds:
1. Metadata DB progress is already committed to `target_version`
2. Shard pruner error propagates and is logged
3. The pruner worker catches the error and retries: [5](#0-4) 

4. On the next iteration, `self.progress()` reads from the metadata DB, which now shows `target_version`
5. The pruner skips the failed range entirely since metadata progress indicates it was already pruned
6. Stale data in shards remains unpruned forever

**Why Initialization Doesn't Help:** [6](#0-5) 

Reconciliation only occurs during initialization. During normal operation, if metadata progress advances past shard progress due to a failure, there is no mechanism to detect or recover from this inconsistency.

This violates the **State Consistency** invariant: the system maintains separate progress markers that can permanently desynchronize, leaving stale data that will never be pruned.

## Impact Explanation

This is a **High Severity** vulnerability per Aptos bug bounty criteria:

1. **Validator Node Slowdowns**: Accumulating stale data degrades database query performance, causing validators to lag in block processing
2. **State Inconsistencies Requiring Intervention**: The metadata progress marker diverges from actual shard state, requiring manual database inspection and potentially hardfork-level intervention to fix
3. **Storage Exhaustion**: Unbounded stale data growth eventually fills disk space, causing validator crashes
4. **Significant Protocol Violation**: The pruning subsystem's guarantee that "stale data older than X is removed" is violated

The impact is system-wide: all validators running with sharding enabled experience the issue when transient failures occur.

## Likelihood Explanation

**High Likelihood**:

1. **Transient failures are common**: Disk I/O errors, temporary resource exhaustion, database lock contention occur regularly in production systems
2. **No special access required**: Any condition causing shard DB write failures triggers the bug
3. **Permanent damage**: Once triggered, the desynchronization is permanent with no self-recovery
4. **Affects default configuration**: Sharding is the default configuration for production deployments

The vulnerability is triggered by normal operational conditions, not targeted attacks. It will occur eventually on any long-running validator node experiencing transient storage issues.

## Recommendation

Implement two-phase commit for pruner progress updates:

1. **Defer metadata progress update**: Only update metadata progress AFTER all shard pruners succeed
2. **Track minimum shard progress**: Maintain metadata progress as `min(all_shard_progress)`
3. **Add reconciliation checks**: Periodically verify metadata progress matches all shard progress
4. **Implement atomic progress tracking**: Use a distributed transaction mechanism or write-ahead log

**Example fix** for `StateKvPruner::prune()`:

```rust
// First run all shard pruners
THREAD_MANAGER.get_background_pool().install(|| {
    self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
        shard_pruner.prune(progress, current_batch_target_version)
            .map_err(|err| anyhow!("Failed to prune state kv shard {}: {err}", shard_pruner.shard_id()))
    })
})?;

// Only update metadata progress after all shards succeed
self.metadata_pruner.prune(progress, current_batch_target_version)?;
```

Additionally, modify `StateKvMetadataPruner::prune()` sharding path to perform actual validation or remove it entirely if it serves no purpose.

## Proof of Concept

```rust
#[test]
fn test_pruner_progress_desync_on_shard_failure() {
    use std::sync::{Arc, Mutex};
    use anyhow::Result;
    
    // Setup: Create StateKvDb with sharding enabled
    let state_kv_db = setup_test_state_kv_db_with_sharding();
    
    // Populate with stale data at versions 1-100
    populate_stale_data(&state_kv_db, 1, 100);
    
    // Create pruner
    let pruner = StateKvPruner::new(Arc::new(state_kv_db)).unwrap();
    pruner.set_target_version(100);
    
    // Inject failure into shard 0 after metadata pruner succeeds
    let fail_flag = Arc::new(Mutex::new(false));
    let fail_flag_clone = fail_flag.clone();
    
    // Mock shard pruner to fail
    inject_shard_failure(&pruner, 0, move || {
        *fail_flag_clone.lock().unwrap()
    });
    
    // Trigger pruning - metadata pruner will succeed, shard pruner will fail
    *fail_flag.lock().unwrap() = true;
    let result = pruner.prune(100);
    assert!(result.is_err()); // First attempt fails
    
    // Verify metadata progress updated despite failure
    let metadata_progress = pruner.metadata_pruner.progress().unwrap();
    assert_eq!(metadata_progress, 100); // Metadata thinks it's done
    
    // Clear failure flag and retry
    *fail_flag.lock().unwrap() = false;
    let result = pruner.prune(100);
    assert!(result.is_ok()); // Second attempt "succeeds"
    
    // Verify stale data still exists in shard 0
    let shard_db = pruner.state_kv_db.db_shard(0);
    let stale_count = count_stale_entries(&shard_db, 1, 100);
    assert!(stale_count > 0, "Stale data should remain but found none"); // BUG: Data unpruned
    
    // Verify pruner will never retry this range
    pruner.set_target_version(200);
    let result = pruner.prune(100);
    assert!(result.is_ok());
    
    let stale_count_after = count_stale_entries(&shard_db, 1, 100);
    assert_eq!(stale_count, stale_count_after); // Range 1-100 never retried
}
```

The test demonstrates that after a shard failure, metadata progress advances while stale data remains, and the pruner never retries the failed range.

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L49-86)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_pruner__prune"]);

        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning state kv data."
            );
            self.metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state kv shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning state kv data is done.");
        }

        Ok(target_version)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs (L28-73)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();

        if self.state_kv_db.enabled_sharding() {
            let num_shards = self.state_kv_db.num_shards();
            // NOTE: This can be done in parallel if it becomes the bottleneck.
            for shard_id in 0..num_shards {
                let mut iter = self
                    .state_kv_db
                    .db_shard(shard_id)
                    .iter::<StaleStateValueIndexByKeyHashSchema>()?;
                iter.seek(&current_progress)?;
                for item in iter {
                    let (index, _) = item?;
                    if index.stale_since_version > target_version {
                        break;
                    }
                }
            }
        } else {
            let mut iter = self
                .state_kv_db
                .metadata_db()
                .iter::<StaleStateValueIndexSchema>()?;
            iter.seek(&current_progress)?;
            for item in iter {
                let (index, _) = item?;
                if index.stale_since_version > target_version {
                    break;
                }
                batch.delete::<StaleStateValueIndexSchema>(&index)?;
                batch.delete::<StateValueSchema>(&(index.state_key, index.version))?;
            }
        }

        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        self.state_kv_db.metadata_db().write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L25-45)
```rust
    pub(in crate::pruner) fn new(
        shard_id: usize,
        db_shard: Arc<DB>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
            metadata_progress,
        )?;
        let myself = Self { shard_id, db_shard };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up state kv shard {shard_id}."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L47-72)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();

        let mut iter = self
            .db_shard
            .iter::<StaleStateValueIndexByKeyHashSchema>()?;
        iter.seek(&current_progress)?;
        for item in iter {
            let (index, _) = item?;
            if index.stale_since_version > target_version {
                break;
            }
            batch.delete::<StaleStateValueIndexByKeyHashSchema>(&index)?;
            batch.delete::<StateValueByKeyHashSchema>(&(index.state_key_hash, index.version))?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
            &DbMetadataValue::Version(target_version),
        )?;

        self.db_shard.write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/state_kv_db.rs (L44-51)
```rust
pub struct StateKvDb {
    state_kv_metadata_db: Arc<DB>,
    state_kv_db_shards: [Arc<DB>; NUM_STATE_SHARDS],
    // TODO(HotState): no separate metadata db for hot state for now.
    #[allow(dead_code)] // TODO(HotState): can remove later.
    hot_state_kv_db_shards: Option<[Arc<DB>; NUM_STATE_SHARDS]>,
    enabled_sharding: bool,
}
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L53-69)
```rust
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
    }
```
