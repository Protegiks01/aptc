# Audit Report

## Title
Time-of-Check to Time-of-Use Race Condition in MemoryRatelimitChecker Day Detection

## Summary
The `MemoryRatelimitChecker.clear_if_new_day()` method contains a race condition where `get_current_time_secs()` is called twice non-atomically, potentially causing inconsistent day detection and allowing multiple concurrent cache clears with request interleaving.

## Finding Description

The `MemoryRatelimitChecker` uses an atomic `current_day` field to track daily rate limits, but the day detection logic contains a critical TOCTOU (Time-of-Check to Time-of-Use) vulnerability. [1](#0-0) 

The vulnerability exists in the `clear_if_new_day()` method where:

1. **Line 54**: `get_current_time_secs()` is called to compute the current day for comparison
2. **Line 55**: The atomic `current_day` is loaded with `Relaxed` ordering
3. **Line 58**: `get_current_time_secs()` is called **AGAIN** to compute the day value for storage

This creates two distinct race conditions:

### Race Condition #1: Inconsistent Time Reads
Between the two calls to `get_current_time_secs()`, the system clock can advance, causing:
- Comparison uses day value from time T1
- Storage uses day value from time T2 (potentially T1 + 1 day in extreme cases)

### Race Condition #2: Multiple Concurrent Day Transitions
With `Ordering::Relaxed`, multiple threads can simultaneously:
1. Read the old `current_day` value (e.g., day 99)
2. See that current time indicates day 100 > 99
3. All enter the if-block concurrently
4. All attempt to store day 100 and clear the cache
5. Multiple cache clears occur with requests potentially interleaving between them

**Attack Scenario:**
```
T0 (23:59:59.900): Thread A enters clear_if_new_day(), computes day 99, no clear
T1 (00:00:00.001): Thread B enters clear_if_new_day(), computes day 100 > 99, enters if
T2 (00:00:00.002): Thread C enters clear_if_new_day(), reads current_day=99 (Relaxed), enters if
T3 (00:00:00.003): Thread B stores day 100, locks mutex, clears cache
T4 (00:00:00.004): Attacker request D enters check(), increments IP counter to 1
T5 (00:00:00.005): Thread C locks mutex, clears cache AGAIN (wiping attacker's count)
T6 (00:00:00.006): Attacker request E enters check(), sees counter=0, allowed
```

This violates the rate limiting invariant: an IP should be limited to `max_requests_per_day` requests per day, but the attacker can exceed this by timing requests between multiple cache clears.

In contrast, the Redis-based implementation correctly calls `get_current_time_secs()` only once: [2](#0-1) 

## Impact Explanation

**Severity Assessment: Does NOT meet High/Medium criteria**

While this is a real race condition, its impact is limited to the **faucet service only**:

1. **No Core Blockchain Impact**: The faucet distributes test tokens and is not part of consensus, state management, execution, or governance
2. **Limited Financial Impact**: Only affects test token distribution, not real funds
3. **No Validator Impact**: Does not affect validator nodes, consensus, or network availability
4. **No API Crash**: The service continues running; only rate limits are bypassed

Per the Aptos bug bounty criteria:
- ❌ Not "Loss of Funds" (test tokens have no value)
- ❌ Not "Consensus/Safety violations" 
- ❌ Not "Validator node slowdowns"
- ❌ Not "API crashes"
- ❌ Not "State inconsistencies" (blockchain state unaffected)

The faucet is an auxiliary service, not a core blockchain component. This issue affects service fairness but not blockchain security.

## Likelihood Explanation

**Likelihood: Medium to High (but impact remains limited)**

The race condition will occur:
- **Guaranteed**: At every day boundary (midnight) when multiple requests arrive concurrently
- **Window**: Microseconds to milliseconds during Relaxed atomic operations
- **Attacker Control**: High - attacker can send concurrent requests at midnight
- **Precision Required**: Moderate - just needs concurrent requests at day boundary

However, the practical exploitation yield is low: attacker might gain a few extra test token requests beyond the daily limit.

## Recommendation

**Fix: Use single time read with consistent value**

```rust
async fn clear_if_new_day(&self) {
    let current_day_value = days_since_tap_epoch(get_current_time_secs());
    
    if current_day_value > self.current_day.load(std::sync::atomic::Ordering::Relaxed) {
        // Use compare-and-swap to ensure only one thread performs the clear
        if self.current_day.compare_exchange(
            self.current_day.load(std::sync::atomic::Ordering::Relaxed),
            current_day_value,
            std::sync::atomic::Ordering::Release,
            std::sync::atomic::Ordering::Relaxed,
        ).is_ok() {
            self.ip_to_requests_today.lock().await.clear();
        }
    }
}
```

**Alternative: Use stronger atomic ordering**

Replace `Ordering::Relaxed` with `Ordering::Acquire` (load) and `Ordering::Release` (store) to provide proper synchronization between threads.

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::Arc;
    use std::time::Duration;
    use tokio;

    #[tokio::test]
    async fn test_day_boundary_race_condition() {
        let config = MemoryRatelimitCheckerConfig {
            max_requests_per_day: 5,
            max_entries_in_map: NonZeroUsize::new(1000).unwrap(),
        };
        
        let checker = Arc::new(MemoryRatelimitChecker::new(config));
        let mut handles = vec![];
        
        // Simulate day boundary: spawn multiple threads that check simultaneously
        for i in 0..10 {
            let checker_clone = checker.clone();
            let handle = tokio::spawn(async move {
                checker_clone.clear_if_new_day().await;
                println!("Thread {} completed clear check", i);
            });
            handles.push(handle);
        }
        
        // Wait for all threads
        for handle in handles {
            handle.await.unwrap();
        }
        
        // Verify: with race condition, cache may have been cleared multiple times
        // and current_day may have inconsistent value
        println!("Current day value: {}", checker.current_day.load(std::sync::atomic::Ordering::Relaxed));
    }
}
```

---

## Notes

**Critical Clarification on Severity:**

This vulnerability, while technically valid as a race condition, **does NOT meet the severity thresholds** for the Aptos blockchain bug bounty program because:

1. **Scope Limitation**: The faucet is explicitly an auxiliary test token distribution service, not a core blockchain component (consensus/execution/storage/governance/staking)

2. **Impact Limitation**: Only affects rate limiting fairness for test tokens; does not impact real funds, consensus safety, or network availability

3. **Bug Bounty Context**: The prompt's focus areas are "consensus, execution, storage, governance, and staking" - the faucet is none of these

The race condition exists and is exploitable, but its practical security impact on the Aptos blockchain is **minimal**. This would be more appropriately classified as a **code quality issue** or **service fairness bug** rather than a blockchain security vulnerability meeting High severity criteria.

### Citations

**File:** crates/aptos-faucet/core/src/checkers/memory_ratelimit.rs (L53-63)
```rust
    async fn clear_if_new_day(&self) {
        if days_since_tap_epoch(get_current_time_secs())
            > self.current_day.load(std::sync::atomic::Ordering::Relaxed)
        {
            self.current_day.store(
                days_since_tap_epoch(get_current_time_secs()),
                std::sync::atomic::Ordering::Relaxed,
            );
            self.ip_to_requests_today.lock().await.clear();
        }
    }
```

**File:** crates/aptos-faucet/core/src/checkers/redis_ratelimit.rs (L186-200)
```rust
    fn get_key_and_secs_until_next_day(
        &self,
        ratelimit_key_prefix: &str,
        ratelimit_key_value: &str,
    ) -> (String, u64) {
        let now_secs = get_current_time_secs();
        let seconds_until_next_day = seconds_until_next_day(now_secs);
        let key = format!(
            "{}:{}:{}",
            ratelimit_key_prefix,
            ratelimit_key_value,
            days_since_tap_epoch(now_secs)
        );
        (key, seconds_until_next_day)
    }
```
