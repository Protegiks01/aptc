# Audit Report

## Title
Configuration Validation Bypass: Missing Input Validation on `shared_mempool_tick_interval_ms` Enables CPU Exhaustion DoS

## Summary
The `MempoolConfig` struct lacks input validation for the `shared_mempool_tick_interval_ms` field, allowing it to be set to 0 via YAML configuration. This creates a tight loop in the broadcast scheduler that continuously executes broadcasts without delay, causing CPU exhaustion and validator node degradation.

## Finding Description

The vulnerability exists across multiple files in the mempool broadcast scheduling system:

**1. Missing Validation in Configuration Sanitizer** [1](#0-0) 

The `ConfigSanitizer::sanitize()` implementation for `MempoolConfig` is empty and returns `Ok(())` without performing any validation, despite a TODO comment indicating that validations should be added. This allows the `shared_mempool_tick_interval_ms` field to be set to any `u64` value, including 0.

**2. Configuration Loading from YAML** [2](#0-1) 

The `MempoolConfig` struct is marked with `#[derive(Deserialize)]` and `#[serde(default)]`, allowing all fields to be loaded from YAML configuration files without validation. [3](#0-2) 

The field `shared_mempool_tick_interval_ms` is defined as a `u64` with a default value of 10ms, but can be overridden to 0 in node configuration YAML.

**3. Broadcast Scheduling with Zero Delay** [4](#0-3) 

When `execute_broadcast` completes, it schedules the next broadcast using either `shared_mempool_tick_interval_ms` (normal mode) or `shared_mempool_backoff_interval_ms` (backoff mode). If `shared_mempool_tick_interval_ms` is 0, the interval becomes 0ms.

**4. Immediate Future Resolution** [5](#0-4) 

The `ScheduledBroadcast::new()` constructor creates a future with deadline = `Instant::now() + Duration::from_millis(interval_ms)`. When `interval_ms` is 0, the deadline is essentially `Instant::now()`, and the sleep operation at line 137-145 either doesn't spawn (if deadline <= now) or sleeps for 0 duration. [6](#0-5) 

The `poll()` implementation checks if `Instant::now() < self.deadline`. With a 0ms interval, this condition immediately evaluates to false, returning `Poll::Ready` instantly.

**5. Tight Loop in Coordinator** [7](#0-6) 

The coordinator's main event loop picks up ready broadcasts immediately. With 0ms scheduling, this creates a tight loop where broadcasts execute continuously without yielding.

**Attack Execution Path:**

1. Node operator sets `shared_mempool_tick_interval_ms: 0` in node configuration YAML (either accidentally, through automation errors, or via compromised configuration management)
2. Node starts with this configuration
3. For each peer, the broadcast scheduler creates `ScheduledBroadcast` futures with 0ms delay
4. These futures become ready immediately upon polling
5. The coordinator executes broadcasts continuously in a tight loop
6. Each broadcast iteration involves:
   - Acquiring mempool mutex lock [8](#0-7) 
   - Reading transactions from timeline
   - Serializing and sending network messages
   - Updating sync state with write locks
7. CPU utilization spikes to near 100% on the affected node
8. Node becomes unresponsive to API requests and consensus messages
9. Validator performance degrades significantly

**Broken Invariants:**

- **Resource Limits Invariant**: "All operations must respect gas, storage, and computational limits" - The tight loop violates CPU resource limits
- **Defense-in-depth**: Configuration validation is a critical security control that is missing

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos Bug Bounty criteria:

1. **"Validator node slowdowns"** - Direct match. Setting this value to 0 causes significant CPU exhaustion, degrading validator performance and potentially causing missed consensus rounds.

2. **"API crashes"** - The CPU exhaustion can make the node's API unresponsive, causing timeouts and failures for API clients.

3. **"Significant protocol violations"** - An unresponsive validator fails to participate properly in consensus, affecting network health.

While this is not Critical severity (it doesn't break consensus safety or cause fund loss), it significantly impacts node availability and network liveness.

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability has moderate likelihood of occurrence because:

1. **Configuration Error Scenarios:**
   - Automated configuration systems could generate incorrect values
   - Copy-paste errors in configuration templates
   - Misunderstanding of the parameter (thinking 0 means "default" or "unlimited")

2. **Attack Scenarios:**
   - Compromised configuration management systems
   - Supply chain attacks on configuration templates
   - Insider threats from malicious operators
   - Configuration injection through other vulnerabilities

3. **Defense-in-Depth Failure:**
   - The lack of validation means ANY misconfiguration or compromise leads directly to impact
   - No safety net to catch erroneous values

4. **Deployment Scale:**
   - This affects all node types (validators, VFNs, fullnodes)
   - Even one misconfigured validator can impact network performance

## Recommendation

Implement strict input validation in the `ConfigSanitizer::sanitize()` method to enforce minimum acceptable values: [1](#0-0) 

**Recommended Fix:**

```rust
impl ConfigSanitizer for MempoolConfig {
    fn sanitize(
        _node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        const MIN_TICK_INTERVAL_MS: u64 = 1;
        const MIN_BACKOFF_INTERVAL_MS: u64 = 100;
        const MIN_ACK_TIMEOUT_MS: u64 = 100;
        const MIN_GC_INTERVAL_MS: u64 = 1000;
        
        if self.shared_mempool_tick_interval_ms < MIN_TICK_INTERVAL_MS {
            return Err(Error::ConfigError(format!(
                "shared_mempool_tick_interval_ms ({}) must be at least {}ms to prevent CPU exhaustion",
                self.shared_mempool_tick_interval_ms, MIN_TICK_INTERVAL_MS
            )));
        }
        
        if self.shared_mempool_backoff_interval_ms < MIN_BACKOFF_INTERVAL_MS {
            return Err(Error::ConfigError(format!(
                "shared_mempool_backoff_interval_ms ({}) must be at least {}ms",
                self.shared_mempool_backoff_interval_ms, MIN_BACKOFF_INTERVAL_MS
            )));
        }
        
        if self.shared_mempool_ack_timeout_ms < MIN_ACK_TIMEOUT_MS {
            return Err(Error::ConfigError(format!(
                "shared_mempool_ack_timeout_ms ({}) must be at least {}ms",
                self.shared_mempool_ack_timeout_ms, MIN_ACK_TIMEOUT_MS
            )));
        }
        
        if self.system_transaction_gc_interval_ms < MIN_GC_INTERVAL_MS {
            return Err(Error::ConfigError(format!(
                "system_transaction_gc_interval_ms ({}) must be at least {}ms",
                self.system_transaction_gc_interval_ms, MIN_GC_INTERVAL_MS
            )));
        }
        
        if self.capacity == 0 {
            return Err(Error::ConfigError(
                "Mempool capacity must be greater than 0".to_string()
            )));
        }
        
        Ok(())
    }
}
```

Additionally, add runtime safeguards in the broadcast scheduling logic to enforce minimum delays even if configuration is bypassed.

## Proof of Concept

**Configuration File (validator.yaml):**
```yaml
mempool:
  shared_mempool_tick_interval_ms: 0  # Malicious/erroneous configuration
  shared_mempool_batch_size: 300
  capacity: 2000000
```

**Reproduction Steps:**

1. Create a test configuration file with `shared_mempool_tick_interval_ms: 0`
2. Start an Aptos node with this configuration
3. Monitor CPU usage of the mempool broadcast coordinator thread
4. Observe:
   - CPU usage spikes to near 100% on coordinator thread
   - Continuous execution of `execute_broadcast` with no delay
   - Mempool lock contention increasing
   - Node API response times degrading
   - Potential missed consensus rounds if running as validator

**Expected Behavior:** Configuration loading should fail with a validation error indicating that `shared_mempool_tick_interval_ms` must be at least 1ms.

**Actual Behavior:** Node starts successfully and enters a CPU exhaustion state due to tight broadcast loop.

**Notes**

This vulnerability demonstrates a critical defense-in-depth failure where configuration validation is completely missing despite being explicitly TODO'd in the code. While exploitation requires configuration access, the lack of validation violates security best practices and enables various attack scenarios including misconfigurations, automation errors, and supply chain attacks. The impact on validator node performance directly matches the High Severity criteria in the Aptos Bug Bounty program.

### Citations

**File:** config/src/config/mempool_config.rs (L39-41)
```rust
#[derive(Clone, Debug, Deserialize, PartialEq, Eq, Serialize)]
#[serde(default, deny_unknown_fields)]
pub struct MempoolConfig {
```

**File:** config/src/config/mempool_config.rs (L71-71)
```rust
    pub shared_mempool_tick_interval_ms: u64,
```

**File:** config/src/config/mempool_config.rs (L176-184)
```rust
impl ConfigSanitizer for MempoolConfig {
    fn sanitize(
        _node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        Ok(()) // TODO: add reasonable verifications
    }
}
```

**File:** mempool/src/shared_mempool/tasks.rs (L108-122)
```rust
    let schedule_backoff = network_interface.is_backoff_mode(&peer);

    let interval_ms = if schedule_backoff {
        smp.config.shared_mempool_backoff_interval_ms
    } else {
        smp.config.shared_mempool_tick_interval_ms
    };

    scheduled_broadcasts.push(ScheduledBroadcast::new(
        Instant::now() + Duration::from_millis(interval_ms),
        peer,
        schedule_backoff,
        executor,
    ))
}
```

**File:** mempool/src/shared_mempool/types.rs (L132-154)
```rust
impl ScheduledBroadcast {
    pub fn new(deadline: Instant, peer: PeerNetworkId, backoff: bool, executor: Handle) -> Self {
        let waker: Arc<Mutex<Option<Waker>>> = Arc::new(Mutex::new(None));
        let waker_clone = waker.clone();

        if deadline > Instant::now() {
            let tokio_instant = tokio::time::Instant::from_std(deadline);
            executor.spawn(async move {
                tokio::time::sleep_until(tokio_instant).await;
                let mut waker = waker_clone.lock();
                if let Some(waker) = waker.take() {
                    waker.wake()
                }
            });
        }

        Self {
            deadline,
            peer,
            backoff,
            waker,
        }
    }
```

**File:** mempool/src/shared_mempool/types.rs (L162-172)
```rust
    fn poll(self: Pin<&mut Self>, context: &mut Context) -> Poll<Self::Output> {
        if Instant::now() < self.deadline {
            let waker_clone = context.waker().clone();
            let mut waker = self.waker.lock();
            *waker = Some(waker_clone);

            Poll::Pending
        } else {
            Poll::Ready((self.peer, self.backoff))
        }
    }
```

**File:** mempool/src/shared_mempool/coordinator.rs (L118-119)
```rust
            (peer, backoff) = scheduled_broadcasts.select_next_some() => {
                tasks::execute_broadcast(peer, backoff, &mut smp, &mut scheduled_broadcasts, executor.clone()).await;
```

**File:** mempool/src/shared_mempool/network.rs (L399-399)
```rust
        let mempool = smp.mempool.lock();
```
