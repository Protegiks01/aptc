# Audit Report

## Title
TOCTOU Race Condition Causing Premature Removal of Active Indexer Data Services

## Summary
A Time-of-Check-Time-of-Use (TOCTOU) race condition exists in the indexer-grpc metadata manager's main service monitoring loop. Between identifying unreachable services and removing them, concurrent heartbeat RPCs can re-register those services with fresh timestamps, yet they are still removed. This causes active, healthy data services to be incorrectly evicted from the service pool, disrupting indexer availability.

## Finding Description

The vulnerability exists in the `start()` function's service health monitoring logic. The function follows this execution pattern:

1. **Collection Phase**: Vectors are initialized to track unreachable services [1](#0-0) 

2. **Health Check Phase**: The code iterates through all registered services and identifies those with stale timestamps (> 60 seconds old), adding their addresses to the removal vectors [2](#0-1) 

3. **Concurrent Execution Window**: A `tokio_scoped` block spawns async ping tasks [3](#0-2) 

4. **Race Condition Window**: During this entire scoped block execution, external gRPC heartbeat requests can arrive at any time. The `GrpcManager` service exposes an async `heartbeat()` RPC endpoint [4](#0-3)  that calls the metadata manager's `handle_heartbeat()` method [5](#0-4) 

5. **Re-registration**: When a heartbeat arrives for a service already marked for removal, `handle_live_data_service_info()` updates that service with a fresh timestamp or re-inserts it into the `DashMap` [6](#0-5) 

6. **Premature Removal**: After the scoped block completes, the code removes all services whose addresses were collected in step 2, regardless of their current state [7](#0-6) 

The `DashMap` concurrent hash map is thread-safe for individual operations but does not prevent this logical race condition. The removal decision is based on stale information collected at the beginning of the loop iteration.

**Attack Scenario:**
1. Data Service A has not sent a heartbeat for 65 seconds
2. The monitoring loop identifies Service A as unreachable at line 226
3. Service A's address is added to `unreachable_live_data_services`
4. During the scoped block execution, Service A reconnects and sends a heartbeat
5. The heartbeat handler successfully updates Service A with a current timestamp
6. Service A is now healthy and ready to serve requests
7. At line 296, Service A is removed from the service pool
8. Clients attempting to connect to Service A will fail, despite it being active

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos bug bounty criteria for the following reasons:

**API Crashes**: The indexer-grpc infrastructure is critical for providing blockchain data access to clients. Premature removal of active data services causes API request failures when the load balancer attempts to route traffic to services that were incorrectly removed. The service selection logic in `pick_live_data_service()` and `pick_historical_data_service()` [8](#0-7)  will exclude these services, reducing overall capacity.

**Significant Protocol Violations**: The indexer service discovery protocol assumes that services actively sending heartbeats remain in the available pool. This race condition violates that fundamental guarantee, breaking the service availability invariant.

**Cascading Service Degradation**: As services are incorrectly removed, the remaining services experience increased load. This can trigger a cascade where legitimate services become temporarily unavailable due to overload, get marked as unreachable, receive heartbeats during the race window, and are then incorrectly removed—compounding the availability problem.

While this does not directly affect consensus or validator operations, it severely impacts the Aptos indexer infrastructure that clients depend on for querying blockchain state and historical data.

## Likelihood Explanation

This vulnerability has a **HIGH likelihood** of occurrence:

**Natural Timing Alignment**: The race window is substantial. The `tokio_scoped` block can take significant time to complete as it spawns multiple concurrent ping operations. With a 60-second staleness threshold, services that were temporarily unreachable but recover just as they're being checked will consistently hit this race condition.

**No Special Privileges Required**: Any data service sending legitimate heartbeats can trigger this condition. No attacker coordination is needed—the vulnerability manifests during normal network fluctuations or service restarts.

**Production Conditions Favor Exploitation**: In production environments with network latency, service deployments, or brief connectivity issues, services frequently transition between reachable and unreachable states. The 1-second main loop iteration [9](#0-8)  combined with async RPC handling creates continuous race opportunities.

**Deterministic Reproducibility**: Unlike some race conditions that require precise nanosecond timing, this vulnerability has a multi-second window (duration of the scoped block) during which the race can occur.

## Recommendation

Implement a **double-check pattern** that validates service health immediately before removal:

```rust
pub(crate) async fn start(&self) -> Result<()> {
    loop {
        let _timer = TIMER
            .with_label_values(&["metadata_manager_main_loop"])
            .start_timer();
        
        tokio_scoped::scope(|s| {
            // ... existing ping logic ...
        });

        // Double-check health status before removal
        let mut services_to_remove = vec![];
        for kv in &self.live_data_services {
            let (address, service) = kv.pair();
            let still_unreachable = service.recent_states.back().is_some_and(|s| {
                Self::is_stale_timestamp(
                    s.timestamp.unwrap_or_default(),
                    Duration::from_secs(60),
                )
            });
            if still_unreachable {
                services_to_remove.push(address.clone());
            }
        }

        for address in services_to_remove {
            COUNTER
                .with_label_values(&["unreachable_live_data_service"])
                .inc();
            self.live_data_services.remove(&address);
        }

        // Same pattern for historical_data_services
        let mut services_to_remove = vec![];
        for kv in &self.historical_data_services {
            let (address, service) = kv.pair();
            let still_unreachable = service.recent_states.back().is_some_and(|s| {
                Self::is_stale_timestamp(
                    s.timestamp.unwrap_or_default(),
                    Duration::from_secs(60),
                )
            });
            if still_unreachable {
                services_to_remove.push(address.clone());
            }
        }

        for address in services_to_remove {
            COUNTER
                .with_label_values(&["unreachable_historical_data_service"])
                .inc();
            self.historical_data_services.remove(&address);
        }

        // ... rest of function ...
    }
}
```

**Alternative Solution**: Use atomic compare-and-swap operations with epoch counters to ensure services are only removed if they haven't received updates since being marked unreachable.

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::Arc;
    use tokio::time::{sleep, Duration};
    use aptos_protos::indexer::v1::LiveDataServiceInfo;

    #[tokio::test]
    async fn test_heartbeat_race_causes_premature_removal() {
        // Setup metadata manager
        let manager = Arc::new(MetadataManager::new(
            1,  // chain_id
            "http://localhost:50051".to_string(),
            vec![],
            vec![],
            None,
        ));

        // Register a service with stale timestamp
        let address = "http://data-service:50052".to_string();
        let mut stale_info = LiveDataServiceInfo {
            timestamp: Some(Timestamp {
                seconds: (SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs() - 70) as i64,
                nanos: 0,
            }),
            min_servable_version: Some(0),
            stream_info: Some(StreamInfo { active_streams: vec![] }),
        };
        manager.handle_live_data_service_info(address.clone(), stale_info).unwrap();

        // Verify service is registered
        assert_eq!(manager.live_data_services.len(), 1);

        // Simulate the race: spawn concurrent heartbeat while main loop identifies removal
        let manager_clone = manager.clone();
        let address_clone = address.clone();
        let heartbeat_task = tokio::spawn(async move {
            // Wait to simulate race timing
            sleep(Duration::from_millis(50)).await;
            
            // Send fresh heartbeat during the scoped block
            let fresh_info = LiveDataServiceInfo {
                timestamp: Some(timestamp_now_proto()),
                min_servable_version: Some(0),
                stream_info: Some(StreamInfo { active_streams: vec![] }),
            };
            manager_clone.handle_live_data_service_info(address_clone, fresh_info).unwrap();
        });

        // Simulate one iteration of the main loop
        let mut unreachable_services = vec![];
        for kv in &manager.live_data_services {
            let (addr, service) = kv.pair();
            let unreachable = service.recent_states.back().is_some_and(|s| {
                MetadataManager::is_stale_timestamp(
                    s.timestamp.unwrap_or_default(),
                    Duration::from_secs(60),
                )
            });
            if unreachable {
                unreachable_services.push(addr.clone());
            }
        }

        // Scoped block simulation (heartbeat arrives here)
        heartbeat_task.await.unwrap();
        sleep(Duration::from_millis(100)).await;

        // Verify service received fresh heartbeat and is now healthy
        let service_entry = manager.live_data_services.get(&address).unwrap();
        let latest_timestamp = service_entry.recent_states.back().unwrap().timestamp.unwrap();
        let now = SystemTime::now().duration_since(UNIX_EPOCH).unwrap();
        let age = now.as_secs() - latest_timestamp.seconds as u64;
        assert!(age < 5, "Service should have fresh timestamp");

        // But it still gets removed due to race condition
        for addr in unreachable_services {
            manager.live_data_services.remove(&addr);
        }

        // VULNERABILITY: Active service was incorrectly removed
        assert_eq!(manager.live_data_services.len(), 0, 
            "Healthy service with fresh heartbeat was incorrectly removed");
    }
}
```

This test demonstrates that a service receiving a heartbeat during the race window is still removed, violating service availability guarantees.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L180-181)
```rust
            let mut unreachable_live_data_services = vec![];
            let mut unreachable_historical_data_services = vec![];
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L182-290)
```rust
            tokio_scoped::scope(|s| {
                for kv in &self.grpc_managers {
                    let address = kv.key().clone();
                    let grpc_manager = kv.value();
                    let client = grpc_manager.client.clone();
                    s.spawn(async move {
                        if let Err(e) = self.heartbeat(client).await {
                            warn!("Failed to send heartbeat to other grpc manager ({address}): {e:?}.");
                        } else {
                            trace!("Successfully sent heartbeat to other grpc manager ({address}).");
                        }
                    });
                }

                for kv in &self.fullnodes {
                    let (address, fullnode) = kv.pair();
                    let need_ping = fullnode.recent_states.back().is_none_or(|s| {
                        Self::is_stale_timestamp(
                            s.timestamp.unwrap_or_default(),
                            Duration::from_secs(1),
                        )
                    });
                    if need_ping {
                        let address = address.clone();
                        let client = fullnode.client.clone();
                        s.spawn(async move {
                            if let Err(e) = self.ping_fullnode(address.clone(), client).await {
                                warn!("Failed to ping FN ({address}): {e:?}.");
                            } else {
                                trace!("Successfully pinged FN ({address}).");
                            }
                        });
                    }
                }

                for kv in &self.live_data_services {
                    let (address, live_data_service) = kv.pair();
                    let unreachable = live_data_service.recent_states.back().is_some_and(|s| {
                        Self::is_stale_timestamp(
                            s.timestamp.unwrap_or_default(),
                            Duration::from_secs(60),
                        )
                    });
                    if unreachable {
                        unreachable_live_data_services.push(address.clone());
                        continue;
                    }
                    let need_ping = live_data_service.recent_states.back().is_none_or(|s| {
                        Self::is_stale_timestamp(
                            s.timestamp.unwrap_or_default(),
                            Duration::from_secs(5),
                        )
                    });
                    if need_ping {
                        let address = address.clone();
                        let client = live_data_service.client.clone();
                        s.spawn(async move {
                            if let Err(e) =
                                self.ping_live_data_service(address.clone(), client).await
                            {
                                warn!("Failed to ping live data service ({address}): {e:?}.");
                            } else {
                                trace!("Successfully pinged live data service ({address}).");
                            }
                        });
                    }
                }

                for kv in &self.historical_data_services {
                    let (address, historical_data_service) = kv.pair();
                    let unreachable =
                        historical_data_service
                            .recent_states
                            .back()
                            .is_some_and(|s| {
                                Self::is_stale_timestamp(
                                    s.timestamp.unwrap_or_default(),
                                    Duration::from_secs(60),
                                )
                            });
                    if unreachable {
                        unreachable_historical_data_services.push(address.clone());
                        continue;
                    }
                    let need_ping = historical_data_service
                        .recent_states
                        .back()
                        .is_none_or(|s| {
                            Self::is_stale_timestamp(
                                s.timestamp.unwrap_or_default(),
                                Duration::from_secs(5),
                            )
                        });
                    if need_ping {
                        let address = address.clone();
                        let client = historical_data_service.client.clone();
                        s.spawn(async move {
                            if let Err(e) = self
                                .ping_historical_data_service(address.clone(), client)
                                .await
                            {
                                warn!("Failed to ping historical data service ({address}): {e:?}.");
                            } else {
                                trace!("Successfully pinged historical data service ({address}).");
                            }
                        });
                    }
                }
            });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L292-304)
```rust
            for address in unreachable_live_data_services {
                COUNTER
                    .with_label_values(&["unreachable_live_data_service"])
                    .inc();
                self.live_data_services.remove(&address);
            }

            for address in unreachable_historical_data_services {
                COUNTER
                    .with_label_values(&["unreachable_historical_data_service"])
                    .inc();
                self.historical_data_services.remove(&address);
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L326-326)
```rust
            tokio::time::sleep(Duration::from_secs(1)).await;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L489-509)
```rust
    fn handle_live_data_service_info(
        &self,
        address: GrpcAddress,
        mut info: LiveDataServiceInfo,
    ) -> Result<()> {
        let mut entry = self
            .live_data_services
            .entry(address.clone())
            .or_insert(LiveDataService::new(address));
        if info.stream_info.is_none() {
            info.stream_info = Some(StreamInfo {
                active_streams: vec![],
            });
        }
        entry.value_mut().recent_states.push_back(info);
        if entry.value().recent_states.len() > MAX_NUM_OF_STATES_TO_KEEP {
            entry.value_mut().recent_states.pop_front();
        }

        Ok(())
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/service.rs (L35-45)
```rust
    async fn handle_heartbeat(
        &self,
        address: String,
        info: Info,
    ) -> anyhow::Result<Response<HeartbeatResponse>> {
        self.metadata_manager.handle_heartbeat(address, info)?;

        Ok(Response::new(HeartbeatResponse {
            known_latest_version: Some(self.metadata_manager.get_known_latest_version()),
        }))
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/service.rs (L71-105)
```rust
    fn pick_live_data_service(&self, starting_version: u64) -> Option<String> {
        let mut candidates = vec![];
        for candidate in self.metadata_manager.get_live_data_services_info() {
            if let Some(info) = candidate.1.back().as_ref() {
                // TODO(grao): Handle the case when the requested starting version is beyond the
                // latest version.
                if info.min_servable_version.is_none()
                    || starting_version < info.min_servable_version.unwrap()
                {
                    continue;
                }
                let num_active_streams = info.stream_info.as_ref().unwrap().active_streams.len();
                candidates.push((candidate.0, num_active_streams));
            }
        }

        Self::pick_data_service_from_candidate(candidates)
    }

    async fn pick_historical_data_service(&self, starting_version: u64) -> Option<String> {
        let file_store_version = self.data_manager.get_file_store_version().await;
        if starting_version >= file_store_version {
            return None;
        }

        let mut candidates = vec![];
        for candidate in self.metadata_manager.get_historical_data_services_info() {
            if let Some(info) = candidate.1.back().as_ref() {
                let num_active_streams = info.stream_info.as_ref().unwrap().active_streams.len();
                candidates.push((candidate.0, num_active_streams));
            }
        }

        Self::pick_data_service_from_candidate(candidates)
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/service.rs (L110-127)
```rust
    async fn heartbeat(
        &self,
        request: Request<HeartbeatRequest>,
    ) -> Result<Response<HeartbeatResponse>, Status> {
        let request = request.into_inner();
        if let Some(service_info) = request.service_info {
            if let Some(address) = service_info.address {
                if let Some(info) = service_info.info {
                    return self
                        .handle_heartbeat(address, info)
                        .await
                        .map_err(|e| Status::internal(format!("Error handling heartbeat: {e}")));
                }
            }
        }

        Err(Status::invalid_argument("Bad request."))
    }
```
