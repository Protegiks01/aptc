# Audit Report

## Title
Consensus Reset Deadlock: Unbounded Wait on Pipeline Tasks Causes Indefinite Node Hang During State Synchronization

## Summary
The BufferManager's reset mechanism can deadlock indefinitely when pipeline execution tasks hang, because the reset acknowledgment is sent only after waiting for all ongoing tasks to complete (with no timeout), while the caller also waits indefinitely for the acknowledgment (with no timeout). This creates a critical liveness vulnerability where validator nodes become permanently unresponsive during state synchronization or epoch transitions.

## Finding Description

The vulnerability exists in the consensus pipeline reset mechanism across three components:

**1. Reset Function with Unbounded Waits**

The `BufferManager::reset()` function contains multiple unbounded waits that can hang indefinitely: [1](#0-0) 

Specifically at lines 573-575, the function polls `ongoing_tasks` counter in an infinite loop with no timeout. This counter tracks active `CountedRequest` instances in the pipeline phases.

**2. ResetAck Sent Only After Reset Completes**

The `process_reset_request()` function sends the acknowledgment only after the entire reset completes: [2](#0-1) 

Line 594 sends the ResetAck only after line 593's `self.reset().await` returns.

**3. Caller Waits Indefinitely Without Timeout**

The execution client that calls reset waits indefinitely for the acknowledgment with no timeout: [3](#0-2) 

Line 705 (`rx.await`) blocks indefinitely waiting for the ResetAck. Similar unbounded waits exist in `end_epoch()` at lines 731, 743, and 756.

**4. Pipeline Tasks Hold TaskGuard Until Completion**

The `PipelinePhase` receives `CountedRequest` instances that hold a `TaskGuard`, which increments `ongoing_tasks` on creation and decrements on drop: [4](#0-3) [5](#0-4) 

The guard is held during `self.processor.process(req).await` at line 99. If this process call hangs, the guard never drops, and `ongoing_tasks` never decrements.

**5. Execution Can Hang Without Timeout**

The `ExecutionWaitPhase` awaits compute results without any timeout: [6](#0-5) 

Line 54 awaits the future indefinitely. This future internally calls `wait_for_compute_result()` which also has no timeout: [7](#0-6) 

**Attack Scenario:**

1. Node receives ordered blocks and sends them to the execution pipeline
2. An `ExecutionWaitRequest` is created and sent to `ExecutionWaitPhase`
3. The phase awaits `wait_for_compute_result()` which hangs due to:
   - Malicious block crafted to cause executor to hang
   - Executor bug causing indefinite wait
   - Resource exhaustion causing slow execution
   - Network issues preventing compute completion
4. The `CountedRequest`'s `TaskGuard` remains alive, keeping `ongoing_tasks > 0`
5. Node needs to perform state sync (triggered by falling behind or epoch change)
6. `sync_for_duration()` or `sync_to_target()` is called
7. These functions call `reset()` to clean up the pipeline
8. `reset()` polls `ongoing_tasks` at line 573, which never reaches 0
9. `reset()` never completes
10. `ResetAck` is never sent (line 594)
11. Caller hangs indefinitely at `rx.await` (line 705)
12. Node cannot complete state sync or epoch transition
13. Node becomes unresponsive and cannot participate in consensus
14. Manual restart required

The same deadlock can occur through the `wait_for_commit_ledger()` and `wait_until_finishes()` calls within reset (lines 550, 555, 568), both of which also await indefinitely without timeouts.

## Impact Explanation

This vulnerability is classified as **HIGH SEVERITY** according to Aptos bug bounty criteria:

- **Validator Node Slowdowns/Hangs**: The node becomes completely unresponsive during reset, unable to process new blocks or participate in consensus.
- **Significant Protocol Violations**: Breaks the liveness invariant that nodes must be able to recover through state synchronization.
- **Loss of Network Availability**: Affected validator loses all consensus participation until manual intervention.

The issue does not qualify as CRITICAL because:
- It does not cause loss of funds or consensus safety violations
- It does not create permanent network partition (other nodes continue)
- It does not freeze funds or require hardfork
- Recovery is possible through manual node restart

However, the impact is severe because:
- State synchronization is a critical recovery mechanism used frequently
- Epoch transitions use the same reset mechanism
- Multiple validators experiencing this simultaneously could degrade network performance
- No automatic recovery mechanism exists
- The deadlock is silent with no timeout alerts

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

This vulnerability is likely to occur because:

1. **Frequent Trigger Conditions**: Reset is called during:
   - State synchronization (when nodes fall behind)
   - Epoch transitions (regular occurrence)
   - Any scenario requiring pipeline cleanup

2. **Multiple Hang Points**: The vulnerability can be triggered through multiple code paths:
   - Execution phase hanging on compute results
   - Commit ledger futures not completing
   - Pipeline abort futures not finishing

3. **No Defensive Timeouts**: The complete absence of timeouts at all levels means any transient issue becomes permanent.

4. **Real-world Triggers**:
   - Heavy network congestion delaying futures
   - Executor bugs (which have occurred historically in blockchain systems)
   - Resource exhaustion under load
   - Edge cases in block execution

5. **Validator Environment**: Production validators run continuously and will eventually encounter edge cases that cause temporary hangs in execution.

Mitigating factors:
- Requires execution to actually hang (not just be slow)
- Most blocks execute successfully under normal conditions
- Validators typically have monitoring and manual intervention capabilities

## Recommendation

Implement comprehensive timeout protection at multiple levels:

**1. Add Timeout to Reset Caller (Primary Fix)**

```rust
async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
    const RESET_TIMEOUT: Duration = Duration::from_secs(30);
    
    // Reset rand manager with timeout
    if let Some(mut reset_tx) = reset_tx_to_rand_manager {
        let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
        reset_tx.send(ResetRequest { tx: ack_tx, signal: ResetSignal::TargetRound(target.commit_info().round()) })
            .await.map_err(|_| Error::RandResetDropped)?;
        
        tokio::time::timeout(RESET_TIMEOUT, ack_rx)
            .await
            .map_err(|_| Error::RandResetTimeout)?
            .map_err(|_| Error::RandResetDropped)?;
    }

    // Reset buffer manager with timeout
    if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
        let (tx, rx) = oneshot::channel::<ResetAck>();
        reset_tx.send(ResetRequest { tx, signal: ResetSignal::TargetRound(target.commit_info().round()) })
            .await.map_err(|_| Error::ResetDropped)?;
            
        tokio::time::timeout(RESET_TIMEOUT, rx)
            .await
            .map_err(|_| Error::ResetTimeout)?
            .map_err(|_| Error::ResetDropped)?;
    }

    Ok(())
}
```

**2. Add Timeout to Ongoing Tasks Wait (Secondary Fix)**

```rust
async fn reset(&mut self) {
    // ... existing cleanup code ...
    
    // Wait for ongoing tasks with timeout
    const ONGOING_TASKS_TIMEOUT: Duration = Duration::from_secs(20);
    let start = Instant::now();
    while self.ongoing_tasks.load(Ordering::SeqCst) > 0 {
        if start.elapsed() > ONGOING_TASKS_TIMEOUT {
            warn!(
                "Reset timeout waiting for ongoing tasks: {} tasks still running",
                self.ongoing_tasks.load(Ordering::SeqCst)
            );
            // Force reset despite ongoing tasks
            break;
        }
        tokio::time::sleep(Duration::from_millis(10)).await;
    }
}
```

**3. Add Timeouts to Pipeline Awaits**

```rust
pub async fn wait_for_compute_result(&self) -> ExecutorResult<(StateComputeResult, Duration)> {
    const COMPUTE_TIMEOUT: Duration = Duration::from_secs(60);
    
    let fut = self.pipeline_futs()
        .ok_or(ExecutorError::InternalError { error: "Pipeline aborted".to_string() })?
        .ledger_update_fut;
    
    tokio::time::timeout(COMPUTE_TIMEOUT, fut)
        .await
        .map_err(|_| ExecutorError::InternalError { error: "Compute result timeout".to_string() })?
        .map(|(compute_result, execution_time, _)| (compute_result, execution_time))
        .map_err(|e| ExecutorError::InternalError { error: e.to_string() })
}
```

**4. Add Error Type for Timeout**

Add new error variants to handle timeout cases appropriately and allow upstream components to distinguish between timeout and other failures.

## Proof of Concept

The following demonstrates the deadlock scenario conceptually:

```rust
// Simulation of the deadlock
#[tokio::test]
async fn test_reset_deadlock_on_hanging_execution() {
    // Setup: Create BufferManager with ongoing_tasks counter
    let ongoing_tasks = Arc::new(AtomicU64::new(0));
    
    // Simulate a CountedRequest that never completes
    let _guard = TaskGuard::new(ongoing_tasks.clone());
    // Guard is held, ongoing_tasks = 1
    
    // Spawn execution task that hangs indefinitely
    tokio::spawn(async move {
        // Simulate hanging execution
        tokio::time::sleep(Duration::from_secs(1000)).await;
        // _guard would be dropped here, but we never reach this point
    });
    
    // Attempt reset
    let (reset_tx, mut reset_rx) = unbounded::<ResetRequest>();
    let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
    
    // Send reset request
    reset_tx.send(ResetRequest {
        tx: ack_tx,
        signal: ResetSignal::TargetRound(100),
    }).await.unwrap();
    
    // Simulate BufferManager waiting for ongoing_tasks
    tokio::spawn(async move {
        if let Some(request) = reset_rx.next().await {
            // This simulates the reset() function waiting
            while ongoing_tasks.load(Ordering::SeqCst) > 0 {
                tokio::time::sleep(Duration::from_millis(10)).await;
                // This loop never exits because ongoing_tasks stays at 1
            }
            // ResetAck would be sent here, but we never reach this
            let _ = request.tx.send(ResetAck::default());
        }
    });
    
    // Caller waits for ack - this hangs indefinitely
    let result = tokio::time::timeout(Duration::from_secs(5), ack_rx).await;
    
    // Expected: timeout occurs because reset never completes
    assert!(result.is_err(), "Should timeout waiting for ResetAck");
}
```

To observe this in production:
1. Deploy validator node with instrumentation on reset operations
2. Monitor for scenarios where execution tasks take unexpectedly long
3. Trigger state sync during such scenarios
4. Observe reset hanging indefinitely with ongoing_tasks > 0
5. Node becomes unresponsive until manual restart

## Notes

This vulnerability represents a fundamental architectural issue where critical recovery mechanisms (reset/state sync) lack defensive timeout protection. The absence of timeouts at multiple levels creates a cascade failure where any single hanging task can deadlock the entire reset process indefinitely. This is particularly dangerous because state synchronization is the primary mechanism for nodes to recover from being behind, making this vulnerability self-perpetuating: once triggered, the node cannot recover without manual intervention.

### Citations

**File:** consensus/src/pipeline/buffer_manager.rs (L546-576)
```rust
    async fn reset(&mut self) {
        while let Some((_, block)) = self.pending_commit_blocks.pop_first() {
            // Those blocks don't have any dependencies, should be able to finish commit_ledger.
            // Abort them can cause error on epoch boundary.
            block.wait_for_commit_ledger().await;
        }
        while let Some(item) = self.buffer.pop_front() {
            for b in item.get_blocks() {
                if let Some(futs) = b.abort_pipeline() {
                    futs.wait_until_finishes().await;
                }
            }
        }
        self.buffer = Buffer::new();
        self.execution_root = None;
        self.signing_root = None;
        self.previous_commit_time = Instant::now();
        self.commit_proof_rb_handle.take();
        // purge the incoming blocks queue
        while let Ok(Some(blocks)) = self.block_rx.try_next() {
            for b in blocks.ordered_blocks {
                if let Some(futs) = b.abort_pipeline() {
                    futs.wait_until_finishes().await;
                }
            }
        }
        // Wait for ongoing tasks to finish before sending back ack.
        while self.ongoing_tasks.load(Ordering::SeqCst) > 0 {
            tokio::time::sleep(Duration::from_millis(10)).await;
        }
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L579-596)
```rust
    async fn process_reset_request(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        info!("Receive reset");

        match signal {
            ResetSignal::Stop => self.stop = true,
            ResetSignal::TargetRound(round) => {
                self.highest_committed_round = round;
                self.latest_round = round;

                let _ = self.drain_pending_commit_proof_till(round);
            },
        }

        self.reset().await;
        let _ = tx.send(ResetAck::default());
        info!("Reset finishes");
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L674-709)
```rust
    async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
        let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager) = {
            let handle = self.handle.read();
            (
                handle.reset_tx_to_rand_manager.clone(),
                handle.reset_tx_to_buffer_manager.clone(),
            )
        };

        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        }

        if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
            // reset execution phase and commit phase
            let (tx, rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::ResetDropped)?;
            rx.await.map_err(|_| Error::ResetDropped)?;
        }

        Ok(())
    }
```

**File:** consensus/src/pipeline/pipeline_phase.rs (L26-45)
```rust
struct TaskGuard {
    counter: Arc<AtomicU64>,
}

impl TaskGuard {
    fn new(counter: Arc<AtomicU64>) -> Self {
        counter.fetch_add(1, Ordering::SeqCst);
        Self { counter }
    }

    fn spawn(&self) -> Self {
        Self::new(self.counter.clone())
    }
}

impl Drop for TaskGuard {
    fn drop(&mut self) {
        self.counter.fetch_sub(1, Ordering::SeqCst);
    }
}
```

**File:** consensus/src/pipeline/pipeline_phase.rs (L88-108)
```rust
    pub async fn start(mut self) {
        // main loop
        while let Some(counted_req) = self.rx.next().await {
            let CountedRequest { req, guard: _guard } = counted_req;
            if self.reset_flag.load(Ordering::SeqCst) {
                continue;
            }
            let response = {
                let _timer = BUFFER_MANAGER_PHASE_PROCESS_SECONDS
                    .with_label_values(&[T::NAME])
                    .start_timer();
                self.processor.process(req).await
            };
            if let Some(tx) = &mut self.maybe_tx {
                if tx.send(response).await.is_err() {
                    debug!("Failed to send response, buffer manager probably dropped");
                    break;
                }
            }
        }
    }
```

**File:** consensus/src/pipeline/execution_wait_phase.rs (L49-56)
```rust
    async fn process(&self, req: ExecutionWaitRequest) -> ExecutionResponse {
        let ExecutionWaitRequest { block_id, fut } = req;

        ExecutionResponse {
            block_id,
            inner: fut.await,
        }
    }
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L549-560)
```rust
    pub async fn wait_for_compute_result(&self) -> ExecutorResult<(StateComputeResult, Duration)> {
        self.pipeline_futs()
            .ok_or(ExecutorError::InternalError {
                error: "Pipeline aborted".to_string(),
            })?
            .ledger_update_fut
            .await
            .map(|(compute_result, execution_time, _)| (compute_result, execution_time))
            .map_err(|e| ExecutorError::InternalError {
                error: e.to_string(),
            })
    }
```
