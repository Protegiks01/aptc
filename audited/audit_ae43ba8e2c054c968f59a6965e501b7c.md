# Audit Report

## Title
Partial Notification Failure During Epoch Transition Causes Validator Desynchronization

## Summary
When a reconfiguration event is processed, the `notify_events()` function can fail after event subscribers have been notified but before reconfiguration subscribers receive their notifications. This creates a critical inconsistency where the blockchain state has transitioned to a new epoch, storage and mempool have been updated, but consensus and other critical components remain stuck in the old epoch.

## Finding Description

The vulnerability exists in the event notification flow during epoch transitions. The sequence of operations is: [1](#0-0) 

When state sync commits transactions, it notifies three systems in order:
1. Storage service (line 98)
2. Mempool commit handler (line 103)  
3. Event subscription service (line 109)

Within the event subscription service, the notification process has two phases: [2](#0-1) 

**The Critical Flaw**: At line 317, `notify_event_subscribers()` successfully notifies all event subscribers (including any component subscribed to specific events). This succeeds and returns `reconfig_event_processed = true`. However, if `notify_reconfiguration_subscribers()` subsequently fails at line 322, the error propagates up, but **event subscribers have already been irreversibly notified**.

The failure points in `notify_reconfiguration_subscribers()` are: [3](#0-2) [4](#0-3) 

Failures can occur from:
1. Database errors reading state at line 289
2. Missing ConfigurationResource at line 297-300  
3. Channel push failures at line 271

**The Impact**: Multiple critical components subscribe to reconfigurations: [5](#0-4) [6](#0-5) 

When the failure occurs, the system state becomes:
- ✓ Storage: New epoch committed to database
- ✓ Mempool: Committed transactions removed from pool
- ✓ Event subscribers: Notified of any events in the transaction
- ✗ Consensus: **NOT** notified of new epoch, remains in old epoch
- ✗ Mempool validator: **NOT** updated with new epoch configs
- ✗ DKG: May have received DKG events but **NOT** the new epoch state
- ✗ JWK Consensus: May have received JWK events but **NOT** the new epoch state

The mempool handles these separately: [7](#0-6) 

Mempool receives commit notifications (to remove transactions) and separate reconfig notifications (to update validator configs). If only the commit notification succeeds, mempool operates with stale epoch configs.

## Impact Explanation

This violates **Critical Invariant #4 (State Consistency)**: "State transitions must be atomic and verifiable via Merkle proofs" - while the Merkle state has transitioned atomically, the validator component state has not.

**Impact Assessment: HIGH Severity**

Per the Aptos bug bounty criteria, this qualifies as "Validator node slowdowns" / "Significant protocol violations":

1. **Validator Dysfunction**: The affected validator cannot participate in consensus for the new epoch because consensus components don't know the new epoch exists
2. **Network Degradation**: If multiple validators experience this simultaneously, consensus performance degrades
3. **State Inconsistency**: Components operate with mismatched epoch states, violating deterministic execution

While not a direct consensus safety break, this creates a **liveness failure** for affected validators and could lead to **epoch transition stalls** if sufficient validators are impacted.

## Likelihood Explanation

**Likelihood: MEDIUM**

The failure requires specific conditions:
1. A reconfiguration transaction must be committed (happens at epoch boundaries)
2. Either:
   - Database transient error during config read (line 289)
   - Config resource corruption (line 297-300)  
   - Channel consumer failures (line 271)

While these are operational failures rather than exploits, they are realistic scenarios in production systems:
- Database connection issues occur during high load
- Disk I/O errors can cause read failures
- Process crashes could close channels

The impact is amplified because epoch transitions are **critical synchronization points** where all validators must coordinate.

## Recommendation

**Implement atomic notification with rollback capability:**

```rust
fn notify_events(&mut self, version: Version, events: Vec<ContractEvent>) -> Result<(), Error> {
    if events.is_empty() {
        return Ok(());
    }

    // First, check if we can successfully read configs if needed
    let reconfig_event_found = events.iter().any(|e| e.is_new_epoch_event());
    if reconfig_event_found {
        // Pre-validate that we can read configs before notifying anyone
        let new_configs = self.read_on_chain_configs(version)?;
        
        // Notify event subscribers
        self.notify_event_subscribers(version, events)?;
        
        // Now notify reconfig subscribers (we know configs are readable)
        for (_, reconfig_subscription) in self.reconfig_subscriptions.iter_mut() {
            reconfig_subscription.notify_subscriber_of_configs(version, new_configs.clone())?;
        }
    } else {
        // No reconfig, just notify event subscribers
        self.notify_event_subscribers(version, events)?;
    }

    Ok(())
}
```

Alternatively, implement a **two-phase commit** where:
1. Validate all operations can succeed (read configs, check channels)
2. Only then perform actual notifications
3. If phase 2 fails, implement compensation logic

Additionally, add **monitoring and automatic recovery**:
```rust
// Log detailed error context for debugging
if let Err(e) = self.notify_reconfiguration_subscribers(version) {
    error!(
        "CRITICAL: Failed to notify reconfig subscribers at version {} after event subscribers were notified: {:?}. \
         This creates inconsistent state. Manual intervention required.",
        version, e
    );
    // Trigger alerts for operator intervention
}
```

## Proof of Concept

This vulnerability manifests as an operational failure rather than a direct exploit. A realistic reproduction scenario:

```rust
// Test demonstrating the partial notification failure
#[tokio::test]
async fn test_partial_notification_on_reconfig_failure() {
    let mut mock_db = MockDbReaderWriter::new();
    
    // Setup: Simulate database that fails on second read
    let read_count = Arc::new(AtomicUsize::new(0));
    let read_count_clone = read_count.clone();
    mock_db.expect_state_view_at_version()
        .returning(move |_| {
            if read_count_clone.fetch_add(1, Ordering::SeqCst) == 0 {
                Ok(MockStateView::new()) // First read succeeds (for event processing)
            } else {
                Err(anyhow!("Database connection lost")) // Second read fails (for reconfig)
            }
        });
    
    let mut service = EventSubscriptionService::new(Arc::new(RwLock::new(mock_db)));
    
    // Create event and reconfig subscribers
    let event_listener = service.subscribe_to_events(
        vec![new_epoch_event_key()], 
        vec![]
    ).unwrap();
    let reconfig_listener = service.subscribe_to_reconfigurations().unwrap();
    
    // Emit a NewEpochEvent at version 100
    let new_epoch_event = ContractEvent::new_epoch_event(2);
    
    // This should fail with partial notifications
    let result = service.notify_events(100, vec![new_epoch_event]);
    
    assert!(result.is_err(), "Should fail due to database error");
    
    // Verify partial notification occurred
    let event_received = event_listener.try_recv();
    assert!(event_received.is_ok(), "Event subscriber WAS notified");
    
    let reconfig_received = reconfig_listener.try_recv();  
    assert!(reconfig_received.is_err(), "Reconfig subscriber was NOT notified");
    
    // This represents the inconsistent state: event subscribers got their notification
    // but reconfig subscribers (consensus, mempool, etc.) did not
}
```

## Notes

This vulnerability is particularly severe during **epoch transitions with DKG**, where: [8](#0-7) 

If DKG components receive the `DKGStartEvent` (via event subscription) but not the reconfiguration notification (with new epoch state), they cannot properly execute the DKG protocol. This could **block epoch transitions network-wide**.

### Citations

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L96-109)
```rust
        // Notify the storage service of the committed transactions
        storage_service_notification_handler
            .notify_storage_service_of_committed_transactions(latest_synced_version)
            .await?;

        // Notify mempool of the committed transactions
        mempool_notification_handler
            .notify_mempool_of_committed_transactions(transactions, blockchain_timestamp_usecs)
            .await?;

        // Notify the event subscription service of the events
        event_subscription_service
            .lock()
            .notify_events(latest_synced_version, events)?;
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L264-275)
```rust
    fn notify_reconfiguration_subscribers(&mut self, version: Version) -> Result<(), Error> {
        if self.reconfig_subscriptions.is_empty() {
            return Ok(()); // No reconfiguration subscribers!
        }

        let new_configs = self.read_on_chain_configs(version)?;
        for (_, reconfig_subscription) in self.reconfig_subscriptions.iter_mut() {
            reconfig_subscription.notify_subscriber_of_configs(version, new_configs.clone())?;
        }

        Ok(())
    }
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L281-307)
```rust
    fn read_on_chain_configs(
        &self,
        version: Version,
    ) -> Result<OnChainConfigPayload<DbBackedOnChainConfig>, Error> {
        let db_state_view = &self
            .storage
            .read()
            .reader
            .state_view_at_version(Some(version))
            .map_err(|error| {
                Error::UnexpectedErrorEncountered(format!(
                    "Failed to create account state view {:?}",
                    error
                ))
            })?;
        let epoch = ConfigurationResource::fetch_config(&db_state_view)
            .ok_or_else(|| {
                Error::UnexpectedErrorEncountered("Configuration resource does not exist!".into())
            })?
            .epoch();

        // Return the new on-chain config payload (containing all found configs at this version).
        Ok(OnChainConfigPayload::new(
            epoch,
            DbBackedOnChainConfig::new(self.storage.read().reader.clone(), version),
        ))
    }
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L311-326)
```rust
    fn notify_events(&mut self, version: Version, events: Vec<ContractEvent>) -> Result<(), Error> {
        if events.is_empty() {
            return Ok(()); // No events!
        }

        // Notify event subscribers and check if a reconfiguration event was processed
        let reconfig_event_processed = self.notify_event_subscribers(version, events)?;

        // If a reconfiguration event was found, also notify the reconfig subscribers
        // of the new configuration values.
        if reconfig_event_processed {
            self.notify_reconfiguration_subscribers(version)
        } else {
            Ok(())
        }
    }
```

**File:** aptos-node/src/state_sync.rs (L63-89)
```rust
    // Create a reconfiguration subscription for mempool
    let mempool_reconfig_subscription = event_subscription_service
        .subscribe_to_reconfigurations()
        .expect("Mempool must subscribe to reconfigurations");

    // Create a reconfiguration subscription for consensus observer (if enabled)
    let consensus_observer_reconfig_subscription =
        if node_config.consensus_observer.observer_enabled {
            Some(
                event_subscription_service
                    .subscribe_to_reconfigurations()
                    .expect("Consensus observer must subscribe to reconfigurations"),
            )
        } else {
            None
        };

    // Create a reconfiguration subscription for consensus
    let consensus_reconfig_subscription = if node_config.base.role.is_validator() {
        Some(
            event_subscription_service
                .subscribe_to_reconfigurations()
                .expect("Consensus must subscribe to reconfigurations"),
        )
    } else {
        None
    };
```

**File:** aptos-node/src/state_sync.rs (L92-115)
```rust
    let dkg_subscriptions = if node_config.base.role.is_validator() {
        let reconfig_events = event_subscription_service
            .subscribe_to_reconfigurations()
            .expect("DKG must subscribe to reconfigurations");
        let dkg_start_events = event_subscription_service
            .subscribe_to_events(vec![], vec!["0x1::dkg::DKGStartEvent".to_string()])
            .expect("Consensus must subscribe to DKG events");
        Some((reconfig_events, dkg_start_events))
    } else {
        None
    };

    // Create reconfiguration subscriptions for JWK consensus
    let jwk_consensus_subscriptions = if node_config.base.role.is_validator() {
        let reconfig_events = event_subscription_service
            .subscribe_to_reconfigurations()
            .expect("JWK consensus must subscribe to reconfigurations");
        let jwk_updated_events = event_subscription_service
            .subscribe_to_events(vec![], vec!["0x1::jwks::ObservedJWKsUpdated".to_string()])
            .expect("JWK consensus must subscribe to DKG events");
        Some((reconfig_events, jwk_updated_events))
    } else {
        None
    };
```

**File:** mempool/src/shared_mempool/coordinator.rs (L267-291)
```rust
/// Spawn a task to restart the transaction validator with the new reconfig data.
async fn handle_mempool_reconfig_event<NetworkClient, TransactionValidator, ConfigProvider>(
    smp: &mut SharedMempool<NetworkClient, TransactionValidator>,
    bounded_executor: &BoundedExecutor,
    config_update: OnChainConfigPayload<ConfigProvider>,
) where
    NetworkClient: NetworkClientInterface<MempoolSyncMsg> + 'static,
    TransactionValidator: TransactionValidation + 'static,
    ConfigProvider: OnChainConfigProvider,
{
    info!(LogSchema::event_log(
        LogEntry::ReconfigUpdate,
        LogEvent::Received
    ));
    let _timer =
        counters::task_spawn_latency_timer(counters::RECONFIG_EVENT_LABEL, counters::SPAWN_LABEL);

    bounded_executor
        .spawn(tasks::process_config_update(
            config_update,
            smp.validator.clone(),
            smp.broadcast_within_validator_network.clone(),
        ))
        .await;
}
```

**File:** aptos-move/framework/aptos-framework/sources/reconfiguration_with_dkg.move (L22-40)
```text
    /// Trigger a reconfiguration with DKG.
    /// Do nothing if one is already in progress.
    public(friend) fun try_start() {
        let incomplete_dkg_session = dkg::incomplete_session();
        if (option::is_some(&incomplete_dkg_session)) {
            let session = option::borrow(&incomplete_dkg_session);
            if (dkg::session_dealer_epoch(session) == reconfiguration::current_epoch()) {
                return
            }
        };
        reconfiguration_state::on_reconfig_start();
        let cur_epoch = reconfiguration::current_epoch();
        dkg::start(
            cur_epoch,
            randomness_config::current(),
            stake::cur_validator_consensus_infos(),
            stake::next_validator_consensus_infos(),
        );
    }
```
