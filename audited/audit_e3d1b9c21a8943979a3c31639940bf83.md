# Audit Report

## Title
Partial Pruner Failure in Sharded Storage Mode Causes Referential Integrity Violation and Query Failures

## Summary
The LedgerPruner's parallel execution of sub-pruners (TransactionInfoPruner, EventStorePruner, etc.) in sharded storage mode lacks cross-database transactional atomicity. If TransactionInfoPruner successfully commits its deletions but EventStorePruner fails, transaction info records are deleted while corresponding events remain, creating dangling references that cause unexpected query failures.

## Finding Description

The vulnerability exists in the pruning architecture when storage sharding is enabled. In this mode, each sub-database (transaction_info_db, event_db, etc.) is a separate RocksDB instance. [1](#0-0) 

The sub-pruners execute in parallel using `par_iter().try_for_each()`. Each pruner independently commits to its own separate database: [2](#0-1) [3](#0-2) 

When storage sharding is enabled, these are separate physical databases: [4](#0-3) 

**The Attack Scenario:**

1. LedgerMetadataPruner successfully prunes versions 100-109 and commits LedgerPrunerProgress to 110
2. In parallel execution:
   - TransactionInfoPruner successfully deletes transaction infos 100-109 and commits to transaction_info_db
   - EventStorePruner fails (disk I/O error, indexer DB failure) and does NOT commit to event_db
3. The `try_for_each()` returns an error, preventing progress update
4. **Result**: Transaction infos 100-109 are deleted, but events 100-109 still exist

When queries attempt to access these versions: [5](#0-4) 

The query will fail with `AptosDbError::NotFound` because transaction info is missing: [6](#0-5) 

This violates the **State Consistency** invariant requiring atomic state transitions and creates a referential integrity violation where events reference non-existent transaction metadata.

## Impact Explanation

This is a **High Severity** issue per Aptos bug bounty criteria:

- **API Crashes**: Queries for supposedly available versions (based on min_readable_version) fail with NotFound errors instead of proper "version pruned" errors
- **Significant Protocol Violations**: Breaks referential integrity between transaction info and events, violating the atomic state transition invariant
- **State Inconsistencies**: Database enters an inconsistent state requiring manual intervention or re-sync to recover

The issue affects validator nodes running with storage sharding enabled, causing:
- Unexpected API query failures for end users
- Inconsistent state across database components
- Potential node reliability degradation

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability requires:
1. Storage sharding to be enabled (common in production deployments for performance)
2. A sub-pruner to fail during the pruning window

Realistic failure scenarios include:
- Disk I/O errors during event_db writes
- Disk space exhaustion affecting one database but not others
- Internal indexer DB failures in EventStorePruner
- File system corruption affecting specific RocksDB instances

While not trivially exploitable by external attackers, these are natural operational failures that can occur in production environments, especially under heavy load or degraded hardware conditions.

## Recommendation

Implement a two-phase commit protocol for sub-pruner operations or ensure all sub-pruners track and synchronize their progress before updating metadata progress.

**Solution 1: Sequential Execution with Progress Tracking**

Replace parallel execution with sequential execution that updates a shared progress tracker only after ALL pruners succeed:

```rust
fn prune(&self, max_versions: usize) -> Result<Version> {
    // ... existing code ...
    
    // Execute sub-pruners SEQUENTIALLY to ensure atomicity
    for sub_pruner in &self.sub_pruners {
        sub_pruner.prune(progress, current_batch_target_version)
            .map_err(|err| {
                // Log which pruner failed for diagnostics
                anyhow!("{} failed to prune: {err}", sub_pruner.name())
            })?;
    }
    
    progress = current_batch_target_version;
    self.record_progress(progress);
    // ...
}
```

**Solution 2: Add Consistency Validation**

Before updating progress, verify all sub-pruners have synchronized progress:

```rust
fn validate_sub_pruner_consistency(&self, target: Version) -> Result<()> {
    for sub_pruner in &self.sub_pruners {
        let sub_progress = sub_pruner.get_progress()?;
        if sub_progress < target {
            bail!("{} progress {} lags behind target {}", 
                  sub_pruner.name(), sub_progress, target);
        }
    }
    Ok(())
}
```

## Proof of Concept

**Rust Test to Demonstrate the Vulnerability:**

```rust
#[test]
fn test_partial_pruner_failure_creates_dangling_events() {
    use tempfile::tempdir;
    
    // Setup: Create AptosDB with sharding enabled
    let tmpdir = tempdir().unwrap();
    let mut rocksdb_configs = RocksdbConfigs::default();
    rocksdb_configs.enable_storage_sharding = true;
    
    let db = AptosDB::new_for_test_with_sharding(&tmpdir, rocksdb_configs);
    
    // Commit transactions with events for versions 100-109
    for version in 100..110 {
        let txn = create_test_transaction_with_events(version);
        db.save_transactions(...).unwrap();
    }
    
    // Simulate partial failure: 
    // 1. Mock EventStorePruner to fail
    // 2. Inject failure into event_db write_schemas
    let event_db = db.ledger_db().event_db();
    inject_write_failure(event_db);
    
    // Trigger pruning
    let pruner = db.ledger_pruner();
    let result = pruner.prune(10);
    
    // Pruning should fail
    assert!(result.is_err());
    
    // Verify inconsistent state:
    // - Transaction infos are deleted
    for version in 100..110 {
        let txn_info_result = db.ledger_db()
            .transaction_info_db()
            .get_transaction_info(version);
        assert!(txn_info_result.is_err()); // Deleted!
    }
    
    // - Events still exist
    for version in 100..110 {
        let events = db.ledger_db()
            .event_db()
            .get_events_by_version(version)
            .unwrap();
        assert!(!events.is_empty()); // Still present - dangling!
    }
    
    // Verify query fails with NotFound instead of proper pruned error
    let query_result = db.get_transactions(105, 1, 200, true);
    match query_result {
        Err(AptosDbError::NotFound(_)) => {
            // This is the bug - should return "pruned" error instead
        },
        _ => panic!("Expected NotFound error for dangling reference"),
    }
}
```

**Notes:**

This vulnerability demonstrates a classic distributed database consistency problem where parallel operations on independent storage systems lack transactional coordination. The issue is particularly severe because it creates a silent data inconsistency that manifests as unexpected query failures, making it difficult to diagnose and potentially requiring manual database repair or full re-synchronization to resolve.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L78-84)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_info_pruner.rs (L25-33)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        TransactionInfoDb::prune(current_progress, target_version, &mut batch)?;
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::TransactionInfoPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
        self.ledger_db.transaction_info_db().write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L43-81)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        let mut indexer_batch = None;

        let indices_batch = if let Some(indexer_db) = self.indexer_db() {
            if indexer_db.event_enabled() {
                indexer_batch = Some(SchemaBatch::new());
            }
            indexer_batch.as_mut()
        } else {
            Some(&mut batch)
        };
        let num_events_per_version = self.ledger_db.event_db().prune_event_indices(
            current_progress,
            target_version,
            indices_batch,
        )?;
        self.ledger_db.event_db().prune_events(
            num_events_per_version,
            current_progress,
            target_version,
            &mut batch,
        )?;
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::EventPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        if let Some(mut indexer_batch) = indexer_batch {
            indexer_batch.put::<InternalIndexerMetadataSchema>(
                &IndexerMetadataKey::EventPrunerProgress,
                &IndexerMetadataValue::Version(target_version),
            )?;
            self.expect_indexer_db()
                .get_inner_db_ref()
                .write_schemas(indexer_batch)?;
        }
        self.ledger_db.event_db().write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L184-277)
```rust
            s.spawn(|_| {
                let event_db_raw = Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(EVENT_DB_NAME),
                        EVENT_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                );
                event_db = Some(EventDb::new(
                    event_db_raw.clone(),
                    EventStore::new(event_db_raw),
                ));
            });
            s.spawn(|_| {
                persisted_auxiliary_info_db = Some(PersistedAuxiliaryInfoDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(PERSISTED_AUXILIARY_INFO_DB_NAME),
                        PERSISTED_AUXILIARY_INFO_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_accumulator_db = Some(TransactionAccumulatorDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_ACCUMULATOR_DB_NAME),
                        TRANSACTION_ACCUMULATOR_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_auxiliary_data_db = Some(TransactionAuxiliaryDataDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_AUXILIARY_DATA_DB_NAME),
                        TRANSACTION_AUXILIARY_DATA_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )))
            });
            s.spawn(|_| {
                transaction_db = Some(TransactionDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_DB_NAME),
                        TRANSACTION_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_info_db = Some(TransactionInfoDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_INFO_DB_NAME),
                        TRANSACTION_INFO_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                write_set_db = Some(WriteSetDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(WRITE_SET_DB_NAME),
                        WRITE_SET_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L287-293)
```rust
            let txn_infos = (start_version..start_version + limit)
                .map(|version| {
                    self.ledger_db
                        .transaction_info_db()
                        .get_transaction_info(version)
                })
                .collect::<Result<Vec<_>>>()?;
```

**File:** storage/aptosdb/src/ledger_db/transaction_info_db.rs (L52-58)
```rust
    pub(crate) fn get_transaction_info(&self, version: Version) -> Result<TransactionInfo> {
        self.db
            .get::<TransactionInfoSchema>(&version)?
            .ok_or_else(|| {
                AptosDbError::NotFound(format!("No TransactionInfo at version {}", version))
            })
    }
```
