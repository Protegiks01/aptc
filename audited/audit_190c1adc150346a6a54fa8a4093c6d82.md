# Audit Report

## Title
Resource Exhaustion via False Data Advertisement in State Sync Bootstrap Process

## Summary
Malicious peers can advertise epoch ending ledger infos and other sync data they don't actually possess, causing validators to waste significant resources (up to ~9.5 minutes per malicious peer) through repeated retry attempts with exponential backoff before the peer is eventually ignored. This creates a resource exhaustion attack vector during the critical bootstrapping phase.

## Finding Description

The state sync system relies on trust-based data advertisement where peers broadcast their available data ranges via `StorageServerSummary` without cryptographic proof of possession. [1](#0-0) 

When validators bootstrap, they query the network for advertised data and create data streams to fetch epoch ending ledger infos. [2](#0-1) 

The vulnerability manifests in the following attack flow:

1. **Advertisement Without Verification**: Malicious peers advertise fake data ranges that don't exist in their storage. The system validates requests against advertised ranges but doesn't verify actual data possession. [3](#0-2) 

2. **Trust-Based Request Routing**: The data client checks if peers claim to have data via `can_service_request()` before routing requests, but this only validates against advertised summaries. [4](#0-3) 

3. **Retry Mechanism Exploitation**: When requests fail (malicious peer doesn't actually have the data), the system retries with exponential backoff up to 5 times per stream. [5](#0-4) 

4. **Resource Waste Calculation**:
   - Retry timeouts with exponential backoff (base 10s, max 60s): 10s + 20s + 40s + 60s + 60s = ~190 seconds per stream
   - Peer scoring takes ~14 bad responses to reach ignore threshold
   - Approximately 3 stream cycles before peer ignored
   - **Total: ~9.5 minutes wasted per malicious peer** [6](#0-5) 

5. **Gradual Mitigation**: While peer scoring eventually reduces malicious peer scores below the ignore threshold, the punishment is gradual (0.95x multiplier per bad response), allowing substantial resource waste. [7](#0-6) 

The `AdvertisedDataError` itself occurs when no valid advertised data is found after malicious peers are ignored, compounding the delay. [8](#0-7) 

## Impact Explanation

This vulnerability meets **Medium Severity** criteria per Aptos bug bounty guidelines:

- **Validator Node Slowdowns**: During bootstrapping, validators waste ~9.5 minutes per malicious peer attempting to sync fake data, significantly delaying their ability to participate in consensus.

- **State Inconsistencies Requiring Intervention**: If multiple coordinated attackers target bootstrapping validators simultaneously, the cumulative delay could prevent validators from syncing within acceptable timeframes, potentially requiring manual intervention to identify and ban malicious peers.

- **Amplification through Coordination**: Multiple malicious peers can multiply the impact linearly. With 10 coordinated attackers, a validator could waste ~95 minutes before syncing completes.

- **Critical Bootstrap Phase Vulnerability**: The attack is most impactful during initial bootstrap when validators desperately need to sync to the latest epoch to participate in consensus.

## Likelihood Explanation

**High Likelihood** - The attack requires minimal sophistication:

1. **Low Barrier to Entry**: Attacker only needs to run a modified storage service node that advertises false data ranges. No validator keys, stake, or consensus participation required.

2. **No Upfront Verification**: The system accepts peer advertisements without cryptographic proof of data possession.

3. **Predictable Exploitation Window**: The ~9.5 minute exploitation window per peer is deterministic and guaranteed by the retry configuration. [9](#0-8) 

4. **Network Reachability**: Any peer that can establish P2P connections can execute this attack.

5. **Bootstrapping is Common**: New validators, recovering validators, and validators rejoining after downtime all undergo bootstrapping, creating frequent attack opportunities.

## Recommendation

Implement a **data possession challenge-response mechanism** before trusting advertised data:

1. **Add Cryptographic Proof Requirement**: Before accepting advertisements, require peers to provide merkle proofs for random samples of their advertised data ranges.

2. **Faster Ban on Advertisement Mismatch**: Implement aggressive scoring penalties specifically for advertisement mismatches (0.5x multiplier instead of 0.95x) to reduce exploitation window from ~9.5 minutes to ~2-3 minutes.

3. **Advertisement Verification During Polling**: When the poller updates peer storage summaries, perform spot-checks by requesting random data within advertised ranges and immediately downgrade peers that fail.

4. **Rate Limit Advertisement Changes**: Prevent malicious peers from repeatedly advertising different fake ranges by rate-limiting how often peers can update their storage summaries.

5. **Implement Request Moderator Enhancement**: Extend the request moderator to track advertisement-vs-delivery failures separately from general request failures, with stricter thresholds:

```rust
// In moderator.rs
pub struct UnhealthyPeerState {
    // ... existing fields ...
    advertisement_mismatch_count: u64,  // NEW: Track false advertisements
    max_advertisement_mismatches: u64,  // NEW: Lower threshold (e.g., 3)
}

// Immediately ignore peer after 3 advertisement mismatches
if advertisement_mismatch_count >= 3 {
    self.ignore_start_time = Some(self.time_service.now());
}
```

## Proof of Concept

**Attack Setup (Malicious Peer):**

```rust
// Modified storage service that advertises fake data
impl StorageReaderInterface for MaliciousStorage {
    fn get_data_summary(&self) -> Result<DataSummary> {
        // Advertise fake epoch ending ledger infos
        Ok(DataSummary {
            epoch_ending_ledger_infos: Some(CompleteDataRange::new(0, 1000)?),
            synced_ledger_info: Some(fake_ledger_info),
            // ... other fake ranges
        })
    }
    
    // When actual requests arrive, return errors or timeout
    fn get_epoch_ending_ledger_infos(&self, ...) -> Result<EpochChangeProof> {
        Err(Error::DataNotFound("Fake advertisement".into()))
    }
}
```

**Exploitation Steps:**

1. Deploy modified storage service node advertising epochs 0-1000
2. Victim validator starts bootstrapping, queries global data summary
3. Victim sees malicious peer's advertisement, creates stream requesting epochs
4. Each request fails, triggering 5 retries with exponential backoff
5. Monitor victim's metrics: `RETRIED_DATA_REQUESTS` counter increases
6. After ~190 seconds, stream terminates
7. Process repeats approximately 3 times before peer score drops below 25.0
8. Total exploitation window: ~9.5 minutes per bootstrapping validator

**Verification:**
- Check `state-sync/data-streaming-service/src/data_stream.rs::request_failure_count` increments
- Observe `max_request_retry` threshold hit after 5 failures
- Confirm peer score reduction in `state-sync/aptos-data-client/src/peer_states.rs`
- Measure total time until peer ignored: ~570 seconds

This demonstrates how the gradual peer scoring mechanism allows prolonged resource waste despite being designed to eventually mitigate the attack.

## Notes

The vulnerability exists because state sync prioritizes network flexibility (accepting advertisements from diverse peers) over upfront verification. While the peer scoring system provides eventual mitigation, the exploitation window is substantial enough to impact validator operations, especially when attackers coordinate or target validators during critical bootstrap phases. The ~9.5 minute delay per attacker may seem modest, but becomes significant when multiple malicious peers coordinate or when validators need to sync urgently to participate in consensus.

### Citations

**File:** state-sync/aptos-data-client/src/global_summary.rs (L63-89)
```rust
/// A summary of all data that is currently advertised in the network.
#[derive(Clone, Eq, PartialEq)]
pub struct AdvertisedData {
    /// The ranges of epoch ending ledger infos advertised, e.g., if a range
    /// is (X,Y), it means all epoch ending ledger infos for epochs X->Y
    /// (inclusive) are available.
    pub epoch_ending_ledger_infos: Vec<CompleteDataRange<Epoch>>,

    /// The ranges of states advertised, e.g., if a range is
    /// (X,Y), it means all states are held for every version X->Y
    /// (inclusive).
    pub states: Vec<CompleteDataRange<Version>>,

    /// The ledger infos corresponding to the highest synced versions
    /// currently advertised.
    pub synced_ledger_infos: Vec<LedgerInfoWithSignatures>,

    /// The ranges of transactions advertised, e.g., if a range is
    /// (X,Y), it means all transactions for versions X->Y (inclusive)
    /// are available.
    pub transactions: Vec<CompleteDataRange<Version>>,

    /// The ranges of transaction outputs advertised, e.g., if a range
    /// is (X,Y), it means all transaction outputs for versions X->Y
    /// (inclusive) are available.
    pub transaction_outputs: Vec<CompleteDataRange<Version>>,
}
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L814-876)
```rust
    async fn fetch_epoch_ending_ledger_infos(
        &mut self,
        global_data_summary: &GlobalDataSummary,
    ) -> Result<(), Error> {
        // Verify the waypoint can be satisfied
        self.verify_waypoint_is_satisfiable(global_data_summary)?;

        // Get the highest advertised epoch that has ended
        let highest_advertised_epoch_end = global_data_summary
            .advertised_data
            .highest_epoch_ending_ledger_info()
            .ok_or_else(|| {
                Error::AdvertisedDataError(
                    "No highest advertised epoch end found in the network!".into(),
                )
            })?;

        // Fetch the highest epoch end known locally
        let highest_known_ledger_info = self.get_highest_known_ledger_info()?;
        let highest_known_ledger_info = highest_known_ledger_info.ledger_info();
        let highest_local_epoch_end = if highest_known_ledger_info.ends_epoch() {
            highest_known_ledger_info.epoch()
        } else if highest_known_ledger_info.epoch() > 0 {
            highest_known_ledger_info
                .epoch()
                .checked_sub(1)
                .ok_or_else(|| {
                    Error::IntegerOverflow("The highest local epoch end has overflown!".into())
                })?
        } else {
            unreachable!("Genesis should always end the first epoch!");
        };

        // Compare the highest local epoch end to the highest advertised epoch end
        if highest_local_epoch_end < highest_advertised_epoch_end {
            info!(LogSchema::new(LogEntry::Bootstrapper).message(&format!(
                "Found higher epoch ending ledger infos in the network! Local: {:?}, advertised: {:?}",
                   highest_local_epoch_end, highest_advertised_epoch_end
            )));
            let next_epoch_end = highest_local_epoch_end.checked_add(1).ok_or_else(|| {
                Error::IntegerOverflow("The next epoch end has overflown!".into())
            })?;
            let epoch_ending_stream = self
                .streaming_client
                .get_all_epoch_ending_ledger_infos(next_epoch_end)
                .await?;
            self.active_data_stream = Some(epoch_ending_stream);
        } else if self.verified_epoch_states.verified_waypoint() {
            info!(LogSchema::new(LogEntry::Bootstrapper).message(
                "No new epoch ending ledger infos to fetch! All peers are in the same epoch!"
            ));
            self.verified_epoch_states
                .set_fetched_epoch_ending_ledger_infos();
        } else {
            return Err(Error::AdvertisedDataError(format!(
                "Our waypoint is unverified, but there's no higher epoch ending ledger infos \
                advertised! Highest local epoch end: {:?}, highest advertised epoch end: {:?}",
                highest_local_epoch_end, highest_advertised_epoch_end
            )));
        };

        Ok(())
    }
```

**File:** state-sync/storage-service/types/src/responses.rs (L688-808)
```rust
impl DataSummary {
    /// Returns true iff the request can be serviced
    pub fn can_service(
        &self,
        aptos_data_client_config: &AptosDataClientConfig,
        time_service: TimeService,
        request: &StorageServiceRequest,
    ) -> bool {
        match &request.data_request {
            GetServerProtocolVersion | GetStorageServerSummary => true,
            GetEpochEndingLedgerInfos(request) => {
                let desired_range =
                    match CompleteDataRange::new(request.start_epoch, request.expected_end_epoch) {
                        Ok(desired_range) => desired_range,
                        Err(_) => return false,
                    };
                self.epoch_ending_ledger_infos
                    .map(|range| range.superset_of(&desired_range))
                    .unwrap_or(false)
            },
            GetNewTransactionOutputsWithProof(_) => can_service_optimistic_request(
                aptos_data_client_config,
                time_service,
                self.synced_ledger_info.as_ref(),
            ),
            GetNewTransactionsWithProof(_) => can_service_optimistic_request(
                aptos_data_client_config,
                time_service,
                self.synced_ledger_info.as_ref(),
            ),
            GetNewTransactionsOrOutputsWithProof(_) => can_service_optimistic_request(
                aptos_data_client_config,
                time_service,
                self.synced_ledger_info.as_ref(),
            ),
            GetNumberOfStatesAtVersion(version) => self
                .states
                .map(|range| range.contains(*version))
                .unwrap_or(false),
            GetStateValuesWithProof(request) => {
                let proof_version = request.version;

                let can_serve_states = self
                    .states
                    .map(|range| range.contains(request.version))
                    .unwrap_or(false);

                let can_create_proof = self
                    .synced_ledger_info
                    .as_ref()
                    .map(|li| li.ledger_info().version() >= proof_version)
                    .unwrap_or(false);

                can_serve_states && can_create_proof
            },
            GetTransactionOutputsWithProof(request) => self
                .can_service_transaction_outputs_with_proof(
                    request.start_version,
                    request.end_version,
                    request.proof_version,
                ),
            GetTransactionsWithProof(request) => self.can_service_transactions_with_proof(
                request.start_version,
                request.end_version,
                request.proof_version,
            ),
            GetTransactionsOrOutputsWithProof(request) => self
                .can_service_transactions_or_outputs_with_proof(
                    request.start_version,
                    request.end_version,
                    request.proof_version,
                ),
            SubscribeTransactionOutputsWithProof(_) => can_service_subscription_request(
                aptos_data_client_config,
                time_service,
                self.synced_ledger_info.as_ref(),
            ),
            SubscribeTransactionsOrOutputsWithProof(_) => can_service_subscription_request(
                aptos_data_client_config,
                time_service,
                self.synced_ledger_info.as_ref(),
            ),
            SubscribeTransactionsWithProof(_) => can_service_subscription_request(
                aptos_data_client_config,
                time_service,
                self.synced_ledger_info.as_ref(),
            ),

            // Transaction data v2 requests (transactions with auxiliary data)
            GetTransactionDataWithProof(request) => match request.transaction_data_request_type {
                TransactionDataRequestType::TransactionData(_) => self
                    .can_service_transactions_with_proof(
                        request.start_version,
                        request.end_version,
                        request.proof_version,
                    ),
                TransactionDataRequestType::TransactionOutputData => self
                    .can_service_transaction_outputs_with_proof(
                        request.start_version,
                        request.end_version,
                        request.proof_version,
                    ),
                TransactionDataRequestType::TransactionOrOutputData(_) => self
                    .can_service_transactions_or_outputs_with_proof(
                        request.start_version,
                        request.end_version,
                        request.proof_version,
                    ),
            },
            GetNewTransactionDataWithProof(_) => can_service_optimistic_request(
                aptos_data_client_config,
                time_service,
                self.synced_ledger_info.as_ref(),
            ),
            SubscribeTransactionDataWithProof(_) => can_service_subscription_request(
                aptos_data_client_config,
                time_service,
                self.synced_ledger_info.as_ref(),
            ),
        }
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L32-43)
```rust
/// Scores for peer rankings based on preferences and behavior.
const MAX_SCORE: f64 = 100.0;
const MIN_SCORE: f64 = 0.0;
const STARTING_SCORE: f64 = 50.0;
/// Add this score on a successful response.
const SUCCESSFUL_RESPONSE_DELTA: f64 = 1.0;
/// Not necessarily a malicious response, but not super useful.
const NOT_USEFUL_MULTIPLIER: f64 = 0.95;
/// Likely to be a malicious response.
const MALICIOUS_MULTIPLIER: f64 = 0.8;
/// Ignore a peer when their score dips below this threshold.
const IGNORE_PEER_THRESHOLD: f64 = 25.0;
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L198-227)
```rust
    /// Returns true if a connected storage service peer can actually fulfill a
    /// request, given our current view of their advertised data summary.
    pub fn can_service_request(
        &self,
        peer: &PeerNetworkId,
        time_service: TimeService,
        request: &StorageServiceRequest,
    ) -> bool {
        // Storage services can always respond to data advertisement requests.
        // We need this outer check, since we need to be able to send data summary
        // requests to new peers (who don't have a peer state yet).
        if request.data_request.is_storage_summary_request()
            || request.data_request.is_protocol_version_request()
        {
            return true;
        }

        // Check if the peer can service the request
        if let Some(peer_state) = self.peer_to_state.get(peer) {
            return match peer_state.get_storage_summary_if_not_ignored() {
                Some(storage_summary) => {
                    storage_summary.can_service(&self.data_client_config, time_service, request)
                },
                None => false, // The peer is temporarily ignored
            };
        }

        // Otherwise, the request cannot be serviced
        false
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L329-392)
```rust
    /// Sends a given request to the data client to be forwarded to the network
    /// and returns a pending client response. If `request_retry` is true
    /// exponential backoff takes affect (i.e., to increase the request timeout).
    fn send_client_request(
        &mut self,
        request_retry: bool,
        data_client_request: DataClientRequest,
    ) -> PendingClientResponse {
        // Create a new pending client response
        let pending_client_response = Arc::new(Mutex::new(Box::new(
            data_notification::PendingClientResponse::new(data_client_request.clone()),
        )));

        // Calculate the request timeout to use, based on the
        // request type and the number of previous failures.
        let request_timeout_ms = if data_client_request.is_optimistic_fetch_request() {
            self.data_client_config.optimistic_fetch_timeout_ms
        } else if data_client_request.is_subscription_request() {
            self.data_client_config.subscription_response_timeout_ms
        } else if !request_retry {
            self.data_client_config.response_timeout_ms
        } else {
            let response_timeout_ms = self.data_client_config.response_timeout_ms;
            let max_response_timeout_ms = self.data_client_config.max_response_timeout_ms;

            // Exponentially increase the timeout based on the number of
            // previous failures (but bounded by the max timeout).
            let request_timeout_ms = min(
                max_response_timeout_ms,
                response_timeout_ms * (u32::pow(2, self.request_failure_count as u32) as u64),
            );

            // Update the retry counter and log the request
            increment_counter_multiple_labels(
                &metrics::RETRIED_DATA_REQUESTS,
                data_client_request.get_label(),
                &request_timeout_ms.to_string(),
            );
            info!(
                (LogSchema::new(LogEntry::RetryDataRequest)
                    .stream_id(self.data_stream_id)
                    .message(&format!(
                        "Retrying data request type: {:?}, with new timeout: {:?} (ms)",
                        data_client_request.get_label(),
                        request_timeout_ms.to_string()
                    )))
            );

            request_timeout_ms
        };

        // Send the request to the network
        let join_handle = spawn_request_task(
            self.data_stream_id,
            data_client_request,
            self.aptos_data_client.clone(),
            pending_client_response.clone(),
            request_timeout_ms,
            self.stream_update_notifier.clone(),
        );
        self.spawned_tasks.push(join_handle);

        pending_client_response
    }
```

**File:** config/src/config/state_sync_config.rs (L265-282)
```rust
impl Default for DataStreamingServiceConfig {
    fn default() -> Self {
        Self {
            dynamic_prefetching: DynamicPrefetchingConfig::default(),
            enable_subscription_streaming: false,
            global_summary_refresh_interval_ms: 50,
            max_concurrent_requests: MAX_CONCURRENT_REQUESTS,
            max_concurrent_state_requests: MAX_CONCURRENT_STATE_REQUESTS,
            max_data_stream_channel_sizes: 50,
            max_notification_id_mappings: 300,
            max_num_consecutive_subscriptions: 45, // At ~3 blocks per second, this should last ~15 seconds
            max_pending_requests: 50,
            max_request_retry: 5,
            max_subscription_stream_lag_secs: 10, // 10 seconds
            progress_check_interval_ms: 50,
        }
    }
}
```

**File:** config/src/config/state_sync_config.rs (L460-484)
```rust
impl Default for AptosDataClientConfig {
    fn default() -> Self {
        Self {
            enable_transaction_data_v2: true,
            data_poller_config: AptosDataPollerConfig::default(),
            data_multi_fetch_config: AptosDataMultiFetchConfig::default(),
            ignore_low_score_peers: true,
            latency_filtering_config: AptosLatencyFilteringConfig::default(),
            latency_monitor_loop_interval_ms: 100,
            max_epoch_chunk_size: MAX_EPOCH_CHUNK_SIZE,
            max_num_output_reductions: 0,
            max_optimistic_fetch_lag_secs: 20, // 20 seconds
            max_response_bytes: CLIENT_MAX_MESSAGE_SIZE_V2 as u64,
            max_response_timeout_ms: 60_000, // 60 seconds
            max_state_chunk_size: MAX_STATE_CHUNK_SIZE,
            max_subscription_lag_secs: 20, // 20 seconds
            max_transaction_chunk_size: MAX_TRANSACTION_CHUNK_SIZE,
            max_transaction_output_chunk_size: MAX_TRANSACTION_OUTPUT_CHUNK_SIZE,
            optimistic_fetch_timeout_ms: 5000,         // 5 seconds
            progress_check_max_stall_time_secs: 86400, // 24 hours (long enough to debug any issues at runtime)
            response_timeout_ms: 10_000,               // 10 seconds
            subscription_response_timeout_ms: 15_000, // 15 seconds (longer than a regular timeout because of prefetching)
            use_compression: true,
        }
    }
```

**File:** state-sync/state-sync-driver/src/error.rs (L13-14)
```rust
    #[error("Advertised data error: {0}")]
    AdvertisedDataError(String),
```
