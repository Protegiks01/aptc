# Audit Report

## Title
JWK Consensus Manager Panic on Unexpected Channel Closure in `tokio::select!` Loop

## Summary
The `IssuerLevelConsensusManager::run()` function uses `tokio::select!` with `select_next_some()` on multiple `FusedStream` channels. When any channel sender is unexpectedly dropped (due to component crash, shutdown error, or network failure), the next loop iteration causes a panic with "SelectNextSome polled after terminated", crashing the validator node's JWK consensus subsystem.

## Finding Description

The JWK consensus manager implements an event loop that processes messages from multiple channels using `tokio::select!`: [1](#0-0) 

The channels implement `FusedStream` which tracks termination state: [2](#0-1) 

When all senders are dropped, the stream becomes terminated: [3](#0-2) 

**The Critical Flaw**: Unlike `futures::select!` (which respects `FusedStream::is_terminated()` and automatically disables terminated branches), `tokio::select!` does NOT check termination status and continues polling all branches every iteration. The `select_next_some()` combinator explicitly panics when called on a terminated `FusedStream`, as documented in test cases: [4](#0-3) 

**Attack Scenario**:
1. Any component holding a channel sender crashes or improperly shuts down (e.g., `jwk_updated_rx`, `rpc_req_rx`, `local_observation_rx`, or `qc_update_rx`)
2. The sender is dropped, causing the receiver to return `Poll::Ready(None)` and set `stream_terminated = true`
3. On the next iteration of `while !this.stopped`, `tokio::select!` polls all branches including the terminated one
4. `select_next_some()` detects it's being polled on a terminated stream and panics with "SelectNextSome polled after terminated"
5. The panic propagates, crashing the JWK consensus manager task
6. The validator node loses JWK consensus participation until manual restart

For comparison, the `RecoveryManager` correctly uses `futures::select!` which handles `FusedStream` gracefully: [5](#0-4) 

## Impact Explanation

**Severity: Medium** per Aptos Bug Bounty criteria:
- **Node Availability Loss**: Validator crashes and stops participating in JWK consensus, requiring manual node restart
- **Consensus Liveness Impact**: If multiple validators are affected simultaneously, JWK consensus could stall network-wide
- **State Inconsistency**: Partial JWK updates may be committed before crash, requiring operator intervention

This meets Medium severity criteria as it causes "State inconsistencies requiring intervention" and impacts node operational availability, but does not result in fund loss or permanent consensus safety violations.

## Likelihood Explanation

**Likelihood: Medium-High**

This vulnerability can be triggered by:
- **Component crashes**: Any upstream component (network layer, state sync, observer threads) crashing will drop its channel sender
- **Improper shutdown**: Race conditions during epoch transitions or node shutdown can drop senders before receivers
- **Network disconnections**: Prolonged network partitions causing observer or RPC components to terminate
- **Memory pressure**: OOM conditions causing background tasks to be killed

The JWK consensus manager creates local observers that spawn background tasks: [6](#0-5) 

If any observer task panics or is terminated, its sender clone is dropped. The identical vulnerability exists in `KeyLevelConsensusManager`: [7](#0-6) 

## Recommendation

**Fix: Replace `tokio::select!` with `futures::select!`**

The `futures::select!` macro properly handles `FusedStream` by checking `is_terminated()` and automatically disabling terminated branches, preventing the panic. This is the pattern used elsewhere in the consensus layer.

```rust
// In jwk_manager/mod.rs, line 139
while !this.stopped {
    let handle_result = futures::select! {  // Changed from tokio::select!
        jwk_updated = jwk_updated_rx.select_next_some() => {
            let ObservedJWKsUpdated { jwks, .. } = jwk_updated;
            this.reset_with_on_chain_state(jwks)
        },
        (_sender, msg) = rpc_req_rx.select_next_some() => {
            this.process_peer_request(msg)
        },
        qc_update = this.qc_update_rx.select_next_some() => {
            this.process_quorum_certified_update(qc_update)
        },
        (issuer, jwks) = local_observation_rx.select_next_some() => {
            let jwks = jwks.into_iter().map(JWKMoveStruct::from).collect();
            this.process_new_observation(issuer, jwks)
        },
        ack_tx = close_rx.select_next_some() => {
            this.tear_down(ack_tx.ok()).await
        }
    };
    // ... error handling
}
```

Apply the same fix to `KeyLevelConsensusManager` in `jwk_manager_per_key.rs`.

**Alternative: Add explicit termination checks**
```rust
while !this.stopped {
    if jwk_updated_rx.is_terminated() && rpc_req_rx.is_terminated() 
        && this.qc_update_rx.is_terminated() && local_observation_rx.is_terminated() {
        warn!("All channels terminated, exiting JWK consensus loop");
        break;
    }
    // ... existing tokio::select! code
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_jwk_manager_panic_on_channel_closure() {
    use aptos_channels::{aptos_channel, message_queues::QueueStyle};
    use futures_util::StreamExt;
    
    // Create a channel and immediately drop the sender
    let (tx, mut rx) = aptos_channel::new::<(), ()>(QueueStyle::KLAST, 1, None);
    drop(tx);
    
    // First select_next_some() returns None (stream terminated)
    assert!(matches!(rx.select_next_some().now_or_never(), None));
    
    // Verify stream is terminated
    assert!(rx.is_terminated());
    
    // Second select_next_some() on terminated stream should panic
    // This simulates what happens in tokio::select! on next loop iteration
    let result = std::panic::catch_unwind(std::panic::AssertUnwindSafe(|| {
        tokio::runtime::Runtime::new().unwrap().block_on(async {
            // Simulate tokio::select! polling the terminated stream
            rx.select_next_some().await
        })
    }));
    
    // Verify the panic occurs with expected message
    assert!(result.is_err());
    // The actual panic message is "SelectNextSome polled after terminated"
}
```

To reproduce the vulnerability in the actual JWK manager, create a scenario where any component holding a sender reference crashes while the manager is running, then observe the panic on the next loop iteration.

## Notes

This is a systemic issue affecting both `IssuerLevelConsensusManager` and `KeyLevelConsensusManager`. The vulnerability demonstrates the importance of using the correct async primitive for the use case: `futures::select!` for `FusedStream` handling vs. `tokio::select!` for general async operations.

### Citations

**File:** crates/aptos-jwk-consensus/src/jwk_manager/mod.rs (L108-134)
```rust
        this.jwk_observers = oidc_providers
            .unwrap_or_default()
            .into_provider_vec()
            .into_iter()
            .filter_map(|provider| {
                let OIDCProvider { name, config_url } = provider;
                let maybe_issuer = String::from_utf8(name);
                let maybe_config_url = String::from_utf8(config_url);
                match (maybe_issuer, maybe_config_url) {
                    (Ok(issuer), Ok(config_url)) => Some(JWKObserver::spawn(
                        this.epoch_state.epoch,
                        this.my_addr,
                        issuer,
                        config_url,
                        Duration::from_secs(10),
                        local_observation_tx.clone(),
                    )),
                    (maybe_issuer, maybe_config_url) => {
                        warn!(
                            "unable to spawn observer, issuer={:?}, config_url={:?}",
                            maybe_issuer, maybe_config_url
                        );
                        None
                    },
                }
            })
            .collect();
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager/mod.rs (L139-157)
```rust
            let handle_result = tokio::select! {
                jwk_updated = jwk_updated_rx.select_next_some() => {
                    let ObservedJWKsUpdated { jwks, .. } = jwk_updated;
                    this.reset_with_on_chain_state(jwks)
                },
                (_sender, msg) = rpc_req_rx.select_next_some() => {
                    this.process_peer_request(msg)
                },
                qc_update = this.qc_update_rx.select_next_some() => {
                    this.process_quorum_certified_update(qc_update)
                },
                (issuer, jwks) = local_observation_rx.select_next_some() => {
                    let jwks = jwks.into_iter().map(JWKMoveStruct::from).collect();
                    this.process_new_observation(issuer, jwks)
                },
                ack_tx = close_rx.select_next_some() => {
                    this.tear_down(ack_tx.ok()).await
                }
            };
```

**File:** crates/channel/src/aptos_channel.rs (L178-181)
```rust
        // all senders have been dropped (and so the stream is terminated)
        } else if shared_state.num_senders == 0 {
            shared_state.stream_terminated = true;
            Poll::Ready(None)
```

**File:** crates/channel/src/aptos_channel.rs (L189-193)
```rust
impl<K: Eq + Hash + Clone, M> FusedStream for Receiver<K, M> {
    fn is_terminated(&self) -> bool {
        self.shared_state.lock().stream_terminated
    }
}
```

**File:** state-sync/data-streaming-service/src/tests/streaming_service.rs (L1552-1591)
```rust
#[tokio::test(flavor = "multi_thread")]
#[should_panic(expected = "SelectNextSome polled after terminated")]
async fn test_terminate_stream() {
    // Create a new streaming client and service
    let streaming_client = create_streaming_client_and_service();

    // Request a state value stream
    let mut stream_listener = streaming_client
        .get_all_state_values(MAX_ADVERTISED_STATES - 1, None)
        .await
        .unwrap();

    // Fetch the first state value notification and then terminate the stream
    let data_notification = get_data_notification(&mut stream_listener).await.unwrap();
    match data_notification.data_payload {
        DataPayload::StateValuesWithProof(_) => {},
        data_payload => unexpected_payload_type!(data_payload),
    }

    // Terminate the stream
    let result = streaming_client
        .terminate_stream_with_feedback(
            stream_listener.data_stream_id,
            Some(NotificationAndFeedback::new(
                data_notification.notification_id,
                NotificationFeedback::InvalidPayloadData,
            )),
        )
        .await;
    assert_ok!(result);

    // Verify the streaming service has removed the stream (polling should panic)
    loop {
        let data_notification = get_data_notification(&mut stream_listener).await.unwrap();
        match data_notification.data_payload {
            DataPayload::StateValuesWithProof(_) => {},
            DataPayload::EndOfStream => panic!("The stream should have terminated!"),
            data_payload => unexpected_payload_type!(data_payload),
        }
    }
```

**File:** consensus/src/recovery_manager.rs (L131-170)
```rust
            futures::select! {
                (peer_id, event) = event_rx.select_next_some() => {
                    let result = match event {
                        VerifiedEvent::ProposalMsg(proposal_msg) => {
                            monitor!(
                                "process_recovery",
                                self.process_proposal_msg(*proposal_msg).await
                            )
                        }
                        VerifiedEvent::VoteMsg(vote_msg) => {
                            monitor!("process_recovery", self.process_vote_msg(*vote_msg).await)
                        }
                        VerifiedEvent::UnverifiedSyncInfo(sync_info) => {
                            monitor!(
                                "process_recovery",
                                self.sync_up(&sync_info, peer_id).await
                            )
                        }
                        unexpected_event => Err(anyhow!("Unexpected event: {:?}", unexpected_event)),
                    }
                    .with_context(|| format!("from peer {}", peer_id));

                    match result {
                        Ok(_) => {
                            info!("Recovery finishes for epoch {}, RecoveryManager stopped. Please restart the node", self.epoch_state.epoch);
                            process::exit(0);
                        },
                        Err(e) => {
                            counters::ERROR_COUNT.inc();
                            warn!(error = ?e, kind = error_kind(&e));
                        }
                    }
                }
                close_req = close_rx.select_next_some() => {
                    if let Ok(ack_sender) = close_req {
                        ack_sender.send(()).expect("[RecoveryManager] Fail to ack shutdown");
                    }
                    break;
                }
            }
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L416-433)
```rust
            let handle_result = tokio::select! {
                jwk_updated = jwk_updated_rx.select_next_some() => {
                    let ObservedJWKsUpdated { jwks, .. } = jwk_updated;
                    this.reset_with_on_chain_state(jwks)
                },
                (_sender, msg) = rpc_req_rx.select_next_some() => {
                    this.process_peer_request(msg)
                },
                qc_update = this.qc_update_rx.select_next_some() => {
                    this.process_quorum_certified_update(qc_update)
                },
                (issuer, jwks) = local_observation_rx.select_next_some() => {
                    this.process_new_observation(issuer, jwks)
                },
                ack_tx = close_rx.select_next_some() => {
                    this.tear_down(ack_tx.ok()).await
                }
            };
```
