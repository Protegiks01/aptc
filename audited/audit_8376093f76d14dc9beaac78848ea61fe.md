# Audit Report

## Title
Consensus Observer Subscription Progress Detection Bypass Enabling Permanent Denial of Sync

## Summary
A malicious validator can exploit the decoupling between message receipt detection and database progress verification in the consensus observer subscription system to prevent observer nodes from syncing. By sending messages that pass initial validation but don't result in database commits, attackers can trigger repeated `SubscriptionProgressStopped` errors, causing observers to continuously reset subscriptions without making progress.

## Finding Description

The consensus observer subscription system has a critical architectural flaw where **message receipt** and **database sync progress** are tracked independently, allowing malicious validators to manipulate the progress detection mechanism.

### Vulnerable Architecture

When a consensus observer receives messages from a validator, two independent health checks occur:

1. **Message Timeout Check**: Verifies messages are being received by checking `last_message_receive_time` [1](#0-0) 

2. **Sync Progress Check**: Verifies the database is making progress by checking `get_latest_ledger_info_version()` [2](#0-1) 

### The Exploit Mechanism

When ANY message is received from an active subscription, `verify_message_for_subscription()` updates the `last_message_receive_time`: [3](#0-2) 

However, this happens **before** messages are validated for actual processability. A malicious validator can craft messages that:

**Pass Initial Validation** (updating message receipt time):
- Valid message structure and format
- Proper cryptographic signatures
- Correct network protocol encoding

**But Fail to Result in DB Commits** due to:

1. **Blocks with missing parent IDs** - Dropped silently after verification: [4](#0-3) 

2. **Blocks for wrong epochs** - Rejected after proof verification: [5](#0-4) 

3. **Blocks without payloads** - Stored as pending but never processed: [6](#0-5) 

4. **Invalid payload verification** - Fails verification checks: [7](#0-6) 

### Attack Execution Path

1. Observer subscribes to a malicious validator
2. Malicious validator sends messages (e.g., `OrderedBlock`, `CommitDecision`) every ~10 seconds
3. Messages pass `verify_message_for_subscription()`, updating `last_message_receive_time`
4. Messages are designed to fail later validation stages (missing parents, wrong epoch, etc.)
5. No actual DB commits occur, so `get_latest_ledger_info_version()` remains unchanged
6. After 15 seconds (`max_subscription_sync_timeout_ms`), `check_syncing_progress()` detects no progress
7. Returns `SubscriptionProgressStopped` error: [8](#0-7) 

8. Subscription is terminated: [9](#0-8) 

9. New subscription is created by selecting "optimal" peers: [10](#0-9) 

### Critical Missing Safeguard: No Persistent Blacklisting

The terminated peers are only excluded **temporarily** during the current subscription creation cycle: [11](#0-10) 

The `terminated_subscription_peers` list is not persisted across cycles. Peer selection is based solely on distance from validators and ping latency, with no tracking of historical subscription health: [12](#0-11) 

### Repeated Attack Cycle

10. Observer may re-subscribe to the same malicious validator (if it's still the most "optimal" peer)
11. Or subscribe to another colluding malicious validator
12. Steps 2-11 repeat indefinitely
13. Observer never catches up to the blockchain state
14. Eventually triggers fallback mode, but can re-enter the same cycle after fallback completes

## Impact Explanation

This vulnerability meets **High Severity** criteria per the Aptos bug bounty program:

**Validator Node Slowdowns**: Consensus observer nodes (typically Validator Fullnodes) are critical infrastructure that:
- Serve API requests for applications and users
- Provide state sync data to other nodes
- Enable real-time blockchain monitoring

By preventing these nodes from syncing, the attack causes:
- Degraded network availability for applications
- Increased latency for state queries
- Potential cascade failures if multiple observer nodes are affected
- Resource exhaustion from repeated subscription resets

**Significant Protocol Violations**: The attack breaks the consensus observer's fundamental guarantee of eventually catching up with the blockchain state, violating the system's availability invariant.

Default configuration parameters enable the attack: [13](#0-12) 

With `max_subscription_sync_timeout_ms: 15_000` (15 seconds) and `progress_check_interval_ms: 5_000` (5 seconds), an attacker needs to send only 2-3 malformed messages per 15-second window to maintain the attack.

## Likelihood Explanation

**High Likelihood** for the following reasons:

1. **Low Attack Complexity**: Malicious validators only need to:
   - Send structurally valid messages every ~10 seconds
   - Craft messages that fail later validation stages (many options available)
   - No sophisticated cryptographic attacks or state manipulation required

2. **Minimal Attacker Requirements**:
   - Single compromised or malicious validator
   - Standard network connectivity
   - Knowledge of message formats (publicly available in codebase)

3. **No Detection or Mitigation**:
   - No persistent tracking of problematic validators
   - No rate limiting on subscription resets
   - No anomaly detection for repeated `SubscriptionProgressStopped` errors
   - No exponential backoff for subscription recreation

4. **Observable Behavior**: The attack is difficult to distinguish from legitimate network issues, making detection and response challenging.

5. **Scalable Impact**: A single malicious validator can affect multiple observer nodes simultaneously if it's selected as an "optimal" peer by multiple observers.

## Recommendation

Implement a **multi-layered defense** approach:

### 1. Persistent Peer Health Scoring
Add a peer reputation system that tracks subscription health across cycles:

```rust
// In SubscriptionManager
struct PeerHealthScore {
    peer_network_id: PeerNetworkId,
    total_subscriptions: u64,
    failed_subscriptions: u64,
    progress_stopped_count: u64,
    last_failure_time: Instant,
    // Score: 0.0 (worst) to 1.0 (best)
    health_score: f64,
}
```

### 2. Exponential Backoff for Failed Peers
When a peer causes `SubscriptionProgressStopped`, implement exponential backoff before allowing resubscription:

```rust
fn calculate_backoff_duration(failure_count: u64) -> Duration {
    let base_duration = Duration::from_secs(60); // 1 minute
    let max_duration = Duration::from_secs(3600); // 1 hour
    let backoff = base_duration * 2u32.pow(failure_count.min(10) as u32);
    std::cmp::min(backoff, max_duration)
}
```

### 3. Separate Progress Validation Gates
Add a preliminary progress check **before** updating `last_message_receive_time`:

```rust
pub fn verify_message_for_subscription(
    &mut self,
    message_sender: PeerNetworkId,
    message: &ConsensusObserverDirectSend,
) -> Result<(), Error> {
    if let Some(active_subscription) = self
        .active_observer_subscriptions
        .lock()
        .get_mut(&message_sender)
    {
        // NEW: Verify message could potentially lead to progress
        if !self.message_could_make_progress(message)? {
            return Err(Error::InvalidMessageError(
                "Message unlikely to result in DB progress".to_string()
            ));
        }
        
        active_subscription.update_last_message_receive_time();
        return Ok(());
    }
    // ... rest of function
}
```

### 4. Enhanced Monitoring and Alerting
Add metrics to detect the attack pattern:
- Counter for `SubscriptionProgressStopped` errors per peer
- Histogram of subscription durations before termination
- Alert when subscription reset rate exceeds threshold

### 5. Diversified Subscription Strategy
Modify peer selection to avoid repeatedly selecting the same peers:
- Maintain a cooldown period for recently terminated subscriptions
- Randomize selection among equally "optimal" peers
- Implement forced rotation after N successful subscriptions to the same peer

## Proof of Concept

```rust
// Simulation of the attack (pseudo-code for reproduction)

#[tokio::test]
async fn test_subscription_progress_bypass_attack() {
    // Setup: Create consensus observer with mock DB reader
    let mut mock_db = MockDatabaseReader::new();
    mock_db
        .expect_get_latest_ledger_info_version()
        .returning(|| Ok(100)); // DB version never increases
    
    let consensus_observer_config = ConsensusObserverConfig {
        max_subscription_sync_timeout_ms: 15_000,
        max_subscription_timeout_ms: 15_000,
        ..Default::default()
    };
    
    let time_service = TimeService::mock();
    let malicious_peer = PeerNetworkId::random();
    
    // Create subscription to malicious peer
    let mut subscription = ConsensusObserverSubscription::new(
        consensus_observer_config,
        Arc::new(mock_db),
        malicious_peer,
        time_service.clone(),
    );
    
    let mut peers_and_metadata = HashMap::new();
    add_metadata_for_peer(&mut peers_and_metadata, malicious_peer, true, false);
    
    // Attack simulation: Send messages every 5 seconds for 20 seconds
    let mock_time = time_service.into_mock();
    
    for i in 0..4 {
        // Malicious validator sends a message
        subscription.update_last_message_receive_time();
        
        // Advance time by 5 seconds
        mock_time.advance(Duration::from_secs(5));
        
        // Check subscription health (should pass until 15 seconds)
        let health_result = subscription.check_subscription_health(
            &peers_and_metadata,
            false
        );
        
        if i < 3 {
            // First 3 checks (0s, 5s, 10s) - should be OK
            assert!(health_result.is_ok());
        } else {
            // At 15+ seconds without DB progress - should fail with SubscriptionProgressStopped
            assert_matches!(
                health_result,
                Err(Error::SubscriptionProgressStopped(_))
            );
        }
    }
    
    // Verify the attack succeeded:
    // 1. Message timeout was prevented (last_message_receive_time updated)
    // 2. Progress timeout was triggered (DB version didn't increase)
    // 3. Subscription would be terminated and recreated
    // 4. Without persistent blacklisting, could subscribe to same peer again
}
```

**Steps to demonstrate the attack in practice:**

1. Deploy a consensus observer node
2. Configure a malicious validator to send `OrderedBlock` messages with:
   - Valid structure and signatures
   - Parent block ID that doesn't exist in the observer's store
   - Send every 10 seconds
3. Observer subscribes to malicious validator
4. Monitor logs for `SubscriptionProgressStopped` errors every ~15 seconds
5. Observe repeated subscription resets
6. Verify observer DB version remains unchanged (no progress)
7. Confirm attack persists indefinitely if malicious validator remains "optimal"

## Notes

This vulnerability represents a **fundamental architectural issue** in the consensus observer subscription system. The separation of concerns between message receipt and progress validation, while elegant in design, creates an exploitable gap that malicious actors can leverage.

The attack is particularly concerning because:
- It's **indistinguishable from legitimate network issues** without deep inspection
- It requires **minimal resources** from the attacker (just periodic message sending)
- It affects **critical infrastructure** (observer nodes that serve applications)
- There's **no automatic recovery mechanism** without manual intervention

The recommended fixes address multiple layers of the attack surface and should be implemented in combination for comprehensive protection.

### Citations

**File:** consensus/src/consensus_observer/observer/subscription.rs (L164-182)
```rust
    /// Verifies that the subscription has not timed out based
    /// on the last received message time.
    fn check_subscription_timeout(&self) -> Result<(), Error> {
        // Calculate the duration since the last message
        let time_now = self.time_service.now();
        let duration_since_last_message = time_now.duration_since(self.last_message_receive_time);

        // Check if the subscription has timed out
        if duration_since_last_message
            > Duration::from_millis(self.consensus_observer_config.max_subscription_timeout_ms)
        {
            return Err(Error::SubscriptionTimeout(format!(
                "Subscription to peer: {} has timed out! No message received for: {:?}",
                self.peer_network_id, duration_since_last_message
            )));
        }

        Ok(())
    }
```

**File:** consensus/src/consensus_observer/observer/subscription.rs (L184-222)
```rust
    /// Verifies that the DB is continuing to sync and commit new data
    fn check_syncing_progress(&mut self) -> Result<(), Error> {
        // Get the current time and synced version from storage
        let time_now = self.time_service.now();
        let current_synced_version =
            self.db_reader
                .get_latest_ledger_info_version()
                .map_err(|error| {
                    Error::UnexpectedError(format!(
                        "Failed to read highest synced version: {:?}",
                        error
                    ))
                })?;

        // Verify that the synced version is increasing appropriately
        let (highest_synced_version, highest_version_timestamp) =
            self.highest_synced_version_and_time;
        if current_synced_version <= highest_synced_version {
            // The synced version hasn't increased. Check if we should terminate
            // the subscription based on the last time the highest synced version was seen.
            let duration_since_highest_seen = time_now.duration_since(highest_version_timestamp);
            let timeout_duration = Duration::from_millis(
                self.consensus_observer_config
                    .max_subscription_sync_timeout_ms,
            );
            if duration_since_highest_seen > timeout_duration {
                return Err(Error::SubscriptionProgressStopped(format!(
                    "The DB is not making sync progress! Highest synced version: {}, elapsed: {:?}",
                    highest_synced_version, duration_since_highest_seen
                )));
            }
            return Ok(()); // We haven't timed out yet
        }

        // Update the highest synced version and time
        self.highest_synced_version_and_time = (current_synced_version, time_now);

        Ok(())
    }
```

**File:** consensus/src/consensus_observer/observer/subscription_manager.rs (L207-227)
```rust
        // Spawn a new subscription creation task
        let subscription_creation_task = tokio::spawn(async move {
            // Identify the terminated subscription peers
            let terminated_subscription_peers = terminated_subscriptions
                .iter()
                .map(|(peer, _)| *peer)
                .collect();

            // Create the new subscriptions
            let new_subscriptions = subscription_utils::create_new_subscriptions(
                consensus_observer_config,
                consensus_observer_client,
                consensus_publisher,
                db_reader,
                time_service,
                connected_peers_and_metadata,
                num_subscriptions_to_create,
                active_subscription_peers,
                terminated_subscription_peers,
            )
            .await;
```

**File:** consensus/src/consensus_observer/observer/subscription_manager.rs (L270-305)
```rust
    /// Terminates any unhealthy subscriptions and returns the list of terminated subscriptions
    fn terminate_unhealthy_subscriptions(
        &mut self,
        connected_peers_and_metadata: &HashMap<PeerNetworkId, PeerMetadata>,
    ) -> Vec<(PeerNetworkId, Error)> {
        // Go through all active subscriptions and terminate any unhealthy ones
        let mut terminated_subscriptions = vec![];
        for subscription_peer in self.get_active_subscription_peers() {
            // To avoid terminating too many subscriptions at once, we should skip
            // the peer optimality check if we've already terminated a subscription.
            let skip_peer_optimality_check = !terminated_subscriptions.is_empty();

            // Check the health of the subscription and terminate it if needed
            if let Err(error) = self.check_subscription_health(
                connected_peers_and_metadata,
                subscription_peer,
                skip_peer_optimality_check,
            ) {
                // Log the subscription termination error
                warn!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "Terminating subscription to peer: {:?}! Termination reason: {:?}",
                        subscription_peer, error
                    ))
                );

                // Unsubscribe from the peer and remove the subscription
                self.unsubscribe_from_peer(subscription_peer);

                // Add the peer to the list of terminated subscriptions
                terminated_subscriptions.push((subscription_peer, error));
            }
        }

        terminated_subscriptions
    }
```

**File:** consensus/src/consensus_observer/observer/subscription_manager.rs (L363-385)
```rust
    pub fn verify_message_for_subscription(
        &mut self,
        message_sender: PeerNetworkId,
    ) -> Result<(), Error> {
        // Check if the message is from an active subscription
        if let Some(active_subscription) = self
            .active_observer_subscriptions
            .lock()
            .get_mut(&message_sender)
        {
            // Update the last message receive time and return early
            active_subscription.update_last_message_receive_time();
            return Ok(());
        }

        // Otherwise, the message is not from an active subscription.
        // Send another unsubscribe request, and return an error.
        self.unsubscribe_from_peer(message_sender);
        Err(Error::InvalidMessageError(format!(
            "Received message from unexpected peer, and not an active subscription: {}!",
            message_sender
        )))
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L704-713)
```rust
        // If all payloads exist, process the block. Otherwise, store it
        // in the pending block store and wait for the payloads to arrive.
        if self.all_payloads_exist(pending_block_with_metadata.ordered_block().blocks()) {
            self.process_ordered_block(pending_block_with_metadata)
                .await;
        } else {
            self.observer_block_data
                .lock()
                .insert_pending_block(pending_block_with_metadata);
        }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L743-752)
```rust
        } else {
            // Drop the block and log an error (the block should always be for the current epoch)
            error!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Received ordered block for a different epoch! Ignoring: {:?}",
                    ordered_block.proof_block_info()
                ))
            );
            return;
        };
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L754-771)
```rust
        // Verify the block payloads against the ordered block
        if let Err(error) = self
            .observer_block_data
            .lock()
            .verify_payloads_against_ordered_block(&ordered_block)
        {
            // Log the error and update the invalid message counter
            error!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Failed to verify block payloads against ordered block! Ignoring: {:?}, from peer: {:?}. Error: {:?}",
                    ordered_block.proof_block_info(),
                    peer_network_id,
                    error
                ))
            );
            increment_invalid_message_counter(&peer_network_id, metrics::ORDERED_BLOCK_LABEL);
            return;
        }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L773-800)
```rust
        // The block was verified correctly. If the block is a child of our
        // last block, we can insert it into the ordered block store.
        let last_ordered_block = self.observer_block_data.lock().get_last_ordered_block();
        if last_ordered_block.id() == ordered_block.first_block().parent_id() {
            // Update the latency metrics for ordered block processing
            update_message_processing_latency_metrics(
                message_received_time,
                &peer_network_id,
                metrics::ORDERED_BLOCK_LABEL,
            );

            // Insert the ordered block into the pending blocks
            self.observer_block_data
                .lock()
                .insert_ordered_block(observed_ordered_block.clone());

            // If state sync is not syncing to a commit, finalize the ordered blocks
            if !self.state_sync_manager.is_syncing_to_commit() {
                self.finalize_ordered_block(ordered_block).await;
            }
        } else {
            warn!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Parent block for ordered block is missing! Ignoring: {:?}",
                    ordered_block.proof_block_info()
                ))
            );
        }
```

**File:** consensus/src/consensus_observer/observer/subscription_utils.rs (L242-273)
```rust
/// Produces a list of sorted peers to service the subscription requests.
/// Any active or unhealthy subscriptions are excluded from the selection process.
/// Likewise, any peers currently subscribed to us are also excluded.
fn sort_peers_for_subscriptions(
    mut connected_peers_and_metadata: HashMap<PeerNetworkId, PeerMetadata>,
    active_subscription_peers: Vec<PeerNetworkId>,
    unhealthy_subscription_peers: Vec<PeerNetworkId>,
    consensus_publisher: Option<Arc<ConsensusPublisher>>,
) -> Option<Vec<PeerNetworkId>> {
    // Remove any peers we're already subscribed to
    for active_subscription_peer in active_subscription_peers {
        let _ = connected_peers_and_metadata.remove(&active_subscription_peer);
    }

    // Remove any unhealthy subscription peers
    for unhealthy_peer in unhealthy_subscription_peers {
        let _ = connected_peers_and_metadata.remove(&unhealthy_peer);
    }

    // Remove any peers that are currently subscribed to us
    if let Some(consensus_publisher) = consensus_publisher {
        for peer_network_id in consensus_publisher.get_active_subscribers() {
            let _ = connected_peers_and_metadata.remove(&peer_network_id);
        }
    }

    // Sort the peers by subscription optimality
    let sorted_peers = sort_peers_by_subscription_optimality(&connected_peers_and_metadata);

    // Return the sorted peers
    Some(sorted_peers)
}
```

**File:** consensus/src/consensus_observer/observer/subscription_utils.rs (L283-312)
```rust
pub fn sort_peers_by_subscription_optimality(
    peers_and_metadata: &HashMap<PeerNetworkId, PeerMetadata>,
) -> Vec<PeerNetworkId> {
    // Group peers and latencies by validator distance, i.e., distance -> [(peer, latency)]
    let mut unsupported_peers = Vec::new();
    let mut peers_and_latencies_by_distance = BTreeMap::new();
    for (peer_network_id, peer_metadata) in peers_and_metadata {
        // Verify that the peer supports consensus observer
        if !supports_consensus_observer(peer_metadata) {
            unsupported_peers.push(*peer_network_id);
            continue; // Skip the peer
        }

        // Get the distance and latency for the peer
        let distance = get_distance_for_peer(peer_network_id, peer_metadata);
        let latency = get_latency_for_peer(peer_network_id, peer_metadata);

        // If the distance is not found, use the maximum distance
        let distance =
            distance.unwrap_or(aptos_peer_monitoring_service_types::MAX_DISTANCE_FROM_VALIDATORS);

        // If the latency is not found, use a large latency
        let latency = latency.unwrap_or(MAX_PING_LATENCY_SECS);

        // Add the peer and latency to the distance group
        peers_and_latencies_by_distance
            .entry(distance)
            .or_insert_with(Vec::new)
            .push((*peer_network_id, OrderedFloat(latency)));
    }
```

**File:** config/src/config/consensus_observer_config.rs (L63-84)
```rust
impl Default for ConsensusObserverConfig {
    fn default() -> Self {
        Self {
            observer_enabled: false,
            publisher_enabled: false,
            max_network_channel_size: 1000,
            max_parallel_serialization_tasks: num_cpus::get(), // Default to the number of CPUs
            network_request_timeout_ms: 5_000,                 // 5 seconds
            garbage_collection_interval_ms: 60_000,            // 60 seconds
            max_num_pending_blocks: 150, // 150 blocks (sufficient for existing production networks)
            progress_check_interval_ms: 5_000, // 5 seconds
            max_concurrent_subscriptions: 2, // 2 streams should be sufficient
            max_subscription_sync_timeout_ms: 15_000, // 15 seconds
            max_subscription_timeout_ms: 15_000, // 15 seconds
            subscription_peer_change_interval_ms: 180_000, // 3 minutes
            subscription_refresh_interval_ms: 600_000, // 10 minutes
            observer_fallback_duration_ms: 600_000, // 10 minutes
            observer_fallback_startup_period_ms: 60_000, // 60 seconds
            observer_fallback_progress_threshold_ms: 10_000, // 10 seconds
            observer_fallback_sync_lag_threshold_ms: 15_000, // 15 seconds
        }
    }
```
