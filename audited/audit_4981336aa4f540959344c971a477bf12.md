# Audit Report

## Title
Consensus Liveness Vulnerability via Timeout Message Queue Exhaustion

## Summary
The bounded `consensus_messages` queue (size 10, FIFO) can drop incoming `RoundTimeoutMsg` messages during high network activity, preventing validators from forming timeout certificates (TCs) required to advance rounds. This can cause affected validators to stall indefinitely, and if enough validators are impacted simultaneously, consensus liveness can be lost across the network.

## Finding Description

The Aptos consensus system uses a bounded per-sender message queue to receive consensus messages from network peers. The critical configuration is: [1](#0-0) 

All consensus messages—including `RoundTimeoutMsg`—compete for space in this same 10-message queue: [2](#0-1) 

When the queue reaches capacity, the FIFO queue style drops **the newest incoming messages**: [3](#0-2) 

Critically, message drops are **silent** — the push operation returns `Ok(())` with only metric updates: [4](#0-3) 

**Attack Scenario:**

1. During periods of high network activity (many proposals, votes, sync info exchanges), a validator's per-sender queues fill up
2. When honest validators broadcast timeout messages after a round fails, these timeout messages are dropped from full queues
3. The affected validator cannot accumulate 2f+1 timeout messages to form a timeout certificate: [5](#0-4) 

4. Without a TC, the validator cannot advance to the next round: [6](#0-5) 

5. The validator remains stuck, repeatedly timing out locally but never forming a TC

**Why SyncInfo Redundancy Fails:**

While timeout messages carry SyncInfo that could propagate TCs indirectly, this recovery mechanism fails when:
- ALL message types are being dropped due to queue saturation
- The validator processes messages slower than the incoming rate
- Multiple validators experience simultaneous queue exhaustion

## Impact Explanation

**Critical Severity** - This vulnerability can cause:

1. **Total Loss of Liveness**: If >f validators experience queue exhaustion simultaneously, consensus cannot advance rounds because insufficient validators can form TCs or participate in voting

2. **Non-Recoverable Stall**: Affected validators remain stuck indefinitely without manual intervention (node restart to clear queues)

3. **Network Partition**: The network effectively splits between validators that can advance rounds and those stuck waiting for TCs

This meets the Critical severity criteria: *"Total loss of liveness/network availability"* and *"Non-recoverable network partition (requires hardfork)"*.

## Likelihood Explanation

**High Likelihood** in production scenarios:

1. **Natural Occurrence**: During epoch transitions, network congestion, or chain reorganizations, message rates legitimately spike. With 100+ validators each broadcasting proposals, votes, order votes, and sync info, a 10-message per-sender queue is easily saturated

2. **Byzantine Amplification**: A single Byzantine validator can intentionally flood peers with valid messages (repeated SyncInfo, stale votes, etc.) to exhaust queue capacity during critical timeout phases

3. **Cascading Effect**: Once some validators stall, remaining validators must handle higher message loads (retransmissions, sync requests), increasing likelihood of additional stalls

4. **No Prioritization**: Timeout messages have no priority over less critical messages like SyncInfo or EpochRetrievalRequest

## Recommendation

Implement **priority-based message queuing** with separate channels for critical consensus messages:

```rust
// In consensus/src/network.rs
pub fn new(...) -> (NetworkTask, NetworkReceivers) {
    // Dedicated timeout message queue with higher capacity
    let (timeout_messages_tx, timeout_messages) = aptos_channel::new(
        QueueStyle::FIFO,
        50,  // Higher capacity for timeout messages
        Some(&counters::TIMEOUT_CHANNEL_MSGS),
    );
    
    // Regular consensus messages (proposals, votes)
    let (consensus_messages_tx, consensus_messages) = aptos_channel::new(
        QueueStyle::FIFO,
        30,  // Increased from 10
        Some(&counters::CONSENSUS_CHANNEL_MSGS),
    );
    
    // Route timeout messages to dedicated channel
    match msg {
        ConsensusMsg::RoundTimeoutMsg(_) => {
            Self::push_msg(peer_id, msg, &timeout_messages_tx);
        },
        // ... other message types to consensus_messages_tx
    }
}
```

**Alternative**: Implement message prioritization within the existing queue by using a priority-aware QueueStyle that ensures timeout messages are never dropped before less critical messages.

## Proof of Concept

```rust
// Test demonstrating timeout message drops causing TC formation failure
#[tokio::test]
async fn test_timeout_message_queue_exhaustion() {
    // Setup: Create network with small queue size
    let (network_task, mut receivers) = NetworkTask::new(
        network_service_events,
        self_receiver,
    );
    
    // Attacker: Flood queue with 10 SyncInfo messages from Byzantine validator
    for i in 0..10 {
        let sync_info = create_dummy_sync_info();
        network_task.consensus_messages_tx.push(
            (byzantine_validator, discriminant(&sync_info)),
            (byzantine_validator, ConsensusMsg::SyncInfo(sync_info))
        ).unwrap();
    }
    
    // Victim: Try to receive timeout messages from honest validators
    let mut timeout_msgs_received = 0;
    for validator in honest_validators.iter() {
        let timeout_msg = create_timeout_msg(validator, round);
        
        // This push will succeed but message is silently dropped!
        let result = network_task.consensus_messages_tx.push(
            (validator.address(), discriminant(&timeout_msg)),
            (validator.address(), ConsensusMsg::RoundTimeoutMsg(timeout_msg))
        );
        
        assert!(result.is_ok()); // Push succeeds...
        
        // But message was dropped due to queue being full
        timeout_msgs_received += 1;
    }
    
    // Attempt TC formation - will fail because timeout messages were dropped
    let reception_result = round_state.insert_round_timeout(
        &timeout,
        &validator_verifier
    );
    
    // Cannot form TC - not enough signatures collected
    assert!(matches!(
        reception_result,
        VoteReceptionResult::VoteAdded(_) // Not enough votes for TC
    ));
    
    // Validator remains stuck in current round - consensus stalled
    assert_eq!(round_state.current_round(), stuck_round);
}
```

## Notes

This vulnerability is particularly dangerous because:
- Message drops are **silent** with no backpressure signal to senders
- Timeout messages are **time-critical** — missing them during the timeout window prevents TC formation
- The 10-message queue size is **fundamentally inadequate** for networks with 100+ validators where each can legitimately broadcast multiple message types per round
- Recovery requires manual intervention (node restarts) rather than self-healing

The bounded queue design trades memory predictability for consensus liveness guarantees, creating a critical vulnerability in the protocol's liveness properties.

### Citations

**File:** consensus/src/network.rs (L757-761)
```rust
        let (consensus_messages_tx, consensus_messages) = aptos_channel::new(
            QueueStyle::FIFO,
            10,
            Some(&counters::CONSENSUS_CHANNEL_MSGS),
        );
```

**File:** consensus/src/network.rs (L863-900)
```rust
                        consensus_msg @ (ConsensusMsg::ProposalMsg(_)
                        | ConsensusMsg::OptProposalMsg(_)
                        | ConsensusMsg::VoteMsg(_)
                        | ConsensusMsg::RoundTimeoutMsg(_)
                        | ConsensusMsg::OrderVoteMsg(_)
                        | ConsensusMsg::SyncInfo(_)
                        | ConsensusMsg::EpochRetrievalRequest(_)
                        | ConsensusMsg::EpochChangeProof(_)) => {
                            if let ConsensusMsg::ProposalMsg(proposal) = &consensus_msg {
                                observe_block(
                                    proposal.proposal().timestamp_usecs(),
                                    BlockStage::NETWORK_RECEIVED,
                                );
                                info!(
                                    LogSchema::new(LogEvent::NetworkReceiveProposal)
                                        .remote_peer(peer_id),
                                    block_round = proposal.proposal().round(),
                                    block_hash = proposal.proposal().id(),
                                );
                            }
                            if let ConsensusMsg::OptProposalMsg(proposal) = &consensus_msg {
                                observe_block(
                                    proposal.timestamp_usecs(),
                                    BlockStage::NETWORK_RECEIVED,
                                );
                                observe_block(
                                    proposal.timestamp_usecs(),
                                    BlockStage::NETWORK_RECEIVED_OPT_PROPOSAL,
                                );
                                info!(
                                    LogSchema::new(LogEvent::NetworkReceiveOptProposal)
                                        .remote_peer(peer_id),
                                    block_author = proposal.proposer(),
                                    block_epoch = proposal.epoch(),
                                    block_round = proposal.round(),
                                );
                            }
                            Self::push_msg(peer_id, consensus_msg, &self.consensus_messages_tx);
```

**File:** crates/channel/src/message_queues.rs (L134-151)
```rust
        if key_message_queue.len() >= self.max_queue_size.get() {
            if let Some(c) = self.counters.as_ref() {
                c.with_label_values(&["dropped"]).inc();
            }
            match self.queue_style {
                // Drop the newest message for FIFO
                QueueStyle::FIFO => Some(message),
                // Drop the oldest message for LIFO
                QueueStyle::LIFO | QueueStyle::KLAST => {
                    let oldest = key_message_queue.pop_front();
                    key_message_queue.push_back(message);
                    oldest
                },
            }
        } else {
            key_message_queue.push_back(message);
            None
        }
```

**File:** crates/channel/src/aptos_channel.rs (L91-112)
```rust
    pub fn push_with_feedback(
        &self,
        key: K,
        message: M,
        status_ch: Option<oneshot::Sender<ElementStatus<M>>>,
    ) -> Result<()> {
        let mut shared_state = self.shared_state.lock();
        ensure!(!shared_state.receiver_dropped, "Channel is closed");
        debug_assert!(shared_state.num_senders > 0);

        let dropped = shared_state.internal_queue.push(key, (message, status_ch));
        // If this or an existing message had to be dropped because of the queue being full, we
        // notify the corresponding status channel if it was registered.
        if let Some((dropped_val, Some(dropped_status_ch))) = dropped {
            // Ignore errors.
            let _err = dropped_status_ch.send(ElementStatus::Dropped(dropped_val));
        }
        if let Some(w) = shared_state.waker.take() {
            w.wake();
        }
        Ok(())
    }
```

**File:** consensus/src/pending_votes.rs (L234-243)
```rust
        let partial_tc = two_chain_votes.partial_2chain_tc_mut();
        let tc_voting_power =
            match validator_verifier.check_voting_power(partial_tc.signers(), true) {
                Ok(_) => {
                    return match partial_tc.aggregate_signatures(validator_verifier) {
                        Ok(tc_with_sig) => {
                            VoteReceptionResult::New2ChainTimeoutCertificate(Arc::new(tc_with_sig))
                        },
                        Err(e) => VoteReceptionResult::ErrorAggregatingTimeoutCertificate(e),
                    };
```

**File:** consensus/src/liveness/round_state.rs (L245-262)
```rust
    pub fn process_certificates(
        &mut self,
        sync_info: SyncInfo,
        verifier: &ValidatorVerifier,
    ) -> Option<NewRoundEvent> {
        if sync_info.highest_ordered_round() > self.highest_ordered_round {
            self.highest_ordered_round = sync_info.highest_ordered_round();
        }
        let new_round = sync_info.highest_round() + 1;
        if new_round > self.current_round {
            let (prev_round_votes, prev_round_timeout_votes) = self.pending_votes.drain_votes();

            // Start a new round.
            self.current_round = new_round;
            self.pending_votes = PendingVotes::new();
            self.vote_sent = None;
            self.timeout_sent = None;
            let timeout = self.setup_timeout(1);
```
