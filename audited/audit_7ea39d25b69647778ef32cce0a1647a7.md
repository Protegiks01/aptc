# Audit Report

## Title
Race Condition in Signing Phase Retry Mechanism Causes Duplicate Signing Operations

## Summary
The consensus pipeline's retry mechanism in `BufferManager::advance_signing_root()` can cause signing requests to be processed multiple times, violating the exactly-once processing invariant. When a signing request is in-flight but hasn't completed, a concurrent call to `advance_signing_root()` spawns an asynchronous retry task that sends a duplicate request after 100ms. By the time this duplicate executes, the original request may have completed, but the duplicate still gets processed, causing `SafetyRules::sign_commit_vote()` to be invoked twice for the same block.

## Finding Description
The vulnerability exists in the interaction between the retry mechanism and in-flight request processing: [1](#0-0) 

The problematic flow occurs when `advance_signing_root()` is called while a signing request is still being processed:

1. **Initial Request**: `advance_signing_root()` sends a `SigningRequest` for block A to the signing phase channel
2. **Request Processing Begins**: The signing phase pulls the request from the channel and begins processing via `PipelinePhase::start()`: [2](#0-1) 

3. **Concurrent Event**: While the signing operation is in progress, another event triggers `advance_signing_root()` again (e.g., receiving a signing response for a different block, or processing a commit message)

4. **Retry Triggered**: The second call finds `cursor == self.signing_root` (both pointing to block A), indicating the root hasn't advanced. This triggers the retry mechanism at line 478-480, which spawns an independent async task: [3](#0-2) 

5. **Race Condition**: The retry task sleeps for 100ms and then unconditionally sends a duplicate request. During this time:
   - The original signing operation completes
   - Response is processed, advancing the buffer item to "signed" state
   - The signing_root moves to the next item
   - **But the retry task has already been spawned and will execute regardless**

6. **Duplicate Signing**: After 100ms, the retry task sends the duplicate request. The signing phase processes it, calling the security-critical signing operation: [4](#0-3) 

7. **Duplicate SafetyRules Invocation**: This results in `sign_commit_vote()` being called twice: [5](#0-4) 

The duplicate response is ultimately discarded by `process_signing_response()` (which checks `if item.is_executed()` and finds the item already signed), but the signing operation itself has already executed, including:
- Cryptographic signature generation (line 415)
- Metrics increment (lines 488-499) 
- Resource consumption
- Potential SafetyRules internal state updates

The root cause is that the retry mechanism spawns an independent task without tracking whether the original request completes before the retry executes.

## Impact Explanation
This issue qualifies as **High Severity** per Aptos bug bounty criteria for "Significant protocol violations":

1. **Exactly-Once Processing Invariant Violation**: The consensus pipeline explicitly requires exactly-once processing to maintain safety guarantees. Duplicate processing of security-critical signing operations violates this fundamental invariant.

2. **SafetyRules Integrity**: `SafetyRules` is the most security-critical component in the consensus system, responsible for preventing equivocation. While duplicate signing of the *same* block doesn't directly cause equivocation, it:
   - Executes security-critical code paths multiple times
   - Could expose bugs if SafetyRules has any non-idempotent state updates
   - Violates safety assumptions about operation uniqueness

3. **Resource Exhaustion**: Duplicate cryptographic operations (BLS signature generation) consume significant CPU resources. If this occurs frequently, it could degrade validator performance.

4. **Monitoring Corruption**: Metrics are incremented twice per signing operation, making it impossible to accurately monitor consensus health and detect actual issues.

5. **Similar Issues in Other Phases**: The same retry pattern exists for execution and persisting phases, potentially affecting those critical operations as well.

## Likelihood Explanation
**Likelihood: High** - This race condition can occur naturally during normal validator operation without any malicious activity:

- The retry logic is triggered whenever `advance_signing_root()` is called while a signing operation is in progress
- `advance_signing_root()` is called frequently: after every signing response (line 965) and after processing commit messages (line 982)
- Signing operations can take variable time depending on SafetyRules implementation and system load
- The 100ms retry delay creates a window where the race condition is likely

The issue becomes more probable under:
- High transaction throughput (more frequent signing operations)
- Validator system load (slower signing operations)
- Network delays (more concurrent events)

## Recommendation
**Fix the retry mechanism to track in-flight requests:**

```rust
// Add to BufferManager struct:
pending_signing_requests: HashMap<HashValue, Instant>,

// In advance_signing_root():
async fn advance_signing_root(&mut self) {
    let cursor = self.signing_root;
    self.signing_root = self
        .buffer
        .find_elem_from(cursor.or_else(|| *self.buffer.head_cursor()), |item| {
            item.is_executed()
        });
    
    if self.signing_root.is_some() {
        let item = self.buffer.get(&self.signing_root);
        let executed_item = item.unwrap_executed_ref();
        let block_id = executed_item.executed_blocks.last().unwrap().id();
        
        let request = self.create_new_request(SigningRequest {
            ordered_ledger_info: executed_item.ordered_proof.clone(),
            commit_ledger_info: executed_item.partial_commit_proof.data().clone(),
            blocks: executed_item.executed_blocks.clone(),
        });
        
        if cursor == self.signing_root {
            // Only retry if request has been pending for too long
            if let Some(sent_time) = self.pending_signing_requests.get(&block_id) {
                if sent_time.elapsed() > Duration::from_millis(500) {
                    // Genuine timeout, retry
                    let sender = self.signing_phase_tx.clone();
                    Self::spawn_retry_request(sender, request, Duration::from_millis(100));
                    self.pending_signing_requests.insert(block_id, Instant::now());
                }
                // Else: request is in-flight, don't retry
            }
        } else {
            // New request
            self.signing_phase_tx
                .send(request)
                .await
                .expect("Failed to send signing request");
            self.pending_signing_requests.insert(block_id, Instant::now());
        }
    }
}

// In process_signing_response():
// Remove from pending when response received
self.pending_signing_requests.remove(&commit_ledger_info.commit_info().id());
```

**Alternative: Use request deduplication in the signing phase** to prevent processing duplicate requests with the same block_id.

## Proof of Concept

```rust
#[tokio::test]
async fn test_duplicate_signing_race_condition() {
    use std::sync::{Arc, atomic::{AtomicU32, Ordering}};
    use std::time::Duration;
    
    // Create a mock signing phase that tracks invocations
    let sign_count = Arc::new(AtomicU32::new(0));
    let sign_count_clone = sign_count.clone();
    
    struct MockSigningPhase {
        sign_count: Arc<AtomicU32>,
    }
    
    impl StatelessPipeline for MockSigningPhase {
        type Request = SigningRequest;
        type Response = SigningResponse;
        const NAME: &'static str = "mock_signing";
        
        async fn process(&self, req: SigningRequest) -> SigningResponse {
            // Simulate signing delay
            tokio::time::sleep(Duration::from_millis(50)).await;
            self.sign_count.fetch_add(1, Ordering::SeqCst);
            
            SigningResponse {
                signature_result: Ok(bls12381::Signature::dummy_signature()),
                commit_ledger_info: req.commit_ledger_info,
            }
        }
    }
    
    // Setup pipeline with mock
    let (tx, rx) = create_channel();
    let (response_tx, mut response_rx) = create_channel();
    let reset_flag = Arc::new(AtomicBool::new(false));
    
    let phase = PipelinePhase::new(
        rx,
        Some(response_tx),
        Box::new(MockSigningPhase { sign_count: sign_count_clone }),
        reset_flag,
    );
    
    tokio::spawn(phase.start());
    
    // Send initial request
    let request = create_test_signing_request();
    let counted_req = CountedRequest::new(request.clone(), Arc::new(AtomicU64::new(0)));
    tx.send(counted_req).await.unwrap();
    
    // Wait a bit for processing to start
    tokio::time::sleep(Duration::from_millis(10)).await;
    
    // Send duplicate request (simulating retry)
    let counted_req2 = CountedRequest::new(request, Arc::new(AtomicU64::new(0)));
    tx.send(counted_req2).await.unwrap();
    
    // Collect responses
    let _resp1 = response_rx.next().await.unwrap();
    let _resp2 = response_rx.next().await.unwrap();
    
    // Verify: signing operation was called TWICE
    assert_eq!(sign_count.load(Ordering::SeqCst), 2, 
               "Expected exactly-once processing, but signing was called twice");
}
```

This test demonstrates that when duplicate requests are sent to the signing phase (as happens with the retry mechanism), the signing operation executes multiple times, violating the exactly-once processing invariant.

## Notes
- The issue affects the **signing phase** most critically due to its security-sensitive nature, but similar retry patterns in execution and persisting phases may have analogous issues
- The vulnerability occurs during normal operation and doesn't require malicious input
- While the duplicate *response* is correctly discarded, the duplicate *operation* has already executed
- The `reset_flag` mechanism (checked at line 92 of `pipeline_phase.rs`) does NOT prevent this issue because it only skips processing during active resets, not during normal retry scenarios
- The comment at line 716 in `buffer_manager.rs` acknowledges the possibility: "it is possible that we already signed this buffer item" - but the code only prevents duplicate response processing, not duplicate operation execution

### Citations

**File:** consensus/src/pipeline/buffer_manager.rs (L293-306)
```rust
    fn spawn_retry_request<T: Send + 'static>(
        mut sender: Sender<T>,
        request: T,
        duration: Duration,
    ) {
        counters::BUFFER_MANAGER_RETRY_COUNT.inc();
        spawn_named!("retry request", async move {
            tokio::time::sleep(duration).await;
            sender
                .send(request)
                .await
                .expect("Failed to send retry request");
        });
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L456-488)
```rust
    async fn advance_signing_root(&mut self) {
        let cursor = self.signing_root;
        self.signing_root = self
            .buffer
            .find_elem_from(cursor.or_else(|| *self.buffer.head_cursor()), |item| {
                item.is_executed()
            });
        sample!(
            SampleRate::Frequency(2),
            info!(
                "Advance signing root from {:?} to {:?}",
                cursor, self.signing_root
            )
        );
        if self.signing_root.is_some() {
            let item = self.buffer.get(&self.signing_root);
            let executed_item = item.unwrap_executed_ref();
            let request = self.create_new_request(SigningRequest {
                ordered_ledger_info: executed_item.ordered_proof.clone(),
                commit_ledger_info: executed_item.partial_commit_proof.data().clone(),
                blocks: executed_item.executed_blocks.clone(),
            });
            if cursor == self.signing_root {
                let sender = self.signing_phase_tx.clone();
                Self::spawn_retry_request(sender, request, Duration::from_millis(100));
            } else {
                self.signing_phase_tx
                    .send(request)
                    .await
                    .expect("Failed to send signing request");
            }
        }
    }
```

**File:** consensus/src/pipeline/pipeline_phase.rs (L88-108)
```rust
    pub async fn start(mut self) {
        // main loop
        while let Some(counted_req) = self.rx.next().await {
            let CountedRequest { req, guard: _guard } = counted_req;
            if self.reset_flag.load(Ordering::SeqCst) {
                continue;
            }
            let response = {
                let _timer = BUFFER_MANAGER_PHASE_PROCESS_SECONDS
                    .with_label_values(&[T::NAME])
                    .start_timer();
                self.processor.process(req).await
            };
            if let Some(tx) = &mut self.maybe_tx {
                if tx.send(response).await.is_err() {
                    debug!("Failed to send response, buffer manager probably dropped");
                    break;
                }
            }
        }
    }
```

**File:** consensus/src/pipeline/signing_phase.rs (L72-98)
```rust
    async fn process(&self, req: SigningRequest) -> SigningResponse {
        let SigningRequest {
            ordered_ledger_info,
            commit_ledger_info,
            blocks,
        } = req;

        let signature_result = if let Some(fut) = blocks
            .last()
            .expect("Blocks can't be empty")
            .pipeline_futs()
        {
            fut.commit_vote_fut
                .clone()
                .await
                .map(|vote| vote.signature().clone())
                .map_err(|e| Error::InternalError(e.to_string()))
        } else {
            self.safety_rule_handle
                .sign_commit_vote(ordered_ledger_info, commit_ledger_info.clone())
        };

        SigningResponse {
            signature_result,
            commit_ledger_info,
        }
    }
```

**File:** consensus/safety-rules/src/safety_rules.rs (L372-418)
```rust
    fn guarded_sign_commit_vote(
        &mut self,
        ledger_info: LedgerInfoWithSignatures,
        new_ledger_info: LedgerInfo,
    ) -> Result<bls12381::Signature, Error> {
        self.signer()?;

        let old_ledger_info = ledger_info.ledger_info();

        if !old_ledger_info.commit_info().is_ordered_only()
            // When doing fast forward sync, we pull the latest blocks and quorum certs from peers
            // and store them in storage. We then compute the root ordered cert and root commit cert
            // from storage and start the consensus from there. But given that we are not storing the
            // ordered cert obtained from order votes in storage, instead of obtaining the root ordered cert
            // from storage, we set root ordered cert to commit certificate.
            // This means, the root ordered cert will not have a dummy executed_state_id in this case.
            // To handle this, we do not raise error if the old_ledger_info.commit_info() matches with
            // new_ledger_info.commit_info().
            && old_ledger_info.commit_info() != new_ledger_info.commit_info()
        {
            return Err(Error::InvalidOrderedLedgerInfo(old_ledger_info.to_string()));
        }

        if !old_ledger_info
            .commit_info()
            .match_ordered_only(new_ledger_info.commit_info())
        {
            return Err(Error::InconsistentExecutionResult(
                old_ledger_info.commit_info().to_string(),
                new_ledger_info.commit_info().to_string(),
            ));
        }

        // Verify that ledger_info contains at least 2f + 1 dostinct signatures
        if !self.skip_sig_verify {
            ledger_info
                .verify_signatures(&self.epoch_state()?.verifier)
                .map_err(|error| Error::InvalidQuorumCertificate(error.to_string()))?;
        }

        // TODO: add guarding rules in unhappy path
        // TODO: add extension check

        let signature = self.sign(&new_ledger_info)?;

        Ok(signature)
    }
```
