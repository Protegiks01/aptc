# Audit Report

## Title
Partial Checkpoint Files Cause Database Corruption and Panic on Recovery

## Summary
The checkpoint creation mechanism in AptosDB lacks atomicity, allowing partial checkpoint directories to persist when checkpoint operations fail mid-execution. When a node attempts to open these incomplete checkpoints, it panics due to missing sub-database files, preventing database recovery and requiring manual intervention.

## Finding Description

The AptosDB checkpoint system creates checkpoints across multiple independent databases sequentially without transaction rollback. When checkpoint creation fails after some sub-databases have been successfully checkpointed, the partial checkpoint directory remains on disk with an inconsistent structure. [1](#0-0) 

The `transaction_db.create_checkpoint()` function delegates to the underlying RocksDB checkpoint API with no failure handling. [2](#0-1) 

The `LedgerDb::create_checkpoint()` orchestrates checkpoint creation for 7 sub-databases sequentially. If any checkpoint fails after line 343, the function returns an error via the `?` operator, leaving previously-created checkpoint files intact. The cleanup at line 336 only runs at the START of checkpoint creation, not on failure. [3](#0-2) 

When opening a checkpoint, `LedgerDb::new()` spawns parallel threads to open all sub-databases using `.unwrap()` on each RocksDB open operation. If any sub-database is missing (due to partial checkpoint), the `.unwrap()` causes a panic. Note the TODO comment at line 281 acknowledging this unhandled data inconsistency.

**Attack Scenario:**

1. Node operator initiates checkpoint: `AptosDB::create_checkpoint(db_path, checkpoint_path, true)`
2. Checkpoint creation proceeds through sub-databases:
   - `metadata_db`: SUCCESS ✓
   - `event_db`: SUCCESS ✓  
   - `persisted_auxiliary_info_db`: SUCCESS ✓
   - `transaction_accumulator_db`: SUCCESS ✓
   - `transaction_auxiliary_data_db`: SUCCESS ✓
   - `transaction_db`: **FAILS** ✗ (disk full / I/O error / permission denied)
3. Function returns error, but checkpoint directory contains 5 of 7 databases
4. On recovery attempt: `LedgerDb::new(checkpoint_path, ...)`
5. Opening `transaction_db` fails because the directory doesn't exist
6. `.unwrap()` triggers panic, preventing node startup [4](#0-3) 

The problem compounds at the `AptosDB` level, where `LedgerDb`, `StateKvDb`, and `StateMerkleDb` checkpoints are created sequentially with no atomicity guarantees. [5](#0-4) [6](#0-5) 

Both `StateKvDb` and `StateMerkleDb` exhibit identical patterns - sequential checkpoint creation with cleanup only at the start, not on failure.

## Impact Explanation

This qualifies as **Medium Severity** per Aptos bug bounty criteria:

- **State inconsistencies requiring intervention**: Partial checkpoints create database directories in inconsistent states that cannot be automatically recovered
- **Node availability impact**: Nodes cannot start/recover when attempting to use partial checkpoints due to panic
- **Operational disruption**: Backup and disaster recovery workflows are compromised, requiring manual detection and cleanup of corrupted checkpoint directories

While this doesn't directly cause fund loss or consensus violations, it breaks the **State Consistency** invariant by creating non-atomic checkpoint operations that leave the database in an unrecoverable state.

## Likelihood Explanation

**HIGH likelihood** in production environments:

- Disk full conditions are common during checkpoint operations (checkpoints consume significant space)
- I/O errors can occur on storage systems under load
- Permission issues may arise in containerized/cloud environments
- No automatic detection or cleanup mechanism exists
- Operators may not notice partial checkpoints until recovery attempt
- The issue is acknowledged but unaddressed (line 281 TODO comment)

## Recommendation

Implement atomic checkpoint creation with rollback on failure:

1. **Create checkpoints in temporary staging directory first**
2. **Validate all sub-databases were created successfully**
3. **Atomically rename staging to final checkpoint directory**
4. **On failure, cleanup staging directory completely**

Recommended fix:

```rust
pub(crate) fn create_checkpoint(
    db_root_path: impl AsRef<Path>,
    cp_root_path: impl AsRef<Path>,
    sharding: bool,
) -> Result<()> {
    // Create temporary staging directory
    let staging_path = cp_root_path.as_ref().with_extension(".tmp");
    std::fs::remove_dir_all(&staging_path).unwrap_or(());
    
    // Create all checkpoints in staging
    let result = Self::create_checkpoint_in_staging(
        db_root_path,
        &staging_path,
        sharding
    );
    
    // On success, atomically move to final location
    if result.is_ok() {
        std::fs::remove_dir_all(&cp_root_path).unwrap_or(());
        std::fs::rename(&staging_path, &cp_root_path)?;
    } else {
        // On failure, cleanup staging directory
        std::fs::remove_dir_all(&staging_path).unwrap_or(());
    }
    
    result
}
```

Additionally, add validation when opening checkpoints to detect partial states and provide clear error messages instead of panics.

## Proof of Concept

```rust
#[cfg(test)]
mod checkpoint_corruption_test {
    use super::*;
    use tempfile::TempDir;
    use std::sync::Arc;
    use std::path::Path;
    
    #[test]
    #[should_panic(expected = "No such file or directory")]
    fn test_partial_checkpoint_causes_panic_on_open() {
        // Setup: Create a valid database
        let db_dir = TempDir::new().unwrap();
        let checkpoint_dir = TempDir::new().unwrap();
        
        // Create initial database with sharding
        let rocksdb_configs = RocksdbConfigs {
            enable_storage_sharding: true,
            ..Default::default()
        };
        let ledger_db = LedgerDb::new(
            db_dir.path(),
            rocksdb_configs,
            None,
            None,
            false,
        ).unwrap();
        
        // Manually create partial checkpoint (simulating failure)
        let cp_ledger_folder = checkpoint_dir.path().join(LEDGER_DB_FOLDER_NAME);
        std::fs::create_dir_all(&cp_ledger_folder).unwrap();
        
        // Only create SOME of the required sub-databases
        ledger_db.metadata_db().create_checkpoint(
            LedgerDb::metadata_db_path(checkpoint_dir.path(), true)
        ).unwrap();
        ledger_db.event_db().create_checkpoint(
            cp_ledger_folder.join(EVENT_DB_NAME)
        ).unwrap();
        
        // DO NOT create transaction_db (simulating failure at line 360)
        // This leaves partial checkpoint on disk
        
        // Attempt to open the partial checkpoint - THIS WILL PANIC
        let _recovered_db = LedgerDb::new(
            checkpoint_dir.path(),
            rocksdb_configs,
            None,
            None,
            false,
        ); // Panics here when trying to open missing transaction_db
    }
}
```

**Notes:**
- This vulnerability affects all checkpoint-based recovery workflows including database backups, disaster recovery, and node migrations
- The issue exists across all checkpoint creation functions: `LedgerDb`, `StateKvDb`, and `StateMerkleDb`
- The acknowledged TODO comment at line 281 suggests developers are aware of potential data inconsistency but have not implemented a solution
- Production environments with unreliable storage or space constraints are particularly vulnerable

### Citations

**File:** storage/aptosdb/src/ledger_db/transaction_db.rs (L36-38)
```rust
    pub(super) fn create_checkpoint(&self, path: impl AsRef<Path>) -> Result<()> {
        self.db.create_checkpoint(path)
    }
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L183-293)
```rust
        THREAD_MANAGER.get_non_exe_cpu_pool().scope(|s| {
            s.spawn(|_| {
                let event_db_raw = Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(EVENT_DB_NAME),
                        EVENT_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                );
                event_db = Some(EventDb::new(
                    event_db_raw.clone(),
                    EventStore::new(event_db_raw),
                ));
            });
            s.spawn(|_| {
                persisted_auxiliary_info_db = Some(PersistedAuxiliaryInfoDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(PERSISTED_AUXILIARY_INFO_DB_NAME),
                        PERSISTED_AUXILIARY_INFO_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_accumulator_db = Some(TransactionAccumulatorDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_ACCUMULATOR_DB_NAME),
                        TRANSACTION_ACCUMULATOR_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_auxiliary_data_db = Some(TransactionAuxiliaryDataDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_AUXILIARY_DATA_DB_NAME),
                        TRANSACTION_AUXILIARY_DATA_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )))
            });
            s.spawn(|_| {
                transaction_db = Some(TransactionDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_DB_NAME),
                        TRANSACTION_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_info_db = Some(TransactionInfoDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_INFO_DB_NAME),
                        TRANSACTION_INFO_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                write_set_db = Some(WriteSetDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(WRITE_SET_DB_NAME),
                        WRITE_SET_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
        });

        // TODO(grao): Handle data inconsistency.

        Ok(Self {
            ledger_metadata_db: LedgerMetadataDb::new(ledger_metadata_db),
            event_db: event_db.unwrap(),
            persisted_auxiliary_info_db: persisted_auxiliary_info_db.unwrap(),
            transaction_accumulator_db: transaction_accumulator_db.unwrap(),
            transaction_auxiliary_data_db: transaction_auxiliary_data_db.unwrap(),
            transaction_db: transaction_db.unwrap(),
            transaction_info_db: transaction_info_db.unwrap(),
            write_set_db: write_set_db.unwrap(),
            enable_storage_sharding: true,
        })
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L311-370)
```rust
    pub(crate) fn create_checkpoint(
        db_root_path: impl AsRef<Path>,
        cp_root_path: impl AsRef<Path>,
        sharding: bool,
    ) -> Result<()> {
        let rocksdb_configs = RocksdbConfigs {
            enable_storage_sharding: sharding,
            ..Default::default()
        };
        let env = None;
        let block_cache = None;
        let ledger_db = Self::new(
            db_root_path,
            rocksdb_configs,
            env,
            block_cache,
            /*readonly=*/ false,
        )?;
        let cp_ledger_db_folder = cp_root_path.as_ref().join(LEDGER_DB_FOLDER_NAME);

        info!(
            sharding = sharding,
            "Creating ledger_db checkpoint at: {cp_ledger_db_folder:?}"
        );

        std::fs::remove_dir_all(&cp_ledger_db_folder).unwrap_or(());
        if sharding {
            std::fs::create_dir_all(&cp_ledger_db_folder).unwrap_or(());
        }

        ledger_db
            .metadata_db()
            .create_checkpoint(Self::metadata_db_path(cp_root_path.as_ref(), sharding))?;

        if sharding {
            ledger_db
                .event_db()
                .create_checkpoint(cp_ledger_db_folder.join(EVENT_DB_NAME))?;
            ledger_db
                .persisted_auxiliary_info_db()
                .create_checkpoint(cp_ledger_db_folder.join(PERSISTED_AUXILIARY_INFO_DB_NAME))?;
            ledger_db
                .transaction_accumulator_db()
                .create_checkpoint(cp_ledger_db_folder.join(TRANSACTION_ACCUMULATOR_DB_NAME))?;
            ledger_db
                .transaction_auxiliary_data_db()
                .create_checkpoint(cp_ledger_db_folder.join(TRANSACTION_AUXILIARY_DATA_DB_NAME))?;
            ledger_db
                .transaction_db()
                .create_checkpoint(cp_ledger_db_folder.join(TRANSACTION_DB_NAME))?;
            ledger_db
                .transaction_info_db()
                .create_checkpoint(cp_ledger_db_folder.join(TRANSACTION_INFO_DB_NAME))?;
            ledger_db
                .write_set_db()
                .create_checkpoint(cp_ledger_db_folder.join(WRITE_SET_DB_NAME))?;
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/db/mod.rs (L172-196)
```rust
    pub fn create_checkpoint(
        db_path: impl AsRef<Path>,
        cp_path: impl AsRef<Path>,
        sharding: bool,
    ) -> Result<()> {
        let start = Instant::now();

        info!(sharding = sharding, "Creating checkpoint for AptosDB.");

        LedgerDb::create_checkpoint(db_path.as_ref(), cp_path.as_ref(), sharding)?;
        if sharding {
            StateKvDb::create_checkpoint(db_path.as_ref(), cp_path.as_ref())?;
            StateMerkleDb::create_checkpoint(
                db_path.as_ref(),
                cp_path.as_ref(),
                sharding,
                /* is_hot = */ true,
            )?;
        }
        StateMerkleDb::create_checkpoint(
            db_path.as_ref(),
            cp_path.as_ref(),
            sharding,
            /* is_hot = */ false,
        )?;
```

**File:** storage/aptosdb/src/state_kv_db.rs (L224-259)
```rust
    pub(crate) fn create_checkpoint(
        db_root_path: impl AsRef<Path>,
        cp_root_path: impl AsRef<Path>,
    ) -> Result<()> {
        // TODO(grao): Support path override here.
        let state_kv_db = Self::open_sharded(
            &StorageDirPaths::from_path(db_root_path),
            RocksdbConfig::default(),
            None,
            None,
            false,
        )?;
        let cp_state_kv_db_path = cp_root_path.as_ref().join(STATE_KV_DB_FOLDER_NAME);

        info!("Creating state_kv_db checkpoint at: {cp_state_kv_db_path:?}");

        std::fs::remove_dir_all(&cp_state_kv_db_path).unwrap_or(());
        std::fs::create_dir_all(&cp_state_kv_db_path).unwrap_or(());

        state_kv_db
            .metadata_db()
            .create_checkpoint(Self::metadata_db_path(cp_root_path.as_ref()))?;

        // TODO(HotState): should handle hot state as well.
        for shard_id in 0..NUM_STATE_SHARDS {
            state_kv_db
                .db_shard(shard_id)
                .create_checkpoint(Self::db_shard_path(
                    cp_root_path.as_ref(),
                    shard_id,
                    /* is_hot = */ false,
                ))?;
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L192-240)
```rust
    pub(crate) fn create_checkpoint(
        db_root_path: impl AsRef<Path>,
        cp_root_path: impl AsRef<Path>,
        sharding: bool,
        is_hot: bool,
    ) -> Result<()> {
        let rocksdb_configs = RocksdbConfigs {
            enable_storage_sharding: sharding,
            ..Default::default()
        };
        // TODO(grao): Support path override here.
        let state_merkle_db = Self::new(
            &StorageDirPaths::from_path(db_root_path),
            rocksdb_configs,
            /*env=*/ None,
            /*block_cache=*/ None,
            /*readonly=*/ false,
            /*max_nodes_per_lru_cache_shard=*/ 0,
            is_hot,
            /* delete_on_restart = */ false,
        )?;
        let cp_state_merkle_db_path = cp_root_path.as_ref().join(db_folder_name(is_hot));

        info!("Creating state_merkle_db checkpoint at: {cp_state_merkle_db_path:?}");

        std::fs::remove_dir_all(&cp_state_merkle_db_path).unwrap_or(());
        if sharding {
            std::fs::create_dir_all(&cp_state_merkle_db_path).unwrap_or(());
        }

        state_merkle_db
            .metadata_db()
            .create_checkpoint(Self::metadata_db_path(
                cp_root_path.as_ref(),
                sharding,
                is_hot,
            ))?;

        if sharding {
            for shard_id in 0..NUM_STATE_SHARDS {
                state_merkle_db
                    .db_shard(shard_id)
                    .create_checkpoint(Self::db_shard_path(
                        cp_root_path.as_ref(),
                        shard_id,
                        is_hot,
                    ))?;
            }
        }
```
