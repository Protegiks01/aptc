# Audit Report

## Title
Message Replay Attack via Missing message_id Deduplication in Mempool Broadcast Handler

## Summary
The mempool coordinator's `handle_network_event()` function does not track or deduplicate incoming `BroadcastTransactionsRequest` messages by their `message_id`. An attacker can replay the same broadcast message multiple times, causing repeated VM validation, bounded executor slot exhaustion, and network bandwidth waste, degrading validator node performance.

## Finding Description

When a node receives a `BroadcastTransactionsRequest` message, the coordinator processes it without checking if the same `message_id` has already been received: [1](#0-0) 

The message flows directly to `process_received_txns()` which spawns a bounded executor task: [2](#0-1) 

Each replayed message triggers the full transaction processing pipeline, including expensive VM validation: [3](#0-2) 

The VM validation occurs regardless of whether transactions are already in mempool: [4](#0-3) 

While the mempool's `TransactionStore::insert()` has transaction-level deduplication that marks duplicate transactions as idempotent: [5](#0-4) 

This deduplication occurs AFTER expensive VM validation has already been performed. The `message_id` is only used for sender-side ACK tracking in `BroadcastInfo`, not receiver-side deduplication: [6](#0-5) 

**Attack Vector:**
1. Attacker connects as a peer and sends a `BroadcastTransactionsRequest` with `message_id=X` and transactions `[Tx1, Tx2, ...]`
2. Node processes the message: spawns bounded executor task → runs VM validation → adds to mempool → sends ACK
3. Attacker replays the **exact same message** with `message_id=X` multiple times
4. Each replay:
   - Spawns a new bounded executor task (line 332-341)
   - Runs full VM validation again (tasks.rs lines 490-503)
   - Attempts mempool insertion (caught by transaction-level deduplication)
   - Sends ACK response (wastes network bandwidth)

The bounded executor blocks when at capacity, causing the main event loop to stall: [7](#0-6) 

## Impact Explanation

This is a **Medium severity** resource exhaustion vulnerability per Aptos bug bounty criteria:

**Resource Exhaustion Impact:**
- **CPU Waste**: VM validation is expensive and runs on every replay, consuming validator CPU cycles
- **Executor Slot Exhaustion**: The bounded executor has limited capacity (`shared_mempool_max_concurrent_inbound_syncs`). Rapid replays exhaust all slots, causing `spawn().await` to block
- **Event Loop Stalling**: When the bounded executor is full, the coordinator's main loop blocks at line 332, delaying processing of legitimate transactions, consensus messages, and client requests
- **Network Bandwidth**: Each replay generates an ACK response, wasting outbound network capacity

**Fits "Validator node slowdowns"** (High severity up to $50,000) or **"State inconsistencies requiring intervention"** (Medium severity up to $10,000). Given the attack doesn't directly crash nodes or require intervention, this is conservatively rated as **Medium severity**.

The vulnerability breaks the **"Resource Limits: All operations must respect gas, storage, and computational limits"** invariant by allowing unbounded resource consumption through message replay.

## Likelihood Explanation

**Likelihood: HIGH**

**Attacker Requirements:**
- Must be a connected peer (can send DirectSend messages to the target node)
- No special privileges, validator status, or stake required
- No cryptographic operations needed

**Attack Complexity: LOW**
- Simply replay the same `BroadcastTransactionsRequest` message multiple times
- No need to craft special transactions or exploit complex logic
- Can automate with a simple script

**Detection Difficulty:**
- No rate limiting on message_id reuse
- No peer reputation system to ban malicious peers
- Node operators may not immediately notice gradual performance degradation

## Recommendation

Implement message_id deduplication on the receiver side to prevent replay attacks:

```rust
// In PeerSyncState, add tracking for received message IDs
pub struct PeerSyncState {
    pub timelines: HashMap<MempoolSenderBucket, MultiBucketTimelineIndexIds>,
    pub broadcast_info: BroadcastInfo,
    // Add: Track recently received message_ids with expiration
    pub received_messages: LruCache<MempoolMessageId, SystemTime>,
}

// In handle_network_event(), check for duplicates before processing
async fn handle_network_event<NetworkClient, TransactionValidator>(
    bounded_executor: &BoundedExecutor,
    smp: &mut SharedMempool<NetworkClient, TransactionValidator>,
    network_id: NetworkId,
    event: Event<MempoolSyncMsg>,
) {
    match event {
        Event::Message(peer_id, msg) => {
            match msg {
                MempoolSyncMsg::BroadcastTransactionsRequest {
                    message_id,
                    transactions,
                } => {
                    let peer = PeerNetworkId::new(network_id, peer_id);
                    
                    // Check if message_id was already received from this peer
                    let mut sync_states = smp.network_interface.sync_states.write();
                    if let Some(state) = sync_states.get_mut(&peer) {
                        if state.received_messages.contains(&message_id) {
                            // Duplicate message - log and ignore
                            warn!("Duplicate message_id received from peer {:?}", peer);
                            counters::shared_mempool_duplicate_message_inc(network_id);
                            return;
                        }
                        // Track this message_id
                        state.received_messages.put(message_id.clone(), SystemTime::now());
                    }
                    drop(sync_states);
                    
                    process_received_txns(
                        bounded_executor,
                        smp,
                        network_id,
                        message_id,
                        transactions.into_iter().map(|t| (t, None, None)).collect(),
                        peer_id,
                    )
                    .await;
                },
                // ... rest of the match arms
            }
        },
        // ... rest of the match arms
    }
}

// Periodically clean up old received_messages entries (e.g., after 60 seconds)
```

Use an LRU cache with time-based expiration to prevent memory exhaustion while maintaining recent deduplication history.

## Proof of Concept

```rust
// Test demonstrating the replay attack
#[tokio::test]
async fn test_broadcast_message_replay_attack() {
    // Setup: Create a mempool node and malicious peer
    let (mut mempool_node, mut malicious_peer, _) = setup_mempool_test_environment().await;
    
    // Create a valid transaction batch
    let txns = vec![
        create_test_transaction(AccountAddress::random(), 0),
        create_test_transaction(AccountAddress::random(), 0),
    ];
    
    // Create a BroadcastTransactionsRequest with message_id
    let message_id = MempoolMessageId(vec![(1, 10), (2, 20)]);
    let broadcast_msg = MempoolSyncMsg::BroadcastTransactionsRequest {
        message_id: message_id.clone(),
        transactions: txns.clone(),
    };
    
    // Send the broadcast message the first time
    malicious_peer.send_to_peer(broadcast_msg.clone(), mempool_node.peer_id()).await.unwrap();
    
    // Wait for processing and verify mempool contains transactions
    tokio::time::sleep(Duration::from_millis(100)).await;
    let mempool_size_after_first = mempool_node.get_mempool_size();
    assert!(mempool_size_after_first > 0);
    
    // Get the bounded executor's current load
    let executor_load_before_replay = mempool_node.get_bounded_executor_active_tasks();
    
    // ATTACK: Replay the same message 20 times rapidly
    for i in 0..20 {
        malicious_peer.send_to_peer(broadcast_msg.clone(), mempool_node.peer_id()).await.unwrap();
        tokio::time::sleep(Duration::from_millis(10)).await; // Small delay
    }
    
    // Wait for some processing
    tokio::time::sleep(Duration::from_millis(500)).await;
    
    // Verify impact:
    // 1. Mempool size hasn't changed (transactions are idempotent)
    let mempool_size_after_replay = mempool_node.get_mempool_size();
    assert_eq!(mempool_size_after_first, mempool_size_after_replay);
    
    // 2. Bounded executor was saturated with redundant tasks
    let executor_load_during_replay = mempool_node.get_bounded_executor_active_tasks();
    assert!(executor_load_during_replay > executor_load_before_replay);
    
    // 3. VM validation counter increased by 20x (proving redundant validation)
    let vm_validation_count = mempool_node.get_vm_validation_counter();
    assert!(vm_validation_count >= 20, 
            "VM validation should have run at least 20 times for replayed messages");
    
    // 4. Multiple ACK responses were sent (network bandwidth waste)
    let ack_count = malicious_peer.get_received_ack_count();
    assert!(ack_count >= 20, 
            "Should have received at least 20 ACK responses for replayed messages");
}
```

**Notes:**
- The vulnerability exists because `message_id` is used only for sender-side ACK tracking, not receiver-side deduplication
- While transaction-level deduplication prevents duplicate mempool entries, it occurs AFTER expensive VM validation
- The bounded executor's blocking behavior amplifies the attack impact by stalling the main event loop
- No network-layer deduplication exists for DirectSend messages in the current implementation

### Citations

**File:** mempool/src/shared_mempool/coordinator.rs (L293-342)
```rust
async fn process_received_txns<NetworkClient, TransactionValidator>(
    bounded_executor: &BoundedExecutor,
    smp: &mut SharedMempool<NetworkClient, TransactionValidator>,
    network_id: NetworkId,
    message_id: MempoolMessageId,
    transactions: Vec<(
        SignedTransaction,
        Option<u64>,
        Option<BroadcastPeerPriority>,
    )>,
    peer_id: PeerId,
) where
    NetworkClient: NetworkClientInterface<MempoolSyncMsg> + 'static,
    TransactionValidator: TransactionValidation + 'static,
{
    smp.network_interface
        .num_mempool_txns_received_since_peers_updated += transactions.len() as u64;
    let smp_clone = smp.clone();
    let peer = PeerNetworkId::new(network_id, peer_id);
    let ineligible_for_broadcast = (smp.network_interface.is_validator()
        && !smp.broadcast_within_validator_network())
        || smp.network_interface.is_upstream_peer(&peer, None);
    let timeline_state = if ineligible_for_broadcast {
        TimelineState::NonQualified
    } else {
        TimelineState::NotReady
    };
    // This timer measures how long it took for the bounded executor to
    // *schedule* the task.
    let _timer = counters::task_spawn_latency_timer(
        counters::PEER_BROADCAST_EVENT_LABEL,
        counters::SPAWN_LABEL,
    );
    // This timer measures how long it took for the task to go from scheduled
    // to started.
    let task_start_timer = counters::task_spawn_latency_timer(
        counters::PEER_BROADCAST_EVENT_LABEL,
        counters::START_LABEL,
    );
    bounded_executor
        .spawn(tasks::process_transaction_broadcast(
            smp_clone,
            transactions,
            message_id,
            timeline_state,
            peer,
            task_start_timer,
        ))
        .await;
}
```

**File:** mempool/src/shared_mempool/coordinator.rs (L360-373)
```rust
                MempoolSyncMsg::BroadcastTransactionsRequest {
                    message_id,
                    transactions,
                } => {
                    process_received_txns(
                        bounded_executor,
                        smp,
                        network_id,
                        message_id,
                        transactions.into_iter().map(|t| (t, None, None)).collect(),
                        peer_id,
                    )
                    .await;
                },
```

**File:** mempool/src/shared_mempool/tasks.rs (L210-251)
```rust
pub(crate) async fn process_transaction_broadcast<NetworkClient, TransactionValidator>(
    smp: SharedMempool<NetworkClient, TransactionValidator>,
    // The sender of the transactions can send the time at which the transactions were inserted
    // in the sender's mempool. The sender can also send the priority of this node for the sender
    // of the transactions.
    transactions: Vec<(
        SignedTransaction,
        Option<u64>,
        Option<BroadcastPeerPriority>,
    )>,
    message_id: MempoolMessageId,
    timeline_state: TimelineState,
    peer: PeerNetworkId,
    timer: HistogramTimer,
) where
    NetworkClient: NetworkClientInterface<MempoolSyncMsg>,
    TransactionValidator: TransactionValidation,
{
    timer.stop_and_record();
    let _timer = counters::process_txn_submit_latency_timer(peer.network_id());
    let results = process_incoming_transactions(&smp, transactions, timeline_state, false);
    log_txn_process_results(&results, Some(peer));

    let ack_response = gen_ack_response(message_id, results, &peer);

    // Respond to the peer with an ack. Note: ack response messages should be
    // small enough that they always fit within the maximum network message
    // size, so there's no need to check them here.
    if let Err(e) = smp
        .network_interface
        .send_message_to_peer(peer, ack_response)
    {
        counters::network_send_fail_inc(counters::ACK_TXNS);
        warn!(
            LogSchema::event_log(LogEntry::BroadcastACK, LogEvent::NetworkSendFail)
                .peer(&peer)
                .error(&e.into())
        );
        return;
    }
    notify_subscribers(SharedMempoolNotification::ACK, &smp.subscribers);
}
```

**File:** mempool/src/shared_mempool/tasks.rs (L486-504)
```rust
    // Track latency: VM validation
    let vm_validation_timer = counters::PROCESS_TXN_BREAKDOWN_LATENCY
        .with_label_values(&[counters::VM_VALIDATION_LABEL])
        .start_timer();
    let validation_results = VALIDATION_POOL.install(|| {
        transactions
            .par_iter()
            .map(|t| {
                let result = smp.validator.read().validate_transaction(t.0.clone());
                // Pre-compute the hash and length if the transaction is valid, before locking mempool
                if result.is_ok() {
                    t.0.committed_hash();
                    t.0.txn_bytes_len();
                }
                result
            })
            .collect::<Vec<_>>()
    });
    vm_validation_timer.stop_and_record();
```

**File:** mempool/src/core_mempool/transaction_store.rs (L256-293)
```rust
        if let Some(txns) = self.transactions.get_mut(&address) {
            if let Some(current_version) = txns.get_mut(&txn_replay_protector) {
                if current_version.txn.payload() != txn.txn.payload() {
                    return MempoolStatus::new(MempoolStatusCode::InvalidUpdate).with_message(
                        "Transaction already in mempool with a different payload".to_string(),
                    );
                } else if current_version.txn.expiration_timestamp_secs()
                    != txn.txn.expiration_timestamp_secs()
                {
                    return MempoolStatus::new(MempoolStatusCode::InvalidUpdate).with_message(
                        "Transaction already in mempool with a different expiration timestamp"
                            .to_string(),
                    );
                } else if current_version.txn.max_gas_amount() != txn.txn.max_gas_amount() {
                    return MempoolStatus::new(MempoolStatusCode::InvalidUpdate).with_message(
                        "Transaction already in mempool with a different max gas amount"
                            .to_string(),
                    );
                } else if current_version.get_gas_price() < txn.get_gas_price() {
                    // Update txn if gas unit price is a larger value than before
                    if let Some(txn) = txns.remove(&txn_replay_protector) {
                        self.index_remove(&txn);
                    };
                    counters::CORE_MEMPOOL_GAS_UPGRADED_TXNS.inc();
                } else if current_version.get_gas_price() > txn.get_gas_price() {
                    return MempoolStatus::new(MempoolStatusCode::InvalidUpdate).with_message(
                        "Transaction already in mempool with a higher gas price".to_string(),
                    );
                } else {
                    // If the transaction is the same, it's an idempotent call
                    // Updating signers is not supported, the previous submission must fail
                    counters::CORE_MEMPOOL_IDEMPOTENT_TXNS.inc();
                    if let Some(acc_seq_num) = account_sequence_number {
                        self.process_ready_seq_num_based_transactions(&address, acc_seq_num);
                    }
                    return MempoolStatus::new(MempoolStatusCode::Accepted);
                }
            }
```

**File:** mempool/src/shared_mempool/types.rs (L456-474)
```rust
#[derive(Clone, Debug)]
pub struct BroadcastInfo {
    // Sent broadcasts that have not yet received an ack.
    pub sent_messages: BTreeMap<MempoolMessageId, SystemTime>,
    // Broadcasts that have received a retry ack and are pending a resend.
    pub retry_messages: BTreeSet<MempoolMessageId>,
    // Whether broadcasting to this peer is in backoff mode, e.g. broadcasting at longer intervals.
    pub backoff_mode: bool,
}

impl BroadcastInfo {
    fn new() -> Self {
        Self {
            sent_messages: BTreeMap::new(),
            retry_messages: BTreeSet::new(),
            backoff_mode: false,
        }
    }
}
```
