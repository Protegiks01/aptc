# Audit Report

## Title
Indexer-GRPC Coordinator Panic on Task Join Failure Causes Service Termination and Data Loss

## Summary
The `process_next_batch()` function in the indexer-grpc stream coordinator unconditionally panics when any spawned blocking task fails, causing immediate service termination and loss of in-flight transaction data. This vulnerability affects the availability of the critical indexer-grpc API service.

## Finding Description

The vulnerability exists in the task error handling logic within `process_next_batch()`. The function spawns multiple blocking tasks for CPU-intensive transaction processing, then waits for all tasks using `futures::future::try_join_all()`. [1](#0-0) 

When any spawned task encounters a `JoinError` (caused by task panics), the coordinator immediately panics: [2](#0-1) 

The spawned blocking tasks contain multiple panic points that can trigger this failure:

**Panic Point 1 - State view unavailability:** [3](#0-2) 

**Panic Point 2 - Block info retrieval failure:** [4](#0-3) 

**Panic Point 3 - Transaction conversion failure:** [5](#0-4) 

**Panic Point 4 - BCS serialization failure:** [6](#0-5) 

The coordinator is spawned as an async task within the GRPC handler: [7](#0-6) 

When the coordinator panics, the entire spawned task terminates, orphaning the GRPC stream and losing any transactions already processed but not yet sent to the client.

The same vulnerability exists in the storage fetching path: [8](#0-7) 

## Impact Explanation

**Severity: High** per Aptos bug bounty category "API crashes"

This vulnerability causes:

1. **Service Disruption**: The indexer-grpc stream abruptly terminates, requiring clients to reconnect and potentially re-request data
2. **Data Loss**: Transactions already fetched and processed but not yet transmitted are permanently lost
3. **Cascading Failures**: If the underlying issue is systemic (e.g., corrupted block data), reconnection attempts will repeatedly trigger the same panic
4. **Infrastructure Impact**: The indexer-grpc service is critical infrastructure for downstream indexers and data consumers

The indexer-grpc service provides the primary interface for external systems to access Aptos transaction data. Its failure directly impacts ecosystem infrastructure including indexers, analytics platforms, and data aggregators.

## Likelihood Explanation

**Likelihood: Medium to High**

While not easily exploitable through malicious user input, this vulnerability can be triggered by:

1. **Database inconsistencies**: Temporary unavailability or corruption in state storage
2. **Data corruption**: On-chain data corruption from previous bugs affecting block info or transaction metadata
3. **Race conditions**: Concurrent access patterns causing transient failures in state view access
4. **Edge cases**: Unusual transaction formats or state conditions triggering conversion errors
5. **System load**: Resource exhaustion causing BCS serialization or conversion operations to fail

Real-world triggers include database maintenance operations, state sync issues, disk corruption, or bugs in transaction processing logic that only manifest with certain transaction types.

## Recommendation

Replace panic-based error handling with graceful error propagation. The coordinator should:

1. Catch task join failures and log detailed error information
2. Cancel remaining in-flight tasks
3. Return an error result instead of panicking
4. Allow the caller to handle the error appropriately (e.g., send error status to client, retry logic)

**Recommended fix:**

```rust
let responses = match futures::future::try_join_all(tasks).await {
    Ok(res) => res.into_iter().flatten().collect::<Vec<_>>(),
    Err(err) => {
        error!(
            "[Indexer Fullnode] Task join failure in transaction processing: {:?}",
            err
        );
        // Return error instead of panicking
        return vec![Err(Status::internal(format!(
            "Failed to process transaction batch: {:?}",
            err
        )))];
    },
};
```

Additionally, the spawned tasks should handle errors gracefully:

```rust
let task = tokio::task::spawn_blocking(move || -> Result<Vec<TransactionsFromNodeResponse>, Status> {
    let raw_txns = batch;
    let api_txns = Self::convert_to_api_txns(context, raw_txns)
        .map_err(|e| Status::internal(format!("Conversion failed: {:?}", e)))?;
    // ... rest of processing
    Ok(responses)
});
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_coordinator_panic_on_task_failure() {
    use std::sync::Arc;
    use tokio::sync::mpsc;
    
    // Simulate a scenario where get_block_info_by_version fails
    // This would require mocking the Context to return errors
    
    // Create a coordinator with intentionally failing context
    let (tx, mut rx) = mpsc::channel(100);
    
    // The actual PoC would require:
    // 1. Mock Context that returns errors from get_block_info_by_version
    // 2. Call process_next_batch()
    // 3. Verify the coordinator panics instead of handling error gracefully
    
    // Expected behavior: Coordinator should panic
    // Desired behavior: Coordinator should return error result
    
    // This demonstrates the vulnerability exists wherever:
    // - context.latest_state_view() fails
    // - get_block_info_by_version() fails  
    // - Transaction conversion fails
    // - BCS serialization fails
}
```

## Notes

The vulnerability is confirmed in the indexer-grpc-fullnode service, which is critical infrastructure but separate from the core consensus layer. While this does not affect consensus safety or validator operations, it qualifies as High severity under the "API crashes" category in the Aptos bug bounty program. The lack of graceful error handling creates a single point of failure where any task panic cascades into total service termination and data loss.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L170-200)
```rust
            let task = tokio::task::spawn_blocking(move || {
                let raw_txns = batch;
                let api_txns = Self::convert_to_api_txns(context, raw_txns);
                let pb_txns = Self::convert_to_pb_txns(api_txns);
                // Apply filter if present.
                let pb_txns = if let Some(ref filter) = filter {
                    pb_txns
                        .into_iter()
                        .filter(|txn| filter.matches(txn))
                        .collect::<Vec<_>>()
                } else {
                    pb_txns
                };
                let mut responses = vec![];
                // Wrap in stream response object and send to channel
                for chunk in pb_txns.chunks(output_batch_size as usize) {
                    for chunk in chunk_transactions(chunk.to_vec(), MESSAGE_SIZE_LIMIT) {
                        let item = TransactionsFromNodeResponse {
                            response: Some(transactions_from_node_response::Response::Data(
                                TransactionsOutput {
                                    transactions: chunk,
                                },
                            )),
                            chain_id: ledger_chain_id as u32,
                        };
                        responses.push(item);
                    }
                }
                responses
            });
            tasks.push(task);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L202-208)
```rust
        let responses = match futures::future::try_join_all(tasks).await {
            Ok(res) => res.into_iter().flatten().collect::<Vec<_>>(),
            Err(err) => panic!(
                "[Indexer Fullnode] Error processing transaction batches: {:?}",
                err
            ),
        };
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L253-261)
```rust

        let transactions_from_storage =
            match futures::future::try_join_all(storage_fetch_tasks).await {
                Ok(res) => res,
                Err(err) => panic!(
                    "[Indexer Fullnode] Error fetching transaction batches: {:?}",
                    err
                ),
            };
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L372-372)
```rust
        let state_view = context.latest_state_view().unwrap();
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L376-384)
```rust
        let (_, _, block_event) = context
            .db
            .get_block_info_by_version(first_version)
            .unwrap_or_else(|_| {
                panic!(
                    "[Indexer Fullnode] Could not get block_info for start version {}",
                    first_version,
                )
            });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L449-464)
```rust
            match res {
                Ok(transaction) => transactions.push((transaction, size_info)),
                Err(err) => {
                    UNABLE_TO_FETCH_TRANSACTION.inc();
                    error!(
                        version = txn_version,
                        error = format!("{:?}", err),
                        "[Indexer Fullnode] Could not convert from OnChainTransactions",
                    );
                    // IN CASE WE NEED TO SKIP BAD TXNS
                    // continue;
                    panic!(
                        "[Indexer Fullnode] Could not convert txn {} from OnChainTransactions: {:?}",
                        txn_version, err
                    );
                },
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L489-491)
```rust
    fn ser_size_u32<T: Serialize>(t: &T) -> u32 {
        bcs::serialized_size(t).expect("serialized_size() failed") as u32
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/fullnode_data_service.rs (L101-117)
```rust
        tokio::spawn(async move {
            // Initialize the coordinator that tracks starting version and processes transactions
            let mut coordinator = IndexerStreamCoordinator::new(
                context,
                starting_version,
                ending_version,
                processor_task_count,
                processor_batch_size,
                output_batch_size,
                tx.clone(),
                // For now the request for this interface doesn't include a txn filter
                // because it is only used for the txn stream filestore worker, which
                // needs every transaction. Later we may add support for txn filtering
                // to this interface too.
                None,
                Some(abort_handle.clone()),
            );
```
