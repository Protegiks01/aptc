# Audit Report

## Title
EventAccumulatorSchema Unbounded Growth Leading to Validator Shutdown and Network Liveness Loss

## Summary
When the EventStorePruner fails to delete old EventAccumulatorSchema entries due to persistent errors, the database grows unboundedly. Eventually, disk space is exhausted, causing transaction commit operations to panic and crash the validator node, resulting in total loss of liveness.

## Finding Description

The EventAccumulatorSchema stores Merkle accumulator hash values for events emitted by each transaction, keyed by `(Version, Position)`. [1](#0-0)  When transactions commit, new accumulator nodes are written to this schema. [2](#0-1) 

The pruner is responsible for deleting old entries. [3](#0-2)  When pruning fails, the error is logged but the pruner worker simply retries indefinitely without halting the node. [4](#0-3) 

**Attack Path:**

1. **Pruning Encounters Persistent Error**: The EventStorePruner fails due to I/O errors, database corruption, or any persistent failure condition during the `prune_event_accumulator` operation
2. **Silent Retry Without Progress**: The PrunerWorker catches the error, logs it, sleeps, and continues the loop without making progress or alerting operators of the critical failure
3. **Unbounded Database Growth**: Meanwhile, new transactions continue to be committed, with each transaction adding approximately `N + log2(N)` EventAccumulatorSchema entries (where N is the number of events). At 5000 TPS with 10 events per transaction, this adds ~360 GB per day
4. **Disk Space Exhaustion**: Eventually, the disk fills completely
5. **Transaction Commit Failures**: Normal transaction commits begin failing with RocksDB I/O errors due to insufficient disk space
6. **Validator Panic and Crash**: The commit operations use `.unwrap()` on write failures, causing the validator to panic. [5](#0-4) [6](#0-5) 
7. **Loss of Liveness**: The validator crashes and cannot restart (disk still full), removing it from consensus participation

**Broken Invariants:**
- **Resource Limits**: The system fails to enforce storage limits when pruning fails
- **Liveness Guarantee**: Validators should continue operating or fail gracefully, not crash unrecoverably

The system has disk space monitoring alerts [7](#0-6)  but no alerts for pruning failures or lag, and no mechanism to halt the node gracefully when pruning persistently fails.

## Impact Explanation

This is a **HIGH severity** vulnerability per the Aptos bug bounty criteria for the following reasons:

1. **Total Loss of Liveness**: When disk space is exhausted, the validator node crashes and cannot be restarted without manual intervention (disk cleanup), causing complete loss of liveness

2. **Network-Wide Impact**: If the pruning failure is caused by a code bug rather than hardware failure, multiple validators could be affected simultaneously, potentially threatening network liveness if enough validators are impacted

3. **No Automatic Recovery**: Unlike transient errors that resolve themselves, disk exhaustion requires manual intervention. The node cannot automatically recover

4. **Cascading Failures**: Once disk is full, the node cannot even write logs or perform basic operations, making diagnosis difficult

This meets the "Validator node slowdowns" and "Significant protocol violations" criteria for High severity, and approaches "Total loss of liveness/network availability" which would be Critical if network-wide.

## Likelihood Explanation

**Medium to High Likelihood:**

1. **Multiple Trigger Conditions**: Pruning can fail due to:
   - Transient I/O errors during disk operations
   - Database corruption
   - RocksDB internal errors
   - Bugs in pruning logic
   - Race conditions in concurrent database access

2. **No Backpressure Mechanism**: The system has no way to stop accepting transactions when pruning is failing, ensuring the problem will worsen over time

3. **Silent Failure Mode**: Pruning errors are only logged with sampling, making it easy for operators to miss the warnings until disk space alerts fire [8](#0-7) 

4. **Production Deployment**: All validators run pruning continuously, providing constant opportunity for failures to occur

5. **No Health Check Integration**: The node health checks do not monitor pruning health, so the node reports as healthy even when pruning is failing

## Recommendation

**Immediate Mitigations:**

1. **Add Pruning Failure Monitoring**: Create alerts when `aptos_pruner_versions{tag="progress"}` stops advancing relative to `aptos_pruner_versions{tag="target"}` for more than 30 minutes

2. **Add Circuit Breaker**: Modify PrunerWorker to halt the node after N consecutive pruning failures (e.g., 10 failures):

```rust
// In pruner_worker.rs
const MAX_CONSECUTIVE_FAILURES: u32 = 10;
let mut consecutive_failures = 0;

fn work(&self) {
    while !self.quit_worker.load(Ordering::SeqCst) {
        let pruner_result = self.pruner.prune(self.batch_size);
        if pruner_result.is_err() {
            consecutive_failures += 1;
            error!(
                error = ?pruner_result.err().unwrap(),
                consecutive_failures = consecutive_failures,
                "Pruner has error."
            );
            
            if consecutive_failures >= MAX_CONSECUTIVE_FAILURES {
                panic!("Pruner failed {} consecutive times. Halting node to prevent disk exhaustion.", MAX_CONSECUTIVE_FAILURES);
            }
            sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            continue;
        }
        consecutive_failures = 0; // Reset on success
        // ...
    }
}
```

3. **Propagate Write Errors**: Replace `.unwrap()` calls in commit operations with proper error handling that can trigger graceful shutdown: [9](#0-8) 

Replace with proper error propagation and recovery logic.

4. **Add Disk Space Pre-Checks**: Before committing transactions, verify sufficient disk space is available (e.g., >10 GB threshold)

**Long-Term Solutions:**

1. **Implement Transaction Rejection**: When pruning is failing, stop accepting new transactions after a grace period
2. **Add Pruning Health to Node Health Checks**: Include pruning status in readiness probes
3. **Automated Disk Space Management**: Automatically adjust pruning windows based on disk usage
4. **Better Error Recovery**: Make RocksDB write failures recoverable where possible instead of panicking

## Proof of Concept

**Reproduction Steps:**

1. Set up a local testnet validator with limited disk space (e.g., 100 GB)
2. Configure aggressive pruning windows to ensure pruning runs frequently
3. Inject a fault into the pruner (e.g., temporarily make the RocksDB directory read-only during pruning)
4. Monitor the pruner logs - observe repeated error messages
5. Generate high transaction load with many events
6. Observe disk space exhaustion over time
7. Observe validator panic when disk is full during transaction commit

**Rust Test Scenario:**

```rust
#[test]
fn test_pruning_failure_causes_disk_exhaustion() {
    // 1. Create AptosDB with limited disk space
    // 2. Mock pruner to always return Err()
    // 3. Commit transactions with events
    // 4. Verify database size grows unbounded
    // 5. Eventually, commits fail with disk full errors
    // 6. Verify no graceful degradation occurs
}
```

**Notes**

This vulnerability demonstrates a critical gap in the error handling and resource management strategy of the Aptos storage layer. While individual components (pruner, commit logic, monitoring) function correctly in isolation, their interaction creates a systemic failure mode where persistent pruning errors lead to catastrophic node failure without graceful degradation or automatic recovery.

The root cause is the architectural assumption that pruning will always eventually succeed, with no contingency planning for persistent failures. The use of `unwrap()` in critical commit paths further compounds the issue by converting recoverable I/O errors into unrecoverable panics.

### Citations

**File:** storage/aptosdb/src/schema/event_accumulator/mod.rs (L24-32)
```rust
define_schema!(
    EventAccumulatorSchema,
    Key,
    HashValue,
    EVENT_ACCUMULATOR_CF_NAME
);

type Key = (Version, Position);

```

**File:** storage/aptosdb/src/ledger_db/event_db.rs (L172-185)
```rust
        if !skip_index {
            // EventAccumulatorSchema updates
            let event_hashes: Vec<HashValue> = events.iter().map(ContractEvent::hash).collect();
            let (_root_hash, writes) =
                MerkleAccumulator::<EmptyReader, EventAccumulatorHasher>::append(
                    &EmptyReader,
                    0,
                    &event_hashes,
                )?;

            writes.into_iter().try_for_each(|(pos, hash)| {
                batch.put::<EventAccumulatorSchema>(&(version, pos), &hash)
            })?;
        }
```

**File:** storage/aptosdb/src/event_store/mod.rs (L319-335)
```rust
    /// Prunes events by accumulator store for a range of version in [begin, end)
    pub(crate) fn prune_event_accumulator(
        &self,
        begin: Version,
        end: Version,
        db_batch: &mut SchemaBatch,
    ) -> anyhow::Result<()> {
        let mut iter = self.event_db.iter::<EventAccumulatorSchema>()?;
        iter.seek(&(begin, Position::from_inorder_index(0)))?;
        while let Some(((version, position), _)) = iter.next().transpose()? {
            if version >= end {
                return Ok(());
            }
            db_batch.delete::<EventAccumulatorSchema>(&(version, position))?;
        }
        Ok(())
    }
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L54-64)
```rust
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L275-318)
```rust
            // TODO(grao): Consider propagating the error instead of panic, if necessary.
            s.spawn(|_| {
                self.commit_events(
                    chunk.first_version,
                    chunk.transaction_outputs,
                    skip_index_and_usage,
                )
                .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .write_set_db()
                    .commit_write_sets(chunk.first_version, chunk.transaction_outputs)
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .transaction_db()
                    .commit_transactions(
                        chunk.first_version,
                        chunk.transactions,
                        skip_index_and_usage,
                    )
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .persisted_auxiliary_info_db()
                    .commit_auxiliary_info(chunk.first_version, chunk.persisted_auxiliary_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_state_kv_and_ledger_metadata(chunk, skip_index_and_usage)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_transaction_infos(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                new_root_hash = self
                    .commit_transaction_accumulator(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
```

**File:** storage/aptosdb/src/state_kv_db.rs (L192-197)
```rust
                    s.spawn(move |_| {
                        // TODO(grao): Consider propagating the error instead of panic, if necessary.
                        self.commit_single_shard(version, shard_id, state_kv_batch)
                            .unwrap_or_else(|err| {
                                panic!("Failed to commit shard {shard_id}: {err}.")
                            });
```

**File:** terraform/helm/monitoring/files/rules/alerts.yml (L91-125)
```yaml
  - alert: Validator Low Disk Space (warning)
    expr: (kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~".*(validator|fullnode)-e.*"} - kubelet_volume_stats_used_bytes) / 1024 / 1024 / 1024 < 200
    for: 1h
    labels:
      severity: warning
      summary: "Less than 200 GB of free space on Aptos Node."
    annotations:
      description: "(This is a warning, deal with it in working hours.) A validator or fullnode pod has less than 200 GB of disk space. Take these steps:
        1. If only a few nodes have this issue, it might be that they are not typically spec'd or customized differently, \
          it's most likely a expansion of the volume is needed soon. Talk to the PE team. Otherwise, it's a bigger issue.
        2. Pass this issue on to the storage team. If you are the storage team, read on.
        3. Go to the dashboard and look for the stacked up column family sizes. \
          If the total size on that chart can't justify low free disk space, we need to log in to a node to see if something other than the AptosDB is eating up disk. \
          Start from things under /opt/aptos/data.
        3 Otherwise, if the total size on that chart is the majority of the disk consumption, zoom out and look for anomalies -- sudden increases overall or on a few \
          specific Column Families, etc. Also check average size of each type of data. Reason about the anomaly with changes in recent releases in mind.
        4 If everything made sense, it's a bigger issue, somehow our gas schedule didn't stop state explosion before an alert is triggered. Our recommended disk \
          spec and/or default pruning configuration, as well as storage gas schedule need updates. Discuss with the ecosystem team and send out a PR on the docs site, \
          form a plan to inform the node operator community and prepare for a on-chain proposal to update the gas schedule."
  - alert: Validator Very Low Disk Space (critical)
    expr: (kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~".*(validator|fullnode)-e.*"} - kubelet_volume_stats_used_bytes) / 1024 / 1024 / 1024 < 50
    for: 5m
    labels:
      severity: critical
      summary: "Less than 50 GB of free space on Aptos Node."
    annotations:
      description: "A validator or fullnode pod has less than 50 GB of disk space -- that's dangerously low. \
        1. A warning level alert of disk space less than 200GB should've fired a few days ago at least, search on slack and understand why it's not dealt with.
        2. Search in the code for the runbook of the warning alert, quickly go through that too determine if it's a bug. Involve the storage team and other team accordingly.
      If no useful information is found, evaluate the trend of disk usage increasing, how long can we run further? If it can't last the night, you have these options to mitigate this:
        1. Expand the disk if it's a cloud volume.
        2. Shorten the pruner windows. Before that, find the latest version of these https://github.com/aptos-labs/aptos-core/blob/48cc64df8a64f2d13012c10d8bd5bf25d94f19dc/config/src/config/storage_config.rs#L166-L218 \
          and read carefully the comments on the prune window config entries -- set safe values.
        3. If you believe this is happening on nodes that are not run by us, involve the PE / Community / Ecosystem teams to coordinate efforts needed on those nodes.
      "
```
