# Audit Report

## Title
Race Condition Between BlockExecutor State Sync and Consensus Operations Causes Validator Panics

## Summary
The `BlockExecutor` has a critical race condition between the `finish()` method (called during state sync) and other BlockExecutor methods (called during consensus operations). The `finish()` method sets the internal `inner` field to `None`, while concurrent methods attempt to access this field after calling `maybe_initialize()`. This creates a time-of-check-time-of-use (TOCTOU) vulnerability that causes validator nodes to panic and crash.

## Finding Description

The `BlockExecutor` struct uses an `Option<BlockExecutorInner<V>>` wrapped in a `RwLock` to manage its internal state: [1](#0-0) 

When state synchronization occurs, the `ExecutionProxy` calls `finish()` to release memory before syncing: [2](#0-1) [3](#0-2) 

The `finish()` method sets `inner` to `None`: [4](#0-3) 

Meanwhile, several BlockExecutor methods use `maybe_initialize()` followed by unwrapping `inner`. For example, `committed_block_id()`: [5](#0-4) 

The `maybe_initialize()` function checks if `inner` is `None` and calls `reset()` if needed: [6](#0-5) 

**The Race Condition:**

1. Thread A (consensus) calls `committed_block_id()` 
2. Thread A executes `maybe_initialize()` at line 82, which completes successfully - `inner` is `Some(...)`
3. Thread B (state sync) calls `finish()` at line 141 of state_computer.rs
4. Thread B sets `inner` to `None` at line 154 of mod.rs
5. Thread A continues execution at lines 83-86, acquires read lock
6. Thread A calls `.as_ref()` which returns `None`
7. Thread A calls `.expect("BlockExecutor is not reset")` which **panics**
8. Validator node crashes

This same race exists in:
- `execute_and_update_state()` (lines 105-112)
- `state_view()` (lines 158-160) 
- `pre_commit_block()` (lines 131-139)
- `commit_ledger()` (lines 141-149) [7](#0-6) [8](#0-7) [9](#0-8) [10](#0-9) 

The `execution_lock` mutex only protects the execution phase itself, not the initialization check: [11](#0-10) 

The `write_mutex` in `ExecutionProxy` only synchronizes state sync operations, not BlockExecutor's internal state access: [12](#0-11) 

## Impact Explanation

This vulnerability has **HIGH severity** impact according to the Aptos bug bounty program:

- **Validator Node Crashes**: The panic causes immediate validator termination, meeting the "API crashes" and "Validator node slowdowns" criteria
- **Network Liveness Impact**: If multiple validators sync simultaneously (common after network partitions or during catch-up), multiple validators crash, affecting consensus liveness
- **Consensus Disruption**: Crashed validators cannot participate in voting, potentially preventing 2/3+ quorum formation
- **Natural Trigger**: This occurs during normal operations - any time a validator needs to state sync while consensus is active

The race window is small but real, and the consequences are severe - complete validator process termination requiring manual restart.

## Likelihood Explanation

**Likelihood: MEDIUM to HIGH**

This vulnerability will trigger when:
1. A validator falls behind and initiates state sync (common scenario)
2. Consensus pipeline is concurrently processing blocks (always happening)
3. The timing window between `maybe_initialize()` completion and `inner` access is hit (small but non-zero probability)

Factors increasing likelihood:
- State sync is a frequent operation for validators catching up
- No synchronization exists between state sync and consensus operations
- Multiple methods are vulnerable to this race
- The window exists across multiple code paths

The vulnerability is **not** maliciously triggered but occurs naturally during normal validator operations, making it particularly concerning.

## Recommendation

**Fix: Add atomic state management with proper synchronization**

Replace the TOCTOU pattern with atomic operations. Two approaches:

**Option 1: Hold the read lock across initialization and access**
```rust
fn committed_block_id(&self) -> HashValue {
    let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "committed_block_id"]);
    
    // Hold write lock during initialization check
    if self.inner.read().is_none() {
        self.reset().expect("Failed to initialize.");
    }
    
    // Now acquire read lock and keep it for access
    let inner_guard = self.inner.read();
    inner_guard
        .as_ref()
        .expect("BlockExecutor is not reset")
        .committed_block_id()
}
```

**Option 2 (Better): Add lifecycle state tracking**
```rust
pub struct BlockExecutor<V> {
    pub db: DbReaderWriter,
    inner: RwLock<Option<BlockExecutorInner<V>>>,
    execution_lock: Mutex<()>,
    state: RwLock<ExecutorState>, // NEW
}

enum ExecutorState {
    Active,
    Syncing,
    Finished,
}

fn finish(&self) {
    *self.state.write() = ExecutorState::Finished;
    *self.inner.write() = None;
}

fn maybe_initialize(&self) -> Result<()> {
    match *self.state.read() {
        ExecutorState::Syncing | ExecutorState::Finished => {
            return Err(anyhow!("Executor is syncing or finished"));
        }
        ExecutorState::Active => {}
    }
    
    if self.inner.read().is_none() {
        self.reset()?;
    }
    Ok(())
}
```

**Option 3 (Simplest): Add state machine guard in ExecutionProxy**

Prevent concurrent access by ensuring state sync has exclusive access:
```rust
// In ExecutionProxy, ensure finish() prevents other operations
async fn sync_for_duration(&self, duration: Duration) -> Result<...> {
    let mut latest_logical_time = self.write_mutex.lock().await;
    
    // NEW: Mark executor as syncing
    self.mark_syncing()?;
    
    self.executor.finish();
    let result = self.state_sync_notifier.sync_for_duration(duration).await;
    self.executor.reset()?;
    
    // NEW: Mark executor as active
    self.mark_active()?;
    
    // ... rest of code
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod race_condition_test {
    use super::*;
    use std::sync::Arc;
    use std::thread;
    use std::time::Duration;
    use aptos_vm::AptosVM;

    #[test]
    #[should_panic(expected = "BlockExecutor is not reset")]
    fn test_race_between_finish_and_committed_block_id() {
        let db = create_test_db(); // Test helper
        let executor: Arc<BlockExecutor<AptosVM>> = Arc::new(BlockExecutor::new(db));
        
        // Initialize the executor
        executor.reset().unwrap();
        
        // Thread 1: Continuously call committed_block_id (simulating consensus)
        let executor1 = executor.clone();
        let handle1 = thread::spawn(move || {
            for _ in 0..1000 {
                let _ = executor1.committed_block_id();
                thread::sleep(Duration::from_micros(10));
            }
        });
        
        // Thread 2: Call finish repeatedly (simulating state sync)
        let executor2 = executor.clone();
        let handle2 = thread::spawn(move || {
            for _ in 0..100 {
                thread::sleep(Duration::from_micros(50));
                executor2.finish();
                thread::sleep(Duration::from_micros(50));
                executor2.reset().unwrap();
            }
        });
        
        // Wait for threads - should panic due to race condition
        handle1.join().unwrap();
        handle2.join().unwrap();
    }
    
    #[test]
    #[should_panic(expected = "BlockExecutor is not reset")]
    fn test_race_between_finish_and_state_view() {
        let db = create_test_db();
        let executor: Arc<BlockExecutor<AptosVM>> = Arc::new(BlockExecutor::new(db));
        executor.reset().unwrap();
        
        let block_id = HashValue::random();
        let executor1 = executor.clone();
        
        // Trigger race with state_view
        let handle1 = thread::spawn(move || {
            for _ in 0..1000 {
                let _ = executor1.state_view(block_id);
            }
        });
        
        let executor2 = executor.clone();
        let handle2 = thread::spawn(move || {
            for _ in 0..100 {
                executor2.finish();
                thread::sleep(Duration::from_micros(100));
                executor2.reset().unwrap();
            }
        });
        
        handle1.join().unwrap();
        handle2.join().unwrap();
    }
}
```

**Notes**

This vulnerability represents a classic TOCTOU race condition in concurrent systems. The check (`maybe_initialize()`) and use (accessing `inner`) are not atomic, allowing state to change between them. The issue is exacerbated by the lack of coordination between the state sync subsystem (which calls `finish()`) and the consensus subsystem (which calls other BlockExecutor methods).

The vulnerability affects validator availability and network liveness, core properties for blockchain consensus systems. While not directly exploitable by external attackers, it can cause validators to crash during normal operations, particularly during periods of high network activity or when validators are catching up after being offline.

### Citations

**File:** execution/executor/src/block_executor/mod.rs (L49-53)
```rust
pub struct BlockExecutor<V> {
    pub db: DbReaderWriter,
    inner: RwLock<Option<BlockExecutorInner<V>>>,
    execution_lock: Mutex<()>,
}
```

**File:** execution/executor/src/block_executor/mod.rs (L67-72)
```rust
    fn maybe_initialize(&self) -> Result<()> {
        if self.inner.read().is_none() {
            self.reset()?;
        }
        Ok(())
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L79-88)
```rust
    fn committed_block_id(&self) -> HashValue {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "committed_block_id"]);

        self.maybe_initialize().expect("Failed to initialize.");
        self.inner
            .read()
            .as_ref()
            .expect("BlockExecutor is not reset")
            .committed_block_id()
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L97-113)
```rust
    fn execute_and_update_state(
        &self,
        block: ExecutableBlock,
        parent_block_id: HashValue,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> ExecutorResult<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "execute_and_state_checkpoint"]);

        self.maybe_initialize()?;
        // guarantee only one block being executed at a time
        let _guard = self.execution_lock.lock();
        self.inner
            .read()
            .as_ref()
            .expect("BlockExecutor is not reset")
            .execute_and_update_state(block, parent_block_id, onchain_config)
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L131-139)
```rust
    fn pre_commit_block(&self, block_id: HashValue) -> ExecutorResult<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "pre_commit_block"]);

        self.inner
            .read()
            .as_ref()
            .expect("BlockExecutor is not reset")
            .pre_commit_block(block_id)
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L141-149)
```rust
    fn commit_ledger(&self, ledger_info_with_sigs: LedgerInfoWithSignatures) -> ExecutorResult<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "commit_ledger"]);

        self.inner
            .read()
            .as_ref()
            .expect("BlockExecutor is not reset")
            .commit_ledger(ledger_info_with_sigs)
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L151-155)
```rust
    fn finish(&self) {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "finish"]);

        *self.inner.write() = None;
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L157-160)
```rust
    fn state_view(&self, block_id: HashValue) -> ExecutorResult<CachedStateView> {
        self.maybe_initialize()?;
        self.inner.read().as_ref().unwrap().state_view(block_id)
    }
```

**File:** consensus/src/state_computer.rs (L58-58)
```rust
    write_mutex: AsyncMutex<LogicalTime>,
```

**File:** consensus/src/state_computer.rs (L136-141)
```rust
        // Grab the logical time lock
        let mut latest_logical_time = self.write_mutex.lock().await;

        // Before state synchronization, we have to call finish() to free the
        // in-memory SMT held by the BlockExecutor to prevent a memory leak.
        self.executor.finish();
```

**File:** consensus/src/state_computer.rs (L178-185)
```rust
        // Grab the logical time lock and calculate the target logical time
        let mut latest_logical_time = self.write_mutex.lock().await;
        let target_logical_time =
            LogicalTime::new(target.ledger_info().epoch(), target.ledger_info().round());

        // Before state synchronization, we have to call finish() to free the
        // in-memory SMT held by BlockExecutor to prevent a memory leak.
        self.executor.finish();
```
