# Audit Report

## Title
Memory Exhaustion DoS via Unbounded Batch Count in Block Proposals

## Summary
A malicious block proposer can include thousands of batches (each containing minimal transactions) in a block payload, causing memory exhaustion on validator nodes during batch processing. The `futures::future::join_all()` call in `request_and_wait_transactions()` creates one future per batch with no upper bound, allowing attackers to exhaust node memory while staying within transaction count and byte size limits.

## Finding Description

The vulnerability exists in the block proposal payload validation and processing flow. While block proposals are validated against total transaction count (`max_receiving_block_txns`) and total byte size (`max_receiving_block_bytes`), there is **no validation on the number of batches** in the payload. [1](#0-0) 

The `ProofWithData` structure stores batches as an unbounded vector: [2](#0-1) 

Payload verification only checks cryptographic signatures, not batch count: [3](#0-2) 

The critical memory exhaustion occurs in `request_and_wait_transactions()`: [4](#0-3) 

For each batch, `request_transactions()` creates a `Shared<Pin<Box<dyn Future<...>>>>`: [5](#0-4) 

**Attack Scenario:**
1. Malicious proposer creates a block with 10,000 batches
2. Each batch contains 1 transaction (total: 10,000 transactions, within the 10,000 limit)
3. Each batch is small in bytes (within byte limit)
4. Block passes signature verification and structural validation
5. During `get_transactions()`, all 10,000 futures are created and stored in memory
6. `futures::future::join_all()` allocates a JoinAll struct holding all futures simultaneously
7. Each future contains Arc references, async state machines, network request contexts
8. Memory exhaustion causes node slowdown or OOM crash

**Why Other Limits Don't Apply:**

While `ProofOfStoreMsg` (used for proof sharing between validators) has batch count validation, this is NOT enforced on block proposal payloads: [6](#0-5) 

Configuration limits like `receiver_max_num_batches` (default: 20) apply to BatchMsg reception, not block proposals: [7](#0-6) 

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos bug bounty program criteria:

- **Validator node slowdowns**: Memory exhaustion from thousands of futures causes severe performance degradation
- **API crashes**: Out-of-memory conditions can crash validator nodes
- **Significant protocol violations**: Breaks the Resource Limits invariant (#9: "All operations must respect gas, storage, and computational limits")

The attack requires the malicious actor to be a valid proposer (validator), but does not require validator collusion or insider access beyond normal proposer rotation. All honest validators processing the malicious block will experience memory exhaustion, potentially causing network-wide degradation.

## Likelihood Explanation

**High Likelihood:**
- Any validator in the rotation can propose malicious blocks
- No special privileges beyond being selected as proposer
- Attack is deterministic and repeatable
- No cryptographic complexity or race conditions required
- Easy to construct: just create many small batches instead of few large ones

**Constraints:**
- Attacker must be a validator (stake requirement)
- Only affects block processing, not block creation
- Impact limited to validators processing the block

## Recommendation

Add batch count validation to block proposal verification. Enforce a maximum number of batches per payload based on the existing `receiver_max_num_batches` configuration.

**Fix in `consensus/consensus-types/src/common.rs`:**

Add validation to `Payload::verify()` method:

```rust
pub fn verify(
    &self,
    verifier: &ValidatorVerifier,
    proof_cache: &ProofCache,
    quorum_store_enabled: bool,
    max_num_batches: usize, // Add parameter
) -> anyhow::Result<()> {
    match (quorum_store_enabled, self) {
        (false, Payload::DirectMempool(_)) => Ok(()),
        (true, Payload::InQuorumStore(proof_with_status)) => {
            ensure!(
                proof_with_status.proofs.len() <= max_num_batches,
                "Too many batches in payload: {} > {}",
                proof_with_status.proofs.len(),
                max_num_batches
            );
            Self::verify_with_cache(&proof_with_status.proofs, verifier, proof_cache)
        },
        (true, Payload::InQuorumStoreWithLimit(proof_with_status)) => {
            ensure!(
                proof_with_status.proof_with_data.proofs.len() <= max_num_batches,
                "Too many batches in payload: {} > {}",
                proof_with_status.proof_with_data.proofs.len(),
                max_num_batches
            );
            Self::verify_with_cache(&proof_with_status.proof_with_data.proofs, verifier, proof_cache)
        },
        // Add similar checks for QuorumStoreInlineHybrid and OptQuorumStore variants
        // ...
    }
}
```

Update all callers to pass `max_num_batches` from configuration (e.g., `config.quorum_store.receiver_max_num_batches`). [8](#0-7) 

## Proof of Concept

```rust
// Test demonstrating the vulnerability
// Location: consensus/src/payload_manager/quorum_store_payload_manager_test.rs

#[tokio::test]
async fn test_excessive_batch_count_dos() {
    use crate::payload_manager::quorum_store_payload_manager::QuorumStorePayloadManager;
    use aptos_consensus_types::proof_of_store::BatchInfo;
    use aptos_consensus_types::common::ProofWithData;
    use aptos_crypto::HashValue;
    
    // Create 10,000 batches each with 1 transaction
    let mut batches = Vec::new();
    for i in 0..10000 {
        let batch_info = BatchInfo::new(
            PeerId::random(),
            HashValue::random(),
            epoch,
            expiration,
            1, // num_txns
            100, // num_bytes
        );
        batches.push((batch_info, vec![PeerId::random()]));
    }
    
    let batch_reader = Arc::new(MockBatchReader::new());
    
    // Measure memory before
    let mem_before = get_memory_usage();
    
    // This call creates 10,000 futures in join_all
    let result = QuorumStorePayloadManager::request_and_wait_transactions(
        batches,
        current_time,
        batch_reader,
    ).await;
    
    // Measure memory after
    let mem_after = get_memory_usage();
    
    // Verify excessive memory allocation
    assert!(mem_after - mem_before > REASONABLE_THRESHOLD,
        "Memory exhaustion attack succeeded: allocated {} bytes for 10k batches",
        mem_after - mem_before);
}
```

**Steps to Reproduce:**
1. Configure a malicious validator node
2. When selected as proposer, create a block with 10,000 batches (1 transaction each)
3. Broadcast the block proposal
4. Monitor memory usage on honest validators processing the block
5. Observe memory spike and potential OOM/slowdown

### Citations

**File:** consensus/src/round_manager.rs (L1178-1193)
```rust
        let payload_len = proposal.payload().map_or(0, |payload| payload.len());
        let payload_size = proposal.payload().map_or(0, |payload| payload.size());
        ensure!(
            num_validator_txns + payload_len as u64 <= self.local_config.max_receiving_block_txns,
            "Payload len {} exceeds the limit {}",
            payload_len,
            self.local_config.max_receiving_block_txns,
        );

        ensure!(
            validator_txns_total_bytes + payload_size as u64
                <= self.local_config.max_receiving_block_bytes,
            "Payload size {} exceeds the limit {}",
            payload_size,
            self.local_config.max_receiving_block_bytes,
        );
```

**File:** consensus/consensus-types/src/common.rs (L127-146)
```rust
#[derive(Deserialize, Serialize, Clone, Debug, PartialEq, Eq)]
pub struct ProofWithData {
    pub proofs: Vec<ProofOfStore<BatchInfo>>,
}

impl ProofWithData {
    pub fn new(proofs: Vec<ProofOfStore<BatchInfo>>) -> Self {
        Self { proofs }
    }

    pub fn empty() -> Self {
        Self::new(vec![])
    }

    pub fn extend(&mut self, other: ProofWithData) {
        self.proofs.extend(other.proofs);
    }

    pub fn num_proofs(&self) -> usize {
        self.proofs.len()
```

**File:** consensus/consensus-types/src/common.rs (L574-632)
```rust
    pub fn verify(
        &self,
        verifier: &ValidatorVerifier,
        proof_cache: &ProofCache,
        quorum_store_enabled: bool,
    ) -> anyhow::Result<()> {
        match (quorum_store_enabled, self) {
            (false, Payload::DirectMempool(_)) => Ok(()),
            (true, Payload::InQuorumStore(proof_with_status)) => {
                Self::verify_with_cache(&proof_with_status.proofs, verifier, proof_cache)
            },
            (true, Payload::InQuorumStoreWithLimit(proof_with_status)) => Self::verify_with_cache(
                &proof_with_status.proof_with_data.proofs,
                verifier,
                proof_cache,
            ),
            (true, Payload::QuorumStoreInlineHybrid(inline_batches, proof_with_data, _))
            | (true, Payload::QuorumStoreInlineHybridV2(inline_batches, proof_with_data, _)) => {
                Self::verify_with_cache(&proof_with_data.proofs, verifier, proof_cache)?;
                Self::verify_inline_batches(
                    inline_batches.iter().map(|(info, txns)| (info, txns)),
                )?;
                Ok(())
            },
            (true, Payload::OptQuorumStore(OptQuorumStorePayload::V1(p))) => {
                let proof_with_data = p.proof_with_data();
                Self::verify_with_cache(&proof_with_data.batch_summary, verifier, proof_cache)?;
                Self::verify_inline_batches(
                    p.inline_batches()
                        .iter()
                        .map(|batch| (batch.info(), batch.transactions())),
                )?;
                Self::verify_opt_batches(verifier, p.opt_batches())?;
                Ok(())
            },
            (true, Payload::OptQuorumStore(OptQuorumStorePayload::V2(p))) => {
                if true {
                    bail!("OptQuorumStorePayload::V2 cannot be accepted yet");
                }
                #[allow(unreachable_code)]
                {
                    let proof_with_data = p.proof_with_data();
                    Self::verify_with_cache(&proof_with_data.batch_summary, verifier, proof_cache)?;
                    Self::verify_inline_batches(
                        p.inline_batches()
                            .iter()
                            .map(|batch| (batch.info(), batch.transactions())),
                    )?;
                    Self::verify_opt_batches(verifier, p.opt_batches())?;
                    Ok(())
                }
            },
            (_, _) => Err(anyhow::anyhow!(
                "Wrong payload type. Expected Payload::InQuorumStore {} got {} ",
                quorum_store_enabled,
                self
            )),
        }
    }
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L111-122)
```rust
    async fn request_and_wait_transactions(
        batches: Vec<(BatchInfo, Vec<PeerId>)>,
        block_timestamp: u64,
        batch_reader: Arc<dyn BatchReader>,
    ) -> ExecutorResult<Vec<SignedTransaction>> {
        let futures = Self::request_transactions(batches, block_timestamp, batch_reader);
        let mut all_txns = Vec::new();
        for result in futures::future::join_all(futures).await {
            all_txns.append(&mut result?);
        }
        Ok(all_txns)
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L663-723)
```rust
    fn get_or_fetch_batch(
        &self,
        batch_info: BatchInfo,
        responders: Vec<PeerId>,
    ) -> Shared<Pin<Box<dyn Future<Output = ExecutorResult<Vec<SignedTransaction>>> + Send>>> {
        let mut responders = responders.into_iter().collect();

        self.inflight_fetch_requests
            .lock()
            .entry(*batch_info.digest())
            .and_modify(|fetch_unit| {
                fetch_unit.responders.lock().append(&mut responders);
            })
            .or_insert_with(|| {
                let responders = Arc::new(Mutex::new(responders));
                let responders_clone = responders.clone();

                let inflight_requests_clone = self.inflight_fetch_requests.clone();
                let batch_store = self.batch_store.clone();
                let requester = self.batch_requester.clone();

                let fut = async move {
                    let batch_digest = *batch_info.digest();
                    defer!({
                        inflight_requests_clone.lock().remove(&batch_digest);
                    });
                    // TODO(ibalajiarun): Support V2 batch
                    if let Ok(mut value) = batch_store.get_batch_from_local(&batch_digest) {
                        Ok(value.take_payload().expect("Must have payload"))
                    } else {
                        // Quorum store metrics
                        counters::MISSED_BATCHES_COUNT.inc();
                        let subscriber_rx = batch_store.subscribe(*batch_info.digest());
                        let payload = requester
                            .request_batch(
                                batch_digest,
                                batch_info.expiration(),
                                responders,
                                subscriber_rx,
                            )
                            .await?;
                        batch_store.persist(vec![PersistedValue::new(
                            batch_info.into(),
                            Some(payload.clone()),
                        )]);
                        Ok(payload)
                    }
                }
                .boxed()
                .shared();

                tokio::spawn(fut.clone());

                BatchFetchUnit {
                    responders: responders_clone,
                    fut,
                }
            })
            .fut
            .clone()
    }
```

**File:** consensus/consensus-types/src/proof_of_store.rs (L566-583)
```rust
    pub fn verify(
        &self,
        max_num_proofs: usize,
        validator: &ValidatorVerifier,
        cache: &ProofCache,
    ) -> anyhow::Result<()> {
        ensure!(!self.proofs.is_empty(), "Empty message");
        ensure!(
            self.proofs.len() <= max_num_proofs,
            "Too many proofs: {} > {}",
            self.proofs.len(),
            max_num_proofs
        );
        for proof in &self.proofs {
            proof.verify(validator, cache)?
        }
        Ok(())
    }
```

**File:** config/src/config/quorum_store_config.rs (L14-122)
```rust
const DEFAULT_MAX_NUM_BATCHES: usize = 10;

#[derive(Clone, Copy, Debug, Deserialize, PartialEq, Serialize)]
#[serde(default, deny_unknown_fields)]
pub struct QuorumStoreBackPressureConfig {
    pub backlog_txn_limit_count: u64,
    pub backlog_per_validator_batch_limit_count: u64,
    pub decrease_duration_ms: u64,
    pub increase_duration_ms: u64,
    pub decrease_fraction: f64,
    pub dynamic_min_txn_per_s: u64,
    pub dynamic_max_txn_per_s: u64,
    pub additive_increase_when_no_backpressure: u64,
}

impl Default for QuorumStoreBackPressureConfig {
    fn default() -> QuorumStoreBackPressureConfig {
        QuorumStoreBackPressureConfig {
            // QS will be backpressured if the remaining total txns is more than this number
            // Roughly, target TPS * commit latency seconds
            backlog_txn_limit_count: 36_000,
            // QS will create batches at the max rate until this number is reached
            backlog_per_validator_batch_limit_count: 20,
            decrease_duration_ms: 1000,
            increase_duration_ms: 1000,
            decrease_fraction: 0.5,
            dynamic_min_txn_per_s: 160,
            dynamic_max_txn_per_s: 12000,
            // When the QS is no longer backpressured, we increase number of txns to be pulled from mempool
            // by this amount every second until we reach dynamic_max_txn_per_s
            additive_increase_when_no_backpressure: 2000,
        }
    }
}

#[derive(Clone, Debug, Deserialize, PartialEq, Serialize)]
#[serde(default, deny_unknown_fields)]
pub struct QuorumStoreConfig {
    pub channel_size: usize,
    pub proof_timeout_ms: usize,
    pub batch_generation_poll_interval_ms: usize,
    pub batch_generation_min_non_empty_interval_ms: usize,
    pub batch_generation_max_interval_ms: usize,
    /// The maximum number of transactions that the batch generator puts in a batch.
    pub sender_max_batch_txns: usize,
    /// The maximum number of bytes that the batch generator puts in a batch.
    pub sender_max_batch_bytes: usize,
    /// The maximum number of batches that the batch generator creates every time it pull transactions
    /// from the mempool. This is NOT the maximum number of batches that the batch generator can create
    /// per second.
    pub sender_max_num_batches: usize,
    /// The maximum number of transactions that the batch generator pulls from the mempool at a time.
    /// After the transactions are pulled, the batch generator splits them into multiple batches. This is NOT
    /// the maximum number of transactions the batch generator includes in batches per second.
    pub sender_max_total_txns: usize,
    /// The maximum number of bytes that the batch generator pulls from the mempool at a time. This is NOT
    /// the maximum number of bytes the batch generator includes in batches per second.
    pub sender_max_total_bytes: usize,
    /// The maximum number of transactions a single batch received from peers could contain.
    pub receiver_max_batch_txns: usize,
    /// The maximum number of bytes a single batch received from peers could contain.
    pub receiver_max_batch_bytes: usize,
    /// The maximum number of batches a BatchMsg received from peers can contain.
    pub receiver_max_num_batches: usize,
    /// The maximum number of transactions a BatchMsg received from peers can contain. Each BatchMsg can contain
    /// multiple batches.
    pub receiver_max_total_txns: usize,
    /// The maximum number of bytes a BatchMsg received from peers can contain. Each BatchMsg can contain
    /// multiple batches.
    pub receiver_max_total_bytes: usize,
    pub batch_request_num_peers: usize,
    pub batch_request_retry_limit: usize,
    pub batch_request_retry_interval_ms: usize,
    pub batch_request_rpc_timeout_ms: usize,
    /// Duration for expiring locally created batches.
    pub batch_expiry_gap_when_init_usecs: u64,
    /// Duration for expiring remotely created batches. The txns are filtered to prevent dupliation across validators.
    pub remote_batch_expiry_gap_when_init_usecs: u64,
    pub memory_quota: usize,
    pub db_quota: usize,
    pub batch_quota: usize,
    pub back_pressure: QuorumStoreBackPressureConfig,
    pub num_workers_for_remote_batches: usize,
    pub batch_buckets: Vec<u64>,
    pub allow_batches_without_pos_in_proposal: bool,
    pub enable_opt_quorum_store: bool,
    pub opt_qs_minimum_batch_age_usecs: u64,
    pub enable_payload_v2: bool,
    pub enable_batch_v2: bool,
}

impl Default for QuorumStoreConfig {
    fn default() -> QuorumStoreConfig {
        QuorumStoreConfig {
            channel_size: 1000,
            proof_timeout_ms: 10000,
            batch_generation_poll_interval_ms: 25,
            batch_generation_min_non_empty_interval_ms: 50,
            batch_generation_max_interval_ms: 250,
            sender_max_batch_txns: DEFEAULT_MAX_BATCH_TXNS,
            // TODO: on next release, remove BATCH_PADDING_BYTES
            sender_max_batch_bytes: 1024 * 1024 - BATCH_PADDING_BYTES,
            sender_max_num_batches: DEFAULT_MAX_NUM_BATCHES,
            sender_max_total_txns: 1500,
            // TODO: on next release, remove DEFAULT_MAX_NUM_BATCHES * BATCH_PADDING_BYTES
            sender_max_total_bytes: 4 * 1024 * 1024 - DEFAULT_MAX_NUM_BATCHES * BATCH_PADDING_BYTES,
            receiver_max_batch_txns: 100,
            receiver_max_batch_bytes: 1024 * 1024 + BATCH_PADDING_BYTES,
            receiver_max_num_batches: 20,
```

**File:** consensus/consensus-types/src/proposal_msg.rs (L82-118)
```rust
    pub fn verify(
        &self,
        sender: Author,
        validator: &ValidatorVerifier,
        proof_cache: &ProofCache,
        quorum_store_enabled: bool,
    ) -> Result<()> {
        if let Some(proposal_author) = self.proposal.author() {
            ensure!(
                proposal_author == sender,
                "Proposal author {:?} doesn't match sender {:?}",
                proposal_author,
                sender
            );
        }
        let (payload_result, sig_result) = rayon::join(
            || {
                self.proposal().payload().map_or(Ok(()), |p| {
                    p.verify(validator, proof_cache, quorum_store_enabled)
                })
            },
            || {
                self.proposal()
                    .validate_signature(validator)
                    .map_err(|e| format_err!("{:?}", e))
            },
        );
        payload_result?;
        sig_result?;

        // if there is a timeout certificate, verify its signatures
        if let Some(tc) = self.sync_info.highest_2chain_timeout_cert() {
            tc.verify(validator).map_err(|e| format_err!("{:?}", e))?;
        }
        // Note that we postpone the verification of SyncInfo until it's being used.
        self.verify_well_formed()
    }
```
