# Audit Report

## Title
Race Condition in DKG Event Handling During Epoch Transitions Causes Silent Event Loss and Potential Quorum Failure

## Summary
The `on_dkg_start_notification()` function in `EpochManager` silently drops `DKGStartEvent` notifications when the `dkg_start_event_tx` channel is `None`. Due to asynchronous event delivery through separate channels and concurrent processing via `tokio::select!`, validators can receive the `DKGStartEvent` before processing the `ReconfigNotification` that creates the channel, causing them to miss the DKG start signal and fail to participate in distributed key generation.

## Finding Description

The vulnerability exists in the DKG event handling mechanism within the epoch manager. The core issue is that `dkg_start_event_tx` is initialized as `None` and only created when randomness is enabled during epoch transitions. [1](#0-0) 

When an event arrives and the channel is `None`, the function silently returns without logging, storing, or retrying: [2](#0-1) 

The race condition occurs because both `dkg_start_events` and `reconfig_events` are processed concurrently via `tokio::select!` with no ordering guarantees: [3](#0-2) 

Events are delivered through separate async channels from the event subscription service: [4](#0-3) 

The channel is only created when randomness is enabled: [5](#0-4) 

**Why In-Progress Session Recovery Fails:**

When `reconfiguration_with_dkg::finish()` completes, it clears the in-progress session BEFORE emitting `NewEpochEvent`: [6](#0-5) [7](#0-6) 

Therefore, the `ReconfigNotification` contains `in_progress_session = None`, and continuously-running validators that miss the event have no recovery mechanism: [8](#0-7) 

## Impact Explanation

**Medium-High Severity** - Protocol violation causing temporary liveness failure requiring manual intervention.

The DKG protocol requires a quorum of 2f+1 voting power to complete: [9](#0-8) [10](#0-9) 

If validators representing >33% of voting power miss the `DKGStartEvent` due to the race condition, the DKG cannot reach quorum. This prevents epoch transition completion and impacts liveness until manual intervention via `try_clear_incomplete_session()`: [11](#0-10) 

Unlike validators that restart (which can recover by reading current on-chain state), continuously-running validators that experience the race have no automatic recovery mechanism.

## Likelihood Explanation

**Medium Likelihood** - Can occur naturally during epoch transitions with async event delivery.

The race condition can occur whenever:
1. Randomness is enabled or re-enabled after being disabled
2. The `dkg_start_event_tx` channel transitions from `None` to `Some`
3. Events are delivered asynchronously through separate channels with no ordering guarantees

The async event delivery uses separate channels with `KLAST` queue style: [12](#0-11) 

While the on-chain events are emitted in a specific order, the async channel delivery and `tokio::select!` processing means different validators may observe different orderings due to network delays, state-sync timing, and scheduler non-determinism. No attacker action is required.

## Recommendation

Add event buffering or retry logic when `dkg_start_event_tx` is `None`:

```rust
fn on_dkg_start_notification(&mut self, notification: EventNotification) -> Result<()> {
    let EventNotification {
        subscribed_events, ..
    } = notification;
    
    for event in subscribed_events {
        if let Ok(dkg_start_event) = DKGStartEvent::try_from(&event) {
            if let Some(tx) = self.dkg_start_event_tx.as_ref() {
                let _ = tx.push((), dkg_start_event);
            } else {
                // Buffer event or log warning instead of silently dropping
                warn!("[DKG] Received DKGStartEvent but channel not initialized - randomness may not be enabled yet");
            }
            return Ok(());
        }
    }
    Ok(())
}
```

Alternatively, ensure strict ordering between reconfig and event notifications, or implement a startup recovery mechanism that checks for missed DKG start events.

## Proof of Concept

The race condition can be demonstrated by:
1. Enabling randomness via governance
2. Observing async event delivery timing across validators
3. Monitoring validators that receive `DKGStartEvent` before creating the channel
4. Verifying affected validators never participate in DKG

Due to the async nature and network timing dependencies, a deterministic PoC requires instrumenting the event delivery system to inject controlled delays, which is beyond the scope of this report. However, the code structure clearly allows for the race condition as analyzed above.

## Notes

The vulnerability's actual likelihood in production depends on network conditions and state-sync timing. While the theoretical race exists, the practical trigger rate may be lower than initially assessed. Nevertheless, the silent event dropping represents a reliability issue that violates the assumed guarantees of critical coordination events, and the lack of recovery mechanisms for continuously-running validators is a design weakness that should be addressed.

### Citations

**File:** dkg/src/epoch_manager.rs (L51-51)
```rust
    dkg_start_event_tx: Option<aptos_channel::Sender<(), DKGStartEvent>>,
```

**File:** dkg/src/epoch_manager.rs (L108-123)
```rust
    fn on_dkg_start_notification(&mut self, notification: EventNotification) -> Result<()> {
        if let Some(tx) = self.dkg_start_event_tx.as_ref() {
            let EventNotification {
                subscribed_events, ..
            } = notification;
            for event in subscribed_events {
                if let Ok(dkg_start_event) = DKGStartEvent::try_from(&event) {
                    let _ = tx.push((), dkg_start_event);
                    return Ok(());
                } else {
                    debug!("[DKG] on_dkg_start_notification: failed in converting a contract event to a dkg start event!");
                }
            }
        }
        Ok(())
    }
```

**File:** dkg/src/epoch_manager.rs (L128-138)
```rust
            let handling_result = tokio::select! {
                notification = self.dkg_start_events.select_next_some() => {
                    self.on_dkg_start_notification(notification)
                },
                reconfig_notification = self.reconfig_events.select_next_some() => {
                    self.on_new_epoch(reconfig_notification).await
                },
                (peer, rpc_request) = network_receivers.rpc_rx.select_next_some() => {
                    self.process_rpc_request(peer, rpc_request)
                },
            };
```

**File:** dkg/src/epoch_manager.rs (L199-225)
```rust
        let randomness_enabled =
            consensus_config.is_vtxn_enabled() && onchain_randomness_config.randomness_enabled();
        if let (true, Some(my_index)) = (randomness_enabled, my_index) {
            let DKGState {
                in_progress: in_progress_session,
                ..
            } = payload.get::<DKGState>().unwrap_or_default();

            let network_sender = self.create_network_sender();
            let rb = ReliableBroadcast::new(
                self.my_addr,
                epoch_state.verifier.get_ordered_account_addresses(),
                Arc::new(network_sender),
                ExponentialBackoff::from_millis(self.rb_config.backoff_policy_base_ms)
                    .factor(self.rb_config.backoff_policy_factor)
                    .max_delay(Duration::from_millis(
                        self.rb_config.backoff_policy_max_delay_ms,
                    )),
                aptos_time_service::TimeService::real(),
                Duration::from_millis(self.rb_config.rpc_timeout_ms),
                BoundedExecutor::new(8, tokio::runtime::Handle::current()),
            );
            let agg_trx_producer = AggTranscriptProducer::new(rb);

            let (dkg_start_event_tx, dkg_start_event_rx) =
                aptos_channel::new(QueueStyle::KLAST, 1, None);
            self.dkg_start_event_tx = Some(dkg_start_event_tx);
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L36-40)
```rust
// Maximum channel sizes for each notification subscriber. If messages are not
// consumed, they will be dropped (oldest messages first). The remaining messages
// will be retrieved using FIFO ordering.
const EVENT_NOTIFICATION_CHANNEL_SIZE: usize = 100;
const RECONFIG_NOTIFICATION_CHANNEL_SIZE: usize = 1; // Note: this should be 1 to ensure only the latest reconfig is consumed
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L310-326)
```rust
impl EventNotificationSender for EventSubscriptionService {
    fn notify_events(&mut self, version: Version, events: Vec<ContractEvent>) -> Result<(), Error> {
        if events.is_empty() {
            return Ok(()); // No events!
        }

        // Notify event subscribers and check if a reconfiguration event was processed
        let reconfig_event_processed = self.notify_event_subscribers(version, events)?;

        // If a reconfiguration event was found, also notify the reconfig subscribers
        // of the new configuration values.
        if reconfig_event_processed {
            self.notify_reconfiguration_subscribers(version)
        } else {
            Ok(())
        }
    }
```

**File:** aptos-move/framework/aptos-framework/sources/reconfiguration_with_dkg.move (L46-61)
```text
    public(friend) fun finish(framework: &signer) {
        system_addresses::assert_aptos_framework(framework);
        dkg::try_clear_incomplete_session(framework);
        consensus_config::on_new_epoch(framework);
        execution_config::on_new_epoch(framework);
        gas_schedule::on_new_epoch(framework);
        std::version::on_new_epoch(framework);
        features::on_new_epoch(framework);
        jwk_consensus_config::on_new_epoch(framework);
        jwks::on_new_epoch(framework);
        keyless_account::on_new_epoch(framework);
        randomness_config_seqnum::on_new_epoch(framework);
        randomness_config::on_new_epoch(framework);
        randomness_api_v0_config::on_new_epoch(framework);
        reconfiguration::reconfigure();
    }
```

**File:** aptos-move/framework/aptos-framework/sources/dkg.move (L99-106)
```text
    /// Delete the currently incomplete session, if it exists.
    public fun try_clear_incomplete_session(fx: &signer) acquires DKGState {
        system_addresses::assert_aptos_framework(fx);
        if (exists<DKGState>(@aptos_framework)) {
            let dkg_state = borrow_global_mut<DKGState>(@aptos_framework);
            dkg_state.in_progress = option::none();
        }
    }
```

**File:** dkg/src/dkg_manager/mod.rs (L140-162)
```rust
        if let Some(session_state) = in_progress_session {
            let DKGSessionState {
                start_time_us,
                metadata,
                ..
            } = session_state;

            if metadata.dealer_epoch == self.epoch_state.epoch {
                info!(
                    epoch = self.epoch_state.epoch,
                    "Found unfinished and current DKG session. Continuing it."
                );
                if let Err(e) = self.setup_deal_broadcast(start_time_us, &metadata).await {
                    error!(epoch = self.epoch_state.epoch, "dkg resumption failed: {e}");
                }
            } else {
                info!(
                    cur_epoch = self.epoch_state.epoch,
                    dealer_epoch = metadata.dealer_epoch,
                    "Found unfinished but stale DKG session. Ignoring it."
                );
            }
        }
```

**File:** types/src/validator_verifier.rs (L206-214)
```rust
    pub fn new(validator_infos: Vec<ValidatorConsensusInfo>) -> Self {
        let total_voting_power = sum_voting_power(&validator_infos);
        let quorum_voting_power = if validator_infos.is_empty() {
            0
        } else {
            total_voting_power * 2 / 3 + 1
        };
        Self::build_index(validator_infos, quorum_voting_power, total_voting_power)
    }
```

**File:** dkg/src/transcript_aggregation/mod.rs (L122-134)
```rust
        let threshold = self.epoch_state.verifier.quorum_voting_power();
        let power_check_result = self
            .epoch_state
            .verifier
            .check_voting_power(trx_aggregator.contributors.iter(), true);
        let new_total_power = match &power_check_result {
            Ok(x) => Some(*x),
            Err(VerifyError::TooLittleVotingPower { voting_power, .. }) => Some(*voting_power),
            _ => None,
        };
        let maybe_aggregated = power_check_result
            .ok()
            .map(|_| trx_aggregator.trx.clone().unwrap());
```
