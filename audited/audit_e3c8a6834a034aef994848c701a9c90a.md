# Audit Report

## Title
Health Checker Initialization Race Condition Leading to Unbounded Memory Leak via Disconnecting Peers

## Summary
During health checker initialization, the `subscribe()` method sends `NewPeer` events for all peers in the `PeersAndMetadata` HashMap without checking their connection state. This includes peers in the `Disconnecting` state. When the subsequent `LostPeer` event is dropped due to channel buffer overflow (via `TrySendError::Full`), the health checker maintains stale entries indefinitely in its unbounded `health_check_data` HashMap, leading to resource exhaustion.

## Finding Description

The vulnerability occurs in the interaction between three components during health checker initialization:

**1. Peer Connection State Management**

When a peer disconnects, the network layer first updates its state to `Disconnecting` while keeping it in the HashMap [1](#0-0) , then later removes it completely via `remove_peer_metadata()` which broadcasts a `LostPeer` event [2](#0-1) .

**2. Vulnerable Subscribe Logic**

During health checker initialization in `HealthCheckerBuilder::new()`, a `NetworkClient` is created with shared `peers_and_metadata` [3](#0-2) . When `HealthChecker::start()` is called, it invokes `subscribe()` to receive connection events.

The `subscribe()` method iterates through ALL peers in the HashMap and sends `NewPeer` events for each one, **without checking their connection state** [4](#0-3) . This means peers in `Disconnecting` state are included in the initial snapshot and treated as newly connected.

**3. Message Dropping on Channel Overflow**

When peer disconnection completes and `LostPeer` is broadcasted, if the health checker's channel buffer (capped at 1000 messages) is full, the `broadcast()` method silently drops the message [5](#0-4) . The comment explicitly states "Drop message" at the `TrySendError::Full` case.

**4. Unbounded Health Data Accumulation**

The health checker maintains a `health_check_data: RwLock<HashMap<PeerId, HealthCheckData>>` with no size limits [6](#0-5) . When it receives `NewPeer` events from the initial snapshot, it creates health data entries [7](#0-6) . If the corresponding `LostPeer` event is dropped, these entries persist indefinitely.

**Attack Scenario:**

1. Attacker controls multiple peers that connect to victim node
2. During health checker initialization or restart, attacker triggers rapid connect/disconnect cycles
3. Multiple peers enter `Disconnecting` state while still in HashMap
4. Health checker calls `subscribe()`, receives `NewPeer` for disconnecting peers
5. Attacker floods the network with events to fill the channel buffer (1000 messages)
6. When disconnection completes, `LostPeer` events are dropped due to `TrySendError::Full`
7. Health checker creates health data for already-disconnected peers
8. Health checker never receives `LostPeer`, so stale entries remain forever
9. On each ping interval, health checker attempts to ping non-existent peers [8](#0-7) 
10. RPC calls fail continuously, wasting CPU and network resources
11. Over time, `health_check_data` grows unbounded, consuming memory

## Impact Explanation

This vulnerability constitutes **Medium Severity** per Aptos bug bounty criteria due to:

**Resource Exhaustion:** The `health_check_data` HashMap has no size limit and can grow unbounded with stale entries. Each stale entry persists indefinitely since there's no cleanup mechanism beyond processing `LostPeer` events.

**Performance Degradation:** On every ping interval (configurable, typically seconds), the health checker iterates through all entries in `health_check_data` and attempts to ping them [9](#0-8) . Failed pings waste CPU cycles and network bandwidth.

**Gradual Availability Impact:** As stale entries accumulate over days/weeks of sustained attack, the node experiences increasing resource pressure. Eventually, this could degrade node performance or cause out-of-memory conditions.

**Invariant Violation:** This breaks Critical Invariant #9 (Resource Limits) - the implementation fails to respect memory constraints, allowing unbounded growth of the health check data structure.

The impact is limited to the affected node and doesn't directly compromise consensus or funds, but can lead to validator node degradation requiring manual intervention.

## Likelihood Explanation

**High Likelihood:**

1. **Timing Window Always Exists:** During any node restart or health checker reinitialization, there's always a window where some peers may be in `Disconnecting` state
2. **Exploitable by Any Peer:** Any network participant can connect and disconnect, no special privileges required
3. **Channel Overflow Is Realistic:** With 1000 message buffer and potential burst of connection events during node startup, overflow is feasible
4. **Persistent Impact:** Once stale entries are created, they never get cleaned up without operator intervention

The attack can be repeated on each health checker restart, gradually accumulating more stale entries over the node's lifetime.

## Recommendation

**Fix 1: Filter Disconnecting Peers in subscribe()**

Modify the `subscribe()` method to only send `NewPeer` events for peers that are actually connected:

```rust
pub fn subscribe(&self) -> tokio::sync::mpsc::Receiver<ConnectionNotification> {
    let (sender, receiver) = tokio::sync::mpsc::channel(NOTIFICATION_BACKLOG);
    let peers_and_metadata = self.peers_and_metadata.read();
    'outer: for (network_id, network_peers_and_metadata) in peers_and_metadata.iter() {
        for (_addr, peer_metadata) in network_peers_and_metadata.iter() {
            // ADD THIS CHECK:
            if !peer_metadata.is_connected() {
                continue;
            }
            
            let event = ConnectionNotification::NewPeer(
                peer_metadata.connection_metadata.clone(),
                *network_id,
            );
            if let Err(err) = sender.try_send(event) {
                warn!("could not send initial NewPeer on subscribe(): {:?}", err);
                break 'outer;
            }
        }
    }
    let mut listeners = self.subscribers.lock();
    listeners.push(sender);
    receiver
}
```

**Fix 2: Add Periodic Cleanup in Health Checker**

Add validation in the health checker to periodically verify peers still exist in `PeersAndMetadata` and remove stale entries:

```rust
// In HealthChecker::start() event loop, add periodic cleanup:
_ = cleanup_ticker.select_next_some() => {
    let peers_and_metadata = self.network_interface.get_peers_and_metadata();
    let mut stale_peers = Vec::new();
    
    for peer_id in self.network_interface.connected_peers() {
        let peer_network_id = PeerNetworkId::new(self.network_context.network_id(), peer_id);
        if peers_and_metadata.get_metadata_for_peer(peer_network_id).is_err() {
            stale_peers.push(peer_id);
        }
    }
    
    for peer_id in stale_peers {
        self.network_interface.remove_peer_and_health_data(&peer_id);
    }
}
```

**Fix 3: Guarantee Message Delivery for Critical Events**

Replace `try_send()` with blocking `send()` for critical events like `LostPeer`, or implement a retry mechanism to ensure delivery.

## Proof of Concept

```rust
#[tokio::test]
async fn test_health_checker_stale_data_on_disconnecting_peers() {
    use std::sync::Arc;
    use aptos_config::network_id::{NetworkContext, NetworkId};
    use aptos_time_service::MockTimeService;
    use crate::application::storage::PeersAndMetadata;
    use crate::application::metadata::ConnectionState;
    use crate::transport::ConnectionMetadata;
    use aptos_types::PeerId;
    
    // Setup: Create PeersAndMetadata with a connected peer
    let network_ids = vec![NetworkId::Validator];
    let peers_and_metadata = PeersAndMetadata::new(&network_ids);
    let peer_id = PeerId::random();
    let connection_metadata = ConnectionMetadata::mock(peer_id);
    let peer_network_id = PeerNetworkId::new(NetworkId::Validator, peer_id);
    
    // Add peer as connected
    peers_and_metadata.insert_connection_metadata(
        peer_network_id,
        connection_metadata.clone()
    ).unwrap();
    
    // Simulate peer starting to disconnect (state = Disconnecting, but still in HashMap)
    peers_and_metadata.update_connection_state(
        peer_network_id,
        ConnectionState::Disconnecting
    ).unwrap();
    
    // Now initialize health checker - it calls subscribe()
    let (network_sender, _network_rx) = /* setup network channel */;
    let mut builder = HealthCheckerBuilder::new(
        NetworkContext::mock(),
        MockTimeService::new(),
        1000, 1000, 3,
        network_sender,
        network_rx,
        peers_and_metadata.clone()
    );
    
    // Fill the channel buffer with dummy events to cause overflow
    for _ in 0..1000 {
        // Send dummy events to fill buffer
    }
    
    // Complete peer disconnection - this should broadcast LostPeer
    // But due to full buffer, LostPeer will be dropped
    peers_and_metadata.remove_peer_metadata(peer_network_id, connection_metadata.connection_id).unwrap();
    
    // Start health checker
    builder.start(tokio::runtime::Handle::current());
    
    // Wait for initial events to be processed
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    // VULNERABILITY: Health checker should NOT have data for this disconnected peer
    // But due to race condition, it does:
    let health_interface = /* access health checker's interface */;
    assert!(health_interface.get_peer_failures(peer_id).is_some(), 
        "Stale health data exists for disconnected peer!");
    
    // Verify the peer is NOT in PeersAndMetadata anymore
    assert!(peers_and_metadata.get_metadata_for_peer(peer_network_id).is_err(),
        "Peer should be removed from PeersAndMetadata");
    
    // The health checker will continuously try to ping this non-existent peer
    // wasting resources on every ping interval
}
```

**Notes:**

The core issue is that `subscribe()` treats all peers in the HashMap as "connected" without checking their actual `connection_state`, while the event broadcast mechanism can drop critical `LostPeer` notifications under load. This combination allows the health checker to maintain indefinite references to disconnected peers, violating resource management invariants and enabling a gradual resource exhaustion attack. The fix requires either ensuring correct state in the initial snapshot or guaranteeing delivery of cleanup events.

### Citations

**File:** network/framework/src/application/metadata.rs (L14-18)
```rust
pub enum ConnectionState {
    Connected,
    Disconnecting,
    Disconnected, // Currently unused (TODO: fix this!)
}
```

**File:** network/framework/src/application/storage.rs (L219-262)
```rust
    pub fn remove_peer_metadata(
        &self,
        peer_network_id: PeerNetworkId,
        connection_id: ConnectionId,
    ) -> Result<PeerMetadata, Error> {
        // Grab the write lock for the peer metadata
        let mut peers_and_metadata = self.peers_and_metadata.write();

        // Fetch the peer metadata for the given network
        let peer_metadata_for_network =
            get_peer_metadata_for_network(&peer_network_id, &mut peers_and_metadata)?;

        // Remove the peer metadata for the peer
        let peer_metadata = if let Entry::Occupied(entry) =
            peer_metadata_for_network.entry(peer_network_id.peer_id())
        {
            // Don't remove the peer if the connection doesn't match!
            // For now, remove the peer entirely, we could in the future
            // have multiple connections for a peer
            let active_connection_id = entry.get().connection_metadata.connection_id;
            if active_connection_id == connection_id {
                let peer_metadata = entry.remove();
                let event = ConnectionNotification::LostPeer(
                    peer_metadata.connection_metadata.clone(),
                    peer_network_id.network_id(),
                );
                self.broadcast(event);
                peer_metadata
            } else {
                return Err(Error::UnexpectedError(format!(
                    "The peer connection id did not match! Given: {:?}, found: {:?}.",
                    connection_id, active_connection_id
                )));
            }
        } else {
            // Unable to find the peer metadata for the given peer
            return Err(missing_peer_metadata_error(&peer_network_id));
        };

        // Update the cached peers and metadata
        self.set_cached_peers_and_metadata(peers_and_metadata.clone());

        Ok(peer_metadata)
    }
```

**File:** network/framework/src/application/storage.rs (L371-395)
```rust
    fn broadcast(&self, event: ConnectionNotification) {
        let mut listeners = self.subscribers.lock();
        let mut to_del = vec![];
        for i in 0..listeners.len() {
            let dest = listeners.get_mut(i).unwrap();
            if let Err(err) = dest.try_send(event.clone()) {
                match err {
                    TrySendError::Full(_) => {
                        // Tried to send to an app, but the app isn't handling its messages fast enough.
                        // Drop message. Maybe increment a metrics counter?
                        sample!(
                            SampleRate::Duration(Duration::from_secs(1)),
                            warn!("PeersAndMetadata.broadcast() failed, some app is slow"),
                        );
                    },
                    TrySendError::Closed(_) => {
                        to_del.push(i);
                    },
                }
            }
        }
        for evict in to_del.into_iter() {
            listeners.swap_remove(evict);
        }
    }
```

**File:** network/framework/src/application/storage.rs (L400-414)
```rust
    pub fn subscribe(&self) -> tokio::sync::mpsc::Receiver<ConnectionNotification> {
        let (sender, receiver) = tokio::sync::mpsc::channel(NOTIFICATION_BACKLOG);
        let peers_and_metadata = self.peers_and_metadata.read();
        'outer: for (network_id, network_peers_and_metadata) in peers_and_metadata.iter() {
            for (_addr, peer_metadata) in network_peers_and_metadata.iter() {
                let event = ConnectionNotification::NewPeer(
                    peer_metadata.connection_metadata.clone(),
                    *network_id,
                );
                if let Err(err) = sender.try_send(event) {
                    warn!("could not send initial NewPeer on subscribe(): {:?}", err);
                    break 'outer;
                }
            }
        }
```

**File:** network/framework/src/protocols/health_checker/builder.rs (L27-55)
```rust
    pub fn new(
        network_context: NetworkContext,
        time_service: TimeService,
        ping_interval_ms: u64,
        ping_timeout_ms: u64,
        ping_failures_tolerated: u64,
        network_sender: NetworkSender<HealthCheckerMsg>,
        network_rx: HealthCheckerNetworkEvents,
        peers_and_metadata: Arc<PeersAndMetadata>,
    ) -> Self {
        let network_senders = hashmap! {network_context.network_id() => network_sender};
        let network_client = NetworkClient::new(
            vec![],
            vec![HealthCheckerRpc],
            network_senders,
            peers_and_metadata,
        );
        let service = HealthChecker::new(
            network_context,
            time_service,
            HealthCheckNetworkInterface::new(network_client, network_rx),
            Duration::from_millis(ping_interval_ms),
            Duration::from_millis(ping_timeout_ms),
            ping_failures_tolerated,
        );
        Self {
            service: Some(service),
        }
    }
```

**File:** network/framework/src/protocols/health_checker/interface.rs (L40-43)
```rust
    health_check_data: RwLock<HashMap<PeerId, HealthCheckData>>,
    network_client: NetworkClient,
    receiver: HealthCheckerNetworkEvents,
}
```

**File:** network/framework/src/protocols/health_checker/interface.rs (L59-61)
```rust
    pub fn connected_peers(&self) -> Vec<PeerId> {
        self.health_check_data.read().keys().cloned().collect()
    }
```

**File:** network/framework/src/protocols/health_checker/interface.rs (L95-101)
```rust
    pub fn create_peer_and_health_data(&mut self, peer_id: PeerId, round: u64) {
        self.health_check_data
            .write()
            .entry(peer_id)
            .and_modify(|health_check_data| health_check_data.round = round)
            .or_insert_with(|| HealthCheckData::new(round));
    }
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L229-264)
```rust
                _ = ticker.select_next_some() => {
                    self.round += 1;
                    let connected = self.network_interface.connected_peers();
                    if connected.is_empty() {
                        trace!(
                            NetworkSchema::new(&self.network_context),
                            round = self.round,
                            "{} No connected peer to ping round: {}",
                            self.network_context,
                            self.round
                        );
                        continue
                    }

                    for peer_id in connected {
                        let nonce = self.rng.r#gen::<u32>();
                        trace!(
                            NetworkSchema::new(&self.network_context),
                            round = self.round,
                            "{} Will ping: {} for round: {} nonce: {}",
                            self.network_context,
                            peer_id.short_str(),
                            self.round,
                            nonce
                        );

                        tick_handlers.push(Self::ping_peer(
                            self.network_context,
                            self.network_interface.network_client(),
                            peer_id,
                            self.round,
                            nonce,
                            self.ping_timeout,
                        ));
                    }
                }
```
