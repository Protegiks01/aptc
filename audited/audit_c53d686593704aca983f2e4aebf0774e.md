# Audit Report

## Title
Priority Inversion in Consensus Observer Pending Block Store Causes Permanent Loss of Ready Blocks

## Summary
The `remove_ready_block()` function in the consensus observer pending block store contains a critical logic flaw that causes permanent loss of ready blocks. When a payload arrives for a block, the function only checks the highest-round pending block for readiness and permanently deletes all lower-round blocks, even if they are ready for processing. This violates the liveness invariant that ready blocks must eventually be processed, causing observer nodes to fall behind the network.

## Finding Description

The vulnerability exists in the `remove_ready_block()` function in the pending block store. [1](#0-0) 

When a block payload arrives, the consensus observer calls `order_ready_pending_block()` [2](#0-1)  which delegates to `remove_ready_pending_block()` [3](#0-2)  to retrieve any pending block that has become ready.

The flawed logic operates as follows:

1. **Splits pending blocks**: The function calculates `split_round = received_payload_round + 1` and splits the BTreeMap, separating blocks with `first_block.round < split_round` from those with higher rounds
2. **Checks only the highest block**: It calls `pop_last()` to retrieve only the highest-round block from the lower partition
3. **Re-inserts if not ready**: If the block is not ready but spans beyond the received round, it moves to the higher partition
4. **Deletes all remaining blocks**: The critical bug occurs where `clear_missing_blocks()` is called, permanently deleting ALL remaining lower-round blocks [4](#0-3) 

**Attack Scenario:**
1. Observer receives OrderedBlock A (rounds 100-104) with all payloads already present (READY)
2. Observer receives OrderedBlock B (rounds 200-204) with only round 200's payload present
3. When payload for round 200 arrives, `remove_ready_block(epoch, 200)` is called
4. Both blocks have `first_block.round â‰¤ 200`, so both remain in the lower partition after split at round 201
5. `pop_last()` retrieves Block B (round 200, the highest)
6. Block B is NOT ready (missing payloads for rounds 201-204)
7. Block B's `last_block().round()` (204) > `received_payload_round` (200), so it gets re-inserted into higher rounds
8. Block A remains in `blocks_without_payloads`
9. **`clear_missing_blocks()` deletes Block A permanently, even though it was ready**
10. The observer never processes Block A and falls behind

The function assumes all blocks with rounds lower than the highest-round block are "out-of-date" (as indicated by the comment at line 214-215), but this assumption is incorrect when a lower-round block is ready while a higher-round block is not.

## Impact Explanation

This is a **Medium Severity** vulnerability aligned with Aptos bug bounty criteria for "Limited Protocol Violations":

**Liveness Failure**: Consensus observer nodes fail to process ready blocks, causing them to permanently fall behind the network. The observer's state becomes inconsistent with validators.

**Manual Intervention Required**: Node operators must restart their observer nodes to resync from scratch, as there is no automatic recovery mechanism once ready blocks are lost.

**Infrastructure Impact**: While this only affects observer nodes (not active validators), observers are critical infrastructure for:
- Fullnode operators monitoring chain state
- Indexing services tracking blockchain data
- Public RPC endpoints serving dApps and users

The impact is limited to observer nodes and does not affect consensus safety or validator operations, which is why this qualifies as Medium rather than Critical severity.

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability can be triggered in two realistic scenarios:

1. **Malicious Attack**: An adversarial network peer can deliberately send OrderedBlock messages with strategically missing payloads to trigger the bug. The consensus observer accepts messages from any network peer [5](#0-4) , and blocks are inserted into the pending store when payloads are missing [6](#0-5) . No special privileges are required.

2. **Normal Network Conditions**: Even without malicious intent, out-of-order message delivery in distributed systems naturally creates this condition. If a high-round payload arrives before a low-round payload, and multiple pending blocks exist at different rounds, ready low-round blocks will be dropped.

The bug is deterministic - once the preconditions are met (multiple pending blocks with varying payload availability), blocks are always lost. The only requirements are:
- Two or more pending blocks at different round ranges
- Lower-round block has all payloads (ready)
- Higher-round block is missing some payloads beyond the received round

## Recommendation

The function should check ALL pending blocks (not just the highest one) to see which ones are ready for processing. The fix should:

1. Iterate through all blocks in the lower partition (not just the last one)
2. Collect all blocks that are ready (have all payloads)
3. Only delete blocks that are truly out-of-date (lower round than the received payload and not ready)
4. Return the ready block with the lowest round for processing

Example fix:
```rust
// Check ALL blocks in the lower partition for readiness
let mut ready_blocks = vec![];
for (epoch_and_round, pending_block) in self.blocks_without_payloads.iter() {
    if block_payload_store.all_payloads_exist(pending_block.ordered_block().blocks()) {
        ready_blocks.push((epoch_and_round.clone(), pending_block.clone()));
    }
}

// Process the lowest-round ready block first
let ready_block = ready_blocks.first().map(|(_, block)| block.clone());

// Keep blocks that are either ready or waiting for higher payloads
// Only delete truly out-of-date blocks
```

## Proof of Concept

```rust
#[test]
fn test_priority_inversion_bug() {
    use aptos_config::config::ConsensusObserverConfig;
    use std::sync::Arc;
    use aptos_infallible::Mutex;
    
    // Create pending block store
    let config = ConsensusObserverConfig {
        max_num_pending_blocks: 10,
        ..Default::default()
    };
    let pending_store = Arc::new(Mutex::new(PendingBlockStore::new(config)));
    let mut payload_store = BlockPayloadStore::new(config);
    
    // Create Block A at rounds 100-104 (all payloads present - READY)
    let block_a = create_ordered_block(0, 100, 5, 0);
    insert_payloads_for_ordered_block(&mut payload_store, &block_a);
    
    // Create Block B at rounds 200-204 (only round 200 payload present)
    let block_b = create_ordered_block(0, 200, 5, 1);
    let first_payload_only = BlockPayload::new(
        block_b.blocks()[0].block_info(),
        BlockTransactionPayload::empty()
    );
    payload_store.insert_block_payload(first_payload_only, true);
    
    // Insert both blocks as pending
    let observed_a = ObservedOrderedBlock::new_for_testing(block_a.clone());
    let pending_a = PendingBlockWithMetadata::new_with_arc(
        PeerNetworkId::random(),
        Instant::now(),
        observed_a
    );
    pending_store.lock().insert_pending_block(pending_a);
    
    let observed_b = ObservedOrderedBlock::new_for_testing(block_b.clone());
    let pending_b = PendingBlockWithMetadata::new_with_arc(
        PeerNetworkId::random(),
        Instant::now(),
        observed_b
    );
    pending_store.lock().insert_pending_block(pending_b);
    
    // Trigger the bug: receive payload for round 200
    let ready_block = pending_store.lock().remove_ready_block(
        0,
        200,
        &mut payload_store
    );
    
    // BUG: Block A should be returned (it's ready), but instead:
    // - Block B is checked (not ready, gets re-inserted)
    // - Block A is deleted permanently
    // - No block is returned
    assert!(ready_block.is_none(), "Expected no block returned due to bug");
    
    // Block A is lost forever
    assert!(!pending_store.lock().existing_pending_block(&block_a),
        "Block A was permanently deleted despite being ready");
}
```

## Notes

This vulnerability demonstrates a classic priority inversion bug where the system incorrectly prioritizes checking high-round blocks over processing ready low-round blocks. The comment at line 214-215 reveals the flawed assumption: "Any earlier blocks are considered out-of-date and will be dropped." This assumption only holds if blocks arrive sequentially, but fails in the presence of out-of-order message delivery, which is common in distributed systems.

The existing tests do not catch this bug because they only test sequential payload arrival scenarios. The test `test_remove_ready_block_multiple_blocks_missing` incrementally inserts payloads for a single block, but never tests the scenario where multiple blocks at different rounds have varying payload availability.

### Citations

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L200-256)
```rust
    pub fn remove_ready_block(
        &mut self,
        received_payload_epoch: u64,
        received_payload_round: Round,
        block_payload_store: &mut BlockPayloadStore,
    ) -> Option<Arc<PendingBlockWithMetadata>> {
        // Calculate the round at which to split the blocks
        let split_round = received_payload_round.saturating_add(1);

        // Split the blocks at the epoch and round
        let mut blocks_at_higher_rounds = self
            .blocks_without_payloads
            .split_off(&(received_payload_epoch, split_round));

        // Check if the last block is ready (this should be the only ready block).
        // Any earlier blocks are considered out-of-date and will be dropped.
        let mut ready_block = None;
        if let Some((epoch_and_round, pending_block)) = self.blocks_without_payloads.pop_last() {
            // If all payloads exist for the block, then the block is ready
            if block_payload_store.all_payloads_exist(pending_block.ordered_block().blocks()) {
                ready_block = Some(pending_block);
            } else {
                // Otherwise, check if we're still waiting for higher payloads for the block
                let last_pending_block_round = pending_block.ordered_block().last_block().round();
                if last_pending_block_round > received_payload_round {
                    blocks_at_higher_rounds.insert(epoch_and_round, pending_block);
                }
            }
        }

        // Check if any out-of-date blocks are going to be dropped
        if !self.blocks_without_payloads.is_empty() {
            info!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Dropped {:?} out-of-date pending blocks before epoch and round: {:?}",
                    self.blocks_without_payloads.len(),
                    (received_payload_epoch, received_payload_round)
                ))
            );
        }

        // TODO: optimize this flow!

        // Clear all blocks from the pending block stores
        self.clear_missing_blocks();

        // Update the pending block stores to only include the blocks at higher rounds
        self.blocks_without_payloads = blocks_at_higher_rounds;
        for pending_block in self.blocks_without_payloads.values() {
            let first_block = pending_block.ordered_block().first_block();
            self.blocks_without_payloads_by_hash
                .insert(first_block.id(), pending_block.clone());
        }

        // Return the ready block (if one exists)
        ready_block
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L341-353)
```rust
    async fn order_ready_pending_block(&mut self, block_epoch: u64, block_round: Round) {
        // Remove any ready pending block
        let pending_block_with_metadata = self
            .observer_block_data
            .lock()
            .remove_ready_pending_block(block_epoch, block_round);

        // Process the ready ordered block (if it exists)
        if let Some(pending_block_with_metadata) = pending_block_with_metadata {
            self.process_ordered_block(pending_block_with_metadata)
                .await;
        }
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L639-714)
```rust
    async fn process_ordered_block_message(
        &mut self,
        peer_network_id: PeerNetworkId,
        message_received_time: Instant,
        ordered_block: OrderedBlock,
    ) {
        // If execution pool is enabled, ignore the message
        if self.get_execution_pool_window_size().is_some() {
            // Log the failure and update the invalid message counter
            warn!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Received ordered block message from peer: {:?}, but execution pool is enabled! Ignoring: {:?}",
                    peer_network_id, ordered_block.proof_block_info()
                ))
            );
            increment_invalid_message_counter(&peer_network_id, metrics::ORDERED_BLOCK_LABEL);
            return;
        }

        // Verify the ordered blocks before processing
        if let Err(error) = ordered_block.verify_ordered_blocks() {
            // Log the error and update the invalid message counter
            error!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Failed to verify ordered blocks! Ignoring: {:?}, from peer: {:?}. Error: {:?}",
                    ordered_block.proof_block_info(),
                    peer_network_id,
                    error
                ))
            );
            increment_invalid_message_counter(&peer_network_id, metrics::ORDERED_BLOCK_LABEL);
            return;
        };

        // Get the epoch and round of the first block
        let first_block = ordered_block.first_block();
        let first_block_epoch_round = (first_block.epoch(), first_block.round());

        // Determine if the block is behind the last ordered block, or if it is already pending
        let last_ordered_block = self.observer_block_data.lock().get_last_ordered_block();
        let block_out_of_date =
            first_block_epoch_round <= (last_ordered_block.epoch(), last_ordered_block.round());
        let block_pending = self
            .observer_block_data
            .lock()
            .existing_pending_block(&ordered_block);

        // If the block is out of date or already pending, ignore it
        if block_out_of_date || block_pending {
            // Update the metrics for the dropped ordered block
            update_metrics_for_dropped_ordered_block_message(peer_network_id, &ordered_block);
            return;
        }

        // Update the metrics for the received ordered block
        update_metrics_for_ordered_block_message(peer_network_id, &ordered_block);

        // Create a new pending block with metadata
        let observed_ordered_block = ObservedOrderedBlock::new(ordered_block);
        let pending_block_with_metadata = PendingBlockWithMetadata::new_with_arc(
            peer_network_id,
            message_received_time,
            observed_ordered_block,
        );

        // If all payloads exist, process the block. Otherwise, store it
        // in the pending block store and wait for the payloads to arrive.
        if self.all_payloads_exist(pending_block_with_metadata.ordered_block().blocks()) {
            self.process_ordered_block(pending_block_with_metadata)
                .await;
        } else {
            self.observer_block_data
                .lock()
                .insert_pending_block(pending_block_with_metadata);
        }
    }
```

**File:** consensus/src/consensus_observer/observer/block_data.rs (L244-251)
```rust
    pub fn remove_ready_pending_block(
        &mut self,
        received_payload_epoch: u64,
        received_payload_round: Round,
    ) -> Option<Arc<PendingBlockWithMetadata>> {
        self.pending_block_store.remove_ready_block(
            received_payload_epoch,
            received_payload_round,
```
