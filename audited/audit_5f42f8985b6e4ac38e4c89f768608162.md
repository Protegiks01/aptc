# Audit Report

## Title
Race Condition in Local File Store: Concurrent Read-Write Operations Can Cause Data Corruption

## Summary
The local file store implementation in the indexer-grpc service lacks proper synchronization between concurrent read and write operations. When multiple clients call `get_transactions()` for the same version while that version is being written to disk, readers can obtain partial or corrupted data due to non-atomic file operations.

## Finding Description

The vulnerability exists in the interaction between the file store processor (writer) and data service (reader) when using the local file store backend.

**Write Path:** [1](#0-0) 

The `upload_transaction_batch()` function spawns async tasks that write transaction batches using `tokio::fs::write()`, which is not atomic. The write operation consists of three phases: (1) open/create file, (2) write data, (3) close file.

**Read Path:** [2](#0-1) 

The `get_raw_file()` function reads files using `tokio::fs::read()`, which can execute concurrently with writes.

**Lack of Synchronization:** [3](#0-2) 

The `LocalFileStoreOperator` struct contains no mutexes, file locks, or synchronization primitives to coordinate read-write access.

**Race Condition Timeline:**

1. File store processor fetches batch from cache and spawns write task [4](#0-3) 

2. Data service determines version is evicted from cache and attempts file read [5](#0-4) 

3. If the read occurs while write is in progress, `tokio::fs::read()` may return:
   - Partial data (if write is mid-stream)
   - Empty data (if write just started)
   - Complete data (if timing is lucky)

4. Decompression/deserialization of partial data will fail or produce incorrect results [6](#0-5) 

**Missing Coordination:**

The data service checks cache coverage status but does NOT verify file store readiness before reading: [7](#0-6) 

The processor updates cache metadata BEFORE files are fully written, creating a window where versions appear "evicted" but files are still being written: [8](#0-7) 

## Impact Explanation

This vulnerability is **Medium severity** per the Aptos bug bounty criteria:

- **Limited to Indexer Service**: Affects only the indexer-grpc service, not core blockchain consensus or validator operations
- **Data Integrity Issues**: Can cause indexer clients to receive corrupted transaction data or experience service disruptions
- **State Inconsistencies**: Different clients querying the same version at different times may receive different results, requiring manual intervention
- **Service Reliability**: Repeated failures during decompression/deserialization can degrade indexer availability

The impact is limited because:
1. This only affects the **local file store backend** (GCS backend is unaffected due to atomic operations)
2. The indexer is an **off-chain auxiliary service** - core blockchain operations continue normally
3. Corrupted reads typically fail at decompression/deserialization rather than silently propagating incorrect data

## Likelihood Explanation

**Likelihood: Medium-High**

This race condition can occur during normal operation:
- **Cache eviction** happens naturally based on TTL (transactions older than cache size are evicted)
- **File processor lag**: The file processor may fall behind the cache worker, creating a gap where versions are evicted but not yet written to file store
- **Concurrent clients**: Multiple indexer clients requesting the same version simultaneously increases collision probability
- **High throughput periods**: During high transaction volume, up to 50 concurrent write tasks execute, widening the race window

The vulnerability is **NOT** GCS-specific (GCS provides atomic reads/writes), making it primarily a concern for local development/testing environments.

## Recommendation

Implement file-level locking to prevent concurrent read-write access:

```rust
use std::sync::Arc;
use tokio::sync::RwLock;

pub struct LocalFileStoreOperator {
    path: PathBuf,
    latest_metadata_update_timestamp: Option<std::time::Instant>,
    storage_format: StorageFormat,
    // Add file locks map
    file_locks: Arc<dashmap::DashMap<u64, Arc<RwLock<()>>>>,
}

async fn get_raw_file(&self, version: u64) -> anyhow::Result<Vec<u8>> {
    let starting_version = version / FILE_ENTRY_TRANSACTION_COUNT * FILE_ENTRY_TRANSACTION_COUNT;
    let lock = self.file_locks.entry(starting_version)
        .or_insert_with(|| Arc::new(RwLock::new(())))
        .clone();
    
    // Acquire read lock
    let _guard = lock.read().await;
    
    let file_entry_key = FileEntry::build_key(version, self.storage_format).to_string();
    let file_path = self.path.join(file_entry_key);
    tokio::fs::read(file_path).await.map_err(|err| { /* ... */ })
}

async fn upload_transaction_batch(&mut self, /* ... */) -> anyhow::Result<(u64, u64)> {
    // ... preparation code ...
    
    for chunk in transactions.chunks(FILE_ENTRY_TRANSACTION_COUNT as usize) {
        let starting_version = chunk.first().unwrap().version;
        let lock = self.file_locks.entry(starting_version)
            .or_insert_with(|| Arc::new(RwLock::new(())))
            .clone();
        
        let task = tokio::spawn(async move {
            // Acquire write lock
            let _guard = lock.write().await;
            tokio::fs::write(txns_path, file_entry.into_inner()).await
        });
        tasks.push(task);
    }
    // ... rest of implementation ...
}
```

Alternative: Use OS-level file locking via `fs2` crate for cross-process safety.

## Proof of Concept

**Scenario**: Simulate concurrent read-write race condition

```rust
use tokio;
use std::path::PathBuf;
use std::sync::Arc;

#[tokio::test]
async fn test_concurrent_read_write_race() {
    let temp_dir = tempfile::tempdir().unwrap();
    let operator = Arc::new(LocalFileStoreOperator::new(
        temp_dir.path().to_path_buf(),
        true, // compression enabled
    ));
    
    // Create test transactions
    let transactions: Vec<Transaction> = (0..1000)
        .map(|i| Transaction { version: i, ..Default::default() })
        .collect();
    
    let write_op = operator.clone();
    let read_op = operator.clone();
    
    // Spawn write task
    let write_handle = tokio::spawn(async move {
        write_op.upload_transaction_batch(1, transactions).await
    });
    
    // Spawn concurrent read tasks (may race with write)
    let mut read_handles = vec![];
    for _ in 0..10 {
        let op = read_op.clone();
        read_handles.push(tokio::spawn(async move {
            // Try to read version 0 while it's being written
            tokio::time::sleep(tokio::time::Duration::from_millis(1)).await;
            op.get_transactions(0, 5).await
        }));
    }
    
    // Wait for all operations
    write_handle.await.unwrap();
    let results: Vec<_> = futures::future::join_all(read_handles)
        .await
        .into_iter()
        .collect();
    
    // Check for inconsistencies or errors
    for (i, result) in results.iter().enumerate() {
        match result {
            Ok(Ok(txns)) => println!("Read {} succeeded with {} txns", i, txns.len()),
            Ok(Err(e)) => println!("Read {} failed: {}", i, e),
            Err(e) => println!("Read {} panicked: {}", i, e),
        }
    }
}
```

**Expected Result**: Without proper locking, some reads will fail with decompression/deserialization errors when they catch the file mid-write.

**Notes**

This vulnerability is specific to the **local file store implementation** used primarily in development/testing environments. Production deployments using GCS are not affected due to GCS's atomic object operations and strong consistency guarantees. [9](#0-8) 

The indexer-grpc service is an off-chain component that does not affect blockchain consensus, validator operations, or on-chain state. However, data integrity in the indexer is important for client applications relying on accurate historical transaction data.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/local.rs (L16-36)
```rust
pub struct LocalFileStoreOperator {
    path: PathBuf,
    /// The timestamp of the latest metadata update; this is to avoid too frequent metadata update.
    latest_metadata_update_timestamp: Option<std::time::Instant>,
    storage_format: StorageFormat,
}

impl LocalFileStoreOperator {
    pub fn new(path: PathBuf, enable_compression: bool) -> Self {
        let storage_format = if enable_compression {
            StorageFormat::Lz4CompressedProto
        } else {
            StorageFormat::JsonBase64UncompressedProto
        };
        Self {
            path,
            latest_metadata_update_timestamp: None,
            storage_format,
        }
    }
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/local.rs (L58-74)
```rust
    async fn get_raw_file(&self, version: u64) -> anyhow::Result<Vec<u8>> {
        let file_entry_key = FileEntry::build_key(version, self.storage_format).to_string();
        let file_path = self.path.join(file_entry_key);
        match tokio::fs::read(file_path).await {
            Ok(file) => Ok(file),
            Err(err) => {
                if err.kind() == std::io::ErrorKind::NotFound {
                    anyhow::bail!("[Indexer File] Transactions file not found. Gap might happen between cache and file store. {}", err)
                } else {
                    anyhow::bail!(
                        "[Indexer File] Error happens when transaction file. {}",
                        err
                    );
                }
            },
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/local.rs (L184-190)
```rust
            let task = tokio::spawn(async move {
                match tokio::fs::write(txns_path, file_entry.into_inner()).await {
                    Ok(_) => Ok(()),
                    Err(err) => Err(anyhow::Error::from(err)),
                }
            });
            tasks.push(task);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store/src/processor.rs (L160-202)
```rust
                let task = tokio::spawn(async move {
                    let fetch_start_time = std::time::Instant::now();
                    let transactions = cache_operator_clone
                        .get_transactions(start_version, FILE_ENTRY_TRANSACTION_COUNT)
                        .await
                        .unwrap();
                    let last_transaction = transactions.last().unwrap().clone();
                    log_grpc_step(
                        SERVICE_TYPE,
                        IndexerGrpcStep::FilestoreFetchTxns,
                        Some(start_version as i64),
                        Some((start_version + FILE_ENTRY_TRANSACTION_COUNT - 1) as i64),
                        None,
                        None,
                        Some(fetch_start_time.elapsed().as_secs_f64()),
                        None,
                        Some(FILE_ENTRY_TRANSACTION_COUNT as i64),
                        None,
                    );
                    for (i, txn) in transactions.iter().enumerate() {
                        assert_eq!(txn.version, start_version + i as u64);
                    }
                    let upload_start_time = std::time::Instant::now();
                    let (start, end) = file_store_operator_clone
                        .upload_transaction_batch(chain_id, transactions)
                        .await
                        .unwrap();
                    log_grpc_step(
                        SERVICE_TYPE,
                        IndexerGrpcStep::FilestoreUploadTxns,
                        Some(start_version as i64),
                        Some((start_version + FILE_ENTRY_TRANSACTION_COUNT - 1) as i64),
                        None,
                        None,
                        Some(upload_start_time.elapsed().as_secs_f64()),
                        None,
                        Some(FILE_ENTRY_TRANSACTION_COUNT as i64),
                        None,
                    );

                    (start, end, last_transaction)
                });
                tasks.push(task);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store/src/processor.rs (L256-273)
```rust
            // Update filestore metadata. First do it in cache for performance then update metadata file
            let start_metadata_upload_time = std::time::Instant::now();
            self.cache_operator
                .update_file_store_latest_version(batch_start_version)
                .await?;
            while self
                .file_store_operator
                .update_file_store_metadata_with_timeout(chain_id, batch_start_version)
                .await
                .is_err()
            {
                tracing::error!(
                    batch_start_version = batch_start_version,
                    "Failed to update file store metadata. Retrying."
                );
                std::thread::sleep(std::time::Duration::from_millis(500));
                METADATA_UPLOAD_FAILURE_COUNT.inc();
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/service.rs (L258-276)
```rust
    let cache_coverage_status = cache_operator
        .check_cache_coverage_status(start_version)
        .await;

    let num_tasks_to_use = match cache_coverage_status {
        Ok(CacheCoverageStatus::DataNotReady) => return DataFetchSubTaskResult::NoResults,
        Ok(CacheCoverageStatus::CacheHit(_)) => 1,
        Ok(CacheCoverageStatus::CacheEvicted) => match transactions_count {
            None => MAX_FETCH_TASKS_PER_REQUEST,
            Some(transactions_count) => {
                let num_tasks = transactions_count / TRANSACTIONS_PER_STORAGE_BLOCK;
                num_tasks.clamp(1, MAX_FETCH_TASKS_PER_REQUEST)
            },
        },
        Err(_) => {
            error!("[Data Service] Failed to get cache coverage status.");
            panic!("Failed to get cache coverage status.");
        },
    };
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/service.rs (L769-773)
```rust
        Ok(CacheBatchGetStatus::EvictedFromCache) => {
            let transactions =
                data_fetch_from_filestore(starting_version, file_store_operator, request_metadata)
                    .await?;
            Ok(TransactionsDataStatus::Success(transactions))
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/compression_util.rs (L264-271)
```rust
            FileEntry::Lz4CompressionProto(bytes) => {
                let mut decompressor = Decoder::new(&bytes[..]).expect("Lz4 decompression failed.");
                let mut decompressed = Vec::new();
                decompressor
                    .read_to_end(&mut decompressed)
                    .expect("Lz4 decompression failed.");
                TransactionsInStorage::decode(decompressed.as_slice())
                    .expect("proto deserialization failed.")
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/gcs.rs (L103-124)
```rust
    async fn get_raw_file(&self, version: u64) -> anyhow::Result<Vec<u8>> {
        let file_entry_key_path = self.get_file_entry_key_path(version);
        match Object::download(&self.bucket_name, file_entry_key_path.as_str()).await {
            Ok(file) => Ok(file),
            Err(cloud_storage::Error::Other(err)) => {
                if err.contains("No such object: ") {
                    anyhow::bail!("[Indexer File] Transactions file not found. Gap might happen between cache and file store. {}", err)
                } else {
                    anyhow::bail!(
                        "[Indexer File] Error happens when downloading transaction file. {}",
                        err
                    );
                }
            },
            Err(err) => {
                anyhow::bail!(
                    "[Indexer File] Error happens when transaction file. {}",
                    err
                );
            },
        }
    }
```
