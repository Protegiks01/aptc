# Audit Report

## Title
Non-Atomic Validator Set Updates Create Race Condition Leading to Network Connectivity Degradation

## Summary
The `handle_update_discovered_peers` function performs non-atomic updates to the validator set by releasing and reacquiring locks between peer updates. This creates intermediate states where concurrent readers can observe a partially updated validator set, potentially causing connectivity issues and temporary network partitioning during epoch transitions.

## Finding Description

The vulnerability exists in the validator set update mechanism used by the network discovery component. When a new validator set is received via REST discovery, the update process in `handle_update_discovered_peers` is not atomic. [1](#0-0) 

The REST discovery stream calls `extract_validator_set_updates`, which returns a complete `PeerSet`. However, when this update is processed by the connectivity manager, the atomicity guarantee is lost: [2](#0-1) 

The update process occurs in three non-atomic phases:

**Phase 1 (lines 903-921):** Acquires write lock, iterates through existing peers to mark obsolete ones for removal, then releases the lock.

**Phase 2 (lines 924-926):** For each peer marked for removal, acquires a NEW write lock, removes the peer, then releases the lock. This happens in a loop with the lock being released and reacquired for EACH peer.

**Phase 3 (lines 929-982):** For each new peer to add/update, acquires a NEW write lock at line 936, updates that single peer, then releases the lock. Again, the lock is released and reacquired for EACH peer individually.

Between these phases and within phase iterations, other threads can read the peer set and observe intermediate states. Critical concurrent read points include: [3](#0-2) [4](#0-3) [5](#0-4) 

**Attack Scenario:**

1. During an epoch transition, a validator set update is triggered
2. The connectivity manager begins processing the update via `handle_update_discovered_peers`
3. After Phase 1 completes (old validators marked for removal) but before Phase 3 completes (new validators fully added)
4. A concurrent connectivity check triggers `choose_peers_to_dial` which reads the peer set at line 577
5. The node observes an incomplete validator set with:
   - Some old validators already removed
   - Some new validators not yet added or only partially updated
6. The node makes dialing decisions based on this incomplete view, missing critical validators
7. If multiple nodes experience this simultaneously during epoch changes, they may fail to establish sufficient connectivity to maintain consensus quorum

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos bug bounty program criteria: "State inconsistencies requiring intervention."

**Specific Impacts:**
- **Temporary Liveness Degradation**: Nodes may fail to dial sufficient validators during epoch transitions, reducing network connectivity
- **Consensus Participation Issues**: Validators experiencing this race condition may temporarily lose connectivity to peers needed for consensus participation
- **Network Partitioning Risk**: If many nodes simultaneously observe different intermediate states during a validator set update, they could dial different subsets of validators, creating temporary network partitions
- **Recovery Time**: The issue self-heals at the next connectivity check interval, but this delay could be problematic during critical epoch transitions

The issue does not cause permanent damage, fund loss, or consensus safety violations, but it can temporarily degrade network availability and liveness during validator set changes—a critical operational moment for the blockchain.

## Likelihood Explanation

**Likelihood: Medium-High**

This vulnerability is likely to manifest under normal operating conditions:

1. **Regular Occurrence**: Validator set updates happen at every epoch transition (regularly scheduled events in Aptos)
2. **Race Window**: The connectivity check interval creates regular opportunities for the race condition
3. **No Special Privileges Required**: This is not an attack—it's a race condition that occurs naturally during concurrent operations
4. **Multiple Read Points**: Three different concurrent readers (`choose_peers_to_dial`, `update_ping_latency_metrics`, latency-based selection) increase the probability of observing intermediate states
5. **Wider Impact with REST Discovery**: Nodes using REST-based discovery (line 42-68 in rest.rs) are particularly vulnerable as they may receive updates more frequently

The race condition doesn't require any attacker action—it occurs naturally when normal system operations interleave.

## Recommendation

Modify `handle_update_discovered_peers` to perform atomic updates by holding the write lock for the entire update operation:

```rust
fn handle_update_discovered_peers(
    &mut self,
    src: DiscoverySource,
    new_discovered_peers: PeerSet,
) {
    info!(
        NetworkSchema::new(&self.network_context),
        "{} Received updated list of discovered peers! Source: {:?}, num peers: {:?}",
        self.network_context,
        src,
        new_discovered_peers.len()
    );

    // Acquire write lock ONCE for the entire operation
    let mut discovered_peers = self.discovered_peers.write();
    let mut keys_updated = false;
    let mut peers_to_check_remove = Vec::new();
    
    // Phase 1: Mark peers for removal (lock already held)
    for (peer_id, peer) in discovered_peers.peer_set.iter_mut() {
        let new_peer = new_discovered_peers.get(peer_id);
        let check_remove = if let Some(new_peer) = new_peer {
            if new_peer.keys.is_empty() {
                keys_updated |= peer.keys.clear_src(src);
            }
            if new_peer.addresses.is_empty() {
                peer.addrs.clear_src(src);
            }
            new_peer.addresses.is_empty() && new_peer.keys.is_empty()
        } else {
            keys_updated |= peer.keys.clear_src(src);
            peer.addrs.clear_src(src);
            true
        };
        if check_remove {
            peers_to_check_remove.push(*peer_id);
        }
    }

    // Phase 2: Remove peers (lock still held)
    for peer_id in peers_to_check_remove {
        if let Entry::Occupied(entry) = discovered_peers.peer_set.entry(peer_id) {
            if entry.get().is_empty() {
                entry.remove();
            }
        }
    }

    // Phase 3: Update/add new peers (lock still held)
    for (peer_id, discovered_peer) in new_discovered_peers {
        if peer_id == self.network_context.peer_id() {
            continue;
        }

        let peer = discovered_peers
            .peer_set
            .entry(peer_id)
            .or_insert_with(|| DiscoveredPeer::new(discovered_peer.role));

        let mut peer_updated = false;
        if peer.keys.update(src, discovered_peer.keys) {
            info!(
                NetworkSchema::new(&self.network_context)
                    .remote_peer(&peer_id)
                    .discovery_source(&src),
                "{} pubkey sets updated for peer: {}, pubkeys: {}",
                self.network_context,
                peer_id.short_str(),
                peer.keys
            );
            keys_updated = true;
            peer_updated = true;
        }

        if peer.addrs.update(src, discovered_peer.addresses) {
            info!(
                NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                network_addresses = &peer.addrs,
                "{} addresses updated for peer: {}, update src: {:?}, addrs: {}",
                self.network_context,
                peer_id.short_str(),
                src,
                &peer.addrs,
            );
            peer_updated = true;
        }

        if peer_updated {
            if let Some(dial_state) = self.dial_states.get_mut(&peer_id) {
                *dial_state = DialState::new(self.backoff_strategy.clone());
            }
        }
    }
    
    // Release lock here atomically after all updates complete
    drop(discovered_peers);

    // Update eligible peers after atomic update
    if keys_updated {
        let new_eligible = self.discovered_peers.read().get_eligible_peers();
        if let Err(error) = self
            .peers_and_metadata
            .set_trusted_peers(&self.network_context.network_id(), new_eligible)
        {
            error!(
                NetworkSchema::new(&self.network_context),
                error = %error,
                "Failed to update trusted peers set"
            );
        }
    }
}
```

The key changes:
1. Acquire the write lock once at the beginning (line 17)
2. Perform all three phases while holding the lock
3. Use direct access to `discovered_peers.peer_set` instead of repeated `write()` calls
4. Explicitly drop the lock only after all updates complete
5. Update eligible peers after the atomic update

## Proof of Concept

```rust
#[cfg(test)]
mod atomicity_test {
    use super::*;
    use std::sync::{Arc, atomic::{AtomicBool, Ordering}};
    use std::thread;
    use std::time::Duration;

    #[test]
    fn test_validator_set_update_race_condition() {
        // Setup: Create a connectivity manager with initial peer set
        let network_context = NetworkContext::mock();
        let discovered_peers = Arc::new(RwLock::new(DiscoveredPeerSet::default()));
        
        // Add initial validators (old epoch)
        {
            let mut peers = discovered_peers.write();
            for i in 0..10 {
                let peer_id = PeerId::random();
                peers.peer_set.insert(
                    peer_id,
                    DiscoveredPeer::new(PeerRole::Validator)
                );
            }
        }
        
        let observed_intermediate_state = Arc::new(AtomicBool::new(false));
        let discovered_peers_clone = discovered_peers.clone();
        let observed_clone = observed_intermediate_state.clone();
        
        // Thread 1: Simulate validator set update (Phase 1, 2, 3 with locks released)
        let updater = thread::spawn(move || {
            // Simulate the non-atomic update pattern from the vulnerable code
            
            // Phase 1: Mark for removal
            {
                let mut peers = discovered_peers_clone.write();
                // Simulate marking old validators for removal
                peers.peer_set.clear();
            }
            
            // Small delay to increase race window
            thread::sleep(Duration::from_millis(5));
            
            // Phase 3: Add new validators one by one
            for i in 0..10 {
                let mut peers = discovered_peers_clone.write();
                let peer_id = PeerId::random();
                peers.peer_set.insert(
                    peer_id,
                    DiscoveredPeer::new(PeerRole::Validator)
                );
                // Lock released here between iterations
            }
        });
        
        // Thread 2: Concurrent reader (choose_peers_to_dial)
        let reader = thread::spawn(move || {
            for _ in 0..100 {
                let peers = discovered_peers.read();
                let peer_count = peers.peer_set.len();
                
                // If we observe less than 10 peers, we caught an intermediate state
                if peer_count < 10 && peer_count > 0 {
                    observed_clone.store(true, Ordering::SeqCst);
                    println!("Race condition detected! Observed {} peers during update", peer_count);
                    break;
                }
                thread::sleep(Duration::from_micros(100));
            }
        });
        
        updater.join().unwrap();
        reader.join().unwrap();
        
        // Assert that we successfully caught the race condition
        assert!(
            observed_intermediate_state.load(Ordering::SeqCst),
            "Race condition should be observable with concurrent reads during updates"
        );
    }
}
```

This test demonstrates that concurrent readers can observe intermediate states during validator set updates, confirming the vulnerability.

## Notes

This vulnerability specifically affects the REST-based discovery mechanism but the same non-atomic update pattern exists for all discovery sources (ValidatorSet, File, Rest, Config). The fix should be applied consistently across all update paths to ensure atomic validator set transitions throughout the network layer.

### Citations

**File:** network/discovery/src/rest.rs (L42-68)
```rust
    fn poll_next(mut self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Option<Self::Item>> {
        // Wait for delay, or add the delay for next call
        futures::ready!(self.interval.as_mut().poll_next(cx));

        // Retrieve the onchain resource at the interval
        // TODO there should be a better way than converting this to a blocking call
        let response = block_on(self.rest_client.get_account_resource_bcs::<ValidatorSet>(
            AccountAddress::ONE,
            "0x1::stake::ValidatorSet",
        ));
        Poll::Ready(match response {
            Ok(inner) => {
                let validator_set = inner.into_inner();
                Some(Ok(extract_validator_set_updates(
                    self.network_context,
                    validator_set,
                )))
            },
            Err(err) => {
                info!(
                    "Failed to retrieve validator set by REST discovery {:?}",
                    err
                );
                Some(Err(DiscoveryError::Rest(err)))
            },
        })
    }
```

**File:** network/framework/src/connectivity_manager/mod.rs (L572-586)
```rust
    async fn choose_peers_to_dial(&mut self) -> Vec<(PeerId, DiscoveredPeer)> {
        // Get the eligible peers to dial
        let network_id = self.network_context.network_id();
        let role = self.network_context.role();
        let roles_to_dial = network_id.upstream_roles(&role);
        let discovered_peers = self.discovered_peers.read().peer_set.clone();
        let eligible_peers: Vec<_> = discovered_peers
            .into_iter()
            .filter(|(peer_id, peer)| {
                peer.is_eligible_to_be_dialed() // The node is eligible to dial
                    && !self.connected.contains_key(peer_id) // The node is not already connected
                    && !self.dial_queue.contains_key(peer_id) // There is no pending dial to this node
                    && roles_to_dial.contains(&peer.role) // We can dial this role
            })
            .collect();
```

**File:** network/framework/src/connectivity_manager/mod.rs (L839-854)
```rust
    fn update_ping_latency_metrics(&self) {
        // Update the pre-dial peer ping latencies
        for (_, peer) in self.discovered_peers.read().peer_set.iter() {
            if let Some(ping_latency_secs) = peer.ping_latency_secs {
                counters::observe_pre_dial_ping_time(&self.network_context, ping_latency_secs);
            }
        }

        // Update the connected peer ping latencies
        for peer_id in self.connected.keys() {
            if let Some(ping_latency_secs) =
                self.discovered_peers.read().get_ping_latency_secs(peer_id)
            {
                counters::observe_connected_ping_time(&self.network_context, ping_latency_secs);
            }
        }
```

**File:** network/framework/src/connectivity_manager/mod.rs (L886-1002)
```rust
    fn handle_update_discovered_peers(
        &mut self,
        src: DiscoverySource,
        new_discovered_peers: PeerSet,
    ) {
        // Log the update event
        info!(
            NetworkSchema::new(&self.network_context),
            "{} Received updated list of discovered peers! Source: {:?}, num peers: {:?}",
            self.network_context,
            src,
            new_discovered_peers.len()
        );

        // Remove peers that no longer have relevant network information
        let mut keys_updated = false;
        let mut peers_to_check_remove = Vec::new();
        for (peer_id, peer) in self.discovered_peers.write().peer_set.iter_mut() {
            let new_peer = new_discovered_peers.get(peer_id);
            let check_remove = if let Some(new_peer) = new_peer {
                if new_peer.keys.is_empty() {
                    keys_updated |= peer.keys.clear_src(src);
                }
                if new_peer.addresses.is_empty() {
                    peer.addrs.clear_src(src);
                }
                new_peer.addresses.is_empty() && new_peer.keys.is_empty()
            } else {
                keys_updated |= peer.keys.clear_src(src);
                peer.addrs.clear_src(src);
                true
            };
            if check_remove {
                peers_to_check_remove.push(*peer_id);
            }
        }

        // Remove peers that no longer have state
        for peer_id in peers_to_check_remove {
            self.discovered_peers.write().remove_peer_if_empty(&peer_id);
        }

        // Make updates to the peers accordingly
        for (peer_id, discovered_peer) in new_discovered_peers {
            // Don't include ourselves, because we don't need to dial ourselves
            if peer_id == self.network_context.peer_id() {
                continue;
            }

            // Create the new `DiscoveredPeer`, role is set when a `Peer` is first discovered
            let mut discovered_peers = self.discovered_peers.write();
            let peer = discovered_peers
                .peer_set
                .entry(peer_id)
                .or_insert_with(|| DiscoveredPeer::new(discovered_peer.role));

            // Update the peer's pubkeys
            let mut peer_updated = false;
            if peer.keys.update(src, discovered_peer.keys) {
                info!(
                    NetworkSchema::new(&self.network_context)
                        .remote_peer(&peer_id)
                        .discovery_source(&src),
                    "{} pubkey sets updated for peer: {}, pubkeys: {}",
                    self.network_context,
                    peer_id.short_str(),
                    peer.keys
                );
                keys_updated = true;
                peer_updated = true;
            }

            // Update the peer's addresses
            if peer.addrs.update(src, discovered_peer.addresses) {
                info!(
                    NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                    network_addresses = &peer.addrs,
                    "{} addresses updated for peer: {}, update src: {:?}, addrs: {}",
                    self.network_context,
                    peer_id.short_str(),
                    src,
                    &peer.addrs,
                );
                peer_updated = true;
            }

            // If we're currently trying to dial this peer, we reset their
            // dial state. As a result, we will begin our next dial attempt
            // from the first address (which might have changed) and from a
            // fresh backoff (since the current backoff delay might be maxed
            // out if we can't reach any of their previous addresses).
            if peer_updated {
                if let Some(dial_state) = self.dial_states.get_mut(&peer_id) {
                    *dial_state = DialState::new(self.backoff_strategy.clone());
                }
            }
        }

        // update eligible peers accordingly
        if keys_updated {
            // For each peer, union all of the pubkeys from each discovery source
            // to generate the new eligible peers set.
            let new_eligible = self.discovered_peers.read().get_eligible_peers();

            // Swap in the new eligible peers set
            if let Err(error) = self
                .peers_and_metadata
                .set_trusted_peers(&self.network_context.network_id(), new_eligible)
            {
                error!(
                    NetworkSchema::new(&self.network_context),
                    error = %error,
                    "Failed to update trusted peers set"
                );
            }
        }
    }
```

**File:** network/framework/src/connectivity_manager/selection.rs (L115-121)
```rust
    let mut peer_ids_and_latency_weights = vec![];
    for peer_id in peer_ids {
        if let Some(ping_latency_secs) = discovered_peers.read().get_ping_latency_secs(peer_id) {
            let latency_weight = convert_latency_to_weight(ping_latency_secs);
            peer_ids_and_latency_weights.push((peer_id, OrderedFloat(latency_weight)));
        }
    }
```
