# Audit Report

## Title
Unvalidated `mempool_txn_pull_timeout_ms` Configuration Parameter Enables Consensus Liveness Failure

## Summary
The `mempool_txn_pull_timeout_ms` configuration parameter lacks validation bounds, allowing configuration errors to set extreme values that break consensus operation. When set to values significantly larger than `quorum_store_pull_timeout_ms`, the DirectMempoolQuorumStore service blocks indefinitely on slow mempool responses, causing sequential request processing to stall and preventing block proposal generation.

## Finding Description

The `mempool_txn_pull_timeout_ms` field in `ConsensusConfig` is defined as a `u64` with no validation constraints: [1](#0-0) 

The field is initialized with a default value of 1000ms but has no sanitization checks in the `ConfigSanitizer` implementation: [2](#0-1) [3](#0-2) 

The timeout is used in `DirectMempoolQuorumStore::pull_internal()` to wait for mempool responses: [4](#0-3) 

**Critical Design Flaw**: `DirectMempoolQuorumStore` processes requests sequentially in a single-threaded event loop: [5](#0-4) 

**Attack Path via Configuration Error**:

1. Operator mistakenly sets `mempool_txn_pull_timeout_ms: 300000` (5 minutes, intending 300ms) or any value >> `quorum_store_pull_timeout_ms` (400ms)
2. Node restarts and loads the misconfigured value
3. When the validator becomes block proposer, `ProposalGenerator` requests payload via `QuorumStoreClient`
4. `QuorumStoreClient::pull_internal()` sends request to `DirectMempoolQuorumStore` with 400ms timeout: [6](#0-5) 

5. `DirectMempoolQuorumStore` forwards request to mempool and waits up to 5 minutes for response
6. `QuorumStoreClient` times out after 400ms with error "did not receive GetBlockResponse on time"
7. **DirectMempoolQuorumStore remains blocked** waiting for mempool response due to sequential processing
8. Next round's payload request arrives but queues behind the blocked request
9. This cascades - all subsequent payload pulls fail, proposal generation fails
10. Validator cannot propose blocks during its turn, breaking consensus liveness

The error propagates through the proposal generation chain: [7](#0-6) [8](#0-7) 

Failures are logged but do not crash the node, resulting in silent liveness degradation: [9](#0-8) 

**Invariant Violation**: This breaks the consensus liveness guarantee. Even with <1/3 Byzantine validators, if the current leader has this misconfiguration, the round cannot produce blocks and must timeout, severely degrading network throughput.

## Impact Explanation

**Severity: Medium**

This qualifies as **Medium severity** per Aptos bug bounty criteria for the following reasons:

1. **State Inconsistencies Requiring Intervention**: The misconfigured validator cannot fulfill its proposer duties, requiring manual configuration correction and node restart to restore normal operation
   
2. **Consensus Liveness Degradation**: While not total network failure (other validators can still propose), the affected validator contributes to round timeouts and reduced block production rate. If multiple validators have this misconfiguration, consensus severely degrades or stalls

3. **Silent Failure Mode**: The node continues running but silently fails to propose, making diagnosis difficult without monitoring

4. **Already Problematic Default Configuration**: The default `mempool_txn_pull_timeout_ms` (1000ms) is already larger than `quorum_store_pull_timeout_ms` (400ms), indicating the timeout hierarchy is misconfigured in the shipped defaults

This does not reach **High/Critical** severity because:
- Requires configuration file modification (not remotely exploitable)
- Does not permanently break the network (correctable via config fix)
- Does not cause fund loss or consensus safety violations

## Likelihood Explanation

**Likelihood: Medium**

Configuration errors of this type are reasonably likely:

1. **No Validation Feedback**: Operators receive no warnings when setting invalid values
2. **Easy Typos**: Mistaking milliseconds for seconds, or adding extra zeros (1000 â†’ 10000 or 100000)
3. **Configuration Template Errors**: Copy-paste from outdated documentation or examples
4. **Automated Configuration Systems**: Infrastructure-as-code tools with unit conversion bugs
5. **Default Configuration Already Problematic**: The shipped defaults demonstrate the timeout relationships are not well-understood, increasing likelihood of further misconfiguration

## Recommendation

**Implement validation for `mempool_txn_pull_timeout_ms` in `ConsensusConfig::sanitize()`**:

Add validation ensuring the timeout hierarchy is maintained:
- `mempool_txn_pull_timeout_ms` < `quorum_store_pull_timeout_ms`
- `mempool_txn_pull_timeout_ms` has reasonable bounds (e.g., 10ms to 5000ms)

```rust
// In ConsensusConfig::sanitize()
fn sanitize_timeout_hierarchy(
    sanitizer_name: &str,
    config: &ConsensusConfig,
) -> Result<(), Error> {
    // Validate mempool timeout is less than quorum store pull timeout
    if config.mempool_txn_pull_timeout_ms >= config.quorum_store_pull_timeout_ms {
        return Err(Error::ConfigSanitizerFailed(
            sanitizer_name.to_owned(),
            format!(
                "mempool_txn_pull_timeout_ms ({}) must be less than quorum_store_pull_timeout_ms ({})",
                config.mempool_txn_pull_timeout_ms,
                config.quorum_store_pull_timeout_ms
            ),
        ));
    }
    
    // Validate reasonable bounds
    const MIN_TIMEOUT_MS: u64 = 10;
    const MAX_TIMEOUT_MS: u64 = 5000;
    if config.mempool_txn_pull_timeout_ms < MIN_TIMEOUT_MS 
        || config.mempool_txn_pull_timeout_ms > MAX_TIMEOUT_MS {
        return Err(Error::ConfigSanitizerFailed(
            sanitizer_name.to_owned(),
            format!(
                "mempool_txn_pull_timeout_ms ({}) must be between {} and {}",
                config.mempool_txn_pull_timeout_ms,
                MIN_TIMEOUT_MS,
                MAX_TIMEOUT_MS
            ),
        ));
    }
    
    Ok(())
}
```

**Also fix the default configuration** to respect the timeout hierarchy:
```rust
mempool_txn_pull_timeout_ms: 300, // Less than quorum_store_pull_timeout_ms (400)
```

**Long-term**: Consider refactoring `DirectMempoolQuorumStore` to process requests concurrently rather than sequentially to prevent one slow request from blocking all subsequent requests.

## Proof of Concept

**Reproduction Steps**:

1. Modify `consensus_config.yaml` or node configuration:
```yaml
consensus:
  mempool_txn_pull_timeout_ms: 300000  # 5 minutes instead of 1 second
```

2. Start validator node with this configuration

3. When the node becomes block proposer, observe:
   - Payload pull requests timeout with "did not receive GetBlockResponse on time"
   - Proposal generation fails with "Fail to retrieve payload"  
   - Warning logged: "Error generating and sending proposal"
   - No blocks proposed during validator's proposer rounds

4. Check metrics:
   - `PROPOSALS_COUNT` stops incrementing during proposer rounds
   - Round timeouts occur consistently
   - Network throughput degrades

5. Fix configuration to `mempool_txn_pull_timeout_ms: 300` and restart

6. Observe normal proposal generation resumes

**Expected Impact**: During the misconfiguration period, the affected validator cannot propose blocks, causing round timeouts and reducing network throughput proportional to that validator's voting power share.

## Notes

The investigation reveals that even the **default configuration has this timeout hierarchy inverted** (`mempool_txn_pull_timeout_ms: 1000ms` > `quorum_store_pull_timeout_ms: 400ms`), suggesting this relationship is not properly validated or documented. This makes accidental misconfigurations even more likely, as operators have no reference for correct timeout relationships.

Additionally, the sequential request processing in `DirectMempoolQuorumStore::start()` creates a single point of failure where one slow/stuck request blocks all subsequent requests. This architectural issue amplifies the impact of timeout misconfigurations.

### Citations

**File:** config/src/config/consensus_config.rs (L47-47)
```rust
    pub mempool_txn_pull_timeout_ms: u64,
```

**File:** config/src/config/consensus_config.rs (L234-234)
```rust
            mempool_txn_pull_timeout_ms: 1000,
```

**File:** config/src/config/consensus_config.rs (L503-533)
```rust
impl ConfigSanitizer for ConsensusConfig {
    fn sanitize(
        node_config: &NodeConfig,
        node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();

        // Verify that the safety rules and quorum store configs are valid
        SafetyRulesConfig::sanitize(node_config, node_type, chain_id)?;
        QuorumStoreConfig::sanitize(node_config, node_type, chain_id)?;

        // Verify that the consensus-only feature is not enabled in mainnet
        if let Some(chain_id) = chain_id {
            if chain_id.is_mainnet() && is_consensus_only_perf_test_enabled() {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "consensus-only-perf-test should not be enabled in mainnet!".to_string(),
                ));
            }
        }

        // Sender block limits must be <= receiver block limits
        Self::sanitize_send_recv_block_limits(&sanitizer_name, &node_config.consensus)?;

        // Quorum store batches must be <= consensus blocks
        Self::sanitize_batch_block_limits(&sanitizer_name, &node_config.consensus)?;

        Ok(())
    }
}
```

**File:** consensus/src/quorum_store/direct_mempool_quorum_store.rs (L69-87)
```rust
        match monitor!(
            "pull_txn",
            timeout(
                Duration::from_millis(self.mempool_txn_pull_timeout_ms),
                callback_rcv
            )
            .await
        ) {
            Err(_) => Err(anyhow::anyhow!(
                "[direct_mempool_quorum_store] did not receive GetBatchResponse on time"
            )),
            Ok(resp) => match resp.map_err(anyhow::Error::from)?? {
                QuorumStoreResponse::GetBatchResponse(txns) => Ok(txns),
                _ => Err(anyhow::anyhow!(
                    "[direct_mempool_quorum_store] did not receive expected GetBatchResponse"
                )),
            },
        }
    }
```

**File:** consensus/src/quorum_store/direct_mempool_quorum_store.rs (L153-164)
```rust
    pub async fn start(mut self) {
        loop {
            let _timer = counters::MAIN_LOOP.start_timer();
            ::futures::select! {
                msg = self.consensus_receiver.select_next_some() => {
                    self.handle_consensus_request(msg).await;
                },
                complete => break,
            }
        }
    }
}
```

**File:** consensus/src/payload_client/user/quorum_store_client.rs (L76-87)
```rust
        match monitor!(
            "pull_payload",
            timeout(Duration::from_millis(self.pull_timeout_ms), callback_rcv).await
        ) {
            Err(_) => {
                Err(anyhow::anyhow!("[consensus] did not receive GetBlockResponse on time").into())
            },
            Ok(resp) => match resp.map_err(anyhow::Error::from)?? {
                GetPayloadResponse::GetPayloadResponse(payload) => Ok(payload),
            },
        }
    }
```

**File:** consensus/src/liveness/proposal_generator.rs (L652-672)
```rust
        let (validator_txns, mut payload) = self
            .payload_client
            .pull_payload(
                PayloadPullParameters {
                    max_poll_time: self.quorum_store_poll_time.saturating_sub(proposal_delay),
                    max_txns: max_block_txns,
                    max_txns_after_filtering: max_block_txns_after_filtering,
                    soft_max_txns_after_filtering: max_txns_from_block_to_execute
                        .unwrap_or(max_block_txns_after_filtering),
                    max_inline_txns: self.max_inline_txns,
                    maybe_optqs_payload_pull_params,
                    user_txn_filter: payload_filter,
                    pending_ordering,
                    pending_uncommitted_blocks: pending_blocks.len(),
                    recent_max_fill_fraction: max_fill_fraction,
                    block_timestamp: timestamp,
                },
                validator_txn_filter,
            )
            .await
            .context("Fail to retrieve payload")?;
```

**File:** consensus/src/round_manager.rs (L508-510)
```rust
                ) {
                    warn!("Error generating and sending proposal: {}", e);
                }
```

**File:** consensus/src/round_manager.rs (L526-534)
```rust
        let proposal_msg = Self::generate_proposal(
            epoch_state.clone(),
            new_round_event,
            sync_info,
            proposal_generator,
            safety_rules,
            proposer_election,
        )
        .await?;
```
