# Audit Report

## Title
State Snapshot Restoration Bypass via Chunk Metadata Manipulation Allowing Incomplete State Recovery

## Summary
The StateSnapshotChunk metadata (first_key, last_key) is not cryptographically protected in the manifest file, allowing manifest tampering to cause incomplete state restoration. When first_key > last_key due to malicious modification or corruption, chunks can be incorrectly skipped during resume-based restoration, and no final root hash verification prevents the node from accepting incomplete state.

## Finding Description
The vulnerability exists in the state snapshot restoration process across multiple components:

**1. Unprotected Manifest Metadata**

The `StateSnapshotChunk` structure stores key range metadata that controls chunk processing during restoration: [1](#0-0) 

While cryptographic proofs protect the actual state data, the manifest JSON file containing the chunks array is not integrity-protected. An attacker with backup storage access can modify first_key and last_key values.

**2. Chunk Skip Logic Trusts Metadata Blindly**

During restoration with resumption, chunks are filtered based on the last_key metadata: [2](#0-1) 

If an attacker sets `last_key < first_key` for a chunk, and the `resume_point` falls between these values (last_key < resume_point < first_key), the chunk is incorrectly skipped even though it contains keys > resume_point.

**3. Missing Final Root Hash Verification**

After all chunks are processed, the restoration completes without verifying that the final tree matches the expected root hash: [3](#0-2) 

The `finish()` call does not validate the root hash. While JellyfishMerkleRestore verifies each chunk's proof incrementally, these proofs can all pass even if chunks are skipped, because each proof represents the *full* tree including unrestored portions via right siblings. [4](#0-3) 

The `finish_impl()` method only freezes and writes nodes without computing and verifying the final root hash matches `expected_root_hash`.

**Attack Scenario:**
1. Attacker compromises backup storage (S3/GCS bucket) or causes storage corruption
2. Modifies StateSnapshotBackup manifest JSON, setting chunk N to have: `first_key = 0xFF...FF`, `last_key = 0x00...00`  
3. Chunk N's actual blob data contains legitimate keys in range [0x60..., 0x90...]
4. Node begins state restoration, crashes mid-process
5. On resume with `resume_point = 0x50...`, the skip logic evaluates: `0x00... <= 0x50...` → true → skip chunk N
6. Keys [0x60..., 0x90...] are never restored
7. All incremental proof verifications pass (proofs include missing data as right siblings)
8. `finish()` completes without error
9. Node has incomplete state with wrong root hash

## Impact Explanation
This vulnerability enables an attacker who compromises backup storage to cause **state inconsistencies requiring intervention** (Medium severity per Aptos bug bounty criteria). 

**Impact:**
- Affected nodes have incomplete Jellyfish Merkle trees missing entire key ranges
- The actual root hash differs from the expected canonical root hash
- State queries for missing keys return incorrect "not found" results
- The node cannot participate in consensus (other validators reject invalid state proofs)
- Node operators must detect the issue and re-restore from a clean backup

This breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs." The restored state is neither complete nor verifiable.

While this is a node-level denial-of-service rather than a network-wide attack, it represents a significant vulnerability in the state restoration system's defense-in-depth mechanisms.

## Likelihood Explanation
**Likelihood: Medium**

The vulnerability requires one of the following conditions:
1. **Backup storage compromise** - Realistic in distributed systems where backup storage (cloud buckets) may have weaker access controls than validator keys
2. **Storage corruption** - Hardware failures, bit flips, or filesystem issues could corrupt manifest JSON
3. **Software bug** - An undiscovered bug in backup creation logic could produce invalid metadata

The attack is feasible because:
- Manifest files are unencrypted JSON with no HMAC or signature
- Backup restoration is a common operation during node bootstrapping, disaster recovery, and state sync
- Resume functionality is specifically designed for interrupted restores, making this code path frequently exercised
- No final validation exists to catch the incomplete state

## Recommendation
Implement final root hash verification after state restoration completes:

```rust
// In storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs
async fn run_impl(self) -> Result<()> {
    // ... existing restoration logic ...
    
    tokio::task::spawn_blocking(move || receiver.lock().take().unwrap().finish()).await??;
    
    // ADD: Verify final root hash matches expected
    let actual_root_hash = self.run_mode.get_actual_root_hash(self.version)?;
    ensure!(
        actual_root_hash == manifest.root_hash,
        "State restoration completed but root hash mismatch. Expected: {}, actual: {}. \
         This indicates incomplete state - some chunks may have been skipped.",
        manifest.root_hash,
        actual_root_hash
    );
    
    self.run_mode.finish();
    Ok(())
}
```

Additionally, consider adding manifest integrity protection:
- Sign the entire manifest structure with validator keys
- Include manifest hash in the cryptographic proof chain
- Validate chunk metadata consistency (first_key <= last_key) during manifest loading

## Proof of Concept
```rust
// Test demonstrating the vulnerability
#[tokio::test]
async fn test_manifest_tampering_incomplete_restore() {
    // 1. Create legitimate backup with state keys [0x10, 0x30, 0x50, 0x70, 0x90]
    let (backup_store, manifest) = create_test_backup().await;
    let expected_root = manifest.root_hash;
    
    // 2. Tamper with manifest: swap first_key and last_key in chunk 2
    let mut tampered_manifest = manifest.clone();
    let chunk = &mut tampered_manifest.chunks[1]; 
    std::mem::swap(&mut chunk.first_key, &mut chunk.last_key);
    // Now chunk has first_key > last_key (reversed)
    
    // 3. Simulate interrupted restore (resume_point = 0x40)
    let restore_db = Arc::new(MockStateStore::new());
    restore_db.set_resume_point(HashValue::from_hex("0x40...").unwrap());
    
    // 4. Restore with tampered manifest
    let controller = StateSnapshotRestoreController::new(
        tampered_manifest,
        restore_db.clone(),
        /* ... */
    );
    
    // 5. Restoration completes without error (BUG!)
    controller.run().await.expect("Should detect incomplete state but doesn't!");
    
    // 6. Verify incomplete state
    let actual_root = restore_db.get_root_hash(version);
    assert_ne!(actual_root, expected_root, "Root hash mismatch not detected!");
    
    // 7. Verify missing keys
    let missing_key = StateKey::from_hex("0x70...").unwrap();
    assert!(restore_db.get(missing_key).is_none(), "Key should be missing due to skipped chunk");
}
```

**Notes**
This vulnerability demonstrates a defense-in-depth failure where multiple layers of validation exist (proof verification) but a critical final check is missing. While test code correctly validates final root hashes, production code does not, creating a security gap exploitable through backup storage compromise.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/manifest.rs (L9-27)
```rust
/// A chunk of a state snapshot manifest, representing accounts in the key range
/// [`first_key`, `last_key`] (right side inclusive).
#[derive(Deserialize, Serialize)]
pub struct StateSnapshotChunk {
    /// index of the first account in this chunk over all accounts.
    pub first_idx: usize,
    /// index of the last account in this chunk over all accounts.
    pub last_idx: usize,
    /// key of the first account in this chunk.
    pub first_key: HashValue,
    /// key of the last account in this chunk.
    pub last_key: HashValue,
    /// Repeated `len(record) + record` where `record` is BCS serialized tuple
    /// `(key, state_value)`
    pub blobs: FileHandle,
    /// BCS serialized `SparseMerkleRangeProof` that proves this chunk adds up to the root hash
    /// indicated in the backup (`StateSnapshotBackup::root_hash`).
    pub proof: FileHandle,
}
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L165-174)
```rust
        let resume_point_opt = receiver.lock().as_mut().unwrap().previous_key_hash()?;
        let chunks = if let Some(resume_point) = resume_point_opt {
            manifest
                .chunks
                .into_iter()
                .skip_while(|chunk| chunk.last_key <= resume_point)
                .collect()
        } else {
            manifest.chunks
        };
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L228-231)
```rust
        tokio::task::spawn_blocking(move || receiver.lock().take().unwrap().finish()).await??;
        self.run_mode.finish();
        Ok(())
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L748-789)
```rust
    /// Finishes the restoration process. This tells the code that there is no more state,
    /// otherwise we can not freeze the rightmost leaf and its ancestors.
    pub fn finish_impl(mut self) -> Result<()> {
        self.wait_for_async_commit()?;
        // Deal with the special case when the entire tree has a single leaf or null node.
        if self.partial_nodes.len() == 1 {
            let mut num_children = 0;
            let mut leaf = None;
            for i in 0..16 {
                if let Some(ref child_info) = self.partial_nodes[0].children[i] {
                    num_children += 1;
                    if let ChildInfo::Leaf(node) = child_info {
                        leaf = Some(node.clone());
                    }
                }
            }

            match num_children {
                0 => {
                    let node_key = NodeKey::new_empty_path(self.version);
                    assert!(self.frozen_nodes.is_empty());
                    self.frozen_nodes.insert(node_key, Node::Null);
                    self.store.write_node_batch(&self.frozen_nodes)?;
                    return Ok(());
                },
                1 => {
                    if let Some(node) = leaf {
                        let node_key = NodeKey::new_empty_path(self.version);
                        assert!(self.frozen_nodes.is_empty());
                        self.frozen_nodes.insert(node_key, node.into());
                        self.store.write_node_batch(&self.frozen_nodes)?;
                        return Ok(());
                    }
                },
                _ => (),
            }
        }

        self.freeze(0);
        self.store.write_node_batch(&self.frozen_nodes)?;
        Ok(())
    }
```
