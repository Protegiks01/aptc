# Audit Report

## Title
Empty ValidatorSet Bypass in on_new_epoch() Causes Complete Network Halt

## Summary
The `on_new_epoch()` function in `stake.move` lacks validation to prevent an empty `active_validators` set from being created during epoch transitions. When all validators fall below the `minimum_stake` threshold, the function creates an empty ValidatorSet that propagates to all nodes, causing complete network partition and consensus halt requiring a hardfork to recover.

## Finding Description

The vulnerability exists in the epoch transition logic where validators are filtered based on their voting power. The `on_new_epoch()` function iterates through all active validators and only includes those meeting the `minimum_stake` requirement in the next epoch's validator set. [1](#0-0) 

The critical flaw is at lines 1372-1401: the code creates an empty `next_epoch_validators` vector, filters validators by `minimum_stake`, and then directly assigns this potentially empty vector to `validator_set.active_validators` without any validation. There is no assertion to ensure at least one validator remains.

While the `leave_validator_set()` function contains protection against removing the last validator: [2](#0-1) 

This check is bypassed during `on_new_epoch()` execution because it operates on a different code path that directly reassigns the entire `active_validators` vector.

**Attack Scenario:**

1. A governance proposal (malicious or buggy) calls `update_required_stake()` to set an extremely high `minimum_stake` value that exceeds all current validators' voting power. [3](#0-2) 

2. During the next epoch transition triggered by `reconfiguration::reconfigure()`: [4](#0-3) 

3. The empty ValidatorSet is published as on-chain configuration and propagated to all nodes via `OnChainConfigPayload`.

4. The `extract_validator_set_updates()` function processes this empty ValidatorSet: [5](#0-4) 

When the ValidatorSet is empty, the iterator produces no items and `.collect()` returns an empty `PeerSet`. [6](#0-5) 

5. The empty PeerSet is sent to `ConnectivityManager` via `UpdateDiscoveredPeers`: [7](#0-6) 

6. The trusted peers set is updated to empty: [8](#0-7) 

7. During the next periodic connectivity check, `close_stale_connections()` disconnects ALL existing peers since none are in the empty trusted set: [9](#0-8) 

8. All new inbound connections are rejected during noise handshake authentication because no peer exists in the empty trusted peers set: [10](#0-9) 

For validator networks using `HandshakeAuthMode::Mutual`, any peer not in the trusted set results in `UnauthenticatedClient` error.

## Impact Explanation

**Severity: CRITICAL** (up to $1,000,000 per Aptos Bug Bounty)

This vulnerability causes:
- **Total loss of liveness/network availability**: All nodes disconnect from each other and cannot reconnect
- **Non-recoverable network partition (requires hardfork)**: The empty ValidatorSet is persisted on-chain with no mechanism to recover
- **Consensus/Safety violations**: No blocks can be produced when all nodes are isolated

The network enters a completely halted state where:
- No validator can communicate with any other validator
- No new blocks can be proposed or voted on
- No transactions can be processed
- The only recovery path is a coordinated hardfork

This breaks the fundamental invariant that `active_validators.length > 0`, which is critical for network operation.

## Likelihood Explanation

**Likelihood: Medium**

While the attack requires governance access, it is realistic because:

1. **Governance Accessibility**: Any participant with sufficient voting power can propose changes to `minimum_stake` through standard governance mechanisms
2. **Accidental Trigger**: This could occur unintentionally through a buggy governance proposal that miscalculates the appropriate `minimum_stake` value
3. **No Rate Limiting**: There are no checks preventing a dramatic increase in `minimum_stake`
4. **Alternative Scenarios**: Mass slashing or simultaneous validator unstaking during an epoch could also trigger this condition

The severity of impact (complete network halt) combined with realistic exploitation paths justifies the high threat level.

## Recommendation

Add validation in `on_new_epoch()` to prevent empty validator sets:

```move
// After line 1401 in stake.move
validator_set.active_validators = next_epoch_validators;
assert!(
    vector::length(&validator_set.active_validators) > 0, 
    error::invalid_state(ELAST_VALIDATOR)
);
```

Additionally, add validation in `update_required_stake()` to prevent setting `minimum_stake` above the current voting power of at least one validator:

```move
// In staking_config.move, after line 278
let validator_set = stake::get_validator_set();
let max_current_stake = stake::get_max_validator_voting_power(&validator_set);
assert!(
    minimum_stake <= max_current_stake,
    error::invalid_argument(EMINIMUM_STAKE_TOO_HIGH)
);
```

## Proof of Concept

```move
#[test(aptos_framework = @0x1)]
#[expected_failure(abort_code = 0x60006, location = stake)]
public entry fun test_empty_validator_set_prevention(
    aptos_framework: &signer,
) acquires ValidatorSet, StakePool, ValidatorConfig, ... {
    // Setup: Initialize with validators having 100 stake each
    initialize_test_environment(aptos_framework);
    
    // Create validators with minimal stake
    let validator_addresses = create_test_validators(aptos_framework, 100);
    
    // Trigger epoch to activate validators
    stake::on_new_epoch();
    
    // Malicious governance proposal: Set minimum_stake impossibly high
    staking_config::update_required_stake(
        aptos_framework,
        10000000000, // Minimum stake set to 10 billion
        100000000000  // Maximum stake
    );
    
    // This should abort with ELAST_VALIDATOR error (0x60006)
    // Currently it does not - creating empty validator set
    stake::on_new_epoch();
    
    // If we reach here, the network would halt
}
```

### Citations

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L1250-1255)
```text
            // Validate that the validator is already part of the validator set.
            let maybe_active_index = find_validator(&validator_set.active_validators, pool_address);
            assert!(option::is_some(&maybe_active_index), error::invalid_state(ENOT_VALIDATOR));
            let validator_info = vector::swap_remove(
                &mut validator_set.active_validators, option::extract(&mut maybe_active_index));
            assert!(vector::length(&validator_set.active_validators) > 0, error::invalid_state(ELAST_VALIDATOR));
```

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L1372-1401)
```text
        let next_epoch_validators = vector::empty();
        let (minimum_stake, _) = staking_config::get_required_stake(&config);
        let vlen = vector::length(&validator_set.active_validators);
        let total_voting_power = 0;
        let i = 0;
        while ({
            spec {
                invariant spec_validators_are_initialized(next_epoch_validators);
                invariant i <= vlen;
            };
            i < vlen
        }) {
            let old_validator_info = vector::borrow_mut(&mut validator_set.active_validators, i);
            let pool_address = old_validator_info.addr;
            let validator_config = borrow_global<ValidatorConfig>(pool_address);
            let stake_pool = borrow_global<StakePool>(pool_address);
            let new_validator_info = generate_validator_info(pool_address, stake_pool, *validator_config);

            // A validator needs at least the min stake required to join the validator set.
            if (new_validator_info.voting_power >= minimum_stake) {
                spec {
                    assume total_voting_power + new_validator_info.voting_power <= MAX_U128;
                };
                total_voting_power = total_voting_power + (new_validator_info.voting_power as u128);
                vector::push_back(&mut next_epoch_validators, new_validator_info);
            };
            i = i + 1;
        };

        validator_set.active_validators = next_epoch_validators;
```

**File:** aptos-move/framework/aptos-framework/sources/configs/staking_config.move (L274-285)
```text
    public fun update_required_stake(
        aptos_framework: &signer,
        minimum_stake: u64,
        maximum_stake: u64,
    ) acquires StakingConfig {
        system_addresses::assert_aptos_framework(aptos_framework);
        validate_required_stake(minimum_stake, maximum_stake);

        let staking_config = borrow_global_mut<StakingConfig>(@aptos_framework);
        staking_config.minimum_stake = minimum_stake;
        staking_config.maximum_stake = maximum_stake;
    }
```

**File:** aptos-move/framework/aptos-framework/sources/reconfiguration.move (L131-138)
```text
        reconfiguration_state::on_reconfig_start();

        // Call stake to compute the new validator set and distribute rewards and transaction fees.
        stake::on_new_epoch();
        storage_gas::on_reconfig();

        assert!(current_time > config_ref.last_reconfiguration_time, error::invalid_state(EINVALID_BLOCK_TIME));
        config_ref.last_reconfiguration_time = current_time;
```

**File:** network/discovery/src/validator_set.rs (L108-150)
```rust
pub(crate) fn extract_validator_set_updates(
    network_context: NetworkContext,
    node_set: ValidatorSet,
) -> PeerSet {
    let is_validator = network_context.network_id().is_validator_network();

    // Decode addresses while ignoring bad addresses
    node_set
        .into_iter()
        .map(|info| {
            let peer_id = *info.account_address();
            let config = info.into_config();

            let addrs = if is_validator {
                config
                    .validator_network_addresses()
                    .map_err(anyhow::Error::from)
            } else {
                config
                    .fullnode_network_addresses()
                    .map_err(anyhow::Error::from)
            }
            .map_err(|err| {
                inc_by_with_context(&DISCOVERY_COUNTS, &network_context, "read_failure", 1);

                warn!(
                    NetworkSchema::new(&network_context),
                    "OnChainDiscovery: Failed to parse any network address: peer: {}, err: {}",
                    peer_id,
                    err
                )
            })
            .unwrap_or_default();

            let peer_role = if is_validator {
                PeerRole::Validator
            } else {
                PeerRole::ValidatorFullNode
            };
            (peer_id, Peer::from_addrs(peer_role, addrs))
        })
        .collect()
}
```

**File:** types/src/on_chain_config/validator_set.rs (L102-110)
```rust
impl IntoIterator for ValidatorSet {
    type IntoIter = Chain<IntoIter<Self::Item>, IntoIter<Self::Item>>;
    type Item = ValidatorInfo;

    fn into_iter(self) -> Self::IntoIter {
        self.active_validators
            .into_iter()
            .chain(self.pending_inactive)
    }
```

**File:** network/discovery/src/lib.rs (L141-156)
```rust
        while let Some(update) = source_stream.next().await {
            if let Ok(update) = update {
                trace!(
                    NetworkSchema::new(&network_context),
                    "{} Sending update: {:?}",
                    network_context,
                    update
                );
                let request = ConnectivityRequest::UpdateDiscoveredPeers(discovery_source, update);
                if let Err(error) = update_channel.try_send(request) {
                    inc_by_with_context(&DISCOVERY_COUNTS, &network_context, "send_failure", 1);
                    warn!(
                        NetworkSchema::new(&network_context),
                        "{} Failed to send update {:?}", network_context, error
                    );
                }
```

**File:** network/framework/src/connectivity_manager/mod.rs (L484-531)
```rust
    async fn close_stale_connections(&mut self) {
        if let Some(trusted_peers) = self.get_trusted_peers() {
            // Identify stale peer connections
            let stale_peers = self
                .connected
                .iter()
                .filter(|(peer_id, _)| !trusted_peers.contains_key(peer_id))
                .filter_map(|(peer_id, metadata)| {
                    // If we're using server only auth, we need to not evict unknown peers
                    // TODO: We should prevent `Unknown` from discovery sources
                    if !self.mutual_authentication
                        && metadata.origin == ConnectionOrigin::Inbound
                        && (metadata.role == PeerRole::ValidatorFullNode
                            || metadata.role == PeerRole::Unknown)
                    {
                        None
                    } else {
                        Some(*peer_id) // The peer is stale
                    }
                });

            // Close existing connections to stale peers
            for stale_peer in stale_peers {
                info!(
                    NetworkSchema::new(&self.network_context).remote_peer(&stale_peer),
                    "{} Closing stale connection to peer {}",
                    self.network_context,
                    stale_peer.short_str()
                );

                if let Err(disconnect_error) = self
                    .connection_reqs_tx
                    .disconnect_peer(stale_peer, DisconnectReason::StaleConnection)
                    .await
                {
                    info!(
                        NetworkSchema::new(&self.network_context)
                            .remote_peer(&stale_peer),
                        error = %disconnect_error,
                        "{} Failed to close stale connection to peer {}, error: {}",
                        self.network_context,
                        stale_peer.short_str(),
                        disconnect_error
                    );
                }
            }
        }
    }
```

**File:** network/framework/src/connectivity_manager/mod.rs (L884-1000)
```rust
    /// Handles an update for newly discovered peers. This typically
    /// occurs at node startup, and on epoch changes.
    fn handle_update_discovered_peers(
        &mut self,
        src: DiscoverySource,
        new_discovered_peers: PeerSet,
    ) {
        // Log the update event
        info!(
            NetworkSchema::new(&self.network_context),
            "{} Received updated list of discovered peers! Source: {:?}, num peers: {:?}",
            self.network_context,
            src,
            new_discovered_peers.len()
        );

        // Remove peers that no longer have relevant network information
        let mut keys_updated = false;
        let mut peers_to_check_remove = Vec::new();
        for (peer_id, peer) in self.discovered_peers.write().peer_set.iter_mut() {
            let new_peer = new_discovered_peers.get(peer_id);
            let check_remove = if let Some(new_peer) = new_peer {
                if new_peer.keys.is_empty() {
                    keys_updated |= peer.keys.clear_src(src);
                }
                if new_peer.addresses.is_empty() {
                    peer.addrs.clear_src(src);
                }
                new_peer.addresses.is_empty() && new_peer.keys.is_empty()
            } else {
                keys_updated |= peer.keys.clear_src(src);
                peer.addrs.clear_src(src);
                true
            };
            if check_remove {
                peers_to_check_remove.push(*peer_id);
            }
        }

        // Remove peers that no longer have state
        for peer_id in peers_to_check_remove {
            self.discovered_peers.write().remove_peer_if_empty(&peer_id);
        }

        // Make updates to the peers accordingly
        for (peer_id, discovered_peer) in new_discovered_peers {
            // Don't include ourselves, because we don't need to dial ourselves
            if peer_id == self.network_context.peer_id() {
                continue;
            }

            // Create the new `DiscoveredPeer`, role is set when a `Peer` is first discovered
            let mut discovered_peers = self.discovered_peers.write();
            let peer = discovered_peers
                .peer_set
                .entry(peer_id)
                .or_insert_with(|| DiscoveredPeer::new(discovered_peer.role));

            // Update the peer's pubkeys
            let mut peer_updated = false;
            if peer.keys.update(src, discovered_peer.keys) {
                info!(
                    NetworkSchema::new(&self.network_context)
                        .remote_peer(&peer_id)
                        .discovery_source(&src),
                    "{} pubkey sets updated for peer: {}, pubkeys: {}",
                    self.network_context,
                    peer_id.short_str(),
                    peer.keys
                );
                keys_updated = true;
                peer_updated = true;
            }

            // Update the peer's addresses
            if peer.addrs.update(src, discovered_peer.addresses) {
                info!(
                    NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                    network_addresses = &peer.addrs,
                    "{} addresses updated for peer: {}, update src: {:?}, addrs: {}",
                    self.network_context,
                    peer_id.short_str(),
                    src,
                    &peer.addrs,
                );
                peer_updated = true;
            }

            // If we're currently trying to dial this peer, we reset their
            // dial state. As a result, we will begin our next dial attempt
            // from the first address (which might have changed) and from a
            // fresh backoff (since the current backoff delay might be maxed
            // out if we can't reach any of their previous addresses).
            if peer_updated {
                if let Some(dial_state) = self.dial_states.get_mut(&peer_id) {
                    *dial_state = DialState::new(self.backoff_strategy.clone());
                }
            }
        }

        // update eligible peers accordingly
        if keys_updated {
            // For each peer, union all of the pubkeys from each discovery source
            // to generate the new eligible peers set.
            let new_eligible = self.discovered_peers.read().get_eligible_peers();

            // Swap in the new eligible peers set
            if let Err(error) = self
                .peers_and_metadata
                .set_trusted_peers(&self.network_context.network_id(), new_eligible)
            {
                error!(
                    NetworkSchema::new(&self.network_context),
                    error = %error,
                    "Failed to update trusted peers set"
                );
            }
```

**File:** network/framework/src/noise/handshake.rs (L366-383)
```rust
        // if mutual auth mode, verify the remote pubkey is in our set of trusted peers
        let network_id = self.network_context.network_id();
        let peer_role = match &self.auth_mode {
            HandshakeAuthMode::Mutual {
                peers_and_metadata, ..
            } => {
                let trusted_peers = peers_and_metadata.get_trusted_peers(&network_id)?;
                let trusted_peer = trusted_peers.get(&remote_peer_id).cloned();
                match trusted_peer {
                    Some(peer) => {
                        Self::authenticate_inbound(remote_peer_short, &peer, &remote_public_key)
                    },
                    None => Err(NoiseHandshakeError::UnauthenticatedClient(
                        remote_peer_short,
                        remote_peer_id,
                    )),
                }
            },
```
