# Audit Report

## Title
Missing Validation for Complete Validator Set Exclusion in OptQuorumStore Payload Pulling

## Summary
The OptQuorumStore (OptQS) payload pulling mechanism lacks validation to detect when `exclude_authors` contains all active validators. This results in zero eligible authors for optimistic batch pulling, causing silent consensus performance degradation without detection or recovery mechanisms.

## Finding Description

The vulnerability exists in the OptQS payload parameter generation and batch pulling logic. When consensus experiences repeated `PayloadUnavailable` timeouts, the `ExponentialWindowFailureTracker` accumulates failing validators into an `exclude_authors` set without validating whether this set encompasses the entire validator set.

**Vulnerable Flow:**

1. **Failure Tracking** - When proposals timeout due to payload unavailability, `ExponentialWindowFailureTracker.get_exclude_authors()` builds an exclusion set from recent failures: [1](#0-0) 

2. **No Validation** - The system creates `OptQSPayloadPullParams` with the `exclude_authors` set without checking if it excludes all validators: [2](#0-1) 

3. **Silent Filtering** - During batch pulling, all authors are filtered out when iterating through `author_to_batches`: [3](#0-2) 

4. **Empty Result** - With an empty iterator, no optimistic batches are pulled, and the function returns an empty result set: [4](#0-3) 

**Attack Scenario:**
During network instability, coordinated outages, or systemic payload unavailability affecting all validators:
- Each round experiences `PayloadUnavailable` timeouts
- All validators accumulate in `exclude_authors` over consecutive failures
- OptQS attempts to pull optimistic batches but filters out all eligible authors
- System falls back to proof-only mode with zero optimistic batches
- No error detection, logging of the severity, or automatic recovery

## Impact Explanation

**HIGH Severity** - This qualifies as "Validator node slowdowns" and "Significant protocol violations" under the Aptos bug bounty program:

1. **Consensus Performance Degradation**: OptQuorumStore is designed to improve throughput by allowing optimistic batch inclusion. Complete exclusion eliminates this optimization, reducing network throughput.

2. **Silent Failure Mode**: The system provides no clear indication that ALL validators are excluded, making diagnosis difficult for operators.

3. **Recovery Dependency**: Recovery requires consecutive successful proposals using only proofs, which may be slow or blocked if proof availability is also compromised.

4. **Cascading Effects**: Reduced throughput can cause transaction backlog, increased latency, and user-facing service degradation.

## Likelihood Explanation

**Medium-to-High Likelihood:**

- **Realistic Trigger**: Network partitions, coordinated validator restarts, or infrastructure issues can cause widespread payload unavailability
- **No Manual Intervention Required**: The failure accumulates automatically through normal consensus operation
- **Exponential Backoff Amplification**: The exponential window growth (doubling on each failure) accelerates the accumulation of excluded validators
- **Limited Recovery Path**: Recovery requires sustained success without optimistic batches, which may be difficult under adverse conditions

The vulnerability is particularly likely during:
- Network infrastructure upgrades affecting multiple validators
- Cloud provider outages impacting validator hosting
- Consensus version upgrades with temporary incompatibilities
- Sustained high load causing widespread timeout

## Recommendation

Add validation in `OptQSPullParamsProvider::get_params()` to detect when `exclude_authors` would exclude all or nearly all validators:

```rust
impl TOptQSPullParamsProvider for OptQSPullParamsProvider {
    fn get_params(&self) -> Option<OptQSPayloadPullParams> {
        if !self.enable_opt_qs {
            return None;
        }

        let tracker = self.failure_tracker.lock();

        counters::OPTQS_LAST_CONSECUTIVE_SUCCESS_COUNT
            .observe(tracker.last_consecutive_success_count as f64);
        if tracker.last_consecutive_success_count < tracker.window {
            warn!(
                "Skipping OptQS: (last_consecutive_successes) {} < {} (window)",
                tracker.last_consecutive_success_count, tracker.window
            );
            return None;
        }

        let exclude_authors = tracker.get_exclude_authors();
        
        // NEW VALIDATION: Check if too many validators are excluded
        let total_validators = tracker.ordered_authors.len();
        let excluded_count = exclude_authors.len();
        let exclusion_ratio = excluded_count as f64 / total_validators as f64;
        
        // If more than 2/3 of validators are excluded, disable OptQS
        // This prevents attempting to pull from an insufficient validator set
        if exclusion_ratio > 0.67 {
            warn!(
                "Skipping OptQS: excluded {}/{} validators ({:.1}% exclusion ratio exceeds 67% threshold)",
                excluded_count, total_validators, exclusion_ratio * 100.0
            );
            counters::OPTQS_EXCESSIVE_EXCLUSION_COUNT.inc();
            return None;
        }
        
        if !exclude_authors.is_empty() {
            let exclude_authors_str: Vec<_> =
                exclude_authors.iter().map(|a| a.short_str()).collect();
            for author in &exclude_authors_str {
                counters::OPTQS_EXCLUDE_AUTHORS_COUNT
                    .with_label_values(&[author.as_str()])
                    .inc();
            }
            warn!("OptQS exclude authors: {:?}", exclude_authors_str);
        }
        
        Some(OptQSPayloadPullParams {
            exclude_authors,
            minimum_batch_age_usecs: self.minimum_batch_age_usecs,
        })
    }
}
```

**Additional Mitigations:**
1. Add alerting when exclusion ratio exceeds thresholds (50%, 60%, 67%)
2. Implement gradual exclusion decay to allow faster recovery
3. Add metrics tracking the exclusion ratio over time
4. Consider per-validator exclusion timeouts rather than window-based tracking

## Proof of Concept

```rust
#[cfg(test)]
mod test_exclude_all_validators {
    use super::*;
    use aptos_bitvec::BitVec;
    use aptos_consensus_types::round_timeout::RoundTimeoutReason;
    use aptos_types::validator_verifier::random_validator_verifier;

    #[test]
    fn test_all_validators_excluded_no_detection() {
        // Setup: Create validator set with 4 validators
        let (_signers, verifier) = random_validator_verifier(4, None, false);
        let ordered_authors = verifier.get_ordered_account_addresses();
        let mut tracker = ExponentialWindowFailureTracker::new(100, ordered_authors.clone());

        // Simulate repeated PayloadUnavailable failures affecting all validators
        for _ in 0..10 {
            let mut missing_authors = BitVec::with_num_bits(4);
            // Mark all validators as missing
            for i in 0..4 {
                missing_authors.set(i);
            }
            tracker.push(NewRoundReason::Timeout(
                RoundTimeoutReason::PayloadUnavailable { missing_authors },
            ));
        }

        // Get excluded authors
        let exclude_authors = tracker.get_exclude_authors();
        
        // VULNERABILITY: All validators are excluded but no error is raised
        assert_eq!(exclude_authors.len(), 4);
        assert_eq!(exclude_authors.len(), ordered_authors.len());
        
        // This would result in zero eligible authors for OptQS batch pulling
        // The system would silently fall back to proof-only mode
        println!("VULNERABILITY: All {} validators excluded, no detection!", exclude_authors.len());
    }

    #[test]
    fn test_proposed_fix_detects_excessive_exclusion() {
        let (_signers, verifier) = random_validator_verifier(4, None, false);
        let ordered_authors = verifier.get_ordered_account_addresses();
        let tracker = Arc::new(Mutex::new(
            ExponentialWindowFailureTracker::new(100, ordered_authors.clone())
        ));
        
        let provider = OptQSPullParamsProvider::new(true, 1000, tracker.clone());

        // Exclude all validators
        for _ in 0..10 {
            let mut missing_authors = BitVec::with_num_bits(4);
            for i in 0..4 {
                missing_authors.set(i);
            }
            tracker.lock().push(NewRoundReason::Timeout(
                RoundTimeoutReason::PayloadUnavailable { missing_authors },
            ));
        }

        // With proposed fix: get_params() should return None
        // when exclusion ratio exceeds threshold
        let params = provider.get_params();
        
        // FIXED: System detects excessive exclusion and disables OptQS
        assert!(params.is_none(), "Should return None when all validators excluded");
    }
}
```

**Expected Behavior:**
- Current: Returns `OptQSPayloadPullParams` with all validators in `exclude_authors`, silently causing zero optimistic batches
- Fixed: Returns `None` to disable OptQS entirely when exclusion ratio is excessive, with clear warning logs

## Notes

This vulnerability affects consensus **performance and liveness** rather than **safety**. The system maintains safety by falling back to proof-based consensus, but this degrades throughput significantly. The lack of detection makes operational diagnosis difficult and prolongs recovery time during incidents.

The 67% threshold in the recommendation is chosen to maintain Byzantine fault tolerance assumptions (< 1/3 Byzantine validators) while providing headroom for legitimate exclusions.

### Citations

**File:** consensus/src/liveness/proposal_status_tracker.rs (L80-98)
```rust
    fn get_exclude_authors(&self) -> HashSet<Author> {
        let mut exclude_authors = HashSet::new();

        let limit = self.window;
        for round_reason in self.past_round_statuses.iter().rev().take(limit) {
            if let NewRoundReason::Timeout(RoundTimeoutReason::PayloadUnavailable {
                missing_authors,
            }) = round_reason
            {
                for author_idx in missing_authors.iter_ones() {
                    if let Some(author) = self.ordered_authors.get(author_idx) {
                        exclude_authors.insert(*author);
                    }
                }
            }
        }

        exclude_authors
    }
```

**File:** consensus/src/liveness/proposal_status_tracker.rs (L145-160)
```rust
        let exclude_authors = tracker.get_exclude_authors();
        if !exclude_authors.is_empty() {
            let exclude_authors_str: Vec<_> =
                exclude_authors.iter().map(|a| a.short_str()).collect();
            for author in &exclude_authors_str {
                counters::OPTQS_EXCLUDE_AUTHORS_COUNT
                    .with_label_values(&[author.as_str()])
                    .inc();
            }
            warn!("OptQS exclude authors: {:?}", exclude_authors_str);
        }
        Some(OptQSPayloadPullParams {
            exclude_authors,
            minimum_batch_age_usecs: self.minimum_batch_age_usecs,
        })
    }
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L596-600)
```rust
        for (_, batches) in self
            .author_to_batches
            .iter()
            .filter(|(author, _)| !exclude_authors.contains(author))
        {
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L626-689)
```rust
        while !iters.is_empty() {
            iters.shuffle(&mut thread_rng());
            iters.retain_mut(|iter| {
                if full {
                    return false;
                }

                if let Some((batch, item)) = iter.next() {
                    if excluded_batches.contains(batch) {
                        excluded_txns += batch.num_txns();
                    } else {
                        // Calculate the number of unique transactions if this batch is included in the result
                        let unique_txns = if let Some(ref txn_summaries) = item.txn_summaries {
                            cur_unique_txns
                                + txn_summaries
                                    .iter()
                                    .filter(|txn_summary| {
                                        !filtered_txns.contains(txn_summary)
                                            && block_timestamp.as_secs()
                                                < txn_summary.expiration_timestamp_secs
                                    })
                                    .count() as u64
                        } else {
                            cur_unique_txns + batch.num_txns()
                        };
                        if cur_all_txns + batch.size() > max_txns
                            || unique_txns > max_txns_after_filtering
                        {
                            // Exceeded the limit for requested bytes or number of transactions.
                            full = true;
                            return false;
                        }
                        cur_all_txns += batch.size();
                        // Add this batch to filtered_txns and calculate the number of
                        // unique transactions added in the result so far.
                        cur_unique_txns +=
                            item.txn_summaries
                                .as_ref()
                                .map_or(batch.num_txns(), |summaries| {
                                    summaries
                                        .iter()
                                        .filter(|summary| {
                                            filtered_txns.insert(**summary)
                                                && block_timestamp.as_secs()
                                                    < summary.expiration_timestamp_secs
                                        })
                                        .count() as u64
                                });
                        assert!(item.proof.is_none() == batches_without_proofs);
                        result.push(item);
                        if cur_all_txns == max_txns
                            || cur_unique_txns == max_txns_after_filtering
                            || cur_unique_txns >= soft_max_txns_after_filtering
                        {
                            full = true;
                            return false;
                        }
                    }
                    true
                } else {
                    false
                }
            })
        }
```
