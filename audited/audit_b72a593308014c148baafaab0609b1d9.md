# Audit Report

## Title
Consensus Event Loop Blocking via Unbounded Historical Epoch Retrieval Requests

## Summary
The `process_epoch_retrieval()` function in the consensus epoch manager performs synchronous database queries for arbitrary historical epoch ranges without validating how old the requested epochs can be. An attacker can repeatedly request epoch proofs from very old epochs (e.g., epoch 0 from genesis), causing synchronous disk I/O operations that block the main consensus event loop and delay processing of critical consensus messages (proposals, votes, timeouts), leading to consensus liveness degradation.

## Finding Description
The vulnerability exists in the epoch retrieval handling logic with three critical components:

**1. Missing start_epoch validation:** [1](#0-0) 

The validation only checks that `end_epoch <= self.epoch()` but imposes no lower bound on `start_epoch`, allowing requests for epochs from genesis (epoch 0) to the current epoch.

**2. Synchronous database query in event loop:** [2](#0-1) 

The `get_epoch_ending_ledger_infos()` call is synchronous and blocks the epoch manager's event loop. The event loop processes all consensus messages including proposals, votes, and sync info: [3](#0-2) 

While processing an epoch retrieval request, the epoch manager cannot process any other consensus messages, causing a direct consensus liveness impact.

**3. Epoch ending ledger infos are never pruned:** [4](#0-3) 

The `LedgerMetadataPruner` only prunes `VersionDataSchema` (state storage usage metrics), not the `LedgerInfoSchema` which stores epoch ending ledger infos. This means all historical epoch data from genesis remains accessible: [5](#0-4) 

**Attack Scenario:**
1. Attacker sends repeated `EpochRetrievalRequest` messages with `start_epoch=0` (or other very old epochs)
2. Each request triggers a database seek to old epochs not in cache, requiring disk I/O
3. The synchronous database query blocks the consensus event loop
4. While blocked, the epoch manager cannot process proposals, votes, or other consensus messages
5. The consensus message queue (size 10) fills up with delayed messages
6. Consensus makes slow or no progress until epoch retrieval completes

While pagination limits each response to 100 epochs: [6](#0-5) 

An attacker can send multiple sequential requests (epochs 0-100, 100-200, etc.) to sustain the attack.

## Impact Explanation
This vulnerability qualifies as **High Severity** per the Aptos bug bounty criteria: "Validator node slowdowns."

**Impact:**
- **Consensus Liveness Degradation**: Blocks processing of proposals and votes during database queries
- **Resource Exhaustion**: Forces repeated disk I/O for historical data not in cache
- **Denial of Service**: Multiple validators can be targeted simultaneously, affecting network-wide consensus
- **Breaks Resource Limits Invariant**: Operations should respect computational limits and not block critical consensus paths

The vulnerability does not cause consensus safety violations (double-spending, chain splits) but directly impacts liveness, which is critical for blockchain operation.

## Likelihood Explanation
**Likelihood: High**

- **Low Attack Complexity**: Any network peer can send `EpochRetrievalRequest` messages without authentication beyond network connectivity
- **No Special Access Required**: Does not require validator privileges or stake
- **Trivial to Execute**: Simple message construction and repeated sending
- **Wide Attack Surface**: All validators are vulnerable simultaneously
- **Difficult to Detect Initially**: Appears as legitimate epoch synchronization traffic

The small queue size (10) provides minimal protection as epoch retrieval requests compete with critical consensus messages in the same queue: [7](#0-6) 

## Recommendation
Implement multiple layers of protection:

**1. Add minimum epoch bound validation:**
```rust
fn process_epoch_retrieval(
    &mut self,
    request: EpochRetrievalRequest,
    peer_id: AccountAddress,
) -> anyhow::Result<()> {
    // Add validation for minimum epoch based on prune window
    let min_epoch = self.epoch().saturating_sub(MAX_EPOCH_HISTORY_WINDOW);
    ensure!(
        request.start_epoch >= min_epoch,
        "[EpochManager] Requested start_epoch {} is too old (minimum: {})",
        request.start_epoch,
        min_epoch
    );
    
    ensure!(
        request.end_epoch <= self.epoch(),
        "[EpochManager] Received EpochRetrievalRequest beyond what we have locally"
    );
    
    // Existing implementation...
}
```

**2. Make database query asynchronous with timeout:**
Move the database query to a background task with timeout to prevent blocking the event loop.

**3. Add per-peer rate limiting:**
Implement rate limiting specifically for `EpochRetrievalRequest` messages per peer to prevent flooding.

**4. Consider separate queue for epoch retrieval:**
Process epoch retrieval requests in a separate queue/task to isolate them from critical consensus messages.

## Proof of Concept
```rust
// Reproduction steps:

// 1. Create a malicious peer that connects to a validator
// 2. Send repeated EpochRetrievalRequest messages:

use aptos_consensus_types::epoch_retrieval::EpochRetrievalRequest;
use aptos_network::protocols::network::Event;

async fn exploit_epoch_retrieval(target_validator: AccountAddress) {
    // Request epochs from genesis to current
    for start in (0..current_epoch).step_by(100) {
        let request = EpochRetrievalRequest {
            start_epoch: start,
            end_epoch: start + 100,
        };
        
        let msg = ConsensusMsg::EpochRetrievalRequest(Box::new(request));
        
        // Send to validator - this will block their consensus event loop
        // during database query processing
        network_client.send_to(target_validator, msg).await;
        
        // Send multiple in rapid succession to maintain pressure
        tokio::time::sleep(Duration::from_millis(10)).await;
    }
}

// 3. Observe on the target validator:
// - Increased message processing latency
// - Delayed proposal/vote processing
// - Consensus round timeouts
// - Metrics showing long epoch_manager_process_consensus_messages duration
```

The vulnerability can be confirmed by monitoring the `epoch_manager_process_consensus_messages` metric and observing increased latency when epoch retrieval requests for old epochs are processed.

## Notes
This vulnerability represents a design flaw where synchronous database operations are performed in the critical consensus message processing path. The lack of bounds checking on historical data access combined with synchronous I/O creates an exploitable DoS vector that degrades consensus liveness without requiring Byzantine validator behavior or stake majority attacks.

### Citations

**File:** consensus/src/epoch_manager.rs (L451-476)
```rust
    fn process_epoch_retrieval(
        &mut self,
        request: EpochRetrievalRequest,
        peer_id: AccountAddress,
    ) -> anyhow::Result<()> {
        debug!(
            LogSchema::new(LogEvent::ReceiveEpochRetrieval)
                .remote_peer(peer_id)
                .epoch(self.epoch()),
            "[EpochManager] receive {}", request,
        );
        let proof = self
            .storage
            .aptos_db()
            .get_epoch_ending_ledger_infos(request.start_epoch, request.end_epoch)
            .map_err(DbError::from)
            .context("[EpochManager] Failed to get epoch proof")?;
        let msg = ConsensusMsg::EpochChangeProof(Box::new(proof));
        if let Err(err) = self.network_sender.send_to(peer_id, msg) {
            warn!(
                "[EpochManager] Failed to send epoch proof to {}, with error: {:?}",
                peer_id, err,
            );
        }
        Ok(())
    }
```

**File:** consensus/src/epoch_manager.rs (L1677-1686)
```rust
            ConsensusMsg::EpochRetrievalRequest(request) => {
                ensure!(
                    request.end_epoch <= self.epoch(),
                    "[EpochManager] Received EpochRetrievalRequest beyond what we have locally"
                );
                monitor!(
                    "process_epoch_retrieval",
                    self.process_epoch_retrieval(*request, peer_id)
                )?;
            },
```

**File:** consensus/src/epoch_manager.rs (L1922-1960)
```rust
    pub async fn start(
        mut self,
        mut round_timeout_sender_rx: aptos_channels::Receiver<Round>,
        mut network_receivers: NetworkReceivers,
    ) {
        // initial start of the processor
        self.await_reconfig_notification().await;
        loop {
            tokio::select! {
                (peer, msg) = network_receivers.consensus_messages.select_next_some() => {
                    monitor!("epoch_manager_process_consensus_messages",
                    if let Err(e) = self.process_message(peer, msg).await {
                        error!(epoch = self.epoch(), error = ?e, kind = error_kind(&e));
                    });
                },
                (peer, msg) = network_receivers.quorum_store_messages.select_next_some() => {
                    monitor!("epoch_manager_process_quorum_store_messages",
                    if let Err(e) = self.process_message(peer, msg).await {
                        error!(epoch = self.epoch(), error = ?e, kind = error_kind(&e));
                    });
                },
                (peer, request) = network_receivers.rpc_rx.select_next_some() => {
                    monitor!("epoch_manager_process_rpc",
                    if let Err(e) = self.process_rpc_request(peer, request) {
                        error!(epoch = self.epoch(), error = ?e, kind = error_kind(&e));
                    });
                },
                round = round_timeout_sender_rx.select_next_some() => {
                    monitor!("epoch_manager_process_round_timeout",
                    self.process_local_timeout(round));
                },
            }
            // Continually capture the time of consensus process to ensure that clock skew between
            // validators is reasonable and to find any unusual (possibly byzantine) clock behavior.
            counters::OP_COUNTERS
                .gauge("time_since_epoch_ms")
                .set(duration_since_epoch().as_millis() as i64);
        }
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_metadata_pruner.rs (L42-56)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();
        for version in current_progress..target_version {
            batch.delete::<VersionDataSchema>(&version)?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::LedgerPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
        self.ledger_metadata_db.write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L117-194)
```rust
impl LedgerPruner {
    pub fn new(
        ledger_db: Arc<LedgerDb>,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Result<Self> {
        info!(name = LEDGER_PRUNER_NAME, "Initializing...");

        let ledger_metadata_pruner = Box::new(
            LedgerMetadataPruner::new(ledger_db.metadata_db_arc())
                .expect("Failed to initialize ledger_metadata_pruner."),
        );

        let metadata_progress = ledger_metadata_pruner.progress()?;

        info!(
            metadata_progress = metadata_progress,
            "Created ledger metadata pruner, start catching up all sub pruners."
        );

        let transaction_store = Arc::new(TransactionStore::new(Arc::clone(&ledger_db)));

        let event_store_pruner = Box::new(EventStorePruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
            internal_indexer_db.clone(),
        )?);
        let persisted_auxiliary_info_pruner = Box::new(PersistedAuxiliaryInfoPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);
        let transaction_accumulator_pruner = Box::new(TransactionAccumulatorPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);

        let transaction_auxiliary_data_pruner = Box::new(TransactionAuxiliaryDataPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);

        let transaction_info_pruner = Box::new(TransactionInfoPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);
        let transaction_pruner = Box::new(TransactionPruner::new(
            Arc::clone(&transaction_store),
            Arc::clone(&ledger_db),
            metadata_progress,
            internal_indexer_db,
        )?);
        let write_set_pruner = Box::new(WriteSetPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);

        let pruner = LedgerPruner {
            target_version: AtomicVersion::new(metadata_progress),
            progress: AtomicVersion::new(metadata_progress),
            ledger_metadata_pruner,
            sub_pruners: vec![
                event_store_pruner,
                persisted_auxiliary_info_pruner,
                transaction_accumulator_pruner,
                transaction_auxiliary_data_pruner,
                transaction_info_pruner,
                transaction_pruner,
                write_set_pruner,
            ],
        };

        info!(
            name = pruner.name(),
            progress = metadata_progress,
            "Initialized."
        );

        Ok(pruner)
    }
```

**File:** storage/aptosdb/src/common.rs (L9-9)
```rust
pub(crate) const MAX_NUM_EPOCH_ENDING_LEDGER_INFO: usize = 100;
```

**File:** consensus/src/network.rs (L757-761)
```rust
        let (consensus_messages_tx, consensus_messages) = aptos_channel::new(
            QueueStyle::FIFO,
            10,
            Some(&counters::CONSENSUS_CHANNEL_MSGS),
        );
```
