# Audit Report

## Title
Proof Version Manipulation Enables Cache Bypass and DoS Attack in Storage Service

## Summary
A Byzantine peer can bypass the LRU response cache by requesting the same transaction output data with different `proof_version` values, forcing the server to repeatedly regenerate historical proofs. Each cache miss triggers logarithmic disk I/O operations to fetch Merkle accumulator sibling hashes, enabling a resource exhaustion attack that can cause disk thrashing and degrade service for all peers.

## Finding Description

The storage service validates `TransactionOutputsWithProofRequest` requests to ensure data availability, but the validation logic contains a critical oversight regarding the `proof_version` parameter. [1](#0-0) 

The `can_service_transaction_outputs_with_proof` method only validates that `proof_version` does not exceed the highest synced version via `can_create_proof`: [2](#0-1) 

**Critical Gap**: The validation does NOT check whether `proof_version` is unreasonably old, within the requested data range, or has been excessively varied for cache bypass purposes.

The request handler uses an LRU cache keyed by the complete request object: [3](#0-2) 

Since the cache key includes `proof_version`, different proof versions for identical data ranges create distinct cache entries. An attacker can exploit this by:

1. Requesting transaction outputs at versions [1000, 1100] with `proof_version = 999`
2. Requesting the same range with `proof_version = 500` (cache miss)
3. Requesting the same range with `proof_version = 100` (cache miss)
4. Continuing with any value in `[transaction_outputs.lowest, highest_synced_version]`

Each cache miss triggers proof generation: [4](#0-3) 

The proof generation requires reading Merkle accumulator sibling hashes from disk: [5](#0-4) [6](#0-5) 

For each unique `proof_version`, the accumulator is reconstructed with `num_leaves = proof_version + 1`, determining a different tree shape and requiring different sibling positions to be read from storage via the `HashReader` interface. [7](#0-6) 

## Impact Explanation

This vulnerability qualifies as **Medium severity** under the Aptos Bug Bounty criteria:

- **Validator node slowdowns**: The excessive disk I/O caused by repeated proof regeneration degrades performance for all connected peers, as storage operations compete for disk bandwidth
- **State inconsistencies requiring intervention**: The cache pollution with numerous variations of the same data can cause the LRU cache to evict legitimate entries, forcing more disk operations and creating cascading performance degradation
- **Resource exhaustion without fund loss**: While this doesn't directly compromise funds or consensus, it enables a sustained DoS attack that could require operator intervention to block malicious peers

The attack requires:
- No privileged access (any network peer can send requests)
- Minimal computational resources (just varying a u64 parameter)
- Can be amplified by multiple coordinated attackers
- Each proof generation requires ~20-30 disk reads (logarithmic in tree size)
- With millions of possible `proof_version` values, the attack can be sustained indefinitely

## Likelihood Explanation

**Very High Likelihood**: 

This vulnerability is trivial to exploit:
- The attacker only needs to vary the `proof_version` field in `GetTransactionOutputsWithProof` requests
- All requests pass validation as long as `proof_version <= highest_synced_version`
- No rate limiting exists specifically for proof_version variations
- The moderator's invalid request counter only increments for requests that fail validation
- Each cache miss is a legitimate operation from the system's perspective

The attack can be automated with a simple script that:
1. Identifies the current highest synced version
2. Generates requests for the same data range with varying proof_versions
3. Sends requests at a rate that maximizes I/O load without triggering network-level protections

Multiple attackers can coordinate to amplify the effect, overwhelming the storage subsystem and causing disk thrashing that affects consensus operations, state sync, and API queries.

## Recommendation

Implement multi-layered protection against proof version manipulation:

**1. Add validation to restrict proof_version to reasonable values:**

```rust
fn can_service_transaction_outputs_with_proof(
    &self,
    start_version: u64,
    end_version: u64,
    proof_version: u64,
) -> bool {
    let desired_range = match CompleteDataRange::new(start_version, end_version) {
        Ok(desired_range) => desired_range,
        Err(_) => return false,
    };

    // NEW: Reject proof_version that is too far from the requested range
    // Allow proof_version to be at most the end_version (typical case) or 
    // slightly newer to accommodate recent blocks
    if proof_version < end_version {
        return false;
    }
    
    // NEW: Reject proof_version that is unreasonably newer than end_version
    // This prevents cache bypass attacks using unnecessarily high proof versions
    let max_proof_version_gap = 1000; // Configure based on expected block production
    if proof_version > end_version + max_proof_version_gap {
        return false;
    }

    let can_service_outputs = self.can_service_transaction_outputs(&desired_range);
    let can_create_proof = self.can_create_proof(proof_version);
    can_service_outputs && can_create_proof
}
```

**2. Normalize proof_version in the cache key to reduce variations:**

In the handler, normalize the proof_version to the end_version for caching purposes when they're within a reasonable range, as proofs at the end_version or slightly newer are functionally equivalent for the client.

**3. Add monitoring and rate limiting:**

Track per-peer metrics for proof_version variation patterns and apply stricter rate limiting when unusual patterns are detected (e.g., same data range requested with >10 different proof_versions within a short time window).

## Proof of Concept

```rust
// Simulated attack demonstrating cache bypass via proof_version manipulation
// This would be run against a storage service node

use aptos_storage_service_types::{
    requests::{DataRequest, StorageServiceRequest, TransactionOutputsWithProofRequest},
};

fn proof_version_cache_bypass_attack() {
    // Target: Request the same transaction output range repeatedly
    let start_version = 1000u64;
    let end_version = 1100u64;
    
    // Attack: Vary proof_version to bypass cache
    // Assume current highest synced version is 1,000,000
    let highest_synced_version = 1_000_000u64;
    
    let mut requests = Vec::new();
    
    // Generate 100 requests with different proof_versions
    // Each will be a cache miss despite requesting identical data
    for i in 0..100 {
        let proof_version = end_version + (i * 1000); // Use different historical proof points
        
        if proof_version <= highest_synced_version {
            let request = StorageServiceRequest::new(
                DataRequest::GetTransactionOutputsWithProof(
                    TransactionOutputsWithProofRequest {
                        proof_version,
                        start_version,
                        end_version,
                    }
                ),
                false,
            );
            requests.push(request);
        }
    }
    
    // Each request in this vector will:
    // 1. Pass validation (proof_version <= highest_synced_version)
    // 2. Result in a cache miss (unique proof_version)
    // 3. Trigger disk I/O to generate the proof at that historical version
    // 4. Consume storage service resources
    
    println!("Generated {} cache-bypassing requests", requests.len());
    println!("Each request forces proof regeneration with ~20-30 disk reads");
    println!("Total forced disk operations: ~{}", requests.len() * 25);
    
    // In a real attack, these would be sent to the storage service
    // in rapid succession to maximize I/O load
}
```

**Attack Execution:**
1. Query the storage server summary to obtain `highest_synced_version`
2. Select a target data range (e.g., recent transaction outputs)
3. Generate requests with `proof_version` values: `[end_version, end_version + 1000, end_version + 2000, ..., highest_synced_version]`
4. Send requests in parallel to maximize disk I/O contention
5. Monitor storage service metrics to observe increased latency and cache thrashing
6. Repeat continuously to sustain the DoS attack

**Expected Impact:**
- LRU cache fills with variations of the same data
- Disk I/O increases linearly with number of unique proof_versions
- Legitimate requests experience increased latency
- Storage service may require restart or peer blacklisting to recover

## Notes

The vulnerability exists because the system treats `proof_version` as a flexible parameter for client needs (e.g., proving data consistency at different points in time) without recognizing that this flexibility enables cache bypass attacks. The fix must balance legitimate use cases (clients may need proofs at specific versions for consistency checks) against abuse potential (attackers varying proof_version solely to bypass caching).

Additionally, the moderator's protection mechanism only counts requests that fail validation, so all these attacks pass through as legitimate requests from the moderator's perspective, making peer-level rate limiting insufficient without proof_version-aware detection.

### Citations

**File:** state-sync/storage-service/types/src/responses.rs (L811-816)
```rust
    fn can_create_proof(&self, proof_version: u64) -> bool {
        self.synced_ledger_info
            .as_ref()
            .map(|li| li.ledger_info().version() >= proof_version)
            .unwrap_or(false)
    }
```

**File:** state-sync/storage-service/types/src/responses.rs (L833-847)
```rust
    fn can_service_transaction_outputs_with_proof(
        &self,
        start_version: u64,
        end_version: u64,
        proof_version: u64,
    ) -> bool {
        let desired_range = match CompleteDataRange::new(start_version, end_version) {
            Ok(desired_range) => desired_range,
            Err(_) => return false,
        };

        let can_service_outputs = self.can_service_transaction_outputs(&desired_range);
        let can_create_proof = self.can_create_proof(proof_version);
        can_service_outputs && can_create_proof
    }
```

**File:** state-sync/storage-service/server/src/handler.rs (L384-404)
```rust
    fn process_cachable_request(
        &self,
        peer_network_id: &PeerNetworkId,
        request: &StorageServiceRequest,
    ) -> aptos_storage_service_types::Result<StorageServiceResponse, Error> {
        // Increment the LRU cache probe counter
        increment_counter(
            &metrics::LRU_CACHE_EVENT,
            peer_network_id.network_id(),
            LRU_CACHE_PROBE.into(),
        );

        // Check if the response is already in the cache
        if let Some(response) = self.lru_response_cache.get(request) {
            increment_counter(
                &metrics::LRU_CACHE_EVENT,
                peer_network_id.network_id(),
                LRU_CACHE_HIT.into(),
            );
            return Ok(response.clone());
        }
```

**File:** state-sync/storage-service/server/src/storage.rs (L698-708)
```rust
        // Create the transaction output list with proof
        let num_fetched_outputs = transactions_and_outputs.len();
        let accumulator_range_proof = if num_fetched_outputs == 0 {
            AccumulatorRangeProof::new_empty() // Return an empty proof if no outputs were fetched
        } else {
            self.storage.get_transaction_accumulator_range_proof(
                start_version,
                num_fetched_outputs as u64,
                proof_version,
            )?
        };
```

**File:** storage/accumulator/src/lib.rs (L389-401)
```rust
    /// Implementation for public interface `MerkleAccumulator::get_range_proof`.
    fn get_range_proof(
        &self,
        first_leaf_index: Option<u64>,
        num_leaves: LeafCount,
    ) -> Result<AccumulatorRangeProof<H>> {
        let (left_siblings, right_siblings) =
            self.get_range_proof_positions(first_leaf_index, num_leaves)?;
        Ok(AccumulatorRangeProof::new(
            self.get_hashes(&left_siblings)?,
            self.get_hashes(&right_siblings)?,
        ))
    }
```

**File:** storage/accumulator/src/lib.rs (L446-457)
```rust
    fn get_sibling_positions(
        &self,
        leaf_index: u64,
        filter: impl Fn(Position) -> bool,
    ) -> Vec<Position> {
        let root_pos = Position::root_from_leaf_count(self.num_leaves);
        Position::from_leaf_index(leaf_index)
            .iter_ancestor_sibling()
            .take(root_pos.level() as usize)
            .filter(|p| filter(*p))
            .collect()
    }
```

**File:** storage/aptosdb/src/ledger_db/transaction_accumulator_db.rs (L77-90)
```rust
    pub fn get_transaction_range_proof(
        &self,
        start_version: Option<Version>,
        num_txns: u64,
        ledger_version: Version,
    ) -> Result<TransactionAccumulatorRangeProof> {
        Accumulator::get_range_proof(
            self,
            ledger_version + 1, /* num_leaves */
            start_version,
            num_txns,
        )
        .map_err(Into::into)
    }
```
