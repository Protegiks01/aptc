# Audit Report

## Title
Byzantine Validator Can Censor Peers Through Selective EpochRetrievalRequest Response Denial

## Summary
A Byzantine validator can prevent specific peers from syncing to new epochs by selectively ignoring their `EpochRetrievalRequest` messages. The consensus epoch manager lacks timeout, retry, and multi-peer fallback mechanisms for epoch retrieval, allowing a malicious validator to censor validators from participating in new epochs indefinitely.

## Finding Description

When a validator detects it is in a lower epoch than a peer, it sends an `EpochRetrievalRequest` to retrieve the epoch change proof needed to transition to the new epoch. However, the implementation has a critical censorship vulnerability: [1](#0-0) 

The `process_different_epoch` method sends the `EpochRetrievalRequest` to exactly **one peer** - the peer that sent the higher-epoch message. If this peer is Byzantine and chooses not to respond, the requesting validator becomes stuck:

1. **No timeout mechanism**: The code sends the request but never implements a timeout to detect non-response
2. **No retry logic**: There is no mechanism to retry the request after a delay
3. **No multi-peer fallback**: The code doesn't query multiple validators for the epoch proof
4. **Single point of failure**: Progress depends entirely on that one peer responding

The request handler at [2](#0-1)  correctly fetches and sends the epoch proof, but a Byzantine validator can simply choose not to execute this response, leaving the requesting node waiting indefinitely.

In contrast, block retrieval implements proper retry logic with multiple peers (documented in search results showing `NUM_RETRIES=5`, `NUM_PEERS_PER_RETRY=3`), but epoch retrieval lacks these protections.

**Attack Scenario:**
1. Epoch E+1 begins, validators V1, V2, V3, and Byzantine validator B transition to new epoch
2. Validator V4 is temporarily behind in epoch E due to network latency
3. Byzantine validator B sends a consensus message (e.g., proposal) to V4
4. V4 detects the epoch mismatch and sends `EpochRetrievalRequest` to B
5. B ignores the request (doesn't send `EpochChangeProof` back)
6. V4 cannot progress to epoch E+1 until it receives messages from other validators

If B can isolate V4 or if V4 doesn't receive messages from other honest validators in epoch E+1, V4 is permanently censored from the new epoch.

## Impact Explanation

**Severity: High** per Aptos Bug Bounty criteria for "Validator node slowdowns" and "Significant protocol violations."

**Consensus Liveness Impact:**
- Censored validators cannot participate in consensus for the new epoch
- If multiple validators are targeted during epoch transition, the network may fail to form quorums
- Epoch transitions become a critical vulnerability window

**Validator Availability:**
- Affected validators cannot propose blocks, vote, or earn rewards
- Network fault tolerance is reduced (effective Byzantine threshold decreases)

**Network Partition Risk:**
- Creates artificial network partitions during epoch transitions
- Validators that successfully sync can progress while censored validators are stuck
- May require manual intervention or hardfork to recover severely affected validators

This breaks the **Consensus Liveness** invariant that honest validators should be able to sync and participate in consensus under the < 1/3 Byzantine assumption.

## Likelihood Explanation

**Likelihood: High during epoch transitions, Medium overall**

**Favorable conditions for attacker:**
- Epoch transitions happen regularly (every few hours on Aptos)
- Network latency naturally causes validators to be at different stages during transitions
- Byzantine validator only needs to send one message to make victim aware of new epoch
- No cost to the attacker (simply don't respond to requests)
- Easy to target specific validators selectively

**Mitigating factors:**
- Victim will eventually receive messages from other honest validators in new epoch
- Well-connected networks with broadcast messages reduce isolation probability
- State sync may provide alternative recovery path (though not explicitly designed for this)

**Realistic Attack Complexity: Low**
- Requires being a validator (but consensus systems assume up to 1/3 Byzantine validators)
- No sophisticated cryptographic attacks needed
- Simple selective message dropping
- Can be implemented passively (just ignore specific requests)

## Recommendation

Implement timeout, retry, and multi-peer fallback mechanisms for epoch retrieval, similar to block retrieval:

```rust
fn process_different_epoch(
    &mut self,
    different_epoch: u64,
    peer_id: AccountAddress,
) -> anyhow::Result<()> {
    // ... existing code ...
    
    match different_epoch.cmp(&self.epoch()) {
        Ordering::Greater => {
            let request = EpochRetrievalRequest {
                start_epoch: self.epoch(),
                end_epoch: different_epoch,
            };
            
            // Instead of single peer request, implement retry logic:
            // 1. Send to initial peer with timeout (e.g., 5 seconds)
            // 2. If timeout, select multiple random peers (e.g., 3 peers) 
            //    from validator set and send to all
            // 3. Retry up to N times (e.g., 5 retries) with exponential backoff
            // 4. Use futures::select to accept first valid response
            
            // Spawn a task to handle the retry logic
            self.spawn_epoch_retrieval_with_retry(request, peer_id, different_epoch);
            Ok(())
        },
        // ... rest of code ...
    }
}

// New method to handle retry logic
async fn spawn_epoch_retrieval_with_retry(
    &mut self,
    request: EpochRetrievalRequest,
    initial_peer: AccountAddress,
    target_epoch: u64,
) {
    const MAX_RETRIES: usize = 5;
    const RETRY_TIMEOUT: Duration = Duration::from_secs(5);
    const PEERS_PER_RETRY: usize = 3;
    
    let mut retry_count = 0;
    
    while retry_count < MAX_RETRIES {
        let peers = if retry_count == 0 {
            vec![initial_peer]
        } else {
            // Select random peers from known validators
            self.select_random_peers(PEERS_PER_RETRY)
        };
        
        // Send requests to all selected peers with timeout
        let result = self.send_epoch_retrieval_with_timeout(
            request.clone(),
            peers,
            RETRY_TIMEOUT,
        ).await;
        
        if result.is_ok() {
            return; // Successfully received proof
        }
        
        retry_count += 1;
        // Exponential backoff
        tokio::time::sleep(RETRY_TIMEOUT * retry_count as u32).await;
    }
    
    warn!("Failed to retrieve epoch proof after {} retries", MAX_RETRIES);
}
```

**Additional recommendations:**
1. Add metrics to track epoch retrieval request timeouts and failures
2. Implement peer reputation tracking to deprioritize non-responsive peers
3. Consider proactive epoch proof broadcasting by validators entering new epoch
4. Add alerting for validators stuck in old epochs

## Proof of Concept

```rust
// Rust test demonstrating the vulnerability
// This would be placed in consensus/src/epoch_manager_test.rs

#[tokio::test]
async fn test_byzantine_epoch_retrieval_censorship() {
    // Setup: Create 4 validators, one Byzantine
    let mut validators = create_test_validators(4);
    let byzantine_validator = validators[0].clone();
    let victim_validator = validators[3].clone();
    
    // Epoch transition occurs: validators 0-2 move to epoch 2
    for v in validators.iter_mut().take(3) {
        v.advance_to_epoch(2).await;
    }
    // Victim validator is still at epoch 1
    assert_eq!(victim_validator.current_epoch(), 1);
    
    // Byzantine validator sends a proposal message to victim
    byzantine_validator.send_proposal_to(
        victim_validator.peer_id(),
        create_test_proposal(epoch: 2, round: 1)
    ).await;
    
    // Victim detects epoch mismatch and sends EpochRetrievalRequest
    // Expected: victim should send request to Byzantine validator
    let retrieval_request = victim_validator
        .wait_for_outgoing_epoch_retrieval_request()
        .await;
    assert_eq!(retrieval_request.recipient, byzantine_validator.peer_id());
    assert_eq!(retrieval_request.start_epoch, 1);
    assert_eq!(retrieval_request.end_epoch, 2);
    
    // Byzantine validator receives request but DOES NOT RESPOND
    // (simulate by not calling respond_to_epoch_retrieval)
    
    // Wait for timeout period (if implemented)
    tokio::time::sleep(Duration::from_secs(10)).await;
    
    // VULNERABILITY: Victim is still stuck at epoch 1
    assert_eq!(victim_validator.current_epoch(), 1);
    
    // Victim cannot participate in consensus for epoch 2
    assert!(!victim_validator.can_participate_in_epoch(2));
    
    // Only way forward is if victim receives message from ANOTHER validator
    let honest_validator = validators[1].clone();
    honest_validator.send_proposal_to(
        victim_validator.peer_id(),
        create_test_proposal(epoch: 2, round: 2)
    ).await;
    
    // Victim sends another request to honest validator
    let second_request = victim_validator
        .wait_for_outgoing_epoch_retrieval_request()
        .await;
    assert_eq!(second_request.recipient, honest_validator.peer_id());
    
    // Honest validator responds correctly
    honest_validator.respond_to_epoch_retrieval(second_request).await;
    
    // NOW victim can advance
    victim_validator.wait_for_epoch_transition(2).await;
    assert_eq!(victim_validator.current_epoch(), 2);
}
```

**Notes**

The vulnerability is exacerbated during epoch transitions when:
1. Network conditions cause natural delays in validators syncing
2. Byzantine validators can strategically target specific peers
3. The victim has limited connectivity to other validators
4. The epoch transition happens during network partitions

This is a systemic design issue in the epoch synchronization protocol that affects consensus liveness and validator participation. The lack of timeout and retry mechanisms is inconsistent with other parts of the codebase (like block retrieval) that properly implement multi-peer fallback strategies.

### Citations

**File:** consensus/src/epoch_manager.rs (L451-476)
```rust
    fn process_epoch_retrieval(
        &mut self,
        request: EpochRetrievalRequest,
        peer_id: AccountAddress,
    ) -> anyhow::Result<()> {
        debug!(
            LogSchema::new(LogEvent::ReceiveEpochRetrieval)
                .remote_peer(peer_id)
                .epoch(self.epoch()),
            "[EpochManager] receive {}", request,
        );
        let proof = self
            .storage
            .aptos_db()
            .get_epoch_ending_ledger_infos(request.start_epoch, request.end_epoch)
            .map_err(DbError::from)
            .context("[EpochManager] Failed to get epoch proof")?;
        let msg = ConsensusMsg::EpochChangeProof(Box::new(proof));
        if let Err(err) = self.network_sender.send_to(peer_id, msg) {
            warn!(
                "[EpochManager] Failed to send epoch proof to {}, with error: {:?}",
                peer_id, err,
            );
        }
        Ok(())
    }
```

**File:** consensus/src/epoch_manager.rs (L478-542)
```rust
    fn process_different_epoch(
        &mut self,
        different_epoch: u64,
        peer_id: AccountAddress,
    ) -> anyhow::Result<()> {
        debug!(
            LogSchema::new(LogEvent::ReceiveMessageFromDifferentEpoch)
                .remote_peer(peer_id)
                .epoch(self.epoch()),
            remote_epoch = different_epoch,
        );
        match different_epoch.cmp(&self.epoch()) {
            Ordering::Less => {
                if self
                    .epoch_state()
                    .verifier
                    .get_voting_power(&self.author)
                    .is_some()
                {
                    // Ignore message from lower epoch if we're part of the validator set, the node would eventually see messages from
                    // higher epoch and request a proof
                    sample!(
                        SampleRate::Duration(Duration::from_secs(1)),
                        debug!("Discard message from lower epoch {} from {}", different_epoch, peer_id);
                    );
                    Ok(())
                } else {
                    // reply back the epoch change proof if we're not part of the validator set since we won't broadcast
                    // timeout in this epoch
                    monitor!(
                        "process_epoch_retrieval",
                        self.process_epoch_retrieval(
                            EpochRetrievalRequest {
                                start_epoch: different_epoch,
                                end_epoch: self.epoch(),
                            },
                            peer_id
                        )
                    )
                }
            },
            // We request proof to join higher epoch
            Ordering::Greater => {
                let request = EpochRetrievalRequest {
                    start_epoch: self.epoch(),
                    end_epoch: different_epoch,
                };
                let msg = ConsensusMsg::EpochRetrievalRequest(Box::new(request));
                if let Err(err) = self.network_sender.send_to(peer_id, msg) {
                    warn!(
                        "[EpochManager] Failed to send epoch retrieval to {}, {:?}",
                        peer_id, err
                    );
                    counters::EPOCH_MANAGER_ISSUES_DETAILS
                        .with_label_values(&["failed_to_send_epoch_retrieval"])
                        .inc();
                }

                Ok(())
            },
            Ordering::Equal => {
                bail!("[EpochManager] Same epoch should not come to process_different_epoch");
            },
        }
    }
```
