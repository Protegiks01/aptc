# Audit Report

## Title
Critical State Inconsistency via Parallel KV/Tree Restoration Failure in StateSnapshotRestore

## Summary
A race condition in `StateSnapshotRestore::add_chunk` allows KV database and Jellyfish Merkle Tree to become permanently desynchronized when tree proof verification fails after KV data has already been committed. This causes different nodes to compute different state roots for identical versions, breaking consensus safety.

## Finding Description

The vulnerability exists in the parallel execution of KV and tree restoration within `StateSnapshotRestore::add_chunk`: [1](#0-0) 

In Default mode, `kv_fn` and `tree_fn` execute in parallel via `IO_POOL.join`. The critical flaw is that `kv_fn` commits data to the database **before** `tree_fn` completes proof verification:

**KV Restoration Path:** [2](#0-1) 

The `write_kv_batch` call atomically commits both the state key-value pairs AND the progress metadata: [3](#0-2) 

**Tree Restoration Path:** [4](#0-3) 

Tree restoration performs the following steps:
1. Skips overlapping keys based on in-memory `previous_leaf` (lines 349-368)
2. Adds keys to in-memory tree structures `partial_nodes` and `previous_leaf` (lines 373-388)
3. **Verifies the SparseMerkleRangeProof** (line 391) - **This can FAIL**
4. Only if verification succeeds, writes frozen nodes to database (lines 394-410)

**The Attack Scenario:**

When a malicious state sync source sends a chunk with an invalid proof:

1. **Chunk 1** arrives with keys [A, B, C] and invalid proof P1
2. Both functions execute in parallel:
   - `kv_fn`: Successfully writes [A, B, C] + progress(C) to DB and **commits**
   - `tree_fn`: Adds keys to in-memory state, then verification **fails** at line 391, database write **never happens**
3. Line 253 checks `r2?` which returns error, but `kv_fn` has already committed
4. In-memory state: `tree_restore.previous_leaf = C`, `partial_nodes` contains tree structure for [A, B, C]
5. Persistent state: KV DB has [A, B, C], Tree DB is **empty**

**Subsequent Chunk Creates Inconsistency:**

When **Chunk 2** arrives with keys [D, E] and valid proof P2:

1. `kv_fn`:
   - Loads progress from DB: progress = C
   - Finds first key > C: D
   - Processes [D, E]
   - Commits to DB

2. `tree_fn`:
   - Uses in-memory `previous_leaf = C` from failed Chunk 1
   - Finds first key > C: D  
   - Processes [D, E] **on top of in-memory partial state from Chunk 1**
   - Verification succeeds (proof P2 is valid for the in-memory tree state)
   - Writes frozen nodes to DB

**Final State:**
- KV DB: Contains [A, B, C, D, E]
- Tree DB: Contains nodes for a tree built from [A, B, C, D, E] in memory
- **BUT**: The tree's base nodes from [A, B, C] were never persisted in Chunk 1!

The tree structure in the database is **incomplete and broken** because it references internal nodes from the in-memory `partial_nodes` that were never written to persistent storage. When other nodes try to verify state or compute the root hash, they will get different results or encounter missing node errors.

The `previous_leaf` recovery mechanism confirms this issue: [5](#0-4) 

The tree relies on `get_rightmost_leaf` to recover state, but if frozen nodes from failed chunks were never written, the recovery will have incomplete information.

## Impact Explanation

**Critical Severity - Consensus Safety Violation**

This vulnerability breaks the **Deterministic Execution** and **State Consistency** invariants:

1. **Different State Roots**: Nodes that successfully restore state will have inconsistent Jellyfish Merkle Trees even with identical KV data. Computing `get_root_hash` will produce different values across nodes.

2. **Consensus Split**: When validators execute the same blocks but have different underlying state tree structures, they will compute different state roots and fail to reach consensus on block commits.

3. **Non-Recoverable Without Manual Intervention**: Once KV and tree are desynchronized, the node cannot automatically recover. The tree structure references nodes that don't exist in the database.

4. **Affects All Nodes During State Sync**: Any node performing state synchronization from a malicious source (or even due to transient network issues causing proof corruption) can be affected.

This meets the **Critical Severity** criteria per Aptos Bug Bounty:
- "Consensus/Safety violations" - Different nodes compute different state roots
- "Non-recoverable network partition" - Requires manual intervention or full re-sync
- "Significant protocol violations" - Breaks state consistency guarantees

## Likelihood Explanation

**High Likelihood**

1. **Easy to Trigger**: Any malicious state sync source can craft invalid proofs to trigger this condition. Network issues causing proof corruption can also trigger it naturally.

2. **No Special Privileges Required**: The attacker only needs to serve state sync data to victims. No validator access or stake required.

3. **Parallel Execution is Default**: The vulnerability is in the Default restore mode which is used by standard state synchronization: [6](#0-5) 

4. **Affects Production Systems**: State synchronization is a critical operation used by new validators joining the network and nodes recovering from downtime.

## Recommendation

**Implement atomic commit for both KV and tree restoration:**

```rust
fn add_chunk(&mut self, chunk: Vec<(K, V)>, proof: SparseMerkleRangeProof) -> Result<()> {
    let kv_fn = || {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_value_add_chunk"]);
        self.kv_restore
            .lock()
            .as_mut()
            .unwrap()
            .add_chunk(chunk.clone())
    };

    let tree_fn = || {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["jmt_add_chunk"]);
        self.tree_restore
            .lock()
            .as_mut()
            .unwrap()
            .add_chunk_impl(chunk.iter().map(|(k, v)| (k, v.hash())).collect(), proof)
    };
    
    match self.restore_mode {
        StateSnapshotRestoreMode::KvOnly => kv_fn()?,
        StateSnapshotRestoreMode::TreeOnly => tree_fn()?,
        StateSnapshotRestoreMode::Default => {
            // FIX: Verify tree proof BEFORE committing KV data
            // Execute tree_fn first to validate proof
            tree_fn()?;
            // Only commit KV if tree verification succeeded
            kv_fn()?;
        },
    }

    Ok(())
}
```

**Alternative Fix**: Modify `StateValueRestore::add_chunk` to defer the DB commit until after tree verification, or implement a two-phase commit protocol.

**Additional Safeguard**: Add a consistency check in `finish()` to verify that the tree root hash matches what would be computed from the KV data.

## Proof of Concept

**Rust Test Scenario:**

```rust
#[test]
fn test_state_restore_inconsistency_on_proof_failure() {
    // Setup: Create a state store with version 100
    let db = Arc::new(setup_test_db());
    let version = 100;
    let expected_root = HashValue::random();
    
    // Create restoration receiver
    let mut receiver = StateSnapshotRestore::new(
        &db.state_merkle_db,
        &db,
        version,
        expected_root,
        false, // async_commit = false
        StateSnapshotRestoreMode::Default,
    ).unwrap();
    
    // Chunk 1: Valid keys but INVALID proof
    let chunk1 = vec![
        (StateKey::raw(b"key1"), StateValue::new_legacy(b"val1".to_vec())),
        (StateKey::raw(b"key2"), StateValue::new_legacy(b"val2".to_vec())),
    ];
    let invalid_proof = SparseMerkleRangeProof::new(vec![HashValue::random()]);
    
    // This should fail due to invalid proof
    let result1 = receiver.add_chunk(chunk1.clone(), invalid_proof);
    assert!(result1.is_err(), "Expected proof verification to fail");
    
    // Check state:
    // - KV should have been committed (bug!)
    // - Tree should have no committed nodes
    let kv_progress = db.get_progress(version).unwrap();
    assert!(kv_progress.is_some(), "KV progress should exist - this is the bug!");
    
    // Chunk 2: New keys with valid proof
    let chunk2 = vec![
        (StateKey::raw(b"key3"), StateValue::new_legacy(b"val3".to_vec())),
    ];
    // Generate valid proof for chunk2 (would require proper setup)
    let valid_proof = generate_valid_proof_for_chunk(&chunk2, version);
    
    // This will succeed but creates inconsistency
    let result2 = receiver.add_chunk(chunk2, valid_proof);
    
    // Finish restoration
    receiver.finish().unwrap();
    
    // Verification: Try to compute root hash from tree
    let tree_root = db.get_root_hash(version);
    
    // The tree root will either:
    // 1. Fail due to missing nodes (frozen nodes from chunk1 never written)
    // 2. Produce incorrect hash due to incomplete tree structure
    
    // This demonstrates the inconsistency between KV and Tree databases
}
```

**Attack Steps:**
1. Setup a malicious state sync source serving state at version V
2. Victim node requests state snapshot for version V
3. Malicious source sends first chunk with invalid SparseMerkleRangeProof
4. Victim's KV commits but tree verification fails
5. Malicious source sends subsequent chunks with valid proofs
6. Victim's tree builds on top of in-memory state from failed chunk
7. Final state: KV and tree are inconsistent, node computes wrong state root
8. Network consensus fails when this node disagrees with others on state root

### Citations

**File:** storage/aptosdb/src/state_restore/mod.rs (L88-127)
```rust
    pub fn add_chunk(&mut self, mut chunk: Vec<(K, V)>) -> Result<()> {
        // load progress
        let progress_opt = self.db.get_progress(self.version)?;

        // skip overlaps
        if let Some(progress) = progress_opt {
            let idx = chunk
                .iter()
                .position(|(k, _v)| CryptoHash::hash(k) > progress.key_hash)
                .unwrap_or(chunk.len());
            chunk = chunk.split_off(idx);
        }

        // quit if all skipped
        if chunk.is_empty() {
            return Ok(());
        }

        // save
        let mut usage = progress_opt.map_or(StateStorageUsage::zero(), |p| p.usage);
        let (last_key, _last_value) = chunk.last().unwrap();
        let last_key_hash = CryptoHash::hash(last_key);

        // In case of TreeOnly Restore, we only restore the usage of KV without actually writing KV into DB
        for (k, v) in chunk.iter() {
            usage.add_item(k.key_size() + v.value_size());
        }

        // prepare the sharded kv batch
        let kv_batch: StateValueBatch<K, Option<V>> = chunk
            .into_iter()
            .map(|(k, v)| ((k, self.version), Some(v)))
            .collect();

        self.db.write_kv_batch(
            self.version,
            &kv_batch,
            StateSnapshotProgress::new(last_key_hash, usage),
        )
    }
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L228-258)
```rust
    fn add_chunk(&mut self, chunk: Vec<(K, V)>, proof: SparseMerkleRangeProof) -> Result<()> {
        let kv_fn = || {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_value_add_chunk"]);
            self.kv_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk(chunk.clone())
        };

        let tree_fn = || {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["jmt_add_chunk"]);
            self.tree_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk_impl(chunk.iter().map(|(k, v)| (k, v.hash())).collect(), proof)
        };
        match self.restore_mode {
            StateSnapshotRestoreMode::KvOnly => kv_fn()?,
            StateSnapshotRestoreMode::TreeOnly => tree_fn()?,
            StateSnapshotRestoreMode::Default => {
                // We run kv_fn with TreeOnly to restore the usage of DB
                let (r1, r2) = IO_POOL.join(kv_fn, tree_fn);
                r1?;
                r2?;
            },
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1152-1159)
```rust
        Ok(Box::new(StateSnapshotRestore::new(
            &self.state_merkle_db,
            self,
            version,
            expected_root_hash,
            false, /* async_commit */
            StateSnapshotRestoreMode::Default,
        )?))
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1244-1279)
```rust
    fn write_kv_batch(
        &self,
        version: Version,
        node_batch: &StateValueBatch,
        progress: StateSnapshotProgress,
    ) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_value_writer_write_chunk"]);
        let mut batch = SchemaBatch::new();
        let mut sharded_schema_batch = self.state_kv_db.new_sharded_native_batches();

        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateSnapshotKvRestoreProgress(version),
            &DbMetadataValue::StateSnapshotProgress(progress),
        )?;

        if self.internal_indexer_db.is_some()
            && self
                .internal_indexer_db
                .as_ref()
                .unwrap()
                .statekeys_enabled()
        {
            let keys = node_batch.keys().map(|key| key.0.clone()).collect();
            self.internal_indexer_db
                .as_ref()
                .unwrap()
                .write_keys_to_indexer_db(&keys, version, progress)?;
        }
        self.shard_state_value_batch(
            &mut sharded_schema_batch,
            node_batch,
            self.state_kv_db.enabled_sharding(),
        )?;
        self.state_kv_db
            .commit(version, Some(batch), sharded_schema_batch)
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L256-263)
```rust
    pub fn previous_key_hash(&self) -> Option<HashValue> {
        if self.finished {
            // Hack: prevent any chunk to be added.
            Some(HashValue::new([0xFF; HashValue::LENGTH]))
        } else {
            self.previous_leaf.as_ref().map(|leaf| *leaf.account_key())
        }
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L339-413)
```rust
    pub fn add_chunk_impl(
        &mut self,
        mut chunk: Vec<(&K, HashValue)>,
        proof: SparseMerkleRangeProof,
    ) -> Result<()> {
        if self.finished {
            info!("State snapshot restore already finished, ignoring entire chunk.");
            return Ok(());
        }

        if let Some(prev_leaf) = &self.previous_leaf {
            let skip_until = chunk
                .iter()
                .find_position(|(key, _hash)| key.hash() > *prev_leaf.account_key());
            chunk = match skip_until {
                None => {
                    info!("Skipping entire chunk.");
                    return Ok(());
                },
                Some((0, _)) => chunk,
                Some((num_to_skip, next_leaf)) => {
                    info!(
                        num_to_skip = num_to_skip,
                        next_leaf = next_leaf,
                        "Skipping leaves."
                    );
                    chunk.split_off(num_to_skip)
                },
            }
        };
        if chunk.is_empty() {
            return Ok(());
        }

        for (key, value_hash) in chunk {
            let hashed_key = key.hash();
            if let Some(ref prev_leaf) = self.previous_leaf {
                ensure!(
                    &hashed_key > prev_leaf.account_key(),
                    "State keys must come in increasing order.",
                )
            }
            self.previous_leaf.replace(LeafNode::new(
                hashed_key,
                value_hash,
                (key.clone(), self.version),
            ));
            self.add_one(key, value_hash);
            self.num_keys_received += 1;
        }

        // Verify what we have added so far is all correct.
        self.verify(proof)?;

        // Write the frozen nodes to storage.
        if self.async_commit {
            self.wait_for_async_commit()?;
            let (tx, rx) = channel();
            self.async_commit_result = Some(rx);

            let mut frozen_nodes = HashMap::new();
            std::mem::swap(&mut frozen_nodes, &mut self.frozen_nodes);
            let store = self.store.clone();

            IO_POOL.spawn(move || {
                let res = store.write_node_batch(&frozen_nodes);
                tx.send(res).unwrap();
            });
        } else {
            self.store.write_node_batch(&self.frozen_nodes)?;
            self.frozen_nodes.clear();
        }

        Ok(())
    }
```
