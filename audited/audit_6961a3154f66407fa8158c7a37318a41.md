# Audit Report

## Title
Backup Storage Cancellation Safety: Resource Exhaustion via Orphaned Upload Processes and Accumulating Partial Files

## Summary
The `BackupStorage` trait's async methods lack cancellation handling, allowing orphaned child processes and partial backup files to accumulate indefinitely when backup operations are cancelled. This leads to storage quota exhaustion and eventual backup system failure. [1](#0-0) 

## Finding Description

The `BackupStorage` trait defines async methods for backup operations without any explicit cancellation handling. When implemented by `CommandAdapter` (used for cloud storage like GCS, S3, Azure), these methods spawn child processes that are not properly cleaned up on cancellation.

**Critical Code Paths:**

1. **CommandAdapter create_for_write** spawns a child process and returns its stdin wrapped in `ChildStdinAsDataSink`: [2](#0-1) 

2. **ChildStdinAsDataSink lacks Drop implementation**, meaning orphaned child processes are never killed when the async operation is cancelled: [3](#0-2) 

3. **Cleanup only triggers on successful shutdown**, not on drop: [4](#0-3) 

**Attack Scenario:**

1. Backup coordinator starts a state snapshot backup, which writes multiple chunks concurrently
2. Each chunk spawns a child process (e.g., `gsutil cp | gzip` or `aws s3 cp`)
3. The backup process is cancelled (node crash, SIGTERM, timeout, panic in async task)
4. The `ChildStdinAsDataSink` futures are dropped without calling `shutdown()`
5. According to tokio documentation, dropping `tokio::process::Child` does NOT kill the process
6. Child processes continue running, uploading partial data to cloud storage
7. Partial chunk files accumulate in storage, consuming quota

**LocalFs is similarly affected:** [5](#0-4) 

Files created with `create_new(true)` remain on disk even if the write operation is cancelled.

**No Cleanup Mechanism Exists:** [6](#0-5) 

The cleanup command is unimplemented, meaning orphaned files are never removed.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program criteria:

1. **Validator node slowdowns**: When storage quota is exhausted, disk I/O operations slow down, affecting validator performance

2. **Significant protocol violations**: Validators are expected to maintain backups for disaster recovery. Storage exhaustion prevents creation of new backups, violating this operational requirement

3. **Cascading availability issues**: 
   - Backup operations fail when storage is full
   - Restore operations may fail if they need temporary storage
   - State synchronization could be affected if it shares storage
   - Eventual node instability when disk space is exhausted

4. **Resource exhaustion attack vector**: An attacker who can repeatedly trigger backup cancellations (via DoS, causing timeouts, or exploiting crash bugs) can deliberately exhaust storage

The vulnerability breaks the **Resource Limits invariant** (#9): "All operations must respect gas, storage, and computational limits" - orphaned processes and files consume unbounded storage.

## Likelihood Explanation

**Likelihood: Medium to High**

Cancellation can occur through multiple realistic scenarios:

1. **Operational scenarios** (High likelihood):
   - Node restarts during backup (maintenance, upgrades)
   - Process killed by systemd/k8s during resource pressure
   - Backup timeout due to network issues
   - OOM killer terminating backup processes

2. **Software failure scenarios** (Medium likelihood):
   - Panic in async backup code (unhandled errors)
   - Graceful shutdown interrupting long-running backups
   - Bug in backup coordinator causing early termination

3. **Attack scenarios** (Lower likelihood but feasible):
   - Triggering resource exhaustion to cause OOM kills
   - Exploiting bugs that cause panics during backup
   - Repeatedly restarting nodes to accumulate orphaned files

The coordinator runs backups continuously: [7](#0-6) 

With continuous operation, cancellations will inevitably occur over time, making this issue a certainty in long-running deployments.

## Recommendation

Implement proper cancellation handling through the following changes:

**1. Implement Drop for ChildStdinAsDataSink and ChildStdoutAsDataSource:**

```rust
impl Drop for ChildStdinAsDataSink<'_> {
    fn drop(&mut self) {
        if let Some(child) = self.child.take() {
            // Kill the child process if we're being dropped before join
            if let Ok(mut child_process) = child.child.try_into() {
                let _ = child_process.start_kill();
            }
        }
    }
}
```

**2. Add cleanup logic for partial files in LocalFs:**

Track files being written and delete them if the operation doesn't complete successfully. Use a guard pattern:

```rust
struct FileWriteGuard {
    path: PathBuf,
    completed: bool,
}

impl Drop for FileWriteGuard {
    fn drop(&mut self) {
        if !self.completed {
            let _ = std::fs::remove_file(&self.path);
        }
    }
}
```

**3. Implement the Cleanup command:** [6](#0-5) 

Add logic to identify and remove orphaned files that aren't referenced in any manifest.

**4. Add cancellation tokens to backup operations:**

Use `tokio_util::sync::CancellationToken` to enable graceful cancellation with cleanup.

## Proof of Concept

```rust
// Test demonstrating orphaned process issue
#[tokio::test]
async fn test_backup_cancellation_leaves_orphans() {
    use std::process::Command;
    use tokio::time::{timeout, Duration};
    
    // Setup: Create a CommandAdapter with a long-running command
    let config = CommandAdapterConfig {
        commands: CommandAdapterCommands {
            create_for_write: "sleep 10 && echo 'file_handle'".to_string(),
            // ... other commands
        },
        env_vars: vec![],
    };
    let adapter = CommandAdapter::new(config);
    
    // Start a write operation
    let write_future = adapter.create_for_write(
        "test_backup",
        &"test_file.chunk".parse().unwrap()
    );
    
    // Cancel it via timeout
    let result = timeout(Duration::from_millis(100), write_future).await;
    assert!(result.is_err()); // Timed out
    
    // Check for orphaned processes
    tokio::time::sleep(Duration::from_secs(1)).await;
    let output = Command::new("pgrep")
        .arg("-f")
        .arg("sleep 10")
        .output()
        .unwrap();
    
    // Orphaned sleep process should still be running
    assert!(!output.stdout.is_empty(), "Orphaned process still running");
    
    // Cleanup
    let _ = Command::new("pkill").arg("-f").arg("sleep 10").output();
}

// Test demonstrating storage accumulation
#[tokio::test]
async fn test_partial_files_accumulate() {
    let temp_dir = tempfile::tempdir().unwrap();
    let local_fs = LocalFs::new(temp_dir.path().to_path_buf());
    
    // Create backup handle
    let backup_handle = local_fs.create_backup(
        &"test_backup".parse().unwrap()
    ).await.unwrap();
    
    // Start multiple write operations and drop them
    for i in 0..10 {
        let (handle, mut writer) = local_fs.create_for_write(
            &backup_handle,
            &format!("chunk_{}.data", i).parse().unwrap()
        ).await.unwrap();
        
        // Write partial data then drop without shutdown
        writer.write_all(b"partial data").await.unwrap();
        // Drop writer here - file remains on disk
    }
    
    // Verify partial files remain
    let file_count = std::fs::read_dir(temp_dir.path().join(&backup_handle))
        .unwrap()
        .count();
    
    assert_eq!(file_count, 10, "All partial files remain on disk");
}
```

**Notes:**

The vulnerability is real and exploitable, leading to operational failure of the backup system through resource exhaustion. While it doesn't directly compromise consensus or funds, it violates availability guarantees and can cascade into validator node issues. The lack of cleanup mechanisms and Drop implementations means this issue will accumulate over time in any production deployment experiencing backup cancellations.

### Citations

**File:** storage/backup/backup-cli/src/storage/mod.rs (L135-188)
```rust
#[async_trait]
pub trait BackupStorage: Send + Sync {
    /// Hint that a bunch of files are gonna be created related to a backup identified by `name`,
    /// which is unique to the content of the backup, i.e. it won't be the same name unless you are
    /// backing up exactly the same thing.
    /// Storage can choose to take actions like create a dedicated folder or do nothing.
    /// Returns a string to identify this operation in potential succeeding file creation requests.
    async fn create_backup(&self, name: &ShellSafeName) -> Result<BackupHandle>;
    /// Ask to create a file for write, `backup_handle` was returned by `create_backup` to identify
    /// the current backup.
    async fn create_for_write(
        &self,
        backup_handle: &BackupHandleRef,
        name: &ShellSafeName,
    ) -> Result<(FileHandle, Box<dyn AsyncWrite + Send + Unpin>)>;
    /// Open file for reading.
    async fn open_for_read(
        &self,
        file_handle: &FileHandleRef,
    ) -> Result<Box<dyn AsyncRead + Send + Unpin>>;
    /// Asks to save a metadata entry and return the File handle of the saved file.
    /// A metadata entry is one line of text.
    /// The backup system doesn't expect a metadata entry to exclusively map to a single file
    /// handle, or the same file handle when accessed later, so there's no need to return one. This
    /// also means a local cache must download each metadata file from remote at least once, to
    /// uncover potential storage glitch sooner.
    /// Behavior on duplicated names is undefined, overwriting the content upon an existing name
    /// is straightforward and acceptable.
    /// See `list_metadata_files`.
    async fn save_metadata_line(
        &self,
        name: &ShellSafeName,
        content: &TextLine,
    ) -> Result<FileHandle> {
        self.save_metadata_lines(name, std::slice::from_ref(content))
            .await
    }
    /// The backup system always asks for all metadata files and cache and build index on top of
    /// the content of them. This means:
    ///   1. The storage is free to reorganise the metadata files, like combining multiple ones to
    /// reduce fragmentation.
    ///   2. But the cache does expect the content stays the same for a file handle, so when
    /// reorganising metadata files, give them new unique names.
    async fn list_metadata_files(&self) -> Result<Vec<FileHandle>>;
    /// Move a metadata file to the metadata file backup folder.
    async fn backup_metadata_file(&self, file_handle: &FileHandleRef) -> Result<()>;
    /// Save a vector of metadata lines to file and return the file handle of saved file.
    /// If the file exists, this will overwrite
    async fn save_metadata_lines(
        &self,
        name: &ShellSafeName,
        lines: &[TextLine],
    ) -> Result<FileHandle>;
}
```

**File:** storage/backup/backup-cli/src/storage/command_adapter/mod.rs (L93-112)
```rust
    async fn create_for_write(
        &self,
        backup_handle: &BackupHandleRef,
        name: &ShellSafeName,
    ) -> Result<(FileHandle, Box<dyn AsyncWrite + Send + Unpin>)> {
        let mut child = self
            .cmd(&self.config.commands.create_for_write, vec![
                EnvVar::backup_handle(backup_handle.to_string()),
                EnvVar::file_name(name.as_ref()),
            ])
            .spawn()?;
        let mut file_handle = FileHandle::new();
        child
            .stdout()
            .read_to_string(&mut file_handle)
            .await
            .err_notes(backup_handle)?;
        file_handle.truncate(file_handle.trim_end().len());
        Ok((file_handle, Box::new(child.into_data_sink())))
    }
```

**File:** storage/backup/backup-cli/src/storage/command_adapter/command.rs (L167-179)
```rust
pub(super) struct ChildStdinAsDataSink<'a> {
    child: Option<SpawnedCommand>,
    join_fut: Option<BoxFuture<'a, Result<()>>>,
}

impl ChildStdinAsDataSink<'_> {
    fn new(child: SpawnedCommand) -> Self {
        Self {
            child: Some(child),
            join_fut: None,
        }
    }
}
```

**File:** storage/backup/backup-cli/src/storage/command_adapter/command.rs (L205-223)
```rust
    fn poll_shutdown(
        mut self: Pin<&mut Self>,
        cx: &mut Context<'_>,
    ) -> Poll<Result<(), tokio::io::Error>> {
        if self.join_fut.is_none() {
            let res = Pin::new(self.child.as_mut().unwrap().stdin()).poll_shutdown(cx);
            if let Poll::Ready(Ok(_)) = res {
                // pipe shutdown successful
                self.join_fut = Some(self.child.take().unwrap().join().boxed())
            } else {
                return res;
            }
        }

        Pin::new(self.join_fut.as_mut().unwrap())
            .poll(cx)
            .map_err(tokio::io::Error::other)
    }
}
```

**File:** storage/backup/backup-cli/src/storage/local_fs/mod.rs (L80-96)
```rust
    async fn create_for_write(
        &self,
        backup_handle: &BackupHandleRef,
        name: &ShellSafeName,
    ) -> Result<(FileHandle, Box<dyn AsyncWrite + Send + Unpin>)> {
        let file_handle = Path::new(backup_handle)
            .join(name.as_ref())
            .path_to_string()?;
        let abs_path = self.dir.join(&file_handle).path_to_string()?;
        let file = OpenOptions::new()
            .write(true)
            .create_new(true)
            .open(&abs_path)
            .await
            .err_notes(&abs_path)?;
        Ok((file_handle, Box::new(file)))
    }
```

**File:** storage/db-tool/src/backup_maintenance.rs (L77-79)
```rust
            Command::Cleanup(_) => {
                // TODO: add cleanup logic for removing obsolete metadata files
            },
```

**File:** storage/backup/backup-cli/src/coordinators/backup.rs (L114-175)
```rust
    pub async fn run(&self) -> Result<()> {
        // Connect to both the local node and the backup storage.
        let backup_state = metadata::cache::sync_and_load(
            &self.metadata_cache_opt,
            Arc::clone(&self.storage),
            self.concurrent_downloads,
        )
        .await?
        .get_storage_state()?;

        // On new DbState retrieved:
        // `watch_db_state` informs `backup_epoch_endings` via channel 1,
        // and the latter informs the other backup type workers via channel 2, after epoch
        // ending is properly backed up, if necessary. This way, the epoch ending LedgerInfo needed
        // for proof verification is always available in the same backup storage.
        let (tx1, rx1) = watch::channel::<Option<DbState>>(None);
        let (tx2, rx2) = watch::channel::<Option<DbState>>(None);

        // Schedule work streams.
        let watch_db_state = IntervalStream::new(interval(Duration::from_secs(1)))
            .then(|_| self.try_refresh_db_state(&tx1))
            .boxed_local();

        let backup_epoch_endings = self
            .backup_work_stream(
                backup_state.latest_epoch_ending_epoch,
                &rx1,
                |slf, last_epoch, db_state| {
                    Self::backup_epoch_endings(slf, last_epoch, db_state, &tx2)
                },
            )
            .boxed_local();
        let backup_state_snapshots = self
            .backup_work_stream(
                backup_state.latest_state_snapshot_epoch,
                &rx2,
                Self::backup_state_snapshot,
            )
            .boxed_local();
        let backup_transactions = self
            .backup_work_stream(
                backup_state.latest_transaction_version,
                &rx2,
                Self::backup_transactions,
            )
            .boxed_local();

        info!("Backup coordinator started.");
        let mut all_work = stream::select_all(vec![
            watch_db_state,
            backup_epoch_endings,
            backup_state_snapshots,
            backup_transactions,
        ]);

        loop {
            all_work
                .next()
                .await
                .ok_or_else(|| anyhow!("Must be a bug: we never returned None."))?
        }
    }
```
