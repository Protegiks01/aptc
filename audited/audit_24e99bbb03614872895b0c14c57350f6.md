# Audit Report

## Title
Byzantine Proposal Flood: Missing Deduplication in OptProposalMsg Verification Enables Resource Exhaustion

## Summary
A Byzantine validator can flood honest validators with multiple `OptProposalMsg` messages containing identical epoch/round but different `block_data`, causing each honest validator to perform expensive cryptographic verification operations for all conflicting proposals before any deduplication occurs. The `verify_well_formed()` function lacks uniqueness checks, allowing resource exhaustion attacks.

## Finding Description

The `verify_well_formed()` function in `OptProposalMsg` validates only structural properties of proposals without checking for duplicate proposals with the same (epoch, round) tuple. [1](#0-0) 

When an `OptProposalMsg` is received from the network, it undergoes full cryptographic verification in the `verify()` method, which includes parallel verification of the payload and grandparent QC using cryptographic operations. [2](#0-1) 

The verification flow in `UnverifiedEvent::verify()` calls the expensive `verify()` method for each received `OptProposalMsg` before any deduplication logic. [3](#0-2) 

After verification, messages are forwarded to a `buffered_proposal_tx` channel configured with `QueueStyle::KLAST` and size 10 per proposer. [4](#0-3) 

While the channel eventually drops older messages, this occurs AFTER full cryptographic verification has already consumed CPU resources. The `forward_event` function performs payload prefetching and inserts each verified proposal into `pending_blocks` without checking for duplicates. [5](#0-4) 

Duplicate detection only occurs much later in `process_opt_proposal()` when checking if a block already exists for the round. [6](#0-5) 

**Attack Scenario:**
1. Byzantine validator is elected proposer for round R in epoch E
2. Creates N different `OptProposalMsg` instances (N >> 10), all with epoch E and round R but varying `block_data` (different transactions, timestamps, parent hashes)
3. Signs all N messages legitimately as the proposer
4. Broadcasts all N messages to honest validators
5. Each honest validator:
   - Receives all N messages from network
   - Spawns verification tasks for each message (bounded executor)
   - Performs cryptographic verification: payload verification, QC verification
   - Forwards verified messages to buffered channel (keeps only latest 10)
   - Eventually processes proposals, rejecting duplicates

The damage is done during step 5: honest validators waste CPU cycles verifying N conflicting proposals, even though only one can eventually be accepted.

## Impact Explanation

This qualifies as **High Severity** per the Aptos bug bounty program: "Validator node slowdowns."

The vulnerability enables resource exhaustion attacks on the consensus layer:
- **CPU Exhaustion**: Each verification involves cryptographic operations (signature verification, QC verification)
- **Network Bandwidth**: All N messages must be transmitted and received
- **Bounded Executor Saturation**: Verification tasks queue up in the bounded executor
- **Consensus Degradation**: Resources spent on invalid proposals reduce capacity for legitimate consensus operations

A coordinated attack by multiple Byzantine validators (allowed up to 1/3 of stake) could significantly degrade network performance by flooding validators with conflicting proposals every round.

## Likelihood Explanation

**Likelihood: High**

**Attacker Requirements:**
- Must be a validator with stake (accessible in permissionless staking)
- Must be elected proposer for a round (occurs naturally through round-robin or deterministic leader election)
- No special privileges beyond normal validator capabilities

**Attack Complexity:**
- Low technical complexity: simply create multiple valid `OptProposalMsg` instances with same (epoch, round)
- Can be executed every time the Byzantine validator is elected proposer
- No cryptographic breaks or insider knowledge required

**Feasibility:**
- The optimistic proposal feature is controlled by `enable_optimistic_proposal_rx` flag
- When enabled, all validators accept and verify these messages
- No rate limiting on proposals per (epoch, round) tuple

## Recommendation

Implement early deduplication based on the (epoch, round, proposer) tuple before performing expensive cryptographic verification.

**Option 1: Add deduplication cache in EpochManager**
Maintain a cache of recently seen (epoch, round, proposer) tuples and reject duplicate proposals before verification:

```rust
// In EpochManager struct
recent_proposals: LruCache<(u64, Round, Author), ()>

// In process_message, before spawning verification task
if let ConsensusMsg::OptProposalMsg(proposal) = &consensus_msg {
    let key = (proposal.epoch(), proposal.round(), proposal.proposer());
    if self.recent_proposals.contains(&key) {
        return Ok(()); // Silent drop of duplicate
    }
    self.recent_proposals.put(key, ());
}
```

**Option 2: Enhance verify_well_formed() with state awareness**
Pass round state to `verify_well_formed()` to check if a proposal for this round has already been processed.

**Option 3: Network-level rate limiting**
Implement per-peer rate limiting for proposals with the same (epoch, round) in the network layer.

Recommendation: Implement Option 1 as it provides the earliest possible filtering with minimal overhead.

## Proof of Concept

```rust
// Rust test demonstrating the vulnerability
// File: consensus/src/round_manager_tests/opt_proposal_flood_test.rs

#[tokio::test]
async fn test_byzantine_opt_proposal_flood() {
    use crate::test_utils::{build_empty_tree, TreeInserter};
    use consensus_types::{
        block::Block,
        common::{Payload, Round},
        opt_block_data::OptBlockData,
        opt_proposal_msg::OptProposalMsg,
        sync_info::SyncInfo,
    };
    use aptos_types::validator_signer::ValidatorSigner;
    
    let mut inserter = TreeInserter::default();
    let round = 3u64;
    let epoch = 1u64;
    
    // Byzantine proposer creates 100 conflicting proposals for same round
    let byzantine_signer = ValidatorSigner::random(None);
    let mut conflicting_proposals = Vec::new();
    
    for i in 0..100 {
        // Create grandparent QC
        let grandparent_round = round.saturating_sub(2);
        let grandparent_qc = inserter.create_qc_for_block(
            inserter.block_at_round(grandparent_round).unwrap(),
            None,
        );
        
        // Create parent block info pointing to grandparent
        let parent = inserter.block_at_round(round - 1).unwrap().block_info();
        
        // Create different block_data for each proposal
        let timestamp = 1000 * round + i; // Different timestamp
        let payload = if i % 2 == 0 {
            Payload::empty(false, true)
        } else {
            Payload::DirectMempool(vec![]) // Alternating payloads
        };
        
        let opt_block_data = OptBlockData::new(
            vec![],
            payload,
            byzantine_signer.author(),
            epoch,
            round,
            timestamp,
            parent.clone(),
            grandparent_qc.clone(),
        );
        
        let sync_info = SyncInfo::new(
            grandparent_qc.clone(),
            grandparent_qc.into_wrapped_ledger_info(),
            None,
        );
        
        let proposal = OptProposalMsg::new(opt_block_data, sync_info);
        
        // All proposals pass verify_well_formed()
        assert!(proposal.verify_well_formed().is_ok());
        
        conflicting_proposals.push(proposal);
    }
    
    // Simulate honest validator receiving and verifying all proposals
    let start = std::time::Instant::now();
    let validator_verifier = inserter.get_validator_verifier();
    let proof_cache = ProofCache::new(1024);
    
    let mut verified_count = 0;
    for proposal in conflicting_proposals {
        // Each proposal undergoes full verification (expensive)
        if proposal
            .verify(
                byzantine_signer.author(),
                &validator_verifier,
                &proof_cache,
                false,
            )
            .is_ok()
        {
            verified_count += 1;
        }
    }
    
    let elapsed = start.elapsed();
    
    println!(
        "Verified {} conflicting proposals in {:?}",
        verified_count, elapsed
    );
    println!("Average time per verification: {:?}", elapsed / verified_count as u32);
    
    // Demonstrate resource waste: all proposals verified despite being for same round
    assert_eq!(verified_count, 100);
    
    // In production, only ONE of these proposals would eventually be accepted,
    // but the validator wasted resources verifying all 100.
}
```

**Notes**

The vulnerability exists because the verification pipeline prioritizes correctness over efficiency, verifying all received messages before applying deduplication logic. While the `buffered_proposal_tx` channel with `KLAST` policy eventually drops older messages, this optimization occurs too lateâ€”after expensive cryptographic operations have already been performed.

The default `internal_per_key_channel_size` of 10 means the channel will keep the latest 10 proposals per proposer, but a Byzantine validator can send arbitrarily many messages, all of which will be verified. [7](#0-6) 

The bounded executor spawns verification tasks asynchronously, but all tasks eventually execute, consuming CPU resources. [8](#0-7) 

This vulnerability breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." The consensus layer should implement early filtering to prevent resource exhaustion from malicious or buggy validators.

### Citations

**File:** consensus/consensus-types/src/opt_proposal_msg.rs (L54-94)
```rust
    pub fn verify_well_formed(&self) -> Result<()> {
        self.block_data
            .verify_well_formed()
            .context("Fail to verify OptProposalMsg's data")?;
        ensure!(
            self.block_data.round() > 1,
            "Proposal for {} has round <= 1",
            self.block_data,
        );
        ensure!(
            self.block_data.epoch() == self.sync_info.epoch(),
            "ProposalMsg has different epoch number from SyncInfo"
        );
        // Ensure the sync info has the grandparent QC
        ensure!(
            self.block_data.grandparent_qc().certified_block().id()
                == self.sync_info.highest_quorum_cert().certified_block().id(),
            "Proposal HQC in SyncInfo certifies {}, but block grandparent id is {}",
            self.sync_info.highest_quorum_cert().certified_block().id(),
            self.block_data.grandparent_qc().certified_block().id(),
        );
        let grandparent_round = self
            .block_data
            .round()
            .checked_sub(2)
            .ok_or_else(|| anyhow::anyhow!("proposal round overflowed!"))?;

        let highest_certified_round = self.block_data.grandparent_qc().certified_block().round();
        ensure!(
            grandparent_round == highest_certified_round,
            "Proposal {} does not have a certified round {}",
            self.block_data,
            grandparent_round
        );
        // Optimistic proposal shouldn't have a timeout certificate
        ensure!(
            self.sync_info.highest_2chain_timeout_cert().is_none(),
            "Optimistic proposal shouldn't have a timeout certificate"
        );
        Ok(())
    }
```

**File:** consensus/consensus-types/src/opt_proposal_msg.rs (L96-123)
```rust
    pub fn verify(
        &self,
        sender: Author,
        validator: &ValidatorVerifier,
        proof_cache: &ProofCache,
        quorum_store_enabled: bool,
    ) -> Result<()> {
        ensure!(
            self.proposer() == sender,
            "OptProposal author {:?} doesn't match sender {:?}",
            self.proposer(),
            sender
        );

        let (payload_verify_result, qc_verify_result) = rayon::join(
            || {
                self.block_data()
                    .payload()
                    .verify(validator, proof_cache, quorum_store_enabled)
            },
            || self.block_data().grandparent_qc().verify(validator),
        );
        payload_verify_result?;
        qc_verify_result?;

        // Note that we postpone the verification of SyncInfo until it's being used.
        self.verify_well_formed()
    }
```

**File:** consensus/src/round_manager.rs (L129-136)
```rust
            UnverifiedEvent::OptProposalMsg(p) => {
                if !self_message {
                    p.verify(peer_id, validator, proof_cache, quorum_store_enabled)?;
                    counters::VERIFY_MSG
                        .with_label_values(&["opt_proposal"])
                        .observe(start_time.elapsed().as_secs_f64());
                }
                VerifiedEvent::OptProposalMsg(p)
```

**File:** consensus/src/round_manager.rs (L843-850)
```rust
    async fn process_opt_proposal(&mut self, opt_block_data: OptBlockData) -> anyhow::Result<()> {
        ensure!(
            self.block_store
                .get_block_for_round(opt_block_data.round())
                .is_none(),
            "Proposal has already been processed for round: {}",
            opt_block_data.round()
        );
```

**File:** consensus/src/epoch_manager.rs (L956-960)
```rust
        let (buffered_proposal_tx, buffered_proposal_rx) = aptos_channel::new(
            QueueStyle::KLAST,
            self.config.internal_per_key_channel_size,
            Some(&counters::ROUND_MANAGER_CHANNEL_MSGS),
        );
```

**File:** consensus/src/epoch_manager.rs (L1587-1622)
```rust
            self.bounded_executor
                .spawn(async move {
                    match monitor!(
                        "verify_message",
                        unverified_event.clone().verify(
                            peer_id,
                            &epoch_state.verifier,
                            &proof_cache,
                            quorum_store_enabled,
                            peer_id == my_peer_id,
                            max_num_batches,
                            max_batch_expiry_gap_usecs,
                        )
                    ) {
                        Ok(verified_event) => {
                            Self::forward_event(
                                quorum_store_msg_tx,
                                round_manager_tx,
                                buffered_proposal_tx,
                                peer_id,
                                verified_event,
                                payload_manager,
                                pending_blocks,
                            );
                        },
                        Err(e) => {
                            error!(
                                SecurityEvent::ConsensusInvalidMessage,
                                remote_peer = peer_id,
                                error = ?e,
                                unverified_event = unverified_event
                            );
                        },
                    }
                })
                .await;
```

**File:** consensus/src/epoch_manager.rs (L1779-1793)
```rust
            opt_proposal_event @ VerifiedEvent::OptProposalMsg(_) => {
                if let VerifiedEvent::OptProposalMsg(p) = &opt_proposal_event {
                    payload_manager.prefetch_payload_data(
                        p.block_data().payload(),
                        p.proposer(),
                        p.timestamp_usecs(),
                    );
                    pending_blocks
                        .lock()
                        .insert_opt_block(p.block_data().clone());
                }

                Self::forward_event_to(buffered_proposal_tx, peer_id, opt_proposal_event)
                    .context("proposal precheck sender")
            },
```

**File:** config/src/config/consensus_config.rs (L242-242)
```rust
            internal_per_key_channel_size: 10,
```
