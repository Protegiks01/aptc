# Audit Report

## Title
State KV Store and Merkle Tree Desynchronization via Independent Oneoff Restore Commands

## Summary
The `Command::run()` function in `restore.rs` allows independent execution of state snapshot and transaction restore operations without proper coordination, enabling state KV store and Jellyfish Merkle tree desynchronization. When a user restores a state snapshot and then performs a KV-only transaction replay, the KV store advances to a newer version while the Merkle tree remains at the old version, violating the fundamental invariant that the Merkle tree must be a cryptographic commitment to the KV state. [1](#0-0) 

## Finding Description
The vulnerability exists in the oneoff restore command execution path. The `RestoreCoordinator` properly coordinates state snapshot and transaction restores in a two-phase approach, but the oneoff commands (`Oneoff::StateSnapshot` and `Oneoff::Transaction`) can be executed independently without this coordination.

**Attack Scenario:**

1. **Restore State Snapshot**: An operator runs the oneoff state snapshot restore with `restore_mode=default`, which restores both the KV store and Merkle tree at version X: [2](#0-1) 

2. **KV-Only Transaction Replay**: The operator then runs oneoff transaction restore with `kv_only_replay=true`, which replays transactions from version X+1 to Y: [3](#0-2) 

The transaction restore controller accepts `kv_only_replay` parameter: [4](#0-3) 

When `kv_only_replay=true`, it calls `replay_kv()` which updates only the KV store: [5](#0-4) 

The `save_transactions_and_replay_kv` function only updates the KV store via `calculate_state_and_put_updates`: [6](#0-5) 

This function writes only to the KV store, not the Merkle tree: [7](#0-6) 

**Result**: The database now has:
- KV store at version Y with state values V_Y
- Merkle tree at version X with root hash H_X
- Transaction info claiming state_checkpoint_hash = H_Y (from backup)

The Merkle tree root hash `get_root_hash(Y)` returns H_X (old hash), but the actual state should have root hash H_Y: [8](#0-7) 

**Broken Invariant**: "State Consistency: State transitions must be atomic and verifiable via Merkle proofs"

The Merkle tree is a cryptographic commitment to the state. When they desynchronize, any Merkle proof generated for state at version Y will be invalid because it uses the wrong tree (at version X).

## Impact Explanation
This vulnerability qualifies as **High Severity** under the Aptos bug bounty program:

1. **Significant Protocol Violation**: The fundamental guarantee that state is verifiable via Merkle proofs is broken. The node cannot serve valid state proofs.

2. **State Inconsistencies**: The database is in an inconsistent state requiring manual intervention to fix. State sync operations will fail when trying to verify state root hashes.

3. **Consensus Risk**: If multiple validators independently restore using this flawed sequence, they could have different Merkle tree states despite having identical KV stores, potentially causing state root hash mismatches during consensus.

4. **Operational Impact**: The node becomes unable to:
   - Serve valid Merkle proofs for state queries
   - Participate in state synchronization as a source node
   - Verify incoming state sync data correctly

While this doesn't directly cause fund loss or network partition, it represents a **significant protocol violation** that corrupts node state and could cascade into consensus issues.

## Likelihood Explanation
**Likelihood: Medium**

This vulnerability requires:
1. Access to the db-tool CLI (typically node operators during backup/restore operations)
2. Knowledge to use the oneoff commands instead of the coordinated BootstrapDB command
3. Specific parameter combination (`restore_mode=default` followed by `kv_only_replay=true`)

However:
- The commands are documented and available in the CLI help
- Operators performing partial restores or testing might naturally try these combinations
- There are no warnings or validation checks preventing this sequence
- The KV-only replay feature exists specifically for optimization, making it a natural choice for operators

The lack of safeguards and the legitimate use case for KV-only replay make this reasonably likely to occur in practice.

## Recommendation

**Add coordination validation** to prevent independent oneoff commands from causing KV/Merkle tree desynchronization:

1. **Short-term fix**: Add a check in `TransactionRestoreController` that prevents `kv_only_replay=true` unless a corresponding tree-only snapshot restore is planned, or document that oneoff commands should not be mixed.

2. **Better fix**: Modify the oneoff transaction restore to automatically restore the tree snapshot when KV-only replay is used:

```rust
// In storage/backup/backup-cli/src/backup_types/transaction/restore.rs
// After KV-only replay completes, check if tree needs updating and either:
// a) Restore a tree-only snapshot at the final version, OR
// b) Rebuild the tree from the KV store, OR  
// c) Error out with clear message that tree must be separately restored
```

3. **Long-term fix**: Introduce a state consistency check that validates KV store version matches Merkle tree version before allowing the database to be used:

```rust
// In storage/aptosdb/src/state_store/mod.rs
pub fn validate_consistency(&self) -> Result<()> {
    let kv_version = self.get_synced_version()?;
    let tree_version = self.state_merkle_db.get_latest_version()?;
    
    ensure!(
        kv_version == tree_version,
        "State inconsistency: KV store at version {} but Merkle tree at version {}",
        kv_version,
        tree_version
    );
    
    // Also verify root hash matches if available
    if let Ok(expected_hash) = self.get_transaction_info(kv_version)
        .and_then(|info| info.ensure_state_checkpoint_hash()) 
    {
        let actual_hash = self.get_root_hash(kv_version)?;
        ensure!(
            expected_hash == actual_hash,
            "State root mismatch at version {}: expected {}, got {}",
            kv_version, expected_hash, actual_hash
        );
    }
    
    Ok(())
}
```

## Proof of Concept

**CLI Reproduction Steps:**

```bash
# Step 1: Restore state snapshot at version 1000 (both KV and tree)
./aptos-db-tool restore Oneoff StateSnapshot \
  --state-manifest /path/to/snapshot_1000/manifest.json \
  --state-into-version 1000 \
  --restore-mode default \
  --target-db-dir /path/to/restored_db \
  --target-version 2000

# Step 2: Restore transactions with KV-only replay from 1001 onwards
./aptos-db-tool restore Oneoff Transaction \
  --transaction-manifest /path/to/txns/manifest.json \
  --replay-transactions-from-version 1001 \
  --kv-only-replay true \
  --target-db-dir /path/to/restored_db \
  --target-version 2000

# Step 3: Verify the inconsistency
# Query the KV store - it will show state at version 2000
# Query the Merkle tree root hash at version 2000 - it will still be the hash from version 1000
# Attempting to generate a Merkle proof for any state at version 2000 will fail or produce invalid proof

# The database is now in an inconsistent state where:
# - state_store.get_synced_version() returns 2000
# - state_merkle_db.get_root_hash(2000) returns the root hash from version 1000
# - Any state proof verification will fail
```

**Validation Test:**

```rust
// This test would demonstrate the inconsistency
#[test]
fn test_kv_merkle_desync_via_oneoff_commands() {
    // 1. Initialize DB with state at version 1000
    let db = create_test_db();
    let state_at_1000 = generate_test_state();
    commit_state(&db, 1000, state_at_1000);
    let root_hash_1000 = db.state_store.get_root_hash(1000).unwrap();
    
    // 2. Create backup at version 1000
    let backup = create_state_snapshot_backup(&db, 1000);
    
    // 3. Restore via oneoff state snapshot (both KV and tree)
    let restored_db = create_empty_db();
    restore_state_snapshot(&restored_db, backup, RestoreMode::Default);
    assert_eq!(restored_db.state_store.get_root_hash(1000).unwrap(), root_hash_1000);
    
    // 4. Create transaction backup from 1001-2000
    let txn_backup = create_transaction_backup(&db, 1001, 2000);
    
    // 5. Restore via oneoff transaction with kv_only_replay=true
    restore_transactions_kv_only(&restored_db, txn_backup, 1001);
    
    // 6. Verify the inconsistency
    let kv_version = restored_db.get_synced_version().unwrap(); // Returns 2000
    assert_eq!(kv_version, 2000);
    
    let tree_root_at_2000 = restored_db.state_store.get_root_hash(2000).unwrap();
    // BUG: tree_root_at_2000 still equals root_hash_1000 (old tree)
    assert_eq!(tree_root_at_2000, root_hash_1000); // This passes, showing the bug
    
    // Expected: tree_root_at_2000 should match the root hash for state at version 2000
    let expected_root_at_2000 = db.state_store.get_root_hash(2000).unwrap();
    assert_ne!(tree_root_at_2000, expected_root_at_2000); // This also passes, confirming desync
}
```

## Notes

The `RestoreCoordinator` properly handles this coordination in its two-phase approach, carefully using `KvOnly`, `TreeOnly`, and `Default` restore modes to ensure the KV store and Merkle tree stay synchronized. The vulnerability only exists when operators bypass the coordinator and use oneoff commands directly. [9](#0-8) 

The coordinator's phase 1 restores KV only, then phase 2 restores the tree at that same version, maintaining consistency. The oneoff commands lack this coordination logic.

### Citations

**File:** storage/db-tool/src/restore.rs (L66-127)
```rust
    pub async fn run(self) -> Result<()> {
        match self {
            Command::Oneoff(oneoff) => {
                match oneoff {
                    Oneoff::EpochEnding {
                        storage,
                        opt,
                        global,
                    } => {
                        EpochEndingRestoreController::new(
                            opt,
                            global.try_into()?,
                            storage.init_storage().await?,
                        )
                        .run(None)
                        .await?;
                    },
                    Oneoff::StateSnapshot {
                        storage,
                        opt,
                        global,
                    } => {
                        StateSnapshotRestoreController::new(
                            opt,
                            global.try_into()?,
                            storage.init_storage().await?,
                            None, /* epoch_history */
                        )
                        .run()
                        .await?;
                    },
                    Oneoff::Transaction {
                        storage,
                        opt,
                        global,
                    } => {
                        TransactionRestoreController::new(
                            opt,
                            global.try_into()?,
                            storage.init_storage().await?,
                            None, /* epoch_history */
                            VerifyExecutionMode::NoVerify,
                        )
                        .run()
                        .await?;
                    },
                }
            },
            Command::BootstrapDB(bootstrap) => {
                RestoreCoordinator::new(
                    bootstrap.opt,
                    bootstrap.global.try_into()?,
                    bootstrap.storage.init_storage().await?,
                )
                .run()
                .await?;
            },
        }

        Ok(())
    }
}
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L74-77)
```rust
    pub replay_from_version: Option<Version>,
    #[clap(long)]
    pub kv_only_replay: Option<bool>,
}
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L320-332)
```rust
            let kv_only = self.replay_from_version.is_some_and(|(_, k)| k);
            let txns_to_execute_stream = self
                .save_before_replay_version(first_version, loaded_chunk_stream, restore_handler)
                .await?;

            if let Some(txns_to_execute_stream) = txns_to_execute_stream {
                if kv_only {
                    self.replay_kv(restore_handler, txns_to_execute_stream)
                        .await?;
                } else {
                    self.replay_transactions(restore_handler, txns_to_execute_stream)
                        .await?;
                }
```

**File:** storage/aptosdb/src/backup/restore_utils.rs (L269-277)
```rust
    if kv_replay && first_version > 0 && state_store.get_usage(Some(first_version - 1)).is_ok() {
        let (ledger_state, _hot_state_updates) = state_store.calculate_state_and_put_updates(
            &StateUpdateRefs::index_write_sets(first_version, write_sets, write_sets.len(), vec![]),
            &mut ledger_db_batch.ledger_metadata_db_batches, // used for storing the storage usage
            state_kv_batches,
        )?;
        // n.b. ideally this is set after the batches are committed
        state_store.set_state_ignoring_summary(ledger_state);
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L809-843)
```rust
    pub fn put_state_values(
        &self,
        state_update_refs: &PerVersionStateUpdateRefs,
        sharded_state_kv_batches: &mut ShardedStateKvSchemaBatch,
    ) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["add_state_kv_batch"]);

        // TODO(aldenhu): put by refs; batch put
        sharded_state_kv_batches
            .par_iter_mut()
            .zip_eq(state_update_refs.shards.par_iter())
            .try_for_each(|(batch, updates)| {
                updates
                    .iter()
                    .filter_map(|(key, update)| {
                        update
                            .state_op
                            .as_write_op_opt()
                            .map(|write_op| (key, update.version, write_op))
                    })
                    .try_for_each(|(key, version, write_op)| {
                        if self.state_kv_db.enabled_sharding() {
                            batch.put::<StateValueByKeyHashSchema>(
                                &(CryptoHash::hash(*key), version),
                                &write_op.as_state_value_opt().cloned(),
                            )
                        } else {
                            batch.put::<StateValueSchema>(
                                &((*key).clone(), version),
                                &write_op.as_state_value_opt().cloned(),
                            )
                        }
                    })
            })
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1056-1058)
```rust
    pub fn get_root_hash(&self, version: Version) -> Result<HashValue> {
        self.state_merkle_db.get_root_hash(version)
    }
```

**File:** storage/backup/backup-cli/src/coordinators/restore.rs (L236-303)
```rust
        if do_phase_1 {
            info!(
                "Start restoring DB from version {} to tree snapshot version {}",
                txn_start_version, tree_snapshot.version,
            );

            // phase 1.a: restore the kv snapshot
            if kv_snapshot.is_some() {
                let kv_snapshot = kv_snapshot.clone().unwrap();
                info!("Start restoring KV snapshot at {}", kv_snapshot.version);

                StateSnapshotRestoreController::new(
                    StateSnapshotRestoreOpt {
                        manifest_handle: kv_snapshot.manifest,
                        version: kv_snapshot.version,
                        validate_modules: false,
                        restore_mode: StateSnapshotRestoreMode::KvOnly,
                    },
                    self.global_opt.clone(),
                    Arc::clone(&self.storage),
                    epoch_history.clone(),
                )
                .run()
                .await?;
            }

            // phase 1.b: save the txn between the first txn of the first chunk and the tree snapshot
            let txn_manifests = transaction_backups
                .iter()
                .filter(|e| {
                    e.first_version <= tree_snapshot.version && e.last_version >= db_next_version
                })
                .map(|e| e.manifest.clone())
                .collect();
            assert!(
                db_next_version == 0
                    || transaction_backups.first().map_or(0, |t| t.first_version)
                        <= db_next_version,
                "Inconsistent state: first txn version {} is larger than db_next_version {}",
                transaction_backups.first().map_or(0, |t| t.first_version),
                db_next_version
            );
            // update the kv to the kv db
            // reset the global
            let mut transaction_restore_opt = self.global_opt.clone();
            // We should replay kv to include the version of tree snapshot so that we can get correct storage usage at that version
            // while restore tree only snapshots
            let kv_replay_version = if let Some(kv_snapshot) = kv_snapshot.as_ref() {
                kv_snapshot.version + 1
            } else {
                db_next_version
            };
            transaction_restore_opt.target_version = tree_snapshot.version;
            TransactionRestoreBatchController::new(
                transaction_restore_opt,
                Arc::clone(&self.storage),
                txn_manifests,
                Some(db_next_version),
                Some((kv_replay_version, true /* only replay KV */)),
                epoch_history.clone(),
                VerifyExecutionMode::NoVerify,
                None,
            )
            .run()
            .await?;
            // update the expected version for the first phase restore
            db_next_version = tree_snapshot.version;
        }
```
