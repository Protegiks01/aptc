# Audit Report

## Title
Quorum Store Back Pressure Inverted Due to Unvalidated `decrease_fraction` Configuration

## Summary
The `decrease_fraction` field in `QuorumStoreBackPressureConfig` lacks validation to ensure it falls within the mathematically correct range of 0.0 to 1.0. When set to values greater than 1.0, the back pressure mechanism inverts its intended behavior, causing transaction ingestion to **increase** rather than decrease during network congestion, leading to validator node slowdowns and potential consensus degradation. [1](#0-0) 

## Finding Description

The Quorum Store implements an AIMD (Additive Increase Multiplicative Decrease) algorithm to dynamically control transaction pull rates from the mempool. When back pressure is detected (too many pending transactions), the system should multiplicatively decrease the pull rate by multiplying it with `decrease_fraction`. [2](#0-1) 

The critical vulnerability occurs at the multiplication operation where `dynamic_pull_txn_per_s` is multiplied by `self.config.back_pressure.decrease_fraction`. The code assumes this value is between 0.0 and 1.0, but **no validation exists** to enforce this constraint. [3](#0-2) 

The `ConfigSanitizer` implementation validates batch limits but completely omits validation of the `back_pressure` configuration fields, including `decrease_fraction`.

**Attack Scenario:**

1. Validator operator sets `decrease_fraction: 2.0` in the node configuration YAML (either accidentally thinking it's a percentage, or through compromised deployment scripts)
2. Node starts and loads the configuration without validation
3. When the network becomes congested and back pressure is triggered (remaining transactions exceed `backlog_txn_limit_count`):
   - The ProofManager detects back pressure and signals the BatchGenerator
   - Instead of reducing the transaction pull rate by multiplying by 0.5 (default), it multiplies by 2.0
   - This **doubles** the pull rate instead of halving it
   - More transactions are pulled from mempool, exacerbating the congestion
4. This creates a positive feedback loop where congestion triggers increased load, making the problem worse
5. The node experiences memory exhaustion, network flooding, and consensus slowdowns [4](#0-3) 

**Invariant Violation:** This breaks the **Resource Limits** invariant that "all operations must respect gas, storage, and computational limits." The back pressure mechanism is specifically designed to enforce these limits, but fails catastrophically when `decrease_fraction` is misconfigured.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty criteria for the following reasons:

**High Severity - Validator Node Slowdowns:**
- When back pressure inverts, affected validators will continuously increase their transaction pull rate during congestion
- This causes excessive memory consumption, network bandwidth usage, and CPU overhead
- The validator's consensus participation degrades, causing block proposal delays and voting delays
- If multiple validators are misconfigured, the entire network experiences consensus slowdown

**Potential Escalation to Network-Wide Impact:**
- A single misconfigured validator floods the network with batches during peak load
- Other validators must process these additional batches, consuming their resources
- Network-wide throughput degrades as validators struggle with overload
- In extreme cases, could approach liveness failure if enough validators are affected

The impact is compounded by the fact that this occurs precisely when the network is already under stress (high transaction load), making the timing maximally damaging.

## Likelihood Explanation

**Likelihood: Medium-High**

This vulnerability has a realistic probability of occurring through:

1. **Accidental Misconfiguration:**
   - Operators may interpret `decrease_fraction` as a percentage and set it to `200%` (2.0) intending to make back pressure more aggressive
   - Copy-paste errors in configuration templates
   - Misunderstanding of the AIMD algorithm parameters

2. **Supply Chain Attacks:**
   - Compromised deployment automation scripts could inject malicious config values
   - Malicious configuration templates in deployment tooling
   - Insider threat from operator with config file write access

3. **Lack of Runtime Protection:**
   - No validation at config load time
   - No bounds checking at runtime during the multiplication
   - No alerting when back pressure behaves abnormally

The lack of any defensive validation makes this a single point of failure with no safety net.

## Recommendation

Add comprehensive validation to the `QuorumStoreBackPressureConfig` struct and sanitizer:

**1. Add validation method to the config struct:**

```rust
impl QuorumStoreBackPressureConfig {
    pub fn validate(&self) -> Result<(), String> {
        if self.decrease_fraction < 0.0 || self.decrease_fraction > 1.0 {
            return Err(format!(
                "decrease_fraction must be between 0.0 and 1.0, got {}",
                self.decrease_fraction
            ));
        }
        if self.dynamic_min_txn_per_s > self.dynamic_max_txn_per_s {
            return Err(format!(
                "dynamic_min_txn_per_s ({}) must be <= dynamic_max_txn_per_s ({})",
                self.dynamic_min_txn_per_s,
                self.dynamic_max_txn_per_s
            ));
        }
        Ok(())
    }
}
```

**2. Update the ConfigSanitizer to call this validation:**

```rust
impl ConfigSanitizer for QuorumStoreConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();

        // Validate back pressure configuration
        node_config
            .consensus
            .quorum_store
            .back_pressure
            .validate()
            .map_err(|e| Error::ConfigSanitizerFailed(sanitizer_name.clone(), e))?;

        // Sanitize the send/recv batch limits
        Self::sanitize_send_recv_batch_limits(
            &sanitizer_name,
            &node_config.consensus.quorum_store,
        )?;

        // Sanitize the batch total limits
        Self::sanitize_batch_total_limits(&sanitizer_name, &node_config.consensus.quorum_store)?;

        Ok(())
    }
}
```

**3. Add runtime bounds checking as defense in depth:**

```rust
// In batch_generator.rs, line 438-441
let safe_decrease_fraction = self.config.back_pressure.decrease_fraction.clamp(0.0, 1.0);
dynamic_pull_txn_per_s = std::cmp::max(
    (dynamic_pull_txn_per_s as f64 * safe_decrease_fraction) as u64,
    self.config.back_pressure.dynamic_min_txn_per_s,
);
```

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use crate::config::{ConsensusConfig, NodeConfig};

    #[test]
    #[should_panic(expected = "decrease_fraction must be between 0.0 and 1.0")]
    fn test_decrease_fraction_above_one() {
        // Create a node config with invalid decrease_fraction > 1.0
        let node_config = NodeConfig {
            consensus: ConsensusConfig {
                quorum_store: QuorumStoreConfig {
                    back_pressure: QuorumStoreBackPressureConfig {
                        decrease_fraction: 2.0, // INVALID: should be <= 1.0
                        ..Default::default()
                    },
                    ..Default::default()
                },
                ..Default::default()
            },
            ..Default::default()
        };

        // This should fail validation
        QuorumStoreConfig::sanitize(
            &node_config,
            NodeType::Validator,
            Some(ChainId::testnet()),
        )
        .unwrap();
    }

    #[test]
    #[should_panic(expected = "decrease_fraction must be between 0.0 and 1.0")]
    fn test_decrease_fraction_negative() {
        let node_config = NodeConfig {
            consensus: ConsensusConfig {
                quorum_store: QuorumStoreConfig {
                    back_pressure: QuorumStoreBackPressureConfig {
                        decrease_fraction: -0.5, // INVALID: should be >= 0.0
                        ..Default::default()
                    },
                    ..Default::default()
                },
                ..Default::default()
            },
            ..Default::default()
        };

        QuorumStoreConfig::sanitize(
            &node_config,
            NodeType::Validator,
            Some(ChainId::testnet()),
        )
        .unwrap();
    }

    #[test]
    fn test_decrease_fraction_valid_range() {
        // Test boundary values
        for fraction in [0.0, 0.5, 1.0] {
            let node_config = NodeConfig {
                consensus: ConsensusConfig {
                    quorum_store: QuorumStoreConfig {
                        back_pressure: QuorumStoreBackPressureConfig {
                            decrease_fraction: fraction,
                            ..Default::default()
                        },
                        ..Default::default()
                    },
                    ..Default::default()
                },
                ..Default::default()
            };

            // These should all pass validation
            QuorumStoreConfig::sanitize(
                &node_config,
                NodeType::Validator,
                Some(ChainId::testnet()),
            )
            .unwrap();
        }
    }
}
```

## Notes

This vulnerability demonstrates a critical gap in configuration validation that could lead to severe operational issues during network congestionâ€”precisely when the system is most vulnerable. While exploitation requires configuration file access (typically limited to node operators), the issue can manifest through accidental misconfiguration, making it a realistic threat. The lack of any validation creates a fragile system where a single parameter error inverts a critical safety mechanism, potentially cascading into network-wide consensus degradation.

### Citations

**File:** config/src/config/quorum_store_config.rs (L18-27)
```rust
pub struct QuorumStoreBackPressureConfig {
    pub backlog_txn_limit_count: u64,
    pub backlog_per_validator_batch_limit_count: u64,
    pub decrease_duration_ms: u64,
    pub increase_duration_ms: u64,
    pub decrease_fraction: f64,
    pub dynamic_min_txn_per_s: u64,
    pub dynamic_max_txn_per_s: u64,
    pub additive_increase_when_no_backpressure: u64,
}
```

**File:** config/src/config/quorum_store_config.rs (L253-272)
```rust
impl ConfigSanitizer for QuorumStoreConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();

        // Sanitize the send/recv batch limits
        Self::sanitize_send_recv_batch_limits(
            &sanitizer_name,
            &node_config.consensus.quorum_store,
        )?;

        // Sanitize the batch total limits
        Self::sanitize_batch_total_limits(&sanitizer_name, &node_config.consensus.quorum_store)?;

        Ok(())
    }
}
```

**File:** consensus/src/quorum_store/batch_generator.rs (L434-443)
```rust
                    if self.back_pressure.txn_count {
                        // multiplicative decrease, every second
                        if back_pressure_decrease_latest.elapsed() >= back_pressure_decrease_duration {
                            back_pressure_decrease_latest = tick_start;
                            dynamic_pull_txn_per_s = std::cmp::max(
                                (dynamic_pull_txn_per_s as f64 * self.config.back_pressure.decrease_fraction) as u64,
                                self.config.back_pressure.dynamic_min_txn_per_s,
                            );
                            trace!("QS: dynamic_max_pull_txn_per_s: {}", dynamic_pull_txn_per_s);
                        }
```

**File:** consensus/src/quorum_store/proof_manager.rs (L245-265)
```rust
    pub(crate) fn qs_back_pressure(&self) -> BackPressure {
        if self.remaining_total_txn_num > self.back_pressure_total_txn_limit
            || self.remaining_total_proof_num > self.back_pressure_total_proof_limit
        {
            sample!(
                SampleRate::Duration(Duration::from_millis(200)),
                info!(
                    "Quorum store is back pressured with {} txns, limit: {}, proofs: {}, limit: {}",
                    self.remaining_total_txn_num,
                    self.back_pressure_total_txn_limit,
                    self.remaining_total_proof_num,
                    self.back_pressure_total_proof_limit
                );
            );
        }

        BackPressure {
            txn_count: self.remaining_total_txn_num > self.back_pressure_total_txn_limit,
            proof_count: self.remaining_total_proof_num > self.back_pressure_total_proof_limit,
        }
    }
```
