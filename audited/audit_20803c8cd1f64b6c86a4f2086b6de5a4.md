# Audit Report

## Title
Unvalidated QuorumCerts Stored During Fast Forward Sync Leading to Node Liveness Failure

## Summary
During fast-forward sync operations, blocks retrieved from network peers have their embedded QuorumCerts extracted and persisted to storage without cryptographic signature verification. These unvalidated QuorumCerts can later become the node's highest quorum certificate, preventing the node from creating valid timeout messages and participating in consensus.

## Finding Description

The vulnerability exists in the fast-forward sync flow where blocks are retrieved from potentially malicious peers during catch-up operations. The critical flaw is in how QuorumCerts embedded within blocks are handled: [1](#0-0) 

QuorumCerts are extracted from retrieved blocks and combined with the verified highest quorum cert. However, the blocks themselves are retrieved without signature validation: [2](#0-1) 

The `retrieve_blocks` method does not call `BlockRetrievalResponse::verify()`, which would perform signature validation including the embedded QuorumCert validation: [3](#0-2) 

These unvalidated QuorumCerts are then saved to persistent storage: [4](#0-3) 

On node restart, these QuorumCerts are loaded from storage and inserted into the BlockStore without re-verification: [5](#0-4) 

The `insert_single_quorum_cert` method only validates structural consistency, not cryptographic signatures: [6](#0-5) 

When an invalid QC with the highest round becomes the `highest_quorum_cert`, it breaks the node's ability to participate in consensus. Specifically, when creating timeout messages, the invalid QC is embedded in the `TwoChainTimeout`: [7](#0-6) 

SafetyRules then verifies the timeout, which includes verifying the embedded QuorumCert: [8](#0-7) [9](#0-8) 

The verification fails because the QuorumCert has invalid signatures, preventing the node from signing timeouts and participating in consensus.

## Impact Explanation

This vulnerability causes **validator node liveness degradation** qualifying as **High Severity** per Aptos bug bounty criteria:

1. **Validator Node Dysfunction**: Affected nodes cannot create valid timeout messages, preventing them from participating in consensus rounds where they need to timeout
2. **Consensus Participation Loss**: The node becomes unable to properly advance rounds or signal timeouts to peers
3. **Persistent State Corruption**: Invalid QuorumCerts persist in storage across restarts, requiring manual intervention
4. **No Safety Violation**: This does not cause chain splits or consensus safety failures, as other nodes will reject proposals/votes from the affected node

The impact is limited to individual nodes rather than network-wide failure, but it effectively renders validator nodes non-functional until storage is cleaned and the node is resynchronized from trusted peers.

## Likelihood Explanation

**High likelihood** of exploitation:

1. **Attack Prerequisites**:
   - Attacker needs to be a network peer that can respond to `BlockRetrievalRequest` messages
   - Target node must be far enough behind to trigger fast-forward sync
   - No privileged access or validator credentials required

2. **Attack Complexity**: Low
   - Attacker simply responds with valid block structure but forged QuorumCert signatures
   - Structural checks (block IDs, parent relationships) can be satisfied with valid data
   - Only the cryptographic signatures need to be forged (can use dummy/invalid signatures)

3. **Triggering Conditions**: Common
   - Fast-forward sync triggers when nodes fall behind (network partitions, restarts, new nodes joining)
   - Validators regularly query peers for blocks during catch-up

## Recommendation

Add signature verification for blocks retrieved during fast-forward sync. The `BlockRetrievalResponse::verify` method already exists but is not being called. Fix the `retrieve_blocks` method:

```rust
// In retrieve_blocks method, after receiving response:
match response {
    Ok(result) if matches!(result.status(), BlockRetrievalStatus::Succeeded) => {
        // ADD THIS VERIFICATION
        result.verify(request.clone(), validator_verifier)?;
        
        let batch = result.blocks().clone();
        progress += batch.len() as u64;
        last_block_id = batch.last().expect("Batch should not be empty").parent_id();
        result_blocks.extend(batch);
    },
    // ... rest of match arms
}
```

Additionally, consider adding signature verification when loading QuorumCerts from storage during recovery, as a defense-in-depth measure.

## Proof of Concept

**Setup:**
1. Configure malicious peer node that responds to `BlockRetrievalRequest`
2. Prepare target validator node that will fall behind and trigger fast-forward sync

**Attack Steps:**

```rust
// Malicious peer responds to BlockRetrievalRequest with:
// 1. Valid blocks with correct parent-child relationships
// 2. Invalid/forged QuorumCert signatures in each block

fn create_malicious_response(request: BlockRetrievalRequest) -> BlockRetrievalResponse {
    let blocks: Vec<Block> = /* fetch real blocks */;
    
    // Replace QuorumCert signatures with invalid ones
    let poisoned_blocks: Vec<Block> = blocks.iter().map(|block| {
        let mut block_data = block.block_data().clone();
        let forged_qc = create_qc_with_invalid_signatures(block.quorum_cert());
        // Reconstruct block with forged QC
        Block::new_from_data(block_data, forged_qc, None)
    }).collect();
    
    BlockRetrievalResponse::new(BlockRetrievalStatus::Succeeded, poisoned_blocks)
}

fn create_qc_with_invalid_signatures(qc: &QuorumCert) -> QuorumCert {
    let vote_data = qc.vote_data().clone();
    let ledger_info = qc.ledger_info().ledger_info().clone();
    
    // Create aggregate signature with invalid/dummy signatures
    let invalid_sig = AggregateSignature::new(
        qc.ledger_info().signatures().get_signers_bitvec().clone(),
        Some(bls12381::Signature::dummy_signature())
    );
    
    QuorumCert::new(
        vote_data,
        LedgerInfoWithSignatures::new(ledger_info, invalid_sig)
    )
}
```

**Verification:**
1. Target node receives malicious blocks during fast-forward sync
2. Invalid QuorumCerts are extracted and saved to storage (line 503 in sync_manager.rs)
3. Node restarts and loads invalid QuorumCerts from storage
4. When node attempts to timeout (round_manager.rs:1012), it creates `TwoChainTimeout` with invalid QC
5. SafetyRules refuses to sign timeout (safety_rules_2chain.rs:28-30) due to signature verification failure
6. Node cannot participate in consensus timeouts

**Expected Result:** Node becomes unable to properly timeout and advance rounds, demonstrating liveness failure.

---

**Notes**

This vulnerability represents a critical gap in the defense-in-depth strategy for consensus security. While the initial `SyncInfo` containing the highest quorum cert is properly verified (round_manager.rs:888), intermediate blocks fetched to fill the gap are not validated. The assumption that structural consistency checks are sufficient is violated when dealing with potentially malicious peers. The existing `BlockRetrievalResponse::verify` method demonstrates that signature verification was intended but not properly integrated into the fast-forward sync code path.

### Citations

**File:** consensus/src/block_storage/sync_manager.rs (L405-411)
```rust
        let mut quorum_certs = vec![highest_quorum_cert.clone()];
        quorum_certs.extend(
            blocks
                .iter()
                .take(blocks.len() - 1)
                .map(|block| block.quorum_cert().clone()),
        );
```

**File:** consensus/src/block_storage/sync_manager.rs (L503-503)
```rust
        storage.save_tree(blocks.clone(), quorum_certs.clone())?;
```

**File:** consensus/src/block_storage/sync_manager.rs (L790-898)
```rust
    async fn retrieve_blocks(
        &mut self,
        block_id: HashValue,
        target_block_retrieval_payload: TargetBlockRetrieval,
        peers: Vec<AccountAddress>,
        num_blocks: u64,
    ) -> anyhow::Result<Vec<Block>> {
        match &target_block_retrieval_payload {
            TargetBlockRetrieval::TargetBlockId(target_block_id) => {
                info!(
                    "Retrieving {} blocks starting from {} with target_block_id {}",
                    num_blocks, block_id, target_block_id
                );
            },
            TargetBlockRetrieval::TargetRound(target_round) => {
                info!(
                    "Retrieving {} blocks starting from {} with target_round {}",
                    num_blocks, block_id, target_round
                );
            },
        }

        let mut progress = 0;
        let mut last_block_id = block_id;
        let mut result_blocks: Vec<Block> = vec![];
        let mut retrieve_batch_size = self.max_blocks_to_request;
        if peers.is_empty() {
            bail!("Failed to fetch block {}: no peers available", block_id);
        }
        while progress < num_blocks {
            // in case this is the last retrieval
            retrieve_batch_size = min(retrieve_batch_size, num_blocks - progress);

            info!(
                "Retrieving chunk: {} blocks starting from {}, original start {}",
                retrieve_batch_size, last_block_id, block_id
            );

            let response = self
                .retrieve_block_chunk(
                    last_block_id,
                    target_block_retrieval_payload,
                    retrieve_batch_size,
                    peers.clone(),
                )
                .await;
            match response {
                Ok(result) if matches!(result.status(), BlockRetrievalStatus::Succeeded) => {
                    // extend the result blocks
                    let batch = result.blocks().clone();
                    progress += batch.len() as u64;
                    last_block_id = batch.last().expect("Batch should not be empty").parent_id();
                    result_blocks.extend(batch);
                },
                Ok(result)
                    if matches!(result.status(), BlockRetrievalStatus::SucceededWithTarget) =>
                {
                    // if we found the target, end the loop
                    let batch = result.blocks().clone();
                    result_blocks.extend(batch);
                    break;
                },
                res => {
                    bail!(
                        "Failed to fetch block {}, for original start {}, returned status {:?}",
                        last_block_id,
                        block_id,
                        res
                    );
                },
            }
        }

        // Confirm retrieval hit the first block we care about
        assert_eq!(
            result_blocks.first().expect("blocks are empty").id(),
            block_id,
            "Expecting in the retrieval response, first block should be {}, but got {}",
            block_id,
            result_blocks.first().expect("blocks are empty").id(),
        );

        // Confirm retrieval hit the last block/round we care about
        // Slightly different logic if using execution pool and not
        match target_block_retrieval_payload {
            TargetBlockRetrieval::TargetBlockId(target_block_id) => {
                ensure!(
                    result_blocks
                        .last()
                        .expect("Expected at least a result_block")
                        .id()
                        == target_block_id
                );
            },
            TargetBlockRetrieval::TargetRound(target_round) => {
                let last_block = result_blocks.last().expect("blocks are empty");
                ensure!(
                    last_block.round() == target_round || last_block.quorum_cert().certified_block().round() < target_round,
                    "Expecting in the retrieval response, last block should be == {} or its parent should be < {}, but got {} and parent {}",
                    target_round,
                    target_round,
                    last_block.round(),
                    last_block.quorum_cert().certified_block().round(),
                );
            },
        }

        Ok(result_blocks)
    }
```

**File:** consensus/consensus-types/src/block_retrieval.rs (L260-281)
```rust
    pub fn verify(
        &self,
        retrieval_request: BlockRetrievalRequest,
        sig_verifier: &ValidatorVerifier,
    ) -> anyhow::Result<()> {
        self.verify_inner(&retrieval_request)?;

        self.blocks
            .iter()
            .try_fold(retrieval_request.block_id(), |expected_id, block| {
                block.validate_signature(sig_verifier)?;
                block.verify_well_formed()?;
                ensure!(
                    block.id() == expected_id,
                    "blocks doesn't form a chain: expect {}, get {}",
                    expected_id,
                    block.id()
                );
                Ok(block.parent_id())
            })
            .map(|_| ())
    }
```

**File:** consensus/src/block_storage/block_store.rs (L299-305)
```rust
        for qc in quorum_certs {
            block_store
                .insert_single_quorum_cert(qc)
                .unwrap_or_else(|e| {
                    panic!("[BlockStore] failed to insert quorum during build{:?}", e)
                });
        }
```

**File:** consensus/src/block_storage/block_store.rs (L519-556)
```rust
    pub fn insert_single_quorum_cert(&self, qc: QuorumCert) -> anyhow::Result<()> {
        // If the parent block is not the root block (i.e not None), ensure the executed state
        // of a block is consistent with its QuorumCert, otherwise persist the QuorumCert's
        // state and on restart, a new execution will agree with it.  A new execution will match
        // the QuorumCert's state on the next restart will work if there is a memory
        // corruption, for example.
        match self.get_block(qc.certified_block().id()) {
            Some(pipelined_block) => {
                ensure!(
                    // decoupled execution allows dummy block infos
                    pipelined_block
                        .block_info()
                        .match_ordered_only(qc.certified_block()),
                    "QC for block {} has different {:?} than local {:?}",
                    qc.certified_block().id(),
                    qc.certified_block(),
                    pipelined_block.block_info()
                );
                observe_block(
                    pipelined_block.block().timestamp_usecs(),
                    BlockStage::QC_ADDED,
                );
                if pipelined_block.block().is_opt_block() {
                    observe_block(
                        pipelined_block.block().timestamp_usecs(),
                        BlockStage::QC_ADDED_OPT_BLOCK,
                    );
                }
                pipelined_block.set_qc(Arc::new(qc.clone()));
            },
            None => bail!("Insert {} without having the block in store first", qc),
        };

        self.storage
            .save_tree(vec![], vec![qc.clone()])
            .context("Insert block failed when saving quorum")?;
        self.inner.write().insert_quorum_cert(qc)
    }
```

**File:** consensus/src/round_manager.rs (L1009-1013)
```rust
                let timeout = TwoChainTimeout::new(
                    self.epoch_state.epoch,
                    round,
                    self.block_store.highest_quorum_cert().as_ref().clone(),
                );
```

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L27-31)
```rust
        if !self.skip_sig_verify {
            timeout
                .verify(&self.epoch_state()?.verifier)
                .map_err(|e| Error::InvalidTimeout(e.to_string()))?;
        }
```

**File:** consensus/consensus-types/src/timeout_2chain.rs (L74-81)
```rust
    pub fn verify(&self, validators: &ValidatorVerifier) -> anyhow::Result<()> {
        ensure!(
            self.hqc_round() < self.round(),
            "Timeout round should be larger than the QC round"
        );
        self.quorum_cert.verify(validators)?;
        Ok(())
    }
```
