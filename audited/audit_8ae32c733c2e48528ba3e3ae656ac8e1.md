# Audit Report

## Title
Byzantine Validators Can Bypass Peer Health Checks Through Unvalidated Timestamp Manipulation

## Summary
Byzantine validators can serve arbitrary `ledger_timestamp_usecs` values through the peer monitoring service without any validation, allowing them to falsely appear synchronized and avoid being deprioritized as unhealthy peers, even when they are significantly behind in blockchain state.

## Finding Description

The peer monitoring service returns node information including `ledger_timestamp_usecs` from local storage without cryptographic validation. [1](#0-0) 

This timestamp is included in the `NodeInformationResponse` as a plain u64 field with no accompanying proof or signature. [2](#0-1) 

The peer monitoring client accepts this response with minimal validation - only checking the response type, not validating the timestamp value itself. [3](#0-2) 

This timestamp is then used in mempool's peer prioritization logic to determine peer health. The `check_peer_metadata_health` function compares the peer's reported `ledger_timestamp_usecs` against the current time to calculate sync lag. [4](#0-3) 

By default, peers with sync lag exceeding 30 seconds are considered unhealthy. [5](#0-4) 

In the intelligent peer prioritization flow, health is checked **before** latency-based comparison, meaning unhealthy peers are immediately deprioritized regardless of their actual network latency. [6](#0-5) 

**Attack Path:**
1. A Byzantine validator modifies their peer monitoring server code or local storage
2. They return a fake `ledger_timestamp_usecs` value that is recent (within the 30-second threshold)
3. Peer monitoring clients accept this value without validation
4. The Byzantine validator passes the health check despite being far behind in actual sync
5. They are prioritized for transaction broadcasts based on their latency, not their actual sync status
6. They receive transaction traffic they cannot properly process due to being out of sync

**Clarification on "Latency Calculations":** 
The timestamp does NOT directly affect latency measurements - latency is correctly calculated as round-trip time by the client. [7](#0-6)  However, the timestamp affects the health check which acts as a gate before latency-based prioritization occurs, allowing Byzantine validators to avoid being "flagged" and deprioritized.

## Impact Explanation

**Medium Severity** - This vulnerability allows Byzantine validators to:
- Bypass sync health checks and receive preferential treatment in peer selection
- Waste network bandwidth by receiving transaction broadcasts they cannot process
- Potentially cause transaction delivery failures and degraded mempool performance
- Execute a limited resource exhaustion attack against honest nodes

This does not:
- Break consensus safety or liveness
- Enable fund theft or state corruption
- Cause permanent network damage

The impact is limited to network efficiency and peer selection optimization, fitting the Medium severity category of "state inconsistencies requiring intervention" as defined in the Aptos bug bounty program.

## Likelihood Explanation

**High Likelihood** - This attack is trivial to execute:
- Requires only modification of a single validator node (no collusion needed)
- No cryptographic barriers to overcome
- Simple code change or storage manipulation
- Immediately effective upon deployment

A Byzantine validator (< 1/3 of network) can exploit this unilaterally. The attack is deterministic and leaves no trace beyond anomalous metrics.

## Recommendation

**Add cryptographic validation of ledger timestamps:**

The `NodeInformationResponse` should include cryptographic proof that the timestamp comes from a valid, committed `LedgerInfo` signed by 2f+1 validators. Options include:

1. **Include LedgerInfoWithSignatures**: Return the full signed `LedgerInfoWithSignatures` instead of just extracting fields, and verify signatures on the client side.

2. **Add timestamp proofs**: Include a signature or hash chain proof linking the timestamp to a known-good ledger state.

3. **Cross-validate with multiple peers**: Compare timestamps across multiple peers and flag anomalies where a peer claims to be more synced than consensus indicates.

**Immediate mitigation:**
```rust
// In peer-monitoring-service/client/src/peer_states/node_info.rs
fn handle_monitoring_service_response(
    &mut self,
    peer_network_id: &PeerNetworkId,
    _peer_metadata: PeerMetadata,
    _monitoring_service_request: PeerMonitoringServiceRequest,
    monitoring_service_response: PeerMonitoringServiceResponse,
    _response_time_secs: f64,
) {
    let node_info_response = match monitoring_service_response {
        PeerMonitoringServiceResponse::NodeInformation(node_information_response) => {
            node_information_response
        },
        _ => {
            self.handle_request_failure();
            return;
        },
    };

    // ADD: Sanity check timestamp is not in the future
    let current_time_usecs = self.time_service.now_unix_time().as_micros() as u64;
    if node_info_response.ledger_timestamp_usecs > current_time_usecs {
        warn!("Peer reported future timestamp, rejecting");
        self.handle_request_failure();
        return;
    }

    self.record_node_info_response(node_info_response);
}
```

## Proof of Concept

**Demonstration of the vulnerability:**

1. **Setup**: A Byzantine validator node running modified peer monitoring server
2. **Modification**: Change the timestamp return value in `storage.rs`:

```rust
// Modified malicious version
fn get_ledger_timestamp_usecs(&self) -> Result<u64, Error> {
    // Always return a recent timestamp regardless of actual state
    let fake_timestamp = current_time_usecs(); // Always current
    Ok(fake_timestamp)
}
```

3. **Expected behavior**: The Byzantine validator's actual ledger is 300 seconds behind (5 minutes)
4. **Malicious behavior**: Returns a timestamp from 5 seconds ago
5. **Impact on victim nodes**: 
   - Victim calls `check_peer_metadata_health` 
   - Calculates sync lag as 5 seconds (fake_timestamp)
   - Compares against threshold of 30 seconds
   - Peer passes health check and gets prioritized
   - Victim sends transactions to Byzantine validator
   - Byzantine validator cannot properly process them (300 seconds behind)

**Verification**: Monitor mempool metrics showing transaction broadcast failures to the Byzantine validator despite them passing health checks.

## Notes

While the security question mentions "manipulate peer latency calculations," the actual vulnerability is in the **health check mechanism** that precedes latency-based prioritization. The peer latency calculations themselves (round-trip ping measurements) remain accurate and cannot be manipulated through timestamps. However, the timestamp manipulation allows Byzantine validators to bypass the initial health filter, which effectively achieves the stated goal of "avoiding being flagged" and remaining in the prioritized peer set inappropriately.

The vulnerability exists because the peer monitoring protocol treats `NodeInformationResponse` fields as trusted self-reported values without cryptographic validation, violating Byzantine fault tolerance principles that require verification of claims from potentially malicious actors.

### Citations

**File:** peer-monitoring-service/server/src/storage.rs (L50-53)
```rust
    fn get_ledger_timestamp_usecs(&self) -> Result<u64, Error> {
        let latest_ledger_info = self.get_latest_ledger_info()?;
        Ok(latest_ledger_info.timestamp_usecs())
    }
```

**File:** peer-monitoring-service/types/src/response.rs (L94-102)
```rust
#[derive(Clone, Debug, Default, Deserialize, Eq, PartialEq, Serialize)]
pub struct NodeInformationResponse {
    pub build_information: BTreeMap<String, String>, // The build information of the node
    pub highest_synced_epoch: u64,                   // The highest synced epoch of the node
    pub highest_synced_version: u64,                 // The highest synced version of the node
    pub ledger_timestamp_usecs: u64, // The latest timestamp of the blockchain (in microseconds)
    pub lowest_available_version: u64, // The lowest stored version of the node (in storage)
    pub uptime: Duration,            // The amount of time the peer has been running
}
```

**File:** peer-monitoring-service/client/src/peer_states/node_info.rs (L79-106)
```rust
    fn handle_monitoring_service_response(
        &mut self,
        peer_network_id: &PeerNetworkId,
        _peer_metadata: PeerMetadata,
        _monitoring_service_request: PeerMonitoringServiceRequest,
        monitoring_service_response: PeerMonitoringServiceResponse,
        _response_time_secs: f64,
    ) {
        // Verify the response type is valid
        let node_info_response = match monitoring_service_response {
            PeerMonitoringServiceResponse::NodeInformation(node_information_response) => {
                node_information_response
            },
            _ => {
                warn!(LogSchema::new(LogEntry::NodeInfoRequest)
                    .event(LogEvent::ResponseError)
                    .peer(peer_network_id)
                    .message(
                        "An unexpected response was received instead of a node info response!"
                    ));
                self.handle_request_failure();
                return;
            },
        };

        // Store the new latency ping result
        self.record_node_info_response(node_info_response);
    }
```

**File:** mempool/src/shared_mempool/priority.rs (L83-92)
```rust
        // First, compare the peers by health (e.g., sync lag)
        let unhealthy_ordering = compare_peer_health(
            &self.mempool_config,
            &self.time_service,
            monitoring_metadata_a,
            monitoring_metadata_b,
        );
        if !unhealthy_ordering.is_eq() {
            return unhealthy_ordering; // Only return if it's not equal
        }
```

**File:** mempool/src/shared_mempool/priority.rs (L562-589)
```rust
fn check_peer_metadata_health(
    mempool_config: &MempoolConfig,
    time_service: &TimeService,
    monitoring_metadata: &Option<&PeerMonitoringMetadata>,
) -> bool {
    monitoring_metadata
        .and_then(|metadata| {
            metadata
                .latest_node_info_response
                .as_ref()
                .map(|node_information_response| {
                    // Get the peer's ledger timestamp and the current timestamp
                    let peer_ledger_timestamp_usecs =
                        node_information_response.ledger_timestamp_usecs;
                    let current_timestamp_usecs = get_timestamp_now_usecs(time_service);

                    // Calculate the max sync lag before the peer is considered unhealthy (in microseconds)
                    let max_sync_lag_secs =
                        mempool_config.max_sync_lag_before_unhealthy_secs as u64;
                    let max_sync_lag_usecs = max_sync_lag_secs * MICROS_PER_SECOND;

                    // Determine if the peer is healthy
                    current_timestamp_usecs.saturating_sub(peer_ledger_timestamp_usecs)
                        < max_sync_lag_usecs
                })
        })
        .unwrap_or(false) // If metadata is missing, consider the peer unhealthy
}
```

**File:** config/src/config/mempool_config.rs (L56-57)
```rust
    /// The maximum amount of time a node can be out of sync before being considered unhealthy
    pub max_sync_lag_before_unhealthy_secs: usize,
```

**File:** peer-monitoring-service/client/src/peer_states/latency_info.rs (L141-195)
```rust
    fn handle_monitoring_service_response(
        &mut self,
        peer_network_id: &PeerNetworkId,
        _peer_metadata: PeerMetadata,
        monitoring_service_request: PeerMonitoringServiceRequest,
        monitoring_service_response: PeerMonitoringServiceResponse,
        response_time_secs: f64,
    ) {
        // Verify the request type is correctly formed
        let latency_ping_request = match monitoring_service_request {
            PeerMonitoringServiceRequest::LatencyPing(latency_ping_request) => latency_ping_request,
            request => {
                error!(LogSchema::new(LogEntry::LatencyPing)
                    .event(LogEvent::UnexpectedErrorEncountered)
                    .peer(peer_network_id)
                    .request(&request)
                    .message("An unexpected request was sent instead of a latency ping!"));
                self.handle_request_failure(peer_network_id);
                return;
            },
        };

        // Verify the response type is valid
        let latency_ping_response = match monitoring_service_response {
            PeerMonitoringServiceResponse::LatencyPing(latency_ping_response) => {
                latency_ping_response
            },
            _ => {
                warn!(LogSchema::new(LogEntry::LatencyPing)
                    .event(LogEvent::ResponseError)
                    .peer(peer_network_id)
                    .message("An unexpected response was received instead of a latency ping!"));
                self.handle_request_failure(peer_network_id);
                return;
            },
        };

        // Verify the latency ping response contains the correct counter
        let request_ping_counter = latency_ping_request.ping_counter;
        let response_ping_counter = latency_ping_response.ping_counter;
        if request_ping_counter != response_ping_counter {
            warn!(LogSchema::new(LogEntry::LatencyPing)
                .event(LogEvent::PeerPingError)
                .peer(peer_network_id)
                .message(&format!(
                    "Peer responded with the incorrect ping counter! Expected: {:?}, found: {:?}",
                    request_ping_counter, response_ping_counter
                )));
            self.handle_request_failure(peer_network_id);
            return;
        }

        // Store the new latency ping result
        self.record_new_latency_and_reset_failures(request_ping_counter, response_time_secs);
    }
```
