# Audit Report

## Title
Concurrent Backup Compaction Creates Conflicting Metadata Leading to Restore Failure

## Summary
Concurrent execution of `BackupCompactor::run()` without synchronization can create overlapping compacted transaction metadata files with conflicting version range claims. When the restore coordinator loads these files, the deduplication mechanism fails to handle overlapping ranges with different manifests, causing `select_transaction_backups()` to fail with a continuity check error, rendering backups non-restorable.

## Finding Description

The `compact_transaction_backup_range()` function generates compacted metadata files with deterministic filenames based on version ranges. [1](#0-0) 

When multiple `BackupCompactor` instances run concurrently, they can create overlapping compacted files. [2](#0-1) 

The critical issue occurs when different compaction operations include different backup metadata (different manifest handles) for overlapping version ranges. The `MetadataView` constructor performs deduplication but only removes exact duplicates. [3](#0-2) 

Since `TransactionBackupMeta` includes the `manifest` field in its equality comparison, two entries with the same version range but different manifests are NOT considered duplicates. [4](#0-3) 

During restore, `select_transaction_backups()` enforces strict continuity checking. When it encounters two metadata entries for the same version range (e.g., both starting at version 500), the second entry fails the continuity check because `backup.first_version (500) != next_ver (600)`. [5](#0-4) 

This breaks the **Backup/Restore Integrity** invariant: backups must be restorable to recover node state.

## Impact Explanation

**Medium Severity** - This qualifies as "State inconsistencies requiring intervention" per Aptos bug bounty categories.

- **Availability Impact**: Complete failure of restore operations for affected version ranges
- **Recovery Complexity**: Requires manual metadata cleanup by operators  
- **Operational Risk**: Critical during disaster recovery scenarios when backups are needed most
- **Scope**: Affects all nodes attempting to restore from corrupted backup metadata

This does NOT constitute Critical severity because:
- No funds are at risk
- Consensus safety is unaffected
- Running nodes continue operating normally
- No permanent data loss (raw backup data remains intact)

## Likelihood Explanation

**Medium Likelihood**

Occurs when:
1. Multiple `BackupCompactor` instances execute concurrently (automation misconfiguration, manual operator intervention during automated compaction)
2. Compaction operations process overlapping version ranges  
3. The underlying backup metadata differs (re-backup after corruption, multiple backup coordinators, race conditions during backup creation)

The storage implementations handle concurrent writes differently:
- **LocalFs**: Second write skips if file exists, but different filenames for overlapping ranges still get created [6](#0-5) 
- **Cloud Storage**: Overwrites occur, but overlapping non-identical ranges persist [7](#0-6) 

No synchronization mechanism prevents concurrent compactor execution. [8](#0-7) 

## Recommendation

Implement distributed locking for backup compaction operations:

```rust
// Add to BackupCompactor
pub struct BackupCompactor {
    storage: Arc<dyn BackupStorage>,
    // ... existing fields
    lock_handle: Option<FileHandle>,  // Distributed lock
}

impl BackupCompactor {
    pub async fn run(mut self) -> Result<()> {
        // Acquire distributed lock before compaction
        let lock_file = "compaction.lock";
        let lock_content = format!("{{\"timestamp\":{},\"pid\":{}}}", 
            unix_timestamp_sec(), std::process::id());
        
        // Try to acquire lock (create_new semantics)
        match self.storage.acquire_compaction_lock(lock_file, &lock_content).await {
            Ok(handle) => {
                self.lock_handle = Some(handle);
            },
            Err(_) => {
                bail!("Another compaction is in progress. Aborting.");
            }
        }
        
        // Perform compaction...
        let result = self.compact_metadata().await;
        
        // Release lock
        if let Some(handle) = self.lock_handle.take() {
            self.storage.release_compaction_lock(&handle).await?;
        }
        
        result
    }
}
```

Additionally, enhance `select_transaction_backups()` to detect and handle overlapping ranges:

```rust
// In MetadataView::select_transaction_backups()
for backup in self.transaction_backups.iter().sorted() {
    if backup.first_version > target_version {
        break;
    }
    
    // Detect overlapping range
    if backup.first_version < next_ver {
        warn!(
            "Detected overlapping transaction backup range: expected {}, got {}. \
            Multiple manifests exist for same range. Using first encountered.",
            next_ver, backup.first_version
        );
        continue;  // Skip overlapping entry
    }
    
    ensure!(
        backup.first_version == next_ver,
        "Transaction backup ranges not continuous, expecting version {}, got {}.",
        next_ver,
        backup.first_version,
    );
    // ... rest of logic
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_concurrent_compaction_conflict() {
    use aptos_backup_cli::metadata::{Metadata, TransactionBackupMeta};
    use aptos_backup_cli::storage::local_fs::LocalFs;
    
    // Create test storage
    let temp_dir = TempPath::new();
    let storage = Arc::new(LocalFs::new(temp_dir.path().to_path_buf()));
    
    // Create overlapping transaction backups with different manifests
    let meta1 = TransactionBackupMeta {
        first_version: 0,
        last_version: 999,
        manifest: "manifest_A".to_string(),
    };
    
    let meta2 = TransactionBackupMeta {
        first_version: 0,
        last_version: 999,  
        manifest: "manifest_B".to_string(),  // Different manifest!
    };
    
    // Simulate concurrent compaction creating two files
    let (lines1, name1) = Metadata::compact_transaction_backup_range(vec![meta1.clone()]).unwrap();
    let (lines2, name2) = Metadata::compact_transaction_backup_range(vec![meta2.clone()]).unwrap();
    
    // Both create files (different content due to different manifests)
    let _handle1 = storage.save_metadata_lines(&name1, &lines1).await.unwrap();
    let _handle2 = storage.save_metadata_lines(&name2, &lines2).await.unwrap();
    
    // Load metadata view
    let metadata_cache_opt = MetadataCacheOpt::new(Some(temp_dir.path()));
    let metaview = sync_and_load(&metadata_cache_opt, storage, 1).await.unwrap();
    
    // Attempt to select transaction backups - should fail with continuity error
    let result = metaview.select_transaction_backups(0, 999);
    
    assert!(result.is_err());
    assert!(result.unwrap_err().to_string().contains("not continuous"));
}
```

## Notes

This vulnerability requires operator-level access to run `BackupCompactor`, which is typically restricted to trusted infrastructure roles. However, it represents a significant operational risk during disaster recovery scenarios when backup restoration is critical. The lack of synchronization violates the principle of safe concurrent operations in distributed systems.

The issue is exacerbated in cloud storage environments where last-write-wins semantics apply, potentially silently overwriting valid metadata with conflicting entries. Manual intervention to identify and remove conflicting metadata files would be required to restore functionality.

### Citations

**File:** storage/backup/backup-cli/src/metadata/mod.rs (L138-143)
```rust
        let name = format!(
            "transaction_compacted_{}-{}.meta",
            first_version,
            next_version - 1
        );
        Ok((res, name.parse()?))
```

**File:** storage/backup/backup-cli/src/metadata/mod.rs (L191-196)
```rust
#[derive(Clone, Debug, Deserialize, Serialize, Eq, PartialEq, Ord, PartialOrd)]
pub struct TransactionBackupMeta {
    pub first_version: Version,
    pub last_version: Version,
    pub manifest: FileHandle,
}
```

**File:** storage/backup/backup-cli/src/coordinators/backup.rs (L409-475)
```rust
    pub async fn run(self) -> Result<()> {
        info!("Backup compaction started");
        // sync the metadata from backup storage
        let mut metaview = metadata::cache::sync_and_load(
            &self.metadata_cache_opt,
            Arc::clone(&self.storage),
            self.concurrent_downloads,
        )
        .await?;

        let files = metaview.get_file_handles();

        info!("Start compacting backup metadata files.");
        let mut new_files: HashSet<FileHandle> = HashSet::new(); // record overwrite file names
        for range in metaview.compact_epoch_ending_backups(self.epoch_ending_file_compact_factor)? {
            let (epoch_range, file_name) =
                Metadata::compact_epoch_ending_backup_range(range.to_vec())?;
            let file_handle = self
                .storage
                .save_metadata_lines(&file_name, epoch_range.as_slice())
                .await?;
            new_files.insert(file_handle);
        }
        for range in metaview.compact_transaction_backups(self.transaction_file_compact_factor)? {
            let (txn_range, file_name) =
                Metadata::compact_transaction_backup_range(range.to_vec())?;
            let file_handle = self
                .storage
                .save_metadata_lines(&file_name, txn_range.as_slice())
                .await?;
            new_files.insert(file_handle);
        }
        for range in metaview.compact_state_backups(self.state_snapshot_file_compact_factor)? {
            let (state_range, file_name) =
                Metadata::compact_statesnapshot_backup_range(range.to_vec())?;
            let file_handle = self
                .storage
                .save_metadata_lines(&file_name, state_range.as_slice())
                .await?;
            new_files.insert(file_handle);
        }

        // Move expired files to the metadata backup folder
        let (to_move, compaction_meta) =
            self.update_compaction_timestamps(&mut metaview, files, new_files)?;
        for file in to_move {
            info!(file = file, "Backup metadata file.");
            self.storage
                .backup_metadata_file(&file)
                .await
                .map_err(|err| {
                    error!(
                        file = file,
                        error = %err,
                        "Backup metadata file failed, ignoring.",
                    )
                })
                .ok();
        }
        // save the metadata compaction timestamps
        let metadata = Metadata::new_compaction_timestamps(compaction_meta);
        self.storage
            .save_metadata_line(&metadata.name(), &metadata.to_text_line()?)
            .await?;

        Ok(())
    }
```

**File:** storage/backup/backup-cli/src/metadata/view.rs (L45-50)
```rust
        epoch_ending_backups.sort_unstable();
        epoch_ending_backups.dedup();
        state_snapshot_backups.sort_unstable();
        state_snapshot_backups.dedup();
        transaction_backups.sort_unstable();
        transaction_backups.dedup();
```

**File:** storage/backup/backup-cli/src/metadata/view.rs (L145-150)
```rust
            ensure!(
                backup.first_version == next_ver,
                "Transaction backup ranges not continuous, expecting version {}, got {}.",
                next_ver,
                backup.first_version,
            );
```

**File:** storage/backup/backup-cli/src/storage/local_fs/mod.rs (L162-176)
```rust
        let file = OpenOptions::new()
            .write(true)
            .create_new(true)
            .open(&path)
            .await;
        match file {
            Ok(mut f) => {
                f.write_all(content.as_bytes()).await.err_notes(&path)?;
                f.shutdown().await.err_notes(&path)?;
            },
            Err(e) if e.kind() == io::ErrorKind::AlreadyExists => {
                info!("File {} already exists, Skip", name.as_ref());
            },
            _ => bail!("Unexpected Error in saving metadata file {}", name.as_ref()),
        }
```

**File:** storage/backup/backup-cli/src/storage/command_adapter/sample_configs/gcp.sample.yaml (L22-26)
```yaml
  save_metadata_line: |
    FILE_HANDLE="metadata/$FILE_NAME"
    echo "$FILE_HANDLE"
    exec 1>&-
    gzip -c | gsutil -q cp - "gs://$BUCKET/$SUB_DIR/$FILE_HANDLE" > /dev/null
```

**File:** storage/db-tool/src/backup_maintenance.rs (L62-76)
```rust
impl Command {
    pub async fn run(self) -> Result<()> {
        match self {
            Command::Compact(opt) => {
                let compactor = BackupCompactor::new(
                    opt.epoch_ending_file_compact_factor,
                    opt.state_snapshot_file_compact_factor,
                    opt.transaction_file_compact_factor,
                    opt.metadata_cache_opt,
                    opt.storage.init_storage().await?,
                    opt.concurrent_downloads.get(),
                    opt.remove_compacted_file_after,
                );
                compactor.run().await?
            },
```
