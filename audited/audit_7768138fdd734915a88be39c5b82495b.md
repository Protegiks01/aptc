# Audit Report

## Title
Coordinated Chunk Reduction Attack Amplifies Resource Exhaustion via Unlimited Binary Search Retries

## Summary
The storage service uses legacy chunk reduction functions with unlimited binary search retries across four request types. When `enable_size_and_time_aware_chunking` is disabled (the default), attackers can send coordinated requests that trigger extensive retry storms, exhausting the shared 64-thread blocking pool and causing denial-of-service on the storage service.

## Finding Description

The storage service implements four legacy functions with binary search-based chunk reduction patterns: [1](#0-0) [2](#0-1) [3](#0-2) [4](#0-3) 

All four functions use an unbounded `while num_*_to_fetch >= 1` loop that repeatedly halves the chunk size when network frame overflow occurs. Each iteration performs expensive database I/O and full BCS serialization: [5](#0-4) 

The configuration defaults to using these legacy implementations: [6](#0-5) 

When network requests arrive, each spawns a blocking task without per-peer request rate limiting: [7](#0-6) 

The runtime has a fixed limit of 64 blocking threads: [8](#0-7) 

**Attack Scenario:**

1. Attacker identifies versions where data is large (transactions with large events, state values with large blobs, etc.)
2. Sends concurrent requests to all four vulnerable endpoints requesting maximum chunk sizes:
   - State values: 4000 items (MAX_STATE_CHUNK_SIZE)
   - Transactions: 3000 items (MAX_TRANSACTION_CHUNK_SIZE) with `include_events=true`
   - Transaction outputs: 3000 items
   - Epoch endings: 200 items

3. Each request triggers binary search with up to log₂(chunk_size) ≈ 12 iterations
4. Each iteration performs database query + BCS serialization (CPU/IO intensive)
5. All requests compete for the same 64 blocking threads
6. Additional legitimate requests queue indefinitely, causing timeouts

**Amplification Effect:**

Coordinated attacks across multiple request types maximize thread pool saturation. With N concurrent requests per type across 4 types, the system processes 4N slow requests simultaneously, each doing ~12 expensive iterations. This violates the **Resource Limits** invariant: operations must respect computational limits.

## Impact Explanation

This vulnerability qualifies as **High Severity** under "Validator node slowdowns" (up to $50,000). The attack:

- **Degrades storage service availability**: Legitimate state sync requests timeout
- **Affects validator operations**: Nodes cannot efficiently sync state, impacting consensus participation
- **Requires no privileges**: Any network peer can execute the attack
- **Has cascading effects**: If validators fall behind, consensus liveness degrades

While not causing direct fund loss or consensus safety violations, it significantly impacts network availability and validator performance, meeting the High severity threshold.

## Likelihood Explanation

**Likelihood: Medium-High**

The attack is practical because:
- Legacy functions are enabled by default (`enable_size_and_time_aware_chunking: false`)
- No per-peer rate limiting on valid requests exists (RequestModerator only tracks invalid requests)
- Attacker can identify large data versions through normal network observation
- LRU cache bypass is trivial (vary request parameters)
- Coordination across request types is straightforward

The main limiting factor is that the attack requires sustained concurrent connections, but with the 64-thread bottleneck, even moderate attack traffic (10-20 concurrent connections per request type) can cause significant impact.

## Recommendation

**Immediate Fix:** Set `enable_size_and_time_aware_chunking: true` by default to use the newer implementations with bounded retry counts. [9](#0-8) 

Change line 198 from `enable_size_and_time_aware_chunking: false` to `enable_size_and_time_aware_chunking: true`.

**Long-term Fixes:**

1. **Add retry limits to legacy functions** (if they must be kept):
   ```rust
   const MAX_CHUNK_REDUCTION_RETRIES: u64 = 10;
   let mut num_reductions = 0;
   while num_*_to_fetch >= 1 && num_reductions < MAX_CHUNK_REDUCTION_RETRIES {
       // ... existing logic ...
       num_reductions += 1;
   }
   ```

2. **Implement per-peer request rate limiting**:
   Add token bucket rate limiting for valid requests (not just invalid request counting)

3. **Add bounded task queue**:
   Use `BoundedExecutor` instead of raw `spawn_blocking` to prevent unbounded queueing: [10](#0-9) 

## Proof of Concept

```rust
#[tokio::test]
async fn test_coordinated_chunk_reduction_attack() {
    // Setup storage service with default config (legacy mode)
    let mut config = StorageServiceConfig::default();
    assert_eq!(config.enable_size_and_time_aware_chunking, false); // Verify legacy mode
    
    let (storage_service, mock_db) = setup_storage_service(config);
    
    // Populate mock DB with large data at known versions
    populate_large_transactions_with_events(&mock_db, 1000, 2000); // 1000 versions with large events
    populate_large_state_values(&mock_db, 1000, 4000); // Large state values
    
    // Launch coordinated attack: 50 concurrent requests to each endpoint
    let mut handles = vec![];
    
    for request_type in ["state_values", "transactions", "outputs", "epochs"] {
        for _ in 0..50 {
            let request = match request_type {
                "state_values" => create_state_value_request(1000, 0, 4000), // Max chunk
                "transactions" => create_transaction_request(1000, 3000, true), // With events
                "outputs" => create_output_request(1000, 3000),
                "epochs" => create_epoch_request(0, 200),
                _ => unreachable!(),
            };
            
            handles.push(tokio::spawn(async move {
                let start = Instant::now();
                let result = send_storage_request(request).await;
                (start.elapsed(), result)
            }));
        }
    }
    
    // Wait for all requests
    let results: Vec<_> = join_all(handles).await;
    
    // Verify attack amplification:
    // 1. Many requests timeout (> 10 seconds)
    let timeout_count = results.iter()
        .filter(|(duration, _)| *duration > Duration::from_secs(10))
        .count();
    assert!(timeout_count > 100, "Expected significant timeouts, got {}", timeout_count);
    
    // 2. Thread pool saturation metrics show 64 threads busy
    let metrics = get_blocking_thread_metrics();
    assert_eq!(metrics.active_threads, 64, "Expected all threads saturated");
    
    // 3. Binary search retry counters show amplification
    let retry_count = get_chunk_truncation_count();
    assert!(retry_count > 500, "Expected extensive retries, got {}", retry_count);
}
```

## Notes

The vulnerability is particularly concerning because:

1. **Default configuration is vulnerable**: Operators must explicitly enable the safer implementation
2. **Attack coordination is the key**: Single request type attacks are less severe, but coordinated attacks across all four types maximize thread pool exhaustion
3. **No visibility into the issue**: Without specific metrics, operators may not realize they're under attack until validators start falling behind
4. **Affects mainnet**: The default configuration impacts production deployments

The newer size-and-time-aware chunking implementation (`ResponseDataProgressTracker`) avoids this issue by incrementally building responses rather than using binary search retries, making it the recommended solution.

### Citations

**File:** state-sync/storage-service/server/src/storage.rs (L300-344)
```rust
    fn get_epoch_ending_ledger_infos_by_size_legacy(
        &self,
        start_epoch: u64,
        expected_end_epoch: u64,
        mut num_ledger_infos_to_fetch: u64,
        max_response_size: u64,
    ) -> Result<EpochChangeProof, Error> {
        while num_ledger_infos_to_fetch >= 1 {
            // The DbReader interface returns the epochs up to: `end_epoch - 1`.
            // However, we wish to fetch epoch endings up to end_epoch (inclusive).
            let end_epoch = start_epoch
                .checked_add(num_ledger_infos_to_fetch)
                .ok_or_else(|| {
                    Error::UnexpectedErrorEncountered("End epoch has overflown!".into())
                })?;
            let epoch_change_proof = self
                .storage
                .get_epoch_ending_ledger_infos(start_epoch, end_epoch)?;
            if num_ledger_infos_to_fetch == 1 {
                return Ok(epoch_change_proof); // We cannot return less than a single item
            }

            // Attempt to divide up the request if it overflows the message size
            let (overflow_frame, num_bytes) =
                check_overflow_network_frame(&epoch_change_proof, max_response_size)?;
            if !overflow_frame {
                return Ok(epoch_change_proof);
            } else {
                metrics::increment_chunk_truncation_counter(
                    metrics::TRUNCATION_FOR_SIZE,
                    DataResponse::EpochEndingLedgerInfos(epoch_change_proof).get_label(),
                );
                let new_num_ledger_infos_to_fetch = num_ledger_infos_to_fetch / 2;
                debug!("The request for {:?} ledger infos was too large (num bytes: {:?}, limit: {:?}). Retrying with {:?}.",
                    num_ledger_infos_to_fetch, num_bytes, max_response_size, new_num_ledger_infos_to_fetch);
                num_ledger_infos_to_fetch = new_num_ledger_infos_to_fetch; // Try again with half the amount of data
            }
        }

        Err(Error::UnexpectedErrorEncountered(format!(
            "Unable to serve the get_epoch_ending_ledger_infos request! Start epoch: {:?}, \
            expected end epoch: {:?}. The data cannot fit into a single network frame!",
            start_epoch, expected_end_epoch
        )))
    }
```

**File:** state-sync/storage-service/server/src/storage.rs (L515-562)
```rust
    fn get_transactions_with_proof_by_size_legacy(
        &self,
        proof_version: u64,
        start_version: u64,
        end_version: u64,
        mut num_transactions_to_fetch: u64,
        include_events: bool,
        max_response_size: u64,
    ) -> Result<TransactionDataWithProofResponse, Error> {
        while num_transactions_to_fetch >= 1 {
            let transaction_list_with_proof = self.storage.get_transactions(
                start_version,
                num_transactions_to_fetch,
                proof_version,
                include_events,
            )?;
            let response = TransactionDataWithProofResponse {
                transaction_data_response_type: TransactionDataResponseType::TransactionData,
                transaction_list_with_proof: Some(transaction_list_with_proof),
                transaction_output_list_with_proof: None,
            };
            if num_transactions_to_fetch == 1 {
                return Ok(response); // We cannot return less than a single item
            }

            // Attempt to divide up the request if it overflows the message size
            let (overflow_frame, num_bytes) =
                check_overflow_network_frame(&response, max_response_size)?;
            if !overflow_frame {
                return Ok(response);
            } else {
                metrics::increment_chunk_truncation_counter(
                    metrics::TRUNCATION_FOR_SIZE,
                    DataResponse::TransactionDataWithProof(response).get_label(),
                );
                let new_num_transactions_to_fetch = num_transactions_to_fetch / 2;
                debug!("The request for {:?} transactions was too large (num bytes: {:?}, limit: {:?}). Retrying with {:?}.",
                    num_transactions_to_fetch, num_bytes, max_response_size, new_num_transactions_to_fetch);
                num_transactions_to_fetch = new_num_transactions_to_fetch; // Try again with half the amount of data
            }
        }

        Err(Error::UnexpectedErrorEncountered(format!(
            "Unable to serve the get_transactions_with_proof request! Proof version: {:?}, \
            start version: {:?}, end version: {:?}, include events: {:?}. The data cannot fit into \
            a single network frame!",
            proof_version, start_version, end_version, include_events,
        )))
```

**File:** state-sync/storage-service/server/src/storage.rs (L739-783)
```rust
    fn get_transaction_outputs_with_proof_by_size_legacy(
        &self,
        proof_version: u64,
        start_version: u64,
        end_version: u64,
        mut num_outputs_to_fetch: u64,
        max_response_size: u64,
    ) -> Result<TransactionDataWithProofResponse, Error> {
        while num_outputs_to_fetch >= 1 {
            let output_list_with_proof = self.storage.get_transaction_outputs(
                start_version,
                num_outputs_to_fetch,
                proof_version,
            )?;
            let response = TransactionDataWithProofResponse {
                transaction_data_response_type: TransactionDataResponseType::TransactionOutputData,
                transaction_list_with_proof: None,
                transaction_output_list_with_proof: Some(output_list_with_proof),
            };
            if num_outputs_to_fetch == 1 {
                return Ok(response); // We cannot return less than a single item
            }

            // Attempt to divide up the request if it overflows the message size
            let (overflow_frame, num_bytes) =
                check_overflow_network_frame(&response, max_response_size)?;
            if !overflow_frame {
                return Ok(response);
            } else {
                metrics::increment_chunk_truncation_counter(
                    metrics::TRUNCATION_FOR_SIZE,
                    DataResponse::TransactionDataWithProof(response).get_label(),
                );
                let new_num_outputs_to_fetch = num_outputs_to_fetch / 2;
                debug!("The request for {:?} outputs was too large (num bytes: {:?}, limit: {:?}). Retrying with {:?}.",
                    num_outputs_to_fetch, num_bytes, max_response_size, new_num_outputs_to_fetch);
                num_outputs_to_fetch = new_num_outputs_to_fetch; // Try again with half the amount of data
            }
        }

        Err(Error::UnexpectedErrorEncountered(format!(
            "Unable to serve the get_transaction_outputs_with_proof request! Proof version: {:?}, \
            start version: {:?}, end version: {:?}. The data cannot fit into a single network frame!",
            proof_version, start_version, end_version
        )))
```

**File:** state-sync/storage-service/server/src/storage.rs (L991-1032)
```rust
    fn get_state_value_chunk_with_proof_by_size_legacy(
        &self,
        version: u64,
        start_index: u64,
        end_index: u64,
        mut num_state_values_to_fetch: u64,
        max_response_size: u64,
    ) -> Result<StateValueChunkWithProof, Error> {
        while num_state_values_to_fetch >= 1 {
            let state_value_chunk_with_proof = self.storage.get_state_value_chunk_with_proof(
                version,
                start_index as usize,
                num_state_values_to_fetch as usize,
            )?;
            if num_state_values_to_fetch == 1 {
                return Ok(state_value_chunk_with_proof); // We cannot return less than a single item
            }

            // Attempt to divide up the request if it overflows the message size
            let (overflow_frame, num_bytes) =
                check_overflow_network_frame(&state_value_chunk_with_proof, max_response_size)?;
            if !overflow_frame {
                return Ok(state_value_chunk_with_proof);
            } else {
                metrics::increment_chunk_truncation_counter(
                    metrics::TRUNCATION_FOR_SIZE,
                    DataResponse::StateValueChunkWithProof(state_value_chunk_with_proof)
                        .get_label(),
                );
                let new_num_state_values_to_fetch = num_state_values_to_fetch / 2;
                debug!("The request for {:?} state values was too large (num bytes: {:?}, limit: {:?}). Retrying with {:?}.",
                    num_state_values_to_fetch, num_bytes, max_response_size, new_num_state_values_to_fetch);
                num_state_values_to_fetch = new_num_state_values_to_fetch; // Try again with half the amount of data
            }
        }

        Err(Error::UnexpectedErrorEncountered(format!(
            "Unable to serve the get_state_value_chunk_with_proof request! Version: {:?}, \
            start index: {:?}, end index: {:?}. The data cannot fit into a single network frame!",
            version, start_index, end_index
        )))
    }
```

**File:** state-sync/storage-service/server/src/storage.rs (L1499-1508)
```rust
fn check_overflow_network_frame<T: ?Sized + Serialize>(
    data: &T,
    max_network_frame_bytes: u64,
) -> aptos_storage_service_types::Result<(bool, u64), Error> {
    let num_serialized_bytes = bcs::to_bytes(&data)
        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?
        .len() as u64;
    let overflow_frame = num_serialized_bytes >= max_network_frame_bytes;
    Ok((overflow_frame, num_serialized_bytes))
}
```

**File:** config/src/config/state_sync_config.rs (L195-218)
```rust
impl Default for StorageServiceConfig {
    fn default() -> Self {
        Self {
            enable_size_and_time_aware_chunking: false,
            enable_transaction_data_v2: true,
            max_epoch_chunk_size: MAX_EPOCH_CHUNK_SIZE,
            max_invalid_requests_per_peer: 500,
            max_lru_cache_size: 500, // At ~0.6MiB per chunk, this should take no more than 0.5GiB
            max_network_channel_size: 4000,
            max_network_chunk_bytes: SERVER_MAX_MESSAGE_SIZE as u64,
            max_network_chunk_bytes_v2: SERVER_MAX_MESSAGE_SIZE_V2 as u64,
            max_num_active_subscriptions: 30,
            max_optimistic_fetch_period_ms: 5000, // 5 seconds
            max_state_chunk_size: MAX_STATE_CHUNK_SIZE,
            max_storage_read_wait_time_ms: 10_000, // 10 seconds
            max_subscription_period_ms: 30_000,    // 30 seconds
            max_transaction_chunk_size: MAX_TRANSACTION_CHUNK_SIZE,
            max_transaction_output_chunk_size: MAX_TRANSACTION_OUTPUT_CHUNK_SIZE,
            min_time_to_ignore_peers_secs: 300, // 5 minutes
            request_moderator_refresh_interval_ms: 1000, // 1 second
            storage_summary_refresh_interval_ms: 100, // Optimal for <= 10 blocks per second
        }
    }
}
```

**File:** state-sync/storage-service/server/src/lib.rs (L389-419)
```rust
        while let Some(network_request) = self.network_requests.next().await {
            // All handler methods are currently CPU-bound and synchronous
            // I/O-bound, so we want to spawn on the blocking thread pool to
            // avoid starving other async tasks on the same runtime.
            let storage = self.storage.clone();
            let config = self.storage_service_config;
            let cached_storage_server_summary = self.cached_storage_server_summary.clone();
            let optimistic_fetches = self.optimistic_fetches.clone();
            let subscriptions = self.subscriptions.clone();
            let lru_response_cache = self.lru_response_cache.clone();
            let request_moderator = self.request_moderator.clone();
            let time_service = self.time_service.clone();
            self.runtime.spawn_blocking(move || {
                Handler::new(
                    cached_storage_server_summary,
                    optimistic_fetches,
                    lru_response_cache,
                    request_moderator,
                    storage,
                    subscriptions,
                    time_service,
                )
                .process_request_and_respond(
                    config,
                    network_request.peer_network_id,
                    network_request.protocol_id,
                    network_request.storage_service_request,
                    network_request.response_sender,
                );
            });
        }
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-51)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;

    // Verify the given name has an appropriate length
    if thread_name.len() > MAX_THREAD_NAME_LENGTH {
        panic!(
            "The given runtime thread name is too long! Max length: {}, given name: {}",
            MAX_THREAD_NAME_LENGTH, thread_name
        );
    }

    // Create the runtime builder
    let atomic_id = AtomicUsize::new(0);
    let thread_name_clone = thread_name.clone();
    let mut builder = Builder::new_multi_thread();
    builder
        .thread_name_fn(move || {
            let id = atomic_id.fetch_add(1, Ordering::SeqCst);
            format!("{}-{}", thread_name_clone, id)
        })
        .on_thread_start(on_thread_start)
        .disable_lifo_slot()
        // Limit concurrent blocking tasks from spawn_blocking(), in case, for example, too many
        // Rest API calls overwhelm the node.
        .max_blocking_threads(MAX_BLOCKING_THREADS)
        .enable_all();
```

**File:** crates/bounded-executor/src/executor.rs (L1-50)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

//! A bounded tokio [`Handle`]. Only a bounded number of tasks can run
//! concurrently when spawned through this executor, defined by the initial
//! `capacity`.

use futures::future::{Future, FutureExt};
use std::sync::Arc;
use tokio::{
    runtime::Handle,
    sync::{OwnedSemaphorePermit, Semaphore},
    task::JoinHandle,
};

#[derive(Clone, Debug)]
pub struct BoundedExecutor {
    semaphore: Arc<Semaphore>,
    executor: Handle,
}

impl BoundedExecutor {
    /// Create a new `BoundedExecutor` from an existing tokio [`Handle`]
    /// with a maximum concurrent task capacity of `capacity`.
    pub fn new(capacity: usize, executor: Handle) -> Self {
        let semaphore = Arc::new(Semaphore::new(capacity));
        Self {
            semaphore,
            executor,
        }
    }

    async fn acquire_permit(&self) -> OwnedSemaphorePermit {
        self.semaphore.clone().acquire_owned().await.unwrap()
    }

    fn try_acquire_permit(&self) -> Option<OwnedSemaphorePermit> {
        self.semaphore.clone().try_acquire_owned().ok()
    }

    /// Spawn a [`Future`] on the `BoundedExecutor`. This function is async and
    /// will block if the executor is at capacity until one of the other spawned
    /// futures completes. This function returns a [`JoinHandle`] that the caller
    /// can `.await` on for the results of the [`Future`].
    pub async fn spawn<F>(&self, future: F) -> JoinHandle<F::Output>
    where
        F: Future + Send + 'static,
        F::Output: Send + 'static,
    {
        let permit = self.acquire_permit().await;
```
