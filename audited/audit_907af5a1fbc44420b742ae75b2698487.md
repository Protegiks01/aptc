# Audit Report

## Title
Critical Write Set Validation Bypass in Backup Restoration Allows State Corruption

## Summary
The `LoadedChunk::load()` function in the backup restoration system fails to validate that deserialized `WriteSet` objects match their cryptographic commitment (`state_change_hash`) in the corresponding `TransactionInfo`. During KV-only replay mode, these unvalidated write sets are applied directly to the blockchain state, enabling an attacker who controls backup files to inject arbitrary state changes that bypass all consensus and execution validation.

## Finding Description

The vulnerability exists in the backup restoration pipeline where write sets are loaded from potentially untrusted backup storage and applied to state without cryptographic validation.

**The Broken Security Guarantee:**

The Aptos blockchain maintains a cryptographic chain of trust:
1. `LedgerInfo` (signed by validators) → contains transaction accumulator root
2. `TransactionAccumulator` → Merkle tree of `TransactionInfo` hashes
3. `TransactionInfo` → contains `state_change_hash` = `hash(WriteSet)`
4. `WriteSet` → actual state modifications [1](#0-0) 

In `LoadedChunk::load()`, the function:
1. Deserializes write sets from backup files (lines 110-136)
2. Verifies the `TransactionListWithProof` which validates transaction hashes and event hashes (line 167)
3. **FAILS to validate** that the deserialized write sets hash to the `state_change_hash` in `TransactionInfo`
4. Returns these unvalidated write sets for state replay (line 184) [2](#0-1) 

The `TransactionListWithProof::verify()` method only validates:
- Transaction hashes match `TransactionInfo` (lines 2318-2332)
- Event root hashes match `TransactionInfo` (lines 2338-2351)
- **Does NOT validate write set hashes**

In contrast, `TransactionOutputListWithProof::verify()` DOES validate write sets: [3](#0-2) 

**Exploitation Path - KV Replay Mode:** [4](#0-3) 

During KV-only replay (lines 593-599), unvalidated write sets are passed to `save_transactions_and_replay_kv()`: [5](#0-4) 

The write sets are directly applied to state via `StateUpdateRefs::index_write_sets()` and `calculate_state_and_put_updates()` without ANY validation that they match the committed `state_change_hash` values.

**Attack Scenario:**
1. Attacker compromises backup storage (S3 bucket, GCS, etc.) or performs MITM during backup transfer
2. Attacker modifies transaction chunk files to replace legitimate write sets with malicious ones (e.g., empty write sets, or write sets that transfer funds to attacker's account)
3. The `TransactionInfo` objects remain unchanged (they're verified against the signed LedgerInfo)
4. During database restoration with KV replay mode, `LoadedChunk::load()` accepts the malicious write sets
5. These malicious write sets are applied directly to the state database
6. Result: State corruption, different nodes restore to different states, consensus violation

## Impact Explanation

**Critical Severity** - This vulnerability breaks multiple fundamental invariants:

1. **Deterministic Execution Violation**: Different nodes restoring from compromised backups will produce different state roots for identical committed transactions, violating the core consensus requirement.

2. **State Consistency Violation**: State transitions are no longer verifiable via cryptographic proofs. The `state_change_hash` in `TransactionInfo` becomes meaningless if not validated.

3. **Consensus Safety Violation**: If different validators restore from different backup sources (or if an attacker selectively compromises backups for specific validators), they will have divergent state, leading to chain splits or inability to reach consensus.

4. **Potential for Fund Loss**: Malicious write sets could transfer funds, mint tokens, or modify account balances without legitimate transaction execution.

This meets the **Critical Severity** criteria:
- Consensus/Safety violations
- Non-recoverable network partition (different nodes with different states)
- State inconsistencies requiring hardfork to resolve

## Likelihood Explanation

**High Likelihood** given certain operational contexts:

1. **Backup Storage Compromise**: Cloud storage buckets are common attack targets. S3 bucket misconfigurations, leaked credentials, or cloud provider breaches could allow write access.

2. **Supply Chain Attacks**: Compromised backup creation tools or backup transfer infrastructure could inject malicious write sets.

3. **Network-based Attacks**: Without integrity verification, backups transferred over network could be modified in transit.

4. **Operational Reality**: Database restoration from backups is a common operational procedure (disaster recovery, new validator onboarding, state sync).

The attack requires:
- Ability to modify backup files (through storage compromise or MITM)
- Knowledge of backup file format
- Target node performing KV-only replay restoration

No validator consensus participation or insider access is required—only the ability to tamper with backup storage.

## Recommendation

Add cryptographic validation of write sets against `state_change_hash` in `LoadedChunk::load()`:

```rust
impl LoadedChunk {
    async fn load(
        manifest: TransactionChunk,
        storage: &Arc<dyn BackupStorage>,
        epoch_history: Option<&Arc<EpochHistory>>,
    ) -> Result<Self> {
        // ... existing deserialization code ...
        
        // AFTER line 167 (after txn_list_with_proof.verify()):
        
        // Validate write sets against state_change_hash in TransactionInfo
        for (idx, (write_set, txn_info)) in write_sets.iter()
            .zip(txn_infos.iter())
            .enumerate() 
        {
            let write_set_hash = CryptoHash::hash(write_set);
            ensure!(
                write_set_hash == txn_info.state_change_hash(),
                "Write set validation failed at index {}: write_set_hash={:?}, expected={:?}",
                idx,
                write_set_hash,
                txn_info.state_change_hash()
            );
        }
        
        Ok(Self {
            manifest,
            txns,
            persisted_aux_info,
            txn_infos,
            event_vecs,
            range_proof,
            write_sets,
        })
    }
}
```

Additional hardening:
1. Use `TransactionOutputListWithProof` instead of `TransactionListWithProof` when write sets are present
2. Add integrity checking (HMAC/signatures) to backup storage
3. Document that backup storage must be trusted/authenticated

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use aptos_crypto::{hash::CryptoHash, HashValue};
    use aptos_types::{
        transaction::{Transaction, TransactionInfo, WriteSet},
        write_set::WriteSetMut,
    };

    #[test]
    fn test_unvalidated_writeset_injection() {
        // Step 1: Create legitimate transaction with write set
        let legitimate_write_set = WriteSet::new(vec![]).unwrap();
        let legitimate_hash = CryptoHash::hash(&legitimate_write_set);
        
        // Step 2: Create TransactionInfo with correct hash
        let txn_info = TransactionInfo::new(
            HashValue::random(),
            legitimate_hash, // state_change_hash
            HashValue::random(),
            None,
            100,
            ExecutionStatus::Success,
            HashValue::random(),
        );
        
        // Step 3: Create malicious write set (different from legitimate)
        let malicious_write_set = WriteSet::new(vec![
            // Attacker's malicious state changes
        ]).unwrap();
        let malicious_hash = CryptoHash::hash(&malicious_write_set);
        
        // Step 4: Verify hashes don't match
        assert_ne!(legitimate_hash, malicious_hash);
        
        // Step 5: Current code would accept malicious_write_set
        // because LoadedChunk::load() never validates it!
        // The vulnerability allows this to be applied to state during KV replay.
        
        // With the fix, this should fail:
        // ensure!(malicious_hash == txn_info.state_change_hash());
    }
}
```

**Reproduction Steps:**
1. Set up backup restoration environment with KV replay mode enabled
2. Create backup files with modified write sets (empty or with malicious state changes)
3. Keep TransactionInfo and LedgerInfo unchanged (so signature verification passes)
4. Trigger database restoration from the compromised backup
5. Observe that malicious write sets are applied to state without validation
6. Verify resulting state differs from legitimate blockchain state

## Notes

The vulnerability is particularly severe because:
- The cryptographic commitment (`state_change_hash`) already exists in `TransactionInfo`
- The validation code already exists in `TransactionOutputListWithProof::verify()` 
- The missing validation appears to be an oversight rather than intentional design
- KV replay mode is designed for performance (skipping re-execution), but this creates a critical security gap when combined with unvalidated write sets from untrusted sources

### Citations

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L100-186)
```rust
    async fn load(
        manifest: TransactionChunk,
        storage: &Arc<dyn BackupStorage>,
        epoch_history: Option<&Arc<EpochHistory>>,
    ) -> Result<Self> {
        let mut file = BufReader::new(storage.open_for_read(&manifest.transactions).await?);
        let mut txns = Vec::new();
        let mut persisted_aux_info = Vec::new();
        let mut txn_infos = Vec::new();
        let mut event_vecs = Vec::new();
        let mut write_sets = Vec::new();

        while let Some(record_bytes) = file.read_record_bytes().await? {
            let (txn, aux_info, txn_info, events, write_set): (
                _,
                PersistedAuxiliaryInfo,
                _,
                _,
                WriteSet,
            ) = match manifest.format {
                TransactionChunkFormat::V0 => {
                    let (txn, txn_info, events, write_set) = bcs::from_bytes(&record_bytes)?;
                    (
                        txn,
                        PersistedAuxiliaryInfo::None,
                        txn_info,
                        events,
                        write_set,
                    )
                },
                TransactionChunkFormat::V1 => bcs::from_bytes(&record_bytes)?,
            };
            txns.push(txn);
            persisted_aux_info.push(aux_info);
            txn_infos.push(txn_info);
            event_vecs.push(events);
            write_sets.push(write_set);
        }

        ensure!(
            manifest.first_version + (txns.len() as Version) == manifest.last_version + 1,
            "Number of items in chunks doesn't match that in manifest. first_version: {}, last_version: {}, items in chunk: {}",
            manifest.first_version,
            manifest.last_version,
            txns.len(),
        );

        let (range_proof, ledger_info) = storage
            .load_bcs_file::<(TransactionAccumulatorRangeProof, LedgerInfoWithSignatures)>(
                &manifest.proof,
            )
            .await?;
        if let Some(epoch_history) = epoch_history {
            epoch_history.verify_ledger_info(&ledger_info)?;
        }

        // make a `TransactionListWithProof` to reuse its verification code.
        let txn_list_with_proof =
            TransactionListWithProofV2::new(TransactionListWithAuxiliaryInfos::new(
                TransactionListWithProof::new(
                    txns,
                    Some(event_vecs),
                    Some(manifest.first_version),
                    TransactionInfoListWithProof::new(range_proof, txn_infos),
                ),
                persisted_aux_info,
            ));
        txn_list_with_proof.verify(ledger_info.ledger_info(), Some(manifest.first_version))?;
        // and disassemble it to get things back.
        let (txn_list_with_proof, persisted_aux_info) = txn_list_with_proof.into_parts();
        let txns = txn_list_with_proof.transactions;
        let range_proof = txn_list_with_proof
            .proof
            .ledger_info_to_transaction_infos_proof;
        let txn_infos = txn_list_with_proof.proof.transaction_infos;
        let event_vecs = txn_list_with_proof.events.expect("unknown to be Some.");

        Ok(Self {
            manifest,
            txns,
            persisted_aux_info,
            txn_infos,
            event_vecs,
            range_proof,
            write_sets,
        })
    }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L553-637)
```rust
    // only apply KV to the DB
    async fn replay_kv(
        &self,
        restore_handler: &RestoreHandler,
        txns_to_execute_stream: impl Stream<
            Item = Result<(
                Transaction,
                PersistedAuxiliaryInfo,
                TransactionInfo,
                WriteSet,
                Vec<ContractEvent>,
            )>,
        >,
    ) -> Result<()> {
        let (first_version, _) = self.replay_from_version.unwrap();
        restore_handler.force_state_version_for_kv_restore(first_version.checked_sub(1))?;

        let mut base_version = first_version;
        let mut offset = 0u64;
        let replay_start = Instant::now();
        let arc_restore_handler = Arc::new(restore_handler.clone());

        let db_commit_stream = txns_to_execute_stream
            .try_chunks(BATCH_SIZE)
            .err_into::<anyhow::Error>()
            .map_ok(|chunk| {
                let (txns, persisted_aux_info, txn_infos, write_sets, events): (
                    Vec<_>,
                    Vec<_>,
                    Vec<_>,
                    Vec<_>,
                    Vec<_>,
                ) = chunk.into_iter().multiunzip();
                let handler = arc_restore_handler.clone();
                base_version += offset;
                offset = txns.len() as u64;
                async move {
                    let _timer = OTHER_TIMERS_SECONDS.timer_with(&["replay_txn_chunk_kv_only"]);
                    tokio::task::spawn_blocking(move || {
                        // we directly save transaction and kvs to DB without involving chunk executor
                        handler.save_transactions_and_replay_kv(
                            base_version,
                            &txns,
                            &persisted_aux_info,
                            &txn_infos,
                            &events,
                            write_sets,
                        )?;
                        // return the last version after the replaying
                        Ok(base_version + offset - 1)
                    })
                    .err_into::<anyhow::Error>()
                    .await
                }
            })
            .try_buffered_x(self.global_opt.concurrent_downloads, 1)
            .and_then(future::ready);

        let total_replayed = db_commit_stream
            .and_then(|version| async move {
                let _timer = OTHER_TIMERS_SECONDS.timer_with(&["commit_txn_chunk_kv_only"]);
                tokio::task::spawn_blocking(move || {
                    // version is the latest version finishing the KV replaying
                    let total_replayed = version - first_version;
                    TRANSACTION_REPLAY_VERSION.set(version as i64);
                    info!(
                        version = version,
                        accumulative_tps =
                            (total_replayed as f64 / replay_start.elapsed().as_secs_f64()) as u64,
                        "KV replayed."
                    );
                    Ok(version)
                })
                .await?
            })
            .try_fold(0, |_total, total| future::ok(total))
            .await?;
        info!(
            total_replayed = total_replayed,
            accumulative_tps =
                (total_replayed as f64 / replay_start.elapsed().as_secs_f64()) as u64,
            "KV Replay finished."
        );
        Ok(())
    }
```

**File:** types/src/transaction/mod.rs (L2300-2354)
```rust
        // Verify the first transaction versions match
        ensure!(
            self.get_first_transaction_version() == first_transaction_version,
            "First transaction version ({:?}) doesn't match given version ({:?}).",
            self.get_first_transaction_version(),
            first_transaction_version,
        );

        // Verify the lengths of the transactions and transaction infos match
        ensure!(
            self.proof.transaction_infos.len() == self.get_num_transactions(),
            "The number of TransactionInfo objects ({}) does not match the number of \
             transactions ({}).",
            self.proof.transaction_infos.len(),
            self.get_num_transactions(),
        );

        // Verify the transaction hashes match those of the transaction infos
        self.transactions
            .par_iter()
            .zip_eq(self.proof.transaction_infos.par_iter())
            .map(|(txn, txn_info)| {
                let txn_hash = CryptoHash::hash(txn);
                ensure!(
                    txn_hash == txn_info.transaction_hash(),
                    "The hash of transaction does not match the transaction info in proof. \
                     Transaction hash: {:x}. Transaction hash in txn_info: {:x}.",
                    txn_hash,
                    txn_info.transaction_hash(),
                );
                Ok(())
            })
            .collect::<Result<Vec<_>>>()?;

        // Verify the transaction infos are proven by the ledger info.
        self.proof
            .verify(ledger_info, self.get_first_transaction_version())?;

        // Verify the events if they exist.
        if let Some(event_lists) = &self.events {
            ensure!(
                event_lists.len() == self.get_num_transactions(),
                "The length of event_lists ({}) does not match the number of transactions ({}).",
                event_lists.len(),
                self.get_num_transactions(),
            );
            event_lists
                .into_par_iter()
                .zip_eq(self.proof.transaction_infos.par_iter())
                .map(|(events, txn_info)| verify_events_against_root_hash(events, txn_info))
                .collect::<Result<Vec<_>>>()?;
        }

        Ok(())
    }
```

**File:** types/src/transaction/mod.rs (L2578-2586)
```rust
            // Verify the write set matches for both the transaction info and output
            let write_set_hash = CryptoHash::hash(&txn_output.write_set);
            ensure!(
                txn_info.state_change_hash() == write_set_hash,
                "The write set in transaction output does not match the transaction info \
                     in proof. Hash of write set in transaction output: {}. Write set hash in txn_info: {}.",
                write_set_hash,
                txn_info.state_change_hash(),
            );
```

**File:** storage/aptosdb/src/backup/restore_utils.rs (L269-277)
```rust
    if kv_replay && first_version > 0 && state_store.get_usage(Some(first_version - 1)).is_ok() {
        let (ledger_state, _hot_state_updates) = state_store.calculate_state_and_put_updates(
            &StateUpdateRefs::index_write_sets(first_version, write_sets, write_sets.len(), vec![]),
            &mut ledger_db_batch.ledger_metadata_db_batches, // used for storing the storage usage
            state_kv_batches,
        )?;
        // n.b. ideally this is set after the batches are committed
        state_store.set_state_ignoring_summary(ledger_state);
    }
```
