# Audit Report

## Title
State Restore Accepts Unverified Chunks in KvOnly Mode Leading to State Corruption

## Summary
The `add_chunk()` function in the state snapshot restore process completely bypasses `SparseMerkleRangeProof` verification when operating in `StateSnapshotRestoreMode::KvOnly` mode. This allows malicious actors controlling backup storage or restore parameters to inject arbitrary state data without cryptographic proof that it belongs to the `expected_root_hash`, breaking critical state consistency invariants.

## Finding Description

The vulnerability exists in the state restore implementation's handling of different restore modes. When `add_chunk()` is called with `StateSnapshotRestoreMode::KvOnly`, the provided `SparseMerkleRangeProof` is completely ignored and no verification occurs. [1](#0-0) 

In the vulnerable code path, when `restore_mode` is `KvOnly`, only `kv_fn()` is executed, which calls `StateValueRestore::add_chunk()` without passing the proof parameter: [2](#0-1) 

The `StateValueRestore::add_chunk()` method accepts chunks without any proof verification - it only loads progress, skips overlaps, and writes data directly to the database.

In contrast, when `TreeOnly` or `Default` modes are used, the `tree_fn()` is invoked, which properly calls `add_chunk_impl()` with cryptographic verification: [3](#0-2) 

The critical verification happens at line 391 where `self.verify(proof)?` is called, performing cryptographic validation: [4](#0-3) 

This verification computes the Merkle root from the chunk data and siblings, then validates it matches `expected_root_hash`: [5](#0-4) 

**Attack Scenario:**

The `KvOnly` mode is actively used in production restore workflows: [6](#0-5) 

An attacker who can manipulate backup storage or restore CLI parameters can:

1. Provide malicious chunks with invalid/absent proofs when restoring with `KvOnly` mode
2. The malicious state data is written to the database without any verification against `expected_root_hash`
3. Even if `TreeOnly` mode is later used to build the Merkle tree correctly, the actual key-value data remains corrupted
4. This creates a state inconsistency where queries return incorrect values that don't match the Merkle tree proofs

The restore process loads chunks and proofs from backup storage, then calls `add_chunk()`: [7](#0-6) 

## Impact Explanation

**Severity: CRITICAL** (up to $1,000,000)

This vulnerability breaks multiple critical invariants:

1. **State Consistency Violation**: State data is accepted without cryptographic proof of validity, violating the invariant that "State transitions must be atomic and verifiable via Merkle proofs"

2. **Deterministic Execution Failure**: Different nodes restoring from manipulated backups will have different state values, breaking the invariant that "All validators must produce identical state roots for identical blocks"

3. **Consensus Safety Risk**: Nodes with corrupted state will produce different execution results, potentially causing consensus splits or forks

4. **State Integrity Compromise**: The fundamental security guarantee that all state data is authenticated by the Merkle root hash is violated

This qualifies as Critical severity because it enables:
- **Consensus/Safety violations**: Nodes can have divergent state leading to network partition
- **State corruption**: Arbitrary state manipulation without cryptographic proof
- **Potential hardfork requirement**: If widespread state corruption occurs, recovery may require coordinated intervention

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability is highly likely to be exploitable because:

1. **Active Production Usage**: The `KvOnly` mode is actively used in the restore coordinator's Phase 1, not a dormant code path

2. **Clear Attack Vector**: An attacker who compromises backup storage infrastructure or manipulates restore CLI parameters can directly exploit this

3. **No Additional Barriers**: No additional security checks prevent this bypass - the proof verification is simply not performed

4. **Practical Impact**: State snapshots are critical for node bootstrapping and disaster recovery, making this a high-value target

5. **Restore Mode Configuration**: The restore mode is controlled via CLI arguments and can be influenced by operational procedures or configuration files [8](#0-7) 

## Recommendation

**Immediate Fix**: Always verify `SparseMerkleRangeProof` regardless of restore mode. The proof verification is essential for state integrity and should never be bypassed.

**Code Fix**:

Modify the `add_chunk()` implementation to always perform proof verification before accepting chunks. One approach is to call `tree_fn()` for verification even in `KvOnly` mode, but skip the tree writing:

```rust
fn add_chunk(&mut self, chunk: Vec<(K, V)>, proof: SparseMerkleRangeProof) -> Result<()> {
    // ALWAYS verify the proof first, regardless of mode
    let proof_verified = self.tree_restore
        .lock()
        .as_mut()
        .unwrap()
        .verify_chunk_proof(chunk.iter().map(|(k, v)| (k, v.hash())).collect(), proof)?;
    
    let kv_fn = || {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_value_add_chunk"]);
        self.kv_restore
            .lock()
            .as_mut()
            .unwrap()
            .add_chunk(chunk.clone())
    };

    let tree_fn = || {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["jmt_add_chunk"]);
        self.tree_restore
            .lock()
            .as_mut()
            .unwrap()
            .add_chunk_impl(chunk.iter().map(|(k, v)| (k, v.hash())).collect(), proof)
    };
    
    match self.restore_mode {
        StateSnapshotRestoreMode::KvOnly => kv_fn()?,
        StateSnapshotRestoreMode::TreeOnly => tree_fn()?,
        StateSnapshotRestoreMode::Default => {
            let (r1, r2) = IO_POOL.join(kv_fn, tree_fn);
            r1?;
            r2?;
        },
    }

    Ok(())
}
```

Alternatively, add a verification-only method to `JellyfishMerkleRestore` that verifies proofs without modifying state, and call it before `kv_fn()` in `KvOnly` mode.

**Additional Hardening**:
1. Add integrity checks after restore completion to verify KV data matches tree structure
2. Implement mandatory proof verification at the protocol level
3. Add monitoring/alerting for restore operations with anomalous patterns
4. Consider requiring cryptographic signatures on backup manifests

## Proof of Concept

**Reproduction Steps:**

1. Set up a state snapshot restore with `StateSnapshotRestoreMode::KvOnly`
2. Provide malicious chunks with arbitrary data and invalid/empty proofs
3. Observe that the chunks are accepted without verification
4. The corrupted state data is written to the database

**Rust Test Case:**

```rust
#[test]
fn test_kvonly_accepts_invalid_proof() {
    use aptos_crypto::HashValue;
    use aptos_types::proof::SparseMerkleRangeProof;
    
    // Create a restore instance with KvOnly mode
    let expected_root_hash = HashValue::random();
    let mut restore = StateSnapshotRestore::new(
        &tree_store,
        &value_store,
        version,
        expected_root_hash,
        false,
        StateSnapshotRestoreMode::KvOnly,
    ).unwrap();
    
    // Create malicious chunk with arbitrary data
    let malicious_chunk = vec![
        (StateKey::random(), StateValue::random()),
    ];
    
    // Create invalid proof (empty siblings)
    let invalid_proof = SparseMerkleRangeProof::new(vec![]);
    
    // This should fail but will succeed in KvOnly mode!
    let result = restore.add_chunk(malicious_chunk, invalid_proof);
    
    // BUG: This succeeds even though proof doesn't verify against expected_root_hash
    assert!(result.is_ok());
}
```

This test demonstrates that `KvOnly` mode accepts chunks without verifying the proof cryptographically validates the data against `expected_root_hash`, allowing arbitrary state corruption.

## Notes

This is a critical design flaw where performance optimization (separating KV and tree restore into phases) inadvertently created a security vulnerability by bypassing essential cryptographic verification. The `SparseMerkleRangeProof` verification is not optional - it is the fundamental security mechanism ensuring state integrity in Aptos. Any code path that bypasses this verification fundamentally breaks the security model of the system.

The vulnerability is particularly dangerous because it's in operational tooling (backup/restore) that is often trusted and may not receive the same security scrutiny as consensus or execution code, yet it can completely compromise state integrity.

### Citations

**File:** storage/aptosdb/src/state_restore/mod.rs (L88-127)
```rust
    pub fn add_chunk(&mut self, mut chunk: Vec<(K, V)>) -> Result<()> {
        // load progress
        let progress_opt = self.db.get_progress(self.version)?;

        // skip overlaps
        if let Some(progress) = progress_opt {
            let idx = chunk
                .iter()
                .position(|(k, _v)| CryptoHash::hash(k) > progress.key_hash)
                .unwrap_or(chunk.len());
            chunk = chunk.split_off(idx);
        }

        // quit if all skipped
        if chunk.is_empty() {
            return Ok(());
        }

        // save
        let mut usage = progress_opt.map_or(StateStorageUsage::zero(), |p| p.usage);
        let (last_key, _last_value) = chunk.last().unwrap();
        let last_key_hash = CryptoHash::hash(last_key);

        // In case of TreeOnly Restore, we only restore the usage of KV without actually writing KV into DB
        for (k, v) in chunk.iter() {
            usage.add_item(k.key_size() + v.value_size());
        }

        // prepare the sharded kv batch
        let kv_batch: StateValueBatch<K, Option<V>> = chunk
            .into_iter()
            .map(|(k, v)| ((k, self.version), Some(v)))
            .collect();

        self.db.write_kv_batch(
            self.version,
            &kv_batch,
            StateSnapshotProgress::new(last_key_hash, usage),
        )
    }
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L228-258)
```rust
    fn add_chunk(&mut self, chunk: Vec<(K, V)>, proof: SparseMerkleRangeProof) -> Result<()> {
        let kv_fn = || {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_value_add_chunk"]);
            self.kv_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk(chunk.clone())
        };

        let tree_fn = || {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["jmt_add_chunk"]);
            self.tree_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk_impl(chunk.iter().map(|(k, v)| (k, v.hash())).collect(), proof)
        };
        match self.restore_mode {
            StateSnapshotRestoreMode::KvOnly => kv_fn()?,
            StateSnapshotRestoreMode::TreeOnly => tree_fn()?,
            StateSnapshotRestoreMode::Default => {
                // We run kv_fn with TreeOnly to restore the usage of DB
                let (r1, r2) = IO_POOL.join(kv_fn, tree_fn);
                r1?;
                r2?;
            },
        }

        Ok(())
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L339-413)
```rust
    pub fn add_chunk_impl(
        &mut self,
        mut chunk: Vec<(&K, HashValue)>,
        proof: SparseMerkleRangeProof,
    ) -> Result<()> {
        if self.finished {
            info!("State snapshot restore already finished, ignoring entire chunk.");
            return Ok(());
        }

        if let Some(prev_leaf) = &self.previous_leaf {
            let skip_until = chunk
                .iter()
                .find_position(|(key, _hash)| key.hash() > *prev_leaf.account_key());
            chunk = match skip_until {
                None => {
                    info!("Skipping entire chunk.");
                    return Ok(());
                },
                Some((0, _)) => chunk,
                Some((num_to_skip, next_leaf)) => {
                    info!(
                        num_to_skip = num_to_skip,
                        next_leaf = next_leaf,
                        "Skipping leaves."
                    );
                    chunk.split_off(num_to_skip)
                },
            }
        };
        if chunk.is_empty() {
            return Ok(());
        }

        for (key, value_hash) in chunk {
            let hashed_key = key.hash();
            if let Some(ref prev_leaf) = self.previous_leaf {
                ensure!(
                    &hashed_key > prev_leaf.account_key(),
                    "State keys must come in increasing order.",
                )
            }
            self.previous_leaf.replace(LeafNode::new(
                hashed_key,
                value_hash,
                (key.clone(), self.version),
            ));
            self.add_one(key, value_hash);
            self.num_keys_received += 1;
        }

        // Verify what we have added so far is all correct.
        self.verify(proof)?;

        // Write the frozen nodes to storage.
        if self.async_commit {
            self.wait_for_async_commit()?;
            let (tx, rx) = channel();
            self.async_commit_result = Some(rx);

            let mut frozen_nodes = HashMap::new();
            std::mem::swap(&mut frozen_nodes, &mut self.frozen_nodes);
            let store = self.store.clone();

            IO_POOL.spawn(move || {
                let res = store.write_node_batch(&frozen_nodes);
                tx.send(res).unwrap();
            });
        } else {
            self.store.write_node_batch(&self.frozen_nodes)?;
            self.frozen_nodes.clear();
        }

        Ok(())
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L624-697)
```rust
    /// Verifies that all states that have been added so far (from the leftmost one to
    /// `self.previous_leaf`) are correct, i.e., we are able to construct `self.expected_root_hash`
    /// by combining all existing states and `proof`.
    #[allow(clippy::collapsible_if)]
    fn verify(&self, proof: SparseMerkleRangeProof) -> Result<()> {
        let previous_leaf = self
            .previous_leaf
            .as_ref()
            .expect("The previous leaf must exist.");

        let previous_key = previous_leaf.account_key();
        // If we have all siblings on the path from root to `previous_key`, we should be able to
        // compute the root hash. The siblings on the right are already in the proof. Now we
        // compute the siblings on the left side, which represent all the states that have ever
        // been added.
        let mut left_siblings = vec![];

        // The following process might add some extra placeholder siblings on the left, but it is
        // nontrivial to determine when the loop should stop. So instead we just add these
        // siblings for now and get rid of them in the next step.
        let mut num_visited_right_siblings = 0;
        for (i, bit) in previous_key.iter_bits().enumerate() {
            if bit {
                // This node is a right child and there should be a sibling on the left.
                let sibling = if i >= self.partial_nodes.len() * 4 {
                    *SPARSE_MERKLE_PLACEHOLDER_HASH
                } else {
                    Self::compute_left_sibling(
                        &self.partial_nodes[i / 4],
                        previous_key.get_nibble(i / 4),
                        (3 - i % 4) as u8,
                    )
                };
                left_siblings.push(sibling);
            } else {
                // This node is a left child and there should be a sibling on the right.
                num_visited_right_siblings += 1;
            }
        }
        ensure!(
            num_visited_right_siblings >= proof.right_siblings().len(),
            "Too many right siblings in the proof.",
        );

        // Now we remove any extra placeholder siblings at the bottom. We keep removing the last
        // sibling if 1) it's a placeholder 2) it's a sibling on the left.
        for bit in previous_key.iter_bits().rev() {
            if bit {
                if *left_siblings.last().expect("This sibling must exist.")
                    == *SPARSE_MERKLE_PLACEHOLDER_HASH
                {
                    left_siblings.pop();
                } else {
                    break;
                }
            } else if num_visited_right_siblings > proof.right_siblings().len() {
                num_visited_right_siblings -= 1;
            } else {
                break;
            }
        }

        // Left siblings must use the same ordering as the right siblings in the proof
        left_siblings.reverse();

        // Verify the proof now that we have all the siblings
        proof
            .verify(
                self.expected_root_hash,
                SparseMerkleLeafNode::new(*previous_key, previous_leaf.value_hash()),
                left_siblings,
            )
            .map_err(Into::into)
    }
```

**File:** types/src/proof/definition.rs (L782-826)
```rust
    pub fn verify(
        &self,
        expected_root_hash: HashValue,
        rightmost_known_leaf: SparseMerkleLeafNode,
        left_siblings: Vec<HashValue>,
    ) -> Result<()> {
        let num_siblings = left_siblings.len() + self.right_siblings.len();
        let mut left_sibling_iter = left_siblings.iter();
        let mut right_sibling_iter = self.right_siblings().iter();

        let mut current_hash = rightmost_known_leaf.hash();
        for bit in rightmost_known_leaf
            .key()
            .iter_bits()
            .rev()
            .skip(HashValue::LENGTH_IN_BITS - num_siblings)
        {
            let (left_hash, right_hash) = if bit {
                (
                    *left_sibling_iter
                        .next()
                        .ok_or_else(|| format_err!("Missing left sibling."))?,
                    current_hash,
                )
            } else {
                (
                    current_hash,
                    *right_sibling_iter
                        .next()
                        .ok_or_else(|| format_err!("Missing right sibling."))?,
                )
            };
            current_hash = SparseMerkleInternalNode::new(left_hash, right_hash).hash();
        }

        ensure!(
            current_hash == expected_root_hash,
            "{}: Root hashes do not match. Actual root hash: {:x}. Expected root hash: {:x}.",
            type_name::<Self>(),
            current_hash,
            expected_root_hash,
        );

        Ok(())
    }
```

**File:** storage/backup/backup-cli/src/coordinators/restore.rs (L242-260)
```rust
            // phase 1.a: restore the kv snapshot
            if kv_snapshot.is_some() {
                let kv_snapshot = kv_snapshot.clone().unwrap();
                info!("Start restoring KV snapshot at {}", kv_snapshot.version);

                StateSnapshotRestoreController::new(
                    StateSnapshotRestoreOpt {
                        manifest_handle: kv_snapshot.manifest,
                        version: kv_snapshot.version,
                        validate_modules: false,
                        restore_mode: StateSnapshotRestoreMode::KvOnly,
                    },
                    self.global_opt.clone(),
                    Arc::clone(&self.storage),
                    epoch_history.clone(),
                )
                .run()
                .await?;
            }
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L49-59)
```rust
#[derive(Parser)]
pub struct StateSnapshotRestoreOpt {
    #[clap(long = "state-manifest")]
    pub manifest_handle: FileHandle,
    #[clap(long = "state-into-version")]
    pub version: Version,
    #[clap(long)]
    pub validate_modules: bool,
    #[clap(long)]
    pub restore_mode: StateSnapshotRestoreMode,
}
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L186-226)
```rust
        let storage = self.storage.clone();
        let futs_iter = chunks.into_iter().enumerate().map(|(chunk_idx, chunk)| {
            let storage = storage.clone();
            async move {
                tokio::spawn(async move {
                    let blobs = Self::read_state_value(&storage, chunk.blobs.clone()).await?;
                    let proof = storage.load_bcs_file(&chunk.proof).await?;
                    Result::<_>::Ok((chunk_idx, chunk, blobs, proof))
                })
                .await?
            }
        });
        let con = self.concurrent_downloads;
        let mut futs_stream = stream::iter(futs_iter).buffered_x(con * 2, con);
        let mut start = None;
        while let Some((chunk_idx, chunk, mut blobs, proof)) = futs_stream.try_next().await? {
            start = start.or_else(|| Some(Instant::now()));
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["add_state_chunk"]);
            let receiver = receiver.clone();
            if self.validate_modules {
                blobs = tokio::task::spawn_blocking(move || {
                    Self::validate_modules(&blobs);
                    blobs
                })
                .await?;
            }
            tokio::task::spawn_blocking(move || {
                receiver.lock().as_mut().unwrap().add_chunk(blobs, proof)
            })
            .await??;
            leaf_idx.set(chunk.last_idx as i64);
            info!(
                chunk = chunk_idx,
                chunks_to_add = chunks_to_add,
                last_idx = chunk.last_idx,
                values_per_second = ((chunk.last_idx + 1 - start_idx) as f64
                    / start.as_ref().unwrap().elapsed().as_secs_f64())
                    as u64,
                "State chunk added.",
            );
        }
```
