# Audit Report

## Title
Mempool Backoff Signal Bypass Enables Transaction Flooding Attack

## Summary
A malicious peer can ignore ACK backoff signals from the mempool and continue flooding a victim node with transaction broadcasts at high rates, bypassing the intended backpressure mechanism. This causes resource exhaustion as the victim node wastes CPU, memory, and bandwidth processing and rejecting transactions.

## Finding Description

The mempool implements a backpressure mechanism where a node with a full mempool sends an ACK response with `backoff=true` to signal the sender to slow down broadcasts. However, this mechanism is entirely **voluntary** and enforced only on the sender's side, with no receiver-side enforcement. [1](#0-0) 

When a node's mempool is full, it generates an ACK response with backoff flags set. The function `gen_ack_response` checks if any transaction was rejected with `MempoolIsFull` status and sets both `retry` and `backoff` to true. [2](#0-1) 

The honest receiving peer is expected to honor this backoff signal by calling `process_broadcast_ack`, which sets the local `backoff_mode` flag: [3](#0-2) 

When in backoff mode, the peer schedules broadcasts at 30-second intervals instead of the normal 10ms interval: [4](#0-3) 

**The Critical Vulnerability:** There is **no enforcement** on the receiving end (the victim node) to ensure peers actually honor the backoff signal. A malicious peer can:

1. Ignore the ACK response entirely (never call `process_broadcast_ack`)
2. Modify their code to not set `backoff_mode=true` even after receiving the ACK
3. Continue broadcasting at the normal 10ms interval

The victim node has no mechanism to:
- Track which peers were sent backoff signals
- Reject or rate-limit incoming broadcasts from peers that should be in backoff mode
- Detect peers that are violating backoff requirements
- Disconnect or ban misbehaving peers

The only protection is a global `BoundedExecutor` that limits concurrent transaction processing: [5](#0-4) 

This executor uses a simple semaphore with no per-peer fairness: [6](#0-5) 

Default configuration allows only 4 concurrent inbound syncs for fullnodes: [7](#0-6) 

A malicious peer can monopolize all 4 slots by continuously sending broadcasts at 10ms intervals (100 broadcasts/second), forcing the victim to validate and reject transactions repeatedly.

## Impact Explanation

**Medium Severity** per Aptos bug bounty criteria - this enables resource exhaustion and degraded service:

1. **CPU Exhaustion**: Victim node wastes CPU cycles on:
   - Transaction validation (VM operations)
   - Signature verification
   - Sequence number checks
   - Generating and sending ACK responses

2. **Memory Pressure**: BoundedExecutor queue fills with tasks from malicious peer, blocking legitimate transactions

3. **Network Bandwidth**: Continuous broadcast messages and ACK responses consume bandwidth

4. **Degraded Performance**: Legitimate peers experience delays as malicious peer monopolizes the 4 concurrent processing slots

5. **Availability Impact**: In extreme cases, the node may become unresponsive to honest peers

This breaks the **Resource Limits** invariant (#9) that "all operations must respect gas, storage, and computational limits" - the malicious peer can force unlimited resource consumption on the victim.

## Likelihood Explanation

**High Likelihood:**

1. **Low Attack Complexity**: Attacker simply needs to modify their node's mempool code to skip the backoff logic - a trivial change
2. **No Authentication Required**: Any network peer can connect and send broadcasts
3. **No Detection**: Victim cannot detect the attack is happening (only sees high broadcast volume)
4. **No Mitigation**: Victim has no recourse except potentially disconnecting the peer manually at network layer
5. **Profitable**: Attacker can degrade competitor nodes or attack specific targets with minimal cost

The attack is especially effective during high network load when mempools naturally fill up, as honest nodes will already be sending backoff signals.

## Recommendation

Implement receiver-side enforcement of backoff signals with per-peer rate limiting:

```rust
// Add to PeerSyncState in types.rs
pub struct BackoffTracking {
    backoff_sent_at: Option<SystemTime>,
    backoff_violations: u32,
}

// In process_transaction_broadcast (tasks.rs), before processing:
fn should_accept_broadcast(
    peer: &PeerNetworkId,
    sync_states: &HashMap<PeerNetworkId, PeerSyncState>,
) -> bool {
    if let Some(state) = sync_states.get(peer) {
        if let Some(backoff_sent_at) = state.backoff_tracking.backoff_sent_at {
            let min_backoff_duration = Duration::from_millis(30_000);
            if SystemTime::now().duration_since(backoff_sent_at).unwrap() < min_backoff_duration {
                // Peer should still be in backoff, reject this broadcast
                counters::backoff_violation_inc(peer.network_id());
                
                // After N violations, disconnect peer
                if state.backoff_tracking.backoff_violations > 10 {
                    disconnect_peer(peer, DisconnectReason::BackoffViolation);
                }
                return false;
            }
        }
    }
    true
}

// Update backoff_sent_at when sending ACK with backoff=true in gen_ack_response
```

Additionally, implement per-peer rate limiting on incoming broadcasts similar to the state-sync `UnhealthyPeerState` mechanism.

## Proof of Concept

**Attack Steps:**

1. **Malicious Peer Setup**: Modify Aptos node code to comment out the backoff enforcement:
```rust
// In network.rs process_broadcast_ack, comment out:
// if backoff {
//     sync_state.broadcast_info.backoff_mode = true;
// }
```

2. **Launch Attack**:
   - Connect malicious node to victim node
   - Send 300 transactions per broadcast at 10ms intervals (100 broadcasts/second = 30,000 txns/second)
   - Victim's mempool fills up quickly

3. **Observe Impact**:
   - Victim sends ACK with `backoff=true`
   - Malicious node ignores it and continues at full rate
   - Victim's BoundedExecutor slots are all consumed by malicious peer
   - Victim continues validating, rejecting, and ACKing transactions
   - Legitimate peers cannot get their transactions processed

**Metrics to Monitor:**
- `aptos_mempool_shared_mempool_transactions_processed{result="MempoolIsFull"}` - increases rapidly
- `aptos_mempool_shared_mempool_pending_broadcasts` - stays at maximum (4 or 20)
- CPU usage spikes on transaction validation
- Network traffic shows continuous broadcast/ACK cycles

This attack requires only modifying the attacking node's code (a few lines) and can effectively degrade or DoS the victim node's mempool processing capability.

## Notes

The vulnerability exists because the mempool's backpressure mechanism was designed assuming honest peer behavior. Unlike the state-sync subsystem which has `UnhealthyPeerState` tracking and peer banning for misbehavior, the mempool lacks any receiver-side enforcement or reputation system. The BoundedExecutor provides only global rate limiting without per-peer fairness, making it trivial for a single malicious peer to monopolize resources.

### Citations

**File:** mempool/src/shared_mempool/tasks.rs (L108-122)
```rust
    let schedule_backoff = network_interface.is_backoff_mode(&peer);

    let interval_ms = if schedule_backoff {
        smp.config.shared_mempool_backoff_interval_ms
    } else {
        smp.config.shared_mempool_tick_interval_ms
    };

    scheduled_broadcasts.push(ScheduledBroadcast::new(
        Instant::now() + Duration::from_millis(interval_ms),
        peer,
        schedule_backoff,
        executor,
    ))
}
```

**File:** mempool/src/shared_mempool/tasks.rs (L233-250)
```rust
    let ack_response = gen_ack_response(message_id, results, &peer);

    // Respond to the peer with an ack. Note: ack response messages should be
    // small enough that they always fit within the maximum network message
    // size, so there's no need to check them here.
    if let Err(e) = smp
        .network_interface
        .send_message_to_peer(peer, ack_response)
    {
        counters::network_send_fail_inc(counters::ACK_TXNS);
        warn!(
            LogSchema::event_log(LogEntry::BroadcastACK, LogEvent::NetworkSendFail)
                .peer(&peer)
                .error(&e.into())
        );
        return;
    }
    notify_subscribers(SharedMempoolNotification::ACK, &smp.subscribers);
```

**File:** mempool/src/shared_mempool/tasks.rs (L254-278)
```rust
fn gen_ack_response(
    message_id: MempoolMessageId,
    results: Vec<SubmissionStatusBundle>,
    peer: &PeerNetworkId,
) -> MempoolSyncMsg {
    let mut backoff_and_retry = false;
    for (_, (mempool_status, _)) in results.into_iter() {
        if mempool_status.code == MempoolStatusCode::MempoolIsFull {
            backoff_and_retry = true;
            break;
        }
    }

    update_ack_counter(
        peer,
        counters::SENT_LABEL,
        backoff_and_retry,
        backoff_and_retry,
    );
    MempoolSyncMsg::BroadcastTransactionsResponse {
        message_id,
        retry: backoff_and_retry,
        backoff: backoff_and_retry,
    }
}
```

**File:** mempool/src/shared_mempool/network.rs (L298-355)
```rust
    pub fn process_broadcast_ack(
        &self,
        peer: PeerNetworkId,
        message_id: MempoolMessageId,
        retry: bool,
        backoff: bool,
        timestamp: SystemTime,
    ) {
        let mut sync_states = self.sync_states.write();

        let sync_state = if let Some(state) = sync_states.get_mut(&peer) {
            state
        } else {
            counters::invalid_ack_inc(peer.network_id(), counters::UNKNOWN_PEER);
            return;
        };

        if let Some(sent_timestamp) = sync_state.broadcast_info.sent_messages.remove(&message_id) {
            let rtt = timestamp
                .duration_since(sent_timestamp)
                .expect("failed to calculate mempool broadcast RTT");

            let network_id = peer.network_id();
            counters::SHARED_MEMPOOL_BROADCAST_RTT
                .with_label_values(&[network_id.as_str()])
                .observe(rtt.as_secs_f64());

            counters::shared_mempool_pending_broadcasts(&peer).dec();
        } else {
            trace!(
                LogSchema::new(LogEntry::ReceiveACK)
                    .peer(&peer)
                    .message_id(&message_id),
                "request ID does not exist or expired"
            );
            return;
        }

        trace!(
            LogSchema::new(LogEntry::ReceiveACK)
                .peer(&peer)
                .message_id(&message_id)
                .backpressure(backoff),
            retry = retry,
        );
        tasks::update_ack_counter(&peer, counters::RECEIVED_LABEL, retry, backoff);

        if retry {
            sync_state.broadcast_info.retry_messages.insert(message_id);
        }

        // Backoff mode can only be turned off by executing a broadcast that was scheduled
        // as a backoff broadcast.
        // This ensures backpressure request from remote peer is honored at least once.
        if backoff {
            sync_state.broadcast_info.backoff_mode = true;
        }
    }
```

**File:** mempool/src/shared_mempool/coordinator.rs (L90-93)
```rust
    // Use a BoundedExecutor to restrict only `workers_available` concurrent
    // worker tasks that can process incoming transactions.
    let workers_available = smp.config.shared_mempool_max_concurrent_inbound_syncs;
    let bounded_executor = BoundedExecutor::new(workers_available, executor.clone());
```

**File:** crates/bounded-executor/src/executor.rs (L16-31)
```rust
#[derive(Clone, Debug)]
pub struct BoundedExecutor {
    semaphore: Arc<Semaphore>,
    executor: Handle,
}

impl BoundedExecutor {
    /// Create a new `BoundedExecutor` from an existing tokio [`Handle`]
    /// with a maximum concurrent task capacity of `capacity`.
    pub fn new(capacity: usize, executor: Handle) -> Self {
        let semaphore = Arc::new(Semaphore::new(capacity));
        Self {
            semaphore,
            executor,
        }
    }
```

**File:** config/src/config/mempool_config.rs (L111-117)
```rust
            shared_mempool_tick_interval_ms: 10,
            shared_mempool_backoff_interval_ms: 30_000,
            shared_mempool_batch_size: 300,
            shared_mempool_max_batch_bytes: MAX_APPLICATION_MESSAGE_SIZE as u64,
            shared_mempool_ack_timeout_ms: 2_000,
            shared_mempool_max_concurrent_inbound_syncs: 4,
            max_broadcasts_per_peer: 20,
```
