# Audit Report

## Title
BlockStore Recovery Accepts Unverified Quorum Certificates Leading to Consensus Safety Violations

## Summary
During BlockStore initialization from recovery_data, blocks and quorum certificates (QCs) loaded from ConsensusDB are inserted without cryptographic signature verification. When combined with SafetyRules running in Local mode (skip_sig_verify=true), this allows corrupted recovery_data containing forged QCs to corrupt a node's internal consensus state, potentially causing it to vote for blocks that violate the 2-chain safety rule of AptosBFT.

## Finding Description

The vulnerability exists in the BlockStore recovery flow where blocks and QCs from persistent storage are loaded without cryptographic validation: [1](#0-0) 

The `insert_single_quorum_cert()` method only validates logical consistency but does NOT verify cryptographic signatures: [2](#0-1) 

When SafetyRules operates in Local mode, it's initialized with `skip_sig_verify=true`: [3](#0-2) 

This causes SafetyRules to skip signature verification when validating proposals: [4](#0-3) 

**Attack Scenario:**

1. ConsensusDB becomes corrupted (through disk corruption, database bugs, or malicious modification) with forged QCs claiming blocks were certified by 2f+1 validators when they weren't.

2. Node restarts and loads recovery_data via `start_round_manager()`: [5](#0-4) 

3. Forged QCs are inserted into BlockStore without signature verification, corrupting the node's view of which blocks were properly certified.

4. The node's SafetyData (preferred_round, one_chain_round) is updated based on these forged QCs through `observe_qc()`: [6](#0-5) 

5. When the node receives legitimate proposals, it evaluates safety rules using corrupted state. With skip_sig_verify=true, SafetyRules doesn't re-verify the stored QCs.

6. The node may vote for proposals that violate the actual consensus state, believing certain blocks were committed when they weren't, potentially causing equivocation or chain splits if multiple nodes have similar corruption.

## Impact Explanation

This is a **Critical Severity** vulnerability (up to $1,000,000) under the Aptos bug bounty program as it enables **Consensus/Safety violations**:

- Violates the core AptosBFT safety guarantee that prevents double-spending and chain splits under < 1/3 Byzantine validators
- A node with corrupted recovery_data can vote for conflicting blocks based on an incorrect view of consensus state
- If multiple validators experience similar corruption (e.g., due to a systematic storage bug), they could collectively form 2f+1 votes for blocks that violate safety rules
- Could lead to non-recoverable network partition requiring manual intervention or hardfork

The vulnerability breaks Invariant #2: "Consensus Safety: AptosBFT must prevent double-spending and chain splits under < 1/3 Byzantine"

## Likelihood Explanation

**Medium to High Likelihood:**

- Storage corruption can occur through:
  - Disk failures or bit rot in validator hardware
  - Bugs in the RocksDB database implementation
  - Improper shutdown during writes
  - File system corruption
  
- SafetyRules Local mode (skip_sig_verify=true) is the default for single-process deployments, which many validators use for performance

- The vulnerability is systematic: if a storage bug affects multiple validators similarly, multiple nodes could have corrupted state simultaneously

- No external attacker action required beyond the initial corruption event

## Recommendation

Add cryptographic verification during BlockStore recovery to validate all blocks and QCs loaded from persistent storage:

```rust
// In BlockStore::build() method
for block in blocks {
    // Verify block signature before insertion
    if let Some(author) = block.author() {
        block.validate_signature(&validator_verifier)
            .context("Block signature verification failed during recovery")?;
    }
    
    if block.round() <= root_block_round {
        block_store.insert_committed_block(block).await
            .unwrap_or_else(|e| {
                panic!("[BlockStore] failed to insert committed block during build {:?}", e)
            });
    } else {
        block_store.insert_block(block).await
            .unwrap_or_else(|e| {
                panic!("[BlockStore] failed to insert block during build {:?}", e)
            });
    }
}

for qc in quorum_certs {
    // Verify QC signatures before insertion
    qc.verify(&validator_verifier)
        .context("QC signature verification failed during recovery")?;
        
    block_store.insert_single_quorum_cert(qc)
        .unwrap_or_else(|e| {
            panic!("[BlockStore] failed to insert quorum during build{:?}", e)
        });
}
```

Additionally, consider:
- Remove or deprecate `skip_sig_verify` mode to enforce consistent verification
- Add checksums or authenticated storage for ConsensusDB data
- Implement recovery data validation in `RecoveryData::new()` to verify signatures before blocks are even passed to BlockStore

## Proof of Concept

```rust
// Simulation of the vulnerability (pseudo-code for demonstration)
#[test]
fn test_corrupted_recovery_data_breaks_consensus() {
    // 1. Setup: Create a forged QC that claims block X was certified
    let forged_qc = create_forged_qc_for_block(
        block_id,
        forged_signatures, // Invalid BLS signatures
    );
    
    // 2. Corrupt ConsensusDB by inserting forged QC
    consensus_db.save_blocks_and_quorum_certificates(
        vec![block],
        vec![forged_qc.clone()],
    );
    
    // 3. Restart node, triggering recovery
    let recovery_data = storage.start(order_vote_enabled, window_size);
    
    // 4. BlockStore loads corrupted data without verification
    let block_store = BlockStore::new(
        storage,
        recovery_data, // Contains forged QC
        execution_client,
        /* ... */
    );
    
    // 5. Verify that forged QC was accepted
    assert!(block_store.get_quorum_cert_for_block(&block_id).is_some());
    
    // 6. SafetyRules makes decision based on corrupted state
    let safety_rules = SafetyRules::new(persistent_storage, true); // skip_sig_verify=true
    
    // 7. Node votes on proposal using corrupted preferred_round
    // This could violate safety if preferred_round is artificially high
    let vote = safety_rules.construct_and_sign_vote_two_chain(&proposal, None);
    
    // Vote may violate safety rules based on actual consensus state
    assert!(vote.is_ok()); // Succeeds but shouldn't have
}
```

**Notes:**
- The actual exploitation requires ability to corrupt ConsensusDB, which limits direct external attack but doesn't protect against storage-level bugs or insider threats
- The vulnerability is particularly dangerous during systematic events (storage bugs, coordinated attacks) that could affect multiple validators
- The lack of verification during recovery creates a trust boundary violation where persistent storage is blindly trusted

### Citations

**File:** consensus/src/block_storage/block_store.rs (L282-305)
```rust
        for block in blocks {
            if block.round() <= root_block_round {
                block_store
                    .insert_committed_block(block)
                    .await
                    .unwrap_or_else(|e| {
                        panic!(
                            "[BlockStore] failed to insert committed block during build {:?}",
                            e
                        )
                    });
            } else {
                block_store.insert_block(block).await.unwrap_or_else(|e| {
                    panic!("[BlockStore] failed to insert block during build {:?}", e)
                });
            }
        }
        for qc in quorum_certs {
            block_store
                .insert_single_quorum_cert(qc)
                .unwrap_or_else(|e| {
                    panic!("[BlockStore] failed to insert quorum during build{:?}", e)
                });
        }
```

**File:** consensus/src/block_storage/block_store.rs (L519-556)
```rust
    pub fn insert_single_quorum_cert(&self, qc: QuorumCert) -> anyhow::Result<()> {
        // If the parent block is not the root block (i.e not None), ensure the executed state
        // of a block is consistent with its QuorumCert, otherwise persist the QuorumCert's
        // state and on restart, a new execution will agree with it.  A new execution will match
        // the QuorumCert's state on the next restart will work if there is a memory
        // corruption, for example.
        match self.get_block(qc.certified_block().id()) {
            Some(pipelined_block) => {
                ensure!(
                    // decoupled execution allows dummy block infos
                    pipelined_block
                        .block_info()
                        .match_ordered_only(qc.certified_block()),
                    "QC for block {} has different {:?} than local {:?}",
                    qc.certified_block().id(),
                    qc.certified_block(),
                    pipelined_block.block_info()
                );
                observe_block(
                    pipelined_block.block().timestamp_usecs(),
                    BlockStage::QC_ADDED,
                );
                if pipelined_block.block().is_opt_block() {
                    observe_block(
                        pipelined_block.block().timestamp_usecs(),
                        BlockStage::QC_ADDED_OPT_BLOCK,
                    );
                }
                pipelined_block.set_qc(Arc::new(qc.clone()));
            },
            None => bail!("Insert {} without having the block in store first", qc),
        };

        self.storage
            .save_tree(vec![], vec![qc.clone()])
            .context("Insert block failed when saving quorum")?;
        self.inner.write().insert_quorum_cert(qc)
    }
```

**File:** consensus/safety-rules/src/safety_rules_manager.rs (L131-136)
```rust
    pub fn new_local(storage: PersistentSafetyStorage) -> Self {
        let safety_rules = SafetyRules::new(storage, true);
        Self {
            internal_safety_rules: SafetyRulesWrapper::Local(Arc::new(RwLock::new(safety_rules))),
        }
    }
```

**File:** consensus/safety-rules/src/safety_rules.rs (L63-85)
```rust
    pub(crate) fn verify_proposal(
        &mut self,
        vote_proposal: &VoteProposal,
    ) -> Result<VoteData, Error> {
        let proposed_block = vote_proposal.block();
        let safety_data = self.persistent_storage.safety_data()?;

        self.verify_epoch(proposed_block.epoch(), &safety_data)?;

        self.verify_qc(proposed_block.quorum_cert())?;
        if !self.skip_sig_verify {
            proposed_block
                .validate_signature(&self.epoch_state()?.verifier)
                .map_err(|error| Error::InvalidProposal(error.to_string()))?;
        }
        proposed_block
            .verify_well_formed()
            .map_err(|error| Error::InvalidProposal(error.to_string()))?;

        vote_proposal
            .gen_vote_data()
            .map_err(|error| Error::InvalidAccumulatorExtension(error.to_string()))
    }
```

**File:** consensus/safety-rules/src/safety_rules.rs (L135-149)
```rust
    pub(crate) fn observe_qc(&self, qc: &QuorumCert, safety_data: &mut SafetyData) -> bool {
        let mut updated = false;
        let one_chain = qc.certified_block().round();
        let two_chain = qc.parent_block().round();
        if one_chain > safety_data.one_chain_round {
            safety_data.one_chain_round = one_chain;
            trace!(
                SafetyLogSchema::new(LogEntry::OneChainRound, LogEvent::Update)
                    .preferred_round(safety_data.one_chain_round)
            );
            updated = true;
        }
        if two_chain > safety_data.preferred_round {
            safety_data.preferred_round = two_chain;
            trace!(
```

**File:** consensus/src/epoch_manager.rs (L801-900)
```rust
    async fn start_round_manager(
        &mut self,
        consensus_key: Arc<PrivateKey>,
        recovery_data: RecoveryData,
        epoch_state: Arc<EpochState>,
        onchain_consensus_config: OnChainConsensusConfig,
        onchain_execution_config: OnChainExecutionConfig,
        onchain_randomness_config: OnChainRandomnessConfig,
        onchain_jwk_consensus_config: OnChainJWKConsensusConfig,
        network_sender: Arc<NetworkSender>,
        payload_client: Arc<dyn PayloadClient>,
        payload_manager: Arc<dyn TPayloadManager>,
        rand_config: Option<RandConfig>,
        fast_rand_config: Option<RandConfig>,
        rand_msg_rx: aptos_channel::Receiver<AccountAddress, IncomingRandGenRequest>,
        secret_sharing_msg_rx: aptos_channel::Receiver<AccountAddress, IncomingSecretShareRequest>,
    ) {
        let epoch = epoch_state.epoch;
        info!(
            epoch = epoch_state.epoch,
            validators = epoch_state.verifier.to_string(),
            root_block = %recovery_data.commit_root_block(),
            "Starting new epoch",
        );

        info!(epoch = epoch, "Update SafetyRules");

        let mut safety_rules =
            MetricsSafetyRules::new(self.safety_rules_manager.client(), self.storage.clone());
        match safety_rules.perform_initialize() {
            Err(e) if matches!(e, Error::ValidatorNotInSet(_)) => {
                warn!(
                    epoch = epoch,
                    error = e,
                    "Unable to initialize safety rules.",
                );
            },
            Err(e) => {
                error!(
                    epoch = epoch,
                    error = e,
                    "Unable to initialize safety rules.",
                );
            },
            Ok(()) => (),
        }

        info!(epoch = epoch, "Create RoundState");
        let round_state =
            self.create_round_state(self.time_service.clone(), self.timeout_sender.clone());

        info!(epoch = epoch, "Create ProposerElection");
        let proposer_election =
            self.create_proposer_election(&epoch_state, &onchain_consensus_config);
        let chain_health_backoff_config =
            ChainHealthBackoffConfig::new(self.config.chain_health_backoff.clone());
        let pipeline_backpressure_config = PipelineBackpressureConfig::new(
            self.config.pipeline_backpressure.clone(),
            self.config.execution_backpressure.clone(),
        );

        let safety_rules_container = Arc::new(Mutex::new(safety_rules));

        self.execution_client
            .start_epoch(
                consensus_key.clone(),
                epoch_state.clone(),
                safety_rules_container.clone(),
                payload_manager.clone(),
                &onchain_consensus_config,
                &onchain_execution_config,
                &onchain_randomness_config,
                rand_config,
                fast_rand_config.clone(),
                rand_msg_rx,
                secret_sharing_msg_rx,
                recovery_data.commit_root_block().round(),
            )
            .await;
        let consensus_sk = consensus_key;

        let signer = Arc::new(ValidatorSigner::new(self.author, consensus_sk));
        let pipeline_builder = self.execution_client.pipeline_builder(signer);
        info!(epoch = epoch, "Create BlockStore");
        // Read the last vote, before "moving" `recovery_data`
        let last_vote = recovery_data.last_vote();
        let block_store = Arc::new(BlockStore::new(
            Arc::clone(&self.storage),
            recovery_data,
            self.execution_client.clone(),
            self.config.max_pruned_blocks_in_mem,
            Arc::clone(&self.time_service),
            self.config.vote_back_pressure_limit,
            payload_manager,
            onchain_consensus_config.order_vote_enabled(),
            onchain_consensus_config.window_size(),
            self.pending_blocks.clone(),
            Some(pipeline_builder),
        ));

```
