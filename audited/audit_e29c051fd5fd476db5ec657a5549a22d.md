# Audit Report

## Title
Stale Epoch in JWK Observers Causes Consensus Failures During Epoch Transitions

## Summary
The `KeyLevelConsensusManager` spawns JWK observers with the current epoch for logging purposes only. Local observations from these observers lack epoch information and are not validated against the manager's epoch state. During epoch transitions, a race condition allows stale observations to trigger consensus with an outdated epoch, causing peer validators to reject requests and resulting in failed consensus rounds.

## Finding Description

The vulnerability exists in the JWK consensus subsystem's handling of observations during epoch transitions. The security guarantee broken is **epoch consistency** in distributed consensus operations. [1](#0-0) 

At this location, observers are spawned with the epoch parameter passed for logging only: [2](#0-1) 

The epoch is only used in logging and never attached to observations sent back to the manager: [3](#0-2) 

When observations return to the manager, they are processed without epoch validation: [4](#0-3) 

The manager then initiates consensus using its stored `epoch_state`: [5](#0-4) 

This creates requests with the manager's epoch: [6](#0-5) 

**Attack Path:**

1. Manager spawns in epoch N with observers
2. Observer fetches JWKs from external OIDC provider
3. Observation queued in `local_observation_rx` channel
4. **Epoch transitions to N+1** at blockchain level
5. Manager still running (close signal not yet processed)
6. Manager processes queued observation from epoch N
7. Starts consensus with `epoch_state` for epoch N
8. Broadcasts `ObservedKeyLevelUpdateRequest { epoch: N, ... }`
9. Peer validators at epoch N+1 receive requests
10. EpochManager filters by epoch: `if Some(N) == Some(N+1)` â†’ FALSE [7](#0-6) 

11. Requests dropped, never forwarded to peer managers
12. Manager receives no responses, consensus fails to reach quorum
13. Observation effectively rejected

This is validated by the aggregation layer's epoch check for peer responses: [8](#0-7) 

The test confirms epoch mismatches cause rejection: [9](#0-8) 

## Impact Explanation

This is a **Medium Severity** vulnerability per Aptos bug bounty criteria:

- **State inconsistencies requiring intervention**: JWK updates fail during epoch boundaries, requiring manual intervention or waiting for the next observation cycle
- **Consensus availability impact**: Legitimate JWK observations cannot reach consensus during epoch transitions
- **No funds at risk**: Does not enable theft or manipulation of funds
- **No safety violation**: Does not break consensus safety or cause chain splits
- **Limited scope**: Only affects JWK consensus subsystem during epoch transitions

The vulnerability impacts the reliability of the JWK consensus mechanism, which is critical for validator authentication via OIDC providers. Failed updates during epoch transitions could delay security-critical key rotations.

## Likelihood Explanation

**Likelihood: Medium-High**

This vulnerability occurs naturally during every epoch transition without requiring attacker action:

1. **Timing window exists**: Between observation queuing and epoch transition processing
2. **Observers run continuously**: Fetch JWKs every 10 seconds regardless of epoch state
3. **Channel buffering**: QueueStyle::KLAST with capacity 100 allows stale observations to persist
4. **No synchronization**: No mechanism prevents observation processing during shutdown window
5. **Network delays**: Epoch transition messages may arrive at different times across validators

The race condition window is small but non-zero. Given that:
- Observations occur every 10 seconds per issuer
- Epoch transitions happen regularly
- Multiple OIDC providers may be monitored

There is a measurable probability that an observation will be in-flight or queued when an epoch transition occurs, triggering this vulnerability.

## Recommendation

**Solution 1: Include epoch in observations and validate on receipt**

Modify `JWKObserver` to include epoch in observations and validate before processing:

```rust
// In jwk_observer.rs, modify to include epoch
pub struct JWKObserver {
    epoch: u64,  // Store epoch
    close_tx: oneshot::Sender<()>,
    join_handle: JoinHandle<()>,
}

// Send epoch with observation
let _ = observation_tx.push((), (epoch, issuer.as_bytes().to_vec(), jwks));

// In jwk_manager_per_key.rs, validate epoch
pub fn process_new_observation(&mut self, observation_epoch: u64, issuer: Issuer, jwks: Vec<JWK>) -> Result<()> {
    ensure!(
        observation_epoch == self.epoch_state.epoch,
        "Rejecting observation from stale epoch {} (current: {})",
        observation_epoch,
        self.epoch_state.epoch
    );
    // Continue processing...
}
```

**Solution 2: Stop processing observations immediately on close signal**

Ensure `stopped` flag is checked before processing any new observations and drain the channel during teardown without processing.

**Solution 3: Use epoch-aware channels**

Implement a channel wrapper that automatically tags and validates epoch for all messages.

## Proof of Concept

```rust
// Test demonstrating the vulnerability
#[tokio::test]
async fn test_stale_epoch_observation_rejection() {
    // Setup: Manager in epoch N
    let epoch_n = 100;
    let epoch_state_n = Arc::new(create_epoch_state(epoch_n));
    let manager = KeyLevelConsensusManager::new(
        consensus_key,
        my_addr,
        epoch_state_n.clone(),
        rb,
        vtxn_pool,
    );
    
    // Observer spawned with epoch N
    let (obs_tx, mut obs_rx) = aptos_channel::new(QueueStyle::KLAST, 100, None);
    let observer = JWKObserver::spawn(
        epoch_n,  // Epoch N passed but only used for logging
        my_addr,
        "https://issuer.com".to_string(),
        "https://issuer.com/.well-known/openid-configuration".to_string(),
        Duration::from_secs(10),
        obs_tx.clone(),
    );
    
    // Simulate observation fetched in epoch N
    let jwks = vec![create_test_jwk()];
    let _ = obs_tx.push((), (b"issuer".to_vec(), jwks.clone()));
    
    // Simulate epoch transition BEFORE observation is processed
    // In production, this would be EpochManager updating epoch_state
    // But manager still has epoch_state_n reference
    
    // Manager processes observation with stale epoch N state
    let (issuer, obs_jwks) = obs_rx.next().await.unwrap();
    
    // This triggers consensus with epoch N
    manager.process_new_observation(issuer, obs_jwks).unwrap();
    
    // Consensus broadcasts ObservedKeyLevelUpdateRequest { epoch: 100, ... }
    // Peer validators at epoch 101 will reject these requests
    // Verify that no quorum is reached due to epoch mismatch
    
    // Expected: Consensus fails to reach quorum
    // Actual: Observation effectively rejected
}
```

**Notes**

The vulnerability is confirmed by examining the code paths:
1. Observers lack epoch context in their observations
2. No validation exists for local observations against current epoch  
3. The epoch transition coordination does not prevent this race condition
4. Peer request filtering by epoch causes legitimate requests to be dropped

This represents a real availability issue in the JWK consensus subsystem that manifests during epoch boundaries, fitting the Medium severity classification for state inconsistencies requiring intervention.

### Citations

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L109-177)
```rust
    pub fn process_new_observation(&mut self, issuer: Issuer, jwks: Vec<JWK>) -> Result<()> {
        debug!(
            epoch = self.epoch_state.epoch,
            issuer = String::from_utf8(issuer.clone()).ok(),
            "Processing new observation."
        );
        let observed_jwks_by_kid: HashMap<KID, JWK> =
            jwks.into_iter().map(|jwk| (jwk.id(), jwk)).collect();
        let effectively_onchain = self
            .onchain_jwks
            .get(&issuer)
            .cloned()
            .unwrap_or_else(|| ProviderJWKsIndexed::new(issuer.clone()));
        let all_kids: HashSet<KID> = effectively_onchain
            .jwks
            .keys()
            .chain(observed_jwks_by_kid.keys())
            .cloned()
            .collect();
        for kid in all_kids {
            let onchain = effectively_onchain.jwks.get(&kid);
            let observed = observed_jwks_by_kid.get(&kid);
            match (onchain, observed) {
                (Some(x), Some(y)) => {
                    if x == y {
                        // No change, drop any in-progress consensus.
                        self.states_by_key.remove(&(issuer.clone(), kid.clone()));
                    } else {
                        // Update detected.
                        let update = KeyLevelUpdate {
                            issuer: issuer.clone(),
                            base_version: effectively_onchain.version,
                            kid: kid.clone(),
                            to_upsert: Some(y.clone()),
                        };
                        self.maybe_start_consensus(update)
                            .context("process_new_observation failed at upsert consensus init")?;
                    }
                },
                (None, Some(y)) => {
                    // Insert detected.
                    let update = KeyLevelUpdate {
                        issuer: issuer.clone(),
                        base_version: effectively_onchain.version,
                        kid: kid.clone(),
                        to_upsert: Some(y.clone()),
                    };
                    self.maybe_start_consensus(update)
                        .context("process_new_observation failed at upsert consensus init")?;
                },
                (Some(_), None) => {
                    // Delete detected.
                    let update = KeyLevelUpdate {
                        issuer: issuer.clone(),
                        base_version: effectively_onchain.version,
                        kid: kid.clone(),
                        to_upsert: None,
                    };
                    self.maybe_start_consensus(update)
                        .context("process_new_observation failed at deletion consensus init")?;
                },
                (None, None) => {
                    unreachable!("`kid` in `union(A, B)` but `kid` not in `A` and not in `B`?")
                },
            }
        }

        Ok(())
    }
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L207-214)
```rust
        let abort_handle = self
            .update_certifier
            .start_produce(
                self.epoch_state.clone(),
                update_translated,
                self.qc_update_tx.clone(),
            )
            .context("maybe_start_consensus failed at update_certifier.start_produce")?;
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L394-401)
```rust
                    (Ok(issuer), Ok(config_url)) => Some(JWKObserver::spawn(
                        this.epoch_state.epoch,
                        this.my_addr,
                        issuer,
                        config_url,
                        Duration::from_secs(10),
                        local_observation_tx.clone(),
                    )),
```

**File:** crates/aptos-jwk-consensus/src/jwk_observer.rs (L22-48)
```rust
    pub fn spawn(
        epoch: u64,
        my_addr: AccountAddress,
        issuer: String,
        config_url: String,
        fetch_interval: Duration,
        observation_tx: aptos_channel::Sender<(), (Issuer, Vec<JWK>)>,
    ) -> Self {
        let (close_tx, close_rx) = oneshot::channel();
        let join_handle = tokio::spawn(Self::start(
            fetch_interval,
            my_addr,
            issuer.clone(),
            config_url.clone(),
            observation_tx,
            close_rx,
        ));
        info!(
            epoch = epoch,
            issuer = issuer,
            config_url = config_url,
            "JWKObserver spawned."
        );
        Self {
            close_tx,
            join_handle,
        }
```

**File:** crates/aptos-jwk-consensus/src/jwk_observer.rs (L70-84)
```rust
        loop {
            tokio::select! {
                _ = interval.tick().fuse() => {
                    let timer = Instant::now();
                    let result = fetch_jwks(open_id_config_url.as_str(), my_addr).await;
                    debug!(issuer = issuer, "observe_result={:?}", result);
                    let secs = timer.elapsed().as_secs_f64();
                    if let Ok(mut jwks) = result {
                        OBSERVATION_SECONDS.with_label_values(&[issuer.as_str(), "ok"]).observe(secs);
                        jwks.sort();
                        let _ = observation_tx.push((), (issuer.as_bytes().to_vec(), jwks));
                    } else {
                        OBSERVATION_SECONDS.with_label_values(&[issuer.as_str(), "err"]).observe(secs);
                    }
                },
```

**File:** crates/aptos-jwk-consensus/src/mode/per_key.rs (L32-40)
```rust
    fn new_rb_request(
        epoch: u64,
        payload: &ProviderJWKs,
    ) -> anyhow::Result<ObservedKeyLevelUpdateRequest> {
        let KeyLevelUpdate { issuer, kid, .. } =
            KeyLevelUpdate::try_from_issuer_level_repr(payload)
                .context("new_rb_request failed with repr translation")?;
        Ok(ObservedKeyLevelUpdateRequest { epoch, issuer, kid })
    }
```

**File:** crates/aptos-jwk-consensus/src/epoch_manager.rs (L94-105)
```rust
    fn process_rpc_request(
        &mut self,
        peer_id: Author,
        rpc_request: IncomingRpcRequest,
    ) -> Result<()> {
        if Some(rpc_request.msg.epoch()) == self.epoch_state.as_ref().map(|s| s.epoch) {
            if let Some(tx) = &self.jwk_rpc_msg_tx {
                let _ = tx.push(peer_id, (peer_id, rpc_request));
            }
        }
        Ok(())
    }
```

**File:** crates/aptos-jwk-consensus/src/observation_aggregation/mod.rs (L60-63)
```rust
        ensure!(
            epoch == self.epoch_state.epoch,
            "adding peer observation failed with invalid epoch",
        );
```

**File:** crates/aptos-jwk-consensus/src/observation_aggregation/tests.rs (L61-70)
```rust
    // `ObservedUpdate` with incorrect epoch should be rejected.
    let result = ob_agg_state.add(addrs[0], ObservedUpdateResponse {
        epoch: 998,
        update: ObservedUpdate {
            author: addrs[0],
            observed: view_0.clone(),
            signature: private_keys[0].sign(&view_0).unwrap(),
        },
    });
    assert!(result.is_err());
```
