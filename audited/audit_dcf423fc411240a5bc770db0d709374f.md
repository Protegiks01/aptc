# Audit Report

## Title
Cross-Block Message Pollution in Sharded Block Executor Causing Consensus Divergence

## Summary
The sharded block executor uses persistent per-round message channels that are never cleared between block executions. This allows cross-shard messages from Block N to be incorrectly consumed during Block N+1, breaking deterministic execution and causing validators to produce different state roots for identical blocks.

## Finding Description

The vulnerability exists in the cross-shard message communication system used by the sharded block executor. The system maintains persistent message channels indexed only by round number, with no block-level isolation mechanism.

**Root Cause - Persistent Channels:**

The `RemoteCrossShardClient` maintains persistent message receivers stored in a `Vec` indexed by round: [1](#0-0) 

Similarly, `LocalCrossShardClient` maintains persistent receivers: [2](#0-1) 

**No Block-Level Isolation:**

When receiving messages, the implementation simply reads from the channel indexed by current round without any block identifier validation: [3](#0-2) [4](#0-3) 

The message structure contains no block identifier: [5](#0-4) 

**Static Instances Reused Across All Blocks:**

The sharded block executor is created as a static `Lazy` instance that persists for the entire program lifetime: [6](#0-5) [7](#0-6) 

These static instances are used in production: [8](#0-7) 

**Channels Created Once and Never Cleaned:**

Channels are created during initialization with one receiver per round: [9](#0-8) 

**StopMsg Only Terminates Receiver Loop:**

When execution completes, a `StopMsg` is sent to stop the receiver thread: [10](#0-9) 

The receiver loop breaks on `StopMsg` but does not drain pending messages: [11](#0-10) 

**Attack Scenario:**

1. Validator A executes Block N with multiple shards and rounds
2. During Round 0 of Block N, Shard 1 sends a cross-shard message to Shard 2
3. Due to network delay or different execution speeds, the message arrives at Shard 2 after Shard 2 has already completed Round 0 and its receiver thread has exited
4. The message remains queued in `message_rxs[0]`
5. Block N+1 begins execution
6. During Round 0 of Block N+1, Shard 2's new receiver thread calls `receive_cross_shard_msg(0)`
7. Shard 2 receives the stale message from Block N instead of messages from Block N+1
8. Shard 2 executes with incorrect cross-shard state, producing different transaction outputs
9. Validator A computes a different state root than other validators who did not experience this race condition
10. **Consensus divergence occurs** - validators cannot agree on the state root for Block N+1

The developers implemented per-round isolation within a block: [12](#0-11) 

However, they missed that these same channels are reused across blocks without any cleanup mechanism.

## Impact Explanation

**Severity: Critical**

This vulnerability directly violates the fundamental consensus invariant: **"All validators must produce identical state roots for identical blocks."**

This qualifies as **Critical Severity** under Aptos bug bounty criteria for **Consensus/Safety Violations**:
- Different validators can produce different state roots for the same block due to timing-dependent message consumption
- Creates potential for chain splits without requiring any Byzantine behavior
- Breaks deterministic execution guarantees

The impact includes:
1. **Consensus Failure**: Validators disagree on state roots and cannot reach consensus
2. **Non-Deterministic Execution**: Same block can produce different results on different runs
3. **Chain Availability Issues**: Repeated consensus failures may stall the chain
4. **Validator Disagreement**: Validators may need manual intervention to reconcile state

The vulnerability is particularly severe because:
- It occurs naturally without malicious actors
- Different validators experience different timing, making the race non-deterministic across the network
- No validation exists to detect or prevent stale message consumption
- Static instances ensure the bug persists for the program's entire lifetime

## Likelihood Explanation

**Likelihood: High**

The vulnerability is highly likely to manifest in production environments because:

1. **Natural Occurrence**: Network delays and variable execution speeds are normal in distributed systems - no attacker action required

2. **Guaranteed Race Window**: Every block execution has a race window between when a receiver thread exits (after receiving StopMsg) and when the next block's receiver thread starts

3. **No Protection Mechanism**: Zero code exists to:
   - Drain channels between blocks
   - Validate message block identifiers
   - Detect stale messages
   - Isolate channels per block

4. **Production Usage**: Sharded execution is actively used in production as evidenced by: [13](#0-12) 

5. **Persistent Architecture**: Static `Lazy` initialization ensures channels live indefinitely, accumulating potential for stale messages across all block executions

In distributed/remote execution mode with network variability, this race condition will eventually trigger, causing consensus divergence. In local mode, thread scheduling variations create similar (though less frequent) race conditions.

## Recommendation

Implement block-level isolation for cross-shard message channels:

**Option 1**: Include block identifier in messages
- Add `block_id` or `block_hash` to `RemoteTxnWrite` struct
- Validate block identifier when consuming messages
- Reject messages from previous blocks

**Option 2**: Recreate channels per block
- Create new channels at the start of each block execution
- Drain and close old channels after block completion
- Ensures complete isolation between blocks

**Option 3**: Drain channels between blocks
- After sending `StopMsg`, drain all remaining messages from channels
- Verify all channels are empty before starting next block
- Log warnings if stale messages are found

**Recommended approach**: Combine Options 1 and 3 for defense in depth - add block identifiers to messages AND drain channels between blocks to catch any lingering messages.

## Proof of Concept

While a full executable PoC requires complex distributed setup, the vulnerability can be demonstrated by:

1. Configure two validator nodes with sharded execution enabled
2. Execute Block N with cross-shard dependencies
3. Inject network delay on cross-shard messages to Shard 2
4. Verify Shard 2 completes Round 0 and sends StopMsg before delayed message arrives
5. Start Block N+1 execution
6. Observe Shard 2 consumes stale message from Block N during Block N+1 Round 0
7. Compare state roots - validators will disagree

The code evidence provided demonstrates that no mechanism prevents this scenario from occurring in production environments with normal network variability.

### Citations

**File:** execution/executor-service/src/remote_cross_shard_client.rs (L14-19)
```rust
pub struct RemoteCrossShardClient {
    // The senders of cross-shard messages to other shards per round.
    message_txs: Arc<Vec<Vec<Mutex<Sender<Message>>>>>,
    // The receivers of cross shard messages from other shards per round.
    message_rxs: Arc<Vec<Mutex<Receiver<Message>>>>,
}
```

**File:** execution/executor-service/src/remote_cross_shard_client.rs (L61-66)
```rust
    fn receive_cross_shard_msg(&self, current_round: RoundId) -> CrossShardMsg {
        let rx = self.message_rxs[current_round].lock().unwrap();
        let message = rx.recv().unwrap();
        let msg: CrossShardMsg = bcs::from_bytes(&message.to_bytes()).unwrap();
        msg
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L92-104)
```rust
        // We need to create channels for each shard and each round. This is needed because individual
        // shards might send cross shard messages to other shards that will be consumed in different rounds.
        // Having a single channel per shard will cause a shard to receiver messages that is not intended in the current round.
        let (cross_shard_msg_txs, cross_shard_msg_rxs): (
            Vec<Vec<Sender<CrossShardMsg>>>,
            Vec<Vec<Receiver<CrossShardMsg>>>,
        ) = (0..num_shards)
            .map(|_| {
                (0..MAX_ALLOWED_PARTITIONING_ROUNDS)
                    .map(|_| unbounded())
                    .unzip()
            })
            .unzip();
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L304-310)
```rust
pub struct LocalCrossShardClient {
    global_message_tx: Sender<CrossShardMsg>,
    // The senders of cross-shard messages to other shards per round.
    message_txs: Vec<Vec<Sender<CrossShardMsg>>>,
    // The receivers of cross shard messages from other shards per round.
    message_rxs: Vec<Receiver<CrossShardMsg>>,
}
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L335-337)
```rust
    fn receive_cross_shard_msg(&self, current_round: RoundId) -> CrossShardMsg {
        self.message_rxs[current_round].recv().unwrap()
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/messages.rs (L13-18)
```rust
#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct RemoteTxnWrite {
    state_key: StateKey,
    // The write op is None if the transaction is aborted.
    write_op: Option<WriteOp>,
}
```

**File:** execution/executor-service/src/local_executor_helper.rs (L14-21)
```rust
pub static SHARDED_BLOCK_EXECUTOR: Lazy<
    Arc<Mutex<ShardedBlockExecutor<CachedStateView, LocalExecutorClient<CachedStateView>>>>,
> = Lazy::new(|| {
    info!("LOCAL_SHARDED_BLOCK_EXECUTOR created");
    Arc::new(Mutex::new(
        LocalExecutorClient::create_local_sharded_block_executor(AptosVM::get_num_shards(), None),
    ))
});
```

**File:** execution/executor-service/src/remote_executor_client.rs (L57-72)
```rust
pub static REMOTE_SHARDED_BLOCK_EXECUTOR: Lazy<
    Arc<
        aptos_infallible::Mutex<
            ShardedBlockExecutor<CachedStateView, RemoteExecutorClient<CachedStateView>>,
        >,
    >,
> = Lazy::new(|| {
    info!("REMOTE_SHARDED_BLOCK_EXECUTOR created");
    Arc::new(aptos_infallible::Mutex::new(
        RemoteExecutorClient::create_remote_sharded_block_executor(
            get_coordinator_address(),
            get_remote_addresses(),
            None,
        ),
    ))
});
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L256-276)
```rust
    fn execute_block_sharded<V: VMBlockExecutor>(
        partitioned_txns: PartitionedTransactions,
        state_view: Arc<CachedStateView>,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<Vec<TransactionOutput>> {
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        } else {
            Ok(V::execute_block_sharded(
                &SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        }
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L163-168)
```rust
                    // Send a self message to stop the cross-shard commit receiver.
                    cross_shard_client_clone.send_cross_shard_msg(
                        shard_id,
                        round,
                        CrossShardMsg::StopMsg,
                    );
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L26-45)
```rust
    pub fn start<S: StateView + Sync + Send>(
        cross_shard_state_view: Arc<CrossShardStateView<S>>,
        cross_shard_client: Arc<dyn CrossShardClient>,
        round: RoundId,
    ) {
        loop {
            let msg = cross_shard_client.receive_cross_shard_msg(round);
            match msg {
                RemoteTxnWriteMsg(txn_commit_msg) => {
                    let (state_key, write_op) = txn_commit_msg.take();
                    cross_shard_state_view
                        .set_value(&state_key, write_op.and_then(|w| w.as_state_value()));
                },
                CrossShardMsg::StopMsg => {
                    trace!("Cross shard commit receiver stopped for round {}", round);
                    break;
                },
            }
        }
    }
```

**File:** aptos-move/aptos-vm/src/aptos_vm.rs (L3123-3148)
```rust
    fn execute_block_sharded<S: StateView + Sync + Send + 'static, C: ExecutorClient<S>>(
        sharded_block_executor: &ShardedBlockExecutor<S, C>,
        transactions: PartitionedTransactions,
        state_view: Arc<S>,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<Vec<TransactionOutput>, VMStatus> {
        let log_context = AdapterLogSchema::new(state_view.id(), 0);
        info!(
            log_context,
            "Executing block, transaction count: {}",
            transactions.num_txns()
        );

        let count = transactions.num_txns();
        let ret = sharded_block_executor.execute_block(
            state_view,
            transactions,
            AptosVM::get_concurrency_level(),
            onchain_config,
        );
        if ret.is_ok() {
            // Record the histogram count for transactions per block.
            BLOCK_TRANSACTION_COUNT.observe(count as f64);
        }
        ret
    }
```
