# Audit Report

## Title
Panic-During-Panic in API Handler Causes Validator Process Termination and Loss of Liveness

## Summary
The `panic_handler` function in `api/src/error_converter.rs` lacks protection against panics within its own execution. If the handler panics while recovering from an initial API panic (e.g., due to logging system failure or memory pressure), the unhandled panic propagates to the global crash handler which calls `process::exit(12)`, terminating the entire validator process. Since the API server runs in the same process as consensus, mempool, and state sync components, this results in complete validator liveness loss.

## Finding Description

The vulnerability exists in the panic recovery chain: [1](#0-0) 

The `panic_handler` function is registered with Poem's CatchPanic middleware: [2](#0-1) 

When an API request triggers a panic, CatchPanic catches it using `std::panic::catch_unwind` and invokes the custom handler. However, if `panic_handler` itself panics during execution of either:
1. The `error!()` logging macro
2. The `Json(...).into_response()` serialization

This new panic escapes the CatchPanic middleware scope and propagates to the global panic handler: [3](#0-2) 

The global handler attempts to log crash information and then terminates the process: [4](#0-3) 

The critical issue is that the API server is bootstrapped in the same process as all validator components: [5](#0-4) 

And all components share the same process space: [6](#0-5) 

**Attack Path:**
1. Attacker sends malformed API request triggering initial panic in handler
2. Poem's CatchPanic middleware catches panic and calls `panic_handler`
3. Under memory pressure or logging system corruption, `panic_handler` attempts to log error
4. Logging operation panics (e.g., OOM during string formatting, corrupted logger state)
5. New panic escapes to global crash handler which calls `process::exit(12)`
6. Entire validator process terminates immediately
7. Consensus participation lost until external restart

**Invariant Violated:**
The validator must maintain continuous liveness and consensus participation. This vulnerability allows an unprivileged attacker to potentially cause validator crashes through carefully timed requests during resource exhaustion.

## Impact Explanation

**Severity: HIGH**

This vulnerability falls under the Aptos bug bounty "High Severity" category for:
- **Validator node slowdowns** - Complete node crash requiring restart
- **API crashes** - API panic cascades to process termination

The impact is severe because:
1. **Complete liveness loss** - The validator cannot participate in consensus, propose blocks, or validate transactions until the process is externally restarted
2. **No graceful recovery** - `process::exit(12)` provides no cleanup, state flush, or reconnection attempt
3. **Chain-wide impact** - If multiple validators are affected simultaneously (e.g., during coordinated attack with network stress), consensus could be impaired
4. **Silent failure mode** - The panic occurs in error handling code, making it harder to detect and debug

While this doesn't reach "Critical" severity (no fund loss or permanent network partition), it represents a serious availability vulnerability.

## Likelihood Explanation

**Likelihood: LOW-MEDIUM**

The vulnerability requires two conditions:

1. **Initial API panic** (MEDIUM likelihood) - Achievable through:
   - Malformed requests exceeding size limits
   - Invalid JSON/BCS payloads
   - Resource exhaustion triggering OOM in request handlers
   
2. **Panic within panic_handler** (LOW likelihood) - Requires:
   - Severe memory pressure causing allocation failure during logging
   - Corrupted logging system state
   - Bug in serde_json or aptos_logger (unlikely but possible)

However, an attacker could increase likelihood by:
- Sending many concurrent requests to exhaust memory
- Targeting the validator during periods of high load
- Exploiting any memory leaks to gradually degrade system state

The global crash handler itself contains additional panic points that could trigger during the first panic's handling: [7](#0-6) 

The `.unwrap()` on line 39, `error!()` macro on line 40, and `flush()` on line 46 all represent additional failure points that could cause a true double-panic resulting in `abort()` rather than `exit(12)`.

## Recommendation

Implement defensive panic handling in the API panic_handler using `std::panic::catch_unwind`:

```rust
pub fn panic_handler(err: Box<dyn Any + Send>) -> Response {
    // Wrap all operations in catch_unwind to prevent cascading panics
    let response = std::panic::catch_unwind(|| {
        // Attempt to log, but don't panic if it fails
        let _ = std::panic::catch_unwind(|| {
            error!("Panic captured: {:?}", err);
        });
        
        build_panic_response("internal error".into())
    });
    
    match response {
        Ok(resp) => resp,
        Err(_) => {
            // If even the error response building panics, return a minimal response
            // without any logging or complex operations
            Response::builder()
                .status(500)
                .body("Internal Server Error")
                .into()
        }
    }
}
```

Additionally, improve the global crash handler to be panic-proof:

```rust
fn handle_panic(panic_info: &PanicHookInfo<'_>) {
    let details = format!("{}", panic_info);
    let backtrace = format!("{:#?}", Backtrace::new());
    
    let info = CrashInfo { details, backtrace };
    
    // Use unwrap_or_default to avoid panic in serialization
    let crash_info = toml::to_string_pretty(&info)
        .unwrap_or_else(|_| format!("Panic: {}", panic_info));
    
    // Don't use error! macro which could panic - use eprintln directly
    let _ = std::panic::catch_unwind(|| {
        eprintln!("{}", crash_info);
    });
    
    // Skip flush if it could panic
    let _ = std::panic::catch_unwind(|| {
        aptos_logger::flush();
    });
    
    if state::get_state() == VMState::VERIFIER || state::get_state() == VMState::DESERIALIZER {
        return;
    }
    
    process::exit(12);
}
```

## Proof of Concept

While a full reproduction requires a running validator, here's a test demonstrating the vulnerability:

```rust
#[test]
#[should_panic]
fn test_panic_in_panic_handler() {
    use std::panic;
    
    // Simulate the Poem CatchPanic pattern
    let result = panic::catch_unwind(|| {
        // Simulate initial panic in API handler
        panic!("Initial API handler panic");
    });
    
    match result {
        Err(initial_panic) => {
            // Simulate calling panic_handler which itself panics
            // This represents logging or serialization failure
            panic!("Panic in panic_handler during error logging");
        }
        Ok(_) => {}
    }
}

#[test]
fn test_safe_panic_handler() {
    use std::panic;
    
    let result = panic::catch_unwind(|| {
        panic!("Initial panic");
    });
    
    match result {
        Err(initial_panic) => {
            // Safe panic handler using catch_unwind
            let recovery = panic::catch_unwind(|| {
                // Simulate logging that might panic
                if true { panic!("logging failed"); }
            });
            
            match recovery {
                Ok(_) => println!("Logged successfully"),
                Err(_) => println!("Logging failed but recovered"),
            }
            
            // Always return a response
            println!("Returning error response");
        }
        Ok(_) => {}
    }
}
```

To reproduce in a live environment:
1. Start an Aptos validator node with the API enabled
2. Create memory pressure by sending many large concurrent API requests
3. While memory is constrained, send a malformed request that triggers a panic
4. Observe that if the panic_handler's logging fails due to OOM, the validator process terminates

## Notes

The vulnerability is exacerbated by the fact that the API server and validator consensus share the same process space. This architectural decision, while efficient, creates a blast radius where API-layer failures can cascade to critical consensus components. 

The VMState-based protection mechanism used for Move bytecode verification panics does NOT extend to the API layer, leaving this attack surface unprotected. [8](#0-7)

### Citations

**File:** api/src/error_converter.rs (L49-52)
```rust
pub fn panic_handler(err: Box<dyn Any + Send>) -> Response {
    error!("Panic captured: {:?}", err);
    build_panic_response("internal error".into())
}
```

**File:** api/src/runtime.rs (L256-256)
```rust
            .with(CatchPanic::new().with_handler(panic_handler))
```

**File:** crates/crash-handler/src/lib.rs (L26-30)
```rust
pub fn setup_panic_handler() {
    panic::set_hook(Box::new(move |pi: &PanicHookInfo<'_>| {
        handle_panic(pi);
    }));
}
```

**File:** crates/crash-handler/src/lib.rs (L39-46)
```rust
    let crash_info = toml::to_string_pretty(&info).unwrap();
    error!("{}", crash_info);
    // TODO / HACK ALARM: Write crash info synchronously via eprintln! to ensure it is written before the process exits which error! doesn't guarantee.
    // This is a workaround until https://github.com/aptos-labs/aptos-core/issues/2038 is resolved.
    eprintln!("{}", crash_info);

    // Wait till the logs have been flushed
    aptos_logger::flush();
```

**File:** crates/crash-handler/src/lib.rs (L48-54)
```rust
    // Do not kill the process if the panics happened at move-bytecode-verifier.
    // This is safe because the `state::get_state()` uses a thread_local for storing states. Thus the state can only be mutated to VERIFIER by the thread that's running the bytecode verifier.
    //
    // TODO: once `can_unwind` is stable, we should assert it. See https://github.com/rust-lang/rust/issues/92988.
    if state::get_state() == VMState::VERIFIER || state::get_state() == VMState::DESERIALIZER {
        return;
    }
```

**File:** crates/crash-handler/src/lib.rs (L56-57)
```rust
    // Kill the process
    process::exit(12);
```

**File:** aptos-node/src/services.rs (L100-111)
```rust
    let api_runtime = if node_config.api.enabled {
        Some(bootstrap_api(
            node_config,
            chain_id,
            db_rw.reader.clone(),
            mempool_client_sender.clone(),
            indexer_reader.clone(),
            api_port_tx,
        )?)
    } else {
        None
    };
```

**File:** aptos-node/src/lib.rs (L853-871)
```rust
    Ok(AptosHandle {
        _admin_service: admin_service,
        _api_runtime: api_runtime,
        _backup_runtime: backup_service,
        _consensus_observer_runtime: consensus_observer_runtime,
        _consensus_publisher_runtime: consensus_publisher_runtime,
        _consensus_runtime: consensus_runtime,
        _dkg_runtime: dkg_runtime,
        _indexer_grpc_runtime: indexer_grpc_runtime,
        _indexer_runtime: indexer_runtime,
        _indexer_table_info_runtime: indexer_table_info_runtime,
        _jwk_consensus_runtime: jwk_consensus_runtime,
        _mempool_runtime: mempool_runtime,
        _network_runtimes: network_runtimes,
        _peer_monitoring_service_runtime: peer_monitoring_service_runtime,
        _state_sync_runtimes: state_sync_runtimes,
        _telemetry_runtime: telemetry_runtime,
        _indexer_db_runtime: internal_indexer_db_runtime,
    })
```
