# Audit Report

## Title
Late Signature Verification in JWK Consensus Enables Validator-Based Lock Contention DoS

## Summary
The JWK consensus `ObservationAggregationState::add()` method verifies BLS signatures after acquiring a mutex lock and performing multiple checks. Combined with the reliable broadcast's automatic retry mechanism, a malicious validator can repeatedly send responses with invalid signatures to cause lock contention and resource exhaustion through unbounded retry loops.

## Finding Description
The signature verification in the observation aggregation occurs late in the validation pipeline: [1](#0-0) 

The vulnerability chain:
1. Lock acquired on `partial_sigs` state before signature verification
2. Signature verification occurs after lock acquisition, duplicate check, and view matching
3. When verification fails (line 89), the error propagates via `?` operator WITHOUT adding the sender to the voter set
4. The reliable broadcast treats signature verification failures as transient RPC errors and automatically retries: [2](#0-1) 

5. Since the malicious validator was never added to `partial_sigs` (line 92 only executes after successful verification), the duplicate voter check at line 77 does NOT filter subsequent attempts
6. Each retry acquires the lock, performs checks, and fails signature verification again

A malicious validator can exploit this by:
- Responding to observation requests with correctly structured but cryptographically invalid signatures
- Causing the requester to retry indefinitely (with exponential backoff)
- Each retry acquires the lock before failing, blocking legitimate validators
- Multiple malicious validators can amplify this attack across observation sessions

## Impact Explanation
**High Severity** - This qualifies as "Validator node slowdowns" per the bug bounty criteria. The vulnerability enables:
- Lock contention preventing legitimate signature aggregation progress
- CPU waste on repeated invalid signature verification attempts  
- Network bandwidth consumption from retry amplification
- Degraded JWK consensus liveness across the validator set

While this doesn't break consensus safety directly, it impairs the JWK update mechanism which is critical for keyless account functionality. The attack can persist across multiple observation sessions and scales with the number of compromised validators.

## Likelihood Explanation
**Medium Likelihood** - Requires a compromised validator with network access. However:
- No validator collusion required (single malicious validator sufficient)
- Attack is sustainable and repeatable
- Detection is difficult (appears as network/cryptographic failures)
- Exponential backoff mitigates but doesn't eliminate the issue
- The vulnerability is inherent to the control flow, not an edge case

## Recommendation
Verify signatures BEFORE acquiring any shared locks. Move signature verification to the earliest possible point:

```rust
fn add(
    &self,
    sender: Author,
    response: Self::Response,
) -> anyhow::Result<Option<Self::Aggregated>> {
    let ObservedUpdateResponse { epoch, update } = response;
    let ObservedUpdate {
        author,
        observed: peer_view,
        signature,
    } = update;
    
    ensure!(epoch == self.epoch_state.epoch, "invalid epoch");
    ensure!(author == sender, "mismatched author");
    
    let peer_power = self.epoch_state.verifier.get_voting_power(&author);
    ensure!(peer_power.is_some(), "illegal signer");
    
    // VERIFY SIGNATURE EARLY - BEFORE LOCK ACQUISITION
    self.epoch_state.verifier.verify(sender, &peer_view, &signature)?;
    
    // Now acquire lock only after signature verification passes
    let mut partial_sigs = self.inner_state.lock();
    if partial_sigs.contains_voter(&sender) {
        return Ok(None);
    }
    
    ensure!(self.local_view == peer_view, "mismatched view");
    
    // Signature already verified above
    partial_sigs.add_signature(sender, signature);
    // ... rest of aggregation logic
}
```

Additionally, consider adding sender tracking to prevent retry storms from validators that repeatedly send invalid signatures.

## Proof of Concept
```rust
// Test demonstrating the vulnerability
// File: crates/aptos-jwk-consensus/src/observation_aggregation/tests.rs

#[tokio::test]
async fn test_invalid_signature_retry_amplification() {
    use aptos_crypto::bls12381::PrivateKey;
    
    let validators = create_test_validators(4);
    let epoch_state = create_test_epoch_state(&validators);
    
    // Malicious validator's key
    let malicious_key = PrivateKey::generate_for_testing();
    let malicious_addr = validators[0].address;
    
    // Legitimate observation
    let observed = create_test_provider_jwks();
    
    // Create response with WRONG signature (signed with different key)
    let wrong_signature = malicious_key.sign(&observed).unwrap();
    let response = ObservedUpdateResponse {
        epoch: epoch_state.epoch,
        update: ObservedUpdate {
            author: malicious_addr,
            observed: observed.clone(),
            signature: wrong_signature,
        },
    };
    
    let agg_state = Arc::new(ObservationAggregationState::<PerIssuerMode>::new(
        Arc::new(epoch_state),
        observed.clone(),
    ));
    
    // First attempt - should fail signature verification
    let result1 = agg_state.add(malicious_addr, response.clone());
    assert!(result1.is_err());
    assert!(result1.unwrap_err().to_string().contains("signature"));
    
    // Second attempt - duplicate check DOES NOT filter because 
    // signature was never added to partial_sigs
    let result2 = agg_state.add(malicious_addr, response.clone());
    assert!(result2.is_err());
    
    // Subsequent attempts continue to fail with lock acquisition overhead
    // In production, reliable broadcast would retry with exponential backoff
    for _ in 0..10 {
        let result = agg_state.add(malicious_addr, response.clone());
        assert!(result.is_err()); // Each acquires lock before failing
    }
    
    // The lock contention would be measurable in production with timing:
    // - Each add() call acquires mutex
    // - Legitimate validators' add() calls are delayed by malicious attempts
}
```

**Notes**

While the network layer provides transport-level authentication via Noise handshake, this does not verify the cryptographic signatures on consensus message payloads. The vulnerability specifically concerns APPLICATION-LAYER signature verification timing within the JWK consensus aggregation logic, not network authentication.

The vulnerability requires a compromised validator, which technically falls under the "Trusted Roles" category. However, Byzantine fault tolerance assumes up to 1/3 malicious validators, and the bug bounty explicitly covers "Validator node slowdowns" as High severity. The system should be resilient against malicious validators attempting DoS attacks through protocol abuse.

### Citations

**File:** crates/aptos-jwk-consensus/src/observation_aggregation/mod.rs (L76-92)
```rust
        let mut partial_sigs = self.inner_state.lock();
        if partial_sigs.contains_voter(&sender) {
            return Ok(None);
        }

        ensure!(
            self.local_view == peer_view,
            "adding peer observation failed with mismatched view"
        );

        // Verify peer signature.
        self.epoch_state
            .verifier
            .verify(sender, &peer_view, &signature)?;

        // All checks passed. Aggregating.
        partial_sigs.add_signature(sender, signature);
```

**File:** crates/reliable-broadcast/src/lib.rs (L183-200)
```rust
                    Some(result) = aggregate_futures.next() => {
                        let (receiver, result) = result.expect("spawned task must succeed");
                        match result {
                            Ok(may_be_aggragated) => {
                                if let Some(aggregated) = may_be_aggragated {
                                    return Ok(aggregated);
                                }
                            },
                            Err(e) => {
                                log_rpc_failure(e, receiver);

                                let backoff_strategy = backoff_policies
                                    .get_mut(&receiver)
                                    .expect("should be present");
                                let duration = backoff_strategy.next().expect("should produce value");
                                rpc_futures
                                    .push(send_message(receiver, Some(duration)));
                            },
```
