# Audit Report

## Title
Unvalidated Storage Summary Injection Enables Peer Selection Manipulation and State Sync Disruption

## Summary
The `update_peer_storage_summary()` function accepts peer-supplied storage summaries without cryptographic validation, allowing malicious peers to inject false data advertisements, manipulate the global data summary, bias peer selection, and waste node resources during state synchronization.

## Finding Description

The vulnerability exists in the peer storage summary update flow where unvalidated peer-supplied data is directly accepted and incorporated into the global data summary. [1](#0-0) 

When the data summary poller queries peers for their storage summaries, the responses are accepted without validation: [2](#0-1) 

The storage summary is then directly stored in the peer state without any signature verification or cryptographic validation: [3](#0-2) 

The `StorageServerSummary` contains critical information including a `synced_ledger_info` field with signatures that should be verified, but isn't: [4](#0-3) [5](#0-4) 

The unvalidated summaries are then aggregated into the global data summary, which directly influences peer selection: [6](#0-5) 

For optimistic fetch requests (critical for state sync), the validation only checks timestamp lag, not signature validity: [7](#0-6) [8](#0-7) 

**Attack Flow:**
1. Malicious peer crafts a `StorageServerSummary` with:
   - Fake `synced_ledger_info` claiming very high version/epoch (with invalid or missing signatures)
   - Current timestamp to pass lag checks
   - False data ranges claiming to have all data
   - Extreme chunk sizes (very high or very low)

2. Node polls peer and receives malicious summary
3. Summary is stored without signature verification
4. Malicious data is aggregated into global summary
5. Node selects malicious peer for data requests (due to advertised high version)
6. Requests timeout or return invalid data, wasting resources
7. Multiple malicious peers can skew optimal chunk sizes via median manipulation

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos Bug Bounty criteria for "Validator node slowdowns" because:

1. **State Sync Performance Degradation**: During initial sync or catch-up, the node will repeatedly select malicious peers advertising the highest versions, wasting network bandwidth and CPU cycles on timeouts and failed requests before the scoring system reduces their priority.

2. **Resource Exhaustion**: Each failed request consumes network bandwidth, connection slots, and processing time. With multiple malicious peers, this compounds significantly.

3. **Chunk Size Manipulation**: The global optimal chunk sizes are calculated using the median of all peer-advertised values. Coordinated malicious peers can skew these values to inefficient extremes (e.g., chunk size of 1 or MAX_U64), degrading performance for ALL subsequent requests, not just those to malicious peers.

4. **Delayed Network Participation**: Nodes attempting to join or catch up to the network will experience significant delays, potentially affecting network decentralization and validator rotation.

While the peer scoring mechanism eventually mitigates individual malicious peers, the initial resource waste and the global chunk size manipulation persist.

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability is highly likely to be exploited because:

1. **Trivial Exploitation**: Any network peer can send malicious storage summaries - no privileged access, cryptographic keys, or complex setup required.

2. **No Authentication**: The storage summary protocol has no cryptographic verification, making it trivial for attackers to craft fake responses.

3. **Immediate Impact**: The attack takes effect immediately upon the node polling the malicious peer, requiring no waiting periods or complex timing.

4. **Low Cost**: An attacker only needs to run a modified peer node that responds to storage summary requests with fabricated data.

5. **Persistent Effect**: Chunk size manipulation affects all future requests, not just those to the malicious peer.

## Recommendation

Implement cryptographic validation of storage summaries before accepting them:

1. **Verify Ledger Info Signatures**: Before accepting a `synced_ledger_info`, verify its signatures against the current validator set:

```rust
pub fn update_peer_storage_summary(&self, peer: PeerNetworkId, summary: StorageServerSummary) {
    // Verify the synced_ledger_info signatures if present
    if let Some(ref ledger_info) = summary.data_summary.synced_ledger_info {
        // Get the validator verifier for the appropriate epoch
        if let Err(e) = self.verify_ledger_info_signatures(ledger_info) {
            warn!("Rejecting storage summary from peer {} due to invalid signatures: {}", peer, e);
            self.peer_states.update_score_error(peer, ErrorType::Malicious);
            return;
        }
    }
    
    self.peer_states.update_summary(peer, summary)
}
```

2. **Sanity Check Advertised Ranges**: Validate that advertised data ranges are reasonable compared to the node's known state.

3. **Bound Chunk Sizes**: Enforce minimum and maximum bounds on advertised chunk sizes before using them in median calculations, preventing extreme outlier manipulation.

4. **Rate Limit Summary Updates**: Prevent malicious peers from flooding with constantly changing summaries.

## Proof of Concept

```rust
// Malicious peer implementation that sends fake storage summary
use aptos_storage_service_types::responses::{
    StorageServerSummary, DataSummary, ProtocolMetadata, CompleteDataRange
};
use aptos_types::ledger_info::LedgerInfoWithSignatures;

// Create fake storage summary
fn create_malicious_summary() -> StorageServerSummary {
    // Create fake ledger info claiming very high version
    let fake_ledger_info = create_fake_ledger_info_with_high_version(
        999999999, // fake high version
        10000,     // fake high epoch
    );
    
    StorageServerSummary {
        protocol_metadata: ProtocolMetadata {
            max_epoch_chunk_size: u64::MAX, // Extreme chunk size to skew median
            max_state_chunk_size: 1,         // Another extreme
            max_transaction_chunk_size: u64::MAX,
            max_transaction_output_chunk_size: 1,
        },
        data_summary: DataSummary {
            synced_ledger_info: Some(fake_ledger_info),
            epoch_ending_ledger_infos: Some(CompleteDataRange::new(0, 10000).unwrap()),
            states: Some(CompleteDataRange::new(0, 999999999).unwrap()),
            transactions: Some(CompleteDataRange::new(0, 999999999).unwrap()),
            transaction_outputs: Some(CompleteDataRange::new(0, 999999999).unwrap()),
        },
    }
}

// When honest node polls this malicious peer:
// 1. Malicious summary is accepted without validation
// 2. Node believes peer has highest version (999999999)
// 3. Node selects this peer for optimistic fetches
// 4. Requests timeout or fail, wasting resources
// 5. Chunk sizes are skewed by extreme values in median calculation
```

## Notes

The vulnerability fundamentally violates the assumption that peer-advertised data should be cryptographically verifiable before trust. While the peer scoring mechanism provides eventual mitigation for individual bad actors, it does not prevent:

1. Initial resource waste during the scoring period
2. Coordinated attacks by multiple malicious peers
3. Persistent chunk size manipulation affecting all requests
4. Disruption during critical sync periods (initial bootstrap, post-downtime catch-up)

The fix requires adding proper cryptographic validation at the point where storage summaries are received, treating all peer-supplied data as untrusted until verified.

### Citations

**File:** state-sync/aptos-data-client/src/client.rs (L213-215)
```rust
    pub fn update_peer_storage_summary(&self, peer: PeerNetworkId, summary: StorageServerSummary) {
        self.peer_states.update_summary(peer, summary)
    }
```

**File:** state-sync/aptos-data-client/src/poller.rs (L406-439)
```rust
        let data_request = DataRequest::GetStorageServerSummary;
        let use_compression = data_summary_poller.data_client_config.use_compression;
        let storage_request = StorageServiceRequest::new(data_request, use_compression);

        // Fetch the storage summary for the peer and stop the timer
        let request_timeout = data_summary_poller.data_client_config.response_timeout_ms;
        let result: crate::error::Result<StorageServerSummary> = data_summary_poller
            .data_client
            .send_request_to_peer_and_decode(peer, storage_request, request_timeout)
            .await
            .map(Response::into_payload);

        // Mark the in-flight poll as now complete
        data_summary_poller.in_flight_request_complete(&peer);

        // Check the storage summary response
        let storage_summary = match result {
            Ok(storage_summary) => storage_summary,
            Err(error) => {
                warn!(
                    (LogSchema::new(LogEntry::StorageSummaryResponse)
                        .event(LogEvent::PeerPollingError)
                        .message("Error encountered when polling peer!")
                        .error(&error)
                        .peer(&peer))
                );
                return;
            },
        };

        // Update the summary for the peer
        data_summary_poller
            .data_client
            .update_peer_storage_summary(peer, storage_summary);
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L325-330)
```rust
    pub fn update_summary(&self, peer: PeerNetworkId, storage_summary: StorageServerSummary) {
        self.peer_to_state
            .entry(peer)
            .or_insert(PeerState::new(self.data_client_config.clone()))
            .update_storage_summary(storage_summary);
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L338-408)
```rust
    /// Calculates a global data summary using all known storage summaries
    pub fn calculate_global_data_summary(&self) -> GlobalDataSummary {
        // Gather all storage summaries, but exclude peers that are ignored
        let storage_summaries: Vec<StorageServerSummary> = self
            .peer_to_state
            .iter()
            .filter_map(|peer_state| {
                peer_state
                    .value()
                    .get_storage_summary_if_not_ignored()
                    .cloned()
            })
            .collect();

        // If we have no peers, return an empty global summary
        if storage_summaries.is_empty() {
            return GlobalDataSummary::empty();
        }

        // Calculate the global data summary using the advertised peer data
        let mut advertised_data = AdvertisedData::empty();
        let mut max_epoch_chunk_sizes = vec![];
        let mut max_state_chunk_sizes = vec![];
        let mut max_transaction_chunk_sizes = vec![];
        let mut max_transaction_output_chunk_sizes = vec![];
        for summary in storage_summaries {
            // Collect aggregate data advertisements
            if let Some(epoch_ending_ledger_infos) = summary.data_summary.epoch_ending_ledger_infos
            {
                advertised_data
                    .epoch_ending_ledger_infos
                    .push(epoch_ending_ledger_infos);
            }
            if let Some(states) = summary.data_summary.states {
                advertised_data.states.push(states);
            }
            if let Some(synced_ledger_info) = summary.data_summary.synced_ledger_info.as_ref() {
                advertised_data
                    .synced_ledger_infos
                    .push(synced_ledger_info.clone());
            }
            if let Some(transactions) = summary.data_summary.transactions {
                advertised_data.transactions.push(transactions);
            }
            if let Some(transaction_outputs) = summary.data_summary.transaction_outputs {
                advertised_data
                    .transaction_outputs
                    .push(transaction_outputs);
            }

            // Collect preferred max chunk sizes
            max_epoch_chunk_sizes.push(summary.protocol_metadata.max_epoch_chunk_size);
            max_state_chunk_sizes.push(summary.protocol_metadata.max_state_chunk_size);
            max_transaction_chunk_sizes.push(summary.protocol_metadata.max_transaction_chunk_size);
            max_transaction_output_chunk_sizes
                .push(summary.protocol_metadata.max_transaction_output_chunk_size);
        }

        // Calculate optimal chunk sizes based on the advertised data
        let optimal_chunk_sizes = calculate_optimal_chunk_sizes(
            &self.data_client_config,
            max_epoch_chunk_sizes,
            max_state_chunk_sizes,
            max_transaction_chunk_sizes,
            max_transaction_output_chunk_sizes,
        );
        GlobalDataSummary {
            advertised_data,
            optimal_chunk_sizes,
        }
    }
```

**File:** state-sync/storage-service/types/src/responses.rs (L613-616)
```rust
pub struct StorageServerSummary {
    pub protocol_metadata: ProtocolMetadata,
    pub data_summary: DataSummary,
}
```

**File:** state-sync/storage-service/types/src/responses.rs (L666-686)
```rust
#[derive(Clone, Debug, Default, Deserialize, Eq, PartialEq, Serialize)]
pub struct DataSummary {
    /// The ledger info corresponding to the highest synced version in storage.
    /// This indicates the highest version and epoch that storage can prove.
    pub synced_ledger_info: Option<LedgerInfoWithSignatures>,
    /// The range of epoch ending ledger infos in storage, e.g., if the range
    /// is [(X,Y)], it means all epoch ending ledger infos for epochs X->Y
    /// (inclusive) are held.
    pub epoch_ending_ledger_infos: Option<CompleteDataRange<Epoch>>,
    /// The range of states held in storage, e.g., if the range is
    /// [(X,Y)], it means all states are held for every version X->Y
    /// (inclusive).
    pub states: Option<CompleteDataRange<Version>>,
    /// The range of transactions held in storage, e.g., if the range is
    /// [(X,Y)], it means all transactions for versions X->Y (inclusive) are held.
    pub transactions: Option<CompleteDataRange<Version>>,
    /// The range of transaction outputs held in storage, e.g., if the range
    /// is [(X,Y)], it means all transaction outputs for versions X->Y
    /// (inclusive) are held.
    pub transaction_outputs: Option<CompleteDataRange<Version>>,
}
```

**File:** state-sync/storage-service/types/src/responses.rs (L894-901)
```rust
fn can_service_optimistic_request(
    aptos_data_client_config: &AptosDataClientConfig,
    time_service: TimeService,
    synced_ledger_info: Option<&LedgerInfoWithSignatures>,
) -> bool {
    let max_lag_secs = aptos_data_client_config.max_optimistic_fetch_lag_secs;
    check_synced_ledger_lag(synced_ledger_info, time_service, max_lag_secs)
}
```

**File:** state-sync/storage-service/types/src/responses.rs (L916-934)
```rust
fn check_synced_ledger_lag(
    synced_ledger_info: Option<&LedgerInfoWithSignatures>,
    time_service: TimeService,
    max_lag_secs: u64,
) -> bool {
    if let Some(synced_ledger_info) = synced_ledger_info {
        // Get the ledger info timestamp (in microseconds)
        let ledger_info_timestamp_usecs = synced_ledger_info.ledger_info().timestamp_usecs();

        // Get the current timestamp and max version lag (in microseconds)
        let current_timestamp_usecs = time_service.now_unix_time().as_micros() as u64;
        let max_version_lag_usecs = max_lag_secs * NUM_MICROSECONDS_IN_SECOND;

        // Return true iff the synced ledger info timestamp is within the max version lag
        ledger_info_timestamp_usecs + max_version_lag_usecs > current_timestamp_usecs
    } else {
        false // No synced ledger info was found!
    }
}
```
