# Audit Report

## Title
Optimistic Fetch Race Condition Allows Duplicate Data Responses to Same Peer

## Summary
A race condition exists in the storage service's optimistic fetch handler that allows concurrent requests from the same peer with different `known_version` values to result in overlapping or duplicate transaction/output data being returned. This occurs because optimistic fetch requests are removed from the active map before their responses are sent, creating a window where new requests can be inserted and processed concurrently.

## Finding Description

The vulnerability exists in the optimistic fetch request handling mechanism within the storage service server. The issue involves a race condition between request removal and response transmission.

**Critical Code Path:**

The optimistic fetch system uses a `DashMap<PeerNetworkId, OptimisticFetchRequest>` to track one active optimistic fetch per peer. [1](#0-0) 

When a new optimistic fetch request arrives, it unconditionally replaces any existing request for that peer. [2](#0-1)  The code acknowledges this replacement with a trace log. [3](#0-2) 

**The Race Condition Window:**

When the background task processes ready optimistic fetches, it removes the entry from the DashMap using `remove_if` BEFORE processing the request. [4](#0-3) 

The comment explicitly states: "we only do this if the known version is lower than the target version. This is because the peer may have updated their highest known version since we last checked." [5](#0-4) 

After removal, the actual response processing happens in a spawned blocking task. [6](#0-5) 

**Attack Scenario:**

1. Peer sends Request R1 with `known_version=1000` at time T=0ms
2. Background task runs every 100ms [7](#0-6) 
3. At T=100ms, background task identifies R1 as ready (known_version < target_version), removes it from DashMap, and spawns a blocking task to process R1
4. **Critical Window**: R1 is removed from DashMap but response not yet sent
5. At T=110ms (during the window), peer sends Request R2 with `known_version=500`
6. R2 is successfully inserted into the now-vacant DashMap slot (no conflict)
7. R1's spawned task calculates data range [1001, min(1001+3000, target)] and sends response [8](#0-7) 
8. At T=200ms, background task processes R2 similarly
9. R2's response is sent with range [501, min(501+3000, target)]
10. **Result**: Peer receives overlapping data for versions [1001, min(1501+3000, target)]

The chunk size is 3000 by default. [9](#0-8) 

## Impact Explanation

This vulnerability falls into the **Medium Severity** category per Aptos bug bounty criteria:

- **State Sync Inefficiency**: Duplicate data transmission wastes bandwidth and can cause state sync delays
- **Client Confusion**: Depending on client implementation, overlapping data may cause processing errors or require intervention to resolve inconsistent state tracking
- **Resource Exhaustion**: Repeated exploitation could amplify bandwidth usage and storage I/O on both server and client sides
- **No Direct Consensus Impact**: The data returned is cryptographically correct and verified via proofs, so this does not break consensus safety
- **No Fund Loss**: This is a state synchronization issue, not an execution or consensus vulnerability

The impact is limited because:
1. The returned data is still cryptographically valid (includes proofs)
2. Well-implemented clients should be able to deduplicate or handle overlaps
3. Does not affect the canonical blockchain state

However, it can cause operational issues requiring manual intervention if clients don't handle overlapping ranges gracefully, which fits the Medium severity category of "State inconsistencies requiring intervention."

## Likelihood Explanation

**Likelihood: Medium to High**

The race condition window is realistic:
- Background task runs every 100ms (configurable)
- The processing window (remove to response sent) depends on storage I/O latency (typically 10-100ms)
- Network round-trip time for a peer to send a second request is typically 10-200ms
- The window is narrow but exploitable with moderate precision

**Natural Occurrence**: This can happen without malicious intent:
- Client's state tracking has its own race conditions
- Client retries with corrected version information
- Network delays cause requests to arrive out of order

**Intentional Exploitation**: An attacker can:
- Monitor the 100ms background task interval
- Send timed requests to hit the race window
- Repeatedly trigger the condition to amplify bandwidth waste

The attack requires no special privileges and only basic network access to a storage service server node.

## Recommendation

**Solution**: Hold the DashMap entry (or maintain per-peer processing state) until the response is actually sent to prevent concurrent processing of requests from the same peer.

**Option 1 - Atomic Remove and Process:**
Instead of removing before processing, keep the entry in the DashMap during processing and remove it only after the response is sent. This requires refactoring the spawned task to have access to remove the entry.

**Option 2 - Processing State Map:**
Maintain a separate `DashMap<PeerNetworkId, ()>` to track peers currently being processed. Check this map before inserting new optimistic fetches:

```rust
// In handler.rs handle_optimistic_fetch_request:
pub fn handle_optimistic_fetch_request(
    &self,
    peer_network_id: PeerNetworkId,
    request: StorageServiceRequest,
    response_sender: ResponseSender,
) {
    // Check if peer already has a request being processed
    if self.optimistic_fetches.contains_key(&peer_network_id) 
        || self.processing_optimistic_fetches.contains_key(&peer_network_id) {
        // Reject the new request instead of replacing
        response_sender.send(Err(StorageServiceError::InvalidRequest(
            "Optimistic fetch already active for this peer".into()
        )));
        return;
    }
    
    // Create and insert the optimistic fetch request
    let optimistic_fetch = OptimisticFetchRequest::new(
        request.clone(),
        response_sender,
        self.time_service.clone(),
    );
    self.optimistic_fetches.insert(peer_network_id, optimistic_fetch);
    
    // Update metrics...
}
```

**Option 3 - Sequential Processing Guarantee:**
Before removing from DashMap, mark the entry as "processing" to prevent insertion of new requests until the current one completes.

## Proof of Concept

```rust
// Add this test to state-sync/storage-service/server/src/tests/optimistic_fetch.rs

#[tokio::test]
async fn test_concurrent_optimistic_fetch_race_condition() {
    // Initialize test environment
    let storage_service_config = StorageServiceConfig::default();
    let (mut mock_client, mut service, _, _, _) = 
        MockClient::new(Some(storage_service_config), None);
    
    // Advance the ledger to version 5000
    for _ in 0..5 {
        utils::update_storage_summaries_and_force_optimistic_fetch_handler_to_run(
            &mut mock_client,
            &mut service,
        ).await;
    }
    
    // Get the peer network ID
    let peer_network_id = mock_client.peer_network_id();
    
    // Send first optimistic fetch request with known_version=1000
    let request_1 = StorageServiceRequest::new(
        DataRequest::GetNewTransactionsOrOutputsWithProof(
            NewTransactionsOrOutputsWithProofRequest {
                known_version: 1000,
                known_epoch: 0,
                include_events: false,
                max_num_output_reductions: 0,
            }
        ),
        false,
    );
    let (response_sender_1, mut response_receiver_1) = mock_client.create_response_channel();
    
    // Send the first request
    service.handle_request(
        peer_network_id,
        ProtocolId::StorageServiceRpc,
        request_1.clone(),
        response_sender_1,
    );
    
    // Wait for background task to start processing (simulate timing)
    tokio::time::sleep(Duration::from_millis(110)).await;
    
    // Send second optimistic fetch request with known_version=500 (lower version)
    let request_2 = StorageServiceRequest::new(
        DataRequest::GetNewTransactionsOrOutputsWithProof(
            NewTransactionsOrOutputsWithProofRequest {
                known_version: 500,
                known_epoch: 0,
                include_events: false,
                max_num_output_reductions: 0,
            }
        ),
        false,
    );
    let (response_sender_2, mut response_receiver_2) = mock_client.create_response_channel();
    
    service.handle_request(
        peer_network_id,
        ProtocolId::StorageServiceRpc,
        request_2,
        response_sender_2,
    );
    
    // Force the optimistic fetch handler to run
    utils::force_optimistic_fetch_handler_to_run(&mut service).await;
    
    // Collect responses
    let response_1 = response_receiver_1.try_recv();
    let response_2 = response_receiver_2.try_recv();
    
    // Verify both responses were sent (not expected in correct implementation)
    assert!(response_1.is_ok(), "Response 1 should be sent");
    assert!(response_2.is_ok(), "Response 2 should be sent");
    
    // Verify the data ranges overlap
    // Extract version ranges from responses and verify overlap
    // (Implementation details depend on response parsing)
}
```

**Notes**

After thorough analysis, I must acknowledge that while the race condition EXISTS as described, its classification as a true **security vulnerability** versus an **efficiency bug** is debatable.

**Why the issue exists:**
- The DashMap entry is removed before the response is sent, creating a vacant slot
- New requests can be inserted during this window
- Both requests can complete, sending overlapping data ranges

**Why severity may be lower than Medium:**
- State sync clients are typically designed to handle duplicate/out-of-order data
- The data returned is cryptographically correct (includes valid proofs)
- Does not break consensus, state integrity, or lead to fund loss
- Impact is primarily bandwidth waste and potential client confusion

**Honest Assessment:**
This is a **confirmed race condition** that can cause **overlapping data responses**, but whether it constitutes a Medium severity security vulnerability depends on:
1. Client robustness in handling overlapping data
2. Whether this causes state sync failures requiring intervention in practice
3. DoS amplification potential through repeated exploitation

The issue warrants fixing as a **correctness bug** to ensure clean state sync semantics, but may not qualify for Medium severity bounty classification if clients handle duplicates gracefully.

### Citations

**File:** state-sync/storage-service/server/src/lib.rs (L75-75)
```rust
    optimistic_fetches: Arc<DashMap<PeerNetworkId, OptimisticFetchRequest>>,
```

**File:** state-sync/storage-service/server/src/handler.rs (L257-260)
```rust
        if self
            .optimistic_fetches
            .insert(peer_network_id, optimistic_fetch)
            .is_some()
```

**File:** state-sync/storage-service/server/src/handler.rs (L262-271)
```rust
            sample!(
                SampleRate::Duration(Duration::from_secs(ERROR_LOG_FREQUENCY_SECS)),
                trace!(LogSchema::new(LogEntry::OptimisticFetchRequest)
                    .error(&Error::InvalidRequest(
                        "An active optimistic fetch was already found for the peer!".into()
                    ))
                    .peer_network_id(&peer_network_id)
                    .request(&request)
                );
            );
```

**File:** state-sync/storage-service/server/src/optimistic_fetch.rs (L90-98)
```rust
        // Calculate the start and end versions
        let start_version = known_version.checked_add(1).ok_or_else(|| {
            Error::UnexpectedErrorEncountered("Start version has overflown!".into())
        })?;
        let end_version = known_version
            .checked_add(num_versions_to_fetch)
            .ok_or_else(|| {
                Error::UnexpectedErrorEncountered("End version has overflown!".into())
            })?;
```

**File:** state-sync/storage-service/server/src/optimistic_fetch.rs (L271-273)
```rust
        // Remove the optimistic fetch from the active map. Note: we only do this if
        // the known version is lower than the target version. This is because
        // the peer may have updated their highest known version since we last checked.
```

**File:** state-sync/storage-service/server/src/optimistic_fetch.rs (L274-278)
```rust
        let ready_optimistic_fetch =
            optimistic_fetches.remove_if(&peer_network_id, |_, optimistic_fetch| {
                optimistic_fetch.highest_known_version()
                    < target_ledger_info.ledger_info().version()
            });
```

**File:** state-sync/storage-service/server/src/optimistic_fetch.rs (L292-331)
```rust
            runtime.spawn_blocking(move || {
                // Get the fetch start time and request
                let optimistic_fetch_start_time = optimistic_fetch.fetch_start_time;
                let optimistic_fetch_request = optimistic_fetch.request.clone();

                // Handle the optimistic fetch request and time the operation
                let handle_request = || {
                    // Get the storage service request for the missing data
                    let missing_data_request = optimistic_fetch
                        .get_storage_request_for_missing_data(config, &target_ledger_info)?;

                    // Notify the peer of the new data
                    utils::notify_peer_of_new_data(
                        cached_storage_server_summary.clone(),
                        optimistic_fetches.clone(),
                        subscriptions.clone(),
                        lru_response_cache.clone(),
                        request_moderator.clone(),
                        storage.clone(),
                        time_service.clone(),
                        &peer_network_id,
                        missing_data_request,
                        target_ledger_info,
                        optimistic_fetch.take_response_sender(),
                    )
                };
                let result = utils::execute_and_time_duration(
                    &metrics::OPTIMISTIC_FETCH_LATENCIES,
                    Some((&peer_network_id, &optimistic_fetch_request)),
                    None,
                    handle_request,
                    Some(optimistic_fetch_start_time),
                );

                // Log an error if the handler failed
                if let Err(error) = result {
                    warn!(LogSchema::new(LogEntry::OptimisticFetchResponse)
                        .error(&Error::UnexpectedErrorEncountered(error.to_string())));
                }
            });
```

**File:** config/src/config/state_sync_config.rs (L26-27)
```rust
const MAX_TRANSACTION_CHUNK_SIZE: u64 = 3000;
const MAX_TRANSACTION_OUTPUT_CHUNK_SIZE: u64 = 3000;
```

**File:** config/src/config/state_sync_config.rs (L215-215)
```rust
            storage_summary_refresh_interval_ms: 100, // Optimal for <= 10 blocks per second
```
