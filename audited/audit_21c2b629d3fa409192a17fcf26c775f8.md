# Audit Report

## Title
Mempool Broadcast Starvation: ACK Timeout Less Than Tick Interval Breaks Transaction Synchronization

## Summary
When `shared_mempool_ack_timeout_ms` is configured to be less than `shared_mempool_tick_interval_ms`, the mempool broadcast mechanism enters a pathological state where expired broadcasts continuously starve new transaction propagation, breaking synchronization between nodes and degrading network liveness.

## Finding Description

The mempool uses two critical timing parameters to manage transaction broadcasts between nodes:
- `shared_mempool_tick_interval_ms`: Interval between broadcast attempts (default: 10ms)
- `shared_mempool_ack_timeout_ms`: Maximum time to wait for an ACK before considering a broadcast expired (default: 2000ms) [1](#0-0) 

The vulnerability lies in the broadcast batch determination logic. When preparing a broadcast, the system checks all pending broadcasts to identify expired messages: [2](#0-1) 

The critical flaw occurs when `ack_timeout_ms < tick_interval_ms`. In this scenario:

1. **T=0ms**: Node broadcasts transaction batch M1 to peer, records timestamp in `sent_messages`
2. **T=50ms**: M1 deadline passes (`ack_timeout_ms = 50ms`)
3. **T=100ms**: Next broadcast tick (`tick_interval_ms = 100ms`) executes
   - The expiration check finds M1 has exceeded its deadline
   - M1 is marked as an "expired" broadcast

The expired broadcast is then prioritized for rebroadcast over any new transactions: [3](#0-2) 

When an expired message exists, the code path returns it for rebroadcast (lines 452-489). The fresh broadcast logic (lines 490-562) that would send new transactions from the mempool timeline is **never reached**: [4](#0-3) 

After rebroadcasting, the timestamp is updated, but the message will expire again before the next tick, creating an infinite rebroadcast loop that blocks all new transaction propagation. [5](#0-4) 

**Critical Impact for Validators**: The situation is especially severe for validators, where `max_broadcasts_per_peer` is automatically set to 2 (compared to 20 for fullnodes): [6](#0-5) 

**No Configuration Validation**: The configuration sanitizer explicitly has a TODO for "reasonable verifications" but performs no actual validation: [7](#0-6) 

**Attack Scenario:**
1. Operator misconfigures node with `tick_interval_ms = 100` and `ack_timeout_ms = 50`
2. Every broadcast to peer P immediately enters rebroadcast loop
3. New transactions in mempool never get broadcast to peer P
4. Peer P never receives transactions from this node
5. Transaction propagation across the validator network degrades
6. Consensus may be impacted if sufficient validators are misconfigured

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos bug bounty program:

1. **Validator Node Slowdowns**: Continuous rebroadcasting of the same transactions wastes CPU, network bandwidth, and memory on both sender and receiver nodes. The receiver must process duplicate transactions repeatedly.

2. **Significant Protocol Violations**: The mempool synchronization protocol is fundamentally broken. The invariant that "new transactions propagate to all peers within a reasonable time" is violated. This affects the network's ability to include transactions in blocks.

3. **Network Liveness Impact**: While not a complete network halt, the degradation in transaction propagation directly affects the network's ability to process transactions efficiently. If multiple validators are misconfigured, transaction throughput decreases significantly.

The impact matches the High Severity criteria: "Validator node slowdowns" and "Significant protocol violations."

## Likelihood Explanation

**Likelihood: Medium to High**

While the default configuration is safe (`tick_interval_ms=10ms`, `ack_timeout_ms=2000ms`), several factors increase the likelihood of this occurring in production:

1. **No Configuration Validation**: The absence of sanity checks allows operators to set invalid configurations without any warning or error.

2. **Non-Obvious Constraint**: The dependency relationship between these two parameters is not documented in the configuration file or comments. An operator might reasonably adjust one without considering the other.

3. **Performance Tuning Scenarios**: An operator trying to "optimize" performance might increase `tick_interval_ms` to reduce broadcast frequency without realizing this requires proportionally increasing `ack_timeout_ms`.

4. **Template Copying**: If one misconfigured node template is copied across multiple validators, the issue propagates network-wide.

5. **Testing Artifacts**: Test configurations that disable timeouts (`ack_timeout_ms = u64::MAX`) might be mistakenly modified to small values in production.

## Recommendation

**Immediate Fix**: Add configuration validation in the `ConfigSanitizer::sanitize()` method:

```rust
impl ConfigSanitizer for MempoolConfig {
    fn sanitize(
        _node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        // Ensure ACK timeout is significantly larger than tick interval
        // to allow broadcasts to receive ACKs before being marked expired
        if self.shared_mempool_ack_timeout_ms <= self.shared_mempool_tick_interval_ms {
            return Err(Error::ConfigSanitizerFailed(
                "mempool".to_string(),
                format!(
                    "shared_mempool_ack_timeout_ms ({}) must be greater than shared_mempool_tick_interval_ms ({})",
                    self.shared_mempool_ack_timeout_ms,
                    self.shared_mempool_tick_interval_ms
                )
            ));
        }
        
        // Add reasonable minimum ratio to account for network latency
        let min_ratio = 10; // ACK timeout should be at least 10x tick interval
        if self.shared_mempool_ack_timeout_ms < self.shared_mempool_tick_interval_ms * min_ratio {
            warn!(
                "shared_mempool_ack_timeout_ms ({}) is less than {}x shared_mempool_tick_interval_ms ({}). \
                 This may cause excessive rebroadcasts in high-latency network conditions.",
                self.shared_mempool_ack_timeout_ms,
                min_ratio,
                self.shared_mempool_tick_interval_ms
            );
        }
        
        Ok(())
    }
}
```

**Additional Recommendations:**

1. **Documentation**: Add clear comments in `MempoolConfig` explaining the relationship between these parameters and the minimum safe ratio.

2. **Metrics Monitoring**: Add metrics to track the ratio of expired rebroadcasts vs fresh broadcasts. A high ratio indicates misconfiguration.

3. **Default Safety Margin**: Consider increasing the default `ack_timeout_ms` to provide more safety margin for high-latency networks.

## Proof of Concept

**Configuration Reproduction:**

Create a node configuration file with the problematic values:

```yaml
mempool:
  shared_mempool_tick_interval_ms: 100
  shared_mempool_ack_timeout_ms: 50
  max_broadcasts_per_peer: 2  # Validator default
```

**Expected Behavior:**
1. Start two nodes (Node A and Node B) with the above configuration
2. Submit transactions to Node A's mempool
3. Observe Node A's broadcast behavior to Node B

**Observable Symptoms:**
- `SHARED_MEMPOOL_BROADCAST_TYPE` metric shows high `expired_broadcast` count
- `shared_mempool_pending_broadcasts` metric stays at `max_broadcasts_per_peer` limit
- Node B never receives new transactions from Node A
- Logs show repeated "Execute broadcast for peer" messages with same transaction hashes

**Verification Query:**
```bash
# Check broadcast type distribution
curl http://localhost:9101/metrics | grep "aptos_shared_mempool_broadcast_type"

# If expired_broadcast count >> new broadcast count, configuration is problematic
```

**Notes:**
- This vulnerability requires configuration error, not code exploitation
- The issue is deterministic once the misconfiguration is in place
- Impact scales with number of misconfigured nodes in the network
- Validators are more severely affected due to lower `max_broadcasts_per_peer` limit

### Citations

**File:** config/src/config/mempool_config.rs (L60-71)
```rust
    /// The maximum amount of time to wait for an ACK of Mempool submission to an upstream node.
    pub shared_mempool_ack_timeout_ms: u64,
    /// The amount of time to backoff between retries of Mempool submission to an upstream node.
    pub shared_mempool_backoff_interval_ms: u64,
    /// Maximum number of transactions to batch for a Mempool submission to an upstream node.
    pub shared_mempool_batch_size: usize,
    /// Maximum number of bytes to batch for a Mempool submission to an upstream node.
    pub shared_mempool_max_batch_bytes: u64,
    /// Maximum Mempool inbound message workers.  Controls concurrency of Mempool consumption.
    pub shared_mempool_max_concurrent_inbound_syncs: usize,
    /// Interval to broadcast to upstream nodes.
    pub shared_mempool_tick_interval_ms: u64,
```

**File:** config/src/config/mempool_config.rs (L176-184)
```rust
impl ConfigSanitizer for MempoolConfig {
    fn sanitize(
        _node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        Ok(()) // TODO: add reasonable verifications
    }
}
```

**File:** config/src/config/mempool_config.rs (L198-203)
```rust
        if node_type.is_validator() {
            // Set the max_broadcasts_per_peer to 2 (default is 20)
            if local_mempool_config_yaml["max_broadcasts_per_peer"].is_null() {
                mempool_config.max_broadcasts_per_peer = 2;
                modified_config = true;
            }
```

**File:** mempool/src/shared_mempool/network.rs (L429-449)
```rust
        // Find earliest message in timeline index that expired.
        // Note that state.broadcast_info.sent_messages is ordered in decreasing order in the timeline index
        for (message, sent_time) in state.broadcast_info.sent_messages.iter() {
            let deadline = sent_time.add(Duration::from_millis(
                self.mempool_config.shared_mempool_ack_timeout_ms,
            ));
            if SystemTime::now().duration_since(deadline).is_ok() {
                expired_message_id = Some(message);
            } else {
                pending_broadcasts += 1;
            }

            // The maximum number of broadcasts sent to a single peer that are pending a response ACK at any point.
            // If the number of un-ACK'ed un-expired broadcasts reaches this threshold, we do not broadcast anymore
            // and wait until an ACK is received or a sent broadcast expires.
            // This helps rate-limit egress network bandwidth and not overload a remote peer or this
            // node's network sender.
            if pending_broadcasts >= self.mempool_config.max_broadcasts_per_peer {
                return Err(BroadcastError::TooManyPendingBroadcasts(peer));
            }
        }
```

**File:** mempool/src/shared_mempool/network.rs (L452-489)
```rust
        let (message_id, transactions, metric_label) =
            match std::cmp::max(expired_message_id, retry_message_id) {
                Some(message_id) => {
                    let metric_label = if Some(message_id) == expired_message_id {
                        Some(counters::EXPIRED_BROADCAST_LABEL)
                    } else {
                        Some(counters::RETRY_BROADCAST_LABEL)
                    };

                    let txns = message_id
                        .decode()
                        .into_iter()
                        .flat_map(|(sender_bucket, start_end_pairs)| {
                            if self.node_type.is_validator() {
                                mempool
                                    .timeline_range(sender_bucket, start_end_pairs)
                                    .into_iter()
                                    .map(|(txn, ready_time)| {
                                        (txn, ready_time, BroadcastPeerPriority::Primary)
                                    })
                                    .collect::<Vec<_>>()
                            } else {
                                self.prioritized_peers_state
                                    .get_sender_bucket_priority_for_peer(&peer, sender_bucket)
                                    .map_or_else(Vec::new, |priority| {
                                        mempool
                                            .timeline_range(sender_bucket, start_end_pairs)
                                            .into_iter()
                                            .map(|(txn, ready_time)| {
                                                (txn, ready_time, priority.clone())
                                            })
                                            .collect::<Vec<_>>()
                                    })
                            }
                        })
                        .collect::<Vec<_>>();
                    (message_id.clone(), txns, metric_label)
                },
```

**File:** mempool/src/shared_mempool/network.rs (L490-562)
```rust
                None => {
                    // Fresh broadcast

                    // If the peer doesn't have any sender_buckets assigned, let's not broadcast to the peer
                    let mut sender_buckets: Vec<(MempoolSenderBucket, BroadcastPeerPriority)> =
                        if self.node_type.is_validator() {
                            (0..self.mempool_config.num_sender_buckets)
                                .map(|sender_bucket| {
                                    (sender_bucket, BroadcastPeerPriority::Primary)
                                })
                                .collect()
                        } else {
                            self.prioritized_peers_state
                                .get_sender_buckets_for_peer(&peer)
                                .ok_or_else(|| {
                                    BroadcastError::PeerNotPrioritized(
                                        peer,
                                        self.prioritized_peers_state.get_peer_priority(&peer),
                                    )
                                })?
                                .clone()
                                .into_iter()
                                .collect()
                        };
                    // Sort sender_buckets based on priority. Primary peer should be first.
                    sender_buckets.sort_by(|(_, priority_a), (_, priority_b)| {
                        if priority_a == priority_b {
                            std::cmp::Ordering::Equal
                        } else if *priority_a == BroadcastPeerPriority::Primary {
                            std::cmp::Ordering::Less
                        } else {
                            std::cmp::Ordering::Greater
                        }
                    });
                    let max_txns = self.mempool_config.shared_mempool_batch_size;
                    let mut output_txns = vec![];
                    let mut output_updates = vec![];
                    for (sender_bucket, peer_priority) in sender_buckets {
                        let before = match peer_priority {
                            BroadcastPeerPriority::Primary => None,
                            BroadcastPeerPriority::Failover => Some(
                                Instant::now()
                                    - Duration::from_millis(
                                        self.mempool_config.shared_mempool_failover_delay_ms,
                                    ),
                            ),
                        };
                        if max_txns > 0 {
                            let old_timeline_id = state.timelines.get(&sender_bucket).unwrap();
                            let (txns, new_timeline_id) = mempool.read_timeline(
                                sender_bucket,
                                old_timeline_id,
                                max_txns,
                                before,
                                peer_priority.clone(),
                            );
                            output_txns.extend(
                                txns.into_iter()
                                    .map(|(txn, ready_time)| {
                                        (txn, ready_time, peer_priority.clone())
                                    })
                                    .collect::<Vec<_>>(),
                            );
                            output_updates
                                .push((sender_bucket, (old_timeline_id.clone(), new_timeline_id)));
                        }
                    }
                    (
                        MempoolMessageId::from_timeline_ids(output_updates),
                        output_txns,
                        None,
                    )
                },
```

**File:** mempool/src/shared_mempool/network.rs (L613-634)
```rust
    fn update_broadcast_state(
        &self,
        peer: PeerNetworkId,
        message_id: MempoolMessageId,
        send_time: SystemTime,
    ) -> Result<usize, BroadcastError> {
        let mut sync_states = self.sync_states.write();
        let state = sync_states
            .get_mut(&peer)
            .ok_or_else(|| BroadcastError::PeerNotFound(peer))?;

        // Update peer sync state with info from above broadcast.
        state.update(&message_id);
        // Turn off backoff mode after every broadcast.
        state.broadcast_info.backoff_mode = false;
        state.broadcast_info.retry_messages.remove(&message_id);
        state
            .broadcast_info
            .sent_messages
            .insert(message_id, send_time);
        Ok(state.broadcast_info.sent_messages.len())
    }
```
