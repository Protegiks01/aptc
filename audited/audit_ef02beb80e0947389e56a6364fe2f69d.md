# Audit Report

## Title
Memory Exhaustion via Unbounded BCS Deserialization in State Snapshot Restoration

## Summary
The `read_state_value()` function in state snapshot restoration performs unbounded BCS deserialization of backup records without size limits, allowing memory exhaustion attacks when processing maliciously crafted backup files. This occurs before cryptographic proof verification can reject invalid data.

## Finding Description

The vulnerability exists in the state snapshot restoration code where backup records are deserialized without any size constraints. [1](#0-0) 

The attack flow proceeds as follows:

1. **Record Reading**: The `read_record_bytes()` function reads a 4-byte (u32) size prefix that can specify up to 4GB per record, then allocates memory accordingly. [2](#0-1) 

2. **Unbounded Deserialization**: Each `record_bytes` is deserialized via `bcs::from_bytes(&record_bytes)?` without using the size-limited variant `bcs::from_bytes_with_limit()` that exists elsewhere in the codebase.

3. **Memory Accumulation**: All deserialized records accumulate in the `chunk` vector in memory before any validation occurs.

4. **Delayed Verification**: Cryptographic proof verification only happens AFTER all records are deserialized and added to internal structures. [3](#0-2) 

**Attack Scenario**: An attacker who compromises backup storage (misconfigured S3 buckets, stolen credentials, supply chain attacks) can inject malicious blob files containing:
- Multiple records with maximum u32 size prefixes (up to 4GB each)
- Each record containing BCS-encoded `(StateKey, StateValue)` pairs with large byte arrays
- Example: 100 records Ã— 100MB = 10GB total memory allocation

The `StateValue` structure contains a `Bytes` field that can hold arbitrary-sized data, and `StateKey` variants can contain large `Vec<u8>` arrays. [4](#0-3) [5](#0-4) 

Contrast this with other parts of the codebase that DO enforce size limits, such as transaction argument validation with `MAX_NUM_BYTES = 1,000,000` (1MB). This demonstrates that unbounded deserialization is a recognized risk that should be mitigated.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program criteria:
- **Validator node slowdowns**: Memory exhaustion causes severe performance degradation
- **API crashes**: Excessive memory allocation can crash the node process
- **Availability impact**: Multiple validators using compromised backup storage experience simultaneous failures during restoration operations (new validator onboarding, disaster recovery, state sync from backups)

The attack breaks the documented invariant: **"Resource Limits: All operations must respect gas, storage, and computational limits"**. Backup restoration is exempt from gas metering but should still respect reasonable memory limits.

## Likelihood Explanation

**Likelihood: Medium**

Prerequisites for exploitation:
1. Attacker gains write access to backup storage through:
   - Misconfigured cloud storage buckets (public write access)
   - Compromised cloud storage credentials
   - Supply chain attacks on backup infrastructure
   - Social engineering operators to use malicious backup sources

2. Victim node operator initiates state snapshot restoration from compromised storage

While backup storage is operator-controlled infrastructure, real-world misconfigurations (public S3 buckets, weak access controls) make this attack feasible. The impact is amplified when multiple validators share backup infrastructure or when standard backup locations become compromised.

## Recommendation

Implement size limits on BCS deserialization to provide defense-in-depth protection, even for trusted backup sources:

```rust
const MAX_STATE_SNAPSHOT_RECORD_SIZE: usize = 50_000_000; // 50MB per record
const MAX_STATE_SNAPSHOT_CHUNK_SIZE: usize = 500_000_000; // 500MB per chunk

async fn read_state_value(
    storage: &Arc<dyn BackupStorage>,
    file_handle: FileHandle,
) -> Result<Vec<(StateKey, StateValue)>> {
    let mut file = storage.open_for_read(&file_handle).await?;
    let mut chunk = vec![];
    let mut total_size = 0;

    while let Some(record_bytes) = file.read_record_bytes().await? {
        ensure!(
            record_bytes.len() <= MAX_STATE_SNAPSHOT_RECORD_SIZE,
            "Record size {} exceeds maximum allowed size {}",
            record_bytes.len(),
            MAX_STATE_SNAPSHOT_RECORD_SIZE
        );
        
        total_size += record_bytes.len();
        ensure!(
            total_size <= MAX_STATE_SNAPSHOT_CHUNK_SIZE,
            "Chunk size {} exceeds maximum allowed size {}",
            total_size,
            MAX_STATE_SNAPSHOT_CHUNK_SIZE
        );
        
        chunk.push(bcs::from_bytes(&record_bytes)?);
    }

    Ok(chunk)
}
```

Additionally, consider validating the u32 size prefix in `read_record_bytes()` to prevent excessive memory allocation before reading data.

## Proof of Concept

**Malicious Backup File Creation**:

```rust
use bytes::{BufMut, BytesMut};
use aptos_types::state_store::{state_key::StateKey, state_value::StateValue};

fn create_malicious_backup_file() -> Vec<u8> {
    let mut file_data = Vec::new();
    
    // Create 100 records, each with 100MB of data
    for i in 0..100 {
        // Create a large StateValue with 100MB of data
        let large_data = vec![0u8; 100_000_000];
        let state_key = StateKey::raw(&[i as u8]);
        let state_value = StateValue::new_legacy(large_data.into());
        
        // BCS-encode the (StateKey, StateValue) pair
        let record_data = bcs::to_bytes(&(state_key, state_value)).unwrap();
        
        // Write u32 size prefix (big-endian)
        let size = record_data.len() as u32;
        file_data.extend_from_slice(&size.to_be_bytes());
        
        // Write record data
        file_data.extend_from_slice(&record_data);
    }
    
    file_data // Total: ~10GB when deserialized
}

// When a validator attempts to restore from this file:
// - read_state_value() will allocate 10GB+ of memory
// - The process will likely crash or hang before proof verification
// - Node becomes unavailable during restoration
```

**Attack Execution**:
1. Attacker uploads malicious blob file to compromised backup storage
2. Validator operator initiates: `aptos-db-tool restore state-snapshot --state-manifest <manifest> --state-into-version <version>`
3. Restoration process reads malicious blob file via `read_state_value()`
4. Memory exhaustion occurs, node crashes or hangs
5. Operator must identify and remediate corrupted backup before restoration succeeds

## Notes

This vulnerability represents a **defense-in-depth failure** rather than a direct protocol vulnerability. While backup storage is operator-controlled infrastructure, the absence of input validation creates an unnecessary attack surface. The existence of size limits elsewhere in the codebase (transaction argument validation, network protocol handlers) demonstrates that unbounded deserialization is a recognized risk.

The cryptographic verification (SparseMerkleRangeProof) eventually rejects invalid data, but memory exhaustion occurs **before** verification can execute, making the cryptographic protections ineffective against this resource exhaustion attack.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L253-266)
```rust
    async fn read_state_value(
        storage: &Arc<dyn BackupStorage>,
        file_handle: FileHandle,
    ) -> Result<Vec<(StateKey, StateValue)>> {
        let mut file = storage.open_for_read(&file_handle).await?;

        let mut chunk = vec![];

        while let Some(record_bytes) = file.read_record_bytes().await? {
            chunk.push(bcs::from_bytes(&record_bytes)?);
        }

        Ok(chunk)
    }
```

**File:** storage/backup/backup-cli/src/utils/read_record_bytes.rs (L44-67)
```rust
    async fn read_record_bytes(&mut self) -> Result<Option<Bytes>> {
        let _timer = BACKUP_TIMER.timer_with(&["read_record_bytes"]);
        // read record size
        let mut size_buf = BytesMut::with_capacity(4);
        self.read_full_buf_or_none(&mut size_buf).await?;
        if size_buf.is_empty() {
            return Ok(None);
        }

        // empty record
        let record_size = u32::from_be_bytes(size_buf.as_ref().try_into()?) as usize;
        if record_size == 0 {
            return Ok(Some(Bytes::new()));
        }

        // read record
        let mut record_buf = BytesMut::with_capacity(record_size);
        self.read_full_buf_or_none(&mut record_buf).await?;
        if record_buf.is_empty() {
            bail!("Hit EOF when reading record.")
        }

        Ok(Some(record_buf.freeze()))
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L373-391)
```rust
        for (key, value_hash) in chunk {
            let hashed_key = key.hash();
            if let Some(ref prev_leaf) = self.previous_leaf {
                ensure!(
                    &hashed_key > prev_leaf.account_key(),
                    "State keys must come in increasing order.",
                )
            }
            self.previous_leaf.replace(LeafNode::new(
                hashed_key,
                value_hash,
                (key.clone(), self.version),
            ));
            self.add_one(key, value_hash);
            self.num_keys_received += 1;
        }

        // Verify what we have added so far is all correct.
        self.verify(proof)?;
```

**File:** types/src/state_store/state_value.rs (L182-188)
```rust
#[derive(Clone, Debug, BCSCryptoHash, CryptoHasher)]
pub struct StateValue {
    data: Bytes,
    metadata: StateValueMetadata,
    maybe_rapid_hash: Option<(u64, usize)>,
}

```

**File:** types/src/state_store/state_key/inner.rs (L46-59)
```rust
#[derive(Clone, CryptoHasher, Eq, PartialEq, Serialize, Deserialize, Ord, PartialOrd, Hash)]
#[cfg_attr(any(test, feature = "fuzzing"), derive(proptest_derive::Arbitrary))]
#[serde(rename = "StateKey")]
pub enum StateKeyInner {
    AccessPath(AccessPath),
    TableItem {
        handle: TableHandle,
        #[serde(with = "serde_bytes")]
        key: Vec<u8>,
    },
    // Only used for testing
    #[serde(with = "serde_bytes")]
    Raw(Vec<u8>),
}
```
