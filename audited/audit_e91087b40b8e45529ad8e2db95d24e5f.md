# Audit Report

## Title
Stale Health Check Failure Counts Persist Across Peer Reconnections Leading to Premature Disconnections

## Summary
The `create_peer_and_health_data()` function in the health checker fails to reset failure counts when a peer reconnects with the same PeerId. This allows stale failure data from a previous connection to persist, causing premature disconnection of peers that should receive a fresh chance on their new connection.

## Finding Description

The health checker maintains failure counts for each peer to determine when to disconnect unhealthy nodes. When a peer reconnects, the system should reset all health state to give the new connection a fair chance. However, the implementation contains a critical flaw. [1](#0-0) 

When `create_peer_and_health_data()` is called for a peer that already exists in the health_check_data HashMap (due to rapid reconnection or race conditions), the `.and_modify()` path only updates the `round` field while leaving the `failures` field unchanged. This violates the expected behavior that each new connection should start with a clean slate.

**Attack Scenario:**

1. Peer connects (connection_id = A), health data initialized with failures = 0
2. Network instability causes 3 ping failures to accumulate (at the default tolerance threshold)
3. Connection A drops due to network disruption
4. Peer quickly reconnects with new connection_id = B
5. Due to asynchronous event processing, `insert_connection_metadata()` for connection B occurs before `remove_peer_metadata()` processes connection A's disconnect [2](#0-1) 

6. Since the peer still exists in PeersAndMetadata, the connection metadata is updated to connection B and a NewPeer event is broadcasted
7. Health checker receives the NewPeer event and calls `create_peer_and_health_data()` [3](#0-2) 

8. Since peer_id still exists in health_check_data with failures = 3, the `.and_modify()` path executes, updating only the round while failures remain at 3
9. When `remove_peer_metadata()` eventually processes connection A's disconnect, it finds connection B's metadata (mismatched connection_id) and returns an error without sending a LostPeer notification [4](#0-3) 

10. The peer now operates on the new connection but carries stale failure count of 3
11. One more ping failure (4th total) triggers immediate disconnection, violating the fairness expectation [5](#0-4) 

The default tolerance is 3 failures before disconnection: [6](#0-5) 

## Impact Explanation

This qualifies as **HIGH severity** according to Aptos bug bounty criteria:

1. **Validator node slowdowns**: Premature disconnection of consensus peers can degrade validator performance and increase block proposal latency
2. **Network fragmentation**: If multiple critical peers are affected simultaneously during network instability, this could contribute to consensus liveness issues
3. **Significant protocol violations**: The health check protocol's fairness guarantee is violated - each connection should get equal treatment

The vulnerability particularly affects validator networks where stable peer connections are critical for consensus participation. During periods of network instability (common in distributed systems), the race condition becomes more likely, potentially causing cascading disconnections.

## Likelihood Explanation

**High likelihood** - This vulnerability can be triggered naturally:

1. **Network instability is common**: Temporary network disruptions, routing changes, and connection resets happen frequently in production networks
2. **Race condition window**: The asynchronous event processing creates a realistic race window where NewPeer can arrive before LostPeer is processed
3. **No privilege required**: Any peer experiencing connection instability can trigger this condition without malicious intent
4. **Accumulative effect**: Once a peer accumulates failures close to threshold, any reconnection during network issues perpetuates the problem

The issue is most severe during:
- Network infrastructure maintenance
- DDoS mitigation triggering temporary routing changes  
- High load causing connection timeouts
- Geographic routing instability

## Recommendation

Fix `create_peer_and_health_data()` to properly reset all state when initializing or reinitializing a peer:

```rust
pub fn create_peer_and_health_data(&mut self, peer_id: PeerId, round: u64) {
    self.health_check_data
        .write()
        .entry(peer_id)
        .and_modify(|health_check_data| {
            health_check_data.round = round;
            health_check_data.failures = 0;  // Reset failures on reconnection
        })
        .or_insert_with(|| HealthCheckData::new(round));
}
```

Alternatively, always remove and re-insert to ensure clean state:

```rust
pub fn create_peer_and_health_data(&mut self, peer_id: PeerId, round: u64) {
    self.health_check_data
        .write()
        .insert(peer_id, HealthCheckData::new(round));
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_reconnection_resets_failures() {
    use crate::protocols::health_checker::interface::{HealthCheckData, HealthCheckNetworkInterface};
    use aptos_types::PeerId;
    
    // Create health checker interface
    let (network_client, _) = create_test_network_client();
    let (_, receiver) = create_test_receiver();
    let mut interface = HealthCheckNetworkInterface::new(network_client, receiver);
    
    let peer_id = PeerId::random();
    
    // Initial connection at round 0
    interface.create_peer_and_health_data(peer_id, 0);
    assert_eq!(interface.get_peer_failures(peer_id), Some(0));
    
    // Simulate 3 ping failures
    interface.increment_peer_round_failure(peer_id, 0);
    interface.increment_peer_round_failure(peer_id, 0);
    interface.increment_peer_round_failure(peer_id, 0);
    assert_eq!(interface.get_peer_failures(peer_id), Some(3));
    
    // Peer "reconnects" (NewPeer event received before LostPeer processed)
    // In the buggy implementation, failures persist
    interface.create_peer_and_health_data(peer_id, 1);
    
    // BUG: Failures should be 0 for new connection, but they persist at 3
    assert_eq!(interface.get_peer_failures(peer_id), Some(3)); // FAILS - should be 0
    
    // One more failure causes immediate disconnect (4 > 3 tolerance)
    interface.increment_peer_round_failure(peer_id, 1);
    assert_eq!(interface.get_peer_failures(peer_id), Some(4)); // Triggers disconnect
}
```

## Notes

This vulnerability demonstrates a state management race condition in the peer health checking system. The core issue is the assumption that `create_peer_and_health_data()` is only called for genuinely new peers, when in reality it can be invoked for existing peers due to asynchronous event processing ordering. The fix should ensure idempotency - calling this function multiple times for the same peer should always result in a clean, initialized state.

### Citations

**File:** network/framework/src/protocols/health_checker/interface.rs (L95-101)
```rust
    pub fn create_peer_and_health_data(&mut self, peer_id: PeerId, round: u64) {
        self.health_check_data
            .write()
            .entry(peer_id)
            .and_modify(|health_check_data| health_check_data.round = round)
            .or_insert_with(|| HealthCheckData::new(round));
    }
```

**File:** network/framework/src/application/storage.rs (L186-214)
```rust
    pub fn insert_connection_metadata(
        &self,
        peer_network_id: PeerNetworkId,
        connection_metadata: ConnectionMetadata,
    ) -> Result<(), Error> {
        // Grab the write lock for the peer metadata
        let mut peers_and_metadata = self.peers_and_metadata.write();

        // Fetch the peer metadata for the given network
        let peer_metadata_for_network =
            get_peer_metadata_for_network(&peer_network_id, &mut peers_and_metadata)?;

        // Update the metadata for the peer or insert a new entry
        peer_metadata_for_network
            .entry(peer_network_id.peer_id())
            .and_modify(|peer_metadata| {
                peer_metadata.connection_metadata = connection_metadata.clone()
            })
            .or_insert_with(|| PeerMetadata::new(connection_metadata.clone()));

        // Update the cached peers and metadata
        self.set_cached_peers_and_metadata(peers_and_metadata.clone());

        let event =
            ConnectionNotification::NewPeer(connection_metadata, peer_network_id.network_id());
        self.broadcast(event);

        Ok(())
    }
```

**File:** network/framework/src/application/storage.rs (L235-252)
```rust
            // Don't remove the peer if the connection doesn't match!
            // For now, remove the peer entirely, we could in the future
            // have multiple connections for a peer
            let active_connection_id = entry.get().connection_metadata.connection_id;
            if active_connection_id == connection_id {
                let peer_metadata = entry.remove();
                let event = ConnectionNotification::LostPeer(
                    peer_metadata.connection_metadata.clone(),
                    peer_network_id.network_id(),
                );
                self.broadcast(event);
                peer_metadata
            } else {
                return Err(Error::UnexpectedError(format!(
                    "The peer connection id did not match! Given: {:?}, found: {:?}.",
                    connection_id, active_connection_id
                )));
            }
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L211-217)
```rust
                        ConnectionNotification::NewPeer(metadata, network_id) => {
                            // PeersAndMetadata is a global singleton across all networks; filter connect/disconnect events to the NetworkId that this HealthChecker instance is watching
                            if network_id == self_network_id {
                                self.network_interface.create_peer_and_health_data(
                                    metadata.remote_peer_id, self.round
                                );
                            }
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L356-392)
```rust
                // If the ping failures are now more than
                // `self.ping_failures_tolerated`, we disconnect from the node.
                // The HealthChecker only performs the disconnect. It relies on
                // ConnectivityManager or the remote peer to re-establish the connection.
                let failures = self
                    .network_interface
                    .get_peer_failures(peer_id)
                    .unwrap_or(0);
                if failures > self.ping_failures_tolerated {
                    info!(
                        NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                        "{} Disconnecting from peer: {}",
                        self.network_context,
                        peer_id.short_str()
                    );
                    let peer_network_id =
                        PeerNetworkId::new(self.network_context.network_id(), peer_id);
                    if let Err(err) = timeout(
                        Duration::from_millis(50),
                        self.network_interface.disconnect_peer(
                            peer_network_id,
                            DisconnectReason::NetworkHealthCheckFailure,
                        ),
                    )
                    .await
                    {
                        warn!(
                            NetworkSchema::new(&self.network_context)
                                .remote_peer(&peer_id),
                            error = ?err,
                            "{} Failed to disconnect from peer: {} with error: {:?}",
                            self.network_context,
                            peer_id.short_str(),
                            err
                        );
                    }
                }
```

**File:** config/src/config/network_config.rs (L40-40)
```rust
pub const PING_FAILURES_TOLERATED: u64 = 3;
```
