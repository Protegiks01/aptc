# Audit Report

## Title
Unbounded Epoch Change Proof Verification Enables Computational DoS Attack on Validators

## Summary
The `EpochChangeProof::verify()` function in `types/src/epoch_change.rs` does not validate the number of ledger info entries before performing expensive BLS signature verification on each entry. An attacker can craft an oversized proof with hundreds of historical epoch changes, forcing validators to perform excessive cryptographic verification and causing temporary denial of service during epoch transitions.

## Finding Description

The vulnerability exists in the epoch change verification flow where validators accept and verify `EpochChangeProof` messages from network peers without bounds checking.

**Attack Flow:**

1. **Proof Reception Without Validation**: When a validator receives an `EpochChangeProof` via consensus networking, it immediately calls `verify()` without checking the number of entries. [1](#0-0) 

2. **Unbounded Verification Loop**: The `verify()` function iterates through all `ledger_info_with_sigs` entries without any size limit, performing expensive BLS signature verification for each. [2](#0-1) 

3. **Expensive Cryptographic Operations**: Each verification involves BLS signature verification with public key aggregation and pairing operations. [3](#0-2) 

4. **Sender-Side Limit Not Enforced on Receiver**: While honest nodes limit proofs to `MAX_NUM_EPOCH_ENDING_LEDGER_INFO = 100` entries when creating them, this limit is not enforced when receiving proofs. [4](#0-3) 

**Exploitation Scenario:**

An attacker can construct a valid `EpochChangeProof` containing hundreds of historical epoch-ending ledger infos (all with legitimate validator signatures, which are public blockchain data). When sent to a validator that is catching up (e.g., after downtime), the victim will verify each epoch sequentially:

- Network message size limit (~62 MiB) allows approximately 200-500+ epoch entries
- Each BLS verification requires ~1-5ms of CPU time
- 500 verifications = 0.5-2.5 seconds of blocking computation
- Multiple malicious peers can amplify the attack

The `skip_while` optimization at line 102 only helps validators that are already caught up - it does not protect validators that are legitimately catching up from excessive verification work when the proof contains more epochs than the intended maximum.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program criteria:
- **Validator node slowdowns**: The attack causes temporary CPU exhaustion on validators during epoch transitions
- **Availability impact**: During catch-up periods, validators become unresponsive due to blocking verification
- **Consensus liveness risk**: If multiple validators are affected simultaneously, consensus could stall

The attack doesn't compromise consensus safety or funds, but significantly degrades validator performance and network availability during epoch transitions, which are critical for the protocol's liveness.

## Likelihood Explanation

**Likelihood: Medium-High**

The attack is practical because:
- **Low barrier to entry**: Any network peer can send consensus messages
- **Public data**: Historical epoch-ending ledger infos with valid signatures are publicly available on-chain
- **No authentication required**: The network accepts proofs before verification
- **Amplification possible**: Multiple attackers or repeated attacks increase impact

Mitigating factors:
- Requires victim validator to be in catch-up mode (behind current epoch)
- Network message size limits cap maximum abuse to ~500 epochs per message
- `skip_while` optimization reduces impact for caught-up validators

## Recommendation

Add explicit bounds checking before verification to enforce the `MAX_NUM_EPOCH_ENDING_LEDGER_INFO` limit on the receiver side:

```rust
// In types/src/epoch_change.rs, EpochChangeProof::verify()
pub fn verify(&self, verifier: &dyn Verifier) -> Result<&LedgerInfoWithSignatures> {
    ensure!(
        !self.ledger_info_with_sigs.is_empty(),
        "The EpochChangeProof is empty"
    );
    
    // ADD THIS CHECK:
    const MAX_PROOF_SIZE: usize = 100; // Or import from common.rs
    ensure!(
        self.ledger_info_with_sigs.len() <= MAX_PROOF_SIZE,
        "EpochChangeProof contains {} entries, exceeding maximum of {}",
        self.ledger_info_with_sigs.len(),
        MAX_PROOF_SIZE
    );
    
    ensure!(
        !verifier
            .is_ledger_info_stale(self.ledger_info_with_sigs.last().unwrap().ledger_info()),
        "The EpochChangeProof is stale as our verifier is already ahead \
         of the entire EpochChangeProof"
    );
    // ... rest of function
}
```

Additionally, consider implementing rate limiting for `EpochChangeProof` messages per peer in the consensus network layer.

## Proof of Concept

```rust
// Test demonstrating oversized proof attack
// Add to types/src/epoch_change.rs tests module

#[test]
fn test_oversized_epoch_change_proof_rejected() {
    use crate::{ledger_info::LedgerInfo, validator_verifier::random_validator_verifier};
    use aptos_crypto::hash::HashValue;

    // Create 200 valid epoch-ending ledger infos (exceeding MAX of 100)
    let mut ledger_infos = vec![];
    let (mut current_signers, mut current_verifier) = random_validator_verifier(1, None, true);
    let mut current_verifier = Arc::new(current_verifier);
    
    for epoch in 1..=200 {
        let (next_signers, next_verifier) = random_validator_verifier(1, None, true);
        let next_verifier = Arc::new(next_verifier);
        let epoch_state = EpochState {
            epoch: epoch + 1,
            verifier: next_verifier.clone(),
        };
        
        let ledger_info = LedgerInfo::new(
            BlockInfo::new(
                epoch,
                0,
                HashValue::zero(),
                HashValue::zero(),
                epoch * 100,
                0,
                Some(epoch_state),
            ),
            HashValue::zero(),
        );
        
        let partial_sigs = PartialSignatures::new(
            current_signers
                .iter()
                .map(|s| (s.author(), s.sign(&ledger_info).unwrap()))
                .collect(),
        );
        
        let agg_sig = current_verifier
            .aggregate_signatures(partial_sigs.signatures_iter())
            .unwrap();
            
        ledger_infos.push(LedgerInfoWithSignatures::new(ledger_info, agg_sig));
        current_signers = next_signers;
        current_verifier = next_verifier;
    }
    
    // Create oversized proof
    let proof = EpochChangeProof::new(ledger_infos, false);
    
    // Start from epoch 1 verifier
    let (_, verifier) = random_validator_verifier(1, None, true);
    let start_verifier = EpochState {
        epoch: 1,
        verifier: Arc::new(verifier),
    };
    
    // Verification should fail due to size limit (after fix)
    // Currently, this would succeed but take excessive time
    let result = proof.verify(&start_verifier);
    
    // After fix, this should return an error about exceeding max size
    assert!(result.is_err());
}
```

**Notes**

This vulnerability violates the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." The lack of bounds checking on proof size allows attackers to force unbounded computation during the verification phase, bypassing intended resource constraints and enabling denial-of-service attacks on validators during critical epoch transition periods.

### Citations

**File:** consensus/src/epoch_manager.rs (L544-547)
```rust
    async fn initiate_new_epoch(&mut self, proof: EpochChangeProof) -> anyhow::Result<()> {
        let ledger_info = proof
            .verify(self.epoch_state())
            .context("[EpochManager] Invalid EpochChangeProof")?;
```

**File:** types/src/epoch_change.rs (L66-118)
```rust
    pub fn verify(&self, verifier: &dyn Verifier) -> Result<&LedgerInfoWithSignatures> {
        ensure!(
            !self.ledger_info_with_sigs.is_empty(),
            "The EpochChangeProof is empty"
        );
        ensure!(
            !verifier
                .is_ledger_info_stale(self.ledger_info_with_sigs.last().unwrap().ledger_info()),
            "The EpochChangeProof is stale as our verifier is already ahead \
             of the entire EpochChangeProof"
        );
        let mut verifier_ref = verifier;

        for ledger_info_with_sigs in self
            .ledger_info_with_sigs
            .iter()
            // Skip any stale ledger infos in the proof prefix. Note that with
            // the assertion above, we are guaranteed there is at least one
            // non-stale ledger info in the proof.
            //
            // It's useful to skip these stale ledger infos to better allow for
            // concurrent client requests.
            //
            // For example, suppose the following:
            //
            // 1. My current trusted state is at epoch 5.
            // 2. I make two concurrent requests to two validators A and B, who
            //    live at epochs 9 and 11 respectively.
            //
            // If A's response returns first, I will ratchet my trusted state
            // to epoch 9. When B's response returns, I will still be able to
            // ratchet forward to 11 even though B's EpochChangeProof
            // includes a bunch of stale ledger infos (for epochs 5, 6, 7, 8).
            //
            // Of course, if B's response returns first, we will reject A's
            // response as it's completely stale.
            .skip_while(|&ledger_info_with_sigs| {
                verifier.is_ledger_info_stale(ledger_info_with_sigs.ledger_info())
            })
        {
            // Try to verify each (epoch -> epoch + 1) jump in the EpochChangeProof.
            verifier_ref.verify(ledger_info_with_sigs)?;
            // While the original verification could've been via waypoints,
            // all the next epoch changes are verified using the (already
            // trusted) validator sets.
            verifier_ref = ledger_info_with_sigs
                .ledger_info()
                .next_epoch_state()
                .ok_or_else(|| format_err!("LedgerInfo doesn't carry a ValidatorSet"))?;
        }

        Ok(self.ledger_info_with_sigs.last().unwrap())
    }
```

**File:** types/src/validator_verifier.rs (L345-386)
```rust
    pub fn verify_multi_signatures<T: CryptoHash + Serialize>(
        &self,
        message: &T,
        multi_signature: &AggregateSignature,
    ) -> std::result::Result<(), VerifyError> {
        // Verify the number of signature is not greater than expected.
        Self::check_num_of_voters(self.len() as u16, multi_signature.get_signers_bitvec())?;
        let mut pub_keys = vec![];
        let mut authors = vec![];
        for index in multi_signature.get_signers_bitvec().iter_ones() {
            let validator = self
                .validator_infos
                .get(index)
                .ok_or(VerifyError::UnknownAuthor)?;
            authors.push(validator.address);
            pub_keys.push(validator.public_key());
        }
        // Verify the quorum voting power of the authors
        self.check_voting_power(authors.iter(), true)?;
        #[cfg(any(test, feature = "fuzzing"))]
        {
            if self.quorum_voting_power == 0 {
                // This should happen only in case of tests.
                // TODO(skedia): Clean up the test behaviors to not rely on empty signature
                // verification
                return Ok(());
            }
        }
        // Verify empty multi signature
        let multi_sig = multi_signature
            .sig()
            .as_ref()
            .ok_or(VerifyError::EmptySignature)?;
        // Verify the optimistically aggregated signature.
        let aggregated_key =
            PublicKey::aggregate(pub_keys).map_err(|_| VerifyError::FailedToAggregatePubKey)?;

        multi_sig
            .verify(message, &aggregated_key)
            .map_err(|_| VerifyError::InvalidMultiSignature)?;
        Ok(())
    }
```

**File:** storage/aptosdb/src/common.rs (L7-9)
```rust
// TODO: Either implement an iteration API to allow a very old client to loop through a long history
// or guarantee that there is always a recent enough waypoint and client knows to boot from there.
pub(crate) const MAX_NUM_EPOCH_ENDING_LEDGER_INFO: usize = 100;
```
