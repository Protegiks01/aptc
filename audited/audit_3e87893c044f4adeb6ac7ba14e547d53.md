# Audit Report

## Title
File Store Backfiller Creates Permanent Data Gaps Due to Missing Transaction Count Validation

## Summary
The file store backfiller in `processor.rs` unconditionally advances its progress counter by `num_transactions_per_folder` after requesting transactions from the fullnode, without validating that exactly that many transactions were actually received and processed. This creates permanent gaps in the file store when the gRPC stream terminates early due to fullnode shutdown, network errors, or client disconnects.

## Finding Description

The `backfill()` function violates the data integrity invariant by failing to validate transaction counts before updating progress tracking. [1](#0-0) 

The request asks for `num_transactions_per_folder` transactions (typically 100,000), but there is no validation that this many transactions were actually received before the progress counter is advanced: [2](#0-1) 

While `FileStoreOperatorV2` tracks the version internally and validates transaction contiguity: [3](#0-2) 

This internal version counter is **never checked** after the stream completes. The backfiller blindly advances progress regardless of how many transactions were actually processed.

The fullnode service can terminate the stream early in several scenarios: [4](#0-3) [5](#0-4) 

When the stream ends, the processing task completes normally: [6](#0-5) 

After scope completion, the progress file is updated with the incorrectly advanced version: [7](#0-6) 

**Attack Scenario:**
1. Backfiller requests versions [0, 100000) with `num_transactions_per_folder = 100000`
2. Fullnode streams transactions [0, 45000) then is gracefully shutdown
3. gRPC stream terminates normally (no error sent)
4. Backfiller processes 45,000 transactions successfully
5. Progress is advanced to version 100000 (line 207)
6. Progress file is written with version 100000
7. On restart, backfiller starts at version 100000
8. **Permanent gap created:** Versions [45000, 100000) are forever missing from the file store

## Impact Explanation

This is a **High Severity** vulnerability based on Aptos bug bounty criteria: "Significant protocol violations" - the indexer file store is a critical data infrastructure component used by downstream analytics, explorers, and indexing services.

**Concrete Impact:**
- **Data Integrity Violation**: Permanent gaps in the historical transaction record
- **Downstream Service Failures**: Indexers, block explorers, and analytics systems consuming the file store will have incomplete data
- **No Automatic Recovery**: Once the progress file is written, the gap is permanent unless manually detected and fixed
- **Silent Failure**: The backfiller logs "Backfilling is finished" even with incomplete data, masking the issue

This breaks the fundamental data consistency guarantee that the file store provides complete, contiguous transaction history.

## Likelihood Explanation

**High Likelihood** - This will occur in production environments:

1. **Fullnode Maintenance**: Graceful shutdowns during upgrades/restarts are routine operations
2. **Network Instability**: Temporary network partitions or timeouts can cause stream termination
3. **Resource Constraints**: OOM conditions or other resource issues can trigger abort handles
4. **No Detection**: The vulnerability is silent - no error is logged when partial data is processed

The issue is **deterministic** - whenever a stream ends prematurely, gaps are created.

## Recommendation

Add transaction count validation before advancing the progress counter. The `FileStoreOperatorV2` already tracks the actual version processed, so we should verify it matches the expected end version:

```rust
s.spawn(async move {
    // ... existing stream processing code ...
    
    // After stream completes, validate we got all transactions
    let expected_end_version = task_version + num_transactions_per_folder;
    let actual_end_version = file_store_operator.version();
    
    if actual_end_version != expected_end_version {
        panic!(
            "Incomplete transaction batch: expected version {}, got {}. Gap detected: [{}, {})",
            expected_end_version,
            actual_end_version,
            actual_end_version,
            expected_end_version
        );
    }
    
    info!(
        "Backfilling versions [{task_version}, {}) is finished.",
        expected_end_version
    );
});
```

Additionally, only advance the version counter if validation passes. Move line 207 inside the scope and make it conditional on successful completion.

## Proof of Concept

```rust
// Integration test demonstrating the vulnerability
#[tokio::test]
async fn test_backfill_gap_on_early_stream_termination() {
    // Setup: Start backfiller requesting versions [0, 100000)
    let processor = create_test_processor(0, 100000).await;
    
    // Simulate fullnode that returns only 45,000 transactions
    // then terminates the stream (e.g., due to shutdown)
    let mock_fullnode = MockFullnode::new()
        .with_transactions(0, 45000)
        .then_terminate_stream();
    
    // Run backfill
    processor.backfill().await.unwrap();
    
    // BUG: Progress file shows version 100,000 
    // but only versions [0, 45000) exist in file store
    let progress = read_progress_file().await;
    assert_eq!(progress.version, 100000); // Progress incorrectly advanced
    
    // Verify the gap: versions [45000, 100000) are missing
    for version in 45000..100000 {
        assert!(file_store.get_transaction(version).is_none());
    }
    
    // On restart, backfiller starts at 100,000, permanently missing the gap
    let processor2 = create_test_processor_from_progress().await;
    assert_eq!(processor2.starting_version, 100000);
}
```

**Notes**

This vulnerability is specific to the file store backfiller infrastructure, not the core Aptos blockchain consensus. However, it represents a critical data integrity failure for the indexing ecosystem that many applications depend on. The fix is straightforward - validate that `file_store_operator.version()` matches the expected end version before advancing progress. The existing contiguity validation in `FileStoreOperatorV2` already provides the necessary tracking; it just needs to be checked after stream completion.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-v2-file-store-backfiller/src/processor.rs (L163-166)
```rust
                        let request = tonic::Request::new(GetTransactionsFromNodeRequest {
                            starting_version: Some(task_version),
                            transactions_count: Some(num_transactions_per_folder),
                        });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-v2-file-store-backfiller/src/processor.rs (L173-205)
```rust
                        while let Some(response_item) = stream.next().await {
                            match response_item {
                                Ok(r) => {
                                    assert!(r.chain_id == chain_id);
                                    match r.response.unwrap() {
                                        Response::Data(data) => {
                                            let transactions = data.transactions;
                                            for transaction in transactions {
                                                file_store_operator
                                                    .buffer_and_maybe_dump_transactions_to_file(
                                                        transaction,
                                                        tx.clone(),
                                                    )
                                                    .await
                                                    .unwrap();
                                            }
                                        },
                                        Response::Status(_) => {
                                            continue;
                                        },
                                    }
                                },
                                Err(e) => {
                                    panic!("Error when getting transactions from fullnode: {e}.")
                                },
                            }
                        }

                        info!(
                            "Backfilling versions [{task_version}, {}) is finished.",
                            task_version + num_transactions_per_folder
                        );
                    });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-v2-file-store-backfiller/src/processor.rs (L207-207)
```rust
                    version += self.num_transactions_per_folder;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-v2-file-store-backfiller/src/processor.rs (L211-220)
```rust
            // Update the progress file.
            let progress_file = ProgressFile {
                version,
                backfill_id: self.backfill_id,
            };
            let bytes =
                serde_json::to_vec(&progress_file).context("Failed to serialize progress file.")?;
            std::fs::write(&self.progress_file_path, &bytes)
                .context("Failed to write progress file.")?;
            info!("Progress file updated to version {}.", version,);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_operator.rs (L50-55)
```rust
        ensure!(
            self.version == transaction.version,
            "Gap is found when buffering transaction, expected: {}, actual: {}",
            self.version,
            transaction.version,
        );
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/fullnode_data_service.rs (L139-142)
```rust
                if abort_handle.load(Ordering::SeqCst) {
                    info!("FullnodeDataService is aborted.");
                    break;
                }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/fullnode_data_service.rs (L143-149)
```rust
                if results.is_empty() {
                    info!(
                        start_version = starting_version,
                        chain_id = ledger_chain_id,
                        "[Indexer Fullnode] Client disconnected."
                    );
                    break;
```
