# Audit Report

## Title
State Inconsistency in Transaction Replay Verification with Lazy Quit Mode

## Summary
The `verify_execution()` function in the chunk executor returns `version + 1` on verification failure when in lazy quit mode, causing mismatched transaction outputs to be committed to the database. This creates a state inconsistency where the database contains state that does not match deterministic VM execution, potentially leading to consensus splits if the corrupted database is used for validator operations.

## Finding Description

In `verify_execution()`, when transaction execution verification fails and `verify_execution_mode.is_lazy_quit()` is true, the function returns `Ok(version + 1)` instead of propagating the error: [1](#0-0) 

This return value becomes `next_begin` in `remove_and_replay_epoch()`, which then calls `remove_and_apply()` with a range that includes the failed transaction: [2](#0-1) 

The critical issue is that `remove_and_apply()` applies the **stored** transaction outputs (from backup data) rather than VM-executed outputs. When verification fails at version V, it means the stored outputs differ from what the VM produced. However, these mismatched outputs are still applied to the database: [3](#0-2) 

These incorrect outputs are then committed to the database via the commit queue. The replay-verify coordinator eventually detects and reports the error, but by then the database has already been corrupted with incorrect state: [4](#0-3) 

**Broken Invariants:**
1. **Deterministic Execution**: The database contains state that doesn't match VM execution from genesis
2. **State Consistency**: State transitions are not atomic - failed transactions are still applied

**Attack Propagation:**
When the `aptos-db-tool replay-verify` command is run with the `--lazy-quit` flag: [5](#0-4) 

The tool writes to a real database in non-read-only mode: [6](#0-5) 

Through the transaction replay mechanism: [7](#0-6) 

And commits to database: [8](#0-7) 

## Impact Explanation

This vulnerability represents a **High Severity** issue under the Aptos bug bounty criteria for "State inconsistencies requiring intervention."

**Specific Impacts:**

1. **Database Corruption**: The tool leaves the database in an inconsistent state where committed transactions don't match their expected VM execution results.

2. **Consensus Split Risk**: If a validator restores from this corrupted database, it will produce different state roots than other validators, causing consensus failures and potential network partition.

3. **State Divergence**: Any node using this database will have state that diverges from the canonical chain state, making it unable to sync or validate blocks correctly.

4. **Silent Failure Mode**: While the tool reports an error at completion, the database is already corrupted. Operators who don't properly clean up after the error could inadvertently use the corrupted database.

The severity is particularly concerning because:
- The tool is used for critical database restoration operations
- Database corruption is not immediately obvious - it appears successful until the final error check
- Recovery requires complete database rebuild from scratch or clean backup

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability is triggered when:
1. An operator runs `aptos-db-tool replay-verify` with the `--lazy-quit` flag
2. The backup data contains transactions whose outputs don't match VM execution (either due to corruption, version incompatibility, or malicious modification)
3. The operator subsequently uses the corrupted database for node operations

**Factors Increasing Likelihood:**
- Database verification is a common operational task
- The `--lazy-quit` flag is legitimately useful for identifying all problematic transactions in a backup (not just the first one)
- Operators might ignore the final error if they believe they can "work around" known issues
- The normal restore path uses `NoVerify` mode, so users might switch to verify mode to debug issues, inadvertently triggering this bug

**Factors Decreasing Likelihood:**
- Normal restoration path doesn't use verification mode
- The tool does report an error at completion
- Requires specific tool usage, not exploitable through network/transaction submission

## Recommendation

**Immediate Fix:** Modify `verify_execution()` to not include failed transactions in the return range when in lazy quit mode. When verification fails at version V, the function should skip that transaction and return `version` instead of `version + 1`, so that `remove_and_apply()` doesn't include the failed transaction.

**Corrected Logic:**
```rust
return if verify_execution_mode.is_lazy_quit() {
    error!("(Not quitting right away.) {}", err);
    verify_execution_mode.mark_seen_error();
    // Return version (not version + 1) to exclude the failed transaction
    Ok(version)
} else {
    Err(err)
};
```

**Additional Improvements:**
1. Add explicit check to prevent database writes if any verification errors occurred in lazy mode
2. Consider using a separate read-only mode for verification that never writes to database
3. Add rollback mechanism to undo any commits if verification fails
4. Add clear warnings in tool documentation about not using databases that showed verification errors

## Proof of Concept

```rust
// Test demonstrating the vulnerability
#[test]
fn test_lazy_quit_state_corruption() {
    use aptos_executor_types::VerifyExecutionMode;
    
    // 1. Setup: Create a chunk executor with a test database
    let db = create_test_db();
    let executor = ChunkExecutor::<AptosVMBlockExecutor>::new(db.clone());
    
    // 2. Create transactions with mismatched outputs
    // Stored outputs differ from what VM would produce
    let txns = vec![create_test_transaction()];
    let stored_outputs = vec![create_incorrect_transaction_output()];
    let correct_txn_info = vec![compute_correct_transaction_info()];
    
    // 3. Run replay with lazy_quit mode
    let verify_mode = VerifyExecutionMode::verify_all().set_lazy_quit(true);
    
    executor.enqueue_chunks(
        txns.clone(),
        vec![PersistedAuxiliaryInfo::None],
        correct_txn_info.clone(),
        vec![stored_outputs[0].write_set().clone()],
        vec![stored_outputs[0].events().to_vec()],
        &verify_mode,
    ).unwrap();
    
    executor.update_ledger().unwrap();
    executor.commit().unwrap();
    
    // 4. Verify database contains incorrect state
    // Even though verification should have failed, the stored (incorrect) 
    // outputs are now in the database
    assert!(verify_mode.seen_error(), "Should have seen verification error");
    
    // 5. Verify state divergence: Re-execute transaction and compare
    let state_from_db = db.reader.get_latest_state_checkpoint_version().unwrap();
    let state_from_reexecution = re_execute_from_genesis(&txns);
    
    // This assertion fails - database state doesn't match VM execution
    assert_ne!(state_from_db, state_from_reexecution, 
               "Database state diverged from deterministic execution!");
}
```

## Notes

This vulnerability specifically affects the database restoration and verification tooling, not the main consensus/execution path. However, its impact on state consistency is severe if corrupted databases are used for node operations. The normal restoration flow using `VerifyExecutionMode::NoVerify` is not affected. [9](#0-8) [10](#0-9) 

The fix should ensure that lazy quit mode only affects error reporting behavior, not the correctness of committed data. Failed transactions should never be applied to the database, regardless of the verification mode.

### Citations

**File:** execution/executor/src/chunk_executor/mod.rs (L562-587)
```rust
            let next_begin = if verify_execution_mode.should_verify() {
                self.verify_execution(
                    transactions,
                    persisted_aux_info,
                    transaction_infos,
                    write_sets,
                    event_vecs,
                    batch_begin,
                    batch_end,
                    verify_execution_mode,
                )?
            } else {
                batch_end
            };
            self.remove_and_apply(
                transactions,
                persisted_aux_info,
                transaction_infos,
                write_sets,
                event_vecs,
                batch_begin,
                next_begin,
            )?;
            chunks_enqueued += 1;
            batch_begin = next_begin;
        }
```

**File:** execution/executor/src/chunk_executor/mod.rs (L636-649)
```rust
            if let Err(err) = txn_out.ensure_match_transaction_info(
                version,
                txn_info,
                Some(write_set),
                Some(events),
            ) {
                return if verify_execution_mode.is_lazy_quit() {
                    error!("(Not quitting right away.) {}", err);
                    verify_execution_mode.mark_seen_error();
                    Ok(version + 1)
                } else {
                    Err(err)
                };
            }
```

**File:** execution/executor/src/chunk_executor/mod.rs (L656-702)
```rust
    fn remove_and_apply(
        &self,
        transactions: &mut Vec<Transaction>,
        persisted_aux_info: &mut Vec<PersistedAuxiliaryInfo>,
        transaction_infos: &mut Vec<TransactionInfo>,
        write_sets: &mut Vec<WriteSet>,
        event_vecs: &mut Vec<Vec<ContractEvent>>,
        begin_version: Version,
        end_version: Version,
    ) -> Result<()> {
        let num_txns = (end_version - begin_version) as usize;
        let txn_infos: Vec<_> = transaction_infos.drain(..num_txns).collect();
        let (transactions, persisted_aux_info, transaction_outputs) = multizip((
            transactions.drain(..num_txns),
            persisted_aux_info.drain(..num_txns),
            txn_infos.iter(),
            write_sets.drain(..num_txns),
            event_vecs.drain(..num_txns),
        ))
        .map(|(txn, persisted_aux_info, txn_info, write_set, events)| {
            (
                txn,
                persisted_aux_info,
                TransactionOutput::new(
                    write_set,
                    events,
                    txn_info.gas_used(),
                    TransactionStatus::Keep(txn_info.status().clone()),
                    TransactionAuxiliaryData::default(), // No auxiliary data if transaction is not executed through VM
                ),
            )
        })
        .multiunzip();

        let chunk = ChunkToApply {
            transactions,
            transaction_outputs,
            persisted_aux_info,
            first_version: begin_version,
        };
        let chunk_verifier = Arc::new(ReplayChunkVerifier {
            transaction_infos: txn_infos,
        });
        self.enqueue_chunk(chunk, chunk_verifier, "replay")?;

        Ok(())
    }
```

**File:** storage/backup/backup-cli/src/coordinators/replay_verify.rs (L207-211)
```rust
        if self.verify_execution_mode.seen_error() {
            Err(ReplayError::TxnMismatch)
        } else {
            Ok(())
        }
```

**File:** storage/db-tool/src/replay_verify.rs (L64-74)
```rust
        let restore_handler = Arc::new(AptosDB::open_kv_only(
            StorageDirPaths::from_path(self.db_dir),
            false,                       /* read_only */
            NO_OP_STORAGE_PRUNER_CONFIG, /* pruner config */
            self.rocksdb_opt.into(),
            false, /* indexer */
            BUFFERED_STATE_TARGET_ITEMS,
            DEFAULT_MAX_NUM_NODES_PER_LRU_CACHE_SHARD,
            None,
        )?)
        .get_restore_handler();
```

**File:** storage/db-tool/src/replay_verify.rs (L85-85)
```rust
            VerifyExecutionMode::verify_except(self.txns_to_skip).set_lazy_quit(self.lazy_quit),
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L657-686)
```rust
        let chunk_replayer = Arc::new(ChunkExecutor::<AptosVMBlockExecutor>::new(db));
        let ledger_update_stream = txns_to_execute_stream
            .try_chunks(BATCH_SIZE)
            .err_into::<anyhow::Error>()
            .map_ok(|chunk| {
                let (txns, persisted_aux_info, txn_infos, write_sets, events): (
                    Vec<_>,
                    Vec<_>,
                    Vec<_>,
                    Vec<_>,
                    Vec<_>,
                ) = chunk.into_iter().multiunzip();
                let chunk_replayer = chunk_replayer.clone();
                let verify_execution_mode = self.verify_execution_mode.clone();

                async move {
                    let _timer = OTHER_TIMERS_SECONDS.timer_with(&["enqueue_chunks"]);

                    tokio::task::spawn_blocking(move || {
                        chunk_replayer.enqueue_chunks(
                            txns,
                            persisted_aux_info,
                            txn_infos,
                            write_sets,
                            events,
                            &verify_execution_mode,
                        )
                    })
                    .await
                    .expect("spawn_blocking failed")
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L714-726)
```rust
                    tokio::task::spawn_blocking(move || {
                        let v = chunk_replayer.commit()?;

                        let total_replayed = v - first_version + 1;
                        TRANSACTION_REPLAY_VERSION.set(v as i64);
                        info!(
                            version = v,
                            accumulative_tps = (total_replayed as f64
                                / replay_start.elapsed().as_secs_f64())
                                as u64,
                            "Transactions replayed."
                        );
                        Ok(total_replayed)
```

**File:** storage/backup/backup-cli/src/coordinators/restore.rs (L296-296)
```rust
                VerifyExecutionMode::NoVerify,
```

**File:** storage/backup/backup-cli/src/coordinators/restore.rs (L367-367)
```rust
                VerifyExecutionMode::NoVerify,
```
