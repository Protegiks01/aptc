# Audit Report

## Title
Inconsistent Reputation Window Sizes Enable Unfair Validator Penalization Despite Recent Good Behavior

## Summary

The leader reputation system uses different window sizes for counting votes (`voter_window_size`) versus proposals/failures (`proposer_window_size`). By default, the proposer window is 10x larger than the voter window. This creates a scenario where validators can be heavily penalized for old proposal failures outside the voter window, even when they demonstrate consistent recent voting participation, violating the stated fairness principle of the reputation system. [1](#0-0) 

## Finding Description

The `NewBlockEventAggregation` struct maintains two separate window sizes for reputation calculations. The default configuration sets:
- `proposer_window_num_validators_multiplier: 10`
- `voter_window_num_validators_multiplier: 1` [2](#0-1) 

For a network with 100 validators, this results in:
- `voter_window_size = 100 blocks` (most recent)
- `proposer_window_size = 1000 blocks` (most recent)

The reputation weight calculation logic checks failure threshold BEFORE considering voting activity: [3](#0-2) 

**Critical Flaw**: The failure rate is calculated over 1000 blocks, but voting participation is only counted over 100 blocks. This means:

1. A validator with old proposal failures (e.g., in blocks 101-1000) will have those failures counted
2. The same validator with recent successful votes (in blocks 1-100) will have those votes counted
3. BUT the failure check executes first (line 541-544), assigning `failed_weight = 1` if the failure threshold is exceeded
4. The voting activity check (line 545) never executes, effectively ignoring the validator's recent good behavior

**Concrete Scenario:**
- Validator A: 95 successful votes in last 100 blocks, but had 2 failed proposals and 10 successful proposals in blocks 101-600
  - `cur_votes = 95`
  - `cur_proposals = 10` 
  - `cur_failed_proposals = 2`
  - Failure rate: 2/(10+2) = 16.7% > 10% threshold
  - **Assigned weight: 1** (failed_weight)

- Validator B: 95 successful votes in last 100 blocks, no historical failures
  - `cur_votes = 95`
  - `cur_proposals = 0`
  - `cur_failed_proposals = 0`
  - **Assigned weight: 1000** (active_weight)

Both validators have **identical recent behavior** but Validator A is **1000x less likely** to be selected as leader due to failures from 500 blocks ago that occurred outside the voter window.

This violates the stated design goal: "we also, in combination with staking rewards logic, need to be reasonably fair." [4](#0-3) 

## Impact Explanation

**Severity: Medium**

This qualifies as Medium severity under "State inconsistencies requiring intervention" because:

1. **Unfair Validator Selection**: Validators who had transient issues (network problems, brief downtime, hardware upgrades) 500-1000 blocks ago continue to be penalized despite 100 blocks of consistent good behavior

2. **Staking Reward Manipulation**: Leader selection directly affects staking rewards. Validators with lower selection weights receive fewer opportunities to propose blocks and earn rewards, creating unfair economic outcomes

3. **Centralization Risk**: Validators with any historical issues face permanent disadvantage, potentially driving them out of the active set and reducing decentralization

4. **Violation of Stated Invariant**: The Aptos invariants include "Staking Security: Validator rewards and penalties must be calculated correctly." The inconsistent window sizes cause incorrect reputation-based reward distribution.

The issue does NOT cause:
- Direct funds loss or theft
- Consensus safety violations
- Complete loss of liveness
- State corruption

Therefore Medium severity is appropriate rather than High or Critical.

## Likelihood Explanation

**Likelihood: High**

This issue occurs naturally and frequently:

1. **Default Configuration**: The 10x window size difference is the default setting used in production
2. **Common Occurrence**: Any validator experiencing temporary issues (network partition, hardware maintenance, software upgrades) will accumulate failed proposals that persist in reputation calculations 10x longer than their voting recovery
3. **No Attacker Required**: The vulnerability manifests through normal network operations without malicious activity
4. **Persistent Impact**: Once a validator is penalized, they cannot "redeem" themselves through voting alone - they must wait for the full proposer window to expire (1000 blocks vs 100 blocks)

The combination of high likelihood and medium impact makes this a significant issue requiring remediation.

## Recommendation

**Option 1: Align Window Sizes (Recommended)**

Use the same window size for both vote counting and proposal counting to ensure consistent time horizons:

```rust
pub fn new(
    voter_window_size: usize,
    proposer_window_size: usize,
    reputation_window_from_stale_end: bool,
) -> Self {
    // Ensure consistent window sizes for fair reputation calculation
    let unified_window_size = std::cmp::max(voter_window_size, proposer_window_size);
    Self {
        voter_window_size: unified_window_size,
        proposer_window_size: unified_window_size,
        reputation_window_from_stale_end,
    }
}
```

**Option 2: Consider Votes in Failure Calculation**

Modify the weight calculation to give validators with active voting participation a chance to offset old failures:

```rust
if cur_failed_proposals * 100
    > (cur_proposals + cur_failed_proposals) * self.failure_threshold_percent
    && cur_votes == 0  // Only penalize if also inactive in voting
{
    self.failed_weight
} else if cur_proposals > 0 || cur_votes > 0 {
    self.active_weight
} else {
    self.inactive_weight
}
```

**Option 3: Time-Weighted Decay**

Implement exponential decay for old failures so recent behavior weighs more heavily than distant past:

```rust
// Weight recent blocks more heavily (e.g., exponential decay)
// This ensures old failures have diminishing impact over time
```

## Proof of Concept

```rust
#[test]
fn test_inconsistent_window_reputation_penalty() {
    // Setup: 100 validators
    let num_validators = 100;
    let voter_window_size = num_validators * 1;  // 100 blocks
    let proposer_window_size = num_validators * 10;  // 1000 blocks
    
    let aggregation = NewBlockEventAggregation::new(
        voter_window_size,
        proposer_window_size,
        false,  // reputation_window_from_stale_end = false (use recent blocks)
    );
    
    // Create 1000 blocks of history
    let mut history = vec![];
    let validators: Vec<Author> = (0..num_validators).map(|i| Author::random()).collect();
    let mut epoch_to_candidates = HashMap::new();
    epoch_to_candidates.insert(1, validators.clone());
    
    // Blocks 1-100: Validator 0 votes successfully
    for round in 1..=100 {
        let mut voters_bitvec = BitVec::with_num_bits(num_validators as u16);
        voters_bitvec.set(0);  // Validator 0 votes
        
        history.push(NewBlockEvent::new(
            1,  // epoch
            round,
            validators[1],  // different proposer
            voters_bitvec,
            vec![],  // no failed proposers
            round - 1,
            0,
        ));
    }
    
    // Blocks 101-112: Validator 0 had 10 successful + 2 failed proposals
    for round in 101..=112 {
        let failed_indices = if round == 105 || round == 110 {
            vec![0]  // Validator 0 failed as proposer
        } else {
            vec![]
        };
        
        history.push(NewBlockEvent::new(
            1,
            round,
            validators[0],  // Validator 0 as proposer
            BitVec::with_num_bits(num_validators as u16),
            failed_indices,
            round - 1,
            0,
        ));
    }
    
    history.reverse();  // Newest to oldest
    
    // Calculate metrics
    let votes = aggregation.count_votes(&epoch_to_candidates, &history);
    let proposals = aggregation.count_proposals(&epoch_to_candidates, &history);
    let failed_proposals = aggregation.count_failed_proposals(&epoch_to_candidates, &history);
    
    let validator_0 = &validators[0];
    let cur_votes = *votes.get(validator_0).unwrap_or(&0);
    let cur_proposals = *proposals.get(validator_0).unwrap_or(&0);
    let cur_failed_proposals = *failed_proposals.get(validator_0).unwrap_or(&0);
    
    // Assertions demonstrating the vulnerability
    assert_eq!(cur_votes, 100, "Should have 100 votes from recent blocks");
    assert_eq!(cur_proposals, 10, "Should have 10 proposals from older blocks");
    assert_eq!(cur_failed_proposals, 2, "Should have 2 failed proposals from older blocks");
    
    // Failure rate calculation
    let failure_threshold_percent = 10;
    let failure_rate_check = cur_failed_proposals * 100 
        > (cur_proposals + cur_failed_proposals) * failure_threshold_percent;
    
    // 2 * 100 = 200
    // (10 + 2) * 10 = 120
    // 200 > 120 = TRUE
    assert!(failure_rate_check, "Validator is penalized despite 100 recent votes!");
    
    println!("VULNERABILITY DEMONSTRATED:");
    println!("Validator 0 has {} recent votes but is penalized for {} old failures", 
             cur_votes, cur_failed_proposals);
    println!("Failure rate: {}/({} + {}) = {:.1}% > {}%", 
             cur_failed_proposals, cur_proposals, cur_failed_proposals,
             (cur_failed_proposals as f64 / (cur_proposals + cur_failed_proposals) as f64) * 100.0,
             failure_threshold_percent);
}
```

## Notes

This vulnerability demonstrates a fundamental design flaw where inconsistent time horizons for different reputation metrics create unfair outcomes. While the larger proposer window was intentionally designed to gather more statistical data (since proposer opportunities are rare), the implementation fails to account for how this asymmetry penalizes validators who have recovered from past issues. The fix should either align the window sizes or modify the penalty logic to account for recent voting activity as a mitigating factor.

### Citations

**File:** consensus/src/liveness/leader_reputation.rs (L232-235)
```rust
    voter_window_size: usize,
    proposer_window_size: usize,
    reputation_window_from_stale_end: bool,
}
```

**File:** consensus/src/liveness/leader_reputation.rs (L467-468)
```rust
/// We want to optimize leader selection to primarily maximize network's throughput,
/// but we also, in combinatoin with staking rewards logic, need to be reasonably fair.
```

**File:** consensus/src/liveness/leader_reputation.rs (L541-549)
```rust
                if cur_failed_proposals * 100
                    > (cur_proposals + cur_failed_proposals) * self.failure_threshold_percent
                {
                    self.failed_weight
                } else if cur_proposals > 0 || cur_votes > 0 {
                    self.active_weight
                } else {
                    self.inactive_weight
                }
```

**File:** types/src/on_chain_config/consensus_config.rs (L498-499)
```rust
                    proposer_window_num_validators_multiplier: 10,
                    voter_window_num_validators_multiplier: 1,
```
