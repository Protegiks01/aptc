# Audit Report

## Title
Critical Race Condition: Module Validation Requirements Lost During Concurrent Abort, Enabling Consensus Disagreement

## Summary

A critical race condition exists in BlockSTMv2's cold validation system where module validation requirements can be permanently lost when `activate_pending_requirements` snapshots a transaction status concurrently with `finish_abort`. This allows transactions executing after module publishing to escape validation entirely, violating the deterministic execution invariant and potentially causing consensus disagreement between validators.

## Finding Description

BlockSTMv2 implements a "cold validation" system for validating module reads after module publishing. When a transaction publishes modules, all subsequent transactions that executed or are executing must validate their module read-sets to ensure they didn't read incompatible module versions.

The vulnerability occurs in the following sequence: [1](#0-0) 

When `activate_pending_requirements` is called, it snapshots each transaction's status to determine which incarnations need validation: [2](#0-1) 

**The Race Condition:**

1. Transaction N finishes execution at incarnation M, transitions to `Executed` state
2. Transaction N-1 commits with module publishing, recording cold validation requirements
3. **Thread A** (dedicated cold validator): Calls `activate_pending_requirements`
   - Calls `requires_module_validation(txn_idx=N)` 
   - Acquires status lock, reads: `Executed`, incarnation M
   - Releases status lock
   - Records: `active_reqs.versions[N] = (M, false)` (not executing)
4. **Thread B** (aborter): Concurrently aborts transaction N incarnation M
   - Calls `finish_abort(N, M, false)`
   - Transitions from `Executed` to `PendingScheduling`, incarnation becomes M+1 [3](#0-2) 

5. **Thread C** (re-executor): Picks up transaction N
   - Starts executing incarnation M+1 (never gets validation requirements deferred)
   - Finishes execution, calls `finish_execution(N, M+1)`
   - Returns **empty** module validation requirements (no requirements were deferred) [4](#0-3) 

6. **Thread A** (continued): Processes validation requirement for (N, M, false)
   - Since `is_deferred=false`, returns `TaskKind::ModuleValidation(N, M, modules)` [5](#0-4) 

7. **Thread D** (validation worker): Processes `TaskKind::ModuleValidation(N, M, modules)`
   - Calls `module_validation_v2(N, incarnation_to_validate=M, modules)`
   - Loads read set from `last_input_output` - gets incarnation M+1 (from step 5)
   - Checks: `blockstm_v2_incarnation` (M+1) > `incarnation_to_validate` (M)
   - **Returns early without validation** - considers it obsolete [6](#0-5) 

**Result:** Transaction N incarnation M+1 executed AFTER module publishing but was NEVER validated against the newly published modules. Its module reads go completely undetected.

The root cause is that `activate_pending_requirements` creates a point-in-time snapshot of transaction incarnations, but if those incarnations abort before validation processing, the new incarnations execute without getting requirements deferred via: [7](#0-6) 

## Impact Explanation

**Severity: CRITICAL** (up to $1,000,000)

This vulnerability breaks the **Deterministic Execution** invariant: "All validators must produce identical state roots for identical blocks."

**Consensus Safety Violation:**

1. Different validators may process the race condition differently based on timing
2. Some validators validate transaction N's module reads, detecting incompatibilities → abort the transaction
3. Other validators skip validation due to the race → transaction succeeds
4. Validators compute **different state roots** for the same block
5. **Consensus disagreement** - network cannot progress, potential chain split

**Attack Scenario:**

An attacker can exploit this by:
1. Deploying a transaction that publishes an incompatible module version
2. Ensuring high contention causes frequent aborts of subsequent transactions
3. Some validators will validate module reads correctly, others will skip validation
4. Deterministic execution breaks → consensus failure

This is a **consensus-level vulnerability** that can cause:
- Non-recoverable network partition requiring a hardfork
- Chain splits with validators on different forks
- Total loss of liveness if >1/3 validators disagree

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

This race condition occurs naturally in high-contention workloads:

1. **Trigger Frequency**: Any block containing module publishing creates cold validation requirements
2. **Concurrent Aborts**: High in contended workloads where transactions frequently abort and re-execute
3. **Race Window**: The window between `activate_pending_requirements` snapshot and validation processing is substantial
4. **No Special Privileges Required**: Any transaction sender can trigger module publishing and contention

The vulnerability is **deterministic once triggered** - if the race condition occurs, validation is definitively lost. The non-determinism is only in whether different validators experience the race at the same time.

**Frequency Amplifiers:**
- Complex Move modules triggering many validations
- High transaction throughput increasing abort rates  
- Long-running transactions increasing race windows

## Recommendation

Add an **incarnation re-check** in `module_validation_v2` to detect if the incarnation being validated is obsolete due to abort:

```rust
fn module_validation_v2(
    idx_to_validate: TxnIndex,
    incarnation_to_validate: Incarnation,
    scheduler: &SchedulerV2,
    updated_module_keys: &BTreeSet<ModuleId>,
    last_input_output: &TxnLastInputOutput<T, E::Output>,
    global_module_cache: &GlobalModuleCache<...>,
    versioned_cache: &MVHashMap<...>,
) -> Result<bool, PanicError> {
    // NEW: Check if incarnation was aborted before we proceed
    let current_incarnation = scheduler.txn_statuses.incarnation(idx_to_validate);
    if current_incarnation > incarnation_to_validate {
        // Incarnation was aborted - need to re-record validation requirement
        // for the new incarnation
        let (new_incarnation, is_executing) = scheduler.txn_statuses
            .requires_module_validation(idx_to_validate)
            .ok_or_else(|| code_invariant_error("Lost validation requirement"))?;
        
        if is_executing {
            // Defer to executing worker
            scheduler.txn_statuses.defer_module_validation(
                idx_to_validate,
                new_incarnation,
                updated_module_keys,
            )?;
        } else {
            // Re-queue validation for new incarnation
            return Err(code_invariant_error(
                "Validation requirement needs re-queueing for new incarnation"
            ));
        }
        return Ok(true); // Handled appropriately
    }
    
    // Existing validation logic...
}
```

**Alternative Fix**: Make `activate_pending_requirements` re-check transaction status under lock before recording:

```rust
let new_versions: BTreeMap<TxnIndex, (Incarnation, bool)> = (starting_idx..ending_idx)
    .filter_map(|txn_idx| {
        // Acquire lock and atomically check status + incarnation
        let status = &statuses.statuses[txn_idx as usize];
        let guard = status.status_with_incarnation.lock();
        
        match &guard.status {
            SchedulingStatus::Executing(_) => Some((txn_idx, (guard.incarnation(), true))),
            SchedulingStatus::Executed => Some((txn_idx, (guard.incarnation(), false))),
            _ => None, // Skip aborted/pending transactions
        }
    })
    .collect();
```

## Proof of Concept

```rust
// Rust test demonstrating the race condition
#[test]
fn test_validation_loss_race_condition() {
    use std::sync::{Arc, Barrier};
    use std::thread;
    
    let num_txns = 10;
    let statuses = Arc::new(ExecutionStatuses::new(num_txns));
    let cold_reqs = Arc::new(ColdValidationRequirements::<ModuleId>::new(num_txns));
    
    // Setup: Transaction 5 is Executed at incarnation 0
    let status = statuses.get_status_mut(5);
    *status.status_with_incarnation.lock() = 
        StatusWithIncarnation::new_for_test(SchedulingStatus::Executed, 0);
    
    // Record cold validation requirements for txn 5
    let mut modules = BTreeSet::new();
    modules.insert(ModuleId::new(AccountAddress::ZERO, Identifier::new("TestModule").unwrap()));
    cold_reqs.record_requirements(0, 4, num_txns, modules.clone()).unwrap();
    
    let barrier = Arc::new(Barrier::new(3));
    
    // Thread 1: activate_pending_requirements (snapshots txn 5 as Executed, inc 0)
    let statuses_t1 = statuses.clone();
    let cold_reqs_t1 = cold_reqs.clone();
    let barrier_t1 = barrier.clone();
    let handle1 = thread::spawn(move || {
        barrier_t1.wait(); // Synchronize
        // This will snapshot txn 5 as (incarnation=0, is_executing=false)
        cold_reqs_t1.activate_pending_requirements(&statuses_t1).unwrap();
    });
    
    // Thread 2: finish_abort (aborts incarnation 0, creates incarnation 1)
    let statuses_t2 = statuses.clone();
    let barrier_t2 = barrier.clone();
    let handle2 = thread::spawn(move || {
        barrier_t2.wait(); // Synchronize
        statuses_t2.start_abort(5, 0).unwrap();
        statuses_t2.finish_abort(5, 0, false).unwrap();
        // Now txn 5 is PendingScheduling with incarnation 1
    });
    
    // Thread 3: Start and finish execution of incarnation 1
    let statuses_t3 = statuses.clone();
    let barrier_t3 = barrier.clone();
    let handle3 = thread::spawn(move || {
        barrier_t3.wait(); // Synchronize
        thread::sleep(Duration::from_millis(10)); // Let abort happen first
        statuses_t3.start_executing(5).unwrap();
        // Execute without deferred requirements
        statuses_t3.finish_execution(5, 1).unwrap(); // Returns empty requirements!
    });
    
    handle1.join().unwrap();
    handle2.join().unwrap();
    handle3.join().unwrap();
    
    // Verify: Active requirements show (5, incarnation=0, is_executing=false)
    let (txn_idx, incarnation, req) = cold_reqs
        .get_validation_requirement_to_process(0, num_txns, &statuses)
        .unwrap()
        .expect("Should have active requirement");
    
    assert_eq!(txn_idx, 5);
    assert_eq!(incarnation, 0); // Stale incarnation!
    assert_eq!(req.is_deferred, false);
    
    // But current incarnation is 1
    assert_eq!(statuses.incarnation(5), 1);
    
    // VULNERABILITY: Validation for incarnation 0 will be skipped due to mismatch
    // Incarnation 1 never got requirements deferred and escapes validation
}
```

## Notes

This vulnerability demonstrates a fundamental limitation of snapshot-based validation tracking in concurrent systems. The fix requires either:
1. Re-validating incarnation freshness before processing
2. Atomic status checks under lock during snapshot creation
3. Retry mechanisms when incarnation mismatches are detected

The issue is exacerbated by the separation between `activate_pending_requirements` (snapshots) and validation processing (consumes snapshots), creating an unbounded time window for the race condition.

### Citations

**File:** aptos-move/block-executor/src/cold_validation.rs (L453-516)
```rust
    fn activate_pending_requirements(
        &self,
        statuses: &ExecutionStatuses,
    ) -> Result<bool, PanicError> {
        let pending_reqs = {
            let mut guard = self.pending_requirements.lock();
            if guard.is_empty() {
                // No requirements to drain.
                return Ok(false);
            }
            std::mem::take(&mut *guard)
        };

        let starting_idx = pending_reqs
            .iter()
            .map(|req| req.from_idx)
            .min()
            .expect("Expected at least one requirement");
        let ending_idx = pending_reqs
            .iter()
            .map(|req| req.to_idx)
            .max()
            .expect("Expected at least one requirement");
        if starting_idx >= ending_idx || ending_idx > self.num_txns {
            return Err(code_invariant_error(format!(
                "Invariant broken, starting idx {} >= ending idx {} or ending idx > num_txns {}",
                starting_idx, ending_idx, self.num_txns
            )));
        }

        let new_versions: BTreeMap<TxnIndex, (Incarnation, bool)> = (starting_idx..ending_idx)
            .filter_map(|txn_idx| {
                statuses
                    .requires_module_validation(txn_idx)
                    .map(|(incarnation, is_executing)| (txn_idx, (incarnation, is_executing)))
            })
            .collect();
        let new_requirements = pending_reqs
            .into_iter()
            .fold(BTreeSet::new(), |mut acc, req| {
                acc.extend(req.requirements);
                acc
            });

        let active_reqs = self.active_requirements.dereference_mut();
        active_reqs.requirements.extend(new_requirements);
        active_reqs.versions.extend(new_versions);

        if active_reqs.versions.is_empty() {
            // It is possible that the active versions map was empty, and no pending
            // requirements needed to be activated (i.e. not executing or executed).
            // In this case, we may update min_idx_with_unprocessed_validation_requirement
            // as validation_requirement_processed does so only when the pending
            // requirements are empty.
            let pending_reqs_guard = self.pending_requirements.lock();
            if pending_reqs_guard.is_empty() {
                self.min_idx_with_unprocessed_validation_requirement
                    .store(u32::MAX, Ordering::Relaxed);
                return Ok(true);
            }
        }

        Ok(false)
    }
```

**File:** aptos-move/block-executor/src/scheduler_status.rs (L571-625)
```rust
    pub(crate) fn finish_execution(
        &self,
        txn_idx: TxnIndex,
        finished_incarnation: Incarnation,
    ) -> Result<Option<BTreeSet<ModuleId>>, PanicError> {
        // TODO(BlockSMTv2): Handle waiting workers when supported (defer waking up).

        let status = &self.statuses[txn_idx as usize];
        let status_guard = &mut *status.status_with_incarnation.lock();

        // An incarnation of a transaction can only increase when both finish_execution and
        // start_abort take effect for the prior incarnation. However, finish_execution is
        // invoked once per incarnation, and thus incarnations must always match.
        if status_guard.incarnation() != finished_incarnation {
            return Err(code_invariant_error(format!(
                "Finish execution of incarnation {}, but inner status {:?}",
                finished_incarnation, status_guard,
            )));
        }

        match status_guard.status {
            SchedulingStatus::Executing(_) => {
                let requirements = if let SchedulingStatus::Executing(requirements) =
                    std::mem::replace(&mut status_guard.status, SchedulingStatus::Executed)
                {
                    requirements
                } else {
                    unreachable!("In Executing variant match arm");
                };

                let new_status_flag = if status.is_stalled() {
                    DependencyStatus::ShouldDefer
                } else {
                    DependencyStatus::IsSafe
                };
                status.swap_dependency_status_any(
                    &[DependencyStatus::WaitForExecution],
                    new_status_flag,
                    "finish_execution",
                )?;

                Ok(Some(requirements))
            },
            SchedulingStatus::Aborted => {
                self.to_pending_scheduling(txn_idx, status_guard, finished_incarnation + 1, true);
                Ok(None)
            },
            SchedulingStatus::PendingScheduling | SchedulingStatus::Executed => {
                Err(code_invariant_error(format!(
                    "Status update to Executed failed, previous inner status {:?}",
                    status_guard
                )))
            },
        }
    }
```

**File:** aptos-move/block-executor/src/scheduler_status.rs (L647-722)
```rust
    pub(crate) fn finish_abort(
        &self,
        txn_idx: TxnIndex,
        aborted_incarnation: Incarnation,
        start_next_incarnation: bool,
    ) -> Result<(), PanicError> {
        let status = &self.statuses[txn_idx as usize];
        let new_incarnation = aborted_incarnation + 1;
        if status.next_incarnation_to_abort.load(Ordering::Relaxed) != new_incarnation {
            // The caller must have already successfully performed a start_abort, while
            // higher incarnation may not have started until the abort finished (here).
            return Err(code_invariant_error(format!(
                "Finish abort of incarnation {}, self.next_incarnation_to_abort = {}",
                aborted_incarnation,
                status.next_incarnation_to_abort.load(Ordering::Relaxed),
            )));
        }

        {
            let status_guard = &mut *status.status_with_incarnation.lock();
            if status_guard.already_aborted(aborted_incarnation)
                || status_guard.never_started_execution(aborted_incarnation)
            {
                return Err(code_invariant_error(format!(
                    "Finish abort of incarnation {}, but inner status {:?}",
                    aborted_incarnation, status_guard
                )));
            }

            match status_guard.status {
                SchedulingStatus::Executing(_) => {
                    if start_next_incarnation {
                        return Err(code_invariant_error(format!(
                            "Finish abort for txn_idx: {} incarnation: {} w. start_next_incarnation \
                            expected Executed Status, got Executing",
                            txn_idx, aborted_incarnation
                        )));
                    }

                    // Module validation requirements are irrelevant as the incarnation was aborted.
                    status_guard.status = SchedulingStatus::Aborted;
                    status.swap_dependency_status_any(
                        &[DependencyStatus::WaitForExecution],
                        DependencyStatus::ShouldDefer,
                        "finish_abort",
                    )?;
                },
                SchedulingStatus::Executed => {
                    self.to_pending_scheduling(
                        txn_idx,
                        status_guard,
                        new_incarnation,
                        !start_next_incarnation,
                    );
                    if start_next_incarnation {
                        let started_incarnation = self.to_executing(txn_idx, status_guard)?;
                        if Some(aborted_incarnation + 1) != started_incarnation {
                            return Err(code_invariant_error(format!(
                                "Finish abort started incarnation {:?} != expected {}",
                                txn_idx,
                                aborted_incarnation + 1
                            )));
                        }
                    }
                },
                SchedulingStatus::PendingScheduling | SchedulingStatus::Aborted => {
                    return Err(code_invariant_error(format!(
                        "Status update to Aborted failed, previous inner status {:?}",
                        status_guard
                    )));
                },
            }
        }

        Ok(())
    }
```

**File:** aptos-move/block-executor/src/scheduler_status.rs (L792-805)
```rust
    // - `None` if the txn is not executing / executed.
    pub(crate) fn requires_module_validation(
        &self,
        txn_idx: TxnIndex,
    ) -> Option<(Incarnation, bool)> {
        let status = &self.statuses[txn_idx as usize];
        let status_guard = status.status_with_incarnation.lock();

        match status_guard.status {
            SchedulingStatus::Executing(_) => Some((status_guard.incarnation(), true)),
            SchedulingStatus::Executed => Some((status_guard.incarnation(), false)),
            SchedulingStatus::PendingScheduling | SchedulingStatus::Aborted => None,
        }
    }
```

**File:** aptos-move/block-executor/src/scheduler_status.rs (L823-860)
```rust
    pub(crate) fn defer_module_validation(
        &self,
        txn_idx: TxnIndex,
        incarnation: Incarnation,
        requirements: &BTreeSet<ModuleId>,
    ) -> Result<Option<bool>, PanicError> {
        let status = &self.statuses[txn_idx as usize];
        let mut status_guard = status.status_with_incarnation.lock();

        if status_guard.incarnation() < incarnation {
            return Err(code_invariant_error(format!(
                "Deferring module validation for txn_idx: {} incarnation: {} < incarnation to validate {}",
                txn_idx, status_guard.incarnation(), incarnation
            )));
        }
        if status_guard.incarnation() > incarnation {
            // Nothing to be done as a higher incarnation has already been created.
            return Ok(None);
        }

        match &mut status_guard.status {
            SchedulingStatus::PendingScheduling => Err(code_invariant_error(format!(
                "Deferring module validation for txn_idx: {} incarnation: {} is pending scheduling",
                txn_idx,
                status_guard.incarnation()
            ))),
            SchedulingStatus::Executing(stored_requirements) => {
                // Note: we can move the clone out of the critical section if needed.
                stored_requirements.extend(requirements.iter().cloned());
                Ok(Some(true))
            },
            SchedulingStatus::Executed => Ok(Some(false)),
            SchedulingStatus::Aborted => {
                // Already aborted, nothing to be done.
                Ok(None)
            },
        }
    }
```

**File:** aptos-move/block-executor/src/scheduler_v2.rs (L1109-1124)
```rust
            if is_deferred {
                let defer_outcome = self.txn_statuses.defer_module_validation(
                    txn_idx,
                    incarnation,
                    modules_to_validate,
                )?;

                if defer_outcome == Some(false) {
                    // defer call did not succeed because the incarnation had finished execution.
                    // Ask the caller (the dedicated worker) to process the requirements normally.
                    return Ok(Some(TaskKind::ModuleValidation(
                        txn_idx,
                        incarnation,
                        modules_to_validate,
                    )));
                }
```

**File:** aptos-move/block-executor/src/executor.rs (L745-754)
```rust
        let blockstm_v2_incarnation = read_set.blockstm_v2_incarnation().ok_or_else(|| {
            code_invariant_error(
                "BlockSTMv2 must be enabled in CapturedReads when validating module reads",
            )
        })?;
        if blockstm_v2_incarnation > incarnation_to_validate || is_speculative_failure {
            // No need to validate as a newer incarnation has already been executed
            // and recorded its output, or the incarnation has resulted in a speculative
            // failure, which means there will be a further re-execution.
            return Ok(true);
```
