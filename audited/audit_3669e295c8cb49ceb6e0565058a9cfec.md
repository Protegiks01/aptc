# Audit Report

## Title
Progress Corruption in Shard Pruner Initialization Leading to Backward Progress Movement

## Summary
The `StateMerkleShardPruner::new()` function contains a critical bug where if the shard's stored progress exceeds the metadata progress parameter, it attempts to prune with an inverted version range. This causes the progress marker to move backward, corrupting pruning metadata and violating the monotonic progress invariant.

## Finding Description

During shard pruner initialization, the code retrieves or initializes the shard's progress from the database: [1](#0-0) 

The `get_or_initialize_subpruner_progress()` function returns existing progress from the database if it exists, which may be **greater** than `metadata_progress`: [2](#0-1) 

When `progress > metadata_progress`, the subsequent prune call has an inverted range: [3](#0-2) 

In the `prune()` function, this inverted range causes `get_stale_node_indices()` to seek to a version higher than the target: [4](#0-3) 

Since the iterator starts at `start_version` (higher value) and only includes indices where `stale_since_version <= target_version` (lower value), no indices match. The function returns an empty result but the prune operation still updates the progress to the lower `target_version`: [5](#0-4) 

This scenario occurs realistically during **database truncation operations**, where commit progress is rolled back but pruner progress markers are NOT reset: [6](#0-5) [7](#0-6) 

The truncation code updates commit progress but never clears `StateMerkleShardPrunerProgress(ShardId)` or `StateKvShardPrunerProgress(ShardId)` keys.

**Systemic Issue**: The same bug exists in multiple pruner components:
- `StateKvShardPruner` [8](#0-7) 
- `TransactionInfoPruner` [9](#0-8) 

## Impact Explanation

**Medium Severity** - State inconsistencies requiring intervention:

1. **Progress Metadata Corruption**: Pruner progress moves backward, indicating versions [X+1, Y] haven't been processed when they actually have been
2. **Monitoring/Alerting Disruption**: Progress metrics become unreliable for operational monitoring
3. **Invariant Violation**: Breaks the State Consistency invariant that metadata accurately reflects database state
4. **Recovery Complications**: Makes debugging and recovery operations more difficult

This does NOT rise to High/Critical because:
- No consensus impact (pruning is background maintenance)
- No loss of funds or liveness issues  
- System continues functioning (next pruning cycle compensates)
- Data structures remain valid (only metadata is incorrect)

However, this is a **significant protocol violation** affecting database integrity guarantees.

## Likelihood Explanation

**Medium-to-High Likelihood** in production environments:

1. **Occurs during standard recovery procedures**: Database truncation is a documented recovery operation
2. **No attacker action required**: Happens automatically after legitimate administrative actions
3. **Affects all nodes** that perform truncation or restore operations
4. **Multiple vulnerable components**: StateKv, StateMerkle, and Ledger pruners all affected

The bug triggers whenever:
- Administrators run database truncation for recovery
- Nodes restore from older backups
- Crash recovery involves commit progress rollback

## Recommendation

Add validation to prevent backward progress movement and reset pruner progress during truncation:

**Fix 1: Add validation in pruner initialization**
```rust
pub(in crate::pruner) fn new(
    shard_id: usize,
    db_shard: Arc<DB>,
    metadata_progress: Version,
) -> Result<Self> {
    let progress = get_or_initialize_subpruner_progress(
        &db_shard,
        &S::progress_metadata_key(Some(shard_id)),
        metadata_progress,
    )?;
    let myself = Self {
        shard_id,
        db_shard,
        _phantom: PhantomData,
    };

    // FIX: Only prune forward, never backward
    if progress < metadata_progress {
        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up {} shard {shard_id}.",
            S::name(),
        );
        myself.prune(progress, metadata_progress, usize::MAX)?;
    } else if progress > metadata_progress {
        // Progress ahead of metadata - likely from truncation
        // Reset to metadata progress without pruning
        warn!(
            shard_progress = progress,
            metadata_progress = metadata_progress,
            "Shard progress ahead of metadata, resetting to metadata progress"
        );
        let mut batch = SchemaBatch::new();
        batch.put::<DbMetadataSchema>(
            &S::progress_metadata_key(Some(shard_id)),
            &DbMetadataValue::Version(metadata_progress),
        )?;
        db_shard.write_schemas(batch)?;
    }

    Ok(myself)
}
```

**Fix 2: Reset pruner progress during truncation**
Add to `truncation_helper.rs`:
```rust
pub(crate) fn reset_pruner_progress(
    metadata_db: &DB,
    target_version: Version,
) -> Result<()> {
    let mut batch = SchemaBatch::new();
    
    // Reset all pruner progress markers
    batch.put::<DbMetadataSchema>(
        &DbMetadataKey::StateMerklePrunerProgress,
        &DbMetadataValue::Version(target_version),
    )?;
    batch.put::<DbMetadataSchema>(
        &DbMetadataKey::StateKvPrunerProgress,
        &DbMetadataValue::Version(target_version),
    )?;
    batch.put::<DbMetadataSchema>(
        &DbMetadataKey::LedgerPrunerProgress,
        &DbMetadataValue::Version(target_version),
    )?;
    
    metadata_db.write_schemas(batch)
}
```

Call this function in truncation operations before rolling back data.

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use aptos_schemadb::schema::KeyCodec;
    use aptos_temppath::TempPath;
    
    #[test]
    fn test_backward_progress_corruption() {
        let tmpdir = TempPath::new();
        let db = Arc::new(DB::open(
            tmpdir.path(),
            "test_db",
            &["test_cf"]
        ).unwrap());
        
        // Simulate state: shard progress at 200, metadata progress at 100
        let shard_id = 0;
        let shard_progress = 200;
        let metadata_progress = 100;
        
        // Write shard progress
        db.put::<DbMetadataSchema>(
            &DbMetadataKey::StateMerkleShardPrunerProgress(shard_id),
            &DbMetadataValue::Version(shard_progress),
        ).unwrap();
        
        // Initialize pruner - this triggers the bug
        let pruner = StateMerkleShardPruner::<StaleNodeIndexSchema>::new(
            shard_id,
            db.clone(),
            metadata_progress,
        ).unwrap();
        
        // Verify progress moved backward (BUG!)
        let final_progress = db
            .get::<DbMetadataSchema>(
                &DbMetadataKey::StateMerkleShardPrunerProgress(shard_id)
            )
            .unwrap()
            .unwrap()
            .expect_version();
            
        assert_eq!(final_progress, metadata_progress); // 100, not 200!
        assert!(final_progress < shard_progress, "Progress moved backward!");
    }
}
```

**Notes**

This vulnerability affects database integrity during recovery operations. While it doesn't directly compromise consensus or funds, it violates critical metadata consistency guarantees. The bug is systemic across multiple pruner implementations and occurs during documented administrative procedures, making it a legitimate concern for production deployments requiring reliable recovery capabilities.

### Citations

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L36-40)
```rust
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &S::progress_metadata_key(Some(shard_id)),
            metadata_progress,
        )?;
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L53-53)
```rust
        myself.prune(progress, metadata_progress, usize::MAX)?;
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L85-90)
```rust
            if done {
                batch.put::<DbMetadataSchema>(
                    &S::progress_metadata_key(Some(self.shard_id)),
                    &DbMetadataValue::Version(target_version),
                )?;
            }
```

**File:** storage/aptosdb/src/pruner/pruner_utils.rs (L44-60)
```rust
pub(crate) fn get_or_initialize_subpruner_progress(
    sub_db: &DB,
    progress_key: &DbMetadataKey,
    metadata_progress: Version,
) -> Result<Version> {
    Ok(
        if let Some(v) = sub_db.get::<DbMetadataSchema>(progress_key)? {
            v.expect_version()
        } else {
            sub_db.put::<DbMetadataSchema>(
                progress_key,
                &DbMetadataValue::Version(metadata_progress),
            )?;
            metadata_progress
        },
    )
}
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L191-217)
```rust
    pub(in crate::pruner::state_merkle_pruner) fn get_stale_node_indices(
        state_merkle_db_shard: &DB,
        start_version: Version,
        target_version: Version,
        limit: usize,
    ) -> Result<(Vec<StaleNodeIndex>, Option<Version>)> {
        let mut indices = Vec::new();
        let mut iter = state_merkle_db_shard.iter::<S>()?;
        iter.seek(&StaleNodeIndex {
            stale_since_version: start_version,
            node_key: NodeKey::new_empty_path(0),
        })?;

        let mut next_version = None;
        while indices.len() < limit {
            if let Some((index, _)) = iter.next().transpose()? {
                next_version = Some(index.stale_since_version);
                if index.stale_since_version <= target_version {
                    indices.push(index);
                    continue;
                }
            }
            break;
        }

        Ok((indices, next_version))
    }
```

**File:** storage/aptosdb/src/utils/truncation_helper.rs (L144-180)
```rust
pub(crate) fn truncate_state_merkle_db(
    state_merkle_db: &StateMerkleDb,
    target_version: Version,
) -> Result<()> {
    let status = StatusLine::new(Progress::new("Truncating State Merkle DB.", target_version));

    loop {
        let current_version = get_current_version_in_state_merkle_db(state_merkle_db)?
            .expect("Current version of state merkle db must exist.");
        status.set_current_version(current_version);
        assert_ge!(current_version, target_version);
        if current_version == target_version {
            break;
        }

        let version_before = find_closest_node_version_at_or_before(
            state_merkle_db.metadata_db(),
            current_version - 1,
        )?
        .expect("Must exist.");

        let mut top_levels_batch = SchemaBatch::new();

        delete_nodes_and_stale_indices_at_or_after_version(
            state_merkle_db.metadata_db(),
            current_version,
            None, // shard_id
            &mut top_levels_batch,
        )?;

        state_merkle_db.commit_top_levels(version_before, top_levels_batch)?;

        truncate_state_merkle_db_shards(state_merkle_db, version_before)?;
    }

    Ok(())
}
```

**File:** storage/aptosdb/src/schema/db_metadata/mod.rs (L49-72)
```rust
pub enum DbMetadataKey {
    LedgerPrunerProgress,
    StateMerklePrunerProgress,
    EpochEndingStateMerklePrunerProgress,
    StateKvPrunerProgress,
    StateSnapshotKvRestoreProgress(Version),
    LedgerCommitProgress,
    StateKvCommitProgress,
    OverallCommitProgress,
    StateKvShardCommitProgress(ShardId),
    StateMerkleCommitProgress,
    StateMerkleShardCommitProgress(ShardId),
    EventPrunerProgress,
    TransactionAccumulatorPrunerProgress,
    TransactionInfoPrunerProgress,
    TransactionPrunerProgress,
    WriteSetPrunerProgress,
    StateMerkleShardPrunerProgress(ShardId),
    EpochEndingStateMerkleShardPrunerProgress(ShardId),
    StateKvShardPrunerProgress(ShardId),
    StateMerkleShardRestoreProgress(ShardId, Version),
    TransactionAuxiliaryDataPrunerProgress,
    PersistedAuxiliaryInfoPrunerProgress,
}
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L25-44)
```rust
    pub(in crate::pruner) fn new(
        shard_id: usize,
        db_shard: Arc<DB>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
            metadata_progress,
        )?;
        let myself = Self { shard_id, db_shard };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up state kv shard {shard_id}."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_info_pruner.rs (L37-56)
```rust
    pub(in crate::pruner) fn new(
        ledger_db: Arc<LedgerDb>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.transaction_info_db_raw(),
            &DbMetadataKey::TransactionInfoPrunerProgress,
            metadata_progress,
        )?;

        let myself = TransactionInfoPruner { ledger_db };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up TransactionInfoPruner."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
```
