# Audit Report

## Title
Epoch Race Condition Enables Double Telemetry Submission in Multi-Instance Deployments

## Summary
During epoch transitions, validators can obtain valid JWTs for both epoch N and epoch N+1 simultaneously when the telemetry service is deployed across multiple instances. This occurs because each service instance maintains an independent validator cache that updates via polling, creating a window where instances temporarily disagree on the current epoch. Validators can exploit this to submit duplicate telemetry data, causing data inconsistencies in backend monitoring systems.

## Finding Description

The telemetry service uses JWT authentication with an epoch-based validation mechanism defined in the `Claims` struct. [1](#0-0) 

JWT tokens are created with the current epoch from the validator cache and are valid for 60 minutes. [2](#0-1) 

The critical vulnerability lies in the JWT validation logic, which checks if the JWT's epoch matches the service instance's cached current epoch. [3](#0-2) 

Each telemetry service instance maintains its own independent in-memory validator cache. [4](#0-3) 

The validator cache is updated by a background polling task at a configurable interval (typically 60-300 seconds based on deployment configuration). [5](#0-4) 

**Exploitation Scenario:**

1. Multiple telemetry service instances (Instance A and Instance B) are deployed for high availability
2. Both instances initially have their validator cache at epoch N
3. On-chain epoch transitions from N to N+1 via block prologue reconfiguration
4. Instance A's cache updater runs first, updating to epoch N+1
5. Instance B's cache hasn't updated yet, still shows epoch N (due to polling interval delay)
6. A validator authenticates to Instance B → receives JWT_B with epoch=N (valid for 60 min)
7. The same validator authenticates to Instance A → receives JWT_A with epoch=N+1 (valid for 60 min)
8. Validator submits telemetry to Instance B with JWT_B → **ACCEPTED** (epoch N matches Instance B's cache)
9. Validator submits telemetry to Instance A with JWT_A → **ACCEPTED** (epoch N+1 matches Instance A's cache)

The telemetry submission endpoints have no deduplication logic based on `run_uuid` or other identifiers. [6](#0-5) [7](#0-6) 

## Impact Explanation

This vulnerability falls under **Medium Severity** per Aptos bug bounty criteria as "State inconsistencies requiring intervention." 

The impact includes:
- **Data Integrity Issues**: Double-counting of telemetry metrics in VictoriaMetrics, Prometheus, and BigQuery backends leads to inflated operational statistics
- **Monitoring Blind Spots**: Incorrect metrics can mask genuine operational issues or attacks
- **Resource Waste**: Duplicate data storage and processing costs
- **Operational Confusion**: DevOps teams making decisions based on incorrect metrics

While this does not directly affect blockchain consensus, validator rewards, or on-chain state, it compromises the operational monitoring infrastructure that is critical for detecting and responding to network issues.

## Likelihood Explanation

**Likelihood: Medium-High** in production environments where:

1. **Multi-instance deployment is standard practice** for high availability and load balancing of auxiliary services
2. **Epoch transitions occur regularly** based on the configured epoch interval (typically every few hours on mainnet)
3. **Cache update intervals** of 60-300 seconds create a significant window of inconsistency
4. **No authentication synchronization** exists between service instances
5. **Validators naturally re-authenticate** before JWT expiration (60 minutes) to maintain connectivity

The exploit requires no special privileges beyond being a registered validator and can occur naturally without malicious intent if validators happen to authenticate during the epoch transition window.

## Recommendation

Implement one or more of the following mitigations:

**Option 1: Shared State Backend**
Replace in-memory caches with a shared cache backend (Redis, etcd) to ensure all instances see consistent epoch values:

```rust
// Replace Arc<RwLock<HashMap>> with shared cache client
pub struct PeerSetCacheUpdater {
    cache_client: Arc<dyn SharedCacheClient>,
    // ...
}
```

**Option 2: Epoch Tolerance Window**
Allow JWTs to be valid for epoch N or N+1 during transitions, but implement submission deduplication based on `run_uuid` + submission timestamp:

```rust
pub async fn authorize_jwt(/*...*/) -> anyhow::Result<Claims, Rejection> {
    // Accept both current and previous epoch during transitions
    if claims.epoch == current_epoch || claims.epoch == current_epoch - 1 {
        if claims.exp > Utc::now().timestamp() as usize {
            Ok(claims)
        } else {
            Err(/* expired */)
        }
    } else {
        Err(/* wrong epoch */)
    }
}
```

**Option 3: Submission Deduplication**
Track submitted telemetry using (chain_id, peer_id, run_uuid, epoch, timestamp) tuples with TTL in a distributed cache to prevent duplicate processing.

**Recommended Approach**: Combine Option 2 (epoch tolerance) with Option 3 (deduplication) for maximum resilience without requiring infrastructure changes.

## Proof of Concept

```rust
// PoC demonstrating the vulnerability
// This would be run as an integration test

#[tokio::test]
async fn test_epoch_race_condition_double_submission() {
    // Setup: Create two telemetry service contexts with independent caches
    let validators_cache_a = Arc::new(RwLock::new(HashMap::new()));
    let validators_cache_b = Arc::new(RwLock::new(HashMap::new()));
    
    let chain_id = ChainId::new(25);
    let peer_id = PeerId::random();
    
    // Both caches initially at epoch 10
    validators_cache_a.write().insert(chain_id, (10, HashMap::new()));
    validators_cache_b.write().insert(chain_id, (10, HashMap::new()));
    
    let context_a = create_test_context(validators_cache_a.clone());
    let context_b = create_test_context(validators_cache_b.clone());
    
    // Simulate epoch transition: Instance A updates to epoch 11
    validators_cache_a.write().insert(chain_id, (11, HashMap::new()));
    // Instance B still at epoch 10 (hasn't polled yet)
    
    // Validator authenticates to both instances
    let jwt_b = create_jwt_token(
        context_b.jwt_service(),
        chain_id,
        peer_id,
        NodeType::Validator,
        10, // Gets epoch 10 from Instance B
        Uuid::new_v4(),
    ).unwrap();
    
    let jwt_a = create_jwt_token(
        context_a.jwt_service(),
        chain_id,
        peer_id,
        NodeType::Validator,
        11, // Gets epoch 11 from Instance A
        Uuid::new_v4(),
    ).unwrap();
    
    // Both JWTs are valid at their respective instances
    let auth_b = authorize_jwt(
        jwt_b.clone(),
        context_b.clone(),
        vec![NodeType::Validator]
    ).await;
    assert!(auth_b.is_ok()); // epoch 10 matches Instance B
    
    let auth_a = authorize_jwt(
        jwt_a.clone(),
        context_a.clone(),
        vec![NodeType::Validator]
    ).await;
    assert!(auth_a.is_ok()); // epoch 11 matches Instance A
    
    // Validator can now submit telemetry twice using different JWTs
    let metrics = Bytes::from("test_metric{} 1.0");
    
    let result_b = handle_metrics_ingest(
        context_b,
        auth_b.unwrap(),
        Some("gzip".into()),
        metrics.clone()
    ).await;
    assert!(result_b.is_ok()); // First submission succeeds
    
    let result_a = handle_metrics_ingest(
        context_a,
        auth_a.unwrap(),
        Some("gzip".into()),
        metrics
    ).await;
    assert!(result_a.is_ok()); // Second submission also succeeds
    
    // Result: Same metrics double-counted in backend systems
}
```

## Notes

This vulnerability is specific to multi-instance deployments of the telemetry service and does not affect single-instance deployments. The issue is architectural rather than a code bug in any specific function. While telemetry is not consensus-critical, data integrity issues in operational monitoring can have cascading effects on incident response and capacity planning.

### Citations

**File:** crates/aptos-telemetry-service/src/types/auth.rs (L28-37)
```rust
#[derive(Debug, Serialize, Deserialize, PartialEq, Eq)]
pub struct Claims {
    pub chain_id: ChainId,
    pub peer_id: PeerId,
    pub node_type: NodeType,
    pub epoch: u64,
    pub exp: usize,
    pub iat: usize,
    pub run_uuid: Uuid,
}
```

**File:** crates/aptos-telemetry-service/src/jwt_auth.rs (L18-42)
```rust
pub fn create_jwt_token(
    jwt_service: &JsonWebTokenService,
    chain_id: ChainId,
    peer_id: PeerId,
    node_type: NodeType,
    epoch: u64,
    uuid: Uuid,
) -> Result<String, Error> {
    let issued = Utc::now().timestamp();
    let expiration = Utc::now()
        .checked_add_signed(chrono::Duration::minutes(60))
        .expect("valid timestamp")
        .timestamp();

    let claims = Claims {
        chain_id,
        peer_id,
        node_type,
        epoch,
        exp: expiration as usize,
        iat: issued as usize,
        run_uuid: uuid,
    };
    jwt_service.encode(claims)
}
```

**File:** crates/aptos-telemetry-service/src/jwt_auth.rs (L44-79)
```rust
pub async fn authorize_jwt(
    token: String,
    context: Context,
    allow_roles: Vec<NodeType>,
) -> anyhow::Result<Claims, Rejection> {
    let decoded: TokenData<Claims> = context.jwt_service().decode(&token).map_err(|e| {
        error!("unable to authorize jwt token: {}", e);
        reject::custom(ServiceError::unauthorized(
            JwtAuthError::InvalidAuthToken.into(),
        ))
    })?;
    let claims = decoded.claims;

    let current_epoch = match context.peers().validators().read().get(&claims.chain_id) {
        Some(info) => info.0,
        None => {
            return Err(reject::custom(ServiceError::unauthorized(
                JwtAuthError::ExpiredAuthToken.into(),
            )));
        },
    };

    if !allow_roles.contains(&claims.node_type) {
        return Err(reject::custom(ServiceError::forbidden(
            JwtAuthError::AccessDenied.into(),
        )));
    }

    if claims.epoch == current_epoch && claims.exp > Utc::now().timestamp() as usize {
        Ok(claims)
    } else {
        Err(reject::custom(ServiceError::unauthorized(
            JwtAuthError::ExpiredAuthToken.into(),
        )))
    }
}
```

**File:** crates/aptos-telemetry-service/src/lib.rs (L179-181)
```rust
        let validators = Arc::new(aptos_infallible::RwLock::new(HashMap::new()));
        let validator_fullnodes = Arc::new(aptos_infallible::RwLock::new(HashMap::new()));
        let peer_locations = Arc::new(aptos_infallible::RwLock::new(HashMap::new()));
```

**File:** crates/aptos-telemetry-service/src/validator_cache.rs (L51-59)
```rust
    pub fn run(self) {
        let mut interval = time::interval(self.update_interval);
        tokio::spawn(async move {
            loop {
                self.update().await;
                interval.tick().await;
            }
        });
    }
```

**File:** crates/aptos-telemetry-service/src/prometheus_push_metrics.rs (L40-46)
```rust
pub async fn handle_metrics_ingest(
    context: Context,
    claims: Claims,
    encoding: Option<String>,
    metrics_body: Bytes,
) -> anyhow::Result<impl Reply, Rejection> {
    debug!("handling prometheus metrics ingest");
```

**File:** crates/aptos-telemetry-service/src/custom_event.rs (L67-73)
```rust
pub(crate) async fn handle_custom_event(
    context: Context,
    claims: Claims,
    mut body: TelemetryDump,
    forwarded_for: Option<String>,
) -> anyhow::Result<impl Reply, Rejection> {
    validate_custom_event_body(&claims, &body)?;
```
