# Audit Report

## Title
Critical Node Crash and Message Loss in Remote Sharded Executor Due to Missing GRPC Retry Logic

## Summary
The `send_message()` function in the secure network layer crashes the entire node process with a panic when GRPC connections fail, instead of retrying. This results in permanent message loss, executor shard crashes, and consensus failures in the sharded block execution system.

## Finding Description

The vulnerability exists in the GRPC client implementation used by Aptos's remote sharded block executor infrastructure. When a GRPC connection fails during message transmission, the code panics and crashes the entire process. [1](#0-0) 

The code explicitly acknowledges the need for retry logic via a TODO comment, but implements a panic instead. This affects critical block execution messages sent between the coordinator and executor shards.

**Attack Flow:**

1. Remote executor shard receives block execution command from coordinator [2](#0-1) 

2. Shard executes the block and attempts to send results back via the network controller [3](#0-2) 

3. If the GRPC connection fails (network disruption, timeout, connection reset), the `send_message()` function panics, crashing the entire shard process

4. The coordinator hangs indefinitely waiting for results that will never arrive [4](#0-3) 

5. Block execution fails, breaking consensus and causing liveness failure

This breaks multiple critical invariants:
- **Deterministic Execution**: Block execution cannot complete when shards crash
- **Consensus Safety**: Coordinator cannot aggregate results from all shards
- **State Consistency**: Partial execution results cannot be committed

The vulnerability is integrated into the production execution path: [5](#0-4) 

## Impact Explanation

This vulnerability qualifies as **Critical Severity** under Aptos Bug Bounty criteria:

1. **Total loss of liveness/network availability**: When remote executor shards are configured, any network disruption causes shard crashes and indefinite coordinator hangs, halting block execution entirely.

2. **Consensus/Safety violations**: The coordinator cannot aggregate execution results from crashed shards, preventing block commitment and breaking consensus.

3. **Permanent message loss**: Execution results containing `TransactionOutput` data are lost permanently with no retry mechanism, violating state consistency guarantees.

An attacker can exploit this by:
- Triggering network disruptions between coordinator and executor shards
- Causing connection timeouts through packet manipulation
- Exploiting network congestion to induce GRPC failures

Each triggered failure crashes an executor shard, and the coordinator hangs indefinitely, requiring manual intervention and potentially a network restart.

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability occurs in production scenarios where remote sharded execution is enabled:

1. Network failures are common in distributed systems (packet loss, connection resets, timeouts)
2. The code uses `connect_lazy()` which defers connection establishment until first use [6](#0-5) 

3. No retry logic exists despite explicit acknowledgment of the need (TODO comment)
4. The panic is unconditional - any GRPC error triggers it
5. Malicious actors can deliberately induce network disruptions

The vulnerability is triggered automatically whenever:
- Network connectivity issues occur between nodes
- GRPC service is temporarily unavailable
- Connection timeouts happen during message transmission
- Remote server experiences temporary failures

## Recommendation

**Immediate Fix**: Implement retry logic with exponential backoff as acknowledged in the TODO comment:

```rust
pub async fn send_message(
    &mut self,
    sender_addr: SocketAddr,
    message: Message,
    mt: &MessageType,
) {
    let request = tonic::Request::new(NetworkMessage {
        message: message.data,
        message_type: mt.get_type(),
    });
    
    // Implement retry with exponential backoff
    let mut retry_count = 0;
    const MAX_RETRIES: u32 = 5;
    const INITIAL_BACKOFF_MS: u64 = 100;
    
    loop {
        match self.remote_channel.simple_msg_exchange(request.clone()).await {
            Ok(_) => return,
            Err(e) => {
                if retry_count >= MAX_RETRIES {
                    error!(
                        "Failed to send message after {} retries: '{}' to {} on node {:?}",
                        MAX_RETRIES, e, self.remote_addr, sender_addr
                    );
                    // Return error instead of panic to allow graceful handling
                    return;
                }
                
                let backoff = INITIAL_BACKOFF_MS * 2u64.pow(retry_count);
                warn!(
                    "Retry {}/{} after {}ms for message to {} on node {:?}: {}",
                    retry_count + 1, MAX_RETRIES, backoff, self.remote_addr, sender_addr, e
                );
                
                tokio::time::sleep(tokio::time::Duration::from_millis(backoff)).await;
                retry_count += 1;
            }
        }
    }
}
```

**Additional Recommendations**:
1. Change function signature to return `Result<(), Error>` for proper error propagation
2. Implement circuit breaker pattern for persistent failures
3. Add connection health checks before sending critical messages
4. Implement message queuing for transient failures
5. Add monitoring/alerting for GRPC connection failures

## Proof of Concept

**Reproduction Steps**:

1. Set up remote sharded executor environment with coordinator and multiple executor shards
2. Configure network addresses for remote execution
3. Start block execution
4. During execution, introduce network disruption (firewall rule, connection drop, or network partition) between coordinator and one shard
5. Observe shard panic and crash when attempting to send execution results
6. Observe coordinator hang indefinitely waiting for results from crashed shard

**Rust Test Reproduction**:

```rust
#[tokio::test]
async fn test_grpc_failure_causes_panic() {
    use aptos_config::utils;
    use std::net::{IpAddr, Ipv4Addr, SocketAddr};
    
    // Create client pointing to non-existent server
    let non_existent_addr = SocketAddr::new(
        IpAddr::V4(Ipv4Addr::LOCALHOST), 
        utils::get_available_port()
    );
    
    let rt = tokio::runtime::Runtime::new().unwrap();
    let mut client = GRPCNetworkMessageServiceClientWrapper::new(
        &rt, 
        non_existent_addr
    );
    
    let sender_addr = SocketAddr::new(
        IpAddr::V4(Ipv4Addr::LOCALHOST), 
        utils::get_available_port()
    );
    
    // This will panic when GRPC connection fails
    // Expected: Should retry and eventually return error
    // Actual: Panics and crashes
    client.send_message(
        sender_addr,
        Message::new(vec![1, 2, 3]),
        &MessageType::new("test".to_string())
    ).await;
    
    // Test should never reach here due to panic
    assert!(false, "Should have panicked");
}
```

**Notes**

This vulnerability is particularly severe because:

1. It affects the **consensus-critical execution path** when remote sharded execution is enabled
2. The panic is **unconditional** - any GRPC error triggers a crash
3. Messages containing **critical execution results** are lost permanently
4. The coordinator has **no timeout mechanism** and hangs indefinitely
5. The developers **acknowledged the issue** with a TODO comment but never implemented the fix
6. This enables **denial-of-service attacks** against validator nodes using remote execution

The vulnerability exists in production code paths and can be triggered by realistic network conditions or deliberately by attackers to disrupt consensus and cause validator outages.

### Citations

**File:** secure/net/src/grpc_network_service/mod.rs (L132-138)
```rust
    async fn get_channel(remote_addr: String) -> NetworkMessageServiceClient<Channel> {
        info!("Trying to connect to remote server at {:?}", remote_addr);
        let conn = tonic::transport::Endpoint::new(remote_addr)
            .unwrap()
            .connect_lazy();
        NetworkMessageServiceClient::new(conn).max_decoding_message_size(MAX_MESSAGE_SIZE)
    }
```

**File:** secure/net/src/grpc_network_service/mod.rs (L140-160)
```rust
    pub async fn send_message(
        &mut self,
        sender_addr: SocketAddr,
        message: Message,
        mt: &MessageType,
    ) {
        let request = tonic::Request::new(NetworkMessage {
            message: message.data,
            message_type: mt.get_type(),
        });
        // TODO: Retry with exponential backoff on failures
        match self.remote_channel.simple_msg_exchange(request).await {
            Ok(_) => {},
            Err(e) => {
                panic!(
                    "Error '{}' sending message to {} on node {:?}",
                    e, self.remote_addr, sender_addr
                );
            },
        }
    }
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L115-119)
```rust
    fn send_execution_result(&self, result: Result<Vec<Vec<TransactionOutput>>, VMStatus>) {
        let remote_execution_result = RemoteExecutionResult::new(result);
        let output_message = bcs::to_bytes(&remote_execution_result).unwrap();
        self.result_tx.send(Message::new(output_message)).unwrap();
    }
```

**File:** secure/net/src/network_controller/outbound_handler.rs (L155-159)
```rust
                grpc_clients
                    .get_mut(remote_addr)
                    .unwrap()
                    .send_message(*socket_addr, msg, message_type)
                    .await;
```

**File:** execution/executor-service/src/remote_executor_client.rs (L163-172)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        trace!("RemoteExecutorClient Waiting for results");
        let mut results = vec![];
        for rx in self.result_rxs.iter() {
            let received_bytes = rx.recv().unwrap().to_bytes();
            let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
            results.push(result.inner?);
        }
        Ok(results)
    }
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L256-276)
```rust
    fn execute_block_sharded<V: VMBlockExecutor>(
        partitioned_txns: PartitionedTransactions,
        state_view: Arc<CachedStateView>,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<Vec<TransactionOutput>> {
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        } else {
            Ok(V::execute_block_sharded(
                &SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        }
    }
```
