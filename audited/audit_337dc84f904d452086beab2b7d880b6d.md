# Audit Report

## Title
Unbounded Staging Area Growth Enables Unrecoverable State and Storage Exhaustion

## Summary
The `large_packages` module lacks size limits on staging areas, allowing users to accumulate unbounded metadata and code chunks across multiple transactions. This creates staging areas that exceed transaction gas limits during finalization or cleanup, resulting in permanently unrecoverable storage waste. While each user's staging area is isolated, the lack of limits enables storage exhaustion attacks.

## Finding Description

The `stage_code_chunk_internal` function in `large_packages.move` accumulates package data without any size validation: [1](#0-0) [2](#0-1) 

There are no checks on:
- Total size of `metadata_serialized` vector
- Total size or count of entries in the `code` SmartTable  
- Number of times `stage_code_chunk` can be called
- Time-based expiration of staging areas

Each call to `stage_code_chunk` is a separate transaction subject only to per-transaction limits (64 KB payload, 8192 write ops, 10 MB write size per transaction). However, a user can make unlimited transactions, each adding more data.

When attempting to finalize via `stage_code_chunk_and_publish_to_account`, the `assemble_module_code` function must read all accumulated chunks: [3](#0-2) 

This happens in a SINGLE transaction subject to gas limits: [4](#0-3) 

**Attack Scenario:**

1. Attacker calls `stage_code_chunk` 10,000 times, each with maximum allowed payload
2. Staging area accumulates gigabytes of metadata and code chunks
3. When finalization is attempted, reading all SmartTable entries exceeds `max_io_gas` (1,000,000,000 gas units)
4. Transaction aborts with `OUT_OF_GAS` error
5. Cleanup also fails because it requires reading the SmartTable to destroy it: [5](#0-4) [6](#0-5) 

The staging area becomes **permanently unrecoverable**, wasting on-chain storage forever.

**Regarding "prevent other users from deploying":**

Each user has their own `StagingArea` resource stored at their account address. One user filling their staging area does NOT directly prevent other users from deploying. However:

1. **Storage pressure attack**: An attacker can create many accounts and fill each with oversized staging areas, increasing global storage utilization toward the 2 billion item / 1 TB targets defined in storage gas parameters: [7](#0-6) 

2. As storage utilization increases, the exponential gas curve makes storage increasingly expensive for all users, potentially making package deployment prohibitively expensive network-wide.

## Impact Explanation

**Medium to High Severity:**

1. **Unrecoverable State** (Medium): Users can accidentally or maliciously create staging areas that cannot be finalized or cleaned up, permanently wasting storage. This violates the "Resource Limits: All operations must respect gas, storage, and computational limits" invariant.

2. **Storage Exhaustion Vector** (High): An attacker with sufficient capital can create many accounts and fill staging areas to increase global storage utilization. The exponential pricing curve means storage costs increase dramatically as the network approaches capacity, effectively pricing out legitimate users from deploying packages.

3. **Permanent Storage Leak**: No mechanism exists to recover oversized staging areas, requiring potential governance intervention or protocol upgrade.

## Likelihood Explanation

**High Likelihood:**

1. **Accidental occurrence**: Legitimate users may unknowingly create oversized staging areas by staging packages in many small chunks without understanding transaction limits
2. **Low attack cost**: Each staging transaction costs only normal gas fees; the attack compounds across many transactions
3. **No warnings**: The protocol provides no indication that a staging area is approaching dangerous sizes
4. **No automatic cleanup**: Staging areas persist indefinitely without expiration

## Recommendation

Add size limits and validation to prevent unbounded growth:

**In `stage_code_chunk_internal`:**
```move
// Add constants for limits
const MAX_STAGING_AREA_SIZE_BYTES: u64 = 50_000_000; // 50 MB
const MAX_CODE_ENTRIES: u64 = 1000; // Max module chunks
const MAX_METADATA_SIZE_BYTES: u64 = 10_000_000; // 10 MB

inline fun stage_code_chunk_internal(...) {
    // ... existing code ...
    
    let staging_area = borrow_global_mut<StagingArea>(owner_address);
    
    // Check metadata size before appending
    if (!vector::is_empty(&metadata_chunk)) {
        let new_size = vector::length(&staging_area.metadata_serialized) + vector::length(&metadata_chunk);
        assert!(new_size <= MAX_METADATA_SIZE_BYTES, error::invalid_argument(ESTAGING_AREA_TOO_LARGE));
        vector::append(&mut staging_area.metadata_serialized, metadata_chunk);
    };
    
    // Check code entry count
    assert!(
        smart_table::length(&staging_area.code) < MAX_CODE_ENTRIES,
        error::invalid_argument(ETOO_MANY_CODE_ENTRIES)
    );
    
    // ... rest of function ...
}
```

**Additionally:**
1. Add a total size tracking field to `StagingArea` struct
2. Implement time-based expiration (e.g., staging areas expire after 1 week)
3. Add a force cleanup function callable by validators/governance for stuck staging areas
4. Emit warnings in SDK when staging area approaches size limits

## Proof of Concept

```move
#[test(user = @0x123)]
fun test_unbounded_staging_area_dos(user: signer) {
    use aptos_experimental::large_packages;
    
    // Stage data in many small chunks
    let i = 0;
    while (i < 5000) {
        // Each chunk is small enough to fit in transaction limits
        let metadata_chunk = vector::empty();
        let code_indices = vector[0];
        let code_chunks = vector[vector::from_slice(b"x", 0, 50000)]; // 50 KB per chunk
        
        large_packages::stage_code_chunk(
            &user,
            metadata_chunk,
            code_indices, 
            code_chunks
        );
        i = i + 1;
    };
    
    // Now staging area contains 5000 * 50KB = 250 MB of data
    // Attempting to finalize will exceed max_io_gas when reading all SmartTable entries
    
    large_packages::stage_code_chunk_and_publish_to_account(
        &user,
        vector::empty(),
        vector[],
        vector[]
    ); // ABORTS with OUT_OF_GAS
    
    // Cleanup also fails
    large_packages::cleanup_staging_area(&user); // ABORTS with OUT_OF_GAS
    
    // Staging area is permanently stuck
}
```

## Notes

The vulnerability is confirmed in the production codebase. While each user's staging area is isolated (preventing direct DoS of other users), the lack of size limits enables:
1. Unrecoverable storage waste when users create oversized staging areas
2. Network-wide storage pressure through coordinated attacks
3. Violation of resource limit invariants

This meets **High Severity** criteria due to the permanent storage impact and potential for network-wide deployment cost inflation through storage exhaustion.

### Citations

**File:** aptos-move/framework/aptos-experimental/sources/large_packages.move (L158-160)
```text
        if (!vector::is_empty(&metadata_chunk)) {
            vector::append(&mut staging_area.metadata_serialized, metadata_chunk);
        };
```

**File:** aptos-move/framework/aptos-experimental/sources/large_packages.move (L167-176)
```text
            if (smart_table::contains(&staging_area.code, idx)) {
                vector::append(
                    smart_table::borrow_mut(&mut staging_area.code, idx), inner_code
                );
            } else {
                smart_table::add(&mut staging_area.code, idx, inner_code);
                if (idx > staging_area.last_module_idx) {
                    staging_area.last_module_idx = idx;
                }
            };
```

**File:** aptos-move/framework/aptos-experimental/sources/large_packages.move (L213-225)
```text
    inline fun assemble_module_code(staging_area: &mut StagingArea): vector<vector<u8>> {
        let last_module_idx = staging_area.last_module_idx;
        let code = vector[];
        let i = 0;
        while (i <= last_module_idx) {
            vector::push_back(
                &mut code,
                *smart_table::borrow(&staging_area.code, i)
            );
            i = i + 1;
        };
        code
    }
```

**File:** aptos-move/framework/aptos-experimental/sources/large_packages.move (L227-231)
```text
    public entry fun cleanup_staging_area(owner: &signer) acquires StagingArea {
        let StagingArea { metadata_serialized: _, code, last_module_idx: _ } =
            move_from<StagingArea>(signer::address_of(owner));
        smart_table::destroy(code);
    }
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L211-224)
```rust
            max_execution_gas: InternalGas,
            { 7.. => "max_execution_gas" },
            920_000_000, // 92ms of execution at 10k gas per ms
        ],
        [
            max_execution_gas_gov: InternalGas,
            { RELEASE_V1_13.. => "max_execution_gas.gov" },
            4_000_000_000,
        ],
        [
            max_io_gas: InternalGas,
            { 7.. => "max_io_gas" },
            1_000_000_000, // 100ms of IO at 10k gas per ms
        ],
```

**File:** aptos-move/framework/aptos-stdlib/sources/data_structures/smart_table.move (L117-125)
```text
    public fun clear<K: drop, V: drop>(self: &mut SmartTable<K, V>) {
        *self.buckets.borrow_mut(0) = vector::empty();
        for (i in 1..self.num_buckets) {
            self.buckets.remove(i);
        };
        self.num_buckets = 1;
        self.level = 0;
        self.size = 0;
    }
```

**File:** aptos-move/framework/aptos-framework/sources/storage_gas.move (L399-410)
```text
        let item_config = UsageGasConfig {
            target_usage: 2 * k * m, // 2 billion
            read_curve: base_8192_exponential_curve(300 * k, 300 * k * 100),
            create_curve: base_8192_exponential_curve(300 * k, 300 * k * 100),
            write_curve: base_8192_exponential_curve(300 * k, 300 * k * 100),
        };
        let byte_config = UsageGasConfig {
            target_usage: 1 * m * m, // 1TB
            read_curve: base_8192_exponential_curve(300, 300 * 100),
            create_curve: base_8192_exponential_curve(5 * k,  5 * k * 100),
            write_curve: base_8192_exponential_curve(5 * k,  5 * k * 100),
        };
```
