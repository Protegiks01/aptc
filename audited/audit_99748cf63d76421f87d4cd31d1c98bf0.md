# Audit Report

## Title
Request Moderator Bypass for Epoch Ending Ledger Info Requests via Optimistic Fetch Amplification

## Summary
The request moderator DOES apply to epoch ending ledger info requests, but only validates request serviceabilityâ€”not rate limits. Attackers can flood storage services with valid epoch ending ledger info requests by exploiting the optimistic fetch mechanism, causing resource exhaustion and validator node degradation.

## Finding Description
The security guarantee broken is **Resource Limits** (Invariant #9): all operations must respect computational and I/O limits.

When an optimistic fetch request is processed and the peer's `known_epoch` is behind the current epoch, the system internally fetches epoch ending ledger info [1](#0-0) . This internal request is created as a `GetEpochEndingLedgerInfos` storage request [2](#0-1)  and processed through the handler [3](#0-2) .

While the request goes through `validate_and_handle_request` which calls the moderator [4](#0-3) , the moderator only validates two conditions: (1) whether the peer is currently ignored due to excessive invalid requests, and (2) whether the request can be serviced [5](#0-4) .

For epoch ending ledger info requests, serviceability is determined solely by whether the requested epoch range exists in storage [6](#0-5) . No per-request-type rate limiting exists.

**Attack Path:**
1. Attacker establishes multiple peer connections (bypassing the one-optimistic-fetch-per-peer limit)
2. Each peer sends an optimistic fetch request with `known_epoch` set to an old but valid epoch value
3. Optimistic fetch requests bypass initial moderation and are stored directly [7](#0-6) 
4. When new data becomes available, all ready optimistic fetches are processed concurrently, spawning blocking tasks for each peer [8](#0-7) 
5. Each task triggers an internal epoch ending ledger info fetch from storage
6. All valid requests are processed concurrently, causing excessive disk I/O and CPU usage
7. Legitimate requests experience degraded performance or timeouts

## Impact Explanation
This vulnerability meets **High Severity** criteria per the Aptos bug bounty program: "Validator node slowdowns" and "API crashes" due to resource exhaustion. 

With the default configuration allowing only one active optimistic fetch per peer [9](#0-8) , an attacker with N peer connections can trigger N concurrent storage reads. The moderator blocks peers only after 500 invalid requests [10](#0-9) , but valid epoch ending ledger info requests are unlimited.

This causes:
- Thread pool exhaustion from concurrent blocking tasks
- Storage I/O saturation from reading epoch ending ledger infos
- Degraded service for legitimate peers
- Potential cascade failures if storage timeouts occur

## Likelihood Explanation
**Likelihood: High**

The attack is trivial to execute:
- No validator privileges required
- No complex cryptographic operations needed
- Sybil peer connections are easy to create
- Old epoch data typically remains available in storage
- No existing rate limiting prevents the attack

The only constraint is network connectivity and the ability to maintain multiple peer connections, which is feasible for any motivated attacker.

## Recommendation
Implement per-request-type rate limiting in the RequestModerator to complement the existing validity checks:

```rust
// In moderator.rs, track request counts per peer per request type
pub struct RequestModerator {
    // ... existing fields ...
    request_counts: Arc<DashMap<(PeerNetworkId, RequestType), RateLimitState>>,
}

struct RateLimitState {
    count: u64,
    window_start: Instant,
    max_requests_per_window: u64,
}

impl RequestModerator {
    pub fn validate_request(&self, ...) -> Result<(), Error> {
        // Existing validation...
        
        // Add rate limiting check
        if self.should_rate_limit_request(peer_network_id, request)? {
            return Err(Error::TooManyRequests(...));
        }
        
        // ... rest of validation
    }
    
    fn should_rate_limit_request(&self, ...) -> Result<bool, Error> {
        // Implement sliding window rate limiting for epoch ending ledger info requests
        // and other expensive operations initiated by optimistic fetches
    }
}
```

Additionally, limit the number of concurrent optimistic fetch processing tasks to prevent thread pool exhaustion.

## Proof of Concept

```rust
// Integration test demonstrating the vulnerability
#[tokio::test]
async fn test_epoch_ending_ledger_info_flood() {
    // Setup: Create storage service with test data for epochs 0-10
    let (mut mock_client, service, _, _, _) = MockClient::new(None, None);
    let storage = service.get_storage();
    
    // Populate storage with epoch ending ledger infos for epochs 0-10
    for epoch in 0..=10 {
        storage.save_epoch_ending_ledger_info(epoch, create_test_ledger_info(epoch));
    }
    
    tokio::spawn(service.start());
    
    // Attack: Create 1000 peer connections, each with optimistic fetch at old epoch
    let mut handles = vec![];
    for i in 0..1000 {
        let peer_id = PeerId::random();
        let peer_network_id = PeerNetworkId::new(NetworkId::Public, peer_id);
        
        // Send optimistic fetch with known_epoch=0 (old but valid)
        let request = StorageServiceRequest::new(
            DataRequest::GetNewTransactionsWithProof(
                NewTransactionsWithProofRequest {
                    known_version: 0,
                    known_epoch: 0,  // Triggers epoch ending ledger info fetch
                    include_events: false,
                }
            ),
            false,
        );
        
        let handle = tokio::spawn(async move {
            mock_client.send_optimistic_fetch_request(peer_network_id, request).await
        });
        handles.push(handle);
    }
    
    // Trigger processing by updating storage to epoch 11
    storage.save_epoch_ending_ledger_info(11, create_test_ledger_info(11));
    
    // Observe: 1000 concurrent epoch ending ledger info fetches overwhelm storage
    // Expected result: Service degradation, increased latency, potential crashes
    
    // Measure impact on legitimate requests
    let legitimate_start = Instant::now();
    let legitimate_request = create_legitimate_request();
    let response = mock_client.send_request(legitimate_request).await;
    let legitimate_latency = legitimate_start.elapsed();
    
    // Assert: Legitimate request experiences significant delay or timeout
    assert!(legitimate_latency > Duration::from_secs(10) || response.is_err());
}
```

## Notes

The vulnerability exists because request moderation was designed to prevent invalid requests (requests that cannot be serviced), but does not implement rate limiting for valid but expensive operations. While optimistic fetches themselves are limited to one per peer, the internal epoch ending ledger info fetches they trigger are not subject to the same constraint when processed concurrently across multiple peers.

This represents a gap in the defense-in-depth model where the moderator's validation logic is insufficient to prevent resource exhaustion attacks using valid but amplified internal requests.

### Citations

**File:** state-sync/storage-service/server/src/optimistic_fetch.rs (L257-260)
```rust
/// active map and notifying the peer of the new data.
pub(crate) async fn handle_ready_optimistic_fetches<T: StorageReaderInterface>(
    runtime: Handle,
    cached_storage_server_summary: Arc<ArcSwap<StorageServerSummary>>,
```

**File:** state-sync/storage-service/server/src/optimistic_fetch.rs (L500-548)
```rust
        let active_task = runtime.spawn_blocking(move || {
            // Check if we have synced beyond the highest known version
            if highest_known_version < highest_synced_version {
                if highest_known_epoch < highest_synced_epoch {
                    // Fetch the epoch ending ledger info from storage (the
                    // peer needs to sync to their epoch ending ledger info).
                    let epoch_ending_ledger_info = match utils::get_epoch_ending_ledger_info(
                        cached_storage_server_summary.clone(),
                        optimistic_fetches.clone(),
                        subscriptions.clone(),
                        highest_known_epoch,
                        lru_response_cache.clone(),
                        request_moderator.clone(),
                        &peer_network_id,
                        storage.clone(),
                        time_service.clone(),
                    ) {
                        Ok(epoch_ending_ledger_info) => epoch_ending_ledger_info,
                        Err(error) => {
                            // Log the failure to fetch the epoch ending ledger info
                            error!(LogSchema::new(LogEntry::OptimisticFetchRefresh)
                                .error(&error)
                                .message(&format!(
                                    "Failed to get the epoch ending ledger info for epoch: {:?} !",
                                    highest_known_epoch
                                )));

                            return;
                        },
                    };

                    // Check that we haven't been sent an invalid optimistic fetch request
                    // (i.e., a request that does not respect an epoch boundary).
                    if epoch_ending_ledger_info.ledger_info().version() <= highest_known_version {
                        peers_with_invalid_optimistic_fetches
                            .lock()
                            .push(peer_network_id);
                    } else {
                        peers_with_ready_optimistic_fetches
                            .lock()
                            .push((peer_network_id, epoch_ending_ledger_info));
                    }
                } else {
                    peers_with_ready_optimistic_fetches
                        .lock()
                        .push((peer_network_id, highest_synced_ledger_info.clone()));
                };
            }
        });
```

**File:** state-sync/storage-service/server/src/utils.rs (L39-46)
```rust
    let data_request = DataRequest::GetEpochEndingLedgerInfos(EpochEndingLedgerInfoRequest {
        start_epoch: epoch,
        expected_end_epoch: epoch,
    });
    let storage_request = StorageServiceRequest::new(
        data_request,
        false, // Don't compress because this isn't going over the wire
    );
```

**File:** state-sync/storage-service/server/src/utils.rs (L58-58)
```rust
    let storage_response = handler.process_request(peer_network_id, storage_request, true);
```

**File:** state-sync/storage-service/server/src/handler.rs (L120-123)
```rust
        if request.data_request.is_optimistic_fetch() {
            self.handle_optimistic_fetch_request(peer_network_id, request, response_sender);
            return;
        }
```

**File:** state-sync/storage-service/server/src/handler.rs (L212-213)
```rust
        self.request_moderator
            .validate_request(peer_network_id, request)?;
```

**File:** state-sync/storage-service/server/src/moderator.rs (L142-185)
```rust
            if let Some(peer_state) = self.unhealthy_peer_states.get(peer_network_id) {
                if peer_state.is_ignored() {
                    return Err(Error::TooManyInvalidRequests(format!(
                        "Peer is temporarily ignored. Unable to handle request: {:?}",
                        request
                    )));
                }
            }

            // Get the latest storage server summary
            let storage_server_summary = self.cached_storage_server_summary.load();

            // Verify the request is serviceable using the current storage server summary
            if !storage_server_summary.can_service(
                &self.aptos_data_client_config,
                self.time_service.clone(),
                request,
            ) {
                // Increment the invalid request count for the peer
                let mut unhealthy_peer_state = self
                    .unhealthy_peer_states
                    .entry(*peer_network_id)
                    .or_insert_with(|| {
                        // Create a new unhealthy peer state (this is the first invalid request)
                        let max_invalid_requests =
                            self.storage_service_config.max_invalid_requests_per_peer;
                        let min_time_to_ignore_peers_secs =
                            self.storage_service_config.min_time_to_ignore_peers_secs;
                        let time_service = self.time_service.clone();

                        UnhealthyPeerState::new(
                            max_invalid_requests,
                            min_time_to_ignore_peers_secs,
                            time_service,
                        )
                    });
                unhealthy_peer_state.increment_invalid_request_count(peer_network_id);

                // Return the validation error
                return Err(Error::InvalidRequest(format!(
                    "The given request cannot be satisfied. Request: {:?}, storage summary: {:?}",
                    request, storage_server_summary
                )));
            }
```

**File:** state-sync/storage-service/types/src/responses.rs (L698-707)
```rust
            GetEpochEndingLedgerInfos(request) => {
                let desired_range =
                    match CompleteDataRange::new(request.start_epoch, request.expected_end_epoch) {
                        Ok(desired_range) => desired_range,
                        Err(_) => return false,
                    };
                self.epoch_ending_ledger_infos
                    .map(|range| range.superset_of(&desired_range))
                    .unwrap_or(false)
            },
```

**File:** config/src/config/state_sync_config.rs (L201-201)
```rust
            max_invalid_requests_per_peer: 500,
```
