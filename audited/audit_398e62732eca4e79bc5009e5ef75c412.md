# Audit Report

## Title
Memory Exhaustion via Maliciously Bloated Epoch States in Consensus Votes

## Summary
Byzantine validators can craft consensus votes containing artificially large `next_epoch_state` fields in `BlockInfo` structures, causing excessive memory allocation during BCS deserialization. These bloated votes pass signature verification and are stored in memory, enabling coordinated memory exhaustion attacks by up to f Byzantine validators per round.

## Finding Description

The vulnerability exists in how consensus votes are validated and stored. The `Vote` structure contains `VoteData` and `LedgerInfo`, both of which embed `BlockInfo` structures that can contain an optional `next_epoch_state: Option<EpochState>`. [1](#0-0) 

Each `EpochState` contains a `ValidatorVerifier` with a vector of validator information. [2](#0-1) 

The on-chain validator set size limit is `MAX_VALIDATOR_SET_SIZE = 65536`. [3](#0-2) 

**Attack Path:**

1. A Byzantine validator crafts a `Vote` where:
   - `VoteData.proposed` contains a `BlockInfo` with `next_epoch_state` populated with 65,536 validators
   - `VoteData.parent` contains a `BlockInfo` with `next_epoch_state` populated with 65,536 validators  
   - `LedgerInfo.commit_info` contains a `BlockInfo` with `next_epoch_state` populated with 65,536 validators

2. Each `ValidatorConsensusInfo` is approximately 136 bytes (32-byte address + 96-byte BLS public key + 8-byte voting power), resulting in ~8.9 MB per `ValidatorVerifier`.

3. Total vote size: 3 × 8.9 MB ≈ 26.7 MB, which is under the network's `MAX_MESSAGE_SIZE` of 64 MiB. [4](#0-3) 

4. The network layer accepts and deserializes the vote using BCS with `RECURSION_LIMIT = 64`. [5](#0-4) 

5. BCS deserialization allocates memory for all 65,536 validators in each of the three `BlockInfo` structures **before** validation occurs.

6. `Vote.verify()` only validates:
   - Consensus data hash matches vote data hash
   - Signature is valid using the **current** epoch's `ValidatorVerifier`
   - Vote data is well-formed (epoch matching, round ordering) [6](#0-5) 

7. **Critically, `Vote.verify()` does NOT validate:**
   - Whether the `next_epoch_state` in any `BlockInfo` matches expected values
   - Whether the validator set size is reasonable
   - Whether `next_epoch_state` should even be present for non-reconfiguration blocks

8. The bloated vote passes verification and is stored via `vote.clone()` in `PendingVotes.author_to_vote`. [7](#0-6) 

9. Each Byzantine validator (up to f < n/3) can send one such bloated vote per round, amplifying the attack:
   - With 100 validators: 33 Byzantine × 27 MB ≈ 891 MB per round
   - With multiple concurrent rounds during sync or delays: multi-GB memory consumption

10. Votes are only drained when advancing to a new round. [8](#0-7) 

## Impact Explanation

**Severity: Medium**

This vulnerability falls under the Medium severity category ($10,000) for "State inconsistencies requiring intervention" and "Validator node slowdowns" per the Aptos bug bounty program.

**Impact:**
- **Memory Exhaustion**: Byzantine validators can force honest validators to allocate hundreds of megabytes to gigabytes of memory for malicious votes
- **Performance Degradation**: Excessive memory usage causes swapping, garbage collection pressure, and CPU overhead
- **DoS Potential**: In extreme cases, can trigger OOM kills on memory-constrained validators
- **Liveness Degradation**: Honest validators struggling with resource exhaustion may timeout and fail to participate effectively in consensus

The attack does NOT directly break consensus safety (no chain splits or double-spending), but significantly impacts performance and availability, which are critical for a high-throughput blockchain.

## Likelihood Explanation

**Likelihood: Medium to High**

- **Attacker Requirements**: Requires Byzantine validator access (up to f < n/3 of stake)
- **Complexity**: Low - straightforward to craft malicious votes with bloated epoch states
- **Detectability**: Difficult to detect without explicit size monitoring, as votes pass cryptographic verification
- **Frequency**: Can be executed continuously every round

The AptosBFT protocol is designed to tolerate up to f Byzantine validators, making this assumption realistic. Any compromised validator or set of colluding validators can execute this attack without detection by the existing validation logic.

## Recommendation

Add explicit validation of `BlockInfo` content during vote verification:

1. **Add size limits to Vote verification** in `consensus/consensus-types/src/vote.rs`:

```rust
pub fn verify(&self, validator: &ValidatorVerifier) -> anyhow::Result<()> {
    // Existing checks...
    ensure!(
        self.ledger_info.consensus_data_hash() == self.vote_data.hash(),
        "Vote's hash mismatch with LedgerInfo"
    );
    
    // NEW: Validate BlockInfo sizes
    self.vote_data.proposed().validate_epoch_state_size()?;
    self.vote_data.parent().validate_epoch_state_size()?;
    self.ledger_info.commit_info().validate_epoch_state_size()?;
    
    // Rest of existing verification...
}
```

2. **Add validation method to BlockInfo** in `types/src/block_info.rs`:

```rust
impl BlockInfo {
    /// Maximum reasonable validator set size for validation
    const MAX_REASONABLE_VALIDATOR_SET_SIZE: usize = 1000;
    
    pub fn validate_epoch_state_size(&self) -> anyhow::Result<()> {
        if let Some(epoch_state) = &self.next_epoch_state {
            let validator_count = epoch_state.verifier.len();
            ensure!(
                validator_count <= Self::MAX_REASONABLE_VALIDATOR_SET_SIZE,
                "Validator set size {} exceeds reasonable limit {}",
                validator_count,
                Self::MAX_REASONABLE_VALIDATOR_SET_SIZE
            );
        }
        Ok(())
    }
}
```

3. **Alternative: Add network-level size checks** before deserialization to reject oversized consensus messages early.

## Proof of Concept

```rust
#[cfg(test)]
mod test_vote_memory_exhaustion {
    use super::*;
    use aptos_types::{
        block_info::BlockInfo,
        validator_verifier::{ValidatorConsensusInfo, ValidatorVerifier},
        ledger_info::LedgerInfo,
        epoch_state::EpochState,
    };
    use aptos_crypto::bls12381;
    
    #[test]
    fn test_bloated_vote_memory_consumption() {
        // Create a ValidatorVerifier with maximum validators
        let mut validator_infos = Vec::new();
        for i in 0..65536 {
            let addr = AccountAddress::from_hex_literal(&format!("0x{:x}", i)).unwrap();
            let public_key = bls12381::PublicKey::dummy_key();
            validator_infos.push(ValidatorConsensusInfo::new(addr, public_key, 1));
        }
        let huge_verifier = ValidatorVerifier::new(validator_infos);
        let huge_epoch_state = EpochState::new(1, huge_verifier);
        
        // Create BlockInfo with bloated epoch state
        let bloated_block_info = BlockInfo::new(
            1, // epoch
            10, // round
            HashValue::zero(),
            HashValue::zero(),
            0, // version
            0, // timestamp
            Some(huge_epoch_state), // This is the bloated part
        );
        
        // Create VoteData with bloated BlockInfo
        let vote_data = VoteData::new(
            bloated_block_info.clone(),
            bloated_block_info.clone(),
        );
        
        // Create LedgerInfo with bloated BlockInfo
        let ledger_info = LedgerInfo::new(
            bloated_block_info,
            vote_data.hash(),
        );
        
        // Create and sign the vote
        let validator_signer = ValidatorSigner::random([0u8; 32]);
        let vote = Vote::new(
            vote_data,
            validator_signer.author(),
            ledger_info,
            &validator_signer,
        ).unwrap();
        
        // Serialize the vote
        let serialized = bcs::to_bytes(&vote).unwrap();
        
        // Verify size is within network limits but still large
        let size_mb = serialized.len() as f64 / (1024.0 * 1024.0);
        println!("Vote size: {:.2} MB", size_mb);
        assert!(size_mb > 20.0, "Vote should be significantly large");
        assert!(size_mb < 64.0, "Vote should be under network limit");
        
        // Deserialize - this allocates all the memory
        let deserialized: Vote = bcs::from_bytes(&serialized).unwrap();
        
        // The vote passes verification despite being bloated
        let current_verifier = ValidatorVerifier::new_single(
            validator_signer.author(),
            validator_signer.public_key(),
        );
        assert!(deserialized.verify(&current_verifier).is_ok());
        
        // Demonstrate memory is actually allocated by cloning (as done in pending_votes)
        let cloned_vote = deserialized.clone();
        
        println!("Successfully created and stored bloated vote consuming ~{:.2} MB", size_mb);
    }
}
```

**Notes:**
- This vulnerability requires Byzantine validator behavior, which is within the f < n/3 fault tolerance assumption
- While the system is designed to tolerate Byzantine validators, it should still protect against resource exhaustion attacks
- The lack of epoch state size validation creates an unnecessary attack surface that can be easily mitigated
- Current validator sets are much smaller (~100-200), making 65,536 validators clearly malicious

### Citations

**File:** types/src/block_info.rs (L24-44)
```rust
/// This structure contains all the information needed for tracking a block
/// without having access to the block or its execution output state. It
/// assumes that the block is the last block executed within the ledger.
#[derive(Clone, Debug, Deserialize, Eq, PartialEq, Serialize)]
#[cfg_attr(any(test, feature = "fuzzing"), derive(Arbitrary))]
pub struct BlockInfo {
    /// The epoch to which the block belongs.
    epoch: u64,
    /// The consensus protocol is executed in rounds, which monotonically increase per epoch.
    round: Round,
    /// The identifier (hash) of the block.
    id: HashValue,
    /// The accumulator root hash after executing this block.
    executed_state_id: HashValue,
    /// The version of the latest transaction after executing this block.
    version: Version,
    /// The timestamp this block was proposed by a proposer.
    timestamp_usecs: u64,
    /// An optional field containing the next epoch info
    next_epoch_state: Option<EpochState>,
}
```

**File:** types/src/epoch_state.rs (L15-30)
```rust
/// EpochState represents a trusted validator set to validate messages from the specific epoch,
/// it could be updated with EpochChangeProof.
#[derive(Clone, Deserialize, Eq, PartialEq, Serialize)]
#[cfg_attr(any(test, feature = "fuzzing"), derive(Arbitrary))]
pub struct EpochState {
    pub epoch: u64,
    pub verifier: Arc<ValidatorVerifier>,
}

impl EpochState {
    pub fn new(epoch: u64, verifier: ValidatorVerifier) -> Self {
        Self {
            epoch,
            verifier: verifier.into(),
        }
    }
```

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L98-100)
```text
    /// Limit the maximum size to u16::max, it's the current limit of the bitvec
    /// https://github.com/aptos-labs/aptos-core/blob/main/crates/aptos-bitvec/src/lib.rs#L20
    const MAX_VALIDATOR_SET_SIZE: u64 = 65536;
```

**File:** config/src/config/network_config.rs (L45-50)
```rust
pub const MAX_MESSAGE_METADATA_SIZE: usize = 128 * 1024; /* 128 KiB: a buffer for metadata that might be added to messages by networking */
pub const MESSAGE_PADDING_SIZE: usize = 2 * 1024 * 1024; /* 2 MiB: a safety buffer to allow messages to get larger during serialization */
pub const MAX_APPLICATION_MESSAGE_SIZE: usize =
    (MAX_MESSAGE_SIZE - MAX_MESSAGE_METADATA_SIZE) - MESSAGE_PADDING_SIZE; /* The message size that applications should check against */
pub const MAX_FRAME_SIZE: usize = 4 * 1024 * 1024; /* 4 MiB large messages will be chunked into multiple frames and streamed */
pub const MAX_MESSAGE_SIZE: usize = 64 * 1024 * 1024; /* 64 MiB */
```

**File:** network/framework/src/protocols/wire/handshake/v1/mod.rs (L38-39)
```rust
pub const USER_INPUT_RECURSION_LIMIT: usize = 32;
pub const RECURSION_LIMIT: usize = 64;
```

**File:** consensus/consensus-types/src/vote.rs (L149-175)
```rust
    /// Verifies that the consensus data hash of LedgerInfo corresponds to the vote info,
    /// and then verifies the signature.
    pub fn verify(&self, validator: &ValidatorVerifier) -> anyhow::Result<()> {
        // TODO(ibalajiarun): Ensure timeout is None if RoundTimeoutMsg is enabled.

        ensure!(
            self.ledger_info.consensus_data_hash() == self.vote_data.hash(),
            "Vote's hash mismatch with LedgerInfo"
        );
        validator
            .optimistic_verify(self.author(), &self.ledger_info, &self.signature)
            .context("Failed to verify Vote")?;
        if let Some((timeout, signature)) = &self.two_chain_timeout {
            ensure!(
                (timeout.epoch(), timeout.round())
                    == (self.epoch(), self.vote_data.proposed().round()),
                "2-chain timeout has different (epoch, round) than Vote"
            );
            timeout.verify(validator)?;
            validator
                .verify(self.author(), &timeout.signing_format(), signature)
                .context("Failed to verify 2-chain timeout signature")?;
        }
        // Let us verify the vote data as well
        self.vote_data().verify()?;
        Ok(())
    }
```

**File:** consensus/src/pending_votes.rs (L315-316)
```rust
        self.author_to_vote
            .insert(vote.author(), (vote.clone(), li_digest));
```

**File:** consensus/src/liveness/round_state.rs (L254-259)
```rust
        if new_round > self.current_round {
            let (prev_round_votes, prev_round_timeout_votes) = self.pending_votes.drain_votes();

            // Start a new round.
            self.current_round = new_round;
            self.pending_votes = PendingVotes::new();
```
