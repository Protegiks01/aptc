# Audit Report

## Title
Missing Cross-Validation in Backup Metadata Enables Chain State Divergence Through Inconsistent Epoch-Version Mappings

## Summary
The `MetadataView` struct in the backup system fails to validate consistency between `epoch_ending_backups`, `state_snapshot_backups`, and `transaction_backups`. This allows inconsistent metadata (mismatched epoch/version pairs across backup types) to pass validation and cause restored nodes to diverge from the original chain state, breaking consensus safety guarantees.

## Finding Description

The backup/restore system violates the **State Consistency** and **Consensus Safety** invariants through two critical flaws:

**Flaw 1: No Cross-Validation in MetadataView Construction** [1](#0-0) 

The `MetadataView::new()` method independently sorts and deduplicates each backup type without validating that:
- Epochs in `StateSnapshotBackupMeta` align with epoch boundaries defined in `EpochEndingBackupMeta`
- Version ranges in `TransactionBackupMeta` correspond to correct epoch boundaries
- Epoch/version pairs are internally consistent across all three backup types

**Flaw 2: Weak Epoch Validation During Restore** [2](#0-1) 

The `EpochHistory::verify_ledger_info()` method contains a critical weakness: when a state snapshot claims an epoch number beyond what exists in the epoch history, it **only logs a warning and returns Ok()** without actual verification (lines 279-287). The TODO comment at line 280 explicitly acknowledges this is an unfixed issue.

**Attack Scenario:**

An attacker provides inconsistent backup metadata to a restore operation:

1. **EpochEndingBackupMeta**: Defines epochs 0-5, covering versions 0-10,000
2. **StateSnapshotBackupMeta**: Claims epoch 10 at version 5,000 (inconsistent!)
3. **TransactionBackupMeta**: Covers versions 0-10,000

During restoration: [3](#0-2) 

The `RestoreCoordinator` selects these backups independently (lines 163, 178, 184, 194, 212, 213) with no cross-validation. The epoch history is built containing only epochs 0-5, but the state snapshot claiming epoch 10 at version 5,000 bypasses verification due to the weak validation path. [4](#0-3) 

When the state snapshot is verified (line 137-138), the mismatched epoch passes through the weak validation, causing version 5,000 to be associated with epoch 10 instead of its correct epoch (likely epoch 3-4).

**Metadata Structure Reference:** [5](#0-4) 

The metadata structures contain both epoch and version information but lack cross-validation logic.

## Impact Explanation

**Severity: Critical** - This vulnerability enables multiple critical impacts that meet the highest bug bounty severity criteria:

1. **Consensus/Safety Violations**: Restored nodes will have incorrect epoch-to-version mappings, causing them to use wrong validator sets for consensus decisions. This breaks the fundamental consensus safety guarantee that all validators agree on chain state.

2. **State Divergence**: The restored chain state diverges from the original chain because epoch-dependent logic (validator set selection, staking rewards, governance voting power) operates with incorrect epoch context. This breaks the **Deterministic Execution** invariant.

3. **Non-Recoverable Network Partition**: Multiple validators restoring from the same inconsistent backup will form a separate network partition with different state roots and epoch boundaries than the legitimate network, requiring a hard fork to resolve.

4. **Validator Set Manipulation**: Incorrect epoch mappings mean transactions execute with the wrong validator set context, potentially allowing unauthorized validators to participate in consensus or legitimate validators to be excluded.

## Likelihood Explanation

**Likelihood: Medium-High** - This vulnerability can realistically occur through several scenarios:

1. **Backup Storage Corruption**: Metadata files get corrupted or accidentally replaced, causing inconsistencies
2. **Manual Operator Error**: Administrators manually managing backups might accidentally mix metadata from different chain states (testnet/mainnet, pre/post-fork)
3. **Multi-Chain Backup Confusion**: Organizations backing up multiple chains to shared storage might experience metadata cross-contamination
4. **Malicious Backup Provider**: During disaster recovery, attackers could provide tampered backups with intentionally inconsistent metadata
5. **Metadata Cache Poisoning**: The metadata cache system could be exploited to inject inconsistent metadata

The vulnerability is particularly dangerous because:
- It produces only warning logs, easily missed during crisis situations
- Operators under pressure during disaster recovery might not notice the inconsistency
- Multiple validators could be simultaneously affected if restoring from the same backup source
- Detection only occurs after the restored node diverges from the network, potentially causing significant damage

## Recommendation

Implement comprehensive cross-validation in `MetadataView` to ensure epoch/version consistency:

```rust
impl MetadataView {
    pub(crate) fn new(metadata_vec: Vec<Metadata>, file_handles: Vec<FileHandle>) -> Result<Self> {
        // ... existing sorting and deduplication code ...
        
        // NEW: Cross-validate consistency between backup types
        Self::validate_epoch_version_consistency(
            &epoch_ending_backups,
            &state_snapshot_backups,
            &transaction_backups,
        )?;
        
        // ... rest of constructor ...
    }
    
    fn validate_epoch_version_consistency(
        epoch_endings: &[EpochEndingBackupMeta],
        state_snapshots: &[StateSnapshotBackupMeta],
        transactions: &[TransactionBackupMeta],
    ) -> Result<()> {
        // Build epoch-to-version mapping from epoch_endings
        let mut epoch_ranges: HashMap<u64, (Version, Version)> = HashMap::new();
        for ee in epoch_endings {
            for epoch in ee.first_epoch..=ee.last_epoch {
                // Validate that each epoch maps to a consistent version range
                // This requires the EpochEndingBackup manifest to contain per-epoch version info
                epoch_ranges.insert(epoch, (ee.first_version, ee.last_version));
            }
        }
        
        // Validate state snapshots have epochs consistent with version ranges
        for ss in state_snapshots {
            if let Some((start_ver, end_ver)) = epoch_ranges.get(&ss.epoch) {
                ensure!(
                    ss.version >= *start_ver && ss.version <= *end_ver,
                    "State snapshot at version {} claims epoch {}, but that epoch covers versions {}-{}",
                    ss.version, ss.epoch, start_ver, end_ver
                );
            } else {
                ensure!(
                    ss.epoch == 0 || epoch_ranges.contains_key(&(ss.epoch - 1)),
                    "State snapshot epoch {} not found in epoch ending backups",
                    ss.epoch
                );
            }
        }
        
        // Validate transaction backup version ranges align with epoch boundaries
        for txn in transactions {
            // Ensure transaction ranges don't span epoch boundaries incorrectly
            // This requires checking that version ranges respect epoch endings
            // Implementation depends on having epoch->version mappings available
        }
        
        Ok(())
    }
}
```

Additionally, strengthen the epoch validation in `EpochHistory::verify_ledger_info()`:

```rust
impl EpochHistory {
    pub fn verify_ledger_info(&self, li_with_sigs: &LedgerInfoWithSignatures) -> Result<()> {
        let epoch = li_with_sigs.ledger_info().epoch();
        ensure!(!self.epoch_endings.is_empty(), "Empty epoch history.");
        
        // REMOVE the weak validation that just warns and returns Ok()
        // REPLACE with strict enforcement:
        ensure!(
            epoch <= self.epoch_endings.len() as u64,
            "Epoch {} exceeds available epoch history (up to epoch {}). \
            This indicates inconsistent backup metadata.",
            epoch,
            self.epoch_endings.len()
        );
        
        // ... rest of validation ...
    }
}
```

## Proof of Concept

**Reproduction Steps:**

1. Create inconsistent backup metadata files manually:

```rust
// Create epoch ending backup claiming epochs 0-5 cover versions 0-10000
let epoch_meta = Metadata::new_epoch_ending_backup(
    0, 5, 
    0, 10000,
    mock_manifest_handle.clone()
);

// Create state snapshot claiming epoch 10 at version 5000 (INCONSISTENT!)
let state_meta = Metadata::new_state_snapshot_backup(
    10, // epoch
    5000, // version
    mock_manifest_handle.clone()
);

// Create transaction backup
let txn_meta = Metadata::new_transaction_backup(
    0, 10000,
    mock_manifest_handle.clone()
);

// MetadataView accepts these without error
let metadata_view = MetadataView::new(
    vec![epoch_meta, state_meta, txn_meta],
    vec![mock_manifest_handle]
);
// NO ERROR - vulnerability confirmed

// During restore, epoch history is built with epochs 0-5
// State snapshot claiming epoch 10 bypasses validation
// Result: version 5000 incorrectly associated with epoch 10
// instead of correct epoch (likely 3-4)
```

2. Execute restore operation with this inconsistent metadata
3. Observe that restored node has mismatched epoch/version mappings
4. Verify that the restored node produces different state roots than the original chain
5. Confirm consensus safety violation when restored node disagrees with network

**Expected Behavior:** `MetadataView::new()` should reject inconsistent metadata with a validation error.

**Actual Behavior:** Inconsistent metadata is accepted, causing state divergence during restoration.

---

## Notes

This vulnerability is explicitly acknowledged in the codebase via the TODO comment but remains unfixed, representing a critical gap in backup system integrity. The lack of cross-validation between backup types combined with weak epoch verification creates a perfect storm for consensus violations during disaster recovery scenarios. The impact is amplified in crisis situations when multiple validators might simultaneously restore from the same compromised backup source.

### Citations

**File:** storage/backup/backup-cli/src/metadata/view.rs (L29-78)
```rust
    pub(crate) fn new(metadata_vec: Vec<Metadata>, file_handles: Vec<FileHandle>) -> Self {
        let mut epoch_ending_backups = Vec::new();
        let mut state_snapshot_backups = Vec::new();
        let mut transaction_backups = Vec::new();
        let mut identity = None;
        let mut compaction_timestamps = Vec::new();

        for meta in metadata_vec {
            match meta {
                Metadata::EpochEndingBackup(e) => epoch_ending_backups.push(e),
                Metadata::StateSnapshotBackup(s) => state_snapshot_backups.push(s),
                Metadata::TransactionBackup(t) => transaction_backups.push(t),
                Metadata::Identity(i) => identity = Some(i),
                Metadata::CompactionTimestamps(t) => compaction_timestamps.push(t),
            }
        }
        epoch_ending_backups.sort_unstable();
        epoch_ending_backups.dedup();
        state_snapshot_backups.sort_unstable();
        state_snapshot_backups.dedup();
        transaction_backups.sort_unstable();
        transaction_backups.dedup();

        let mut compaction_meta_opt = compaction_timestamps.iter().max().cloned();
        if let Some(ref mut compaction_meta) = compaction_meta_opt {
            // insert new_files into the previous_compaction_timestamps
            for file in file_handles.into_iter() {
                // if file is not in timestamps, set it to None, otherwise, keep it the same
                compaction_meta
                    .compaction_timestamps
                    .entry(file)
                    .or_insert(None);
            }
        } else {
            // Create new compaction timestamp meta with new files only
            let compaction_timestamps = file_handles.into_iter().map(|file| (file, None)).collect();
            compaction_meta_opt = Some(CompactionTimestampsMeta {
                file_compacted_at: duration_since_epoch().as_secs(),
                compaction_timestamps,
            });
        };

        Self {
            epoch_ending_backups,
            state_snapshot_backups,
            transaction_backups,
            _identity: identity,
            compaction_timestamps: compaction_meta_opt,
        }
    }
```

**File:** storage/backup/backup-cli/src/backup_types/epoch_ending/restore.rs (L276-312)
```rust
    pub fn verify_ledger_info(&self, li_with_sigs: &LedgerInfoWithSignatures) -> Result<()> {
        let epoch = li_with_sigs.ledger_info().epoch();
        ensure!(!self.epoch_endings.is_empty(), "Empty epoch history.",);
        if epoch > self.epoch_endings.len() as u64 {
            // TODO(aldenhu): fix this from upper level
            warn!(
                epoch = epoch,
                epoch_history_until = self.epoch_endings.len(),
                "Epoch is too new and can't be verified. Previous chunks are verified and node \
                won't be able to start if this data is malicious."
            );
            return Ok(());
        }
        if epoch == 0 {
            ensure!(
                li_with_sigs.ledger_info() == &self.epoch_endings[0],
                "Genesis epoch LedgerInfo info doesn't match.",
            );
        } else if let Some(wp_trusted) = self
            .trusted_waypoints
            .get(&li_with_sigs.ledger_info().version())
        {
            let wp_li = Waypoint::new_any(li_with_sigs.ledger_info());
            ensure!(
                *wp_trusted == wp_li,
                "Waypoints don't match. In backup: {}, trusted: {}",
                wp_li,
                wp_trusted,
            );
        } else {
            self.epoch_endings[epoch as usize - 1]
                .next_epoch_state()
                .ok_or_else(|| anyhow!("Shouldn't contain non- epoch bumping LIs."))?
                .verify(li_with_sigs)?;
        };
        Ok(())
    }
```

**File:** storage/backup/backup-cli/src/coordinators/restore.rs (L117-231)
```rust
        let metadata_view = metadata::cache::sync_and_load(
            &self.metadata_cache_opt,
            Arc::clone(&self.storage),
            self.global_opt.concurrent_downloads,
        )
        .await?;

        // calculate the start_version and replay_version
        let max_txn_ver = metadata_view
            .max_transaction_version()?
            .ok_or_else(|| anyhow!("No transaction backup found."))?;
        let target_version = std::cmp::min(self.global_opt.target_version, max_txn_ver);
        info!(
            "User specified target version: {}, max transaction version: {}, Target version is set to {}",
            self.global_opt.target_version, max_txn_ver, target_version
        );

        COORDINATOR_TARGET_VERSION.set(target_version as i64);
        let lhs = self.ledger_history_start_version();

        let latest_tree_version = self
            .global_opt
            .run_mode
            .get_state_snapshot_before(Version::MAX);
        let tree_completed = {
            match latest_tree_version {
                Some((ver, _)) => self
                    .global_opt
                    .run_mode
                    .get_state_snapshot_before(ver)
                    .is_some(),
                None => false,
            }
        };

        let mut db_next_version = self
            .global_opt
            .run_mode
            .get_next_expected_transaction_version()?;

        let kv_snapshot = match self.global_opt.run_mode.get_in_progress_state_kv_snapshot() {
            Ok(Some(ver)) => {
                if db_next_version >= ver {
                    // already restored the kv snapshot, no need to restore again
                    None
                } else {
                    let snapshot = metadata_view.select_state_snapshot(ver)?;
                    ensure!(
                        snapshot.is_some() && snapshot.as_ref().unwrap().version == ver,
                        "cannot find in-progress state snapshot {}",
                        ver
                    );
                    snapshot
                }
            },
            Ok(None) | Err(_) => {
                assert_eq!(
                    db_next_version, 0,
                    "DB should be empty if no in-progress state snapshot found"
                );
                metadata_view
                    .select_state_snapshot(std::cmp::min(lhs, max_txn_ver))
                    .expect("Cannot find any snapshot before ledger history start version")
            },
        };

        let tree_snapshot = if let Some((latest_tree_version, _)) = latest_tree_version {
            let snapshot = metadata_view.select_state_snapshot(latest_tree_version)?;

            ensure!(
                snapshot.is_some() && snapshot.as_ref().unwrap().version == latest_tree_version,
                "cannot find tree snapshot {}",
                latest_tree_version
            );
            snapshot.unwrap()
        } else {
            metadata_view
                .select_state_snapshot(target_version)?
                .expect("Cannot find tree snapshot before target version")
        };

        let mut do_phase_1 = if let Some(kv_snapshot) = kv_snapshot.as_ref() {
            // if we have a kv snapshot, we need to restore the state between lhs and rs
            // if the version are equal, we don't need to restore phase 1. we can directly restore a snapshot with both tree and KV, and then replay txn till the target_version
            kv_snapshot.version < tree_snapshot.version
        } else {
            // if we don't have a kv snapshot, we need to restore the state between db_next_version and rs
            db_next_version < tree_snapshot.version
        };
        let txn_start_version = if let Some(kv_snapshot) = kv_snapshot.as_ref() {
            kv_snapshot.version
        } else {
            db_next_version
        };
        let transaction_backups =
            metadata_view.select_transaction_backups(txn_start_version, target_version)?;
        let epoch_ending_backups = metadata_view.select_epoch_ending_backups(target_version)?;
        let epoch_handles = epoch_ending_backups
            .iter()
            .filter(|e| e.first_version <= target_version)
            .map(|backup| backup.manifest.clone())
            .collect();
        let epoch_history = if !self.skip_epoch_endings {
            Some(Arc::new(
                EpochHistoryRestoreController::new(
                    epoch_handles,
                    self.global_opt.clone(),
                    self.storage.clone(),
                )
                .run()
                .await?,
            ))
        } else {
            None
        };
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L113-139)
```rust
    async fn run_impl(self) -> Result<()> {
        if self.version > self.target_version {
            warn!(
                "Trying to restore state snapshot to version {}, which is newer than the target version {}, skipping.",
                self.version,
                self.target_version,
            );
            return Ok(());
        }

        let manifest: StateSnapshotBackup =
            self.storage.load_json_file(&self.manifest_handle).await?;
        let (txn_info_with_proof, li): (TransactionInfoWithProof, LedgerInfoWithSignatures) =
            self.storage.load_bcs_file(&manifest.proof).await?;
        txn_info_with_proof.verify(li.ledger_info(), manifest.version)?;
        let state_root_hash = txn_info_with_proof
            .transaction_info()
            .ensure_state_checkpoint_hash()?;
        ensure!(
            state_root_hash == manifest.root_hash,
            "Root hash mismatch with that in proof. root hash: {}, expected: {}",
            manifest.root_hash,
            state_root_hash,
        );
        if let Some(epoch_history) = self.epoch_history.as_ref() {
            epoch_history.verify_ledger_info(&li)?;
        }
```

**File:** storage/backup/backup-cli/src/metadata/mod.rs (L175-196)
```rust
#[derive(Clone, Debug, Deserialize, Serialize, Eq, PartialEq, Ord, PartialOrd)]
pub struct EpochEndingBackupMeta {
    pub first_epoch: u64,
    pub last_epoch: u64,
    pub first_version: Version,
    pub last_version: Version,
    pub manifest: FileHandle,
}

#[derive(Clone, Debug, Deserialize, Serialize, Eq, PartialEq, Ord, PartialOrd)]
pub struct StateSnapshotBackupMeta {
    pub epoch: u64,
    pub version: Version,
    pub manifest: FileHandle,
}

#[derive(Clone, Debug, Deserialize, Serialize, Eq, PartialEq, Ord, PartialOrd)]
pub struct TransactionBackupMeta {
    pub first_version: Version,
    pub last_version: Version,
    pub manifest: FileHandle,
}
```
