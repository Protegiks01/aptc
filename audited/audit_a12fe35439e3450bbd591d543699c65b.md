# Audit Report

## Title
Token Ownership State Corruption via Transaction Ordering Violation in Indexer

## Summary
The token processor's `process_transactions()` function uses `HashMap.extend()` to merge token ownership data across transactions without validating that transactions are processed in version order. If transactions are processed out of order, later transaction states can be overwritten by earlier ones, causing permanent data loss in the indexer database.

## Finding Description

The vulnerability exists in the token ownership deduplication logic: [1](#0-0) 

During transaction processing, each transaction returns a `HashMap<CurrentTokenOwnershipPK, CurrentTokenOwnership>` where the key is `(token_data_id_hash, property_version, owner_address)`: [2](#0-1) 

These hashmaps are merged using `extend()` in the main processing loop: [3](#0-2) 

The code contains a comment assuming transactions are always in increasing version order, but provides **no validation**. The `HashMap.extend()` operation unconditionally overwrites existing entries with the same key, regardless of the `last_transaction_version` value stored in the ownership record.

**Attack Scenario:**
If transactions are processed in order [v101, v100] instead of [v100, v101]:

1. Transaction v101 processes first, adding ownership entry: `{token_id: {amount: 2, last_transaction_version: 101}}`
2. Transaction v100 processes second, `extend()` **overwrites**: `{token_id: {amount: 1, last_transaction_version: 100}}`  
3. The HashMap is converted to a Vec, containing only the v100 state
4. Database insertion occurs with the v100 data

While the database has a protective WHERE clause: [4](#0-3) 

This protection **fails for initial inserts** (when the database has no existing record for that ownership). The v100 state gets inserted, and the v101 state is permanently lost.

## Impact Explanation

**Severity: Medium** (per Aptos Bug Bounty categories)

This vulnerability causes **state inconsistencies requiring intervention**:

1. **Data Loss**: Transaction records at higher versions are permanently lost from the indexer
2. **Audit Trail Corruption**: Historical queries return incomplete/incorrect ownership data  
3. **Dependent System Failures**: Applications relying on indexer data make decisions based on stale state
4. **Recovery Complexity**: Requires re-indexing from genesis to fix corrupted ownership records

The impact is limited to the indexer subsystem and does NOT affect:
- Blockchain consensus (indexer is read-only)
- Validator operations
- Fund security on-chain
- Transaction execution

However, many ecosystem applications depend on indexer data for NFT marketplaces, wallets, and analytics, making this a significant data integrity issue.

## Likelihood Explanation

**Likelihood: Low-to-Medium**

The vulnerability requires transactions to be delivered out of order to the processor. Analysis of the transaction fetcher shows it should maintain order: [5](#0-4) 

The database layer returns transactions in sequential version order. However, order violations could occur through:

1. **Bugs in parallel fetching**: The fetcher spawns multiple tasks; a race condition could interleave batches incorrectly
2. **Future code modifications**: Changes to fetching logic without preserving ordering guarantees
3. **Custom indexer deployments**: Modified indexers that don't maintain order constraints
4. **System failures during processing**: Crash-recovery scenarios where batch processing resumes incorrectly

The same pattern exists in other processors, indicating a systemic defensive programming issue: [6](#0-5) 

## Recommendation

**Add explicit transaction version validation:**

```rust
async fn process_transactions(
    &self,
    transactions: Vec<Transaction>,
    start_version: u64,
    end_version: u64,
) -> Result<ProcessingResult, TransactionProcessingError> {
    // Validate transactions are in order
    for window in transactions.windows(2) {
        let v1 = window[0].version().ok_or_else(|| 
            TransactionProcessingError::InvalidTransaction("Missing version".to_string())
        )?;
        let v2 = window[1].version().ok_or_else(||
            TransactionProcessingError::InvalidTransaction("Missing version".to_string())  
        )?;
        if v1 >= v2 {
            return Err(TransactionProcessingError::OutOfOrderTransaction(
                format!("Transaction versions out of order: {} >= {}", v1, v2)
            ));
        }
    }
    
    // Existing processing logic...
}
```

**Alternative: Version-aware HashMap merge:**

Instead of unconditional `extend()`, only update if the new version is higher:

```rust
for (key, new_ownership) in current_token_ownerships {
    all_current_token_ownerships
        .entry(key)
        .and_modify(|existing| {
            if new_ownership.last_transaction_version > existing.last_transaction_version {
                *existing = new_ownership.clone();
            }
        })
        .or_insert(new_ownership);
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_out_of_order_overwrites_newer_version() {
        let mut all_current_token_ownerships = HashMap::new();
        
        // Simulate processing transaction v101 first
        let ownership_v101 = CurrentTokenOwnership {
            token_data_id_hash: "token_1".to_string(),
            property_version: BigDecimal::from(0),
            owner_address: "alice".to_string(),
            amount: BigDecimal::from(2),
            last_transaction_version: 101,
            // ... other fields
        };
        
        let mut txn_101_ownerships = HashMap::new();
        txn_101_ownerships.insert(
            ("token_1".to_string(), BigDecimal::from(0), "alice".to_string()),
            ownership_v101,
        );
        
        all_current_token_ownerships.extend(txn_101_ownerships);
        
        // Simulate processing transaction v100 second (OUT OF ORDER)
        let ownership_v100 = CurrentTokenOwnership {
            token_data_id_hash: "token_1".to_string(),
            property_version: BigDecimal::from(0),
            owner_address: "alice".to_string(),
            amount: BigDecimal::from(1),
            last_transaction_version: 100,
            // ... other fields
        };
        
        let mut txn_100_ownerships = HashMap::new();
        txn_100_ownerships.insert(
            ("token_1".to_string(), BigDecimal::from(0), "alice".to_string()),
            ownership_v100,
        );
        
        all_current_token_ownerships.extend(txn_100_ownerships);
        
        // VULNERABILITY: v101 data is lost, v100 remains
        let final_ownership = all_current_token_ownerships.get(&(
            "token_1".to_string(),
            BigDecimal::from(0),
            "alice".to_string(),
        )).unwrap();
        
        assert_eq!(final_ownership.last_transaction_version, 100);
        assert_eq!(final_ownership.amount, BigDecimal::from(1));
        // Expected v101 with amount=2, but got v100 with amount=1
    }
}
```

## Notes

This issue affects the **indexer subsystem only**, not consensus or on-chain state. While the blockchain itself maintains correct state, applications querying the indexer would receive corrupted historical data. The protective WHERE clause in database inserts provides partial mitigation but fails for initial insertions, allowing stale data to persist indefinitely.

### Citations

**File:** crates/indexer/src/processors/token_processor.rs (L406-407)
```rust
            Some(" WHERE current_token_ownerships.last_transaction_version <= excluded.last_transaction_version "),
        )?;
```

**File:** crates/indexer/src/processors/token_processor.rs (L873-876)
```rust
        let mut all_current_token_ownerships: HashMap<
            CurrentTokenOwnershipPK,
            CurrentTokenOwnership,
        > = HashMap::new();
```

**File:** crates/indexer/src/processors/token_processor.rs (L906-907)
```rust
            // Given versions will always be increasing here (within a single batch), we can just override current values
            all_current_token_ownerships.extend(current_token_ownerships);
```

**File:** crates/indexer/src/models/token_models/tokens.rs (L36-37)
```rust
// PK of current_token_ownerships, i.e. token_data_id_hash + property_version + owner_address, used to dedupe
pub type CurrentTokenOwnershipPK = (TokenDataIdHash, BigDecimal, Address);
```

**File:** crates/indexer/src/indexer/fetcher.rs (L391-422)
```rust
            max_retry_time_millis,
            max_retry_time: Duration::from_millis(max_retry_time_millis),
            transaction_fetch_batch_size,
            max_pending_batches,
            max_tasks: std::cmp::max(max_tasks, 1),
        }
    }
}

impl Default for TransactionFetcherOptions {
    fn default() -> Self {
        TransactionFetcherOptions::new(None, None, None, None, 5)
    }
}

pub struct TransactionFetcher {
    starting_version: u64,
    options: TransactionFetcherOptions,
    pub context: Arc<Context>,
    fetcher_handle: Option<JoinHandle<()>>,
    transactions_sender: Option<mpsc::Sender<Vec<Transaction>>>,
    transaction_receiver: mpsc::Receiver<Vec<Transaction>>,
}

impl TransactionFetcher {
    pub fn new(
        context: Arc<Context>,
        starting_version: u64,
        options: TransactionFetcherOptions,
    ) -> Self {
        let (transactions_sender, transaction_receiver) =
            mpsc::channel::<Vec<Transaction>>(options.max_pending_batches);
```

**File:** crates/indexer/src/processors/stake_processor.rs (L337-338)
```rust
            let current_stake_pool_voter = CurrentStakingPoolVoter::from_transaction(txn).unwrap();
            all_current_stake_pool_voters.extend(current_stake_pool_voter);
```
