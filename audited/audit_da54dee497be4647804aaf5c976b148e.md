# Audit Report

## Title
Single Worker Bottleneck in Quorum Store Causes Validators to Timeout on Proposals Due to Batch Processing Delays

## Summary

Setting `num_workers_for_remote_batches` to 1 in `QuorumStoreConfig` creates a critical bottleneck where a single worker processes all remote batches from all validators sequentially. This causes the NetworkListener to block when the worker's channel saturates, preventing timely processing of batches referenced in consensus proposals. Validators then timeout waiting for batch payloads, resulting in repeated `PayloadUnavailable` timeouts and degraded consensus participation. [1](#0-0) [2](#0-1) 

## Finding Description

The vulnerability exists in the Quorum Store's batch processing architecture. When `num_workers_for_remote_batches` is set to 1 (which is explicitly done for local test nodes), the system creates only a single `BatchCoordinator` worker to process ALL incoming batches from ALL validators in the network. [3](#0-2) 

The `NetworkListener` distributes incoming `BatchMsg` events to workers using round-robin assignment: [4](#0-3) 

With a single worker, all batches are sent to the same channel. The `.send().await` operation blocks asynchronously when the channel buffer (size 1000) is full, stalling the entire `NetworkListener` main loop. This prevents processing of ALL quorum store messages including `SignedBatchInfo` and `ProofOfStoreMsg`. [5](#0-4) 

Each `BatchCoordinator` worker processes batches sequentially through `handle_batches_msg`, which involves validation, transaction filtering, sending to batch generator, and persistence: [6](#0-5) 

When consensus receives a proposal, it checks if batches referenced in the proposal are locally available. If batches are missing, the validator waits until the round deadline: [7](#0-6) 

If batches don't arrive before the deadline (because NetworkListener is blocked processing the backlog), the validator computes a timeout with reason `PayloadUnavailable`: [8](#0-7) 

**Attack Scenario:**

1. Validator runs with `num_workers_for_remote_batches = 1` (default for local test nodes)
2. In a network with 100+ validators, each validator sends batches at normal rates
3. The single worker becomes saturated processing batches sequentially
4. Channel buffer (1000 batches) fills with pending batch messages
5. NetworkListener blocks when trying to send more batches to the full channel
6. While blocked, NetworkListener cannot process newly arriving batches from peers
7. Consensus receives a proposal referencing batches that are in transit but not yet processed
8. Validator waits for batches until round deadline (initial timeout: 1000ms)
9. Batches don't arrive in time due to NetworkListener blockage
10. Validator times out with `PayloadUnavailable` and cannot vote
11. This repeats across multiple rounds, causing the validator to miss consensus participation

## Impact Explanation

This is a **High Severity** issue under the Aptos bug bounty program criteria:

- **Validator node slowdowns**: Validators experience repeated timeouts waiting for batch payloads, preventing them from voting on proposals in a timely manner
- **Significant protocol violations**: Validators cannot effectively participate in consensus rounds when they should be able to under normal network conditions

While the validator doesn't completely fall offline or miss rounds entirely, it repeatedly times out on proposals due to batch unavailability. In a production network with high transaction throughput and many validators, this severely degrades the affected validator's consensus participation and reduces network effectiveness. [9](#0-8) 

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability is highly likely to manifest in production-like scenarios:

1. **Configuration is in active use**: The code explicitly sets `num_workers_for_remote_batches = 1` for local test nodes, meaning this configuration is actively deployed

2. **Scales poorly with validator count**: In networks with 100+ validators (like mainnet), each validator sending batches creates significant load on the single worker

3. **No validation or warning**: There's no sanitizer check warning operators that `num_workers_for_remote_batches = 1` is dangerous for multi-validator networks

4. **Natural network conditions trigger it**: Normal batch dissemination traffic from many validators is sufficient to saturate a single worker

5. **Blocking is guaranteed**: The use of `.await` on channel send operations means blocking will occur when the buffer fills

The channel infrastructure is created during quorum store initialization: [10](#0-9) 

## Recommendation

**Immediate Fix:**

1. Add a minimum value check in the config sanitizer to prevent `num_workers_for_remote_batches` from being set below a safe threshold (e.g., 4) for multi-validator networks:

```rust
impl ConfigSanitizer for QuorumStoreConfig {
    fn sanitize(
        node_config: &NodeConfig,
        node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        
        // Ensure minimum workers for production networks
        if node_type == NodeType::Validator 
            && node_config.consensus.quorum_store.num_workers_for_remote_batches < 4 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                format!(
                    "num_workers_for_remote_batches ({}) is too low for validator nodes. Minimum: 4",
                    node_config.consensus.quorum_store.num_workers_for_remote_batches
                ),
            ));
        }
        
        // Existing sanitizers...
        Self::sanitize_send_recv_batch_limits(&sanitizer_name, &node_config.consensus.quorum_store)?;
        Self::sanitize_batch_total_limits(&sanitizer_name, &node_config.consensus.quorum_store)?;
        
        Ok(())
    }
}
```

2. Update the local test node configuration to only use `num_workers_for_remote_batches = 1` for true single-node networks, and use a higher value when multiple validators are configured

3. Consider making the NetworkListener's batch sending non-blocking with proper error handling and backpressure signals instead of blocking the entire listener

## Proof of Concept

**Reproduction Steps:**

1. Configure a test network with 100+ validator nodes
2. Set `num_workers_for_remote_batches = 1` in the quorum store config for one validator
3. Generate high transaction throughput (>5000 TPS) to trigger frequent batch creation
4. Monitor metrics on the affected validator:
   - `CONSENSUS_PROPOSAL_PAYLOAD_AVAILABILITY` with label "missing" increases
   - `AGGREGATED_ROUND_TIMEOUT_REASON` shows `PayloadUnavailable` timeouts
   - `BATCH_COORDINATOR_NUM_BATCH_REQS{worker="0"}` shows saturation at single worker
   - Round timeout messages show "waiting for payload" logs

5. Observe that the validator repeatedly times out waiting for batches and cannot vote on proposals in time

**Expected Behavior:** With `num_workers_for_remote_batches = 10` (default), batches are processed in parallel across workers, preventing saturation and allowing timely batch processing for consensus proposals.

**Actual Behavior:** With `num_workers_for_remote_batches = 1`, the single worker becomes a bottleneck, NetworkListener blocks, batches are delayed, and the validator times out on proposals with `PayloadUnavailable`.

## Notes

The vulnerability is already present in the codebase for local test nodes and could affect validators that incorrectly configure this parameter too low for production networks. While the default value is 10 (which is better), the explicit override to 1 for test configurations shows this is a real deployment scenario. The lack of validation or warnings makes it easy for operators to deploy with dangerous configurations.

### Citations

**File:** config/src/config/quorum_store_config.rs (L96-96)
```rust
    pub num_workers_for_remote_batches: usize,
```

**File:** config/src/config/quorum_store_config.rs (L108-108)
```rust
            channel_size: 1000,
```

**File:** config/src/config/quorum_store_config.rs (L137-138)
```rust
            // number of batch coordinators to handle QS batch messages, should be >= 1
            num_workers_for_remote_batches: 10,
```

**File:** aptos-node/src/lib.rs (L481-484)
```rust
    node_config
        .consensus
        .quorum_store
        .num_workers_for_remote_batches = 1;
```

**File:** consensus/src/quorum_store/network_listener.rs (L77-93)
```rust
                        // Round-robin assignment to batch coordinator.
                        let idx = next_batch_coordinator_idx;
                        next_batch_coordinator_idx = (next_batch_coordinator_idx + 1)
                            % self.remote_batch_coordinator_tx.len();
                        trace!(
                            "QS: peer_id {:?},  # network_worker {}, hashed to idx {}",
                            author,
                            self.remote_batch_coordinator_tx.len(),
                            idx
                        );
                        counters::BATCH_COORDINATOR_NUM_BATCH_REQS
                            .with_label_values(&[&idx.to_string()])
                            .inc();
                        self.remote_batch_coordinator_tx[idx]
                            .send(BatchCoordinatorCommand::NewBatches(author, batches))
                            .await
                            .expect("Could not send remote batch");
```

**File:** consensus/src/quorum_store/batch_coordinator.rs (L173-245)
```rust
    pub(crate) async fn handle_batches_msg(
        &mut self,
        author: PeerId,
        batches: Vec<Batch<BatchInfoExt>>,
    ) {
        if let Err(e) = self.ensure_max_limits(&batches) {
            error!("Batch from {}: {}", author, e);
            counters::RECEIVED_BATCH_MAX_LIMIT_FAILED.inc();
            return;
        }

        let Some(batch) = batches.first() else {
            error!("Empty batch received from {}", author.short_str().as_str());
            return;
        };

        // Filter the transactions in the batches. If any transaction is rejected,
        // the message will be dropped, and all batches will be rejected.
        if self.transaction_filter_config.is_enabled() {
            let transaction_filter = &self.transaction_filter_config.batch_transaction_filter();
            for batch in batches.iter() {
                for transaction in batch.txns() {
                    if !transaction_filter.allows_transaction(
                        batch.batch_info().batch_id(),
                        batch.author(),
                        batch.digest(),
                        transaction,
                    ) {
                        error!(
                            "Transaction {}, in batch {}, from {}, was rejected by the filter. Dropping {} batches!",
                            transaction.committed_hash(),
                            batch.batch_info().batch_id(),
                            author.short_str().as_str(),
                            batches.len()
                        );
                        counters::RECEIVED_BATCH_REJECTED_BY_FILTER.inc();
                        return;
                    }
                }
            }
        }

        let approx_created_ts_usecs = batch
            .info()
            .expiration()
            .saturating_sub(self.batch_expiry_gap_when_init_usecs);

        if approx_created_ts_usecs > 0 {
            observe_batch(
                approx_created_ts_usecs,
                batch.author(),
                BatchStage::RECEIVED,
            );
        }

        let mut persist_requests = vec![];
        for batch in batches.into_iter() {
            // TODO: maybe don't message batch generator if the persist is unsuccessful?
            if let Err(e) = self
                .sender_to_batch_generator
                .send(BatchGeneratorCommand::RemoteBatch(batch.clone()))
                .await
            {
                warn!("Failed to send batch to batch generator: {}", e);
            }
            persist_requests.push(batch.into());
        }
        counters::RECEIVED_BATCH_COUNT.inc_by(persist_requests.len() as u64);
        if author != self.my_peer_id {
            counters::RECEIVED_REMOTE_BATCH_COUNT.inc_by(persist_requests.len() as u64);
        }
        self.persist_and_send_digests(persist_requests, approx_created_ts_usecs);
    }
```

**File:** consensus/src/round_manager.rs (L968-982)
```rust
    fn compute_timeout_reason(&self, round: Round) -> RoundTimeoutReason {
        if self.round_state().vote_sent().is_some() {
            return RoundTimeoutReason::NoQC;
        }

        match self.block_store.get_block_for_round(round) {
            None => RoundTimeoutReason::ProposalNotReceived,
            Some(block) => {
                if let Err(missing_authors) = self.block_store.check_payload(block.block()) {
                    RoundTimeoutReason::PayloadUnavailable { missing_authors }
                } else {
                    RoundTimeoutReason::Unknown
                }
            },
        }
```

**File:** consensus/src/round_manager.rs (L1262-1278)
```rust
        if block_store.check_payload(&proposal).is_err() {
            debug!("Payload not available locally for block: {}", proposal.id());
            counters::CONSENSUS_PROPOSAL_PAYLOAD_AVAILABILITY
                .with_label_values(&["missing"])
                .inc();
            let start_time = Instant::now();
            let deadline = self.round_state.current_round_deadline();
            let future = async move {
                (
                    block_store.wait_for_payload(&proposal, deadline).await,
                    proposal,
                    start_time,
                )
            }
            .boxed();
            self.futures.push(future);
            return Ok(());
```

**File:** consensus/src/quorum_store/quorum_store_builder.rs (L192-199)
```rust
        let mut remote_batch_coordinator_cmd_tx = Vec::new();
        let mut remote_batch_coordinator_cmd_rx = Vec::new();
        for _ in 0..config.num_workers_for_remote_batches {
            let (batch_coordinator_cmd_tx, batch_coordinator_cmd_rx) =
                tokio::sync::mpsc::channel(config.channel_size);
            remote_batch_coordinator_cmd_tx.push(batch_coordinator_cmd_tx);
            remote_batch_coordinator_cmd_rx.push(batch_coordinator_cmd_rx);
        }
```
