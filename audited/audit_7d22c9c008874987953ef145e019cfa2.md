# Audit Report

## Title
Unvalidated Epoch Parameter in QuorumStore Builder Causes Panic on Epoch Rollback

## Summary
The `InnerBuilder::new()` function in the QuorumStore subsystem accepts an epoch parameter without validation. If the epoch is ever set to zero or rolls back to a value lower than a previously stored epoch, the node will panic and crash when attempting to clean the batch ID database, causing a persistent denial of service.

## Finding Description

The `InnerBuilder::new()` function accepts an epoch parameter without any validation: [1](#0-0) 

The epoch is passed directly to child components without checking:
- Whether epoch is non-zero
- Whether epoch is greater than any previously stored epoch
- Whether epoch is sequentially increasing from the last known epoch

This unvalidated epoch flows to the `BatchGenerator` component: [2](#0-1) 

The `BatchGenerator::new()` function then calls `clean_and_get_batch_id()` with the unvalidated epoch: [3](#0-2) 

The critical vulnerability exists in `clean_and_get_batch_id()`: [4](#0-3) 

At line 171, there is an assertion `assert!(current_epoch >= epoch)` that validates the current epoch is not less than any stored epoch from the database. If this assertion fails, the node panics and crashes.

**Attack Scenario:**
1. Validator node operates normally at epoch 5, storing batch IDs in the database with key `epoch=5`
2. Due to a governance bug, state sync issue, or other system failure, the epoch value rolls back to epoch 3 (or 0)
3. The `InnerBuilder::new()` is called with `epoch=3` (no validation)
4. The `BatchGenerator::new()` calls `clean_and_get_batch_id(3)`
5. The method iterates through stored epochs, finds `epoch=5` in the database
6. Assertion `3 >= 5` fails
7. Node panics with assertion failure and crashes
8. Node cannot restart successfully because the same panic will occur on every restart attempt until the database is manually cleaned

This breaks the **Consensus Availability** invariant, as crashed validator nodes cannot participate in consensus.

## Impact Explanation

**Severity: HIGH**

This vulnerability causes:
- **Validator Node Crash**: The panic permanently crashes the node
- **Persistent DoS**: The node cannot restart without manual database intervention
- **Consensus Impact**: If multiple validators experience epoch rollback, consensus liveness is compromised

Per the Aptos bug bounty criteria, this qualifies as **High Severity** ($50,000):
- "Validator node slowdowns" - exceeded, as this causes complete validator failure
- "Significant protocol violations" - consensus participation is lost

While it does not reach Critical severity (which requires permanent network-wide failure), it represents a significant vulnerability that can be triggered if epoch monotonicity is ever violated through any system bug.

## Likelihood Explanation

**Likelihood: MEDIUM-LOW** (with important caveats)

This vulnerability requires epoch rollback or epoch 0 to occur, which should not happen under normal operation. However, it could be triggered by:

1. **On-chain Governance Vulnerability**: A bug allowing manipulation of the `ConfigurationResource` epoch field
2. **Move VM Bug**: A bug in epoch handling during reconfiguration
3. **State Sync Issue**: Incorrect epoch information during state synchronization
4. **Database Corruption**: Local database state becoming inconsistent

The vulnerability represents a **defense-in-depth failure**. While the system should prevent epoch rollback at higher layers, the QuorumStore builder should defensively validate its inputs rather than assuming correctness.

The epoch originates from on-chain `ConfigurationResource`: [5](#0-4) 

And is used to create the `EpochState` without validation: [6](#0-5) 

## Recommendation

Add epoch validation in `InnerBuilder::new()` to enforce monotonicity:

```rust
pub(crate) fn new(
    epoch: u64,
    // ... other parameters
) -> anyhow::Result<Self> {
    // Validate epoch is non-zero
    ensure!(epoch > 0, "Epoch must be greater than zero");
    
    // Validate epoch is not less than any stored epoch
    let stored_epochs = quorum_store_storage.get_all_stored_epochs()?;
    for stored_epoch in stored_epochs {
        ensure!(
            epoch >= stored_epoch,
            "Epoch rollback detected: current={}, stored={}",
            epoch,
            stored_epoch
        );
    }
    
    // ... rest of construction
}
```

Additionally, replace the assertion in `clean_and_get_batch_id()` with proper error handling: [4](#0-3) 

Change line 171 from:
```rust
assert!(current_epoch >= epoch);
```

To:
```rust
ensure!(
    current_epoch >= epoch,
    "Epoch rollback detected in database cleanup: current={}, stored={}",
    current_epoch,
    epoch
);
```

This prevents the panic and returns a proper error that can be handled gracefully.

## Proof of Concept

```rust
#[test]
fn test_epoch_rollback_causes_panic() {
    use tempfile::tempdir;
    
    // Create temporary database
    let tmpdir = tempdir().unwrap();
    let db = QuorumStoreDB::new(tmpdir.path());
    
    // Simulate normal operation: store batch_id for epoch 5
    let batch_id_5 = BatchId::new(1000);
    db.save_batch_id(5, batch_id_5).unwrap();
    
    // Attempt epoch rollback: try to clean with epoch 3
    // This should panic due to assertion failure
    let result = std::panic::catch_unwind(|| {
        db.clean_and_get_batch_id(3)
    });
    
    assert!(result.is_err(), "Expected panic on epoch rollback");
}
```

**Notes**

This vulnerability is a defense-in-depth failure. While epoch rollback should not occur under correct system operation, the lack of validation means that any bug causing epoch non-monotonicity will result in validator crashes. The assertion-based validation is too aggressive and causes permanent node failure rather than graceful error handling.

The vulnerability cannot be directly exploited by an unprivileged attacker but represents a systemic weakness that amplifies the impact of other potential bugs in epoch management, governance, or state synchronization.

### Citations

**File:** consensus/src/quorum_store/quorum_store_builder.rs (L161-234)
```rust
    pub(crate) fn new(
        epoch: u64,
        author: Author,
        num_validators: u64,
        config: QuorumStoreConfig,
        transaction_filter_config: BatchTransactionFilterConfig,
        consensus_to_quorum_store_receiver: Receiver<GetPayloadCommand>,
        quorum_store_to_mempool_sender: Sender<QuorumStoreRequest>,
        mempool_txn_pull_timeout_ms: u64,
        aptos_db: Arc<dyn DbReader>,
        network_sender: NetworkSender,
        verifier: Arc<ValidatorVerifier>,
        proof_cache: ProofCache,
        quorum_store_storage: Arc<dyn QuorumStoreStorage>,
        broadcast_proofs: bool,
        consensus_key: Arc<PrivateKey>,
    ) -> Self {
        let (coordinator_tx, coordinator_rx) = futures_channel::mpsc::channel(config.channel_size);
        let (batch_generator_cmd_tx, batch_generator_cmd_rx) =
            tokio::sync::mpsc::channel(config.channel_size);
        let (proof_coordinator_cmd_tx, proof_coordinator_cmd_rx) =
            tokio::sync::mpsc::channel(config.channel_size);
        let (proof_manager_cmd_tx, proof_manager_cmd_rx) =
            tokio::sync::mpsc::channel(config.channel_size);
        let (back_pressure_tx, back_pressure_rx) = tokio::sync::mpsc::channel(config.channel_size);
        let (quorum_store_msg_tx, quorum_store_msg_rx) =
            aptos_channel::new::<AccountAddress, (Author, VerifiedEvent)>(
                QueueStyle::FIFO,
                config.channel_size,
                None,
            );
        let mut remote_batch_coordinator_cmd_tx = Vec::new();
        let mut remote_batch_coordinator_cmd_rx = Vec::new();
        for _ in 0..config.num_workers_for_remote_batches {
            let (batch_coordinator_cmd_tx, batch_coordinator_cmd_rx) =
                tokio::sync::mpsc::channel(config.channel_size);
            remote_batch_coordinator_cmd_tx.push(batch_coordinator_cmd_tx);
            remote_batch_coordinator_cmd_rx.push(batch_coordinator_cmd_rx);
        }

        Self {
            epoch,
            author,
            num_validators,
            config,
            transaction_filter_config,
            consensus_to_quorum_store_receiver,
            quorum_store_to_mempool_sender,
            mempool_txn_pull_timeout_ms,
            aptos_db,
            network_sender,
            verifier,
            proof_cache,
            coordinator_tx,
            coordinator_rx: Some(coordinator_rx),
            batch_generator_cmd_tx,
            batch_generator_cmd_rx: Some(batch_generator_cmd_rx),
            proof_coordinator_cmd_tx,
            proof_coordinator_cmd_rx: Some(proof_coordinator_cmd_rx),
            proof_manager_cmd_tx,
            proof_manager_cmd_rx: Some(proof_manager_cmd_rx),
            back_pressure_tx,
            back_pressure_rx: Some(back_pressure_rx),
            quorum_store_storage,
            quorum_store_msg_tx,
            quorum_store_msg_rx: Some(quorum_store_msg_rx),
            remote_batch_coordinator_cmd_tx,
            remote_batch_coordinator_cmd_rx,
            batch_store: None,
            batch_reader: None,
            broadcast_proofs,
            consensus_key,
        }
    }
```

**File:** consensus/src/quorum_store/quorum_store_builder.rs (L302-310)
```rust
        let batch_generator = BatchGenerator::new(
            self.epoch,
            self.author,
            self.config.clone(),
            self.quorum_store_storage.clone(),
            self.batch_store.clone().unwrap(),
            self.quorum_store_to_mempool_sender,
            self.mempool_txn_pull_timeout_ms,
        );
```

**File:** consensus/src/quorum_store/batch_generator.rs (L78-100)
```rust
    pub(crate) fn new(
        epoch: u64,
        my_peer_id: PeerId,
        config: QuorumStoreConfig,
        db: Arc<dyn QuorumStoreStorage>,
        batch_writer: Arc<dyn BatchWriter>,
        mempool_tx: Sender<QuorumStoreRequest>,
        mempool_txn_pull_timeout_ms: u64,
    ) -> Self {
        let batch_id = if let Some(mut id) = db
            .clean_and_get_batch_id(epoch)
            .expect("Could not read from db")
        {
            // If the node shut down mid-batch, then this increment is needed
            id.increment();
            id
        } else {
            BatchId::new(aptos_infallible::duration_since_epoch().as_micros() as u64)
        };
        debug!("Initialized with batch_id of {}", batch_id);
        let mut incremented_batch_id = batch_id;
        incremented_batch_id.increment();
        db.save_batch_id(epoch, incremented_batch_id)
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L163-179)
```rust
    fn clean_and_get_batch_id(&self, current_epoch: u64) -> Result<Option<BatchId>, DbError> {
        let mut iter = self.db.iter::<BatchIdSchema>()?;
        iter.seek_to_first();
        let epoch_batch_id = iter
            .map(|res| res.map_err(Into::into))
            .collect::<Result<HashMap<u64, BatchId>>>()?;
        let mut ret = None;
        for (epoch, batch_id) in epoch_batch_id {
            assert!(current_epoch >= epoch);
            if epoch < current_epoch {
                self.delete_batch_id(epoch)?;
            } else {
                ret = Some(batch_id);
            }
        }
        Ok(ret)
    }
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L296-300)
```rust
        let epoch = ConfigurationResource::fetch_config(&db_state_view)
            .ok_or_else(|| {
                Error::UnexpectedErrorEncountered("Configuration resource does not exist!".into())
            })?
            .epoch();
```

**File:** consensus/src/epoch_manager.rs (L1171-1174)
```rust
        let epoch_state = Arc::new(EpochState {
            epoch: payload.epoch(),
            verifier: verifier.into(),
        });
```
