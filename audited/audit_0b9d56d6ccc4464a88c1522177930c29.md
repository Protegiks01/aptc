# Audit Report

## Title
Race Condition in BlockTree Rebuild Causes Validator Panic and Loss of Liveness

## Summary
A race condition exists during `BlockStore::rebuild()` where the BlockTree is replaced with a new tree that has `ordered_root_id` pointing to `commit_root_block`, but this block hasn't been inserted yet. Concurrent consensus message processing can trigger `ordered_root()` calls during this window, causing a panic that crashes the validator node.

## Finding Description

The vulnerability occurs in the `BlockStore::build()` function when called during rebuild operations. The attack surface involves the following code path: [1](#0-0) 

In `BlockTree::new()`, the `ordered_root_id` is initialized to `commit_root_id` (the first parameter), but only `window_root` is inserted into the `id_to_block` HashMap initially: [2](#0-1) 

When `window_root_block` is Some and differs from `commit_root_block` (which happens when execution pool is enabled and window_size > 0), the tree is created with:
- `ordered_root_id = commit_root_block.id()`  
- `id_to_block` containing only `window_root_block.id()` [3](#0-2) 

The critical race condition occurs in `BlockStore::build()`: [4](#0-3) 

At line 260, the new incomplete tree is atomically swapped into the shared `Arc<RwLock<BlockTree>>`. After the write lock is released, other threads can immediately access this tree via read locks. However, the `commit_root_block` hasn't been inserted yet - it's inserted later in the async loop: [5](#0-4) 

This loop contains `await` points where the tokio runtime can schedule other tasks. Meanwhile, the RoundManager (running in a separate tokio task) continuously processes incoming consensus messages. These can trigger calls to `ordered_root()`: [6](#0-5) 

When `ordered_root()` is called before `commit_root_block` is inserted: [7](#0-6) 

The `get_block(&self.ordered_root_id)` returns `None` because `ordered_root_id` points to `commit_root_block.id()` which isn't in `id_to_block` yet, causing the panic at line 200: **"Root must exist"**.

**Attack Scenario:**
1. Validator falls behind or restarts, triggering `rebuild()` during state sync
2. `BlockTree::new()` creates tree with `ordered_root_id = commit_root_block.id()`
3. Tree is swapped in (line 260), only containing `window_root_block` 
4. Block insertion loop begins with async await points
5. Network peer sends QuorumCert message
6. RoundManager task processes it, calls `need_fetch_for_quorum_cert()`
7. This invokes `ordered_root()` before `commit_root_block` insertion completes
8. Panic occurs, validator crashes

## Impact Explanation

This is a **HIGH severity** vulnerability per Aptos bug bounty criteria:

**Validator Node Crashes**: The panic causes immediate process termination, removing the validator from consensus participation. This affects network liveness and validator rewards.

**Widespread Impact During Network Stress**: If multiple validators simultaneously fall behind (e.g., during network partitions, rapid state growth, or coordinated state sync), they all rebuild concurrently. A single malicious peer or natural timing can trigger crashes across multiple validators simultaneously, severely impacting network liveness.

**Non-Deterministic Exploitation**: The race condition's timing-dependent nature makes it difficult to diagnose and can cause recurring crashes during recovery attempts, prolonging downtime.

This matches the bug bounty's HIGH severity category: "Validator node slowdowns" and "Significant protocol violations" - though this is more severe as it causes complete crashes rather than slowdowns.

## Likelihood Explanation

**HIGH likelihood** due to:

1. **Frequent Trigger Conditions**: State sync and rebuild operations occur regularly during:
   - Validator restarts (maintenance, crashes, upgrades)
   - Nodes falling behind due to network issues
   - New validators joining the network
   - Post-partition recovery

2. **Wide Race Window**: The async block insertion loop (lines 282-298) contains multiple await points. Each block insertion can take milliseconds, creating a substantial window (potentially seconds for many blocks) where the race can occur.

3. **Continuous Concurrent Access**: The RoundManager continuously processes network messages. During any rebuild, hundreds of consensus messages may arrive, each potentially triggering `ordered_root()` calls through various code paths.

4. **No Synchronization**: There's no mechanism preventing concurrent reads during rebuild. The RwLock allows unlimited concurrent readers once the write lock is released.

## Recommendation

**Option 1: Hold Write Lock During Block Insertion (Simplest)**

Modify `build()` to keep the write lock held until all critical blocks are inserted:

```rust
let inner = if let Some(tree_to_replace) = tree_to_replace {
    let mut tree_guard = tree_to_replace.write();
    *tree_guard = tree;
    
    // Insert commit_root_block immediately while holding lock
    if let Some(window_block) = &window_root_block {
        if window_block.id() != root_block_id {
            // Need to insert commit_root_block before releasing lock
            let commit_pipelined = PipelinedBlock::new_ordered(
                *commit_root_block.clone(),
                OrderedBlockWindow::empty()
            );
            tree_guard.insert_block(commit_pipelined)
                .expect("Failed to insert commit root during rebuild");
        }
    }
    drop(tree_guard); // Explicitly release
    tree_to_replace
} else {
    Arc::new(RwLock::new(tree))
};
```

**Option 2: Initialize BlockTree Correctly**

Modify `BlockTree::new()` to insert both window_root and commit_root_block when they differ:

```rust
pub(super) fn new(
    commit_root_id: HashValue,
    window_root: PipelinedBlock,
    // ... other params
) -> Self {
    let window_root_id = window_root.id();
    
    let mut id_to_block = HashMap::new();
    id_to_block.insert(window_root_id, LinkableBlock::new(window_root));
    
    // If commit_root differs, insert it too
    if commit_root_id != window_root_id {
        // Need to pass commit_root_block as parameter or handle differently
        // This requires signature changes
    }
    
    // ... rest of initialization
}
```

**Recommended**: Option 1 is simpler and safer. It ensures atomicity of the tree replacement by keeping the write lock until the tree is in a valid state.

## Proof of Concept

The following Rust test would demonstrate the race condition (conceptual - requires integration test infrastructure):

```rust
#[tokio::test(flavor = "multi_thread")]
async fn test_rebuild_race_condition() {
    // Setup: Create BlockStore with window_size enabled
    let (block_store, mut network_rx) = setup_block_store_with_window();
    
    // Create recovery data where window_root != commit_root
    let recovery_data = create_recovery_data_with_gap();
    
    // Spawn task that continuously calls ordered_root()
    let block_store_clone = block_store.clone();
    let reader_handle = tokio::spawn(async move {
        loop {
            // This should panic if race condition occurs
            let _ = block_store_clone.ordered_root();
            tokio::time::sleep(Duration::from_micros(1)).await;
        }
    });
    
    // Trigger rebuild
    let rebuild_handle = tokio::spawn(async move {
        block_store.rebuild(
            recovery_data.root,
            recovery_data.root_metadata,
            recovery_data.blocks,
            recovery_data.quorum_certs,
        ).await;
    });
    
    // Race condition should manifest as panic in reader_handle
    // Expected: "thread panicked at 'Root must exist'"
}
```

The test would need to be carefully timed to hit the race window, but in production environments with real network latency and message processing, this race is highly likely to occur.

## Notes

This vulnerability demonstrates a critical invariant violation: **The BlockTree's root references must always point to blocks that exist in the tree**. The race condition violates this by creating a window where `ordered_root_id` references a non-existent block. This breaks the liveness guarantee that validators should remain operational during normal network conditions, including state synchronization.

### Citations

**File:** consensus/src/block_storage/block_tree.rs (L104-148)
```rust
    pub(super) fn new(
        commit_root_id: HashValue,
        window_root: PipelinedBlock,
        root_quorum_cert: QuorumCert,
        root_ordered_cert: WrappedLedgerInfo,
        root_commit_cert: WrappedLedgerInfo,
        max_pruned_blocks_in_mem: usize,
        highest_2chain_timeout_cert: Option<Arc<TwoChainTimeoutCertificate>>,
    ) -> Self {
        assert_eq!(window_root.epoch(), root_ordered_cert.commit_info().epoch());
        assert!(window_root.round() <= root_ordered_cert.commit_info().round());
        let window_root_id = window_root.id();

        // Build the tree from the window root block which is <= the commit root block.
        let mut id_to_block = HashMap::new();
        let mut round_to_ids = BTreeMap::new();
        round_to_ids.insert(window_root.round(), window_root_id);
        id_to_block.insert(window_root_id, LinkableBlock::new(window_root));
        counters::NUM_BLOCKS_IN_TREE.set(1);

        let root_quorum_cert = Arc::new(root_quorum_cert);
        let mut id_to_quorum_cert = HashMap::new();
        id_to_quorum_cert.insert(
            root_quorum_cert.certified_block().id(),
            Arc::clone(&root_quorum_cert),
        );

        let pruned_block_ids = VecDeque::with_capacity(max_pruned_blocks_in_mem);

        BlockTree {
            id_to_block,
            ordered_root_id: commit_root_id,
            commit_root_id, // initially we set commit_root_id = root_id
            window_root_id,
            highest_certified_block_id: commit_root_id,
            highest_quorum_cert: Arc::clone(&root_quorum_cert),
            highest_ordered_cert: Arc::new(root_ordered_cert),
            highest_commit_cert: Arc::new(root_commit_cert),
            id_to_quorum_cert,
            pruned_block_ids,
            max_pruned_blocks_in_mem,
            highest_2chain_timeout_cert,
            round_to_ids,
        }
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L198-201)
```rust
    pub(super) fn ordered_root(&self) -> Arc<PipelinedBlock> {
        self.get_block(&self.ordered_root_id)
            .expect("Root must exist")
    }
```

**File:** consensus/src/block_storage/block_store.rs (L219-236)
```rust
        let window_root = match window_root_block {
            None => {
                PipelinedBlock::new(
                    *commit_root_block,
                    vec![],
                    // Create a dummy state_compute_result with necessary fields filled in.
                    result.clone(),
                )
            },
            Some(window_block) => {
                PipelinedBlock::new(
                    *window_block,
                    vec![],
                    // Create a dummy state_compute_result with necessary fields filled in.
                    result.clone(),
                )
            },
        };
```

**File:** consensus/src/block_storage/block_store.rs (L250-264)
```rust
        let tree = BlockTree::new(
            root_block_id,
            window_root,
            root_qc,
            root_ordered_cert,
            root_commit_cert,
            max_pruned_blocks_in_mem,
            highest_2chain_timeout_cert.map(Arc::new),
        );
        let inner = if let Some(tree_to_replace) = tree_to_replace {
            *tree_to_replace.write() = tree;
            tree_to_replace
        } else {
            Arc::new(RwLock::new(tree))
        };
```

**File:** consensus/src/block_storage/block_store.rs (L282-298)
```rust
        for block in blocks {
            if block.round() <= root_block_round {
                block_store
                    .insert_committed_block(block)
                    .await
                    .unwrap_or_else(|e| {
                        panic!(
                            "[BlockStore] failed to insert committed block during build {:?}",
                            e
                        )
                    });
            } else {
                block_store.insert_block(block).await.unwrap_or_else(|e| {
                    panic!("[BlockStore] failed to insert block during build {:?}", e)
                });
            }
        }
```

**File:** consensus/src/block_storage/sync_manager.rs (L95-100)
```rust
    /// Checks if quorum certificate can be inserted in block store without RPC
    /// Returns the enum to indicate the detailed status.
    pub fn need_fetch_for_quorum_cert(&self, qc: &QuorumCert) -> NeedFetchResult {
        if qc.certified_block().round() < self.ordered_root().round() {
            return NeedFetchResult::QCRoundBeforeRoot;
        }
```
