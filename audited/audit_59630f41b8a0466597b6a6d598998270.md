# Audit Report

## Title
Unbounded Channel Memory Exhaustion in Consensus State Sync Notifications Leading to Validator OOM

## Summary
The `notify_new_commit()` function uses an unbounded MPSC channel to send transaction commit notifications from consensus to state sync. When state sync processing is slower than the notification send rate (constrained by a 5-second timeout), large transaction vectors (up to 6MB per block) accumulate in the unbounded channel buffer, causing progressive memory exhaustion and eventual Out-of-Memory (OOM) crashes on validator nodes.

## Finding Description
The vulnerability exists in the interaction between the consensus pipeline and state sync notification system. The root cause is a combination of three design flaws:

**1. Unbounded Channel Without Memory Limits** [1](#0-0) 

The notification channel is created as unbounded, meaning it has no limit on the number of queued messages or total memory consumption.

**2. Immediate Memory Allocation on Send** [2](#0-1) 

The consensus pipeline clones all committed transactions (via `.to_vec()`) before sending. This creates a complete copy of transaction data (up to 6MB per block based on `max_receiving_block_bytes`). [3](#0-2) 

The send operation completes immediately for unbounded channels, allocating memory before state sync processes the notification.

**3. Timeout-Based Backpressure Bypass** [4](#0-3) 

While `notify_new_commit()` waits for a response with a 5-second timeout, if state sync doesn't respond in time, the error is logged but execution continues: [5](#0-4) 

**Attack Scenario:**

An attacker submits expensive transactions that slow down state sync processing (e.g., transactions with large write sets, expensive Move computations, or operations that cause database lock contention). The state sync handler performs multiple operations including notifying mempool, event subscription service, and storage service: [6](#0-5) 

Additionally, state sync creates another clone of all transactions during processing: [7](#0-6) 

When state sync processing takes longer than 5 seconds per notification:
1. Block N sends notification at T=0 (6MB allocated in channel)
2. Block N times out at T=5, proceeds to next block
3. Block N+1 sends notification at T=5 (another 6MB allocated)
4. Pattern continues: 1 notification per 5 seconds
5. State sync processes slower than 1 per 5 seconds
6. Notifications accumulate: 12/minute = 720/hour
7. Memory usage: 720 × 6MB = ~4.3GB/hour at maximum block size
8. After several hours: OOM crash

The block size limits validate this is within acceptable range: [8](#0-7) [9](#0-8) 

## Impact Explanation
**High Severity** - This vulnerability causes validator node crashes through memory exhaustion, meeting the "Validator node slowdowns" and system availability impact criteria. 

The impact is severe because:
- **Validator Availability**: OOM crashes force validator restarts, causing temporary unavailability
- **Network Liveness**: If multiple validators experience this simultaneously during high load, network liveness could be affected
- **Consensus Disruption**: Validator crashes reduce the active validator set, potentially approaching the <2/3 threshold required for consensus progress
- **Resource Limit Violation**: Breaks Invariant #9 (Resource Limits must be respected)

This doesn't reach Critical severity because:
- Doesn't cause permanent data loss or corruption
- Validators can recover by restarting
- Requires sustained high load or attacker-induced slowdown
- Doesn't directly enable theft or consensus safety violations

## Likelihood Explanation
**Medium-High Likelihood**

The vulnerability can manifest under two scenarios:

**Scenario 1: Legitimate High Load**
- During periods of sustained high transaction throughput (approaching 10K TPS)
- When disk I/O becomes saturated (common with consumer-grade SSDs)
- During state sync catch-up operations that compete for resources
- Probability: Moderate (can occur naturally in production)

**Scenario 2: Attacker-Induced**
- Attacker submits transactions with expensive operations (large write sets, complex Move computations)
- Targets state sync bottlenecks (mempool notification, storage service updates)
- Requires only transaction submission privileges (no validator access needed)
- Cost: Relatively low (gas fees for expensive transactions)
- Probability: High given attacker motivation

The timeout value is generous at 5 seconds: [10](#0-9) 

This makes the attack window realistic, as creating conditions where state sync takes >5 seconds is achievable through transaction crafting.

## Recommendation
Implement bounded channels with explicit memory limits and proper backpressure:

**Solution 1: Use Bounded Channel (Preferred)**
```rust
// In new_consensus_notifier_listener_pair()
const MAX_PENDING_NOTIFICATIONS: usize = 10;
const MAX_NOTIFICATION_SIZE_MB: usize = 60; // 10 notifications × 6MB

let (notification_sender, notification_receiver) = 
    mpsc::channel(MAX_PENDING_NOTIFICATIONS);
```

**Solution 2: Add Memory-Based Backpressure**
```rust
// Track memory usage and block when limit exceeded
struct MemoryAwareNotifier {
    sender: mpsc::UnboundedSender<ConsensusNotification>,
    pending_bytes: Arc<AtomicUsize>,
    max_pending_bytes: usize,
}

async fn notify_new_commit(&self, transactions: Vec<Transaction>, ...) -> Result<(), Error> {
    let notification_size = estimate_size(&transactions);
    
    // Wait until memory usage drops below threshold
    while self.pending_bytes.load(Ordering::Relaxed) + notification_size 
        > self.max_pending_bytes 
    {
        tokio::time::sleep(Duration::from_millis(100)).await;
    }
    
    self.pending_bytes.fetch_add(notification_size, Ordering::Relaxed);
    // ... send notification ...
    // On response, decrease pending_bytes
}
```

**Solution 3: Avoid Cloning (Most Efficient)**
```rust
// Use Arc to avoid cloning large transaction vectors
let txns = Arc::new(compute_result.transactions_to_commit().to_vec());
let events = Arc::new(compute_result.subscribable_events().to_vec());

state_sync_notifier
    .notify_new_commit(Arc::clone(&txns), Arc::clone(&events))
    .await
```

**Additional Safeguards:**
1. Add metrics for channel depth and memory usage
2. Implement circuit breaker pattern to pause consensus when state sync falls behind
3. Consider using the sequential dependency more strictly (propagate timeout errors instead of logging)

## Proof of Concept
```rust
// Rust reproduction demonstrating memory accumulation

use tokio::runtime::Runtime;
use futures::channel::mpsc;
use std::sync::Arc;
use std::time::{Duration, Instant};

#[derive(Clone)]
struct LargeTransaction {
    data: Vec<u8>, // Simulate large transaction payload
}

async fn simulate_slow_state_sync(
    mut receiver: mpsc::UnboundedReceiver<Vec<LargeTransaction>>,
    processing_delay: Duration,
) {
    while let Some(txns) = receiver.next().await {
        println!("State sync processing {} transactions ({}MB)", 
            txns.len(), 
            txns.len() * 1024 / 1_000_000
        );
        tokio::time::sleep(processing_delay).await; // Simulate slow processing
        println!("State sync completed processing");
    }
}

async fn simulate_consensus_pipeline(
    sender: mpsc::UnboundedSender<Vec<LargeTransaction>>,
    num_blocks: usize,
) {
    for block_num in 0..num_blocks {
        // Simulate transaction vector (1000 transactions of 1KB each = ~1MB per block)
        let transactions: Vec<LargeTransaction> = (0..1000)
            .map(|_| LargeTransaction { data: vec![0u8; 1024] })
            .collect();
        
        let txn_clone = transactions.clone(); // This is the problematic clone
        println!("Block {} sending notification (~1MB)", block_num);
        
        sender.unbounded_send(txn_clone).unwrap();
        
        // Wait 5 seconds (simulating timeout)
        tokio::time::sleep(Duration::from_secs(5)).await;
        
        // Report approximate memory in channel
        println!("Estimated queued notifications: {}", block_num + 1);
    }
}

fn main() {
    let rt = Runtime::new().unwrap();
    rt.block_on(async {
        let (sender, receiver) = mpsc::unbounded();
        
        // State sync processes slowly: 10 seconds per notification
        tokio::spawn(simulate_slow_state_sync(receiver, Duration::from_secs(10)));
        
        // Consensus sends notifications every 5 seconds (after timeout)
        // This means 2 notifications queued for every 1 processed
        simulate_consensus_pipeline(sender, 20).await;
        
        tokio::time::sleep(Duration::from_secs(60)).await;
    });
}

// Expected output shows notifications accumulating:
// Block 0 sending notification (~1MB)
// State sync processing 1000 transactions (1MB)
// Block 1 sending notification (~1MB)  <- Previous still processing
// Block 2 sending notification (~1MB)  <- Queue growing
// State sync completed processing
// ...
// Memory grows: 20 blocks × 1MB = 20MB in this small example
// In production: 720 blocks/hour × 6MB = 4.3GB/hour
```

**Notes:**
- This vulnerability affects all validators under the same load conditions
- The unbounded channel design violates resource limit invariants
- Actual exploitation requires either sustained high legitimate load or crafted expensive transactions
- The fix should prioritize bounded channels or memory-aware backpressure over the current timeout-only approach

### Citations

**File:** state-sync/inter-component/consensus-notifications/src/lib.rs (L62-62)
```rust
    let (notification_sender, notification_receiver) = mpsc::unbounded();
```

**File:** state-sync/inter-component/consensus-notifications/src/lib.rs (L109-113)
```rust
        if let Err(error) = self
            .notification_sender
            .clone()
            .send(commit_notification)
            .await
```

**File:** state-sync/inter-component/consensus-notifications/src/lib.rs (L122-137)
```rust
        if let Ok(response) = timeout(
            Duration::from_millis(self.commit_timeout_ms),
            callback_receiver,
        )
        .await
        {
            match response {
                Ok(consensus_notification_response) => consensus_notification_response.get_result(),
                Err(error) => Err(Error::UnexpectedErrorEncountered(format!(
                    "Consensus commit notification failure: {:?}",
                    error
                ))),
            }
        } else {
            Err(Error::TimeoutWaitingForStateSync)
        }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L1165-1166)
```rust
        let txns = compute_result.transactions_to_commit().to_vec();
        let subscribable_events = compute_result.subscribable_events().to_vec();
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L1167-1174)
```rust
        if let Err(e) = monitor!(
            "notify_state_sync",
            state_sync_notifier
                .notify_new_commit(txns, subscribable_events)
                .await
        ) {
            error!(error = ?e, "Failed to notify state synchronizer");
        }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L330-341)
```rust
        let committed_transactions = CommittedTransactions {
            events: commit_notification.get_subscribable_events().clone(),
            transactions: commit_notification.get_transactions().clone(),
        };
        utils::handle_committed_transactions(
            committed_transactions,
            self.storage.clone(),
            self.mempool_notification_handler.clone(),
            self.event_subscription_service.clone(),
            self.storage_service_notification_handler.clone(),
        )
        .await;
```

**File:** config/src/config/consensus_config.rs (L20-28)
```rust
const MAX_SENDING_BLOCK_TXNS_AFTER_FILTERING: u64 = 1800;
const MAX_SENDING_OPT_BLOCK_TXNS_AFTER_FILTERING: u64 = 1000;
const MAX_SENDING_BLOCK_TXNS: u64 = 5000;
pub(crate) static MAX_RECEIVING_BLOCK_TXNS: Lazy<u64> =
    Lazy::new(|| 10000.max(2 * MAX_SENDING_BLOCK_TXNS));
// stop reducing size at this point, so 1MB transactions can still go through
const MIN_BLOCK_BYTES_OVERRIDE: u64 = 1024 * 1024 + BATCH_PADDING_BYTES as u64;
// We should reduce block size only until two QS batch sizes.
const MIN_BLOCK_TXNS_AFTER_FILTERING: u64 = DEFEAULT_MAX_BATCH_TXNS as u64 * 2;
```

**File:** consensus/src/round_manager.rs (L1180-1193)
```rust
        ensure!(
            num_validator_txns + payload_len as u64 <= self.local_config.max_receiving_block_txns,
            "Payload len {} exceeds the limit {}",
            payload_len,
            self.local_config.max_receiving_block_txns,
        );

        ensure!(
            validator_txns_total_bytes + payload_size as u64
                <= self.local_config.max_receiving_block_bytes,
            "Payload size {} exceeds the limit {}",
            payload_size,
            self.local_config.max_receiving_block_bytes,
        );
```

**File:** config/src/config/state_sync_config.rs (L138-138)
```rust
            commit_notification_timeout_ms: 5000,
```
