# Audit Report

## Title
Non-Atomic Checkpoint Creation Allows Partial Checkpoints Without Integrity Validation

## Summary
The AptosDB checkpoint creation process lacks atomicity guarantees and completion validation. When checkpoint creation fails midway through, partial checkpoint directories remain without any marker indicating incompleteness. While attempting to restore from such checkpoints would cause node startup failures rather than silent state corruption, the lack of validation mechanisms represents a violation of operational safety guarantees.

## Finding Description

The checkpoint creation flow exhibits multiple atomicity gaps:

**1. CLI Entry Point** [1](#0-0) 

The `run()` function creates the output directory but provides no cleanup if `AptosDB::create_checkpoint` fails.

**2. Sequential Component Checkpoints** [2](#0-1) 

`AptosDB::create_checkpoint` creates multiple database component checkpoints sequentially (LedgerDb, StateKvDb, StateMerkleDb). If any component fails, earlier successful checkpoints remain without rollback.

**3. Intra-Component Shard Failures** [3](#0-2) 

Within `StateKvDb::create_checkpoint`, shard checkpoints are created in a loop. If shard 10 of 16 fails, shards 0-9 remain created.

**4. Low-Level RocksDB Checkpoint** [4](#0-3) 

The underlying `DB::create_checkpoint` delegates to RocksDB's native checkpoint API with no post-creation validation.

**Failure Scenarios:**
- Disk space exhaustion during multi-shard checkpoint creation
- Process termination (OOM, SIGKILL) between component checkpoints  
- I/O errors on specific shards or databases
- Permission errors on checkpoint directory creation

**What Prevents State Corruption:**
Upon attempting to restore from a partial checkpoint, `AptosDB::open_dbs` and component-level `open_sharded` methods would fail when encountering missing database directories or shards, causing node startup failure rather than silent state corruption.

## Impact Explanation

**Severity Assessment: Medium** (per "State inconsistencies requiring intervention")

While this does NOT meet Critical severity because:
- Partial checkpoints cannot be silently used (node fails to start)
- No attacker-controllable trigger for checkpoint creation
- Does not directly enable consensus violations or fund loss

It DOES constitute a Medium severity operational safety issue:
- Failed checkpoints leave inconsistent filesystem state requiring manual cleanup
- No completion marker prevents accidental use of partial checkpoints
- Violates operational assumptions about checkpoint atomicity
- Could cause confusion in disaster recovery scenarios
- Requires manual intervention to identify and clean up failed checkpoints

## Likelihood Explanation

**Likelihood: Medium**

Checkpoint creation failures are realistic operational events:
- Disk space exhaustion is common in long-running nodes
- Process crashes during maintenance operations occur regularly
- I/O errors on distributed storage systems are expected

However:
- Operators typically monitor checkpoint operations and notice failures
- Failed checkpoints would be caught during restoration attempts
- Modern deployment practices include health checks that would detect issues

## Recommendation

Implement checkpoint atomicity and validation:

```rust
pub fn create_checkpoint(
    db_path: impl AsRef<Path>,
    cp_path: impl AsRef<Path>,
    sharding: bool,
) -> Result<()> {
    let start = Instant::now();
    let temp_cp_path = cp_path.as_ref().with_extension(".tmp");
    
    // Create in temporary directory first
    std::fs::create_dir_all(&temp_cp_path)?;
    
    // Create all checkpoints
    LedgerDb::create_checkpoint(db_path.as_ref(), &temp_cp_path, sharding)?;
    if sharding {
        StateKvDb::create_checkpoint(db_path.as_ref(), &temp_cp_path)?;
        StateMerkleDb::create_checkpoint(db_path.as_ref(), &temp_cp_path, sharding, true)?;
    }
    StateMerkleDb::create_checkpoint(db_path.as_ref(), &temp_cp_path, sharding, false)?;
    
    // Write completion marker
    let marker_path = temp_cp_path.join("CHECKPOINT_COMPLETE");
    std::fs::write(&marker_path, format!("timestamp: {}", start.elapsed().as_secs()))?;
    
    // Atomic rename to final location
    std::fs::rename(&temp_cp_path, cp_path.as_ref())?;
    
    info!("Made AptosDB checkpoint with validation.");
    Ok(())
}

// Validation function for restoration
pub fn validate_checkpoint(cp_path: impl AsRef<Path>) -> Result<()> {
    let marker_path = cp_path.as_ref().join("CHECKPOINT_COMPLETE");
    ensure!(marker_path.exists(), "Checkpoint incomplete - missing marker file");
    // Additional validation: check all expected directories exist
    Ok(())
}
```

## Proof of Concept

```rust
#[test]
fn test_partial_checkpoint_detection() {
    use std::fs;
    use tempfile::TempDir;
    
    let source_dir = TempDir::new().unwrap();
    let checkpoint_dir = TempDir::new().unwrap();
    
    // Create a source database
    let db = AptosDB::new_for_test(&source_dir);
    drop(db);
    
    // Simulate partial checkpoint by creating only LedgerDb
    let ledger_checkpoint = checkpoint_dir.path().join("ledger_db");
    fs::create_dir_all(&ledger_checkpoint).unwrap();
    LedgerDb::create_checkpoint(&source_dir, &checkpoint_dir, false).unwrap();
    
    // Attempt to open checkpoint should fail due to missing StateKvDb
    let result = AptosDB::open(
        StorageDirPaths::from_path(&checkpoint_dir),
        false,
        PrunerConfig::default(),
        RocksdbConfigs::default(),
        false,
        100,
        100,
        None,
        HotStateConfig::default(),
    );
    
    assert!(result.is_err(), "Opening partial checkpoint should fail");
    assert!(result.unwrap_err().to_string().contains("state_kv_db"));
}
```

## Notes

**Critical Distinction:** This finding documents a **code quality and operational reliability issue**, not an exploitable security vulnerability. The system fails safely when attempting to use partial checkpoints (node startup error rather than state corruption), which prevents the worst-case scenario. However, the lack of checkpoint integrity validation violates operational safety principles and could cause issues in disaster recovery scenarios requiring manual intervention.

**Invariants:** While this does not directly violate the "State Consistency" invariant (because partial checkpoints cannot be used), it does create operational risks around checkpoint management and validation that should be addressed for production robustness.

### Citations

**File:** storage/aptosdb/src/db_debugger/checkpoint/mod.rs (L20-29)
```rust
    pub fn run(self) -> Result<()> {
        ensure!(!self.output_dir.exists(), "Output dir already exists.");
        fs::create_dir_all(&self.output_dir)?;
        let sharding_config = self.db_dir.sharding_config.clone();
        AptosDB::create_checkpoint(
            self.db_dir,
            self.output_dir,
            sharding_config.enable_storage_sharding,
        )
    }
```

**File:** storage/aptosdb/src/db/mod.rs (L172-205)
```rust
    pub fn create_checkpoint(
        db_path: impl AsRef<Path>,
        cp_path: impl AsRef<Path>,
        sharding: bool,
    ) -> Result<()> {
        let start = Instant::now();

        info!(sharding = sharding, "Creating checkpoint for AptosDB.");

        LedgerDb::create_checkpoint(db_path.as_ref(), cp_path.as_ref(), sharding)?;
        if sharding {
            StateKvDb::create_checkpoint(db_path.as_ref(), cp_path.as_ref())?;
            StateMerkleDb::create_checkpoint(
                db_path.as_ref(),
                cp_path.as_ref(),
                sharding,
                /* is_hot = */ true,
            )?;
        }
        StateMerkleDb::create_checkpoint(
            db_path.as_ref(),
            cp_path.as_ref(),
            sharding,
            /* is_hot = */ false,
        )?;

        info!(
            db_path = db_path.as_ref(),
            cp_path = cp_path.as_ref(),
            time_ms = %start.elapsed().as_millis(),
            "Made AptosDB checkpoint."
        );
        Ok(())
    }
```

**File:** storage/aptosdb/src/state_kv_db.rs (L224-259)
```rust
    pub(crate) fn create_checkpoint(
        db_root_path: impl AsRef<Path>,
        cp_root_path: impl AsRef<Path>,
    ) -> Result<()> {
        // TODO(grao): Support path override here.
        let state_kv_db = Self::open_sharded(
            &StorageDirPaths::from_path(db_root_path),
            RocksdbConfig::default(),
            None,
            None,
            false,
        )?;
        let cp_state_kv_db_path = cp_root_path.as_ref().join(STATE_KV_DB_FOLDER_NAME);

        info!("Creating state_kv_db checkpoint at: {cp_state_kv_db_path:?}");

        std::fs::remove_dir_all(&cp_state_kv_db_path).unwrap_or(());
        std::fs::create_dir_all(&cp_state_kv_db_path).unwrap_or(());

        state_kv_db
            .metadata_db()
            .create_checkpoint(Self::metadata_db_path(cp_root_path.as_ref()))?;

        // TODO(HotState): should handle hot state as well.
        for shard_id in 0..NUM_STATE_SHARDS {
            state_kv_db
                .db_shard(shard_id)
                .create_checkpoint(Self::db_shard_path(
                    cp_root_path.as_ref(),
                    shard_id,
                    /* is_hot = */ false,
                ))?;
        }

        Ok(())
    }
```

**File:** storage/schemadb/src/lib.rs (L356-362)
```rust
    pub fn create_checkpoint<P: AsRef<Path>>(&self, path: P) -> DbResult<()> {
        rocksdb::checkpoint::Checkpoint::new(&self.inner)
            .into_db_res()?
            .create_checkpoint(path)
            .into_db_res()?;
        Ok(())
    }
```
