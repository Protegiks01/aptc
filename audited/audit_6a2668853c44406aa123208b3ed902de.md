# Audit Report

## Title
Premature Abort of JWK Consensus Tasks Due to Cloneable Guard Wrapper

## Summary
The `QuorumCertProcessGuard` incorrectly implements `Clone`, allowing the abort handle to be cloned and dropped prematurely. When `maybe_start_consensus()` checks if consensus is already running, it clones the `ConsensusState` containing the guard, and upon dropping this clone, the underlying async task is aborted even though consensus should still be in progress. This causes JWK consensus to silently fail, preventing critical security updates to JWT keys from being committed to the blockchain.

## Finding Description

The vulnerability exists in the JWK consensus system's guard wrapper design. The `QuorumCertProcessGuard` struct incorrectly derives `Clone`: [1](#0-0) 

This guard wraps an `AbortHandle` and implements `Drop` to abort the async task when the guard is dropped: [2](#0-1) 

The guard is stored within `ConsensusState::InProgress`: [3](#0-2) 

The bug is triggered in `maybe_start_consensus()`. When checking if consensus is already started, the code calls `.cloned()` on the HashMap value, which clones the entire `ConsensusState::InProgress` including the `QuorumCertProcessGuard`: [4](#0-3) 

After the match expression completes, the cloned state goes out of scope and is dropped. The `Drop` implementation of the cloned guard then calls `abort()` on the cloned `AbortHandle`, which aborts the running async task. The async task is created here: [5](#0-4) 

According to Rust's `futures` library semantics, when an `AbortHandle` is cloned, all clones share the same abort signal. Calling `abort()` on any clone aborts the task. This means the consensus process is terminated even though the original guard remains in the HashMap and the function returns early believing consensus is still running.

**Comparison with Correct Pattern**: The codebase contains `DropGuard` which implements the same RAII pattern but correctly does NOT derive `Clone`: [6](#0-5) 

**Attack Scenario**:
1. JWK observer detects a key change and calls `process_new_observation()` which triggers `maybe_start_consensus()`: [7](#0-6) 

2. First call to `maybe_start_consensus()` starts the quorum certificate building process and stores the guard: [8](#0-7) 

3. Observer polls again after 10 seconds (configured polling interval): [9](#0-8) 

4. Second call to `maybe_start_consensus()` with same key clones the state at line 183, checks that consensus is already started, returns early, but the cloned state is dropped, aborting the task.

5. The quorum-certified update never completes, and `process_quorum_certified_update()` is never invoked. The main event loop waits for a `qc_update` that will never arrive: [10](#0-9) 

## Impact Explanation

This is a **High Severity** vulnerability under the Aptos bug bounty program:

1. **Critical Security Function Failure**: JWK (JSON Web Key) consensus is responsible for updating JWT verification keys used in authentication. This functionality is enabled in production when the feature flag is active: [11](#0-10) 

2. **Silent Failure**: The bug causes consensus to silently abort without error logging. Validators believe consensus is running but it has actually stopped.

3. **Authentication Impact**: If external identity providers rotate their JWT signing keys, the blockchain cannot update, potentially breaking authentication for users and applications.

4. **No Recovery Mechanism**: Once aborted, there's no automatic retry. The consensus state remains `InProgress` but the underlying task is dead, and no `qc_update` event will be received to complete the process.

5. **Affects All Validators**: Every validator running the per-key JWK consensus mode is affected identically, making this a network-wide issue.

## Likelihood Explanation

**Likelihood: Very High**

This bug triggers naturally during normal operation without any attacker involvement:

1. **Periodic Polling**: The JWK observer thread polls identity provider endpoints every 10 seconds by design.

2. **Consensus Latency**: Building a quorum certificate requires communication with 2f+1 validators over the network via reliable broadcast, which typically takes several seconds in realistic network conditions.

3. **Guaranteed Trigger**: If consensus takes longer than the 10-second polling interval (very common), the bug triggers on the second observation when `maybe_start_consensus()` is called again with the same update.

4. **Every Key Affected**: Each (issuer, kid) pair that needs updating will hit this bug independently.

5. **Production Frequency**: In production, JWK updates occur whenever identity providers rotate keys (typically monthly or weekly for security).

The bug will manifest in every deployment using per-key JWK consensus mode with any realistic network conditions.

## Recommendation

Remove the `Clone` derivation from `QuorumCertProcessGuard`:

Change line 79 of `crates/aptos-jwk-consensus/src/types.rs` from:
```rust
#[derive(Clone, Debug)]
```
to:
```rust
#[derive(Debug)]
```

Then modify `maybe_start_consensus()` to avoid cloning the state. Instead of using `.cloned()`, match on a reference:

```rust
fn maybe_start_consensus(&mut self, update: KeyLevelUpdate) -> Result<()> {
    let consensus_already_started = match self
        .states_by_key
        .get(&(update.issuer.clone(), update.kid.clone()))
    {
        Some(ConsensusState::InProgress { my_proposal, .. })
        | Some(ConsensusState::Finished { my_proposal, .. }) => {
            my_proposal.observed.to_upsert == update.to_upsert
        },
        _ => false,
    };

    if consensus_already_started {
        return Ok(());
    }
    // ... rest of the function
}
```

This matches the pattern used correctly by `DropGuard` in the reliable broadcast module.

## Proof of Concept

The vulnerability can be demonstrated by examining the execution flow:

1. Observer spawns with 10-second interval polling
2. First observation triggers consensus start, spawning an async task
3. Task begins building quorum certificate (takes >10 seconds typically)
4. Second observation after 10 seconds calls `maybe_start_consensus()` again
5. Line 183 clones the `ConsensusState::InProgress` including the guard
6. Match completes, cloned state drops, `Drop` impl calls `abort()` on cloned handle
7. Async task is aborted, never completes, `qc_update_rx` never receives the update
8. JWK consensus stuck in `InProgress` state indefinitely

This can be verified by adding logging to the `Drop` implementation and observing that it's called twice for the same consensus session - once from the premature clone drop, and once when the state is actually replaced or removed.

## Notes

This vulnerability demonstrates a critical design flaw where a RAII guard incorrectly implements `Clone`. The codebase already has the correct pattern in `DropGuard` which does not implement `Clone`. The fix is straightforward - remove the `Clone` derivation and adjust the code that unnecessarily clones the state.

### Citations

**File:** crates/aptos-jwk-consensus/src/types.rs (L79-82)
```rust
#[derive(Clone, Debug)]
pub struct QuorumCertProcessGuard {
    pub handle: AbortHandle,
}
```

**File:** crates/aptos-jwk-consensus/src/types.rs (L96-101)
```rust
impl Drop for QuorumCertProcessGuard {
    fn drop(&mut self) {
        let QuorumCertProcessGuard { handle } = self;
        handle.abort();
    }
}
```

**File:** crates/aptos-jwk-consensus/src/types.rs (L103-115)
```rust
#[derive(Debug, Clone)]
pub enum ConsensusState<T: Debug + Clone + Eq + PartialEq> {
    NotStarted,
    InProgress {
        my_proposal: T,
        abort_handle_wrapper: QuorumCertProcessGuard,
    },
    Finished {
        vtxn_guard: TxnGuard,
        my_proposal: T,
        quorum_certified: QuorumCertifiedUpdate,
    },
}
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L109-177)
```rust
    pub fn process_new_observation(&mut self, issuer: Issuer, jwks: Vec<JWK>) -> Result<()> {
        debug!(
            epoch = self.epoch_state.epoch,
            issuer = String::from_utf8(issuer.clone()).ok(),
            "Processing new observation."
        );
        let observed_jwks_by_kid: HashMap<KID, JWK> =
            jwks.into_iter().map(|jwk| (jwk.id(), jwk)).collect();
        let effectively_onchain = self
            .onchain_jwks
            .get(&issuer)
            .cloned()
            .unwrap_or_else(|| ProviderJWKsIndexed::new(issuer.clone()));
        let all_kids: HashSet<KID> = effectively_onchain
            .jwks
            .keys()
            .chain(observed_jwks_by_kid.keys())
            .cloned()
            .collect();
        for kid in all_kids {
            let onchain = effectively_onchain.jwks.get(&kid);
            let observed = observed_jwks_by_kid.get(&kid);
            match (onchain, observed) {
                (Some(x), Some(y)) => {
                    if x == y {
                        // No change, drop any in-progress consensus.
                        self.states_by_key.remove(&(issuer.clone(), kid.clone()));
                    } else {
                        // Update detected.
                        let update = KeyLevelUpdate {
                            issuer: issuer.clone(),
                            base_version: effectively_onchain.version,
                            kid: kid.clone(),
                            to_upsert: Some(y.clone()),
                        };
                        self.maybe_start_consensus(update)
                            .context("process_new_observation failed at upsert consensus init")?;
                    }
                },
                (None, Some(y)) => {
                    // Insert detected.
                    let update = KeyLevelUpdate {
                        issuer: issuer.clone(),
                        base_version: effectively_onchain.version,
                        kid: kid.clone(),
                        to_upsert: Some(y.clone()),
                    };
                    self.maybe_start_consensus(update)
                        .context("process_new_observation failed at upsert consensus init")?;
                },
                (Some(_), None) => {
                    // Delete detected.
                    let update = KeyLevelUpdate {
                        issuer: issuer.clone(),
                        base_version: effectively_onchain.version,
                        kid: kid.clone(),
                        to_upsert: None,
                    };
                    self.maybe_start_consensus(update)
                        .context("process_new_observation failed at deletion consensus init")?;
                },
                (None, None) => {
                    unreachable!("`kid` in `union(A, B)` but `kid` not in `A` and not in `B`?")
                },
            }
        }

        Ok(())
    }
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L179-194)
```rust
    fn maybe_start_consensus(&mut self, update: KeyLevelUpdate) -> Result<()> {
        let consensus_already_started = match self
            .states_by_key
            .get(&(update.issuer.clone(), update.kid.clone()))
            .cloned()
        {
            Some(ConsensusState::InProgress { my_proposal, .. })
            | Some(ConsensusState::Finished { my_proposal, .. }) => {
                my_proposal.observed.to_upsert == update.to_upsert
            },
            _ => false,
        };

        if consensus_already_started {
            return Ok(());
        }
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L207-228)
```rust
        let abort_handle = self
            .update_certifier
            .start_produce(
                self.epoch_state.clone(),
                update_translated,
                self.qc_update_tx.clone(),
            )
            .context("maybe_start_consensus failed at update_certifier.start_produce")?;

        self.states_by_key.insert(
            (update.issuer.clone(), update.kid.clone()),
            ConsensusState::InProgress {
                my_proposal: ObservedKeyLevelUpdate {
                    author: self.my_addr,
                    observed: update,
                    signature,
                },
                abort_handle_wrapper: QuorumCertProcessGuard {
                    handle: abort_handle,
                },
            },
        );
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L399-400)
```rust
                        Duration::from_secs(10),
                        local_observation_tx.clone(),
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L415-441)
```rust
        while !this.stopped {
            let handle_result = tokio::select! {
                jwk_updated = jwk_updated_rx.select_next_some() => {
                    let ObservedJWKsUpdated { jwks, .. } = jwk_updated;
                    this.reset_with_on_chain_state(jwks)
                },
                (_sender, msg) = rpc_req_rx.select_next_some() => {
                    this.process_peer_request(msg)
                },
                qc_update = this.qc_update_rx.select_next_some() => {
                    this.process_quorum_certified_update(qc_update)
                },
                (issuer, jwks) = local_observation_rx.select_next_some() => {
                    this.process_new_observation(issuer, jwks)
                },
                ack_tx = close_rx.select_next_some() => {
                    this.tear_down(ack_tx.ok()).await
                }
            };

            if let Err(e) = handle_result {
                error!(
                    epoch = this.epoch_state.epoch,
                    "KeyLevelJWKManager error from handling: {e:#}"
                );
            }
        }
```

**File:** crates/aptos-jwk-consensus/src/update_certifier.rs (L80-82)
```rust
        let (abort_handle, abort_registration) = AbortHandle::new_pair();
        tokio::spawn(Abortable::new(task, abort_registration));
        Ok(abort_handle)
```

**File:** crates/reliable-broadcast/src/lib.rs (L222-236)
```rust
pub struct DropGuard {
    abort_handle: AbortHandle,
}

impl DropGuard {
    pub fn new(abort_handle: AbortHandle) -> Self {
        Self { abort_handle }
    }
}

impl Drop for DropGuard {
    fn drop(&mut self) {
        self.abort_handle.abort();
    }
}
```

**File:** crates/aptos-jwk-consensus/src/epoch_manager.rs (L228-235)
```rust
                if features.is_enabled(FeatureFlag::JWK_CONSENSUS_PER_KEY_MODE) {
                    Box::new(KeyLevelConsensusManager::new(
                        Arc::new(my_sk),
                        self.my_addr,
                        epoch_state.clone(),
                        rb,
                        self.vtxn_pool.clone(),
                    ))
```
