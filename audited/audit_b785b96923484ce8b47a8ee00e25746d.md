# Audit Report

## Title
Memory Exhaustion via Unbounded Transaction Backup Manifest Deserialization

## Summary
The `TransactionBackup` manifest deserialization lacks limits on the number of chunks, allowing an attacker with control over backup storage to create malicious manifests that consume excessive memory during verification, potentially causing OOM crashes of the backup verification service.

## Finding Description

The vulnerability exists in the backup verification workflow where transaction backup manifests are loaded and processed without validating the number of chunks in the manifest. [1](#0-0) 

The manifest contains an unbounded `chunks` vector that is deserialized entirely into memory when loaded: [2](#0-1) 

The `load_json_file()` method uses `read_to_end()` which loads the entire file into a `Vec<u8>` without size limits, then deserializes it completely. 

During automated backup verification (scheduled via Kubernetes CronJob), the system loads manifests from configured storage: [3](#0-2) 

An attacker who compromises backup storage credentials can craft a malicious manifest with millions of `TransactionChunk` entries. When this manifest is loaded:

1. The entire JSON file is read into memory (potentially 20-30GB for compact JSON)
2. Deserialization creates Rust structs consuming an additional 30-50GB  
3. The `verify()` function iterates over all chunks without early size validation [4](#0-3) 

Memory calculation example:
- Each `TransactionChunk` in compact JSON: ~120-150 bytes
- For 200 million chunks: ~30GB JSON file  
- Deserialized struct overhead: ~400 bytes per chunk = ~80GB
- Total memory consumption: >110GB

The backup verification CronJob has a 60GB memory limit, making it vulnerable to OOM: [5](#0-4) 

## Impact Explanation

This issue represents a **Medium severity** operational security vulnerability rather than a High severity validator impact because:

1. **Not a validator crash**: The backup verification runs in a separate Kubernetes CronJob container, isolated from the validator node itself. An OOM in the verification job does not crash the validator or affect consensus.

2. **Operational impact**: The vulnerability prevents successful backup verification, which is critical for disaster recovery capabilities. Operators would be unable to verify backup integrity.

3. **Requires infrastructure compromise**: The attacker must compromise backup storage credentials (S3, GCS, etc.) or social engineer operators to verify malicious backups.

This falls under "State inconsistencies requiring intervention" (Medium severity) as it prevents proper backup verification, which is part of state management and disaster recovery procedures.

## Likelihood Explanation

**Medium likelihood** in environments with:
- Shared backup storage infrastructure
- Weak access controls on backup storage  
- Multiple operators with backup storage access

**Low likelihood** in well-secured deployments with:
- Strict IAM policies on backup storage
- Regular security audits
- Limited operator access

The attack requires compromising backup storage infrastructure, which is non-trivial but achievable through credential theft, misconfigured IAM policies, or supply chain attacks.

## Recommendation

Add validation for maximum chunk count in the `TransactionBackup::verify()` method:

```rust
// In manifest.rs, add constant
const MAX_CHUNKS_PER_MANIFEST: usize = 1_000_000; // Reasonable limit for legitimate backups

impl TransactionBackup {
    pub fn verify(&self) -> Result<()> {
        // Add chunk count validation
        ensure!(
            self.chunks.len() <= MAX_CHUNKS_PER_MANIFEST,
            "Too many chunks in manifest: {}. Maximum allowed: {}",
            self.chunks.len(),
            MAX_CHUNKS_PER_MANIFEST,
        );
        
        // existing validations...
        ensure!(
            self.first_version <= self.last_version,
            "Bad version range: [{}, {}]",
            self.first_version,
            self.last_version,
        );
        // ...rest of existing code
    }
}
```

Additionally, implement file size validation in `load_json_file()`:

```rust
// In storage_ext.rs
const MAX_MANIFEST_SIZE_BYTES: usize = 100 * 1024 * 1024; // 100MB

async fn load_json_file<T: DeserializeOwned>(&self, file_handle: &FileHandleRef) -> Result<T> {
    let bytes = self.read_all(file_handle).await?;
    ensure!(
        bytes.len() <= MAX_MANIFEST_SIZE_BYTES,
        "Manifest file too large: {} bytes. Maximum: {} bytes",
        bytes.len(),
        MAX_MANIFEST_SIZE_BYTES,
    );
    Ok(serde_json::from_slice(&bytes)?)
}
```

## Proof of Concept

Create a malicious manifest generator:

```rust
use serde_json::json;
use std::fs::File;
use std::io::Write;

fn main() -> anyhow::Result<()> {
    let mut chunks = Vec::new();
    
    // Create 50 million minimal chunks (will generate ~7GB JSON)
    for i in 0..50_000_000 {
        chunks.push(json!({
            "first_version": i,
            "last_version": i,
            "transactions": format!("tx_{}.chunk", i),
            "proof": format!("tx_{}.proof", i),
            "format": "V1"
        }));
    }
    
    let manifest = json!({
        "first_version": 0,
        "last_version": 49_999_999,
        "chunks": chunks
    });
    
    let mut file = File::create("malicious_manifest.json")?;
    file.write_all(serde_json::to_string(&manifest)?.as_bytes())?;
    
    println!("Created malicious manifest with 50M chunks");
    Ok(())
}
```

Deploy this to compromised backup storage and wait for the automated CronJob to attempt verification. The verification process will attempt to load the entire manifest, consuming >60GB memory and triggering OOM kill.

**Notes**

This vulnerability exists but has limited real-world impact due to architectural isolation. The backup verification service runs in a separate container with resource limits, preventing it from affecting validator node operation. However, it does represent a denial-of-service vector against backup verification capabilities, which could be exploited to hide backup corruption or prevent disaster recovery validation.

The vulnerability requires compromising backup storage infrastructure rather than exploiting a pure code-level bug, placing it at the boundary between operational security and software vulnerability. For high-security deployments, implementing the recommended limits would provide defense-in-depth against storage compromise scenarios.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/transaction/manifest.rs (L42-47)
```rust
#[derive(Deserialize, Serialize)]
pub struct TransactionBackup {
    pub first_version: Version,
    pub last_version: Version,
    pub chunks: Vec<TransactionChunk>,
}
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/manifest.rs (L50-64)
```rust
    pub fn verify(&self) -> Result<()> {
        // check number of waypoints
        ensure!(
            self.first_version <= self.last_version,
            "Bad version range: [{}, {}]",
            self.first_version,
            self.last_version,
        );

        // check chunk ranges
        ensure!(!self.chunks.is_empty(), "No chunks.");

        let mut next_version = self.first_version;
        for chunk in &self.chunks {
            ensure!(
```

**File:** storage/backup/backup-cli/src/utils/storage_ext.rs (L24-37)
```rust
    async fn read_all(&self, file_handle: &FileHandleRef) -> Result<Vec<u8>> {
        let mut file = self.open_for_read(file_handle).await?;
        let mut bytes = Vec::new();
        file.read_to_end(&mut bytes).await?;
        Ok(bytes)
    }

    async fn load_bcs_file<T: DeserializeOwned>(&self, file_handle: &FileHandleRef) -> Result<T> {
        Ok(bcs::from_bytes(&self.read_all(file_handle).await?)?)
    }

    async fn load_json_file<T: DeserializeOwned>(&self, file_handle: &FileHandleRef) -> Result<T> {
        Ok(serde_json::from_slice(&self.read_all(file_handle).await?)?)
    }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L341-353)
```rust
    fn loaded_chunk_stream(&self) -> Peekable<impl Stream<Item = Result<LoadedChunk>> + use<>> {
        let con = self.global_opt.concurrent_downloads;

        let manifest_handle_stream = stream::iter(self.manifest_handles.clone());

        let storage = self.storage.clone();
        let manifest_stream = manifest_handle_stream
            .map(move |hdl| {
                let storage = storage.clone();
                async move { storage.load_json_file(&hdl).await.err_notes(&hdl) }
            })
            .buffered_x(con * 3, con)
            .and_then(|m: TransactionBackup| future::ready(m.verify().map(|_| m)));
```

**File:** terraform/helm/fullnode/values.yaml (L155-159)
```yaml
  resources:
    limits:
      cpu: 32
      memory: 60Gi
    requests:
```
