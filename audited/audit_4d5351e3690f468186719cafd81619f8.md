# Audit Report

## Title
Consensus Observer Subscription Prioritization via Stale Network Distance Data

## Summary
The consensus observer's `get_distance_for_peer()` function retrieves peer distance from validators without any timestamp validation or freshness check. When network information requests fail or timeout, stale distance data persists indefinitely, allowing attackers to maintain favorable subscription priority even after disconnecting from validators.

## Finding Description

The consensus observer selects peers for subscriptions based on their reported distance from the validator set, prioritizing peers with lower distances. This distance information comes from `latest_network_info_response` in the peer monitoring metadata. [1](#0-0) 

The critical flaw exists in the data structures and update mechanisms:

1. **No timestamp in data structures**: Neither `PeerMonitoringMetadata` nor `NetworkInformationResponse` contains timestamp fields to track when the distance data was collected. [2](#0-1) [3](#0-2) 

2. **Stale data not cleared on failure**: When network information requests fail or timeout, the failure handler only increments a counter but never clears the outdated `recorded_network_info_response`. [4](#0-3) 

3. **Distance used for prioritization**: The stale distance is used to sort peers for subscription selection, with lower distances receiving higher priority. [5](#0-4) 

4. **Ongoing optimality checks**: The same stale distance is used during periodic subscription optimality checks to determine if subscriptions should be maintained. [6](#0-5) 

**Attack Scenario:**

1. Attacker runs a VFN peer that initially connects to validators, establishing `distance_from_validators = 1`
2. Consensus observer queries this peer and records distance = 1 in `latest_network_info_response`
3. Attacker disconnects from validators (actual distance becomes 100 or MAX_DISTANCE_FROM_VALIDATORS)
4. Attacker either stops responding to network info requests OR continues responding with stale/false distance = 1
5. Observer's peer monitoring service fails to get fresh data (timeouts/failures accumulate)
6. The old `latest_network_info_response` with distance = 1 is never cleared or invalidated
7. During subscription selection and optimality checks, observer still sees distance = 1
8. Attacker peer maintains high priority despite being disconnected from validators
9. Observer subscribes to attacker's degraded peer instead of well-connected alternatives

## Impact Explanation

**Severity: Medium** - This vulnerability causes state inconsistencies requiring intervention but does not directly compromise consensus safety or cause fund loss.

Impact on the consensus observer:
- **Degraded Consensus Data Quality**: Observer subscribes to poorly connected peers, receiving delayed or incomplete consensus information
- **Suboptimal Peer Selection**: Well-connected peers are deprioritized in favor of peers with stale favorable distance metrics
- **Subscription Churn**: When the subscription eventually fails (via timeout checks), the observer must resubscribe, causing gaps in consensus observation
- **Multiple Attack Vectors**: Multiple attackers with stale data can crowd out all legitimate well-connected peers from subscription consideration

This matches the **Medium Severity** category: "State inconsistencies requiring intervention" - the observer's view of consensus becomes inconsistent with reality, and operators may need to manually intervene by restarting the node or clearing peer metadata to restore proper subscription selection.

## Likelihood Explanation

**Likelihood: Medium-High**

The vulnerability requires:
- Attacker initially establishes legitimate low distance (distance = 1 for VFN) - **Easy**: Any VFN can achieve this
- Attacker loses or deliberately severs validator connections - **Easy**: Network disruptions or intentional disconnection
- Observer fails to detect stale data - **Guaranteed**: No timestamp checks exist

Factors increasing likelihood:
- Network information refresh interval is 60 seconds by default, creating windows where stale data is used
- Request failures are common in distributed systems (network partitions, timeouts, peer overload)
- No mechanism exists to age-out or invalidate old distance data
- Subscription refresh interval is 10 minutes, allowing prolonged use of stale data [7](#0-6) 

Factors limiting exploitation:
- Subscription timeout (15 seconds) will eventually detect non-responsive peers
- Syncing progress checks may detect degraded peers if they provide bad consensus data
- But these checks happen AFTER subscription is established, wasting resources

## Recommendation

Implement timestamp-based freshness validation for network information responses:

**Solution 1: Add timestamp to NetworkInformationResponse**
```rust
// In peer-monitoring-service/types/src/response.rs
#[derive(Clone, Debug, Deserialize, Eq, PartialEq, Serialize)]
pub struct NetworkInformationResponse {
    pub connected_peers: BTreeMap<PeerNetworkId, ConnectionMetadata>,
    pub distance_from_validators: u64,
    pub timestamp_usecs: u64, // Add timestamp field
}
```

**Solution 2: Clear stale data on consecutive failures**
```rust
// In peer-monitoring-service/client/src/peer_states/network_info.rs
fn handle_request_failure(&mut self) {
    self.request_tracker.write().record_response_failure();
    
    // Clear stale data after threshold consecutive failures
    let num_failures = self.request_tracker.read().get_num_consecutive_failures();
    if num_failures >= MAX_TOLERABLE_FAILURES {
        self.recorded_network_info_response = None;
    }
}
```

**Solution 3: Validate freshness in get_distance_for_peer()**
```rust
// In consensus/src/consensus_observer/observer/subscription_utils.rs
fn get_distance_for_peer(
    peer_network_id: &PeerNetworkId,
    peer_metadata: &PeerMetadata,
    time_service: &TimeService,
) -> Option<u64> {
    let peer_monitoring_metadata = peer_metadata.get_peer_monitoring_metadata();
    let network_info = peer_monitoring_metadata.latest_network_info_response.as_ref()?;
    
    // Validate timestamp freshness
    let current_time_usecs = time_service.now_unix_time().as_micros();
    let age_usecs = current_time_usecs.saturating_sub(network_info.timestamp_usecs);
    const MAX_STALENESS_USECS: u64 = 300_000_000; // 5 minutes
    
    if age_usecs > MAX_STALENESS_USECS {
        warn!("Rejecting stale distance data for peer {:?}, age: {}s", 
              peer_network_id, age_usecs / 1_000_000);
        return None;
    }
    
    Some(network_info.distance_from_validators)
}
```

**Recommended approach**: Implement all three solutions for defense in depth.

## Proof of Concept

```rust
// Consensus observer subscription test demonstrating stale data usage
#[tokio::test]
async fn test_stale_distance_maintains_priority() {
    // Setup: Create observer with mock time service
    let consensus_observer_config = ConsensusObserverConfig::default();
    let network_ids = &[NetworkId::Validator, NetworkId::Vfn];
    let (peers_and_metadata, consensus_observer_client, _) =
        create_consensus_observer_client(network_ids);
    let time_service = TimeService::mock();
    
    // Step 1: Create VFN peer with good distance (distance = 1)
    let vfn_peer = create_peer_and_connection(
        NetworkId::Vfn,
        peers_and_metadata.clone(),
        1, // Initial distance from validators
        Some(0.1), // Low latency
        true,
    );
    
    // Step 2: Get connected peers and verify VFN has good distance
    let connected_peers = peers_and_metadata.get_connected_peers_and_metadata().unwrap();
    let vfn_metadata = connected_peers.get(&vfn_peer).unwrap();
    assert_eq!(
        get_distance_for_peer(&vfn_peer, vfn_metadata),
        Some(1)
    );
    
    // Step 3: Simulate VFN disconnecting from validators
    // In reality, VFN's distance becomes 100, but we can't update it
    // because network info requests will fail/timeout
    
    // Step 4: Advance time significantly (simulating request failures)
    let mock_time = time_service.into_mock();
    mock_time.advance(Duration::from_secs(3600)); // 1 hour passes
    
    // Step 5: Verify stale distance is still used
    let connected_peers = peers_and_metadata.get_connected_peers_and_metadata().unwrap();
    let vfn_metadata = connected_peers.get(&vfn_peer).unwrap();
    
    // BUG: Distance = 1 is still returned despite being stale
    assert_eq!(
        get_distance_for_peer(&vfn_peer, vfn_metadata),
        Some(1) // Should be None or MAX_DISTANCE, but returns stale value
    );
    
    // Step 6: Verify peer maintains high priority in subscription selection
    let sorted_peers = sort_peers_by_subscription_optimality(&connected_peers);
    
    // BUG: VFN peer is prioritized due to stale distance = 1
    assert_eq!(sorted_peers[0], vfn_peer); // Should be deprioritized!
}
```

## Notes

This vulnerability is confirmed valid and exploitable. The lack of timestamp validation on network distance data allows attackers to maintain artificially favorable subscription priority after losing connectivity to validators. While subscription timeout mechanisms provide eventual detection, the observer wastes resources subscribing to degraded peers and misses opportunities to connect to actually well-connected alternatives. The 10-minute subscription refresh interval and 3-minute peer change check interval create extended windows for exploitation.

### Citations

**File:** consensus/src/consensus_observer/observer/subscription_utils.rs (L196-218)
```rust
fn get_distance_for_peer(
    peer_network_id: &PeerNetworkId,
    peer_metadata: &PeerMetadata,
) -> Option<u64> {
    // Get the distance for the peer
    let peer_monitoring_metadata = peer_metadata.get_peer_monitoring_metadata();
    let distance = peer_monitoring_metadata
        .latest_network_info_response
        .as_ref()
        .map(|response| response.distance_from_validators);

    // If the distance is missing, log a warning
    if distance.is_none() {
        warn!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "Unable to get distance for peer! Peer: {:?}",
                peer_network_id
            ))
        );
    }

    distance
}
```

**File:** consensus/src/consensus_observer/observer/subscription_utils.rs (L283-312)
```rust
pub fn sort_peers_by_subscription_optimality(
    peers_and_metadata: &HashMap<PeerNetworkId, PeerMetadata>,
) -> Vec<PeerNetworkId> {
    // Group peers and latencies by validator distance, i.e., distance -> [(peer, latency)]
    let mut unsupported_peers = Vec::new();
    let mut peers_and_latencies_by_distance = BTreeMap::new();
    for (peer_network_id, peer_metadata) in peers_and_metadata {
        // Verify that the peer supports consensus observer
        if !supports_consensus_observer(peer_metadata) {
            unsupported_peers.push(*peer_network_id);
            continue; // Skip the peer
        }

        // Get the distance and latency for the peer
        let distance = get_distance_for_peer(peer_network_id, peer_metadata);
        let latency = get_latency_for_peer(peer_network_id, peer_metadata);

        // If the distance is not found, use the maximum distance
        let distance =
            distance.unwrap_or(aptos_peer_monitoring_service_types::MAX_DISTANCE_FROM_VALIDATORS);

        // If the latency is not found, use a large latency
        let latency = latency.unwrap_or(MAX_PING_LATENCY_SECS);

        // Add the peer and latency to the distance group
        peers_and_latencies_by_distance
            .entry(distance)
            .or_insert_with(Vec::new)
            .push((*peer_network_id, OrderedFloat(latency)));
    }
```

**File:** peer-monitoring-service/types/src/lib.rs (L44-51)
```rust
#[derive(Clone, Default, Deserialize, PartialEq, Serialize)]
pub struct PeerMonitoringMetadata {
    pub average_ping_latency_secs: Option<f64>, // The average latency ping for the peer
    pub latest_ping_latency_secs: Option<f64>,  // The latest latency ping for the peer
    pub latest_network_info_response: Option<NetworkInformationResponse>, // The latest network info response
    pub latest_node_info_response: Option<NodeInformationResponse>, // The latest node info response
    pub internal_client_state: Option<String>, // A detailed client state string for debugging and logging
}
```

**File:** peer-monitoring-service/types/src/response.rs (L51-55)
```rust
#[derive(Clone, Debug, Deserialize, Eq, PartialEq, Serialize)]
pub struct NetworkInformationResponse {
    pub connected_peers: BTreeMap<PeerNetworkId, ConnectionMetadata>, // Connected peers
    pub distance_from_validators: u64, // The distance of the peer from the validator set
}
```

**File:** peer-monitoring-service/client/src/peer_states/network_info.rs (L66-69)
```rust
    /// Handles a request failure for the specified peer
    fn handle_request_failure(&self) {
        self.request_tracker.write().record_response_failure();
    }
```

**File:** consensus/src/consensus_observer/observer/subscription.rs (L100-162)
```rust
    fn check_subscription_peer_optimality(
        &mut self,
        peers_and_metadata: &HashMap<PeerNetworkId, PeerMetadata>,
        skip_peer_optimality_check: bool,
    ) -> Result<(), Error> {
        // Get the last optimality check time and connected peers
        let (last_optimality_check_time, last_optimality_check_peers) =
            self.last_optimality_check_time_and_peers.clone();

        // If we're skipping the peer optimality check, update the last check time and return
        let time_now = self.time_service.now();
        if skip_peer_optimality_check {
            self.last_optimality_check_time_and_peers = (time_now, last_optimality_check_peers);
            return Ok(());
        }

        // Determine if enough time has elapsed to force a refresh
        let duration_since_last_check = time_now.duration_since(last_optimality_check_time);
        let refresh_interval = Duration::from_millis(
            self.consensus_observer_config
                .subscription_refresh_interval_ms,
        );
        let force_refresh = duration_since_last_check >= refresh_interval;

        // Determine if the peers have changed since the last check.
        // Note: we only check for peer changes periodically to avoid
        // excessive subscription churn due to peer connects/disconnects.
        let current_connected_peers = peers_and_metadata.keys().cloned().collect();
        let peer_check_interval = Duration::from_millis(
            self.consensus_observer_config
                .subscription_peer_change_interval_ms,
        );
        let peers_changed = duration_since_last_check >= peer_check_interval
            && current_connected_peers != last_optimality_check_peers;

        // Determine if we should perform the optimality check
        if !force_refresh && !peers_changed {
            return Ok(()); // We don't need to check optimality yet
        }

        // Otherwise, update the last peer optimality check time and peers
        self.last_optimality_check_time_and_peers = (time_now, current_connected_peers);

        // Sort the peers by subscription optimality
        let sorted_peers =
            subscription_utils::sort_peers_by_subscription_optimality(peers_and_metadata);

        // Verify that this peer is one of the most optimal peers
        let max_concurrent_subscriptions =
            self.consensus_observer_config.max_concurrent_subscriptions as usize;
        if !sorted_peers
            .iter()
            .take(max_concurrent_subscriptions)
            .any(|peer| peer == &self.peer_network_id)
        {
            return Err(Error::SubscriptionSuboptimal(format!(
                "Subscription to peer: {} is no longer optimal! New optimal peers: {:?}",
                self.peer_network_id, sorted_peers
            )));
        }

        Ok(())
    }
```

**File:** config/src/config/consensus_observer_config.rs (L63-84)
```rust
impl Default for ConsensusObserverConfig {
    fn default() -> Self {
        Self {
            observer_enabled: false,
            publisher_enabled: false,
            max_network_channel_size: 1000,
            max_parallel_serialization_tasks: num_cpus::get(), // Default to the number of CPUs
            network_request_timeout_ms: 5_000,                 // 5 seconds
            garbage_collection_interval_ms: 60_000,            // 60 seconds
            max_num_pending_blocks: 150, // 150 blocks (sufficient for existing production networks)
            progress_check_interval_ms: 5_000, // 5 seconds
            max_concurrent_subscriptions: 2, // 2 streams should be sufficient
            max_subscription_sync_timeout_ms: 15_000, // 15 seconds
            max_subscription_timeout_ms: 15_000, // 15 seconds
            subscription_peer_change_interval_ms: 180_000, // 3 minutes
            subscription_refresh_interval_ms: 600_000, // 10 minutes
            observer_fallback_duration_ms: 600_000, // 10 minutes
            observer_fallback_startup_period_ms: 60_000, // 60 seconds
            observer_fallback_progress_threshold_ms: 10_000, // 10 seconds
            observer_fallback_sync_lag_threshold_ms: 15_000, // 15 seconds
        }
    }
```
