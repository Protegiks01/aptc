# Audit Report

## Title
Sharded Executor Starvation: Missing Timeout Allows Single Slow Shard to Cause Complete Chain Halt

## Summary
The sharded block executor lacks timeout protection when waiting for shard execution results, allowing a single slow, crashed, or malicious shard to indefinitely block all block execution and halt the entire blockchain.

## Finding Description

The sharded executor coordinator waits for results from all shards using blocking channel receives without any timeout mechanism. This creates a critical single point of failure where any single shard can halt the entire chain.

**Execution Flow:**
1. Consensus calls `execute_and_update_state` on the block executor [1](#0-0) 

2. This triggers `DoGetExecutionOutput::execute_block_sharded` which delegates to the sharded executor [2](#0-1) 

3. The `ShardedBlockExecutor::execute_block` calls the executor client to dispatch work to shards [3](#0-2) 

4. For local execution, `LocalExecutorClient::execute_block` sends commands to all shards, then calls `get_output_from_shards()` to wait for results [4](#0-3) 

**The Critical Vulnerability** lies in `get_output_from_shards()`: [5](#0-4) 

The coordinator iterates through all shards and performs a **blocking** `recv()` on each result channel without any timeout. If any shard fails to send a result (due to crash, hang, or malicious behavior), the coordinator blocks **indefinitely** at that `recv()` call.

The same vulnerability exists in remote execution: [6](#0-5) 

**Attack/Failure Scenarios:**
- **Bug/Crash**: A shard thread crashes or panics before calling `send_execution_result`, leaving the coordinator waiting forever [7](#0-6) 

- **Resource Exhaustion**: A shard encounters an infinite loop, deadlock, or runs out of memory during execution
- **Malicious Shard**: A compromised remote shard operator deliberately withholds results [8](#0-7) 

- **Network Partition**: In remote sharding, network issues prevent result delivery

**No Timeout Protection At Any Level:**
The blocking occurs in `tokio::task::spawn_blocking` without timeout: [9](#0-8) 

There is no timeout mechanism in the consensus layer, executor layer, or shard coordination layer.

## Impact Explanation

**Severity: High (Total Loss of Liveness)**

This vulnerability causes complete blockchain halt:

1. **Consensus Starvation**: When one shard fails to respond, the coordinator blocks indefinitely waiting for results. This prevents the block from being executed.

2. **Chain Halt**: Since consensus cannot proceed without successful block execution, the entire blockchain stops making progress. No new blocks can be committed.

3. **All Shards Affected**: Even though only one shard is problematic, all other healthy shards are also blocked waiting, wasting their computational resources.

4. **No Recovery**: Without external intervention (node restart), the system cannot recover automatically. The blocking `recv()` has no timeout or cancellation mechanism.

This meets the **High Severity** criteria from the Aptos Bug Bounty program: "Validator node slowdowns" and "Significant protocol violations". It could also be argued as **Critical Severity** ("Total loss of liveness/network availability") since the entire chain halts.

## Likelihood Explanation

**Likelihood: Medium-to-High**

This vulnerability is likely to occur because:

1. **No Attacker Required**: Simple bugs, panics, or resource exhaustion on any shard can trigger this. Every shard execution is a potential failure point.

2. **Complexity of Sharded Execution**: The sharded executor involves complex concurrent execution with cross-shard communication, state synchronization, and dependency management - all increasing the probability of edge cases and bugs.

3. **Multiple Trigger Points**: The vulnerability can be triggered at any point where a shard thread could fail:
   - During transaction execution
   - During cross-shard message handling  
   - During state view operations
   - Due to memory/resource limits

4. **Production Risk**: In remote sharded deployments, network issues, hardware failures, or operational errors can cause shards to become unresponsive.

While the sharded executor may not be enabled by default in all deployments, when it is used (especially for high-throughput scenarios), this vulnerability poses a significant risk.

## Recommendation

Implement timeout protection with automatic retry/fallback mechanisms:

```rust
fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
    let _timer = WAIT_FOR_SHARDED_OUTPUT_SECONDS.start_timer();
    trace!("LocalExecutorClient Waiting for results");
    let mut results = vec![];
    
    for (i, rx) in self.result_rxs.iter().enumerate() {
        // Add configurable timeout (e.g., 30 seconds)
        let timeout_duration = Duration::from_secs(30);
        
        match rx.recv_timeout(timeout_duration) {
            Ok(result) => {
                results.push(result?);
            },
            Err(RecvTimeoutError::Timeout) => {
                error!("Shard {} execution timeout after {:?}", i, timeout_duration);
                // Increment timeout counter for monitoring
                SHARD_TIMEOUT_COUNT.inc();
                return Err(VMStatus::Error(
                    StatusCode::UNKNOWN_INVARIANT_VIOLATION_ERROR,
                    Some("Shard execution timeout".to_string())
                ));
            },
            Err(RecvTimeoutError::Disconnected) => {
                error!("Shard {} channel disconnected", i);
                return Err(VMStatus::Error(
                    StatusCode::UNKNOWN_INVARIANT_VIOLATION_ERROR, 
                    Some(format!("Shard {} disconnected", i))
                ));
            }
        }
    }
    Ok(results)
}
```

**Additional Improvements:**
1. Add health monitoring and metrics for shard execution times
2. Implement graceful degradation (fallback to non-sharded execution on timeout)
3. Add shard health checks and automatic shard replacement/restart
4. Consider using `select!` to wait for all shards concurrently rather than sequentially
5. Add configuration for timeout duration based on network/deployment characteristics

## Proof of Concept

```rust
#[cfg(test)]
mod test_shard_starvation {
    use super::*;
    use std::sync::Arc;
    use std::thread;
    use std::time::Duration;
    use crossbeam_channel::unbounded;
    
    #[test]
    #[should_panic(timeout = "5s")]
    fn test_single_shard_blocks_coordinator() {
        // Setup: Create 3 shards
        let num_shards = 3;
        let (command_txs, command_rxs): (Vec<_>, Vec<_>) = 
            (0..num_shards).map(|_| unbounded()).unzip();
        let (result_txs, result_rxs): (Vec<_>, Vec<_>) = 
            (0..num_shards).map(|_| unbounded()).unzip();
        
        // Spawn shard workers
        for i in 0..num_shards {
            let cmd_rx = command_rxs[i].clone();
            let res_tx = result_txs[i].clone();
            
            thread::spawn(move || {
                // Shard 1 deliberately hangs (simulating crash/malicious behavior)
                if i == 1 {
                    thread::sleep(Duration::from_secs(3600)); // Never responds
                    return;
                }
                
                // Other shards work normally
                while let Ok(_cmd) = cmd_rx.recv() {
                    // Simulate execution
                    thread::sleep(Duration::from_millis(100));
                    // Send result
                    res_tx.send(Ok(vec![vec![]])).unwrap();
                }
            });
        }
        
        // Coordinator sends work to all shards
        for tx in &command_txs {
            tx.send(ExecutorShardCommand::ExecuteSubBlocks(
                Arc::new(mock_state_view),
                mock_sub_blocks,
                8,
                mock_config,
            )).unwrap();
        }
        
        // Try to get results - THIS WILL HANG INDEFINITELY
        // waiting for shard 1 which never responds
        let mut results = vec![];
        for (i, rx) in result_rxs.iter().enumerate() {
            println!("Waiting for shard {}", i);
            // This recv() blocks forever on shard 1
            results.push(rx.recv().unwrap());
        }
        
        // This line is never reached - test times out demonstrating DoS
        assert_eq!(results.len(), num_shards);
    }
}
```

**Demonstration Steps:**
1. Configure a sharded executor with multiple shards (local or remote)
2. Inject a fault in one shard (e.g., add `thread::sleep` before sending results, or trigger a panic)
3. Submit a block for execution
4. Observe that the coordinator hangs indefinitely at `get_output_from_shards()`
5. Verify that consensus cannot proceed and no new blocks are committed
6. Monitor that all other healthy shards are also stuck waiting

This proves that a single shard failure causes total loss of liveness across the entire blockchain.

## Notes

The vulnerability affects both **local sharded execution** (multi-threaded) and **remote sharded execution** (distributed). While remote execution requires a malicious shard operator to exploit deliberately, local execution can be triggered by simple bugs or resource exhaustion, making this a realistic threat even without malicious actors.

The lack of timeout protection violates the fundamental availability requirement of a blockchain system - the ability to make progress even in the presence of faults. This represents a critical gap in the fault tolerance design of the sharded executor.

### Citations

**File:** consensus/src/pipeline/pipeline_builder.rs (L857-868)
```rust
        tokio::task::spawn_blocking(move || {
            executor
                .execute_and_update_state(
                    (block.id(), txns, auxiliary_info).into(),
                    block.parent_id(),
                    onchain_execution_config,
                )
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
        Ok(start.elapsed())
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L256-276)
```rust
    fn execute_block_sharded<V: VMBlockExecutor>(
        partitioned_txns: PartitionedTransactions,
        state_view: Arc<CachedStateView>,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<Vec<TransactionOutput>> {
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        } else {
            Ok(V::execute_block_sharded(
                &SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        }
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/mod.rs (L70-94)
```rust
    pub fn execute_block(
        &self,
        state_view: Arc<S>,
        transactions: PartitionedTransactions,
        concurrency_level_per_shard: usize,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<Vec<TransactionOutput>, VMStatus> {
        let _timer = SHARDED_BLOCK_EXECUTION_SECONDS.start_timer();
        let num_executor_shards = self.executor_client.num_shards();
        NUM_EXECUTOR_SHARDS.set(num_executor_shards as i64);
        assert_eq!(
            num_executor_shards,
            transactions.num_shards(),
            "Block must be partitioned into {} sub-blocks",
            num_executor_shards
        );
        let (sharded_output, global_output) = self
            .executor_client
            .execute_block(
                state_view,
                transactions,
                concurrency_level_per_shard,
                onchain_config,
            )?
            .into_inner();
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L164-175)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        let _timer = WAIT_FOR_SHARDED_OUTPUT_SECONDS.start_timer();
        trace!("LocalExecutorClient Waiting for results");
        let mut results = vec![];
        for (i, rx) in self.result_rxs.iter().enumerate() {
            results.push(
                rx.recv()
                    .unwrap_or_else(|_| panic!("Did not receive output from shard {}", i))?,
            );
        }
        Ok(results)
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L183-213)
```rust
    fn execute_block(
        &self,
        state_view: Arc<S>,
        transactions: PartitionedTransactions,
        concurrency_level_per_shard: usize,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<ShardedExecutionOutput, VMStatus> {
        assert_eq!(transactions.num_shards(), self.num_shards());
        let (sub_blocks, global_txns) = transactions.into();
        for (i, sub_blocks_for_shard) in sub_blocks.into_iter().enumerate() {
            self.command_txs[i]
                .send(ExecutorShardCommand::ExecuteSubBlocks(
                    state_view.clone(),
                    sub_blocks_for_shard,
                    concurrency_level_per_shard,
                    onchain_config.clone(),
                ))
                .unwrap();
        }

        // This means that we are executing the global transactions concurrently with the individual shards but the
        // global transactions will be blocked for cross shard transaction results. This hopefully will help with
        // finishing the global transactions faster but we need to evaluate if this causes thread contention. If it
        // does, then we can simply move this call to the end of the function.
        let mut global_output = self.global_executor.execute_global_txns(
            global_txns,
            state_view.as_ref(),
            onchain_config,
        )?;

        let mut sharded_output = self.get_output_from_shards()?;
```

**File:** execution/executor-service/src/remote_executor_client.rs (L163-172)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        trace!("RemoteExecutorClient Waiting for results");
        let mut results = vec![];
        for rx in self.result_rxs.iter() {
            let received_bytes = rx.recv().unwrap().to_bytes();
            let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
            results.push(result.inner?);
        }
        Ok(results)
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/coordinator_client.rs (L9-13)
```rust
pub trait CoordinatorClient<S: StateView + Sync + Send + 'static>: Send + Sync {
    fn receive_execute_command(&self) -> ExecutorShardCommand<S>;

    fn send_execution_result(&self, result: Result<Vec<Vec<TransactionOutput>>, VMStatus>);
}
```

**File:** execution/executor-service/src/remote_executor_service.rs (L15-55)
```rust
pub struct ExecutorService {
    shard_id: ShardId,
    controller: NetworkController,
    executor_service: Arc<ShardedExecutorService<RemoteStateViewClient>>,
}

impl ExecutorService {
    pub fn new(
        shard_id: ShardId,
        num_shards: usize,
        num_threads: usize,
        self_address: SocketAddr,
        coordinator_address: SocketAddr,
        remote_shard_addresses: Vec<SocketAddr>,
    ) -> Self {
        let service_name = format!("executor_service-{}", shard_id);
        let mut controller = NetworkController::new(service_name, self_address, 5000);
        let coordinator_client = Arc::new(RemoteCoordinatorClient::new(
            shard_id,
            &mut controller,
            coordinator_address,
        ));
        let cross_shard_client = Arc::new(RemoteCrossShardClient::new(
            &mut controller,
            remote_shard_addresses,
        ));

        let executor_service = Arc::new(ShardedExecutorService::new(
            shard_id,
            num_shards,
            num_threads,
            coordinator_client,
            cross_shard_client,
        ));

        Self {
            shard_id,
            controller,
            executor_service,
        }
    }
```
