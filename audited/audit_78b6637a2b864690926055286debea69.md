# Audit Report

## Title
Out-of-Bounds Array Access in Remote State View Service Causing Denial of Service

## Summary
The `RemoteStateViewService::handle_message()` function performs an unchecked array access using a deserialized `shard_id` from network messages, leading to a panic when `shard_id >= kv_tx.len()`. This creates a denial of service vulnerability that can crash the coordinator's state view service thread when shard configuration is inconsistent or when malicious messages are received.

## Finding Description

The vulnerability exists in the remote state view service, which is part of Aptos's sharded block execution infrastructure. The issue manifests in two related locations:

**Primary Vulnerability**: In [1](#0-0) , the code performs an unchecked array indexing operation using `shard_id` extracted from a deserialized network message.

The vulnerable code path is:

1. The `RemoteStateViewService` is initialized with `remote_shard_addresses` parameter [2](#0-1) 

2. A `kv_tx` vector is created with length equal to `remote_shard_addresses.len()` [3](#0-2) 

3. When handling incoming messages, a `RemoteKVRequest` is deserialized from untrusted network data [4](#0-3) 

4. The `shard_id` field from this request (defined as `pub type ShardId = usize` [5](#0-4) ) can have any value from 0 to `usize::MAX`

5. This unbounded `shard_id` is used directly to index the `kv_tx` array without validation [1](#0-0) 

**Secondary Vulnerability**: During initialization, a similar unchecked access occurs [6](#0-5)  where `remote_shard_addresses[shard_id]` is accessed without bounds checking.

**Attack Scenarios**:

1. **Configuration Mismatch**: The coordinator's `RemoteExecutorClient` is configured with N remote shard addresses [7](#0-6) , but a shard is deployed with `shard_id >= N`. When that shard sends KV requests, the coordinator panics.

2. **Network-Level Attack**: If network authentication is insufficient, an attacker can craft malicious `RemoteKVRequest` messages with out-of-bounds `shard_id` values to crash the coordinator.

3. **Version Mismatch**: During rolling updates or configuration drift, shards may have inconsistent shard count configurations, triggering the bug.

The `RemoteKVRequest` structure [8](#0-7)  contains a `shard_id` field that is serialized/deserialized using BCS, with no inherent bounds validation.

## Impact Explanation

**Severity: Medium** (per Aptos Bug Bounty criteria)

This vulnerability causes:
- **Validator Node Slowdowns/Crashes**: The coordinator's state view service thread panics and terminates, preventing sharded block execution
- **Availability Impact**: The sharded executor system becomes unavailable, affecting validator performance
- **State Inconsistencies**: If the crash occurs mid-block execution, it may require manual intervention to recover

The issue aligns with **Medium Severity** criteria:
- "State inconsistencies requiring intervention" - The crashed service requires restart and configuration fixes
- Could escalate to **High Severity** if it causes persistent validator node crashes during consensus participation

While this doesn't directly compromise funds or consensus safety, it impacts validator availability and could indirectly affect consensus liveness if multiple validators are affected.

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability is likely to occur in production environments due to:

1. **Multi-Process Deployment**: The sharded executor runs as separate processes [9](#0-8) , each configured independently via CLI arguments. Configuration errors are common in distributed deployments.

2. **No Validation**: There is no upfront validation in `ExecutorService::new()` [10](#0-9)  to ensure `shard_id < num_shards` or that all shards have consistent configuration.

3. **Network Exposure**: The NetworkController [11](#0-10)  doesn't show explicit authentication mechanisms in the visible code, potentially allowing message injection.

4. **Operational Complexity**: Managing consistent shard counts across coordinator and executor processes increases the chance of misconfiguration.

## Recommendation

Add bounds validation before array access in `handle_message()`:

```rust
pub fn handle_message(
    message: Message,
    state_view: Arc<RwLock<Option<Arc<S>>>>,
    kv_tx: Arc<Vec<Sender<Message>>>,
) {
    // ... existing deserialization code ...
    let (shard_id, state_keys) = req.into();
    
    // VALIDATION: Check shard_id is within bounds
    if shard_id >= kv_tx.len() {
        aptos_logger::error!(
            "Invalid shard_id {} received in RemoteKVRequest, expected < {}",
            shard_id,
            kv_tx.len()
        );
        return; // Drop invalid request instead of panicking
    }
    
    // ... rest of function ...
    kv_tx[shard_id].send(message).unwrap();
}
```

Additionally, add validation in `ProcessExecutorService::new()`:

```rust
pub fn new(
    shard_id: ShardId,
    num_shards: usize,
    num_threads: usize,
    coordinator_address: SocketAddr,
    remote_shard_addresses: Vec<SocketAddr>,
) -> Self {
    // VALIDATION: Ensure configuration consistency
    assert_eq!(
        remote_shard_addresses.len(),
        num_shards,
        "remote_shard_addresses.len() must equal num_shards"
    );
    assert!(
        shard_id < num_shards,
        "shard_id {} must be less than num_shards {}",
        shard_id,
        num_shards
    );
    
    let self_address = remote_shard_addresses[shard_id];
    // ... rest of function ...
}
```

## Proof of Concept

The following Rust test demonstrates the vulnerability:

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::net::{IpAddr, Ipv4Addr, SocketAddr};
    
    #[test]
    #[should_panic(expected = "index out of bounds")]
    fn test_shard_id_out_of_bounds_panic() {
        // Setup: Create RemoteStateViewService with 2 shards
        let mut controller = NetworkController::new(
            "test".to_string(),
            SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 8080),
            5000,
        );
        
        let remote_shard_addresses = vec![
            SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 8081),
            SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 8082),
        ];
        
        let service = RemoteStateViewService::<CachedStateView>::new(
            &mut controller,
            remote_shard_addresses,
            None,
        );
        
        // Attack: Create RemoteKVRequest with out-of-bounds shard_id
        let malicious_request = RemoteKVRequest::new(
            999, // shard_id = 999, but only 2 shards configured
            vec![],
        );
        
        let message = Message::new(bcs::to_bytes(&malicious_request).unwrap());
        
        // This will panic with "index out of bounds"
        RemoteStateViewService::handle_message(
            message,
            Arc::new(RwLock::new(None)),
            service.kv_tx.clone(),
        );
    }
}
```

**Notes:**
- The vulnerability is confirmed by examining the code paths and lack of bounds checking
- The attack is realistic in distributed deployments with independent shard configuration  
- The fix is straightforward: add bounds validation before array access
- This issue could also be prevented by enforcing configuration validation at deployment time

### Citations

**File:** execution/executor-service/src/remote_state_view_service.rs (L27-27)
```rust
        remote_shard_addresses: Vec<SocketAddr>,
```

**File:** execution/executor-service/src/remote_state_view_service.rs (L40-48)
```rust
        let command_txs = remote_shard_addresses
            .iter()
            .map(|address| {
                controller.create_outbound_channel(*address, kv_response_type.to_string())
            })
            .collect_vec();
        Self {
            kv_rx: result_rx,
            kv_tx: Arc::new(command_txs),
```

**File:** execution/executor-service/src/remote_state_view_service.rs (L86-89)
```rust
        let req: RemoteKVRequest = bcs::from_bytes(&message.data).unwrap();
        drop(bcs_deser_timer);

        let (shard_id, state_keys) = req.into();
```

**File:** execution/executor-service/src/remote_state_view_service.rs (L121-121)
```rust
        kv_tx[shard_id].send(message).unwrap();
```

**File:** types/src/block_executor/partitioner.rs (L16-16)
```rust
pub type ShardId = usize;
```

**File:** execution/executor-service/src/process_executor_service.rs (L24-24)
```rust
        let self_address = remote_shard_addresses[shard_id];
```

**File:** execution/executor-service/src/remote_executor_client.rs (L121-125)
```rust
        let state_view_service = Arc::new(RemoteStateViewService::new(
            controller_mut_ref,
            remote_shard_addresses,
            None,
        ));
```

**File:** execution/executor-service/src/lib.rs (L68-81)
```rust
pub struct RemoteKVRequest {
    pub(crate) shard_id: ShardId,
    pub(crate) keys: Vec<StateKey>,
}

impl RemoteKVRequest {
    pub fn new(shard_id: ShardId, keys: Vec<StateKey>) -> Self {
        Self { shard_id, keys }
    }

    pub fn into(self) -> (ShardId, Vec<StateKey>) {
        (self.shard_id, self.keys)
    }
}
```

**File:** execution/executor-service/src/main.rs (L37-43)
```rust
    let _exe_service = ProcessExecutorService::new(
        args.shard_id,
        args.num_shards,
        args.num_executor_threads,
        args.coordinator_address,
        args.remote_executor_addresses,
    );
```

**File:** execution/executor-service/src/remote_executor_service.rs (L22-55)
```rust
    pub fn new(
        shard_id: ShardId,
        num_shards: usize,
        num_threads: usize,
        self_address: SocketAddr,
        coordinator_address: SocketAddr,
        remote_shard_addresses: Vec<SocketAddr>,
    ) -> Self {
        let service_name = format!("executor_service-{}", shard_id);
        let mut controller = NetworkController::new(service_name, self_address, 5000);
        let coordinator_client = Arc::new(RemoteCoordinatorClient::new(
            shard_id,
            &mut controller,
            coordinator_address,
        ));
        let cross_shard_client = Arc::new(RemoteCrossShardClient::new(
            &mut controller,
            remote_shard_addresses,
        ));

        let executor_service = Arc::new(ShardedExecutorService::new(
            shard_id,
            num_shards,
            num_threads,
            coordinator_client,
            cross_shard_client,
        ));

        Self {
            shard_id,
            controller,
            executor_service,
        }
    }
```

**File:** secure/net/src/network_controller/mod.rs (L84-150)
```rust
pub struct NetworkController {
    inbound_handler: Arc<Mutex<InboundHandler>>,
    outbound_handler: OutboundHandler,
    inbound_rpc_runtime: Runtime,
    outbound_rpc_runtime: Runtime,
    inbound_server_shutdown_tx: Option<oneshot::Sender<()>>,
    outbound_task_shutdown_tx: Option<Sender<Message>>,
    listen_addr: SocketAddr,
}

impl NetworkController {
    pub fn new(service: String, listen_addr: SocketAddr, timeout_ms: u64) -> Self {
        let inbound_handler = Arc::new(Mutex::new(InboundHandler::new(
            service.clone(),
            listen_addr,
            timeout_ms,
        )));
        let outbound_handler = OutboundHandler::new(service, listen_addr, inbound_handler.clone());
        info!("Network controller created for node {}", listen_addr);
        Self {
            inbound_handler,
            outbound_handler,
            inbound_rpc_runtime: Runtime::new().unwrap(),
            outbound_rpc_runtime: Runtime::new().unwrap(),
            // we initialize the shutdown handles when we start the network controller
            inbound_server_shutdown_tx: None,
            outbound_task_shutdown_tx: None,
            listen_addr,
        }
    }

    pub fn create_outbound_channel(
        &mut self,
        remote_peer_addr: SocketAddr,
        message_type: String,
    ) -> Sender<Message> {
        let (outbound_sender, outbound_receiver) = unbounded();

        self.outbound_handler
            .register_handler(message_type, remote_peer_addr, outbound_receiver);

        outbound_sender
    }

    pub fn create_inbound_channel(&mut self, message_type: String) -> Receiver<Message> {
        let (inbound_sender, inbound_receiver) = unbounded();

        self.inbound_handler
            .lock()
            .unwrap()
            .register_handler(message_type, inbound_sender);

        inbound_receiver
    }

    pub fn start(&mut self) {
        info!(
            "Starting network controller started for at {}",
            self.listen_addr
        );
        self.inbound_server_shutdown_tx = self
            .inbound_handler
            .lock()
            .unwrap()
            .start(&self.inbound_rpc_runtime);
        self.outbound_task_shutdown_tx = self.outbound_handler.start(&self.outbound_rpc_runtime);
    }
```
