# Audit Report

## Title
BCS Bomb in State Snapshot Restore: Unbounded Memory Allocation from Untrusted Length Prefix

## Summary
The state snapshot restore process reads length-prefixed records from backup files without validating the length value before memory allocation. An attacker who can provide or modify backup files can craft malicious length prefixes that cause the deserializer to allocate gigabytes of memory, resulting in memory exhaustion and denial of service.

## Finding Description

The vulnerability exists in the `read_record_bytes()` function which deserializes state snapshot chunks during backup restoration. [1](#0-0) 

The blobs file format contains repeated length-prefixed records where each record is a BCS-serialized `(StateKey, StateValue)` tuple. During restoration, the `read_state_value()` function reads these records: [2](#0-1) 

The critical vulnerability occurs in `read_record_bytes()` where the function:
1. Reads a 4-byte big-endian length prefix from the untrusted backup file
2. Converts it to a usize without any validation
3. **Immediately allocates memory** for the full claimed size using `BytesMut::with_capacity(record_size)` [3](#0-2) 

The vulnerability is on line 60 where `BytesMut::with_capacity(record_size)` attempts to allocate memory for whatever size was claimed in the length prefix. Since `record_size` comes from a `u32` field (line 54), it can claim up to 4,294,967,295 bytes (~4GB) per record. The allocation happens **before** any actual data is read or validated.

**Attack Propagation Path:**

1. Attacker gains access to backup storage (compromised S3 bucket, credential leak, insider access) or provides a malicious backup file to a node operator
2. Attacker modifies a StateSnapshotChunk's blobs file to include malicious length prefixes (e.g., `0xFF FF FF FF` = 4GB)
3. Node operator initiates restore from the compromised backup: [4](#0-3) 
4. The restore controller spawns concurrent tasks to download and deserialize chunks
5. When processing the malicious chunk, `read_record_bytes()` reads the inflated length prefix and attempts to allocate 4GB
6. This causes either:
   - Successful allocation exhausting available memory
   - Allocation failure/panic/OOM kill
   - Multiple chunks with malicious records compounding the attack
7. The restore process crashes or the entire node becomes unresponsive

**Broken Invariant:**
This violates Critical Invariant #9: "All operations must respect gas, storage, and computational limits." The restore operation should enforce reasonable bounds on memory allocation, but instead trusts untrusted input without validation.

## Impact Explanation

This is a **Medium Severity** vulnerability per the Aptos bug bounty criteria:

- **Denial of Service**: The restore process crashes or gets OOM-killed, preventing successful restoration
- **Validator Node Impact**: If this occurs on a validator node during disaster recovery, it prevents the validator from rejoining the network
- **Memory Exhaustion**: Can affect the entire system, not just the restore process
- **Cascading Failures**: Multiple malicious records across chunks can compound the attack

The impact is classified as Medium rather than High because:
- It requires access to backup storage or social engineering to provide malicious backups
- It's a DoS attack, not funds loss or consensus violation
- Normal network operation is unaffected (only restoration is impacted)
- Recovery is possible by using a different backup source

However, the severity should not be underestimated as backup restoration is a critical disaster recovery mechanism, and preventing its operation can have significant consequences for validator availability.

## Likelihood Explanation

**Likelihood: Medium to High**

The likelihood is elevated because:

1. **Access Requirements**: While the attacker needs backup access, this is achievable through:
   - Misconfigured cloud storage (S3 buckets with overly permissive policies)
   - Compromised credentials for backup storage
   - Insider threats from personnel with backup access
   - Social engineering node operators to use "helpful" backup files

2. **Attack Complexity**: Very low - requires only:
   - Modifying 4 bytes in a backup file (the length prefix)
   - No cryptographic bypass needed
   - No deep protocol knowledge required

3. **Detection Difficulty**: The malicious backup appears valid until deserialization begins

4. **Real-World Scenarios**: 
   - Disaster recovery situations where operators may use any available backup
   - Testing/staging environments with less secure backup storage
   - Community-provided backups for bootstrap purposes

## Recommendation

Implement strict size validation before memory allocation. Add a maximum record size constant and validate the length prefix against it:

```rust
// In storage/backup/backup-cli/src/utils/read_record_bytes.rs

// Maximum size for a single state snapshot record (e.g., 100 MB)
// Legitimate StateValue records should be much smaller
const MAX_RECORD_SIZE: usize = 100 * 1024 * 1024;

async fn read_record_bytes(&mut self) -> Result<Option<Bytes>> {
    let _timer = BACKUP_TIMER.timer_with(&["read_record_bytes"]);
    // read record size
    let mut size_buf = BytesMut::with_capacity(4);
    self.read_full_buf_or_none(&mut size_buf).await?;
    if size_buf.is_empty() {
        return Ok(None);
    }

    // empty record
    let record_size = u32::from_be_bytes(size_buf.as_ref().try_into()?) as usize;
    
    // SECURITY: Validate record size before allocation
    if record_size > MAX_RECORD_SIZE {
        bail!(
            "Record size {} exceeds maximum allowed size {}. \
            Possible corrupted or malicious backup file.",
            record_size,
            MAX_RECORD_SIZE
        );
    }
    
    if record_size == 0 {
        return Ok(Some(Bytes::new()));
    }

    // read record
    let mut record_buf = BytesMut::with_capacity(record_size);
    self.read_full_buf_or_none(&mut record_buf).await?;
    if record_buf.is_empty() {
        bail!("Hit EOF when reading record.")
    }

    Ok(Some(record_buf.freeze()))
}
```

Additional hardening recommendations:
1. Add checksum validation for backup files
2. Implement incremental memory allocation for large records
3. Add monitoring/alerting for abnormal memory usage during restore
4. Document secure backup storage practices

## Proof of Concept

```rust
// Test demonstrating the vulnerability
// Place in storage/backup/backup-cli/src/utils/read_record_bytes.rs

#[cfg(test)]
mod vulnerability_tests {
    use super::*;
    use std::io::Cursor;

    #[tokio::test]
    async fn test_bcs_bomb_memory_exhaustion() {
        // Craft malicious backup data:
        // 4-byte length prefix claiming 1GB (0x40000000)
        // followed by minimal actual data (will fail on read, but allocation happens first)
        let malicious_length: u32 = 1_000_000_000; // 1GB
        let mut malicious_data = malicious_length.to_be_bytes().to_vec();
        malicious_data.extend_from_slice(b"tiny"); // Only 4 bytes of actual data
        
        let mut cursor = Cursor::new(malicious_data);
        
        // This should either:
        // 1. Fail with OOM
        // 2. Successfully allocate 1GB (demonstrating the vulnerability)
        // 3. With the fix: return an error about size exceeding maximum
        let result = cursor.read_record_bytes().await;
        
        // Without the fix, this allocation attempt occurs:
        // BytesMut::with_capacity(1_000_000_000)
        // which will either succeed (bad) or OOM (worse)
        
        match result {
            Err(e) => {
                // With the fix, should get "exceeds maximum allowed size"
                println!("Correctly rejected: {}", e);
            },
            Ok(Some(_)) => {
                panic!("Should not succeed with malicious input");
            },
            Ok(None) => {
                panic!("Should not return None");
            }
        }
    }
    
    #[tokio::test]
    async fn test_maximum_u32_length_prefix() {
        // Most extreme case: claim maximum u32 size (~4GB)
        let malicious_length: u32 = u32::MAX;
        let malicious_data = malicious_length.to_be_bytes().to_vec();
        
        let mut cursor = Cursor::new(malicious_data);
        let result = cursor.read_record_bytes().await;
        
        // Should fail with size validation error (with fix)
        // or trigger OOM/allocation failure (without fix)
        assert!(result.is_err(), "Should reject maximum size claim");
    }
}
```

**Notes:**
- The vulnerability affects all backup restoration operations using `ReadRecordBytes` trait
- The attack can be amplified by including multiple malicious records in different chunks
- Current code has no MAX_RECORD_SIZE constant or validation anywhere in the codebase
- The restore process uses concurrent downloads [5](#0-4)  which means multiple allocation bombs could trigger simultaneously

### Citations

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/manifest.rs (L21-22)
```rust
    /// Repeated `len(record) + record` where `record` is BCS serialized tuple
    /// `(key, state_value)`
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L186-197)
```rust
        let storage = self.storage.clone();
        let futs_iter = chunks.into_iter().enumerate().map(|(chunk_idx, chunk)| {
            let storage = storage.clone();
            async move {
                tokio::spawn(async move {
                    let blobs = Self::read_state_value(&storage, chunk.blobs.clone()).await?;
                    let proof = storage.load_bcs_file(&chunk.proof).await?;
                    Result::<_>::Ok((chunk_idx, chunk, blobs, proof))
                })
                .await?
            }
        });
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L198-199)
```rust
        let con = self.concurrent_downloads;
        let mut futs_stream = stream::iter(futs_iter).buffered_x(con * 2, con);
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L253-266)
```rust
    async fn read_state_value(
        storage: &Arc<dyn BackupStorage>,
        file_handle: FileHandle,
    ) -> Result<Vec<(StateKey, StateValue)>> {
        let mut file = storage.open_for_read(&file_handle).await?;

        let mut chunk = vec![];

        while let Some(record_bytes) = file.read_record_bytes().await? {
            chunk.push(bcs::from_bytes(&record_bytes)?);
        }

        Ok(chunk)
    }
```

**File:** storage/backup/backup-cli/src/utils/read_record_bytes.rs (L44-67)
```rust
    async fn read_record_bytes(&mut self) -> Result<Option<Bytes>> {
        let _timer = BACKUP_TIMER.timer_with(&["read_record_bytes"]);
        // read record size
        let mut size_buf = BytesMut::with_capacity(4);
        self.read_full_buf_or_none(&mut size_buf).await?;
        if size_buf.is_empty() {
            return Ok(None);
        }

        // empty record
        let record_size = u32::from_be_bytes(size_buf.as_ref().try_into()?) as usize;
        if record_size == 0 {
            return Ok(Some(Bytes::new()));
        }

        // read record
        let mut record_buf = BytesMut::with_capacity(record_size);
        self.read_full_buf_or_none(&mut record_buf).await?;
        if record_buf.is_empty() {
            bail!("Hit EOF when reading record.")
        }

        Ok(Some(record_buf.freeze()))
    }
```
