# Audit Report

## Title
Premature Garbage Collection of Peer States Enables Reputation Reset Attacks

## Summary
The `garbage_collect_peer_states()` function aggressively removes all peer states for disconnected peers without any grace period, allowing both legitimate peers to lose earned reputation during brief network disruptions and malicious peers to reset low scores by reconnecting. [1](#0-0) 

## Finding Description

The Aptos data client maintains peer reputation through a scoring system where peers earn scores (0-100) based on their behavior. Peers with scores below 25.0 are ignored to protect against malicious actors. [2](#0-1) 

However, the garbage collection mechanism has no grace period for temporarily disconnected peers. The flow is:

1. **Frequent GC execution**: Garbage collection runs every 100ms via the data poller [3](#0-2) [4](#0-3) 

2. **Aggressive peer removal**: When a peer disconnects, `remove_peer_metadata()` immediately removes it from the connected peers list [5](#0-4) 

3. **Complete state loss**: The GC removes ALL peer state including score, request history, and storage summary [6](#0-5) 

4. **Fresh start on reconnect**: When the peer reconnects, it gets a new `PeerState` with default score 50.0 [7](#0-6) 

**Attack Scenario:**
A malicious peer that has been ignored (score ≤ 25.0) can:
1. Disconnect from the network
2. Wait for garbage collection (≤100ms)
3. Reconnect with a fresh score of 50.0
4. Bypass the ignore threshold and send requests again
5. Repeat to continuously evade reputation penalties [8](#0-7) 

## Impact Explanation

This vulnerability meets **Medium Severity** criteria per the Aptos bug bounty program ("State inconsistencies requiring intervention"):

1. **Operational Impact**: High-quality peers (score 90+) lose earned reputation during brief network hiccups, forcing nodes to re-learn peer quality through trial-and-error, degrading state sync performance.

2. **Security Impact**: The peer reputation system is a security mechanism designed to protect against malicious peers. This vulnerability completely undermines it by allowing score resets through reconnection.

3. **No Critical Impact**: This does not cause consensus violations, fund loss, or network partition, limiting severity to Medium.

## Likelihood Explanation

**High Likelihood** - This will occur regularly in production:

1. **Network instability**: Brief disconnections happen frequently in distributed networks due to transient network issues, load balancing, or connection maintenance.

2. **Easy exploitation**: Malicious peers can trivially disconnect and reconnect programmatically to reset their scores whenever they drop below the ignore threshold.

3. **Aggressive timing**: With 100ms GC cycles, even momentary disconnections trigger state loss.

## Recommendation

Implement a grace period before garbage collecting peer states:

```rust
// In peer_states.rs, modify PeerState to track disconnection time
pub struct PeerState {
    // ... existing fields ...
    last_seen_time: Option<Instant>,
}

// In client.rs, add grace period configuration
const PEER_GC_GRACE_PERIOD_SECS: u64 = 300; // 5 minutes

// Modify garbage_collect_peer_states to check grace period
fn garbage_collect_peer_states(&self) -> crate::error::Result<(), Error> {
    let all_connected_peers = self.get_all_connected_peers()?;
    let now = self.time_service.now();
    
    // Retain peers that are either connected OR within grace period
    self.peer_states.garbage_collect_peer_states_with_grace_period(
        all_connected_peers,
        now,
        Duration::from_secs(PEER_GC_GRACE_PERIOD_SECS),
    );
    
    Ok(())
}
```

In `peer_states.rs`:
```rust
pub fn garbage_collect_peer_states_with_grace_period(
    &self,
    connected_peers: HashSet<PeerNetworkId>,
    current_time: Instant,
    grace_period: Duration,
) {
    self.peer_to_state.retain(|peer_network_id, peer_state| {
        // Keep if connected
        if connected_peers.contains(peer_network_id) {
            // Update last seen time
            peer_state.last_seen_time = Some(current_time);
            return true;
        }
        
        // Keep if within grace period
        if let Some(last_seen) = peer_state.last_seen_time {
            if current_time.duration_since(last_seen) < grace_period {
                return true;
            }
        }
        
        // Otherwise, garbage collect
        false
    });
}
```

## Proof of Concept

```rust
// Test demonstrating the vulnerability
#[tokio::test]
async fn test_premature_peer_state_gc() {
    // Setup data client with peer
    let (data_client, _poller) = setup_data_client();
    let peer = create_test_peer();
    
    // Peer earns high score through successful responses
    for _ in 0..50 {
        data_client.peer_states.update_score_success(peer);
    }
    let high_score = data_client.peer_states
        .get_peer_to_states()
        .get(&peer)
        .unwrap()
        .get_score();
    assert_eq!(high_score, 100.0); // Max score
    
    // Simulate brief disconnect
    disconnect_peer(peer);
    
    // Run garbage collection
    data_client.update_global_summary_cache().unwrap();
    
    // Peer state is lost
    assert!(data_client.peer_states.get_peer_to_states().get(&peer).is_none());
    
    // Peer reconnects
    reconnect_peer(peer);
    update_peer_storage_summary(&data_client, peer);
    
    // Peer has default score, lost all history
    let new_score = data_client.peer_states
        .get_peer_to_states()
        .get(&peer)
        .unwrap()
        .get_score();
    assert_eq!(new_score, 50.0); // STARTING_SCORE, not 100.0!
}

#[tokio::test]
async fn test_malicious_peer_score_reset() {
    let (data_client, _poller) = setup_data_client();
    let malicious_peer = create_test_peer();
    
    // Malicious peer gets low score
    for _ in 0..30 {
        data_client.peer_states.update_score_error(
            malicious_peer,
            ErrorType::Malicious,
        );
    }
    let low_score = data_client.peer_states
        .get_peer_to_states()
        .get(&malicious_peer)
        .unwrap()
        .get_score();
    assert!(low_score < 25.0); // Below IGNORE_PEER_THRESHOLD
    
    // Peer disconnects and reconnects to reset score
    disconnect_peer(malicious_peer);
    data_client.update_global_summary_cache().unwrap();
    reconnect_peer(malicious_peer);
    update_peer_storage_summary(&data_client, malicious_peer);
    
    // Score is reset, bypassing ignore threshold
    let reset_score = data_client.peer_states
        .get_peer_to_states()
        .get(&malicious_peer)
        .unwrap()
        .get_score();
    assert_eq!(reset_score, 50.0); // Above ignore threshold!
}
```

**Notes:**

This vulnerability affects the state synchronization layer's peer selection mechanism. While it does not compromise consensus safety or cause fund loss, it significantly degrades operational efficiency and enables malicious actors to bypass reputation-based peer filtering. The lack of grace period is a design oversight that should be addressed to maintain robust peer management in production networks.

### Citations

**File:** state-sync/aptos-data-client/src/client.rs (L233-243)
```rust
    /// Garbage collects the peer states to remove data for disconnected peers
    fn garbage_collect_peer_states(&self) -> crate::error::Result<(), Error> {
        // Get all connected peers
        let all_connected_peers = self.get_all_connected_peers()?;

        // Garbage collect the disconnected peers
        self.peer_states
            .garbage_collect_peer_states(all_connected_peers);

        Ok(())
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L32-43)
```rust
/// Scores for peer rankings based on preferences and behavior.
const MAX_SCORE: f64 = 100.0;
const MIN_SCORE: f64 = 0.0;
const STARTING_SCORE: f64 = 50.0;
/// Add this score on a successful response.
const SUCCESSFUL_RESPONSE_DELTA: f64 = 1.0;
/// Not necessarily a malicious response, but not super useful.
const NOT_USEFUL_MULTIPLIER: f64 = 0.95;
/// Likely to be a malicious response.
const MALICIOUS_MULTIPLIER: f64 = 0.8;
/// Ignore a peer when their score dips below this threshold.
const IGNORE_PEER_THRESHOLD: f64 = 25.0;
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L84-93)
```rust
impl PeerState {
    pub fn new(data_client_config: Arc<AptosDataClientConfig>) -> Self {
        Self {
            data_client_config,
            received_responses_by_type: Arc::new(DashMap::new()),
            sent_requests_by_type: Arc::new(DashMap::new()),
            storage_summary: None,
            score: STARTING_SCORE,
        }
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L151-160)
```rust
    /// Returns true iff the peer is currently ignored
    fn is_ignored(&self) -> bool {
        // Only ignore peers if the config allows it
        if !self.data_client_config.ignore_low_score_peers {
            return false;
        }

        // Otherwise, ignore peers with a low score
        self.score <= IGNORE_PEER_THRESHOLD
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L332-336)
```rust
    /// Garbage collects the peer states to remove data for disconnected peers
    pub fn garbage_collect_peer_states(&self, connected_peers: HashSet<PeerNetworkId>) {
        self.peer_to_state
            .retain(|peer_network_id, _| connected_peers.contains(peer_network_id));
    }
```

**File:** state-sync/aptos-data-client/src/poller.rs (L276-293)
```rust
    // Create the poll loop ticker
    let data_poller_config = poller.data_client_config.data_poller_config;
    let data_polling_interval = Duration::from_millis(data_poller_config.poll_loop_interval_ms);
    let poll_loop_ticker = poller.time_service.interval(data_polling_interval);
    futures::pin_mut!(poll_loop_ticker);

    // Start the poller
    let mut polling_round: u64 = 0;
    info!((LogSchema::new(LogEntry::DataSummaryPoller).message("Starting the Aptos data poller!")));
    loop {
        // Wait for the next round before polling
        poll_loop_ticker.next().await;

        // Increment the round counter
        polling_round = polling_round.wrapping_add(1);

        // Update the global storage summary
        if let Err(error) = poller.data_client.update_global_summary_cache() {
```

**File:** config/src/config/state_sync_config.rs (L346-358)
```rust
impl Default for AptosDataPollerConfig {
    fn default() -> Self {
        Self {
            additional_polls_per_peer_bucket: 1,
            min_polls_per_second: 5,
            max_num_in_flight_priority_polls: 30,
            max_num_in_flight_regular_polls: 30,
            max_polls_per_second: 20,
            peer_bucket_size: 10,
            poll_loop_interval_ms: 100,
        }
    }
}
```

**File:** network/framework/src/application/storage.rs (L216-262)
```rust
    /// Removes the peer metadata from the container. If the peer
    /// doesn't exist, or the connection id doesn't match, an error is
    /// returned. Otherwise, the existing peer metadata is returned.
    pub fn remove_peer_metadata(
        &self,
        peer_network_id: PeerNetworkId,
        connection_id: ConnectionId,
    ) -> Result<PeerMetadata, Error> {
        // Grab the write lock for the peer metadata
        let mut peers_and_metadata = self.peers_and_metadata.write();

        // Fetch the peer metadata for the given network
        let peer_metadata_for_network =
            get_peer_metadata_for_network(&peer_network_id, &mut peers_and_metadata)?;

        // Remove the peer metadata for the peer
        let peer_metadata = if let Entry::Occupied(entry) =
            peer_metadata_for_network.entry(peer_network_id.peer_id())
        {
            // Don't remove the peer if the connection doesn't match!
            // For now, remove the peer entirely, we could in the future
            // have multiple connections for a peer
            let active_connection_id = entry.get().connection_metadata.connection_id;
            if active_connection_id == connection_id {
                let peer_metadata = entry.remove();
                let event = ConnectionNotification::LostPeer(
                    peer_metadata.connection_metadata.clone(),
                    peer_network_id.network_id(),
                );
                self.broadcast(event);
                peer_metadata
            } else {
                return Err(Error::UnexpectedError(format!(
                    "The peer connection id did not match! Given: {:?}, found: {:?}.",
                    connection_id, active_connection_id
                )));
            }
        } else {
            // Unable to find the peer metadata for the given peer
            return Err(missing_peer_metadata_error(&peer_network_id));
        };

        // Update the cached peers and metadata
        self.set_cached_peers_and_metadata(peers_and_metadata.clone());

        Ok(peer_metadata)
    }
```
