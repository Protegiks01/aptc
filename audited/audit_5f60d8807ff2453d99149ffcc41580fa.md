# Audit Report

## Title
Per-Message-Type Round-Robin Scheduling Enables Validator Bandwidth Manipulation and Consensus Censorship

## Summary
The `PerKeyQueue` implementation in consensus message channels uses `(AccountAddress, Discriminant<ConsensusMsg>)` as the round-robin key instead of just `AccountAddress`. This allows validators sending multiple message types to occupy more slots in the round-robin queue, obtaining disproportionate message processing bandwidth. Malicious validators can exploit this to systematically delay honest validators' critical consensus messages, causing round timeouts and effective censorship. [1](#0-0) 

## Finding Description

The vulnerability stems from the granularity of round-robin scheduling in the message queue system used for consensus communication.

**Root Cause:**

The consensus message channels use `PerKeyQueue` with keys of type `(AccountAddress, Discriminant<ConsensusMsg>)`: [2](#0-1) 

When messages are pushed to the channel, the key includes both the sender's address AND the message type discriminant: [3](#0-2) 

The round-robin implementation serves one message per key per round: [4](#0-3) 

**Attack Mechanism:**

The `ConsensusMsg` enum contains approximately 25 different message type variants: [5](#0-4) 

A malicious validator can send messages of many different types (ProposalMsg, VoteMsg, OrderVoteMsg, RoundTimeoutMsg, SyncInfo, BatchMsg, SignedBatchInfo, ProofOfStoreMsg, DAGMessage, RandGenMessage, SecretShareMsg, etc.), creating multiple entries in the round-robin queue.

**Exploitation Scenario:**

1. Network has 100 validators
2. Honest validators typically send 3-5 message types each (ProposalMsg, VoteMsg, OrderVoteMsg, SyncInfo) = ~400 keys total
3. 10 malicious validators send all 15 legitimately-used message types each = 150 additional keys
4. Total round-robin keys: ~550

Per-validator bandwidth distribution:
- Malicious validator with 15 types: 15/550 = 2.7% of total bandwidth
- Honest validator with 4 types: 4/550 = 0.7% of total bandwidth  
- Malicious validators get 3.75× more bandwidth per validator

5. When an honest validator H is the consensus leader and broadcasts a ProposalMsg:
   - H's ProposalMsg is added to their `(H, ProposalMsg)` queue
   - With 550 active keys each having messages, H's proposal must wait for ~550 round-robin cycles
   - If each message processing takes even 2ms (including network reception, deserialization, spawning to bounded executor), the delay is 1100ms
   - This exceeds the default round timeout of 1000ms: [6](#0-5) 

6. Result: H's proposal arrives after timeout, round fails, H appears offline, loses leader reputation

**Invariant Violation:**

This breaks the **Consensus Liveness** invariant. Honest validators should be able to participate in consensus when functioning correctly. The unfair scheduling creates artificial delays that cause correctly-functioning validators to miss rounds and appear faulty.

## Impact Explanation

This qualifies as **High Severity** per Aptos bug bounty criteria:

1. **Validator node slowdowns**: Honest validators experience systematic message processing delays, causing them to appear slow or unresponsive during their leadership rounds

2. **Significant protocol violations**: The round-robin fairness assumption (that each validator gets equal message processing bandwidth) is violated. Validators can game the system by sending diverse message types

3. **Consensus liveness degradation**: Repeated round timeouts due to delayed leader proposals slow down overall chain progress

4. **Validator reputation damage**: Honest validators unfairly lose reputation points when their proposals timeout, affecting their future leader selection probability and potentially reducing staking rewards

5. **Asymmetric attack advantage**: Malicious validators need only send legitimately-formatted messages of various types - no protocol violations required, making detection difficult

The impact falls short of Critical (no funds loss, no permanent network partition) but clearly meets High severity for causing measurable consensus degradation and validator censorship.

## Likelihood Explanation

**High Likelihood:**

1. **Low attacker requirements**: Any validator can execute this attack without special privileges or >1/3 stake control

2. **Legitimate message diversity**: All message types used in the attack (ProposalMsg, VoteMsg, OrderVoteMsg, BatchMsg, etc.) are legitimately sent during normal consensus operation, making malicious traffic indistinguishable from heavy but valid activity

3. **No rate limiting**: The code contains no per-validator rate limiting or message type diversity checks: [7](#0-6) 

4. **Network conditions amplify impact**: Under high load when many validators are active, the round-robin queue naturally has many keys, making the attack more effective without requiring explicit attacker coordination

5. **Detection difficulty**: Distinguishing malicious bandwidth manipulation from legitimate heavy consensus activity (e.g., during epoch transitions, state sync, or high transaction volumes) is extremely difficult

## Recommendation

**Solution 1: Per-Validator Round-Robin (Recommended)**

Change the round-robin key from `(AccountAddress, Discriminant<ConsensusMsg>)` to just `AccountAddress`. Messages from the same validator should share a single round-robin slot regardless of message type.

Modify `network.rs` to use `AccountAddress` as the key: [8](#0-7) 

Change the push call to use only `peer_id`: [9](#0-8) 

This ensures each validator gets exactly one round-robin slot, achieving true per-validator fairness.

**Solution 2: Weighted Round-Robin**

If per-message-type separation is intentionally desired for prioritization, implement a weighted round-robin where each validator's total slots are capped (e.g., maximum 3 message types counted toward round-robin, others share slots).

**Solution 3: Per-Validator Rate Limiting**

Add per-validator message rate limiting before the channel to prevent any single validator from flooding with excessive message diversity.

## Proof of Concept

```rust
// Test demonstrating unfair bandwidth distribution in PerKeyQueue
// Add to crates/channel/src/message_queues_test.rs

#[test]
fn test_unfair_multi_type_bandwidth() {
    use std::mem::discriminant;
    
    #[derive(Debug, PartialEq, Eq)]
    enum MsgType {
        Proposal,
        Vote,
        Timeout,
        Sync,
        Batch,
    }
    
    let mut q = PerKeyQueue::new(QueueStyle::FIFO, NonZeroUsize!(10), None);
    
    // Malicious validator sends 5 different message types
    let malicious = AccountAddress::new([0u8; AccountAddress::LENGTH]);
    for msg_type in [MsgType::Proposal, MsgType::Vote, MsgType::Timeout, MsgType::Sync, MsgType::Batch] {
        for i in 0..5 {
            q.push((malicious, discriminant(&msg_type)), format!("M-{:?}-{}", msg_type, i));
        }
    }
    
    // Honest validator sends only 1 message type
    let honest = AccountAddress::new([1u8; AccountAddress::LENGTH]);
    for i in 0..5 {
        q.push((honest, discriminant(&MsgType::Proposal)), format!("H-Proposal-{}", i));
    }
    
    // Process 30 messages (6 rounds)
    let mut malicious_count = 0;
    let mut honest_count = 0;
    
    for _ in 0..30 {
        if let Some(msg) = q.pop() {
            if msg.starts_with("M-") {
                malicious_count += 1;
            } else {
                honest_count += 1;
            }
        }
    }
    
    // Malicious validator got 25 messages (5 types * 5 messages)
    // Honest validator got 5 messages (1 type * 5 messages)
    // This demonstrates 5:1 bandwidth ratio despite both having 25 total messages!
    assert_eq!(malicious_count, 25);
    assert_eq!(honest_count, 5);
    println!("Malicious: {}, Honest: {} - 5x unfair advantage!", malicious_count, honest_count);
}
```

This test demonstrates that a validator sending 5 message types gets 5× more bandwidth than a validator sending 1 type, even when both have the same total number of messages queued. In a real consensus scenario with 25 message types and round timeouts of 1000ms, this unfairness can cause critical message delays leading to validator censorship.

**Notes**

The vulnerability is architectural rather than a simple coding error. The per-message-type keying was likely intended to provide fine-grained fairness or prioritization, but it creates an exploitable asymmetry. The fix requires changing the fundamental queueing strategy to ensure per-validator fairness while potentially adding separate prioritization logic if needed for different message types.

### Citations

**File:** crates/channel/src/message_queues.rs (L45-63)
```rust
pub(crate) struct PerKeyQueue<K: Eq + Hash + Clone, T> {
    /// QueueStyle for the messages stored per key
    queue_style: QueueStyle,
    /// per_key_queue maintains a map from a Key to a queue
    /// of all the messages from that Key. A Key is usually
    /// represented by AccountAddress
    per_key_queue: HashMap<K, VecDeque<T>>,
    /// This is a (round-robin)queue of Keys which have pending messages
    /// This queue will be used for performing round robin among
    /// Keys for choosing the next message
    round_robin_queue: VecDeque<K>,
    /// Maximum number of messages to store per key
    max_queue_size: NonZeroUsize,
    /// Number of messages dequeued since last GC
    num_popped_since_gc: u32,
    /// Optional counters for recording # enqueued, # dequeued, and # dropped
    /// messages
    counters: Option<&'static IntCounterVec>,
}
```

**File:** crates/channel/src/message_queues.rs (L156-167)
```rust
    pub(crate) fn pop(&mut self) -> Option<T> {
        let key = match self.round_robin_queue.pop_front() {
            Some(v) => v,
            _ => {
                return None;
            },
        };

        let (message, is_q_empty) = self.pop_from_key_queue(&key);
        if !is_q_empty {
            self.round_robin_queue.push_back(key);
        }
```

**File:** consensus/src/network.rs (L194-207)
```rust
    /// Provide a LIFO buffer for each (Author, MessageType) key
    pub consensus_messages: aptos_channel::Receiver<
        (AccountAddress, Discriminant<ConsensusMsg>),
        (AccountAddress, ConsensusMsg),
    >,
    pub quorum_store_messages: aptos_channel::Receiver<
        (AccountAddress, Discriminant<ConsensusMsg>),
        (AccountAddress, ConsensusMsg),
    >,
    pub rpc_rx: aptos_channel::Receiver<
        (AccountAddress, Discriminant<IncomingRpcRequest>),
        (AccountAddress, IncomingRpcRequest),
    >,
}
```

**File:** consensus/src/network.rs (L757-769)
```rust
        let (consensus_messages_tx, consensus_messages) = aptos_channel::new(
            QueueStyle::FIFO,
            10,
            Some(&counters::CONSENSUS_CHANNEL_MSGS),
        );
        let (quorum_store_messages_tx, quorum_store_messages) = aptos_channel::new(
            QueueStyle::FIFO,
            // TODO: tune this value based on quorum store messages with backpressure
            50,
            Some(&counters::QUORUM_STORE_CHANNEL_MSGS),
        );
        let (rpc_tx, rpc_rx) =
            aptos_channel::new(QueueStyle::FIFO, 10, Some(&counters::RPC_CHANNEL_MSGS));
```

**File:** consensus/src/network.rs (L799-813)
```rust
    fn push_msg(
        peer_id: AccountAddress,
        msg: ConsensusMsg,
        tx: &aptos_channel::Sender<
            (AccountAddress, Discriminant<ConsensusMsg>),
            (AccountAddress, ConsensusMsg),
        >,
    ) {
        if let Err(e) = tx.push((peer_id, discriminant(&msg)), (peer_id, msg)) {
            warn!(
                remote_peer = peer_id,
                error = ?e, "Error pushing consensus msg",
            );
        }
    }
```

**File:** consensus/src/network_interface.rs (L39-105)
```rust
#[derive(Clone, Debug, Deserialize, Serialize)]
pub enum ConsensusMsg {
    /// DEPRECATED: Once this is introduced in the next release, please use
    /// [`ConsensusMsg::BlockRetrievalRequest`](ConsensusMsg::BlockRetrievalRequest) going forward
    /// This variant was renamed from `BlockRetrievalRequest` to `DeprecatedBlockRetrievalRequest`
    /// RPC to get a chain of block of the given length starting from the given block id.
    DeprecatedBlockRetrievalRequest(Box<BlockRetrievalRequestV1>),
    /// Carries the returned blocks and the retrieval status.
    BlockRetrievalResponse(Box<BlockRetrievalResponse>),
    /// Request to get a EpochChangeProof from current_epoch to target_epoch
    EpochRetrievalRequest(Box<EpochRetrievalRequest>),
    /// ProposalMsg contains the required information for the proposer election protocol to make
    /// its choice (typically depends on round and proposer info).
    ProposalMsg(Box<ProposalMsg>),
    /// This struct describes basic synchronization metadata.
    SyncInfo(Box<SyncInfo>),
    /// A vector of LedgerInfo with contiguous increasing epoch numbers to prove a sequence of
    /// epoch changes from the first LedgerInfo's epoch.
    EpochChangeProof(Box<EpochChangeProof>),
    /// VoteMsg is the struct that is ultimately sent by the voter in response for receiving a
    /// proposal.
    VoteMsg(Box<VoteMsg>),
    /// CommitProposal is the struct that is sent by the validator after execution to propose
    /// on the committed state hash root.
    CommitVoteMsg(Box<CommitVote>),
    /// CommitDecision is the struct that is sent by the validator after collecting no fewer
    /// than 2f + 1 signatures on the commit proposal. This part is not on the critical path, but
    /// it can save slow machines to quickly confirm the execution result.
    CommitDecisionMsg(Box<CommitDecision>),
    /// Quorum Store: Send a Batch of transactions.
    BatchMsg(Box<BatchMsg<BatchInfo>>),
    /// Quorum Store: Request the payloads of a completed batch.
    BatchRequestMsg(Box<BatchRequest>),
    /// Quorum Store: Response to the batch request.
    BatchResponse(Box<Batch<BatchInfo>>),
    /// Quorum Store: Send a signed batch digest. This is a vote for the batch and a promise that
    /// the batch of transactions was received and will be persisted until batch expiration.
    SignedBatchInfo(Box<SignedBatchInfoMsg<BatchInfo>>),
    /// Quorum Store: Broadcast a certified proof of store (a digest that received 2f+1 votes).
    ProofOfStoreMsg(Box<ProofOfStoreMsg<BatchInfo>>),
    /// DAG protocol message
    DAGMessage(DAGNetworkMessage),
    /// Commit message
    CommitMessage(Box<CommitMessage>),
    /// Randomness generation message
    RandGenMessage(RandGenMessage),
    /// Quorum Store: Response to the batch request.
    BatchResponseV2(Box<BatchResponse>),
    /// OrderVoteMsg is the struct that is broadcasted by a validator on receiving quorum certificate
    /// on a block.
    OrderVoteMsg(Box<OrderVoteMsg>),
    /// RoundTimeoutMsg is broadcasted by a validator once it decides to timeout the current round.
    RoundTimeoutMsg(Box<RoundTimeoutMsg>),
    /// RPC to get a chain of block of the given length starting from the given block id, using epoch and round.
    BlockRetrievalRequest(Box<BlockRetrievalRequest>),
    /// OptProposalMsg contains the optimistic proposal and sync info.
    OptProposalMsg(Box<OptProposalMsg>),
    /// Quorum Store: Send a Batch of transactions.
    BatchMsgV2(Box<BatchMsg<BatchInfoExt>>),
    /// Quorum Store: Send a signed batch digest with BatchInfoExt. This is a vote for the batch and a promise that
    /// the batch of transactions was received and will be persisted until batch expiration.
    SignedBatchInfoMsgV2(Box<SignedBatchInfoMsg<BatchInfoExt>>),
    /// Quorum Store: Broadcast a certified proof of store (a digest that received 2f+1 votes) with BatchInfoExt.
    ProofOfStoreMsgV2(Box<ProofOfStoreMsg<BatchInfoExt>>),
    /// Secret share message: Used to share secrets per consensus round
    SecretShareMsg(SecretShareNetworkMessage),
}
```

**File:** config/src/config/consensus_config.rs (L235-239)
```rust
            round_initial_timeout_ms: 1000,
            // 1.2^6 ~= 3
            // Timeout goes from initial_timeout to initial_timeout*3 in 6 steps
            round_timeout_backoff_exponent_base: 1.2,
            round_timeout_backoff_max_exponent: 6,
```

**File:** consensus/src/epoch_manager.rs (L1528-1625)
```rust
    async fn process_message(
        &mut self,
        peer_id: AccountAddress,
        consensus_msg: ConsensusMsg,
    ) -> anyhow::Result<()> {
        fail_point!("consensus::process::any", |_| {
            Err(anyhow::anyhow!("Injected error in process_message"))
        });

        if let ConsensusMsg::ProposalMsg(proposal) = &consensus_msg {
            observe_block(
                proposal.proposal().timestamp_usecs(),
                BlockStage::EPOCH_MANAGER_RECEIVED,
            );
        }
        if let ConsensusMsg::OptProposalMsg(proposal) = &consensus_msg {
            if !self.config.enable_optimistic_proposal_rx {
                bail!(
                    "Unexpected OptProposalMsg. Feature is disabled. Author: {}, Epoch: {}, Round: {}",
                    proposal.block_data().author(),
                    proposal.epoch(),
                    proposal.round()
                )
            }
            observe_block(
                proposal.timestamp_usecs(),
                BlockStage::EPOCH_MANAGER_RECEIVED,
            );
            observe_block(
                proposal.timestamp_usecs(),
                BlockStage::EPOCH_MANAGER_RECEIVED_OPT_PROPOSAL,
            );
        }
        // we can't verify signatures from a different epoch
        let maybe_unverified_event = self.check_epoch(peer_id, consensus_msg).await?;

        if let Some(unverified_event) = maybe_unverified_event {
            // filter out quorum store messages if quorum store has not been enabled
            match self.filter_quorum_store_events(peer_id, &unverified_event) {
                Ok(true) => {},
                Ok(false) => return Ok(()), // This occurs when the quorum store is not enabled, but the recovery mode is enabled. We filter out the messages, but don't raise any error.
                Err(err) => return Err(err),
            }
            // same epoch -> run well-formedness + signature check
            let epoch_state = self
                .epoch_state
                .clone()
                .ok_or_else(|| anyhow::anyhow!("Epoch state is not available"))?;
            let proof_cache = self.proof_cache.clone();
            let quorum_store_enabled = self.quorum_store_enabled;
            let quorum_store_msg_tx = self.quorum_store_msg_tx.clone();
            let buffered_proposal_tx = self.buffered_proposal_tx.clone();
            let round_manager_tx = self.round_manager_tx.clone();
            let my_peer_id = self.author;
            let max_num_batches = self.config.quorum_store.receiver_max_num_batches;
            let max_batch_expiry_gap_usecs =
                self.config.quorum_store.batch_expiry_gap_when_init_usecs;
            let payload_manager = self.payload_manager.clone();
            let pending_blocks = self.pending_blocks.clone();
            self.bounded_executor
                .spawn(async move {
                    match monitor!(
                        "verify_message",
                        unverified_event.clone().verify(
                            peer_id,
                            &epoch_state.verifier,
                            &proof_cache,
                            quorum_store_enabled,
                            peer_id == my_peer_id,
                            max_num_batches,
                            max_batch_expiry_gap_usecs,
                        )
                    ) {
                        Ok(verified_event) => {
                            Self::forward_event(
                                quorum_store_msg_tx,
                                round_manager_tx,
                                buffered_proposal_tx,
                                peer_id,
                                verified_event,
                                payload_manager,
                                pending_blocks,
                            );
                        },
                        Err(e) => {
                            error!(
                                SecurityEvent::ConsensusInvalidMessage,
                                remote_peer = peer_id,
                                error = ?e,
                                unverified_event = unverified_event
                            );
                        },
                    }
                })
                .await;
        }
        Ok(())
    }
```
