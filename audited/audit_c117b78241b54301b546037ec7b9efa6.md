# Audit Report

## Title
Indexer File Store Backfiller Service Crash-Loop Vulnerability Due to Inadequate Error Handling

## Summary
The indexer-grpc-file-store-backfiller service contains multiple `.expect()` calls that cause immediate process termination without retry logic. While line 69 specifically can be triggered by progress file errors during execution, the broader issue includes line 65 which handles both corrupted progress files and gRPC connection failures during initialization. Corrupted progress files cause permanent crash-loops requiring manual intervention, breaking service availability guarantees.

## Finding Description

The `IndexerGrpcFileStoreBackfillerConfig::run()` function contains two critical `.expect()` calls: [1](#0-0) [2](#0-1) 

**Line 69 specifically** can be triggered by:

1. **Progress file read/parse errors during validation mode**: When the service reads a corrupted progress file during validation, it returns an error that triggers the line 69 `.expect()`: [3](#0-2) 

2. **Progress file write errors during execution**: File system errors during progress file updates in both backfill and validation modes return errors to line 69: [4](#0-3) [5](#0-4) 

**Line 65** can be triggered by:

1. **Corrupted progress file during initialization**: When the service starts and attempts to parse a corrupted progress file: [6](#0-5) 

2. **gRPC connection failures during initialization**: When establishing the initial gRPC connection fails: [7](#0-6) 

**Additional crash points**: The service also contains direct `panic!()` calls for gRPC streaming failures that bypass error returns entirely: [8](#0-7) 

**Critical Issue - Crash Loop Scenario**: When the progress file becomes corrupted (via filesystem errors, hardware issues, or incomplete writes), the service enters a crash-loop:
1. Service starts and reads corrupted progress file
2. Line 65 `.expect()` triggers, process terminates
3. Kubernetes/orchestrator restarts the service
4. Service reads same corrupted file → repeat

The crash-loop continues until manual intervention (deleting/fixing the progress file).

**Inconsistency with Other Components**: Other indexer-grpc components implement retry logic for transient errors. For example, the create_grpc_client function includes exponential backoff, and data services implement multiple retry strategies. The backfiller lacks this defensive programming pattern.

## Impact Explanation

This qualifies as **HIGH severity** per Aptos bug bounty criteria under "API crashes". The indexer-grpc-file-store-backfiller provides critical data access APIs for the Aptos ecosystem. Service unavailability impacts:

- **Indexer API availability**: External applications cannot access historical transaction data
- **Operational burden**: Requires manual intervention by operators to identify and fix corrupted files
- **No graceful degradation**: Service terminates immediately without attempting recovery
- **Permanent failure state**: Crash-loops persist until manual fix

While this does not affect consensus or validator operations directly, API infrastructure availability is explicitly covered under HIGH severity in the bug bounty program.

## Likelihood Explanation

**HIGH likelihood** - This issue will occur in production environments due to:

1. **File system errors are common**: Disk failures, full filesystems, permission issues, and incomplete writes occur regularly in production
2. **Network failures are expected**: gRPC connections fail due to network partitions, server restarts, firewall changes, and resource exhaustion
3. **No protection mechanisms**: The code has zero retry logic or error recovery
4. **Persistent failure mode**: Once triggered, the issue persists until manual intervention
5. **Real-world operational scenarios**: The progress file is written every 5 seconds during backfill operations, creating numerous opportunities for write failures

## Recommendation

Implement comprehensive error handling with retry logic and graceful degradation:

```rust
#[async_trait::async_trait]
impl RunnableConfig for IndexerGrpcFileStoreBackfillerConfig {
    async fn run(&self) -> Result<()> {
        // Add retry logic for processor creation
        let mut processor = retry_with_backoff(|| async {
            Processor::new(
                self.fullnode_grpc_address.clone(),
                self.file_store_config.clone(),
                self.chain_id,
                self.enable_cache_compression,
                self.progress_file_path.clone(),
                self.starting_version,
                self.transactions_count,
                self.validation_mode,
                self.backfill_processing_task_count,
                self.validating_task_count,
            )
            .await
        })
        .await
        .context("Failed to create file store processor after retries")?;

        // Add retry loop for execution with exponential backoff
        loop {
            match processor.run().await {
                Ok(_) => return Ok(()),
                Err(e) => {
                    // Check if error is recoverable (file I/O, network)
                    if is_recoverable_error(&e) {
                        tracing::warn!("Recoverable error occurred, retrying: {:?}", e);
                        tokio::time::sleep(Duration::from_secs(5)).await;
                        continue;
                    } else {
                        return Err(e).context("File store processor failed with non-recoverable error");
                    }
                }
            }
        }
    }
}
```

Additional improvements:
1. Add progress file validation and automatic repair/reset on corruption
2. Implement gRPC reconnection logic with exponential backoff
3. Use `.context()` instead of `.expect()` to preserve error information
4. Add metrics for error rates and retry attempts
5. Implement circuit breaker pattern for repeated failures

## Proof of Concept

**Scenario 1: Corrupted Progress File at Startup**

```bash
# Create a valid progress file
echo '{"version": 1000}' > /tmp/progress.json

# Start the backfiller service with this progress file
# Service runs normally

# Corrupt the progress file (simulate filesystem corruption)
echo 'corrupted{invalid json' > /tmp/progress.json

# Restart the service
# Expected: Service crashes immediately at line 65 with:
# "Failed to create file store processor: Failed to parse progress file"
# Kubernetes restarts → crash-loop continues indefinitely
```

**Scenario 2: Progress File Write Failure During Execution**

```rust
// Simulate filesystem becoming read-only during execution
// or disk full condition during progress file update

// In processor.rs line 245:
std::fs::write(&progress_file_path, &bytes)
    .context("Failed to write progress file")?;
// This returns error → propagates to line 69 → service crashes

// Manual intervention required:
// 1. Fix filesystem issue
// 2. Delete/fix progress file
// 3. Restart service
```

**Scenario 3: gRPC Connection Failure**

```rust
// During initialization, if fullnode gRPC is unavailable:
// Line 113 in processor.rs fails
let stream = grpc_client
    .get_transactions_from_node(request)
    .await?  // Error here
    .into_inner();
// Propagates to line 65 .expect() → immediate crash
// No retry attempt, service terminates
```

**Notes**

This vulnerability specifically affects the indexer-grpc-file-store-backfiller component, which is part of the Aptos data infrastructure layer. To precisely answer the security question:

- **Line 69** can be triggered by corrupted progress files during execution (read/parse/write errors) ✓
- **Line 69** cannot be triggered by gRPC streaming failures (those panic at line 272) ✗
- **Line 65** can be triggered by both corrupted progress files AND gRPC initialization failures ✓

The broader architectural issue is the absence of retry logic and graceful error handling throughout the backfiller service, which is inconsistent with error handling patterns found in other indexer-grpc components that implement sophisticated retry mechanisms.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store-backfiller/src/lib.rs (L64-65)
```rust
        .await
        .expect("Failed to create file store processor");
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store-backfiller/src/lib.rs (L66-69)
```rust
        processor
            .run()
            .await
            .expect("File store processor exited unexpectedly");
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store-backfiller/src/processor.rs (L79-91)
```rust
        let progress_file: ProgressFile = match std::fs::read(&progress_file_path) {
            Ok(bytes) => serde_json::from_slice(&bytes).context("Failed to parse progress file")?,
            Err(_) => {
                let progress_file = ProgressFile {
                    version: starting_version,
                };
                let bytes = serde_json::to_vec(&progress_file)
                    .context("Failed to serialize progress file")?;
                std::fs::write(&progress_file_path, bytes)
                    .context("Failed to write progress file")?;
                progress_file
            },
        };
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store-backfiller/src/processor.rs (L105-114)
```rust
        // Create a grpc client to the fullnode.
        let mut grpc_client = create_grpc_client(fullnode_grpc_address.clone()).await;
        let request = tonic::Request::new(GetTransactionsFromNodeRequest {
            starting_version: Some(expected_starting_version),
            transactions_count,
        });
        let stream = grpc_client
            .get_transactions_from_node(request)
            .await?
            .into_inner();
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store-backfiller/src/processor.rs (L240-246)
```rust
                    let progress_file = ProgressFile {
                        version: next_version_to_process,
                    };
                    let bytes = serde_json::to_vec(&progress_file)
                        .context("Failed to serialize progress file")?;
                    std::fs::write(&progress_file_path, &bytes)
                        .context("Failed to write progress file")?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store-backfiller/src/processor.rs (L266-274)
```rust
            let item = grpc_stream.next().await;
            let item = item.unwrap();
            let response = match item {
                Ok(response) => response,
                Err(e) => {
                    tracing::error!("Failed to get response: {:?}", e);
                    panic!("Failed to get response: {:?}", e);
                },
            };
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store-backfiller/src/processor.rs (L306-314)
```rust
        let progress_file = {
            let bytes =
                std::fs::read(&self.progress_file_path).context("Failed to read progress file");
            match bytes {
                Ok(bytes) => {
                    serde_json::from_slice(&bytes).context("Failed to parse progress file")?
                },
                _ => ProgressFile { version: 0 },
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store-backfiller/src/processor.rs (L371-376)
```rust
                let progress_file = ProgressFile {
                    version: current_version,
                };
                let bytes = serde_json::to_vec(&progress_file)
                    .context("Failed to serialize progress file")?;
                std::fs::write(&progress_file_path, &bytes).context("io error")?;
```
