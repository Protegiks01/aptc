# Audit Report

## Title
Epoch Ending Ledger Info Truncation Causes Synchronization Failures During Protocol Upgrades

## Summary
The `MAX_NUM_EPOCH_ENDING_LEDGER_INFO` constant (100) in the database layer silently truncates epoch ending ledger info responses, while the storage service incorrectly sets the `more` flag to `false` in the new size-aware chunking implementation. This causes the `TrustedState` verification logic to reject valid state proofs with an "Inconsistent epoch change proof and latest ledger info" error when nodes need to sync through more than 100 epochs during protocol upgrades that shorten epoch duration.

## Finding Description
The vulnerability exists in the interaction between three components:

1. **Database Layer Iterator Truncation**: The `get_epoch_ending_ledger_info_iterator` function silently limits responses to 100 epochs regardless of the requested range. [1](#0-0) 

The iterator computes `limit = min(end_epoch - start_epoch, 100)` and silently truncates the end_epoch, meaning if a client requests epochs 0-200, it only receives epochs 0-99.

2. **Storage Service Incorrect `more` Flag**: The new size-aware chunking implementation always sets `more=false` when creating the `EpochChangeProof`, even when the iterator returns incomplete data due to the 100-epoch limit. [2](#0-1) 

When the iterator returns `None` prematurely (because of the 100-epoch truncation), the storage service logs a warning but still creates an `EpochChangeProof` with `more=false`, misleading clients about data completeness.

3. **TrustedState Verification Failure**: The `TrustedState::verify_and_ratchet_inner` method uses the `more` flag to determine if an epoch change proof is valid when the latest ledger info is in a higher epoch than the proof. [3](#0-2) 

When `latest_li.ledger_info().epoch() > new_epoch` and `epoch_change_proof.more` is false, the method bails with "Inconsistent epoch change proof and latest ledger info". However, if `more` were correctly set to true, it would accept the partial proof and allow continued synchronization.

**Attack Scenario:**
During a protocol upgrade that shortens epoch duration (e.g., from 6 hours to 1 hour), a node that is offline or bootstrapping needs to sync through 150 epochs:

1. Node requests epochs 0-200 from storage service
2. Database iterator silently truncates to epochs 0-99
3. Storage service returns `EpochChangeProof { ledger_info_with_sigs: [epochs 0-99], more: false }`
4. Node's latest advertised ledger info is in epoch 150
5. `TrustedState::verify_and_ratchet_inner` checks: epoch 150 > epoch 99 (new_epoch from proof)
6. Checks if `epoch_change_proof.more` is true â†’ it's false (incorrectly)
7. Bails with "Inconsistent epoch change proof and latest ledger info"
8. Node fails to synchronize and cannot join the network

This breaks the **State Consistency** invariant - nodes must be able to synchronize to the latest state during and after protocol upgrades. [4](#0-3) 

The constant is defined with a TODO comment acknowledging the limitation, but no iteration API was implemented, and the storage service doesn't properly handle the truncation.

## Impact Explanation
**Severity: HIGH** (up to $50,000 per Aptos Bug Bounty criteria)

This vulnerability causes **significant protocol violations** during protocol upgrades:

1. **Network Partition Risk**: Nodes using the new chunking implementation (enabled by default on non-mainnet networks per config optimizer) cannot synchronize when more than 100 epochs need to be fetched. This could partition testnet/devnet during upgrades. [5](#0-4) 

2. **Upgrade Deployment Failures**: Protocol upgrades that intentionally shorten epoch duration to improve network agility would fail to deploy on testnet, preventing validation before mainnet deployment.

3. **Bootstrap Failures**: New validator nodes or nodes recovering from extended downtime cannot bootstrap successfully if they fall behind by more than 100 epochs.

The vulnerability is currently mitigated on mainnet because the default config sets `enable_size_and_time_aware_chunking: false`, using the legacy implementation which correctly handles the `more` flag. [6](#0-5) 

However, this represents a **latent critical vulnerability** that would activate if:
- The new chunking is enabled on mainnet (as intended per the `ENABLE_SIZE_AND_TIME_AWARE_CHUNKING = true` constant)
- A protocol upgrade shortens epoch duration
- Validators fall behind by 100+ epochs during upgrade transition

## Likelihood Explanation
**Likelihood: MEDIUM-HIGH**

The vulnerability will trigger automatically when:
1. Network uses new size-aware chunking (currently: testnets, future: mainnet)
2. Protocol upgrade changes epoch duration to be significantly shorter
3. Any node needs to sync through >100 epochs (new nodes, offline nodes, slow validators)

Current situation:
- **Testnets/Devnets**: Vulnerable now
- **Mainnet**: Protected by legacy implementation, but planned migration to new chunking makes this a time-bomb

Epoch count scenarios that trigger the bug:
- Normal operation: Epochs typically last hours/days, unlikely to fall 100 epochs behind
- **Protocol upgrade**: If epoch duration changes from 6h to 30min (12x reduction), nodes offline for 2 days would need 96 epochs - close to the limit
- **Multiple upgrades**: Cumulative effect of multiple epoch-shortening upgrades could easily exceed 100 epochs between waypoints

## Recommendation
Implement proper pagination awareness in the storage service's new chunking implementation:

```rust
fn get_epoch_ending_ledger_infos_by_size(
    &self,
    start_epoch: u64,
    expected_end_epoch: u64,
    max_response_size: u64,
    use_size_and_time_aware_chunking: bool,
) -> Result<EpochChangeProof, Error> {
    // Calculate the number of ledger infos to fetch
    let expected_num_ledger_infos = inclusive_range_len(start_epoch, expected_end_epoch)?;
    let max_num_ledger_infos = self.config.max_epoch_chunk_size;
    let num_ledger_infos_to_fetch = min(expected_num_ledger_infos, max_num_ledger_infos);

    if !use_size_and_time_aware_chunking {
        return self.get_epoch_ending_ledger_infos_by_size_legacy(...);
    }

    // Calculate the end epoch for storage
    let end_epoch = start_epoch
        .checked_add(num_ledger_infos_to_fetch)
        .ok_or_else(|| Error::UnexpectedErrorEncountered("End epoch has overflown!".into()))?;

    // Get the epoch ending ledger info iterator
    let mut epoch_ending_ledger_info_iterator = self
        .storage
        .get_epoch_ending_ledger_info_iterator(start_epoch, end_epoch)?;

    let mut epoch_ending_ledger_infos = vec![];
    let mut response_progress_tracker = ResponseDataProgressTracker::new(...);
    
    // Track the actual number of epochs received
    let mut epochs_received = 0u64;

    while !response_progress_tracker.is_response_complete() {
        match epoch_ending_ledger_info_iterator.next() {
            Some(Ok(epoch_ending_ledger_info)) => {
                let num_serialized_bytes = get_num_serialized_bytes(&epoch_ending_ledger_info)?;
                if response_progress_tracker.data_items_fits_in_response(true, num_serialized_bytes) {
                    epoch_ending_ledger_infos.push(epoch_ending_ledger_info);
                    response_progress_tracker.add_data_item(num_serialized_bytes);
                    epochs_received += 1;
                } else {
                    break;
                }
            },
            Some(Err(error)) => return Err(Error::StorageErrorEncountered(error.to_string())),
            None => break,
        }
    }

    // FIX: Correctly determine if there's more data
    // Data is incomplete if we expected more epochs than we received
    let more = epochs_received < num_ledger_infos_to_fetch;
    
    // Create the epoch change proof with correct 'more' flag
    let epoch_change_proof = EpochChangeProof::new(epoch_ending_ledger_infos, more);

    response_progress_tracker.update_data_truncation_metrics(...);
    Ok(epoch_change_proof)
}
```

**Alternative Fix**: Increase `MAX_NUM_EPOCH_ENDING_LEDGER_INFO` to match `max_epoch_chunk_size` (200) or make it configurable to align with the storage service configuration.

## Proof of Concept

```rust
#[cfg(test)]
mod test_epoch_sync_truncation {
    use super::*;
    use aptos_types::{
        epoch_change::EpochChangeProof,
        ledger_info::LedgerInfoWithSignatures,
        trusted_state::TrustedState,
    };
    
    #[test]
    fn test_sync_failure_with_truncated_epoch_proof() {
        // Simulate a scenario where we need to sync through 150 epochs
        // but the database only returns 100 due to MAX_NUM_EPOCH_ENDING_LEDGER_INFO
        
        // 1. Create initial trusted state at epoch 0
        let initial_waypoint = create_waypoint_at_epoch_0();
        let trusted_state = TrustedState::from(initial_waypoint);
        
        // 2. Request epochs 0-150 from storage service
        let storage = create_test_storage_with_150_epochs();
        
        // 3. Storage service with new chunking enabled
        let epoch_change_proof = storage.get_epoch_ending_ledger_infos_by_size(
            0,     // start_epoch
            150,   // expected_end_epoch  
            10_000_000,  // max_response_size
            true,  // use_size_and_time_aware_chunking = TRUE (bug path)
        ).unwrap();
        
        // 4. Verify the bug: should return 100 epochs with more=true
        //    but actually returns 100 epochs with more=false
        assert_eq!(epoch_change_proof.ledger_info_with_sigs.len(), 100);
        assert_eq!(epoch_change_proof.more, false); // BUG: Should be true!
        
        // 5. Create latest ledger info at epoch 150
        let latest_li = create_ledger_info_at_epoch(150);
        
        // 6. Try to verify and ratchet - this should fail with the bug
        let result = trusted_state.verify_and_ratchet_inner(
            &latest_li,
            &epoch_change_proof,
        );
        
        // 7. Verify synchronization fails with "Inconsistent epoch change proof"
        assert!(result.is_err());
        assert!(result.unwrap_err().to_string().contains("Inconsistent epoch change proof"));
        
        // 8. Show that with correct 'more=true', it would succeed
        let correct_proof = EpochChangeProof::new(
            epoch_change_proof.ledger_info_with_sigs,
            true  // Correct value
        );
        let result = trusted_state.verify_and_ratchet_inner(&latest_li, &correct_proof);
        assert!(result.is_ok()); // Verification succeeds with correct 'more' flag
    }
}
```

**Notes:**
- The vulnerability affects non-mainnet networks immediately (testnets, devnets)
- Mainnet is currently protected by using the legacy implementation, but migration to new chunking is planned
- The 100-epoch hardcoded limit should be aligned with storage service configuration or made configurable
- The TODO comment in the original constant definition acknowledges the need for better iteration support, which was never implemented

### Citations

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L572-595)
```rust
    fn get_epoch_ending_ledger_info_iterator(
        &self,
        start_epoch: u64,
        end_epoch: u64,
    ) -> Result<Box<dyn Iterator<Item = Result<LedgerInfoWithSignatures>> + '_>> {
        gauged_api("get_epoch_ending_ledger_info_iterator", || {
            self.check_epoch_ending_ledger_infos_request(start_epoch, end_epoch)?;
            let limit = std::cmp::min(
                end_epoch.saturating_sub(start_epoch),
                MAX_NUM_EPOCH_ENDING_LEDGER_INFO as u64,
            );
            let end_epoch = start_epoch.saturating_add(limit);

            let iter = self
                .ledger_db
                .metadata_db()
                .get_epoch_ending_ledger_info_iter(start_epoch, end_epoch)?;

            Ok(Box::new(iter)
                as Box<
                    dyn Iterator<Item = Result<LedgerInfoWithSignatures>> + '_,
                >)
        })
    }
```

**File:** state-sync/storage-service/server/src/storage.rs (L255-296)
```rust
        // Fetch as many epoch ending ledger infos as possible
        while !response_progress_tracker.is_response_complete() {
            match epoch_ending_ledger_info_iterator.next() {
                Some(Ok(epoch_ending_ledger_info)) => {
                    // Calculate the number of serialized bytes for the epoch ending ledger info
                    let num_serialized_bytes = get_num_serialized_bytes(&epoch_ending_ledger_info)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;

                    // Add the ledger info to the list
                    if response_progress_tracker
                        .data_items_fits_in_response(true, num_serialized_bytes)
                    {
                        epoch_ending_ledger_infos.push(epoch_ending_ledger_info);
                        response_progress_tracker.add_data_item(num_serialized_bytes);
                    } else {
                        break; // Cannot add any more data items
                    }
                },
                Some(Err(error)) => {
                    return Err(Error::StorageErrorEncountered(error.to_string()));
                },
                None => {
                    // Log a warning that the iterator did not contain all the expected data
                    warn!(
                        "The epoch ending ledger info iterator is missing data! \
                        Start epoch: {:?}, expected end epoch: {:?}, num ledger infos to fetch: {:?}",
                        start_epoch, expected_end_epoch, num_ledger_infos_to_fetch
                    );
                    break;
                },
            }
        }

        // Create the epoch change proof
        let epoch_change_proof = EpochChangeProof::new(epoch_ending_ledger_infos, false);

        // Update the data truncation metrics
        response_progress_tracker
            .update_data_truncation_metrics(DataResponse::get_epoch_ending_ledger_info_label());

        Ok(epoch_change_proof)
    }
```

**File:** types/src/trusted_state.rs (L183-187)
```rust
            } else if latest_li.ledger_info().epoch() > new_epoch && epoch_change_proof.more {
                epoch_change_li
            } else {
                bail!("Inconsistent epoch change proof and latest ledger info");
            };
```

**File:** storage/aptosdb/src/common.rs (L7-9)
```rust
// TODO: Either implement an iteration API to allow a very old client to loop through a long history
// or guarantee that there is always a recent enough waypoint and client knows to boot from there.
pub(crate) const MAX_NUM_EPOCH_ENDING_LEDGER_INFO: usize = 100;
```

**File:** config/src/config/state_sync_config.rs (L195-218)
```rust
impl Default for StorageServiceConfig {
    fn default() -> Self {
        Self {
            enable_size_and_time_aware_chunking: false,
            enable_transaction_data_v2: true,
            max_epoch_chunk_size: MAX_EPOCH_CHUNK_SIZE,
            max_invalid_requests_per_peer: 500,
            max_lru_cache_size: 500, // At ~0.6MiB per chunk, this should take no more than 0.5GiB
            max_network_channel_size: 4000,
            max_network_chunk_bytes: SERVER_MAX_MESSAGE_SIZE as u64,
            max_network_chunk_bytes_v2: SERVER_MAX_MESSAGE_SIZE_V2 as u64,
            max_num_active_subscriptions: 30,
            max_optimistic_fetch_period_ms: 5000, // 5 seconds
            max_state_chunk_size: MAX_STATE_CHUNK_SIZE,
            max_storage_read_wait_time_ms: 10_000, // 10 seconds
            max_subscription_period_ms: 30_000,    // 30 seconds
            max_transaction_chunk_size: MAX_TRANSACTION_CHUNK_SIZE,
            max_transaction_output_chunk_size: MAX_TRANSACTION_OUTPUT_CHUNK_SIZE,
            min_time_to_ignore_peers_secs: 300, // 5 minutes
            request_moderator_refresh_interval_ms: 1000, // 1 second
            storage_summary_refresh_interval_ms: 100, // Optimal for <= 10 blocks per second
        }
    }
}
```

**File:** config/src/config/state_sync_config.rs (L620-633)
```rust
        // Potentially enable size and time-aware chunking for all networks except Mainnet
        let mut modified_config = false;
        if let Some(chain_id) = chain_id {
            if ENABLE_SIZE_AND_TIME_AWARE_CHUNKING
                && !chain_id.is_mainnet()
                && local_storage_config_yaml["enable_size_and_time_aware_chunking"].is_null()
            {
                storage_service_config.enable_size_and_time_aware_chunking = true;
                modified_config = true;
            }
        }

        Ok(modified_config)
    }
```
