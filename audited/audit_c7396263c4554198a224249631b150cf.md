# Audit Report

## Title
Unbounded Memory Leak and Pipeline Stall in Secret Sharing BlockQueue Due to Missing Pruning Mechanism

## Summary
The `BlockQueue` in the secret sharing consensus subsystem lacks any mechanism to remove or skip `QueueItem` entries that fail to receive sufficient secret shares. When a round cannot achieve the threshold of secret shares (due to network partitions, validator failures, or Byzantine behavior), the affected item remains in the queue indefinitely, blocking all subsequent items from being processed and causing unbounded memory growth.

## Finding Description

The secret sharing subsystem maintains a `BlockQueue` that holds ordered blocks awaiting secret share aggregation. The vulnerability exists in the queue's dequeue logic: [1](#0-0) 

The `dequeue_ready_prefix()` method only removes items from the front of the queue that have `is_fully_secret_shared() == true`, and immediately stops when it encounters a non-ready item. There is no timeout, round-based pruning, or any other mechanism to handle items that will never become ready. [2](#0-1) 

An item is only considered "fully secret shared" when all rounds in `pending_secret_key_rounds` have been removed. This happens when secret shares are aggregated: [3](#0-2) 

The aggregation only completes when the threshold is reached: [4](#0-3) 

If the threshold is never reached (e.g., only 60% of validators respond when 67% is required), the item remains pending forever. The reliable broadcast mechanism retries indefinitely with exponential backoff but never gives up: [5](#0-4) 

Meanwhile, new blocks continue to be added to the queue: [6](#0-5) 

The only cleanup mechanism is `process_reset()`, which completely clears the queue but is only triggered during epoch transitions or stop signals, not as regular pruning: [7](#0-6) 

**Attack Scenario:**

1. Network partition or Byzantine validators cause < threshold validators to respond for round N
2. The `QueueItem` for round N remains in `pending_secret_key_rounds` state indefinitely
3. All subsequent rounds (N+1, N+2, ...) are successfully secret-shared but cannot be dequeued because round N blocks the queue
4. New blocks continue arriving and being added to the queue
5. Memory grows unboundedly as `BTreeMap<Round, QueueItem>` accumulates entries
6. Eventually, the node runs out of memory and crashes, or the pipeline stalls completely

This violates the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits."

## Impact Explanation

This issue qualifies as **High Severity** under the Aptos bug bounty program:

1. **Validator Node Crashes**: Unbounded memory accumulation will eventually cause out-of-memory errors, crashing validator nodes. This falls under "Validator node slowdowns" but extends to actual crashes.

2. **Pipeline Stall**: The secret sharing pipeline becomes blocked, preventing any blocks from being processed even if they have valid secret shares. This is a "Significant protocol violation."

3. **Potential Liveness Failure**: If multiple validators experience this simultaneously (due to a common network partition or Byzantine validator), the network could lose liveness, approaching **Critical severity** ("Total loss of liveness/network availability").

4. **Low Byzantine Threshold**: This can be triggered by fewer than 1/3 Byzantine validators simply refusing to provide shares for specific rounds, or naturally through network instability.

The impact is amplified because:
- It affects consensus-critical infrastructure
- It persists across multiple rounds until epoch change
- It can be triggered repeatedly
- Memory leaks are cumulative and eventually fatal

## Likelihood Explanation

This vulnerability has **HIGH likelihood** of occurring:

1. **Natural Triggers**: Network partitions, temporary validator outages, or high latency can prevent threshold from being reached without any malicious behavior.

2. **Byzantine Amplification**: Any Byzantine validator (< 1/3 requirement) can deliberately withhold shares to trigger this condition.

3. **No Safeguards**: The code has no timeout, no pruning, no skip mechanism, and no bounds on queue size.

4. **Persistent Impact**: Once triggered, the effect persists until the next epoch transition, which could be many rounds away.

5. **Observable Metric**: The code includes queue size monitoring, suggesting the developers are aware queue growth is a concern: [8](#0-7) 

## Recommendation

Implement a multi-layered defense strategy:

### 1. Round-Based Pruning
Add a mechanism to remove items that are too far behind the current consensus round:

```rust
pub fn prune_stale_items(&mut self, current_round: Round, max_lag: u64) {
    let stale_threshold = current_round.saturating_sub(max_lag);
    self.queue.retain(|&round, item| {
        if round < stale_threshold {
            warn!(
                "Pruning stale queue item at round {} (current: {}, threshold: {})",
                round, current_round, stale_threshold
            );
            for block in item.blocks() {
                observe_block(block.timestamp_usecs(), BlockStage::SECRET_SHARING_PRUNED);
            }
            false
        } else {
            true
        }
    });
}
```

### 2. Timeout-Based Expiration
Track insertion time for each item and expire items after a timeout period (e.g., 30 seconds):

```rust
pub struct QueueItem {
    ordered_blocks: OrderedBlocks,
    offsets_by_round: HashMap<Round, usize>,
    pending_secret_key_rounds: HashSet<Round>,
    share_requester_handles: Option<Vec<DropGuard>>,
    insertion_time: Instant, // Add this field
}

pub fn prune_expired_items(&mut self, timeout: Duration) {
    let now = Instant::now();
    self.queue.retain(|&round, item| {
        if now.duration_since(item.insertion_time) > timeout {
            warn!("Pruning expired queue item at round {} after {:?}", round, timeout);
            false
        } else {
            true
        }
    });
}
```

### 3. Bounded Queue Size
Add a maximum queue size and drop oldest items when exceeded:

```rust
const MAX_QUEUE_SIZE: usize = 100;

pub fn push_back(&mut self, item: QueueItem) {
    // ... existing code ...
    
    while self.queue.len() > MAX_QUEUE_SIZE {
        if let Some((oldest_round, _)) = self.queue.pop_first() {
            warn!("Queue size limit exceeded, dropping oldest item at round {}", oldest_round);
        }
    }
}
```

### 4. Integrate into Main Loop
Call pruning in the `SecretShareManager::start()` loop:

```rust
_ = interval.tick().fuse() => {
    self.observe_queue();
    self.block_queue.prune_stale_items(
        self.secret_share_store.lock().highest_known_round,
        100 // max 100 rounds lag
    );
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_memory_leak_when_threshold_not_reached() {
        let mut queue = BlockQueue::new();
        
        // Simulate 1000 rounds being added to the queue
        for round in 1..=1000 {
            let mut pending_rounds = HashSet::new();
            pending_rounds.insert(round);
            
            // Create a mock OrderedBlocks with one block
            let ordered_blocks = create_mock_ordered_blocks(round);
            
            let item = QueueItem::new(
                ordered_blocks,
                None,
                pending_rounds, // Never becomes empty
            );
            
            queue.push_back(item);
        }
        
        // Try to dequeue - should return empty because first item is not ready
        let ready = queue.dequeue_ready_prefix();
        assert_eq!(ready.len(), 0, "Should not dequeue anything when first item is pending");
        
        // Queue still has all 1000 items
        assert_eq!(queue.queue().len(), 1000, "All items remain in queue");
        
        // This demonstrates the memory leak: items accumulate indefinitely
        // In production, this would continue until OOM
    }
    
    #[test]
    fn test_blocking_effect_on_subsequent_items() {
        let mut queue = BlockQueue::new();
        
        // Add round 1 that will never be ready
        let mut pending_rounds_1 = HashSet::new();
        pending_rounds_1.insert(1);
        let item1 = QueueItem::new(
            create_mock_ordered_blocks(1),
            None,
            pending_rounds_1,
        );
        queue.push_back(item1);
        
        // Add round 2 that IS ready (empty pending set)
        let item2 = QueueItem::new(
            create_mock_ordered_blocks(2),
            None,
            HashSet::new(), // Ready immediately
        );
        queue.push_back(item2);
        
        // Try to dequeue - round 2 is ready but blocked by round 1
        let ready = queue.dequeue_ready_prefix();
        assert_eq!(ready.len(), 0, "Round 2 cannot be dequeued despite being ready");
        
        // Both items still in queue
        assert_eq!(queue.queue().len(), 2, "Both items stuck in queue");
    }
}
```

**Notes:**

The vulnerability is confirmed through code analysis. The `BlockQueue` has no mechanism to handle items that never reach the secret sharing threshold. While the system is designed to work under normal conditions (â‰¥ 2/3 validator participation), it fails to gracefully handle edge cases where this assumption is violated. This creates a critical availability risk in production environments where network instability or partial Byzantine behavior can trigger unbounded memory growth and pipeline stalls.

### Citations

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L60-62)
```rust
    pub fn is_fully_secret_shared(&self) -> bool {
        self.pending_secret_key_rounds.is_empty()
    }
```

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L64-77)
```rust
    pub fn set_secret_shared_key(&mut self, round: Round, key: SecretSharedKey) {
        let offset = self.offset(round);
        if self.pending_secret_key_rounds.contains(&round) {
            observe_block(
                self.blocks()[offset].timestamp_usecs(),
                BlockStage::SECRET_SHARING_ADD_DECISION,
            );
            let block = &self.blocks_mut()[offset];
            if let Some(tx) = block.pipeline_tx().lock().as_mut() {
                tx.secret_shared_key_tx.take().map(|tx| tx.send(Some(key)));
            }
            self.pending_secret_key_rounds.remove(&round);
        }
    }
```

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L112-127)
```rust
    pub fn dequeue_ready_prefix(&mut self) -> Vec<OrderedBlocks> {
        let mut ready_prefix = vec![];
        while let Some((_starting_round, item)) = self.queue.first_key_value() {
            if item.is_fully_secret_shared() {
                let (_, item) = self.queue.pop_first().expect("First key must exist");
                for block in item.blocks() {
                    observe_block(block.timestamp_usecs(), BlockStage::SECRET_SHARING_READY);
                }
                let QueueItem { ordered_blocks, .. } = item;
                ready_prefix.push(ordered_blocks);
            } else {
                break;
            }
        }
        ready_prefix
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L38-46)
```rust
    pub fn try_aggregate(
        self,
        secret_share_config: &SecretShareConfig,
        metadata: SecretShareMetadata,
        decision_tx: Sender<SecretSharedKey>,
    ) -> Either<Self, SecretShare> {
        if self.total_weight < secret_share_config.threshold() {
            return Either::Left(self);
        }
```

**File:** crates/reliable-broadcast/src/lib.rs (L167-205)
```rust
            loop {
                tokio::select! {
                    Some((receiver, result)) = rpc_futures.next() => {
                        let aggregating = aggregating.clone();
                        let future = executor.spawn(async move {
                            (
                                    receiver,
                                    result
                                        .and_then(|msg| {
                                            msg.try_into().map_err(|e| anyhow::anyhow!("{:?}", e))
                                        })
                                        .and_then(|ack| aggregating.add(receiver, ack)),
                            )
                        }).await;
                        aggregate_futures.push(future);
                    },
                    Some(result) = aggregate_futures.next() => {
                        let (receiver, result) = result.expect("spawned task must succeed");
                        match result {
                            Ok(may_be_aggragated) => {
                                if let Some(aggregated) = may_be_aggragated {
                                    return Ok(aggregated);
                                }
                            },
                            Err(e) => {
                                log_rpc_failure(e, receiver);

                                let backoff_strategy = backoff_policies
                                    .get_mut(&receiver)
                                    .expect("should be present");
                                let duration = backoff_strategy.next().expect("should produce value");
                                rpc_futures
                                    .push(send_message(receiver, Some(duration)));
                            },
                        }
                    },
                    else => unreachable!("Should aggregate with all responses")
                }
            }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L112-130)
```rust
    async fn process_incoming_blocks(&mut self, blocks: OrderedBlocks) {
        let rounds: Vec<u64> = blocks.ordered_blocks.iter().map(|b| b.round()).collect();
        info!(rounds = rounds, "Processing incoming blocks.");

        let mut share_requester_handles = Vec::new();
        let mut pending_secret_key_rounds = HashSet::new();
        for block in blocks.ordered_blocks.iter() {
            let handle = self.process_incoming_block(block).await;
            share_requester_handles.push(handle);
            pending_secret_key_rounds.insert(block.round());
        }

        let queue_item = QueueItem::new(
            blocks,
            Some(share_requester_handles),
            pending_secret_key_rounds,
        );
        self.block_queue.push_back(queue_item);
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L172-184)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.secret_share_store
            .lock()
            .update_highest_known_round(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L380-383)
```rust
    pub fn observe_queue(&self) {
        let queue = &self.block_queue.queue();
        DEC_QUEUE_SIZE.set(queue.len() as i64);
    }
```
