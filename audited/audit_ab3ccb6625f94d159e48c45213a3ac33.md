# Audit Report

## Title
Consensus Critical Path Blocking via Sparse Merkle Tree Drop Backpressure Causing Liveness Failures

## Summary
The `get_state_summary()` method is invoked on the consensus critical path during `ledger_update()` and contains a synchronous blocking call to `wait_for_backlog_drop(8)`. An attacker can trigger conditions where the `SUBTREE_DROPPER` queue accumulates more than 8 pending drop tasks through a combination of large state modifications and block tree pruning, causing `wait_for_backlog_drop(8)` to block for extended periods, resulting in consensus timeouts and validator liveness failures.

## Finding Description

The vulnerability exists in the interaction between block execution, state summary retrieval, and asynchronous sparse merkle tree cleanup on the consensus critical path.

**Critical Path Analysis:**

During the consensus pipeline's `ledger_update` phase, the executor retrieves the persisted state summary: [1](#0-0) 

This calls `ProvableStateSummary::new_persisted()`: [2](#0-1) 

Which invokes `get_persisted_state_summary()`: [3](#0-2) 

Leading to `get_state_summary()` which contains the blocking call: [4](#0-3) 

The `wait_for_backlog_drop(8)` method blocks the calling thread until pending drop tasks ≤ 8: [5](#0-4) 

**Attack Vector:**

The `SUBTREE_DROPPER` has limited capacity (max_tasks=32, num_threads=8): [6](#0-5) 

Each `SparseMerkleTree` `Inner` drop schedules exactly one subtree for async dropping: [7](#0-6) 

Each executed block contains a `StateCheckpointOutput` with a `StateSummary` that holds TWO `SparseMerkleTree` instances: [8](#0-7) 

When the executor's `BlockTree` prunes committed blocks, it drops the old root: [9](#0-8) 

If the old root has children representing speculative forks (which the executor BlockTree supports): [10](#0-9) 

Then dropping the old root causes a cascade where all fork blocks are dropped, each scheduling 2 `SubTree` drops (hot_state_summary + global_state_summary).

**Exploitation Scenario:**

1. **Phase 1 - State Tree Inflation:** Attacker submits transactions containing extensive state modifications (touching thousands of distinct state keys via Move smart contracts). This creates `SparseMerkleTree` instances with deep node hierarchies (depth proportional to log(N) for N keys).

2. **Phase 2 - Fork Creation:** Network conditions or Byzantine behavior causes consensus to create speculative execution forks in the executor's BlockTree (e.g., competing block proposals on the same parent).

3. **Phase 3 - Pruning Cascade:** When consensus commits a block, `BlockTree.prune()` drops the old root and all non-committed fork branches. Each dropped block contains 2 large SMTs.

4. **Phase 4 - Drop Queue Saturation:** If 5+ blocks with large SMTs are dropped simultaneously (creating 10+ drop tasks), the queue exceeds the threshold of 8 tasks. The deep node hierarchies cause individual `SubTree` drops to take significant time (seconds for trees with millions of nodes due to recursive Arc reference counting).

5. **Phase 5 - Consensus Blocking:** The next block's `ledger_update()` invokes `get_state_summary()`, which blocks on `wait_for_backlog_drop(8)` waiting for the drop queue to drain below 8 tasks. This blocks the consensus critical path, preventing the validator from completing ledger updates within the consensus timeout window.

## Impact Explanation

**High Severity** per Aptos Bug Bounty criteria - "Validator node slowdowns":

- **Liveness Impact:** Blocking `ledger_update()` prevents validators from generating `StateComputeResult` within consensus round timeouts, causing the validator to miss votes and potentially trigger round timeouts.

- **Network-Wide Effect:** If multiple validators are simultaneously affected (e.g., all processing similar workloads with large state modifications), the network could experience degraded liveness or temporary halts.

- **Non-Safety Nature:** This is a liveness attack, not a safety violation. It does not cause chain splits or double-spending, but prevents block commitment.

- **Resource Exhaustion:** The attack exploits legitimate resource management (async dropping) but causes it to interfere with time-critical consensus operations.

## Likelihood Explanation

**Medium-High Likelihood** under specific but achievable conditions:

**Prerequisites:**
- Attacker can submit transactions with extensive state modifications (limited only by gas, but high gas transactions are economically feasible)
- Network experiences proposal competition creating executor forks (occurs naturally during leader rotation or Byzantine scenarios)
- Multiple blocks accumulate before pruning (occurs during normal operation)

**Complexity:**
- Low complexity for state inflation (standard Move contract interactions)
- Medium complexity for timing the attack with pruning (requires understanding consensus behavior)
- No validator access required

**Realistic Scenarios:**
- High-throughput periods with many state-heavy transactions naturally create large SMTs
- Network partition/recovery scenarios naturally create forks
- The 8-task threshold is relatively low given the 32-task capacity, making saturation achievable

## Recommendation

**Immediate Mitigation:**

Remove the synchronous blocking call from the consensus critical path by restructuring `get_state_summary()`:

```rust
pub fn get_state_summary(&self) -> StateSummary {
    let _timer = OTHER_TIMERS_SECONDS.timer_with(&["get_persisted_state_summary"]);
    
    // REMOVED: SUBTREE_DROPPER.wait_for_backlog_drop(Self::MAX_PENDING_DROPS);
    // Backpressure should not block consensus critical operations
    
    self.summary.lock().clone()
}
```

**Long-term Solutions:**

1. **Async State Summary Retrieval:** Refactor `ledger_update()` to retrieve state summaries asynchronously before they're needed on the critical path, allowing backpressure to apply during non-critical periods.

2. **Increase Drop Capacity:** Increase `SUBTREE_DROPPER` thread count and threshold:
```rust
pub static SUBTREE_DROPPER: Lazy<AsyncConcurrentDropper> =
    Lazy::new(|| AsyncConcurrentDropper::new("smt_subtree", 64, 16));
```

3. **Priority Dropping:** Implement priority queues where drops triggered during consensus operations are deprioritized, allowing new execution to proceed.

4. **Non-blocking Backpressure:** Apply backpressure at block execution time (not ledger update) by checking drop queue size before executing new blocks, avoiding consensus path blockage.

## Proof of Concept

```rust
// File: storage/aptosdb/src/state_store/persisted_state_test.rs
#[test]
fn test_get_state_summary_blocks_on_drop_backlog() {
    use aptos_scratchpad::SUBTREE_DROPPER;
    use std::sync::Arc;
    use std::thread;
    use std::time::{Duration, Instant};
    
    // Simulate large SubTree drops that take time
    struct SlowDrop {
        size: usize,
    }
    
    impl Drop for SlowDrop {
        fn drop(&mut self) {
            // Simulate recursive drop of large tree
            thread::sleep(Duration::from_millis(self.size as u64));
        }
    }
    
    // Fill the drop queue beyond threshold
    for i in 0..12 {
        let slow = SlowDrop { size: 500 }; // 500ms per drop
        SUBTREE_DROPPER.schedule_drop(Arc::new(slow));
    }
    
    // Now measure blocking time on get_state_summary
    let persisted_state = PersistedState::new_empty(HotStateConfig::default());
    let start = Instant::now();
    
    // This should block until queue drains to ≤ 8
    let _ = persisted_state.get_state_summary();
    
    let elapsed = start.elapsed();
    
    // With 12 tasks queued, 8 threads, and 500ms drops,
    // should block for ~2 seconds (4 tasks need to complete)
    assert!(elapsed > Duration::from_secs(1),
            "get_state_summary should block on consensus critical path: {:?}", elapsed);
    
    println!("VULNERABILITY CONFIRMED: get_state_summary blocked for {:?}", elapsed);
}
```

**Reproduction Steps:**

1. Deploy Move contracts that perform extensive state writes (e.g., creating 10,000+ resource instances)
2. Submit multiple transactions invoking these contracts in rapid succession
3. During network instability causing proposal forks, monitor validator logs for consensus timeouts
4. Observe correlation between large block drops (visible in `smt_subtree` metrics) and ledger_update latency spikes

## Notes

The vulnerability stems from a design decision to apply backpressure on the "getting side" (execution) rather than the "dropping side" to prevent old SMT bases from being locked. While this design choice has merit for avoiding lock contention, it critically places a potentially long-blocking operation (`wait_for_backlog_drop`) on the consensus critical path where timeouts are measured in hundreds of milliseconds.

The comment in the code acknowledges this placement: [11](#0-10) 

However, the consensus liveness implications were apparently not fully considered, as `ledger_update()` is time-critical and cannot tolerate multi-second blocks. The vulnerability is exacerbated by the relatively low threshold (8 tasks) compared to the maximum capacity (32 tasks), and the fact that large state trees can make individual drops take significant time.

### Citations

**File:** execution/executor/src/block_executor/mod.rs (L315-320)
```rust
                output.set_state_checkpoint_output(DoStateCheckpoint::run(
                    &output.execution_output,
                    parent_block.output.ensure_result_state_summary()?,
                    &ProvableStateSummary::new_persisted(self.db.reader.as_ref())?,
                    None,
                )?);
```

**File:** storage/storage-interface/src/state_store/state_summary.rs (L30-37)
```rust
#[derive(Clone, Debug)]
pub struct StateSummary {
    /// The next version. If this is 0, the state is the "pre-genesis" empty state.
    next_version: Version,
    pub hot_state_summary: SparseMerkleTree,
    pub global_state_summary: SparseMerkleTree,
    hot_state_config: HotStateConfig,
}
```

**File:** storage/storage-interface/src/state_store/state_summary.rs (L285-287)
```rust
    pub fn new_persisted(db: &'db (dyn DbReader + Sync)) -> Result<Self> {
        Ok(Self::new(db.get_persisted_state_summary()?, db))
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L256-258)
```rust
    fn get_persisted_state_summary(&self) -> Result<StateSummary> {
        Ok(self.persisted_state.get_state_summary())
    }
```

**File:** storage/aptosdb/src/state_store/persisted_state.rs (L31-39)
```rust
    pub fn get_state_summary(&self) -> StateSummary {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["get_persisted_state_summary"]);

        // The back pressure is on the getting side (which is the execution side) so that it's less
        // likely for a lot of blocks locking the same old base SMT.
        SUBTREE_DROPPER.wait_for_backlog_drop(Self::MAX_PENDING_DROPS);

        self.summary.lock().clone()
    }
```

**File:** crates/aptos-drop-helper/src/async_concurrent_dropper.rs (L128-133)
```rust
    fn wait_for_backlog_drop(&self, no_more_than: usize) {
        let mut num_tasks = self.lock.lock();
        while *num_tasks > no_more_than {
            num_tasks = self.cvar.wait(num_tasks).expect("lock poisoned.");
        }
    }
```

**File:** storage/scratchpad/src/sparse_merkle/dropper.rs (L9-10)
```rust
pub static SUBTREE_DROPPER: Lazy<AsyncConcurrentDropper> =
    Lazy::new(|| AsyncConcurrentDropper::new("smt_subtree", 32, 8));
```

**File:** storage/scratchpad/src/sparse_merkle/mod.rs (L117-134)
```rust
impl Drop for Inner {
    fn drop(&mut self) {
        // Drop the root in a different thread, because that's the slowest part.
        SUBTREE_DROPPER.schedule_drop(self.root.take());

        let mut stack = self.drain_children_for_drop();
        while let Some(descendant) = stack.pop() {
            if Arc::strong_count(&descendant) == 1 {
                // The only ref is the one we are now holding, so the
                // descendant will be dropped after we free the `Arc`, which results in a chain
                // of such structures being dropped recursively and that might trigger a stack
                // overflow. To prevent that we follow the chain further to disconnect things
                // beforehand.
                stack.extend(descendant.drain_children_for_drop());
            }
        }
        self.log_generation("drop");
    }
```

**File:** execution/executor/src/block_executor/block_tree/mod.rs (L27-32)
```rust
pub struct Block {
    pub id: HashValue,
    pub output: PartialStateComputeResult,
    children: Mutex<Vec<Arc<Block>>>,
    block_lookup: Arc<BlockLookup>,
}
```

**File:** execution/executor/src/block_executor/block_tree/mod.rs (L264-267)
```rust
        let old_root = std::mem::replace(&mut *self.root.lock(), root);

        // send old root to async task to drop it
        Ok(DEFAULT_DROPPER.schedule_drop_with_waiter(old_root))
```
