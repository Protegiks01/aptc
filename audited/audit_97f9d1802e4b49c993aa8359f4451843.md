# Audit Report

## Title
State View Race Condition in Remote Executor Error Path Causes Cross-Block State Contamination

## Summary
The `RemoteExecutorClient::execute_block()` function fails to call `drop_state_view()` in the error path, allowing stale state view references to persist when execution errors occur. Combined with per-key locking in `RemoteStateViewService::handle_message()`, this creates a race condition where in-flight state view requests from a failed block can read from the next block's state view, producing responses with mixed state values that violate deterministic execution invariants.

## Finding Description

The vulnerability exists in two interconnected components:

**Component 1: Missing Cleanup in Error Path** [1](#0-0) 

When `get_output_from_shards()` returns a `VMStatus` error on line 208, the `?` operator causes an early return, bypassing the `drop_state_view()` call on line 210. This leaves the old state view reference active in `RemoteStateViewService`.

**Component 2: Per-Key Read Locking** [2](#0-1) 

The `handle_message()` function acquires and releases the read lock **for each individual key** within the `.map()` closure. Between processing different keys, the read lock is released, creating a window where another thread can acquire the write lock and swap the state view via `set_state_view()`.

**Attack Scenario:**

1. **Block N Execution Starts**: `execute_block()` calls `set_state_view(state_view_V)` for block N at version V
2. **Execution Commands Sent**: Commands are sent to 4 executor shards
3. **Shards Request State**: All shards begin sending batched `RemoteKVRequest` messages (200 keys per batch) to the coordinator
4. **Multiple Threads Spawn**: Multiple `handle_message()` threads spawn to process these requests
5. **Shard 0 Encounters Error**: Shard 0 encounters a VM error (e.g., transaction causes out-of-gas or Move abort) and sends `RemoteExecutionResult` with error
6. **Early Return**: `get_output_from_shards()` receives the error from shard 0 and immediately returns `Err(VMStatus)` **without waiting for shards 1-3**
7. **State View Not Dropped**: The `?` operator causes early return, **skipping `drop_state_view()`**. The state view V remains set.
8. **Mutex Released**: The `REMOTE_SHARDED_BLOCK_EXECUTOR` mutex is released when `execute_block()` returns [3](#0-2) 

9. **Shards 1-3 Still Processing**: Meanwhile, `handle_message()` threads processing requests from shards 1-3 are still active, with some threads mid-way through their 200-key batches
10. **Next Block Starts**: Block N+1 execution starts (or retry of block N), acquiring the mutex and calling `set_state_view(state_view_V+1)` [4](#0-3) 

11. **State View Swapped**: The write lock is acquired and the state view is overwritten with V+1
12. **Race Condition Triggered**: The still-running `handle_message()` threads now acquire read locks for their remaining keys and read from state view V+1 instead of V
13. **Contaminated Response**: Responses are sent back containing a mixture of state values from version V (keys processed before swap) and version V+1 (keys processed after swap)

**Invariant Violation:**

This violates the critical invariant: **"All validators must produce identical state roots for identical blocks"** and **"State transitions must be atomic and verifiable via Merkle proofs"**. A single state view request intended for block N returns values from two different ledger versions, creating a state view that never existed at any atomic point in time.

## Impact Explanation

**Severity: High**

This vulnerability causes **state inconsistency** and **non-deterministic execution**:

1. **Consensus Violation**: Different validators may receive different state values depending on the timing of the race condition, potentially causing them to compute different state roots for the same block
2. **Merkle Proof Verification Failure**: The mixed state values cannot be verified against either version V or V+1's Merkle root, breaking state authenticity guarantees  
3. **Execution Non-Determinism**: The same block could produce different outputs on different nodes based on race timing

Per Aptos bug bounty criteria, this qualifies as **High Severity** due to "Significant protocol violations" that could lead to consensus inconsistencies.

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability is triggerable when:
1. **VM Errors Occur**: Any transaction causing VM errors (out of gas, Move aborts, failed assertions) can trigger the error path. Attackers can craft such transactions.
2. **Multi-Shard Execution**: The default sharded executor configuration uses multiple shards, making concurrent `handle_message()` threads the normal case
3. **Network/Processing Latency**: If shards 1-3 are slower to complete than shard 0, their `handle_message()` threads will overlap with the next block execution [5](#0-4) 

With batches of 200 keys per request and potentially multiple batches per shard, the processing window is substantial (milliseconds to seconds depending on database performance), making the race window exploitable.

## Recommendation

**Fix 1: Add Drop Guard for State View Cleanup**

Ensure `drop_state_view()` is called even in error paths using RAII or defer pattern:

```rust
impl<S: StateView + Sync + Send + 'static> ExecutorClient<S> for RemoteExecutorClient<S> {
    fn execute_block(
        &self,
        state_view: Arc<S>,
        transactions: PartitionedTransactions,
        concurrency_level_per_shard: usize,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<ShardedExecutionOutput, VMStatus> {
        trace!("RemoteExecutorClient Sending block to shards");
        self.state_view_service.set_state_view(state_view);
        
        // Use a guard to ensure cleanup happens even on error
        let _cleanup_guard = scopeguard::guard((), |_| {
            self.state_view_service.drop_state_view();
        });
        
        let (sub_blocks, global_txns) = transactions.into();
        if !global_txns.is_empty() {
            panic!("Global transactions are not supported yet");
        }
        for (shard_id, sub_blocks) in sub_blocks.into_iter().enumerate() {
            // ... send commands ...
        }
        
        let execution_results = self.get_output_from_shards()?;
        Ok(ShardedExecutionOutput::new(execution_results, vec![]))
    }
}
```

**Fix 2: Hold Read Lock for Entire Batch**

Acquire the read lock once for all keys instead of per-key:

```rust
pub fn handle_message(
    message: Message,
    state_view: Arc<RwLock<Option<Arc<S>>>>,
    kv_tx: Arc<Vec<Sender<Message>>>,
) {
    // ... deserialization ...
    
    let (shard_id, state_keys) = req.into();
    
    // Acquire lock ONCE for the entire batch
    let state_view_guard = state_view.read().unwrap();
    let state_view_ref = state_view_guard.as_ref().unwrap();
    
    let resp = state_keys
        .into_iter()
        .map(|state_key| {
            let state_value = state_view_ref
                .get_state_value(&state_key)
                .unwrap();
            (state_key, state_value)
        })
        .collect_vec();
    
    drop(state_view_guard); // Explicitly drop before sending
    
    // ... send response ...
}
```

**Fix 3: Wait for All Shards Before Returning**

Modify `get_output_from_shards()` to drain all shard responses even when errors occur, preventing orphaned in-flight requests.

## Proof of Concept

```rust
#[cfg(test)]
mod race_condition_test {
    use super::*;
    use std::sync::{Arc, atomic::{AtomicBool, Ordering}};
    use std::thread;
    use std::time::Duration;
    
    #[test]
    fn test_state_view_race_on_error() {
        // Setup: Create remote executor client with state view service
        let mut controller = NetworkController::new(/*...*/);
        let client = RemoteExecutorClient::new(vec![/* shard addresses */], controller, None);
        
        // Simulate Block N execution
        let state_view_v1 = Arc::new(/* state at version 1 */);
        let state_view_v2 = Arc::new(/* state at version 2 */);
        
        let race_detected = Arc::new(AtomicBool::new(false));
        let race_detected_clone = race_detected.clone();
        
        // Thread 1: Simulate handle_message processing many keys slowly
        thread::spawn(move || {
            // Simulate slow state view request processing
            for i in 0..200 {
                thread::sleep(Duration::from_millis(10));
                // Check if state view changed mid-processing
                let current_view = /* read current state_view */;
                if i < 100 && current_view == state_view_v2 {
                    race_detected_clone.store(true, Ordering::SeqCst);
                }
            }
        });
        
        thread::sleep(Duration::from_millis(500)); // Let thread process ~50 keys
        
        // Thread 2: Simulate error return and next block setting new state view
        // This mimics execute_block returning early on error without drop_state_view()
        client.state_view_service.set_state_view(state_view_v2);
        
        thread::sleep(Duration::from_millis(2000)); // Wait for completion
        
        assert!(race_detected.load(Ordering::SeqCst), 
                "Race condition detected: handle_message read from new state view mid-processing");
    }
}
```

**Notes**

The vulnerability requires the confluence of: (1) sharded execution with VM errors, (2) missing error path cleanup, and (3) per-key locking granularity. While the `REMOTE_SHARDED_BLOCK_EXECUTOR` mutex prevents concurrent block executions from the same node, it does not prevent the state view swap race when errors cause early returns. The fix requires both proper resource cleanup (RAII pattern) and atomic batch-level locking to ensure state view consistency guarantees.

### Citations

**File:** execution/executor-service/src/remote_executor_client.rs (L180-212)
```rust
    fn execute_block(
        &self,
        state_view: Arc<S>,
        transactions: PartitionedTransactions,
        concurrency_level_per_shard: usize,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<ShardedExecutionOutput, VMStatus> {
        trace!("RemoteExecutorClient Sending block to shards");
        self.state_view_service.set_state_view(state_view);
        let (sub_blocks, global_txns) = transactions.into();
        if !global_txns.is_empty() {
            panic!("Global transactions are not supported yet");
        }
        for (shard_id, sub_blocks) in sub_blocks.into_iter().enumerate() {
            let senders = self.command_txs.clone();
            let execution_request = RemoteExecutionRequest::ExecuteBlock(ExecuteBlockCommand {
                sub_blocks,
                concurrency_level: concurrency_level_per_shard,
                onchain_config: onchain_config.clone(),
            });

            senders[shard_id]
                .lock()
                .unwrap()
                .send(Message::new(bcs::to_bytes(&execution_request).unwrap()))
                .unwrap();
        }

        let execution_results = self.get_output_from_shards()?;

        self.state_view_service.drop_state_view();
        Ok(ShardedExecutionOutput::new(execution_results, vec![]))
    }
```

**File:** execution/executor-service/src/remote_state_view_service.rs (L54-57)
```rust
    pub fn set_state_view(&self, state_view: Arc<S>) {
        let mut state_view_lock = self.state_view.write().unwrap();
        *state_view_lock = Some(state_view);
    }
```

**File:** execution/executor-service/src/remote_state_view_service.rs (L95-107)
```rust
        let resp = state_keys
            .into_iter()
            .map(|state_key| {
                let state_value = state_view
                    .read()
                    .unwrap()
                    .as_ref()
                    .unwrap()
                    .get_state_value(&state_key)
                    .unwrap();
                (state_key, state_value)
            })
            .collect_vec();
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L256-276)
```rust
    fn execute_block_sharded<V: VMBlockExecutor>(
        partitioned_txns: PartitionedTransactions,
        state_view: Arc<CachedStateView>,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<Vec<TransactionOutput>> {
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        } else {
            Ok(V::execute_block_sharded(
                &SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        }
    }
```

**File:** execution/executor-service/src/remote_state_view.rs (L27-27)
```rust
pub static REMOTE_STATE_KEY_BATCH_SIZE: usize = 200;
```
