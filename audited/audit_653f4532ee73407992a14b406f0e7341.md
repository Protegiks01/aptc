# Audit Report

## Title
Consensus Observer Liveness Failure Due to Silent Payload Dropping Without Recovery Mechanism

## Summary
The consensus observer's payload store silently drops incoming payloads when the store exceeds `max_num_pending_blocks` (default: 150). When ordered blocks later arrive requiring these dropped payloads, they are also dropped, causing the node to stop making progress. While a fallback mechanism eventually recovers after 10 seconds of no progress, this creates a temporary liveness failure window of 10-70 seconds.

## Finding Description

The vulnerability exists in the consensus observer component, which allows validator full nodes to observe consensus decisions without participating in consensus. The issue involves three interconnected components:

**1. Silent Payload Dropping:**
When `BlockPayloadStore::insert_block_payload()` receives a new payload but the store already contains >= `max_num_pending_blocks` entries, it silently drops the payload with only a warning log. [1](#0-0) 

**2. Ordered Block Rejection:**
When an `OrderedBlock` later arrives requiring the dropped payload, `verify_payloads_against_ordered_block()` checks if all required payloads exist. For missing payloads, it returns an error. [2](#0-1) 

The ordered block is then dropped with an error log, and the node cannot progress. [3](#0-2) 

**3. Delayed Recovery:**
The node relies on the `ObserverFallbackManager` to detect lack of progress and trigger state sync fallback. This check runs every 5 seconds and triggers fallback mode if no progress is made for 10 seconds after a 60-second startup period. [4](#0-3) [5](#0-4) 

**Attack Scenario:**

1. **Payload Store Saturation:** An attacker or network congestion causes the payload store to accumulate 150+ entries (the default `max_num_pending_blocks` limit). [6](#0-5) 

2. **Critical Payload Drop:** A legitimate payload for round X arrives but is silently dropped because the limit is exceeded.

3. **Ordered Block Arrival:** The ordered block for round X arrives and verification fails due to missing payload.

4. **Progress Halt:** The node cannot commit round X, and because it cannot progress, older payloads cannot be garbage collected via `remove_committed_blocks()`. [7](#0-6) 

5. **Liveness Window:** The node remains stalled for 10-70 seconds (10 seconds after the 60-second startup period) until the fallback manager detects no progress and triggers state sync. [8](#0-7) 

This breaks the **liveness invariant** - consensus observer nodes should continuously make progress or quickly recover from transient failures.

## Impact Explanation

**Severity: High**

This vulnerability meets the **High Severity** criteria from the Aptos bug bounty program:
- **"Validator node slowdowns"** - Consensus observer nodes (typically validator full nodes) experience 10-70 seconds of complete liveness failure
- **"Significant protocol violations"** - Temporary denial of service affecting node availability

While not Critical severity (requires recovery via hardfork), this is not a permanent failure. The automatic recovery mechanism via the fallback manager ensures eventual recovery through state sync. However, the 10-70 second window represents a significant operational impact:

- **Validator Full Nodes (VFNs):** Cannot observe consensus, breaking their role in the network architecture
- **Service Disruption:** APIs and queries relying on these nodes fail during the liveness window
- **Cascading Effects:** If multiple observer nodes are affected simultaneously (e.g., during network-wide bursts), it degrades overall network observability

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability can be triggered through multiple realistic scenarios:

**1. Legitimate Network Congestion:**
- High-throughput periods (e.g., 1000+ TPS) with quorum store batching can generate payloads faster than ordered blocks are committed
- If a node falls slightly behind, payloads accumulate rapidly
- 150 blocks Ã— ~1 second per round = ~2.5 minutes of backlog to fill the store

**2. Malicious Publisher Attack:**
- A compromised or malicious consensus publisher can deliberately flood observer nodes with valid but unnecessary payloads
- No authentication prevents publishers from sending excess payloads
- Attack cost is low (only network bandwidth)

**3. Network Partition/Delay:**
- Temporary network delays cause payload messages to arrive faster than ordered blocks
- Observer node accumulates payloads while waiting for ordered blocks
- When ordered block finally arrives, its payload may have been dropped

**Attacker Requirements:**
- For malicious attack: Ability to send consensus observer messages (achievable by any peer)
- For natural occurrence: High network throughput or temporary node lag

**Complexity:** Low - exploit requires only message flooding or waiting for network congestion.

## Recommendation

Implement a **payload request and retry mechanism** to recover from dropped payloads:

```rust
pub fn insert_block_payload(
    &mut self,
    block_payload: BlockPayload,
    verified_payload_signatures: bool,
) {
    let max_num_pending_blocks = self.consensus_observer_config.max_num_pending_blocks as usize;
    if self.block_payloads.lock().len() >= max_num_pending_blocks {
        // NEW: Attempt to garbage collect old payloads first
        self.garbage_collect_old_unverified_payloads();
        
        // If still full, check if this payload is newer than oldest
        if self.block_payloads.lock().len() >= max_num_pending_blocks {
            let oldest_entry = self.block_payloads.lock().first_entry();
            let new_payload_epoch_round = (block_payload.epoch(), block_payload.round());
            
            if let Some(oldest) = oldest_entry {
                if new_payload_epoch_round > *oldest.key() {
                    // Drop the oldest payload instead
                    let removed = oldest.remove();
                    warn!(
                        LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                            "Payload store full. Dropping oldest payload: {:?} to make room for newer: {:?}",
                            removed, block_payload.block()
                        ))
                    );
                } else {
                    // Drop the new payload but log more prominently
                    error!(
                        LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                            "Dropping payload for block {:?} due to store limit",
                            block_payload.block()
                        ))
                    );
                    return;
                }
            }
        }
    }
    
    // Rest of insertion logic...
}

// NEW: Add garbage collection for unverified old payloads
fn garbage_collect_old_unverified_payloads(&mut self) {
    let cutoff_rounds = 100; // Keep only recent unverified payloads
    let mut to_remove = vec![];
    
    let block_payloads = self.block_payloads.lock();
    if let Some((highest_epoch, highest_round)) = block_payloads.last_key_value() {
        for (epoch_round, status) in block_payloads.iter() {
            if let BlockPayloadStatus::AvailableAndUnverified(_) = status {
                if epoch_round.1 < highest_round.saturating_sub(cutoff_rounds) {
                    to_remove.push(*epoch_round);
                }
            }
        }
    }
    drop(block_payloads);
    
    // Remove old unverified payloads
    for epoch_round in to_remove {
        self.block_payloads.lock().remove(&epoch_round);
    }
}
```

**Additional Recommendations:**

1. **Add payload re-request mechanism:** When `verify_payloads_against_ordered_block()` fails, actively request the missing payload from peers instead of just dropping the ordered block

2. **Reduce fallback threshold:** Lower `observer_fallback_progress_threshold_ms` from 10 seconds to 5 seconds for faster recovery

3. **Increase monitoring:** Add metrics for dropped payloads and trigger alerts when drop rate exceeds threshold

4. **Dynamic limit adjustment:** Allow `max_num_pending_blocks` to auto-scale based on network throughput

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    
    #[test]
    fn test_payload_dropping_causes_liveness_failure() {
        // Create consensus observer config with small limit
        let max_num_pending_blocks = 5;
        let consensus_observer_config = ConsensusObserverConfig {
            max_num_pending_blocks,
            ..ConsensusObserverConfig::default()
        };
        
        let mut block_payload_store = BlockPayloadStore::new(consensus_observer_config);
        
        // Fill the payload store to capacity
        let mut filled_blocks = vec![];
        for i in 0..max_num_pending_blocks {
            let block = create_block_payload(0, i as Round);
            block_payload_store.insert_block_payload(block.clone(), true);
            filled_blocks.push(create_pipelined_block(0, i as Round));
        }
        
        // Verify store is full
        assert_eq!(block_payload_store.block_payloads.lock().len(), max_num_pending_blocks as usize);
        
        // Attempt to insert critical payload - it will be DROPPED
        let critical_round = max_num_pending_blocks as Round;
        let critical_payload = create_block_payload(0, critical_round);
        block_payload_store.insert_block_payload(critical_payload.clone(), true);
        
        // Verify the critical payload was NOT inserted (silently dropped)
        assert_eq!(block_payload_store.block_payloads.lock().len(), max_num_pending_blocks as usize);
        assert!(!block_payload_store.block_payloads.lock().contains_key(&(0, critical_round)));
        
        // Now create an ordered block requiring the critical payload
        let critical_block = create_pipelined_block(0, critical_round);
        let ordered_block = OrderedBlock::new(
            vec![Arc::new(critical_block)],
            create_empty_ledger_info(0),
        );
        
        // Attempt to verify - this FAILS because payload is missing
        let result = block_payload_store.verify_payloads_against_ordered_block(&ordered_block);
        
        // VULNERABILITY: Ordered block is rejected, node cannot progress
        assert!(result.is_err());
        assert_matches!(result.unwrap_err(), Error::InvalidMessageError(_));
        
        // At this point:
        // 1. The payload was silently dropped
        // 2. The ordered block is rejected
        // 3. The node cannot commit this round
        // 4. No recovery mechanism exists except fallback manager timeout (10-70 seconds)
        // 5. LIVENESS FAILURE until fallback triggers
    }
}
```

## Notes

This vulnerability affects **consensus observer nodes only**, not consensus participants (validators). However, consensus observer is enabled by default on Validator Full Nodes (VFNs) in production networks, making this a significant operational concern. [9](#0-8) 

The vulnerability requires either network stress conditions or active exploitation, but the attack surface is accessible to any network peer. The automatic recovery via fallback manager prevents permanent failure but does not eliminate the 10-70 second liveness window, which constitutes a denial of service attack vector.

### Citations

**File:** consensus/src/consensus_observer/observer/payload_store.rs (L84-95)
```rust
        // Verify that the number of payloads doesn't exceed the maximum
        let max_num_pending_blocks = self.consensus_observer_config.max_num_pending_blocks as usize;
        if self.block_payloads.lock().len() >= max_num_pending_blocks {
            warn!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Exceeded the maximum number of payloads: {:?}. Dropping block: {:?}!",
                    max_num_pending_blocks,
                    block_payload.block(),
                ))
            );
            return; // Drop the block if we've exceeded the maximum
        }
```

**File:** consensus/src/consensus_observer/observer/payload_store.rs (L122-129)
```rust
    pub fn remove_committed_blocks(&self, committed_blocks: &[Arc<PipelinedBlock>]) {
        // Get the highest epoch and round for the committed blocks
        let (highest_epoch, highest_round) = committed_blocks
            .last()
            .map_or((0, 0), |block| (block.epoch(), block.round()));

        // Remove the blocks
        self.remove_blocks_for_epoch_round(highest_epoch, highest_round);
```

**File:** consensus/src/consensus_observer/observer/payload_store.rs (L201-208)
```rust
                Entry::Vacant(_) => {
                    // The payload is missing (this should never happen)
                    return Err(Error::InvalidMessageError(format!(
                        "Payload verification failed! Missing block payload for epoch: {:?} and round: {:?}",
                        ordered_block.epoch(),
                        ordered_block.round()
                    )));
                },
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L191-200)
```rust
        if let Err(error) = self.observer_fallback_manager.check_syncing_progress() {
            // Log the error and enter fallback mode
            warn!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Failed to make syncing progress! Entering fallback mode! Error: {:?}",
                    error
                ))
            );
            self.enter_fallback_mode().await;
            return;
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L754-771)
```rust
        // Verify the block payloads against the ordered block
        if let Err(error) = self
            .observer_block_data
            .lock()
            .verify_payloads_against_ordered_block(&ordered_block)
        {
            // Log the error and update the invalid message counter
            error!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Failed to verify block payloads against ordered block! Ignoring: {:?}, from peer: {:?}. Error: {:?}",
                    ordered_block.proof_block_info(),
                    peer_network_id,
                    error
                ))
            );
            increment_invalid_message_counter(&peer_network_id, metrics::ORDERED_BLOCK_LABEL);
            return;
        }
```

**File:** consensus/src/consensus_observer/observer/fallback_manager.rs (L97-111)
```rust
        if latest_ledger_info_version <= highest_synced_version {
            // The synced version hasn't increased. Check if we should enter fallback mode.
            let duration_since_highest_seen = time_now.duration_since(highest_version_timestamp);
            let fallback_threshold = Duration::from_millis(
                self.consensus_observer_config
                    .observer_fallback_progress_threshold_ms,
            );
            if duration_since_highest_seen > fallback_threshold {
                Err(Error::ObserverProgressStopped(format!(
                    "Consensus observer is not making progress! Highest synced version: {}, elapsed: {:?}",
                    highest_synced_version, duration_since_highest_seen
                )))
            } else {
                Ok(()) // We haven't passed the fallback threshold yet
            }
```

**File:** config/src/config/consensus_observer_config.rs (L12-13)
```rust
const ENABLE_ON_VALIDATORS: bool = true;
const ENABLE_ON_VALIDATOR_FULLNODES: bool = true;
```

**File:** config/src/config/consensus_observer_config.rs (L72-72)
```rust
            max_num_pending_blocks: 150, // 150 blocks (sufficient for existing production networks)
```

**File:** config/src/config/consensus_observer_config.rs (L80-82)
```rust
            observer_fallback_startup_period_ms: 60_000, // 60 seconds
            observer_fallback_progress_threshold_ms: 10_000, // 10 seconds
            observer_fallback_sync_lag_threshold_ms: 15_000, // 15 seconds
```
