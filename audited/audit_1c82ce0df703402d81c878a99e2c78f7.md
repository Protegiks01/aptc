# Audit Report

## Title
Unbounded Memory Allocation in Indexer File Store Reader Leading to Denial of Service

## Summary
The `get_raw_file()` function in the indexer's local file store operator loads entire transaction files into memory without size validation, creating a potential denial-of-service vulnerability. An attacker who can influence file contents (either through on-chain transactions or direct file system access) can cause out-of-memory conditions and crash the indexer service.

## Finding Description

The vulnerability exists in the file reading mechanism: [1](#0-0) 

The function uses `tokio::fs::read()` which loads the entire file into memory with no size check. Each transaction file contains exactly 1000 transactions as enforced by the batch upload logic: [2](#0-1) 

Individual transactions can contain significant data when serialized to protobuf. The VM enforces these limits on transaction outputs: [3](#0-2) 

Each transaction can have up to 10MB of write operations and 10MB of events. The protobuf conversion includes full write set data: [4](#0-3) 

This means transactions are serialized with their complete state changes. While gas limits constrain individual transaction sizes in practice, files with multiple large transactions can still reach hundreds of megabytes. The indexer reads these files without validation when serving data or during recovery operations.

**Attack Vectors:**
1. **On-chain attack:** Submit transactions that maximize write operations and events within gas limits. When 1000 such transactions are batched into a file, the cumulative size can be substantial.
2. **Insider threat:** Direct file system access allows creation of arbitrarily large files that will crash the indexer when read.
3. **Concurrent load:** Multiple simultaneous read operations on large files can exhaust available memory even with moderately-sized files.

## Impact Explanation

This vulnerability falls under **High Severity** according to Aptos bug bounty criteria:
- **API crashes**: The indexer-grpc service crashes when attempting to read oversized files, disrupting blockchain data availability
- **Service disruption**: Indexers are critical infrastructure for dApps, wallets, and explorers that depend on transaction data

The impact affects indexer availability but not consensus, as the indexer runs separately from validator nodes. However, it breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits" - memory limits are not respected during file reads.

## Likelihood Explanation

**Medium to High likelihood:**

**On-chain exploitation:** While creating maximum-size transactions is expensive, it's technically feasible. An attacker with sufficient funds can:
- Submit transactions with substantial write operations (e.g., large table updates, module deployments)
- Target specific version ranges to create large files
- Force indexers to crash when accessing those files

**Insider exploitation:** Any operator with file system access can trivially exploit this by placing large files in the storage directory.

**Concurrent load scenario:** Under normal operation with high transaction throughput, multiple large files being read simultaneously could trigger OOM even without malicious intent.

The v2 file store operator implements size limits (50MB), but v1 remains vulnerable: [5](#0-4) 

## Recommendation

Implement file size validation before loading files into memory:

```rust
async fn get_raw_file(&self, version: u64) -> anyhow::Result<Vec<u8>> {
    const MAX_FILE_SIZE: u64 = 100 * 1024 * 1024; // 100MB limit
    
    let file_entry_key = FileEntry::build_key(version, self.storage_format).to_string();
    let file_path = self.path.join(file_entry_key);
    
    // Check file size before reading
    match tokio::fs::metadata(&file_path).await {
        Ok(metadata) => {
            let file_size = metadata.len();
            if file_size > MAX_FILE_SIZE {
                anyhow::bail!(
                    "[Indexer File] File size {} exceeds maximum allowed size {}",
                    file_size,
                    MAX_FILE_SIZE
                );
            }
        },
        Err(err) if err.kind() == std::io::ErrorKind::NotFound => {
            anyhow::bail!("[Indexer File] Transactions file not found. {}", err)
        },
        Err(err) => {
            anyhow::bail!("[Indexer File] Error accessing transaction file. {}", err)
        },
    }
    
    match tokio::fs::read(file_path).await {
        Ok(file) => Ok(file),
        Err(err) => {
            anyhow::bail!("[Indexer File] Error reading transaction file. {}", err)
        },
    }
}
```

Apply the same fix to the GCS implementation: [6](#0-5) 

## Proof of Concept

```rust
#[cfg(test)]
mod test_file_size_dos {
    use super::*;
    use tempfile::tempdir;
    
    #[tokio::test]
    async fn test_large_file_causes_oom() {
        // Create a temporary directory
        let temp_dir = tempdir().unwrap();
        let temp_path = temp_dir.path().to_path_buf();
        
        // Create a local file store operator
        let operator = LocalFileStoreOperator::new(temp_path.clone(), false);
        
        // Create a maliciously large file (1GB)
        let file_name = "files/0.json";
        let file_path = temp_path.join(file_name);
        std::fs::create_dir_all(file_path.parent().unwrap()).unwrap();
        
        // Write 1GB of data
        let chunk = vec![0u8; 1024 * 1024]; // 1MB
        let mut file = std::fs::File::create(&file_path).unwrap();
        for _ in 0..1024 { // 1024 x 1MB = 1GB
            file.write_all(&chunk).unwrap();
        }
        
        // Attempt to read - this should fail with OOM or take excessive memory
        // In production, this would crash the indexer
        let result = operator.get_raw_file(0).await;
        
        // Without size limits, this either:
        // 1. Consumes 1GB of memory (DoS)
        // 2. Crashes with OOM
        // With proper validation, it should fail gracefully
        assert!(result.is_err() || result.unwrap().len() == 1024 * 1024 * 1024);
    }
}
```

## Notes

While the newer v2 file store operator implements proper size limits, the v1 operator remains in the codebase and vulnerable. Organizations running indexers with the v1 implementation face this risk. The fix should be backported to v1 for defense-in-depth, even if v2 is the recommended version.

The GCS implementation has the same vulnerability pattern and requires the same fix. Both local and cloud storage backends should validate file sizes before loading into memory.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/local.rs (L58-74)
```rust
    async fn get_raw_file(&self, version: u64) -> anyhow::Result<Vec<u8>> {
        let file_entry_key = FileEntry::build_key(version, self.storage_format).to_string();
        let file_path = self.path.join(file_entry_key);
        match tokio::fs::read(file_path).await {
            Ok(file) => Ok(file),
            Err(err) => {
                if err.kind() == std::io::ErrorKind::NotFound {
                    anyhow::bail!("[Indexer File] Transactions file not found. Gap might happen between cache and file store. {}", err)
                } else {
                    anyhow::bail!(
                        "[Indexer File] Error happens when transaction file. {}",
                        err
                    );
                }
            },
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/local.rs (L148-163)
```rust
    /// TODO: rewrite this function to be similar to the general version
    async fn upload_transaction_batch(
        &mut self,
        chain_id: u64,
        transactions: Vec<Transaction>,
    ) -> anyhow::Result<(u64, u64)> {
        let start_version = transactions.first().unwrap().version;
        let batch_size = transactions.len();
        anyhow::ensure!(
            start_version % FILE_ENTRY_TRANSACTION_COUNT == 0,
            "Starting version has to be a multiple of BLOB_STORAGE_SIZE."
        );
        anyhow::ensure!(
            batch_size % FILE_ENTRY_TRANSACTION_COUNT as usize == 0,
            "The number of transactions to upload has to be multiplier of BLOB_STORAGE_SIZE."
        );
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L154-172)
```rust
            max_bytes_per_write_op: NumBytes,
            { 5.. => "max_bytes_per_write_op" },
            1 << 20, // a single state item is 1MB max
        ],
        [
            max_bytes_all_write_ops_per_transaction: NumBytes,
            { 5.. => "max_bytes_all_write_ops_per_transaction" },
            10 << 20, // all write ops from a single transaction are 10MB max
        ],
        [
            max_bytes_per_event: NumBytes,
            { 5.. => "max_bytes_per_event" },
            1 << 20, // a single event is 1MB max
        ],
        [
            max_bytes_all_events_per_transaction: NumBytes,
            { 5.. => "max_bytes_all_events_per_transaction"},
            10 << 20, // all events from a single transaction are 10MB max
        ],
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/convert.rs (L410-435)
```rust
        WriteSetChange::WriteModule(write_module) => transaction::WriteSetChange {
            r#type: transaction::write_set_change::Type::WriteModule as i32,
            change: Some(transaction::write_set_change::Change::WriteModule(
                transaction::WriteModule {
                    address: write_module.address.to_string(),
                    state_key_hash: convert_hex_string_to_bytes(&write_module.state_key_hash),
                    data: Some(convert_move_module_bytecode(&write_module.data)),
                },
            )),
        },
        WriteSetChange::WriteResource(write_resource) => transaction::WriteSetChange {
            r#type: transaction::write_set_change::Type::WriteResource as i32,
            change: Some(transaction::write_set_change::Change::WriteResource(
                transaction::WriteResource {
                    address: write_resource.address.to_string(),
                    state_key_hash: convert_hex_string_to_bytes(&write_resource.state_key_hash),
                    r#type: Some(convert_move_struct_tag(&write_resource.data.typ)),
                    type_str: write_resource.data.typ.to_string(),
                    data: serde_json::to_string(&write_resource.data.data).unwrap_or_else(|_| {
                        panic!(
                            "Could not convert move_resource data to json '{:?}'",
                            write_resource.data
                        )
                    }),
                },
            )),
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/file_store_uploader.rs (L27-27)
```rust
const MAX_SIZE_PER_FILE: usize = 50 * (1 << 20);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/gcs.rs (L103-124)
```rust
    async fn get_raw_file(&self, version: u64) -> anyhow::Result<Vec<u8>> {
        let file_entry_key_path = self.get_file_entry_key_path(version);
        match Object::download(&self.bucket_name, file_entry_key_path.as_str()).await {
            Ok(file) => Ok(file),
            Err(cloud_storage::Error::Other(err)) => {
                if err.contains("No such object: ") {
                    anyhow::bail!("[Indexer File] Transactions file not found. Gap might happen between cache and file store. {}", err)
                } else {
                    anyhow::bail!(
                        "[Indexer File] Error happens when downloading transaction file. {}",
                        err
                    );
                }
            },
            Err(err) => {
                anyhow::bail!(
                    "[Indexer File] Error happens when transaction file. {}",
                    err
                );
            },
        }
    }
```
