# Audit Report

## Title
Non-Atomic Cross-Shard Writes Enable State Inconsistency After Node Crash

## Summary
When storage sharding is enabled, `LedgerDb::write_schemas()` writes to 8 separate RocksDB instances sequentially without atomic cross-database guarantees. A crash during these sequential writes can leave databases in an inconsistent state where some sub-databases contain data for version N while others don't, violating the atomicity invariant and potentially causing consensus divergence.

## Finding Description

The LedgerDb module coordinates writes across 8 sub-databases: `write_set_db`, `transaction_info_db`, `transaction_db`, `persisted_auxiliary_info_db`, `event_db`, `transaction_accumulator_db`, `transaction_auxiliary_data_db`, and `ledger_metadata_db`. [1](#0-0) 

This implementation writes to each sub-database sequentially by calling `write_schemas()` on each one. When sharding is enabled, each sub-database is a **separate RocksDB instance** with its own write-ahead log and durability guarantees. [2](#0-1) 

The vulnerability manifests during the parallel write execution in `calculate_and_commit_ledger_and_state_kv()`, where 7 threads write to different databases concurrently using `.unwrap()` (which panics on error): [3](#0-2) 

The code explicitly acknowledges this issue via TODO comments: [4](#0-3) 

**Attack Scenario:**

1. Node commits transaction at version N with storage sharding enabled
2. During `LedgerDb::write_schemas()` sequential execution:
   - `transaction_info_db.write_schemas()` completes and flushes to disk ✓
   - `transaction_accumulator_db.write_schemas()` completes and flushes to disk ✓
   - **Node crashes** (power failure, OOM kill, disk full, hardware fault)
   - `event_db.write_schemas()` never executes ✗
   - `transaction_db.write_schemas()` never executes ✗

3. State after crash:
   - `transaction_info_db` contains TransactionInfo for version N
   - `transaction_accumulator_db` contains accumulator nodes for version N
   - `event_db` is missing events for version N
   - `transaction_db` is missing transaction data for version N
   - `OverallCommitProgress` remains at N-1 (not yet updated)

4. On restart, `sync_commit_progress()` attempts recovery: [5](#0-4) 

5. The truncation process itself has the same vulnerability - it writes sequentially: [6](#0-5) 

**The Critical Issue:** If a second crash occurs during truncation (after line 358 writes `LedgerCommitProgress` but before line 360 completes), the system enters a permanently inconsistent state where progress markers claim version N is truncated but partial data remains in some databases.

## Impact Explanation

**Severity: Medium** (up to $10,000 per Aptos bug bounty criteria)

This vulnerability breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs."

**Concrete Impacts:**

1. **Consensus Divergence Risk**: If different validators experience crashes at different points during writes, they may end up with different subsets of data for the same version, leading to different state root calculations when attempting to re-execute blocks.

2. **Proof Verification Failures**: When nodes attempt to generate proofs for transactions at the affected version, the mismatch between `transaction_info` (which references an `event_root_hash`) and the actual events stored in `event_db` will cause proof verification to fail when serving data to peers.

3. **Recovery Complexity**: The maximum commit progress difference is set to 1,000,000 versions: [7](#0-6) 

If crashes occur repeatedly during high-throughput periods, the window of inconsistency could span hundreds of thousands of transactions.

This qualifies as "State inconsistencies requiring intervention" under Medium severity, as manual intervention may be needed to recover nodes stuck in inconsistent states.

## Likelihood Explanation

**Likelihood: Medium to High** in production environments

The vulnerability triggers under these realistic conditions:

1. **Storage sharding must be enabled** - This is the default configuration for production validators
2. **Node crash during write window** - Power failures, OOM kills, disk-full conditions, and hardware faults are common in distributed systems
3. **High transaction throughput** - Increases the probability of crashes during the critical write window

The parallel write architecture using `.unwrap()` means ANY write failure causes immediate panic: [8](#0-7) 

Field experience shows that validator nodes experience crashes during:
- Memory pressure causing OOM kills
- Disk space exhaustion during heavy write loads  
- Power failures in data centers
- Kernel panics or hardware failures

The TODO comments indicate this is a **known architectural weakness** that hasn't been addressed.

## Recommendation

Implement atomic cross-database commits using one of these approaches:

**Option 1: Two-Phase Commit Protocol**

```rust
pub fn write_schemas(&self, schemas: LedgerDbSchemaBatches) -> Result<()> {
    // Phase 1: Prepare all writes (validate, stage in WAL)
    let mut prepared_writes = Vec::new();
    prepared_writes.push(self.write_set_db.prepare_write(schemas.write_set_db_batches)?);
    prepared_writes.push(self.transaction_info_db.prepare_write(schemas.transaction_info_db_batches)?);
    // ... prepare all databases ...
    
    // Phase 2: Commit all or rollback all
    for prepared in &prepared_writes {
        prepared.commit()?;
    }
    
    Ok(())
}
```

**Option 2: Single Database with Column Families**

Instead of separate RocksDB instances, use a single instance with multiple column families. This provides true atomic writes across all sub-databases:

```rust
// All sub-databases share one RocksDB instance
pub fn write_schemas(&self, schemas: LedgerDbSchemaBatches) -> Result<()> {
    let mut combined_batch = SchemaBatch::new();
    
    // Merge all batches into one
    combined_batch.merge(schemas.write_set_db_batches);
    combined_batch.merge(schemas.transaction_info_db_batches);
    // ... merge all batches ...
    
    // Single atomic write
    self.shared_db.write_schemas(combined_batch)
}
```

**Option 3: Per-Database Progress Markers**

Implement fine-grained progress tracking per database as suggested by the TODO: [9](#0-8) 

Each database maintains its own commit progress, and recovery reconstructs consistent state by finding the minimum progress across all databases.

## Proof of Concept

This vulnerability cannot be easily demonstrated through a Move test as it requires simulating system crashes at precise moments. However, here's a Rust integration test outline:

```rust
#[test]
fn test_crash_during_sharded_write() {
    // 1. Create AptosDB with sharding enabled
    let db = create_test_db_with_sharding();
    
    // 2. Commit transactions up to version 100
    commit_test_transactions(&db, 0, 100);
    
    // 3. Simulate crash during write_schemas() for version 101
    //    by using a mock that succeeds for first N databases then panics
    let crash_after_db_index = 3;
    let batches = prepare_batches_for_version(101);
    
    // 4. Attempt write with crash simulation
    std::panic::catch_unwind(|| {
        db.ledger_db.write_schemas_with_crash_simulation(batches, crash_after_db_index)
    }).expect_err("Should panic during write");
    
    // 5. Verify inconsistent state
    let txn_info = db.ledger_db.transaction_info_db().get_transaction_info(101);
    let events = db.ledger_db.event_db().get_events_by_version(101);
    
    assert!(txn_info.is_ok(), "transaction_info written");
    assert!(events.is_err(), "events NOT written - INCONSISTENT STATE");
    
    // 6. Restart and verify recovery detects inconsistency
    drop(db);
    let recovered_db = open_test_db_from_disk();
    
    // 7. Verify OverallCommitProgress is 100, not 101
    let progress = recovered_db.ledger_db.metadata_db().get_synced_version().unwrap();
    assert_eq!(progress, Some(100), "Recovery should truncate to last consistent version");
}
```

**Notes:**
- This PoC demonstrates the architectural vulnerability
- Real-world reproduction requires actual system crashes or disk failures
- The impact is observable when validators experience crashes during production write operations
- Monitoring should track `LedgerCommitProgress` vs `OverallCommitProgress` divergence as an indicator of this issue occurring

### Citations

**File:** storage/aptosdb/src/ledger_db/mod.rs (L122-294)
```rust
    pub(crate) fn new<P: AsRef<Path>>(
        db_root_path: P,
        rocksdb_configs: RocksdbConfigs,
        env: Option<&Env>,
        block_cache: Option<&Cache>,
        readonly: bool,
    ) -> Result<Self> {
        let sharding = rocksdb_configs.enable_storage_sharding;
        let ledger_metadata_db_path = Self::metadata_db_path(db_root_path.as_ref(), sharding);
        let ledger_metadata_db = Arc::new(Self::open_rocksdb(
            ledger_metadata_db_path.clone(),
            if sharding {
                LEDGER_METADATA_DB_NAME
            } else {
                LEDGER_DB_NAME
            },
            &rocksdb_configs.ledger_db_config,
            env,
            block_cache,
            readonly,
        )?);

        info!(
            ledger_metadata_db_path = ledger_metadata_db_path,
            sharding = sharding,
            "Opened ledger metadata db!"
        );

        if !sharding {
            info!("Individual ledger dbs are not enabled!");
            return Ok(Self {
                ledger_metadata_db: LedgerMetadataDb::new(Arc::clone(&ledger_metadata_db)),
                event_db: EventDb::new(
                    Arc::clone(&ledger_metadata_db),
                    EventStore::new(Arc::clone(&ledger_metadata_db)),
                ),
                persisted_auxiliary_info_db: PersistedAuxiliaryInfoDb::new(Arc::clone(
                    &ledger_metadata_db,
                )),
                transaction_accumulator_db: TransactionAccumulatorDb::new(Arc::clone(
                    &ledger_metadata_db,
                )),
                transaction_auxiliary_data_db: TransactionAuxiliaryDataDb::new(Arc::clone(
                    &ledger_metadata_db,
                )),
                transaction_db: TransactionDb::new(Arc::clone(&ledger_metadata_db)),
                transaction_info_db: TransactionInfoDb::new(Arc::clone(&ledger_metadata_db)),
                write_set_db: WriteSetDb::new(Arc::clone(&ledger_metadata_db)),
                enable_storage_sharding: false,
            });
        }

        let ledger_db_folder = db_root_path.as_ref().join(LEDGER_DB_FOLDER_NAME);

        let mut event_db = None;
        let mut persisted_auxiliary_info_db = None;
        let mut transaction_accumulator_db = None;
        let mut transaction_auxiliary_data_db = None;
        let mut transaction_db = None;
        let mut transaction_info_db = None;
        let mut write_set_db = None;
        THREAD_MANAGER.get_non_exe_cpu_pool().scope(|s| {
            s.spawn(|_| {
                let event_db_raw = Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(EVENT_DB_NAME),
                        EVENT_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                );
                event_db = Some(EventDb::new(
                    event_db_raw.clone(),
                    EventStore::new(event_db_raw),
                ));
            });
            s.spawn(|_| {
                persisted_auxiliary_info_db = Some(PersistedAuxiliaryInfoDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(PERSISTED_AUXILIARY_INFO_DB_NAME),
                        PERSISTED_AUXILIARY_INFO_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_accumulator_db = Some(TransactionAccumulatorDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_ACCUMULATOR_DB_NAME),
                        TRANSACTION_ACCUMULATOR_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_auxiliary_data_db = Some(TransactionAuxiliaryDataDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_AUXILIARY_DATA_DB_NAME),
                        TRANSACTION_AUXILIARY_DATA_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )))
            });
            s.spawn(|_| {
                transaction_db = Some(TransactionDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_DB_NAME),
                        TRANSACTION_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_info_db = Some(TransactionInfoDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_INFO_DB_NAME),
                        TRANSACTION_INFO_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                write_set_db = Some(WriteSetDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(WRITE_SET_DB_NAME),
                        WRITE_SET_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
        });

        // TODO(grao): Handle data inconsistency.

        Ok(Self {
            ledger_metadata_db: LedgerMetadataDb::new(ledger_metadata_db),
            event_db: event_db.unwrap(),
            persisted_auxiliary_info_db: persisted_auxiliary_info_db.unwrap(),
            transaction_accumulator_db: transaction_accumulator_db.unwrap(),
            transaction_auxiliary_data_db: transaction_auxiliary_data_db.unwrap(),
            transaction_db: transaction_db.unwrap(),
            transaction_info_db: transaction_info_db.unwrap(),
            write_set_db: write_set_db.unwrap(),
            enable_storage_sharding: true,
        })
    }
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L531-548)
```rust
    pub fn write_schemas(&self, schemas: LedgerDbSchemaBatches) -> Result<()> {
        self.write_set_db
            .write_schemas(schemas.write_set_db_batches)?;
        self.transaction_info_db
            .write_schemas(schemas.transaction_info_db_batches)?;
        self.transaction_db
            .write_schemas(schemas.transaction_db_batches)?;
        self.persisted_auxiliary_info_db
            .write_schemas(schemas.persisted_auxiliary_info_db_batches)?;
        self.event_db.write_schemas(schemas.event_db_batches)?;
        self.transaction_accumulator_db
            .write_schemas(schemas.transaction_accumulator_db_batches)?;
        self.transaction_auxiliary_data_db
            .write_schemas(schemas.transaction_auxiliary_data_db_batches)?;
        // TODO: remove this after sharding migration
        self.ledger_metadata_db
            .write_schemas(schemas.ledger_metadata_db_batches)
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L263-322)
```rust
    fn calculate_and_commit_ledger_and_state_kv(
        &self,
        chunk: &ChunkToCommit,
        skip_index_and_usage: bool,
    ) -> Result<HashValue> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["save_transactions__work"]);

        let mut new_root_hash = HashValue::zero();
        THREAD_MANAGER.get_non_exe_cpu_pool().scope(|s| {
            // TODO(grao): Write progress for each of the following databases, and handle the
            // inconsistency at the startup time.
            //
            // TODO(grao): Consider propagating the error instead of panic, if necessary.
            s.spawn(|_| {
                self.commit_events(
                    chunk.first_version,
                    chunk.transaction_outputs,
                    skip_index_and_usage,
                )
                .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .write_set_db()
                    .commit_write_sets(chunk.first_version, chunk.transaction_outputs)
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .transaction_db()
                    .commit_transactions(
                        chunk.first_version,
                        chunk.transactions,
                        skip_index_and_usage,
                    )
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .persisted_auxiliary_info_db()
                    .commit_auxiliary_info(chunk.first_version, chunk.persisted_auxiliary_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_state_kv_and_ledger_metadata(chunk, skip_index_and_usage)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_transaction_infos(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                new_root_hash = self
                    .commit_transaction_accumulator(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
        });

        Ok(new_root_hash)
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L107-107)
```rust
pub const MAX_COMMIT_PROGRESS_DIFFERENCE: u64 = 1_000_000;
```

**File:** storage/aptosdb/src/state_store/mod.rs (L410-502)
```rust
    pub fn sync_commit_progress(
        ledger_db: Arc<LedgerDb>,
        state_kv_db: Arc<StateKvDb>,
        state_merkle_db: Arc<StateMerkleDb>,
        crash_if_difference_is_too_large: bool,
    ) {
        let ledger_metadata_db = ledger_db.metadata_db();
        if let Some(overall_commit_progress) = ledger_metadata_db
            .get_synced_version()
            .expect("DB read failed.")
        {
            info!(
                overall_commit_progress = overall_commit_progress,
                "Start syncing databases..."
            );
            let ledger_commit_progress = ledger_metadata_db
                .get_ledger_commit_progress()
                .expect("Failed to read ledger commit progress.");
            assert_ge!(ledger_commit_progress, overall_commit_progress);

            let state_kv_commit_progress = state_kv_db
                .metadata_db()
                .get::<DbMetadataSchema>(&DbMetadataKey::StateKvCommitProgress)
                .expect("Failed to read state K/V commit progress.")
                .expect("State K/V commit progress cannot be None.")
                .expect_version();
            assert_ge!(state_kv_commit_progress, overall_commit_progress);

            // LedgerCommitProgress was not guaranteed to commit after all ledger changes finish,
            // have to attempt truncating every column family.
            info!(
                ledger_commit_progress = ledger_commit_progress,
                "Attempt ledger truncation...",
            );
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");

            // State K/V commit progress isn't (can't be) written atomically with the data,
            // because there are shards, so we have to attempt truncation anyway.
            info!(
                state_kv_commit_progress = state_kv_commit_progress,
                "Start state KV truncation..."
            );
            let difference = state_kv_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_state_kv_db(
                &state_kv_db,
                state_kv_commit_progress,
                overall_commit_progress,
                std::cmp::max(difference as usize, 1), /* batch_size */
            )
            .expect("Failed to truncate state K/V db.");

            let state_merkle_max_version = get_max_version_in_state_merkle_db(&state_merkle_db)
                .expect("Failed to get state merkle max version.")
                .expect("State merkle max version cannot be None.");
            if state_merkle_max_version > overall_commit_progress {
                let difference = state_merkle_max_version - overall_commit_progress;
                if crash_if_difference_is_too_large {
                    assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
                }
            }
            let state_merkle_target_version = find_tree_root_at_or_before(
                ledger_metadata_db,
                &state_merkle_db,
                overall_commit_progress,
            )
            .expect("DB read failed.")
            .unwrap_or_else(|| {
                panic!(
                    "Could not find a valid root before or at version {}, maybe it was pruned?",
                    overall_commit_progress
                )
            });
            if state_merkle_target_version < state_merkle_max_version {
                info!(
                    state_merkle_max_version = state_merkle_max_version,
                    target_version = state_merkle_target_version,
                    "Start state merkle truncation..."
                );
                truncate_state_merkle_db(&state_merkle_db, state_merkle_target_version)
                    .expect("Failed to truncate state merkle db.");
            }
        } else {
            info!("No overall commit progress was found!");
        }
    }
```

**File:** storage/aptosdb/src/utils/truncation_helper.rs (L325-361)
```rust
fn truncate_ledger_db_single_batch(
    ledger_db: &LedgerDb,
    transaction_store: &TransactionStore,
    start_version: Version,
) -> Result<()> {
    let mut batch = LedgerDbSchemaBatches::new();

    delete_transaction_index_data(
        ledger_db,
        transaction_store,
        start_version,
        &mut batch.transaction_db_batches,
    )?;
    delete_per_epoch_data(
        &ledger_db.metadata_db_arc(),
        start_version,
        &mut batch.ledger_metadata_db_batches,
    )?;
    delete_per_version_data(ledger_db, start_version, &mut batch)?;

    delete_event_data(ledger_db, start_version, &mut batch.event_db_batches)?;

    truncate_transaction_accumulator(
        ledger_db.transaction_accumulator_db_raw(),
        start_version,
        &mut batch.transaction_accumulator_db_batches,
    )?;

    let mut progress_batch = SchemaBatch::new();
    progress_batch.put::<DbMetadataSchema>(
        &DbMetadataKey::LedgerCommitProgress,
        &DbMetadataValue::Version(start_version - 1),
    )?;
    ledger_db.metadata_db().write_schemas(progress_batch)?;

    ledger_db.write_schemas(batch)
}
```
