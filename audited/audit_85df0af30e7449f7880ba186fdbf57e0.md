# Audit Report

## Title
Byzantine Validators Can Force Repeated Expensive Signature Verifications in Randomness Generation

## Summary
The `AugDataCertBuilder::add()` function in the randomness generation protocol performs expensive BLS signature verification before checking if a signature from the same validator has already been received. Byzantine validators can exploit this by sending invalid signatures, forcing honest validators to waste CPU cycles on cryptographic operations that will ultimately fail, then retry indefinitely with exponential backoff.

## Finding Description

The vulnerability exists in the signature aggregation logic for augmented data certification during randomness generation. When collecting signatures from validators, the code performs an expensive cryptographic verification operation before any deduplication check. [1](#0-0) 

The attack flow:

1. **Initial Request**: Honest validator broadcasts `AugData` via reliable broadcast to all validators
2. **Byzantine Response**: A Byzantine validator receives the RPC request and responds with an invalid (but structurally well-formed) `AugDataSignature`
3. **Expensive Verification**: The honest validator calls `ack.verify()` which performs BLS signature verification - a computationally expensive cryptographic operation
4. **Verification Failure**: The signature verification fails, returning an error
5. **Retry Mechanism**: The reliable broadcast protocol retries the RPC with exponential backoff
6. **Repeat Attack**: The Byzantine validator sends another invalid signature on retry
7. **Indefinite Loop**: This continues indefinitely until enough honest validators respond

The signature verification delegates to the validator verifier: [2](#0-1) 

The reliable broadcast mechanism retries failed requests with exponential backoff but has no maximum retry limit: [3](#0-2) 

The backoff policy configuration shows retries start at 2ms and cap at 3 seconds: [4](#0-3) 

**Invariant Violation**: This breaks Critical Invariant #9 (Resource Limits) which states "All operations must respect gas, storage, and computational limits." Byzantine validators can force unbounded expensive cryptographic operations.

## Impact Explanation

This qualifies as **Medium Severity** per the Aptos bug bounty program criteria:

- **Validator node slowdowns**: Byzantine validators can force honest validators to waste CPU cycles on repeated BLS signature verifications
- Each verification is cryptographically expensive (elliptic curve operations)
- With f Byzantine validators (where f < n/3), an honest validator may perform up to f Ã— retry_count expensive verifications per round
- While this doesn't break consensus safety (honest validators will eventually get 2f+1 valid signatures), it significantly degrades performance
- The attack scales with the number of Byzantine validators in the network
- Network throughput for randomness generation is reduced as validators spend CPU on wasteful verification

This does not reach High/Critical severity because:
- No funds are lost or stolen
- Consensus safety is not violated (liveness is impacted but not broken)
- The network can still make progress, just slower

## Likelihood Explanation

**Likelihood: High**

The attack is highly likely to occur because:

1. **Low Attacker Requirements**: Any validator can execute this attack without special privileges
2. **Simple Execution**: Byzantine validator only needs to return invalid signatures in response to `AugData` RPC requests
3. **No Detection**: The protocol cannot distinguish between network issues and malicious behavior
4. **Immediate Impact**: Every round of randomness generation is affected
5. **No Cost to Attacker**: Byzantine validators pay no penalty for sending invalid signatures
6. **Amplification**: The retry mechanism amplifies the attack by causing multiple verification attempts

The attack requires:
- Byzantine validator participation (< 33% of stake per BFT assumptions)
- Basic ability to respond to RPC requests with crafted invalid signatures

## Recommendation

Implement early deduplication before expensive signature verification:

```rust
fn add(&self, peer: Author, ack: Self::Response) -> anyhow::Result<Option<Self::Aggregated>> {
    // Check if we already have a signature from this peer (cheap check)
    {
        let partial_signatures_guard = self.partial_signatures.lock();
        if partial_signatures_guard.contains_voter(&peer) {
            // Already have signature from this peer, skip expensive verification
            return Ok(None);
        }
    }
    
    // Perform expensive verification only if we don't have this peer's signature
    ack.verify(peer, &self.epoch_state.verifier, &self.aug_data)?;
    
    // Acquire lock and add signature
    let mut partial_signatures_guard = self.partial_signatures.lock();
    
    // Double-check in case another thread added it while we were verifying
    if partial_signatures_guard.contains_voter(&peer) {
        return Ok(None);
    }
    
    partial_signatures_guard.add_signature(peer, ack.into_signature());
    
    let qc_aug_data = self
        .epoch_state
        .verifier
        .check_voting_power(partial_signatures_guard.signatures().keys(), true)
        .ok()
        .map(|_| {
            let aggregated_signature = self
                .epoch_state
                .verifier
                .aggregate_signatures(partial_signatures_guard.signatures_iter())
                .expect("Signature aggregation should succeed");
            CertifiedAugData::new(self.aug_data.clone(), aggregated_signature)
        });
    Ok(qc_aug_data)
}
```

Additional recommendation: Add retry limits to the reliable broadcast mechanism to prevent indefinite retries against consistently malicious validators.

## Proof of Concept

```rust
#[cfg(test)]
mod dos_test {
    use super::*;
    use aptos_crypto::bls12381::Signature;
    use aptos_types::validator_verifier::ValidatorVerifier;
    use std::sync::Arc;
    
    #[test]
    fn test_byzantine_signature_verification_dos() {
        // Setup: Create AugDataCertBuilder with mock epoch state
        let (signer, validator_verifier) = setup_test_validators(4);
        let epoch_state = Arc::new(create_epoch_state(validator_verifier));
        let aug_data = create_test_aug_data();
        let builder = AugDataCertBuilder::new(aug_data.clone(), epoch_state.clone());
        
        // Attack: Byzantine validator sends invalid signature
        let byzantine_peer = get_byzantine_validator_address();
        let invalid_sig = AugDataSignature::new(1, Signature::dummy_signature());
        
        // Measure CPU time for verification
        let start = std::time::Instant::now();
        
        // This performs expensive BLS verification before checking for duplicates
        let result = builder.add(byzantine_peer, invalid_sig.clone());
        
        let verification_time = start.elapsed();
        
        // Verification should fail but took significant CPU time
        assert!(result.is_err());
        assert!(verification_time.as_millis() > 0); // Non-zero CPU time wasted
        
        // Simulate retry - Byzantine validator sends another invalid signature
        let start2 = std::time::Instant::now();
        let result2 = builder.add(byzantine_peer, invalid_sig);
        let verification_time2 = start2.elapsed();
        
        // Each retry performs full expensive verification again
        assert!(result2.is_err());
        assert!(verification_time2.as_millis() > 0);
        
        // Total CPU wasted = verification_time + verification_time2 + ... (unlimited retries)
        println!("CPU wasted per invalid signature: {:?}ms", verification_time.as_millis());
    }
}
```

**Notes**

The vulnerability is exploitable within the current protocol design. The reliable broadcast mechanism assumes that failed verifications are due to transient network issues and will retry indefinitely. A Byzantine validator can abuse this by consistently sending invalid signatures, forcing honest validators to repeatedly perform expensive cryptographic operations. While the exponential backoff limits the rate of retries, the unbounded retry count means validators waste CPU cycles that could be used for consensus progress. This is particularly impactful during randomness generation, which is critical for leader selection and needs to complete quickly.

### Citations

**File:** consensus/src/rand/rand_gen/reliable_broadcast_state.rs (L48-66)
```rust
    fn add(&self, peer: Author, ack: Self::Response) -> anyhow::Result<Option<Self::Aggregated>> {
        ack.verify(peer, &self.epoch_state.verifier, &self.aug_data)?;
        let mut parital_signatures_guard = self.partial_signatures.lock();
        parital_signatures_guard.add_signature(peer, ack.into_signature());
        let qc_aug_data = self
            .epoch_state
            .verifier
            .check_voting_power(parital_signatures_guard.signatures().keys(), true)
            .ok()
            .map(|_| {
                let aggregated_signature = self
                    .epoch_state
                    .verifier
                    .aggregate_signatures(parital_signatures_guard.signatures_iter())
                    .expect("Signature aggregation should succeed");
                CertifiedAugData::new(self.aug_data.clone(), aggregated_signature)
            });
        Ok(qc_aug_data)
    }
```

**File:** consensus/src/rand/rand_gen/types.rs (L515-522)
```rust
    pub fn verify<D: TAugmentedData>(
        &self,
        author: Author,
        verifier: &ValidatorVerifier,
        data: &AugData<D>,
    ) -> anyhow::Result<()> {
        Ok(verifier.verify(author, data, &self.signature)?)
    }
```

**File:** crates/reliable-broadcast/src/lib.rs (L183-201)
```rust
                    Some(result) = aggregate_futures.next() => {
                        let (receiver, result) = result.expect("spawned task must succeed");
                        match result {
                            Ok(may_be_aggragated) => {
                                if let Some(aggregated) = may_be_aggragated {
                                    return Ok(aggregated);
                                }
                            },
                            Err(e) => {
                                log_rpc_failure(e, receiver);

                                let backoff_strategy = backoff_policies
                                    .get_mut(&receiver)
                                    .expect("should be present");
                                let duration = backoff_strategy.next().expect("should produce value");
                                rpc_futures
                                    .push(send_message(receiver, Some(duration)));
                            },
                        }
```

**File:** config/src/config/dag_consensus_config.rs (L112-123)
```rust
impl Default for ReliableBroadcastConfig {
    fn default() -> Self {
        Self {
            // A backoff policy that starts at 100ms and doubles each iteration up to 3secs.
            backoff_policy_base_ms: 2,
            backoff_policy_factor: 50,
            backoff_policy_max_delay_ms: 3000,

            rpc_timeout_ms: 1000,
        }
    }
}
```
