# Audit Report

## Title
Race Condition in Block Pipeline Construction Causes Validator Node Panic

## Summary
A time-of-check-time-of-use (TOCTOU) race condition in the block insertion pipeline allows multiple threads to construct StateComputeResults for the same block concurrently. When both threads attempt to set the same OnceCell values in PartialStateComputeResult, one thread panics with "already set" error, crashing the validator node and causing loss of consensus participation. [1](#0-0) 

## Finding Description

The vulnerability exists in the block insertion flow where a non-atomic check-then-act pattern creates a race window:

**Step 1: Initial Check (Non-Atomic)**
The `insert_block` method checks if a block exists before proceeding, but this check is not atomic with the subsequent pipeline construction: [2](#0-1) 

**Step 2: Pipeline Construction (No Lock)**
In `insert_block_inner`, the pipeline is built without holding any lock that prevents concurrent execution for the same block: [3](#0-2) 

**Step 3: Ledger Update Spawns Without Protection**
The ledger_update future is spawned as a separate task that will call the executor without the execution_lock: [4](#0-3) 

**Step 4: Executor Lock Only Protects Execute Phase**
The BlockExecutor's execution_lock only protects `execute_and_update_state`, NOT `ledger_update`: [5](#0-4) 

**Step 5: OnceCell Panic on Duplicate Set**
When both threads reach the ledger_update phase and try to set the state_checkpoint_output, the OnceCell panics: [6](#0-5) 

The same panic occurs for ledger_update_output: [7](#0-6) 

**Exploitation Path:**

1. Thread A receives a proposal block and calls `insert_block(block)`
2. Thread A passes the existence check (line 413) because block not yet in tree
3. Thread B receives the same proposal (due to network retry, delayed processing, or malicious duplicate) and calls `insert_block(block)` concurrently
4. Thread B also passes the existence check (line 413) because Thread A hasn't completed insertion
5. Both threads create separate PipelinedBlock instances and build separate pipelines
6. Both pipelines spawn separate ledger_update futures
7. Both futures call `executor.ledger_update(block_id, parent_id)` on the same block
8. Both retrieve the same Block instance from the block_tree
9. Both pass the early-return check (line 291-294) as complete_result is not yet set
10. Both attempt to call `output.set_state_checkpoint_output(...)` on the same PartialStateComputeResult
11. First thread succeeds, second thread panics with "StateCheckpointOutput already set"
12. Validator node crashes

The code even acknowledges this scenario is possible: [8](#0-7) 

However, the assumption that "insert_block call is idempotent" is INCORRECT due to the race condition.

## Impact Explanation

**Severity: CRITICAL**

This vulnerability meets the **Critical Severity** criteria for multiple reasons:

1. **Validator Node Crash**: The panic causes an unrecoverable crash of the validator node, not a graceful error. This requires node restart and causes loss of consensus participation.

2. **Liveness Impact**: If multiple validators experience this race condition simultaneously (which is possible during network issues or epoch transitions when blocks are being re-processed), it could lead to temporary loss of consensus quorum, affecting network liveness.

3. **Consensus Participation Loss**: Each crashed validator cannot participate in voting, potentially delaying block finalization and reducing network security margin below the 2/3 honest validator threshold.

4. **No Graceful Recovery**: The panic is an unhandled error that terminates the validator process, requiring manual intervention to restart.

This breaks the **Consensus Safety** and **State Consistency** invariants by allowing validator nodes to crash during normal consensus operations, potentially leading to liveness failures.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

This race condition can occur in several realistic scenarios:

1. **Network Delays & Retries**: When network conditions cause proposal delays, the retry mechanism can trigger concurrent processing of the same block.

2. **Backpressure Recovery**: The code explicitly mentions delayed proposal processing due to backpressure (round_manager.rs line 1250-1251), creating conditions for duplicate processing.

3. **Malicious Exploitation**: A malicious validator or network peer could deliberately send duplicate proposals in rapid succession to trigger the race condition on target validators.

4. **Epoch Transitions**: During epoch changes, blocks may be re-processed or validated multiple times, increasing race condition probability.

5. **High Load Scenarios**: Under high transaction load with multiple concurrent proposals, the race window widens.

The window between check (line 413) and pipeline construction (line 490) combined with the async nature of the pipeline execution creates sufficient opportunity for the race to manifest.

## Recommendation

**Fix: Add atomic check-and-insert with pipeline construction protection**

The core fix requires making the block existence check atomic with pipeline construction. Implement a two-phase approach:

1. **Atomic Block Registration**: Use an atomic operation to register intent to insert a block before building the pipeline:

```rust
// In BlockStore
pub async fn insert_block(&self, block: Block) -> anyhow::Result<Arc<PipelinedBlock>> {
    // First, atomically check and register this block
    let registration_result = self.inner.write().register_block_for_insertion(block.id());
    
    match registration_result {
        BlockRegistration::Existing(existing_block) => {
            return Ok(existing_block);
        },
        BlockRegistration::NewlyRegistered => {
            // Proceed with insertion
        }
    }
    
    // ... rest of insertion logic
}
```

2. **Guard OnceCell Sets**: Add defensive checks in PartialStateComputeResult:

```rust
pub fn set_state_checkpoint_output(&self, state_checkpoint_output: StateCheckpointOutput) {
    if let Err(_) = self.state_checkpoint_output.set(state_checkpoint_output) {
        // Already set - this is a race condition, log and return gracefully
        warn!("StateCheckpointOutput already set - concurrent ledger_update detected");
        return;
    }
}
```

3. **Add Idempotency Check**: In BlockExecutorInner::ledger_update, make the complete_result check atomic with the set operations using a mutex or atomic flag.

## Proof of Concept

```rust
// Reproduction test (add to execution/executor/src/block_executor/block_tree/test.rs)

#[tokio::test]
async fn test_concurrent_ledger_update_race() {
    use std::sync::Arc;
    use tokio::task::JoinSet;
    
    // Setup executor and block tree
    let db = Arc::new(create_test_db());
    let executor = Arc::new(BlockExecutor::<AptosVM>::new(db.clone()));
    
    // Execute a block first to add it to block tree
    let block_id = HashValue::random();
    let parent_id = executor.committed_block_id();
    
    let txns = vec![/* test transactions */];
    executor.execute_and_update_state(
        (block_id, txns, vec![]).into(),
        parent_id,
        BlockExecutorConfigFromOnchain::default(),
    ).unwrap();
    
    // Now spawn two concurrent ledger_update calls for the same block
    let executor1 = executor.clone();
    let executor2 = executor.clone();
    
    let mut tasks = JoinSet::new();
    
    tasks.spawn(async move {
        executor1.ledger_update(block_id, parent_id)
    });
    
    tasks.spawn(async move {
        executor2.ledger_update(block_id, parent_id)
    });
    
    // One task should succeed, one should panic
    let result1 = tasks.join_next().await.unwrap();
    let result2 = tasks.join_next().await.unwrap();
    
    // At least one will panic with "already set"
    assert!(result1.is_err() || result2.is_err());
}
```

To trigger in production:
1. Deploy a modified consensus node that sends duplicate proposals
2. Target a victim validator
3. Send the same proposal block twice in rapid succession
4. The victim validator's `insert_block` will be called concurrently from different processing contexts
5. Race condition triggers OnceCell panic
6. Validator node crashes

The vulnerability can also be triggered naturally during high-load scenarios or network instability without malicious intent, making it a critical reliability issue even without active exploitation.

### Citations

**File:** consensus/src/block_storage/block_store.rs (L412-437)
```rust
    pub async fn insert_block(&self, block: Block) -> anyhow::Result<Arc<PipelinedBlock>> {
        if let Some(existing_block) = self.get_block(block.id()) {
            return Ok(existing_block);
        }
        ensure!(
            self.inner.read().ordered_root().round() < block.round(),
            "Block with old round"
        );

        let block_window = self
            .inner
            .read()
            .get_ordered_block_window(&block, self.window_size)?;
        let blocks = block_window.blocks();
        for block in blocks {
            if let Some(payload) = block.payload() {
                self.payload_manager.prefetch_payload_data(
                    payload,
                    block.author().expect("Payload block must have author"),
                    block.timestamp_usecs(),
                );
            }
        }

        let pipelined_block = PipelinedBlock::new_ordered(block, block_window);
        self.insert_block_inner(pipelined_block).await
```

**File:** consensus/src/block_storage/block_store.rs (L464-496)
```rust
        if let Some(pipeline_builder) = &self.pipeline_builder {
            let parent_block = self
                .get_block(pipelined_block.parent_id())
                .ok_or_else(|| anyhow::anyhow!("Parent block not found"))?;

            // need weak pointer to break the cycle between block tree -> pipeline block -> callback
            let block_tree = Arc::downgrade(&self.inner);
            let storage = self.storage.clone();
            let id = pipelined_block.id();
            let round = pipelined_block.round();
            let window_size = self.window_size;
            let callback = Box::new(
                move |finality_proof: WrappedLedgerInfo,
                      commit_decision: LedgerInfoWithSignatures| {
                    if let Some(tree) = block_tree.upgrade() {
                        tree.write().commit_callback(
                            storage,
                            id,
                            round,
                            finality_proof,
                            commit_decision,
                            window_size,
                        );
                    }
                },
            );
            pipeline_builder.build_for_consensus(
                &pipelined_block,
                parent_block.pipeline_futs().ok_or_else(|| {
                    anyhow::anyhow!("Parent future doesn't exist, potentially epoch ended")
                })?,
                callback,
            );
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L502-511)
```rust
        let ledger_update_fut = spawn_shared_fut(
            Self::ledger_update(
                rand_check_fut.clone(),
                execute_fut.clone(),
                parent.ledger_update_fut.clone(),
                self.executor.clone(),
                block.clone(),
            ),
            None,
        );
```

**File:** execution/executor/src/block_executor/mod.rs (L97-129)
```rust
    fn execute_and_update_state(
        &self,
        block: ExecutableBlock,
        parent_block_id: HashValue,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> ExecutorResult<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "execute_and_state_checkpoint"]);

        self.maybe_initialize()?;
        // guarantee only one block being executed at a time
        let _guard = self.execution_lock.lock();
        self.inner
            .read()
            .as_ref()
            .expect("BlockExecutor is not reset")
            .execute_and_update_state(block, parent_block_id, onchain_config)
    }

    fn ledger_update(
        &self,
        block_id: HashValue,
        parent_block_id: HashValue,
    ) -> ExecutorResult<StateComputeResult> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "ledger_update"]);

        self.inner
            .read()
            .as_ref()
            .ok_or_else(|| ExecutorError::InternalError {
                error: "BlockExecutor is not reset".into(),
            })?
            .ledger_update(block_id, parent_block_id)
    }
```

**File:** execution/executor/src/types/partial_state_compute_result.rs (L76-80)
```rust
    pub fn set_state_checkpoint_output(&self, state_checkpoint_output: StateCheckpointOutput) {
        self.state_checkpoint_output
            .set(state_checkpoint_output)
            .expect("StateCheckpointOutput already set");
    }
```

**File:** execution/executor/src/types/partial_state_compute_result.rs (L88-92)
```rust
    pub fn set_ledger_update_output(&self, ledger_update_output: LedgerUpdateOutput) {
        self.ledger_update_output
            .set(ledger_update_output)
            .expect("LedgerUpdateOutput already set");
    }
```

**File:** consensus/src/round_manager.rs (L1250-1259)
```rust
        // are out of the backpressure. Please note that delayed processing of proposal is not
        // guaranteed to add the block to the block store if we don't get out of the backpressure
        // before the timeout, so this is needed to ensure that the proposed block is added to
        // the block store irrespective. Also, it is possible that delayed processing of proposal
        // tries to add the same block again, which is okay as `insert_block` call
        // is idempotent.
        self.block_store
            .insert_block(proposal.clone())
            .await
            .context("[RoundManager] Failed to insert the block into BlockStore")?;
```
