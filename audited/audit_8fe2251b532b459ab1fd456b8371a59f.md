# Audit Report

## Title
Memory Exhaustion via Unbounded Capacity in IdSet::with_capacity() with Late Validation

## Summary
The `IdSet::with_capacity()` function accepts arbitrarily large capacity values without validation, leading to potential memory exhaustion. While validation exists in `DigestKey::digest()`, it occurs after memory allocation in `IdSet::from_slice()`, creating a Time-of-Check-Time-of-Use ordering vulnerability. Current mitigations limit exploitation, but the structural issue remains.

## Finding Description

The `with_capacity()` function computes `capacity.next_power_of_two()` without upper bound validation: [1](#0-0) 

When processing encrypted transactions in consensus, the attack flow is:

1. `FPTXWeighted::digest()` receives ciphertexts and calls `IdSet::from_slice()`: [2](#0-1) 

2. `IdSet::from_slice()` creates an IdSet with unchecked capacity, then pushes all IDs: [3](#0-2) 

3. Memory allocation occurs during the push loop (line 66), growing the `poly_roots` vector.

4. **ONLY AFTER** allocation does `DigestKey::digest()` validate capacity: [4](#0-3) 

This violates the invariant: "Resource Limits: All operations must respect gas, storage, and computational limits."

**Current Mitigation:**
The consensus pipeline currently hard-limits encrypted transactions to 10, marked as temporary: [5](#0-4) 

## Impact Explanation

**Severity: HIGH** (Validator node crashes/slowdowns - up to $50,000 per bug bounty)

If the temporary 10-transaction limit is removed (as indicated by TODO/FIXME), an attacker could:
- Submit blocks with thousands of encrypted transactions
- Trigger multi-GB memory allocations before validation occurs
- Crash validator nodes via OOM conditions
- Disrupt consensus by causing validator unavailability

With 1 million ciphertexts:
- `capacity = 2^20 = 1,048,576`
- Each `Fr` element ≈ 32 bytes
- Total allocation ≈ 32 MB before validation check
- Additional allocations in `compute_mult_tree()` double this

## Likelihood Explanation

**Current Likelihood: LOW** (Due to hard limit)
**Future Likelihood: MEDIUM-HIGH** (If limit is removed as planned)

The vulnerability structure exists but is currently mitigated. The TODO comment indicates planned removal of limits, which would enable exploitation. The attack requires:
- Ability to submit encrypted transactions (available to any user)
- Block acceptance by consensus (normal operation)
- No special permissions or validator access needed

## Recommendation

**Fix 1: Add upper bound validation in `with_capacity()` before allocation:**

```rust
pub fn with_capacity(capacity: usize) -> Option<Self> {
    // Reject unreasonably large capacities before allocation
    const MAX_CAPACITY: usize = 1 << 20; // 1M, adjust based on system limits
    if capacity > MAX_CAPACITY {
        return None;
    }
    let capacity = capacity.next_power_of_two();
    Some(Self {
        poly_roots: Vec::new(),
        capacity,
        poly_coeffs: UncomputedCoeffs,
    })
}
```

**Fix 2: Validate capacity in `FPTXWeighted::digest()` BEFORE calling `from_slice()`:**

```rust
fn digest(
    digest_key: &Self::DigestKey,
    cts: &[Self::Ciphertext],
    round: Self::Round,
) -> anyhow::Result<(Self::Digest, Self::EvalProofsPromise)> {
    // Validate BEFORE allocation
    let capacity = cts.len().next_power_of_two();
    if capacity > digest_key.capacity() {
        return Err(anyhow!(
            "Batch size {} exceeds digest key capacity {}",
            capacity,
            digest_key.capacity()
        ));
    }
    
    let mut ids: IdSet<UncomputedCoeffs> =
        IdSet::from_slice(&cts.iter().map(|ct| ct.id()).collect::<Vec<Id>>())
            .ok_or(anyhow!("Failed to create IdSet"))?;
    digest_key.digest(&mut ids, round)
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod poc {
    use super::*;
    use crate::shared::ids::{Id, IdSet};
    use ark_std::{rand::thread_rng, UniformRand};

    #[test]
    #[should_panic(expected = "memory allocation")]
    fn test_unbounded_capacity_memory_exhaustion() {
        // Attempt to create IdSet with huge capacity
        // This will succeed without validation
        let huge_capacity = 1_000_000_000; // 1 billion
        
        let mut idset = IdSet::with_capacity(huge_capacity).unwrap();
        
        // Now try to add many IDs - this will allocate massive memory
        let mut rng = thread_rng();
        for _ in 0..10000 {
            idset.add(&Id::new(Fr::rand(&mut rng)));
        }
        // Memory allocation occurs here, BEFORE any capacity validation
    }
}
```

## Notes

This vulnerability demonstrates a classic defense-in-depth failure: the validation check exists but occurs after the resource-intensive operation. The current 10-transaction limit in the consensus pipeline provides temporary protection, but the structural issue persists. When that limit is removed (as planned per the TODO comment), the vulnerability becomes directly exploitable, allowing unprivileged attackers to crash validator nodes via memory exhaustion attacks.

### Citations

**File:** crates/aptos-batch-encryption/src/shared/ids/mod.rs (L63-68)
```rust
    pub fn from_slice(ids: &[Id]) -> Option<Self> {
        let mut result = Self::with_capacity(ids.len())?;
        for id in ids {
            result.add(id);
        }
        Some(result)
```

**File:** crates/aptos-batch-encryption/src/shared/ids/mod.rs (L71-78)
```rust
    pub fn with_capacity(capacity: usize) -> Option<Self> {
        let capacity = capacity.next_power_of_two();
        Some(Self {
            poly_roots: Vec::new(),
            capacity,
            poly_coeffs: UncomputedCoeffs,
        })
    }
```

**File:** crates/aptos-batch-encryption/src/schemes/fptx_weighted.rs (L325-327)
```rust
        let mut ids: IdSet<UncomputedCoeffs> =
            IdSet::from_slice(&cts.iter().map(|ct| ct.id()).collect::<Vec<Id>>())
                .ok_or(anyhow!(""))?;
```

**File:** crates/aptos-batch-encryption/src/shared/digest.rs (L116-121)
```rust
        } else if ids.capacity() > self.tau_powers_g1[round].len() - 1 {
            Err(anyhow!(
                "Tried to compute a batch digest with size {}, where setup supports up to size {}",
                ids.capacity(),
                self.tau_powers_g1[round].len() - 1
            ))?
```

**File:** consensus/src/pipeline/decryption_pipeline_builder.rs (L68-76)
```rust
        // TODO(ibalajiarun): FIXME
        let len = 10;
        let encrypted_txns = if encrypted_txns.len() > len {
            let mut to_truncate = encrypted_txns;
            to_truncate.truncate(len);
            to_truncate
        } else {
            encrypted_txns
        };
```
