# Audit Report

## Title
Byzantine Validators Can Cause Liveness Degradation and Transaction Censorship via Batch Withholding Attack

## Summary
Byzantine validators can create valid ProofOfStore certificates for transaction batches but then refuse to serve those batches when requested by honest validators. This causes honest validators to enter an infinite retry loop in the block materialization phase, leading to round timeouts, consensus performance degradation, and effective transaction censorship. The attack exploits the assumption that ProofOfStore guarantees network availability, which is violated when signers act maliciously.

## Finding Description

The vulnerability exists in the QuorumStore batch retrieval mechanism. When a validator needs to execute a block containing batches with ProofOfStore certificates, it must fetch the actual transaction data from validators who signed the ProofOfStore (the "responders").

**Attack Flow:**

1. **ProofOfStore Creation**: Byzantine validators participate in creating a ProofOfStore for a batch by signing the batch metadata. This requires participation from validators representing 2f+1 voting power (quorum).

2. **Block Proposal**: The ProofOfStore is included in a block proposal by the round leader.

3. **Batch Request Phase**: When honest validators attempt to execute the block, they call the batch retrieval flow:
   - [1](#0-0) 
   - [2](#0-1) 
   - [3](#0-2) 

4. **Infinite Retry Loop**: The critical vulnerability lies in the materialize function:
   - [4](#0-3) 
   
   This function contains an infinite loop that retries indefinitely when batch materialization fails. If Byzantine validators refuse to serve batches (by returning invalid `BatchResponse::NotFound` or errors), the `request_batch` function will exhaust retries and return `ExecutorError::CouldNotGetData`, causing the materialize loop to retry every 100ms forever.

5. **Payload Availability Check Bypass**: The system has a payload availability check, but it incorrectly assumes ProofOfStore guarantees availability:
   - [5](#0-4) 
   
   For `InQuorumStore`, `QuorumStoreInlineHybrid`, and `QuorumStoreInlineHybridV2` payload types, the check simply returns `Ok(())` without verifying actual batch availability. Only `OptQuorumStore` payloads perform real availability checks.

6. **Round Timeout**: Eventually, the round timeout mechanism will fire:
   - [6](#0-5) 
   
   However, because `check_payload_availability` returned `Ok()`, the timeout reason will be `Unknown` rather than `PayloadUnavailable`, and the chain moves to the next round after significant delay.

**Invariant Violations:**
- **Liveness Guarantee**: The protocol assumes < 1/3 Byzantine validators cannot prevent progress, but this attack causes repeated timeouts
- **ProofOfStore Availability Guarantee**: The assumption that ProofOfStore guarantees batch retrievability is violated

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty criteria:

1. **Validator Node Slowdowns**: Honest validators waste CPU cycles in infinite retry loops (100ms intervals), degrading node performance across the network.

2. **Significant Protocol Violations**: The attack violates the liveness guarantee of AptosBFT consensus, causing forced round timeouts every time a malicious batch is included.

3. **Transaction Censorship**: Byzantine validators can selectively censor specific transactions by:
   - Creating ProofOfStore for batches containing target transactions
   - Refusing to serve those batches
   - Causing rounds containing those batches to timeout
   - Repeating the attack to permanently prevent transaction inclusion

4. **Network-Wide Impact**: With default round timeout of 1000ms and exponential backoff, each attacked round causes 1-3 seconds of wasted time. If Byzantine validators control sufficient batches in each round, they can significantly degrade overall network throughput.

**Attack Requirements:**
- Byzantine validators need < 1/3 stake (within BFT threat model)
- They must participate in ProofOfStore creation (requires being online and signing)
- No additional privileges or exploits needed

The impact is significant but not Critical because:
- The chain eventually progresses (liveness is degraded, not eliminated)
- No funds are at risk
- No permanent state corruption occurs
- Recovery happens automatically via round timeouts

## Likelihood Explanation

**Likelihood: Medium to High**

The attack is likely to occur because:

1. **Low Technical Barrier**: Byzantine validators simply need to refuse batch requests - no complex exploit or timing required.

2. **Within BFT Threat Model**: The attack requires < 1/3 Byzantine validators, which is the standard BFT adversary assumption. If this adversary exists, they can easily execute this attack.

3. **Incentive Exists**: Validators might be incentivized to:
   - Censor competitor transactions
   - Extort users (serve batches only for payment)
   - Attack network availability as part of larger attack strategy

4. **Detection Difficulty**: The attack appears as network issues or peer failures, making it hard to distinguish from legitimate network problems initially.

5. **Current Code State**: The infinite retry loop and incorrect payload availability check are currently in production code.

**Mitigating Factors:**
- Requires validator collusion (though < 1/3 is standard threat)
- Round timeout mechanism provides automatic recovery
- Network metrics and monitoring may detect patterns

## Recommendation

**Fix 1: Add Timeout to Materialize Loop**

Replace the infinite retry loop in `materialize()` with a bounded retry mechanism:

```rust
async fn materialize(
    preparer: Arc<BlockPreparer>,
    block: Arc<Block>,
    qc_rx: oneshot::Receiver<Arc<QuorumCert>>,
) -> TaskResult<MaterializeResult> {
    let mut tracker = Tracker::start_waiting("materialize", &block);
    tracker.start_working();

    let qc_rx = async {
        match qc_rx.await {
            Ok(qc) => Some(qc),
            Err(_) => {
                warn!("[BlockPreparer] qc tx cancelled for block {}", block.id());
                None
            },
        }
    }
    .shared();
    
    // Add timeout and retry limit
    const MAX_RETRIES: usize = 10;
    const RETRY_DELAY_MS: u64 = 100;
    
    let result = {
        let mut attempts = 0;
        loop {
            match preparer.materialize_block(&block, qc_rx.clone()).await {
                Ok(input_txns) => break Ok(input_txns),
                Err(e) => {
                    attempts += 1;
                    if attempts >= MAX_RETRIES {
                        error!(
                            "[BlockPreparer] failed to prepare block {} after {} attempts: {}",
                            block.id(),
                            attempts,
                            e
                        );
                        break Err(TaskError::InternalError(anyhow::anyhow!(
                            "Failed to materialize block after {} attempts", attempts
                        )));
                    }
                    warn!(
                        "[BlockPreparer] failed to prepare block {}, retrying ({}/{}): {}",
                        block.id(),
                        attempts,
                        MAX_RETRIES,
                        e
                    );
                    tokio::time::sleep(Duration::from_millis(RETRY_DELAY_MS)).await;
                },
            }
        }
    };
    result
}
```

**Fix 2: Improve Payload Availability Check**

Modify `check_payload_availability()` to actually verify batch data existence for all payload types: [5](#0-4) 

Change lines 358-407 to check `batch_reader.exists()` for ProofOfStore batches similar to OptQuorumStore handling.

**Fix 3: Penalty Mechanism**

Implement tracking and penalties for validators who consistently fail to serve batches they signed ProofOfStore for. This creates economic disincentive for the attack.

## Proof of Concept

The following demonstrates the attack scenario:

**Setup:**
1. Network with 4 validators: V1 (honest), V2 (honest), V3 (Byzantine), V4 (Byzantine)
2. Byzantine validators V3 and V4 control > 1/3 stake (within BFT threat model)

**Attack Steps:**

```rust
// Step 1: Byzantine validators participate in ProofOfStore creation
// They sign batch_info for a batch containing target transactions
let batch = create_batch_with_transactions(target_txns);
let signed_infos = collect_signatures(batch_info, vec![V1, V2, V3, V4]); // 4/4 = quorum
let proof_of_store = create_proof_of_store(batch_info, signed_infos);

// Step 2: Proof included in block proposal
let block = create_block_with_proof(proof_of_store);

// Step 3: V1 (honest) tries to execute block
// Calls materialize_block -> get_transactions -> request_batch
// V3 and V4 are signers, so they're in responders list

// Step 4: V3 and V4 refuse to serve batch
// In their batch_serve task, they either:
// a) Return BatchResponse::NotFound with invalid/no LedgerInfo
// b) Return Err() for the request
// c) Simply don't respond (timeout)

// Step 5: V1's request_batch exhausts retries (default: 3 retries)
// Returns ExecutorError::CouldNotGetData

// Step 6: V1 enters infinite retry loop in materialize()
// Loop iteration every 100ms, checking:
while true {
    match materialize_block() {
        Err(ExecutorError::CouldNotGetData) => {
            sleep(100ms);
            continue; // INFINITE LOOP
        }
    }
}

// Step 7: After round timeout (~1000ms), V1 broadcasts RoundTimeout
// Reason is "Unknown" not "PayloadUnavailable" due to check_payload bug
// Chain moves to next round, but with significant delay

// Step 8: V3/V4 repeat attack in subsequent rounds for continuous censorship
```

**Expected Behavior:** Block execution fails after bounded retries, validators timeout with `PayloadUnavailable` reason, and misbehaving signers are tracked.

**Actual Behavior:** Validators stuck in infinite retry loop until round timeout, with `Unknown` reason, and no accountability for Byzantine batch servers.

---

**Notes:**

The vulnerability requires Byzantine validators (< 1/3 stake) which is within the standard BFT threat model. However, the current implementation makes this attack trivially easy to execute and provides no accountability mechanism. The fix should add bounded retries, proper payload availability checking, and validator reputation tracking.

### Citations

**File:** consensus/src/block_preparer.rs (L42-69)
```rust
    pub async fn materialize_block(
        &self,
        block: &Block,
        block_qc_fut: Shared<impl Future<Output = Option<Arc<QuorumCert>>>>,
    ) -> ExecutorResult<(Vec<SignedTransaction>, Option<u64>, Option<u64>)> {
        fail_point!("consensus::prepare_block", |_| {
            use aptos_executor_types::ExecutorError;
            use std::{thread, time::Duration};
            thread::sleep(Duration::from_millis(10));
            Err(ExecutorError::CouldNotGetData)
        });
        //TODO(ibalajiarun): measure latency
        let (txns, max_txns_from_block_to_execute, block_gas_limit) = tokio::select! {
                // Poll the block qc future until a QC is received. Ignore None outcomes.
                Some(qc) = block_qc_fut => {
                    let block_voters = Some(qc.ledger_info().get_voters_bitvec().clone());
                    self.payload_manager.get_transactions(block, block_voters).await
                },
                result = self.payload_manager.get_transactions(block, None) => {
                   result
                }
        }?;
        TXNS_IN_BLOCK
            .with_label_values(&["before_filter"])
            .observe(txns.len() as f64);

        Ok((txns, max_txns_from_block_to_execute, block_gas_limit))
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L663-720)
```rust
    fn get_or_fetch_batch(
        &self,
        batch_info: BatchInfo,
        responders: Vec<PeerId>,
    ) -> Shared<Pin<Box<dyn Future<Output = ExecutorResult<Vec<SignedTransaction>>> + Send>>> {
        let mut responders = responders.into_iter().collect();

        self.inflight_fetch_requests
            .lock()
            .entry(*batch_info.digest())
            .and_modify(|fetch_unit| {
                fetch_unit.responders.lock().append(&mut responders);
            })
            .or_insert_with(|| {
                let responders = Arc::new(Mutex::new(responders));
                let responders_clone = responders.clone();

                let inflight_requests_clone = self.inflight_fetch_requests.clone();
                let batch_store = self.batch_store.clone();
                let requester = self.batch_requester.clone();

                let fut = async move {
                    let batch_digest = *batch_info.digest();
                    defer!({
                        inflight_requests_clone.lock().remove(&batch_digest);
                    });
                    // TODO(ibalajiarun): Support V2 batch
                    if let Ok(mut value) = batch_store.get_batch_from_local(&batch_digest) {
                        Ok(value.take_payload().expect("Must have payload"))
                    } else {
                        // Quorum store metrics
                        counters::MISSED_BATCHES_COUNT.inc();
                        let subscriber_rx = batch_store.subscribe(*batch_info.digest());
                        let payload = requester
                            .request_batch(
                                batch_digest,
                                batch_info.expiration(),
                                responders,
                                subscriber_rx,
                            )
                            .await?;
                        batch_store.persist(vec![PersistedValue::new(
                            batch_info.into(),
                            Some(payload.clone()),
                        )]);
                        Ok(payload)
                    }
                }
                .boxed()
                .shared();

                tokio::spawn(fut.clone());

                BatchFetchUnit {
                    responders: responders_clone,
                    fut,
                }
            })
```

**File:** consensus/src/quorum_store/batch_requester.rs (L101-181)
```rust
    pub(crate) async fn request_batch(
        &self,
        digest: HashValue,
        expiration: u64,
        responders: Arc<Mutex<BTreeSet<PeerId>>>,
        mut subscriber_rx: oneshot::Receiver<PersistedValue<BatchInfoExt>>,
    ) -> ExecutorResult<Vec<SignedTransaction>> {
        let validator_verifier = self.validator_verifier.clone();
        let mut request_state = BatchRequesterState::new(responders, self.retry_limit);
        let network_sender = self.network_sender.clone();
        let request_num_peers = self.request_num_peers;
        let my_peer_id = self.my_peer_id;
        let epoch = self.epoch;
        let retry_interval = Duration::from_millis(self.retry_interval_ms as u64);
        let rpc_timeout = Duration::from_millis(self.rpc_timeout_ms as u64);

        monitor!("batch_request", {
            let mut interval = time::interval(retry_interval);
            let mut futures = FuturesUnordered::new();
            let request = BatchRequest::new(my_peer_id, epoch, digest);
            loop {
                tokio::select! {
                    _ = interval.tick() => {
                        // send batch request to a set of peers of size request_num_peers
                        if let Some(request_peers) = request_state.next_request_peers(request_num_peers) {
                            for peer in request_peers {
                                futures.push(network_sender.request_batch(request.clone(), peer, rpc_timeout));
                            }
                        } else if futures.is_empty() {
                            // end the loop when the futures are drained
                            break;
                        }
                    },
                    Some(response) = futures.next() => {
                        match response {
                            Ok(BatchResponse::Batch(batch)) => {
                                counters::RECEIVED_BATCH_RESPONSE_COUNT.inc();
                                let payload = batch.into_transactions();
                                return Ok(payload);
                            }
                            // Short-circuit if the chain has moved beyond expiration
                            Ok(BatchResponse::NotFound(ledger_info)) => {
                                counters::RECEIVED_BATCH_NOT_FOUND_COUNT.inc();
                                if ledger_info.commit_info().epoch() == epoch
                                    && ledger_info.commit_info().timestamp_usecs() > expiration
                                    && ledger_info.verify_signatures(&validator_verifier).is_ok()
                                {
                                    counters::RECEIVED_BATCH_EXPIRED_COUNT.inc();
                                    debug!("QS: batch request expired, digest:{}", digest);
                                    return Err(ExecutorError::CouldNotGetData);
                                }
                            }
                            Ok(BatchResponse::BatchV2(_)) => {
                                error!("Batch V2 response is not supported");
                            }
                            Err(e) => {
                                counters::RECEIVED_BATCH_RESPONSE_ERROR_COUNT.inc();
                                debug!("QS: batch request error, digest:{}, error:{:?}", digest, e);
                            }
                        }
                    },
                    result = &mut subscriber_rx => {
                        match result {
                            Ok(persisted_value) => {
                                counters::RECEIVED_BATCH_FROM_SUBSCRIPTION_COUNT.inc();
                                let (_, maybe_payload) = persisted_value.unpack();
                                return Ok(maybe_payload.expect("persisted value must exist"));
                            }
                            Err(err) => {
                                debug!("channel closed: {}", err);
                            }
                        };
                    },
                }
            }
            counters::RECEIVED_BATCH_REQUEST_TIMEOUT_COUNT.inc();
            debug!("QS: batch request timed out, digest:{}", digest);
            Err(ExecutorError::CouldNotGetData)
        })
    }
}
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L615-648)
```rust
    async fn materialize(
        preparer: Arc<BlockPreparer>,
        block: Arc<Block>,
        qc_rx: oneshot::Receiver<Arc<QuorumCert>>,
    ) -> TaskResult<MaterializeResult> {
        let mut tracker = Tracker::start_waiting("materialize", &block);
        tracker.start_working();

        let qc_rx = async {
            match qc_rx.await {
                Ok(qc) => Some(qc),
                Err(_) => {
                    warn!("[BlockPreparer] qc tx cancelled for block {}", block.id());
                    None
                },
            }
        }
        .shared();
        // the loop can only be abort by the caller
        let result = loop {
            match preparer.materialize_block(&block, qc_rx.clone()).await {
                Ok(input_txns) => break input_txns,
                Err(e) => {
                    warn!(
                        "[BlockPreparer] failed to prepare block {}, retrying: {}",
                        block.id(),
                        e
                    );
                    tokio::time::sleep(Duration::from_millis(100)).await;
                },
            }
        };
        Ok(result)
    }
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L349-407)
```rust
    fn check_payload_availability(&self, block: &Block) -> Result<(), BitVec> {
        let Some(payload) = block.payload() else {
            return Ok(());
        };

        match payload {
            Payload::DirectMempool(_) => {
                unreachable!("QuorumStore doesn't support DirectMempool payload")
            },
            Payload::InQuorumStore(_) => Ok(()),
            Payload::InQuorumStoreWithLimit(_) => Ok(()),
            Payload::QuorumStoreInlineHybrid(inline_batches, proofs, _)
            | Payload::QuorumStoreInlineHybridV2(inline_batches, proofs, _) => {
                fn update_availability_metrics<'a>(
                    batch_reader: &Arc<dyn BatchReader>,
                    is_proof_label: &str,
                    batch_infos: impl Iterator<Item = &'a BatchInfo>,
                ) {
                    for (author, chunk) in &batch_infos.chunk_by(|info| info.author()) {
                        let (available_count, missing_count) = chunk
                            .map(|info| batch_reader.exists(info.digest()))
                            .fold((0, 0), |(available_count, missing_count), item| {
                                if item.is_some() {
                                    (available_count + 1, missing_count)
                                } else {
                                    (available_count, missing_count + 1)
                                }
                            });
                        counters::CONSENSUS_PROPOSAL_PAYLOAD_BATCH_AVAILABILITY_IN_QS
                            .with_label_values(&[
                                &author.to_hex_literal(),
                                is_proof_label,
                                "available",
                            ])
                            .inc_by(available_count as u64);
                        counters::CONSENSUS_PROPOSAL_PAYLOAD_BATCH_AVAILABILITY_IN_QS
                            .with_label_values(&[
                                &author.to_hex_literal(),
                                is_proof_label,
                                "missing",
                            ])
                            .inc_by(missing_count as u64);
                    }
                }

                update_availability_metrics(
                    &self.batch_reader,
                    "false",
                    inline_batches.iter().map(|(batch_info, _)| batch_info),
                );
                update_availability_metrics(
                    &self.batch_reader,
                    "true",
                    proofs.proofs.iter().map(|proof| proof.info()),
                );

                // The payload is considered available because it contains only proofs that guarantee network availabiliy
                // or inlined transactions.
                Ok(())
```

**File:** consensus/src/round_manager.rs (L968-983)
```rust
    fn compute_timeout_reason(&self, round: Round) -> RoundTimeoutReason {
        if self.round_state().vote_sent().is_some() {
            return RoundTimeoutReason::NoQC;
        }

        match self.block_store.get_block_for_round(round) {
            None => RoundTimeoutReason::ProposalNotReceived,
            Some(block) => {
                if let Err(missing_authors) = self.block_store.check_payload(block.block()) {
                    RoundTimeoutReason::PayloadUnavailable { missing_authors }
                } else {
                    RoundTimeoutReason::Unknown
                }
            },
        }
    }
```
