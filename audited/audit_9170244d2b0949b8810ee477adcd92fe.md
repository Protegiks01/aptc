# Audit Report

## Title
Metrics Impersonation Attack: ConsensusTimeoutsChecker Lacks Cryptographic Authentication of Metrics Data

## Summary
The `ConsensusTimeoutsChecker` and other metrics-based checkers in the node-checker system do not verify cryptographic signatures on metrics data, allowing malicious node operators to impersonate metrics endpoints and bypass AIT3 validator certification requirements by providing fake health metrics.

## Finding Description

The `check()` function in `ConsensusTimeoutsChecker` retrieves metrics from a target node's Prometheus endpoint without any cryptographic verification that the metrics originated from the claimed node. [1](#0-0) 

The metrics are fetched via an unauthenticated HTTP GET request in the `MetricsProvider::get_scrape()` method, which simply parses the Prometheus text format without signature verification. [2](#0-1) 

While the node-checker system includes a `HandshakeChecker` that validates node identity via a cryptographic noise connection, this authentication is **completely separate** from the metrics collection process. [3](#0-2) 

The providers are built independently from the same base URL but use different ports and protocols - noise uses an authenticated connection while metrics uses plain HTTP/HTTPS without additional authentication. [4](#0-3) 

**Attack Scenario:**

1. A malicious validator operator registers for AIT3 with a legitimate public key (controls the private key)
2. Passes the `HandshakeChecker` by establishing a valid noise connection, proving control of the private key
3. On the metrics port, runs a fake Prometheus endpoint that returns fabricated perfect metrics (zero consensus timeouts, high performance)
4. The `ConsensusTimeoutsChecker` scrapes these fake metrics without signature verification
5. Results are pushed to BigQuery showing the node as healthy, bypassing quality certification requirements

The API endpoint accepts arbitrary URLs without enforcing HTTPS or validating that metrics and noise services are on the same host. [5](#0-4) 

There is no binding between the authenticated noise connection and the metrics endpoint - they are independent checks that can be satisfied by different services.

## Impact Explanation

This vulnerability allows malicious node operators to bypass AIT3 validator certification requirements, which is used to "confirm operators are running quality, operational VFNs as part of AIT3." [6](#0-5) 

**Impact Classification: HIGH Severity - Significant Protocol Violations**

The node-checker is part of the official Aptos validator certification process. Bypassing it:
- Undermines trust in the validator selection and certification process
- Allows low-quality or malicious validators to gain undeserved reputation
- Compromises the integrity of AIT incentive programs
- Potentially affects validator rewards distribution based on falsified health metrics
- Could allow validators with poor consensus performance to hide consensus timeouts and other issues

While this doesn't directly compromise consensus safety or fund security, it represents a **significant protocol violation** in the validator governance and certification system, meeting the High severity criteria.

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability is highly likely to be exploited because:

1. **Low Barrier to Entry**: Any node operator can register for AIT3 without special privileges
2. **Simple Exploitation**: Running a fake Prometheus metrics endpoint is trivial - attackers only need to return properly formatted metrics
3. **Clear Financial Incentive**: AIT programs often include rewards/incentives, creating motivation to fake node health
4. **Difficult Detection**: Without signature verification, distinguishing real from fake metrics is challenging
5. **No Authentication Required**: The metrics endpoint requires zero authentication or proof of authenticity

The attack requires minimal technical sophistication - an operator simply runs a mock HTTP server returning perfect metrics alongside their actual validator node.

## Recommendation

Implement cryptographic authentication for metrics data using one of these approaches:

**Option 1: Signed Metrics (Preferred)**
Add digital signatures to metrics data that can be verified against the node's public key:

```rust
// In MetricsProvider::get_scrape()
pub async fn get_scrape(&self, public_key: Option<x25519::PublicKey>) -> Result<Scrape, ProviderError> {
    let response = self.client.get(self.metrics_url.clone()).send().await?;
    let body = response.text().await?;
    
    // Verify signature if public_key is provided
    if let Some(pk) = public_key {
        let signature_header = response.headers().get("X-Aptos-Metrics-Signature");
        if let Some(sig) = signature_header {
            verify_metrics_signature(&body, sig.as_bytes(), &pk)?;
        } else {
            return Err(ProviderError::ParseError(anyhow!("Missing metrics signature")));
        }
    }
    
    Scrape::parse(body.lines().map(|l| Ok(l.to_string())))
        .map_err(|e| ProviderError::ParseError(anyhow!(e)))
}
```

**Option 2: Authenticated Metrics Endpoint**
Extend the noise connection to support authenticated metrics retrieval, binding metrics to the validated node identity.

**Option 3: TLS with Certificate Pinning**
Enforce HTTPS and implement certificate pinning to ensure metrics come from the expected host, though this is weaker than cryptographic signatures.

Additionally:
- Enforce HTTPS for all metrics endpoints
- Validate that metrics URL matches the handshake-verified node address
- Add configuration to mark metrics authentication as required vs optional
- Log warnings when metrics lack authentication

## Proof of Concept

```rust
// fake_metrics_server.rs - Demonstrates the attack
use warp::Filter;

#[tokio::main]
async fn main() {
    // Fake metrics endpoint returning perfect health
    let metrics_route = warp::path!("metrics")
        .map(|| {
            // Return fake metrics showing zero consensus timeouts
            r#"# TYPE aptos_consensus_timeout_count counter
aptos_consensus_timeout_count 0
# TYPE aptos_consensus_proposals_count counter
aptos_consensus_proposals_count 10000
"#
        });

    println!("Fake metrics server running on port 9101");
    println!("Returns perfect metrics regardless of actual node health");
    warp::serve(metrics_route).run(([0, 0, 0, 0], 9101)).await;
}

// Attack execution:
// 1. Run legitimate validator node on noise port (e.g., 6180)
// 2. Run this fake metrics server on metrics port (9101)
// 3. Register with node-checker using:
//    node_url=http://your-node.com
//    metrics_port=9101
//    noise_port=6180
//    public_key=<your_real_public_key>
// 4. Pass HandshakeChecker via real noise connection
// 5. Pass ConsensusTimeoutsChecker with fake metrics
// 6. Receive AIT3 certification despite poor actual performance
```

## Notes

This vulnerability affects all metrics-based checkers in the node-checker system, including:
- `ConsensusTimeoutsChecker`
- `ConsensusRoundChecker` 
- `ConsensusProposalsChecker`
- `MinimumPeersChecker`
- `BuildVersionChecker`
- `HardwareChecker`

All rely on the same unauthenticated `MetricsProvider` and are vulnerable to the same impersonation attack. The issue is systemic to the metrics collection architecture, not specific to consensus timeout checking.

While the `HandshakeChecker` provides cryptographic node authentication, it operates independently and does not protect metrics-based checks. A comprehensive fix requires binding metrics authentication to the verified node identity established by the handshake process.

### Citations

**File:** ecosystem/node-checker/src/checker/consensus_timeouts.rs (L95-153)
```rust
    async fn check(
        &self,
        providers: &ProviderCollection,
    ) -> Result<Vec<CheckResult>, CheckerError> {
        let target_metrics_provider = get_provider!(
            providers.target_metrics_provider,
            self.config.common.required,
            MetricsProvider
        );

        let first_scrape = match target_metrics_provider.provide().await {
            Ok(scrape) => scrape,
            Err(e) => {
                return Ok(vec![Self::build_result(
                    "Failed to check consensus timeouts".to_string(),
                    0,
                    format!(
                        "Failed to scrape metrics from your node (1st time): {:#}",
                        e
                    ),
                )])
            },
        };

        tokio::time::sleep(target_metrics_provider.config.common.check_delay()).await;

        let second_scrape = match target_metrics_provider.provide().await {
            Ok(scrape) => scrape,
            Err(e) => {
                return Ok(vec![Self::build_result(
                    "Failed to check consensus timeouts".to_string(),
                    0,
                    format!(
                        "Failed to scrape metrics from your node (2nd time): {:#}",
                        e
                    ),
                )])
            },
        };

        let mut check_results = vec![];

        let previous_round = self
            .get_consensus_timeouts(&first_scrape, "first")
            .unwrap(&mut check_results);

        let latest_round = self
            .get_consensus_timeouts(&second_scrape, "second")
            .unwrap(&mut check_results);

        if !check_results.is_empty() {
            return Ok(check_results);
        }

        Ok(vec![self.build_check_result(
            previous_round.unwrap(),
            latest_round.unwrap(),
        )])
    }
```

**File:** ecosystem/node-checker/src/provider/metrics.rs (L59-85)
```rust
    pub async fn get_scrape(&self) -> Result<Scrape, ProviderError> {
        let response = self
            .client
            .get(self.metrics_url.clone())
            .send()
            .await
            .with_context(|| format!("Failed to get data from {}", self.metrics_url))
            .map_err(|e| ProviderError::RetryableEndpointError("/metrics", e))?;
        let body = response
            .text()
            .await
            .with_context(|| {
                format!(
                    "Failed to process response body from {} as text",
                    self.metrics_url
                )
            })
            .map_err(|e| ProviderError::ParseError(anyhow!(e)))?;
        Scrape::parse(body.lines().map(|l| Ok(l.to_string())))
            .with_context(|| {
                format!(
                    "Failed to parse response text from {} as a Prometheus scrape",
                    self.metrics_url
                )
            })
            .map_err(|e| ProviderError::ParseError(anyhow!(e)))
    }
```

**File:** ecosystem/node-checker/src/checker/handshake.rs (L44-78)
```rust
    async fn check(
        &self,
        providers: &ProviderCollection,
    ) -> Result<Vec<CheckResult>, CheckerError> {
        let target_noise_provider = get_provider!(
            providers.target_noise_provider,
            self.config.common.required,
            NoiseProvider
        );

        Ok(vec![
            match target_noise_provider.establish_connection().await {
                Ok(message) => Self::build_result(
                    "Noise connection established successfully".to_string(),
                    100,
                    format!(
                        "{}. This indicates your noise port ({}) is open and the node is \
                    running with the private key matching the provided public key.",
                        message,
                        target_noise_provider.network_address.find_port().unwrap()
                    ),
                ),
                Err(err) => Self::build_result(
                    "Failed to establish noise connection".to_string(),
                    0,
                    format!(
                        "{:#}. Either the noise port ({}) is closed or the node is not \
                    running with the private key matching the provided public key.",
                        err,
                        target_noise_provider.network_address.find_port().unwrap()
                    ),
                ),
            },
        ])
    }
```

**File:** ecosystem/node-checker/src/runner/sync_runner.rs (L104-152)
```rust
        if let Ok(metrics_client) = target_node_address.get_metrics_client(Duration::from_secs(4)) {
            let metrics_client = Arc::new(metrics_client);
            provider_collection.target_metrics_provider = Some(MetricsProvider::new(
                self.provider_configs.metrics.clone(),
                metrics_client.clone(),
                target_node_address.url.clone(),
                target_node_address.get_metrics_port().unwrap(),
            ));
            provider_collection.target_system_information_provider =
                Some(SystemInformationProvider::new(
                    self.provider_configs.system_information.clone(),
                    metrics_client,
                    target_node_address.url.clone(),
                    target_node_address.get_metrics_port().unwrap(),
                ));
        }

        // Build the ApiIndexProvider for the target node.
        if let Ok(api_client) = target_node_address.get_api_client(Duration::from_secs(4)) {
            let api_index_provider = Arc::new(ApiIndexProvider::new(
                self.provider_configs.api_index.clone(),
                api_client,
            ));
            provider_collection.target_api_index_provider = Some(api_index_provider.clone());

            // From here, since we have an API provider, we can try to make a noise provider.
            if let (Some(_), Some(_)) = (
                target_node_address.get_noise_port(),
                target_node_address.get_public_key(),
            ) {
                // If the noise port and public key were provided but we can't parse
                // them as a network address, just fail early.
                let noise_address = match target_node_address.as_noise_network_address() {
                    Ok(noise_address) => noise_address,
                    Err(err) => {
                        return Ok(CheckSummary::from(vec![CheckResult::new(
                            "RequestHandler".to_string(),
                            "Invalid public key".to_string(),
                            0,
                            format!("Failed to build noise address: {:#}", err),
                        )]));
                    },
                };
                provider_collection.target_noise_provider = Some(NoiseProvider::new(
                    self.provider_configs.noise.clone(),
                    noise_address,
                    api_index_provider,
                ));
            }
```

**File:** ecosystem/node-checker/src/server/api.rs (L30-87)
```rust
    async fn check(
        &self,
        /// The ID of the baseline node configuration to use for the evaluation, e.g. devnet_fullnode
        baseline_configuration_id: Query<String>,
        /// The URL of the node to check, e.g. http://44.238.19.217 or http://fullnode.mysite.com
        node_url: Query<Url>,
        /// If given, we will assume the metrics service is available at the given port.
        metrics_port: Query<Option<u16>>,
        /// If given, we will assume the API is available at the given port.
        api_port: Query<Option<u16>>,
        /// If given, we will assume that clients can communicate with your node via noise at the given port.
        noise_port: Query<Option<u16>>,
        /// A public key for the node, e.g. 0x44fd1324c66371b4788af0b901c9eb8088781acb29e6b8b9c791d5d9838fbe1f.
        /// This is only necessary for certain checkers, e.g. HandshakeChecker.
        public_key: Query<Option<String>>,
    ) -> poem::Result<Json<CheckSummary>> {
        // Ensure the public key, if given, is in a valid format.
        let public_key = match public_key.0 {
            Some(public_key) => match x25519::PublicKey::from_encoded_string(&public_key) {
                Ok(public_key) => Some(public_key),
                Err(e) => {
                    return Err(poem::Error::from((
                        StatusCode::BAD_REQUEST,
                        anyhow!("Invalid public key \"{}\": {:#}", public_key, e),
                    )))
                },
            },
            None => None,
        };

        let baseline_configuration = self
            .baseline_configurations
            .0
            .get(&baseline_configuration_id.0)
            .context(format!(
                "Baseline configuration {} does not exist",
                baseline_configuration_id.0
            ))
            .map_err(|e| poem::Error::from((StatusCode::BAD_REQUEST, e)))?;

        // Within a single NHC run we want to use the same client so that cookies
        // can be collected and used. This is important because the nodes we're
        // talking to might be a behind a LB that does cookie based sticky routing.
        // If we don't do this, we can get read inconsistency, e.g. where we read
        // that the node has transaction version X, but then we fail to retrieve the
        // transaction at the version because the LB routes us to a different node.
        // In this function, which comprises a single NHC run, we build a NodeAddress
        // for the target and use that throughout the request. Further functions
        // deeper down might clone these structs, but that is fine, because the
        // important part, the CookieStore (Jar) is in an Arc, so each time we clone
        // the struct we're just cloning the reference to the same jar.
        let target_node_address = NodeAddress::new(
            node_url.0,
            api_port.0,
            metrics_port.0,
            noise_port.0,
            public_key,
        );
```

**File:** ecosystem/node-checker/fn-check-client/README.md (L1-10)
```markdown
# Validator FullNode (VFN) NHC periodic checker

## Description
This tool is a client to NHC that does the following:
1. Get the validator set from any node participating in a network we want to test.
2. Process that to create a map from operator account address to VFN network addresses.
3. Query NHC for each VFN.
4. Push the results to BigQuery.

The original intent behind this tool is to confirm operators are running quality, operational VFNs as part of AIT3. This tool can be easily adapted for other use cases down the line.
```
