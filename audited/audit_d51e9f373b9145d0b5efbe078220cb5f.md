# Audit Report

## Title
Logical Time Inconsistency in sync_for_duration() Due to Non-Atomic State Updates

## Summary
The `sync_for_duration()` function in `consensus/src/state_computer.rs` can leave the logical time in an inconsistent state when `executor.reset()` fails after successfully updating `latest_logical_time`. This violates the atomicity of state synchronization operations and can cause consensus nodes to reject valid sync targets or fail to execute blocks. [1](#0-0) 

## Finding Description

The vulnerability exists in the state update sequence within `sync_for_duration()`:

1. The function locks `write_mutex` and obtains `latest_logical_time`
2. Calls `executor.finish()` to clear in-memory state
3. Performs state synchronization via `state_sync_notifier.sync_for_duration(duration)`
4. **If sync succeeds**: Updates `*latest_logical_time` to the new (epoch, round) from the synced ledger info
5. Calls `executor.reset()` which can fail with various database errors
6. Returns the overall result [2](#0-1) 

**The Critical Issue:** When state sync succeeds but `executor.reset()` subsequently fails, the function leaves the system in an inconsistent state:
- `latest_logical_time` has been permanently updated to (epoch E, round R)
- The database state reflects the successful sync at (E, R)
- The executor's `inner` state remains `None` (cleared by `finish()`, not restored by failed `reset()`)
- The function returns an error, signaling failure to the caller

The `reset()` operation reads from the database to reconstruct the block tree and can fail due to:
- Database I/O errors during `get_latest_ledger_info()` or `get_pre_committed_ledger_summary()`
- Version mismatches between ledger summary and ledger info
- Memory allocation failures during `BlockTree::new()` [3](#0-2) [4](#0-3) 

**Monotonicity Check Bypass:** The inconsistent `latest_logical_time` directly affects `sync_to_target()`, which contains a monotonicity check that skips synchronization if the current logical time is already ahead of the target: [5](#0-4) 

**Exploitation Scenario:**

1. Node at epoch=1, round=10 with `latest_logical_time=(1,10)`
2. `sync_for_duration(5s)` is called during catch-up
3. State sync succeeds, advances database to epoch=1, round=20
4. `*latest_logical_time = LogicalTime(1,20)` executes
5. `executor.reset()` fails due to transient I/O error reading database
6. Function returns error, but `latest_logical_time` remains at (1,20)
7. Consensus receives QC for epoch=1, round=15 and calls `sync_to_target(round=15)`
8. Monotonicity check: `(1,20) >= (1,15)` → TRUE, sync is skipped
9. Consensus believes it's synced to round 15, but:
   - Database is actually at round 20
   - `executor.inner` is still `None`
   - Node cannot execute blocks at round 16+ due to missing parent blocks

When consensus attempts to execute a block at round 16:
- `execute_and_update_state()` calls `maybe_initialize()` which attempts `reset()`
- If `reset()` now succeeds, the block tree is initialized from database at round 20
- Parent block lookup for round 15 fails → `ExecutorError::BlockNotFound`
- Node cannot participate in consensus despite believing it's synced [6](#0-5) 

This breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs." The logical time and executor state are out of sync, violating atomicity.

## Impact Explanation

**Severity: Medium** - State inconsistencies requiring intervention

This vulnerability causes consensus nodes to enter an inconsistent state where:
1. The node believes it has synced to a certain round but cannot execute blocks
2. Valid sync targets are incorrectly rejected due to the advanced `latest_logical_time`
3. The node effectively drops out of consensus participation until manual recovery

The impact qualifies as **Medium Severity** per Aptos bug bounty criteria:
- "State inconsistencies requiring intervention" - The node requires restart or manual database recovery
- "Validator node slowdowns" - Affected validators cannot participate in consensus

While this doesn't directly cause consensus safety violations or fund loss, it does degrade network liveness. If multiple validators are affected simultaneously (e.g., during network partitions with frequent state sync), it could reduce the validator set below the 2/3 threshold needed for progress.

The issue does NOT qualify as Critical because:
- It doesn't cause permanent data loss or corruption
- It doesn't enable theft or minting of funds
- It doesn't break consensus safety (different nodes committing different blocks)
- Recovery is possible via node restart

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability triggers when:
1. A node falls behind and requires state synchronization
2. State sync completes successfully
3. The subsequent `executor.reset()` call fails

Factors increasing likelihood:
- **Transient I/O errors**: Database reads can fail due to disk issues, network storage problems, or resource exhaustion
- **High load periods**: During network congestion or state sync storms, database resources are stressed
- **Version mismatches**: The `ensure!` check in `root_from_db()` can fail if state sync commits data but ledger info is not fully synchronized
- **Epoch boundaries**: State sync activity spikes during epoch transitions

Factors decreasing likelihood:
- Modern databases and filesystems have error recovery mechanisms
- The `maybe_initialize()` recovery path can succeed on retry
- Most database operations complete successfully under normal conditions

Real-world triggering scenarios:
- Database corruption or filesystem errors
- Memory pressure causing allocation failures
- Race conditions between state sync writes and executor reads
- Cloud infrastructure instability (disk throttling, I/O limits)

## Recommendation

**Fix: Implement atomic state updates with rollback on failure**

The operation must be made atomic - either both `latest_logical_time` update AND `executor.reset()` succeed, or neither takes effect:

```rust
async fn sync_for_duration(
    &self,
    duration: Duration,
) -> Result<LedgerInfoWithSignatures, StateSyncError> {
    let mut latest_logical_time = self.write_mutex.lock().await;
    
    // Store the original logical time for rollback
    let original_logical_time = *latest_logical_time;
    
    self.executor.finish();
    
    fail_point!("consensus::sync_for_duration", |_| {
        Err(anyhow::anyhow!("Injected error in sync_for_duration").into())
    });
    
    let result = monitor!(
        "sync_for_duration",
        self.state_sync_notifier.sync_for_duration(duration).await
    );
    
    // Only update logical time if both sync AND reset succeed
    match &result {
        Ok(latest_synced_ledger_info) => {
            let ledger_info = latest_synced_ledger_info.ledger_info();
            let synced_logical_time = LogicalTime::new(
                ledger_info.epoch(), 
                ledger_info.round()
            );
            
            // Try to reset executor before committing logical time update
            if let Err(reset_error) = self.executor.reset() {
                // Reset failed - rollback by keeping original logical time
                error!(
                    "Failed to reset executor after sync: {:?}. Keeping logical time at {:?}",
                    reset_error, original_logical_time
                );
                // Return error to indicate sync failure
                return Err(StateSyncError::from(anyhow::anyhow!(
                    "Executor reset failed after successful sync: {}",
                    reset_error
                )));
            }
            
            // Both sync and reset succeeded - commit the logical time update
            *latest_logical_time = synced_logical_time;
        }
        Err(_) => {
            // Sync failed - attempt reset to restore executor state
            // but don't update logical time
            let _ = self.executor.reset();
        }
    }
    
    result.map_err(|error| {
        let anyhow_error: anyhow::Error = error.into();
        anyhow_error.into()
    })
}
```

**Additional Fix: Also fix sync_to_target() which has a worse bug**

The `sync_to_target()` function unconditionally updates `latest_logical_time` even if state sync fails, which is more severe: [7](#0-6) 

This must be changed to only update logical time after BOTH sync and reset succeed.

## Proof of Concept

```rust
#[tokio::test]
async fn test_sync_for_duration_logical_time_inconsistency() {
    use aptos_executor_test_helpers::integration_test_impl::create_db_and_executor;
    use aptos_types::ledger_info::LedgerInfoWithSignatures;
    use fail::FailScenario;
    
    let scenario = FailScenario::setup();
    let (db, executor) = create_db_and_executor();
    
    // Create state computer with executor
    let state_computer = ExecutionProxy::new(
        executor,
        Arc::new(DummyTxnNotifier),
        Arc::new(DummyStateSync),
        BlockTransactionFilterConfig::default(),
        false,
        None,
    );
    
    // Initial state: epoch=1, round=10
    let initial_li = create_ledger_info(1, 10);
    
    // Configure fail point to cause reset() to fail after sync succeeds
    fail::cfg("executor::block_executor_reset", "return").unwrap();
    
    // Attempt sync_for_duration - sync will succeed but reset will fail
    let result = state_computer.sync_for_duration(Duration::from_secs(5)).await;
    
    // Verify function returned error due to reset failure
    assert!(result.is_err());
    
    // BUG: latest_logical_time was updated despite error return
    // This should not be possible but currently is
    
    // Now try to sync to a legitimate target at epoch=1, round=15
    let target_li = create_ledger_info(1, 15);
    let sync_result = state_computer.sync_to_target(target_li).await;
    
    // BUG: sync_to_target will skip syncing because latest_logical_time
    // was incorrectly advanced to round 20, even though we should sync to round 15
    // This demonstrates the monotonicity check bypass
    
    scenario.teardown();
}
```

**Notes:**
- This vulnerability requires database I/O failures to trigger, which can be simulated using fail points
- The fix ensures atomic updates by checking reset() success before committing logical time changes
- The same pattern should be applied to `sync_to_target()` which has an even more severe bug where it updates logical time unconditionally

### Citations

**File:** consensus/src/state_computer.rs (L132-174)
```rust
    async fn sync_for_duration(
        &self,
        duration: Duration,
    ) -> Result<LedgerInfoWithSignatures, StateSyncError> {
        // Grab the logical time lock
        let mut latest_logical_time = self.write_mutex.lock().await;

        // Before state synchronization, we have to call finish() to free the
        // in-memory SMT held by the BlockExecutor to prevent a memory leak.
        self.executor.finish();

        // Inject an error for fail point testing
        fail_point!("consensus::sync_for_duration", |_| {
            Err(anyhow::anyhow!("Injected error in sync_for_duration").into())
        });

        // Invoke state sync to synchronize for the specified duration. Here, the
        // ChunkExecutor will process chunks and commit to storage. However, after
        // block execution and commits, the internal state of the ChunkExecutor may
        // not be up to date. So, it is required to reset the cache of the
        // ChunkExecutor in state sync when requested to sync.
        let result = monitor!(
            "sync_for_duration",
            self.state_sync_notifier.sync_for_duration(duration).await
        );

        // Update the latest logical time
        if let Ok(latest_synced_ledger_info) = &result {
            let ledger_info = latest_synced_ledger_info.ledger_info();
            let synced_logical_time = LogicalTime::new(ledger_info.epoch(), ledger_info.round());
            *latest_logical_time = synced_logical_time;
        }

        // Similarly, after state synchronization, we have to reset the cache of
        // the BlockExecutor to guarantee the latest committed state is up to date.
        self.executor.reset()?;

        // Return the result
        result.map_err(|error| {
            let anyhow_error: anyhow::Error = error.into();
            anyhow_error.into()
        })
    }
```

**File:** consensus/src/state_computer.rs (L188-194)
```rust
        if *latest_logical_time >= target_logical_time {
            warn!(
                "State sync target {:?} is lower than already committed logical time {:?}",
                target_logical_time, *latest_logical_time
            );
            return Ok(());
        }
```

**File:** consensus/src/state_computer.rs (L216-232)
```rust
        let result = monitor!(
            "sync_to_target",
            self.state_sync_notifier.sync_to_target(target).await
        );

        // Update the latest logical time
        *latest_logical_time = target_logical_time;

        // Similarly, after state synchronization, we have to reset the cache of
        // the BlockExecutor to guarantee the latest committed state is up to date.
        self.executor.reset()?;

        // Return the result
        result.map_err(|error| {
            let anyhow_error: anyhow::Error = error.into();
            anyhow_error.into()
        })
```

**File:** execution/executor/src/block_executor/mod.rs (L90-95)
```rust
    fn reset(&self) -> Result<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "reset"]);

        *self.inner.write() = Some(BlockExecutorInner::new(self.db.clone())?);
        Ok(())
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L97-113)
```rust
    fn execute_and_update_state(
        &self,
        block: ExecutableBlock,
        parent_block_id: HashValue,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> ExecutorResult<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "execute_and_state_checkpoint"]);

        self.maybe_initialize()?;
        // guarantee only one block being executed at a time
        let _guard = self.execution_lock.lock();
        self.inner
            .read()
            .as_ref()
            .expect("BlockExecutor is not reset")
            .execute_and_update_state(block, parent_block_id, onchain_config)
    }
```

**File:** execution/executor/src/block_executor/block_tree/mod.rs (L207-228)
```rust
    fn root_from_db(block_lookup: &Arc<BlockLookup>, db: &Arc<dyn DbReader>) -> Result<Arc<Block>> {
        let ledger_info_with_sigs = db.get_latest_ledger_info()?;
        let ledger_info = ledger_info_with_sigs.ledger_info();
        let ledger_summary = db.get_pre_committed_ledger_summary()?;

        ensure!(
            ledger_summary.version() == Some(ledger_info.version()),
            "Missing ledger info at the end of the ledger. latest version {:?}, LI version {}",
            ledger_summary.version(),
            ledger_info.version(),
        );

        let id = if ledger_info.ends_epoch() {
            epoch_genesis_block_id(ledger_info)
        } else {
            ledger_info.consensus_block_id()
        };

        let output = PartialStateComputeResult::new_empty(ledger_summary);

        block_lookup.fetch_or_add_block(id, output, None)
    }
```
