# Audit Report

## Title
Race Condition in CachedStateView Allows Reading Inconsistent State Versions from Shared HotStateBase

## Summary
The `CachedStateView` struct stores an `Arc<dyn HotStateView>` pointer to a shared `HotStateBase` that can be asynchronously mutated by a background Committer thread. When reading state during block execution, the view may retrieve values from versions newer than its `base_version`, violating deterministic execution guarantees and potentially causing consensus failures.

## Finding Description

The vulnerability stems from how `CachedStateView` shares state with the hot state cache: [1](#0-0) 

When `CachedStateView` is created, it calls `reader.get_persisted_state()` which returns: [2](#0-1) 

The `State` is cloned (creating an immutable snapshot), but the `base` is just an `Arc::clone`, meaning multiple `CachedStateView` instances share the **same** `HotStateBase` which is backed by concurrent `DashMap` structures: [3](#0-2) 

A background Committer thread asynchronously updates this shared `HotStateBase`: [4](#0-3) 

The critical flaw is in `CachedStateView.get_unmemorized()`, which reads from hot state **without any version validation**: [5](#0-4) 

Notice that when reading from hot state (line 239), there is **no check** that the returned `StateSlot`'s `value_version` is `<= base_version`. In contrast, when reading from cold storage (line 244-246), it explicitly passes `base_version` to ensure version consistency.

The `StateSlot` enum contains version information: [6](#0-5) 

But this version is **never validated** against `base_version` when reading from hot state.

**Attack Scenario:**

1. Validator V1 begins executing Block B at version N, creating a `CachedStateView` with `base_version = N`
2. The `CachedStateView` receives an `Arc` to `HotStateBase` currently containing data up to version N
3. Transaction T1 in Block B reads key K1 from hot state → gets value at version N ✓
4. **Concurrently**, the Committer thread updates `HotStateBase` to version N+1, modifying keys including K2
5. Transaction T2 in Block B (or even T1 continuing) reads key K2 from hot state → gets value at version N+1 ✗
6. Block B now sees an **inconsistent state snapshot**: K1 from version N, K2 from version N+1

Different validators executing the same block at slightly different times may observe different version combinations, producing different execution results and state roots, **breaking consensus**.

## Impact Explanation

This vulnerability directly violates the **Deterministic Execution** invariant: "All validators must produce identical state roots for identical blocks."

**Critical Severity** - This qualifies for the highest severity tier under Aptos bug bounty criteria:
- **Consensus/Safety violations**: Different validators can produce different state roots for the same block due to timing-dependent reads
- **Non-recoverable network partition**: If validators diverge on state roots, the network cannot reach consensus and may require coordinated intervention or hard fork
- **Total loss of liveness**: Validators unable to agree on block validity leads to chain halt

The impact affects **all validators** network-wide, as any validator can encounter this race condition during normal operation.

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability can manifest naturally without attacker action:

1. **Asynchronous Architecture**: The Committer thread runs independently and continuously processes state updates: [7](#0-6) 

2. **No Synchronization**: The code comments explicitly state that pre-committing and committing can run concurrently: [8](#0-7) 

3. **Timing Window**: Block execution can take milliseconds to seconds, during which multiple commit cycles may occur. The race window exists for the entire execution duration.

4. **Natural Occurrence**: Under normal load with multiple blocks being processed, committed, and executed in parallel, the timing conditions will naturally arise.

## Recommendation

Implement version validation when reading from hot state. The hot state view should filter results to only return values with `value_version <= base_version`:

**Option 1**: Modify `HotStateView` trait to accept a version parameter:
```rust
pub trait HotStateView: Send + Sync {
    fn get_state_slot(&self, state_key: &StateKey, max_version: Version) -> Option<StateSlot>;
}
```

**Option 2**: Add version checking in `CachedStateView.get_unmemorized()`:
```rust
} else if let Some(slot) = self.hot.get_state_slot(state_key) {
    // Validate that the slot's version is not newer than our base version
    if let Some(value_version) = slot.expect_value_version_opt() {
        if let Some(base_ver) = self.base_version() {
            if value_version > base_ver {
                // Skip hot state, fall through to cold DB read
                // (handle as cache miss)
            } else {
                COUNTER.inc_with(&["sv_hit_hot"]);
                slot
            }
        }
    }
}
```

**Option 3**: Use versioned Arc references - create a new `HotStateBase` instance for each version rather than mutating a shared one (memory-intensive but eliminates race).

## Proof of Concept

```rust
// Reproduction test (pseudo-code demonstrating the race)
#[test]
fn test_hot_state_version_inconsistency() {
    let db = create_test_db();
    let state_v100 = create_state_at_version(100);
    
    // Create CachedStateView at version 100
    let view = CachedStateView::new(
        StateViewId::BlockExecution { block_id: BlockId::new(1) },
        Arc::clone(&db.reader),
        state_v100,
    ).unwrap();
    
    // Simulate concurrent hot state update
    let state_v101 = create_state_at_version(101);
    let update = create_state_update(KEY_X, VALUE_NEW, 101);
    
    // Spawn thread that updates hot state to v101
    thread::spawn(move || {
        thread::sleep(Duration::from_millis(10));
        db.commit_state(state_v101);
    });
    
    // Read from view while hot state is being updated
    thread::sleep(Duration::from_millis(5));
    let value_before = view.get_state_value(KEY_Y).unwrap(); // May get v100
    
    thread::sleep(Duration::from_millis(10));
    let value_after = view.get_state_value(KEY_X).unwrap(); // May get v101!
    
    // Assert that both values should be from same version
    // This assertion may FAIL due to race condition
    assert_eq!(value_before.version(), value_after.version());
}
```

The test demonstrates that a single `CachedStateView` with `base_version = 100` can read values from different versions (100 and 101) depending on timing of the background Committer thread updates.

## Notes

The codebase shows awareness of similar version consistency issues. In `persisted_state.rs`, there's an explicit comment about version ordering: [9](#0-8) 

However, this safeguard only addresses summary vs. state synchronization, not the race condition between `CachedStateView` reads and concurrent `HotStateBase` mutations. The vulnerability requires additional version validation in the read path itself.

### Citations

**File:** storage/storage-interface/src/state_store/state_view/cached_state_view.rs (L98-114)
```rust
pub struct CachedStateView {
    /// For logging and debugging purpose, identifies what this view is for.
    id: StateViewId,

    /// The in-memory state on top of known persisted state.
    speculative: StateDelta,

    /// Persisted hot state. To be fetched if a key isn't in `speculative`.
    hot: Arc<dyn HotStateView>,

    /// Persisted base state. To be fetched if a key isn't in either `speculative` or `hot_state`.
    /// `self.speculative.base_version()` is targeted in db fetches.
    cold: Arc<dyn DbReader>,

    /// State values (with update versions) read across the lifetime of the state view.
    memorized: ShardedStateCache,
}
```

**File:** storage/storage-interface/src/state_store/state_view/cached_state_view.rs (L233-253)
```rust
    fn get_unmemorized(&self, state_key: &StateKey) -> Result<StateSlot> {
        COUNTER.inc_with(&["sv_unmemorized"]);

        let ret = if let Some(slot) = self.speculative.get_state_slot(state_key) {
            COUNTER.inc_with(&["sv_hit_speculative"]);
            slot
        } else if let Some(slot) = self.hot.get_state_slot(state_key) {
            COUNTER.inc_with(&["sv_hit_hot"]);
            slot
        } else if let Some(base_version) = self.base_version() {
            COUNTER.inc_with(&["sv_cold"]);
            StateSlot::from_db_get(
                self.cold
                    .get_state_value_with_version_by_version(state_key, base_version)?,
            )
        } else {
            StateSlot::ColdVacant
        };

        Ok(ret)
    }
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L72-78)
```rust
#[derive(Debug)]
pub struct HotStateBase<K = StateKey, V = StateSlot>
where
    K: Eq + std::hash::Hash,
{
    shards: [Shard<K, V>; NUM_STATE_SHARDS],
}
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L131-136)
```rust
    pub fn get_committed(&self) -> (Arc<dyn HotStateView>, State) {
        let state = self.committed.lock().clone();
        let base = self.base.clone();

        (base, state)
    }
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L192-205)
```rust
    fn run(&mut self) {
        info!("HotState committer thread started.");

        while let Some(to_commit) = self.next_to_commit() {
            self.commit(&to_commit);
            *self.committed.lock() = to_commit;

            GAUGE.set_with(&["hot_state_items"], self.base.len() as i64);
            GAUGE.set_with(&["hot_state_key_bytes"], self.total_key_bytes as i64);
            GAUGE.set_with(&["hot_state_value_bytes"], self.total_value_bytes as i64);
        }

        info!("HotState committer quitting.");
    }
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L235-261)
```rust
    fn commit(&mut self, to_commit: &State) {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["hot_state_commit"]);

        let mut n_insert = 0;
        let mut n_update = 0;
        let mut n_evict = 0;

        let delta = to_commit.make_delta(&self.committed.lock());
        for shard_id in 0..NUM_STATE_SHARDS {
            for (key, slot) in delta.shards[shard_id].iter() {
                if slot.is_hot() {
                    let key_size = key.size();
                    self.total_key_bytes += key_size;
                    self.total_value_bytes += slot.size();
                    if let Some(old_slot) = self.base.shards[shard_id].insert(key, slot) {
                        self.total_key_bytes -= key_size;
                        self.total_value_bytes -= old_slot.size();
                        n_update += 1;
                    } else {
                        n_insert += 1;
                    }
                } else if let Some((key, old_slot)) = self.base.shards[shard_id].remove(&key) {
                    self.total_key_bytes -= key.size();
                    self.total_value_bytes -= old_slot.size();
                    n_evict += 1;
                }
            }
```

**File:** types/src/state_store/state_slot.rs (L24-40)
```rust
pub enum StateSlot {
    ColdVacant,
    HotVacant {
        hot_since_version: Version,
        lru_info: LRUEntry<StateKey>,
    },
    ColdOccupied {
        value_version: Version,
        value: StateValue,
    },
    HotOccupied {
        value_version: Version,
        value: StateValue,
        hot_since_version: Version,
        lru_info: LRUEntry<StateKey>,
    },
}
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L46-49)
```rust
            // Pre-committing and committing in concurrency is allowed but not pre-committing at the
            // same time from multiple threads, the same for committing.
            // Consensus and state sync must hand over to each other after all pending execution and
            // committing complete.
```

**File:** storage/aptosdb/src/state_store/persisted_state.rs (L53-59)
```rust
        // n.b. Summary must be updated before committing the hot state, otherwise in the execution
        // pipeline we risk having a state generated based on a persisted version (v2) that's newer
        // than that of the summary (v1). That causes issue down the line where we commit the diffs
        // between a later snapshot (v3) and a persisted snapshot (v1) to the JMT, at which point
        // we will not be able to calculate the difference (v1 - v3) because the state links only
        // to as far as v2 (code will panic)
        *self.summary.lock() = summary;
```
