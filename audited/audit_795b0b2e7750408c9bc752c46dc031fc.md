# Audit Report

## Title
Cascading Task Failure in Indexer gRPC Manager Causing Total Service Unavailability

## Summary
The `start()` function in the indexer-grpc-manager uses `tokio_scoped::scope` to spawn multiple critical tasks with `.unwrap()` calls. When any single task panics or returns an error, the scope cancels all other tasks, causing total service failure instead of graceful degradation. This creates an availability vulnerability where transient failures in one component (file store, metadata manager, or server) can cascade to shut down the entire indexer-grpc-manager service.

## Finding Description

The `GrpcManager::start()` function spawns four concurrent tasks using `tokio_scoped::scope`: [1](#0-0) 

Each spawned task contains operations that can fail and panic via `.unwrap()`:

1. **Metadata Manager Task** - Line 109 calls `.unwrap()` on the Result returned by `metadata_manager.start()`: [2](#0-1) 

2. **File Store Uploader Task** - Line 119 calls `.unwrap()` on the Result from `file_store_uploader.start()`: [3](#0-2) 

3. **gRPC Server Task** - Line 124 calls `.unwrap()` on the server Result: [4](#0-3) 

4. **Data Manager Task** - Contains a panic at line 187 if the recovery channel is closed: [5](#0-4) 

Additionally, nested `tokio_scoped::scope` blocks within these tasks contain their own `.unwrap()` calls that can propagate failures: [6](#0-5) 

**Failure Propagation Mechanism:**

When any task panics (via `.unwrap()` on an error):
1. The panic is caught by `tokio_scoped::scope`
2. The scope immediately cancels all other running tasks
3. The entire service terminates, including the gRPC server
4. No graceful degradation or partial operation is possible

**Realistic Failure Scenarios:**

- **File Store I/O Errors**: Disk full, network timeouts, permission issues → `do_upload()` fails → unwrap panics → all tasks cancelled
- **Server Binding Failures**: Port already in use, permission denied → `server.serve()` fails → unwrap panics → all tasks cancelled  
- **Metadata Operations**: Network issues connecting to other managers or fullnodes → metadata operations fail → all tasks cancelled
- **Recovery Channel Closure**: If file store uploader fails during initialization → channel closed → DataManager panics → all tasks cancelled

## Impact Explanation

**Severity: High** (per Aptos Bug Bounty categories)

This issue qualifies as **High Severity** because it causes:
1. **API crashes** - explicitly listed as High Severity in the bug bounty program
2. **Total service unavailability** - the indexer-grpc-manager becomes completely non-operational
3. **No graceful degradation** - cached data cannot be served even if only background operations fail

While the indexer-grpc-manager is an auxiliary service (not core consensus), its total failure impacts:
- Data availability for applications querying indexed blockchain data
- API consumers who depend on the indexer for transaction history
- Monitoring and analytics systems

The impact is amplified because:
- A single transient error in any component causes complete service failure
- There is no fault isolation between independent subsystems
- Recovery requires full service restart rather than individual component recovery

## Likelihood Explanation

**Likelihood: High**

This issue has high likelihood of occurrence because:

1. **Multiple Failure Points**: Four independent tasks, each with multiple failure modes
2. **External Dependencies**: File store (GCS/S3), network connections to fullnodes and other managers
3. **I/O Operations**: Frequent disk and network I/O operations that can fail transiently
4. **No Retry Logic**: `.unwrap()` provides no retry capability for transient errors
5. **Operational Reality**: Network timeouts, disk I/O errors, and service disruptions are common in production

Common operational scenarios that trigger this:
- Temporary network partition between indexer and file store
- Disk space exhaustion during file uploads
- Port conflicts or permission issues during server startup
- Fullnode connectivity issues
- File store rate limiting or quota exhaustion

## Recommendation

**Solution: Implement Proper Error Handling with Graceful Degradation**

Replace `tokio_scoped::scope` with individual `tokio::spawn` calls and implement proper error handling:

```rust
pub(crate) fn start(&self, service_config: &ServiceConfig) -> Result<()> {
    let service = GrpcManagerServer::new(GrpcManagerService::new(
        self.chain_id,
        self.metadata_manager.clone(),
        self.data_manager.clone(),
    ))
    .send_compressed(CompressionEncoding::Zstd)
    .accept_compressed(CompressionEncoding::Zstd)
    .max_encoding_message_size(MAX_MESSAGE_SIZE)
    .max_decoding_message_size(MAX_MESSAGE_SIZE);
    
    let server = Server::builder()
        .http2_keepalive_interval(Some(HTTP2_PING_INTERVAL_DURATION))
        .http2_keepalive_timeout(Some(HTTP2_PING_TIMEOUT_DURATION))
        .add_service(service);

    let (tx, rx) = channel();
    
    // Spawn metadata manager with error recovery
    let metadata_manager = self.metadata_manager.clone();
    tokio::spawn(async move {
        loop {
            if let Err(e) = metadata_manager.start().await {
                error!("Metadata manager failed: {e:?}, restarting in 5s...");
                tokio::time::sleep(Duration::from_secs(5)).await;
            }
        }
    });
    
    // Spawn data manager (continues running even if other tasks fail)
    let data_manager = self.data_manager.clone();
    let is_master = self.is_master;
    tokio::spawn(async move { 
        data_manager.start(is_master, rx).await 
    });
    
    // Spawn file store uploader with error recovery (if master)
    if self.is_master {
        let file_store_uploader = self.file_store_uploader.clone();
        let data_manager = self.data_manager.clone();
        tokio::spawn(async move {
            loop {
                let mut uploader = file_store_uploader.lock().await;
                if let Err(e) = uploader.start(data_manager.clone(), tx.clone()).await {
                    error!("File store uploader failed: {e:?}, restarting in 10s...");
                    drop(uploader);
                    tokio::time::sleep(Duration::from_secs(10)).await;
                }
            }
        });
    }
    
    // Spawn gRPC server (critical - should attempt restart)
    tokio::spawn(async move {
        info!("Starting GrpcManager at {}.", service_config.listen_address);
        if let Err(e) = server.serve(service_config.listen_address).await {
            error!("gRPC server failed: {e:?}");
            // Could attempt restart or notify monitoring
        }
    });

    Ok(())
}
```

**Additional improvements:**
1. Remove `.unwrap()` calls within spawned tasks and implement proper error handling
2. Add retry logic with exponential backoff for transient failures
3. Implement health check endpoints that report degraded state
4. Use `tokio::select!` with cancellation tokens for clean shutdown
5. Add circuit breakers for failing components

## Proof of Concept

```rust
#[tokio::test]
async fn test_cascading_failure_in_grpc_manager() {
    use std::sync::Arc;
    use std::sync::atomic::{AtomicBool, Ordering};
    
    // Simulate the current implementation
    let metadata_started = Arc::new(AtomicBool::new(false));
    let data_started = Arc::new(AtomicBool::new(false));
    let server_started = Arc::new(AtomicBool::new(false));
    
    let metadata_flag = metadata_started.clone();
    let data_flag = data_started.clone();
    let server_flag = server_started.clone();
    
    let result = std::panic::catch_unwind(|| {
        tokio_scoped::scope(|s| {
            // Task 1: Metadata manager
            s.spawn(async move {
                metadata_flag.store(true, Ordering::SeqCst);
                tokio::time::sleep(Duration::from_millis(100)).await;
                // Simulate continuous operation
            });
            
            // Task 2: Data manager  
            s.spawn(async move {
                data_flag.store(true, Ordering::SeqCst);
                tokio::time::sleep(Duration::from_millis(100)).await;
                // Simulate continuous operation
            });
            
            // Task 3: File store that will panic
            s.spawn(async move {
                tokio::time::sleep(Duration::from_millis(50)).await;
                panic!("File store I/O error!"); // Simulates .unwrap() panic
            });
            
            // Task 4: gRPC server
            s.spawn(async move {
                server_flag.store(true, Ordering::SeqCst);
                tokio::time::sleep(Duration::from_millis(100)).await;
                // Simulate continuous operation
            });
        });
    });
    
    // Verify that the panic occurred
    assert!(result.is_err(), "Expected panic from file store task");
    
    // Verify all tasks were started
    assert!(metadata_started.load(Ordering::SeqCst));
    assert!(data_started.load(Ordering::SeqCst));
    assert!(server_started.load(Ordering::SeqCst));
    
    // Key finding: When one task panics, tokio_scoped cancels all other tasks
    // This means the gRPC server stops even though only file store failed
    println!("Demonstrated: Single task failure causes total service shutdown");
}
```

## Notes

- This vulnerability affects only the **indexer-grpc-manager** auxiliary service, not core blockchain consensus or transaction processing
- The blockchain continues operating normally even if the indexer-grpc-manager fails
- This is primarily an **availability and reliability issue** for the indexer API, not a funds or consensus security vulnerability
- The issue can be triggered by common operational failures (network issues, disk errors) without malicious input
- Similar patterns may exist in other components using `tokio_scoped::scope` with `.unwrap()` calls

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/grpc_manager.rs (L107-126)
```rust
        tokio_scoped::scope(|s| {
            s.spawn(async move {
                self.metadata_manager.start().await.unwrap();
            });
            s.spawn(async move { self.data_manager.start(self.is_master, rx).await });
            if self.is_master {
                s.spawn(async move {
                    self.file_store_uploader
                        .lock()
                        .await
                        .start(self.data_manager.clone(), tx)
                        .await
                        .unwrap();
                });
            }
            s.spawn(async move {
                info!("Starting GrpcManager at {}.", service_config.listen_address);
                server.serve(service_config.listen_address).await.unwrap();
            });
        });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L185-188)
```rust
            match file_store_uploader_recover_rx.await {
                Ok(_) => {},
                Err(_) => panic!("Should not happen!"),
            };
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/file_store_uploader.rs (L138-146)
```rust
            s.spawn(async move {
                while let Some((transactions, batch_metadata, end_batch)) = rx.recv().await {
                    let bytes_to_upload = batch_metadata.files.last().unwrap().size_bytes as u64;
                    self.do_upload(transactions, batch_metadata, end_batch)
                        .await
                        .unwrap();
                    FILE_STORE_UPLOADED_BYTES.inc_by(bytes_to_upload);
                }
            });
```
