# Audit Report

## Title
Corrupted Event Storage Halts Validator Catchup Permanently

## Summary
When event storage is corrupted and BCS deserialization fails during state sync, validators are unable to catch up with the network. The system repeatedly attempts to deserialize the same corrupted event, fails each time, resets the data stream, and retries indefinitely. There is no mechanism to skip corrupted events, resulting in permanent validator unavailability.

## Finding Description

The vulnerability exists in the event retrieval and deserialization flow during validator state synchronization. When a validator attempts to catch up with the network, it fetches transactions including their associated events from storage.

**Critical Flow Path:**

1. **Event Deserialization**: Events are stored in BCS format and deserialized using `bcs::from_bytes()`: [1](#0-0) 

2. **Iterator Error Propagation**: The `EventsByVersionIter` peeks at events and propagates any deserialization errors immediately without skipping: [2](#0-1) 

3. **Transaction Fetching Failure**: During state sync, when fetching transactions with events, any deserialization error causes the entire operation to fail: [3](#0-2) 

4. **Stream Reset and Retry**: The error triggers stream termination and the validator attempts to initialize a new stream: [4](#0-3) 

5. **Infinite Loop**: The new stream fetches the same transaction with the corrupted event, repeating the cycle indefinitely: [5](#0-4) 

**Security Guarantee Broken:**

This violates the **liveness invariant** - validators must be able to synchronize with the network to participate in consensus. A validator stuck on corrupted storage cannot:
- Catch up to the current ledger state
- Participate in consensus voting
- Serve API requests for recent data
- Process new transactions

## Impact Explanation

**Severity: High** (Validator node slowdown/unavailability)

Per Aptos bug bounty criteria, this qualifies as High severity because it causes:
- **Validator node unavailability**: The affected validator cannot catch up and remains out of service
- **Significant protocol violation**: Validators should have resilience mechanisms to handle storage issues

The impact is:
- Affected validators are permanently stuck at the version before the corrupted event
- Loss of validator participation reduces network resilience
- If multiple validators experience similar corruption (e.g., from a widespread bug), it could affect consensus quorum
- The validator operator must manually intervene to fix storage or skip versions, which may not be possible without data loss

While this does not directly cause consensus safety violations or fund loss, it significantly degrades network availability and violates the expectation that validators can recover from transient storage issues.

## Likelihood Explanation

**Likelihood: Medium**

Event storage corruption can occur due to:
1. **Software bugs**: Serialization bugs, race conditions, or crashes during writes could corrupt event data
2. **Hardware failures**: Disk corruption, memory errors, or filesystem issues
3. **Operator errors**: Improper node shutdown, storage migration issues, or backup restoration problems
4. **Version mismatches**: Upgrading/downgrading nodes with incompatible event formats

While direct attacker-controlled corruption requires node access, the vulnerability is that the system has **no recovery mechanism** for an error condition that can naturally occur. Production blockchain nodes should be resilient to storage errors, as they are expected to run continuously for extended periods across diverse hardware.

The likelihood increases with:
- Longer node operation time (more exposure to hardware/software failures)
- Complex event structures that are more prone to serialization bugs
- Network upgrades that change event formats

## Recommendation

Implement graceful error handling for corrupted event data with multiple recovery strategies:

**Option 1: Skip and Log (Recommended for non-critical events)**
```rust
// In EventsByVersionIter::next_impl()
fn next_impl(&mut self) -> Result<Option<Vec<ContractEvent>>> {
    if self.expected_next_version >= self.end_version {
        return Ok(None);
    }

    let mut ret = Vec::new();
    while let Some(res) = self.inner.peek() {
        match res {
            Ok(((version, _index), _event)) => {
                if *version != self.expected_next_version {
                    break;
                }
                let ((_version, _index), event) =
                    self.inner.next().transpose()?.expect("Known to exist.");
                ret.push(event);
            }
            Err(e) => {
                // Log the error and skip corrupted event
                error!(
                    "Skipping corrupted event at version {}: {}",
                    self.expected_next_version, e
                );
                // Consume the errored item and continue
                let _ = self.inner.next();
                // Optionally: emit a metric for monitoring
            }
        }
    }
    self.expected_next_version = self
        .expected_next_version
        .checked_add(1)
        .ok_or_else(|| AptosDbError::Other("expected version overflowed.".to_string()))?;
    Ok(Some(ret))
}
```

**Option 2: Mark and Continue**
Store a marker indicating which events failed deserialization, allowing:
- Validators to continue syncing past the corruption
- Monitoring/alerting when corrupted events are detected
- Later recovery when the issue is resolved

**Option 3: Configurable Behavior**
Add a configuration option to control behavior on event deserialization failure:
- `FailFast`: Current behavior (halt on error)
- `SkipCorrupted`: Skip and continue (with logging)
- `RetryWithBackoff`: Retry with exponential backoff before skipping

The storage service should also be updated to handle partial event retrieval: [6](#0-5) 

## Proof of Concept

```rust
// Test case demonstrating the issue
#[tokio::test]
async fn test_corrupted_event_blocks_catchup() {
    // Setup: Initialize a validator with some synced state
    let (mut validator, storage) = setup_test_validator().await;
    
    // Corrupt an event in storage by writing invalid BCS data
    let version_to_corrupt = 100;
    let corrupted_event_data = vec![0xFF, 0xFF, 0xFF]; // Invalid BCS
    
    storage.put_event(version_to_corrupt, 0, corrupted_event_data);
    
    // Attempt state sync from version 99 to 200
    let result = validator
        .sync_to_target(LedgerInfoWithSignatures::new(
            /* target version 200 */
        ))
        .await;
    
    // Validator should fail to sync past the corrupted event
    assert!(result.is_err());
    
    // Verify validator is stuck at version 99
    let synced_version = validator.get_synced_version().await;
    assert_eq!(synced_version, 99);
    
    // Verify continuous retry behavior
    for _ in 0..5 {
        tokio::time::sleep(Duration::from_secs(1)).await;
        let attempt = validator.sync_to_target(/* same target */).await;
        assert!(attempt.is_err()); // Should keep failing
        assert_eq!(validator.get_synced_version().await, 99); // Stuck
    }
}
```

## Notes

Events are part of the deterministic execution output and are included in transaction proofs. However, for state sync purposes, events are primarily informational - the critical state transitions are captured in the write sets and transaction infos. The current implementation treats event deserialization as critical to the sync process, but with proper error handling, validators could continue syncing the essential state even when events are corrupted.

A production-grade blockchain system should distinguish between:
- **Critical data**: Transaction info, write sets, state values (must be valid)
- **Auxiliary data**: Events, logs (can be skipped with warnings if corrupted)

This would allow validators to maintain liveness even when facing storage corruption in non-critical components.

### Citations

**File:** storage/aptosdb/src/schema/event/mod.rs (L54-56)
```rust
    fn decode_value(data: &[u8]) -> Result<Self> {
        bcs::from_bytes(data).map_err(Into::into)
    }
```

**File:** storage/aptosdb/src/utils/iterators.rs (L269-272)
```rust
        while let Some(res) = self.inner.peek() {
            let ((version, _index), _event) = res
                .as_ref()
                .map_err(|e| AptosDbError::Other(format!("Hit error iterating events: {}", e)))?;
```

**File:** state-sync/storage-service/server/src/storage.rs (L451-456)
```rust
                Some((Err(error), _, _, _))
                | Some((_, Err(error), _, _))
                | Some((_, _, Err(error), _))
                | Some((_, _, _, Err(error))) => {
                    return Err(Error::StorageErrorEncountered(error.to_string()));
                },
```

**File:** state-sync/state-sync-driver/src/driver.rs (L494-533)
```rust
    /// Handles an error notification sent by the storage synchronizer
    async fn handle_error_notification(&mut self, error_notification: ErrorNotification) {
        warn!(LogSchema::new(LogEntry::SynchronizerNotification)
            .error_notification(error_notification.clone())
            .message("Received an error notification from the storage synchronizer!"));

        // Terminate the currently active streams
        let notification_id = error_notification.notification_id;
        let notification_feedback = NotificationFeedback::InvalidPayloadData;
        if self.bootstrapper.is_bootstrapped() {
            if let Err(error) = self
                .continuous_syncer
                .handle_storage_synchronizer_error(NotificationAndFeedback::new(
                    notification_id,
                    notification_feedback,
                ))
                .await
            {
                error!(LogSchema::new(LogEntry::SynchronizerNotification)
                    .message(&format!(
                        "Failed to terminate the active stream for the continuous syncer! Error: {:?}",
                        error
                    )));
            }
        } else if let Err(error) = self
            .bootstrapper
            .handle_storage_synchronizer_error(NotificationAndFeedback::new(
                notification_id,
                notification_feedback,
            ))
            .await
        {
            error!(
                LogSchema::new(LogEntry::SynchronizerNotification).message(&format!(
                    "Failed to terminate the active stream for the bootstrapper! Error: {:?}",
                    error
                ))
            );
        };
    }
```

**File:** state-sync/state-sync-driver/src/continuous_syncer.rs (L501-522)
```rust
    pub async fn handle_storage_synchronizer_error(
        &mut self,
        notification_and_feedback: NotificationAndFeedback,
    ) -> Result<(), Error> {
        // Reset the active stream
        self.reset_active_stream(Some(notification_and_feedback))
            .await?;

        // Fallback to output syncing if we need to
        if let ContinuousSyncingMode::ExecuteTransactionsOrApplyOutputs =
            self.get_continuous_syncing_mode()
        {
            self.output_fallback_handler.fallback_to_outputs();
            metrics::set_gauge(
                &metrics::DRIVER_FALLBACK_MODE,
                ExecutingComponent::ContinuousSyncer.get_label(),
                1,
            );
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/backup/backup_handler.rs (L87-92)
```rust
            let event_vec = event_vec_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "Events not found when Transaction exists., version {}",
                    version
                ))
            })??;
```
