# Audit Report

## Title
Byzantine Validators Can Force Infinite Retry Loops in Commit Reliable Broadcast Leading to Resource Exhaustion

## Summary
Byzantine validators can maliciously send `Nack` responses to all commit reliable broadcast RPCs, triggering infinite retry loops that cause resource exhaustion on honest validator nodes. The vulnerability exists because the retry mechanism uses an unbounded exponential backoff policy without a maximum retry limit.

## Finding Description

The commit reliable broadcast system is designed to ensure commit votes and decisions are delivered to all validators with automatic retry on failures. However, the implementation contains a critical flaw in handling `Nack` responses.

When `RBNetworkSender::send_rb_rpc_raw()` receives a `Nack` response, it bails with an error message "Received nack, will retry": [1](#0-0) 

This error propagates to the `ReliableBroadcast::multicast()` retry logic, which schedules a new retry using an exponential backoff strategy: [2](#0-1) 

The critical issue is that the backoff policy is configured as an **infinite iterator** without any maximum retry limit. The retry logic expects the backoff strategy to always produce values: [3](#0-2) 

The `ExponentialBackoff` is configured with only a `max_delay` cap, producing an infinite sequence of durations: [4](#0-3) 

**Attack Path:**

1. Honest validator initiates a commit vote/decision broadcast via `do_reliable_broadcast()`: [5](#0-4) 

2. The broadcast creates an `AckState` that tracks acknowledgments from ALL validators: [6](#0-5) 

3. Byzantine validator receives the RPC and maliciously responds with `CommitMessage::Nack` instead of `CommitMessage::Ack`

4. The `send_rb_rpc_raw()` function matches the Nack and bails, triggering retry logic

5. The retry scheduler calls `backoff_strategy.next().expect("should produce value")` which always succeeds due to infinite iterator

6. A new retry is scheduled and added to `rpc_futures`

7. The `AckState` only completes when ALL validators acknowledge, so it waits indefinitely for Byzantine validators: [7](#0-6) 

8. Steps 3-6 repeat indefinitely, with exponentially increasing delays capped at 5 seconds

9. With multiple Byzantine validators and multiple concurrent broadcasts (different blocks, votes), resource exhaustion occurs

## Impact Explanation

This vulnerability qualifies as **High Severity** per Aptos Bug Bounty criteria under the "Validator node slowdowns" category.

**Resource consumption**: Each retry cycle consumes:
- Network bandwidth for repeated RPC attempts every 5 seconds (after backoff reaches max_delay)
- CPU cycles for message serialization/deserialization
- Executor task pool slots for pending futures
- Logging and error handling overhead

With multiple Byzantine validators (up to 1/3 of the validator set under AptosBFT assumptions) and multiple concurrent broadcasts (each block generates commit votes/decisions), the resource exhaustion compounds multiplicatively. This leads to:

- Degraded consensus performance
- Increased latency in block commitment  
- Potential node instability in extreme cases
- Wasted network bandwidth affecting overall network health

The attack does not directly compromise consensus safety or cause fund loss, which prevents it from being Critical severity. However, it significantly impacts network availability and validator operations, meeting the "Validator node slowdowns" criterion for High severity.

## Likelihood Explanation

The likelihood of this vulnerability being exploited is **High**:

1. **Attacker requirements**: Only requires Byzantine validator participation, which is already assumed in the AptosBFT threat model (< 1/3 Byzantine validators)

2. **Ease of exploitation**: Byzantine validators can trivially modify their code to always return `Nack` responses instead of processing votes legitimately. No complex exploit chain or timing requirements needed.

3. **Detection difficulty**: The attack appears as legitimate network issues or transient failures, making it hard to distinguish from normal network problems

4. **Amplification**: A single Byzantine validator can force infinite retries on all honest validators attempting to broadcast to it. With k Byzantine validators and n concurrent broadcasts, the attack scales to kÃ—n infinite retry loops.

5. **No authentication barriers**: The `Nack` response is a valid protocol message defined in the `CommitMessage` enum that doesn't require special privileges beyond being a validator: [8](#0-7) 

## Recommendation

Implement one or more of the following mitigations:

1. **Add maximum retry limit**: Modify the backoff policy to limit retries:
```rust
let rb_backoff_policy = ExponentialBackoff::from_millis(2)
    .factor(50)
    .max_delay(Duration::from_secs(5))
    .take(10); // Limit to 10 retries per peer
```

2. **Use quorum-based acknowledgment**: Modify `AckState` to complete when a quorum (2f+1) of validators acknowledge, rather than waiting for all validators.

3. **Add broadcast timeout**: Wrap the broadcast task with a timeout to prevent indefinite execution.

4. **Track and penalize Nack behavior**: Implement metrics to detect validators that consistently respond with Nack and consider reputation/slashing mechanisms.

## Proof of Concept

The vulnerability can be demonstrated by:

1. Setting up a test network with Byzantine validators
2. Modifying Byzantine validator code to always respond with `Nack` in `process_commit_message()`: [9](#0-8) 
3. Observing that honest validators continuously retry broadcasts to Byzantine validators
4. Monitoring resource consumption (network bandwidth, CPU, memory) increasing over time
5. Measuring degraded consensus performance and increased block commitment latency

The test would use the reliable broadcast test framework shown in: [10](#0-9) 

However, modify the `TestRBSender` to simulate a Byzantine validator that never stops failing (returns errors indefinitely), demonstrating the infinite retry behavior.

## Notes

- This is a protocol-level vulnerability, not an external "Network DoS attack" (DDoS, BGP hijacking, etc.) which would be out of scope
- The vulnerability exploits a design flaw in the retry mechanism where Byzantine participants (< 1/3) can cause disproportionate resource consumption
- The broadcast mechanism waits for ALL validators to acknowledge, even though consensus only requires quorum (2f+1)
- The `.expect("should produce value")` at line 197 indicates developers assumed the iterator would always produce values, suggesting this infinite retry scenario was not anticipated

### Citations

**File:** consensus/src/pipeline/commit_reliable_broadcast.rs (L22-33)
```rust
#[derive(Clone, Debug, Serialize, Deserialize)]
/// Network message for the pipeline phase
pub enum CommitMessage {
    /// Vote on execution result
    Vote(CommitVote),
    /// Quorum proof on execution result
    Decision(CommitDecision),
    /// Ack on either vote or decision
    Ack(()),
    /// Nack is non-acknowledgement, we got your message, but it was bad/we were bad
    Nack,
}
```

**File:** consensus/src/pipeline/commit_reliable_broadcast.rs (L72-76)
```rust
    pub fn new(validators: impl Iterator<Item = Author>) -> Arc<Self> {
        Arc::new(Self {
            validators: Mutex::new(validators.collect()),
        })
    }
```

**File:** consensus/src/pipeline/commit_reliable_broadcast.rs (L99-105)
```rust
        let mut validators = self.validators.lock();
        if validators.remove(&peer) {
            if validators.is_empty() {
                Ok(Some(()))
            } else {
                Ok(None)
            }
```

**File:** consensus/src/pipeline/commit_reliable_broadcast.rs (L126-128)
```rust
            ConsensusMsg::CommitMessage(resp) if matches!(*resp, CommitMessage::Nack) => {
                bail!("Received nack, will retry")
            },
```

**File:** crates/reliable-broadcast/src/lib.rs (L191-200)
```rust
                            Err(e) => {
                                log_rpc_failure(e, receiver);

                                let backoff_strategy = backoff_policies
                                    .get_mut(&receiver)
                                    .expect("should be present");
                                let duration = backoff_strategy.next().expect("should produce value");
                                rpc_futures
                                    .push(send_message(receiver, Some(duration)));
                            },
```

**File:** consensus/src/pipeline/buffer_manager.rs (L208-210)
```rust
        let rb_backoff_policy = ExponentialBackoff::from_millis(2)
            .factor(50)
            .max_delay(Duration::from_secs(5));
```

**File:** consensus/src/pipeline/buffer_manager.rs (L269-285)
```rust
    fn do_reliable_broadcast(&self, message: CommitMessage) -> Option<DropGuard> {
        // If consensus observer is enabled, we don't need to broadcast
        if self.consensus_observer_config.observer_enabled {
            return None;
        }

        // Otherwise, broadcast the message and return the drop guard
        let (abort_handle, abort_registration) = AbortHandle::new_pair();
        let task = self.reliable_broadcast.broadcast(
            message,
            AckState::new(
                self.epoch_state
                    .verifier
                    .get_ordered_account_addresses_iter(),
            ),
        );
        tokio::spawn(Abortable::new(task, abort_registration));
```

**File:** consensus/src/pipeline/buffer_manager.rs (L736-822)
```rust
    fn process_commit_message(&mut self, commit_msg: IncomingCommitRequest) -> Option<HashValue> {
        let IncomingCommitRequest {
            req,
            protocol,
            response_sender,
        } = commit_msg;
        match req {
            CommitMessage::Vote(vote) => {
                // find the corresponding item
                let author = vote.author();
                let commit_info = vote.commit_info().clone();
                debug!("Receive commit vote {} from {}", commit_info, author);
                let target_block_id = vote.commit_info().id();
                let current_cursor = self
                    .buffer
                    .find_elem_by_key(*self.buffer.head_cursor(), target_block_id);
                if current_cursor.is_some() {
                    let mut item = self.buffer.take(&current_cursor);
                    let new_item = match item.add_signature_if_matched(vote) {
                        Ok(()) => {
                            let response =
                                ConsensusMsg::CommitMessage(Box::new(CommitMessage::Ack(())));
                            if let Ok(bytes) = protocol.to_bytes(&response) {
                                let _ = response_sender.send(Ok(bytes.into()));
                            }
                            item.try_advance_to_aggregated(&self.epoch_state.verifier)
                        },
                        Err(e) => {
                            error!(
                                error = ?e,
                                author = author,
                                commit_info = commit_info,
                                "Failed to add commit vote",
                            );
                            reply_nack(protocol, response_sender);
                            item
                        },
                    };
                    self.buffer.set(&current_cursor, new_item);
                    if self.buffer.get(&current_cursor).is_aggregated() {
                        return Some(target_block_id);
                    } else {
                        return None;
                    }
                } else if self.try_add_pending_commit_vote(vote) {
                    reply_ack(protocol, response_sender);
                } else {
                    reply_nack(protocol, response_sender); // TODO: send_commit_vote() doesn't care about the response and this should be direct send not RPC
                }
            },
            CommitMessage::Decision(commit_proof) => {
                let target_block_id = commit_proof.ledger_info().commit_info().id();
                info!(
                    "Receive commit decision {}",
                    commit_proof.ledger_info().commit_info()
                );
                let cursor = self
                    .buffer
                    .find_elem_by_key(*self.buffer.head_cursor(), target_block_id);
                if cursor.is_some() {
                    let item = self.buffer.take(&cursor);
                    let new_item = item.try_advance_to_aggregated_with_ledger_info(
                        commit_proof.ledger_info().clone(),
                    );
                    let aggregated = new_item.is_aggregated();
                    self.buffer.set(&cursor, new_item);

                    reply_ack(protocol, response_sender);
                    if aggregated {
                        return Some(target_block_id);
                    }
                } else if self.try_add_pending_commit_proof(commit_proof.into_inner()) {
                    reply_ack(protocol, response_sender);
                } else {
                    reply_nack(protocol, response_sender); // TODO: send_commit_proof() doesn't care about the response and this should be direct send not RPC
                }
            },
            CommitMessage::Ack(_) => {
                // It should be filtered out by verify, so we log errors here
                error!("Unexpected ack message");
            },
            CommitMessage::Nack => {
                error!("Unexpected NACK message");
            },
        }
        None
    }
```

**File:** crates/reliable-broadcast/src/tests.rs (L144-167)
```rust
#[tokio::test]
async fn test_reliable_broadcast() {
    let (_, validator_verifier) = random_validator_verifier(5, None, false);
    let validators = validator_verifier.get_ordered_account_addresses();
    let self_author = validators[0];
    let failures = HashMap::from([(validators[0], 1), (validators[2], 3)]);
    let sender = Arc::new(TestRBSender::<TestRBMessage>::new(failures));
    let rb = ReliableBroadcast::new(
        self_author,
        validators.clone(),
        sender,
        FixedInterval::from_millis(10),
        TimeService::real(),
        Duration::from_millis(500),
        BoundedExecutor::new(2, Handle::current()),
    );
    let message = TestMessage(vec![42; validators.len() - 1]);
    let aggregating = Arc::new(TestBroadcastStatus {
        threshold: validators.len(),
        received: Arc::new(Mutex::new(HashSet::new())),
    });
    let fut = rb.broadcast(message, aggregating);
    assert_ok_eq!(fut.await, validators.into_iter().collect());
}
```
