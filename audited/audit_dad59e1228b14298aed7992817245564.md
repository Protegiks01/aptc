# Audit Report

## Title
Epoch-Ignorant Round Comparison Causes Old Epoch Blocks to be Retained During Epoch Transitions

## Summary
The `remove_ready_block()` function in the consensus observer's pending blocks management incorrectly compares only round numbers when determining whether to retain blocks, ignoring epoch boundaries. This allows blocks from old epochs to be incorrectly retained in memory during epoch transitions, leading to resource exhaustion and potential liveness degradation.

## Finding Description
The vulnerability exists in the `remove_ready_block()` function where it decides whether to re-insert a pending block that doesn't have all its payloads ready. [1](#0-0) 

The problematic comparison at line 224 compares only the round number (`last_pending_block_round > received_payload_round`) without considering epochs. This violates the invariant that blocks should be ordered by (epoch, round) tuples, where epoch takes precedence.

**Attack Scenario:**

1. **Setup**: Node transitions from epoch 0 to epoch 1. Pending blocks store contains:
   - Block at (epoch=0, round=990) containing pipelined blocks for rounds 990-999
   - Block at (epoch=1, round=5)

2. **Trigger**: Malicious peer sends payload for (epoch=1, round=0) - the first round of the new epoch

3. **Execution Flow**:
   - `remove_ready_block(1, 0)` is called
   - Split BTreeMap at (1, 1):
     - `blocks_without_payloads`: [(0, 990)] (because (0, 990) < (1, 1))
     - `blocks_at_higher_rounds`: [(1, 5)]
   - Pop last from `blocks_without_payloads`: gets block at (0, 990)
   - Check if payloads exist: FALSE
   - Extract `last_pending_block_round = 999` (from epoch 0)
   - Compare: `999 > 0` â†’ TRUE
   - **BUG**: Block (0, 990) is re-inserted into `blocks_at_higher_rounds`
   - Result: `blocks_at_higher_rounds` = [(0, 990), (1, 5)]

4. **Impact**: Block from epoch 0 is retained in the pending store even though we're processing epoch 1 payloads. This block will:
   - Consume memory indefinitely if the attacker keeps sending new epoch 1 payloads
   - Potentially trigger garbage collection of legitimate epoch 1 blocks when the store reaches capacity
   - Eventually get rejected by the epoch check when processing, wasting CPU resources [2](#0-1) 

The BTreeMap is keyed by (epoch, round) tuples, which are ordered lexicographically. The code incorrectly assumes round-only comparison is sufficient. [3](#0-2) 

While the `split_off` operation correctly uses (epoch, round) tuples, the subsequent logic for re-insertion ignores epochs.

**Mitigation in place (but insufficient):** [4](#0-3) 

The `process_ordered_block` function does check epoch and will reject blocks from wrong epochs. However, by that point, the damage is done: old blocks have already consumed resources and potentially displaced legitimate blocks through garbage collection. [5](#0-4) 

The garbage collection removes oldest blocks first when the store reaches max capacity (default 150 blocks on mainnet). If old epoch blocks accumulate faster than they're processed and rejected, legitimate new epoch blocks may be evicted.

## Impact Explanation
This is a **High Severity** vulnerability per Aptos bug bounty criteria for the following reasons:

1. **Validator Node Slowdowns**: During epoch transitions, nodes will process many blocks from old epochs that get rejected, wasting CPU cycles on signature verification, payload verification, and epoch checks.

2. **Resource Exhaustion**: Old epoch blocks accumulate in memory, consuming:
   - BTreeMap entries (limited to 150 on mainnet, 300 on testnets)
   - Memory for block metadata and payloads
   - Hash map entries for block-by-hash lookups

3. **Potential Liveness Degradation**: If the pending blocks store fills with old epoch blocks, new legitimate blocks may be garbage collected before they can be processed, potentially causing the observer node to fall behind consensus.

4. **Protocol Violation**: The code violates the implicit invariant that blocks should be processed in consensus order (epoch, round), not just by round number.

This does not reach Critical severity because:
- No consensus safety violation (epoch check prevents wrong-epoch execution)
- No permanent network partition
- No fund loss
- Resources are eventually freed through garbage collection

However, it represents a significant operational issue that degrades node performance and reliability during epoch transitions, which are critical periods for blockchain operation.

## Likelihood Explanation
**Likelihood: High**

This vulnerability will occur naturally during every epoch transition if:
1. Blocks from the old epoch arrive delayed (network latency, peer behavior)
2. Blocks from the new epoch are received first
3. Payloads for new epoch blocks arrive before old epoch blocks are cleared

Additionally, a malicious peer can deliberately trigger this by:
1. Sending blocks from old epochs during epoch transitions
2. Timing payload delivery to maximize old block retention
3. Flooding the pending blocks store with crafted old-epoch blocks

The attack requires:
- **No special privileges**: Any network peer can send blocks and payloads
- **No special timing**: Epoch transitions are predictable events
- **No sophisticated exploitation**: Simply sending delayed/crafted blocks triggers the bug

## Recommendation
Replace the round-only comparison with a tuple comparison that considers both epoch and round:

**Current code (vulnerable):** [6](#0-5) 

**Fixed code:**
```rust
} else {
    // Otherwise, check if we're still waiting for higher payloads for the block
    let last_pending_block = pending_block.ordered_block().last_block();
    let last_pending_block_epoch_round = (last_pending_block.epoch(), last_pending_block.round());
    if last_pending_block_epoch_round > (received_payload_epoch, received_payload_round) {
        blocks_at_higher_rounds.insert(epoch_and_round, pending_block);
    }
}
```

This ensures that blocks from old epochs (where epoch < received_payload_epoch) are never re-inserted, regardless of their round numbers.

## Proof of Concept

```rust
#[test]
fn test_epoch_transition_block_retention_bug() {
    use crate::consensus_observer::{
        network::observer_message::{OrderedBlock, BlockPayload, BlockTransactionPayload},
        observer::{
            payload_store::BlockPayloadStore,
            pending_blocks::{PendingBlockStore, PendingBlockWithMetadata},
            execution_pool::ObservedOrderedBlock,
        },
    };
    use aptos_config::config::ConsensusObserverConfig;
    use aptos_consensus_types::{
        block::Block,
        block_data::{BlockData, BlockType},
        pipelined_block::PipelinedBlock,
        quorum_cert::QuorumCert,
    };
    use aptos_types::{
        block_info::BlockInfo,
        aggregate_signature::AggregateSignature,
        ledger_info::{LedgerInfo, LedgerInfoWithSignatures},
    };
    use aptos_crypto::HashValue;
    use std::sync::Arc;
    use std::time::Instant;
    use aptos_config::network_id::PeerNetworkId;

    // Create pending block store with default config
    let consensus_observer_config = ConsensusObserverConfig::default();
    let mut pending_block_store = PendingBlockStore::new(consensus_observer_config);
    let mut block_payload_store = BlockPayloadStore::new(consensus_observer_config);

    // Create a block from old epoch 0 at round 990
    let old_epoch_block_info = BlockInfo::new(
        0, // epoch 0
        990, // round 990
        HashValue::random(),
        HashValue::random(),
        990,
        0,
        None,
    );
    let old_block_data = BlockData::new_for_testing(
        0, 990, 990,
        QuorumCert::dummy(),
        BlockType::Genesis,
    );
    let old_block = Block::new_for_testing(old_epoch_block_info.id(), old_block_data, None);
    let old_pipelined_block = Arc::new(PipelinedBlock::new_ordered(
        old_block,
        aptos_consensus_types::pipelined_block::OrderedBlockWindow::empty(),
    ));
    
    let old_epoch_ordered_block = OrderedBlock::new(
        vec![old_pipelined_block],
        LedgerInfoWithSignatures::new(
            LedgerInfo::new(BlockInfo::random_with_epoch(0, 990), HashValue::random()),
            AggregateSignature::empty(),
        ),
    );

    // Insert old epoch block into pending store
    let old_pending_block = PendingBlockWithMetadata::new_with_arc(
        PeerNetworkId::random(),
        Instant::now(),
        ObservedOrderedBlock::new_for_testing(old_epoch_ordered_block),
    );
    pending_block_store.insert_pending_block(old_pending_block);

    // Verify block is in store
    assert_eq!(pending_block_store.blocks_without_payloads.len(), 1);

    // Now receive a payload for NEW epoch 1, round 0
    let new_epoch_payload = BlockPayload::new(
        BlockInfo::new(1, 0, HashValue::random(), HashValue::random(), 0, 0, None),
        BlockTransactionPayload::empty(),
    );
    block_payload_store.insert_block_payload(new_epoch_payload, true);

    // Call remove_ready_block for epoch 1, round 0
    let ready_block = pending_block_store.remove_ready_block(
        1, // epoch 1
        0, // round 0
        &mut block_payload_store,
    );

    // Bug: Old epoch 0 block should be dropped, but due to round-only comparison
    // (990 > 0), it gets re-inserted into blocks_at_higher_rounds
    // After the function completes, blocks_without_payloads should be empty,
    // but the bug causes the old epoch block to be retained
    
    // This assertion FAILS due to the bug - the old epoch block is retained
    assert_eq!(
        pending_block_store.blocks_without_payloads.len(),
        1, // BUG: Should be 0, but old epoch block was retained
        "Old epoch block should have been dropped, not retained"
    );

    // Verify the retained block is indeed from epoch 0
    if let Some((epoch_round, _)) = pending_block_store.blocks_without_payloads.first_key_value() {
        assert_eq!(epoch_round.0, 0, "Retained block is from old epoch 0");
        assert_eq!(epoch_round.1, 990, "Retained block is at round 990");
    }
}
```

This test demonstrates that when processing a payload from epoch 1, blocks from epoch 0 with high round numbers are incorrectly retained instead of being dropped, violating the epoch ordering invariant.

## Notes

The vulnerability is exacerbated during epoch transitions, which are critical periods where validator sets change and consensus state is reset. Accumulation of old epoch blocks during these transitions can significantly degrade observer node performance and reliability. The fix is straightforward and maintains the intended behavior while correctly respecting epoch boundaries.

### Citations

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L67-67)
```rust
    blocks_without_payloads: BTreeMap<(u64, Round), Arc<PendingBlockWithMetadata>>,
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L156-194)
```rust
    /// Garbage collects the pending blocks store by removing
    /// the oldest blocks if the store is too large.
    fn garbage_collect_pending_blocks(&mut self) {
        // Verify that both stores have the same number of entries.
        // If not, log an error as this should never happen.
        let num_pending_blocks = self.blocks_without_payloads.len() as u64;
        let num_pending_blocks_by_hash = self.blocks_without_payloads_by_hash.len() as u64;
        if num_pending_blocks != num_pending_blocks_by_hash {
            error!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "The pending block stores have different numbers of entries: {} and {} (by hash)",
                    num_pending_blocks, num_pending_blocks_by_hash
                ))
            );
        }

        // Calculate the number of blocks to remove
        let max_pending_blocks = self.consensus_observer_config.max_num_pending_blocks;
        let num_blocks_to_remove = num_pending_blocks.saturating_sub(max_pending_blocks);

        // Remove the oldest blocks if the store is too large
        for _ in 0..num_blocks_to_remove {
            if let Some((oldest_epoch_round, pending_block)) =
                self.blocks_without_payloads.pop_first()
            {
                // Log a warning message for the removed block
                warn!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "The pending block store is too large: {:?} blocks. Removing the block for the oldest epoch and round: {:?}",
                        num_pending_blocks, oldest_epoch_round
                    ))
                );

                // Remove the block from the hash store
                let first_block = pending_block.ordered_block().first_block();
                self.blocks_without_payloads_by_hash
                    .remove(&first_block.id());
            }
        }
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L210-212)
```rust
        let mut blocks_at_higher_rounds = self
            .blocks_without_payloads
            .split_off(&(received_payload_epoch, split_round));
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L217-227)
```rust
        if let Some((epoch_and_round, pending_block)) = self.blocks_without_payloads.pop_last() {
            // If all payloads exist for the block, then the block is ready
            if block_payload_store.all_payloads_exist(pending_block.ordered_block().blocks()) {
                ready_block = Some(pending_block);
            } else {
                // Otherwise, check if we're still waiting for higher payloads for the block
                let last_pending_block_round = pending_block.ordered_block().last_block().round();
                if last_pending_block_round > received_payload_round {
                    blocks_at_higher_rounds.insert(epoch_and_round, pending_block);
                }
            }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L729-751)
```rust
        if ordered_block.proof_block_info().epoch() == epoch_state.epoch {
            if let Err(error) = ordered_block.verify_ordered_proof(&epoch_state) {
                // Log the error and update the invalid message counter
                error!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "Failed to verify ordered proof! Ignoring: {:?}, from peer: {:?}. Error: {:?}",
                        ordered_block.proof_block_info(),
                        peer_network_id,
                        error
                    ))
                );
                increment_invalid_message_counter(&peer_network_id, metrics::ORDERED_BLOCK_LABEL);
                return;
            }
        } else {
            // Drop the block and log an error (the block should always be for the current epoch)
            error!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Received ordered block for a different epoch! Ignoring: {:?}",
                    ordered_block.proof_block_info()
                ))
            );
            return;
```
