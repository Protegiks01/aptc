# Audit Report

## Title
Database Pruning Race Condition in Reconfiguration Notifications Causes Node Failures During Epoch Transitions

## Summary
The `read_on_chain_configs()` function creates a `DbBackedOnChainConfig` object that stores a specific database version for lazy fetching of on-chain configurations. This object is sent to subscribers who may read from it at an arbitrary later time. However, the underlying database can be pruned between when the config object is created and when it's actually used, causing reads to fail with pruning errors and preventing nodes from processing epoch reconfigurations.

## Finding Description

The vulnerability exists in a time-of-check-time-of-use (TOCTOU) race condition between database version snapshot creation and actual state reads. [1](#0-0) 

In `read_on_chain_configs()`, a state view is created at a specific version, and immediately after, a `DbBackedOnChainConfig` object is constructed that stores this version for later use. This config object is embedded in an `OnChainConfigPayload` and sent to subscribers via reconfiguration notifications. [2](#0-1) 

The `DbBackedOnChainConfig` implements lazy loading—it stores the database reader and version but doesn't fetch configs until `get<T>()` is called. When subscribers eventually call `get<T>()`, it invokes `get_state_value_by_version()` with the stored version. [3](#0-2) 

The `get_state_value_by_version()` method performs a pruning check via `error_if_state_kv_pruned()`: [4](#0-3) 

This check compares the requested version against the `min_readable_version` tracked by the state KV pruner. If the version has been pruned, an error is returned.

The pruning logic automatically updates `min_readable_version` as new transactions commit: [5](#0-4) 

With the default configuration, the state KV pruner has a prune window of 90 million versions: [6](#0-5) 

**Exploitation Scenario:**

Subscribers like the consensus epoch manager receive reconfiguration notifications and call multiple `on_chain_configs.get<T>()` methods to fetch various configurations: [7](#0-6) 

If there's any delay in processing (network congestion, resource exhaustion, backlog of notifications, node restart), and the blockchain continues committing transactions at high throughput, the original reconfiguration version can be pruned before the subscriber reads from it.

On a chain processing 5,000 TPS:
- 5,000 tx/sec × 86,400 sec/day = 432,000,000 transactions/day
- Time to fill 90M window: ~90M ÷ 432M/day ≈ 5 hours

A node that's delayed by just a few hours on a high-throughput chain will attempt to read from a pruned version, causing the reconfiguration to fail.

## Impact Explanation

This is a **HIGH severity** issue per Aptos bug bounty criteria, causing:

1. **Validator Node Failures**: Nodes unable to fetch on-chain configs cannot start new epochs, leading to validator failures
2. **Consensus Liveness Issues**: If multiple validators are affected, consensus could stall
3. **Protocol Violation**: Breaks the invariant that reconfiguration notifications should be reliably processable
4. **Service Degradation**: Affected nodes must restart and re-sync, causing downtime

While not directly exploitable by an attacker (pruning is automatic), this is a critical reliability vulnerability that can cause cascading node failures, especially during periods of high throughput or when nodes experience temporary delays.

## Likelihood Explanation

**High Likelihood** on production networks because:

1. **High Throughput Chains**: Aptos mainnet regularly processes thousands of TPS, making the 90M version window reachable in hours
2. **Common Delay Scenarios**: 
   - Node restarts during epoch transitions
   - Network congestion causing notification backlogs
   - Resource exhaustion (CPU/memory) slowing processing
   - State sync catching up after downtime
3. **Amplification**: Once one node is affected, it may fall further behind, making the issue worse
4. **Custom Configurations**: Operators using smaller prune windows face higher risk

The issue is especially likely during major network events (governance proposals, large airdrops) that combine high transaction volume with potential node delays.

## Recommendation

**Immediate Fix**: Eagerly fetch all required configs in `read_on_chain_configs()` instead of lazy loading:

```rust
fn read_on_chain_configs(
    &self,
    version: Version,
) -> Result<OnChainConfigPayload<EagerOnChainConfig>, Error> {
    let db_state_view = &self
        .storage
        .read()
        .reader
        .state_view_at_version(Some(version))
        .map_err(|error| {
            Error::UnexpectedErrorEncountered(format!(
                "Failed to create account state view {:?}",
                error
            ))
        })?;
    
    // Fetch epoch immediately
    let epoch = ConfigurationResource::fetch_config(&db_state_view)
        .ok_or_else(|| {
            Error::UnexpectedErrorEncountered("Configuration resource does not exist!".into())
        })?
        .epoch();

    // Eagerly fetch all commonly needed configs
    let configs_snapshot = EagerOnChainConfig::fetch_all(&db_state_view)?;
    
    Ok(OnChainConfigPayload::new(epoch, configs_snapshot))
}
```

**Alternative Fix**: Add version validity checking when creating `DbBackedOnChainConfig`:

```rust
impl DbBackedOnChainConfig {
    pub fn new(reader: Arc<dyn DbReader>, version: Version) -> Result<Self> {
        // Verify version is still readable
        let min_readable = reader.get_min_readable_version()?;
        ensure!(
            version >= min_readable,
            "Version {} has been pruned (min readable: {})",
            version,
            min_readable
        );
        Ok(Self { reader, version })
    }
}
```

**Long-term Fix**: Implement a retention guarantee for recent epoch boundaries, ensuring reconfiguration versions are never pruned for a minimum time period (e.g., 24 hours).

## Proof of Concept

```rust
#[cfg(test)]
mod pruning_race_test {
    use super::*;
    
    #[tokio::test]
    async fn test_reconfiguration_read_after_pruning() {
        // Setup: Create a test node with small prune window
        let mut node_config = NodeConfig::default();
        node_config.storage.ledger_pruner_config.prune_window = 1000; // Small window for testing
        
        let (mut subscription_service, db) = setup_test_environment(node_config);
        
        // Step 1: Create reconfiguration notification at version 1000
        let reconfig_version = 1000;
        subscription_service.notify_initial_configs(reconfig_version).unwrap();
        
        // Step 2: Subscribe and receive notification
        let mut listener = subscription_service.subscribe_to_reconfigurations().unwrap();
        let notification = listener.next().await.unwrap();
        
        // Step 3: Simulate many transactions to trigger pruning
        for v in 1001..3000 {
            db.save_transactions(&[], v, &[], true, None).unwrap();
        }
        db.prune(2000).unwrap(); // Force pruning past version 1000
        
        // Step 4: Try to read config from the notification
        // This should fail with pruning error
        let result = notification.on_chain_configs.get::<ValidatorSet>();
        
        assert!(result.is_err());
        assert!(result.unwrap_err().to_string().contains("pruned"));
    }
}
```

This test demonstrates that a `DbBackedOnChainConfig` created at version 1000 becomes unusable after that version is pruned, even though the config object was validly created and delivered to subscribers.

### Citations

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L281-307)
```rust
    fn read_on_chain_configs(
        &self,
        version: Version,
    ) -> Result<OnChainConfigPayload<DbBackedOnChainConfig>, Error> {
        let db_state_view = &self
            .storage
            .read()
            .reader
            .state_view_at_version(Some(version))
            .map_err(|error| {
                Error::UnexpectedErrorEncountered(format!(
                    "Failed to create account state view {:?}",
                    error
                ))
            })?;
        let epoch = ConfigurationResource::fetch_config(&db_state_view)
            .ok_or_else(|| {
                Error::UnexpectedErrorEncountered("Configuration resource does not exist!".into())
            })?
            .epoch();

        // Return the new on-chain config payload (containing all found configs at this version).
        Ok(OnChainConfigPayload::new(
            epoch,
            DbBackedOnChainConfig::new(self.storage.read().reader.clone(), version),
        ))
    }
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L385-413)
```rust
#[derive(Clone)]
pub struct DbBackedOnChainConfig {
    pub reader: Arc<dyn DbReader>,
    pub version: Version,
}

impl DbBackedOnChainConfig {
    pub fn new(reader: Arc<dyn DbReader>, version: Version) -> Self {
        Self { reader, version }
    }
}

impl OnChainConfigProvider for DbBackedOnChainConfig {
    fn get<T: OnChainConfig>(&self) -> Result<T> {
        let bytes = self
            .reader
            .get_state_value_by_version(&StateKey::on_chain_config::<T>()?, self.version)?
            .ok_or_else(|| {
                anyhow!(
                    "no config {} found in aptos root account state",
                    T::CONFIG_ID
                )
            })?
            .bytes()
            .clone();

        T::deserialize_into_config(&bytes)
    }
}
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L631-642)
```rust
    fn get_state_value_by_version(
        &self,
        state_store_key: &StateKey,
        version: Version,
    ) -> Result<Option<StateValue>> {
        gauged_api("get_state_value_by_version", || {
            self.error_if_state_kv_pruned("StateValue", version)?;

            self.state_store
                .get_state_value_by_version(state_store_key, version)
        })
    }
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L305-315)
```rust
    pub(super) fn error_if_state_kv_pruned(&self, data_type: &str, version: Version) -> Result<()> {
        let min_readable_version = self.state_store.state_kv_pruner.get_min_readable_version();
        ensure!(
            version >= min_readable_version,
            "{} at version {} is pruned, min available version is {}.",
            data_type,
            version,
            min_readable_version
        );
        Ok(())
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_pruner_manager.rs (L128-142)
```rust
    fn set_pruner_target_db_version(&self, latest_version: Version) {
        assert!(self.pruner_worker.is_some());
        let min_readable_version = latest_version.saturating_sub(self.prune_window);
        self.min_readable_version
            .store(min_readable_version, Ordering::SeqCst);

        PRUNER_VERSIONS
            .with_label_values(&["state_kv_pruner", "min_readable"])
            .set(min_readable_version as i64);

        self.pruner_worker
            .as_ref()
            .unwrap()
            .set_target_db_version(min_readable_version);
    }
```

**File:** config/src/config/storage_config.rs (L387-395)
```rust
impl Default for LedgerPrunerConfig {
    fn default() -> Self {
        LedgerPrunerConfig {
            enable: true,
            prune_window: 90_000_000,
            batch_size: 5_000,
            user_pruning_window_offset: 200_000,
        }
    }
```

**File:** consensus/src/consensus_observer/observer/epoch_state.rs (L140-210)
```rust
    // Fetch the next reconfiguration notification
    let reconfig_notification = reconfig_events
        .next()
        .await
        .expect("Failed to get reconfig notification!");

    // Extract the epoch state from the reconfiguration notification
    let on_chain_configs = reconfig_notification.on_chain_configs;
    let validator_set: ValidatorSet = on_chain_configs
        .get()
        .expect("Failed to get the validator set from the on-chain configs!");
    let epoch_state = Arc::new(EpochState::new(
        on_chain_configs.epoch(),
        (&validator_set).into(),
    ));

    // Extract the consensus config (or use the default if it's missing)
    let onchain_consensus_config: anyhow::Result<OnChainConsensusConfig> = on_chain_configs.get();
    if let Err(error) = &onchain_consensus_config {
        error!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "Failed to read on-chain consensus config! Error: {:?}",
                error
            ))
        );
    }
    let consensus_config = onchain_consensus_config.unwrap_or_default();

    // Extract the execution config (or use the default if it's missing)
    let onchain_execution_config: anyhow::Result<OnChainExecutionConfig> = on_chain_configs.get();
    if let Err(error) = &onchain_execution_config {
        error!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "Failed to read on-chain execution config! Error: {:?}",
                error
            ))
        );
    }
    let execution_config =
        onchain_execution_config.unwrap_or_else(|_| OnChainExecutionConfig::default_if_missing());

    // Extract the randomness config sequence number (or use the default if it's missing)
    let onchain_randomness_config_seq_num: anyhow::Result<RandomnessConfigSeqNum> =
        on_chain_configs.get();
    if let Err(error) = &onchain_randomness_config_seq_num {
        warn!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "Failed to read on-chain randomness config seq num! Error: {:?}",
                error
            ))
        );
    }
    let onchain_randomness_config_seq_num = onchain_randomness_config_seq_num
        .unwrap_or_else(|_| RandomnessConfigSeqNum::default_if_missing());

    // Extract the randomness config
    let onchain_randomness_config: anyhow::Result<RandomnessConfigMoveStruct> =
        on_chain_configs.get();
    if let Err(error) = &onchain_randomness_config {
        error!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "Failed to read on-chain randomness config! Error: {:?}",
                error
            ))
        );
    }
    let onchain_randomness_config = OnChainRandomnessConfig::from_configs(
        node_config.randomness_override_seq_num,
        onchain_randomness_config_seq_num.seq_num,
        onchain_randomness_config.ok(),
    );
```
