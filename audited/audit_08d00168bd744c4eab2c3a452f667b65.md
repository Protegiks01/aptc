# Audit Report

## Title
Unbounded Timeout Certificate Storage Allows Resource Exhaustion Attack

## Summary
The `save_highest_2chain_timeout_certificate()` function lacks size validation on the serialized timeout certificate before storage. An attacker can craft a malicious `TwoChainTimeoutCertificate` with an oversized `rounds` vector (up to ~8.4 million entries, ~64 MB) that passes signature verification but causes memory exhaustion during deserialization and storage bloat in ConsensusDB. [1](#0-0) 

## Finding Description

The vulnerability exists in the timeout certificate storage path where no size validation occurs before persisting the serialized certificate to RocksDB. The attack exploits a discrepancy between the `AggregateSignatureWithRounds` constructor validation and BCS deserialization behavior:

**Constructor Validation (bypassed during deserialization):** [2](#0-1) 

**Verification Logic (uses zip, silently truncates excess rounds):** [3](#0-2) 

**Attack Path:**

1. Attacker crafts a malicious `TwoChainTimeoutCertificate`:
   - Valid `TwoChainTimeout` with legitimate epoch, round, and QuorumCert
   - `AggregateSignature` with N valid signers (e.g., 100 validators)
   - `rounds` vector with M entries where M >> N (e.g., 8,388,606 rounds = ~64 MB)
   - First N rounds are valid for signature verification, remaining ~8.4 million are padding

2. Attacker serializes via BCS (total size ~64 MB, within network limit): [4](#0-3) 

3. Attacker sends malicious TC to victim validator via `SyncInfo` message

4. Victim validator receives and deserializes:
   - Network layer accepts (< 64 MiB limit)
   - BCS deserialization allocates Vec<u64> with 8.4 million entries (~64 MB)
   - No constructor validation occurs during deserialization

5. Victim validator verifies the TC: [5](#0-4) 
   
   The verification calls `tc.verify()` which uses `get_voters_and_rounds()`. This function zips signers with rounds, taking only the first N rounds where N = number of signers. If the first N rounds are valid, **verification passes** despite the oversized vector.

6. Victim validator stores full 64 MB TC: [6](#0-5) 

The serialization produces a ~64 MB blob that is stored directly in RocksDB with no size checks.

**Invariant Violation:**
This breaks **Invariant #9: Resource Limits** - all operations must respect gas, storage, and computational limits. The code allows storage of arbitrarily large timeout certificates (up to network message limit) despite the legitimate maximum being only ~521 KB for the maximum validator set size of 65,536. [7](#0-6) 

## Impact Explanation

**Medium Severity** per Aptos Bug Bounty criteria:

1. **Memory Exhaustion**: Each malicious TC deserialization allocates ~64 MB of memory. Multiple messages can cause memory pressure on validator nodes.

2. **Storage Bloat**: ConsensusDB stores the full 64 MB TC per validator. While only the highest-round TC is kept (not accumulated), repeated attacks with incrementing rounds force storage updates.

3. **Network Bandwidth Waste**: Each validator receives and processes ~64 MB messages, consuming bandwidth and processing resources.

4. **Validator Slowdowns**: Deserializing, verifying, and storing 64 MB objects repeatedly impacts validator performance, meeting the "Validator node slowdowns" impact category (High severity), but the limited scope (only highest TC stored) and network-level mitigation (64 MiB hard limit) reduce this to Medium.

5. **State Inconsistencies**: Validators storing oversized TCs have bloated ConsensusDB state requiring manual intervention to clean up.

This does not cause consensus safety violations or fund loss, but creates operational issues requiring intervention, fitting **Medium Severity** criteria.

## Likelihood Explanation

**High Likelihood:**

1. **Low Attacker Requirements**: Any network peer can send `SyncInfo` messages containing timeout certificates. No validator privileges required.

2. **Easy Exploitation**: Attacker needs to:
   - Craft a TC with valid first N rounds for signature verification
   - Pad the rounds vector to maximum size within network limit
   - Send via standard consensus network protocol

3. **No Detection Mechanisms**: The vulnerability is silent - there are no size warnings or validations that would alert operators.

4. **Persistent Impact**: Once stored, the oversized TC remains in ConsensusDB until manually cleaned or replaced by a higher-round TC.

The only limiting factor is the network message size limit (64 MiB), which still allows TCs ~122x larger than the legitimate maximum.

## Recommendation

Add explicit validation of the `rounds` vector size before storage and during deserialization:

**Option 1: Validate in `save_highest_2chain_timeout_certificate()`**

```rust
pub fn save_highest_2chain_timeout_certificate(&self, tc: Vec<u8>) -> Result<(), DbError> {
    // Validate size before storage
    const MAX_TC_SIZE: usize = 1024 * 1024; // 1 MB reasonable limit
    if tc.len() > MAX_TC_SIZE {
        return Err(anyhow::anyhow!(
            "Timeout certificate size {} exceeds maximum {}", 
            tc.len(), 
            MAX_TC_SIZE
        ).into());
    }
    
    let mut batch = SchemaBatch::new();
    batch.put::<SingleEntrySchema>(&SingleEntryKey::Highest2ChainTimeoutCert, &tc)?;
    self.commit(batch)?;
    Ok(())
}
```

**Option 2: Validate in `AggregateSignatureWithRounds` after deserialization**

Add a validation method and call it after BCS deserialization:

```rust
impl AggregateSignatureWithRounds {
    pub fn validate(&self) -> Result<(), VerifyError> {
        if self.sig.get_num_voters() != self.rounds.len() {
            return Err(VerifyError::InvalidAggregatedSignature);
        }
        Ok(())
    }
}
```

Call in `TwoChainTimeoutCertificate::verify()`:
```rust
pub fn verify(&self, validators: &ValidatorVerifier) -> anyhow::Result<()> {
    // Add validation at start
    self.signatures_with_rounds.validate()
        .context("Mismatched rounds vector length")?;
    
    // ... rest of verification
}
```

**Option 3: Use custom Deserialize implementation**

Implement custom `Deserialize` for `AggregateSignatureWithRounds` that calls `new()` constructor to enforce the assertion.

**Recommended: Implement all three layers** for defense-in-depth.

## Proof of Concept

```rust
#[cfg(test)]
mod test_oversized_tc {
    use super::*;
    use aptos_types::validator_verifier::random_validator_verifier;
    use consensus_types::timeout_2chain::{TwoChainTimeout, AggregateSignatureWithRounds};
    
    #[test]
    fn test_oversized_rounds_vector_passes_verification() {
        // Setup validators
        let (signers, validators) = random_validator_verifier(100, None, false);
        
        // Create legitimate timeout with 100 signers
        let timeout = create_valid_timeout(100, &signers, &validators);
        
        // Manually construct AggregateSignatureWithRounds with oversized rounds vector
        let mut malicious_tc_bytes = bcs::to_bytes(&timeout).unwrap();
        
        // Deserialize, modify rounds vector to be huge, re-serialize
        let mut tc: TwoChainTimeoutCertificate = bcs::from_bytes(&malicious_tc_bytes).unwrap();
        
        // Create oversized rounds vector (8 million entries = ~64 MB)
        let mut oversized_rounds = tc.signatures_with_rounds().rounds().clone();
        oversized_rounds.resize(8_000_000, 0u64);
        
        // Manually construct malicious structure
        let malicious_aswr = AggregateSignatureWithRounds {
            sig: tc.signatures_with_rounds().sig().clone(),
            rounds: oversized_rounds,
        };
        
        let malicious_tc = TwoChainTimeoutCertificate {
            timeout: tc.timeout().clone(),
            signatures_with_rounds: malicious_aswr,
        };
        
        // Serialize malicious TC
        let malicious_bytes = bcs::to_bytes(&malicious_tc).unwrap();
        assert!(malicious_bytes.len() > 60_000_000); // ~64 MB
        
        // Deserialize - this allocates huge memory
        let deserialized_tc: TwoChainTimeoutCertificate = 
            bcs::from_bytes(&malicious_bytes).unwrap();
        
        // Verification PASSES because only first 100 rounds are used
        assert!(deserialized_tc.verify(&validators).is_ok());
        
        // Storage would succeed without size validation
        let db = ConsensusDB::new(temp_dir);
        assert!(db.save_highest_2chain_timeout_certificate(malicious_bytes).is_ok());
    }
}
```

**Notes:**
- The vulnerability requires the attacker to craft a TC that passes BCS deserialization with a well-formed but oversized structure
- The zip operation in `get_voters_and_rounds()` silently truncates the rounds vector, causing verification to succeed with only the first N valid rounds
- The maximum validator set size constraint (65,536) means legitimate TCs should never exceed ~521 KB, making anything larger highly suspicious
- Defense-in-depth requires validation at multiple layers: deserialization, verification, and storage

### Citations

**File:** consensus/src/consensusdb/mod.rs (L108-113)
```rust
    pub fn save_highest_2chain_timeout_certificate(&self, tc: Vec<u8>) -> Result<(), DbError> {
        let mut batch = SchemaBatch::new();
        batch.put::<SingleEntrySchema>(&SingleEntryKey::Highest2ChainTimeoutCert, &tc)?;
        self.commit(batch)?;
        Ok(())
    }
```

**File:** consensus/consensus-types/src/timeout_2chain.rs (L360-363)
```rust
    pub fn new(sig: AggregateSignature, rounds: Vec<Round>) -> Self {
        assert_eq!(sig.get_num_voters(), rounds.len());
        Self { sig, rounds }
    }
```

**File:** consensus/consensus-types/src/timeout_2chain.rs (L379-388)
```rust
    pub fn get_voters_and_rounds(
        &self,
        ordered_validator_addresses: &[AccountAddress],
    ) -> Vec<(AccountAddress, Round)> {
        self.sig
            .get_signers_addresses(ordered_validator_addresses)
            .into_iter()
            .zip(self.rounds.clone())
            .collect()
    }
```

**File:** config/src/config/network_config.rs (L50-50)
```rust
pub const MAX_MESSAGE_SIZE: usize = 64 * 1024 * 1024; /* 64 MiB */
```

**File:** consensus/consensus-types/src/sync_info.rs (L205-207)
```rust
                if let Some(tc) = &self.highest_2chain_timeout_cert {
                    tc.verify(validator)?;
                }
```

**File:** consensus/src/persistent_liveness_storage.rs (L598-605)
```rust
    fn save_highest_2chain_timeout_cert(
        &self,
        highest_timeout_cert: &TwoChainTimeoutCertificate,
    ) -> Result<()> {
        Ok(self
            .db
            .save_highest_2chain_timeout_certificate(bcs::to_bytes(highest_timeout_cert)?)?)
    }
```

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L98-100)
```text
    /// Limit the maximum size to u16::max, it's the current limit of the bitvec
    /// https://github.com/aptos-labs/aptos-core/blob/main/crates/aptos-bitvec/src/lib.rs#L20
    const MAX_VALIDATOR_SET_SIZE: u64 = 65536;
```
