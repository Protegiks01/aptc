# Audit Report

## Title
Latency Monitor Task Dies Permanently After Panic, Disabling Sync Issue Detection

## Summary
The latency monitor task in the Aptos data client can permanently terminate after a panic without any restart mechanism, leaving the node without critical sync latency monitoring. This creates operational blind spots where future synchronization issues remain undetected until manual inspection.

## Finding Description

The latency monitor is spawned as an independent tokio task but its `JoinHandle` is immediately discarded, meaning no code monitors the task's health or attempts to restart it upon failure. [1](#0-0) 

The latency monitor contains a `ProgressChecker` that deliberately panics when no sync progress is made for the configured duration (default 24 hours): [2](#0-1) 

This panic is by design to alert operators to critical sync stalls. However, when the panic occurs:

1. The tokio task terminates and the panic is isolated to that task
2. The main node process continues running normally
3. No restart logic exists - the task is gone permanently
4. The latency monitor's infinite loop never resumes: [3](#0-2) 

**Attack Scenario:**

1. A node experiences legitimate network issues or peer problems causing a 24-hour sync stall
2. The `ProgressChecker` panics as designed to alert operators
3. The latency monitor task terminates
4. The node eventually recovers from the network issues and resumes syncing
5. **However**, latency monitoring is now permanently disabled
6. Future sync degradation goes undetected because critical metrics stop updating:
   - `SYNC_LATENCIES` (seen_to_sync, propose_to_seen, propose_to_sync)
   - `BLOCK_TIMESTAMP_LAG_LABEL` [4](#0-3) 

7. Operators relying on these metrics have stale data and won't notice the node is falling behind
8. The `ProgressChecker` safety mechanism itself is now disabled, so even severe 24+ hour stalls won't trigger alerts

## Impact Explanation

This is a **Medium severity** issue per the Aptos bug bounty criteria:

- **State inconsistencies requiring intervention**: The node can fall significantly behind without operators being aware, potentially requiring manual intervention to diagnose and resolve
- **Operational availability impact**: While not total loss of liveness, degraded sync performance can affect the node's ability to participate effectively in consensus or serve API requests
- **Disabled safety mechanism**: The `ProgressChecker` is specifically designed to detect critical sync failures, but once the monitor task dies, this safety net is removed

The vulnerability does not directly cause:
- Consensus violations or safety breaks
- Direct fund loss or theft
- Network-wide liveness failures

However, it creates dangerous operational blind spots where nodes can silently degrade without detection, potentially leading to secondary issues if operators cannot identify and remediate sync problems in time.

## Likelihood Explanation

**Likelihood: Medium to High**

This issue is likely to occur because:

1. **Natural triggers exist**: Network partitions, peer churn, disk I/O issues, or upstream validator problems can all cause 24-hour sync stalls
2. **No special attacker access required**: An attacker causing targeted network disruption against specific nodes (DDoS, BGP manipulation) could trigger the condition
3. **Silent failure mode**: Once the task dies, there's no explicit error signal - metrics just stop updating, which may go unnoticed
4. **Default 24-hour timeout**: The default configuration makes this achievable in realistic network degradation scenarios [5](#0-4) 

The vulnerability becomes critical when combined with:
- Operators primarily using automated monitoring dashboards
- Long periods between manual node health checks
- Subsequent network issues after the initial panic

## Recommendation

Implement a supervised task pattern with automatic restart capability:

```rust
fn start_latency_monitor(
    data_client_config: Arc<AptosDataClientConfig>,
    data_client: AptosDataClient,
    storage: Arc<dyn DbReader>,
    time_service: TimeService,
    runtime: Option<Handle>,
) -> JoinHandle<()> {
    // Spawn a supervisor task that restarts the latency monitor on failure
    let supervisor = async move {
        loop {
            // Create the latency monitor
            let latency_monitor = LatencyMonitor::new(
                data_client_config.clone(),
                Arc::new(data_client.clone()),
                storage.clone(),
                time_service.clone(),
            );

            // Spawn the monitor and await its completion
            let monitor_handle = tokio::spawn(async move {
                latency_monitor.start_latency_monitor().await
            });

            // Wait for the task to complete (via panic or cancellation)
            match monitor_handle.await {
                Ok(_) => {
                    // Normal termination (should never happen as it's an infinite loop)
                    warn!(
                        (LogSchema::new(LogEntry::LatencyMonitor)
                            .message("Latency monitor terminated unexpectedly. Restarting..."))
                    );
                },
                Err(e) if e.is_panic() => {
                    // Task panicked - log and restart
                    error!(
                        (LogSchema::new(LogEntry::LatencyMonitor)
                            .message(&format!(
                                "Latency monitor panicked: {:?}. Restarting in 5 seconds...",
                                e
                            )))
                    );
                    tokio::time::sleep(Duration::from_secs(5)).await;
                },
                Err(e) => {
                    // Task cancelled
                    warn!(
                        (LogSchema::new(LogEntry::LatencyMonitor)
                            .message(&format!(
                                "Latency monitor cancelled: {:?}. Restarting...",
                                e
                            )))
                    );
                },
            }
        }
    };

    // Spawn the supervisor
    if let Some(runtime) = runtime {
        runtime.spawn(supervisor)
    } else {
        tokio::spawn(supervisor)
    }
}
```

Additionally, consider:
1. Adding a health check metric that updates every loop iteration to detect task death
2. Converting the `ProgressChecker` panic to a logged error with metric increment instead
3. Implementing graceful degradation where the monitor logs warnings but continues running

## Proof of Concept

```rust
#[tokio::test]
async fn test_latency_monitor_dies_after_panic() {
    use std::sync::Arc;
    use aptos_time_service::TimeService;
    use aptos_config::config::AptosDataClientConfig;
    use crate::tests::mock::{create_mock_data_client, create_mock_db_reader};
    
    // Create a latency monitor with very short stall timeout (1 second)
    let mut data_client_config = AptosDataClientConfig::default();
    data_client_config.progress_check_max_stall_time_secs = 1;
    let data_client_config = Arc::new(data_client_config);
    
    let data_client = create_mock_data_client();
    let storage = create_mock_db_reader();
    let time_service = TimeService::mock();
    
    // Spawn the latency monitor
    let join_handle = tokio::spawn(async move {
        let latency_monitor = LatencyMonitor::new(
            data_client_config,
            Arc::new(data_client),
            storage,
            time_service.clone(),
        );
        latency_monitor.start_latency_monitor().await
    });
    
    // Wait for 2 seconds - the monitor should panic due to no progress
    tokio::time::sleep(Duration::from_secs(2)).await;
    
    // Check if the task has terminated
    let result = tokio::time::timeout(Duration::from_millis(100), join_handle).await;
    
    match result {
        Ok(Ok(_)) => panic!("Task should not complete normally"),
        Ok(Err(e)) if e.is_panic() => {
            // Expected: task panicked
            println!("âœ“ Latency monitor panicked as expected");
        },
        Ok(Err(e)) => panic!("Unexpected error: {:?}", e),
        Err(_) => panic!("Task still running - should have panicked"),
    }
    
    // The vulnerability: no restart mechanism exists
    // In production, the task is now permanently dead
    // Future sync issues will not be detected
}
```

**Notes:**

This vulnerability represents a gap between the intended safety mechanism (panic on critical sync failure) and the actual operational behavior (permanent monitoring loss). The `ProgressChecker` is designed to catch severe sync problems, but the lack of task supervision means this safety mechanism paradoxically creates a new vulnerability where subsequent issues go undetected.

### Citations

**File:** state-sync/aptos-data-client/src/poller.rs (L468-490)
```rust
/// Spawns the dedicated latency monitor
fn start_latency_monitor(
    data_client_config: Arc<AptosDataClientConfig>,
    data_client: AptosDataClient,
    storage: Arc<dyn DbReader>,
    time_service: TimeService,
    runtime: Option<Handle>,
) -> JoinHandle<()> {
    // Create the latency monitor
    let latency_monitor = LatencyMonitor::new(
        data_client_config,
        Arc::new(data_client),
        storage,
        time_service,
    );

    // Spawn the latency monitor
    if let Some(runtime) = runtime {
        runtime.spawn(async move { latency_monitor.start_latency_monitor().await })
    } else {
        tokio::spawn(async move { latency_monitor.start_latency_monitor().await })
    }
}
```

**File:** state-sync/aptos-data-client/src/latency_monitor.rs (L78-149)
```rust
        loop {
            // Wait for the next round
            loop_ticker.next().await;

            // Get the highest synced version from storage
            let highest_synced_version = match self.storage.ensure_synced_version() {
                Ok(version) => version,
                Err(error) => {
                    sample!(
                        SampleRate::Duration(Duration::from_secs(LATENCY_MONITOR_LOG_FREQ_SECS)),
                        warn!(
                            (LogSchema::new(LogEntry::LatencyMonitor)
                                .event(LogEvent::StorageReadFailed)
                                .message(&format!("Unable to read the highest synced version: {:?}", error)))
                        );
                    );
                    continue; // Continue to the next round
                },
            };

            // Check if we've made sufficient progress since the last loop iteration
            progress_checker.check_syncing_progress(highest_synced_version);

            // Get the latest block timestamp from storage
            let latest_block_timestamp_usecs = match self
                .storage
                .get_block_timestamp(highest_synced_version)
            {
                Ok(block_timestamp_usecs) => block_timestamp_usecs,
                Err(error) => {
                    sample!(
                        SampleRate::Duration(Duration::from_secs(LATENCY_MONITOR_LOG_FREQ_SECS)),
                        warn!(
                            (LogSchema::new(LogEntry::LatencyMonitor)
                                .event(LogEvent::StorageReadFailed)
                                .message(&format!("Unable to read the latest block timestamp: {:?}", error)))
                        );
                    );
                    continue; // Continue to the next round
                },
            };

            // Update the block timestamp lag
            self.update_block_timestamp_lag(latest_block_timestamp_usecs);

            // Update the latency metrics for all versions that we've now synced
            self.update_latency_metrics(highest_synced_version);

            // Get the highest advertised version from the global data summary
            let advertised_data = &self.data_client.get_global_data_summary().advertised_data;
            let highest_advertised_version = match advertised_data.highest_synced_ledger_info() {
                Some(ledger_info) => ledger_info.ledger_info().version(),
                None => {
                    sample!(
                        SampleRate::Duration(Duration::from_secs(LATENCY_MONITOR_LOG_FREQ_SECS)),
                        warn!(
                            (LogSchema::new(LogEntry::LatencyMonitor)
                                .event(LogEvent::AggregateSummary)
                                .message("Unable to get the highest advertised version!"))
                        );
                    );
                    continue; // Continue to the next round
                },
            };

            // Update the advertised version timestamps
            self.update_advertised_version_timestamps(
                highest_synced_version,
                highest_advertised_version,
            );
        }
    }
```

**File:** state-sync/aptos-data-client/src/latency_monitor.rs (L151-233)
```rust
    /// Updates the block timestamp lag metric (i.e., the difference between
    /// the latest block timestamp and the current time).
    fn update_block_timestamp_lag(&self, latest_block_timestamp_usecs: u64) {
        // Get the current time (in microseconds)
        let timestamp_now_usecs = self.get_timestamp_now_usecs();

        // Calculate the block timestamp lag (saturating at 0)
        let timestamp_lag_usecs = timestamp_now_usecs.saturating_sub(latest_block_timestamp_usecs);
        let timestamp_lag_duration = Duration::from_micros(timestamp_lag_usecs);

        // Update the block timestamp lag metric
        metrics::observe_value_with_label(
            &metrics::SYNC_LATENCIES,
            metrics::BLOCK_TIMESTAMP_LAG_LABEL,
            timestamp_lag_duration.as_secs_f64(),
        );
    }

    /// Updates the latency metrics for all versions that have now been synced
    fn update_latency_metrics(&mut self, highest_synced_version: u64) {
        // Split the advertised versions into synced and unsynced versions
        let unsynced_advertised_versions = self
            .advertised_versions
            .split_off(&(highest_synced_version + 1));

        // Update the metrics for all synced versions
        for (synced_version, advertised_version_metadata) in self.advertised_versions.iter() {
            // Update the seen to synced latencies
            let duration_from_seen_to_synced = calculate_duration_from_seen_to_synced(
                advertised_version_metadata,
                self.time_service.clone(),
            );
            metrics::observe_value_with_label(
                &metrics::SYNC_LATENCIES,
                metrics::SEEN_TO_SYNC_LATENCY_LABEL,
                duration_from_seen_to_synced.as_secs_f64(),
            );

            // Update the proposal latencies
            match self.storage.get_block_timestamp(*synced_version) {
                Ok(block_timestamp_usecs) => {
                    // Update the propose to seen latencies
                    let seen_timestamp_usecs = advertised_version_metadata.seen_timestamp_usecs;
                    if let Some(duration_from_propose_to_seen) = calculate_duration_from_proposal(
                        block_timestamp_usecs,
                        seen_timestamp_usecs,
                    ) {
                        metrics::observe_value_with_label(
                            &metrics::SYNC_LATENCIES,
                            metrics::PROPOSE_TO_SEEN_LATENCY_LABEL,
                            duration_from_propose_to_seen.as_secs_f64(),
                        );
                    }

                    // Update the propose to synced latencies
                    let timestamp_now_usecs = self.get_timestamp_now_usecs();
                    if let Some(duration_from_propose_to_sync) =
                        calculate_duration_from_proposal(block_timestamp_usecs, timestamp_now_usecs)
                    {
                        metrics::observe_value_with_label(
                            &metrics::SYNC_LATENCIES,
                            metrics::PROPOSE_TO_SYNC_LATENCY_LABEL,
                            duration_from_propose_to_sync.as_secs_f64(),
                        );
                    }
                },
                Err(error) => {
                    sample!(
                        SampleRate::Duration(Duration::from_secs(LATENCY_MONITOR_LOG_FREQ_SECS)),
                        warn!(
                            (LogSchema::new(LogEntry::LatencyMonitor)
                                .event(LogEvent::StorageReadFailed)
                                .message(&format!("Unable to read the block timestamp for version {}: {:?}", synced_version, error)))
                        );
                    );
                },
            }
        }

        // Update the advertised versions with those we still need to sync
        self.advertised_versions = unsynced_advertised_versions;
    }

```

**File:** state-sync/aptos-data-client/src/latency_monitor.rs (L314-333)
```rust
    fn check_syncing_progress(&mut self, highest_synced_version: u64) {
        // Check if we've made progress since the last iteration
        let time_now = self.time_service.now();
        if highest_synced_version > self.highest_synced_version {
            // We've made progress, so reset the progress state
            self.last_sync_progress_time = time_now;
            self.highest_synced_version = highest_synced_version;
            return;
        }

        // Otherwise, check if we've stalled for too long
        let elapsed_time = time_now.duration_since(self.last_sync_progress_time);
        if elapsed_time >= self.progress_check_max_stall_duration {
            panic!(
                "No syncing progress has been made for {:?}! Highest synced version: {}. \
                We recommend restarting the node and checking if the issue persists.",
                elapsed_time, highest_synced_version
            );
        }
    }
```

**File:** config/src/config/state_sync_config.rs (L479-479)
```rust
            progress_check_max_stall_time_secs: 86400, // 24 hours (long enough to debug any issues at runtime)
```
