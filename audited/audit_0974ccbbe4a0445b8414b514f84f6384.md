# Audit Report

## Title
Clock Skew-Based Request Validation Inconsistency Causing State Sync Network Fragmentation

## Summary
Nodes use their local system time (via `SystemTime::now()`) to validate subscription and optimistic fetch requests in the state sync service. When nodes have clock skew exceeding the configured lag thresholds (default 20 seconds), they will inconsistently accept or reject identical requests, potentially fragmenting the state sync network and isolating nodes with misconfigured clocks.

## Finding Description

The state sync service validates subscription and optimistic fetch requests by checking if the synced ledger info timestamp is within an acceptable lag threshold of the node's current system time. This validation occurs in the request moderator before processing any state sync requests. [1](#0-0) 

The moderator calls the storage server summary's `can_service()` method, which evaluates different request types: [2](#0-1) 

For subscription and optimistic fetch requests, the validation delegates to helper functions that check ledger lag: [3](#0-2) [4](#0-3) 

The critical vulnerability lies in the `check_synced_ledger_lag()` function, which compares the ledger timestamp against the node's current system time: [5](#0-4) 

The `time_service.now_unix_time()` method uses `SystemTime::now()` which is **not monotonic** and varies based on each node's system clock: [6](#0-5) [7](#0-6) 

The default configuration sets both lag thresholds to 20 seconds: [8](#0-7) 

**Attack Scenario:**

Consider three nodes with clock skew:
- **Node A**: Clock is 25 seconds fast (ahead of real time)
- **Node B**: Clock is correct (synchronized)  
- **Node C**: Clock is 10 seconds slow (behind real time)

All three nodes have the same synced ledger info with timestamp T (in microseconds). A client attempts to subscribe to all three nodes at real time T+15 seconds.

**Node A's validation** (fast clock):
- Ledger timestamp: T
- Current time: T + 40 seconds (15 real + 25 skew)
- Lag check: T + 20 seconds > T + 40? **FALSE** → **REJECTS** with `InvalidRequest`

**Node B's validation** (correct clock):
- Ledger timestamp: T  
- Current time: T + 15 seconds
- Lag check: T + 20 seconds > T + 15? **TRUE** → **ACCEPTS**

**Node C's validation** (slow clock):
- Ledger timestamp: T
- Current time: T + 5 seconds (15 real - 10 skew)
- Lag check: T + 20 seconds > T + 5? **TRUE** → **ACCEPTS**

When Node A rejects the request, the moderator increments the invalid request count for the peer. After exceeding the threshold, the peer is temporarily ignored: [9](#0-8) 

This creates a feedback loop where nodes with fast clocks reject legitimate subscription requests, mark requesting peers as "unhealthy," and eventually ignore them entirely. The inverse problem affects nodes with slow clocks—they may accept subscriptions from peers whose data is actually too stale.

**System Design Contradiction:**

The Aptos consensus protocol is explicitly designed to work without requiring synchronized clocks: [10](#0-9) 

However, the state sync validation implicitly requires clock synchronization, creating an architectural inconsistency. Nodes can participate in consensus but become isolated from state sync due to clock skew.

## Impact Explanation

**Severity: Medium**

This vulnerability meets the Medium severity criteria for "State inconsistencies requiring intervention" per the Aptos bug bounty program. While it does not directly compromise consensus safety (which operates independently of clock synchronization), it can cause:

1. **State Sync Network Fragmentation**: Nodes with clock skew form isolated groups that cannot effectively sync state via subscriptions. Nodes with fast clocks reject most subscription requests, while nodes with slow clocks accept stale data.

2. **Peer Isolation**: Nodes with significantly misconfigured clocks (>20 seconds deviation) may be unable to establish subscription streams with the majority of the network, forcing them to rely on less efficient synchronization methods or falling behind.

3. **Cascading Failures**: As nodes mark each other as "unhealthy" due to clock-skew-induced validation failures, the peer scoring system penalizes legitimate nodes, further degrading network connectivity.

4. **Operational Disruption**: Node operators must manually intervene to identify clock synchronization issues, as the symptoms (rejected subscription requests) don't clearly indicate the root cause.

The vulnerability does **not** affect:
- Consensus safety or liveness (consensus is clock-independent)
- Fund security or transaction execution
- On-chain governance or staking mechanisms

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability will manifest when:
1. Nodes have clock skew exceeding 20 seconds (the default `max_subscription_lag_secs` and `max_optimistic_fetch_lag_secs`)
2. Nodes attempt to establish subscription streams for state sync

**Factors increasing likelihood:**
- Deployments without proper NTP configuration
- Virtual machines or containers with clock drift
- Geographically distributed validators with inconsistent time sources
- System administrators manually adjusting clocks during debugging

**Factors decreasing likelihood:**
- Most production deployments use NTP, keeping clocks within 10-100ms typically
- The 20-second threshold provides reasonable tolerance for normal clock drift
- The Aptos team's awareness of clock skew (evidenced by consensus design) suggests operational guidance likely recommends proper time synchronization

**Real-world occurrence**: While severe 20+ second clock skew is uncommon in well-maintained infrastructure, it's not rare in:
- Development/testing environments
- Misconfigured cloud instances
- Networks experiencing NTP service disruptions
- Validator nodes recovering from system failures

The test suite explicitly validates this behavior: [11](#0-10) 

## Recommendation

**Solution 1: Use Monotonic Clock or Relative Timestamps (Preferred)**

Replace the absolute time comparison with a monotonic clock-based approach or relative timestamp validation that doesn't depend on wall clock synchronization:

```rust
// In check_synced_ledger_lag, track when the ledger info was received
// rather than comparing its timestamp to system time
fn check_synced_ledger_lag(
    synced_ledger_info: Option<&LedgerInfoWithSignatures>,
    synced_ledger_received_at: Option<Instant>, // New parameter
    time_service: TimeService,
    max_lag_secs: u64,
) -> bool {
    if let (Some(synced_ledger_info), Some(received_at)) = 
        (synced_ledger_info, synced_ledger_received_at) {
        
        // Check how long ago we received this ledger info
        let elapsed = time_service.now().duration_since(received_at);
        elapsed.as_secs() <= max_lag_secs
    } else {
        false
    }
}
```

This approach uses `time_service.now()` which returns an `Instant` (monotonic clock) rather than `now_unix_time()` (wall clock).

**Solution 2: Increase Lag Threshold**

Increase the default lag thresholds to better tolerate expected clock skew:

```rust
// In config/src/config/state_sync_config.rs
impl Default for AptosDataClientConfig {
    fn default() -> Self {
        Self {
            // ... other fields ...
            max_optimistic_fetch_lag_secs: 60, // Increased from 20
            max_subscription_lag_secs: 60,      // Increased from 20
            // ... other fields ...
        }
    }
}
```

**Solution 3: Add Clock Skew Detection and Warnings**

Implement active monitoring to detect and alert on clock skew before it causes validation failures:

```rust
// Compare local time against consensus timestamps
fn detect_clock_skew(&self, ledger_info: &LedgerInfoWithSignatures) -> Option<Duration> {
    let ledger_timestamp = Duration::from_micros(
        ledger_info.ledger_info().timestamp_usecs()
    );
    let local_timestamp = self.time_service.now_unix_time();
    
    // Calculate absolute difference
    if local_timestamp > ledger_timestamp {
        Some(local_timestamp - ledger_timestamp)
    } else {
        Some(ledger_timestamp - local_timestamp)
    }
}
```

Log warnings when skew exceeds safe thresholds, allowing operators to proactively address clock synchronization issues.

**Recommended Fix**: Implement Solution 1 (monotonic clock) as it eliminates the root cause by removing dependency on wall clock synchronization. This aligns with the consensus protocol's design philosophy of not requiring synchronized clocks.

## Proof of Concept

**Rust Test Case** (add to `state-sync/storage-service/types/src/tests.rs`):

```rust
#[test]
fn test_clock_skew_causes_validation_inconsistency() {
    use aptos_time_service::TimeService;
    use aptos_config::config::AptosDataClientConfig;
    
    // Create data summary with synced ledger at timestamp T
    let base_timestamp_usecs = 1_000_000_000_000; // T = 1M seconds
    let data_summary = create_data_summary_with_timestamp(base_timestamp_usecs);
    let data_client_config = AptosDataClientConfig::default();
    
    // Node A: Fast clock (25 seconds ahead)
    let time_service_fast = TimeService::mock();
    time_service_fast.clone().into_mock().set_now_unix_time(
        Duration::from_micros(base_timestamp_usecs + 25_000_000)
    );
    
    // Node B: Correct clock
    let time_service_correct = TimeService::mock();
    time_service_correct.clone().into_mock().set_now_unix_time(
        Duration::from_micros(base_timestamp_usecs)
    );
    
    // Create identical subscription request
    let subscription_request = create_subscription_request(0, true);
    
    // Node A rejects (clock ahead by 25 sec > 20 sec threshold)
    let can_service_fast = data_summary.can_service(
        &data_client_config,
        time_service_fast,
        &subscription_request,
    );
    assert!(!can_service_fast, "Node with fast clock should reject");
    
    // Node B accepts (clock correct, within threshold)
    let can_service_correct = data_summary.can_service(
        &data_client_config,
        time_service_correct,
        &subscription_request,
    );
    assert!(can_service_correct, "Node with correct clock should accept");
    
    // Same request, different validation results → Network fragmentation
}
```

**Reproduction Steps** (manual testing):

1. Deploy two Aptos validator nodes with identical configurations
2. On Node 1, manually advance the system clock by 30 seconds: `sudo date -s "+30 seconds"`
3. From a client, attempt to establish subscription streams to both nodes
4. Observe that Node 1 rejects subscription requests with `InvalidRequest` errors
5. Monitor Node 1's logs for "Ignoring peer due to too many invalid requests" warnings
6. Observe that the client can successfully subscribe to Node 2 but not Node 1
7. Verify state sync network metrics show Node 1 with fewer active subscriptions

This demonstrates how clock skew causes identical requests to be handled inconsistently, leading to state sync network fragmentation.

---

**Notes:**

This vulnerability represents a subtle architectural mismatch: the consensus layer explicitly avoids requiring clock synchronization, but the state sync layer implicitly depends on it. While the 20-second threshold provides reasonable tolerance for well-configured systems, it creates an operational footgun for deployments with clock synchronization issues. The vulnerability is exacerbated by the peer scoring system, which can permanently isolate nodes with clock skew by marking them as "unhealthy" after repeated validation failures.

### Citations

**File:** state-sync/storage-service/server/src/moderator.rs (L134-196)
```rust
    pub fn validate_request(
        &self,
        peer_network_id: &PeerNetworkId,
        request: &StorageServiceRequest,
    ) -> Result<(), Error> {
        // Validate the request and time the operation
        let validate_request = || {
            // If the peer is being ignored, return an error
            if let Some(peer_state) = self.unhealthy_peer_states.get(peer_network_id) {
                if peer_state.is_ignored() {
                    return Err(Error::TooManyInvalidRequests(format!(
                        "Peer is temporarily ignored. Unable to handle request: {:?}",
                        request
                    )));
                }
            }

            // Get the latest storage server summary
            let storage_server_summary = self.cached_storage_server_summary.load();

            // Verify the request is serviceable using the current storage server summary
            if !storage_server_summary.can_service(
                &self.aptos_data_client_config,
                self.time_service.clone(),
                request,
            ) {
                // Increment the invalid request count for the peer
                let mut unhealthy_peer_state = self
                    .unhealthy_peer_states
                    .entry(*peer_network_id)
                    .or_insert_with(|| {
                        // Create a new unhealthy peer state (this is the first invalid request)
                        let max_invalid_requests =
                            self.storage_service_config.max_invalid_requests_per_peer;
                        let min_time_to_ignore_peers_secs =
                            self.storage_service_config.min_time_to_ignore_peers_secs;
                        let time_service = self.time_service.clone();

                        UnhealthyPeerState::new(
                            max_invalid_requests,
                            min_time_to_ignore_peers_secs,
                            time_service,
                        )
                    });
                unhealthy_peer_state.increment_invalid_request_count(peer_network_id);

                // Return the validation error
                return Err(Error::InvalidRequest(format!(
                    "The given request cannot be satisfied. Request: {:?}, storage summary: {:?}",
                    request, storage_server_summary
                )));
            }

            Ok(()) // The request is valid
        };
        utils::execute_and_time_duration(
            &metrics::STORAGE_REQUEST_VALIDATION_LATENCY,
            Some((peer_network_id, request)),
            None,
            validate_request,
            None,
        )
    }
```

**File:** state-sync/storage-service/types/src/responses.rs (L621-631)
```rust
    pub fn can_service(
        &self,
        aptos_data_client_config: &AptosDataClientConfig,
        time_service: TimeService,
        request: &StorageServiceRequest,
    ) -> bool {
        self.protocol_metadata.can_service(request)
            && self
                .data_summary
                .can_service(aptos_data_client_config, time_service, request)
    }
```

**File:** state-sync/storage-service/types/src/responses.rs (L760-774)
```rust
            SubscribeTransactionOutputsWithProof(_) => can_service_subscription_request(
                aptos_data_client_config,
                time_service,
                self.synced_ledger_info.as_ref(),
            ),
            SubscribeTransactionsOrOutputsWithProof(_) => can_service_subscription_request(
                aptos_data_client_config,
                time_service,
                self.synced_ledger_info.as_ref(),
            ),
            SubscribeTransactionsWithProof(_) => can_service_subscription_request(
                aptos_data_client_config,
                time_service,
                self.synced_ledger_info.as_ref(),
            ),
```

**File:** state-sync/storage-service/types/src/responses.rs (L894-912)
```rust
fn can_service_optimistic_request(
    aptos_data_client_config: &AptosDataClientConfig,
    time_service: TimeService,
    synced_ledger_info: Option<&LedgerInfoWithSignatures>,
) -> bool {
    let max_lag_secs = aptos_data_client_config.max_optimistic_fetch_lag_secs;
    check_synced_ledger_lag(synced_ledger_info, time_service, max_lag_secs)
}

/// Returns true iff a subscription data request can be serviced
/// by the peer with the given synced ledger info.
fn can_service_subscription_request(
    aptos_data_client_config: &AptosDataClientConfig,
    time_service: TimeService,
    synced_ledger_info: Option<&LedgerInfoWithSignatures>,
) -> bool {
    let max_lag_secs = aptos_data_client_config.max_subscription_lag_secs;
    check_synced_ledger_lag(synced_ledger_info, time_service, max_lag_secs)
}
```

**File:** state-sync/storage-service/types/src/responses.rs (L916-934)
```rust
fn check_synced_ledger_lag(
    synced_ledger_info: Option<&LedgerInfoWithSignatures>,
    time_service: TimeService,
    max_lag_secs: u64,
) -> bool {
    if let Some(synced_ledger_info) = synced_ledger_info {
        // Get the ledger info timestamp (in microseconds)
        let ledger_info_timestamp_usecs = synced_ledger_info.ledger_info().timestamp_usecs();

        // Get the current timestamp and max version lag (in microseconds)
        let current_timestamp_usecs = time_service.now_unix_time().as_micros() as u64;
        let max_version_lag_usecs = max_lag_secs * NUM_MICROSECONDS_IN_SECOND;

        // Return true iff the synced ledger info timestamp is within the max version lag
        ledger_info_timestamp_usecs + max_version_lag_usecs > current_timestamp_usecs
    } else {
        false // No synced ledger info was found!
    }
}
```

**File:** crates/aptos-time-service/src/real.rs (L35-37)
```rust
    fn now_unix_time(&self) -> Duration {
        aptos_infallible::duration_since_epoch()
    }
```

**File:** crates/aptos-infallible/src/time.rs (L8-13)
```rust
/// Gives the duration since the Unix epoch, notice the expect.
pub fn duration_since_epoch() -> Duration {
    SystemTime::now()
        .duration_since(SystemTime::UNIX_EPOCH)
        .expect("System time is before the UNIX_EPOCH")
}
```

**File:** config/src/config/state_sync_config.rs (L471-475)
```rust
            max_optimistic_fetch_lag_secs: 20, // 20 seconds
            max_response_bytes: CLIENT_MAX_MESSAGE_SIZE_V2 as u64,
            max_response_timeout_ms: 60_000, // 60 seconds
            max_state_chunk_size: MAX_STATE_CHUNK_SIZE,
            max_subscription_lag_secs: 20, // 20 seconds
```

**File:** consensus/README.md (L33-35)
```markdown
### Extensions and Modifications

We reformulate the safety conditions and provide extended proofs of safety, liveness, and optimistic responsiveness. We also implement a number of additional features. First, we make the protocol more resistant to non-determinism bugs, by having validators collectively sign the resulting state of a block rather than just the sequence of transactions. This also allows clients to use quorum certificates to authenticate reads from the database. Second, we design a round_state that emits explicit timeouts, and validators rely on a quorum of those to move to the next round — without requiring synchronized clocks. Third, we intend to design an unpredictable leader election mechanism in which the leader of a round is determined by the proposer of the latest committed block using a verifiable rand ... (truncated)
```

**File:** state-sync/storage-service/types/src/tests.rs (L205-224)
```rust
        // Elapse the time service by the max subscription lag
        time_service
            .clone()
            .into_mock()
            .advance_secs(max_subscription_lag_secs);

        // Verify that subscription requests can no longer be serviced
        // (as the max lag has been exceeded for the given data summary).
        for compression in [true, false] {
            let known_versions = vec![0, 1, highest_synced_version, highest_synced_version * 2];
            verify_can_service_subscription_requests(
                use_request_v2,
                &data_client_config,
                &data_summary,
                time_service.clone(),
                compression,
                known_versions,
                false,
            );
        }
```
