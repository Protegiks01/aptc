# Audit Report

## Title
DropHelper Blocking Behavior Causes Validator Timeouts During Consensus Block Pruning

## Summary
The `DropHelper` implementation uses a bounded async drop queue with a capacity of 32 concurrent drops. When blocks are pruned during consensus operations, dropping `LedgerUpdateOutput` instances can block synchronously if the queue is full, causing validator timeouts and missed consensus rounds.

## Finding Description

The `LedgerUpdateOutput` struct wraps its `Inner` data in an `Arc<DropHelper<Inner>>`. [1](#0-0) 

When `DropHelper` is dropped, it schedules the wrapped value for asynchronous cleanup via `DEFAULT_DROPPER.schedule_drop()`. [2](#0-1) 

The `DEFAULT_DROPPER` has a maximum capacity of 32 concurrent drops. [3](#0-2) 

When the queue is full, `schedule_drop` blocks the calling thread waiting for capacity by calling `num_tasks_tracker.inc()`, which synchronously waits on a condition variable. [4](#0-3) 

During consensus operations, blocks are pruned in batches via `BlockTree::process_pruned_blocks()`, which can remove multiple blocks when `max_pruned_blocks_in_mem` (default 100) is exceeded. [5](#0-4) 

Each removed block triggers the drop of its `StateComputeResult`, which contains a `LedgerUpdateOutput`. [6](#0-5) 

When `StateComputeResult` is replaced during execution, the old value is dropped synchronously while holding locks. [7](#0-6) 

**Attack Scenario:**
1. During state sync catch-up, reorg, or high-throughput periods, consensus rapidly produces and prunes blocks
2. If 33+ `LedgerUpdateOutput` instances need to be dropped before the async dropper threads process them
3. The 33rd drop blocks on the consensus thread executing `process_pruned_blocks()` or `set_compute_result()`
4. This blocking causes the validator to miss round deadlines and timeout
5. Multiple validators experiencing this simultaneously degrades consensus liveness

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty criteria: "Validator node slowdowns". The blocking can occur during critical consensus phases, causing validators to miss rounds and reducing network throughput. While it doesn't cause consensus safety violations or fund loss, it directly impacts consensus liveness and validator performance.

In extreme cases with sustained high block production rates, this could contribute to consensus stalls if multiple validators timeout simultaneously.

## Likelihood Explanation

**Likelihood: Medium-to-High**

This issue occurs naturally during:
- **State sync**: Validators catching up prune many historical blocks
- **Chain reorganizations**: Alternative chains are pruned after finality
- **High throughput**: Fast block production with large execution results
- **Memory pressure**: Triggered garbage collection of old blocks

The default configuration (32 max concurrent drops, 100 max pruned blocks) makes this feasible. The `Inner` struct contains `Vec<TransactionInfo>` and `Vec<HashValue>` which can be large for blocks with many transactions, making individual drops slower and increasing queue saturation probability.

## Recommendation

**Option 1: Increase Drop Queue Capacity (Immediate Fix)**
Increase `DEFAULT_DROPPER` max_tasks from 32 to a higher value (e.g., 256) to reduce blocking probability: [3](#0-2) 

Change to:
```rust
pub static DEFAULT_DROPPER: Lazy<AsyncConcurrentDropper> =
    Lazy::new(|| AsyncConcurrentDropper::new("default", 256, 16));
```

**Option 2: Non-Blocking Drop for Consensus-Critical Structures (Better Fix)**
Create a separate unbounded drop queue specifically for consensus-critical structures like `LedgerUpdateOutput`, or use a try-schedule approach that logs warnings instead of blocking when capacity is exceeded.

**Option 3: Eager Cleanup (Best Fix)**
Pre-emptively clear large vectors in `Inner` before dropping by calling `into_inner()` and dropping fields individually, reducing individual drop times and queue saturation.

## Proof of Concept

```rust
use aptos_drop_helper::DropHelper;
use std::sync::Arc;
use std::time::Instant;

#[test]
fn test_drop_blocking() {
    // Simulate large Inner structs
    struct LargeInner {
        data: Vec<Vec<u8>>,
    }
    
    impl LargeInner {
        fn new() -> Self {
            Self {
                data: vec![vec![0u8; 1024]; 1000], // 1MB per instance
            }
        }
    }
    
    // Create 40 instances wrapped in DropHelper
    let instances: Vec<_> = (0..40)
        .map(|_| Arc::new(DropHelper::new(LargeInner::new())))
        .collect();
    
    // Drop all at once
    let start = Instant::now();
    drop(instances);
    let elapsed = start.elapsed();
    
    // If blocking occurs, this will take significantly longer
    // than if all drops were truly async
    println!("Drop time: {:?}", elapsed);
    assert!(elapsed.as_secs() < 5, "Drop blocked for too long");
}
```

To test in consensus context:
1. Configure a local testnet with rapid block production
2. Monitor drop queue metrics: `aptos_drop_helper_num_tasks`
3. Trigger state sync or block pruning during high throughput
4. Observe validator round timeouts correlating with queue saturation

## Notes

The blocking behavior is documented in the code comments [8](#0-7) , indicating developers are aware of the general blocking risk. However, the specific security implications during consensus-critical operations (block pruning, state result updates) may not have been fully considered. The default capacity of 32 is relatively small given that `max_pruned_blocks_in_mem` is 100 [9](#0-8) , creating a capacity mismatch that increases blocking likelihood.

### Citations

**File:** execution/executor-types/src/ledger_update_output.rs (L17-21)
```rust
#[derive(Clone, Debug, Default, Deref)]
pub struct LedgerUpdateOutput {
    #[deref]
    inner: Arc<DropHelper<Inner>>,
}
```

**File:** crates/aptos-drop-helper/src/lib.rs (L19-20)
```rust
pub static DEFAULT_DROPPER: Lazy<AsyncConcurrentDropper> =
    Lazy::new(|| AsyncConcurrentDropper::new("default", 32, 8));
```

**File:** crates/aptos-drop-helper/src/lib.rs (L51-55)
```rust
impl<T: Send + 'static> Drop for DropHelper<T> {
    fn drop(&mut self) {
        DEFAULT_DROPPER.schedule_drop(self.inner.take());
    }
}
```

**File:** crates/aptos-drop-helper/src/async_concurrent_dropper.rs (L16-21)
```rust
/// A helper to send things to a thread pool for asynchronous dropping.
///
/// Be aware that there is a bounded number of concurrent drops, as a result:
///   1. when it's "out of capacity", `schedule_drop` will block until a slot to be available.
///   2. if the `Drop` implementation tries to lock things, there can be a potential deadlock due
///      to another thing being waiting for a slot to be available.
```

**File:** crates/aptos-drop-helper/src/async_concurrent_dropper.rs (L112-119)
```rust
    fn inc(&self) {
        let mut num_tasks = self.lock.lock();
        while *num_tasks >= self.max_tasks {
            num_tasks = self.cvar.wait(num_tasks).expect("lock poisoned.");
        }
        *num_tasks += 1;
        GAUGE.set_with(&[self.name, "num_tasks"], *num_tasks as i64);
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L496-510)
```rust
    pub(super) fn process_pruned_blocks(&mut self, mut newly_pruned_blocks: VecDeque<HashValue>) {
        counters::NUM_BLOCKS_IN_TREE.sub(newly_pruned_blocks.len() as i64);
        // The newly pruned blocks are pushed back to the deque pruned_block_ids.
        // In case the overall number of the elements is greater than the predefined threshold,
        // the oldest elements (in the front of the deque) are removed from the tree.
        self.pruned_block_ids.append(&mut newly_pruned_blocks);
        if self.pruned_block_ids.len() > self.max_pruned_blocks_in_mem {
            let num_blocks_to_remove = self.pruned_block_ids.len() - self.max_pruned_blocks_in_mem;
            for _ in 0..num_blocks_to_remove {
                if let Some(id) = self.pruned_block_ids.pop_front() {
                    self.remove_block(id);
                }
            }
        }
    }
```

**File:** execution/executor-types/src/state_compute_result.rs (L29-34)
```rust
#[derive(Clone, Debug)]
pub struct StateComputeResult {
    pub execution_output: ExecutionOutput,
    pub state_checkpoint_output: StateCheckpointOutput,
    pub ledger_update_output: LedgerUpdateOutput,
}
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L277-330)
```rust
    pub fn set_compute_result(
        &self,
        state_compute_result: StateComputeResult,
        execution_time: Duration,
    ) {
        let mut to_commit = 0;
        let mut to_retry = 0;
        for txn in state_compute_result.compute_status_for_input_txns() {
            match txn {
                TransactionStatus::Keep(_) => to_commit += 1,
                TransactionStatus::Retry => to_retry += 1,
                _ => {},
            }
        }

        let execution_summary = ExecutionSummary {
            payload_len: self
                .block
                .payload()
                .map_or(0, |payload| payload.len_for_execution()),
            to_commit,
            to_retry,
            execution_time,
            root_hash: state_compute_result.root_hash(),
            gas_used: state_compute_result
                .execution_output
                .block_end_info
                .as_ref()
                .map(|info| info.block_effective_gas_units()),
        };
        *self.state_compute_result.lock() = state_compute_result;

        // We might be retrying execution, so it might have already been set.
        // Because we use this for statistics, it's ok that we drop the newer value.
        if let Some(previous) = self.execution_summary.get() {
            if previous.root_hash == execution_summary.root_hash
                || previous.root_hash == *ACCUMULATOR_PLACEHOLDER_HASH
            {
                warn!(
                    "Skipping re-inserting execution result, from {:?} to {:?}",
                    previous, execution_summary
                );
            } else {
                error!(
                    "Re-inserting execution result with different root hash: from {:?} to {:?}",
                    previous, execution_summary
                );
            }
        } else {
            self.execution_summary
                .set(execution_summary)
                .expect("inserting into empty execution summary");
        }
    }
```

**File:** config/src/config/consensus_config.rs (L232-232)
```rust
            max_pruned_blocks_in_mem: 100,
```
