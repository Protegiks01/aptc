# Audit Report

## Title
Defer Block Failure in Batch Store Leading to Memory Leaks and Potential Consensus Liveness Failure

## Summary
The `defer!` block in `get_or_fetch_batch()` can fail to execute when the `inflight_fetch_requests` Mutex becomes poisoned, preventing cleanup of batch fetch entries and causing memory leaks. This occurs due to the use of `aptos_infallible::Mutex` which panics on poisoned locks, creating a cascading failure scenario where cleanup attempts themselves panic. [1](#0-0) 

## Finding Description

The vulnerability stems from the interaction between Rust's panic unwinding mechanism, the `scopeguard` defer! macro, and the `aptos_infallible::Mutex` implementation.

**Architecture:**
The `BatchReaderImpl` maintains an `inflight_fetch_requests` map that tracks ongoing batch fetch operations: [2](#0-1) 

This map uses `aptos_infallible::Mutex`, which automatically unwraps lock results and panics on poisoned locks: [3](#0-2) 

**Vulnerability Flow:**

1. **Lock Acquisition:** When `get_or_fetch_batch()` is called, it acquires the `inflight_fetch_requests` lock: [4](#0-3) 

2. **Panic While Holding Lock:** Several operations while holding this lock can panic:
   - Line 674: Acquiring the inner `responders` lock (if poisoned) [5](#0-4) 
   - Line 677: Memory allocation for Arc/Mutex
   - Line 714: Task spawning on runtime shutdown or OOM [6](#0-5) 

3. **Lock Poisoning:** When a panic occurs while holding the lock (lines 670-721), the `inflight_fetch_requests` Mutex becomes poisoned.

4. **Cleanup Failure:** Previously spawned async tasks complete and their `defer!` blocks execute: [1](#0-0) 

   However, when `inflight_requests_clone.lock()` is called on the poisoned Mutex, `aptos_infallible::Mutex` panics with "Cannot currently handle a poisoned lock", preventing the `remove(&batch_digest)` call from executing.

5. **Memory Leak:** The `BatchFetchUnit` entry remains in the map indefinitely, containing:
   - `Arc<Mutex<BTreeSet<PeerId>>>` (responders)
   - `Shared<Pin<Box<dyn Future<...>>>>` (the future)

**Cascading Failure:** Once the lock is poisoned, ALL subsequent calls to `get_or_fetch_batch()` fail immediately at line 670, causing complete consensus liveness failure as batches cannot be fetched for block execution.

**Realistic Trigger Scenarios:**

1. **Inner Lock Poisoning:** The responders lock in `BatchRequester::next_request_peers()` can panic with division by zero if the signers set is empty: [7](#0-6) [8](#0-7) 

2. **Memory Exhaustion:** Under memory pressure, allocations at lines 677, 714, or 716-719 fail, causing panics while the outer lock is held.

## Impact Explanation

This vulnerability meets **Medium Severity** criteria per the Aptos bug bounty program:
- **State inconsistencies requiring intervention:** Memory leaks accumulate as batch entries are not cleaned up
- **Validator node slowdowns:** Memory exhaustion eventually degrades performance

However, the broader impact includes **High Severity** characteristics:
- **Significant protocol violations:** Once triggered, the entire batch fetching system fails
- **Consensus liveness impact:** Blocks requiring batch data cannot be executed, stalling consensus
- **Validator node crashes:** Eventually leads to out-of-memory conditions

The vulnerability breaks the **Resource Limits** invariant (#9): "All operations must respect gas, storage, and computational limits" - the system fails to properly clean up resources, violating memory management guarantees.

## Likelihood Explanation

**Likelihood: LOW to MEDIUM**

**Required Conditions:**
1. A panic must occur while holding the `inflight_fetch_requests` lock (lines 670-721)
2. Possible through: memory exhaustion, empty responders set, or runtime shutdown during spawn

**Factors Increasing Likelihood:**
- Under sustained load or resource pressure, OOM conditions become more likely
- Cascading failures: once one lock is poisoned, cleanup failures poison more locks
- No recovery mechanism: the system cannot self-heal from lock poisoning

**Factors Decreasing Likelihood:**
- Empty responders lists are unlikely in normal operation (requires quorum signatures)
- Modern systems typically have sufficient memory for consensus operations
- Requires exceptional runtime conditions

**Attacker Exploitability:** INDIRECT - An external attacker cannot directly trigger this, but can contribute to resource exhaustion through transaction spam, potentially increasing the likelihood of OOM-induced panics.

## Recommendation

**Fix 1: Use std::sync::Mutex directly and handle poison errors**

Replace `aptos_infallible::Mutex` with `std::sync::Mutex` for `inflight_fetch_requests` and explicitly handle poisoned locks:

```rust
// In BatchReaderImpl
inflight_fetch_requests: Arc<std::sync::Mutex<HashMap<HashValue, BatchFetchUnit>>>,

// In get_or_fetch_batch()
match self.inflight_fetch_requests.lock() {
    Ok(mut guard) => {
        // Normal operation
        guard.entry(*batch_info.digest())
            .and_modify(...)
            .or_insert_with(...)
            .fut
            .clone()
    },
    Err(poisoned) => {
        // Recover from poison by clearing the map
        let mut guard = poisoned.into_inner();
        guard.clear();
        // Retry the operation
        guard.entry(*batch_info.digest())
            .or_insert_with(...)
            .fut
            .clone()
    }
}

// In defer! block
if let Ok(mut guard) = inflight_requests_clone.lock() {
    guard.remove(&batch_digest);
} else {
    // Log error but don't panic
    error!("Failed to cleanup batch fetch entry due to poisoned lock: {}", batch_digest);
}
```

**Fix 2: Use lock-free data structures**

Replace the Mutex-protected HashMap with `DashMap` (already used elsewhere in the codebase):

```rust
// In BatchReaderImpl
inflight_fetch_requests: Arc<DashMap<HashValue, BatchFetchUnit>>,

// No explicit locking needed
// In defer! block
defer!({
    inflight_requests_clone.remove(&batch_digest);
});
```

**Fix 3: Add panic guards**

Wrap critical sections that could poison locks with `catch_unwind`:

```rust
use std::panic::catch_unwind;

let result = catch_unwind(|| {
    self.inflight_fetch_requests
        .lock()
        .entry(*batch_info.digest())
        // ...
});

match result {
    Ok(fut) => fut,
    Err(_) => {
        // Handle panic, return error instead
        return create_error_future();
    }
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::Arc;
    use aptos_infallible::Mutex;
    use std::collections::HashMap;
    use aptos_crypto::HashValue;

    #[test]
    #[should_panic(expected = "Cannot currently handle a poisoned lock")]
    fn test_defer_cleanup_failure_on_poisoned_lock() {
        // Simulate the inflight_requests map
        let inflight_requests = Arc::new(Mutex::new(HashMap::<HashValue, i32>::new()));
        let inflight_clone = inflight_requests.clone();
        
        // Thread 1: Poison the lock by panicking while holding it
        let handle1 = std::thread::spawn(move || {
            let _guard = inflight_requests.lock();
            panic!("Simulating panic while holding lock");
        });
        
        // Wait for thread 1 to panic and poison the lock
        let _ = handle1.join();
        
        // Thread 2: Simulate defer! cleanup attempting to acquire the poisoned lock
        // This will panic with "Cannot currently handle a poisoned lock"
        let batch_digest = HashValue::random();
        let cleanup = move || {
            // This is what the defer! block does
            inflight_clone.lock().remove(&batch_digest);
        };
        
        // This should panic, demonstrating the vulnerability
        cleanup();
    }
    
    #[test]
    fn test_memory_leak_from_failed_cleanup() {
        // Demonstrate that entries are not cleaned up when defer! fails
        let inflight_requests = Arc::new(Mutex::new(HashMap::<HashValue, i32>::new()));
        
        // Insert an entry
        let digest1 = HashValue::random();
        inflight_requests.lock().insert(digest1, 1);
        
        // Poison the lock
        let inflight_clone = inflight_requests.clone();
        let handle = std::thread::spawn(move || {
            let _guard = inflight_clone.lock();
            panic!("Poison");
        });
        let _ = handle.join();
        
        // Verify the entry is still there (cleanup would have removed it)
        // We can't access it due to poison, but it's still consuming memory
        assert!(inflight_requests.is_poisoned());
    }
}
```

**Note:** This PoC demonstrates the core issue. A full reproduction in the Aptos consensus context would require setting up the entire quorum store infrastructure, which is complex. The key insight is that `aptos_infallible::Mutex` combined with `defer!` in async contexts creates a fragile cleanup mechanism that fails under exceptional conditions.

### Citations

**File:** consensus/src/quorum_store/batch_store.rs (L651-651)
```rust
    inflight_fetch_requests: Arc<Mutex<HashMap<HashValue, BatchFetchUnit>>>,
```

**File:** consensus/src/quorum_store/batch_store.rs (L670-672)
```rust
        self.inflight_fetch_requests
            .lock()
            .entry(*batch_info.digest())
```

**File:** consensus/src/quorum_store/batch_store.rs (L673-675)
```rust
            .and_modify(|fetch_unit| {
                fetch_unit.responders.lock().append(&mut responders);
            })
```

**File:** consensus/src/quorum_store/batch_store.rs (L686-688)
```rust
                    defer!({
                        inflight_requests_clone.lock().remove(&batch_digest);
                    });
```

**File:** consensus/src/quorum_store/batch_store.rs (L714-714)
```rust
                tokio::spawn(fut.clone());
```

**File:** crates/aptos-infallible/src/mutex.rs (L19-23)
```rust
    pub fn lock(&self) -> MutexGuard<'_, T> {
        self.0
            .lock()
            .expect("Cannot currently handle a poisoned lock")
    }
```

**File:** consensus/src/quorum_store/batch_requester.rs (L45-45)
```rust
            self.next_index = rng.r#gen::<usize>() % signers.len();
```

**File:** consensus/src/quorum_store/batch_requester.rs (L59-59)
```rust
            self.next_index = (self.next_index + num_peers) % signers.len();
```
