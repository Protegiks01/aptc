# Audit Report

## Title
State Sync Logical Time Update Before Verification Allows Consensus to Operate on Stale State

## Summary
The `sync_to_target()` function in `ExecutionProxy` unconditionally updates the logical time to the target value before checking if state synchronization succeeded. If state sync fails after partially committing data, subsequent retry attempts will incorrectly assume synchronization is complete, causing consensus to operate with inconsistent state.

## Finding Description

The vulnerability exists in the `ExecutionProxy::sync_to_target()` implementation where the logical time tracking is updated before validating that state synchronization completed successfully. [1](#0-0) 

The critical flaw is that the logical time is updated at line 222 **unconditionally** - it executes regardless of whether the `result` from state sync is `Ok` or `Err`. The function then returns this result to the caller, but the damage is already done.

The early return check at line 188 compounds this issue: [2](#0-1) 

This creates a dangerous scenario:

1. **Initial sync attempt**: Node at version 50, target is version 100
2. **Partial success**: State sync processes chunks 1-7 successfully (database now at version 70)
3. **Verification failure**: Chunk 8 fails verification (e.g., state root mismatch, corrupted data from malicious peer)
4. **State sync returns error**: The chunk executor detects the verification failure and returns an error [3](#0-2) 

5. **Logical time updated anyway**: Despite the error, line 222 updates `logical_time` to (epoch, round) = (1, 100)
6. **Error propagated to caller**: Consensus receives the error
7. **Retry attempt**: Consensus retries `sync_to_target()` with the same target
8. **False success**: Check at line 188 evaluates as `100 >= 100`, function returns `Ok()` without syncing
9. **State inconsistency**: Database contains version 70, but consensus believes it's at version 100

The state sync notification handler only checks version equality, not state root equality: [4](#0-3) 

After `sync_to_target()` completes, consensus proceeds with block storage recovery without verifying the actual database state: [5](#0-4) 

## Impact Explanation

**Severity: Critical** (Consensus/Safety Violations)

This vulnerability breaks the fundamental **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs." The impact includes:

1. **Consensus Safety Violations**: Validators operating with different state views may produce conflicting blocks, potentially causing chain forks
2. **Network Partition Risk**: Nodes with incorrect state will reject valid blocks or propose invalid ones, leading to network-wide inconsistency
3. **Non-Recoverable State**: The affected node believes it's synchronized but is actually behind, requiring manual intervention or a hard fork to resolve
4. **Byzantine Behavior**: A correctly functioning node appears Byzantine to honest validators due to state mismatch

This qualifies as **Critical Severity** under the Aptos bug bounty program as it causes consensus/safety violations and can lead to non-recoverable network partitions.

## Likelihood Explanation

**Likelihood: Medium-to-High**

This vulnerability can be triggered by:

1. **Malicious Peers**: Attackers can send corrupted chunks that pass initial validation but fail verification at commit time
2. **Network Failures**: Transient network errors during chunk fetching can cause partial sync failures
3. **Storage Errors**: Database write failures during chunk commit leave the database in a partially synced state
4. **Verification Failures**: Any mismatch between expected and actual state roots triggers this bug

The attack requires no special privileges - any network peer can send data to syncing nodes. The retry behavior is automatic in consensus, making exploitation reliable once triggered. Multiple validators experiencing this simultaneously could cause network-wide consensus failures.

## Recommendation

The logical time should only be updated if state synchronization succeeds. Move the logical time update inside a conditional check:

```rust
// In ExecutionProxy::sync_to_target()
let result = monitor!(
    "sync_to_target",
    self.state_sync_notifier.sync_to_target(target).await
);

// Only update logical time if sync succeeded
if result.is_ok() {
    *latest_logical_time = target_logical_time;
}

// Reset the executor regardless of success/failure
self.executor.reset()?;

// Return the result
result.map_err(|error| {
    let anyhow_error: anyhow::Error = error.into();
    anyhow_error.into()
})
```

Additionally, consider adding a verification check after successful sync:

```rust
// After successful sync, verify database is at target version
if result.is_ok() {
    let synced_version = self.executor.committed_version()?;
    let target_version = target.ledger_info().version();
    ensure!(
        synced_version >= target_version,
        "State sync completed but database version {} is less than target {}",
        synced_version, target_version
    );
    *latest_logical_time = target_logical_time;
}
```

## Proof of Concept

This vulnerability can be demonstrated with the following test scenario:

```rust
// Test case for sync_to_target logical time bug
#[tokio::test]
async fn test_sync_to_target_partial_failure() {
    // Setup: Create ExecutionProxy with mock state sync notifier
    let mock_state_sync = MockStateSyncNotifier::new();
    let execution_proxy = ExecutionProxy::new(
        mock_executor,
        mock_txn_notifier,
        Arc::new(mock_state_sync.clone()),
        txn_filter_config,
        false,
        None,
    );

    // Create target at version 100
    let target = create_ledger_info_with_signatures(100);

    // Configure mock to:
    // 1. Commit chunks up to version 70
    // 2. Return error for subsequent chunks
    mock_state_sync.set_behavior(
        MockBehavior::PartialSuccess { 
            success_until: 70, 
            then_fail: true 
        }
    );

    // First sync attempt - should fail
    let result = execution_proxy.sync_to_target(target.clone()).await;
    assert!(result.is_err()); // State sync failed
    
    // Check database is at version 70
    let db_version = get_committed_version();
    assert_eq!(db_version, 70);

    // Second sync attempt with same target
    let result = execution_proxy.sync_to_target(target.clone()).await;
    
    // BUG: Returns Ok() without syncing
    assert!(result.is_ok()); // Should fail but passes!
    
    // Database is still at version 70, not 100
    let db_version = get_committed_version();
    assert_eq!(db_version, 70); // Not 100 as expected!
    
    // Consensus now thinks state is at version 100
    // but database is actually at version 70
    // This causes consensus violations when proposing/voting
}
```

## Notes

This vulnerability affects all components that rely on `sync_to_target()` including:
- Block storage sync manager (fast_forward_sync)
- Recovery manager (epoch recovery)
- DAG state synchronization
- Consensus observer state sync

The verification does happen at the storage layer when committing ledger info with signatures: [6](#0-5) 

However, this verification only occurs when a chunk ends exactly at a ledger info version. If state sync fails before reaching such a checkpoint, the database remains in a partially synced state while consensus incorrectly believes synchronization is complete.

### Citations

**File:** consensus/src/state_computer.rs (L187-194)
```rust
        // The pipeline phase already committed beyond the target block timestamp, just return.
        if *latest_logical_time >= target_logical_time {
            warn!(
                "State sync target {:?} is lower than already committed logical time {:?}",
                target_logical_time, *latest_logical_time
            );
            return Ok(());
        }
```

**File:** consensus/src/state_computer.rs (L216-232)
```rust
        let result = monitor!(
            "sync_to_target",
            self.state_sync_notifier.sync_to_target(target).await
        );

        // Update the latest logical time
        *latest_logical_time = target_logical_time;

        // Similarly, after state synchronization, we have to reset the cache of
        // the BlockExecutor to guarantee the latest committed state is up to date.
        self.executor.reset()?;

        // Return the result
        result.map_err(|error| {
            let anyhow_error: anyhow::Error = error.into();
            anyhow_error.into()
        })
```

**File:** execution/executor/src/chunk_executor/chunk_result_verifier.rs (L80-88)
```rust
        if li.version() + 1 == txn_accumulator.num_leaves() {
            // If the chunk corresponds to the target LI, the target LI can be added to storage.
            ensure!(
                li.transaction_accumulator_hash() == txn_accumulator.root_hash(),
                "Root hash in target ledger info does not match local computation. {:?} != {:?}",
                li,
                txn_accumulator,
            );
            Ok(Some(self.verified_target_li.clone()))
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L198-207)
```rust
            ConsensusSyncRequest::SyncTarget(sync_target_notification) => {
                // Get the sync target version and latest synced version
                let sync_target = sync_target_notification.get_target();
                let sync_target_version = sync_target.ledger_info().version();
                let latest_synced_version = latest_synced_ledger_info.ledger_info().version();

                // Check if we've satisfied the target
                latest_synced_version >= sync_target_version
            },
        }
```

**File:** consensus/src/block_storage/sync_manager.rs (L512-524)
```rust
        execution_client
            .sync_to_target(highest_commit_cert.ledger_info().clone())
            .await?;

        // we do not need to update block_tree.highest_commit_decision_ledger_info here
        // because the block_tree is going to rebuild itself.

        let recovery_data = match storage.start(order_vote_enabled, window_size) {
            LivenessStorageData::FullRecoveryData(recovery_data) => recovery_data,
            _ => panic!("Failed to construct recovery data after fast forward sync"),
        };

        Ok(recovery_data)
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L556-569)
```rust
        // Verify the root hash.
        let db_root_hash = self
            .ledger_db
            .transaction_accumulator_db()
            .get_root_hash(version)?;
        let li_root_hash = ledger_info_with_sig
            .ledger_info()
            .transaction_accumulator_hash();
        ensure!(
            db_root_hash == li_root_hash,
            "Root hash pre-committed doesn't match LedgerInfo. pre-commited: {:?} vs in LedgerInfo: {:?}",
            db_root_hash,
            li_root_hash,
        );
```
