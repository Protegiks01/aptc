# Audit Report

## Title
State Snapshot Restore Allows Attacker to Corrupt Validator State by Removing Chunks from Backup Manifest

## Summary
An attacker controlling backup storage can remove chunks from a state snapshot manifest, causing the restore process to complete successfully with incomplete state. Unlike transaction restore which validates chunk completeness, state snapshot restore lacks validation that all chunks are present and that the final restored root hash matches the expected root hash, allowing silent state corruption.

## Finding Description
The state snapshot restore process fails to validate two critical properties:

1. **No manifest chunk completeness validation**: Unlike `TransactionBackup::verify()` which explicitly checks that chunks are consecutive and complete, `StateSnapshotBackup` has no corresponding validation method. [1](#0-0) [2](#0-1) 

2. **No final root hash validation**: After all chunks are processed and `finish()` is called, there is no check that the actual root hash of the restored tree matches the expected root hash. [3](#0-2) 

The attack exploits the incremental proof verification mechanism. Each chunk's `SparseMerkleRangeProof` contains "right siblings" representing the hash of state to the right of the current chunk. During verification, these hashes are used to prove the chunk is part of the expected tree, but they are not stored - only the actual added nodes are persisted. [4](#0-3) [5](#0-4) 

**Attack Flow:**
1. Attacker controls backup storage and modifies the manifest JSON to remove the last N chunks while keeping the legitimate `root_hash` and cryptographic proof intact
2. `StateSnapshotRestoreController` loads the modified manifest and validates the root hash against the signed ledger info (this passes because the attacker kept the legitimate proof) [6](#0-5) 

3. For each remaining chunk, the system calls `add_chunk()` and `verify()` - these succeed because the proofs are legitimate and include the hash of missing chunks as "right siblings" [7](#0-6) 

4. `finish()` is called, which freezes only the nodes that were actually added and writes the incomplete tree to the database with no validation [8](#0-7) 

5. The restored database contains incomplete state (missing keys from the removed chunks) but the restore operation returns `Ok(())`

This breaks the **Deterministic Execution** and **State Consistency** invariants - validators restoring from manipulated backups will have different state than validators with complete state, leading to consensus failures.

## Impact Explanation
**Critical Severity** - This vulnerability allows an attacker controlling backup storage to silently corrupt validator state during restore operations. The impact includes:

- **Consensus Safety Violation**: Validators restoring from corrupted backups will have incomplete state, causing them to produce different state roots than honest validators when processing the same blocks
- **State Inconsistency**: Missing state keys make the validator unable to properly validate transactions that access those keys
- **Network Partition Risk**: Affected validators may fail to reach consensus with the honest network, potentially requiring manual intervention or hard fork to recover

The vulnerability meets Critical severity criteria as it violates consensus safety and causes non-recoverable state corruption requiring manual intervention.

## Likelihood Explanation
**High Likelihood** - The attack is feasible for any attacker with access to backup storage:

- **Low Attacker Requirements**: Only requires write access to backup storage (cloud storage bucket, S3, etc.), which may be compromised through credential theft or misconfiguration
- **Simple Execution**: Attack only requires modifying a JSON manifest file to remove chunk entries - no cryptographic forgery needed
- **Silent Failure**: No validation catches the attack, making it likely to succeed undetected until the corrupted validator attempts to participate in consensus
- **Realistic Scenario**: Organizations commonly use cloud storage for backups, and misconfigurations or compromised credentials are common attack vectors

## Recommendation
Implement two critical validations:

1. **Add manifest validation for StateSnapshotBackup** similar to TransactionBackup:

```rust
impl StateSnapshotBackup {
    pub fn verify(&self) -> Result<()> {
        ensure!(!self.chunks.is_empty(), "No chunks.");
        
        // Verify chunks are in sorted order by first_key
        for i in 1..self.chunks.len() {
            ensure!(
                self.chunks[i].first_key > self.chunks[i-1].last_key,
                "Chunks not in sorted order or have gaps. Chunk {} first_key: {:x}, previous chunk last_key: {:x}",
                i, self.chunks[i].first_key, self.chunks[i-1].last_key
            );
            ensure!(
                self.chunks[i].first_idx == self.chunks[i-1].last_idx + 1,
                "Chunk indices not consecutive. Expected first_idx: {}, actual: {}",
                self.chunks[i-1].last_idx + 1, self.chunks[i].first_idx
            );
        }
        Ok(())
    }
}
```

Call this in `StateSnapshotRestoreController::run_impl()` after loading the manifest:

```rust
let manifest: StateSnapshotBackup = self.storage.load_json_file(&self.manifest_handle).await?;
manifest.verify()?; // Add this validation
```

2. **Add explicit root hash validation after restore completes**:

```rust
pub fn finish_impl(mut self) -> Result<()> {
    self.wait_for_async_commit()?;
    // ... existing freeze logic ...
    self.freeze(0);
    self.store.write_node_batch(&self.frozen_nodes)?;
    
    // Validate the restored root hash matches expected
    let actual_root_hash = self.store.get_node_option(&NodeKey::new_empty_path(self.version), "restore")?
        .ok_or_else(|| anyhow!("Root node not found after restore"))?
        .hash();
    ensure!(
        actual_root_hash == self.expected_root_hash,
        "Restored root hash {:x} does not match expected {:x}. State restore incomplete or corrupted.",
        actual_root_hash, self.expected_root_hash
    );
    
    Ok(())
}
```

## Proof of Concept

```rust
#[test]
fn test_state_snapshot_restore_detects_missing_chunks() {
    use crate::backup_types::state_snapshot::manifest::{StateSnapshotBackup, StateSnapshotChunk};
    use aptos_crypto::HashValue;
    
    // Create a manifest with 3 chunks
    let mut manifest = StateSnapshotBackup {
        version: 100,
        epoch: 1,
        root_hash: HashValue::random(),
        chunks: vec![
            StateSnapshotChunk {
                first_idx: 0,
                last_idx: 100,
                first_key: HashValue::zero(),
                last_key: HashValue::from_u64(100),
                blobs: "chunk0.blob".into(),
                proof: "chunk0.proof".into(),
            },
            StateSnapshotChunk {
                first_idx: 101,
                last_idx: 200,
                first_key: HashValue::from_u64(101),
                last_key: HashValue::from_u64(200),
                blobs: "chunk1.blob".into(),
                proof: "chunk1.proof".into(),
            },
            StateSnapshotChunk {
                first_idx: 201,
                last_idx: 300,
                first_key: HashValue::from_u64(201),
                last_key: HashValue::from_u64(300),
                blobs: "chunk2.blob".into(),
                proof: "chunk2.proof".into(),
            },
        ],
        proof: "state.proof".into(),
    };
    
    // Attacker removes the last chunk
    manifest.chunks.pop();
    
    // Currently, there's no validation - this would succeed
    // After fix with verify(), this should fail:
    // assert!(manifest.verify().is_err());
    
    // The vulnerability: restore would complete "successfully" with incomplete state
    // After fix, the final root hash check in finish_impl() would also catch this
}
```

## Notes
The root cause is the difference in validation rigor between transaction restore (which uses version numbers that must be consecutive) and state snapshot restore (which uses hash-based keys without completeness checks). The proof verification system validates individual chunks correctly but cannot detect when chunks are entirely absent from the manifest. This is further compounded by the lack of final root hash validation, creating a defense-in-depth failure.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/transaction/manifest.rs (L50-88)
```rust
    pub fn verify(&self) -> Result<()> {
        // check number of waypoints
        ensure!(
            self.first_version <= self.last_version,
            "Bad version range: [{}, {}]",
            self.first_version,
            self.last_version,
        );

        // check chunk ranges
        ensure!(!self.chunks.is_empty(), "No chunks.");

        let mut next_version = self.first_version;
        for chunk in &self.chunks {
            ensure!(
                chunk.first_version == next_version,
                "Chunk ranges not continuous. Expected first version: {}, actual: {}.",
                next_version,
                chunk.first_version,
            );
            ensure!(
                chunk.last_version >= chunk.first_version,
                "Chunk range invalid. [{}, {}]",
                chunk.first_version,
                chunk.last_version,
            );
            next_version = chunk.last_version + 1;
        }

        // check last version in chunk matches manifest
        ensure!(
            next_version - 1 == self.last_version, // okay to -1 because chunks is not empty.
            "Last version in chunks: {}, in manifest: {}",
            next_version - 1,
            self.last_version,
        );

        Ok(())
    }
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/manifest.rs (L1-51)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use crate::storage::FileHandle;
use aptos_crypto::HashValue;
use aptos_types::transaction::Version;
use serde::{Deserialize, Serialize};

/// A chunk of a state snapshot manifest, representing accounts in the key range
/// [`first_key`, `last_key`] (right side inclusive).
#[derive(Deserialize, Serialize)]
pub struct StateSnapshotChunk {
    /// index of the first account in this chunk over all accounts.
    pub first_idx: usize,
    /// index of the last account in this chunk over all accounts.
    pub last_idx: usize,
    /// key of the first account in this chunk.
    pub first_key: HashValue,
    /// key of the last account in this chunk.
    pub last_key: HashValue,
    /// Repeated `len(record) + record` where `record` is BCS serialized tuple
    /// `(key, state_value)`
    pub blobs: FileHandle,
    /// BCS serialized `SparseMerkleRangeProof` that proves this chunk adds up to the root hash
    /// indicated in the backup (`StateSnapshotBackup::root_hash`).
    pub proof: FileHandle,
}

/// State snapshot backup manifest, representing a complete state view at specified version.
#[derive(Deserialize, Serialize)]
pub struct StateSnapshotBackup {
    /// Version at which this state snapshot is taken.
    pub version: Version,
    /// Epoch in which this state snapshot is taken.
    pub epoch: u64,
    /// Hash of the state tree root.
    pub root_hash: HashValue,
    /// All account blobs in chunks.
    pub chunks: Vec<StateSnapshotChunk>,
    /// BCS serialized
    /// `Tuple(TransactionInfoWithProof, LedgerInfoWithSignatures)`.
    ///   - The `TransactionInfoWithProof` is at `Version` above, and carries the same `root_hash`
    /// above; It proves that at specified version the root hash is as specified in a chain
    /// represented by the LedgerInfo below.
    ///   - The signatures on the `LedgerInfoWithSignatures` has a version greater than or equal to
    /// the version of this backup but is within the same epoch, so the signatures on it can be
    /// verified by the validator set in the same epoch, which can be provided by an
    /// `EpochStateBackup` recovered prior to this to the DB; Requiring it to be in the same epoch
    /// limits the requirement on such `EpochStateBackup` to no older than the same epoch.
    pub proof: FileHandle,
}
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L339-413)
```rust
    pub fn add_chunk_impl(
        &mut self,
        mut chunk: Vec<(&K, HashValue)>,
        proof: SparseMerkleRangeProof,
    ) -> Result<()> {
        if self.finished {
            info!("State snapshot restore already finished, ignoring entire chunk.");
            return Ok(());
        }

        if let Some(prev_leaf) = &self.previous_leaf {
            let skip_until = chunk
                .iter()
                .find_position(|(key, _hash)| key.hash() > *prev_leaf.account_key());
            chunk = match skip_until {
                None => {
                    info!("Skipping entire chunk.");
                    return Ok(());
                },
                Some((0, _)) => chunk,
                Some((num_to_skip, next_leaf)) => {
                    info!(
                        num_to_skip = num_to_skip,
                        next_leaf = next_leaf,
                        "Skipping leaves."
                    );
                    chunk.split_off(num_to_skip)
                },
            }
        };
        if chunk.is_empty() {
            return Ok(());
        }

        for (key, value_hash) in chunk {
            let hashed_key = key.hash();
            if let Some(ref prev_leaf) = self.previous_leaf {
                ensure!(
                    &hashed_key > prev_leaf.account_key(),
                    "State keys must come in increasing order.",
                )
            }
            self.previous_leaf.replace(LeafNode::new(
                hashed_key,
                value_hash,
                (key.clone(), self.version),
            ));
            self.add_one(key, value_hash);
            self.num_keys_received += 1;
        }

        // Verify what we have added so far is all correct.
        self.verify(proof)?;

        // Write the frozen nodes to storage.
        if self.async_commit {
            self.wait_for_async_commit()?;
            let (tx, rx) = channel();
            self.async_commit_result = Some(rx);

            let mut frozen_nodes = HashMap::new();
            std::mem::swap(&mut frozen_nodes, &mut self.frozen_nodes);
            let store = self.store.clone();

            IO_POOL.spawn(move || {
                let res = store.write_node_batch(&frozen_nodes);
                tx.send(res).unwrap();
            });
        } else {
            self.store.write_node_batch(&self.frozen_nodes)?;
            self.frozen_nodes.clear();
        }

        Ok(())
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L624-697)
```rust
    /// Verifies that all states that have been added so far (from the leftmost one to
    /// `self.previous_leaf`) are correct, i.e., we are able to construct `self.expected_root_hash`
    /// by combining all existing states and `proof`.
    #[allow(clippy::collapsible_if)]
    fn verify(&self, proof: SparseMerkleRangeProof) -> Result<()> {
        let previous_leaf = self
            .previous_leaf
            .as_ref()
            .expect("The previous leaf must exist.");

        let previous_key = previous_leaf.account_key();
        // If we have all siblings on the path from root to `previous_key`, we should be able to
        // compute the root hash. The siblings on the right are already in the proof. Now we
        // compute the siblings on the left side, which represent all the states that have ever
        // been added.
        let mut left_siblings = vec![];

        // The following process might add some extra placeholder siblings on the left, but it is
        // nontrivial to determine when the loop should stop. So instead we just add these
        // siblings for now and get rid of them in the next step.
        let mut num_visited_right_siblings = 0;
        for (i, bit) in previous_key.iter_bits().enumerate() {
            if bit {
                // This node is a right child and there should be a sibling on the left.
                let sibling = if i >= self.partial_nodes.len() * 4 {
                    *SPARSE_MERKLE_PLACEHOLDER_HASH
                } else {
                    Self::compute_left_sibling(
                        &self.partial_nodes[i / 4],
                        previous_key.get_nibble(i / 4),
                        (3 - i % 4) as u8,
                    )
                };
                left_siblings.push(sibling);
            } else {
                // This node is a left child and there should be a sibling on the right.
                num_visited_right_siblings += 1;
            }
        }
        ensure!(
            num_visited_right_siblings >= proof.right_siblings().len(),
            "Too many right siblings in the proof.",
        );

        // Now we remove any extra placeholder siblings at the bottom. We keep removing the last
        // sibling if 1) it's a placeholder 2) it's a sibling on the left.
        for bit in previous_key.iter_bits().rev() {
            if bit {
                if *left_siblings.last().expect("This sibling must exist.")
                    == *SPARSE_MERKLE_PLACEHOLDER_HASH
                {
                    left_siblings.pop();
                } else {
                    break;
                }
            } else if num_visited_right_siblings > proof.right_siblings().len() {
                num_visited_right_siblings -= 1;
            } else {
                break;
            }
        }

        // Left siblings must use the same ordering as the right siblings in the proof
        left_siblings.reverse();

        // Verify the proof now that we have all the siblings
        proof
            .verify(
                self.expected_root_hash,
                SparseMerkleLeafNode::new(*previous_key, previous_leaf.value_hash()),
                left_siblings,
            )
            .map_err(Into::into)
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L750-789)
```rust
    pub fn finish_impl(mut self) -> Result<()> {
        self.wait_for_async_commit()?;
        // Deal with the special case when the entire tree has a single leaf or null node.
        if self.partial_nodes.len() == 1 {
            let mut num_children = 0;
            let mut leaf = None;
            for i in 0..16 {
                if let Some(ref child_info) = self.partial_nodes[0].children[i] {
                    num_children += 1;
                    if let ChildInfo::Leaf(node) = child_info {
                        leaf = Some(node.clone());
                    }
                }
            }

            match num_children {
                0 => {
                    let node_key = NodeKey::new_empty_path(self.version);
                    assert!(self.frozen_nodes.is_empty());
                    self.frozen_nodes.insert(node_key, Node::Null);
                    self.store.write_node_batch(&self.frozen_nodes)?;
                    return Ok(());
                },
                1 => {
                    if let Some(node) = leaf {
                        let node_key = NodeKey::new_empty_path(self.version);
                        assert!(self.frozen_nodes.is_empty());
                        self.frozen_nodes.insert(node_key, node.into());
                        self.store.write_node_batch(&self.frozen_nodes)?;
                        return Ok(());
                    }
                },
                _ => (),
            }
        }

        self.freeze(0);
        self.store.write_node_batch(&self.frozen_nodes)?;
        Ok(())
    }
```

**File:** types/src/proof/definition.rs (L782-826)
```rust
    pub fn verify(
        &self,
        expected_root_hash: HashValue,
        rightmost_known_leaf: SparseMerkleLeafNode,
        left_siblings: Vec<HashValue>,
    ) -> Result<()> {
        let num_siblings = left_siblings.len() + self.right_siblings.len();
        let mut left_sibling_iter = left_siblings.iter();
        let mut right_sibling_iter = self.right_siblings().iter();

        let mut current_hash = rightmost_known_leaf.hash();
        for bit in rightmost_known_leaf
            .key()
            .iter_bits()
            .rev()
            .skip(HashValue::LENGTH_IN_BITS - num_siblings)
        {
            let (left_hash, right_hash) = if bit {
                (
                    *left_sibling_iter
                        .next()
                        .ok_or_else(|| format_err!("Missing left sibling."))?,
                    current_hash,
                )
            } else {
                (
                    current_hash,
                    *right_sibling_iter
                        .next()
                        .ok_or_else(|| format_err!("Missing right sibling."))?,
                )
            };
            current_hash = SparseMerkleInternalNode::new(left_hash, right_hash).hash();
        }

        ensure!(
            current_hash == expected_root_hash,
            "{}: Root hashes do not match. Actual root hash: {:x}. Expected root hash: {:x}.",
            type_name::<Self>(),
            current_hash,
            expected_root_hash,
        );

        Ok(())
    }
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L123-145)
```rust
        let manifest: StateSnapshotBackup =
            self.storage.load_json_file(&self.manifest_handle).await?;
        let (txn_info_with_proof, li): (TransactionInfoWithProof, LedgerInfoWithSignatures) =
            self.storage.load_bcs_file(&manifest.proof).await?;
        txn_info_with_proof.verify(li.ledger_info(), manifest.version)?;
        let state_root_hash = txn_info_with_proof
            .transaction_info()
            .ensure_state_checkpoint_hash()?;
        ensure!(
            state_root_hash == manifest.root_hash,
            "Root hash mismatch with that in proof. root hash: {}, expected: {}",
            manifest.root_hash,
            state_root_hash,
        );
        if let Some(epoch_history) = self.epoch_history.as_ref() {
            epoch_history.verify_ledger_info(&li)?;
        }

        let receiver = Arc::new(Mutex::new(Some(self.run_mode.get_state_restore_receiver(
            self.version,
            manifest.root_hash,
            self.restore_mode,
        )?)));
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L228-230)
```rust
        tokio::task::spawn_blocking(move || receiver.lock().take().unwrap().finish()).await??;
        self.run_mode.finish();
        Ok(())
```
