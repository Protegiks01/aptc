# Audit Report

## Title
Race Condition in Table Info Backup Allows Concurrent Snapshot Overwrites Leading to Data Loss

## Summary
The `generate_blob_name()` function creates GCS blob names using only `chain_id` and `epoch`, without any node-specific identifier. When multiple indexer fullnodes backup to the same GCS bucket, they generate identical blob names for the same epoch, causing the second upload to silently overwrite the first. This results in non-deterministic data loss and potential restore failures.

## Finding Description

The indexer table-info backup system uses `generate_blob_name()` to create blob names for snapshots: [1](#0-0) 

This function generates filenames in the format `files/chain_id_{chain_id}_epoch_{epoch}.tar.gz` using ONLY the chain ID and epoch number. There is no node identifier, timestamp, UUID, or any other unique component.

When `backup_db_snapshot_and_update_metadata()` uploads a snapshot, it uses this non-unique filename: [2](#0-1) 

The upload operation uses standard GCS upload without any conditional preconditions: [3](#0-2) 

While there is a check to avoid re-uploading already backed-up epochs: [4](#0-3) 

This check is NOT atomic with the upload operation, creating a classic Time-of-Check-Time-of-Use (TOCTOU) race condition. The developers acknowledge this limitation: [5](#0-4) 

**Attack Scenario:**

1. Organization deploys two indexer fullnodes (Node A and Node B) for high availability, both configured to backup to `gs://shared-table-info-backup`
2. Both nodes process blockchain transactions and detect epoch 100 boundary around the same time
3. Node A: Checks metadata → sees epoch 99 → creates snapshot for epoch 100
4. Node B: Checks metadata → sees epoch 99 → creates snapshot for epoch 100 (race window)
5. Node A: Uploads `files/chain_id_1_epoch_100.tar.gz` → Success
6. Node A: Updates metadata to epoch 100
7. Node B: Uploads `files/chain_id_1_epoch_100.tar.gz` → **SILENTLY OVERWRITES Node A's snapshot**
8. Node B: Updates metadata to epoch 100 (already at 100, no error)

Result: Node A's snapshot is permanently lost. Which node's snapshot survives is unpredictable and depends on network timing.

## Impact Explanation

This vulnerability causes:

1. **Data Loss**: Backup snapshots are permanently lost when overwritten
2. **Non-Deterministic Behavior**: Which snapshot survives depends on race timing
3. **Silent Failure**: No error is reported when overwrites occur
4. **Restore Unreliability**: Restored data may come from an unexpected node, potentially with slight variations due to timing differences at epoch boundaries
5. **Operational Risk**: Multi-region HA deployments become unreliable

According to Aptos bug bounty criteria, this qualifies as **High Severity** because it affects operational reliability of critical indexer infrastructure that serves API queries. While not affecting core consensus, the indexer table-info system is essential for dApp functionality, and data loss in this component can cause "Significant protocol violations" in the indexer backup/restore protocol and requires "intervention" to recover.

## Likelihood Explanation

**Likelihood: High**

The vulnerability is highly likely to occur in production deployments because:

1. **Common Configuration**: Multi-node HA deployments are standard practice for production systems
2. **Natural Race Condition**: Nodes processing the same blockchain naturally reach epoch boundaries within seconds of each other
3. **No Protection**: The epoch check provides no real protection against concurrent uploads
4. **Silent Failure**: Operators won't detect the issue until a restore is needed
5. **Explicitly Acknowledged**: The TODO comment indicates developers are aware concurrent backups aren't handled

The issue requires no attacker action—it occurs naturally in normal multi-node deployments configured to use the same GCS bucket.

## Recommendation

Implement one of the following solutions:

**Option 1: Add Node Identifier to Blob Names**
```rust
pub fn generate_blob_name(chain_id: u64, epoch: u64, node_id: &str) -> String {
    format!(
        "{}/chain_id_{}_epoch_{}_node_{}.tar.gz",
        FILE_FOLDER_NAME, chain_id, epoch, node_id
    )
}
```
Modify configuration to include a unique node identifier (hostname, UUID, etc.) and pass it to `generate_blob_name()`.

**Option 2: Use GCS Conditional Preconditions**
```rust
.upload_streamed_object(
    &UploadObjectRequest {
        bucket: self.bucket_name.clone(),
        if_generation_match: Some(0), // Only succeed if object doesn't exist
        ..Default::default()
    },
    // ... rest of upload
)
```
This prevents overwrites entirely. The second node's upload would fail with a precondition error, which can be handled gracefully.

**Option 3: Single Designated Backup Node**
Document that only ONE node per chain_id should be configured in `Backup` mode. Add runtime checks to verify no other node is actively backing up (e.g., using distributed locks in the metadata file).

**Recommended: Option 2** provides the strongest guarantee with minimal configuration changes.

## Proof of Concept

```rust
// Reproduction steps (pseudo-code for clarity):

async fn reproduce_race_condition() {
    let bucket = "test-backup-bucket";
    let chain_id = 1u64;
    let epoch = 100u64;
    
    // Simulate two nodes
    let operator_a = GcsBackupRestoreOperator::new(bucket.to_string()).await;
    let operator_b = GcsBackupRestoreOperator::new(bucket.to_string()).await;
    
    // Both create snapshots
    let snapshot_a = create_test_snapshot("node_a_data");
    let snapshot_b = create_test_snapshot("node_b_data");
    
    // Concurrent uploads (in reality, this happens naturally at epoch boundaries)
    let handle_a = tokio::spawn(async move {
        operator_a.backup_db_snapshot_and_update_metadata(
            chain_id, epoch, snapshot_a
        ).await
    });
    
    let handle_b = tokio::spawn(async move {
        operator_b.backup_db_snapshot_and_update_metadata(
            chain_id, epoch, snapshot_b
        ).await
    });
    
    // Both succeed, but one overwrites the other
    handle_a.await.unwrap().unwrap();
    handle_b.await.unwrap().unwrap();
    
    // Verify: Download the snapshot
    let restored = operator_a.restore_db_snapshot(chain_id, /* ... */).await;
    
    // ISSUE: Which node's data did we get? Node A's or Node B's?
    // Node A's snapshot is lost forever if Node B uploaded second.
    assert!(/* Cannot determine which snapshot survived */);
}
```

To demonstrate in a real environment:
1. Deploy two indexer fullnodes with identical `table_info_service_mode: Backup("shared-bucket")`
2. Let both process the same chain to an epoch boundary
3. Check GCS bucket - only one snapshot exists despite two nodes uploading
4. Check logs - both nodes report "Successfully uploaded snapshot" with no errors
5. Attempt restore - result is non-deterministic based on upload timing

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/mod.rs (L15-20)
```rust
pub fn generate_blob_name(chain_id: u64, epoch: u64) -> String {
    format!(
        "{}/chain_id_{}_epoch_{}.tar.gz",
        FILE_FOLDER_NAME, chain_id, epoch
    )
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L217-217)
```rust
        let filename = generate_blob_name(chain_id, epoch);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L223-237)
```rust
        match self
            .gcs_client
            .upload_streamed_object(
                &UploadObjectRequest {
                    bucket: self.bucket_name.clone(),
                    ..Default::default()
                },
                file_stream,
                &UploadType::Simple(Media {
                    name: filename.clone().into(),
                    content_type: Borrowed(TAR_FILE_TYPE),
                    content_length: None,
                }),
            )
            .await
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L576-585)
```rust
        if metadata.epoch >= epoch {
            aptos_logger::info!(
                epoch = epoch,
                snapshot_folder_name = snapshot_folder_name,
                "[Table Info] Snapshot already backed up. Skipping the backup."
            );
            // Remove the snapshot directory.
            std::fs::remove_dir_all(snapshot_dir).unwrap();
            return;
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L599-599)
```rust
    // TODO: add checks to handle concurrent backup jobs.
```
