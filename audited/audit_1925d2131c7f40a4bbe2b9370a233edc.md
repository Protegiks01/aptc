# Audit Report

## Title
DKG Epoch Manager Panic on Shutdown Leads to Node Liveness Failure

## Summary
The `shutdown_current_processor()` function in the DKG epoch manager uses `.unwrap()` when sending shutdown signals to the DKGManager task. If the DKGManager task has already terminated (due to panic, resource exhaustion, or other failures), the unwrap will panic, crashing the entire EpochManager and preventing the node from processing any future epoch changes. [1](#0-0) 

## Finding Description

The DKG epoch manager spawns a DKGManager task to handle distributed key generation for each epoch. When a new epoch begins, the epoch manager must cleanly shut down the previous DKGManager before starting a new one. [2](#0-1) 

The shutdown is coordinated through a oneshot channel. However, the shutdown code uses `.unwrap()` on both the send and await operations, which will panic if the DKGManager task has already terminated: [1](#0-0) 

**Attack Scenario:**

1. The DKGManager task terminates unexpectedly due to:
   - Internal panic (e.g., RNG initialization failure at line 328 of dkg_manager/mod.rs) [3](#0-2) 
   - Resource exhaustion (OOM, tokio runtime limits)
   - Unhandled errors in event processing

2. When the DKGManager task exits, it drops the `close_rx` oneshot receiver

3. On the next epoch change, `on_new_epoch()` calls `shutdown_current_processor()` [4](#0-3) 

4. The `tx.send(ack_tx).unwrap()` call attempts to send through a oneshot channel whose receiver has been dropped, causing it to return `Err`

5. The `.unwrap()` panics, crashing the entire EpochManager task

6. The node can no longer process epoch changes and becomes non-functional

The error handling in the main event loop only catches `Result::Err`, not panics, so the panic propagates and crashes the EpochManager: [5](#0-4) 

**Critical Evidence of Bug:**

The nearly identical JWK consensus epoch manager correctly handles this scenario by ignoring send errors rather than using `.unwrap()`: [6](#0-5) 

This inconsistency confirms that the DKG implementation contains a defensive programming error that was correctly handled in the JWK implementation.

## Impact Explanation

**Severity: High**

This vulnerability causes a **liveness failure** that prevents the affected validator node from processing epoch changes. Once the EpochManager crashes, the node cannot:
- Transition to new epochs
- Participate in consensus for future epochs
- Process validator set changes
- Update on-chain configuration

According to the Aptos bug bounty criteria, this qualifies as **High Severity** due to:
- "Validator node slowdowns" - The node becomes completely non-functional for epoch processing
- "Significant protocol violations" - Breaks the invariant that nodes must be able to process epoch transitions

While this affects individual nodes rather than the entire network, multiple nodes experiencing this issue simultaneously could impact network stability and consensus participation.

## Likelihood Explanation

**Likelihood: Medium**

The likelihood is medium because:

**Triggers:**
- DKGManager contains at least one `.unwrap()` that can panic under rare RNG failure conditions
- Resource exhaustion (OOM, file descriptor limits) can terminate tasks unexpectedly
- Bugs in event handling code could cause panics
- System-level issues (tokio runtime problems, hardware failures)

**Realistic Scenarios:**
- High-load periods causing resource exhaustion
- Edge cases in DKG protocol handling causing panics
- Node restarts or crashes leaving DKGManager in inconsistent state
- Multiple rapid epoch changes causing race conditions

Once triggered, the vulnerability deterministically causes the EpochManager to crash on the next epoch change, making the node non-functional until manual restart.

## Recommendation

Replace the `.unwrap()` calls with proper error handling that ignores send failures, matching the pattern used in the JWK consensus epoch manager:

```rust
async fn shutdown_current_processor(&mut self) {
    if let Some(tx) = self.dkg_manager_close_tx.take() {
        let (ack_tx, ack_rx) = oneshot::channel();
        let _ = tx.send(ack_tx);  // Ignore error if receiver dropped
        let _ = ack_rx.await;      // Ignore error if no response
    }
}
```

This defensive approach ensures that even if the DKGManager has terminated unexpectedly, the EpochManager can continue to function and process epoch changes. The `.take()` pattern ensures that this function remains idempotent for multiple calls.

**Additional hardening:**
Consider adding logging when send failures occur to aid debugging:
```rust
async fn shutdown_current_processor(&mut self) {
    if let Some(tx) = self.dkg_manager_close_tx.take() {
        let (ack_tx, ack_rx) = oneshot::channel();
        if tx.send(ack_tx).is_err() {
            warn!("DKGManager already terminated during shutdown");
        }
        let _ = ack_rx.await;
    }
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_shutdown_after_dkg_manager_crash() {
    use futures_channel::oneshot;
    use tokio::time::{sleep, Duration};
    
    // Simulate the DKGManager task terminating early
    let (close_tx, close_rx) = oneshot::channel::<oneshot::Sender<()>>();
    
    // Spawn a DKGManager-like task that immediately drops the receiver
    tokio::spawn(async move {
        let _close_rx = close_rx; // Receiver held briefly
        sleep(Duration::from_millis(10)).await;
        // Task exits here, dropping close_rx
    });
    
    // Wait for task to terminate
    sleep(Duration::from_millis(50)).await;
    
    // Now attempt shutdown like epoch_manager does
    // This will panic with the current implementation
    let (ack_tx, ack_rx) = oneshot::channel();
    
    // This send will fail because close_rx was dropped
    let send_result = close_tx.send(ack_tx);
    assert!(send_result.is_err(), "Send should fail when receiver dropped");
    
    // In the current implementation, .unwrap() here would panic
    // causing EpochManager crash and liveness failure
    
    // With the fix (let _ = ...), this would be handled gracefully
    let _ = send_result; // Fixed version
}
```

This test demonstrates that when the DKGManager task terminates and drops its `close_rx` receiver, attempting to send through the `close_tx` oneshot sender returns an error. The current code's `.unwrap()` would panic at this point, while the recommended fix handles it gracefully.

### Citations

**File:** dkg/src/epoch_manager.rs (L125-144)
```rust
    pub async fn start(mut self, mut network_receivers: NetworkReceivers) {
        self.await_reconfig_notification().await;
        loop {
            let handling_result = tokio::select! {
                notification = self.dkg_start_events.select_next_some() => {
                    self.on_dkg_start_notification(notification)
                },
                reconfig_notification = self.reconfig_events.select_next_some() => {
                    self.on_new_epoch(reconfig_notification).await
                },
                (peer, rpc_request) = network_receivers.rpc_rx.select_next_some() => {
                    self.process_rpc_request(peer, rpc_request)
                },
            };

            if let Err(e) = handling_result {
                error!("{}", e);
            }
        }
    }
```

**File:** dkg/src/epoch_manager.rs (L253-258)
```rust
            tokio::spawn(dkg_manager.run(
                in_progress_session,
                dkg_start_event_rx,
                dkg_rpc_msg_rx,
                dkg_manager_close_rx,
            ));
```

**File:** dkg/src/epoch_manager.rs (L263-268)
```rust
    async fn on_new_epoch(&mut self, reconfig_notification: ReconfigNotification<P>) -> Result<()> {
        self.shutdown_current_processor().await;
        self.start_new_epoch(reconfig_notification.on_chain_configs)
            .await?;
        Ok(())
    }
```

**File:** dkg/src/epoch_manager.rs (L270-276)
```rust
    async fn shutdown_current_processor(&mut self) {
        if let Some(tx) = self.dkg_manager_close_tx.take() {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ack_tx).unwrap();
            ack_rx.await.unwrap();
        }
    }
```

**File:** dkg/src/dkg_manager/mod.rs (L325-329)
```rust
        let mut rng = if cfg!(feature = "smoke-test") {
            StdRng::from_seed(self.my_addr.into_bytes())
        } else {
            StdRng::from_rng(thread_rng()).unwrap()
        };
```

**File:** crates/aptos-jwk-consensus/src/epoch_manager.rs (L266-274)
```rust
    async fn shutdown_current_processor(&mut self) {
        if let Some(tx) = self.jwk_manager_close_tx.take() {
            let (ack_tx, ack_rx) = oneshot::channel();
            let _ = tx.send(ack_tx);
            let _ = ack_rx.await;
        }

        self.jwk_updated_event_txs = None;
    }
```
