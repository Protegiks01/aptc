# Audit Report

## Title
Peer Score Reset on Reconnection Enables Persistent Malicious Data Serving to State Sync

## Summary
A malicious peer can exploit the `STARTING_SCORE=50.0` initialization to repeatedly bypass reputation penalties by disconnecting and reconnecting, allowing them to continuously serve invalid data to a node's state sync system and waste verification resources.

## Finding Description

The peer scoring system in `peer_states.rs` assigns each peer a score between 0-100 to track their reliability. When a peer serves malicious data (e.g., invalid cryptographic proofs), their score is multiplied by `MALICIOUS_MULTIPLIER=0.8`. Once a peer's score drops below `IGNORE_PEER_THRESHOLD=25.0`, they are ignored by the data client. [1](#0-0) 

However, when a peer disconnects, their state is removed during garbage collection: [2](#0-1) 

When the same peer reconnects, a new `PeerState` is created with `STARTING_SCORE=50.0`: [3](#0-2) [4](#0-3) 

**Attack Path:**
1. Malicious peer connects with score 50.0
2. Peer serves data with invalid proofs, triggering `ProofVerificationError`
3. Each malicious response: score × 0.8 (50→40→32→25.6)
4. After 3-4 malicious responses, score drops below 25.0 and peer is ignored
5. Peer disconnects, triggering garbage collection that removes their state
6. Peer reconnects, receives fresh `STARTING_SCORE=50.0`
7. Repeat steps 2-6 indefinitely

The proof verification feedback mechanism confirms this path: [5](#0-4) [6](#0-5) 

The test suite confirms that peer states are garbage collected on disconnect and recreated on reconnect: [7](#0-6) 

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program for the following reasons:

1. **Validator Node Slowdowns**: Each malicious response requires CPU-intensive proof verification before detection. The node wastes resources on 3-4 verification attempts per connection cycle, and the attacker can repeat this indefinitely by reconnecting.

2. **State Sync Disruption**: While the node eventually syncs (proofs are verified before commit, preventing data corruption), a malicious peer can significantly delay state sync progress for an individual node by forcing repeated verification failures.

3. **Significant Protocol Violation**: The peer reputation system is designed to isolate bad actors, but this vulnerability allows malicious peers to completely bypass reputation history by exploiting the score reset mechanism.

The impact is localized to individual nodes (not network-wide), but any node connecting to the malicious peer is affected. The attack does not cause fund loss, consensus violations, or permanent network issues, placing it in the High (not Critical) severity category.

## Likelihood Explanation

This vulnerability is **highly likely** to be exploited because:

1. **Low Attack Complexity**: The attacker only needs to connect to a victim node via the public network, send invalid data, and cycle connections. No special privileges or complex timing is required.

2. **Easy Automation**: The attack can be fully automated with a simple script that connects, sends invalid proofs, waits for score drop, disconnects, and reconnects.

3. **Difficult Detection**: Without persistent peer reputation tracking, operators cannot easily distinguish between a malicious peer cycling connections and a legitimate peer experiencing network issues.

4. **No Rate Limiting**: There is no persistent ban list or connection rate limiting to prevent the same peer from repeatedly reconnecting after being ignored.

## Recommendation

Implement persistent peer reputation tracking that survives disconnections. Options include:

**Option 1: Time-based reputation decay** (Recommended)
- Store peer reputation in a persistent cache (not removed on disconnect)
- Implement time-based decay: scores gradually improve over hours/days if peer doesn't reconnect
- Add maximum cache size with LRU eviction to prevent memory exhaustion
- Configure reasonable decay rate (e.g., +1 point per hour)

**Option 2: Connection-based penalties**
- Track repeated connect/disconnect cycles within a time window
- Penalize peers that reconnect too frequently after being ignored
- Implement exponential backoff for reconnection attempts

**Implementation sketch for Option 1:**

```rust
// Add to peer_states.rs
struct PersistentPeerReputation {
    score: f64,
    last_updated: Instant,
    disconnect_count: u32,
}

// Modify PeerStates to maintain separate persistent reputation map
// that is NOT garbage collected with regular peer states
// On peer reconnection, initialize score from persistent reputation
// instead of always using STARTING_SCORE
```

Additionally, add configuration options:
- `enable_persistent_peer_reputation`: Enable persistent tracking
- `reputation_decay_rate_per_hour`: Score recovery rate for disconnected peers
- `max_reputation_cache_size`: Maximum entries in persistent cache

## Proof of Concept

```rust
#[tokio::test]
async fn test_score_reset_on_reconnection_exploit() {
    // Create data client with peer ignoring enabled
    let data_client_config = AptosDataClientConfig {
        ignore_low_score_peers: true,
        ..Default::default()
    };
    let (mut mock_network, _, client, poller) =
        MockNetwork::new(None, Some(data_client_config), None);

    // Add a malicious peer
    let malicious_peer = mock_network.add_peer(PeerPriority::HighPriority);
    
    // Advertise data for the peer
    let storage_summary = utils::create_storage_summary(100);
    client.update_peer_storage_summary(malicious_peer, storage_summary.clone());
    client.update_global_summary_cache().unwrap();

    // Spawn handler that serves invalid data
    tokio::spawn(async move {
        while let Some(network_request) = mock_network.next_request(malicious_peer.network_id()).await {
            let data_response = DataResponse::TransactionsWithProof(TransactionListWithProof::new_empty());
            network_request.response_sender.send(Ok(StorageServiceResponse::new(data_response, true).unwrap()));
        }
    });

    // Cycle 1: Send 4 requests with bad feedback, verify peer gets ignored
    for _ in 0..4 {
        let response = client.get_transactions_with_proof(100, 0, 100, false, 1000).await;
        if let Ok(response) = response {
            response.context.response_callback.notify_bad_response(ResponseError::ProofVerificationError);
        }
    }
    
    // Verify peer is now ignored (score below 25.0)
    client.update_global_summary_cache().unwrap();
    let global_summary = client.get_global_data_summary();
    assert!(global_summary.advertised_data.transactions.is_empty()); // Peer ignored

    // Disconnect peer (triggers garbage collection)
    mock_network.disconnect_peer(malicious_peer);
    client.update_global_summary_cache().unwrap();
    
    // Reconnect peer and poll to recreate state
    mock_network.reconnect_peer(malicious_peer);
    poll_peers(&mut mock_network, &poller, PeerPriority::HighPriority, hashset![malicious_peer]).await;
    client.update_peer_storage_summary(malicious_peer, storage_summary);
    client.update_global_summary_cache().unwrap();

    // Cycle 2: Verify peer can serve 3-4 more malicious responses before being ignored again
    let mut successful_malicious_responses = 0;
    for _ in 0..4 {
        let response = client.get_transactions_with_proof(100, 0, 100, false, 1000).await;
        if let Ok(response) = response {
            response.context.response_callback.notify_bad_response(ResponseError::ProofVerificationError);
            successful_malicious_responses += 1;
        }
    }
    
    // Assert that the peer successfully served multiple malicious responses after reconnection
    assert!(successful_malicious_responses >= 3, 
        "Peer should be able to serve ~3-4 malicious responses after score reset on reconnection");
}
```

## Notes

The vulnerability is confirmed by examining the garbage collection test which demonstrates that peer states are completely removed on disconnection and recreated on reconnection, with no mechanism to persist reputation history. While this design choice may be intentional for memory management, it creates an exploitable security vulnerability that allows malicious peers to bypass the reputation system indefinitely through connection cycling.

### Citations

**File:** state-sync/aptos-data-client/src/peer_states.rs (L33-43)
```rust
const MAX_SCORE: f64 = 100.0;
const MIN_SCORE: f64 = 0.0;
const STARTING_SCORE: f64 = 50.0;
/// Add this score on a successful response.
const SUCCESSFUL_RESPONSE_DELTA: f64 = 1.0;
/// Not necessarily a malicious response, but not super useful.
const NOT_USEFUL_MULTIPLIER: f64 = 0.95;
/// Likely to be a malicious response.
const MALICIOUS_MULTIPLIER: f64 = 0.8;
/// Ignore a peer when their score dips below this threshold.
const IGNORE_PEER_THRESHOLD: f64 = 25.0;
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L54-62)
```rust
impl From<ResponseError> for ErrorType {
    fn from(error: ResponseError) -> Self {
        match error {
            ResponseError::InvalidData | ResponseError::InvalidPayloadDataType => {
                ErrorType::NotUseful
            },
            ResponseError::ProofVerificationError => ErrorType::Malicious,
        }
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L85-93)
```rust
    pub fn new(data_client_config: Arc<AptosDataClientConfig>) -> Self {
        Self {
            data_client_config,
            received_responses_by_type: Arc::new(DashMap::new()),
            sent_requests_by_type: Arc::new(DashMap::new()),
            storage_summary: None,
            score: STARTING_SCORE,
        }
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L325-330)
```rust
    pub fn update_summary(&self, peer: PeerNetworkId, storage_summary: StorageServerSummary) {
        self.peer_to_state
            .entry(peer)
            .or_insert(PeerState::new(self.data_client_config.clone()))
            .update_storage_summary(storage_summary);
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L333-336)
```rust
    pub fn garbage_collect_peer_states(&self, connected_peers: HashSet<PeerNetworkId>) {
        self.peer_to_state
            .retain(|peer_network_id, _| connected_peers.contains(peer_network_id));
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L1383-1394)
```rust
fn extract_response_error(
    notification_feedback: &NotificationFeedback,
) -> Result<ResponseError, Error> {
    match notification_feedback {
        NotificationFeedback::InvalidPayloadData => Ok(ResponseError::InvalidData),
        NotificationFeedback::PayloadTypeIsIncorrect => Ok(ResponseError::InvalidPayloadDataType),
        NotificationFeedback::PayloadProofFailed => Ok(ResponseError::ProofVerificationError),
        _ => Err(Error::UnexpectedErrorEncountered(format!(
            "Invalid notification feedback given: {:?}",
            notification_feedback
        ))),
    }
```

**File:** state-sync/aptos-data-client/src/tests/peers.rs (L493-513)
```rust
        // Disconnect peer 1 and update the global data summary
        mock_network.disconnect_peer(peer_1);
        client.update_global_summary_cache().unwrap();

        // Verify we have peer states for only peer 2 and 3
        verify_peer_states(&client, hashset![peer_2, peer_3]);

        // Disconnect peer 2 and update the global data summary
        mock_network.disconnect_peer(peer_2);
        client.update_global_summary_cache().unwrap();

        // Verify we have peer states for only peer 3
        verify_peer_states(&client, hashset![peer_3]);

        // Reconnect peer 1, poll it and update the global data summary
        mock_network.reconnect_peer(peer_1);
        poll_peers(&mut mock_network, &poller, peer_priority, hashset![peer_1]).await;
        client.update_global_summary_cache().unwrap();

        // Verify we have peer states for peers 1 and 3
        verify_peer_states(&client, hashset![peer_1, peer_3]);
```
