# Audit Report

## Title
Non-Atomic Multi-Shard Write in Jellyfish Merkle Tree Restoration Causes State Corruption on Partial Failure

## Summary
The `finish_impl()` function in the Jellyfish Merkle tree restoration process writes all frozen nodes (including the root) via `write_node_batch()`, which performs non-atomic sequential writes across 16 shards. If any shard write fails after others have succeeded, the tree is left in a corrupted state where internal nodes reference non-existent children, violating the tree's structural integrity invariant.

## Finding Description
The vulnerability exists in the state restoration logic for Jellyfish Merkle trees. When `finish_impl()` is called to complete the restoration process, it freezes all remaining partial nodes (including the root) and writes them to storage in a single batch. [1](#0-0) 

The `write_node_batch()` implementation in `StateMerkleDb` splits nodes by shard based on their nibble path prefix and writes them sequentially: [2](#0-1) 

The underlying `commit_no_progress()` function writes shards sequentially without transaction isolation across shards: [3](#0-2) 

**The Critical Issue**: Each `write_schemas()` call is atomic within its shard, but the sequence of shard writes is NOT atomic. If shard N fails after shards 0 through N-1 have been committed, those committed writes cannot be rolled back.

**Corruption Scenario**:
1. During `finish_impl()`, all partial nodes are frozen, including internal node I2 with children L3 and L4
2. Nodes are distributed across shards: L3→shard 3, I2→shard 5, L4→shard 7, Root→top-level
3. Sequential writes execute: shard 3 (L3) succeeds, shard 5 (I2) succeeds, shard 7 (L4) **fails**
4. Storage now contains I2 with hash H(L3, L4), but L4 is missing
5. On recovery, the system finds rightmost leaf L3, reconstructs partial nodes, and finds I2 as a "complete" child with its stored hash
6. **Result**: I2's hash assumes L4 exists, but L4 is absent from storage—the tree structure is corrupted [4](#0-3) 

The recovery mechanism cannot detect this corruption because it trusts hashes of nodes loaded from storage.

## Impact Explanation
This vulnerability falls under **Medium Severity** per Aptos bug bounty criteria:
- **State inconsistencies requiring intervention**: The corrupted tree cannot verify Merkle proofs correctly and may cause node failures when attempting to traverse to missing children
- **Not Critical** because: it requires I/O/hardware failures to trigger (not directly exploitable by attackers), and the corruption is limited to the affected node (doesn't violate network-wide consensus)
- **Not High** because: it doesn't cause validator slowdowns or API crashes unless the corrupted state is actively used

The impact is limited but real: nodes experiencing such corruption during state sync would need manual intervention to recover, potentially causing downtime.

## Likelihood Explanation
**Moderate likelihood** in production environments:
- I/O failures during state sync are not uncommon (disk full, hardware errors, process crashes)
- State sync happens frequently as new nodes join or existing nodes catch up
- The multi-shard write amplifies failure probability (16+ sequential write points)
- However, requires actual I/O/hardware failure—not maliciously triggerable without physical access

## Recommendation
Implement atomic write semantics across all shards using a two-phase commit protocol or batch all shard writes into a single atomic operation:

**Option 1**: Use RocksDB transactions to make multi-shard writes atomic
**Option 2**: Implement a write-ahead log with rollback capability
**Option 3**: Write all nodes to a staging area first, then atomically commit all shards

The fix should ensure that either ALL frozen nodes are written successfully, or NONE are written (with automatic rollback on any failure).

## Proof of Concept

```rust
// Rust test demonstrating the corruption scenario
#[test]
fn test_partial_write_corruption() {
    // Setup: Create a MockTreeStore that simulates shard write failure
    let store = Arc::new(MockFailingTreeStore::new(
        fail_on_shard: Some(7), // Simulate failure on shard 7
    ));
    
    let version = 0;
    let expected_root_hash = HashValue::random();
    
    // Create restorer and add chunks
    let mut restorer = JellyfishMerkleRestore::new(
        store.clone(), 
        version, 
        expected_root_hash,
        false
    ).unwrap();
    
    // Add leaves that will distribute across multiple shards
    // L3 goes to shard 3, I2 goes to shard 5, L4 goes to shard 7
    restorer.add_chunk_impl(chunks, proof).unwrap();
    
    // Attempt to finish - this should fail at shard 7
    let result = restorer.finish_impl();
    assert!(result.is_err());
    
    // Verify corruption: I2 exists but L4 doesn't
    let i2_key = NodeKey::new(version, nibble_path_for_shard_5);
    let l4_key = NodeKey::new(version, nibble_path_for_shard_7);
    
    assert!(store.get_node_option(&i2_key, "test").unwrap().is_some());
    assert!(store.get_node_option(&l4_key, "test").unwrap().is_none());
    
    // I2's hash assumes L4 exists - this is the corruption
    let i2_node = store.get_node_option(&i2_key, "test").unwrap().unwrap();
    if let Node::Internal(internal) = i2_node {
        // Hash was computed with L4 as a child, but L4 is missing
        assert!(internal.hash() != compute_correct_hash_without_l4());
    }
}
```

**Notes**:
- This vulnerability violates invariant #4: "State Consistency: State transitions must be atomic and verifiable via Merkle proofs"
- The corruption is detectable if root hash verification is enforced, but causes node failure rather than silent corruption
- The issue affects only nodes experiencing I/O failures during state sync, not the network consensus layer
- Fix requires architectural changes to ensure atomicity across distributed shard writes

### Citations

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L267-310)
```rust
    fn recover_partial_nodes(
        store: &dyn TreeReader<K>,
        version: Version,
        rightmost_leaf_node_key: NodeKey,
    ) -> Result<Vec<InternalInfo<K>>> {
        ensure!(
            !rightmost_leaf_node_key.nibble_path().is_empty(),
            "Root node would not be written until entire restoration process has completed \
             successfully.",
        );

        // Start from the parent of the rightmost leaf. If this internal node exists in storage, it
        // is not a partial node. Go to the parent node and repeat until we see a node that does
        // not exist. This node and all its ancestors will be the partial nodes.
        let mut node_key = rightmost_leaf_node_key.gen_parent_node_key();
        while store.get_node_option(&node_key, "restore")?.is_some() {
            node_key = node_key.gen_parent_node_key();
        }

        // Next we reconstruct all the partial nodes up to the root node, starting from the bottom.
        // For all of them, we scan all its possible child positions and see if there is one at
        // each position. If the node is not the bottom one, there is additionally a partial node
        // child at the position `previous_child_index`.
        let mut partial_nodes = vec![];
        // Initialize `previous_child_index` to `None` for the first iteration of the loop so the
        // code below treats it differently.
        let mut previous_child_index = None;

        loop {
            let mut internal_info = InternalInfo::new_empty(node_key.clone());

            for i in 0..previous_child_index.unwrap_or(16) {
                let child_node_key = node_key.gen_child_node_key(version, (i as u8).into());
                if let Some(node) = store.get_node_option(&child_node_key, "restore")? {
                    let child_info = match node {
                        Node::Internal(internal_node) => ChildInfo::Internal {
                            hash: Some(internal_node.hash()),
                            leaf_count: Some(internal_node.leaf_count()),
                        },
                        Node::Leaf(leaf_node) => ChildInfo::Leaf(leaf_node),
                        Node::Null => unreachable!("Child cannot be Null"),
                    };
                    internal_info.set_child(i, child_info);
                }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L786-788)
```rust
        self.freeze(0);
        self.store.write_node_batch(&self.frozen_nodes)?;
        Ok(())
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L174-190)
```rust
    pub(crate) fn commit_no_progress(
        &self,
        top_level_batch: SchemaBatch,
        batches_for_shards: Vec<SchemaBatch>,
    ) -> Result<()> {
        ensure!(
            batches_for_shards.len() == NUM_STATE_SHARDS,
            "Shard count mismatch."
        );
        let mut batches = batches_for_shards.into_iter();
        for shard_id in 0..NUM_STATE_SHARDS {
            let state_merkle_batch = batches.next().unwrap();
            self.state_merkle_db_shards[shard_id].write_schemas(state_merkle_batch)?;
        }

        self.state_merkle_metadata_db.write_schemas(top_level_batch)
    }
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L918-932)
```rust
    fn write_node_batch(&self, node_batch: &NodeBatch) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["tree_writer_write_batch"]);
        // Get the top level batch and sharded batch from raw NodeBatch
        let mut top_level_batch = SchemaBatch::new();
        let mut jmt_shard_batches: Vec<SchemaBatch> = Vec::with_capacity(NUM_STATE_SHARDS);
        jmt_shard_batches.resize_with(NUM_STATE_SHARDS, SchemaBatch::new);
        node_batch.iter().try_for_each(|(node_key, node)| {
            if let Some(shard_id) = node_key.get_shard_id() {
                jmt_shard_batches[shard_id].put::<JellyfishMerkleNodeSchema>(node_key, node)
            } else {
                top_level_batch.put::<JellyfishMerkleNodeSchema>(node_key, node)
            }
        })?;
        self.commit_no_progress(top_level_batch, jmt_shard_batches)
    }
```
