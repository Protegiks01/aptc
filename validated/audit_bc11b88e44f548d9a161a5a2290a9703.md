# Audit Report

## Title
Malicious Validator Can Cause Permanent Liveness Failure via Invalid Block Data in Decoupled Execution Mode

## Summary
A Byzantine validator can exploit the decoupled execution architecture to cause permanent network-wide liveness failure. By proposing a structurally valid block with payload that fails execution, the block becomes stuck in "Ordered" state and can never be committed, halting all consensus progress. The vulnerability is exploitable in the default configuration where `discard_failed_blocks = false`.

## Finding Description

The vulnerability exploits a critical flaw in how Aptos handles execution failures in decoupled execution mode. The attack flow is as follows:

**1. Decoupled Execution Architecture:**
Aptos hardcodes `decoupled_execution` to always return `true`, meaning validators vote on block ordering before execution completes. [1](#0-0) 

When decoupled execution is enabled, validators generate vote data using `ACCUMULATOR_PLACEHOLDER_HASH` as a dummy executed state ID rather than waiting for actual execution results. [2](#0-1) 

**2. Preliminary Validation Only:**
The `process_proposal` function performs only structural validation (proposer validity, transaction limits, timestamp checks) without executing transactions. [3](#0-2) 

**3. Execution Failure Handling:**
When a block fails execution, the default configuration has `discard_failed_blocks = false`, causing execution errors to propagate. [4](#0-3) 

The `process_execution_response` function receives the error, logs it, and returns early without advancing the block's state from "Ordered" to "Executed". [5](#0-4) 

**4. Aggregation Impossibility:**
The `try_advance_to_aggregated_with_ledger_info` function explicitly cannot advance an "Ordered" block to "Aggregated" state when execution hasn't completed. The code contains the comment "can't aggregate it without execution, only store the signatures" and keeps the block in Ordered state. [6](#0-5) 

**5. No Automatic Retry:**
The `advance_execution_root` function returns a retry signal when the execution root hasn't advanced, but all call sites ignore this return value, preventing any automatic retry. [7](#0-6) [8](#0-7) [9](#0-8) 

**6. Commit Blockage:**
The `advance_head` function is only called when blocks successfully reach "Aggregated" state. Since failed blocks never aggregate, `advance_head` is never invoked for them, permanently blocking all subsequent commits. [10](#0-9) [11](#0-10) 

**7. No Automatic Recovery:**
There is no timeout or automatic cleanup mechanism for stuck blocks. The only automatic reset occurs at epoch boundaries, which doesn't help if the network cannot progress to commit blocks. [12](#0-11) 

## Impact Explanation

This vulnerability achieves **Critical Severity** under the Aptos bug bounty category "Total Loss of Liveness/Network Availability":

- **Complete Network Halt**: Once triggered, the entire network cannot commit any new blocks. All validators process the same ordered block that fails execution, causing network-wide impact.

- **Non-Recoverable Without Manual Intervention**: The default configuration provides no automatic recovery. The mitigation feature `discard_failed_blocks` exists but is explicitly disabled by default. [13](#0-12) 

A smoke test confirms this behavior - when execution failures occur with `discard_failed_blocks = false`, the chain stops progressing and cannot reach subsequent versions. [14](#0-13) 

Recovery requires coordinated manual configuration changes across all validators to enable `discard_failed_blocks = true` and restart nodes, or potentially a hard fork.

- **Maximum Impact**: A single Byzantine validator (within BFT < 1/3 assumption) can execute the attack during their proposal turn. All honest validators are affected simultaneously. No transactions can be committed network-wide, making user funds inaccessible. The attack is repeatable.

## Likelihood Explanation

**High Likelihood** due to:

1. **Low Attack Complexity**: The attacker only needs to be a validator in rotation and craft block data that fails execution while passing preliminary structural validation.

2. **BFT Threat Model Compliance**: A single Byzantine validator can execute the attack (within the < 1/3 Byzantine tolerance assumption). No collusion required.

3. **Immediate Impact**: The attack takes effect within one consensus round after the malicious block is ordered.

4. **Default Configuration Vulnerable**: The critical `discard_failed_blocks` flag defaults to `false`, making production networks vulnerable unless explicitly configured otherwise.

5. **No Pre-execution Detection**: Decoupled execution means the malicious block passes all preliminary checks and receives votes before execution is attempted, providing no opportunity for rejection before impact.

6. **Realistic Execution Failures**: Blocks can legitimately fail execution due to Move VM errors, BlockSTM invariant violations, or speculative execution aborts, making it feasible for a malicious validator to craft such payloads. [15](#0-14) 

## Recommendation

**Immediate Mitigations:**

1. **Enable discard_failed_blocks by default**: Change the default value to `true` to automatically handle execution failures without halting the network.

2. **Implement automatic retry**: Use the retry signal returned by `advance_execution_root` at all call sites to retry failed execution attempts.

3. **Add execution validation before voting**: In decoupled mode, add a preliminary execution check or execution sampling to detect obviously malicious payloads before voting.

**Long-term Solutions:**

1. **Execution proof verification**: Require proposers to provide execution proofs that can be quickly verified before voting, even in decoupled mode.

2. **Timeout mechanism**: Implement automatic eviction of blocks stuck in Ordered state for extended periods.

3. **Back-pressure enhancement**: Extend the back-pressure mechanism to detect and handle stuck blocks, not just high round backlogs.

## Proof of Concept

The existing smoke test demonstrates this vulnerability. When a failpoint is injected to cause execution failures with `discard_failed_blocks = false`, the chain cannot progress to the next version within the timeout period, confirming that execution failures cause permanent liveness failure in the default configuration. [16](#0-15) 

A malicious validator can trigger this by proposing a block containing transactions with:
- Invalid Move bytecode that passes initial validation but fails VM verification
- Resource group serialization errors
- Delayed field invariant violations
- Speculative execution abort conditions

These cause `FatalVMError` or `FatalBlockExecutorError` during execution, which with `discard_failed_blocks = false` propagate as errors, triggering the vulnerability.

## Notes

This vulnerability represents a critical design flaw in the interaction between decoupled execution and error handling. While decoupled execution improves performance, the lack of proper fallback mechanisms for execution failures creates a severe liveness vulnerability. The existence of the `discard_failed_blocks` feature indicates the Aptos team anticipated this issue, but leaving it disabled by default leaves production networks vulnerable to single Byzantine validators causing network-wide halts.

### Citations

**File:** types/src/on_chain_config/consensus_config.rs (L239-241)
```rust
    pub fn decoupled_execution(&self) -> bool {
        true
    }
```

**File:** consensus/consensus-types/src/vote_proposal.rs (L87-101)
```rust
    /// Generate vote data depends on the config.
    pub fn gen_vote_data(&self) -> anyhow::Result<VoteData> {
        if self.decoupled_execution {
            Ok(self.vote_data_ordering_only())
        } else {
            let proposed_block = self.block();
            let new_tree = self.accumulator_extension_proof().verify(
                proposed_block
                    .quorum_cert()
                    .certified_block()
                    .executed_state_id(),
            )?;
            Ok(self.vote_data_with_extension_proof(&new_tree))
        }
    }
```

**File:** consensus/src/round_manager.rs (L1111-1214)
```rust
    async fn process_proposal(&mut self, proposal: Block) -> anyhow::Result<()> {
        let author = proposal
            .author()
            .expect("Proposal should be verified having an author");

        if !self.vtxn_config.enabled()
            && matches!(
                proposal.block_data().block_type(),
                BlockType::ProposalExt(_)
            )
        {
            counters::UNEXPECTED_PROPOSAL_EXT_COUNT.inc();
            bail!("ProposalExt unexpected while the vtxn feature is disabled.");
        }

        if let Some(vtxns) = proposal.validator_txns() {
            for vtxn in vtxns {
                let vtxn_type_name = vtxn.type_name();
                ensure!(
                    is_vtxn_expected(&self.randomness_config, &self.jwk_consensus_config, vtxn),
                    "unexpected validator txn: {:?}",
                    vtxn_type_name
                );
                vtxn.verify(self.epoch_state.verifier.as_ref())
                    .context(format!("{} verify failed", vtxn_type_name))?;
            }
        }

        let (num_validator_txns, validator_txns_total_bytes): (usize, usize) =
            proposal.validator_txns().map_or((0, 0), |txns| {
                txns.iter().fold((0, 0), |(count_acc, size_acc), txn| {
                    (count_acc + 1, size_acc + txn.size_in_bytes())
                })
            });

        let num_validator_txns = num_validator_txns as u64;
        let validator_txns_total_bytes = validator_txns_total_bytes as u64;
        let vtxn_count_limit = self.vtxn_config.per_block_limit_txn_count();
        let vtxn_bytes_limit = self.vtxn_config.per_block_limit_total_bytes();
        let author_hex = author.to_hex();
        PROPOSED_VTXN_COUNT
            .with_label_values(&[&author_hex])
            .inc_by(num_validator_txns);
        PROPOSED_VTXN_BYTES
            .with_label_values(&[&author_hex])
            .inc_by(validator_txns_total_bytes);
        info!(
            vtxn_count_limit = vtxn_count_limit,
            vtxn_count_proposed = num_validator_txns,
            vtxn_bytes_limit = vtxn_bytes_limit,
            vtxn_bytes_proposed = validator_txns_total_bytes,
            proposer = author_hex,
            "Summarizing proposed validator txns."
        );

        ensure!(
            num_validator_txns <= vtxn_count_limit,
            "process_proposal failed with per-block vtxn count limit exceeded: limit={}, actual={}",
            self.vtxn_config.per_block_limit_txn_count(),
            num_validator_txns
        );
        ensure!(
            validator_txns_total_bytes <= vtxn_bytes_limit,
            "process_proposal failed with per-block vtxn bytes limit exceeded: limit={}, actual={}",
            self.vtxn_config.per_block_limit_total_bytes(),
            validator_txns_total_bytes
        );
        let payload_len = proposal.payload().map_or(0, |payload| payload.len());
        let payload_size = proposal.payload().map_or(0, |payload| payload.size());
        ensure!(
            num_validator_txns + payload_len as u64 <= self.local_config.max_receiving_block_txns,
            "Payload len {} exceeds the limit {}",
            payload_len,
            self.local_config.max_receiving_block_txns,
        );

        ensure!(
            validator_txns_total_bytes + payload_size as u64
                <= self.local_config.max_receiving_block_bytes,
            "Payload size {} exceeds the limit {}",
            payload_size,
            self.local_config.max_receiving_block_bytes,
        );

        ensure!(
            self.proposer_election.is_valid_proposal(&proposal),
            "[RoundManager] Proposer {} for block {} is not a valid proposer for this round or created duplicate proposal",
            author,
            proposal,
        );

        // If the proposal contains any inline transactions that need to be denied
        // (e.g., due to filtering) drop the message and do not vote for the block.
        if let Err(error) = self
            .block_store
            .check_denied_inline_transactions(&proposal, &self.block_txn_filter_config)
        {
            counters::REJECTED_PROPOSAL_DENY_TXN_COUNT.inc();
            bail!(
                "[RoundManager] Proposal for block {} contains denied inline transactions: {}. Dropping proposal!",
                proposal.id(),
                error
            );
        }
```

**File:** config/src/config/execution_config.rs (L78-96)
```rust
impl Default for ExecutionConfig {
    fn default() -> ExecutionConfig {
        ExecutionConfig {
            genesis: None,
            genesis_file_location: PathBuf::new(),
            // use min of (num of cores/2, DEFAULT_CONCURRENCY_LEVEL) as default concurrency level
            concurrency_level: 0,
            num_proof_reading_threads: 32,
            paranoid_type_verification: true,
            paranoid_hot_potato_verification: true,
            discard_failed_blocks: false,
            processed_transactions_detailed_counters: false,
            genesis_waypoint: None,
            blockstm_v2_enabled: false,
            layout_caches_enabled: true,
            // TODO: consider setting to be true by default.
            async_runtime_checks: false,
        }
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L429-452)
```rust
    fn advance_execution_root(&mut self) -> Option<HashValue> {
        let cursor = self.execution_root;
        self.execution_root = self
            .buffer
            .find_elem_from(cursor.or_else(|| *self.buffer.head_cursor()), |item| {
                item.is_ordered()
            });
        if self.execution_root.is_some() && cursor == self.execution_root {
            // Schedule retry.
            self.execution_root
        } else {
            sample!(
                SampleRate::Frequency(2),
                info!(
                    "Advance execution root from {:?} to {:?}",
                    cursor, self.execution_root
                )
            );
            // Otherwise do nothing, because the execution wait phase is driven by the response of
            // the execution schedule phase, which is in turn fed as soon as the ordered blocks
            // come in.
            None
        }
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L530-534)
```rust
                if commit_proof.ledger_info().ends_epoch() {
                    // the epoch ends, reset to avoid executing more blocks, execute after
                    // this persisting request will result in BlockNotFound
                    self.reset().await;
                }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L617-627)
```rust
        let executed_blocks = match inner {
            Ok(result) => result,
            Err(e) => {
                log_executor_error_occurred(
                    e,
                    &counters::BUFFER_MANAGER_RECEIVED_EXECUTOR_ERROR_COUNT,
                    block_id,
                );
                return;
            },
        };
```

**File:** consensus/src/pipeline/buffer_manager.rs (L675-680)
```rust
        let aggregated = new_item.is_aggregated();
        self.buffer.set(&current_cursor, new_item);
        if aggregated {
            self.advance_head(block_id).await;
        }
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L800-806)
```rust
                    let aggregated = new_item.is_aggregated();
                    self.buffer.set(&cursor, new_item);

                    reply_ack(protocol, response_sender);
                    if aggregated {
                        return Some(target_block_id);
                    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L943-943)
```rust
                        self.advance_execution_root();
```

**File:** consensus/src/pipeline/buffer_manager.rs (L957-957)
```rust
                    self.advance_execution_root();
```

**File:** consensus/src/pipeline/buffer_item.rs (L272-286)
```rust
            Self::Ordered(ordered_item) => {
                let ordered = *ordered_item;
                assert!(ordered
                    .ordered_proof
                    .commit_info()
                    .match_ordered_only(commit_proof.commit_info()));
                // can't aggregate it without execution, only store the signatures
                debug!(
                    "{} received commit decision in ordered stage",
                    commit_proof.commit_info()
                );
                Self::Ordered(Box::new(OrderedItem {
                    commit_proof: Some(commit_proof),
                    ..ordered
                }))
```

**File:** testsuite/smoke-test/src/execution.rs (L20-70)
```rust
async fn fallback_test() {
    let swarm = SwarmBuilder::new_local(1)
        .with_init_config(Arc::new(|_, config, _| {
            config.api.failpoints_enabled = true;
            config.execution.discard_failed_blocks = true;
        }))
        .with_aptos()
        .build()
        .await;

    swarm
        .wait_for_all_nodes_to_catchup_to_epoch(2, Duration::from_secs(60))
        .await
        .expect("Epoch 2 taking too long to come!");

    let client = swarm.validators().next().unwrap().rest_client();

    client
        .set_failpoint(
            "aptos_vm::vm_wrapper::execute_transaction".to_string(),
            "100%return".to_string(),
        )
        .await
        .unwrap();

    let version_milestone_0 = get_current_version(&client).await;
    let version_milestone_1 = version_milestone_0 + 1;
    // We won't reach next version.
    assert!(swarm
        .wait_for_all_nodes_to_catchup_to_version(version_milestone_1, Duration::from_secs(5))
        .await
        .is_err());

    client
        .set_failpoint(
            "aptos_vm::vm_wrapper::execute_transaction".to_string(),
            "0%return".to_string(),
        )
        .await
        .unwrap();

    let version_milestone_2 = version_milestone_1 + 5;
    println!(
        "Current version: {}, the chain should tolerate discarding failed blocks, waiting for {}.",
        version_milestone_0, version_milestone_2
    );
    swarm
        .wait_for_all_nodes_to_catchup_to_version(version_milestone_2, Duration::from_secs(30))
        .await
        .expect("milestone 2 taking too long");
}
```

**File:** aptos-move/block-executor/src/errors.rs (L30-38)
```rust
/// If the unrecoverable error occurs during sequential execution (e.g. fallback),
/// the error is propagated back to the caller (block execution is aborted).
#[derive(Clone, Debug, PartialEq, Eq)]
pub enum BlockExecutionError<E> {
    /// unrecoverable BlockSTM error
    FatalBlockExecutorError(PanicError),
    /// unrecoverable VM error
    FatalVMError(E),
}
```
