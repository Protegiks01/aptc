# Audit Report

## Title
Validator Transaction Re-inclusion Vulnerability Due to Incomplete Filter Coverage After Commit Root Advancement

## Summary
The validator transaction exclude filter in `ProposalGenerator::generate_proposal_inner()` only checks uncommitted blocks (from parent to commit root), allowing already-committed validator transactions to be re-included in new proposals if they remain in the pool after the commit root advances past their original block. This causes execution failures due to on-chain state checks preventing double-execution.

## Finding Description

The vulnerability exists in the validator transaction filtering mechanism during block proposal generation. When a proposer generates a new block, it constructs a filter from pending blocks to exclude validator transactions already in the proposal chain: [1](#0-0) [2](#0-1) 

The `path_from_commit_root()` method only returns blocks between the commit root and the parent block, excluding any blocks that have been committed and pruned: [3](#0-2) [4](#0-3) 

When blocks are committed, older blocks are pruned from the block tree and no longer accessible: [5](#0-4) [6](#0-5) 

Meanwhile, validator transactions remain in the pool because their `TxnGuard` is only dropped on epoch changes, not on block commits: [7](#0-6) [8](#0-7) [9](#0-8) 

The pool's `pull()` method filters based on the incomplete filter, allowing re-inclusion: [10](#0-9) 

When a validator transaction (e.g., DKG result) is re-executed after already being committed, the on-chain state check fails: [11](#0-10) 

Proposal validation does not check for duplicate validator transactions across committed blocks: [12](#0-11) 

**Attack Scenario:**
1. Round 10: Block A containing VTxn1 (DKG result) is committed and executed
2. `DKGState.in_progress` is moved to `last_completed`
3. VTxn1's `TxnGuard` remains in `DKGManager::state` (epoch hasn't changed)
4. Rounds 11-30: Commit root advances, Block A is pruned from block tree
5. Round 31: New proposer builds Block B; filter only includes blocks from round 30 to commit_root
6. Block A (round 10) is not in the filter (pruned)
7. VTxn1 still in pool → pulled again → included in Block B
8. Block B execution: `dkg::finish()` aborts with `EDKG_NOT_IN_PROGRESS`

This breaks deterministic execution: blocks containing re-included validator transactions fail execution.

## Impact Explanation

This is a **Medium Severity** vulnerability per Aptos bug bounty criteria:

- **State inconsistencies requiring intervention**: Blocks containing re-included validator transactions fail execution with abort codes
- **Execution failures**: Validators encounter `EDKG_NOT_IN_PROGRESS` errors when processing such blocks
- **Temporary consensus disruption**: Proposers may create blocks that fail execution on some validators

The impact is medium rather than critical because:
1. No funds are lost or stolen
2. The network recovers when the epoch changes and guards are dropped
3. No permanent consensus safety violations
4. Requires specific timing conditions within an epoch

## Likelihood Explanation

**Likelihood: Medium-High** within epoch boundaries

The vulnerability occurs naturally during normal operation when:
1. A validator transaction (DKG result, JWK update) is committed early in an epoch
2. The commit root advances significantly (20+ rounds), causing pruning
3. The epoch has not yet changed (guard still held)
4. A new proposer generates a block

This is particularly likely with:
- **Long epochs** (hundreds of rounds) providing time for commit root advancement
- **DKG results** committed early but guards held until epoch end
- **Active validators** frequently proposing blocks

The vulnerability triggers naturally in production environments without requiring malicious actors or coordinated attacks.

## Recommendation

The filter should include validator transactions from all committed blocks within the current epoch, not just uncommitted blocks. Two possible fixes:

**Option 1**: Extend the filter to track committed validator transactions within the epoch:
```rust
// Maintain epoch-level tracking of committed validator transactions
let committed_vtxn_hashes: HashSet<HashValue> = self
    .get_committed_vtxns_in_epoch()
    .map(ValidatorTransaction::hash)
    .collect();

let pending_validator_txn_hashes: HashSet<HashValue> = pending_blocks
    .iter()
    .filter_map(|block| block.validator_txns())
    .flatten()
    .map(ValidatorTransaction::hash)
    .collect();

let all_vtxn_hashes: HashSet<HashValue> = committed_vtxn_hashes
    .union(&pending_validator_txn_hashes)
    .cloned()
    .collect();

let validator_txn_filter = vtxn_pool::TransactionFilter::PendingTxnHashSet(all_vtxn_hashes);
```

**Option 2**: Drop the TxnGuard immediately when the validator transaction is committed, not waiting for epoch change:
```rust
// In the block commit callback, notify DKGManager to drop guard
impl DKGManager {
    fn on_txn_committed(&mut self, txn_hash: HashValue) {
        if let InnerState::Finished { vtxn_guard, .. } = &self.state {
            // Drop the guard when transaction is committed
            drop(vtxn_guard);
            self.state = InnerState::NotStarted;
        }
    }
}
```

Option 2 is preferred as it prevents the transaction from remaining in the pool after commitment.

## Proof of Concept

A proof of concept would require simulating:
1. An epoch with DKG completion in early rounds
2. Commit root advancement causing pruning of the DKG block
3. A new proposal generation that pulls the already-committed DKG transaction
4. Execution attempt that aborts with `EDKG_NOT_IN_PROGRESS`

This can be demonstrated through a consensus integration test that:
- Commits a DKG result in round 10
- Advances commit root to round 30+ (triggering pruning)
- Generates a new proposal at round 31
- Verifies the DKG transaction is re-included
- Confirms execution failure with the expected abort code

---

**Notes:**

This vulnerability represents a genuine protocol-level issue where the validator transaction lifecycle management (guards held until epoch change) is incompatible with the block tree pruning mechanism (blocks removed after commit root advancement). The filter construction assumes all relevant validator transactions are in the unpruned block tree, which is violated when the commit root advances significantly within an epoch.

The vulnerability is particularly concerning for DKG results and JWK consensus config updates, which are committed early in epochs but need to remain in the pool for the full epoch duration under the current design. Production environments with long epochs (hundreds of rounds) are highly susceptible to this issue.

### Citations

**File:** consensus/src/liveness/proposal_generator.rs (L575-578)
```rust
        let mut pending_blocks = self
            .block_store
            .path_from_commit_root(parent_id)
            .ok_or_else(|| format_err!("Parent block {} already pruned", parent_id))?;
```

**File:** consensus/src/liveness/proposal_generator.rs (L643-650)
```rust
        let pending_validator_txn_hashes: HashSet<HashValue> = pending_blocks
            .iter()
            .filter_map(|block| block.validator_txns())
            .flatten()
            .map(ValidatorTransaction::hash)
            .collect();
        let validator_txn_filter =
            vtxn_pool::TransactionFilter::PendingTxnHashSet(pending_validator_txn_hashes);
```

**File:** consensus/src/block_storage/block_tree.rs (L405-433)
```rust
    pub(super) fn find_blocks_to_prune(
        &self,
        next_window_root_id: HashValue,
    ) -> VecDeque<HashValue> {
        // Nothing to do if this is the window root
        if next_window_root_id == self.window_root_id {
            return VecDeque::new();
        }

        let mut blocks_pruned = VecDeque::new();
        let mut blocks_to_be_pruned = vec![self.linkable_window_root()];

        while let Some(block_to_remove) = blocks_to_be_pruned.pop() {
            block_to_remove.executed_block().abort_pipeline();
            // Add the children to the blocks to be pruned (if any), but stop when it reaches the
            // new root
            for child_id in block_to_remove.children() {
                if next_window_root_id == *child_id {
                    continue;
                }
                blocks_to_be_pruned.push(
                    self.get_linkable_block(child_id)
                        .expect("Child must exist in the tree"),
                );
            }
            // Track all the block ids removed
            blocks_pruned.push_back(block_to_remove.id());
        }
        blocks_pruned
```

**File:** consensus/src/block_storage/block_tree.rs (L519-545)
```rust
    pub(super) fn path_from_root_to_block(
        &self,
        block_id: HashValue,
        root_id: HashValue,
        root_round: u64,
    ) -> Option<Vec<Arc<PipelinedBlock>>> {
        let mut res = vec![];
        let mut cur_block_id = block_id;
        loop {
            match self.get_block(&cur_block_id) {
                Some(ref block) if block.round() <= root_round => {
                    break;
                },
                Some(block) => {
                    cur_block_id = block.parent_id();
                    res.push(block);
                },
                None => return None,
            }
        }
        // At this point cur_block.round() <= self.root.round()
        if cur_block_id != root_id {
            return None;
        }
        // Called `.reverse()` to get the chronically increased order.
        res.reverse();
        Some(res)
```

**File:** consensus/src/block_storage/block_tree.rs (L555-560)
```rust
    pub(super) fn path_from_commit_root(
        &self,
        block_id: HashValue,
    ) -> Option<Vec<Arc<PipelinedBlock>>> {
        self.path_from_root_to_block(block_id, self.commit_root_id, self.commit_root().round())
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L567-600)
```rust
    pub fn commit_callback(
        &mut self,
        storage: Arc<dyn PersistentLivenessStorage>,
        block_id: HashValue,
        block_round: Round,
        finality_proof: WrappedLedgerInfo,
        commit_decision: LedgerInfoWithSignatures,
        window_size: Option<u64>,
    ) {
        let current_round = self.commit_root().round();
        let committed_round = block_round;
        let commit_proof = finality_proof
            .create_merged_with_executed_state(commit_decision)
            .expect("Inconsistent commit proof and evaluation decision, cannot commit block");

        debug!(
            LogSchema::new(LogEvent::CommitViaBlock).round(current_round),
            committed_round = committed_round,
            block_id = block_id,
        );

        let window_root_id = self.find_window_root(block_id, window_size);
        let ids_to_remove = self.find_blocks_to_prune(window_root_id);

        if let Err(e) = storage.prune_tree(ids_to_remove.clone().into_iter().collect()) {
            // it's fine to fail here, as long as the commit succeeds, the next restart will clean
            // up dangling blocks, and we need to prune the tree to keep the root consistent with
            // executor.
            warn!(error = ?e, "fail to delete block");
        }
        self.process_pruned_blocks(ids_to_remove);
        self.update_window_root(window_root_id);
        self.update_highest_commit_cert(commit_proof);
    }
```

**File:** dkg/src/dkg_manager/mod.rs (L32-45)
```rust
enum InnerState {
    NotStarted,
    InProgress {
        start_time: Duration,
        my_transcript: DKGTranscript,
        abort_handle: AbortHandle,
    },
    Finished {
        vtxn_guard: TxnGuard,
        start_time: Duration,
        my_transcript: DKGTranscript,
        proposed: bool,
    },
}
```

**File:** dkg/src/dkg_manager/mod.rs (L217-252)
```rust
    fn process_close_cmd(&mut self, ack_tx: Option<oneshot::Sender<()>>) -> Result<()> {
        self.stopped = true;

        match std::mem::take(&mut self.state) {
            InnerState::NotStarted => {},
            InnerState::InProgress { abort_handle, .. } => {
                abort_handle.abort();
            },
            InnerState::Finished {
                vtxn_guard,
                start_time,
                ..
            } => {
                let epoch_change_time = duration_since_epoch();
                let secs_since_dkg_start =
                    epoch_change_time.as_secs_f64() - start_time.as_secs_f64();
                DKG_STAGE_SECONDS
                    .with_label_values(&[self.my_addr.to_hex().as_str(), "epoch_change"])
                    .observe(secs_since_dkg_start);
                info!(
                    epoch = self.epoch_state.epoch,
                    my_addr = self.my_addr,
                    secs_since_dkg_start = secs_since_dkg_start,
                    "[DKG] txn executed and entering new epoch.",
                );

                drop(vtxn_guard);
            },
        }

        if let Some(tx) = ack_tx {
            let _ = tx.send(());
        }

        Ok(())
    }
```

**File:** crates/validator-transaction-pool/src/lib.rs (L152-199)
```rust
    pub fn pull(
        &mut self,
        deadline: Instant,
        mut max_items: u64,
        mut max_bytes: u64,
        filter: TransactionFilter,
    ) -> Vec<ValidatorTransaction> {
        let mut ret = vec![];
        let mut seq_num_lower_bound = 0;

        // Check deadline at the end of every iteration to ensure validator txns get a chance no matter what current proposal delay is.
        while max_items >= 1 && max_bytes >= 1 {
            // Find the seq_num of the first txn that satisfies the quota.
            if let Some(seq_num) = self
                .txn_queue
                .range(seq_num_lower_bound..)
                .filter(|(_, item)| {
                    item.txn.size_in_bytes() as u64 <= max_bytes
                        && !filter.should_exclude(&item.txn)
                })
                .map(|(seq_num, _)| *seq_num)
                .next()
            {
                // Update the quota usage.
                // Send the pull notification if requested.
                let PoolItem {
                    txn,
                    pull_notification_tx,
                    ..
                } = self.txn_queue.get(&seq_num).unwrap();
                if let Some(tx) = pull_notification_tx {
                    let _ = tx.push((), txn.clone());
                }
                max_items -= 1;
                max_bytes -= txn.size_in_bytes() as u64;
                seq_num_lower_bound = seq_num + 1;
                ret.push(txn.as_ref().clone());

                if Instant::now() >= deadline {
                    break;
                }
            } else {
                break;
            }
        }

        ret
    }
```

**File:** crates/validator-transaction-pool/src/lib.rs (L202-206)
```rust
impl Drop for TxnGuard {
    fn drop(&mut self) {
        self.pool.lock().try_delete(self.seq_num);
    }
}
```

**File:** aptos-move/framework/aptos-framework/sources/dkg.move (L90-97)
```text
    public(friend) fun finish(transcript: vector<u8>) acquires DKGState {
        let dkg_state = borrow_global_mut<DKGState>(@aptos_framework);
        assert!(option::is_some(&dkg_state.in_progress), error::invalid_state(EDKG_NOT_IN_PROGRESS));
        let session = option::extract(&mut dkg_state.in_progress);
        session.transcript = transcript;
        dkg_state.last_completed = option::some(session);
        dkg_state.in_progress = option::none();
    }
```

**File:** consensus/src/round_manager.rs (L1126-1137)
```rust
        if let Some(vtxns) = proposal.validator_txns() {
            for vtxn in vtxns {
                let vtxn_type_name = vtxn.type_name();
                ensure!(
                    is_vtxn_expected(&self.randomness_config, &self.jwk_consensus_config, vtxn),
                    "unexpected validator txn: {:?}",
                    vtxn_type_name
                );
                vtxn.verify(self.epoch_state.verifier.as_ref())
                    .context(format!("{} verify failed", vtxn_type_name))?;
            }
        }
```
