# Audit Report

## Title
Unbounded Memory Exhaustion via Byzantine Validator JWK Consensus Request Flooding in Per-Key Mode

## Summary
Byzantine validators can exhaust validator node memory by flooding the JWK consensus system with arbitrary (issuer, kid) pairs through `KeyLevelObservationRequest` messages. The vulnerability stems from two logic bugs: unconditional HashMap entry creation without validation against on-chain providers, and broken cleanup logic that indefinitely retains malicious entries due to incorrect version comparison.

## Finding Description

The JWK consensus system in per-key mode maintains a HashMap to track consensus state for each (issuer, kid) pair. [1](#0-0) 

When a `KeyLevelObservationRequest` arrives, the `process_peer_request()` function unconditionally creates a HashMap entry using `entry().or_default()` without validating that the issuer exists in the on-chain `SupportedOIDCProviders` list. [2](#0-1) 

Both `Issuer` and `KID` are type aliases for `Vec<u8>` with no inherent length restrictions. [3](#0-2) 

The function returns early for `NotStarted` states without sending a response, but the HashMap entry persists in memory. [4](#0-3) 

**Critical Bug #1 - Broken Cleanup Logic:**
The `reset_with_on_chain_state()` function contains a critical bug. It uses `unwrap_or_default()` for both sides of a version comparison, which returns 0 for non-existent issuers. This causes the retention condition to evaluate `0 == 0` = `true`, keeping malicious entries indefinitely. [5](#0-4) 

This contrasts with the correct implementation in `IssuerLevelConsensusManager`, which properly checks issuer existence using `onchain_issuer_set.contains(issuer)`. [6](#0-5) 

**Critical Bug #2 - Missing Validation:**
The epoch manager only validates that the RPC request epoch matches the current epoch, but does not validate issuer membership in `SupportedOIDCProviders`. [7](#0-6) 

The network layer enforces a limit of 100 concurrent inbound RPCs per peer, but this does not prevent sequential batches from accumulating unbounded entries. [8](#0-7) 

**Attack Execution Path:**
1. Byzantine validator (≤1/3 of validator set) sends `KeyLevelObservationRequest` with arbitrary (issuer, kid) pairs
2. Request passes epoch validation at the EpochManager level
3. `process_peer_request()` creates HashMap entry with `ConsensusState::NotStarted`
4. Function returns early, leaving entry in memory
5. Attacker sends unique pairs in sequential batches of 100 to bypass concurrent RPC limits
6. Cleanup logic fails to remove entries for non-existent issuers due to `0 == 0` bug
7. HashMap grows unboundedly until memory exhaustion occurs

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program's impact categories:

- **Validator Node Slowdowns (High)**: Memory exhaustion causes performance degradation affecting consensus participation and block processing speed
- **Resource Exhaustion via Protocol Logic Bugs**: Two distinct logic bugs (missing validation + incorrect cleanup condition) allow unbounded memory consumption from protocol messages, not network-level packet flooding
- **Degraded Network Performance**: If multiple validators are simultaneously targeted, overall network throughput and consensus efficiency decline

This is not a simple network DoS attack (which would be out of scope), but rather a protocol-level resource exhaustion vulnerability caused by logic bugs in state management that violate the security requirement that all operations respect resource limits.

## Likelihood Explanation

**High Likelihood:**

The attack requires only a Byzantine validator (≤1/3 of validator set) sending RPC messages with arbitrary byte sequences:
- **Trivially executable**: No special timing, state conditions, or complex coordination required
- **Within standard threat model**: Byzantine validators are part of the BFT adversary model
- **Unmitigated**: Two logic bugs (missing validation + cleanup bug) ensure successful exploitation
- **Persistent**: Cleanup logic bug ensures malicious entries survive across epoch transitions
- **Sequential attack viable**: Network layer's 100 concurrent RPC limit can be bypassed with sequential batches over time

## Recommendation

**Fix #1 - Correct Cleanup Logic:**
Replace the version comparison with an existence check, similar to the issuer-level implementation:

```rust
self.states_by_key.retain(|(issuer, _), _| {
    new_onchain_jwks.contains_key(issuer)
});
```

**Fix #2 - Add Validation:**
In `process_peer_request()`, validate that the issuer exists in the on-chain provider list before creating HashMap entries. Cache the `SupportedOIDCProviders` list and check membership before calling `entry().or_default()`.

**Fix #3 - Add Bounds:**
Implement a maximum size limit for `states_by_key` HashMap to prevent unbounded growth even if logic bugs exist.

## Proof of Concept

The vulnerability can be demonstrated through a Rust integration test that:
1. Spawns a `KeyLevelConsensusManager` with a known `SupportedOIDCProviders` list
2. Sends `KeyLevelObservationRequest` messages with arbitrary (issuer, kid) pairs not in the provider list
3. Observes that `states_by_key.len()` grows unboundedly
4. Triggers `reset_with_on_chain_state()` and verifies entries are incorrectly retained due to `0 == 0` comparison
5. Measures memory consumption growth over repeated batches

The PoC would require mocking the network layer and epoch state, then directly calling `process_peer_request()` with malicious requests and observing HashMap growth without bound.

## Notes

This vulnerability represents a critical discrepancy between the per-key and per-issuer consensus modes. The issuer-level mode has correct cleanup logic that checks issuer existence, while the per-key mode incorrectly compares versions. This suggests the bug was introduced during the per-key mode implementation without sufficient review against the established pattern. The vulnerability is particularly concerning because it requires only Byzantine validator behavior (≤1/3), which is within the standard BFT threat model, and results in persistent resource exhaustion that survives epoch transitions.

### Citations

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L59-59)
```rust
    states_by_key: HashMap<(Issuer, KID), ConsensusState<ObservedKeyLevelUpdate>>,
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L244-254)
```rust
        self.states_by_key.retain(|(issuer, _), _| {
            new_onchain_jwks
                .get(issuer)
                .map(|jwks| jwks.version)
                .unwrap_or_default()
                == self
                    .onchain_jwks
                    .get(issuer)
                    .map(|jwks| jwks.version)
                    .unwrap_or_default()
        });
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L274-277)
```rust
                let consensus_state = self
                    .states_by_key
                    .entry((issuer.clone(), kid.clone()))
                    .or_default();
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L279-285)
```rust
                    ConsensusState::NotStarted => {
                        debug!(
                            issuer = String::from_utf8(issuer.clone()).ok(),
                            kid = String::from_utf8(kid.clone()).ok(),
                            "key-level jwk consensus not started"
                        );
                        return Ok(());
```

**File:** types/src/jwks/mod.rs (L36-38)
```rust
pub type Issuer = Vec<u8>;
/// Type for JWK Key ID.
pub type KID = Vec<u8>;
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager/mod.rs (L236-253)
```rust
        let onchain_issuer_set: HashSet<Issuer> = on_chain_state
            .entries
            .iter()
            .map(|entry| entry.issuer.clone())
            .collect();
        let local_issuer_set: HashSet<Issuer> = self.states_by_issuer.keys().cloned().collect();

        for issuer in local_issuer_set.difference(&onchain_issuer_set) {
            info!(
                epoch = self.epoch_state.epoch,
                op = "delete",
                issuer = issuer.clone(),
                "reset_with_on_chain_state"
            );
        }

        self.states_by_issuer
            .retain(|issuer, _| onchain_issuer_set.contains(issuer));
```

**File:** crates/aptos-jwk-consensus/src/epoch_manager.rs (L99-103)
```rust
        if Some(rpc_request.msg.epoch()) == self.epoch_state.as_ref().map(|s| s.epoch) {
            if let Some(tx) = &self.jwk_rpc_msg_tx {
                let _ = tx.push(peer_id, (peer_id, rpc_request));
            }
        }
```

**File:** network/framework/src/constants.rs (L15-15)
```rust
pub const MAX_CONCURRENT_INBOUND_RPCS: u32 = 100;
```
