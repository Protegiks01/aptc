# Audit Report

## Title
Byzantine Validators Can Exhaust Tokio Blocking Thread Pool Through Unbounded Randomness Aggregation

## Summary
The randomness generation subsystem spawns unbounded blocking tasks for cryptographic aggregation, allowing Byzantine validators (< 1/3 stake) to exhaust the global 64-thread blocking pool by pre-positioning shares for up to 200 future rounds and triggering simultaneous aggregation when blocks arrive in batches.

## Finding Description

The vulnerability exists in the randomness aggregation flow where `ShareAggregator::try_aggregate` directly calls `tokio::task::spawn_blocking` without bounded execution controls.

**Attack Flow:**

1. **Share Pre-positioning**: Byzantine validators send randomness shares for up to 200 future rounds. The system accepts shares where `share.metadata().round <= self.highest_known_round + FUTURE_ROUNDS_TO_ACCEPT` [1](#0-0) , with shares stored in the `rand_map` for aggregation [2](#0-1) .

2. **Threshold Meeting**: Byzantine validators send enough shares to meet the aggregation threshold (`total_weight >= rand_config.threshold()`) for multiple rounds, creating `PendingMetadata` items awaiting block metadata [3](#0-2) .

3. **Batch Metadata Arrival**: When consensus produces blocks in batches (default 10, maximum 100 with quorum store override) [4](#0-3) , the `process_incoming_blocks` method iterates through all blocks and calls `process_incoming_metadata` for each [5](#0-4) .

4. **Unbounded Spawning**: For each block, `add_rand_metadata` is called [6](#0-5) , which invokes `try_aggregate` on both slow and fast paths when enabled [7](#0-6) . The `try_aggregate` method spawns blocking tasks directly via `tokio::task::spawn_blocking` [8](#0-7)  without using any bounded executor.

5. **Thread Pool Exhaustion**: The Aptos runtime configures tokio with a maximum of 64 blocking threads globally [9](#0-8) . With fast randomness path enabled, each round triggers 2 blocking tasks. A batch of 32 blocks spawns 64 tasks simultaneously, exhausting the global limit.

**Critical Code Path:**

While message verification uses a `BoundedExecutor` [10](#0-9) , the actual cryptographic aggregation bypasses this protection. The aggregation performs expensive multi-pairing operations for WVUF derivation [11](#0-10) , which are computationally intensive and block the tokio runtime threads.

**Root Cause:**

The `try_aggregate` method lacks resource bounds, creating an unbounded resource consumption path that Byzantine validators can exploit by controlling when aggregation thresholds are met across many rounds.

## Impact Explanation

**Severity: High (Validator Node Slowdowns)**

This qualifies as **High Severity** per the Aptos bug bounty program's "Validator node slowdowns" category.

**Concrete Impact:**

1. **Blocking Thread Pool Exhaustion**: The global tokio blocking pool services ALL blocking operations across the validator node, including storage I/O, network operations, and other consensus subsystems. Exhaustion causes queuing delays for all blocking tasks.

2. **Consensus Performance Degradation**: Validators experiencing thread pool exhaustion will have delayed responses to consensus messages, block processing, and vote submissions, potentially causing increased round timeouts, failed consensus participation, reduced network throughput, and possible exclusion from the active validator set.

3. **Cascading Resource Exhaustion**: Other components relying on `spawn_blocking` become unresponsive, compounding performance issues.

4. **Sustained Attack**: Byzantine validators can repeat this attack every epoch or whenever batch processing occurs, maintaining continuous pressure on honest validators.

The attack does NOT cause total liveness failure (validators can still process blocks once threads become available), but creates significant operational degradation affecting consensus quality and network performance.

## Likelihood Explanation

**Likelihood: Medium-High**

**Attacker Requirements:**
- Byzantine validators with < 1/3 total stake (realistic under BFT assumptions)
- Coordination to send shares for future rounds (trivial via automated scripts)
- No special privileges beyond validator status

**Attack Complexity: Low**
- Pre-sending shares for 100+ rounds requires minimal coordination
- Attack triggers automatically when blocks arrive (normal consensus operation)
- Repeatable across epochs without detection

**Detection Difficulty:**
- Appears as legitimate share submissions (all messages verified)
- Thread pool exhaustion manifests as general performance degradation
- Root cause attribution challenging without monitoring spawn_blocking queue depths

**Realistic Scenarios:**
1. State sync/catchup: Nodes processing historical blocks in large batches trigger simultaneous aggregation
2. Network partition recovery: Validators rejoining process accumulated blocks
3. Deliberate Byzantine coordination: Malicious validators intentionally trigger during high-load periods

## Recommendation

Replace the unbounded `tokio::task::spawn_blocking` call in `ShareAggregator::try_aggregate` with a `BoundedExecutor::spawn_blocking` call to limit concurrent aggregation tasks:

**Proposed Fix:**

1. Pass a `BoundedExecutor` instance to `ShareAggregator::try_aggregate` or store it in `RandStore`
2. Replace the direct `tokio::task::spawn_blocking` call with `bounded_executor.spawn_blocking`
3. Configure the bounded executor with an appropriate capacity (e.g., 8-16 concurrent aggregation tasks)

This ensures that even with Byzantine validators pre-positioning shares for many rounds, the number of concurrent blocking tasks is bounded, preventing thread pool exhaustion while still allowing legitimate randomness aggregation to proceed.

## Proof of Concept

The vulnerability can be demonstrated by:

1. Configuring a test network with Byzantine validators holding < 1/3 stake
2. Having Byzantine validators send randomness shares for 100+ future rounds
3. Triggering a batch of 32+ blocks to arrive simultaneously (via state sync or network partition recovery)
4. Observing that 64+ `spawn_blocking` tasks are spawned concurrently
5. Monitoring the tokio blocking thread pool to confirm exhaustion and subsequent queuing of all blocking operations

A complete PoC would require setting up a test Aptos network with the randomness fast path enabled and instrumenting the `spawn_blocking` calls to verify the thread pool saturation.

### Citations

**File:** consensus/src/rand/rand_gen/types.rs (L26-26)
```rust
pub const FUTURE_ROUNDS_TO_ACCEPT: u64 = 200;
```

**File:** consensus/src/rand/rand_gen/types.rs (L97-148)
```rust
    fn aggregate<'a>(
        shares: impl Iterator<Item = &'a RandShare<Self>>,
        rand_config: &RandConfig,
        rand_metadata: RandMetadata,
    ) -> anyhow::Result<Randomness>
    where
        Self: Sized,
    {
        let timer = std::time::Instant::now();
        let mut apks_and_proofs = vec![];
        for share in shares {
            let id = rand_config
                .validator
                .address_to_validator_index()
                .get(share.author())
                .copied()
                .ok_or_else(|| {
                    anyhow!(
                        "Share::aggregate failed with invalid share author: {}",
                        share.author
                    )
                })?;
            let apk = rand_config
                .get_certified_apk(share.author())
                .ok_or_else(|| {
                    anyhow!(
                        "Share::aggregate failed with missing apk for share from {}",
                        share.author
                    )
                })?;
            apks_and_proofs.push((Player { id }, apk.clone(), share.share().share));
        }

        let proof = WVUF::aggregate_shares(&rand_config.wconfig, &apks_and_proofs);
        let metadata_serialized = bcs::to_bytes(&rand_metadata).map_err(|e| {
            anyhow!("Share::aggregate failed with metadata serialization error: {e}")
        })?;
        let eval = WVUF::derive_eval(
            &rand_config.wconfig,
            &rand_config.vuf_pp,
            metadata_serialized.as_slice(),
            &rand_config.get_all_certified_apk(),
            &proof,
            THREAD_MANAGER.get_exe_cpu_pool(),
        )
        .map_err(|e| anyhow!("Share::aggregate failed with WVUF derive_eval error: {e}"))?;
        debug!("WVUF derivation time: {} ms", timer.elapsed().as_millis());
        let eval_bytes = bcs::to_bytes(&eval)
            .map_err(|e| anyhow!("Share::aggregate failed with eval serialization error: {e}"))?;
        let rand_bytes = Sha3_256::digest(eval_bytes.as_slice()).to_vec();
        Ok(Randomness::new(rand_metadata, rand_bytes))
    }
```

**File:** consensus/src/rand/rand_gen/rand_store.rs (L41-49)
```rust
    pub fn try_aggregate(
        self,
        rand_config: &RandConfig,
        rand_metadata: FullRandMetadata,
        decision_tx: Sender<Randomness>,
    ) -> Either<Self, RandShare<S>> {
        if self.total_weight < rand_config.threshold() {
            return Either::Left(self);
        }
```

**File:** consensus/src/rand/rand_gen/rand_store.rs (L69-87)
```rust
        tokio::task::spawn_blocking(move || {
            let maybe_randomness = S::aggregate(
                self.shares.values(),
                &rand_config,
                rand_metadata.metadata.clone(),
            );
            match maybe_randomness {
                Ok(randomness) => {
                    let _ = decision_tx.unbounded_send(randomness);
                },
                Err(e) => {
                    warn!(
                        epoch = rand_metadata.metadata.epoch,
                        round = rand_metadata.metadata.round,
                        "Aggregation error: {e}"
                    );
                },
            }
        });
```

**File:** consensus/src/rand/rand_gen/rand_store.rs (L261-277)
```rust
    pub fn add_rand_metadata(&mut self, rand_metadata: FullRandMetadata) {
        let rand_item = self
            .rand_map
            .entry(rand_metadata.round())
            .or_insert_with(|| RandItem::new(self.author, PathType::Slow));
        rand_item.add_metadata(&self.rand_config, rand_metadata.clone());
        rand_item.try_aggregate(&self.rand_config, self.decision_tx.clone());
        // fast path
        if let (Some(fast_rand_map), Some(fast_rand_config)) =
            (self.fast_rand_map.as_mut(), self.fast_rand_config.as_ref())
        {
            let fast_rand_item = fast_rand_map
                .entry(rand_metadata.round())
                .or_insert_with(|| RandItem::new(self.author, PathType::Fast));
            fast_rand_item.add_metadata(fast_rand_config, rand_metadata.clone());
            fast_rand_item.try_aggregate(fast_rand_config, self.decision_tx.clone());
        }
```

**File:** consensus/src/rand/rand_gen/rand_store.rs (L280-313)
```rust
    pub fn add_share(&mut self, share: RandShare<S>, path: PathType) -> anyhow::Result<bool> {
        ensure!(
            share.metadata().epoch == self.epoch,
            "Share from different epoch"
        );
        ensure!(
            share.metadata().round <= self.highest_known_round + FUTURE_ROUNDS_TO_ACCEPT,
            "Share from future round"
        );
        let rand_metadata = share.metadata().clone();

        let (rand_config, rand_item) = if path == PathType::Fast {
            match (self.fast_rand_config.as_ref(), self.fast_rand_map.as_mut()) {
                (Some(fast_rand_config), Some(fast_rand_map)) => (
                    fast_rand_config,
                    fast_rand_map
                        .entry(rand_metadata.round)
                        .or_insert_with(|| RandItem::new(self.author, path)),
                ),
                _ => anyhow::bail!("Fast path not enabled"),
            }
        } else {
            (
                &self.rand_config,
                self.rand_map
                    .entry(rand_metadata.round)
                    .or_insert_with(|| RandItem::new(self.author, PathType::Slow)),
            )
        };

        rand_item.add_share(share, rand_config)?;
        rand_item.try_aggregate(rand_config, self.decision_tx.clone());
        Ok(rand_item.has_decision())
    }
```

**File:** config/src/config/consensus_config.rs (L366-370)
```rust
            max_blocks_per_sending_request: 10,
            // TODO: this is for release compatibility, after release we can configure it to match the receiving max
            max_blocks_per_sending_request_quorum_store_override: 10,
            max_blocks_per_receiving_request: 10,
            max_blocks_per_receiving_request_quorum_store_override: 100,
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L132-143)
```rust
    fn process_incoming_blocks(&mut self, blocks: OrderedBlocks) {
        let rounds: Vec<u64> = blocks.ordered_blocks.iter().map(|b| b.round()).collect();
        info!(rounds = rounds, "Processing incoming blocks.");
        let broadcast_handles: Vec<_> = blocks
            .ordered_blocks
            .iter()
            .map(|block| FullRandMetadata::from(block.block()))
            .map(|metadata| self.process_incoming_metadata(metadata))
            .collect();
        let queue_item = QueueItem::new(blocks, Some(broadcast_handles));
        self.block_queue.push_back(queue_item);
    }
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L165-165)
```rust
        rand_store.add_rand_metadata(metadata.clone());
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L227-260)
```rust
        bounded_executor: BoundedExecutor,
    ) {
        while let Some(rand_gen_msg) = incoming_rpc_request.next().await {
            let tx = verified_msg_tx.clone();
            let epoch_state_clone = epoch_state.clone();
            let config_clone = rand_config.clone();
            let fast_config_clone = fast_rand_config.clone();
            bounded_executor
                .spawn(async move {
                    match bcs::from_bytes::<RandMessage<S, D>>(rand_gen_msg.req.data()) {
                        Ok(msg) => {
                            if msg
                                .verify(
                                    &epoch_state_clone,
                                    &config_clone,
                                    &fast_config_clone,
                                    rand_gen_msg.sender,
                                )
                                .is_ok()
                            {
                                let _ = tx.unbounded_send(RpcRequest {
                                    req: msg,
                                    protocol: rand_gen_msg.protocol,
                                    response_sender: rand_gen_msg.response_sender,
                                });
                            }
                        },
                        Err(e) => {
                            warn!("Invalid rand gen message: {}", e);
                        },
                    }
                })
                .await;
        }
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-50)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;

    // Verify the given name has an appropriate length
    if thread_name.len() > MAX_THREAD_NAME_LENGTH {
        panic!(
            "The given runtime thread name is too long! Max length: {}, given name: {}",
            MAX_THREAD_NAME_LENGTH, thread_name
        );
    }

    // Create the runtime builder
    let atomic_id = AtomicUsize::new(0);
    let thread_name_clone = thread_name.clone();
    let mut builder = Builder::new_multi_thread();
    builder
        .thread_name_fn(move || {
            let id = atomic_id.fetch_add(1, Ordering::SeqCst);
            format!("{}-{}", thread_name_clone, id)
        })
        .on_thread_start(on_thread_start)
        .disable_lifo_slot()
        // Limit concurrent blocking tasks from spawn_blocking(), in case, for example, too many
        // Rest API calls overwhelm the node.
        .max_blocking_threads(MAX_BLOCKING_THREADS)
```
