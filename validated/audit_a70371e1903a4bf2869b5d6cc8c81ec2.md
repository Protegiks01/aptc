# Audit Report

## Title
Storage Corruption Errors Silently Ignored During Block Persistence Leading to Consensus Safety Violation

## Summary
The `PersistingPhase::process()` function silently ignores storage corruption errors during block commitment, causing consensus to believe blocks are committed when they are not actually persisted to storage. This leads to critical state inconsistencies where a validator's in-memory consensus state diverges from its on-disk storage state, resulting in consensus safety violations at the node level.

## Finding Description

The vulnerability exists in the error handling path of the block persistence pipeline. When `PersistingPhase::process()` commits blocks to storage, it calls `wait_for_commit_ledger()` which silently discards all errors from the underlying storage operation. [1](#0-0) 

This calls into `PipelinedBlock::wait_for_commit_ledger()` which explicitly ignores the result with `let _ = ...`: [2](#0-1) 

The `commit_ledger_fut` eventually executes the actual storage commit operation via the executor: [3](#0-2) 

At the storage layer, `commit_ledger()` performs the actual database write via `write_schemas()`: [4](#0-3) 

The `write_schemas()` operation can fail with RocksDB corruption errors, which are properly converted to AptosDbError: [5](#0-4) 

However, because `wait_for_commit_ledger()` discards this error, `PersistingPhase::process()` always returns success: [6](#0-5) 

When the BufferManager receives this successful response, it updates its committed round state: [7](#0-6) 

**Exploitation Scenario:**
1. Storage corruption occurs (hardware failure, filesystem bug, etc.) during block commit at round N
2. RocksDB returns `ErrorKind::Corruption` during `write_schemas()` operation
3. Error propagates through `commit_ledger()` chain but is discarded by `wait_for_commit_ledger()`
4. `PersistingPhase::process()` returns `Ok(N)` indicating successful commit
5. `BufferManager` updates `highest_committed_round = N` and cleans up pending blocks
6. Consensus moves forward believing round N is committed
7. Storage layer still has last committed round at M < N (commit actually failed)
8. On node restart, `highest_committed_round` is reinitialized from storage which returns the actual committed round M < N
9. Node has inconsistent state - consensus believed it committed round N but storage only has round M committed

This breaks the critical consensus safety invariant that a validator's view of committed state must be consistent and monotonically increasing.

## Impact Explanation

This is **HIGH Severity** per Aptos bug bounty criteria, potentially **CRITICAL** if multiple validators are affected:

1. **Node-Level Consensus Safety Violation**: The affected validator develops an inconsistent view of committed state. Its in-memory consensus state believes blocks are committed while its storage does not contain them. On restart, the node effectively "rolls back" its committed state, violating the monotonicity guarantee of consensus.

2. **Client Impact**: Clients querying the affected validator receive incorrect information about transaction finality. Transactions reported as committed may not actually be persisted, potentially leading to loss of funds if users rely on this information.

3. **Silent Failure**: The complete absence of error handling, logging, or alerting makes this vulnerability extremely difficult to detect and diagnose in production environments.

4. **Network-Wide Risk**: If multiple validators experience storage corruption simultaneously (e.g., due to a common infrastructure issue, filesystem bug, or coordinated hardware failures), the network could have validators with different views of committed state, potentially causing consensus divergence.

5. **Liveness Impact**: On restart, the affected validator may have difficulty rejoining consensus if it has significantly diverged from other validators in its view of committed state.

## Likelihood Explanation

**Likelihood: Low to Medium**

While storage corruption events are relatively rare in well-maintained infrastructure, they do occur in practice:

1. **Hardware Failures**: Disk failures, memory corruption, and power loss during writes can cause storage corruption
2. **Filesystem Bugs**: Underlying filesystem implementation bugs can corrupt data
3. **Software Stack Issues**: Bugs in RocksDB or the storage stack could cause corruption

The key concern is that **there is zero error handling** - when storage corruption does occur during the commit operation, this vulnerability will trigger with 100% certainty. There is no retry mechanism, no error detection, and no recovery path. The silent nature of the failure significantly amplifies the impact.

## Recommendation

Add proper error handling and propagation in the persistence pipeline:

1. **Modify `wait_for_commit_ledger()` to propagate errors**:
```rust
pub async fn wait_for_commit_ledger(&self) -> Result<(), ExecutorError> {
    if let Some(fut) = self.pipeline_futs() {
        fut.commit_ledger_fut.await
            .map(|_| ())
            .map_err(|e| ExecutorError::InternalError {
                error: format!("Commit ledger failed: {}", e),
            })
    } else {
        Err(ExecutorError::InternalError {
            error: "Pipeline aborted".to_string(),
        })
    }
}
```

2. **Modify `PersistingPhase::process()` to handle errors**:
```rust
async fn process(&self, req: PersistingRequest) -> PersistingResponse {
    // ... existing code ...
    for b in &blocks {
        // ... send commit_proof_tx ...
        b.wait_for_commit_ledger().await?;  // Propagate error
    }
    // ... rest of function
}
```

3. **Add logging and monitoring** for commit failures to enable detection and alerting

4. **Implement retry mechanisms** for transient storage errors

5. **Add health checks** that verify consensus-storage consistency

## Proof of Concept

While a complete PoC requires simulating storage corruption, the vulnerability can be verified through code inspection of the error handling path:

1. Trace from `write_schemas()` returning a corruption error
2. Follow error propagation through `commit_ledger()` 
3. Observe error being discarded in `wait_for_commit_ledger()`
4. Confirm `PersistingPhase::process()` returns `Ok(round)` regardless
5. Verify `BufferManager` updates `highest_committed_round` based on this success response

The vulnerability is evident from the code structure where error results are explicitly discarded with `let _ = ...` pattern at the critical juncture between storage operations and consensus state updates.

## Notes

- This vulnerability affects the decoupled execution pipeline (when `consensus.decoupled = true`)
- The transaction data itself is written during the `pre_commit` phase, so the actual data is not lost - only the commit metadata fails to update
- However, this still represents a serious consistency violation as the commit progress marker determines what the node considers committed on restart
- The impact is primarily at the single-validator level unless multiple validators experience coordinated storage corruption
- No PoC with actual storage corruption simulation is provided, but the code path is definitively vulnerable to this scenario

### Citations

**File:** consensus/src/pipeline/persisting_phase.rs (L71-71)
```rust
            b.wait_for_commit_ledger().await;
```

**File:** consensus/src/pipeline/persisting_phase.rs (L74-74)
```rust
        let response = Ok(blocks.last().expect("Blocks can't be empty").round());
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L562-568)
```rust
    pub async fn wait_for_commit_ledger(&self) {
        // may be aborted (e.g. by reset)
        if let Some(fut) = self.pipeline_futs() {
            // this may be cancelled
            let _ = fut.commit_ledger_fut.await;
        }
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L1098-1105)
```rust
        tokio::task::spawn_blocking(move || {
            executor
                .commit_ledger(ledger_info_with_sigs_clone)
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
        Ok(Some(ledger_info_with_sigs))
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L103-107)
```rust
            ledger_batch.put::<DbMetadataSchema>(
                &DbMetadataKey::OverallCommitProgress,
                &DbMetadataValue::Version(version),
            )?;
            self.ledger_db.metadata_db().write_schemas(ledger_batch)?;
```

**File:** storage/schemadb/src/lib.rs (L389-407)
```rust
fn to_db_err(rocksdb_err: rocksdb::Error) -> AptosDbError {
    match rocksdb_err.kind() {
        ErrorKind::Incomplete => AptosDbError::RocksDbIncompleteResult(rocksdb_err.to_string()),
        ErrorKind::NotFound
        | ErrorKind::Corruption
        | ErrorKind::NotSupported
        | ErrorKind::InvalidArgument
        | ErrorKind::IOError
        | ErrorKind::MergeInProgress
        | ErrorKind::ShutdownInProgress
        | ErrorKind::TimedOut
        | ErrorKind::Aborted
        | ErrorKind::Busy
        | ErrorKind::Expired
        | ErrorKind::TryAgain
        | ErrorKind::CompactionTooLarge
        | ErrorKind::ColumnFamilyDropped
        | ErrorKind::Unknown => AptosDbError::OtherRocksDbError(rocksdb_err.to_string()),
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L968-973)
```rust
                Some(Ok(round)) = self.persisting_phase_rx.next() => {
                    // see where `need_backpressure()` is called.
                    self.pending_commit_votes = self.pending_commit_votes.split_off(&(round + 1));
                    self.highest_committed_round = round;
                    self.pending_commit_blocks = self.pending_commit_blocks.split_off(&(round + 1));
                },
```
