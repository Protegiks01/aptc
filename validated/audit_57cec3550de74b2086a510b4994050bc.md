# Audit Report

## Title
Sharding Configuration Toggle Causes Permanent Storage Leak via Orphaned Stale State Value Indices

## Summary
When the `enable_storage_sharding` configuration flag is toggled from `false` to `true` during AIP-97 migration, stale state value indices created under the old schema (`StaleStateValueIndexSchema`) are never pruned, causing unbounded storage growth that eventually leads to disk exhaustion and validator node failure. The pruning logic only operates on indices matching the current sharding configuration, leaving orphaned data in the old schema indefinitely.

## Finding Description

AptosDB uses two mutually exclusive schemas for tracking stale state values that need pruning. The write path conditionally selects one schema based on the `enable_sharding` flag: [1](#0-0) 

When sharding is disabled, indices use `StaleStateValueIndexSchema` (stores full `StateKey`). When enabled, indices use `StaleStateValueIndexByKeyHashSchema` (stores `state_key_hash`).

The critical vulnerability exists in the pruning logic, which contains a single conditional branch with no migration path: [2](#0-1) 

When `enabled_sharding()` returns `true`, the pruner iterates through sharded databases (lines 38-50) but performs **NO deletion operations**. The loop merely iterates and breaks - there are no `batch.delete` calls in this branch. When `false`, it properly deletes from `StaleStateValueIndexSchema` (lines 52-64). The actual deletion for the sharded case is delegated to `StateKvShardPruner`: [3](#0-2) [4](#0-3) 

The shard pruners vector is only populated when `enabled_sharding()` returns `true` and only processes the new shard databases, not the old ledger_db.

**Attack Scenario:**

1. A validator node runs with `enable_storage_sharding = false` for versions 0-1,000,000:
   - Stale indices accumulate in ledger_db under `StaleStateValueIndexSchema`
   - State values stored under `StateValueSchema`

2. Node operator enables sharding during AIP-97 migration. The database structure changes: [5](#0-4) 

When sharding is disabled, all databases point to the same `ledger_db`. When enabled, separate metadata and shard databases are created (lines 82-150 of the same file).

3. From version 1,000,001 onwards:
   - New stale indices written to `StaleStateValueIndexByKeyHashSchema` in shard databases
   - New state values written to `StateValueByKeyHashSchema`

4. When pruning executes:
   - Pruner checks `enabled_sharding()` which returns `true`
   - Metadata pruner goes to lines 38-50 (no deletion - just iterates)
   - Shard pruners handle only new shard databases
   - **The 1,000,000 old entries in ledger_db are NEVER pruned**

The configuration enforcement on production networks confirms this affects all nodes: [6](#0-5) 

All mainnet/testnet nodes MUST enable sharding or face a panic, meaning this transition occurred network-wide during AIP-97 deployment.

**Root Cause:** The pruning logic has no code to detect schema transitions, migrate data between schemas, clean up the old schema after migration, or handle coexistence of both schemas. The metadata pruner's sharding-enabled branch (lines 38-50) performs zero deletion operations, merely iterating through the new shard databases without any cleanup logic.

## Impact Explanation

**Severity: HIGH** (up to $50,000 per Aptos Bug Bounty)

This vulnerability causes:

1. **Unbounded Storage Growth**: Every state update creates a stale index entry. With millions of transactions, orphaned indices accumulate at ~100+ bytes per entry. Over months, this grows to hundreds of GB of unrecoverable disk space.

2. **Validator Node Failure**: When disk space exhausts, the node crashes and cannot restart. This requires manual intervention to restore operation, affecting network participation.

3. **State Inconsistency**: The database contains data that should have been pruned according to the configured pruning window, violating the storage retention policy.

4. **Network-Wide Impact**: The AIP-97 migration to sharding affects ALL validators simultaneously, meaning all nodes that transitioned accumulate orphaned data.

This qualifies as **"Validator node slowdowns"** (HIGH severity) because storage exhaustion causes performance degradation leading to eventual node failure. It also represents state inconsistencies requiring manual intervention to clean up orphaned data.

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability triggers automatically under the following conditions:

1. **Mainnet/Testnet Deployment**: The configuration explicitly requires sharding to be enabled for production networks, meaning ALL nodes must toggle this flag during AIP-97 migration.

2. **No Migration Logic**: There is no automatic migration or cleanup mechanism in the codebase. The transition is a pure configuration toggle with no cross-schema cleanup. The metadata pruner's sharding branch literally performs no deletions.

3. **Permanent Accumulation**: Once created, orphaned indices never disappear. They accumulate indefinitely until manual intervention.

4. **Systematic Issue**: This is not an edge case but a fundamental architectural flaw affecting every node that transitioned sharding configurations.

The likelihood is nearly 100% for any node that transitioned from non-sharded to sharded configuration during the AIP-97 deployment.

## Recommendation

Implement a migration path that:

1. **Detect schema transition**: Check if old schema entries exist in ledger_db when sharding is enabled
2. **Migrate and clean**: During pruning, also iterate and delete from the old `StaleStateValueIndexSchema` in ledger_db
3. **One-time cleanup**: Add a migration routine that runs once to clean up orphaned entries from the old schema

Example fix for the metadata pruner:

```rust
if self.state_kv_db.enabled_sharding() {
    // Clean up old schema entries from ledger_db if they exist
    // This handles the migration case
    let mut iter = self.state_kv_db.metadata_db().iter::<StaleStateValueIndexSchema>()?;
    iter.seek(&current_progress)?;
    for item in iter {
        let (index, _) = item?;
        if index.stale_since_version > target_version {
            break;
        }
        batch.delete::<StaleStateValueIndexSchema>(&index)?;
        batch.delete::<StateValueSchema>(&(index.state_key, index.version))?;
    }
    
    // Continue with normal shard iteration (no changes needed here)
    for shard_id in 0..num_shards {
        // existing code...
    }
}
```

## Proof of Concept

While a full PoC requires a complete node setup with transition, the vulnerability can be validated by examining the code:

1. Deploy a node with `enable_storage_sharding = false`
2. Run transactions to accumulate stale indices in `StaleStateValueIndexSchema`
3. Enable `enable_storage_sharding = true` and restart
4. Observe that new entries go to `StaleStateValueIndexByKeyHashSchema` in shard DBs
5. Run pruning and observe that old `StaleStateValueIndexSchema` entries remain in ledger_db
6. Verify disk usage continues to grow from orphaned entries

The code evidence clearly shows the metadata pruner's sharding branch (lines 38-50) performs no deletions, confirming the vulnerability without requiring runtime testing.

## Notes

This is a logic vulnerability in the storage pruning system that affects operational security. While not immediately catastrophic, the gradual storage exhaustion is inevitable and leads to validator node failure without manual database cleanup. The vulnerability is particularly severe because it affects all production nodes that underwent the AIP-97 sharding migration, making it a network-wide issue rather than an isolated case.

### Citations

**File:** storage/aptosdb/src/state_store/mod.rs (L985-1015)
```rust
    fn put_state_kv_index(
        batch: &mut NativeBatch,
        enable_sharding: bool,
        stale_since_version: Version,
        version: Version,
        key: &StateKey,
    ) {
        if enable_sharding {
            batch
                .put::<StaleStateValueIndexByKeyHashSchema>(
                    &StaleStateValueByKeyHashIndex {
                        stale_since_version,
                        version,
                        state_key_hash: key.hash(),
                    },
                    &(),
                )
                .unwrap();
        } else {
            batch
                .put::<StaleStateValueIndexSchema>(
                    &StaleStateValueIndex {
                        stale_since_version,
                        version,
                        state_key: (*key).clone(),
                    },
                    &(),
                )
                .unwrap();
        }
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs (L35-65)
```rust
        if self.state_kv_db.enabled_sharding() {
            let num_shards = self.state_kv_db.num_shards();
            // NOTE: This can be done in parallel if it becomes the bottleneck.
            for shard_id in 0..num_shards {
                let mut iter = self
                    .state_kv_db
                    .db_shard(shard_id)
                    .iter::<StaleStateValueIndexByKeyHashSchema>()?;
                iter.seek(&current_progress)?;
                for item in iter {
                    let (index, _) = item?;
                    if index.stale_since_version > target_version {
                        break;
                    }
                }
            }
        } else {
            let mut iter = self
                .state_kv_db
                .metadata_db()
                .iter::<StaleStateValueIndexSchema>()?;
            iter.seek(&current_progress)?;
            for item in iter {
                let (index, _) = item?;
                if index.stale_since_version > target_version {
                    break;
                }
                batch.delete::<StaleStateValueIndexSchema>(&index)?;
                batch.delete::<StateValueSchema>(&(index.state_key, index.version))?;
            }
        }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L47-72)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();

        let mut iter = self
            .db_shard
            .iter::<StaleStateValueIndexByKeyHashSchema>()?;
        iter.seek(&current_progress)?;
        for item in iter {
            let (index, _) = item?;
            if index.stale_since_version > target_version {
                break;
            }
            batch.delete::<StaleStateValueIndexByKeyHashSchema>(&index)?;
            batch.delete::<StateValueByKeyHashSchema>(&(index.state_key_hash, index.version))?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
            &DbMetadataValue::Version(target_version),
        )?;

        self.db_shard.write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L124-137)
```rust
        let shard_pruners = if state_kv_db.enabled_sharding() {
            let num_shards = state_kv_db.num_shards();
            let mut shard_pruners = Vec::with_capacity(num_shards);
            for shard_id in 0..num_shards {
                shard_pruners.push(StateKvShardPruner::new(
                    shard_id,
                    state_kv_db.db_shard_arc(shard_id),
                    metadata_progress,
                )?);
            }
            shard_pruners
        } else {
            Vec::new()
        };
```

**File:** storage/aptosdb/src/state_kv_db.rs (L63-80)
```rust
        if !sharding {
            info!("State K/V DB is not enabled!");
            return Ok(Self {
                state_kv_metadata_db: Arc::clone(&ledger_db),
                state_kv_db_shards: arr![Arc::clone(&ledger_db); 16],
                hot_state_kv_db_shards: None,
                enabled_sharding: false,
            });
        }

        Self::open_sharded(
            db_paths,
            rocksdb_configs.state_kv_db_config,
            env,
            block_cache,
            readonly,
        )
    }
```

**File:** config/src/config/storage_config.rs (L664-668)
```rust
            if (chain_id.is_testnet() || chain_id.is_mainnet())
                && config_yaml["rocksdb_configs"]["enable_storage_sharding"].as_bool() != Some(true)
            {
                panic!("Storage sharding (AIP-97) is not enabled in node config. Please follow the guide to migration your node, and set storage.rocksdb_configs.enable_storage_sharding to true explicitly in your node config. https://aptoslabs.notion.site/DB-Sharding-Migration-Public-Full-Nodes-1978b846eb7280b29f17ceee7d480730");
            }
```
