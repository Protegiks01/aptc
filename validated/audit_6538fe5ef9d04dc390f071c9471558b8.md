Based on my thorough analysis of the Aptos Core codebase, I have validated this security claim and found it to be a **VALID VULNERABILITY**.

# Audit Report

## Title
Critical Consensus Safety Violation: Silent Commit Failures in Persisting Phase Allow False Commitment Tracking

## Summary
The persisting phase in Aptos consensus unconditionally reports success even when individual block commits fail to persist to storage. This causes the buffer manager to incorrectly advance `highest_committed_round` for blocks that were never actually written to disk, creating dangerous state divergence between a node's in-memory belief about committed blocks and the actual ledger state.

## Finding Description

The vulnerability exists in a broken error handling chain across the consensus pipeline:

**1. Error Suppression in Persisting Phase**

The `PersistingPhase::process()` method always returns success regardless of commit failures: [1](#0-0) 

At line 74, the method unconditionally returns `Ok(blocks.last().expect("Blocks can't be empty").round())`, completely ignoring any errors that occurred during commit operations.

**2. Silent Error Discarding**

The `wait_for_commit_ledger()` method explicitly discards all results from the commit operation: [2](#0-1) 

Line 566 uses `let _ = fut.commit_ledger_fut.await` which throws away the Result type, silencing all errors including database failures, block-not-found errors, and pruning failures.

**3. Blind Trust in Buffer Manager**

The buffer manager directly updates `highest_committed_round` based solely on the persisting phase's response without any error handling: [3](#0-2) 

Line 971 sets `self.highest_committed_round = round` with no error case handling. The pattern match at line 968 only handles `Some(Ok(round))` with no branch for `Some(Err(e))`.

**4. Legitimate Commit Failures**

The underlying `commit_ledger` implementation has multiple failure paths: [4](#0-3) 

- Line 381: `self.block_tree.get_block(block_id)?` can return `ExecutorError::BlockNotFound`
- Line 390: `self.db.writer.commit_ledger(...)` can fail with I/O errors from database operations
- Lines 383-385: A fail_point exists specifically for testing commit failures, proving this is a known failure scenario

**5. Error Propagation Chain**

Errors from the executor are converted but then discarded: [5](#0-4) 

The `commit_ledger` function at lines 1099-1104 calls the executor and propagates errors with `?`, but these errors are then ignored by `wait_for_commit_ledger()`.

**Attack Scenario (Operational Failure Trigger):**

1. Consensus pipeline sends blocks B1, B2, B3 to persisting phase
2. B1 commits successfully to storage
3. B2's commit encounters database write failure (disk full/I/O error) or block tree inconsistency
4. The error is silently ignored by `wait_for_commit_ledger()` (line 566)
5. Persisting phase continues processing and returns `Ok(B3.round())`
6. Buffer manager sets `highest_committed_round = B3.round()`
7. **Node believes B2 and B3 are committed, but only B1 exists in storage**
8. Buffer manager cleans up pending blocks for rounds â‰¤ B3 (line 972)
9. On node restart, storage shows only B1 committed, but blocks B2, B3 were already cleaned from memory

## Impact Explanation

**Severity: Critical** (Total Loss of Liveness / Consensus Safety Violation)

This vulnerability qualifies under Critical Severity Categories #2 (Consensus/Safety Violations) and #4 (Total Loss of Liveness).

The `highest_committed_round` field is critical to consensus operations: [6](#0-5) 

Line 340 shows commit votes are filtered based on `highest_committed_round`. An incorrectly high value causes valid votes to be rejected. [7](#0-6) 

Line 909 shows backpressure calculations depend on this value, affecting block acceptance and processing.

**Consequences:**

- **Consensus Participation Failure**: Nodes reject valid commit votes, disrupting quorum formation
- **Permanent Data Loss**: Blocks cleaned from memory before being persisted are lost forever
- **State Divergence on Restart**: Node restarts with storage at round N but believed round N+K was committed
- **Network Liveness Loss**: If multiple validators hit disk errors simultaneously (realistic in infrastructure failures), consensus cannot progress
- **Validator Decommissioning Required**: Affected validators must resync entire state from peers

## Likelihood Explanation

**Likelihood: Medium-High**

This triggers automatically during operational failures:

1. **Database Write Failures**: Disk full, I/O errors, filesystem corruption - common in production environments
2. **Hardware Failures**: SSD failures, RAID degradation, network storage issues
3. **Known Testable Failure**: The fail_point at line 383-385 proves developers know this failure can occur [8](#0-7) 

4. **Block Tree Race Conditions**: The `BlockNotFound` error path indicates race conditions exist [9](#0-8) 

These scenarios occur during high load, resource exhaustion, rapid epoch transitions, or concurrent state sync operations. No attacker action required - this is a latent bug in error handling.

## Recommendation

Implement proper error handling throughout the commit chain:

1. **Fix `wait_for_commit_ledger()` to propagate errors**:
```rust
pub async fn wait_for_commit_ledger(&self) -> Result<(), ExecutorError> {
    if let Some(fut) = self.pipeline_futs() {
        fut.commit_ledger_fut.await?;
    }
    Ok(())
}
```

2. **Fix `PersistingPhase::process()` to handle and return errors**:
```rust
async fn process(&self, req: PersistingRequest) -> PersistingResponse {
    let PersistingRequest { blocks, commit_ledger_info } = req;
    
    for b in &blocks {
        if let Some(tx) = b.pipeline_tx().lock().as_mut() {
            tx.commit_proof_tx.take().map(|tx| tx.send(commit_ledger_info.clone()));
        }
        b.wait_for_commit_ledger().await?; // Propagate errors
    }
    
    let response = Ok(blocks.last().expect("Blocks can't be empty").round());
    if commit_ledger_info.ledger_info().ends_epoch() {
        self.commit_msg_tx.send_epoch_change(EpochChangeProof::new(vec![commit_ledger_info], false)).await;
    }
    response
}
```

3. **Fix buffer manager to handle errors**:
```rust
Some(Ok(round)) = self.persisting_phase_rx.next() => {
    self.pending_commit_votes = self.pending_commit_votes.split_off(&(round + 1));
    self.highest_committed_round = round;
    self.pending_commit_blocks = self.pending_commit_blocks.split_off(&(round + 1));
},
Some(Err(e)) = self.persisting_phase_rx.next() => {
    error!("Persisting phase failed: {:?}", e);
    // Trigger recovery: request state sync, alert monitoring, etc.
}
```

4. **Add retry logic with exponential backoff** for transient storage failures
5. **Add alerting/monitoring** when commit failures occur

## Proof of Concept

The fail_point mechanism already demonstrates this vulnerability can be triggered: [8](#0-7) 

A test case can inject failures at this point and observe that the persisting phase still returns success while storage remains uncommitted.

**Notes:**

This is a serious consensus infrastructure bug that breaks the fundamental assumption that committed rounds in memory match committed rounds in storage. While it requires operational failures to trigger (not malicious input), such failures are realistic in production environments. The presence of the fail_point proves developers anticipated commit failures but the error handling chain is incomplete. This vulnerability could cause permanent data loss and consensus liveness failures across multiple validators during infrastructure incidents.

### Citations

**File:** consensus/src/pipeline/persisting_phase.rs (L59-81)
```rust
    async fn process(&self, req: PersistingRequest) -> PersistingResponse {
        let PersistingRequest {
            blocks,
            commit_ledger_info,
        } = req;

        for b in &blocks {
            if let Some(tx) = b.pipeline_tx().lock().as_mut() {
                tx.commit_proof_tx
                    .take()
                    .map(|tx| tx.send(commit_ledger_info.clone()));
            }
            b.wait_for_commit_ledger().await;
        }

        let response = Ok(blocks.last().expect("Blocks can't be empty").round());
        if commit_ledger_info.ledger_info().ends_epoch() {
            self.commit_msg_tx
                .send_epoch_change(EpochChangeProof::new(vec![commit_ledger_info], false))
                .await;
        }
        response
    }
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L562-568)
```rust
    pub async fn wait_for_commit_ledger(&self) {
        // may be aborted (e.g. by reset)
        if let Some(fut) = self.pipeline_futs() {
            // this may be cancelled
            let _ = fut.commit_ledger_fut.await;
        }
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L335-350)
```rust
    fn try_add_pending_commit_vote(&mut self, vote: CommitVote) -> bool {
        let block_id = vote.commit_info().id();
        let round = vote.commit_info().round();

        // Don't need to store commit vote if we have already committed up to that round
        if round <= self.highest_committed_round {
            true
        } else
        // Store the commit vote only if it is for one of the next 100 rounds.
        if round > self.highest_committed_round
            && self.highest_committed_round + self.max_pending_rounds_in_commit_vote_cache > round
        {
            self.pending_commit_votes
                .entry(round)
                .or_default()
                .insert(vote.author(), vote);
```

**File:** consensus/src/pipeline/buffer_manager.rs (L906-910)
```rust
    fn need_back_pressure(&self) -> bool {
        const MAX_BACKLOG: Round = 20;

        self.back_pressure_enabled && self.highest_committed_round + MAX_BACKLOG < self.latest_round
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L968-973)
```rust
                Some(Ok(round)) = self.persisting_phase_rx.next() => {
                    // see where `need_backpressure()` is called.
                    self.pending_commit_votes = self.pending_commit_votes.split_off(&(round + 1));
                    self.highest_committed_round = round;
                    self.pending_commit_blocks = self.pending_commit_blocks.split_off(&(round + 1));
                },
```

**File:** execution/executor/src/block_executor/mod.rs (L362-395)
```rust
    fn commit_ledger(&self, ledger_info_with_sigs: LedgerInfoWithSignatures) -> ExecutorResult<()> {
        let _timer = OTHER_TIMERS.timer_with(&["commit_ledger"]);

        let block_id = ledger_info_with_sigs.ledger_info().consensus_block_id();
        info!(
            LogSchema::new(LogEntry::BlockExecutor).block_id(block_id),
            "commit_ledger"
        );

        // Check for any potential retries
        // TODO: do we still have such retries?
        let committed_block = self.block_tree.root_block();
        if committed_block.num_persisted_transactions()?
            == ledger_info_with_sigs.ledger_info().version() + 1
        {
            return Ok(());
        }

        // Confirm the block to be committed is tracked in the tree.
        self.block_tree.get_block(block_id)?;

        fail_point!("executor::commit_blocks", |_| {
            Err(anyhow::anyhow!("Injected error in commit_blocks.").into())
        });

        let target_version = ledger_info_with_sigs.ledger_info().version();
        self.db
            .writer
            .commit_ledger(target_version, Some(&ledger_info_with_sigs), None)?;

        self.block_tree.prune(ledger_info_with_sigs.ledger_info())?;

        Ok(())
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L1098-1105)
```rust
        tokio::task::spawn_blocking(move || {
            executor
                .commit_ledger(ledger_info_with_sigs_clone)
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
        Ok(Some(ledger_info_with_sigs))
```

**File:** execution/executor-types/src/error.rs (L14-15)
```rust
    #[error("Cannot find speculation result for block id {0}")]
    BlockNotFound(HashValue),
```
