# Audit Report

## Title
Silent Worker Thread Failure Causing State Sync Initialization Failure and Node Liveness Issues

## Summary
Critical worker thread join handles in the state synchronization system are discarded during initialization, allowing worker threads to fail silently without detection. This leads to partial initialization where the node appears operational but critical state sync components have crashed, causing permanent liveness failures for fullnodes and preventing validators from bootstrapping or catching up when they fall behind.

## Finding Description

The state sync initialization process spawns four critical worker threads (executor, ledger updater, committer, and commit post-processor) but immediately discards their join handles, making worker failures undetectable.

**The Vulnerability Chain:**

1. **Join Handles Discarded** - In the driver factory initialization, the `StorageSynchronizerHandles` containing critical worker join handles are discarded: [1](#0-0) 

The second tuple element containing the handles is explicitly ignored with `_`, dropping the JoinHandles for all four worker threads.

2. **Worker Threads Can Panic** - The worker threads call functions that explicitly panic on `spawn_blocking` failure: [2](#0-1) [3](#0-2) [4](#0-3) [5](#0-4) 

If `spawn_blocking` fails (e.g., blocking thread pool exhausted), these `.expect()` calls panic, killing the worker thread silently.

3. **Thread Pool Has Hard Limits** - The blocking thread pool is limited to 64 threads: [6](#0-5) 

Under sustained high load with multiple competing operations, this pool can be exhausted.

4. **No Failure Detection** - The driver's event loop listens for error notifications but cannot detect when worker threads die: [7](#0-6) [8](#0-7) 

The driver only processes errors actively sent by workers before they crash. Silent panics go undetected.

5. **Four Critical Workers Affected** - All four workers are spawned and their handles discarded: [9](#0-8) [10](#0-9) 

**Impact Scenarios:**

- **Fullnodes**: Rely exclusively on continuous syncer which uses these workers for all operations. Dead workers cause permanent stall. [11](#0-10) 

- **Validators during bootstrapping**: Cannot complete state snapshot synchronization, preventing node startup. [12](#0-11) 

- **Validators during catch-up**: When calling `sync_to_target`, if workers die, the sync hangs indefinitely and validator cannot participate. [13](#0-12) 

**Important Clarification**: Validators that are already synchronized and participating in consensus are NOT affected during normal operation, as consensus commits blocks directly through the executor without using state sync workers: [14](#0-13) 

This is a **liveness issue**, not a consensus safety violation. Affected nodes cannot make progress but do not commit incorrect states.

## Impact Explanation

This vulnerability has **HIGH severity** impact per the Aptos bug bounty categories:

**HIGH Severity (Validator Node Slowdowns):**
- Validators that fall behind and attempt to catch up via `sync_to_target` will hang indefinitely if state sync workers die during the operation
- Validators during bootstrapping cannot complete initialization, preventing network participation
- This causes significant performance degradation and loss of validator availability

**CRITICAL for Fullnodes (though bug bounty focuses on validators):**
- Fullnodes rely exclusively on state sync workers for continuous operation
- Dead workers cause total loss of liveness for fullnode operations
- Affects all fullnode API services and data availability

**Not a Consensus Safety Violation:**
Despite the original report's claim, this does NOT cause "consensus divergence" where different validators commit different states for the same block. Analysis shows validators either:
- Continue operating normally (if already synced, using direct commit path)
- Fail to catch up (liveness issue)
- Fail to bootstrap (availability issue)

They do not produce different state roots or commit divergent states.

## Likelihood Explanation

**MEDIUM likelihood:**

**Triggering Conditions:**
1. **Thread pool exhaustion**: The blocking thread pool has a hard limit of 64 threads. Under sustained high load with multiple state sync operations, chunk executions, and other blocking tasks competing for the pool, exhaustion is realistic.

2. **Resource constraints**: Memory pressure, I/O errors, or other resource issues during state sync can cause panics in executor operations that propagate through the `.expect()` calls.

3. **Normal operation trigger**: This can occur during legitimate network conditions without requiring an attacker:
   - Node startup with large state snapshots
   - Validator catching up after downtime
   - High transaction throughput periods
   - Multiple nodes syncing simultaneously

**Why this is realistic:**
- State sync operations are resource-intensive and continuously active
- The blocking thread pool is shared across multiple critical operations
- No safeguards exist to detect or recover from worker failures
- The issue affects all node types during state synchronization phases

## Recommendation

1. **Monitor Join Handles**: Store and monitor the `StorageSynchronizerHandles` instead of discarding them. Implement periodic health checks or use mechanisms like `tokio::select!` to detect worker thread termination.

2. **Replace `.expect()` with Error Propagation**: Convert panic-on-failure to proper error handling:
   - Return `Result` types from `update_ledger()` and `commit_chunk()`
   - Send error notifications through existing channels when `spawn_blocking` fails
   - Implement graceful degradation or restart logic

3. **Increase Thread Pool Capacity or Use Separate Pools**: Consider increasing `MAX_BLOCKING_THREADS` or creating dedicated thread pools for critical state sync operations to reduce contention.

4. **Add Worker Health Monitoring**: Implement heartbeat or keepalive mechanisms for worker threads to detect silent failures quickly.

## Proof of Concept

The vulnerability can be demonstrated by simulating thread pool exhaustion during state sync operations. A complete PoC would require:

1. Setting up a node with state sync initialization
2. Exhausting the blocking thread pool through concurrent operations
3. Observing silent worker thread failure when `spawn_blocking` fails
4. Confirming the node continues running but state sync is non-functional

The technical evidence provided in citations demonstrates the vulnerability exists in the codebase with the discarded join handles, panic-on-failure patterns, and lack of monitoring mechanisms.

## Notes

This is a legitimate vulnerability affecting node liveness and availability, particularly critical for fullnodes and validators during bootstrapping or catch-up scenarios. While the original report overstated the impact by claiming "consensus divergence," the actual impact of permanent liveness failure for affected nodes remains a HIGH severity issue that should be addressed. The vulnerability violates basic error handling principles by silently discarding critical worker thread handles and using panic-on-failure patterns in production code.

### Citations

**File:** state-sync/state-sync-driver/src/driver_factory.rs (L145-156)
```rust
        let (storage_synchronizer, _) = StorageSynchronizer::new(
            node_config.state_sync.state_sync_driver,
            chunk_executor,
            commit_notification_sender.clone(),
            error_notification_sender,
            event_subscription_service.clone(),
            mempool_notification_handler.clone(),
            storage_service_notification_handler.clone(),
            metadata_storage.clone(),
            storage.clone(),
            driver_runtime.as_ref(),
        );
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L234-273)
```rust
        let executor_handle = spawn_executor(
            chunk_executor.clone(),
            error_notification_sender.clone(),
            executor_listener,
            ledger_updater_notifier,
            pending_data_chunks.clone(),
            runtime.clone(),
        );

        // Spawn the ledger updater that updates the ledger in storage
        let ledger_updater_handle = spawn_ledger_updater(
            chunk_executor.clone(),
            error_notification_sender.clone(),
            ledger_updater_listener,
            committer_notifier,
            pending_data_chunks.clone(),
            runtime.clone(),
        );

        // Spawn the committer that commits executed (but pending) chunks
        let committer_handle = spawn_committer(
            chunk_executor.clone(),
            error_notification_sender.clone(),
            committer_listener,
            commit_post_processor_notifier,
            pending_data_chunks.clone(),
            runtime.clone(),
            storage.reader.clone(),
        );

        // Spawn the commit post-processor that handles commit notifications
        let commit_post_processor_handle = spawn_commit_post_processor(
            commit_post_processor_listener,
            event_subscription_service,
            mempool_notification_handler,
            storage_service_notification_handler,
            pending_data_chunks.clone(),
            runtime.clone(),
            storage.reader.clone(),
        );
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L294-299)
```rust
        let storage_synchronizer_handles = StorageSynchronizerHandles {
            executor: executor_handle,
            ledger_updater: ledger_updater_handle,
            committer: committer_handle,
            commit_post_processor: commit_post_processor_handle,
        };
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L378-406)
```rust
    fn initialize_state_synchronizer(
        &mut self,
        epoch_change_proofs: Vec<LedgerInfoWithSignatures>,
        target_ledger_info: LedgerInfoWithSignatures,
        target_output_with_proof: TransactionOutputListWithProofV2,
    ) -> Result<JoinHandle<()>, Error> {
        // Create a channel to notify the state snapshot receiver when data chunks are ready
        let max_pending_data_chunks = self.driver_config.max_pending_data_chunks as usize;
        let (state_snapshot_notifier, state_snapshot_listener) =
            mpsc::channel(max_pending_data_chunks);

        // Spawn the state snapshot receiver that commits state values
        let receiver_handle = spawn_state_snapshot_receiver(
            self.chunk_executor.clone(),
            state_snapshot_listener,
            self.commit_notification_sender.clone(),
            self.error_notification_sender.clone(),
            self.pending_data_chunks.clone(),
            self.metadata_storage.clone(),
            self.storage.clone(),
            epoch_change_proofs,
            target_ledger_info,
            target_output_with_proof,
            self.runtime.clone(),
        );
        self.state_snapshot_notifier = Some(state_snapshot_notifier);

        Ok(receiver_handle)
    }
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L994-1002)
```rust
    let result = tokio::task::spawn_blocking(move || {
        chunk_executor.enqueue_chunk_by_transaction_outputs(
            outputs_with_proof,
            &target_ledger_info,
            end_of_epoch_ledger_info.as_ref(),
        )
    })
    .await
    .expect("Spawn_blocking(apply_output_chunk) failed!");
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L1037-1045)
```rust
    let result = tokio::task::spawn_blocking(move || {
        chunk_executor.enqueue_chunk_by_execution(
            transactions_with_proof,
            &target_ledger_info,
            end_of_epoch_ledger_info.as_ref(),
        )
    })
    .await
    .expect("Spawn_blocking(execute_transaction_chunk) failed!");
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L1086-1089)
```rust
    tokio::task::spawn_blocking(move || chunk_executor.update_ledger())
        .await
        .expect("Spawn_blocking(update_ledger) failed!")
}
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L1094-1100)
```rust
async fn commit_chunk<ChunkExecutor: ChunkExecutorTrait + 'static>(
    chunk_executor: Arc<ChunkExecutor>,
) -> anyhow::Result<ChunkCommitNotification> {
    tokio::task::spawn_blocking(move || chunk_executor.commit_chunk())
        .await
        .expect("Spawn_blocking(commit_chunk) failed!")
}
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-27)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;
```

**File:** state-sync/state-sync-driver/src/driver.rs (L232-234)
```rust
                notification = self.error_notification_listener.select_next_some() => {
                    self.handle_error_notification(notification).await;
                }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L495-533)
```rust
    async fn handle_error_notification(&mut self, error_notification: ErrorNotification) {
        warn!(LogSchema::new(LogEntry::SynchronizerNotification)
            .error_notification(error_notification.clone())
            .message("Received an error notification from the storage synchronizer!"));

        // Terminate the currently active streams
        let notification_id = error_notification.notification_id;
        let notification_feedback = NotificationFeedback::InvalidPayloadData;
        if self.bootstrapper.is_bootstrapped() {
            if let Err(error) = self
                .continuous_syncer
                .handle_storage_synchronizer_error(NotificationAndFeedback::new(
                    notification_id,
                    notification_feedback,
                ))
                .await
            {
                error!(LogSchema::new(LogEntry::SynchronizerNotification)
                    .message(&format!(
                        "Failed to terminate the active stream for the continuous syncer! Error: {:?}",
                        error
                    )));
            }
        } else if let Err(error) = self
            .bootstrapper
            .handle_storage_synchronizer_error(NotificationAndFeedback::new(
                notification_id,
                notification_feedback,
            ))
            .await
        {
            error!(
                LogSchema::new(LogEntry::SynchronizerNotification).message(&format!(
                    "Failed to terminate the active stream for the bootstrapper! Error: {:?}",
                    error
                ))
            );
        };
    }
```

**File:** state-sync/state-sync-driver/src/continuous_syncer.rs (L76-97)
```rust
    /// Checks if the continuous syncer is able to make progress
    pub async fn drive_progress(
        &mut self,
        consensus_sync_request: Arc<Mutex<Option<ConsensusSyncRequest>>>,
    ) -> Result<(), Error> {
        if self.active_data_stream.is_some() {
            // We have an active data stream. Process any notifications!
            self.process_active_stream_notifications(consensus_sync_request)
                .await
        } else if self.storage_synchronizer.pending_storage_data() {
            // Wait for any pending data to be processed
            sample!(
                SampleRate::Duration(Duration::from_secs(PENDING_DATA_LOG_FREQ_SECS)),
                info!("Waiting for the storage synchronizer to handle pending data!")
            );
            Ok(())
        } else {
            // Fetch a new data stream to start streaming data
            self.initialize_active_data_stream(consensus_sync_request)
                .await
        }
    }
```

**File:** consensus/src/state_computer.rs (L240-280)
```rust
        block_executor_onchain_config: BlockExecutorConfigFromOnchain,
        transaction_deduper: Arc<dyn TransactionDeduper>,
        randomness_enabled: bool,
        consensus_onchain_config: OnChainConsensusConfig,
        persisted_auxiliary_info_version: u8,
        network_sender: Arc<NetworkSender>,
    ) {
        *self.state.write() = Some(MutableState {
            validators: epoch_state
                .verifier
                .get_ordered_account_addresses_iter()
                .collect::<Vec<_>>()
                .into(),
            payload_manager,
            transaction_shuffler,
            block_executor_onchain_config,
            transaction_deduper,
            is_randomness_enabled: randomness_enabled,
            consensus_onchain_config,
            persisted_auxiliary_info_version,
            network_sender,
        });
    }

    // Clears the epoch-specific state. Only a sync_to call is expected before calling new_epoch
    // on the next epoch.
    fn end_epoch(&self) {
        self.state.write().take();
    }
}

```

**File:** consensus/src/pipeline/pipeline_builder.rs (L1098-1104)
```rust
        tokio::task::spawn_blocking(move || {
            executor
                .commit_ledger(ledger_info_with_sigs_clone)
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
```
