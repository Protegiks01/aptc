# Audit Report

## Title
Secret Share Manager Not Reset During Consensus Sync Operations Leading to Consensus Liveness Failure

## Summary
The `reset()` method in `ExecutionProxyClient` fails to reset the `SecretShareManager` during state synchronization operations (`sync_to_target()` and `sync_for_duration()`), while correctly resetting the `RandManager` and `BufferManager`. This causes the `SecretShareManager` to maintain stale state after a validator syncs, leading to rejection of valid secret shares, coordinator deadlocks, and consensus liveness failures.

## Finding Description

When a validator falls behind and needs to synchronize state, the consensus layer calls either `sync_to_target()` or `sync_for_duration()` to catch up. Both methods call the `reset()` method to reset internal consensus components to the new synchronized state. [1](#0-0) 

The `reset()` method retrieves and resets only two of the three consensus managers. It retrieves `reset_tx_to_rand_manager` and `reset_tx_to_buffer_manager` from the handle, but does NOT retrieve or reset `reset_tx_to_secret_share_manager`: [2](#0-1) 

In contrast, the `end_epoch()` method correctly retrieves and resets all three managers including `reset_tx_to_secret_share_manager`: [3](#0-2) 

The `SecretShareManager` maintains critical state that must be synchronized, including the `highest_known_round` field used for validation. When a reset request is received, the `process_reset()` method properly updates this state: [4](#0-3) 

However, since `reset()` never sends this reset request to the `SecretShareManager`, the state remains stale. When secret shares arrive after sync, they are validated against `highest_known_round + FUTURE_ROUNDS_TO_ACCEPT` (200 rounds): [5](#0-4) [6](#0-5) 

If a validator syncs forward by more than 200 rounds without resetting the secret share manager, all incoming secret shares will be rejected as "from future round".

**Attack Scenario:**

1. Secret sharing is enabled (when `secret_sharing_config` is `Some` during epoch start)
2. Validator is processing blocks at round N with `highest_known_round = N`
3. Validator falls behind and needs to sync to round N+300
4. Consensus calls `sync_to_target()` which invokes `reset()`
5. RandManager and BufferManager are reset to round N+300
6. **SecretShareManager is NOT reset and maintains `highest_known_round = N`**
7. Validator resumes consensus at round N+300
8. New blocks arrive requiring secret sharing for decryption
9. Incoming secret shares for round N+300 are rejected (N+300 > N+200)
10. The coordinator waits indefinitely for secret shares that will never arrive

The coordinator requires both `rand_ready` and `secret_ready` flags before forwarding blocks to execution: [7](#0-6) 

Without valid secret shares, blocks remain stuck in the coordinator indefinitely, causing consensus liveness failure for that validator. The decryption pipeline explicitly waits for the secret shared key: [8](#0-7) 

## Impact Explanation

This is a **High Severity** vulnerability per Aptos bug bounty criteria:

**Validator Node Slowdowns/Liveness Failure**: Validators that sync while secret sharing is enabled will be unable to process blocks requiring secret shares, causing them to fall out of consensus. This degrades network performance and can lead to validator set disruption.

**Consensus Liveness Issues**: If multiple validators experience this issue simultaneously (e.g., after a network partition), the network could experience significant liveness degradation until validators are manually restarted or the epoch changes (which triggers `end_epoch()` that correctly resets all managers).

This aligns with the High Severity category (up to $50,000) in the Aptos bug bounty program for validator node slowdowns and consensus liveness issues. It does not constitute Critical severity because:
- It does not directly cause loss of funds
- It does not enable permanent chain splits (recoverable via epoch change or restart)
- It requires specific conditions (sync + secret sharing enabled)

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability will trigger whenever:
1. Secret sharing is enabled in production (encrypted transactions feature)
2. A validator falls behind by more than 200 rounds (approximately 20 seconds at 10 rounds/second)
3. The validator calls `sync_to_target()` or `sync_for_duration()` to catch up

These conditions are realistic in production environments:
- Validators regularly experience temporary network issues
- Validators restart and need to sync
- The 200-round threshold is relatively small

The bug is deterministic and will occur every time these conditions are met. The primary uncertainty is whether secret sharing is currently enabled in production, which depends on whether the encrypted transactions feature is active. Regardless of current deployment status, this is a latent bug that requires fixing before or when the feature becomes active.

## Recommendation

Modify the `reset()` method to include `reset_tx_to_secret_share_manager` in the same way that `end_epoch()` does:

```rust
async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
    let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager, reset_tx_to_secret_share_manager) = {
        let handle = self.handle.read();
        (
            handle.reset_tx_to_rand_manager.clone(),
            handle.reset_tx_to_buffer_manager.clone(),
            handle.reset_tx_to_secret_share_manager.clone(), // ADD THIS LINE
        )
    };

    if let Some(mut reset_tx) = reset_tx_to_rand_manager {
        let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::RandResetDropped)?;
        ack_rx.await.map_err(|_| Error::RandResetDropped)?;
    }

    // ADD THIS BLOCK
    if let Some(mut reset_tx) = reset_tx_to_secret_share_manager {
        let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::SecretShareResetDropped)?;
        ack_rx.await.map_err(|_| Error::SecretShareResetDropped)?;
    }

    if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
        let (tx, rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::ResetDropped)?;
        rx.await.map_err(|_| Error::ResetDropped)?;
    }

    Ok(())
}
```

Also update the comment at line 653 to reflect that all three managers are reset, not just rand and buffer managers.

## Proof of Concept

The vulnerability can be demonstrated by examining the code flow:

1. A validator at round 100 with secret sharing enabled has `SecretShareManager.highest_known_round = 100`
2. Validator falls behind and needs to sync to round 400
3. `sync_to_target()` is called with target round 400
4. `reset()` method executes but only resets RandManager and BufferManager
5. `SecretShareManager.highest_known_round` remains at 100
6. New blocks arrive at round 400 requiring secret shares
7. Secret shares for round 400 are rejected because `400 > 100 + 200`
8. Coordinator blocks waiting for `secret_ready` flag that will never be set
9. Validator cannot progress, experiencing liveness failure

A complete Rust integration test would require setting up a full consensus environment with secret sharing enabled, which is complex. However, the bug is clearly evident from the code inspection showing the inconsistency between `reset()` and `end_epoch()` methods.

## Notes

The vulnerability exists in the production consensus code and affects the encrypted transactions feature when enabled. The fix is straightforward - ensure consistency between `reset()` and `end_epoch()` methods by including `SecretShareManager` in the reset process. The epoch change mechanism (`end_epoch()`) provides temporary mitigation by correctly resetting all managers, but validators will experience liveness issues between epoch changes when they perform sync operations.

### Citations

**File:** consensus/src/pipeline/execution_client.rs (L332-360)
```rust
            loop {
                let entry = select! {
                    Some(ordered_blocks) = ordered_block_rx.next() => {
                        let _ = rand_manager_input_tx.send(ordered_blocks.clone()).await;
                        let _ = secret_share_manager_input_tx.send(ordered_blocks.clone()).await;
                        let first_block_id = ordered_blocks.ordered_blocks.first().expect("Cannot be empty").id();
                        inflight_block_tracker.insert(first_block_id, (ordered_blocks, false, false));
                        inflight_block_tracker.entry(first_block_id)
                    },
                    Some(rand_ready_block) = rand_ready_block_rx.next() => {
                        let first_block_id = rand_ready_block.ordered_blocks.first().expect("Cannot be empty").id();
                        inflight_block_tracker.entry(first_block_id).and_modify(|result| {
                            result.1 = true;
                        })
                    },
                    Some(secret_ready_block) = secret_ready_block_rx.next() => {
                        let first_block_id = secret_ready_block.ordered_blocks.first().expect("Cannot be empty").id();
                        inflight_block_tracker.entry(first_block_id).and_modify(|result| {
                            result.2 = true;
                        })
                    },
                };
                let Entry::Occupied(o) = entry else {
                    unreachable!("Entry must exist");
                };
                if o.get().1 && o.get().2 {
                    let (_, (ordered_blocks, _, _)) = o.remove_entry();
                    let _ = ready_block_tx.send(ordered_blocks).await;
                }
```

**File:** consensus/src/pipeline/execution_client.rs (L642-672)
```rust
    async fn sync_for_duration(
        &self,
        duration: Duration,
    ) -> Result<LedgerInfoWithSignatures, StateSyncError> {
        fail_point!("consensus::sync_for_duration", |_| {
            Err(anyhow::anyhow!("Injected error in sync_for_duration").into())
        });

        // Sync for the specified duration
        let result = self.execution_proxy.sync_for_duration(duration).await;

        // Reset the rand and buffer managers to the new synced round
        if let Ok(latest_synced_ledger_info) = &result {
            self.reset(latest_synced_ledger_info).await?;
        }

        result
    }

    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });

        // Reset the rand and buffer managers to the target round
        self.reset(&target).await?;

        // TODO: handle the state sync error (e.g., re-push the ordered
        // blocks to the buffer manager when it's reset but sync fails).
        self.execution_proxy.sync_to_target(target).await
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L674-709)
```rust
    async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
        let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager) = {
            let handle = self.handle.read();
            (
                handle.reset_tx_to_rand_manager.clone(),
                handle.reset_tx_to_buffer_manager.clone(),
            )
        };

        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        }

        if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
            // reset execution phase and commit phase
            let (tx, rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::ResetDropped)?;
            rx.await.map_err(|_| Error::ResetDropped)?;
        }

        Ok(())
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L711-760)
```rust
    async fn end_epoch(&self) {
        let (
            reset_tx_to_rand_manager,
            reset_tx_to_buffer_manager,
            reset_tx_to_secret_share_manager,
        ) = {
            let mut handle = self.handle.write();
            handle.reset()
        };

        if let Some(mut tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop rand manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop rand manager");
        }

        if let Some(mut tx) = reset_tx_to_secret_share_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop secret share manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop secret share manager");
        }

        if let Some(mut tx) = reset_tx_to_buffer_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop buffer manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop buffer manager");
        }
        self.execution_proxy.end_epoch();
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L172-184)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.secret_share_store
            .lock()
            .update_highest_known_round(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L259-266)
```rust
    pub fn add_share(&mut self, share: SecretShare) -> anyhow::Result<bool> {
        let weight = self.secret_share_config.get_peer_weight(share.author());
        let metadata = share.metadata();
        ensure!(metadata.epoch == self.epoch, "Share from different epoch");
        ensure!(
            metadata.round <= self.highest_known_round + FUTURE_ROUNDS_TO_ACCEPT,
            "Share from future round"
        );
```

**File:** consensus/src/rand/secret_sharing/types.rs (L16-16)
```rust
pub const FUTURE_ROUNDS_TO_ACCEPT: u64 = 200;
```

**File:** consensus/src/pipeline/decryption_pipeline_builder.rs (L115-119)
```rust
        let maybe_decryption_key = secret_shared_key_rx
            .await
            .expect("decryption key should be available");
        // TODO(ibalajiarun): account for the case where decryption key is not available
        let decryption_key = maybe_decryption_key.expect("decryption key should be available");
```
