# Audit Report

## Title
Fast Sync Storage Wrapper Race Condition Causes Inconsistent Transaction Proof Generation During Status Transitions

## Summary
The `FastSyncStorageWrapper` in Aptos Core's storage layer contains a race condition where long-running read operations can span a fast sync status transition, causing transaction iterators and proof generation to read from different underlying databases. This produces cryptographically inconsistent responses containing transactions from genesis (version 0) and accumulator proofs from the restored snapshot (version N), violating state consistency guarantees.

## Finding Description

The `FastSyncStorageWrapper` maintains two separate `AptosDB` instances during fast sync bootstrapping:
- `temporary_db_with_genesis`: Contains genesis data at version 0
- `db_for_fast_sync`: Contains the restored state snapshot at version N [1](#0-0) 

The `DbReader` trait implementation delegates all operations through `get_read_delegatee()`, which calls `get_aptos_db_read_ref()` to select the appropriate database: [2](#0-1) [3](#0-2) 

The delegation macro ensures **every** `DbReader` method call independently invokes `get_read_delegatee()`: [4](#0-3) 

**The Vulnerability**: When the storage service processes transaction requests using size-aware chunking, it creates iterators at one point in time, consumes them over a duration, then generates proofs. If `finalize_state_snapshot()` executes during iterator consumption, the fast sync status transitions from STARTED to FINISHED: [5](#0-4) 

This causes the following race condition sequence in `get_transactions_with_proof_by_size`:

1. Iterator creation reads from `temporary_db_with_genesis` (status != FINISHED): [6](#0-5) 

2. Iterators are consumed while status may transition: [7](#0-6) 

3. Proof generation reads from `db_for_fast_sync` (status now FINISHED): [8](#0-7) 

The storage service operates independently in its own runtime with no synchronization to the bootstrapping process: [9](#0-8) 

Genesis is applied to the temporary database during initialization: [10](#0-9) 

**Result**: The response contains transactions/transaction infos from version 0 (genesis database) paired with accumulator proofs from version N (snapshot database), creating cryptographically inconsistent data that cannot be verified.

## Impact Explanation

**Severity: HIGH**

This vulnerability meets the HIGH severity criteria under "Significant protocol violations":

1. **State Synchronization Corruption**: Peer nodes receiving these responses will fail cryptographic verification because the accumulator proof cannot verify transactions from a different ledger version. This causes state sync to abort with verification errors, preventing nodes from synchronizing.

2. **Network-Wide Propagation**: During fast sync bootstrapping, nodes serve data to peers through the storage service. Multiple nodes bootstrapping simultaneously (common during network upgrades or testnet resets) could all serve corrupted data, propagating the inconsistency throughout the network.

3. **Protocol Invariant Violation**: This directly breaks the fundamental guarantee that all data in a cryptographic proof bundle must originate from the same consistent ledger state. Merkle accumulator proofs become mathematically unverifiable when paired with transactions from a different version.

4. **Recoverable but Disruptive**: While nodes can retry synchronization, the inconsistent data causes immediate verification failures, delays in node bootstrapping, and potential cascading failures if multiple nodes encounter this simultaneously during critical network events.

The vulnerability does not require any trusted role compromise and affects the core state synchronization protocol used by all Aptos nodes.

## Likelihood Explanation

**Likelihood: MEDIUM**

The vulnerability requires specific timing but is practically exploitable:

**Triggering Conditions**:
- Storage service request must start during STARTED status
- Request must use size-aware chunking with iterators (enabled by default)
- `finalize_state_snapshot()` must complete during iterator consumption
- Window duration is configurable via `max_storage_read_wait_time_ms`

**Exploitation Factors**:
1. Fast sync is a standard operation during node bootstrapping (not rare)
2. Any peer node can trigger storage service requests (no special access required)
3. Multiple nodes bootstrap simultaneously during network upgrades (natural conditions)
4. Large transaction batches increase the race window duration (attacker can request large ranges)
5. Only a single `RwLock` on status, no synchronization between reads and status transitions
6. The window is measurable (seconds) rather than microseconds

**Real-World Scenario**: When validator nodes bootstrap after a network upgrade or during testnet resets, they simultaneously request transaction data from peers. If any peer is completing fast sync, the race window naturally occurs.

## Recommendation

Implement atomic read operations that guarantee all data in a single response originates from the same database instance. Options include:

1. **Acquire read lock for entire operation**: Hold a read lock on `fast_sync_status` from iterator creation through proof generation, preventing status transitions during the operation.

2. **Capture database reference once**: At the start of each storage service request, capture a single database reference and use it for all operations in that request, ensuring consistency.

3. **Version-aware delegation**: Make `get_aptos_db_read_ref()` return a versioned reference that remains stable for the duration of related operations.

**Recommended Fix**:
```rust
// In FastSyncStorageWrapper, add a method to get a consistent reader
pub fn get_consistent_reader(&self) -> ConsistentReader {
    let db_ref = if self.is_fast_sync_bootstrap_finished() {
        self.db_for_fast_sync.clone()
    } else {
        self.temporary_db_with_genesis.clone()
    };
    ConsistentReader { db: db_ref }
}
```

Then modify storage service methods to use the consistent reader for all operations within a single request.

## Proof of Concept

While a full PoC requires orchestrating timing between threads, the vulnerability can be demonstrated through code inspection:

```rust
// Conceptual test showing the race condition
#[test]
fn test_fast_sync_race_condition() {
    // 1. Setup: FastSyncStorageWrapper in STARTED status
    // 2. Thread 1: Begin storage service request
    //    - Create iterators (reads from temporary_db_with_genesis)
    // 3. Thread 2: Call finalize_state_snapshot()
    //    - Status changes to FINISHED
    // 4. Thread 1: Continue request
    //    - Generate proofs (reads from db_for_fast_sync)
    // 5. Assert: Transaction data and proofs are from different databases
}
```

The vulnerability is evident from the code structure: each `DbReader` method independently checks status through `get_read_delegatee()`, with no mechanism to ensure consistency across multiple calls within a single logical operation.

## Notes

This vulnerability is particularly concerning because:
- It violates core cryptographic proof guarantees that are fundamental to blockchain integrity
- The inconsistent data is served to peers, potentially propagating across the network
- It occurs during a critical operation (node bootstrapping) when network reliability is important
- No synchronization exists between the storage service and fast sync completion

The window is limited to the fast sync bootstrapping period, making this a targeted issue rather than a constant vulnerability. However, during that window, the impact on state synchronization integrity is significant and merits HIGH severity classification.

### Citations

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L31-38)
```rust
pub struct FastSyncStorageWrapper {
    // Used for storing genesis data during fast sync
    temporary_db_with_genesis: Arc<AptosDB>,
    // Used for restoring fast sync snapshot and all the read/writes afterwards
    db_for_fast_sync: Arc<AptosDB>,
    // This is for reading the fast_sync status to determine which db to use
    fast_sync_status: Arc<RwLock<FastSyncStatus>>,
}
```

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L126-132)
```rust
    pub(crate) fn get_aptos_db_read_ref(&self) -> &AptosDB {
        if self.is_fast_sync_bootstrap_finished() {
            self.db_for_fast_sync.as_ref()
        } else {
            self.temporary_db_with_genesis.as_ref()
        }
    }
```

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L154-169)
```rust
    fn finalize_state_snapshot(
        &self,
        version: Version,
        output_with_proof: TransactionOutputListWithProofV2,
        ledger_infos: &[LedgerInfoWithSignatures],
    ) -> Result<()> {
        let status = self.get_fast_sync_status();
        assert_eq!(status, FastSyncStatus::STARTED);
        self.get_aptos_db_write_ref().finalize_state_snapshot(
            version,
            output_with_proof,
            ledger_infos,
        )?;
        let mut status = self.fast_sync_status.write();
        *status = FastSyncStatus::FINISHED;
        Ok(())
```

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L188-192)
```rust
impl DbReader for FastSyncStorageWrapper {
    fn get_read_delegatee(&self) -> &dyn DbReader {
        self.get_aptos_db_read_ref()
    }
}
```

**File:** storage/storage-interface/src/lib.rs (L99-111)
```rust
macro_rules! delegate_read {
    ($(
        $(#[$($attr:meta)*])*
        fn $name:ident(&self $(, $arg: ident : $ty: ty)* $(,)?) -> $return_type:ty;
    )+) => {
        $(
            $(#[$($attr)*])*
            fn $name(&self, $($arg: $ty),*) -> $return_type {
                self.get_read_delegatee().$name($($arg),*)
            }
        )+
    };
}
```

**File:** state-sync/storage-service/server/src/storage.rs (L374-394)
```rust
        let transaction_iterator = self
            .storage
            .get_transaction_iterator(start_version, num_transactions_to_fetch)?;
        let transaction_info_iterator = self
            .storage
            .get_transaction_info_iterator(start_version, num_transactions_to_fetch)?;
        let transaction_events_iterator = if include_events {
            self.storage
                .get_events_iterator(start_version, num_transactions_to_fetch)?
        } else {
            // If events are not included, create a fake iterator (they will be dropped anyway)
            Box::new(std::iter::repeat_n(
                Ok(vec![]),
                num_transactions_to_fetch as usize,
            ))
        };
        let persisted_auxiliary_info_iterator =
            self.storage.get_persisted_auxiliary_info_iterator(
                start_version,
                num_transactions_to_fetch as usize,
            )?;
```

**File:** state-sync/storage-service/server/src/storage.rs (L418-471)
```rust
        while !response_progress_tracker.is_response_complete() {
            match multizip_iterator.next() {
                Some((Ok(transaction), Ok(info), Ok(events), Ok(persisted_auxiliary_info))) => {
                    // Calculate the number of serialized bytes for the data items
                    let num_transaction_bytes = get_num_serialized_bytes(&transaction)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_info_bytes = get_num_serialized_bytes(&info)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_events_bytes = get_num_serialized_bytes(&events)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_auxiliary_info_bytes =
                        get_num_serialized_bytes(&persisted_auxiliary_info).map_err(|error| {
                            Error::UnexpectedErrorEncountered(error.to_string())
                        })?;

                    // Add the data items to the lists
                    let total_serialized_bytes = num_transaction_bytes
                        + num_info_bytes
                        + num_events_bytes
                        + num_auxiliary_info_bytes;
                    if response_progress_tracker
                        .data_items_fits_in_response(true, total_serialized_bytes)
                    {
                        transactions.push(transaction);
                        transaction_infos.push(info);
                        transaction_events.push(events);
                        persisted_auxiliary_infos.push(persisted_auxiliary_info);

                        response_progress_tracker.add_data_item(total_serialized_bytes);
                    } else {
                        break; // Cannot add any more data items
                    }
                },
                Some((Err(error), _, _, _))
                | Some((_, Err(error), _, _))
                | Some((_, _, Err(error), _))
                | Some((_, _, _, Err(error))) => {
                    return Err(Error::StorageErrorEncountered(error.to_string()));
                },
                None => {
                    // Log a warning that the iterators did not contain all the expected data
                    warn!(
                        "The iterators for transactions, transaction infos, events and \
                        persisted auxiliary infos are missing data! Start version: {:?}, \
                        end version: {:?}, num transactions to fetch: {:?}, num fetched: {:?}.",
                        start_version,
                        end_version,
                        num_transactions_to_fetch,
                        transactions.len()
                    );
                    break;
                },
            }
        }
```

**File:** state-sync/storage-service/server/src/storage.rs (L474-478)
```rust
        let accumulator_range_proof = self.storage.get_transaction_accumulator_range_proof(
            start_version,
            transactions.len() as u64,
            proof_version,
        )?;
```

**File:** state-sync/storage-service/server/src/lib.rs (L384-420)
```rust
    pub async fn start(mut self) {
        // Spawn the continuously running tasks
        self.spawn_continuous_storage_summary_tasks().await;

        // Handle the storage requests as they arrive
        while let Some(network_request) = self.network_requests.next().await {
            // All handler methods are currently CPU-bound and synchronous
            // I/O-bound, so we want to spawn on the blocking thread pool to
            // avoid starving other async tasks on the same runtime.
            let storage = self.storage.clone();
            let config = self.storage_service_config;
            let cached_storage_server_summary = self.cached_storage_server_summary.clone();
            let optimistic_fetches = self.optimistic_fetches.clone();
            let subscriptions = self.subscriptions.clone();
            let lru_response_cache = self.lru_response_cache.clone();
            let request_moderator = self.request_moderator.clone();
            let time_service = self.time_service.clone();
            self.runtime.spawn_blocking(move || {
                Handler::new(
                    cached_storage_server_summary,
                    optimistic_fetches,
                    lru_response_cache,
                    request_moderator,
                    storage,
                    subscriptions,
                    time_service,
                )
                .process_request_and_respond(
                    config,
                    network_request.peer_network_id,
                    network_request.protocol_id,
                    network_request.storage_service_request,
                    network_request.response_sender,
                );
            });
        }
    }
```

**File:** aptos-node/src/storage.rs (L75-77)
```rust
        Either::Right(fast_sync_db_wrapper) => {
            let temp_db = fast_sync_db_wrapper.get_temporary_db_with_genesis();
            maybe_apply_genesis(&DbReaderWriter::from_arc(temp_db), node_config)?;
```
