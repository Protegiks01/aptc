Based on my comprehensive technical validation of this security claim against the Aptos Core codebase, I have completed all phases of the validation framework. Here is my final determination:

# Audit Report

## Title
Consensus Observer Block Store Manipulation via Unverified Future Epoch Commit Decisions

## Summary
A malicious network peer can send commit decisions with crafted epoch/round values for future epochs to consensus observer nodes, bypassing signature verification and causing the observer to manipulate its block store incorrectly. This leads to state inconsistency and operational failures on observer nodes.

## Finding Description

The consensus observer implements a critical verification bypass for commit decisions from future epochs, allowing attackers to manipulate the block store with unverified data.

**Verification Bypass:**

The code only performs signature verification when the commit epoch matches the current epoch. [1](#0-0) 

For future epoch commits or when verification fails, the code falls through to lines 503-527 where it processes the unverified commit decision by calling `update_blocks_for_state_sync_commit()`. [2](#0-1) 

The TODO comment explicitly acknowledges this problematic handling. [3](#0-2) 

**Block Store Manipulation:**

The unverified commit decision is passed to `update_blocks_for_state_sync_commit()`, which directly updates the root ledger info and manipulates the ordered blocks without any validation. [4](#0-3) 

The `remove_blocks_for_commit()` function uses `BTreeMap::split_off()` with attacker-controlled epoch/round values to partition the ordered blocks. [5](#0-4) 

**Attack Scenarios:**

1. **Remove ALL blocks**: Sending `CommitDecision` with `epoch = u64::MAX, round = u64::MAX` causes `split_off(&(u64::MAX, u64::MAX))` to return an empty map, replacing all ordered blocks with an empty collection.

2. **Remove NO blocks**: Sending `CommitDecision` with `epoch = 0, round = 0` causes `split_off(&(0, 1))` to return all blocks, preventing legitimate block removal and causing memory accumulation until the `max_num_pending_blocks` limit is reached.

The root ledger info is also corrupted with unverified data. [6](#0-5) 

**No Publisher Authorization:**

Any connected peer can become a publisher without authorization checks. [7](#0-6) 

## Impact Explanation

**Severity: MEDIUM** 

This vulnerability causes operational degradation of consensus observer nodes, which are used for network observability and transaction monitoring. While the technical impact is significant for observer functionality, it does not directly affect:

- Consensus safety or liveness (observers are not validators)
- Fund security (observers don't execute state-changing transactions)
- Validator operations (unless VFNs explicitly use observer mode)

Impact on affected observer nodes:
1. **Liveness Failures**: Removing all blocks disrupts the execution pipeline, preventing the observer from processing legitimate consensus progress
2. **State Inconsistency**: Internal state becomes inconsistent with actual consensus, violating operational invariants
3. **Resource Exhaustion**: Memory accumulation when blocks aren't removed, eventually hitting capacity limits
4. **Root State Corruption**: Observer's view of committed state becomes incorrect

This aligns with **Limited Protocol Violations** in the MEDIUM severity category, as it causes state inconsistencies requiring manual intervention but does not affect core consensus or cause fund loss.

## Likelihood Explanation

**Likelihood: Medium**

The attack requires:
- Attacker operates a network peer supporting the ConsensusObserver protocol (no authentication required)
- Observer node subscribes to the attacker's peer based on distance/latency metrics
- No special timing, race conditions, or sophisticated techniques

Peer selection is based on optimality metrics, making subscription to an attacker's peer possible but not guaranteed. [8](#0-7) 

The verification bypass is deterministic and always occurs for future epoch commits.

## Recommendation

Implement signature verification for all commit decisions before processing, regardless of epoch:

1. Store the validator sets for future epochs when they become available through epoch transition messages
2. Verify commit proof signatures against the appropriate epoch's validator set before calling `update_blocks_for_state_sync_commit()`
3. If verification fails or validator set is unavailable, reject the commit decision and do not update internal state
4. Add authorization checks for publishers to ensure only legitimate consensus participants can publish updates

## Proof of Concept

A proof of concept would involve:
1. Setting up a malicious peer that implements the ConsensusObserver protocol
2. Having an observer node subscribe to this peer
3. Sending a `CommitDecision` message with `epoch = u64::MAX, round = u64::MAX`
4. Observing that the observer's `ordered_blocks` BTreeMap becomes empty
5. Confirming the observer cannot process subsequent legitimate blocks

### Citations

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L468-482)
```rust
        if commit_epoch == epoch_state.epoch {
            // Verify the commit decision
            if let Err(error) = commit_decision.verify_commit_proof(&epoch_state) {
                // Log the error and update the invalid message counter
                error!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "Failed to verify commit decision! Ignoring: {:?}, from peer: {:?}. Error: {:?}",
                        commit_decision.proof_block_info(),
                        peer_network_id,
                        error
                    ))
                );
                increment_invalid_message_counter(&peer_network_id, metrics::COMMIT_DECISION_LABEL);
                return;
            }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L497-498)
```rust
        // TODO: identify the best way to handle an invalid commit decision
        // for a future epoch. In such cases, we currently rely on state sync.
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L503-527)
```rust
        let epoch_changed = commit_epoch > last_block.epoch();
        if epoch_changed || commit_round > last_block.round() {
            // If we're waiting for state sync to transition into a new epoch,
            // we should just wait and not issue a new state sync request.
            if self.state_sync_manager.is_syncing_through_epoch() {
                info!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "Already waiting for state sync to reach new epoch: {:?}. Dropping commit decision: {:?}!",
                        self.observer_block_data.lock().root().commit_info(),
                        commit_decision.proof_block_info()
                    ))
                );
                return;
            }

            // Otherwise, we should start the state sync process for the commit.
            // Update the block data (to the commit decision).
            self.observer_block_data
                .lock()
                .update_blocks_for_state_sync_commit(&commit_decision);

            // Start state syncing to the commit decision
            self.state_sync_manager
                .sync_to_commit(commit_decision, epoch_changed);
        }
```

**File:** consensus/src/consensus_observer/observer/block_data.rs (L275-291)
```rust
    pub fn update_blocks_for_state_sync_commit(&mut self, commit_decision: &CommitDecision) {
        // Get the commit proof, epoch and round
        let commit_proof = commit_decision.commit_proof();
        let commit_epoch = commit_decision.epoch();
        let commit_round = commit_decision.round();

        // Update the root
        self.update_root(commit_proof.clone());

        // Update the block payload store
        self.block_payload_store
            .remove_blocks_for_epoch_round(commit_epoch, commit_round);

        // Update the ordered block store
        self.ordered_block_store
            .remove_blocks_for_commit(commit_proof);
    }
```

**File:** consensus/src/consensus_observer/observer/block_data.rs (L300-302)
```rust
    pub fn update_root(&mut self, new_root: LedgerInfoWithSignatures) {
        self.root = new_root;
    }
```

**File:** consensus/src/consensus_observer/observer/ordered_blocks.rs (L112-124)
```rust
    pub fn remove_blocks_for_commit(&mut self, commit_ledger_info: &LedgerInfoWithSignatures) {
        // Determine the epoch and round to split off
        let split_off_epoch = commit_ledger_info.ledger_info().epoch();
        let split_off_round = commit_ledger_info.commit_info().round().saturating_add(1);

        // Remove the blocks from the ordered blocks
        self.ordered_blocks = self
            .ordered_blocks
            .split_off(&(split_off_epoch, split_off_round));

        // Update the highest committed epoch and round
        self.update_highest_committed_epoch_round(commit_ledger_info);
    }
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L181-193)
```rust
            ConsensusObserverRequest::Subscribe => {
                // Add the peer to the set of active subscribers
                self.add_active_subscriber(peer_network_id);
                info!(LogSchema::new(LogEntry::ConsensusPublisher)
                    .event(LogEvent::Subscription)
                    .message(&format!(
                        "New peer subscribed to consensus updates! Peer: {:?}",
                        peer_network_id
                    )));

                // Send a simple subscription ACK
                response_sender.send(ConsensusObserverResponse::SubscribeAck);
            },
```

**File:** consensus/src/consensus_observer/observer/subscription_utils.rs (L1-50)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use crate::consensus_observer::{
    common::logging::{LogEntry, LogSchema},
    network::{
        observer_client::ConsensusObserverClient,
        observer_message::{
            ConsensusObserverMessage, ConsensusObserverRequest, ConsensusObserverResponse,
        },
    },
    observer::subscription::ConsensusObserverSubscription,
    publisher::consensus_publisher::ConsensusPublisher,
};
use aptos_config::{config::ConsensusObserverConfig, network_id::PeerNetworkId};
use aptos_logger::{error, info, warn};
use aptos_network::{
    application::{interface::NetworkClient, metadata::PeerMetadata},
    ProtocolId,
};
use aptos_storage_interface::DbReader;
use aptos_time_service::TimeService;
use ordered_float::OrderedFloat;
use std::{
    collections::{BTreeMap, HashMap},
    sync::Arc,
};

// A useful constant for representing the maximum ping latency
const MAX_PING_LATENCY_SECS: f64 = 10_000.0;

/// Attempts to create the given number of new subscriptions
/// from the connected peers and metadata. Any active or unhealthy
/// subscriptions are excluded from the selection process.
pub async fn create_new_subscriptions(
    consensus_observer_config: ConsensusObserverConfig,
    consensus_observer_client: Arc<
        ConsensusObserverClient<NetworkClient<ConsensusObserverMessage>>,
    >,
    consensus_publisher: Option<Arc<ConsensusPublisher>>,
    db_reader: Arc<dyn DbReader>,
    time_service: TimeService,
    connected_peers_and_metadata: HashMap<PeerNetworkId, PeerMetadata>,
    num_subscriptions_to_create: usize,
    active_subscription_peers: Vec<PeerNetworkId>,
    unhealthy_subscription_peers: Vec<PeerNetworkId>,
) -> Vec<ConsensusObserverSubscription> {
    // Sort the potential peers for subscription requests
    let mut sorted_potential_peers = match sort_peers_for_subscriptions(
        connected_peers_and_metadata,
```
