# Audit Report

## Title
Non-Atomic Pruner Progress Updates Cause Node Initialization Failures and State Inconsistencies

## Summary
The `write_pruner_progress()` function in LedgerDb performs sequential, non-atomic updates across 8 sub-databases. Partial failures leave inconsistent pruner progress values, causing EventStorePruner to fail during initialization with integer underflow errors when attempting to catch up with inverted version ranges.

## Finding Description

The vulnerability exists in the ledger pruner progress persistence mechanism. When fast sync completes, `save_min_readable_version()` calls `LedgerDb::write_pruner_progress()`, which performs 8 sequential write operations using the `?` operator for early return on failure: [1](#0-0) 

Each write is a separate database operation. If a crash, disk full, or I/O error occurs after some writes succeed, subsequent databases remain unupdated, creating inconsistent pruner progress across sub-databases.

**Critical Failure Path:**

When a node restarts after partial writes, `LedgerPruner::new()` reads `metadata_progress` from `ledger_metadata_db`: [2](#0-1) 

Each sub-pruner then initializes and attempts to catch up. EventStorePruner reads its own progress and calls `prune(progress, metadata_progress)`: [3](#0-2) 

**Integer Underflow Bug:** When EventStorePruner's progress (e.g., 1000) exceeds metadata_progress (e.g., 800), the prune method calls `prune_event_indices()` which performs `(end - start) as usize`: [4](#0-3) 

With `end=800` and `start=1000`, this causes integer underflow in release mode (wrapping to ~`u64::MAX - 199`). When passed to `get_events_by_version_iter()`, the `checked_add` detects overflow and returns an error: [5](#0-4) 

This error propagates up, preventing LedgerPruner initialization and causing complete node startup failure.

**Inconsistent Protection:** TransactionPruner has validation to prevent this scenario: [6](#0-5) 

However, EventStorePruner and other sub-pruners (WriteSetPruner, TransactionAccumulatorPruner, TransactionInfoPruner, PersistedAuxiliaryInfoPruner, TransactionAuxiliaryDataPruner) lack this validation in their prune methods, making them vulnerable during catch-up initialization.

## Impact Explanation

**High Severity** - This vulnerability causes complete validator node initialization failure, aligning with the "Validator Node Slowdowns" category in the Aptos bug bounty program.

**Primary Impact:**
1. **Total Node Initialization Failure**: Nodes experiencing partial pruner progress updates cannot restart. The overflow error during catch-up prevents LedgerPruner initialization, blocking all node operations.

2. **Validator Unavailability**: Affected validators cannot participate in consensus, serve API requests, or sync state until manual database intervention repairs the inconsistent progress values.

3. **No Automatic Recovery**: The system has no self-healing mechanism for this condition. Operators must manually edit database metadata to restore consistent pruner progress values.

This is called from the fast sync finalization path: [7](#0-6) 

## Likelihood Explanation

**High Likelihood** - Multiple realistic operational scenarios can trigger this vulnerability:

1. **Disk Space Exhaustion**: During fast sync when disk fills after some sub-database writes succeed
2. **I/O Errors**: Storage hardware failures, filesystem errors, or NFS disconnections during the write sequence
3. **Process Termination**: Node crashes, OOM kills, SIGKILL signals, or power failures during the update window
4. **Storage Sharding**: With separate RocksDB instances per sub-database, each write is a separate failure point

The vulnerability window is small (8 sequential writes, milliseconds), but fast sync operations are frequent when validators join the network, recover from downtime, or perform state synchronization.

## Recommendation

Implement atomic batch writes for all pruner progress updates:

```rust
pub(crate) fn write_pruner_progress(&self, version: Version) -> Result<()> {
    let mut batch = SchemaBatch::new();
    
    // Write all progress updates to a single batch
    batch.put::<DbMetadataSchema>(
        &DbMetadataKey::EventPrunerProgress,
        &DbMetadataValue::Version(version),
    )?;
    // ... (add all 8 sub-database progress keys to batch)
    
    // Single atomic write
    self.ledger_metadata_db.write_schemas(batch)
}
```

Additionally, add validation in EventStorePruner and other affected sub-pruners:

```rust
fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
    ensure!(target_version >= current_progress, 
        "target_version {} must be >= current_progress {}", 
        target_version, current_progress);
    // ... existing prune logic
}
```

## Proof of Concept

Create inconsistent pruner state by simulating partial write failure, then attempt node restart:

```rust
// Simulate partial write failure
let ledger_db = setup_ledger_db();
ledger_db.event_db.write_pruner_progress(1000)?;
// Simulate crash before remaining writes
// ledger_metadata_db still at 800

// Attempt initialization - will fail with overflow error
let result = LedgerPruner::new(Arc::new(ledger_db), None);
assert!(result.is_err());
assert!(result.unwrap_err().to_string().contains("TooManyRequested"));
```

## Notes

This vulnerability specifically affects EventStorePruner due to its arithmetic on version ranges before iteration. Other sub-pruners using `for version in begin..end` patterns are resilient (empty range becomes no-op), but still suffer from the non-atomic update issue that creates inconsistent state. The core fix requires atomic batch writes across all sub-databases.

### Citations

**File:** storage/aptosdb/src/ledger_db/mod.rs (L375-385)
```rust
        self.event_db.write_pruner_progress(version)?;
        self.persisted_auxiliary_info_db
            .write_pruner_progress(version)?;
        self.transaction_accumulator_db
            .write_pruner_progress(version)?;
        self.transaction_auxiliary_data_db
            .write_pruner_progress(version)?;
        self.transaction_db.write_pruner_progress(version)?;
        self.transaction_info_db.write_pruner_progress(version)?;
        self.write_set_db.write_pruner_progress(version)?;
        self.ledger_metadata_db.write_pruner_progress(version)?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L124-129)
```rust
        let ledger_metadata_pruner = Box::new(
            LedgerMetadataPruner::new(ledger_db.metadata_db_arc())
                .expect("Failed to initialize ledger_metadata_pruner."),
        );

        let metadata_progress = ledger_metadata_pruner.progress()?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L90-106)
```rust
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.event_db_raw(),
            &DbMetadataKey::EventPrunerProgress,
            metadata_progress,
        )?;

        let myself = EventStorePruner {
            ledger_db,
            internal_indexer_db,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up EventStorePruner."
        );
        myself.prune(progress, metadata_progress)?;
```

**File:** storage/aptosdb/src/ledger_db/event_db.rs (L111-113)
```rust
            start_version.checked_add(num_versions as u64).ok_or(
                AptosDbError::TooManyRequested(num_versions as u64, Version::MAX),
            )?,
```

**File:** storage/aptosdb/src/ledger_db/event_db.rs (L202-202)
```rust
        for events in self.get_events_by_version_iter(start, (end - start) as usize)? {
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L111-111)
```rust
        ensure!(end >= start, "{} must be >= {}", end, start);
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L225-225)
```rust
            self.ledger_pruner.save_min_readable_version(version)?;
```
