# Audit Report

## Title
Memory Exhaustion Vulnerability in State Merkle Pruner During Shard Catch-Up

## Summary
The state merkle pruner's shard initialization logic can cause out-of-memory (OOM) crashes when catching up large version ranges. During shard pruner initialization, the code uses an unbounded limit (`usize::MAX`) to load all stale node indices from potentially millions of versions into memory at once, causing validator crashes when significant gaps exist between shard progress and metadata progress.

## Finding Description

The vulnerability exists in the shard pruner initialization path where memory-bounded batch processing is bypassed.

**Critical Vulnerability Path:**

During shard pruner initialization, the catch-up logic calls `prune()` with an unbounded limit: [1](#0-0) 

This propagates through the `prune()` method which invokes `get_stale_node_indices` with the unbounded `max_nodes_to_prune` parameter: [2](#0-1) 

The `get_stale_node_indices` function accumulates ALL stale node indices into a Vec when `limit` equals `usize::MAX`: [3](#0-2) 

**Why This Violates Security Guarantees:**

The codebase defines a `batch_size` configuration (default 1,000) specifically to limit memory consumption during pruning operations: [4](#0-3) 

During normal operation, the `PrunerWorker` respects this batch size: [5](#0-4) 

However, during initialization, this safeguard is completely bypassed by using `usize::MAX`.

**Triggering Scenario:**

The gap between shard progress and metadata progress can occur when:
1. Validator crashes after metadata pruner updates but before shard pruner completes
2. Shard pruner encounters failures and stops updating its progress
3. Database restoration from an old backup where shard DBs are behind metadata DB
4. Extended operational issues causing shards to fall behind over time

The `get_or_initialize_subpruner_progress` function retrieves existing shard progress from the database: [6](#0-5) 

If this returns an old progress value (lines 50-51) while metadata_progress is far ahead, the catch-up call attempts to load the entire gap into memory.

**Memory Impact:**

Based on the configuration comment indicating "300k JMT nodes" for a 10k transaction block, a gap of 10 million versions could generate billions of stale node indices. With `StaleNodeIndex` structures containing version numbers and `NodeKey` objects with `NibblePath` (which includes a Vec), memory consumption easily exceeds typical validator RAM (64-128 GB).

## Impact Explanation

**High Severity** per Aptos Bug Bounty criteria:

This vulnerability causes **Validator Node Crashes** which is explicitly listed as HIGH severity in the bug bounty program. The impact includes:

1. **Immediate validator termination**: OOM condition kills the validator process
2. **Loss of consensus participation**: Crashed validators cannot participate in AptosBFT consensus
3. **Repeated failure loop**: Every restart attempt triggers the same OOM crash until manual intervention
4. **Potential network impact**: If multiple validators encounter this simultaneously (e.g., after coordinated restarts or upgrades), network liveness could be degraded

This does NOT reach Critical severity because:
- No permanent network partition occurs
- No fund loss or theft is possible
- Recovery is achievable through manual intervention (reducing gap or applying code fix)
- It requires specific operational conditions rather than being universally triggerable

The severity aligns with the "Validator Node Slowdowns/Crashes (High)" category in the Aptos Bug Bounty program.

## Likelihood Explanation

**Medium Likelihood:**

The vulnerability requires three conditions:
1. **Sharding enabled**: Increasingly common for scalability
2. **Significant gap exists**: Between shard progress and metadata progress  
3. **Validator initialization**: System restart or pruner reinitialization

Realistic triggering scenarios include:
- **Post-crash recovery**: Validator crashes mid-pruning, leaving shards behind
- **Pruner failures**: Silent failures in shard pruning that accumulate over time
- **Database restoration**: Recovery from backups with temporal inconsistencies
- **Operational issues**: Extended periods where shard pruners fail while metadata progresses

This is NOT a high likelihood vulnerability because:
- Normal operation keeps metadata and shard progress synchronized
- Gap accumulation requires sustained failures or crashes
- Not externally exploitable by attackers
- Requires validator operator actions or system events

However, it represents a serious operational risk that affects validator availability during legitimate maintenance and recovery procedures.

## Recommendation

**Immediate Fix:**

Replace the unbounded `usize::MAX` with the configured `batch_size` during initialization. Modify the shard pruner initialization to respect memory limits:

```rust
// In StateMerkleShardPruner::new()
// Instead of: myself.prune(progress, metadata_progress, usize::MAX)?;
// Use a reasonable batch size or implement chunked catch-up:

const CATCHUP_BATCH_SIZE: usize = 10_000; // Or use config.batch_size
let mut current = progress;
while current < metadata_progress {
    let target = std::cmp::min(current + CATCHUP_BATCH_SIZE, metadata_progress);
    myself.prune(current, target, CATCHUP_BATCH_SIZE)?;
    current = target;
}
```

**Alternative Solution:**

Add memory-bounded iteration to catch-up logic that processes in chunks while tracking progress, similar to how the metadata pruner handles incremental versioning.

## Proof of Concept

While a full PoC would require setting up a validator environment with significant state history, the vulnerability is demonstrable through code inspection:

1. Deploy validator with sharding enabled
2. Run for extended period accumulating millions of versions
3. Introduce shard/metadata progress gap via controlled crash during pruning
4. Restart validator and observe memory consumption spike during `StateMerkleShardPruner::new()`

The code path is deterministic and the memory allocation is unbounded as shown in the citations above.

## Notes

- The metadata pruner also uses `usize::MAX` but is less vulnerable due to incremental versioning logic that typically processes single versions at a time: [7](#0-6) 
- The vulnerability affects both `StaleNodeIndexSchema` and `StaleNodeIndexCrossEpochSchema` implementations
- The parallel shard pruning architecture prevents shard-level failures from blocking overall progress, potentially allowing gaps to accumulate undetected: [8](#0-7)

### Citations

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L53-53)
```rust
        myself.prune(progress, metadata_progress, usize::MAX)?;
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L66-71)
```rust
            let (indices, next_version) = StateMerklePruner::get_stale_node_indices(
                &self.db_shard,
                current_progress,
                target_version,
                max_nodes_to_prune,
            )?;
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L168-189)
```rust
    fn prune_shards(
        &self,
        current_progress: Version,
        target_version: Version,
        batch_size: usize,
    ) -> Result<()> {
        THREAD_MANAGER
            .get_background_pool()
            .install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(current_progress, target_version, batch_size)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state merkle shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })
            .map_err(Into::into)
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L191-217)
```rust
    pub(in crate::pruner::state_merkle_pruner) fn get_stale_node_indices(
        state_merkle_db_shard: &DB,
        start_version: Version,
        target_version: Version,
        limit: usize,
    ) -> Result<(Vec<StaleNodeIndex>, Option<Version>)> {
        let mut indices = Vec::new();
        let mut iter = state_merkle_db_shard.iter::<S>()?;
        iter.seek(&StaleNodeIndex {
            stale_since_version: start_version,
            node_key: NodeKey::new_empty_path(0),
        })?;

        let mut next_version = None;
        while indices.len() < limit {
            if let Some((index, _)) = iter.next().transpose()? {
                next_version = Some(index.stale_since_version);
                if index.stale_since_version <= target_version {
                    indices.push(index);
                    continue;
                }
            }
            break;
        }

        Ok((indices, next_version))
    }
```

**File:** config/src/config/storage_config.rs (L398-412)
```rust
impl Default for StateMerklePrunerConfig {
    fn default() -> Self {
        StateMerklePrunerConfig {
            enable: true,
            // This allows a block / chunk being executed to have access to a non-latest state tree.
            // It needs to be greater than the number of versions the state committing thread is
            // able to commit during the execution of the block / chunk. If the bad case indeed
            // happens due to this being too small, a node restart should recover it.
            // Still, defaulting to 1M to be super safe.
            prune_window: 1_000_000,
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
        }
    }
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L55-55)
```rust
            let pruner_result = self.pruner.prune(self.batch_size);
```

**File:** storage/aptosdb/src/pruner/pruner_utils.rs (L44-60)
```rust
pub(crate) fn get_or_initialize_subpruner_progress(
    sub_db: &DB,
    progress_key: &DbMetadataKey,
    metadata_progress: Version,
) -> Result<Version> {
    Ok(
        if let Some(v) = sub_db.get::<DbMetadataSchema>(progress_key)? {
            v.expect_version()
        } else {
            sub_db.put::<DbMetadataSchema>(
                progress_key,
                &DbMetadataValue::Version(metadata_progress),
            )?;
            metadata_progress
        },
    )
}
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs (L45-58)
```rust
        let next_version = self.next_version.load(Ordering::SeqCst);
        // This max here is only to handle the case when next version is not initialized.
        let target_version_for_this_round = max(next_version, current_progress);
        if target_version_for_this_round > target_version {
            return Ok(None);
        }

        // When next_version is not initialized, this call is used to initialize it.
        let (indices, next_version) = StateMerklePruner::get_stale_node_indices(
            &self.metadata_db,
            current_progress,
            target_version_for_this_round,
            usize::MAX,
        )?;
```
