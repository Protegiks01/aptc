# Audit Report

## Title
Validator Transaction Re-inclusion Vulnerability Due to Incomplete Filter Coverage After Commit Root Advancement

## Summary
The validator transaction exclude filter in `ProposalGenerator::generate_proposal_inner()` only checks uncommitted blocks (from parent to commit root), allowing already-committed validator transactions to be re-included in new proposals if they remain in the pool after the commit root advances past their original block. This causes execution failures and temporary consensus disruption.

## Finding Description

The vulnerability exists in the validator transaction filtering mechanism during block proposal generation. The filter is constructed from pending (uncommitted) blocks by calling `path_from_commit_root(parent_id)` which only returns blocks in the path from the parent to the commit root: [1](#0-0) 

The `path_from_root_to_block` implementation traverses backwards from the target block and only includes blocks with `round() > root_round`, explicitly excluding blocks older than the commit root: [2](#0-1) 

The filter is then constructed from validator transactions in these pending blocks: [3](#0-2) 

However, validator transactions remain in the pool until their `TxnGuard` is dropped. The drop implementation removes the transaction: [4](#0-3) 

For DKG transactions, the guard is only dropped during epoch change via `process_close_cmd`: [5](#0-4) 

When a previously-committed DKG result is re-executed, the VM checks if an in-progress DKG session exists: [6](#0-5) 

Since the first execution moved `in_progress` to `last_completed`, this check fails and returns an expected error that results in `TransactionStatus::Discard`: [7](#0-6) 

**Attack Scenario:**
1. Round 10: Block A containing DKG result VTxn1 is committed and executed successfully
2. DKGState.in_progress is moved to last_completed on-chain
3. VTxn1's TxnGuard remains held in DKGManager until epoch change
4. Rounds 11-30: Commit root advances to round 30 (Block A is now older than commit root)
5. Round 31: New proposer constructs filter from pending blocks (round 30 to commit root)
6. Block A (round 10) is not in this path
7. VTxn1's hash is not in the filter
8. VTxn1 is pulled from pool and included in Block B
9. Validators receive Block B and attempt execution
10. VM detects in_progress = None, returns error code 0x30002
11. Transaction status becomes Discard
12. Block B is rejected by validators, consensus delayed by one round

This breaks deterministic execution: a proposer unknowingly creates an invalid block containing an already-executed validator transaction.

## Impact Explanation

This is a **Medium Severity** vulnerability per Aptos bug bounty criteria:

**Limited Protocol Violations - Temporary liveness issues:**
- Blocks containing re-included validator transactions fail execution with discard status
- The proposer wastes their proposal turn for that round
- Validators reject the block or don't vote for it
- Consensus is delayed by one round but recovers in the next round
- No permanent network damage or fund loss occurs

The impact is medium rather than critical because:
1. Network recovers automatically in the next round with a different proposer
2. No consensus safety violation (all validators agree the block is invalid)
3. No funds are lost, stolen, or frozen
4. The issue self-resolves at epoch boundaries when the TxnGuard is dropped

However, it causes operational disruption requiring monitoring and potentially manual intervention if it occurs repeatedly within an epoch.

## Likelihood Explanation

**Likelihood: Medium** within epoch boundaries

The vulnerability occurs naturally without malicious actors when:
1. A validator transaction (DKG result, JWK update) is committed in a block early in the epoch
2. The commit root advances significantly (20+ rounds)
3. The epoch has not yet changed (guard not dropped)
4. A new proposer creates a block at a later round

This is particularly likely for:
- **DKG results**: Committed early in epoch, guard held until epoch boundary
- **Long epochs**: Epochs lasting hundreds of rounds provide ample opportunity
- **Active networks**: High block production rates cause rapid commit root advancement

The vulnerability is realistic in production because:
- Epochs typically last hundreds of rounds in mainnet
- DKG results are naturally committed early in the epoch lifecycle
- No attacker coordination is needed - happens through normal protocol operation
- The filter design assumes validator transactions are short-lived, but they persist until epoch change

## Recommendation

Modify the filter construction to include validator transactions from **all committed blocks**, not just uncommitted (pending) blocks:

```rust
// Option 1: Track committed validator transactions separately
let committed_validator_txn_hashes: HashSet<HashValue> = self
    .block_store
    .get_committed_validator_txns_in_epoch()
    .map(ValidatorTransaction::hash)
    .collect();

let pending_validator_txn_hashes: HashSet<HashValue> = pending_blocks
    .iter()
    .filter_map(|block| block.validator_txns())
    .flatten()
    .map(ValidatorTransaction::hash)
    .collect();

let all_validator_txn_hashes: HashSet<HashValue> = committed_validator_txn_hashes
    .union(&pending_validator_txn_hashes)
    .cloned()
    .collect();

let validator_txn_filter = vtxn_pool::TransactionFilter::PendingTxnHashSet(all_validator_txn_hashes);
```

**Alternative:** Drop the TxnGuard immediately after a block containing the validator transaction is committed, rather than waiting for epoch change.

## Proof of Concept

While a full PoC would require running a test network, the vulnerability can be demonstrated by:

1. Setting up a local devnet with DKG enabled
2. Triggering a DKG result early in an epoch (round 10)
3. Advancing the commit root by producing 20+ blocks
4. Observing the validator transaction pool still contains the DKG result
5. Manually triggering block proposal with the DKG result re-included
6. Observing execution failure with error code 0x30002 (MissingResourceInprogressDKGSession)
7. Confirming validators reject the block

The core components for reproduction:
- Monitor `VTxnPoolState` to confirm transaction persistence
- Monitor `DKGManager::state` to confirm TxnGuard is held
- Monitor block execution output to observe discard status
- Verify commit root advancement via `BlockStore::commit_root()`

## Notes

**Minor Discrepancy:** The report claims the error would be `EDKG_NOT_IN_PROGRESS` (error code 2), but the actual error code would be `MissingResourceInprogressDKGSession` (0x30002) because the Rust VM check occurs before the Move function call. Both result in the same outcome (TransactionStatus::Discard), so this doesn't invalidate the core vulnerability.

The fundamental issue is that the filter scope doesn't match the validator transaction lifecycle: transactions remain in the pool across commit root advancements but the filter only covers uncommitted blocks.

### Citations

**File:** consensus/src/liveness/proposal_generator.rs (L575-581)
```rust
        let mut pending_blocks = self
            .block_store
            .path_from_commit_root(parent_id)
            .ok_or_else(|| format_err!("Parent block {} already pruned", parent_id))?;
        // Avoid txn manager long poll if the root block has txns, so that the leader can
        // deliver the commit proof to others without delay.
        pending_blocks.push(self.block_store.commit_root());
```

**File:** consensus/src/liveness/proposal_generator.rs (L643-650)
```rust
        let pending_validator_txn_hashes: HashSet<HashValue> = pending_blocks
            .iter()
            .filter_map(|block| block.validator_txns())
            .flatten()
            .map(ValidatorTransaction::hash)
            .collect();
        let validator_txn_filter =
            vtxn_pool::TransactionFilter::PendingTxnHashSet(pending_validator_txn_hashes);
```

**File:** consensus/src/block_storage/block_tree.rs (L520-545)
```rust
        &self,
        block_id: HashValue,
        root_id: HashValue,
        root_round: u64,
    ) -> Option<Vec<Arc<PipelinedBlock>>> {
        let mut res = vec![];
        let mut cur_block_id = block_id;
        loop {
            match self.get_block(&cur_block_id) {
                Some(ref block) if block.round() <= root_round => {
                    break;
                },
                Some(block) => {
                    cur_block_id = block.parent_id();
                    res.push(block);
                },
                None => return None,
            }
        }
        // At this point cur_block.round() <= self.root.round()
        if cur_block_id != root_id {
            return None;
        }
        // Called `.reverse()` to get the chronically increased order.
        res.reverse();
        Some(res)
```

**File:** crates/validator-transaction-pool/src/lib.rs (L202-206)
```rust
impl Drop for TxnGuard {
    fn drop(&mut self) {
        self.pool.lock().try_delete(self.seq_num);
    }
}
```

**File:** dkg/src/dkg_manager/mod.rs (L217-252)
```rust
    fn process_close_cmd(&mut self, ack_tx: Option<oneshot::Sender<()>>) -> Result<()> {
        self.stopped = true;

        match std::mem::take(&mut self.state) {
            InnerState::NotStarted => {},
            InnerState::InProgress { abort_handle, .. } => {
                abort_handle.abort();
            },
            InnerState::Finished {
                vtxn_guard,
                start_time,
                ..
            } => {
                let epoch_change_time = duration_since_epoch();
                let secs_since_dkg_start =
                    epoch_change_time.as_secs_f64() - start_time.as_secs_f64();
                DKG_STAGE_SECONDS
                    .with_label_values(&[self.my_addr.to_hex().as_str(), "epoch_change"])
                    .observe(secs_since_dkg_start);
                info!(
                    epoch = self.epoch_state.epoch,
                    my_addr = self.my_addr,
                    secs_since_dkg_start = secs_since_dkg_start,
                    "[DKG] txn executed and entering new epoch.",
                );

                drop(vtxn_guard);
            },
        }

        if let Some(tx) = ack_tx {
            let _ = tx.send(());
        }

        Ok(())
    }
```

**File:** aptos-move/aptos-vm/src/validator_txns/dkg.rs (L68-77)
```rust
            Err(Expected(failure)) => {
                // Pretend we are inside Move, and expected failures are like Move aborts.
                Ok((
                    VMStatus::MoveAbort {
                        location: AbortLocation::Script,
                        code: failure as u64,
                        message: None,
                    },
                    VMOutput::empty_with_status(TransactionStatus::Discard(StatusCode::ABORTED)),
                ))
```

**File:** aptos-move/aptos-vm/src/validator_txns/dkg.rs (L91-97)
```rust
        let dkg_state =
            OnChainConfig::fetch_config(resolver).ok_or(Expected(MissingResourceDKGState))?;
        let config_resource = ConfigurationResource::fetch_config(resolver)
            .ok_or(Expected(MissingResourceConfiguration))?;
        let DKGState { in_progress, .. } = dkg_state;
        let in_progress_session_state =
            in_progress.ok_or(Expected(MissingResourceInprogressDKGSession))?;
```
