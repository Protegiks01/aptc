# Audit Report

## Title
Partial Shard Commit During Crash Can Cause Permanent Node Orphaning Leading to Unbounded Database Growth

## Summary
A critical asymmetry exists in the state merkle database recovery mechanism where stale node indices are deleted based on `stale_since_version` while actual nodes are deleted based on their creation `version`. This causes nodes to become permanently orphaned and unprunable when crashes occur during sharded commits, leading to unbounded database growth.

## Finding Description

The vulnerability stems from an asymmetric truncation logic in the state merkle database recovery process:

**1. Stale Node Index Structure**

The `StaleNodeIndex` contains two fields with different version semantics:
- `stale_since_version`: The version when the node became stale
- `node_key`: Contains the node's original creation version [1](#0-0) 

The `NodeKey` stores the version at which the node was created: [2](#0-1) 

**2. Non-Atomic Cross-Shard Commit**

Shards are committed in parallel with no cross-shard atomicity guarantee. The overall progress marker is written only after all shard commits complete: [3](#0-2) 

The overall commit progress is written in the top-level batch: [4](#0-3) 

**3. Asymmetric Recovery Logic**

On restart, recovery truncates shards based on overall progress. However, the truncation logic contains a critical asymmetry: [5](#0-4) 

Stale indices are deleted where `stale_since_version >= version`: [6](#0-5) 

But nodes are deleted where `node_key.version >= version` (line 615 seeks by NodeKey which contains creation version).

**4. Orphaning Scenario**

Consider a node created at version 99 that becomes stale at version 100:
- `StaleNodeIndex { stale_since_version: 100, node_key: NodeKey(version=99, ...) }` is written to shard
- Shard commits successfully
- System crashes before overall progress (100) is written
- On recovery with overall progress at 99:
  - Stale index deleted (because `stale_since_version=100 >= 100`)
  - Node survives (because `node_key.version=99 < 100`)
  - Node is now orphaned with no index entry

**5. Unprunable Forever**

The pruner operates by scanning stale node indices: [7](#0-6) 

Without an index entry, orphaned nodes can never be discovered or pruned.

Recovery is triggered on database initialization: [8](#0-7) 

## Impact Explanation

**Severity: High**

This qualifies as **High Severity** per Aptos bug bounty criteria for:

1. **Validator Node Slowdowns**: Accumulated orphaned nodes cause:
   - Increased disk I/O for database operations
   - Slower backup/restore operations
   - Degraded iterator performance
   - Slower state sync for new nodes

2. **Significant Protocol Violations**: 
   - Breaks the state consistency invariant
   - Causes unbounded database growth
   - Creates non-deterministic database state across validators

3. **Economic Impact**: Storage costs increase linearly with orphaned nodes, creating unfair operational burden on validators.

While this does not directly break consensus safety (orphaned nodes are unreferenced), it causes significant validator performance degradation and protocol violations, meeting the High Severity criteria of "Validator Node Slowdowns" and "Significant Protocol Violations."

## Likelihood Explanation

**Likelihood: Medium-High**

The vulnerability is highly likely to occur because:

1. **Frequent Trigger Conditions**: Crashes during commits can occur through:
   - Validator restarts for upgrades/maintenance
   - OOM kills during high load
   - Hardware failures or power outages
   - Kernel panics

2. **High Exposure Window**: State merkle commits occur every block, providing frequent opportunities for crashes during the commit window.

3. **Cumulative Impact**: Each crash orphans multiple nodes. Over weeks/months of 24/7 validator operation, the cumulative impact is significant.

4. **Realistic Operational Scenario**: Validators regularly experience restarts and occasional failures, making this a practical concern rather than theoretical edge case.

## Recommendation

Implement atomic truncation by deleting nodes based on the same version criteria as stale indices:

```rust
fn delete_nodes_and_stale_indices_at_or_after_version(
    db: &DB,
    version: Version,
    shard_id: Option<usize>,
    batch: &mut SchemaBatch,
) -> Result<()> {
    // Delete stale indices where stale_since_version >= version
    delete_stale_node_index_at_or_after_version::<StaleNodeIndexSchema>(db, version, batch)?;
    delete_stale_node_index_at_or_after_version::<StaleNodeIndexCrossEpochSchema>(db, version, batch)?;

    // FIX: Also delete nodes that became stale at >= version
    // This requires scanning stale indices to find node_keys to delete
    let mut iter = db.iter::<StaleNodeIndexSchema>()?;
    iter.seek(&StaleNodeIndex {
        stale_since_version: version,
        node_key: NodeKey::new_empty_path(0),
    })?;
    
    for item in iter {
        let (index, _) = item?;
        if index.stale_since_version < version {
            break;
        }
        // Delete the actual node referenced by this stale index
        batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
    }

    StateMerkleDb::put_progress(version.checked_sub(1), shard_id, batch)
}
```

Alternatively, consider implementing true atomic commits across all shards using a two-phase commit protocol.

## Proof of Concept

The vulnerability can be demonstrated through the following test scenario:

1. Create a state merkle tree node at version 99
2. Update the tree at version 100, making the version 99 node stale
3. Simulate a crash after shard commit but before metadata commit
4. Verify that recovery deletes the stale index but not the node
5. Verify that the pruner cannot find the orphaned node

The test would require simulating the crash scenario by directly manipulating the database state, which is feasible through Rust unit tests in the storage layer.

### Citations

**File:** storage/jellyfish-merkle/src/lib.rs (L193-201)
```rust
#[derive(Clone, Debug, Eq, Hash, Ord, PartialEq, PartialOrd)]
#[cfg_attr(any(test, feature = "fuzzing"), derive(Arbitrary))]
pub struct StaleNodeIndex {
    /// The version since when the node is overwritten and becomes stale.
    pub stale_since_version: Version,
    /// The [`NodeKey`](node_type/struct.NodeKey.html) identifying the node associated with this
    /// record.
    pub node_key: NodeKey,
}
```

**File:** storage/jellyfish-merkle/src/node_type/mod.rs (L46-54)
```rust
/// The unique key of each node.
#[derive(Clone, Debug, Hash, Eq, PartialEq, Ord, PartialOrd)]
#[cfg_attr(any(test, feature = "fuzzing"), derive(Arbitrary))]
pub struct NodeKey {
    // The version at which the node is created.
    version: Version,
    // The nibble path this node represents in the tree.
    nibble_path: NibblePath,
}
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L147-171)
```rust
    pub(crate) fn commit(
        &self,
        version: Version,
        top_levels_batch: impl IntoRawBatch,
        batches_for_shards: Vec<impl IntoRawBatch + Send>,
    ) -> Result<()> {
        ensure!(
            batches_for_shards.len() == NUM_STATE_SHARDS,
            "Shard count mismatch."
        );
        THREAD_MANAGER.get_io_pool().install(|| {
            batches_for_shards
                .into_par_iter()
                .enumerate()
                .for_each(|(shard_id, batch)| {
                    self.db_shard(shard_id)
                        .write_schemas(batch)
                        .unwrap_or_else(|err| {
                            panic!("Failed to commit state merkle shard {shard_id}: {err}")
                        });
                })
        });

        self.commit_top_levels(version, top_levels_batch)
    }
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L393-409)
```rust
    pub(crate) fn put_progress(
        version: Option<Version>,
        shard_id: Option<usize>,
        batch: &mut impl WriteBatch,
    ) -> Result<()> {
        let key = if let Some(shard_id) = shard_id {
            DbMetadataKey::StateMerkleShardCommitProgress(shard_id)
        } else {
            DbMetadataKey::StateMerkleCommitProgress
        };

        if let Some(version) = version {
            batch.put::<DbMetadataSchema>(&key, &DbMetadataValue::Version(version))
        } else {
            batch.delete::<DbMetadataSchema>(&key)
        }
    }
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L668-677)
```rust
        if !readonly {
            if let Some(overall_state_merkle_commit_progress) =
                get_state_merkle_commit_progress(&state_merkle_db)?
            {
                truncate_state_merkle_db_shards(
                    &state_merkle_db,
                    overall_state_merkle_commit_progress,
                )?;
            }
        }
```

**File:** storage/aptosdb/src/utils/truncation_helper.rs (L583-601)
```rust
fn delete_stale_node_index_at_or_after_version<S>(
    db: &DB,
    version: Version,
    batch: &mut SchemaBatch,
) -> Result<()>
where
    S: Schema<Key = StaleNodeIndex>,
    Version: SeekKeyCodec<S>,
{
    let mut iter = db.iter::<S>()?;
    iter.seek(&version)?;
    for item in iter {
        let (index, _) = item?;
        assert_ge!(index.stale_since_version, version);
        batch.delete::<S>(&index)?;
    }

    Ok(())
}
```

**File:** storage/aptosdb/src/utils/truncation_helper.rs (L603-622)
```rust
fn delete_nodes_and_stale_indices_at_or_after_version(
    db: &DB,
    version: Version,
    shard_id: Option<usize>,
    batch: &mut SchemaBatch,
) -> Result<()> {
    delete_stale_node_index_at_or_after_version::<StaleNodeIndexSchema>(db, version, batch)?;
    delete_stale_node_index_at_or_after_version::<StaleNodeIndexCrossEpochSchema>(
        db, version, batch,
    )?;

    let mut iter = db.iter::<JellyfishMerkleNodeSchema>()?;
    iter.seek(&NodeKey::new_empty_path(version))?;
    for item in iter {
        let (key, _) = item?;
        batch.delete::<JellyfishMerkleNodeSchema>(&key)?;
    }

    StateMerkleDb::put_progress(version.checked_sub(1), shard_id, batch)
}
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L191-217)
```rust
    pub(in crate::pruner::state_merkle_pruner) fn get_stale_node_indices(
        state_merkle_db_shard: &DB,
        start_version: Version,
        target_version: Version,
        limit: usize,
    ) -> Result<(Vec<StaleNodeIndex>, Option<Version>)> {
        let mut indices = Vec::new();
        let mut iter = state_merkle_db_shard.iter::<S>()?;
        iter.seek(&StaleNodeIndex {
            stale_since_version: start_version,
            node_key: NodeKey::new_empty_path(0),
        })?;

        let mut next_version = None;
        while indices.len() < limit {
            if let Some((index, _)) = iter.next().transpose()? {
                next_version = Some(index.stale_since_version);
                if index.stale_since_version <= target_version {
                    indices.push(index);
                    continue;
                }
            }
            break;
        }

        Ok((indices, next_version))
    }
```
