# Audit Report

## Title
Zero-Entropy DKG Contribution via Identity Element in DealtPubKey Allows Undetectable Randomness Manipulation

## Summary
A malicious validator can deal a zero `InputSecret` in the DKG protocol, resulting in a `DealtPubKey` that is the identity element in G2. This passes all cryptographic verification checks and allows the attacker to contribute zero entropy to the distributed randomness generation, violating the protocol's stated security guarantees that randomness "cannot be biased in any way by validators."

## Finding Description

The vulnerability exists in Aptos's PVSS-based DKG implementation. The `InputSecret` struct implements the `Zero` trait, allowing creation of a zero-valued secret that leads to an identity element as the dealt public key. [1](#0-0) 

The `Convert::to()` trait implementation performs scalar multiplication without validating that the result is non-identity: [2](#0-1) 

When a zero InputSecret is used, the polynomial constant term becomes zero: [3](#0-2) 

The `DealtPubKey::new()` constructor accepts any G2 element without validation: [4](#0-3) 

**Verification Bypasses:**

All verification checks accept identity elements: [5](#0-4) 

The Low Degree Test computes a multi-scalar multiplication where identity elements contribute correctly to the zero-sum check: [6](#0-5) 

During transcript aggregation, identity elements contribute zero entropy: [7](#0-6) 

This violates the explicit security guarantee stated in the randomness module: [8](#0-7) 

**Attack Execution:**

A Byzantine validator can exploit this by modifying their DKG manager to use `InputSecret::zero()` instead of random generation: [9](#0-8) 

## Impact Explanation

**Severity: High**

This vulnerability enables Byzantine validators to undetectably reduce the entropy of on-chain randomness. While it doesn't directly cause consensus splits, it violates critical security guarantees:

1. **Undetectable Bias**: The stated security property that "randomness cannot be biased in any way by validators" is violated. Byzantine validators can participate in DKG while contributing zero entropy, and honest validators cannot detect this.

2. **Entropy Reduction**: If `f` Byzantine validators (where `f < n/3`) all contribute zero entropy, the effective random seed only contains contributions from `(n-f)` validators, reducing the security parameter by up to 33%.

3. **Randomness Manipulation Risk**: The reduced entropy could enable attacks on validator selection, transaction ordering, or applications relying on on-chain randomness for security-critical decisions.

4. **Security Threshold Erosion**: The DKG protocol's security relies on honest validators contributing randomness. This vulnerability allows Byzantine validators to appear compliant while undermining the entropy pool.

The transcript is verified during block processing: [10](#0-9) 

## Likelihood Explanation

**Likelihood: High**

1. **Easy to Execute**: A Byzantine validator only needs to replace `InputSecret::generate()` with `InputSecret::zero()` in their node software. The `Zero` trait implementation makes this trivial.

2. **No Detection Mechanism**: There are no checks validating that `InputSecret` is non-zero before dealing, nor any validation that `DealtPubKey` is non-identity. The deserialization accepts identity elements: [11](#0-10) 

3. **Within Threat Model**: The Aptos BFT consensus explicitly assumes up to `f < n/3` Byzantine validators. This attack requires only that a validator control their own node software, which is expected in the Byzantine threat model.

4. **High Impact When Exploited**: Even 10-20% of validators exploiting this significantly reduces entropy, weakening the randomness security without any visible protocol violations.

## Recommendation

Add explicit validation to reject identity elements:

1. **At InputSecret Creation**: Check `InputSecret::is_zero()` before dealing
2. **At DealtPubKey Creation**: Validate the G2 element is not identity
3. **During Verification**: Add explicit checks that `V0` is not the identity element

Example fix for `DealtPubKey::new()`:
```rust
pub fn new(G: E::G2Affine) -> Result<Self, Error> {
    if bool::from(G.is_identity()) {
        return Err(Error::IdentityElementNotAllowed);
    }
    Ok(Self { G })
}
```

Similar validation should be added in the `Convert::to()` implementation and during transcript verification.

## Proof of Concept

```rust
use aptos_dkg::pvss::chunky::{input_secret::InputSecret, public_parameters::PublicParameters};
use aptos_dkg::traits::Convert;
use ark_bls12_381::Bls12_381;
use num_traits::Zero;

#[test]
fn test_zero_entropy_contribution() {
    // Malicious validator creates zero InputSecret
    let zero_secret = InputSecret::<ark_bls12_381::Fr>::zero();
    assert!(zero_secret.is_zero());
    
    // Convert to DealtPubKey
    let pp = PublicParameters::<Bls12_381>::default();
    let dealt_pk = zero_secret.to(&pp);
    
    // The dealt public key is the identity element
    let g2_element = dealt_pk.as_g2();
    assert!(bool::from(g2_element.into_group().is_identity()));
    
    // This transcript would pass all verification checks
    // but contributes zero entropy to the final randomness
}
```

### Citations

**File:** crates/aptos-dkg/src/pvss/chunky/input_secret.rs (L38-46)
```rust
impl<F: ark_ff::Field> Zero for InputSecret<F> {
    fn zero() -> Self {
        InputSecret { a: F::ZERO }
    }

    fn is_zero(&self) -> bool {
        self.a.is_zero()
    }
}
```

**File:** crates/aptos-dkg/src/pvss/chunky/public_parameters.rs (L134-146)
```rust
impl<E: Pairing> traits::Convert<keys::DealtPubKey<E>, PublicParameters<E>>
    for InputSecret<E::ScalarField>
{
    /// Computes the public key associated with the given input secret.
    /// NOTE: In the SCRAPE PVSS, a `DealtPublicKey` cannot be computed from a `DealtSecretKey` directly.
    fn to(&self, pp: &PublicParameters<E>) -> keys::DealtPubKey<E> {
        keys::DealtPubKey::new(
            pp.get_commitment_base()
                .mul(self.get_secret_a())
                .into_affine(),
        )
    }
}
```

**File:** crates/aptos-dkg/src/pvss/chunky/weighted_transcript.rs (L125-286)
```rust
    fn verify<A: Serialize + Clone>(
        &self,
        sc: &Self::SecretSharingConfig,
        pp: &Self::PublicParameters,
        spks: &[Self::SigningPubKey],
        eks: &[Self::EncryptPubKey],
        sid: &A,
    ) -> anyhow::Result<()> {
        if eks.len() != sc.get_total_num_players() {
            bail!(
                "Expected {} encryption keys, but got {}",
                sc.get_total_num_players(),
                eks.len()
            );
        }
        if self.subtrs.Cs.len() != sc.get_total_num_players() {
            bail!(
                "Expected {} arrays of chunked ciphertexts, but got {}",
                sc.get_total_num_players(),
                self.subtrs.Cs.len()
            );
        }
        if self.subtrs.Vs.len() != sc.get_total_num_players() {
            bail!(
                "Expected {} arrays of commitment elements, but got {}",
                sc.get_total_num_players(),
                self.subtrs.Vs.len()
            );
        }

        // Initialize the **identical** PVSS SoK context
        let sok_cntxt = (
            &spks[self.dealer.id],
            sid.clone(),
            self.dealer.id,
            DST.to_vec(),
        ); // As above, this is a bit hacky... though we have access to `self` now

        {
            // Verify the PoK
            let eks_inner: Vec<_> = eks.iter().map(|ek| ek.ek).collect();
            let lagr_g1: &[E::G1Affine] = match &pp.pk_range_proof.ck_S.msm_basis {
                SrsBasis::Lagrange { lagr: lagr_g1 } => lagr_g1,
                SrsBasis::PowersOfTau { .. } => {
                    bail!("Expected a Lagrange basis, received powers of tau basis instead")
                },
            };
            let hom = hkzg_chunked_elgamal::WeightedHomomorphism::<E>::new(
                lagr_g1,
                pp.pk_range_proof.ck_S.xi_1,
                &pp.pp_elgamal,
                &eks_inner,
            );
            if let Err(err) = hom.verify(
                &TupleCodomainShape(
                    self.sharing_proof.range_proof_commitment.clone(),
                    chunked_elgamal::WeightedCodomainShape {
                        chunks: self.subtrs.Cs.clone(),
                        randomness: self.subtrs.Rs.clone(),
                    },
                ),
                &self.sharing_proof.SoK,
                &sok_cntxt,
            ) {
                bail!("PoK verification failed: {:?}", err);
            }

            // Verify the range proof
            if let Err(err) = self.sharing_proof.range_proof.verify(
                &pp.pk_range_proof.vk,
                sc.get_total_weight() * num_chunks_per_scalar::<E::ScalarField>(pp.ell) as usize,
                pp.ell as usize,
                &self.sharing_proof.range_proof_commitment,
            ) {
                bail!("Range proof batch verification failed: {:?}", err);
            }
        }

        let mut rng = rand::thread_rng(); // TODO: make `rng` a parameter of fn verify()?

        // Do the SCRAPE LDT
        let ldt = LowDegreeTest::random(
            &mut rng,
            sc.get_threshold_weight(),
            sc.get_total_weight() + 1,
            true,
            &sc.get_threshold_config().domain,
        ); // includes_zero is true here means it includes a commitment to f(0), which is in V[n]
        let mut Vs_flat: Vec<_> = self.subtrs.Vs.iter().flatten().cloned().collect();
        Vs_flat.push(self.subtrs.V0);
        // could add an assert_eq here with sc.get_total_weight()
        ldt.low_degree_test_group(&Vs_flat)?;

        // let eks_inner: Vec<_> = eks.iter().map(|ek| ek.ek).collect();
        // let hom = hkzg_chunked_elgamal::WeightedHomomorphism::new(
        //     &pp.pk_range_proof.ck_S.lagr_g1,
        //     pp.pk_range_proof.ck_S.xi_1,
        //     &pp.pp_elgamal,
        //     &eks_inner,
        // );
        // let (sigma_bases, sigma_scalars, beta_powers) = hom.verify_msm_terms(
        //         &TupleCodomainShape(
        //             self.sharing_proof.range_proof_commitment.clone(),
        //             chunked_elgamal::WeightedCodomainShape {
        //                 chunks: self.subtrs.Cs.clone(),
        //                 randomness: self.subtrs.Rs.clone(),
        //             },
        //         ),
        //         &self.sharing_proof.SoK,
        //         &sok_cntxt,
        //     );
        // let ldt_msm_terms = ldt.ldt_msm_input(&Vs_flat)?;
        // use aptos_crypto::arkworks::msm::verify_msm_terms_with_start;
        // verify_msm_terms_with_start(ldt_msm_terms, sigma_bases, sigma_scalars, beta_powers);

        // Now compute the final MSM // TODO: merge this multi_exp with the PoK verification, as in YOLO YOSO? // TODO2: and use the iterate stuff you developed? it's being forgotten here
        let mut base_vec = Vec::new();
        let mut exp_vec = Vec::new();

        let beta = sample_field_element(&mut rng);
        let powers_of_beta = utils::powers(beta, sc.get_total_weight() + 1);

        let Cs_flat: Vec<_> = self.subtrs.Cs.iter().flatten().cloned().collect();
        assert_eq!(
            Cs_flat.len(),
            sc.get_total_weight(),
            "Number of ciphertexts does not equal number of weights"
        ); // TODO what if zero weight?
           // could add an assert_eq here with sc.get_total_weight()

        for i in 0..Cs_flat.len() {
            for j in 0..Cs_flat[i].len() {
                let base = Cs_flat[i][j];
                let exp = pp.powers_of_radix[j] * powers_of_beta[i];
                base_vec.push(base);
                exp_vec.push(exp);
            }
        }

        let weighted_Cs = E::G1::msm(&E::G1::normalize_batch(&base_vec), &exp_vec)
            .expect("Failed to compute MSM of Cs in chunky");

        let weighted_Vs = E::G2::msm(
            &E::G2::normalize_batch(&Vs_flat[..sc.get_total_weight()]), // Don't use the last entry of `Vs_flat`
            &powers_of_beta[..sc.get_total_weight()],
        )
        .expect("Failed to compute MSM of Vs in chunky");

        let res = E::multi_pairing(
            [
                weighted_Cs.into_affine(),
                *pp.get_encryption_public_params().message_base(),
            ],
            [pp.get_commitment_base(), (-weighted_Vs).into_affine()],
        ); // Making things affine here rather than converting the two bases to group elements, since that's probably what they would be converted to anyway: https://github.com/arkworks-rs/algebra/blob/c1f4f5665504154a9de2345f464b0b3da72c28ec/ec/src/models/bls12/g1.rs#L14

        if PairingOutput::<E>::ZERO != res {
            return Err(anyhow::anyhow!("Expected zero during multi-pairing check"));
        }

        Ok(())
    }
```

**File:** crates/aptos-dkg/src/pvss/chunky/weighted_transcript.rs (L394-395)
```rust
        // Aggregate the V0s
        self.V0 += other.V0;
```

**File:** crates/aptos-dkg/src/pvss/chunky/weighted_transcript.rs (L509-536)
```rust
        let mut f = vec![*s.get_secret_a()]; // constant term of polynomial
        f.extend(sample_field_elements::<E::ScalarField, _>(
            sc.get_threshold_weight() - 1,
            rng,
        )); // these are the remaining coefficients; total degree is `t - 1`, so the reconstruction threshold is `t`

        // Generate its `n` evaluations (shares) by doing an FFT over the whole domain, then truncating
        let mut f_evals = sc.get_threshold_config().domain.fft(&f);
        f_evals.truncate(sc.get_total_weight());
        debug_assert_eq!(f_evals.len(), sc.get_total_weight());

        // Encrypt the chunked shares and generate the sharing proof
        let (Cs, Rs, sharing_proof) =
            Self::encrypt_chunked_shares(&f_evals, eks, pp, sc, sok_cntxt, rng);

        // Add constant term for the `\mathbb{G}_2` commitment (we're doing this **after** the previous step
        // because we're now mutating `f_evals` by enlarging it; this is an unimportant technicality however,
        // it has no impact on computational complexity whatsoever as we could simply modify the `commit_to_scalars()`
        // function to take another input)
        f_evals.push(f[0]); // or *s.get_secret_a()

        // Commit to polynomial evaluations + constant term
        let G_2 = pp.get_commitment_base();
        let flattened_Vs = arkworks::commit_to_scalars(&G_2, &f_evals);
        debug_assert_eq!(flattened_Vs.len(), sc.get_total_weight() + 1);

        let Vs = sc.group_by_player(&flattened_Vs); // This won't use the last item in `flattened_Vs` because of `sc`
        let V0 = *flattened_Vs.last().unwrap();
```

**File:** crates/aptos-dkg/src/pvss/chunky/keys.rs (L102-110)
```rust
impl<E: Pairing> DealtPubKey<E> {
    pub fn new(G: E::G2Affine) -> Self {
        Self { G }
    }

    pub fn as_g2(&self) -> E::G2Affine {
        self.G
    }
}
```

**File:** crates/aptos-crypto/src/arkworks/scrape.rs (L168-191)
```rust
    pub fn low_degree_test_group<C: CurveGroup<ScalarField = F>>(
        &self,
        evals: &[C],
    ) -> anyhow::Result<()> {
        // Step 1: build MSM input
        let msm_input = self.ldt_msm_input(evals)?;

        // Early return in the trivial case
        if msm_input.bases.is_empty() {
            return Ok(());
        }

        // Step 2: perform MSM
        let result = C::msm(&msm_input.bases, &msm_input.scalars).unwrap();

        // Step 3: enforce expected zero
        ensure!(
            result == C::ZERO,
            "the LDT MSM should have returned zero, but returned {}",
            result
        );

        Ok(())
    }
```

**File:** aptos-move/framework/aptos-framework/sources/randomness.move (L1-7)
```text
/// This module provides access to *instant* secure randomness generated by the Aptos validators, as documented in
/// [AIP-41](https://github.com/aptos-foundation/AIPs/blob/main/aips/aip-41.md).
///
/// Secure randomness means (1) the randomness cannot be predicted ahead of time by validators, developers or users
/// and (2) the randomness cannot be biased in any way by validators, developers or users.
///
/// Security holds under the same proof-of-stake assumption that secures the Aptos network.
```

**File:** dkg/src/dkg_manager/mod.rs (L325-339)
```rust
        let mut rng = if cfg!(feature = "smoke-test") {
            StdRng::from_seed(self.my_addr.into_bytes())
        } else {
            StdRng::from_rng(thread_rng()).unwrap()
        };
        let input_secret = DKG::InputSecret::generate(&mut rng);

        let trx = DKG::generate_transcript(
            &mut rng,
            &public_params,
            &input_secret,
            self.my_index as u64,
            &self.dealer_sk,
            &self.dealer_pk,
        );
```

**File:** aptos-move/aptos-vm/src/validator_txns/dkg.rs (L104-112)
```rust
        // Deserialize transcript and verify it.
        let pub_params = DefaultDKG::new_public_params(&in_progress_session_state.metadata);
        let transcript = bcs::from_bytes::<<DefaultDKG as DKGTrait>::Transcript>(
            dkg_node.transcript_bytes.as_slice(),
        )
        .map_err(|_| Expected(TranscriptDeserializationFailed))?;

        DefaultDKG::verify_transcript(&pub_params, &transcript)
            .map_err(|_| Expected(TranscriptVerificationFailed))?;
```

**File:** crates/aptos-crypto/src/blstrs/mod.rs (L113-128)
```rust
/// Helper method to *securely* parse a sequence of bytes into a `G2Projective` point.
/// NOTE: This function will check for prime-order subgroup membership in $\mathbb{G}_2$.
pub fn g2_proj_from_bytes(bytes: &[u8]) -> Result<G2Projective, CryptoMaterialError> {
    let slice = match <&[u8; G2_PROJ_NUM_BYTES]>::try_from(bytes) {
        Ok(slice) => slice,
        Err(_) => return Err(CryptoMaterialError::WrongLengthError),
    };

    let a = G2Projective::from_compressed(slice);

    if a.is_some().unwrap_u8() == 1u8 {
        Ok(a.unwrap())
    } else {
        Err(CryptoMaterialError::DeserializationError)
    }
}
```
