# Audit Report

## Title
Storage Divergence During Block Pruning Causes Validator Node Restart Failure

## Summary
The consensus block storage layer's `commit_callback()` function tolerates pruning failures during normal operation but updates in-memory state regardless, creating state divergence between disk and memory. During node restart, if pruning fails again, the recovery path panics with no fallback mechanism, preventing the validator from restarting.

## Finding Description

The vulnerability exists in the block pruning logic within the consensus layer. During normal block commits, when `storage.prune_tree()` fails to delete old blocks from ConsensusDB, the code logs a warning but proceeds to update in-memory state: [1](#0-0) 

This creates divergence where in-memory structures reflect pruned blocks, but disk storage still contains them. The code comment assumes "the next restart will clean up dangling blocks."

However, during restart recovery, the pruning operation uses `.expect()` which panics if it fails: [2](#0-1) 

The recovery logic identifies dangling blocks through `RecoveryData::new()` and attempts to prune them: [3](#0-2) 

The underlying deletion operation can fail due to RocksDB errors (disk I/O, disk full, filesystem corruption): [4](#0-3) [5](#0-4) 

**Failure Scenario:**
1. Normal operation: Pruning fails → warning logged → in-memory state updated → divergence created
2. Node restarts (crash, maintenance, etc.)
3. Recovery identifies dangling blocks to prune
4. Pruning fails again (persistent storage issue)
5. Node panics: "unable to prune dangling blocks during restart"
6. Node cannot restart while storage issue persists

## Impact Explanation

This is a **MEDIUM severity** vulnerability per Aptos bug bounty criteria:

- **State inconsistencies requiring manual intervention**: The validator cannot restart without manual intervention to fix the storage issue
- **Temporary liveness issues**: The affected validator cannot participate in consensus until storage is repaired
- **Limited scope**: Affects individual validators experiencing persistent storage problems, not the entire network

This does NOT qualify as HIGH or CRITICAL because:
- It does not cause network-wide liveness failure (other validators continue operating)
- It does not cause consensus safety violations or state divergence across validators
- It requires persistent storage errors, not attacker-controlled inputs
- It's recoverable by fixing the underlying storage issue (freeing disk space, repairing hardware)

## Likelihood Explanation

**MEDIUM likelihood** in production environments:

**Trigger Conditions:**
- Disk space exhaustion during high blockchain activity
- Filesystem corruption from hardware failures or power loss
- I/O errors on degraded storage media
- Permission issues after system updates

**Mitigating Factors:**
- Professional validator operators should have disk space monitoring
- Storage errors are relatively uncommon in well-maintained infrastructure
- Requires TWO failures: one during operation AND persistence through restart

The likelihood is elevated because ConsensusDB operations occur frequently (every block commit), increasing exposure to transient storage errors that could become persistent issues.

## Recommendation

Replace the `.expect()` with proper error handling that falls back to partial recovery mode:

```rust
match (self as &dyn PersistentLivenessStorage)
    .prune_tree(initial_data.take_blocks_to_prune()) {
    Ok(_) => {
        // Pruning succeeded, continue with full recovery
    }
    Err(e) => {
        error!(error = ?e, "Failed to prune dangling blocks during restart, falling back to partial recovery");
        // Return PartialRecoveryData instead of panicking
        return LivenessStorageData::PartialRecoveryData(ledger_recovery_data);
    }
}
```

This allows the node to restart using `RecoveryManager` to sync blocks from peers, bypassing the pruning failure.

Additionally, consider:
1. Adding retry logic with exponential backoff for pruning operations
2. Implementing monitoring/alerting for pruning failures during normal operation
3. Adding disk space checks before attempting pruning operations

## Proof of Concept

This vulnerability cannot be easily demonstrated with a standalone PoC as it requires inducing persistent RocksDB write failures. The issue is validated through code analysis showing the error handling path.

To reproduce in a test environment:
1. Configure a validator with limited disk space
2. Allow disk to fill during blockchain operation
3. Observe pruning failure warnings in logs
4. Force node restart (kill process)
5. Node will panic during recovery with "unable to prune dangling blocks during restart"

The vulnerability is confirmed by the existence of the `.expect()` call with no fallback mechanism when `RecoveryData::new()` succeeds but `prune_tree()` fails.

## Notes

While this is a valid bug that should be fixed, its impact is limited to individual validators experiencing persistent storage issues. The Aptos network as a whole can tolerate such validators being temporarily offline (assuming < 1/3 of validators are affected). The vulnerability is in the error handling logic rather than the core consensus protocol, making it an operational reliability issue rather than a critical consensus vulnerability.

### Citations

**File:** consensus/src/block_storage/block_tree.rs (L591-599)
```rust
        if let Err(e) = storage.prune_tree(ids_to_remove.clone().into_iter().collect()) {
            // it's fine to fail here, as long as the commit succeeds, the next restart will clean
            // up dangling blocks, and we need to prune the tree to keep the root consistent with
            // executor.
            warn!(error = ?e, "fail to delete block");
        }
        self.process_pruned_blocks(ids_to_remove);
        self.update_window_root(window_root_id);
        self.update_highest_commit_cert(commit_proof);
```

**File:** consensus/src/persistent_liveness_storage.rs (L398-402)
```rust
        let blocks_to_prune = Some(Self::find_blocks_to_prune(
            root_id,
            &mut blocks,
            &mut quorum_certs,
        ));
```

**File:** consensus/src/persistent_liveness_storage.rs (L499-504)
```rust
    fn prune_tree(&self, block_ids: Vec<HashValue>) -> Result<()> {
        if !block_ids.is_empty() {
            // quorum certs that certified the block_ids will get removed
            self.db.delete_blocks_and_quorum_certificates(block_ids)?;
        }
        Ok(())
```

**File:** consensus/src/persistent_liveness_storage.rs (L570-572)
```rust
                (self as &dyn PersistentLivenessStorage)
                    .prune_tree(initial_data.take_blocks_to_prune())
                    .expect("unable to prune dangling blocks during restart");
```

**File:** consensus/src/consensusdb/mod.rs (L139-152)
```rust
    pub fn delete_blocks_and_quorum_certificates(
        &self,
        block_ids: Vec<HashValue>,
    ) -> Result<(), DbError> {
        if block_ids.is_empty() {
            return Err(anyhow::anyhow!("Consensus block ids is empty!").into());
        }
        let mut batch = SchemaBatch::new();
        block_ids.iter().try_for_each(|hash| {
            batch.delete::<BlockSchema>(hash)?;
            batch.delete::<QCSchema>(hash)
        })?;
        self.commit(batch)
    }
```
