# Audit Report

## Title
Channel Backpressure Handling Vulnerability Causes Transaction Starvation in DirectMempool Mode

## Summary
The `DirectMempoolQuorumStore::pull_internal()` function uses `try_send()` with a channel buffer size of 1 and no retry mechanism, causing consensus to fail transaction requests when mempool processing is slow, resulting in empty block proposals that starve user transactions.

## Finding Description

The vulnerability exists in the consensus-to-mempool communication channel when `DirectMempoolQuorumStore` is used (quorum store disabled mode). The critical issue is a combination of:

1. **Minimal Channel Buffer**: The consensus-to-mempool channel is created with `INTRA_NODE_CHANNEL_BUFFER_SIZE = 1` [1](#0-0) 

2. **Non-blocking Send Without Fallback**: The `pull_internal()` function uses `try_send()` which immediately fails if the channel is full, with no retry logic or fallback to blocking send [2](#0-1) 

3. **Synchronous Request Processing**: The mempool coordinator processes quorum store requests synchronously in its main event loop without spawning separate tasks [3](#0-2) 

4. **Blocking Operations in Handler**: The request handler acquires locks and performs garbage collection that can block for extended periods [4](#0-3) 

5. **Empty Block Fallback**: When `pull_internal()` fails, consensus returns an empty transaction list and proposes empty blocks [5](#0-4) 

**Attack Scenario:**
The mempool coordinator handles multiple event sources sequentially (client_events, quorum_store_requests, reconfig events, network events, scheduled_broadcasts, peer updates). When mempool processing becomes slow or the coordinator is busy with other events:

1. Consensus sends Request 1 via `try_send()` - succeeds (buffer: 0â†’1)
2. Coordinator is busy processing other events (network messages, broadcasts, reconfig)
3. Mempool begins processing Request 1 (acquires lock, runs GC, fetches batch)
4. While processing, consensus attempts Request 2
5. `try_send()` fails immediately with "channel full" error (buffer already has 1 message)
6. Consensus logs "GetBatch failed", returns empty vec
7. Leader proposes empty block despite transactions being available in mempool

This violates the implicit liveness invariant that validators should include available transactions when proposing blocks.

## Impact Explanation

**Severity: High** - "Validator node slowdowns"

This qualifies as High severity under the Aptos bug bounty criteria because:

1. **Transaction Starvation**: Affected validators continuously propose empty blocks, preventing user transaction processing on their proposals
2. **Liveness Degradation**: While consensus safety is maintained, transaction liveness is impaired for affected validators
3. **Cascading Effect**: In high-load scenarios, multiple validators could experience this simultaneously, significantly reducing network transaction throughput
4. **Operational Impact**: Validators appear healthy (producing blocks, participating in consensus) while silently failing their core function of processing transactions

The issue does not qualify as Critical because:
- Block production continues (with empty blocks)
- Consensus safety is not violated
- Network does not halt or partition
- Other validators can still process transactions

## Likelihood Explanation

**Likelihood: Medium**

This vulnerability has moderate likelihood of occurrence because:

1. **Realistic Trigger Conditions**:
   - Normal network congestion can cause mempool GC to take longer
   - Lock contention under high transaction volume
   - Coordinator busy with network events/broadcasts delays quorum store request processing
   - Large mempool state requiring extended processing time

2. **Race Condition Window**: With a buffer size of only 1, the race window is extremely narrow - any second request while coordinator processes other events or mempool is busy will fail

3. **Configuration Dependent**: Only affects nodes running in DirectMempool mode (quorum_store_enabled = false). While not the default production configuration [6](#0-5) , it is a supported operational mode that can be enabled via on-chain governance [7](#0-6) 

## Recommendation

Implement proper backpressure handling following the pattern demonstrated in the storage synchronizer [8](#0-7) :

```rust
async fn pull_internal(
    &self,
    max_items: u64,
    max_bytes: u64,
    return_non_full: bool,
    exclude_txns: Vec<TransactionSummary>,
) -> Result<Vec<SignedTransaction>, anyhow::Error> {
    let (callback, callback_rcv) = oneshot::channel();
    let exclude_txns: BTreeMap<_, _> = exclude_txns
        .into_iter()
        .map(|txn| (txn, TransactionInProgress::new(0)))
        .collect();
    let msg = QuorumStoreRequest::GetBatchRequest(
        max_items,
        max_bytes,
        return_non_full,
        exclude_txns,
        callback,
    );
    
    // Try non-blocking send first
    let mut sender = self.mempool_sender.clone();
    match sender.try_send(msg.clone()) {
        Ok(_) => {}, // Success
        Err(error) => {
            if error.is_full() {
                // Channel is full, log backpressure and fall back to blocking send
                warn!("Mempool channel full, using blocking send");
                sender.send(msg).await.map_err(anyhow::Error::from)?;
            } else {
                return Err(anyhow::Error::from(error));
            }
        }
    }
    
    // wait for response
    match monitor!(
        "pull_txn",
        timeout(
            Duration::from_millis(self.mempool_txn_pull_timeout_ms),
            callback_rcv
        )
        .await
    ) {
        Err(_) => Err(anyhow::anyhow!(
            "[direct_mempool_quorum_store] did not receive GetBatchResponse on time"
        )),
        Ok(resp) => match resp.map_err(anyhow::Error::from)?? {
            QuorumStoreResponse::GetBatchResponse(txns) => Ok(txns),
            _ => Err(anyhow::anyhow!(
                "[direct_mempool_quorum_store] did not receive expected GetBatchResponse"
            )),
        },
    }
}
```

Alternatively, increase `INTRA_NODE_CHANNEL_BUFFER_SIZE` to a larger value (e.g., 10 or more) to provide adequate queuing capacity.

## Proof of Concept

The vulnerability can be demonstrated by creating a test that simulates the coordinator being busy:

1. Create DirectMempoolQuorumStore with buffer size 1
2. Send first request - succeeds
3. Introduce delay in mempool processing (simulate GC)
4. Send second request before first completes - fails with channel full
5. Verify empty block is returned

The test infrastructure already demonstrates this scenario implicitly, as the production code uses buffer size 1 [9](#0-8) , while tests use larger buffers to avoid this issue [10](#0-9) .

## Notes

This vulnerability only manifests when `quorum_store_enabled = false`, which is not the default production configuration. However, this is a valid supported operational mode that can be enabled via on-chain governance, making this a latent vulnerability in production code that warrants fixing to prevent future issues if the configuration is changed.

### Citations

**File:** aptos-node/src/services.rs (L47-47)
```rust
const INTRA_NODE_CHANNEL_BUFFER_SIZE: usize = 1;
```

**File:** aptos-node/src/services.rs (L185-186)
```rust
    let (consensus_to_mempool_sender, consensus_to_mempool_receiver) =
        mpsc::channel(INTRA_NODE_CHANNEL_BUFFER_SIZE);
```

**File:** consensus/src/quorum_store/direct_mempool_quorum_store.rs (L64-67)
```rust
        self.mempool_sender
            .clone()
            .try_send(msg)
            .map_err(anyhow::Error::from)?;
```

**File:** consensus/src/quorum_store/direct_mempool_quorum_store.rs (L106-115)
```rust
        let (txns, result) = match self
            .pull_internal(max_txns, max_bytes, return_non_full, exclude_txns)
            .await
        {
            Err(_) => {
                error!("GetBatch failed");
                (vec![], counters::REQUEST_FAIL_LABEL)
            },
            Ok(txns) => (txns, counters::REQUEST_SUCCESS_LABEL),
        };
```

**File:** mempool/src/shared_mempool/coordinator.rs (L112-114)
```rust
            msg = quorum_store_requests.select_next_some() => {
                tasks::process_quorum_store_request(&smp, msg);
            },
```

**File:** mempool/src/shared_mempool/tasks.rs (L650-674)
```rust
                let lock_timer = counters::mempool_service_start_latency_timer(
                    counters::GET_BLOCK_LOCK_LABEL,
                    counters::REQUEST_SUCCESS_LABEL,
                );
                let mut mempool = smp.mempool.lock();
                lock_timer.observe_duration();

                {
                    let _gc_timer = counters::mempool_service_start_latency_timer(
                        counters::GET_BLOCK_GC_LABEL,
                        counters::REQUEST_SUCCESS_LABEL,
                    );
                    // gc before pulling block as extra protection against txns that may expire in consensus
                    // Note: this gc operation relies on the fact that consensus uses the system time to determine block timestamp
                    let curr_time = aptos_infallible::duration_since_epoch();
                    mempool.gc_by_expiration_time(curr_time);
                }

                let max_txns = cmp::max(max_txns, 1);
                let _get_batch_timer = counters::mempool_service_start_latency_timer(
                    counters::GET_BLOCK_GET_BATCH_LABEL,
                    counters::REQUEST_SUCCESS_LABEL,
                );
                txns =
                    mempool.get_batch(max_txns, max_bytes, return_non_full, exclude_transactions);
```

**File:** types/src/on_chain_config/consensus_config.rs (L30-36)
```rust
    pub fn default_for_genesis() -> Self {
        Self::JolteonV2 {
            main: ConsensusConfigV1::default(),
            quorum_store_enabled: true,
            order_vote_enabled: true,
        }
    }
```

**File:** consensus/src/epoch_manager.rs (L756-763)
```rust
        } else {
            info!("Building DirectMempool");
            QuorumStoreBuilder::DirectMempool(DirectMempoolInnerBuilder::new(
                consensus_to_quorum_store_rx,
                self.quorum_store_to_mempool_sender.clone(),
                self.config.mempool_txn_pull_timeout_ms,
            ))
        };
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L1267-1310)
```rust

/// Sends the given message along the specified channel, and monitors
/// if the channel hits backpressure (i.e., the channel is full).
async fn send_and_monitor_backpressure<T: Clone>(
    channel: &mut mpsc::Sender<T>,
    channel_label: &str,
    message: T,
) -> Result<(), Error> {
    match channel.try_send(message.clone()) {
        Ok(_) => Ok(()), // The message was sent successfully
        Err(error) => {
            // Otherwise, try_send failed. Handle the error.
            if error.is_full() {
                // The channel is full, log the backpressure and update the metrics.
                info!(
                    LogSchema::new(LogEntry::StorageSynchronizer).message(&format!(
                        "The {:?} channel is full! Backpressure will kick in!",
                        channel_label
                    ))
                );
                metrics::set_gauge(
                    &metrics::STORAGE_SYNCHRONIZER_PIPELINE_CHANNEL_BACKPRESSURE,
                    channel_label,
                    1, // We hit backpressure
                );

                // Call the blocking send (we still need to send the data chunk with backpressure)
                let result = channel.send(message).await.map_err(|error| {
                    Error::UnexpectedError(format!(
                        "Failed to send storage data chunk to: {:?}. Error: {:?}",
                        channel_label, error
                    ))
                });

                // Reset the gauge for the pipeline channel to inactive (we're done sending the message)
                metrics::set_gauge(
                    &metrics::STORAGE_SYNCHRONIZER_PIPELINE_CHANNEL_BACKPRESSURE,
                    channel_label,
                    0, // Backpressure is no longer active
                );

                result
            } else {
                // Otherwise, return the error (there's nothing else we can do)
```

**File:** consensus/src/quorum_store/tests/direct_mempool_quorum_store_test.rs (L20-23)
```rust
    let (quorum_store_to_mempool_sender, mut quorum_store_to_mempool_receiver) =
        mpsc::channel(1_024);
    let (mut consensus_to_quorum_store_sender, consensus_to_quorum_store_receiver) =
        mpsc::channel(1_024);
```
