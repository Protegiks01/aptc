# Audit Report

## Title
Resource Exhaustion in QuorumStore V2 Batch Recovery During Node Restart

## Summary
The QuorumStore batch recovery mechanism loads all persisted V2 batches into memory without pagination during node restart, potentially causing out-of-memory (OOM) crashes when large numbers of batches have accumulated. This prevents validator nodes from successfully restarting and participating in consensus.

## Finding Description

The vulnerability exists in the batch recovery flow when a consensus node restarts within the same epoch. The system attempts to repopulate its in-memory cache by loading all persisted batches from the database without any streaming or pagination mechanism.

The `get_all_batches_v2()` function creates an iterator over the entire `BatchV2Schema` column family and collects all entries into a HashMap: [1](#0-0) 

This function is invoked during `BatchStore::new()` construction when `is_new_epoch` is false: [2](#0-1) 

The critical issue occurs in `populate_cache_and_gc_expired_batches_v2()` where all batches are loaded into the `db_content` HashMap before any filtering or garbage collection: [3](#0-2) 

The filtering to remove expired batches happens only after all data is already resident in memory: [4](#0-3) 

**Resource Bounds Analysis:**

Each validator can accumulate batches subject to per-peer quotas, with a default of 300,000 batches per peer: [5](#0-4) 

The `PersistedValue` structure stores both batch metadata and optional transaction payloads that are serialized to the database: [6](#0-5) [7](#0-6) 

With multiple validators in the network, each contributing up to 300,000 batches, and each batch containing metadata plus optional transaction payloads, the total memory requirement during recovery can significantly exceed available system memory, especially in production networks with many validators and long-running epochs.

**Execution Flow:**

1. During normal operation, validators create batches that persist to the database with full payloads
2. Expired batches are cleaned up incrementally via `update_certified_timestamp()` during runtime: [8](#0-7) 

3. When a node restarts mid-epoch, `BatchStore::new()` is called with `is_new_epoch=false`
4. `populate_cache_and_gc_expired_batches_v2()` invokes `get_all_batches_v2()`
5. All batches (including metadata and stored payloads) load into memory simultaneously
6. If accumulated batches exceed available memory, the node crashes with OOM
7. The node cannot restart, preventing consensus participation

The same vulnerability exists during epoch transitions in `gc_previous_epoch_batches_from_db_v2()`: [9](#0-8) 

## Impact Explanation

**Severity: HIGH**

This vulnerability qualifies as **HIGH severity** under Aptos bug bounty criteria, specifically matching the "Validator node slowdowns" category through OOM crashes during restart operations.

**Concrete Impact:**
- The affected validator cannot restart and participate in consensus
- Requires operational intervention to recover (manual database cleanup, memory scaling, or configuration changes)
- Breaks resource constraint guarantees by loading unbounded data relative to available memory
- Despite per-peer quotas limiting what can be stored, there are no limits on what must be loaded during recovery

**Potential Escalation to CRITICAL:**
If multiple validators restart simultaneously (e.g., during coordinated network upgrades or widespread infrastructure events) and sufficient validators crash, this could threaten network liveness and constitute "Total loss of liveness/network availability."

The vulnerability affects operational availability of individual validator nodes and potentially the broader network if many validators are affected concurrently.

## Likelihood Explanation

**Likelihood: MEDIUM to HIGH**

This issue manifests naturally during normal network operation without requiring malicious behavior:

1. **Natural Accumulation**: Batches accumulate during regular validator operation within designed quota limits (300,000 per peer). This is expected system behavior.

2. **Common Trigger Events**: Node restarts occur frequently for legitimate operational reasons:
   - Planned software upgrades and patches
   - Crash recovery from unrelated issues
   - Routine maintenance operations
   - Infrastructure events (hardware failures, network issues, container restarts)

3. **Time Dependency**: The longer an epoch runs, the more batches accumulate. Longer epochs increase risk proportionally.

4. **Network Scale Dependency**: Larger validator sets create more per-peer quotas and higher total batch counts, increasing memory requirements during recovery.

5. **Configuration Dependent**: Nodes with limited memory relative to the network's batch accumulation rate are at higher risk.

The likelihood increases over time within an epoch and scales with network size. In production networks with many validators and moderate-to-long epoch durations, this poses a real operational risk that will naturally manifest without requiring attacker involvement.

## Recommendation

Implement streaming/pagination for batch recovery to avoid loading all batches into memory simultaneously:

**Option 1: Streaming Recovery**
Modify `get_all_batches_v2()` and `populate_cache_and_gc_expired_batches_v2()` to process batches incrementally:
- Process database iterator in chunks
- Filter and insert to cache incrementally
- Batch delete operations for expired batches

**Option 2: Bounded Recovery**
Implement a maximum batch recovery limit with logic to prioritize:
- Most recently created batches
- Batches with nearest expiration times
- Batches from active consensus rounds

**Option 3: Background Recovery**
- Load only critical batch metadata during startup for fast recovery
- Fetch full payloads lazily on demand
- Implement background task to warm cache gradually

**Example Fix Skeleton:**
```rust
fn populate_cache_and_gc_expired_batches_v2_streaming(
    db: Arc<dyn QuorumStoreStorage>,
    current_epoch: u64,
    last_certified_time: u64,
    expiration_buffer_usecs: u64,
    batch_store: &BatchStore,
) {
    const BATCH_CHUNK_SIZE: usize = 10_000;
    let mut iter = db.iter::<BatchV2Schema>().expect("Failed to create iterator");
    iter.seek_to_first();
    
    let gc_timestamp = last_certified_time.saturating_sub(expiration_buffer_usecs);
    let mut expired_keys = Vec::new();
    let mut processed = 0;
    
    for result in iter {
        let (digest, value) = result.expect("Failed to read batch");
        
        if value.expiration() < gc_timestamp {
            expired_keys.push(digest);
        } else {
            batch_store.insert_to_cache(&value)
                .expect("Storage limit exceeded");
        }
        
        processed += 1;
        if processed % BATCH_CHUNK_SIZE == 0 {
            // Periodically delete expired batches to free memory
            if !expired_keys.is_empty() {
                let keys_to_delete = std::mem::take(&mut expired_keys);
                tokio::task::spawn_blocking(move || {
                    db.delete_batches_v2(keys_to_delete).ok();
                });
            }
        }
    }
    
    // Final cleanup
    if !expired_keys.is_empty() {
        tokio::task::spawn_blocking(move || {
            db.delete_batches_v2(expired_keys).ok();
        });
    }
}
```

## Proof of Concept

While a full PoC would require a production-scale deployment, the vulnerability can be demonstrated by:

1. Configuring a test network with multiple validators
2. Running the network for an extended period to accumulate batches near quota limits
3. Monitoring memory usage during a mid-epoch validator restart
4. Observing memory spike during `get_all_batches_v2()` execution

The vulnerability is inherent in the code structure and can be validated through code inspection showing that `.collect()` materializes the entire iterator into a HashMap before any filtering occurs.

### Citations

**File:** consensus/src/quorum_store/quorum_store_db.rs (L133-138)
```rust
    fn get_all_batches_v2(&self) -> Result<HashMap<HashValue, PersistedValue<BatchInfoExt>>> {
        let mut iter = self.db.iter::<BatchV2Schema>()?;
        iter.seek_to_first();
        iter.map(|res| res.map_err(Into::into))
            .collect::<Result<HashMap<HashValue, PersistedValue<BatchInfoExt>>>>()
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L161-175)
```rust
        } else {
            Self::populate_cache_and_gc_expired_batches_v1(
                db_clone.clone(),
                epoch,
                last_certified_time,
                expiration_buffer_usecs,
                &batch_store,
            );
            Self::populate_cache_and_gc_expired_batches_v2(
                db_clone,
                epoch,
                last_certified_time,
                expiration_buffer_usecs,
                &batch_store,
            );
```

**File:** consensus/src/quorum_store/batch_store.rs (L212-220)
```rust
    fn gc_previous_epoch_batches_from_db_v2(db: Arc<dyn QuorumStoreStorage>, current_epoch: u64) {
        let db_content = db
            .get_all_batches_v2()
            .expect("failed to read data from db");
        info!(
            epoch = current_epoch,
            "QS: Read batches from storage. Len: {}",
            db_content.len(),
        );
```

**File:** consensus/src/quorum_store/batch_store.rs (L292-326)
```rust
    fn populate_cache_and_gc_expired_batches_v2(
        db: Arc<dyn QuorumStoreStorage>,
        current_epoch: u64,
        last_certified_time: u64,
        expiration_buffer_usecs: u64,
        batch_store: &BatchStore,
    ) {
        let db_content = db
            .get_all_batches_v2()
            .expect("failed to read v1 data from db");
        info!(
            epoch = current_epoch,
            "QS: Read v1 batches from storage. Len: {}, Last Cerified Time: {}",
            db_content.len(),
            last_certified_time
        );

        let mut expired_keys = Vec::new();
        let gc_timestamp = last_certified_time.saturating_sub(expiration_buffer_usecs);
        for (digest, value) in db_content {
            let expiration = value.expiration();
            trace!(
                "QS: Batchreader recovery content exp {:?}, digest {}",
                expiration,
                digest
            );

            if expiration < gc_timestamp {
                expired_keys.push(digest);
            } else {
                batch_store
                    .insert_to_cache(&value)
                    .expect("Storage limit exceeded upon BatchReader construction");
            }
        }
```

**File:** consensus/src/quorum_store/batch_store.rs (L530-539)
```rust
    pub fn update_certified_timestamp(&self, certified_time: u64) {
        trace!("QS: batch reader updating time {:?}", certified_time);
        self.last_certified_time
            .fetch_max(certified_time, Ordering::SeqCst);

        let expired_keys = self.clear_expired_payload(certified_time);
        if let Err(e) = self.db.delete_batches(expired_keys) {
            debug!("Error deleting batches: {:?}", e)
        }
    }
```

**File:** config/src/config/quorum_store_config.rs (L133-135)
```rust
            memory_quota: 120_000_000,
            db_quota: 300_000_000,
            batch_quota: 300_000,
```

**File:** consensus/src/quorum_store/types.rs (L21-25)
```rust
#[derive(Clone, Eq, Deserialize, Serialize, PartialEq, Debug)]
pub struct PersistedValue<T> {
    info: T,
    maybe_payload: Option<Vec<SignedTransaction>>,
}
```

**File:** consensus/src/quorum_store/schema.rs (L49-56)
```rust
pub(crate) struct BatchV2Schema;

impl Schema for BatchV2Schema {
    type Key = HashValue;
    type Value = PersistedValue<BatchInfoExt>;

    const COLUMN_FAMILY_NAME: aptos_schemadb::ColumnFamilyName = BATCH_V2_CF_NAME;
}
```
