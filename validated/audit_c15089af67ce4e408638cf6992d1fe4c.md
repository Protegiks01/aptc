# Audit Report

## Title
Missing Player ID Validation in Secret Share Aggregation Enables Denial of Service via Duplicate Virtual Player IDs

## Summary
A malicious validator can forge the `Player` ID field in their decryption key share to match another validator's ID, bypassing cryptographic verification and causing duplicate virtual player indices during weighted Lagrange interpolation reconstruction. This prevents successful aggregation of decryption keys, resulting in denial of service for encrypted transaction processing in the consensus pipeline.

## Finding Description

The secret sharing implementation in Aptos's FPTX weighted threshold encryption scheme fails to validate that the `Player` ID embedded in a decryption key share matches the expected player index derived from the share's `Author`. This breaks the critical invariant that each validator's shares must correspond to their unique position in the weighted threshold scheme.

**Vulnerability Chain:**

**1. Missing Player ID Validation**

The `verify()` function maps the share's `Author` to an expected validator index but never validates that the `Player` ID in the share matches this index: [1](#0-0) 

The TODO comment at line 78 explicitly acknowledges incomplete bounds checking, but the deeper issue is that the `Player` ID field (the first element of the `WeightedBIBEDecryptionKeyShare` tuple) is never validated against the expected index.

**2. Verification Bypass**

The cryptographic verification uses the verification key indexed by the `Author`, not the `Player` ID from the share: [2](#0-1) 

At line 167, the verification explicitly uses `self.weighted_player` (from the `WeightedBIBEVerificationKey` structure indexed by `Author`) rather than the `Player` ID from `dk_share.0`. This allows a malicious validator to provide a cryptographically valid share (signed with their own key) but with an arbitrary `Player` ID field.

**3. Duplicate Virtual Players During Reconstruction**

During weighted reconstruction, shares are flattened into virtual players based on their claimed `Player` ID: [3](#0-2) 

At line 430, the reconstruction iterates over `(player, sub_shares)` where `player` comes directly from the share tuple. If two shares claim the same `Player` ID, both will call `get_virtual_player` with the same player index at line 436, creating duplicate indices in `flattened_shares`.

The `get_virtual_player` function deterministically maps player and position to virtual player ID: [4](#0-3) 

**4. Failure in Lagrange Interpolation**

The duplicate virtual player indices are passed to `lagrange_for_subset`, which computes a vanishing polynomial with duplicate roots: [5](#0-4) 

When a polynomial has duplicate roots, its derivative evaluates to zero at those points. At line 282, `batch_inversion` is called on the derivative evaluations. The `ark_ff::batch_inversion` function cannot invert zero values, causing the reconstruction to fail.

**5. Attack Execution Path**

The vulnerability is exploitable through the consensus pipeline's encrypted transaction decryption phase. When shares are received from validators, they undergo validation: [6](#0-5) 

Line 45 ensures the `Author` matches the peer (preventing impersonation), and line 52 calls cryptographic verification. However, neither check validates the `Player` ID field.

Shares are stored by `Author` in a HashMap: [7](#0-6) 

This allows both the malicious share (with forged `Player` ID) and the legitimate share (with correct `Player` ID) to coexist, both claiming the same player index but stored under different author keys.

During aggregation: [8](#0-7) 

All shares are passed to reconstruction at line 56, where the duplicate Player IDs cause the Lagrange coefficient computation to fail, preventing successful decryption key reconstruction.

**Attack Scenario:**

1. Malicious validator V₁ (Author A₁, legitimate Player P₁) derives their share normally
2. V₁ modifies the share tuple from `(P₁, values)` to `(P₂, values)` where P₂ is another validator's ID
3. V₁ broadcasts this malformed share; verification passes because only the cryptographic signature is checked against A₁'s verification key
4. The legitimate validator V₂ broadcasts their share with Player ID P₂
5. Both shares are stored separately (keyed by distinct Authors A₁ and A₂) and included in aggregation
6. During reconstruction, both shares flatten to overlapping virtual player indices
7. Lagrange coefficient computation encounters duplicate indices → derivative = 0 → reconstruction fails → encrypted transactions cannot be processed

## Impact Explanation

**Severity: HIGH** (per Aptos bug bounty criteria - "Validator Node Slowdowns/Crashes")

This vulnerability enables a single Byzantine validator to cause:

- **Denial of service for encrypted transactions**: Failure to reconstruct decryption keys prevents processing encrypted transaction payloads in the consensus pipeline. While the aggregation error is caught and logged, it prevents validators from successfully decrypting encrypted transactions.
- **Protocol violation**: Violates Byzantine fault tolerance assumptions that the system should tolerate up to ⅓ malicious validators without availability loss
- **Consensus availability impact**: Affects the ability to process encrypted transactions, though not all consensus operations

The attack affects consensus availability for encrypted transaction processing. This aligns with HIGH severity ("Validator Node Slowdowns/Crashes") per the Aptos bug bounty program, as it causes denial of service for a critical consensus subsystem.

## Likelihood Explanation

**Likelihood: HIGH**

The attack is trivial to execute for any malicious validator:

- **No timing dependencies or race conditions**: The attack works deterministically
- **No collusion required**: Single validator can execute independently
- **Guaranteed failure**: Duplicate indices always cause zero derivative and reconstruction failure
- **Simple modification**: Only requires changing a single field in the share structure before broadcasting

The attack succeeds whenever:
1. The malicious validator is in the active validator set (has weight > 0)
2. At least one other validator exists to target
3. Encrypted transactions are being processed (triggering secret sharing reconstruction)

The vulnerability exists in production code actively used in the consensus pipeline for FPTX-based encrypted transactions.

## Recommendation

Add validation in the `verify()` function to ensure the `Player` ID in the share matches the expected player index:

```rust
pub fn verify(&self, config: &SecretShareConfig) -> anyhow::Result<()> {
    let index = config.get_id(self.author());
    let decryption_key_share = self.share().clone();
    
    // Validate that the Player ID in the share matches the expected index
    ensure!(
        decryption_key_share.player().get_id() == index,
        "Player ID mismatch: expected {}, got {}",
        index,
        decryption_key_share.player().get_id()
    );
    
    ensure!(
        index < config.verification_keys.len(),
        "Player index {} out of bounds",
        index
    );
    
    config.verification_keys[index]
        .verify_decryption_key_share(&self.metadata.digest, &decryption_key_share)?;
    Ok(())
}
```

This ensures that the Player ID field cannot be forged to match another validator's ID, preventing duplicate virtual player indices during reconstruction.

## Proof of Concept

A complete PoC would require:
1. Setting up a test validator set with weighted secret sharing configuration
2. Creating a malicious validator that modifies the Player ID in their share before broadcasting
3. Demonstrating that both the malicious and legitimate shares pass validation
4. Showing that reconstruction fails when both shares are aggregated due to duplicate virtual player indices

The vulnerability is confirmed through code analysis showing the missing validation and the mathematical impossibility of Lagrange interpolation with duplicate evaluation points.

## Notes

The TODO comment at line 78 of `types/src/secret_sharing.rs` indicates the developers were aware of incomplete validation but may not have recognized the full security implications of the missing Player ID check. This vulnerability demonstrates that the missing validation enables a single Byzantine validator to cause denial of service for encrypted transaction processing through duplicate Player ID injection.

### Citations

**File:** types/src/secret_sharing.rs (L75-82)
```rust
    pub fn verify(&self, config: &SecretShareConfig) -> anyhow::Result<()> {
        let index = config.get_id(self.author());
        let decryption_key_share = self.share().clone();
        // TODO(ibalajiarun): Check index out of bounds
        config.verification_keys[index]
            .verify_decryption_key_share(&self.metadata.digest, &decryption_key_share)?;
        Ok(())
    }
```

**File:** crates/aptos-batch-encryption/src/schemes/fptx_weighted.rs (L149-169)
```rust
    pub fn verify_decryption_key_share(
        &self,
        digest: &Digest,
        dk_share: &WeightedBIBEDecryptionKeyShare,
    ) -> Result<()> {
        (self.vks_g2.len() == dk_share.1.len())
            .then_some(())
            .ok_or(BatchEncryptionError::DecryptionKeyVerifyError)?;

        self.vks_g2
            .iter()
            .map(|vk_g2| BIBEVerificationKey {
                mpk_g2: self.mpk_g2,
                vk_g2: *vk_g2,
                player: self.weighted_player, // arbitrary
            })
            .zip(&dk_share.1)
            .try_for_each(|(vk, dk_share)| {
                vk.verify_decryption_key_share(digest, &(self.weighted_player, dk_share.clone()))
            })
    }
```

**File:** crates/aptos-crypto/src/weighted_config.rs (L177-184)
```rust
    pub fn get_virtual_player(&self, player: &Player, j: usize) -> Player {
        // println!("WeightedConfig::get_virtual_player({player}, {i})");
        assert_lt!(j, self.weights[player.id]);

        let id = self.get_share_index(player.id, j).unwrap();

        Player { id }
    }
```

**File:** crates/aptos-crypto/src/weighted_config.rs (L423-450)
```rust
    fn reconstruct(
        sc: &WeightedConfigArkworks<F>,
        shares: &[ShamirShare<Self::ShareValue>],
    ) -> anyhow::Result<Self> {
        let mut flattened_shares = Vec::with_capacity(sc.get_total_weight());

        // println!();
        for (player, sub_shares) in shares {
            // println!(
            //     "Flattening {} share(s) for player {player}",
            //     sub_shares.len()
            // );
            for (pos, share) in sub_shares.iter().enumerate() {
                let virtual_player = sc.get_virtual_player(player, pos);

                // println!(
                //     " + Adding share {pos} as virtual player {virtual_player}: {:?}",
                //     share
                // );
                // TODO(Performance): Avoiding the cloning here might be nice
                let tuple = (virtual_player, share.clone());
                flattened_shares.push(tuple);
            }
        }
        flattened_shares.truncate(sc.get_threshold_weight());

        SK::reconstruct(sc.get_threshold_config(), &flattened_shares)
    }
```

**File:** crates/aptos-crypto/src/arkworks/shamir.rs (L253-290)
```rust
    pub fn lagrange_for_subset(&self, indices: &[usize]) -> Vec<F> {
        // Step 0: check that subset is large enough
        assert!(
            indices.len() >= self.t,
            "subset size {} is smaller than threshold t={}",
            indices.len(),
            self.t
        );

        let xs_vec: Vec<F> = indices.iter().map(|i| self.domain.element(*i)).collect();

        // Step 1: compute poly w/ roots at all x in xs, compute eval at 0
        let vanishing_poly = vanishing_poly::from_roots(&xs_vec);
        let vanishing_poly_at_0 = vanishing_poly.coeffs[0]; // vanishing_poly(0) = const term

        // Step 2 (numerators): for each x in xs, divide poly eval from step 1 by (-x) using batch inversion
        let mut neg_xs: Vec<F> = xs_vec.iter().map(|&x| -x).collect();
        batch_inversion(&mut neg_xs);
        let numerators: Vec<F> = neg_xs
            .iter()
            .map(|&inv_neg_x| vanishing_poly_at_0 * inv_neg_x)
            .collect();

        // Step 3a (denominators): Compute derivative of poly from step 1, and its evaluations
        let derivative = vanishing_poly.differentiate();
        let derivative_evals = derivative.evaluate_over_domain(self.domain).evals; // TODO: with a filter perhaps we don't have to store all evals, but then batch inversion becomes a bit more tedious

        // Step 3b: Only keep the relevant evaluations, then perform a batch inversion
        let mut denominators: Vec<F> = indices.iter().map(|i| derivative_evals[*i]).collect();
        batch_inversion(&mut denominators);

        // Step 4: compute Lagrange coefficients
        numerators
            .into_iter()
            .zip(denominators)
            .map(|(numerator, denom_inv)| numerator * denom_inv)
            .collect()
    }
```

**File:** consensus/src/rand/secret_sharing/reliable_broadcast_state.rs (L44-59)
```rust
    fn add(&self, peer: Author, share: Self::Response) -> anyhow::Result<Option<()>> {
        ensure!(share.author() == &peer, "Author does not match");
        ensure!(
            share.metadata() == &self.secret_share_metadata,
            "Metadata does not match: local {:?}, received {:?}",
            self.secret_share_metadata,
            share.metadata()
        );
        share.verify(&self.secret_share_config)?;
        info!(LogSchema::new(LogEvent::ReceiveReactiveSecretShare)
            .epoch(share.epoch())
            .round(share.metadata().round)
            .remote_peer(*share.author()));
        let mut store = self.secret_share_store.lock();
        let aggregated = store.add_share(share)?.then_some(());
        Ok(aggregated)
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L32-36)
```rust
    pub fn add_share(&mut self, share: SecretShare, weight: u64) {
        if self.shares.insert(share.author, share).is_none() {
            self.total_weight += weight;
        }
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L55-70)
```rust
        tokio::task::spawn_blocking(move || {
            let maybe_key = SecretShare::aggregate(self.shares.values(), &dec_config);
            match maybe_key {
                Ok(key) => {
                    let dec_key = SecretSharedKey::new(metadata, key);
                    let _ = decision_tx.unbounded_send(dec_key);
                },
                Err(e) => {
                    warn!(
                        epoch = metadata.epoch,
                        round = metadata.round,
                        "Aggregation error: {e}"
                    );
                },
            }
        });
```
