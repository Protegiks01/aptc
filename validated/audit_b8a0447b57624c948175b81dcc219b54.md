# Audit Report

## Title
Consensus Observer Bypasses Signature Verification for Future-Epoch CommitDecisions Enabling Critical State Corruption

## Summary
The consensus observer component accepts `CommitDecision` messages from subscribed peers and skips signature verification when the epoch doesn't match the current epoch. These unverified commit decisions are then used to corrupt critical node state including the execution client's logical time and buffer manager's committed round tracking, leading to permanent node dysfunction.

## Finding Description

The vulnerability exists in how the consensus observer processes `CommitDecision` messages from network peers. The attack exploits a conditional verification bypass combined with premature state corruption.

**1. Unvalidated Deserialization**

The `CommitDecision` struct derives `Deserialize`, allowing direct deserialization from network messages without validation: [1](#0-0) 

While a `verify()` method exists to check signatures: [2](#0-1) 

**2. Conditional Verification Bypass**

In the consensus observer's message processing flow, verification is conditionally skipped: [3](#0-2) 

When `commit_epoch != epoch_state.epoch`, the verification at line 470 is skipped entirely. However, the unverified `CommitDecision` is still used at line 526: [4](#0-3) 

**3. State Corruption Before Sync Validation**

The unverified commit decision triggers state sync through `sync_to_commit()`, which calls the execution client: [5](#0-4) 

Critically, in `execution_client.sync_to_target()`, the reset occurs BEFORE sync validation: [6](#0-5) 

The TODO comment at lines 669-670 explicitly acknowledges this design flaw.

**4. Buffer Manager Corruption**

The reset extracts the round from the UNVERIFIED target and uses it to reset the buffer manager: [7](#0-6) 

This sets `highest_committed_round` and `latest_round` to attacker-controlled values: [8](#0-7) 

**5. Logical Time Corruption**

The execution proxy's `sync_to_target()` extracts epoch and round from the unverified target to construct `target_logical_time`: [9](#0-8) 

At line 222, the node's `latest_logical_time` is updated to these malicious values: [10](#0-9) 

**6. Root Ledger Info Corruption**

The observer's root ledger info is updated via `update_blocks_for_state_sync_commit()`: [11](#0-10) 

The `update_root()` method directly assigns the unverified ledger info without validation: [12](#0-11) 

**Attack Scenario:**
1. Attacker operates a node on the public network advertising ConsensusObserver protocol support
2. VFN observer subscribes to attacker's node based on distance/latency metrics (no trust requirements)
3. Attacker sends `CommitDecision` with `epoch = current_epoch + 1`, `round = u64::MAX - 100`, invalid signatures
4. Observer skips verification (different epoch) but uses the unverified decision
5. Buffer manager, logical time, and root are corrupted BEFORE sync validation
6. Even if state sync fails, corruption persists due to premature reset
7. Observer cannot process legitimate blocks (they appear "behind" corrupted state)

## Impact Explanation

This vulnerability has **High Severity** impact per Aptos bug bounty criteria:

**1. API Crashes (High Severity)**
VFN observers serve API requests for applications. The corrupted state prevents accurate blockchain state queries, causing effective "API crashes" as defined in the bounty program.

**2. VFN Observer Node Dysfunction**
The corrupted logical time and buffer manager state prevent processing any legitimate consensus messages, as they all have lower logical time values than the corrupted state. The node becomes permanently non-functional.

**3. No Recovery Mechanism**
The TODO comment at execution_client.rs:669-670 explicitly acknowledges the lack of recovery when "reset but sync fails," confirming permanent corruption without manual intervention.

VFN observers are enabled by default per the configuration: [13](#0-12) 

This meets the **High Severity** criteria of "API crashes" and node dysfunction affecting network infrastructure.

## Likelihood Explanation

**Likelihood: Medium**

The attack is feasible because:

1. **Default Enabled Configuration**: VFN observers have both observer and publisher features enabled by default
2. **No Trust Requirements**: Peer selection is based on distance/latency metrics, not trust relationships: [14](#0-13) 

3. **Public Network Access**: VFN observers can subscribe to peers on any connected network, including public fullnodes
4. **Simple Attack Vector**: Only requires sending a single malformed CommitDecision message with future epoch

The attack does not require:
- Compromising trusted nodes
- Majority stake control
- Complex timing coordination
- Expensive resources

## Recommendation

**Immediate Fix:**
1. **Verify Before Reset**: Move signature verification before state corruption. Verify commit decisions for ALL epochs, not just current epoch:
```rust
// In consensus_observer.rs, verify ALL commit decisions
if let Err(error) = commit_decision.verify_commit_proof(&epoch_state) {
    error!("Failed to verify commit decision from future epoch");
    return;
}
```

2. **Implement Recovery**: As noted in the TODO comment, implement recovery mechanism to restore buffer manager state if sync fails after reset

3. **Validate Epoch Bounds**: Add sanity checks to reject commit decisions from epochs too far in the future (e.g., > current_epoch + 1)

**Long-term Fix:**
4. **Trust-Based Peer Selection**: Implement trust scoring for subscription peers, prioritizing known validators/VFNs over untrusted public peers
5. **State Validation**: Add validation in `update_root()`, `buffer_manager.process_reset_request()`, and `sync_to_target()` to reject unreasonable values

## Proof of Concept

While a full executable PoC requires network setup, the attack path is verified through the codebase:

1. Configure attacker node to advertise ConsensusObserver protocol support
2. Connect to target VFN observer's public network interface
3. Wait for VFN to subscribe (automatic based on peer selection algorithm)
4. Send `CommitDecision` message with:
   - epoch = (current_epoch + 1)
   - round = u64::MAX - 100
   - version = u64::MAX - 100
   - Empty or invalid signature aggregation
5. Observe VFN observer's buffer_manager.highest_committed_round = u64::MAX - 100
6. Verify VFN observer rejects all legitimate consensus messages as "behind" committed round
7. Confirm VFN observer API returns stale/incorrect state

The vulnerability is demonstrable by code inspection showing the verification bypass and premature state corruption paths.

## Notes

This vulnerability exploits a design flaw where state corruption occurs before validating that state sync can succeed. The explicit TODO comment in the code confirms the developers were aware of the general issue but it remains unaddressed. The default-enabled configuration on VFN observers and lack of trust requirements for subscription peers make this readily exploitable against production infrastructure.

### Citations

**File:** consensus/consensus-types/src/pipeline/commit_decision.rs (L10-13)
```rust
#[derive(Deserialize, Serialize, Clone, PartialEq, Eq)]
pub struct CommitDecision {
    ledger_info: LedgerInfoWithSignatures,
}
```

**File:** consensus/consensus-types/src/pipeline/commit_decision.rs (L47-59)
```rust
    /// Verifies that the signatures carried in the message forms a valid quorum,
    /// and then verifies the signature.
    pub fn verify(&self, validator: &ValidatorVerifier) -> anyhow::Result<()> {
        ensure!(
            !self.ledger_info.commit_info().is_ordered_only(),
            "Unexpected ordered only commit info"
        );
        // We do not need to check the author because as long as the signature tree
        // is valid, the message should be valid.
        self.ledger_info
            .verify_signatures(validator)
            .context("Failed to verify Commit Decision")
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L466-495)
```rust
        // If the commit decision is for the current epoch, verify and process it
        let epoch_state = self.get_epoch_state();
        if commit_epoch == epoch_state.epoch {
            // Verify the commit decision
            if let Err(error) = commit_decision.verify_commit_proof(&epoch_state) {
                // Log the error and update the invalid message counter
                error!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "Failed to verify commit decision! Ignoring: {:?}, from peer: {:?}. Error: {:?}",
                        commit_decision.proof_block_info(),
                        peer_network_id,
                        error
                    ))
                );
                increment_invalid_message_counter(&peer_network_id, metrics::COMMIT_DECISION_LABEL);
                return;
            }

            // Update the latency metrics for commit processing
            update_message_processing_latency_metrics(
                message_received_time,
                &peer_network_id,
                metrics::COMMIT_DECISION_LABEL,
            );

            // Update the pending blocks with the commit decision
            if self.process_commit_decision_for_pending_block(&commit_decision) {
                return; // The commit decision was successfully processed
            }
        }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L500-527)
```rust
        // Otherwise, we failed to process the commit decision. If the commit
        // is for a future epoch or round, we need to state sync.
        let last_block = self.observer_block_data.lock().get_last_ordered_block();
        let epoch_changed = commit_epoch > last_block.epoch();
        if epoch_changed || commit_round > last_block.round() {
            // If we're waiting for state sync to transition into a new epoch,
            // we should just wait and not issue a new state sync request.
            if self.state_sync_manager.is_syncing_through_epoch() {
                info!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "Already waiting for state sync to reach new epoch: {:?}. Dropping commit decision: {:?}!",
                        self.observer_block_data.lock().root().commit_info(),
                        commit_decision.proof_block_info()
                    ))
                );
                return;
            }

            // Otherwise, we should start the state sync process for the commit.
            // Update the block data (to the commit decision).
            self.observer_block_data
                .lock()
                .update_blocks_for_state_sync_commit(&commit_decision);

            // Start state syncing to the commit decision
            self.state_sync_manager
                .sync_to_commit(commit_decision, epoch_changed);
        }
```

**File:** consensus/src/consensus_observer/observer/state_sync_manager.rs (L218-231)
```rust
                // Sync to the commit decision
                if let Err(error) = execution_client
                    .clone()
                    .sync_to_target(commit_decision.commit_proof().clone())
                    .await
                {
                    error!(
                        LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                            "Failed to sync to commit decision: {:?}! Error: {:?}",
                            commit_decision, error
                        ))
                    );
                    return;
                }
```

**File:** consensus/src/pipeline/execution_client.rs (L661-672)
```rust
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });

        // Reset the rand and buffer managers to the target round
        self.reset(&target).await?;

        // TODO: handle the state sync error (e.g., re-push the ordered
        // blocks to the buffer manager when it's reset but sync fails).
        self.execution_proxy.sync_to_target(target).await
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L695-706)
```rust
        if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
            // reset execution phase and commit phase
            let (tx, rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::ResetDropped)?;
            rx.await.map_err(|_| Error::ResetDropped)?;
        }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L578-596)
```rust
    /// It pops everything in the buffer and if reconfig flag is set, it stops the main loop
    async fn process_reset_request(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        info!("Receive reset");

        match signal {
            ResetSignal::Stop => self.stop = true,
            ResetSignal::TargetRound(round) => {
                self.highest_committed_round = round;
                self.latest_round = round;

                let _ = self.drain_pending_commit_proof_till(round);
            },
        }

        self.reset().await;
        let _ = tx.send(ResetAck::default());
        info!("Reset finishes");
    }
```

**File:** consensus/src/state_computer.rs (L177-194)
```rust
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        // Grab the logical time lock and calculate the target logical time
        let mut latest_logical_time = self.write_mutex.lock().await;
        let target_logical_time =
            LogicalTime::new(target.ledger_info().epoch(), target.ledger_info().round());

        // Before state synchronization, we have to call finish() to free the
        // in-memory SMT held by BlockExecutor to prevent a memory leak.
        self.executor.finish();

        // The pipeline phase already committed beyond the target block timestamp, just return.
        if *latest_logical_time >= target_logical_time {
            warn!(
                "State sync target {:?} is lower than already committed logical time {:?}",
                target_logical_time, *latest_logical_time
            );
            return Ok(());
        }
```

**File:** consensus/src/state_computer.rs (L221-227)
```rust
        // Update the latest logical time
        *latest_logical_time = target_logical_time;

        // Similarly, after state synchronization, we have to reset the cache of
        // the BlockExecutor to guarantee the latest committed state is up to date.
        self.executor.reset()?;

```

**File:** consensus/src/consensus_observer/observer/block_data.rs (L273-291)
```rust
    /// Updates the block data for the given commit decision
    /// that will be used by state sync to catch us up.
    pub fn update_blocks_for_state_sync_commit(&mut self, commit_decision: &CommitDecision) {
        // Get the commit proof, epoch and round
        let commit_proof = commit_decision.commit_proof();
        let commit_epoch = commit_decision.epoch();
        let commit_round = commit_decision.round();

        // Update the root
        self.update_root(commit_proof.clone());

        // Update the block payload store
        self.block_payload_store
            .remove_blocks_for_epoch_round(commit_epoch, commit_round);

        // Update the ordered block store
        self.ordered_block_store
            .remove_blocks_for_commit(commit_proof);
    }
```

**File:** consensus/src/consensus_observer/observer/block_data.rs (L299-302)
```rust
    /// Updates the root ledger info
    pub fn update_root(&mut self, new_root: LedgerInfoWithSignatures) {
        self.root = new_root;
    }
```

**File:** config/src/config/consensus_observer_config.rs (L119-128)
```rust
            NodeType::ValidatorFullnode => {
                if ENABLE_ON_VALIDATOR_FULLNODES
                    && !observer_manually_set
                    && !publisher_manually_set
                {
                    // Enable both the observer and the publisher for VFNs
                    consensus_observer_config.observer_enabled = true;
                    consensus_observer_config.publisher_enabled = true;
                    modified_config = true;
                }
```

**File:** consensus/src/consensus_observer/observer/subscription_utils.rs (L275-356)
```rust
/// Sorts the peers by subscription optimality (in descending order of
/// optimality). This requires: (i) sorting the peers by distance from the
/// validator set and ping latency (lower values are more optimal); and (ii)
/// filtering out peers that don't support consensus observer.
///
/// Note: we prioritize distance over latency as we want to avoid close
/// but not up-to-date peers. If peers don't have sufficient metadata
/// for sorting, they are given a lower priority.
pub fn sort_peers_by_subscription_optimality(
    peers_and_metadata: &HashMap<PeerNetworkId, PeerMetadata>,
) -> Vec<PeerNetworkId> {
    // Group peers and latencies by validator distance, i.e., distance -> [(peer, latency)]
    let mut unsupported_peers = Vec::new();
    let mut peers_and_latencies_by_distance = BTreeMap::new();
    for (peer_network_id, peer_metadata) in peers_and_metadata {
        // Verify that the peer supports consensus observer
        if !supports_consensus_observer(peer_metadata) {
            unsupported_peers.push(*peer_network_id);
            continue; // Skip the peer
        }

        // Get the distance and latency for the peer
        let distance = get_distance_for_peer(peer_network_id, peer_metadata);
        let latency = get_latency_for_peer(peer_network_id, peer_metadata);

        // If the distance is not found, use the maximum distance
        let distance =
            distance.unwrap_or(aptos_peer_monitoring_service_types::MAX_DISTANCE_FROM_VALIDATORS);

        // If the latency is not found, use a large latency
        let latency = latency.unwrap_or(MAX_PING_LATENCY_SECS);

        // Add the peer and latency to the distance group
        peers_and_latencies_by_distance
            .entry(distance)
            .or_insert_with(Vec::new)
            .push((*peer_network_id, OrderedFloat(latency)));
    }

    // If there are peers that don't support consensus observer, log them
    if !unsupported_peers.is_empty() {
        info!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "Found {} peers that don't support consensus observer! Peers: {:?}",
                unsupported_peers.len(),
                unsupported_peers
            ))
        );
    }

    // Sort the peers by distance and latency. Note: BTreeMaps are
    // sorted by key, so the entries will be sorted by distance in ascending order.
    let mut sorted_peers_and_latencies = Vec::new();
    for (_, mut peers_and_latencies) in peers_and_latencies_by_distance {
        // Sort the peers by latency
        peers_and_latencies.sort_by_key(|(_, latency)| *latency);

        // Add the peers to the sorted list (in sorted order)
        sorted_peers_and_latencies.extend(peers_and_latencies);
    }

    // Log the sorted peers and latencies
    info!(
        LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
            "Sorted {} peers by subscription optimality! Peers and latencies: {:?}",
            sorted_peers_and_latencies.len(),
            sorted_peers_and_latencies
        ))
    );

    // Only return the sorted peers (without the latencies)
    sorted_peers_and_latencies
        .into_iter()
        .map(|(peer, _)| peer)
        .collect()
}

/// Returns true iff the peer metadata indicates support for consensus observer
fn supports_consensus_observer(peer_metadata: &PeerMetadata) -> bool {
    peer_metadata.supports_protocol(ProtocolId::ConsensusObserver)
        && peer_metadata.supports_protocol(ProtocolId::ConsensusObserverRpc)
}
```
