# Audit Report

## Title
BlockSTMv2 Aggregator V1 Write-Without-Read Validation Bypass Causes Deterministic Node Crash

## Summary
In BlockSTMv2, the `validate_aggregator_v1_reads()` function fails to validate aggregator delta operations when transactions write to aggregators without first reading them. This allows transactions to commit deltas with inconsistent delta histories that will deterministically fail materialization, causing all validator nodes to panic and halt block execution.

## Finding Description

The vulnerability exists in the aggregator v1 validation logic for BlockSTMv2. When a transaction writes to an aggregator without reading it first, the aggregator is initialized with a speculative base value of 0 and an empty `DeltaHistory`. [1](#0-0) 

The validation function only checks keys that were READ (present in `aggregator_v1_reads`). For write-only aggregators, the check evaluates to `false` because the key is not in `data_reads`, allowing the transaction to pass validation without verifying that the delta history is consistent with the actual storage base value. The code explicitly states "Not assuming read-before-write here: if there was a read, it must also be captured as an aggregator_v1 read." [2](#0-1) 

During materialization, when the delta is applied to the actual base value from storage, the history validation can fail. The materialization code expects this to always succeed and uses `.expect()` which causes a panic. [3](#0-2) 

The history validation checks whether the deltas that succeeded during speculative execution would also succeed against the actual base value from storage. If the base value plus the max achieved positive delta exceeds the max_value, the validation fails. [4](#0-3) [5](#0-4) 

**Attack Path:**
1. Attacker observes aggregator A in storage with value=150, max_value=200
2. Attacker submits transaction T1 that calls `aggregator::add(&mut agg_a, 100)` without reading A first
3. During execution, `get_aggregator()` creates aggregator with value=0, state=PositiveDelta, history=empty
4. T1 applies +100 successfully (0+100 < 200), producing DeltaOp with delta=+100, history={max_achieved_positive_delta: 100}
5. At validation, `validate_aggregator_v1_reads()` skips validation of A (write-only, not in `aggregator_v1_reads`)
6. T1 commits successfully
7. During materialization, delta materialization reads base value 150 from storage
8. History validation checks: 150+100 <= 200 â†’ FALSE (250 > 200)
9. Returns `DeltaApplicationFailure` error
10. The `.expect("Materializing delta w. base value set must succeed")` panics the node

This breaks the **Deterministic Execution** invariant because all validators will execute the same steps and crash at the same point, causing total network liveness failure.

## Impact Explanation

**Severity: CRITICAL**

This vulnerability causes:
1. **Total loss of liveness/network availability**: All validators crash deterministically when processing the malicious transaction
2. **Non-recoverable without intervention**: Block execution halts permanently until the transaction is excluded
3. **Consensus violation**: Network cannot make progress, violating safety/liveness guarantees

The attack requires no special privileges beyond the ability to deploy Move modules and can be triggered by any transaction sender. The node panic occurs during post-commit processing, meaning the transaction is already committed to the block, making recovery complex and potentially requiring emergency intervention or rollback. This aligns with the Aptos bug bounty Critical severity category: "Network halts due to protocol bug. All validators unable to progress."

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability is highly likely to be exploited because:
1. **Easy to trigger**: Attacker only needs to deploy a Move module that creates aggregators and exposes public entry functions that write to them without reading
2. **No special access required**: Any user can deploy Move modules and submit transactions
3. **Predictable outcome**: Attacker can query blockchain state off-chain, calculate exact conditions where delta history will be inconsistent, and craft the attack transaction
4. **Deterministic impact**: All validators crash identically, guaranteeing network halt

The aggregator framework is designed to allow write-only operations (add/sub without read) to enable parallel execution, as documented in the framework. [6](#0-5) 

## Recommendation

The validation logic should be modified to validate write-only aggregators against their actual base values from storage before committing. Specifically:

1. In `validate_aggregator_v1_reads()`, for all keys in `aggregator_write_keys`, read the base value from storage and validate the delta history against it, even if the key was not read during transaction execution.

2. Alternatively, require that aggregators must be read before being written to in BlockSTMv2, or perform history validation at execution time rather than materialization time.

3. Consider removing the `.expect()` at line 1112 of `executor.rs` and handling the error gracefully to prevent panic-based DoS, though this would only mitigate the crash, not fix the underlying validation bypass.

## Proof of Concept

A Move module can be deployed that creates aggregators and exposes public entry functions similar to the test module: [7](#0-6) 

The attacker would:
1. Deploy such a module and create an aggregator with `max_value=200`
2. Call the `add` function multiple times to set the aggregator value to 150
3. Query the blockchain state to confirm the value is 150
4. Submit a transaction calling `add` with value 100 (without reading first)
5. All validators will crash during materialization when 150+100 > 200

## Notes

The vulnerability is in the core BlockSTMv2 execution logic, not in test code. While the test module demonstrates the pattern, any user-deployed Move module with similar aggregator usage would trigger the same vulnerability. The aggregator framework explicitly encourages write-only operations for parallelism, but the validation logic does not properly handle this case in BlockSTMv2.

### Citations

**File:** aptos-move/aptos-aggregator/src/aggregator_v1_extension.rs (L298-310)
```rust
    pub fn get_aggregator(
        &mut self,
        id: AggregatorID,
        max_value: u128,
    ) -> PartialVMResult<&mut Aggregator> {
        let aggregator = self.aggregators.entry(id).or_insert(Aggregator {
            value: 0,
            state: AggregatorState::PositiveDelta,
            max_value,
            history: Some(DeltaHistory::new()),
        });
        Ok(aggregator)
    }
```

**File:** aptos-move/block-executor/src/captured_reads.rs (L993-1010)
```rust
        if ret {
            // Additional invariant check (that AggregatorV1 reads are captured for
            // aggregator write keys). This protects against the case where aggregator v1
            // state value read was read by a wrong interface (e.g. via resource API).
            for key in aggregator_write_keys {
                if self.data_reads.contains_key(&key) && !self.aggregator_v1_reads.contains(&key) {
                    // Not assuming read-before-write here: if there was a read, it must also be
                    // captured as an aggregator_v1 read.
                    return Err(code_invariant_error(format!(
                        "Captured read at aggregator key {:?} not found among AggregatorV1 reads",
                        key
                    )));
                }
            }
        }

        Ok(ret)
    }
```

**File:** aptos-move/block-executor/src/executor.rs (L1091-1113)
```rust
                let committed_delta = versioned_cache
                    .data()
                    .materialize_delta(&k, txn_idx)
                    .unwrap_or_else(|op| {
                        // TODO[agg_v1](cleanup): this logic should improve with the new AGGR data structure
                        // TODO[agg_v1](cleanup): and the ugly base_view parameter will also disappear.
                        let storage_value = base_view
                            .get_state_value(&k)
                            .expect("Error reading the base value for committed delta in storage");

                        let w: T::Value = TransactionWrite::from_state_value(storage_value);
                        let value_u128 = w
                            .as_u128()
                            .expect("Aggregator base value deserialization error")
                            .expect("Aggregator base value must exist");

                        versioned_cache.data().set_base_value(
                            k.clone(),
                            ValueWithLayout::RawFromStorage(TriompheArc::new(w)),
                        );
                        op.apply_to(value_u128)
                            .expect("Materializing delta w. base value set must succeed")
                    });
```

**File:** aptos-move/aptos-aggregator/src/delta_math.rs (L148-197)
```rust
    pub fn validate_against_base_value(
        &self,
        base_value: u128,
        max_value: u128,
    ) -> Result<(), DelayedFieldsSpeculativeError> {
        let math = BoundedMath::new(max_value);
        // We need to make sure the following 4 conditions are satisified.
        //     base_value + max_achieved_positive_delta <= self.max_value
        //     base_value >= min_achieved_negative_delta
        //     base_value + min_overflow_positive_delta > self.max_value
        //     base_value < max_underflow_negative_delta
        math.unsigned_add(base_value, self.max_achieved_positive_delta)
            .map_err(|_e| DelayedFieldsSpeculativeError::DeltaApplication {
                base_value,
                max_value,
                delta: SignedU128::Positive(self.max_achieved_positive_delta),
                reason: DeltaApplicationFailureReason::Overflow,
            })?;
        math.unsigned_subtract(base_value, self.min_achieved_negative_delta)
            .map_err(|_e| DelayedFieldsSpeculativeError::DeltaApplication {
                base_value,
                max_value,
                delta: SignedU128::Negative(self.min_achieved_negative_delta),
                reason: DeltaApplicationFailureReason::Underflow,
            })?;

        if let Some(min_overflow_positive_delta) = self.min_overflow_positive_delta {
            if base_value <= max_value - min_overflow_positive_delta {
                return Err(DelayedFieldsSpeculativeError::DeltaApplication {
                    base_value,
                    max_value,
                    delta: SignedU128::Positive(min_overflow_positive_delta),
                    reason: DeltaApplicationFailureReason::ExpectedOverflow,
                });
            }
        }

        if let Some(max_underflow_negative_delta) = self.max_underflow_negative_delta {
            if base_value >= max_underflow_negative_delta {
                return Err(DelayedFieldsSpeculativeError::DeltaApplication {
                    base_value,
                    max_value,
                    delta: SignedU128::Negative(max_underflow_negative_delta),
                    reason: DeltaApplicationFailureReason::ExpectedUnderflow,
                });
            }
        }

        Ok(())
    }
```

**File:** aptos-move/aptos-aggregator/src/delta_change_set.rs (L114-116)
```rust
    pub fn apply_to(&self, base: u128) -> Result<u128, PanicOr<DelayedFieldsSpeculativeError>> {
        merge_data_and_delta(base, &self.update, &self.history, self.max_value)
    }
```

**File:** aptos-move/framework/aptos-framework/sources/aggregator/aggregator.move (L1-48)
```text
/// This module provides an interface for aggregators. Aggregators are similar to
/// unsigned integers and support addition and subtraction (aborting on underflow
/// or on overflowing a custom upper limit). The difference from integers is that
/// aggregators allow to perform both additions and subtractions in parallel across
/// multiple transactions, enabling parallel execution. For example, if the first
/// transaction is doing `add(X, 1)` for aggregator resource `X`, and the second
/// is doing `sub(X,3)`, they can be executed in parallel avoiding a read-modify-write
/// dependency.
/// However, reading the aggregator value (i.e. calling `read(X)`) is an expensive
/// operation and should be avoided as much as possible because it reduces the
/// parallelism. Moreover, **aggregators can only be created by Aptos Framework (0x1)
/// at the moment.**
module aptos_framework::aggregator {

    /// The value of aggregator overflows. Raised by native code.
    const EAGGREGATOR_OVERFLOW: u64 = 1;

    /// The value of aggregator underflows (goes below zero). Raised by native code.
    const EAGGREGATOR_UNDERFLOW: u64 = 2;

    /// Aggregator feature is not supported. Raised by native code.
    const ENOT_SUPPORTED: u64 = 3;

    /// Represents an integer which supports parallel additions and subtractions
    /// across multiple transactions. See the module description for more details.
    struct Aggregator has store {
        handle: address,
        key: address,
        limit: u128,
    }

    /// Returns `limit` exceeding which aggregator overflows.
    public fun limit(aggregator: &Aggregator): u128 {
        aggregator.limit
    }

    /// Adds `value` to aggregator. Aborts on overflowing the limit.
    public native fun add(aggregator: &mut Aggregator, value: u128);

    /// Subtracts `value` from aggregator. Aborts on going below zero.
    public native fun sub(aggregator: &mut Aggregator, value: u128);

    /// Returns a value stored in this aggregator.
    public native fun read(aggregator: &Aggregator): u128;

    /// Destroys an aggregator and removes it from its `AggregatorFactory`.
    public native fun destroy(aggregator: Aggregator);
}
```

**File:** aptos-move/e2e-move-tests/src/tests/aggregator.data/pack/sources/aggregator_test.move (L45-50)
```text
    public entry fun add(account: &signer, i: u64, value: u128) acquires AggregatorStore {
        let addr = signer::address_of(account);
        let aggregators = &mut borrow_global_mut<AggregatorStore>(addr).aggregators;
        let aggregator = table::borrow_mut(aggregators, i);
        aggregator::add(aggregator, value);
    }
```
