# Audit Report

## Title
Byzantine Validators Can Withhold Batch Data Despite Valid ProofOfStore, Causing Block Execution Failures

## Summary
The `check_payload_availability()` function for `InQuorumStore` payloads accepts blocks with `ProofOfStore` without verifying that actual batch transaction data is locally available. This allows Byzantine validators to create valid blocks containing `ProofOfStore` for batches whose data they withhold, causing honest validators to accept and vote for these blocks but fail to execute them, resulting in consensus liveness degradation.

## Finding Description

### The Vulnerability

When a block proposal with an `InQuorumStore` payload is received, the payload availability check immediately returns success without verifying local data availability: [1](#0-0) 

In contrast, `OptQuorumStore` payloads correctly check whether batches exist locally before accepting: [2](#0-1) 

The code contains a comment revealing the flawed assumption: [3](#0-2) 

### Why This Assumption Is Incorrect

A `ProofOfStore` only contains metadata and signatures, not actual transaction data: [4](#0-3) 

The `BatchInfo` structure contains only metadata fields: [5](#0-4) 

The `ProofOfStore` proves that >2/3 validators **signed** the batch metadata, but does NOT guarantee they currently **have** the batch data. Data can become unavailable due to:
- Batch expiration and garbage collection
- Validators going offline
- Byzantine validators deliberately withholding data

### The Attack Scenario

1. A Byzantine proposer creates a batch and distributes it to validators
2. Honest validators receive, store, and sign the batch
3. The proposer collects >2/3 signatures and creates a valid `ProofOfStore`
4. The proposer ensures batch data becomes unavailable (through expiration, offline signers, or Byzantine signers withholding)
5. The proposer includes the `ProofOfStore` in a block proposal
6. Honest validators receive the proposal:
   - `check_payload_availability()` returns `Ok()` without checking local availability
   - Validators vote for the block **before execution completes**: [6](#0-5) 

7. The block gets >2/3 votes and is committed
8. During pipeline execution, the `materialize` phase attempts to fetch batch data: [7](#0-6) 

9. The batch requester times out after retry attempts: [8](#0-7) 

10. The error propagates back, but `materialize()` has an **infinite retry loop**: [9](#0-8) 

11. The block cannot be executed, and since blocks must execute in order (parent-child dependency), all descendant blocks are also blocked, causing **consensus liveness degradation**.

## Impact Explanation

This vulnerability has **Critical** severity according to Aptos bug bounty criteria for "Total loss of liveness/network availability":

**Liveness Degradation Mechanism:**
- Validators accept and commit blocks with unavailable batch data
- The execution pipeline gets stuck in infinite retry attempting to materialize transactions
- Blocks must execute sequentially (ordered by parent-child relationships)
- If a committed block cannot execute, all subsequent blocks are blocked
- Network cannot make execution progress despite consensus continuing to propose blocks

**Violates Core Consensus Invariant:**
The protocol allows blocks into the committed chain that cannot be processed, breaking the fundamental guarantee that accepted blocks must be executable. This violates the deterministic execution requirement of Byzantine Fault Tolerant consensus.

**Attack Feasibility:**
- Requires attacker to control ≥1 validator (< 1/3 Byzantine threshold)
- Attacker must be selected as proposer (regular occurrence in round-robin)
- With multiple Byzantine validators, sustained attacks possible during proposer turns

## Likelihood Explanation

**Likelihood: Medium to High**

**Preconditions (All Realistic):**
1. Attacker controls at least one validator node (< 1/3 Byzantine assumption in BFT systems) ✅
2. Attacker's validator selected as proposer for a round (regular rotation) ✅
3. Can create batches and gather signatures through normal quorum store protocol ✅
4. Can engineer data unavailability through expiration, offline signers, or Byzantine accomplices ✅

**Execution Complexity: Low**
- No cryptographic breaks required
- No complex timing attacks needed
- Simply withhold batch data after obtaining valid signatures
- Deterministic once preconditions met

**Mitigating Factors:**
- Batch requester retries with multiple peers provide resilience if enough honest signers remain online
- Batch expiration checks skip some expired batches
- However, fundamental flaw remains: system accepts blocks it cannot execute

## Recommendation

Implement local batch availability checking for `InQuorumStore` payloads, consistent with `OptQuorumStore`:

```rust
Payload::InQuorumStore(proof_with_data) => {
    let mut missing_authors = BitVec::with_num_bits(self.ordered_authors.len() as u16);
    for proof in &proof_with_data.proofs {
        if self.batch_reader.exists(proof.info().digest()).is_none() {
            let index = *self
                .address_to_validator_index
                .get(&proof.info().author())
                .expect("Payload author should have been verified");
            missing_authors.set(index as u16);
        }
    }
    if missing_authors.all_zeros() {
        Ok(())
    } else {
        Err(missing_authors)
    }
}
```

This ensures validators only accept blocks whose batch data is locally available or can be waited for via `wait_for_payload()`.

## Proof of Concept

While a full end-to-end PoC would require setting up a multi-validator testnet, the vulnerability can be demonstrated through the following logic flow:

1. Deploy a Byzantine validator that creates batches and collects signatures
2. After obtaining `ProofOfStore` with >2/3 signatures, have the Byzantine validator and accomplices delete batch data or go offline
3. Propose a block containing the `ProofOfStore`
4. Observe that honest validators' `check_payload_availability()` returns `Ok()` at line 358
5. Observe that validators vote for the block (voting happens before execution)
6. Observe that during execution, `batch_requester.request_batch()` times out at line 178
7. Observe that `materialize()` enters infinite retry loop at lines 634-646
8. Observe that execution is blocked and subsequent blocks cannot be processed

The code citations provided throughout this report serve as evidence of each step in this attack path.

## Notes

The vulnerability exists because the system conflates two separate properties:
1. **Cryptographic proof of batch validity** (provided by `ProofOfStore` signatures)
2. **Actual data availability** (NOT guaranteed by `ProofOfStore`)

The comment at lines 405-406 explicitly reveals this false assumption: "proofs that guarantee network availability" - but `ProofOfStore` only proves past signatures, not current data availability. The correct behavior is demonstrated by `OptQuorumStore` which checks local availability before accepting blocks.

### Citations

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L89-109)
```rust
    fn request_transactions(
        batches: Vec<(BatchInfo, Vec<PeerId>)>,
        block_timestamp: u64,
        batch_reader: Arc<dyn BatchReader>,
    ) -> Vec<Shared<Pin<Box<dyn Future<Output = ExecutorResult<Vec<SignedTransaction>>> + Send>>>>
    {
        let mut futures = Vec::new();
        for (batch_info, responders) in batches {
            trace!(
                "QSE: requesting batch {:?}, time = {}",
                batch_info,
                block_timestamp
            );
            if block_timestamp <= batch_info.expiration() {
                futures.push(batch_reader.get_batch(batch_info, responders.clone()));
            } else {
                debug!("QSE: skipped expired batch {}", batch_info.digest());
            }
        }
        futures
    }
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L358-358)
```rust
            Payload::InQuorumStore(_) => Ok(()),
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L405-407)
```rust
                // The payload is considered available because it contains only proofs that guarantee network availabiliy
                // or inlined transactions.
                Ok(())
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L409-441)
```rust
            Payload::OptQuorumStore(OptQuorumStorePayload::V1(p)) => {
                let mut missing_authors = BitVec::with_num_bits(self.ordered_authors.len() as u16);
                for batch in p.opt_batches().deref() {
                    if self.batch_reader.exists(batch.digest()).is_none() {
                        let index = *self
                            .address_to_validator_index
                            .get(&batch.author())
                            .expect("Payload author should have been verified");
                        missing_authors.set(index as u16);
                    }
                }
                if missing_authors.all_zeros() {
                    Ok(())
                } else {
                    Err(missing_authors)
                }
            },
            Payload::OptQuorumStore(OptQuorumStorePayload::V2(p)) => {
                let mut missing_authors = BitVec::with_num_bits(self.ordered_authors.len() as u16);
                for batch in p.opt_batches().deref() {
                    if self.batch_reader.exists(batch.digest()).is_none() {
                        let index = *self
                            .address_to_validator_index
                            .get(&batch.author())
                            .expect("Payload author should have been verified");
                        missing_authors.set(index as u16);
                    }
                }
                if missing_authors.all_zeros() {
                    Ok(())
                } else {
                    Err(missing_authors)
                }
```

**File:** consensus/consensus-types/src/proof_of_store.rs (L49-58)
```rust
pub struct BatchInfo {
    author: PeerId,
    batch_id: BatchId,
    epoch: u64,
    expiration: u64,
    digest: HashValue,
    num_txns: u64,
    num_bytes: u64,
    gas_bucket_start: u64,
}
```

**File:** consensus/consensus-types/src/proof_of_store.rs (L619-622)
```rust
pub struct ProofOfStore<T> {
    info: T,
    multi_signature: AggregateSignature,
}
```

**File:** consensus/src/round_manager.rs (L1262-1285)
```rust
        if block_store.check_payload(&proposal).is_err() {
            debug!("Payload not available locally for block: {}", proposal.id());
            counters::CONSENSUS_PROPOSAL_PAYLOAD_AVAILABILITY
                .with_label_values(&["missing"])
                .inc();
            let start_time = Instant::now();
            let deadline = self.round_state.current_round_deadline();
            let future = async move {
                (
                    block_store.wait_for_payload(&proposal, deadline).await,
                    proposal,
                    start_time,
                )
            }
            .boxed();
            self.futures.push(future);
            return Ok(());
        }

        counters::CONSENSUS_PROPOSAL_PAYLOAD_AVAILABILITY
            .with_label_values(&["available"])
            .inc();

        self.check_backpressure_and_process_proposal(proposal).await
```

**File:** consensus/src/quorum_store/batch_requester.rs (L175-179)
```rust
            }
            counters::RECEIVED_BATCH_REQUEST_TIMEOUT_COUNT.inc();
            debug!("QS: batch request timed out, digest:{}", digest);
            Err(ExecutorError::CouldNotGetData)
        })
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L634-648)
```rust
        let result = loop {
            match preparer.materialize_block(&block, qc_rx.clone()).await {
                Ok(input_txns) => break input_txns,
                Err(e) => {
                    warn!(
                        "[BlockPreparer] failed to prepare block {}, retrying: {}",
                        block.id(),
                        e
                    );
                    tokio::time::sleep(Duration::from_millis(100)).await;
                },
            }
        };
        Ok(result)
    }
```
