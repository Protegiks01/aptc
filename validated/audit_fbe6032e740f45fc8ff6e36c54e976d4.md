# Audit Report

## Title
State Snapshot Restore Missing Final Root Hash Verification Allows Incomplete State Restoration

## Summary
The state snapshot restoration process verifies each chunk's `SparseMerkleRangeProof` but lacks final verification after `finish_impl()` to ensure the restored tree's root hash matches the expected root hash. An attacker controlling backup storage can modify the manifest to remove chunks, resulting in incomplete but seemingly-validated state restoration.

## Finding Description

The vulnerability exists in the Jellyfish Merkle Tree restoration system's interaction between chunk verification and finalization.

**Manifest Loading - Unauthenticated Chunks Array:**
The `StateSnapshotBackup` manifest is loaded as unauthenticated JSON from backup storage. [1](#0-0)  The `load_json_file` implementation simply deserializes JSON without signature verification. [2](#0-1) 

While the `root_hash` field is authenticated via `TransactionInfoWithProof` and `LedgerInfoWithSignatures`, [3](#0-2)  the `chunks` array itself in the `StateSnapshotBackup` structure is just a Vec field with no separate cryptographic authentication. [4](#0-3) 

**Chunk-by-Chunk Verification:**
Each chunk is verified in `add_chunk_impl()` which calls `self.verify(proof)?` to validate that accumulated state plus the proof's `right_siblings` can reconstruct the expected root hash. [5](#0-4)  The verification method computes left siblings from accumulated state and validates the complete Merkle path. [6](#0-5) 

**The Critical Flaw - Missing Final Verification:**
After all chunks are processed, `finish_impl()` freezes remaining nodes and writes them to storage WITHOUT verifying the final root hash matches `self.expected_root_hash`. [7](#0-6)  The method simply calls `self.freeze(0)` and writes frozen nodes without any comparison against the expected root hash.

**Post-Finalization Flow:**
The storage synchronizer calls `finish_box()` [8](#0-7)  and then `finalize_state_snapshot()` [9](#0-8)  but neither performs root hash verification. The `finalize_state_snapshot` implementation saves transactions and ledger infos but does not verify the state tree's root hash. [10](#0-9) 

**Root Hash Check Limitation:**
The only root hash verification occurs in the `JellyfishMerkleRestore::new()` constructor when checking if a previous restore already completed, used for resume detection, not for validating the current restoration. [11](#0-10) 

**Attack Vector:**
1. Attacker compromises backup storage (cloud storage breach, compromised backup provider, MITM)
2. Attacker modifies the manifest's `chunks` array to remove chunks (e.g., keep chunks 0-2, remove 3-9)
3. Each remaining chunk has valid `SparseMerkleRangeProof` where `right_siblings` cryptographically commit to keys in removed chunks
4. During restoration, each chunk verifies successfully: `accumulated_keys + proof.right_siblings = expected_root_hash`
5. The proof's `right_siblings` promise keys to be added later
6. Removed chunks never arrive
7. `finish_impl()` completes without verifying all promised keys were delivered
8. Final tree is incomplete with incorrect root hash

## Impact Explanation

**Impact Classification: MEDIUM Severity**

This qualifies as a "Limited Protocol Violation" under the Medium severity category:

1. **State Consistency Violation**: Restored state does not match the authenticated root hash, breaking cryptographic verifiability guarantees.

2. **Operational Impact**: Node restoring from compromised backup has incomplete state but restore reports success. Node operates with incorrect state until inconsistency detected through:
   - State sync failures when querying missing keys
   - Transaction execution errors accessing missing state
   - Merkle proof verification failures

3. **Manual Intervention Required**: Detection and recovery require manual intervention - operators must identify corrupted state, remove it, and re-restore from trusted source.

4. **Limited Scope**: Does not directly:
   - Enable fund theft or minting
   - Cause consensus violations (consensus works correctly)
   - Affect properly synced nodes (only affects nodes restoring from compromised backups)
   - Create permanent network issues (fixable by re-restoration)

## Likelihood Explanation

**Likelihood: Medium**

Attack requires:
1. **Attacker control over backup storage**: Compromised backup system, malicious backup provider, cloud storage breach, or MITM on backup downloads
2. **Victim performing state snapshot restoration**: Common for new validators joining network or nodes recovering from failures
3. **No validator stake or network position required**: Attack operates at backup/restore protocol level

Attack is straightforward once backup storage compromised:
1. Download legitimate backup manifest
2. Modify JSON manifest to remove chunk entries (chunks array is unauthenticated)
3. Victim downloads and restores from modified backup
4. Restoration completes successfully despite incomplete state

Common backup storage compromise scenarios:
- Third-party backup service provider breach
- Cloud storage credential compromise
- Supply chain attacks on backup infrastructure
- Unencrypted backup transfer interception

Likelihood tempered by:
- Many organizations implement additional integrity checks on backups
- Backup storage compromise is significant security event
- Nodes can detect issue through subsequent state sync operations

## Recommendation

Add final root hash verification in `finish_impl()` before writing to storage:

```rust
pub fn finish_impl(mut self) -> Result<()> {
    self.wait_for_async_commit()?;
    
    // ... existing special case handling ...
    
    self.freeze(0);
    
    // ADD: Verify final root hash matches expected
    let root_node_key = NodeKey::new_empty_path(self.version);
    if let Some(root_node) = self.frozen_nodes.get(&root_node_key) {
        let actual_root_hash = root_node.hash();
        ensure!(
            actual_root_hash == self.expected_root_hash,
            "Final root hash mismatch. Expected: {}, got: {}",
            self.expected_root_hash,
            actual_root_hash
        );
    } else {
        return Err(anyhow!("Root node not found after finalization"));
    }
    
    self.store.write_node_batch(&self.frozen_nodes)?;
    Ok(())
}
```

Additionally, consider authenticating the chunks array structure in the manifest using cryptographic signatures or including chunk metadata in the authenticated proof.

## Proof of Concept

The vulnerability can be demonstrated by:
1. Creating a legitimate state snapshot backup with 10 chunks
2. Modifying the manifest JSON to remove chunks 5-9
3. Running restoration with modified manifest
4. Observing that restoration completes successfully
5. Verifying that `get_root_hash(version)` returns different hash than expected
6. Confirming queries for keys in removed chunks fail

Test case structure (requires integration test setup):
```rust
// Create full backup
let full_backup = create_state_snapshot(db, version, 10_chunks);

// Modify manifest to remove chunks
let mut manifest = load_manifest(&full_backup.manifest_handle);
manifest.chunks.truncate(5); // Keep only first 5 chunks
save_modified_manifest(&manifest);

// Attempt restore
let restore = StateSnapshotRestore::new(..., expected_root_hash, ...)?;
for chunk in manifest.chunks {
    restore.add_chunk(load_chunk(chunk), load_proof(chunk))?;
}
restore.finish()?; // Should fail with root hash mismatch but currently succeeds

// Verify incorrect state
let actual_hash = db.get_root_hash(version)?;
assert_ne!(actual_hash, expected_root_hash); // Demonstrates vulnerability
```

### Citations

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L123-124)
```rust
        let manifest: StateSnapshotBackup =
            self.storage.load_json_file(&self.manifest_handle).await?;
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L125-136)
```rust
        let (txn_info_with_proof, li): (TransactionInfoWithProof, LedgerInfoWithSignatures) =
            self.storage.load_bcs_file(&manifest.proof).await?;
        txn_info_with_proof.verify(li.ledger_info(), manifest.version)?;
        let state_root_hash = txn_info_with_proof
            .transaction_info()
            .ensure_state_checkpoint_hash()?;
        ensure!(
            state_root_hash == manifest.root_hash,
            "Root hash mismatch with that in proof. root hash: {}, expected: {}",
            manifest.root_hash,
            state_root_hash,
        );
```

**File:** storage/backup/backup-cli/src/utils/storage_ext.rs (L35-36)
```rust
    async fn load_json_file<T: DeserializeOwned>(&self, file_handle: &FileHandleRef) -> Result<T> {
        Ok(serde_json::from_slice(&self.read_all(file_handle).await?)?)
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/manifest.rs (L29-50)
```rust
/// State snapshot backup manifest, representing a complete state view at specified version.
#[derive(Deserialize, Serialize)]
pub struct StateSnapshotBackup {
    /// Version at which this state snapshot is taken.
    pub version: Version,
    /// Epoch in which this state snapshot is taken.
    pub epoch: u64,
    /// Hash of the state tree root.
    pub root_hash: HashValue,
    /// All account blobs in chunks.
    pub chunks: Vec<StateSnapshotChunk>,
    /// BCS serialized
    /// `Tuple(TransactionInfoWithProof, LedgerInfoWithSignatures)`.
    ///   - The `TransactionInfoWithProof` is at `Version` above, and carries the same `root_hash`
    /// above; It proves that at specified version the root hash is as specified in a chain
    /// represented by the LedgerInfo below.
    ///   - The signatures on the `LedgerInfoWithSignatures` has a version greater than or equal to
    /// the version of this backup but is within the same epoch, so the signatures on it can be
    /// verified by the validator set in the same epoch, which can be provided by an
    /// `EpochStateBackup` recovered prior to this to the DB; Requiring it to be in the same epoch
    /// limits the requirement on such `EpochStateBackup` to no older than the same epoch.
    pub proof: FileHandle,
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L196-206)
```rust
        let (finished, partial_nodes, previous_leaf) = if let Some(root_node) =
            tree_reader.get_node_option(&NodeKey::new_empty_path(version), "restore")?
        {
            info!("Previous restore is complete, checking root hash.");
            ensure!(
                root_node.hash() == expected_root_hash,
                "Previous completed restore has root hash {}, expecting {}",
                root_node.hash(),
                expected_root_hash,
            );
            (true, vec![], None)
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L339-413)
```rust
    pub fn add_chunk_impl(
        &mut self,
        mut chunk: Vec<(&K, HashValue)>,
        proof: SparseMerkleRangeProof,
    ) -> Result<()> {
        if self.finished {
            info!("State snapshot restore already finished, ignoring entire chunk.");
            return Ok(());
        }

        if let Some(prev_leaf) = &self.previous_leaf {
            let skip_until = chunk
                .iter()
                .find_position(|(key, _hash)| key.hash() > *prev_leaf.account_key());
            chunk = match skip_until {
                None => {
                    info!("Skipping entire chunk.");
                    return Ok(());
                },
                Some((0, _)) => chunk,
                Some((num_to_skip, next_leaf)) => {
                    info!(
                        num_to_skip = num_to_skip,
                        next_leaf = next_leaf,
                        "Skipping leaves."
                    );
                    chunk.split_off(num_to_skip)
                },
            }
        };
        if chunk.is_empty() {
            return Ok(());
        }

        for (key, value_hash) in chunk {
            let hashed_key = key.hash();
            if let Some(ref prev_leaf) = self.previous_leaf {
                ensure!(
                    &hashed_key > prev_leaf.account_key(),
                    "State keys must come in increasing order.",
                )
            }
            self.previous_leaf.replace(LeafNode::new(
                hashed_key,
                value_hash,
                (key.clone(), self.version),
            ));
            self.add_one(key, value_hash);
            self.num_keys_received += 1;
        }

        // Verify what we have added so far is all correct.
        self.verify(proof)?;

        // Write the frozen nodes to storage.
        if self.async_commit {
            self.wait_for_async_commit()?;
            let (tx, rx) = channel();
            self.async_commit_result = Some(rx);

            let mut frozen_nodes = HashMap::new();
            std::mem::swap(&mut frozen_nodes, &mut self.frozen_nodes);
            let store = self.store.clone();

            IO_POOL.spawn(move || {
                let res = store.write_node_batch(&frozen_nodes);
                tx.send(res).unwrap();
            });
        } else {
            self.store.write_node_batch(&self.frozen_nodes)?;
            self.frozen_nodes.clear();
        }

        Ok(())
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L624-697)
```rust
    /// Verifies that all states that have been added so far (from the leftmost one to
    /// `self.previous_leaf`) are correct, i.e., we are able to construct `self.expected_root_hash`
    /// by combining all existing states and `proof`.
    #[allow(clippy::collapsible_if)]
    fn verify(&self, proof: SparseMerkleRangeProof) -> Result<()> {
        let previous_leaf = self
            .previous_leaf
            .as_ref()
            .expect("The previous leaf must exist.");

        let previous_key = previous_leaf.account_key();
        // If we have all siblings on the path from root to `previous_key`, we should be able to
        // compute the root hash. The siblings on the right are already in the proof. Now we
        // compute the siblings on the left side, which represent all the states that have ever
        // been added.
        let mut left_siblings = vec![];

        // The following process might add some extra placeholder siblings on the left, but it is
        // nontrivial to determine when the loop should stop. So instead we just add these
        // siblings for now and get rid of them in the next step.
        let mut num_visited_right_siblings = 0;
        for (i, bit) in previous_key.iter_bits().enumerate() {
            if bit {
                // This node is a right child and there should be a sibling on the left.
                let sibling = if i >= self.partial_nodes.len() * 4 {
                    *SPARSE_MERKLE_PLACEHOLDER_HASH
                } else {
                    Self::compute_left_sibling(
                        &self.partial_nodes[i / 4],
                        previous_key.get_nibble(i / 4),
                        (3 - i % 4) as u8,
                    )
                };
                left_siblings.push(sibling);
            } else {
                // This node is a left child and there should be a sibling on the right.
                num_visited_right_siblings += 1;
            }
        }
        ensure!(
            num_visited_right_siblings >= proof.right_siblings().len(),
            "Too many right siblings in the proof.",
        );

        // Now we remove any extra placeholder siblings at the bottom. We keep removing the last
        // sibling if 1) it's a placeholder 2) it's a sibling on the left.
        for bit in previous_key.iter_bits().rev() {
            if bit {
                if *left_siblings.last().expect("This sibling must exist.")
                    == *SPARSE_MERKLE_PLACEHOLDER_HASH
                {
                    left_siblings.pop();
                } else {
                    break;
                }
            } else if num_visited_right_siblings > proof.right_siblings().len() {
                num_visited_right_siblings -= 1;
            } else {
                break;
            }
        }

        // Left siblings must use the same ordering as the right siblings in the proof
        left_siblings.reverse();

        // Verify the proof now that we have all the siblings
        proof
            .verify(
                self.expected_root_hash,
                SparseMerkleLeafNode::new(*previous_key, previous_leaf.value_hash()),
                left_siblings,
            )
            .map_err(Into::into)
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L750-789)
```rust
    pub fn finish_impl(mut self) -> Result<()> {
        self.wait_for_async_commit()?;
        // Deal with the special case when the entire tree has a single leaf or null node.
        if self.partial_nodes.len() == 1 {
            let mut num_children = 0;
            let mut leaf = None;
            for i in 0..16 {
                if let Some(ref child_info) = self.partial_nodes[0].children[i] {
                    num_children += 1;
                    if let ChildInfo::Leaf(node) = child_info {
                        leaf = Some(node.clone());
                    }
                }
            }

            match num_children {
                0 => {
                    let node_key = NodeKey::new_empty_path(self.version);
                    assert!(self.frozen_nodes.is_empty());
                    self.frozen_nodes.insert(node_key, Node::Null);
                    self.store.write_node_batch(&self.frozen_nodes)?;
                    return Ok(());
                },
                1 => {
                    if let Some(node) = leaf {
                        let node_key = NodeKey::new_empty_path(self.version);
                        assert!(self.frozen_nodes.is_empty());
                        self.frozen_nodes.insert(node_key, node.into());
                        self.store.write_node_batch(&self.frozen_nodes)?;
                        return Ok(());
                    }
                },
                _ => (),
            }
        }

        self.freeze(0);
        self.store.write_node_batch(&self.frozen_nodes)?;
        Ok(())
    }
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L1123-1128)
```rust
    state_snapshot_receiver.finish_box().map_err(|error| {
        format!(
            "Failed to finish the state value synchronization! Error: {:?}",
            error
        )
    })?;
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L1129-1136)
```rust
    storage
        .writer
        .finalize_state_snapshot(
            version,
            target_output_with_proof.clone(),
            epoch_change_proofs,
        )
        .map_err(|error| format!("Failed to finalize the state snapshot! Error: {:?}", error))?;
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L125-241)
```rust
    fn finalize_state_snapshot(
        &self,
        version: Version,
        output_with_proof: TransactionOutputListWithProofV2,
        ledger_infos: &[LedgerInfoWithSignatures],
    ) -> Result<()> {
        let (output_with_proof, persisted_aux_info) = output_with_proof.into_parts();
        gauged_api("finalize_state_snapshot", || {
            // Ensure the output with proof only contains a single transaction output and info
            let num_transaction_outputs = output_with_proof.get_num_outputs();
            let num_transaction_infos = output_with_proof.proof.transaction_infos.len();
            ensure!(
                num_transaction_outputs == 1,
                "Number of transaction outputs should == 1, but got: {}",
                num_transaction_outputs
            );
            ensure!(
                num_transaction_infos == 1,
                "Number of transaction infos should == 1, but got: {}",
                num_transaction_infos
            );

            // TODO(joshlind): include confirm_or_save_frozen_subtrees in the change set
            // bundle below.

            // Update the merkle accumulator using the given proof
            let frozen_subtrees = output_with_proof
                .proof
                .ledger_info_to_transaction_infos_proof
                .left_siblings();
            restore_utils::confirm_or_save_frozen_subtrees(
                self.ledger_db.transaction_accumulator_db_raw(),
                version,
                frozen_subtrees,
                None,
            )?;

            // Create a single change set for all further write operations
            let mut ledger_db_batch = LedgerDbSchemaBatches::new();
            let mut sharded_kv_batch = self.state_kv_db.new_sharded_native_batches();
            let mut state_kv_metadata_batch = SchemaBatch::new();
            // Save the target transactions, outputs, infos and events
            let (transactions, outputs): (Vec<Transaction>, Vec<TransactionOutput>) =
                output_with_proof
                    .transactions_and_outputs
                    .into_iter()
                    .unzip();
            let events = outputs
                .clone()
                .into_iter()
                .map(|output| output.events().to_vec())
                .collect::<Vec<_>>();
            let wsets: Vec<WriteSet> = outputs
                .into_iter()
                .map(|output| output.write_set().clone())
                .collect();
            let transaction_infos = output_with_proof.proof.transaction_infos;
            // We should not save the key value since the value is already recovered for this version
            restore_utils::save_transactions(
                self.state_store.clone(),
                self.ledger_db.clone(),
                version,
                &transactions,
                &persisted_aux_info,
                &transaction_infos,
                &events,
                wsets,
                Some((
                    &mut ledger_db_batch,
                    &mut sharded_kv_batch,
                    &mut state_kv_metadata_batch,
                )),
                false,
            )?;

            // Save the epoch ending ledger infos
            restore_utils::save_ledger_infos(
                self.ledger_db.metadata_db(),
                ledger_infos,
                Some(&mut ledger_db_batch.ledger_metadata_db_batches),
            )?;

            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::LedgerCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;
            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::OverallCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;

            // Apply the change set writes to the database (atomically) and update in-memory state
            //
            // state kv and SMT should use shared way of committing.
            self.ledger_db.write_schemas(ledger_db_batch)?;

            self.ledger_pruner.save_min_readable_version(version)?;
            self.state_store
                .state_merkle_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .epoch_snapshot_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .state_kv_pruner
                .save_min_readable_version(version)?;

            restore_utils::update_latest_ledger_info(self.ledger_db.metadata_db(), ledger_infos)?;
            self.state_store.reset();

            Ok(())
        })
    }
```
