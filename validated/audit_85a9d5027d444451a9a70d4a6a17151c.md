# Audit Report

## Title
Dynamic Prefetching Freeze Timer Manipulation Enables Persistent State Sync Performance Degradation

## Summary
A malicious network peer can exploit the global freeze timer in Aptos state sync's dynamic prefetching mechanism to force synchronization operations to run at minimum concurrency (3 requests instead of up to 30) indefinitely. By maintaining a 25:1 success-to-failure ratio, the attacker maintains a perfect reputation score (100.0) that prevents exclusion while causing timeouts frequently enough to keep the freeze timer perpetually active.

## Finding Description

The vulnerability exists in the interaction between the dynamic prefetching system and peer reputation scoring within the state sync data streaming service.

**Core Mechanism:**

The dynamic prefetching system adjusts concurrent request limits based on network performance. On successful responses, it increases the limit [1](#0-0) . On timeouts or failures, it decreases the limit and activates a freeze timer [2](#0-1) .

**Critical Flaw - Unconditional Timer Reset:**

Every failure unconditionally resets the freeze timer to the current time [3](#0-2) , regardless of when the previous failure occurred. The freeze duration is configured to 30 seconds by default [4](#0-3) . This allows an attacker to maintain a permanent freeze by causing failures more frequently than every 30 seconds.

**Freeze Prevents Scaling:**

During the freeze period, the check at line 111 prevents any increases to the prefetch limit [5](#0-4) . The freeze is determined by comparing elapsed time since the last timeout [6](#0-5) .

**Peer Reputation Bypass:**

The peer scoring system adds 1.0 per successful response [7](#0-6)  and multiplies by 0.95 per "NotUseful" error [8](#0-7) . The score is capped at 100.0 [9](#0-8) , and peers are only ignored when their score drops below 25.0 [10](#0-9) . The scoring logic is implemented in update_score_success and update_score_error methods [11](#0-10) .

**Mathematical Analysis:**

With a 25:1 success-to-failure ratio:
- After 25 successes: score increases by 25
- After 1 failure: score multiplies by 0.95
- At steady state S: S = (S + 25) × 0.95
- Solving: S = 475, but capped at MAX_SCORE = 100.0
- Result: Peer maintains maximum score, far above the 25.0 exclusion threshold

**Attack Execution Path:**

1. Malicious peer responds successfully to 25 requests (score → 100.0)
2. Malicious peer causes 1 timeout (score: 100.0 × 0.95 = 95.0)
3. Repeat cycle every ~26 seconds (less than 30-second freeze duration)

**Why This Works:**

- The freeze timer is stored in DynamicPrefetchingState as a global field [12](#0-11) , affecting the entire data stream regardless of which peer caused the timeout
- Data client errors trigger decrease_max_concurrent_requests() [13](#0-12) 
- Network timeouts (RpcError::TimedOut) are classified as ErrorType::NotUseful [14](#0-13)  and [15](#0-14) 
- Each failure decreases the limit by 2 [16](#0-15) , eventually reaching the minimum of 3 [17](#0-16) 
- Dynamic prefetching is enabled by default [18](#0-17) , making this vulnerability universally applicable
- Even with multi-fetch enabled (default configuration), one malicious peer's timeout affects the global freeze timer for all peers

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty category "Validator Node Slowdowns" which explicitly includes "DoS through resource exhaustion" as a valid impact.

**Concrete Impact:**
- State synchronization operates at 10% normal capacity (3 vs 30 maximum concurrent requests)
- New validators attempting to join the network experience ~10x slower synchronization
- Existing validators that fall behind (e.g., after maintenance or downtime) take ~10x longer to catch up to the blockchain tip
- Network's ability to onboard new validators is severely degraded, impacting decentralization
- Network resilience is compromised as validators cannot quickly recover from transient issues

**Severity Justification:**

While this does not completely halt operations (minimum concurrency is 3, not 0) or directly affect consensus, it significantly impairs critical network operations:
1. Validator onboarding becomes prohibitively slow
2. Network recovery from validator failures is degraded
3. Overall network decentralization and resilience are compromised

The performance degradation is persistent and requires no ongoing effort from the attacker once the timing pattern is established. This aligns with the "Validator Node Slowdowns (High)" category where "significant performance degradation affecting consensus" through "DoS via resource exhaustion" is explicitly recognized.

## Likelihood Explanation

**Likelihood: High**

**Attacker Capabilities Required:**
- Operate network peer nodes (achievable by any party - Aptos is a permissionless P2P network)
- Control response timing to selectively timeout requests (trivial - attacker controls their node)
- No validator privileges, stake requirements, or special access needed

**Attack Feasibility:**
- Attack pattern is straightforward: 25 successes, 1 timeout, repeat every 26 seconds
- Timing requirements are not strict - any failure frequency under 30 seconds maintains the freeze
- Default configuration enables dynamic prefetching [18](#0-17) 
- Peer scoring with ignore_low_score_peers is enabled by default [19](#0-18) , but the attack maintains maximum scores

**Practical Execution:**
- Malicious peer will be selected regularly due to high reputation score (100.0)
- Even with multi-fetch selecting multiple peers, one malicious peer's timeout resets the global freeze timer
- Attack is economically viable - no resource costs beyond running a peer node

## Recommendation

**Primary Fix: Make Freeze Timer Per-Peer**

The freeze timer should be tracked per-peer rather than globally. Modify `DynamicPrefetchingState` to maintain a map of peer-specific freeze timestamps:

```rust
pub struct DynamicPrefetchingState {
    streaming_service_config: DataStreamingServiceConfig,
    // Change from global to per-peer tracking
    peer_timeout_instants: Arc<DashMap<PeerNetworkId, Instant>>,
    max_dynamic_concurrent_requests: u64,
    time_service: TimeService,
}
```

Modify `is_prefetching_value_frozen()` to check if ANY peer in the current request set is frozen, and `decrease_max_concurrent_requests()` to accept a `PeerNetworkId` parameter for per-peer tracking.

**Secondary Mitigation: Enhanced Peer Scoring**

Implement pattern detection for peers with consistent low failure rates (e.g., exactly 1 in 25-30 requests):
- Track failure patterns over time windows
- Apply stricter scoring for peers exhibiting suspicious timing patterns
- Consider reducing the NOT_USEFUL_MULTIPLIER or increasing the IGNORE_PEER_THRESHOLD for repeated timeouts

**Tertiary Mitigation: Adaptive Freeze Duration**

Make freeze duration adaptive based on recent timeout patterns rather than fixed at 30 seconds:
- Shorter freeze for isolated timeouts
- Longer freeze for frequent timeouts from same peer
- This prevents exploitation of fixed timing windows

## Proof of Concept

```rust
#[cfg(test)]
mod vulnerability_test {
    use super::*;
    use aptos_time_service::TimeService;
    use aptos_config::config::{DataStreamingServiceConfig, DynamicPrefetchingConfig};

    #[test]
    fn test_freeze_timer_manipulation_attack() {
        // Setup: Create prefetching state with default config
        let mut config = DataStreamingServiceConfig::default();
        config.dynamic_prefetching = DynamicPrefetchingConfig {
            enable_dynamic_prefetching: true,
            initial_prefetching_value: 10,
            max_prefetching_value: 30,
            min_prefetching_value: 3,
            prefetching_value_increase: 1,
            prefetching_value_decrease: 2,
            timeout_freeze_duration_secs: 30,
            ..Default::default()
        };
        
        let time_service = TimeService::mock();
        let mut state = DynamicPrefetchingState::new(config, time_service.clone());
        let time_service = time_service.into_mock();
        
        // Simulate attack: 25 successes, 1 failure, repeat
        let mut current_concurrency = 10;
        
        // Cycle 1: Build up concurrency
        for _ in 0..25 {
            state.increase_max_concurrent_requests();
        }
        assert!(state.get_max_concurrent_requests(&create_mock_stream_engine()) > 10);
        
        // First timeout at T=0
        state.decrease_max_concurrent_requests();
        assert!(state.is_prefetching_value_frozen());
        
        // Advance 26 seconds (less than 30-second freeze)
        time_service.advance_secs(26);
        
        // Try to increase - should be blocked by freeze
        let concurrency_before = state.get_max_concurrent_requests(&create_mock_stream_engine());
        state.increase_max_concurrent_requests();
        let concurrency_after = state.get_max_concurrent_requests(&create_mock_stream_engine());
        assert_eq!(concurrency_before, concurrency_after, "Freeze should prevent increase");
        
        // Second timeout at T=26, resets freeze timer
        state.decrease_max_concurrent_requests();
        assert!(state.is_prefetching_value_frozen(), "Freeze timer should be reset");
        
        // Continue attack for multiple cycles
        for cycle in 0..10 {
            time_service.advance_secs(26);
            
            // Freeze should still be active
            assert!(state.is_prefetching_value_frozen(), 
                "Freeze should persist across cycles (cycle {})", cycle);
            
            // Cause another timeout before freeze expires
            state.decrease_max_concurrent_requests();
        }
        
        // After 10 cycles of decreases, should hit minimum
        let final_concurrency = state.get_max_concurrent_requests(&create_mock_stream_engine());
        assert_eq!(final_concurrency, 3, "Should reach minimum concurrency");
        
        // Verify freeze is still active
        assert!(state.is_prefetching_value_frozen(), 
            "Attack maintains permanent freeze");
    }
}
```

## Notes

This vulnerability represents a fundamental design flaw in the state sync architecture where a global resource (the freeze timer) can be manipulated by individual malicious peers. The peer reputation system, while well-designed for detecting consistently bad peers, fails to detect this sophisticated attack pattern that maintains high scores through strategic timing.

The vulnerability is particularly concerning because:
1. It affects all nodes performing state synchronization (new validators, recovering validators)
2. The attacker needs only maintain timing discipline, not computational resources
3. The impact compounds over time as concurrency is repeatedly decreased
4. The default configuration makes all nodes vulnerable without requiring specific misconfigurations

The recommended fix of per-peer freeze tracking aligns with the principle that peer-specific issues should not have global consequences, maintaining system resilience even when some peers are malicious or unreliable.

### Citations

**File:** state-sync/data-streaming-service/src/dynamic_prefetching.rs (L18-19)
```rust
    // The instant the last timeout occurred (if any)
    last_timeout_instant: Option<Instant>,
```

**File:** state-sync/data-streaming-service/src/dynamic_prefetching.rs (L62-76)
```rust
        match self.last_timeout_instant {
            Some(last_failure_time) => {
                // Get the time since the last failure and max freeze duration
                let time_since_last_failure =
                    self.time_service.now().duration_since(last_failure_time);
                let max_freeze_duration = Duration::from_secs(
                    self.get_dynamic_prefetching_config()
                        .timeout_freeze_duration_secs,
                );

                // Check if the time since the last failure is less than the freeze duration
                time_since_last_failure < max_freeze_duration
            },
            None => false, // No failures have occurred
        }
```

**File:** state-sync/data-streaming-service/src/dynamic_prefetching.rs (L109-126)
```rust
    pub fn increase_max_concurrent_requests(&mut self) {
        // If dynamic prefetching is disabled, or the value is currently frozen, do nothing
        if !self.is_dynamic_prefetching_enabled() || self.is_prefetching_value_frozen() {
            return;
        }

        // Otherwise, get and increase the current max
        let dynamic_prefetching_config = self.get_dynamic_prefetching_config();
        let amount_to_increase = dynamic_prefetching_config.prefetching_value_increase;
        let max_dynamic_concurrent_requests = self
            .max_dynamic_concurrent_requests
            .saturating_add(amount_to_increase);

        // Bound the value by the configured maximum
        let max_prefetching_value = dynamic_prefetching_config.max_prefetching_value;
        self.max_dynamic_concurrent_requests =
            min(max_dynamic_concurrent_requests, max_prefetching_value);
    }
```

**File:** state-sync/data-streaming-service/src/dynamic_prefetching.rs (L130-150)
```rust
    pub fn decrease_max_concurrent_requests(&mut self) {
        // If dynamic prefetching is disabled, do nothing
        if !self.is_dynamic_prefetching_enabled() {
            return;
        }

        // Update the last failure time
        self.last_timeout_instant = Some(self.time_service.now());

        // Otherwise, get and decrease the current max
        let dynamic_prefetching_config = self.get_dynamic_prefetching_config();
        let amount_to_decrease = dynamic_prefetching_config.prefetching_value_decrease;
        let max_dynamic_concurrent_requests = self
            .max_dynamic_concurrent_requests
            .saturating_sub(amount_to_decrease);

        // Bound the value by the configured minimum
        let min_prefetching_value = dynamic_prefetching_config.min_prefetching_value;
        self.max_dynamic_concurrent_requests =
            max(max_dynamic_concurrent_requests, min_prefetching_value);
    }
```

**File:** config/src/config/state_sync_config.rs (L315-315)
```rust
            enable_dynamic_prefetching: true,
```

**File:** config/src/config/state_sync_config.rs (L319-319)
```rust
            min_prefetching_value: 3,
```

**File:** config/src/config/state_sync_config.rs (L321-321)
```rust
            prefetching_value_decrease: 2,
```

**File:** config/src/config/state_sync_config.rs (L322-322)
```rust
            timeout_freeze_duration_secs: 30,
```

**File:** config/src/config/state_sync_config.rs (L466-466)
```rust
            ignore_low_score_peers: true,
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L33-33)
```rust
const MAX_SCORE: f64 = 100.0;
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L37-37)
```rust
const SUCCESSFUL_RESPONSE_DELTA: f64 = 1.0;
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L39-39)
```rust
const NOT_USEFUL_MULTIPLIER: f64 = 0.95;
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L43-43)
```rust
const IGNORE_PEER_THRESHOLD: f64 = 25.0;
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L163-174)
```rust
    fn update_score_success(&mut self) {
        self.score = f64::min(self.score + SUCCESSFUL_RESPONSE_DELTA, MAX_SCORE);
    }

    /// Updates the score of the peer according to an error
    fn update_score_error(&mut self, error: ErrorType) {
        let multiplier = match error {
            ErrorType::NotUseful => NOT_USEFUL_MULTIPLIER,
            ErrorType::Malicious => MALICIOUS_MULTIPLIER,
        };
        self.score = f64::max(self.score * multiplier, MIN_SCORE);
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L531-532)
```rust
                        self.dynamic_prefetching_state
                            .decrease_max_concurrent_requests();
```

**File:** state-sync/aptos-data-client/src/client.rs (L839-841)
```rust
                        RpcError::TimedOut => {
                            Error::TimeoutWaitingForResponse(rpc_error.to_string())
                        },
```

**File:** state-sync/aptos-data-client/src/client.rs (L865-865)
```rust
                self.notify_bad_response(id, peer, &request, ErrorType::NotUseful);
```
