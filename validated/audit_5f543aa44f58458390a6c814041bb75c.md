# Audit Report

## Title
Non-Atomic Cross-Database Pruning Operations Enable Permanent State Inconsistency

## Summary
The `DBSubPruner` trait implementations for `EventStorePruner` and `TransactionPruner` perform two separate atomic writes to different RocksDB databases (internal_indexer_db and ledger_db) without cross-database transaction coordination. Process crashes between these writes create permanent database inconsistencies where indices are deleted but corresponding data remains, breaking state integrity and requiring manual intervention.

## Finding Description

The `DBSubPruner` trait defines a `prune()` method with no atomicity guarantees across multiple database writes. [1](#0-0) 

When the internal indexer is enabled, `EventStorePruner::prune()` performs two separate write operations to different RocksDB instances:

1. First write to `internal_indexer_db` (deletes indices and updates progress) [2](#0-1) 

2. Second write to `ledger_db` (deletes events and updates progress) [3](#0-2) 

These are completely separate RocksDB databases, confirmed by the initialization code. [4](#0-3) 

`TransactionPruner::prune()` exhibits the identical pattern. [5](#0-4) 

**Critical Flaw**: On restart after a crash, the pruner reads progress ONLY from ledger_db. [6](#0-5) 

**Attack Scenario**:
1. Pruner executes with current_progress=50, target_version=100
2. First write to internal_indexer_db succeeds: indices deleted, progress=100
3. **Process crashes** (power failure, OOM kill, hardware fault)
4. Second write to ledger_db never executes: events remain, progress=50
5. On restart, pruner reads progress=50 from ledger_db
6. Result: Events 50-100 exist without indices in internal_indexer_db

When queries attempt to access these events via indices, they fail with errors. [7](#0-6) 

**No Recovery Mechanism Exists**: The `sync_commit_progress()` function only synchronizes ledger_db, state_kv_db, and state_merkle_db, explicitly excluding internal_indexer_db. [8](#0-7) 

## Impact Explanation

**Severity: Medium** (State inconsistencies requiring manual intervention - $10,000 bounty tier)

This vulnerability causes:

1. **Permanent Database Corruption**: Inconsistent state persists across restarts with no automatic recovery mechanism

2. **API Functionality Breaks**: Event and transaction queries via indices return errors, affecting applications relying on indexer API

3. **Node-Specific Inconsistency**: Different nodes may have different inconsistent states depending on crash timing, leading to divergent query results (though consensus itself remains intact as underlying data exists in ledger_db)

4. **Manual Intervention Required**: Recovery requires database resync, manual repair scripts, or deletion of orphaned data

5. **Non-Deterministic Manifestation**: Only occurs when crashes hit narrow window between writes, making diagnosis difficult

This matches the Aptos bug bounty **Medium Severity** category: "State inconsistencies requiring manual intervention" as it creates permanent data integrity violations requiring operator action to resolve.

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability manifests when:
- Internal indexer is enabled (common for API nodes)
- Pruning is active (regular occurrence)
- Process crashes between the two database writes

**Increasing Factors**:
- Many production nodes enable internal indexer for API functionality
- Pruning runs periodically on active nodes
- Multiple crash vectors: power failures, OOM kills, kernel panics, SIGKILL
- Both EventStorePruner and TransactionPruner affected

**Decreasing Factors**:
- Narrow crash window between two sequential writes
- Well-configured nodes have stability
- Each write is individually durable via RocksDB

Across a distributed network of nodes running 24/7, this will eventually manifest on some nodes, requiring manual intervention.

## Recommendation

Implement a two-phase commit protocol or write-ahead log for cross-database pruning operations:

1. **Option 1 - Single Source of Truth**: Store pruner progress only in internal_indexer_db when enabled, reading from there on restart
2. **Option 2 - Consistency Validation**: Add startup checks comparing progress between databases and recovery logic to rebuild missing indices
3. **Option 3 - Atomic Coordinator**: Implement a transaction coordinator that ensures both databases are written or neither is, with rollback on failure

Additionally, extend `sync_commit_progress()` to include internal_indexer_db consistency validation.

## Proof of Concept

While a full PoC requires simulating process crashes during pruning, the vulnerability is verified through code analysis showing:

1. Two separate `write_schemas()` calls to different databases [9](#0-8) 
2. Progress read only from ledger_db on restart [10](#0-9) 
3. No recovery mechanism for cross-database consistency [11](#0-10) 

The vulnerability can be reproduced by:
1. Enabling internal indexer with events
2. Starting pruner operation
3. Sending SIGKILL after first write completes but before second write
4. Restarting node and observing query failures for pruned version range

### Citations

**File:** storage/aptosdb/src/pruner/db_sub_pruner.rs (L6-14)
```rust
/// Defines the trait for sub-pruner of a parent DB pruner
pub trait DBSubPruner {
    /// Returns the name of the sub pruner.
    fn name(&self) -> &str;

    /// Performs the actual pruning, a target version is passed, which is the target the pruner
    /// tries to prune.
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()>;
}
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L43-81)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        let mut indexer_batch = None;

        let indices_batch = if let Some(indexer_db) = self.indexer_db() {
            if indexer_db.event_enabled() {
                indexer_batch = Some(SchemaBatch::new());
            }
            indexer_batch.as_mut()
        } else {
            Some(&mut batch)
        };
        let num_events_per_version = self.ledger_db.event_db().prune_event_indices(
            current_progress,
            target_version,
            indices_batch,
        )?;
        self.ledger_db.event_db().prune_events(
            num_events_per_version,
            current_progress,
            target_version,
            &mut batch,
        )?;
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::EventPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        if let Some(mut indexer_batch) = indexer_batch {
            indexer_batch.put::<InternalIndexerMetadataSchema>(
                &IndexerMetadataKey::EventPrunerProgress,
                &IndexerMetadataValue::Version(target_version),
            )?;
            self.expect_indexer_db()
                .get_inner_db_ref()
                .write_schemas(indexer_batch)?;
        }
        self.ledger_db.event_db().write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L85-109)
```rust
    pub(in crate::pruner) fn new(
        ledger_db: Arc<LedgerDb>,
        metadata_progress: Version,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.event_db_raw(),
            &DbMetadataKey::EventPrunerProgress,
            metadata_progress,
        )?;

        let myself = EventStorePruner {
            ledger_db,
            internal_indexer_db,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up EventStorePruner."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/indexer/src/db_ops.rs (L37-48)
```rust
pub fn open_internal_indexer_db<P: AsRef<Path>>(
    db_path: P,
    rocksdb_config: &RocksdbConfig,
) -> Result<DB> {
    let env = None;
    Ok(DB::open(
        db_path,
        INTERNAL_INDEXER_DB_NAME,
        internal_indexer_column_families(),
        &gen_rocksdb_options(rocksdb_config, env, false),
    )?)
}
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L67-73)
```rust
                indexer_db.get_inner_db_ref().write_schemas(index_batch)?;
            } else {
                self.transaction_store
                    .prune_transaction_by_account(&candidate_transactions, &mut batch)?;
            }
        }
        self.ledger_db.transaction_db().write_schemas(batch)
```

**File:** storage/indexer/src/db_indexer.rs (L232-238)
```rust
            if seq != cur_seq {
                let msg = if cur_seq == start_seq_num {
                    "First requested event is probably pruned."
                } else {
                    "DB corruption: Sequence number not continuous."
                };
                bail!("{} expected: {}, actual: {}", msg, cur_seq, seq);
```

**File:** storage/aptosdb/src/state_store/mod.rs (L410-425)
```rust
    pub fn sync_commit_progress(
        ledger_db: Arc<LedgerDb>,
        state_kv_db: Arc<StateKvDb>,
        state_merkle_db: Arc<StateMerkleDb>,
        crash_if_difference_is_too_large: bool,
    ) {
        let ledger_metadata_db = ledger_db.metadata_db();
        if let Some(overall_commit_progress) = ledger_metadata_db
            .get_synced_version()
            .expect("DB read failed.")
        {
            info!(
                overall_commit_progress = overall_commit_progress,
                "Start syncing databases..."
            );
            let ledger_commit_progress = ledger_metadata_db
```
