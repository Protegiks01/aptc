# Audit Report

## Title
TOCTOU Race Condition Between Event Query Pruning Checks and Data Access Causes Missing Data

## Summary
A Time-Of-Check-Time-Of-Use (TOCTOU) race condition exists between the `error_if_ledger_pruned` validation and the actual event data access. Concurrent pruning operations can delete event data after the pruning check passes but before the data is read, causing queries to return empty results for events that should be accessible. This violates data consistency guarantees and causes data loss from the perspective of API consumers.

## Finding Description

The vulnerability exists in the event query flow where pruning validation and data access are non-atomic operations. [1](#0-0) 

The `error_if_ledger_pruned` check reads the `min_readable_version` from an atomic variable at line 518, which loads the value atomically: [2](#0-1) 

However, the pruner updates `min_readable_version` **before** actually pruning the data. When `maybe_set_pruner_target_db_version` is called after committing new versions: [3](#0-2) 

This immediately updates `min_readable_version` to the new target value: [4](#0-3) 

At lines 165-166, `min_readable_version` is stored atomically, and then at lines 172-175, the pruner worker thread is notified. The pruner then executes asynchronously in a background thread: [5](#0-4) 

The EventStorePruner actually deletes the data through batch operations: [6](#0-5) 

Meanwhile, `get_events_by_version` returns an empty vector when data is missing, which is indistinguishable from legitimate empty events: [7](#0-6) 

**Race Condition Scenario:**
1. **T0**: Reader thread calls `error_if_ledger_pruned(version=950)`, reads `min_readable_version=900`, check passes
2. **T1**: System commits version 1100, calls `maybe_set_pruner_target_db_version(1100)`  
3. **T2**: `set_pruner_target_db_version` updates `min_readable_version=1000` immediately (line 165-166)
4. **T3**: Pruner worker wakes up and calls `prune()` to delete versions [900, 1000)
5. **T4**: EventStorePruner deletes version 950 data via `write_schemas` (line 80)
6. **T5**: Reader thread creates iterator at line 520-523, RocksDB snapshot taken AFTER deletion
7. **T6**: `get_events_by_version(950)` finds no data, returns `Ok(vec![])` at line 68

The caller cannot distinguish between "no events occurred" versus "events were pruned during the race window."

This breaks the **State Consistency** invariant: queries that pass validation checks should return consistent, reliable data.

## Impact Explanation

**Medium Severity** - State inconsistencies requiring manual intervention

This vulnerability aligns with the Aptos Bug Bounty category: **MEDIUM (up to $10,000): Limited Protocol Violations - State inconsistencies requiring manual intervention**

Impact breakdown:

1. **Missing Data**: API consumers receive empty event lists for transactions that actually had events, leading to incorrect application state in indexers, analytics platforms, and wallets
2. **Non-deterministic Query Results**: The same query at the same version can return different results depending on race timing
3. **Data Integrity Violations**: Applications relying on event data (critical for smart contract observability) will have permanent gaps in their records
4. **No Error Indication**: Callers receive `Ok(vec![])` and cannot detect that data was lost versus legitimately empty events

While this doesn't directly cause consensus violations or fund loss, it creates **state inconsistencies** that affect node reliability and data availability guarantees, requiring manual intervention to identify and address data gaps in dependent systems.

## Likelihood Explanation

**High Likelihood**

This race condition will occur naturally during normal node operation:

- **Continuous Background Pruning**: Pruning runs continuously in background threads on all full nodes [8](#0-7) 
- **Frequent Read Queries**: Event queries occur with every API call and indexer operation
- **Race Window Exists**: The microsecond-to-millisecond gap between pruning check (line 518) and iterator creation (lines 520-523) provides a race window [9](#0-8) 
- **No Synchronization**: No mutex, lock, or atomic transaction protects the check-then-access sequence
- **Normal Operation Only**: Requires only normal node operation under load, no attacker involvement
- **Production Occurrence**: Will naturally occur on production networks under concurrent load

## Recommendation

Implement atomic read operations that guarantee snapshot isolation between the pruning check and data access:

**Option 1: Atomic Check-and-Read**
Merge the pruning check and data access into a single atomic operation that uses the same RocksDB snapshot:

```rust
pub(crate) fn get_events_by_version_atomic(&self, version: Version) -> Result<Vec<ContractEvent>> {
    // Create iterator first (establishes RocksDB snapshot)
    let mut iter = self.db.iter::<EventSchema>()?;
    
    // Check pruning using the snapshot's view
    let min_readable_version = self.ledger_pruner.get_min_readable_version();
    ensure!(
        version >= min_readable_version,
        "Events at version {} are pruned, min available version is {}.",
        version,
        min_readable_version
    );
    
    // Access data using same snapshot
    let mut events = vec![];
    iter.seek(&version)?;
    while let Some(((ver, _index), event)) = iter.next().transpose()? {
        if ver != version {
            break;
        }
        events.push(event);
    }
    
    Ok(events)
}
```

**Option 2: Read Lock on Pruning**
Use an RwLock to coordinate between readers and the pruner, ensuring pruning operations cannot proceed while reads are in progress for the affected version range.

## Proof of Concept

This vulnerability manifests during normal concurrent operation. A Rust test demonstrating the race:

```rust
#[test]
fn test_toctou_race_condition() {
    use std::sync::{Arc, atomic::{AtomicBool, Ordering}};
    use std::thread;
    
    let db = Arc::new(setup_test_db());
    let race_occurred = Arc::new(AtomicBool::new(false));
    
    // Simulate concurrent reader and pruner
    let reader_handle = {
        let db = Arc::clone(&db);
        let race_flag = Arc::clone(&race_occurred);
        thread::spawn(move || {
            // Check pruning
            let min_version = db.ledger_pruner.get_min_readable_version();
            let query_version = min_version + 50;
            
            if query_version >= min_version {
                thread::sleep(Duration::from_millis(10)); // Simulate race window
                
                // Attempt to read
                let events = db.event_db().get_events_by_version(query_version).unwrap();
                
                // If we pass the check but get empty results, race occurred
                if events.is_empty() {
                    race_flag.store(true, Ordering::SeqCst);
                }
            }
        })
    };
    
    // Simulate pruner updating min_readable_version then deleting
    thread::sleep(Duration::from_millis(5));
    db.ledger_pruner.maybe_set_pruner_target_db_version(new_version);
    
    reader_handle.join().unwrap();
    assert!(race_occurred.load(Ordering::SeqCst), "Race condition did not manifest");
}
```

## Notes

This vulnerability is particularly insidious because:

1. **Silent Failure**: The API returns success (`Ok`) with empty results, providing no indication of data loss
2. **Widespread Impact**: Affects all event-based applications (indexers, analytics, dApps relying on events)
3. **Detection Difficulty**: Applications cannot distinguish between legitimate empty events and missing data due to the race
4. **Permanent Data Loss**: Once pruning completes, the data is permanently deleted from the node

The fix requires ensuring atomic consistency between the pruning validation check and the actual data access operation, either through snapshot isolation or explicit synchronization mechanisms.

### Citations

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L511-529)
```rust
    fn get_events_iterator(
        &self,
        start_version: Version,
        limit: u64,
    ) -> Result<Box<dyn Iterator<Item = Result<Vec<ContractEvent>>> + '_>> {
        gauged_api("get_events_iterator", || {
            error_if_too_many_requested(limit, MAX_REQUEST_LIMIT)?;
            self.error_if_ledger_pruned("Transaction", start_version)?;

            let iter = self
                .ledger_db
                .event_db()
                .get_events_by_version_iter(start_version, limit as usize)?;
            Ok(Box::new(iter)
                as Box<
                    dyn Iterator<Item = Result<Vec<ContractEvent>>> + '_,
                >)
        })
    }
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L261-271)
```rust
    pub(super) fn error_if_ledger_pruned(&self, data_type: &str, version: Version) -> Result<()> {
        let min_readable_version = self.ledger_pruner.get_min_readable_version();
        ensure!(
            version >= min_readable_version,
            "{} at version {} is pruned, min available version is {}.",
            data_type,
            version,
            min_readable_version
        );
        Ok(())
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L628-629)
```rust
            self.ledger_pruner
                .maybe_set_pruner_target_db_version(version);
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L107-139)
```rust
    /// Creates a worker thread that waits on a channel for pruning commands.
    pub fn new(
        ledger_db: Arc<LedgerDb>,
        ledger_pruner_config: LedgerPrunerConfig,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Self {
        let pruner_worker = if ledger_pruner_config.enable {
            Some(Self::init_pruner(
                Arc::clone(&ledger_db),
                ledger_pruner_config,
                internal_indexer_db,
            ))
        } else {
            None
        };

        let min_readable_version =
            pruner_utils::get_ledger_pruner_progress(&ledger_db).expect("Must succeed.");

        PRUNER_VERSIONS
            .with_label_values(&["ledger_pruner", "min_readable"])
            .set(min_readable_version as i64);

        Self {
            ledger_db,
            prune_window: ledger_pruner_config.prune_window,
            pruner_worker,
            pruning_batch_size: ledger_pruner_config.batch_size,
            latest_version: Arc::new(Mutex::new(min_readable_version)),
            user_pruning_window_offset: ledger_pruner_config.user_pruning_window_offset,
            min_readable_version: AtomicVersion::new(min_readable_version),
        }
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L162-176)
```rust
    fn set_pruner_target_db_version(&self, latest_version: Version) {
        assert!(self.pruner_worker.is_some());
        let min_readable_version = latest_version.saturating_sub(self.prune_window);
        self.min_readable_version
            .store(min_readable_version, Ordering::SeqCst);

        PRUNER_VERSIONS
            .with_label_values(&["ledger_pruner", "min_readable"])
            .set(min_readable_version as i64);

        self.pruner_worker
            .as_ref()
            .unwrap()
            .set_target_db_version(min_readable_version);
    }
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L53-69)
```rust
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L43-81)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        let mut indexer_batch = None;

        let indices_batch = if let Some(indexer_db) = self.indexer_db() {
            if indexer_db.event_enabled() {
                indexer_batch = Some(SchemaBatch::new());
            }
            indexer_batch.as_mut()
        } else {
            Some(&mut batch)
        };
        let num_events_per_version = self.ledger_db.event_db().prune_event_indices(
            current_progress,
            target_version,
            indices_batch,
        )?;
        self.ledger_db.event_db().prune_events(
            num_events_per_version,
            current_progress,
            target_version,
            &mut batch,
        )?;
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::EventPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        if let Some(mut indexer_batch) = indexer_batch {
            indexer_batch.put::<InternalIndexerMetadataSchema>(
                &IndexerMetadataKey::EventPrunerProgress,
                &IndexerMetadataValue::Version(target_version),
            )?;
            self.expect_indexer_db()
                .get_inner_db_ref()
                .write_schemas(indexer_batch)?;
        }
        self.ledger_db.event_db().write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/ledger_db/event_db.rs (L67-81)
```rust
    pub(crate) fn get_events_by_version(&self, version: Version) -> Result<Vec<ContractEvent>> {
        let mut events = vec![];

        let mut iter = self.db.iter::<EventSchema>()?;
        // Grab the first event and then iterate until we get all events for this version.
        iter.seek(&version)?;
        while let Some(((ver, _index), event)) = iter.next().transpose()? {
            if ver != version {
                break;
            }
            events.push(event);
        }

        Ok(events)
    }
```
