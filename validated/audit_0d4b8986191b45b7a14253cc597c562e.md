# Audit Report

## Title
Validator Crash Risk During Epoch Transitions Due to Non-Atomic Reset in Consensus Pipeline

## Summary
The `ExecutionProxyClient` implements non-atomic, sequential resets of consensus pipeline components (`rand_manager`, `buffer_manager`, `secret_share_manager`) in both the `reset()` and `end_epoch()` methods. If early component resets succeed but later ones fail due to channel closures or task panics, validator nodes will crash during epoch transitions via `.expect()` panics, potentially affecting network liveness. [1](#0-0) 

## Finding Description

**Non-Atomic Reset Pattern:**

The `reset()` method performs sequential resets without atomicity or rollback: [2](#0-1) 

Each component is reset independently using channel-based communication. If `rand_manager` reset succeeds but `buffer_manager` reset fails (channel send error at line 698-704), the error propagates via `?` operator, leaving components in inconsistent states.

**Critical Vulnerability in end_epoch():**

The more severe manifestation occurs in `end_epoch()`, which uses `.expect()` instead of error propagation: [3](#0-2) 

If any reset operation fails (lines 728, 731, 741, 744, 754, 757), the validator node **panics immediately** via `.expect()`. This occurs during epoch transitions when all validators execute end_epoch simultaneously.

**Root Cause Analysis:**

When buffer_manager reset fails, the channel send or acknowledgment receipt operations fail: [4](#0-3) 

Error mapping discards underlying channel error details, making diagnosis difficult.

**Epoch Change Path:**

The epoch manager invokes sync_to_target with `.expect()`, causing panics on reset failures: [5](#0-4) 

**Component State After Partial Reset:**

Rand Manager state after successful reset: [6](#0-5) [7](#0-6) 

Buffer Manager state update on successful reset: [8](#0-7) 

## Impact Explanation

This qualifies as **High Severity** per Aptos bug bounty criteria:

**Validator Node Crashes:** During epoch transitions, if any component reset fails after earlier ones succeed, the validator immediately crashes via panic (lines 728, 731, 754, 757 in end_epoch). This is worse than slowdownsâ€”it's complete unavailability.

**Consensus Liveness Risk:** Epoch transitions occur simultaneously across all validators. A common cause (resource pressure, memory exhaustion, configuration issue) could trigger channel closures in multiple validators, causing synchronized crashes. If >1/3 validators crash, the network loses liveness until manual intervention.

**No Automated Recovery:** Panicked validators require manual restart. During critical epoch transitions, this creates a coordination failure window where the network cannot make progress.

**Breaks Availability Guarantees:** Validators must maintain availability during protocol operations. Panicking during normal epoch transitions violates this fundamental requirement.

## Likelihood Explanation

**Likelihood: Medium**

Channel closures causing reset failures can occur through:

1. **Task Panics:** If the buffer_manager or rand_manager task panics due to bugs, resource exhaustion, or unexpected state, the channel closes. The reset sender will receive a send error, triggering the panic path.

2. **Resource Exhaustion:** Under memory pressure, tasks may be terminated by the runtime, closing channels and triggering failures during the next reset operation.

3. **Epoch Transitions:** All validators execute end_epoch simultaneously during epoch boundaries (regular occurrence in Aptos). Common infrastructure issues (network storms, synchronized load spikes) could affect multiple validators.

4. **No Attacker Required:** This is a reliability vulnerability triggered by operational conditions, not requiring malicious action.

The TODO comment acknowledges unhandled error scenarios: [9](#0-8) 

## Recommendation

Implement atomic reset with compensation logic:

```rust
async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
    let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager) = {
        let handle = self.handle.read();
        (
            handle.reset_tx_to_rand_manager.clone(),
            handle.reset_tx_to_buffer_manager.clone(),
        )
    };
    
    // Phase 1: Collect all reset acknowledgments
    let mut reset_states = Vec::new();
    
    if let Some(mut reset_tx) = reset_tx_to_rand_manager {
        let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
        reset_tx.send(ResetRequest { tx: ack_tx, signal: ResetSignal::TargetRound(target.commit_info().round()) })
            .await.map_err(|_| Error::RandResetDropped)?;
        ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        reset_states.push("rand");
    }
    
    // If any subsequent reset fails, implement rollback for completed resets
    if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
        let (tx, rx) = oneshot::channel::<ResetAck>();
        if let Err(e) = reset_tx.send(ResetRequest { tx, signal: ResetSignal::TargetRound(target.commit_info().round()) }).await {
            // Rollback: restore previous state for completed resets
            self.rollback_resets(reset_states).await?;
            return Err(Error::ResetDropped);
        }
        rx.await.map_err(|_| Error::ResetDropped)?;
    }
    
    Ok(())
}
```

Additionally, replace `.expect()` with proper error handling in `end_epoch()` to prevent panics during epoch transitions.

## Proof of Concept

```rust
// Simulated scenario demonstrating the panic path
#[tokio::test]
async fn test_partial_reset_panic_during_epoch_transition() {
    // Setup: Create ExecutionProxyClient with rand_manager and buffer_manager
    // 
    // 1. Simulate epoch transition calling end_epoch()
    // 2. Ensure rand_manager task is running
    // 3. Kill buffer_manager task (simulating panic/resource exhaustion)
    // 4. Observe that end_epoch() panics at line 754 or 757
    //    when buffer_manager reset channel is closed
    //
    // Expected: Validator node crashes with panic message:
    // "[EpochManager] Fail to drop buffer manager"
    //
    // Impact: During actual epoch transitions, if this occurs on >1/3 validators,
    // network loses liveness until manual recovery
}
```

## Notes

The vulnerability is more severe in the `end_epoch()` path than described in the original finding. The `.expect()` calls guarantee immediate validator crashes during epoch transitions, which is a critical availability issue. The recovery path via `sync_to_target` in the recovery manager logs errors and continues, making it less severe. The primary concern is synchronized validator crashes during epoch boundaries caused by common operational failures affecting multiple nodes.

### Citations

**File:** consensus/src/pipeline/execution_client.rs (L669-670)
```rust
        // TODO: handle the state sync error (e.g., re-push the ordered
        // blocks to the buffer manager when it's reset but sync fails).
```

**File:** consensus/src/pipeline/execution_client.rs (L674-709)
```rust
    async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
        let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager) = {
            let handle = self.handle.read();
            (
                handle.reset_tx_to_rand_manager.clone(),
                handle.reset_tx_to_buffer_manager.clone(),
            )
        };

        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        }

        if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
            // reset execution phase and commit phase
            let (tx, rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::ResetDropped)?;
            rx.await.map_err(|_| Error::ResetDropped)?;
        }

        Ok(())
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L721-758)
```rust
        if let Some(mut tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop rand manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop rand manager");
        }

        if let Some(mut tx) = reset_tx_to_secret_share_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop secret share manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop secret share manager");
        }

        if let Some(mut tx) = reset_tx_to_buffer_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop buffer manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop buffer manager");
        }
```

**File:** consensus/src/pipeline/errors.rs (L15-18)
```rust
    #[error("Reset host dropped")]
    ResetDropped,
    #[error("Rand Reset host dropped")]
    RandResetDropped,
```

**File:** consensus/src/epoch_manager.rs (L558-565)
```rust
        self.execution_client
            .sync_to_target(ledger_info.clone())
            .await
            .context(format!(
                "[EpochManager] State sync to new epoch {}",
                ledger_info
            ))
            .expect("Failed to sync to new epoch");
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L184-194)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.rand_store.lock().reset(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/rand/rand_gen/rand_store.rs (L253-259)
```rust
    pub fn reset(&mut self, round: u64) {
        self.update_highest_known_round(round);
        // remove future rounds items in case they're already decided
        // otherwise if the block re-enters the queue, it'll be stuck
        let _ = self.rand_map.split_off(&round);
        let _ = self.fast_rand_map.as_mut().map(|map| map.split_off(&round));
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L579-596)
```rust
    async fn process_reset_request(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        info!("Receive reset");

        match signal {
            ResetSignal::Stop => self.stop = true,
            ResetSignal::TargetRound(round) => {
                self.highest_committed_round = round;
                self.latest_round = round;

                let _ = self.drain_pending_commit_proof_till(round);
            },
        }

        self.reset().await;
        let _ = tx.send(ResetAck::default());
        info!("Reset finishes");
    }
```
