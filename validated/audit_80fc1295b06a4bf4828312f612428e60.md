# Audit Report

## Title
Critical Consensus Message Loss via Premature Broadcast Abort in DAG Driver

## Summary
The DAG consensus implementation contains a critical vulnerability where reliable broadcast tasks for consensus messages are prematurely aborted when evicted from a bounded queue, causing certified nodes to be lost without proper delivery to all validators. This can lead to consensus liveness failures affecting network availability.

## Finding Description

The vulnerability exists in the interaction between the DAG driver's bounded broadcast handle queue and the reliable broadcast mechanism that waits for all validators to respond.

The `rb_handles` field stores broadcast abort handles in a `BoundedVecDeque` with capacity set to `window_size_config`: [1](#0-0) 

The default window size is 10 rounds: [2](#0-1) 

The bounded queue is initialized with this capacity: [3](#0-2) 

When broadcasting a new node, if the queue is full, `push_back()` evicts the oldest handle: [4](#0-3) 

The evicted `DropGuard` triggers abort when dropped: [5](#0-4) 

**The Critical Issue**: The broadcast mechanism waits for responses from ALL validators, not just quorum. The `SignatureBuilder` returns `Some(())` only when all validators have responded: [6](#0-5) 

Similarly, `CertificateAckState` waits for all validators: [7](#0-6) 

The reliable broadcast loop continues until the aggregating status returns `Some(aggregated)`: [8](#0-7) 

**Attack Scenario**:
1. Validator V broadcasts a node at round R
2. Signature collection reaches quorum (2f+1) and creates certificate
3. Certified node broadcast begins but some validators are slow/partitioned
4. Rounds R+1 through R+10 complete quickly
5. At round R+11, the new broadcast evicts round R's handle
6. Round R's broadcast is aborted before all validators receive the certified node

**Recovery Mechanism Fails**: When validators encounter this node as a parent dependency, they attempt to fetch it. However, fetch is limited to validators who signed the certificate: [9](#0-8) 

If those specific validators (the 2f+1 signers) either didn't receive the certified node due to the abort OR are currently unreachable, the fetch fails. This prevents validators from progressing consensus for any nodes depending on this parent.

The exponential backoff configuration shows broadcasts can take up to 3 seconds with retries: [10](#0-9) 

State sync uses the same fetch mechanism with the same limitation: [11](#0-10) 

Developer TODO comments indicate awareness of potential issues with the bounded queue: [12](#0-11) 

## Impact Explanation

This vulnerability meets **Critical Severity** criteria per the Aptos bug bounty program under "Total Loss of Liveness/Network Availability":

- **Consensus Liveness Failure**: Validators missing certified nodes cannot add any nodes that depend on them as parents, effectively halting DAG progression for those validators.

- **Network-Wide Impact**: If multiple validators are affected by different missing nodes, the network cannot achieve quorum for new rounds, causing total network liveness loss.

- **Cascading Failures**: Missing nodes from round R block all dependent nodes in rounds R+1, R+2, etc., creating a cascade that prevents forward progress.

- **Limited Recovery**: Recovery requires that at least one of the 2f+1 certificate signers comes back online with the certified node. If those specific validators remain unavailable, the issue persists indefinitely without manual intervention.

This violates the fundamental consensus liveness guarantee that the system should make progress when 2f+1 validators are available, because the required 2f+1 validators (historical signers) may differ from the currently available 2f+1 validators.

## Likelihood Explanation

This vulnerability has **Medium to High** likelihood:

**Contributing Factors**:
- Broadcasts taking longer than 10 rounds is feasible: with exponential backoff reaching 3 seconds and sub-second round times in optimal conditions, 10 rounds can complete in 5-10 seconds while broadcasts may still be retrying
- Network latency spikes are common in production environments
- Validator restarts or temporary partitions occur regularly during normal operations
- No attacker required - this happens naturally during normal network variance

**Realistic Scenario**: A validator with temporarily poor connectivity collects signatures and creates a certificate, but before the certified node broadcast completes to all validators, the network conditions improve and rounds progress quickly, evicting the slow broadcast. The validator then crashes or remains partitioned, making the certified node unrecoverable.

## Recommendation

1. **Remove or significantly increase the bounded queue size**: The queue should accommodate the maximum expected broadcast duration, not just the number of rounds. Consider making it unbounded or sizing it based on time rather than rounds.

2. **Change broadcast completion semantics**: Modify `SignatureBuilder` and `CertificateAckState` to return `Some(())` after quorum is reached, not after all validators respond. The broadcast can continue in the background for stragglers without blocking queue eviction.

3. **Add broadcast persistence**: Before evicting a handle, verify the broadcast has reached quorum of validators. Store completed broadcasts in a persistent cache that can serve fetch requests.

4. **Expand fetch responders**: Allow fetching from any validator that has the node, not just certificate signers. Validators should gossip certified nodes they receive to peers.

5. **Implement timeout-based eviction**: Instead of round-based eviction, evict handles after a time-based timeout (e.g., 30 seconds) to ensure broadcasts have adequate time to complete.

## Proof of Concept

A proof of concept would require a testnet environment with:

1. Deploy DAG consensus with 4 validators (f=1)
2. Configure rapid round progression (sub-second rounds)
3. Introduce network partition affecting 1 validator during certified node broadcast
4. Allow 10+ rounds to complete quickly
5. Verify the broadcast handle is evicted
6. Observe that partitioned validator cannot fetch the missing certified node when it recovers
7. Demonstrate that dependent nodes cannot be added, halting consensus progress

The proof would demonstrate that the code behavior matches the analysis, confirming the vulnerability is exploitable in production configurations.

## Notes

The TODO comments in the codebase indicate developers were aware of potential issues with the bounded queue implementation but may not have fully analyzed this specific message loss scenario. The vulnerability is particularly concerning because it can cause liveness failures even when the current validator set has more than 2f+1 available validators, if the historical certificate signers are unavailable.

### Citations

**File:** consensus/src/dag/dag_driver.rs (L57-57)
```rust
    rb_handles: Mutex<BoundedVecDeque<(DropGuard, u64)>>,
```

**File:** consensus/src/dag/dag_driver.rs (L103-103)
```rust
            rb_handles: Mutex::new(BoundedVecDeque::new(window_size_config as usize)),
```

**File:** consensus/src/dag/dag_driver.rs (L371-372)
```rust
        // TODO: a bounded vec queue can hold more than window rounds, but we want to limit
        // by number of rounds.
```

**File:** types/src/on_chain_config/consensus_config.rs (L594-594)
```rust
            dag_ordering_causal_history_window: 10,
```

**File:** crates/aptos-collections/src/bounded_vec_deque.rs (L28-38)
```rust
    pub fn push_back(&mut self, item: T) -> Option<T> {
        let oldest = if self.is_full() {
            self.inner.pop_front()
        } else {
            None
        };

        self.inner.push_back(item);
        assert!(self.inner.len() <= self.capacity);
        oldest
    }
```

**File:** crates/reliable-broadcast/src/lib.rs (L167-204)
```rust
            loop {
                tokio::select! {
                    Some((receiver, result)) = rpc_futures.next() => {
                        let aggregating = aggregating.clone();
                        let future = executor.spawn(async move {
                            (
                                    receiver,
                                    result
                                        .and_then(|msg| {
                                            msg.try_into().map_err(|e| anyhow::anyhow!("{:?}", e))
                                        })
                                        .and_then(|ack| aggregating.add(receiver, ack)),
                            )
                        }).await;
                        aggregate_futures.push(future);
                    },
                    Some(result) = aggregate_futures.next() => {
                        let (receiver, result) = result.expect("spawned task must succeed");
                        match result {
                            Ok(may_be_aggragated) => {
                                if let Some(aggregated) = may_be_aggragated {
                                    return Ok(aggregated);
                                }
                            },
                            Err(e) => {
                                log_rpc_failure(e, receiver);

                                let backoff_strategy = backoff_policies
                                    .get_mut(&receiver)
                                    .expect("should be present");
                                let duration = backoff_strategy.next().expect("should produce value");
                                rpc_futures
                                    .push(send_message(receiver, Some(duration)));
                            },
                        }
                    },
                    else => unreachable!("Should aggregate with all responses")
                }
```

**File:** crates/reliable-broadcast/src/lib.rs (L232-236)
```rust
impl Drop for DropGuard {
    fn drop(&mut self) {
        self.abort_handle.abort();
    }
}
```

**File:** consensus/src/dag/types.rs (L600-604)
```rust
        if partial_signatures.signatures().len() == self.epoch_state.verifier.len() {
            Ok(Some(()))
        } else {
            Ok(None)
        }
```

**File:** consensus/src/dag/types.rs (L656-660)
```rust
        if received.len() == self.num_validators {
            Ok(Some(()))
        } else {
            Ok(None)
        }
```

**File:** consensus/src/dag/dag_fetcher.rs (L108-110)
```rust
            LocalFetchRequest::CertifiedNode(node, _) => {
                node.signatures().get_signers_addresses(validators)
            },
```

**File:** config/src/config/dag_consensus_config.rs (L115-118)
```rust
            // A backoff policy that starts at 100ms and doubles each iteration up to 3secs.
            backoff_policy_base_ms: 2,
            backoff_policy_factor: 50,
            backoff_policy_max_delay_ms: 3000,
```

**File:** consensus/src/dag/dag_state_sync.rs (L229-232)
```rust
        let responders = node
            .certificate()
            .signatures()
            .get_signers_addresses(&self.epoch_state.verifier.get_ordered_account_addresses());
```
