# Audit Report

## Title
Race Condition Between State Sync finish() and Consensus Pipeline pre_commit_block() Causes Validator Crash

## Summary
A race condition exists between the consensus pipeline's `pre_commit_block()` operation and state synchronization's `finish()` call. When a node falls behind and triggers state sync, the `BlockExecutor::finish()` method sets the internal executor state to `None`. Meanwhile, pending pre-commit tasks from the consensus pipeline can still execute and call `pre_commit_block()`, which panics with `.expect("BlockExecutor is not reset")`, causing validator node crashes.

## Finding Description

The vulnerability stems from insufficient synchronization between state sync operations and consensus pipeline tasks accessing the `BlockExecutor`.

**Vulnerable Code Structure:**

The `BlockExecutor` uses `.expect()` calls that panic when `inner` is `None`: [1](#0-0) 

The same pattern exists in `commit_ledger()` and `execute_and_update_state()`: [2](#0-1) [3](#0-2) 

The `finish()` method sets `inner` to `None`: [4](#0-3) 

**State Sync Triggers:**

State sync calls `finish()` before syncing: [5](#0-4) [6](#0-5) 

**Missing Synchronization:**

The consensus pipeline's `pre_commit` task calls the executor without acquiring `write_mutex`: [7](#0-6) 

The `write_mutex` is only used in `state_computer.rs` for state sync operations, not by consensus pipeline operations.

**Multiple Vulnerable Paths:**

State sync can be triggered through multiple paths that don't properly abort pending pipeline tasks:

1. Consensus observer sync to commit: [8](#0-7) 

2. Consensus observer sync for fallback: [9](#0-8) 

3. DAG state sync: [10](#0-9) 

**Race Condition Execution Flow:**

1. Block X enters consensus pipeline and spawns async pre-commit task
2. Pre-commit task waits for order proof at: [11](#0-10) 
3. Node detects it's falling behind, state sync is triggered
4. State sync acquires `write_mutex` and calls `executor.finish()`, setting `inner = None`
5. Order proof arrives, pre-commit task proceeds
6. Task spawns blocking operation calling `executor.pre_commit_block(block.id())`
7. **PANIC** at `.expect("BlockExecutor is not reset")` because `inner` is `None`
8. Validator crashes or consensus thread panics

**Why Existing Protections Fail:**

The `abort_pipeline_for_state_sync()` mechanism exists but has critical gaps: [12](#0-11) 

However, it's only called in `fast_forward_sync` and only when `maybe_block_store` is Some: [13](#0-12) 

The consensus observer and DAG state sync paths bypass this protection entirely. Additionally, the `spawn_blocking` task at the executor call is not abortable via the abort handle mechanism.

## Impact Explanation

**HIGH Severity** per Aptos bug bounty criteria - Validator Node Crashes:

This vulnerability causes validator nodes to crash during normal operation, meeting the HIGH severity criteria for "Validator node crashes" and "API crashes" in the Aptos bug bounty program.

**Consensus Availability Impact:**
- Individual validator crashes temporarily reduce the active validator set
- Multiple validators catching up simultaneously could crash together during network recovery scenarios
- Repeated crashes during catch-up prevent validators from rejoining the network
- Affects network liveness if enough validators crash simultaneously

The severity is amplified because:
1. **No malicious activity required** - occurs during legitimate network conditions
2. **Natural trigger** - nodes falling behind is common in distributed systems
3. **Repeated vulnerability** - automatic restart hits the same race during catch-up
4. **Multiple trigger paths** - consensus observer, DAG sync, and block store sync all vulnerable

## Likelihood Explanation

**HIGH Likelihood** due to:

**Natural Occurrence:**
- Nodes regularly fall behind due to network latency, temporary slowdowns, or brief outages
- Epoch transitions with validator set changes increase likelihood
- Network partition and recovery scenarios trigger simultaneous catch-up across multiple nodes

**Race Window Characteristics:**
- Pre-commit tasks explicitly wait for order/commit proofs, creating time windows
- Async task scheduling allows interleaving of state sync and consensus operations
- No synchronization barrier prevents concurrent access
- Multiple code paths can trigger the race (not a single narrow condition)

**Verification of Trigger Conditions:**
- State sync triggering when nodes fall behind is by design
- Pre-commit waiting for proofs is part of normal consensus pipeline operation
- The lack of `write_mutex` acquisition in consensus pipeline is structural, not a timing issue

The combination of natural triggers, multiple vulnerable paths, and structural lack of synchronization makes this HIGH likelihood.

## Recommendation

**Immediate Fix:**

1. **Add synchronization to consensus pipeline operations**: Acquire `write_mutex` before calling any `BlockExecutor` method from consensus pipeline tasks.

2. **Use error returns instead of panics**: Replace `.expect()` calls with `.ok_or_else()` that return errors (like `ledger_update()` does): [14](#0-13) 

3. **Abort pipeline before all state sync operations**: Ensure `abort_pipeline_for_state_sync()` is called in consensus observer and DAG state sync paths, not just in `fast_forward_sync()`.

**Suggested Code Fix:**

For `pre_commit_block()`:
```rust
fn pre_commit_block(&self, block_id: HashValue) -> ExecutorResult<()> {
    let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "pre_commit_block"]);
    
    self.inner
        .read()
        .as_ref()
        .ok_or_else(|| ExecutorError::InternalError {
            error: "BlockExecutor is not reset".into(),
        })?
        .pre_commit_block(block_id)
}
```

Apply the same pattern to `commit_ledger()` and `execute_and_update_state()`.

Additionally, ensure all state sync paths call `abort_pipeline_for_state_sync()` before calling `finish()`.

## Proof of Concept

A proof of concept would require:

1. Start a validator node with consensus pipeline enabled
2. Inject blocks into the consensus pipeline to trigger pre-commit tasks
3. Delay order proof delivery to keep pre-commit tasks waiting
4. Trigger state sync via consensus observer fallback or explicit sync request
5. Deliver order proof after state sync has called `finish()`
6. Observe panic and validator crash

Due to the async nature and timing requirements, this is best demonstrated through code inspection showing:
- The lack of synchronization (verified via grep showing `write_mutex` only in `state_computer.rs`)
- The `.expect()` panic points (verified in `block_executor/mod.rs`)
- The multiple unsynchronized state sync trigger paths (verified in consensus observer and DAG code)

The vulnerability is structural rather than requiring precise timing, making it reliably triggerable in production environments where nodes naturally fall behind.

## Notes

This vulnerability represents a **critical synchronization flaw** in the interaction between the consensus pipeline and state sync subsystems. The use of `.expect()` for what should be recoverable conditions, combined with insufficient coordination between async tasks, creates a validator crash vulnerability during normal operational scenarios.

The fix requires both:
1. **Defensive programming**: Replace panics with error returns
2. **Proper synchronization**: Either use the existing `write_mutex` consistently or implement a proper pipeline abort mechanism that works across all state sync paths

### Citations

**File:** execution/executor/src/block_executor/mod.rs (L97-112)
```rust
    fn execute_and_update_state(
        &self,
        block: ExecutableBlock,
        parent_block_id: HashValue,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> ExecutorResult<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "execute_and_state_checkpoint"]);

        self.maybe_initialize()?;
        // guarantee only one block being executed at a time
        let _guard = self.execution_lock.lock();
        self.inner
            .read()
            .as_ref()
            .expect("BlockExecutor is not reset")
            .execute_and_update_state(block, parent_block_id, onchain_config)
```

**File:** execution/executor/src/block_executor/mod.rs (L115-129)
```rust
    fn ledger_update(
        &self,
        block_id: HashValue,
        parent_block_id: HashValue,
    ) -> ExecutorResult<StateComputeResult> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "ledger_update"]);

        self.inner
            .read()
            .as_ref()
            .ok_or_else(|| ExecutorError::InternalError {
                error: "BlockExecutor is not reset".into(),
            })?
            .ledger_update(block_id, parent_block_id)
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L131-139)
```rust
    fn pre_commit_block(&self, block_id: HashValue) -> ExecutorResult<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "pre_commit_block"]);

        self.inner
            .read()
            .as_ref()
            .expect("BlockExecutor is not reset")
            .pre_commit_block(block_id)
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L141-149)
```rust
    fn commit_ledger(&self, ledger_info_with_sigs: LedgerInfoWithSignatures) -> ExecutorResult<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "commit_ledger"]);

        self.inner
            .read()
            .as_ref()
            .expect("BlockExecutor is not reset")
            .commit_ledger(ledger_info_with_sigs)
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L151-155)
```rust
    fn finish(&self) {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "finish"]);

        *self.inner.write() = None;
    }
```

**File:** consensus/src/state_computer.rs (L136-141)
```rust
        // Grab the logical time lock
        let mut latest_logical_time = self.write_mutex.lock().await;

        // Before state synchronization, we have to call finish() to free the
        // in-memory SMT held by the BlockExecutor to prevent a memory leak.
        self.executor.finish();
```

**File:** consensus/src/state_computer.rs (L178-185)
```rust
        // Grab the logical time lock and calculate the target logical time
        let mut latest_logical_time = self.write_mutex.lock().await;
        let target_logical_time =
            LogicalTime::new(target.ledger_info().epoch(), target.ledger_info().round());

        // Before state synchronization, we have to call finish() to free the
        // in-memory SMT held by BlockExecutor to prevent a memory leak.
        self.executor.finish();
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L1048-1048)
```rust
        order_proof_fut.await?;
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L1067-1073)
```rust
        tokio::task::spawn_blocking(move || {
            executor
                .pre_commit_block(block.id())
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
```

**File:** consensus/src/consensus_observer/observer/state_sync_manager.rs (L150-153)
```rust
                let latest_synced_ledger_info = match execution_client
                    .clone()
                    .sync_for_duration(fallback_duration)
                    .await
```

**File:** consensus/src/consensus_observer/observer/state_sync_manager.rs (L219-222)
```rust
                if let Err(error) = execution_client
                    .clone()
                    .sync_to_target(commit_decision.commit_proof().clone())
                    .await
```

**File:** consensus/src/dag/dag_state_sync.rs (L257-257)
```rust
        self.execution_client.sync_to_target(commit_li).await?;
```

**File:** consensus/src/block_storage/block_store.rs (L617-627)
```rust
    pub async fn abort_pipeline_for_state_sync(&self) {
        let blocks = self.inner.read().get_all_blocks();
        // the blocks are not ordered by round here, so we need to abort all then wait
        let futs: Vec<_> = blocks
            .into_iter()
            .filter_map(|b| b.abort_pipeline())
            .collect();
        for f in futs {
            f.wait_until_finishes().await;
        }
    }
```

**File:** consensus/src/block_storage/sync_manager.rs (L506-514)
```rust
        if let Some(block_store) = maybe_block_store {
            monitor!(
                "abort_pipeline_for_state_sync",
                block_store.abort_pipeline_for_state_sync().await
            );
        }
        execution_client
            .sync_to_target(highest_commit_cert.ledger_info().clone())
            .await?;
```
