# Audit Report

## Title
Hot State LRU Head/Tail Corruption Due to Non-Atomic DashMap Updates Leading to Validator Crashes

## Summary
A race condition exists in the hot state commit process where LRU linked list updates are applied non-atomically to a concurrent DashMap. During commits, multiple entries can temporarily violate the doubly-linked list invariant (multiple heads or tails), and concurrent readers observing this inconsistent state trigger panics during LRU operations, causing validator crashes and loss of liveness.

## Finding Description

The hot state management system maintains an LRU cache using a doubly-linked list structure stored in `StateSlot` entries. Each hot slot contains an `lru_info: LRUEntry<StateKey>` with `prev` and `next` pointers. [1](#0-0)  The critical invariant is that at most one entry can have `prev=None` (the head) and at most one can have `next=None` (the tail).

**The Vulnerability:**

The `Committer` thread processes hot state commits asynchronously. [2](#0-1)  When committing, it iterates through delta entries and inserts them one-by-one into DashMap shards. [3](#0-2)  Each `insert()` operation is atomic for that single key, but there is no transaction or lock protecting the set of related LRU pointer updates.

When inserting a new head (e.g., entry D with `prev=None, next=A`), the old head A must also be updated (to `prev=D, next=B`). Between these sequential insertions, both D and A temporarily have `prev=None`, violating the LRU invariant that only one entry should be the head.

**Race Window Exposure:**

Concurrent execution threads call `get_committed()` which locks the committed State, clones it (getting snapshot of old head/tail metadata), releases the lock, and returns an Arc reference to the live DashMap being modified. [4](#0-3)  The committed State is only updated after all DashMap insertions complete. [5](#0-4) 

When execution creates a new `HotStateLRU` during state updates, it initializes with head/tail metadata from the old State snapshot but reads slots from the concurrently-modified DashMap. [6](#0-5)  This creates inconsistent state where the LRU metadata points to one head, but the DashMap contains a different structure with multiple entries claiming head status.

**Crash Points:**

When this corrupted LRU performs operations, it encounters critical failures:

1. `expect_hot_slot()` panics if expected keys don't exist or became cold. [7](#0-6) 

2. During eviction, the code expects the tail entry to have a `prev` pointer, and panics if `prev=None` (corrupted to think it's the head). [8](#0-7) 

The LRU validation check only runs in debug builds via `debug_assert!`, not in production. [9](#0-8) 

**Attack Path:**
1. Any user submits transactions triggering hot state updates (normal operation)
2. Committer thread applies DashMap updates sequentially
3. Execution thread calls `get_committed()` during the race window
4. Execution creates `HotStateLRU` with inconsistent metadata/data
5. LRU operations encounter unexpected state and panic
6. Validator process crashes, losing liveness

## Impact Explanation

This vulnerability qualifies as **High Severity** per Aptos bug bounty criteria:

**Validator Node Crashes**: The panics terminate the validator process, requiring manual restart. This is a concrete liveness failure affecting validator participation in consensus.

**Not Consensus Safety**: While severe, this does not break consensus safety because hot state is a cache optimization layer that doesn't affect state root commitments or consensus decisions. All validators would compute the same state roots even with corrupted hot state caches.

**Network Impact**: If multiple validators hit this race simultaneously during high-traffic periods, the network could temporarily lose consensus quorum, degrading network performance. However, this is validator-level crashes, not a network-layer DoS attack.

This aligns with the bug bounty category: **"Validator Node Crashes (High)"** rather than Critical-level consensus or fund loss issues.

## Likelihood Explanation

**Likelihood: Medium to High**

The race condition occurs naturally during normal validator operation:
- Every hot state commit creates a race window (microseconds per commit)
- High transaction throughput means frequent commits
- Parallel execution threads frequently call `get_committed()` [10](#0-9) 
- No special attacker privileges needed - any transaction submission triggers hot state updates
- 16 concurrent shards amplify the collision probability

The race window is small but the commit frequency at scale makes collisions likely. An attacker could increase likelihood by submitting transaction patterns that maximize hot state churn, though this also incurs gas costs.

## Recommendation

**Solution: Add transactional semantics to DashMap updates**

Option 1: Use a single Mutex/RwLock protecting the entire commit operation to ensure atomicity across all shard updates.

Option 2: Implement a versioned snapshot mechanism where readers get a consistent snapshot ID that corresponds to a fully-committed DashMap state, blocking until the commit completes if accessing mid-commit.

Option 3: Use a double-buffering approach where commits write to an inactive buffer, then atomically swap the active buffer after all updates complete.

The commit function should ensure that either:
- All LRU pointer updates complete before any reader can observe the new state, OR
- Readers are blocked/queued until the commit completes atomically

Additionally, the `debug_assert!` validation at line 269 should be promoted to a full assertion in production builds during the migration period to catch any remaining races.

## Proof of Concept

A full PoC requires a multi-threaded Rust test that:

1. Spawns a Committer thread performing rapid hot state commits
2. Spawns multiple reader threads calling `get_committed()` and creating `HotStateLRU` instances
3. Performs LRU operations (insert, evict) on the readers
4. Demonstrates panics when readers observe mid-commit state

The PoC would need to instrument timing to increase race window observation (e.g., adding sleeps between DashMap inserts) to make the bug reproducible in test environments. In production, the race manifests naturally at high transaction volumes without artificial timing manipulation.

**Notes**

This is a real concurrency bug in production code affecting validator stability. While the race window is small, the high frequency of commits at scale combined with concurrent execution makes this exploitable during normal network operation. The severity is appropriately HIGH (validator crashes/liveness) rather than CRITICAL (consensus/funds) because hot state is an optimization layer that doesn't affect state commitments or consensus safety.

### Citations

**File:** types/src/state_store/state_slot.rs (L24-40)
```rust
pub enum StateSlot {
    ColdVacant,
    HotVacant {
        hot_since_version: Version,
        lru_info: LRUEntry<StateKey>,
    },
    ColdOccupied {
        value_version: Version,
        value: StateValue,
    },
    HotOccupied {
        value_version: Version,
        value: StateValue,
        hot_since_version: Version,
        lru_info: LRUEntry<StateKey>,
    },
}
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L131-136)
```rust
    pub fn get_committed(&self) -> (Arc<dyn HotStateView>, State) {
        let state = self.committed.lock().clone();
        let base = self.base.clone();

        (base, state)
    }
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L160-170)
```rust
pub struct Committer {
    base: Arc<HotStateBase>,
    committed: Arc<Mutex<State>>,
    rx: Receiver<State>,
    total_key_bytes: usize,
    total_value_bytes: usize,
    /// Points to the newest entry. `None` if empty.
    heads: [Option<StateKey>; NUM_STATE_SHARDS],
    /// Points to the oldest entry. `None` if empty.
    tails: [Option<StateKey>; NUM_STATE_SHARDS],
}
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L192-202)
```rust
    fn run(&mut self) {
        info!("HotState committer thread started.");

        while let Some(to_commit) = self.next_to_commit() {
            self.commit(&to_commit);
            *self.committed.lock() = to_commit;

            GAUGE.set_with(&["hot_state_items"], self.base.len() as i64);
            GAUGE.set_with(&["hot_state_key_bytes"], self.total_key_bytes as i64);
            GAUGE.set_with(&["hot_state_value_bytes"], self.total_value_bytes as i64);
        }
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L243-261)
```rust
        for shard_id in 0..NUM_STATE_SHARDS {
            for (key, slot) in delta.shards[shard_id].iter() {
                if slot.is_hot() {
                    let key_size = key.size();
                    self.total_key_bytes += key_size;
                    self.total_value_bytes += slot.size();
                    if let Some(old_slot) = self.base.shards[shard_id].insert(key, slot) {
                        self.total_key_bytes -= key_size;
                        self.total_value_bytes -= old_slot.size();
                        n_update += 1;
                    } else {
                        n_insert += 1;
                    }
                } else if let Some((key, old_slot)) = self.base.shards[shard_id].remove(&key) {
                    self.total_key_bytes -= key.size();
                    self.total_value_bytes -= old_slot.size();
                    n_evict += 1;
                }
            }
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L269-269)
```rust
            debug_assert!(self.validate_lru(shard_id).is_ok());
```

**File:** storage/storage-interface/src/state_store/state.rs (L197-204)
```rust
                    let mut lru = HotStateLRU::new(
                        NonZeroUsize::new(self.hot_state_config.max_items_per_shard).unwrap(),
                        Arc::clone(&persisted_hot_state),
                        overlay,
                        hot_metadata.latest.clone(),
                        hot_metadata.oldest.clone(),
                        hot_metadata.num_items,
                    );
```

**File:** storage/storage-interface/src/state_store/hot_state.rs (L92-103)
```rust
        while self.num_items > self.capacity.get() {
            let slot = self
                .delete(&current)
                .expect("There must be entries to evict when current size is above capacity.");
            let prev_key = slot
                .prev()
                .cloned()
                .expect("There must be at least one newer entry (num_items > capacity >= 1).");
            evicted.push((current.clone(), slot.clone()));
            self.pending.insert(current, slot.to_cold());
            current = prev_key;
            self.num_items -= 1;
```

**File:** storage/storage-interface/src/state_store/hot_state.rs (L157-160)
```rust
    fn expect_hot_slot(&self, key: &StateKey) -> StateSlot {
        let slot = self.get_slot(key).expect("Given key is expected to exist.");
        assert!(slot.is_hot(), "Given key is expected to be hot.");
        slot
```

**File:** storage/aptosdb/src/state_store/persisted_state.rs (L46-48)
```rust
    pub fn get_state(&self) -> (Arc<dyn HotStateView>, State) {
        self.hot_state.get_committed()
    }
```
