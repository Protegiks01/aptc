# Audit Report

## Title
Memory Leak in Batch Store Subscribe Mechanism Leading to Unbounded Memory Growth

## Summary
The `subscribe()` function in the batch store creates oneshot channels for batch persistence notifications but fails to clean up channel senders when batch requests fail, causing unbounded memory accumulation in the `persist_subscribers` map that persists until epoch boundaries.

## Finding Description

The vulnerability exists in the batch subscription mechanism used during consensus batch fetching. When a validator node needs to fetch a batch that doesn't exist locally, the `get_or_fetch_batch()` function calls `subscribe()` which creates a oneshot channel and adds the sender to the `persist_subscribers` DashMap. [1](#0-0) 

The subscription mechanism registers the sender in the map, with a race condition check that attempts to notify subscribers if the batch already exists locally. [1](#0-0) 

Subscribers are only cleaned up in two scenarios:

1. **Successful notification** via `notify_subscribers()` when a batch is successfully persisted [2](#0-1) 

2. **Batch expiration** via `clear_expired_payload()` when a batch that exists in cache expires [3](#0-2) 

However, when batch fetching fails in `get_or_fetch_batch()`, the error propagates at line 703 without persisting the batch. [4](#0-3)  This means lines 704-707 that persist the batch and trigger `notify_subscribers()` are never executed, leaving the subscriber permanently leaked.

The `request_batch()` function can fail due to timeouts (after exhausting retry limit), network errors, or expired batches, returning errors without any cleanup of subscribers. [5](#0-4) 

**Attack Scenario:**
1. Consensus requests a batch that doesn't exist locally via `get_or_fetch_batch()`
2. `subscribe(digest)` is called, adding a sender to `persist_subscribers` 
3. `request_batch()` times out, receives network errors, or gets expired batch responses
4. Error propagates via `await?` on line 703, batch is never persisted
5. Sender remains in `persist_subscribers` indefinitely (until epoch change)
6. Repeated failures across many batches cause unbounded memory growth

The `clear_expired_payload()` cleanup only triggers for batches that were successfully added to cache [3](#0-2) , which requires `insert_to_cache()` to have been called [6](#0-5) . Failed batch fetches never reach cache insertion, so this cleanup path never triggers.

This breaks the **Resource Limits** invariant - consensus nodes should respect memory constraints, but leaked subscribers accumulate without bound within an epoch.

## Impact Explanation

**HIGH SEVERITY** - This qualifies as "Validator node slowdowns" per the Aptos bug bounty program.

The memory leak has cascading effects on validator node stability:
- Each leaked subscriber contains a `oneshot::Sender<PersistedValue<BatchInfoExt>>` stored in the DashMap [7](#0-6) 
- In high-throughput networks with frequent batch request failures (network issues, malicious peers), leaks accumulate rapidly
- Memory pressure causes garbage collection overhead and eventual slowdowns
- Severe cases can lead to out-of-memory conditions, forcing validator restarts
- Impacts consensus liveness when multiple validators experience degraded performance

While the leak is cleared at epoch boundaries when new `BatchStore` instances are created [8](#0-7) , epochs can last hours, allowing significant accumulation.

## Likelihood Explanation

**HIGH LIKELIHOOD** - This occurs naturally in distributed consensus without requiring sophisticated attacks:

**Natural Triggers:**
- Network partitions causing batch request timeouts [9](#0-8) 
- Transient connectivity issues between validators causing network errors [10](#0-9) 
- Batch expiration scenarios [11](#0-10) 
- Normal Byzantine fault scenarios with invalid responses

**Attack Amplification:**
- Malicious peers can deliberately send invalid batch responses or delay responses to trigger timeouts
- Attackers can create proposals referencing non-existent batches
- Network-level disruption can be used to maximize batch fetch failures

The vulnerability is triggered during normal consensus operation whenever batch fetching fails, making it highly likely to occur even without malicious intent.

## Recommendation

Add cleanup of subscribers when `request_batch()` fails in the `get_or_fetch_batch()` function. The fix should:

1. Wrap the `request_batch()` call in error handling that removes the subscriber on failure
2. Store the digest before calling `request_batch()` so cleanup can occur
3. On error, call `persist_subscribers.remove(&digest)` to clean up the leaked sender

Example fix approach:
```rust
let batch_digest = *batch_info.digest();
let subscriber_rx = batch_store.subscribe(batch_digest);
let result = requester.request_batch(...).await;
match result {
    Ok(payload) => {
        batch_store.persist(...);
        Ok(payload)
    }
    Err(e) => {
        // Clean up leaked subscriber
        batch_store.persist_subscribers.remove(&batch_digest);
        Err(e)
    }
}
```

Alternatively, implement a Drop guard that automatically cleans up on scope exit when the receiver is dropped without receiving a value.

## Proof of Concept

The vulnerability can be demonstrated by:
1. Forcing `request_batch()` to timeout by removing responder peers
2. Observing that `persist_subscribers` map size grows with each failed request
3. Monitoring memory usage over time showing unbounded growth

While a full PoC is not provided, the code flow is deterministic and can be traced through the citations provided above. The leak occurs on any path where line 703 returns an error before lines 704-707 execute.

**Notes**

This is a resource management bug in the consensus layer that affects validator performance. The vulnerability is particularly concerning because:

1. It accumulates silently without obvious symptoms until memory pressure becomes severe
2. Multiple validators experiencing this simultaneously can impact network consensus performance
3. The cleanup only occurs at epoch boundaries, which can be hours apart
4. Both natural network issues and malicious behavior can trigger the leak

The fix is straightforward - ensure cleanup occurs on all exit paths from `get_or_fetch_batch()`, not just the success path.

### Citations

**File:** consensus/src/quorum_store/batch_store.rs (L124-124)
```rust
    persist_subscribers: DashMap<HashValue, Vec<oneshot::Sender<PersistedValue<BatchInfoExt>>>>,
```

**File:** consensus/src/quorum_store/batch_store.rs (L129-179)
```rust
    pub(crate) fn new(
        epoch: u64,
        is_new_epoch: bool,
        last_certified_time: u64,
        db: Arc<dyn QuorumStoreStorage>,
        memory_quota: usize,
        db_quota: usize,
        batch_quota: usize,
        validator_signer: ValidatorSigner,
        expiration_buffer_usecs: u64,
    ) -> Self {
        let db_clone = db.clone();
        let batch_store = Self {
            epoch: OnceCell::with_value(epoch),
            last_certified_time: AtomicU64::new(last_certified_time),
            db_cache: DashMap::new(),
            peer_quota: DashMap::new(),
            expirations: Mutex::new(TimeExpirations::new()),
            db,
            memory_quota,
            db_quota,
            batch_quota,
            validator_signer,
            persist_subscribers: DashMap::new(),
            expiration_buffer_usecs,
        };

        if is_new_epoch {
            tokio::task::spawn_blocking(move || {
                Self::gc_previous_epoch_batches_from_db_v1(db_clone.clone(), epoch);
                Self::gc_previous_epoch_batches_from_db_v2(db_clone, epoch);
            });
        } else {
            Self::populate_cache_and_gc_expired_batches_v1(
                db_clone.clone(),
                epoch,
                last_certified_time,
                expiration_buffer_usecs,
                &batch_store,
            );
            Self::populate_cache_and_gc_expired_batches_v2(
                db_clone,
                epoch,
                last_certified_time,
                expiration_buffer_usecs,
                &batch_store,
            );
        }

        batch_store
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L358-417)
```rust
    pub(crate) fn insert_to_cache(
        &self,
        value: &PersistedValue<BatchInfoExt>,
    ) -> anyhow::Result<bool> {
        let digest = *value.digest();
        let author = value.author();
        let expiration_time = value.expiration();

        {
            // Acquire dashmap internal lock on the entry corresponding to the digest.
            let cache_entry = self.db_cache.entry(digest);

            if let Occupied(entry) = &cache_entry {
                match entry.get().expiration().cmp(&expiration_time) {
                    std::cmp::Ordering::Equal => return Ok(false),
                    std::cmp::Ordering::Greater => {
                        debug!(
                            "QS: already have the digest with higher expiration {}",
                            digest
                        );
                        return Ok(false);
                    },
                    std::cmp::Ordering::Less => {},
                }
            };
            let value_to_be_stored = if self
                .peer_quota
                .entry(author)
                .or_insert(QuotaManager::new(
                    self.db_quota,
                    self.memory_quota,
                    self.batch_quota,
                ))
                .update_quota(value.num_bytes() as usize)?
                == StorageMode::PersistedOnly
            {
                PersistedValue::new(value.batch_info().clone(), None)
            } else {
                value.clone()
            };

            match cache_entry {
                Occupied(entry) => {
                    let (k, prev_value) = entry.replace_entry(value_to_be_stored);
                    debug_assert!(k == digest);
                    self.free_quota(prev_value);
                },
                Vacant(slot) => {
                    slot.insert(value_to_be_stored);
                },
            }
        }

        // Add expiration for the inserted entry, no need to be atomic w. insertion.
        #[allow(clippy::unwrap_used)]
        {
            self.expirations.lock().add_item(digest, expiration_time);
        }
        Ok(true)
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L443-472)
```rust
    pub(crate) fn clear_expired_payload(&self, certified_time: u64) -> Vec<HashValue> {
        // To help slow nodes catch up via execution without going to state sync we keep the blocks for 60 extra seconds
        // after the expiration time. This will help remote peers fetch batches that just expired but are within their
        // execution window.
        let expiration_time = certified_time.saturating_sub(self.expiration_buffer_usecs);
        let expired_digests = self.expirations.lock().expire(expiration_time);
        let mut ret = Vec::new();
        for h in expired_digests {
            let removed_value = match self.db_cache.entry(h) {
                Occupied(entry) => {
                    // We need to check up-to-date expiration again because receiving the same
                    // digest with a higher expiration would update the persisted value and
                    // effectively extend the expiration.
                    if entry.get().expiration() <= expiration_time {
                        self.persist_subscribers.remove(entry.get().digest());
                        Some(entry.remove())
                    } else {
                        None
                    }
                },
                Vacant(_) => unreachable!("Expired entry not in cache"),
            };
            // No longer holding the lock on db_cache entry.
            if let Some(value) = removed_value {
                self.free_quota(value);
                ret.push(h);
            }
        }
        ret
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L591-602)
```rust
    fn subscribe(&self, digest: HashValue) -> oneshot::Receiver<PersistedValue<BatchInfoExt>> {
        let (tx, rx) = oneshot::channel();
        self.persist_subscribers.entry(digest).or_default().push(tx);

        // This is to account for the race where this subscribe call happens after the
        // persist call.
        if let Ok(value) = self.get_batch_from_local(&digest) {
            self.notify_subscribers(value)
        }

        rx
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L604-610)
```rust
    fn notify_subscribers(&self, value: PersistedValue<BatchInfoExt>) {
        if let Some((_, subscribers)) = self.persist_subscribers.remove(value.digest()) {
            for subscriber in subscribers {
                subscriber.send(value.clone()).ok();
            }
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L684-723)
```rust
                let fut = async move {
                    let batch_digest = *batch_info.digest();
                    defer!({
                        inflight_requests_clone.lock().remove(&batch_digest);
                    });
                    // TODO(ibalajiarun): Support V2 batch
                    if let Ok(mut value) = batch_store.get_batch_from_local(&batch_digest) {
                        Ok(value.take_payload().expect("Must have payload"))
                    } else {
                        // Quorum store metrics
                        counters::MISSED_BATCHES_COUNT.inc();
                        let subscriber_rx = batch_store.subscribe(*batch_info.digest());
                        let payload = requester
                            .request_batch(
                                batch_digest,
                                batch_info.expiration(),
                                responders,
                                subscriber_rx,
                            )
                            .await?;
                        batch_store.persist(vec![PersistedValue::new(
                            batch_info.into(),
                            Some(payload.clone()),
                        )]);
                        Ok(payload)
                    }
                }
                .boxed()
                .shared();

                tokio::spawn(fut.clone());

                BatchFetchUnit {
                    responders: responders_clone,
                    fut,
                }
            })
            .fut
            .clone()
    }
```

**File:** consensus/src/quorum_store/batch_requester.rs (L101-180)
```rust
    pub(crate) async fn request_batch(
        &self,
        digest: HashValue,
        expiration: u64,
        responders: Arc<Mutex<BTreeSet<PeerId>>>,
        mut subscriber_rx: oneshot::Receiver<PersistedValue<BatchInfoExt>>,
    ) -> ExecutorResult<Vec<SignedTransaction>> {
        let validator_verifier = self.validator_verifier.clone();
        let mut request_state = BatchRequesterState::new(responders, self.retry_limit);
        let network_sender = self.network_sender.clone();
        let request_num_peers = self.request_num_peers;
        let my_peer_id = self.my_peer_id;
        let epoch = self.epoch;
        let retry_interval = Duration::from_millis(self.retry_interval_ms as u64);
        let rpc_timeout = Duration::from_millis(self.rpc_timeout_ms as u64);

        monitor!("batch_request", {
            let mut interval = time::interval(retry_interval);
            let mut futures = FuturesUnordered::new();
            let request = BatchRequest::new(my_peer_id, epoch, digest);
            loop {
                tokio::select! {
                    _ = interval.tick() => {
                        // send batch request to a set of peers of size request_num_peers
                        if let Some(request_peers) = request_state.next_request_peers(request_num_peers) {
                            for peer in request_peers {
                                futures.push(network_sender.request_batch(request.clone(), peer, rpc_timeout));
                            }
                        } else if futures.is_empty() {
                            // end the loop when the futures are drained
                            break;
                        }
                    },
                    Some(response) = futures.next() => {
                        match response {
                            Ok(BatchResponse::Batch(batch)) => {
                                counters::RECEIVED_BATCH_RESPONSE_COUNT.inc();
                                let payload = batch.into_transactions();
                                return Ok(payload);
                            }
                            // Short-circuit if the chain has moved beyond expiration
                            Ok(BatchResponse::NotFound(ledger_info)) => {
                                counters::RECEIVED_BATCH_NOT_FOUND_COUNT.inc();
                                if ledger_info.commit_info().epoch() == epoch
                                    && ledger_info.commit_info().timestamp_usecs() > expiration
                                    && ledger_info.verify_signatures(&validator_verifier).is_ok()
                                {
                                    counters::RECEIVED_BATCH_EXPIRED_COUNT.inc();
                                    debug!("QS: batch request expired, digest:{}", digest);
                                    return Err(ExecutorError::CouldNotGetData);
                                }
                            }
                            Ok(BatchResponse::BatchV2(_)) => {
                                error!("Batch V2 response is not supported");
                            }
                            Err(e) => {
                                counters::RECEIVED_BATCH_RESPONSE_ERROR_COUNT.inc();
                                debug!("QS: batch request error, digest:{}, error:{:?}", digest, e);
                            }
                        }
                    },
                    result = &mut subscriber_rx => {
                        match result {
                            Ok(persisted_value) => {
                                counters::RECEIVED_BATCH_FROM_SUBSCRIPTION_COUNT.inc();
                                let (_, maybe_payload) = persisted_value.unpack();
                                return Ok(maybe_payload.expect("persisted value must exist"));
                            }
                            Err(err) => {
                                debug!("channel closed: {}", err);
                            }
                        };
                    },
                }
            }
            counters::RECEIVED_BATCH_REQUEST_TIMEOUT_COUNT.inc();
            debug!("QS: batch request timed out, digest:{}", digest);
            Err(ExecutorError::CouldNotGetData)
        })
    }
```
