# Audit Report

## Title
Resource Exhaustion via Indefinite Secret Share Requester Tasks Due to DropGuard Lifecycle Bug

## Summary
The `spawn_share_requester_task()` function in `SecretShareManager` spawns async tasks that retry indefinitely when secret sharing threshold is not met. DropGuards holding abort handles are only dropped when blocks complete secret sharing or during epoch reset, causing unbounded task accumulation that leads to validator node resource exhaustion.

## Finding Description

The vulnerability exists in the lifecycle management of spawned secret share requester tasks in the consensus layer's secret sharing protocol.

When processing incoming blocks, `SecretShareManager::process_incoming_blocks()` spawns a share requester task for each block. [1](#0-0) 

Each task is created by `spawn_share_requester_task()` which spawns an async task using `tokio::spawn()` (not through BoundedExecutor, so unbounded) that calls `ReliableBroadcast::multicast()`. [2](#0-1) 

The multicast operation implements an infinite retry loop with exponential backoff. When an RPC fails, it retrieves the next backoff duration and retries indefinitely until aggregation completes. [3](#0-2) 

Aggregation only completes when `total_weight >= secret_share_config.threshold()`. If insufficient validators respond (< threshold), aggregation never completes and the task continues retrying. [4](#0-3) 

The spawned tasks are managed by DropGuards that hold AbortHandles. These DropGuards are stored in `QueueItem.share_requester_handles`. [5](#0-4) 

QueueItems are only removed from the BlockQueue when `is_fully_secret_shared()` returns true, which checks if `pending_secret_key_rounds` is empty. [6](#0-5) 

When blocks never reach the threshold for secret sharing, QueueItems remain in the queue indefinitely (within an epoch), and their DropGuards are never dropped, so the spawned tasks continue running.

The exponential backoff is configured with a maximum delay of 3000ms (3 seconds), meaning tasks will retry every 3 seconds at most. [7](#0-6) 

The only cleanup occurs during epoch reset when the entire BlockQueue is replaced with a new instance, dropping all QueueItems and their DropGuards. [8](#0-7) 

**Attack Scenario:**
1. Network partition or validator downtime causes < threshold validators to respond
2. Secret sharing aggregation never completes for affected blocks
3. QueueItems remain in BlockQueue indefinitely (until epoch reset)
4. DropGuards are never dropped, so spawned tasks continue running
5. Tasks retry requests every 3 seconds maximum via exponential backoff
6. With Aptos producing ~1 block/second, hundreds of tasks accumulate within an epoch
7. Each task consumes: tokio task slot, memory for async future state, CPU for periodic retries, network bandwidth for RPC calls
8. Resource exhaustion causes validator node performance degradation
9. Blocks without completed secret sharing block downstream processing in the pipeline

This breaks the **Resource Limits** security invariant - protocol operations must respect computational and resource constraints.

## Impact Explanation

**Medium Severity** per Aptos bug bounty criteria:

- **State inconsistencies requiring intervention**: Blocks accumulate in the BlockQueue without being processed downstream, creating pipeline blockage that may require manual intervention
- **Validator node slowdowns**: Accumulated tasks consume tokio runtime resources, memory, CPU cycles for retries, and network bandwidth, degrading node performance over time
- Does NOT directly cause fund loss or consensus safety violations
- Impact is gradual - accumulates over rounds within an epoch (hours)
- Gets cleaned up automatically on epoch transition when BlockQueue is reset

The issue realistically occurs during network partitions, validator downtime, or with < 1/3 Byzantine validators withholding secret shares, making it exploitable in production environments.

## Likelihood Explanation

**Moderate to High Likelihood:**

- **Network partitions and validator downtime** are realistic operational scenarios that naturally trigger this condition
- **Byzantine validators (< 1/3)** can deliberately withhold shares to exploit this vulnerability without exceeding the Byzantine tolerance threshold
- **High block production rate**: Aptos consensus produces blocks at approximately 1 block/second, enabling rapid task accumulation (hundreds of tasks per epoch)
- **No timeout mechanism**: The multicast operation has no maximum retry count or overall timeout - it retries indefinitely until aggregation succeeds
- **Exponential backoff is unbounded**: tokio-retry's ExponentialBackoff is an infinite iterator that continues producing delays indefinitely
- **Tasks spawned without limits**: Uses `tokio::spawn()` directly rather than BoundedExecutor, so no cap on concurrent tasks

The attack does not require validator compromise or Byzantine behavior for natural occurrence (network issues suffice), though Byzantine validators can deliberately exploit it.

## Recommendation

Implement one or more of the following mitigations:

1. **Add timeout to multicast operations**: Modify `ReliableBroadcast::multicast()` to accept an optional overall timeout parameter and abort aggregation attempts that exceed it

2. **Implement maximum retry limit**: Configure ExponentialBackoff with a maximum number of retries before giving up on aggregation

3. **Add stale task cleanup**: Periodically prune QueueItems that have been pending for too long (e.g., > N rounds old) even if secret sharing hasn't completed

4. **Use BoundedExecutor**: Spawn share requester tasks through BoundedExecutor to limit concurrent tasks

5. **Add circuit breaker**: If BlockQueue size exceeds a threshold, stop spawning new share requester tasks until queue drains

Recommended fix example:
```rust
// In spawn_share_requester_task(), add timeout wrapper:
let task_with_timeout = async move {
    match tokio::time::timeout(Duration::from_secs(60), task).await {
        Ok(_) => {},
        Err(_) => warn!("Share requester task timed out"),
    }
};
```

## Proof of Concept

The vulnerability can be demonstrated by:

1. Setting up a test network with validators configured to withhold secret shares
2. Observing BlockQueue size metric (`DEC_QUEUE_SIZE`) growing unboundedly
3. Monitoring tokio task count increasing over time
4. Measuring validator node CPU/memory consumption rising as tasks accumulate
5. Verifying that tasks are not cleaned up until epoch transition

The core issue is architectural - tasks retry indefinitely without cleanup, and this can be confirmed by code inspection of the execution paths cited above without requiring a full PoC implementation.

## Notes

This is a protocol-level resource management bug, NOT a network DoS attack. The vulnerability arises from improper lifecycle management of internally-spawned async tasks within the consensus protocol. The tasks are created by the protocol itself in response to legitimate blocks, and the bug is that they accumulate without proper cleanup when aggregation conditions are not met.

The distinction from "Network DoS" (which is out of scope) is critical: this is not about flooding the network with malicious packets, but rather about the protocol's own task management failing to enforce resource limits on its internal operations.

### Citations

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L112-130)
```rust
    async fn process_incoming_blocks(&mut self, blocks: OrderedBlocks) {
        let rounds: Vec<u64> = blocks.ordered_blocks.iter().map(|b| b.round()).collect();
        info!(rounds = rounds, "Processing incoming blocks.");

        let mut share_requester_handles = Vec::new();
        let mut pending_secret_key_rounds = HashSet::new();
        for block in blocks.ordered_blocks.iter() {
            let handle = self.process_incoming_block(block).await;
            share_requester_handles.push(handle);
            pending_secret_key_rounds.insert(block.round());
        }

        let queue_item = QueueItem::new(
            blocks,
            Some(share_requester_handles),
            pending_secret_key_rounds,
        );
        self.block_queue.push_back(queue_item);
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L172-184)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.secret_share_store
            .lock()
            .update_highest_known_round(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L237-277)
```rust
    fn spawn_share_requester_task(&self, metadata: SecretShareMetadata) -> DropGuard {
        let rb = self.reliable_broadcast.clone();
        let aggregate_state = Arc::new(SecretShareAggregateState::new(
            self.secret_share_store.clone(),
            metadata.clone(),
            self.config.clone(),
        ));
        let epoch_state = self.epoch_state.clone();
        let secret_share_store = self.secret_share_store.clone();
        let task = async move {
            // TODO(ibalajiarun): Make this configurable
            tokio::time::sleep(Duration::from_millis(300)).await;
            let maybe_existing_shares = secret_share_store.lock().get_all_shares_authors(&metadata);
            if let Some(existing_shares) = maybe_existing_shares {
                let epoch = epoch_state.epoch;
                let request = RequestSecretShare::new(metadata.clone());
                let targets = epoch_state
                    .verifier
                    .get_ordered_account_addresses_iter()
                    .filter(|author| !existing_shares.contains(author))
                    .collect::<Vec<_>>();
                info!(
                    epoch = epoch,
                    round = metadata.round,
                    "[SecretShareManager] Start broadcasting share request for {}",
                    targets.len(),
                );
                rb.multicast(request, aggregate_state, targets)
                    .await
                    .expect("Broadcast cannot fail");
                info!(
                    epoch = epoch,
                    round = metadata.round,
                    "[SecretShareManager] Finish broadcasting share request",
                );
            }
        };
        let (abort_handle, abort_registration) = AbortHandle::new_pair();
        tokio::spawn(Abortable::new(task, abort_registration));
        DropGuard::new(abort_handle)
    }
```

**File:** crates/reliable-broadcast/src/lib.rs (L167-204)
```rust
            loop {
                tokio::select! {
                    Some((receiver, result)) = rpc_futures.next() => {
                        let aggregating = aggregating.clone();
                        let future = executor.spawn(async move {
                            (
                                    receiver,
                                    result
                                        .and_then(|msg| {
                                            msg.try_into().map_err(|e| anyhow::anyhow!("{:?}", e))
                                        })
                                        .and_then(|ack| aggregating.add(receiver, ack)),
                            )
                        }).await;
                        aggregate_futures.push(future);
                    },
                    Some(result) = aggregate_futures.next() => {
                        let (receiver, result) = result.expect("spawned task must succeed");
                        match result {
                            Ok(may_be_aggragated) => {
                                if let Some(aggregated) = may_be_aggragated {
                                    return Ok(aggregated);
                                }
                            },
                            Err(e) => {
                                log_rpc_failure(e, receiver);

                                let backoff_strategy = backoff_policies
                                    .get_mut(&receiver)
                                    .expect("should be present");
                                let duration = backoff_strategy.next().expect("should produce value");
                                rpc_futures
                                    .push(send_message(receiver, Some(duration)));
                            },
                        }
                    },
                    else => unreachable!("Should aggregate with all responses")
                }
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L38-46)
```rust
    pub fn try_aggregate(
        self,
        secret_share_config: &SecretShareConfig,
        metadata: SecretShareMetadata,
        decision_tx: Sender<SecretSharedKey>,
    ) -> Either<Self, SecretShare> {
        if self.total_weight < secret_share_config.threshold() {
            return Either::Left(self);
        }
```

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L16-22)
```rust
/// Maintain the ordered blocks received from consensus and corresponding secret shares
pub struct QueueItem {
    ordered_blocks: OrderedBlocks,
    offsets_by_round: HashMap<Round, usize>,
    pending_secret_key_rounds: HashSet<Round>,
    share_requester_handles: Option<Vec<DropGuard>>,
}
```

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L112-127)
```rust
    pub fn dequeue_ready_prefix(&mut self) -> Vec<OrderedBlocks> {
        let mut ready_prefix = vec![];
        while let Some((_starting_round, item)) = self.queue.first_key_value() {
            if item.is_fully_secret_shared() {
                let (_, item) = self.queue.pop_first().expect("First key must exist");
                for block in item.blocks() {
                    observe_block(block.timestamp_usecs(), BlockStage::SECRET_SHARING_READY);
                }
                let QueueItem { ordered_blocks, .. } = item;
                ready_prefix.push(ordered_blocks);
            } else {
                break;
            }
        }
        ready_prefix
    }
```

**File:** config/src/config/dag_consensus_config.rs (L112-123)
```rust
impl Default for ReliableBroadcastConfig {
    fn default() -> Self {
        Self {
            // A backoff policy that starts at 100ms and doubles each iteration up to 3secs.
            backoff_policy_base_ms: 2,
            backoff_policy_factor: 50,
            backoff_policy_max_delay_ms: 3000,

            rpc_timeout_ms: 1000,
        }
    }
}
```
