# Audit Report

## Title
Permanent Consensus Liveness Failure Due to Missing Retry Logic for ExecutorError::CouldNotGetData

## Summary
A critical architectural flaw in the consensus pipeline's BufferManager causes permanent blockchain halt when block execution fails with `ExecutorError::CouldNotGetData`. The execution phase lacks retry logic that exists for the signing phase, causing blocks to remain stuck in "Ordered" state indefinitely until epoch change or manual intervention.

## Finding Description

The consensus pipeline processes blocks through a sequential BufferManager. When execution fails with `CouldNotGetData` (a timeout error defined as "request timeout" [1](#0-0) ), a critical asymmetry in error handling causes permanent liveness failure.

**Error Sources:**
The `CouldNotGetData` error is returned when batch requests timeout [2](#0-1)  or when batches cannot be retrieved [3](#0-2)  and when batches are not found in the database [4](#0-3) .

**Flawed Error Handling:**
When `process_execution_response` receives an error, it logs the error and returns early without advancing the block's state [5](#0-4) . The block remains in "Ordered" state in the buffer.

**Ignored Retry Signal:**
The `advance_execution_root` method detects when a block hasn't progressed (cursor equals execution_root) and returns `Some(block_id)` with an explicit comment "Schedule retry" [6](#0-5) . However, this return value is completely ignored in all three locations it's called in the main event loop [7](#0-6) [8](#0-7) [9](#0-8) .

**Architectural Inconsistency:**
The signing phase properly implements retry logic using `spawn_retry_request` when the signing root hasn't advanced [10](#0-9) . The execution phase completely lacks this mechanism.

**Sequential Processing Bottleneck:**
The Buffer is implemented as a linked list structure [11](#0-10) . When one block is stuck in "Ordered" state, all subsequent blocks cannot proceed because execution is sequential. New execution requests are only sent when new blocks are initially ordered [12](#0-11) , never as retries.

**Insufficient Recovery Mechanisms:**
The periodic interval tick only updates metrics and rebroadcasts commit votes [13](#0-12)  - it does not retry execution. The only recovery paths are epoch changes (infrequent, typically hours) or manual reset signals [14](#0-13) .

## Impact Explanation

This vulnerability represents **Critical Severity** per Aptos bug bounty criteria category 4: "Total Loss of Liveness/Network Availability - Network halts due to protocol bug; All validators unable to progress."

When a block execution fails with `CouldNotGetData`:
1. The block permanently remains in "Ordered" state with no automatic retry
2. All validators that ordered this block (â‰¥2/3 of the network by consensus rules) will experience the same execution failure
3. The sequential buffer architecture prevents all subsequent blocks from executing
4. New rounds can propose blocks, but they queue behind the stuck block
5. The entire blockchain halts until epoch change (automatic but hours apart) or manual reset signal

This directly violates the liveness guarantee that is fundamental to the AptosBFT consensus protocol. The blockchain cannot make progress despite having operational validators and valid proposals.

## Likelihood Explanation

**High Likelihood** due to multiple realistic trigger paths:

**Natural Occurrence (No Attacker Required):**
- Network timeouts are common in distributed systems
- Batch expiration when blockchain progresses faster than batch data propagates
- Temporary network partitions causing batch request failures
- Database retrieval failures

**Intentional Attack (Within Threat Model):**
- A malicious validator selected as leader can propose blocks containing batches they withhold
- Requires only being selected as leader (probabilistic, based on stake)
- Does not require >1/3 Byzantine validators
- Falls within the <1/3 Byzantine fault tolerance model

The vulnerability is particularly dangerous because:
- It's a protocol-level bug, not requiring complex exploitation
- The failure is deterministic once triggered
- All validators ordering the same block will hit the same issue
- Recovery is not automatic within reasonable timeframes

## Recommendation

Implement retry logic for the execution phase consistent with the signing phase:

```rust
async fn advance_execution_root(&mut self) -> Option<HashValue> {
    let cursor = self.execution_root;
    self.execution_root = self
        .buffer
        .find_elem_from(cursor.or_else(|| *self.buffer.head_cursor()), |item| {
            item.is_ordered()
        });
    if self.execution_root.is_some() && cursor == self.execution_root {
        // Return block_id to schedule retry
        self.execution_root
    } else {
        None
    }
}
```

In the main event loop, handle the return value:

```rust
Some(response) = self.execution_wait_phase_rx.next() => {
    monitor!("buffer_manager_process_execution_wait_response", {
        self.process_execution_response(response).await;
        if let Some(block_id) = self.advance_execution_root() {
            // Schedule retry for stuck block
            let item = self.buffer.get(&Some(block_id));
            let request = self.create_new_request(ExecutionRequest {
                ordered_blocks: item.get_blocks().clone(),
            });
            let sender = self.execution_schedule_phase_tx.clone();
            Self::spawn_retry_request(sender, request, Duration::from_millis(100));
        }
        if self.signing_root.is_none() {
            self.advance_signing_root().await;
        }
    });
}
```

Additionally, implement exponential backoff for retries to avoid overwhelming the system during persistent failures.

## Proof of Concept

While a full end-to-end PoC requires a multi-validator testnet with controlled network conditions, the vulnerability can be demonstrated through code inspection:

1. The execution request is sent exactly once per block ordering [15](#0-14) 
2. The ExecutionSchedulePhase creates the execution future once [16](#0-15) 
3. If this future fails with `CouldNotGetData`, no new future is ever created
4. The return value indicating retry is needed is ignored [8](#0-7) 
5. The signing phase demonstrates the correct pattern exists in the codebase [10](#0-9)  but wasn't applied to execution

The architectural flaw is evident: the comment "Schedule retry" exists in the code [17](#0-16)  but the implementation is incomplete.

## Notes

This vulnerability demonstrates a critical oversight in error handling where the execution phase lacks retry logic that the signing phase correctly implements. The sequential nature of the buffer combined with the missing retry mechanism creates a single point of failure that can halt the entire blockchain. Recovery mechanisms (epoch changes, manual resets) exist but are insufficient for maintaining the liveness guarantees expected from a production blockchain.

### Citations

**File:** execution/executor-types/src/error.rs (L41-42)
```rust
    #[error("request timeout")]
    CouldNotGetData,
```

**File:** consensus/src/quorum_store/batch_requester.rs (L149-150)
```rust
                                    debug!("QS: batch request expired, digest:{}", digest);
                                    return Err(ExecutorError::CouldNotGetData);
```

**File:** consensus/src/quorum_store/batch_requester.rs (L176-178)
```rust
            counters::RECEIVED_BATCH_REQUEST_TIMEOUT_COUNT.inc();
            debug!("QS: batch request timed out, digest:{}", digest);
            Err(ExecutorError::CouldNotGetData)
```

**File:** consensus/src/quorum_store/batch_store.rs (L555-557)
```rust
                Ok(None) | Err(_) => {
                    warn!("Could not get batch from db");
                    Err(ExecutorError::CouldNotGetData)
```

**File:** consensus/src/pipeline/buffer_manager.rs (L397-410)
```rust
        let request = self.create_new_request(ExecutionRequest {
            ordered_blocks: ordered_blocks.clone(),
        });
        if let Some(consensus_publisher) = &self.consensus_publisher {
            let message = ConsensusObserverMessage::new_ordered_block_message(
                ordered_blocks.clone(),
                ordered_proof.clone(),
            );
            consensus_publisher.publish_message(message);
        }
        self.execution_schedule_phase_tx
            .send(request)
            .await
            .expect("Failed to send execution schedule request");
```

**File:** consensus/src/pipeline/buffer_manager.rs (L436-438)
```rust
        if self.execution_root.is_some() && cursor == self.execution_root {
            // Schedule retry.
            self.execution_root
```

**File:** consensus/src/pipeline/buffer_manager.rs (L478-480)
```rust
            if cursor == self.signing_root {
                let sender = self.signing_phase_tx.clone();
                Self::spawn_retry_request(sender, request, Duration::from_millis(100));
```

**File:** consensus/src/pipeline/buffer_manager.rs (L579-595)
```rust
    async fn process_reset_request(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        info!("Receive reset");

        match signal {
            ResetSignal::Stop => self.stop = true,
            ResetSignal::TargetRound(round) => {
                self.highest_committed_round = round;
                self.latest_round = round;

                let _ = self.drain_pending_commit_proof_till(round);
            },
        }

        self.reset().await;
        let _ = tx.send(ResetAck::default());
        info!("Reset finishes");
```

**File:** consensus/src/pipeline/buffer_manager.rs (L617-626)
```rust
        let executed_blocks = match inner {
            Ok(result) => result,
            Err(e) => {
                log_executor_error_occurred(
                    e,
                    &counters::BUFFER_MANAGER_RECEIVED_EXECUTOR_ERROR_COUNT,
                    block_id,
                );
                return;
            },
```

**File:** consensus/src/pipeline/buffer_manager.rs (L943-943)
```rust
                        self.advance_execution_root();
```

**File:** consensus/src/pipeline/buffer_manager.rs (L957-957)
```rust
                    self.advance_execution_root();
```

**File:** consensus/src/pipeline/buffer_manager.rs (L979-979)
```rust
                            self.advance_execution_root();
```

**File:** consensus/src/pipeline/buffer_manager.rs (L986-990)
```rust
                _ = interval.tick().fuse() => {
                    monitor!("buffer_manager_process_interval_tick", {
                    self.update_buffer_manager_metrics();
                    self.rebroadcast_commit_votes_if_needed().await
                    });
```

**File:** consensus/src/pipeline/buffer.rs (L8-24)
```rust
pub struct LinkedItem<T: Hashable> {
    // use option so we don't need T to be cloneable
    elem: Option<T>,
    // index is for find_element_by_key to have a starting position (similar to find_element)
    index: u64,
    next: Option<HashValue>,
}

pub type Cursor = Option<HashValue>;

/// Buffer implementes an ordered dictionary
/// It supports push_back, pop_front, and lookup by HashValue
pub struct Buffer<T: Hashable> {
    map: HashMap<HashValue, LinkedItem<T>>,
    count: u64,
    head: Cursor,
    tail: Cursor,
```

**File:** consensus/src/pipeline/execution_schedule_phase.rs (L51-80)
```rust
    async fn process(&self, req: ExecutionRequest) -> ExecutionWaitRequest {
        let ExecutionRequest { mut ordered_blocks } = req;

        let block_id = match ordered_blocks.last() {
            Some(block) => block.id(),
            None => {
                return ExecutionWaitRequest {
                    block_id: HashValue::zero(),
                    fut: Box::pin(async { Err(aptos_executor_types::ExecutorError::EmptyBlocks) }),
                }
            },
        };

        for b in &ordered_blocks {
            if let Some(tx) = b.pipeline_tx().lock().as_mut() {
                tx.rand_tx.take().map(|tx| tx.send(b.randomness().cloned()));
            }
        }

        let fut = async move {
            for b in ordered_blocks.iter_mut() {
                let (compute_result, execution_time) = b.wait_for_compute_result().await?;
                b.set_compute_result(compute_result, execution_time);
            }
            Ok(ordered_blocks)
        }
        .boxed();

        ExecutionWaitRequest { block_id, fut }
    }
```
