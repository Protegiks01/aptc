# Audit Report

## Title
Consensus Safety Violation: On-Disk Persistent Storage Lacks fsync, Allows Vote Forgetting and Equivocation After Crash

## Summary
A validator running Aptos Core with OnDiskStorage as the SafetyRules persistent storage backend can forget its most recent vote and re-sign a conflicting vote for the same round after a crash, due to missing file synchronization during critical safety data persistence. This enables consensus safety violations (equivocation and forks) under crash scenarios.

## Finding Description
The OnDiskStorage implementation writes safety data to disk without calling fsync, sync_all, or sync_data. The `write()` method creates a temporary file, writes the data, and atomically renames it to the target file, but returns success without ensuring the data has been physically written to disk. [1](#0-0) 

When a validator votes, it persists critical safety state through `PersistentSafetyStorage::set_safety_data()`, which updates the last_voted_round and last_vote. [2](#0-1) 

The voting flow in `guarded_construct_and_sign_vote_two_chain()` calls this persistence after creating the vote. [3](#0-2) 

The consensus safety check in `verify_and_update_last_vote_round()` prevents double-voting by verifying that the new round is greater than the last_voted_round stored in SafetyData. [4](#0-3) 

However, if the system crashes after the vote is created but before the OS flushes the write buffers to disk, the persisted state reverts to the pre-vote state. Upon restart, the validator will believe it never voted in that round and can sign a conflicting vote.

**OnDiskStorage is deployed in production configurations:** The production validator base configuration explicitly uses on_disk_storage as the backend. [5](#0-4) 

The docker compose validator configuration also uses on_disk_storage: [6](#0-5) 

The genesis builder also configures validators to use OnDiskStorage. [7](#0-6) 

**Configuration sanitizer allows OnDiskStorage:** The SafetyRulesConfig sanitizer only prevents InMemoryStorage on mainnet validators, but explicitly allows OnDiskStorage. [8](#0-7) 

## Impact Explanation
This is a **Critical** severity vulnerability per Aptos bug bounty criteria under "Consensus/Safety Violations (Critical)":

- **Breaks BFT Safety Invariant**: Validators can equivocally sign (double-vote) for the same round, violating the fundamental safety property that prevents conflicting blocks from both being committed. The SafetyRules module is specifically designed to prevent equivocation under all circumstances, including crash scenarios.

- **Enables Chain Forks**: With enough validators experiencing crashes and re-voting, conflicting blocks can both achieve quorum, leading to chain splits.

- **Double-Spending Risk**: Chain forks enable double-spending attacks as different validators see different transaction histories.

- **Non-Recoverable**: If exploited at scale, may require hard-fork intervention to resolve.

The vulnerability directly enables consensus safety violations. While BFT protocols tolerate Byzantine failures, the SafetyRules module's entire purpose is to ensure that honest validators never equivocate, even under crash conditions. This implementation bug defeats that protection.

## Likelihood Explanation
**Highly Likely**:

1. **Realistic Trigger**: Node crashes occur in production due to hardware failures, software panics, OOM kills, power loss, operator error (kill -9), orchestration system restarts, or cloud VM migrations.

2. **OS Write Caching**: Standard operating system behavior is to buffer file writes in memory for performance. Without explicit fsync calls, data may remain in buffers for seconds or minutes before being written to disk.

3. **No Special Access Required**: Any crash scenario triggers the vulnerability - no attacker control needed beyond observing natural crash-and-restart patterns.

4. **Production Deployment Confirmed**: The codebase evidence shows OnDiskStorage is used in production configurations despite documentation warnings against it. [9](#0-8) 

## Recommendation
The `write()` method in OnDiskStorage should call `file.sync_all()` after `write_all()` and before the `fs::rename()` operation to ensure data durability:

```rust
fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
    let contents = serde_json::to_vec(data)?;
    let mut file = File::create(self.temp_path.path())?;
    file.write_all(&contents)?;
    file.sync_all()?;  // Add this line
    fs::rename(&self.temp_path, &self.file_path)?;
    Ok(())
}
```

Additionally, production validator configurations should be updated to use Vault storage backend instead of OnDiskStorage, and the config sanitizer should prevent OnDiskStorage on mainnet validators similar to how it prevents InMemoryStorage.

## Proof of Concept
The vulnerability can be demonstrated through the following scenario:

1. Deploy a validator using the production validator-base.yaml configuration (which uses OnDiskStorage)
2. Have the validator vote on a block proposal in round N
3. Immediately after the vote is created but before OS buffer flush, send SIGKILL to the validator process
4. Restart the validator
5. Send a conflicting block proposal for round N
6. The validator will vote again on round N, creating an equivocation

The network-level equivocation detection will flag this behavior, but the damage is done - the validator has signed two conflicting votes for the same round, violating the safety invariant that SafetyRules is designed to uphold.

**Notes**

This vulnerability exists because of a gap between documentation warnings and actual deployment practices. While the README explicitly warns that OnDiskStorage "should not be used in production environments," the production Helm charts, Docker Compose configurations, and genesis builder all default to OnDiskStorage. The configuration sanitizer allows this despite blocking the similarly unsafe InMemoryStorage option.

The missing fsync is not a design choice but an implementation oversight that defeats the purpose of persistent safety storage. Even under the BFT threat model that tolerates up to f Byzantine failures, the SafetyRules module exists specifically to ensure honest validators never equivocate. This bug causes that protection to fail under normal crash scenarios.

### Citations

**File:** secure/storage/src/on_disk.rs (L64-70)
```rust
    fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
        let contents = serde_json::to_vec(data)?;
        let mut file = File::create(self.temp_path.path())?;
        file.write_all(&contents)?;
        fs::rename(&self.temp_path, &self.file_path)?;
        Ok(())
    }
```

**File:** consensus/safety-rules/src/persistent_safety_storage.rs (L150-170)
```rust
    pub fn set_safety_data(&mut self, data: SafetyData) -> Result<(), Error> {
        let _timer = counters::start_timer("set", SAFETY_DATA);
        counters::set_state(counters::EPOCH, data.epoch as i64);
        counters::set_state(counters::LAST_VOTED_ROUND, data.last_voted_round as i64);
        counters::set_state(
            counters::HIGHEST_TIMEOUT_ROUND,
            data.highest_timeout_round as i64,
        );
        counters::set_state(counters::PREFERRED_ROUND, data.preferred_round as i64);

        match self.internal_store.set(SAFETY_DATA, data.clone()) {
            Ok(_) => {
                self.cached_safety_data = Some(data);
                Ok(())
            },
            Err(error) => {
                self.cached_safety_data = None;
                Err(Error::SecureStorageUnexpectedError(error.to_string()))
            },
        }
    }
```

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L53-95)
```rust
    pub(crate) fn guarded_construct_and_sign_vote_two_chain(
        &mut self,
        vote_proposal: &VoteProposal,
        timeout_cert: Option<&TwoChainTimeoutCertificate>,
    ) -> Result<Vote, Error> {
        // Exit early if we cannot sign
        self.signer()?;

        let vote_data = self.verify_proposal(vote_proposal)?;
        if let Some(tc) = timeout_cert {
            self.verify_tc(tc)?;
        }
        let proposed_block = vote_proposal.block();
        let mut safety_data = self.persistent_storage.safety_data()?;

        // if already voted on this round, send back the previous vote
        // note: this needs to happen after verifying the epoch as we just check the round here
        if let Some(vote) = safety_data.last_vote.clone() {
            if vote.vote_data().proposed().round() == proposed_block.round() {
                return Ok(vote);
            }
        }

        // Two voting rules
        self.verify_and_update_last_vote_round(
            proposed_block.block_data().round(),
            &mut safety_data,
        )?;
        self.safe_to_vote(proposed_block, timeout_cert)?;

        // Record 1-chain data
        self.observe_qc(proposed_block.quorum_cert(), &mut safety_data);
        // Construct and sign vote
        let author = self.signer()?.author();
        let ledger_info = self.construct_ledger_info_2chain(proposed_block, vote_data.hash())?;
        let signature = self.sign(&ledger_info)?;
        let vote = Vote::new_with_signature(vote_data, author, ledger_info, signature);

        safety_data.last_vote = Some(vote.clone());
        self.persistent_storage.set_safety_data(safety_data)?;

        Ok(vote)
    }
```

**File:** consensus/safety-rules/src/safety_rules.rs (L213-232)
```rust
    pub(crate) fn verify_and_update_last_vote_round(
        &self,
        round: Round,
        safety_data: &mut SafetyData,
    ) -> Result<(), Error> {
        if round <= safety_data.last_voted_round {
            return Err(Error::IncorrectLastVotedRound(
                round,
                safety_data.last_voted_round,
            ));
        }

        safety_data.last_voted_round = round;
        trace!(
            SafetyLogSchema::new(LogEntry::LastVotedRound, LogEvent::Update)
                .last_voted_round(safety_data.last_voted_round)
        );

        Ok(())
    }
```

**File:** terraform/helm/aptos-node/files/configs/validator-base.yaml (L14-17)
```yaml
    backend:
      type: "on_disk_storage"
      path: secure-data.json
      namespace: ~
```

**File:** docker/compose/aptos-node/validator.yaml (L11-14)
```yaml
    backend:
      type: "on_disk_storage"
      path: secure-data.json
      namespace: ~
```

**File:** crates/aptos-genesis/src/builder.rs (L620-623)
```rust
        // Use a file based storage backend for safety rules
        let mut storage = OnDiskStorageConfig::default();
        storage.set_data_dir(validator.dir.clone());
        config.consensus.safety_rules.backend = SecureBackend::OnDiskStorage(storage);
```

**File:** config/src/config/safety_rules_config.rs (L85-96)
```rust
        if let Some(chain_id) = chain_id {
            // Verify that the secure backend is appropriate for mainnet validators
            if chain_id.is_mainnet()
                && node_type.is_validator()
                && safety_rules_config.backend.is_in_memory()
            {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "The secure backend should not be set to in memory storage in mainnet!"
                        .to_string(),
                ));
            }
```

**File:** secure/storage/README.md (L37-42)
```markdown
- `OnDisk`: Similar to InMemory, the OnDisk secure storage implementation provides another
useful testing implementation: an on-disk storage engine, where the storage backend is
implemented using a single file written to local disk. In a similar fashion to the in-memory
storage, on-disk should not be used in production environments as it provides no security
guarantees (e.g., encryption before writing to disk). Moreover, OnDisk storage does not
currently support concurrent data accesses.
```
