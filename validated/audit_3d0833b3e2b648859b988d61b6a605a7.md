# Audit Report

## Title
Layout Cache Pollution from Aborted Transactions Enables Consensus Divergence

## Summary
The global struct layout cache in the block executor suffers from a race condition where aborted transactions can permanently pollute the cache with stale layouts. The Vacant-only insertion strategy combined with the lack of cleanup mechanisms allows incorrect layouts to persist after module republishing, potentially causing different validators to produce different state roots for identical blocks, violating the Deterministic Execution invariant.

## Finding Description

This vulnerability exists in the interaction between parallel transaction execution, module republishing, and the global struct layout cache.

The `store_struct_layout_entry()` function uses a Vacant-only check that prevents updates once a layout is cached. [1](#0-0)  This means once a layout for a given `StructKey` is inserted, subsequent insertions are silently ignored.

Critically, `StructKey` uses `StructNameIndex`, which remains stable across module versions—the same struct name always maps to the same index. [2](#0-1)  This means layout cache entries for `struct S` from module `M v1` and `M v2` share the same cache key.

When modules are published during block execution, `flush_layout_cache()` is called to clear all cached layouts. [3](#0-2)  However, there is no cleanup mechanism for layout cache entries inserted by transactions that subsequently abort due to validation failure.

The race condition occurs because layout insertion happens during transaction execution [4](#0-3) , not after validation. This creates a timing window where:

1. Transaction T2 publishes module M v2 and flushes the layout cache
2. Transaction T3 (executing in parallel) computes layout L_v1 from M v1
3. T3 calls `store_struct_layout_entry()` **after** T2's flush but **before** T3's validation
4. Since the cache was just flushed, the entry is Vacant, so L_v1 is inserted
5. T3 validates and fails because M was overridden [5](#0-4) 
6. T3 re-executes with M v2, computes correct layout L_v2, but cannot insert it (Vacant-only check)
7. Future transactions receive stale layout L_v1 when requesting layout for structs from M v2

The `load_layout_from_cache()` function does not validate that the cached layout matches the current module version. [6](#0-5)  It re-reads modules for gas charging and validation triggering but blindly returns the cached layout regardless of whether it was computed from a different module version.

## Impact Explanation

**Critical Severity - Consensus/Safety Violation**

This vulnerability directly violates the Deterministic Execution invariant that all validators must produce identical state roots for identical blocks.

Due to the non-deterministic timing of parallel execution, different validators may experience different race outcomes:
- Validator A: T3's layout insertion occurs before T2's flush → stale layout is cleaned up
- Validator B: T3's layout insertion occurs after T2's flush → stale layout persists

When subsequent transactions serialize/deserialize structs using these layouts:
- Validator A: Uses correct layout L_v2 → produces state root R_A  
- Validator B: Uses stale layout L_v1 → produces state root R_B
- Since layouts define struct field structure, R_A ≠ R_B

This consensus divergence can lead to:
- Chain splits requiring hard fork intervention
- Permanent network partition
- Loss of finality guarantees
- Potential for double-spend attacks if validators disagree on transaction outcomes

Per Aptos Bug Bounty criteria, consensus/safety violations warrant **Critical Severity** classification.

## Likelihood Explanation

**Likelihood: Medium-High**

The vulnerability triggers when:
1. Module republishing occurs within a block (common during contract upgrades)
2. Parallel execution is enabled (default in Aptos production)
3. Specific timing: transaction stores layout after cache flush but before validation
4. The affected struct is used in subsequent transactions within the same or future blocks

While the timing window is narrow, it is realistic in production environments with high transaction throughput and parallel execution. Module upgrades are routine operations on Aptos mainnet. The likelihood increases with:
- Higher transaction volume (more parallel threads)
- Frequent module upgrades
- Popular structs used across multiple transactions

No special privileges are required—any account with sufficient gas can publish modules and trigger this condition.

## Recommendation

Implement one or more of the following mitigations:

1. **Version-aware cache keys**: Include module version or a content hash in `StructKey` to distinguish layouts from different module versions.

2. **Cleanup on abort**: When a transaction aborts during validation, clean up any layout cache entries it inserted during its aborted execution.

3. **Allow cache updates**: Modify `store_struct_layout_entry()` to allow updates when the defining modules have changed versions, not just vacant insertions.

4. **Layout validation on load**: In `load_layout_from_cache()`, recompute a hash or signature of the layout from current modules and verify it matches the cached entry before returning it.

5. **Flush on validation failure**: Immediately flush the layout cache when module validation failures are detected, before re-execution begins.

## Proof of Concept

A complete PoC would require a multi-threaded Rust test that:
1. Sets up parallel block execution with layout caching enabled
2. Transaction T1 uses struct S from module M v1
3. Transaction T2 publishes module M v2 (compatible upgrade adding a field)
4. Transaction T3 executes in parallel, triggering the race condition
5. Transaction T4 uses struct S and verifies different validators could cache different layouts
6. Demonstrates resulting state root divergence

The scenario described is technically sound based on code analysis, though a runnable PoC would strengthen the submission.

## Notes

The vulnerability is particularly insidious because:
- The race window is timing-dependent and may not occur deterministically
- Module validation correctly detects overridden modules but cannot prevent the layout pollution
- The Vacant-only pattern is typically safe for concurrent insertions but fails when combined with cache flushes and transaction aborts
- Different validators experiencing different timing will silently diverge without immediate detection

### Citations

**File:** aptos-move/block-executor/src/code_cache_global.rs (L181-190)
```rust
    pub(crate) fn store_struct_layout_entry(
        &self,
        key: &StructKey,
        entry: LayoutCacheEntry,
    ) -> PartialVMResult<()> {
        if let dashmap::Entry::Vacant(e) = self.struct_layouts.entry(*key) {
            e.insert(entry);
        }
        Ok(())
    }
```

**File:** third_party/move/move-vm/types/src/loaded_data/struct_name_indexing.rs (L46-49)
```rust
/// A data structure to cache struct identifiers (address, module name, struct name) and use
/// indices instead, to save on the memory consumption and avoid unnecessary cloning. It
/// guarantees that the same struct name identifier always corresponds to a unique index.
pub struct StructNameIndexMap(RwLock<IndexMap<StructIdentifier>>);
```

**File:** aptos-move/block-executor/src/txn_last_input_output.rs (L572-576)
```rust
        if published {
            // Record validation requirements after the modules are published.
            global_module_cache.flush_layout_cache();
            scheduler.record_validation_requirements(txn_idx, module_ids_for_v2)?;
        }
```

**File:** third_party/move/move-vm/runtime/src/storage/ty_layout_converter.rs (L118-128)
```rust
                let mut modules = DefiningModules::new();
                let layout = self.type_to_type_layout_with_delayed_fields_impl::<false>(
                    gas_meter,
                    traversal_context,
                    &mut modules,
                    ty,
                    check_option_type,
                )?;
                let cache_entry = LayoutCacheEntry::new(layout.clone(), modules);
                self.struct_definition_loader
                    .store_layout_to_cache(&key, cache_entry)?;
```

**File:** aptos-move/block-executor/src/captured_reads.rs (L1050-1089)
```rust
    pub(crate) fn validate_module_reads(
        &self,
        global_module_cache: &GlobalModuleCache<K, DC, VC, S>,
        per_block_module_cache: &SyncModuleCache<K, DC, VC, S, Option<TxnIndex>>,
        maybe_updated_module_keys: Option<&BTreeSet<K>>,
    ) -> bool {
        if self.non_delayed_field_speculative_failure {
            return false;
        }

        let validate = |key: &K, read: &ModuleRead<DC, VC, S>| match read {
            ModuleRead::GlobalCache(_) => global_module_cache.contains_not_overridden(key),
            ModuleRead::PerBlockCache(previous) => {
                let current_version = per_block_module_cache.get_module_version(key);
                let previous_version = previous.as_ref().map(|(_, version)| *version);
                current_version == previous_version
            },
        };

        match maybe_updated_module_keys {
            Some(updated_module_keys) if updated_module_keys.len() <= self.module_reads.len() => {
                // When updated_module_keys is smaller, iterate over it and lookup in module_reads
                updated_module_keys
                    .iter()
                    .filter(|&k| self.module_reads.contains_key(k))
                    .all(|key| validate(key, self.module_reads.get(key).unwrap()))
            },
            Some(updated_module_keys) => {
                // When module_reads is smaller, iterate over it and filter by updated_module_keys
                self.module_reads
                    .iter()
                    .filter(|(k, _)| updated_module_keys.contains(k))
                    .all(|(key, read)| validate(key, read))
            },
            None => self
                .module_reads
                .iter()
                .all(|(key, read)| validate(key, read)),
        }
    }
```

**File:** third_party/move/move-vm/runtime/src/storage/loader/lazy.rs (L203-221)
```rust
    fn load_layout_from_cache(
        &self,
        gas_meter: &mut impl DependencyGasMeter,
        traversal_context: &mut TraversalContext,
        key: &StructKey,
    ) -> Option<PartialVMResult<LayoutWithDelayedFields>> {
        let entry = self.module_storage.get_struct_layout(key)?;
        let (layout, modules) = entry.unpack();
        for module_id in modules.iter() {
            // Re-read all modules for this layout, so that transaction gets invalidated
            // on module publish. Also, we re-read them in exactly the same way as they
            // were traversed during layout construction, so gas charging should be exactly
            // the same as on the cache miss.
            if let Err(err) = self.charge_module(gas_meter, traversal_context, module_id) {
                return Some(Err(err));
            }
        }
        Some(Ok(layout))
    }
```
