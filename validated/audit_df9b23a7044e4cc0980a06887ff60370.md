# Audit Report

## Title
Race Condition in BlockExecutor Leads to Validator Crash During State Synchronization

## Summary
The production `BlockExecutor` implementation contains a critical race condition where `finish()` can clear internal state while `execute_and_update_state()` is executing in a background blocking task. The protection mechanism intended to prevent this race is insufficient because it does not wait for `tokio::task::spawn_blocking` tasks to complete, allowing validator nodes to panic and crash.

## Finding Description

**The Core TOCTOU Vulnerability:**

The `BlockExecutor` maintains state in an `RwLock<Option<BlockExecutorInner<V>>>`. [1](#0-0) 

The `maybe_initialize()` method checks if `inner` is `None` using a read lock, releases it, then calls `reset()` if needed. [2](#0-1) 

The `finish()` method sets `inner` to `None` without acquiring `execution_lock`. [3](#0-2) 

The `execute_and_update_state()` method calls `maybe_initialize()`, then acquires `execution_lock`, but expects `inner` to be `Some`. The `execution_lock` does NOT protect against concurrent `finish()` calls. [4](#0-3) 

**Insufficient Protection Mechanism:**

Block execution is spawned as a blocking task using `tokio::task::spawn_blocking`. [5](#0-4) 

State synchronization calls `executor.finish()` to free memory. [6](#0-5) 

The `abort_pipeline_for_state_sync()` method attempts to protect against this race by aborting pipeline tasks. [7](#0-6) 

However, when `abort()` is called on a `spawn_blocking` JoinHandle, the blocking task **continues to run** on the thread pool. The JoinHandle is marked as cancelled, and awaiting it returns immediately with an error. The `wait_until_finishes()` call completes without actually waiting for the blocking task to finish. [8](#0-7) 

**Attack Flow:**

1. Validator processes blocks through consensus pipeline
2. Block execution starts via `spawn_blocking` calling `execute_and_update_state()`
3. Node receives sync info indicating it has fallen behind
4. `fast_forward_sync` is triggered with `maybe_block_store = Some(self)` [9](#0-8) 
5. `abort_pipeline_for_state_sync()` aborts and "waits" for tasks
6. JoinHandles return immediately (cancelled), but blocking tasks continue running
7. `sync_to_target()` calls `executor.finish()`, setting `inner = None`
8. Meanwhile, the still-running blocking task accesses `inner.read().as_ref().expect()` and panics

This breaks atomicity invariants - state transitions are not properly synchronized during critical operations.

## Impact Explanation

**Severity: High** (per Aptos bug bounty criteria)

This vulnerability causes validator node crashes through panic, which qualifies as **"API crashes"** in the High Severity category (up to $50,000 reward).

**Concrete Impacts:**

1. **Validator Availability**: Affected validators panic and crash, becoming unavailable until manual restart
2. **Network Liveness Risk**: Multiple validators crashing simultaneously during sync events could impact network consensus
3. **Operational Disruption**: Node operators must manually intervene to restart crashed validators
4. **Network Resilience**: Reduces overall network resilience during synchronization periods

The panic occurs in production validator code, making this a real operational risk for the Aptos network.

## Likelihood Explanation

**Likelihood: Medium**

**Factors Increasing Likelihood:**
- State synchronization is triggered when validators fall behind peers (normal network operation)
- Network delays, partitions, or high load naturally trigger sync operations
- Block execution uses `spawn_blocking`, which cannot be pre-emptively cancelled
- The protection mechanism has a fundamental flaw - it doesn't wait for blocking tasks
- Race window exists across multiple methods that access `inner`

**Factors Decreasing Likelihood:**
- Requires precise timing between ongoing execution and state sync trigger
- Well-connected validators may not frequently fall behind
- The race window may be narrow during typical low-latency operation

**Realistic Trigger Scenarios:**
- Validator falling behind due to network issues during active consensus
- High load periods where sync is needed while processing blocks
- Brief network partitions causing rapid sync/execute overlap

The Medium likelihood is appropriate because while the vulnerability requires specific timing, the triggering conditions (falling behind during execution) occur naturally in distributed networks.

## Recommendation

Fix the race condition by ensuring proper synchronization between `finish()` and execution operations:

**Option 1**: Make `finish()` acquire `execution_lock` before clearing state:
```rust
fn finish(&self) {
    let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "finish"]);
    let _exec_guard = self.execution_lock.lock();  // Add this
    *self.inner.write() = None;
}
```

**Option 2**: Implement proper cancellation for blocking tasks by using a cancellation token that blocking tasks check periodically, or refactor to use async execution that can be properly cancelled.

**Option 3**: Ensure `abort_pipeline_for_state_sync()` actually waits for blocking tasks to complete by not relying on aborted JoinHandles.

## Proof of Concept

Due to the nature of race conditions, a PoC requires precise timing control. The vulnerability can be demonstrated by:

1. Starting block execution via the consensus pipeline
2. Immediately triggering state sync (simulating node falling behind)
3. Observing the panic when `execute_and_update_state()` accesses cleared state

The race is inherent in the code structure where:
- `spawn_blocking` tasks cannot be pre-emptively cancelled
- `abort()` + `await` on blocking JoinHandles doesn't wait for task completion  
- `finish()` doesn't synchronize with `execution_lock`
- `execute_and_update_state()` expects `inner` to remain valid after `maybe_initialize()`

A full reproduction would require instrumenting the code to force the race window, but the vulnerability is evident from the code structure and Tokio's documented behavior for `spawn_blocking` cancellation.

**Notes:**

The report mentions a recovery scenario where `maybe_block_store = None` skips protection. However, during recovery mode, `RecoveryManager` runs instead of `RoundManager`, so there's no concurrent block execution. That specific scenario is not exploitable. The vulnerability is real during **normal consensus operation** when the node falls behind and triggers state sync while actively executing blocks. [10](#0-9)

### Citations

**File:** execution/executor/src/block_executor/mod.rs (L49-53)
```rust
pub struct BlockExecutor<V> {
    pub db: DbReaderWriter,
    inner: RwLock<Option<BlockExecutorInner<V>>>,
    execution_lock: Mutex<()>,
}
```

**File:** execution/executor/src/block_executor/mod.rs (L67-72)
```rust
    fn maybe_initialize(&self) -> Result<()> {
        if self.inner.read().is_none() {
            self.reset()?;
        }
        Ok(())
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L97-113)
```rust
    fn execute_and_update_state(
        &self,
        block: ExecutableBlock,
        parent_block_id: HashValue,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> ExecutorResult<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "execute_and_state_checkpoint"]);

        self.maybe_initialize()?;
        // guarantee only one block being executed at a time
        let _guard = self.execution_lock.lock();
        self.inner
            .read()
            .as_ref()
            .expect("BlockExecutor is not reset")
            .execute_and_update_state(block, parent_block_id, onchain_config)
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L151-155)
```rust
    fn finish(&self) {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "finish"]);

        *self.inner.write() = None;
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L857-867)
```rust
        tokio::task::spawn_blocking(move || {
            executor
                .execute_and_update_state(
                    (block.id(), txns, auxiliary_info).into(),
                    block.parent_id(),
                    onchain_execution_config,
                )
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
```

**File:** consensus/src/state_computer.rs (L141-141)
```rust
        self.executor.finish();
```

**File:** consensus/src/block_storage/block_store.rs (L617-627)
```rust
    pub async fn abort_pipeline_for_state_sync(&self) {
        let blocks = self.inner.read().get_all_blocks();
        // the blocks are not ordered by round here, so we need to abort all then wait
        let futs: Vec<_> = blocks
            .into_iter()
            .filter_map(|b| b.abort_pipeline())
            .collect();
        for f in futs {
            f.wait_until_finishes().await;
        }
    }
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L104-113)
```rust
    pub async fn wait_until_finishes(self) {
        let _ = join5(
            self.execute_fut,
            self.ledger_update_fut,
            self.pre_commit_fut,
            self.commit_ledger_fut,
            self.notify_state_sync_fut,
        )
        .await;
    }
```

**File:** consensus/src/block_storage/sync_manager.rs (L295-307)
```rust
        let (root, root_metadata, blocks, quorum_certs) = Self::fast_forward_sync(
            &highest_quorum_cert,
            &highest_commit_cert,
            retriever,
            self.storage.clone(),
            self.execution_client.clone(),
            self.payload_manager.clone(),
            self.order_vote_enabled,
            self.window_size,
            Some(self),
        )
        .await?
        .take();
```

**File:** consensus/src/recovery_manager.rs (L104-114)
```rust
        let recovery_data = BlockStore::fast_forward_sync(
            sync_info.highest_quorum_cert(),
            sync_info.highest_commit_cert(),
            &mut retriever,
            self.storage.clone(),
            self.execution_client.clone(),
            self.payload_manager.clone(),
            self.order_vote_enabled,
            self.window_size,
            None,
        )
```
