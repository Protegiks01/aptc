# Audit Report

## Title
Epoch Boundary Consensus Safety Violation Due to Inconsistent Historical Proposer Data

## Summary
At epoch transitions, validators with different historical data availability (due to pruning, fast sync, or database errors) can elect different proposers for the same round, causing a consensus safety violation. The issue stems from fallback logic that silently uses only the current epoch when historical epoch data fetch fails, while other validators successfully use multiple epochs of history.

## Finding Description

The vulnerability exists in the leader reputation-based proposer election mechanism used by Aptos consensus. When validators start a new epoch, they create a proposer election handler that attempts to fetch historical validator data from previous epochs. [1](#0-0) 

The critical flaw occurs in the `extract_epoch_proposers` method's error handling. When `get_epoch_ending_ledger_infos` fails (due to pruned data, database corruption, or missing historical information), the code falls back to using **only the current epoch's validators** instead of propagating the error or ensuring consistency across all validators. [2](#0-1) 

This creates a divergence scenario where:
1. **Validator A** (full historical data): Creates `epoch_to_proposers` map with epochs {N, N-1, N-2, N-3, N-4}
2. **Validator B** (pruned/missing data): Falls back to `epoch_to_proposers` map with only epoch {N}

When calculating proposer weights, the historical event filtering only considers events from epochs present in the `epoch_to_proposers` map. [3](#0-2) 

This filtering causes validators with different epoch maps to count different historical events, leading to different reputation weights being assigned to each validator. [4](#0-3) 

Even with identical seeds, different weights result in different proposer selections via the deterministic weighted random selection. [5](#0-4) [6](#0-5) 

**Exploitation Scenario:**
This can occur naturally (no malicious intent required) when:
- Validators have different pruning window configurations (default is 90M versions, but `use_history_from_previous_epoch_max_count` defaults to 5 epochs)
- A validator recently fast-synced and lacks historical epoch-ending ledger infos
- Database issues or state sync lag causes incomplete historical data [7](#0-6) [8](#0-7) 

## Impact Explanation

This vulnerability constitutes a **CRITICAL SEVERITY** consensus safety violation according to Aptos bug bounty criteria:

1. **Consensus Safety Broken**: Different validators elect different proposers for the same round, violating the fundamental requirement that all honest validators must agree on the leader
2. **Vote Splitting**: Validators expecting different proposers will vote for different blocks, preventing quorum formation
3. **Liveness Failure**: Consensus rounds may stall indefinitely if votes are split across multiple candidates
4. **Potential Chain Split**: In extreme cases with network partitions, different validator sets could commit divergent blocks

This directly breaks consensus safety - "AptosBFT must prevent double-spending and chain splits under < 1/3 Byzantine". The issue occurs without any Byzantine behavior, purely through operational configuration differences.

The impact qualifies as **"Consensus/Safety Violations"** under Critical Severity ($1,000,000 tier) as it can cause consensus failure with less than 1/3 Byzantine validators.

## Likelihood Explanation

**LIKELIHOOD: HIGH**

This vulnerability has high likelihood of occurring in production:

1. **Heterogeneous Deployments**: Different validators may have different pruning configurations based on their operational requirements (archival nodes vs. regular validators)

2. **Default Configuration Mismatch**: The default ledger pruner window (90M versions) may not cover the full history required by `use_history_from_previous_epoch_max_count` (5 epochs), creating an inherent risk

3. **Fast Sync Operations**: Validators bootstrapping via fast sync will not have historical epoch-ending ledger infos prior to their sync point

4. **Network Growth**: As new validators join the network at different times, they will have varying amounts of historical data

5. **No Warning Mechanism**: The error is logged but consensus continues normally, making the divergence silent and difficult to detect until consensus stalls

The bug requires no attacker action - it emerges naturally from operational diversity in validator configurations.

## Recommendation

The issue should be fixed by ensuring all validators use the same `epoch_to_proposers` map. Options include:

1. **Propagate the error**: If historical data cannot be fetched, halt epoch start and require state sync to complete before participating in consensus
2. **Consensus on history**: Include a hash of the `epoch_to_proposers` map in the epoch state and validate it matches across all validators
3. **Mandatory retention**: Enforce that all validators must retain at least `use_history_from_previous_epoch_max_count` epochs of history before participating
4. **Fallback to simpler election**: If historical data is unavailable, fall back to a simpler proposer election mechanism (RotatingProposer) that doesn't require history, and ensure ALL validators use this fallback

The recommended fix is option 1, with validation added:

```rust
fn extract_epoch_proposers(...) -> Result<HashMap<u64, Vec<AccountAddress>>> {
    if epoch_state.epoch > first_epoch_to_consider {
        self.storage
            .aptos_db()
            .get_epoch_ending_ledger_infos(first_epoch_to_consider - 1, epoch_state.epoch)
            .map_err(Into::into)
            .and_then(|proof| {
                ensure!(...);
                extract_epoch_to_proposers(proof, epoch_state.epoch, &proposers, needed_rounds)
            })
            // REMOVE the unwrap_or_else fallback
            // Let the error propagate up and halt consensus start
    } else {
        Ok(HashMap::from([(epoch_state.epoch, proposers)]))
    }
}
```

## Proof of Concept

The vulnerability can be demonstrated by:

1. Configure two validator nodes with different pruning windows
2. Run Validator A with ledger pruner window covering 5+ epochs
3. Run Validator B with ledger pruner window covering only 2 epochs  
4. Wait for epoch transition
5. Observe that both validators log their selected proposers for the same round
6. Verify that the proposers differ due to different weight calculations

The divergence will manifest as validators voting for different blocks, preventing quorum formation and causing consensus to stall.

**Notes**

This is a genuine consensus safety vulnerability that can occur without any Byzantine behavior. The silent fallback logic at the core of the issue allows validators to continue with inconsistent state, leading to inevitable consensus failure. The vulnerability is particularly dangerous because it appears to "work" until the first round where the weight differences cause different proposer selections, at which point consensus irrecoverably stalls for that round.

### Citations

**File:** consensus/src/epoch_manager.rs (L287-407)
```rust
    fn create_proposer_election(
        &self,
        epoch_state: &EpochState,
        onchain_config: &OnChainConsensusConfig,
    ) -> Arc<dyn ProposerElection + Send + Sync> {
        let proposers = epoch_state
            .verifier
            .get_ordered_account_addresses_iter()
            .collect::<Vec<_>>();
        match &onchain_config.proposer_election_type() {
            ProposerElectionType::RotatingProposer(contiguous_rounds) => {
                Arc::new(RotatingProposer::new(proposers, *contiguous_rounds))
            },
            // We don't really have a fixed proposer!
            ProposerElectionType::FixedProposer(contiguous_rounds) => {
                let proposer = choose_leader(proposers);
                Arc::new(RotatingProposer::new(vec![proposer], *contiguous_rounds))
            },
            ProposerElectionType::LeaderReputation(leader_reputation_type) => {
                let (
                    heuristic,
                    window_size,
                    weight_by_voting_power,
                    use_history_from_previous_epoch_max_count,
                ) = match &leader_reputation_type {
                    LeaderReputationType::ProposerAndVoter(proposer_and_voter_config)
                    | LeaderReputationType::ProposerAndVoterV2(proposer_and_voter_config) => {
                        let proposer_window_size = proposers.len()
                            * proposer_and_voter_config.proposer_window_num_validators_multiplier;
                        let voter_window_size = proposers.len()
                            * proposer_and_voter_config.voter_window_num_validators_multiplier;
                        let heuristic: Box<dyn ReputationHeuristic> =
                            Box::new(ProposerAndVoterHeuristic::new(
                                self.author,
                                proposer_and_voter_config.active_weight,
                                proposer_and_voter_config.inactive_weight,
                                proposer_and_voter_config.failed_weight,
                                proposer_and_voter_config.failure_threshold_percent,
                                voter_window_size,
                                proposer_window_size,
                                leader_reputation_type.use_reputation_window_from_stale_end(),
                            ));
                        (
                            heuristic,
                            std::cmp::max(proposer_window_size, voter_window_size),
                            proposer_and_voter_config.weight_by_voting_power,
                            proposer_and_voter_config.use_history_from_previous_epoch_max_count,
                        )
                    },
                };

                let seek_len = onchain_config.leader_reputation_exclude_round() as usize
                    + onchain_config.max_failed_authors_to_store()
                    + PROPOSER_ROUND_BEHIND_STORAGE_BUFFER;

                let backend = Arc::new(AptosDBBackend::new(
                    window_size,
                    seek_len,
                    self.storage.aptos_db(),
                ));
                let voting_powers: Vec<_> = if weight_by_voting_power {
                    proposers
                        .iter()
                        .map(|p| {
                            epoch_state
                                .verifier
                                .get_voting_power(p)
                                .expect("INVARIANT VIOLATION: proposer not in verifier set")
                        })
                        .collect()
                } else {
                    vec![1; proposers.len()]
                };

                let epoch_to_proposers = self.extract_epoch_proposers(
                    epoch_state,
                    use_history_from_previous_epoch_max_count,
                    proposers,
                    (window_size + seek_len) as u64,
                );

                info!(
                    "Starting epoch {}: proposers across epochs for leader election: {:?}",
                    epoch_state.epoch,
                    epoch_to_proposers
                        .iter()
                        .map(|(epoch, proposers)| (epoch, proposers.len()))
                        .sorted()
                        .collect::<Vec<_>>()
                );

                let proposer_election = Box::new(LeaderReputation::new(
                    epoch_state.epoch,
                    epoch_to_proposers,
                    voting_powers,
                    backend,
                    heuristic,
                    onchain_config.leader_reputation_exclude_round(),
                    leader_reputation_type.use_root_hash_for_seed(),
                    self.config.window_for_chain_health,
                ));
                // LeaderReputation is not cheap, so we can cache the amount of rounds round_manager needs.
                Arc::new(CachedProposerElection::new(
                    epoch_state.epoch,
                    proposer_election,
                    onchain_config.max_failed_authors_to_store()
                        + PROPOSER_ELECTION_CACHING_WINDOW_ADDITION,
                ))
            },
            ProposerElectionType::RoundProposer(round_proposers) => {
                // Hardcoded to the first proposer
                let default_proposer = proposers
                    .first()
                    .expect("INVARIANT VIOLATION: proposers is empty");
                Arc::new(RoundProposer::new(
                    round_proposers.clone(),
                    *default_proposer,
                ))
            },
        }
    }
```

**File:** consensus/src/epoch_manager.rs (L409-449)
```rust
    fn extract_epoch_proposers(
        &self,
        epoch_state: &EpochState,
        use_history_from_previous_epoch_max_count: u32,
        proposers: Vec<AccountAddress>,
        needed_rounds: u64,
    ) -> HashMap<u64, Vec<AccountAddress>> {
        // Genesis is epoch=0
        // First block (after genesis) is epoch=1, and is the only block in that epoch.
        // It has no votes, so we skip it unless we are in epoch 1, as otherwise it will
        // skew leader elections for exclude_round number of rounds.
        let first_epoch_to_consider = std::cmp::max(
            if epoch_state.epoch == 1 { 1 } else { 2 },
            epoch_state
                .epoch
                .saturating_sub(use_history_from_previous_epoch_max_count as u64),
        );
        // If we are considering beyond the current epoch, we need to fetch validators for those epochs
        if epoch_state.epoch > first_epoch_to_consider {
            self.storage
                .aptos_db()
                .get_epoch_ending_ledger_infos(first_epoch_to_consider - 1, epoch_state.epoch)
                .map_err(Into::into)
                .and_then(|proof| {
                    ensure!(
                        proof.ledger_info_with_sigs.len() as u64
                            == (epoch_state.epoch - (first_epoch_to_consider - 1))
                    );
                    extract_epoch_to_proposers(proof, epoch_state.epoch, &proposers, needed_rounds)
                })
                .unwrap_or_else(|err| {
                    error!(
                        "Couldn't create leader reputation with history across epochs, {:?}",
                        err
                    );
                    HashMap::from([(epoch_state.epoch, proposers)])
                })
        } else {
            HashMap::from([(epoch_state.epoch, proposers)])
        }
    }
```

**File:** consensus/src/liveness/leader_reputation.rs (L297-326)
```rust
    fn history_iter<'a>(
        history: &'a [NewBlockEvent],
        epoch_to_candidates: &'a HashMap<u64, Vec<Author>>,
        window_size: usize,
        from_stale_end: bool,
    ) -> impl Iterator<Item = &'a NewBlockEvent> {
        let sub_history = if from_stale_end {
            let start = if history.len() > window_size {
                history.len() - window_size
            } else {
                0
            };

            &history[start..]
        } else {
            if let (Some(first), Some(last)) = (history.first(), history.last()) {
                assert!((first.epoch(), first.round()) >= (last.epoch(), last.round()));
            }
            let end = if history.len() > window_size {
                window_size
            } else {
                history.len()
            };

            &history[..end]
        };
        sub_history
            .iter()
            .filter(move |&meta| epoch_to_candidates.contains_key(&meta.epoch()))
    }
```

**File:** consensus/src/liveness/leader_reputation.rs (L521-553)
```rust
impl ReputationHeuristic for ProposerAndVoterHeuristic {
    fn get_weights(
        &self,
        epoch: u64,
        epoch_to_candidates: &HashMap<u64, Vec<Author>>,
        history: &[NewBlockEvent],
    ) -> Vec<u64> {
        assert!(epoch_to_candidates.contains_key(&epoch));

        let (votes, proposals, failed_proposals) =
            self.aggregation
                .get_aggregated_metrics(epoch_to_candidates, history, &self.author);

        epoch_to_candidates[&epoch]
            .iter()
            .map(|author| {
                let cur_votes = *votes.get(author).unwrap_or(&0);
                let cur_proposals = *proposals.get(author).unwrap_or(&0);
                let cur_failed_proposals = *failed_proposals.get(author).unwrap_or(&0);

                if cur_failed_proposals * 100
                    > (cur_proposals + cur_failed_proposals) * self.failure_threshold_percent
                {
                    self.failed_weight
                } else if cur_proposals > 0 || cur_votes > 0 {
                    self.active_weight
                } else {
                    self.inactive_weight
                }
            })
            .collect()
    }
}
```

**File:** consensus/src/liveness/leader_reputation.rs (L695-734)
```rust
impl ProposerElection for LeaderReputation {
    fn get_valid_proposer_and_voting_power_participation_ratio(
        &self,
        round: Round,
    ) -> (Author, VotingPowerRatio) {
        let target_round = round.saturating_sub(self.exclude_round);
        let (sliding_window, root_hash) = self.backend.get_block_metadata(self.epoch, target_round);
        let voting_power_participation_ratio =
            self.compute_chain_health_and_add_metrics(&sliding_window, round);
        let mut weights =
            self.heuristic
                .get_weights(self.epoch, &self.epoch_to_proposers, &sliding_window);
        let proposers = &self.epoch_to_proposers[&self.epoch];
        assert_eq!(weights.len(), proposers.len());

        // Multiply weights by voting power:
        let stake_weights: Vec<u128> = weights
            .iter_mut()
            .enumerate()
            .map(|(i, w)| *w as u128 * self.voting_powers[i] as u128)
            .collect();

        let state = if self.use_root_hash {
            [
                root_hash.to_vec(),
                self.epoch.to_le_bytes().to_vec(),
                round.to_le_bytes().to_vec(),
            ]
            .concat()
        } else {
            [
                self.epoch.to_le_bytes().to_vec(),
                round.to_le_bytes().to_vec(),
            ]
            .concat()
        };

        let chosen_index = choose_index(stake_weights, state);
        (proposers[chosen_index], voting_power_participation_ratio)
    }
```

**File:** consensus/src/liveness/proposer_election.rs (L49-69)
```rust
pub(crate) fn choose_index(mut weights: Vec<u128>, state: Vec<u8>) -> usize {
    let mut total_weight = 0;
    // Create cumulative weights vector
    // Since we own the vector, we can safely modify it in place
    for w in &mut weights {
        total_weight = total_weight
            .checked_add(w)
            .expect("Total stake shouldn't exceed u128::MAX");
        *w = total_weight;
    }
    let chosen_weight = next_in_range(state, total_weight);
    weights
        .binary_search_by(|w| {
            if *w <= chosen_weight {
                Ordering::Less
            } else {
                Ordering::Greater
            }
        })
        .expect_err("Comparison never returns equals, so it's always guaranteed to be error")
}
```

**File:** config/src/config/storage_config.rs (L387-395)
```rust
impl Default for LedgerPrunerConfig {
    fn default() -> Self {
        LedgerPrunerConfig {
            enable: true,
            prune_window: 90_000_000,
            batch_size: 5_000,
            user_pruning_window_offset: 200_000,
        }
    }
```

**File:** types/src/on_chain_config/consensus_config.rs (L490-506)
```rust
                    active_weight: 1000,
                    inactive_weight: 10,
                    failed_weight: 1,
                    failure_threshold_percent: 10, // = 10%
                    // In each round we get stastics for the single proposer
                    // and large number of validators. So the window for
                    // the proposers needs to be significantly larger
                    // to have enough useful statistics.
                    proposer_window_num_validators_multiplier: 10,
                    voter_window_num_validators_multiplier: 1,
                    weight_by_voting_power: true,
                    use_history_from_previous_epoch_max_count: 5,
                }),
            ),
        }
    }
}
```
