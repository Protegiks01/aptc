# Audit Report

## Title
Non-Atomic Event Pruning Causes Database Inconsistency When Internal Indexer is Enabled

## Summary
The event pruning operation in `EventStorePruner` writes to two separate RocksDB database instances non-atomically when the internal indexer is enabled. This creates a window where a crash can leave EventSchema (actual events) and EventByKeySchema/EventByVersionSchema (event indices) in an inconsistent state.

## Finding Description

When the internal indexer is enabled, EventByKeySchema and EventByVersionSchema are stored in a separate RocksDB instance (the internal indexer database), while EventSchema remains in the event_db. [1](#0-0) 

The pruning logic performs two separate, non-atomic database writes: [2](#0-1) 

When internal indexer is enabled, the execution flow creates separate batches (lines 47-54), where `indexer_batch` receives index deletions and `batch` receives event deletions. These are written to different databases:
- Line 78: Writes `indexer_batch` to internal indexer database
- Line 80: Writes `batch` to event_db

If a crash (power failure, OOM, SIGKILL) occurs between these two writes, the system enters an inconsistent state where indices are deleted but events remain.

Event queries depend on these indices being consistent with the events: [3](#0-2) 

When indices are missing but events exist, queries fail with error messages "First requested event is probably pruned" or "DB corruption: Sequence number not continuous" even though the events still exist in the database.

## Impact Explanation

This qualifies as **MEDIUM severity** according to Aptos Bug Bounty criteria, as it causes "State inconsistencies requiring manual intervention":

1. **Database Inconsistency**: Event indices and actual events become desynchronized
2. **Query Failures**: Event queries by key will fail unexpectedly when indices are missing
3. **Manual Intervention Required**: Recovery requires rebuilding indices from scratch, rolling back to last consistent checkpoint, or manual database repair

The issue does not reach HIGH/CRITICAL severity because:
- Events are metadata, not part of consensus state
- Does not affect consensus safety or liveness
- Does not cause fund loss or permanent network issues
- APIs return errors rather than crashing the process

## Likelihood Explanation

**MEDIUM likelihood**:

**Trigger Conditions:**
1. Internal indexer enabled (`event_enabled() == true`)
2. Pruning active (`ledger_pruner_config.enable == true`)
3. Crash during narrow window between database writes

**Likelihood Factors:**
- Pruning runs periodically on archive nodes and validators with pruning enabled
- Window is narrow (microseconds to milliseconds) but occurs repeatedly
- Any crash source qualifies: power failure, OOM, process termination
- Eventually occurs given sufficient runtime across distributed nodes

## Recommendation

Implement atomic writes across both databases using one of these approaches:

1. **Single Database Solution**: Store all event-related schemas (EventSchema, EventByKeySchema, EventByVersionSchema) in the same database instance when internal indexer is enabled, eliminating the cross-database transaction issue.

2. **Two-Phase Commit**: Implement a two-phase commit protocol or write-ahead log to ensure atomicity across both databases.

3. **Write Ordering**: Write to event_db first (which contains the actual events), then to the indexer database. On recovery, detect missing indices and rebuild them from existing events. This makes the operation idempotent.

4. **Transaction Log**: Maintain a transaction log of pruning operations that can be replayed on restart to detect and repair inconsistencies.

## Proof of Concept

No executable PoC provided. The vulnerability is demonstrated through code analysis showing two non-atomic database writes that can be interrupted by process termination.

## Notes

1. **Severity Classification**: While the report claims HIGH severity, this more accurately fits MEDIUM severity as defined by the bug bounty program. The impact is "state inconsistencies requiring manual intervention" rather than consensus violations or API process crashes.

2. **Scope**: Events are auxiliary metadata for queries, not part of the consensus-critical state. The inconsistency affects node reliability and query functionality but does not compromise blockchain safety or liveness.

3. **Production Impact**: This is a legitimate reliability issue that can occur in production environments running both internal indexer and pruning, requiring operational awareness and monitoring.

4. **Recovery**: The system can recover through manual intervention without requiring a hardfork or network-wide coordination, distinguishing this from critical consensus issues.

### Citations

**File:** storage/indexer_schemas/src/schema/mod.rs (L40-51)
```rust
pub fn internal_indexer_column_families() -> Vec<ColumnFamilyName> {
    vec![
        /* empty cf */ DEFAULT_COLUMN_FAMILY_NAME,
        INTERNAL_INDEXER_METADATA_CF_NAME,
        EVENT_BY_KEY_CF_NAME,
        EVENT_BY_VERSION_CF_NAME,
        ORDERED_TRANSACTION_BY_ACCOUNT_CF_NAME,
        STATE_KEYS_CF_NAME,
        TRANSLATED_V1_EVENT_CF_NAME,
        EVENT_SEQUENCE_NUMBER_CF_NAME,
    ]
}
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L43-81)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        let mut indexer_batch = None;

        let indices_batch = if let Some(indexer_db) = self.indexer_db() {
            if indexer_db.event_enabled() {
                indexer_batch = Some(SchemaBatch::new());
            }
            indexer_batch.as_mut()
        } else {
            Some(&mut batch)
        };
        let num_events_per_version = self.ledger_db.event_db().prune_event_indices(
            current_progress,
            target_version,
            indices_batch,
        )?;
        self.ledger_db.event_db().prune_events(
            num_events_per_version,
            current_progress,
            target_version,
            &mut batch,
        )?;
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::EventPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        if let Some(mut indexer_batch) = indexer_batch {
            indexer_batch.put::<InternalIndexerMetadataSchema>(
                &IndexerMetadataKey::EventPrunerProgress,
                &IndexerMetadataValue::Version(target_version),
            )?;
            self.expect_indexer_db()
                .get_inner_db_ref()
                .write_schemas(indexer_batch)?;
        }
        self.ledger_db.event_db().write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/event_store/mod.rs (L107-143)
```rust
    pub fn lookup_events_by_key(
        &self,
        event_key: &EventKey,
        start_seq_num: u64,
        limit: u64,
        ledger_version: u64,
    ) -> Result<
        Vec<(
            u64,     // sequence number
            Version, // transaction version it belongs to
            u64,     // index among events for the same transaction
        )>,
    > {
        let mut iter = self.event_db.iter::<EventByKeySchema>()?;
        iter.seek(&(*event_key, start_seq_num))?;

        let mut result = Vec::new();
        let mut cur_seq = start_seq_num;
        for res in iter.take(limit as usize) {
            let ((path, seq), (ver, idx)) = res?;
            if path != *event_key || ver > ledger_version {
                break;
            }
            if seq != cur_seq {
                let msg = if cur_seq == start_seq_num {
                    "First requested event is probably pruned."
                } else {
                    "DB corruption: Sequence number not continuous."
                };
                db_other_bail!("{} expected: {}, actual: {}", msg, cur_seq, seq);
            }
            result.push((seq, ver, idx));
            cur_seq += 1;
        }

        Ok(result)
    }
```
