# Audit Report

## Title
Proof Version Manipulation DoS Attack via Historical Proof Regeneration Bypass

## Summary
A Byzantine peer can repeatedly request transaction outputs with an arbitrarily old `proof_version` that references pruned accumulator nodes, forcing the storage service to perform expensive I/O operations before failing at proof generation. The validation logic fails to check if the `proof_version` is within the available (non-pruned) range, and because validation passes, the malicious peer is never marked as unhealthy, allowing unlimited repetition of this resource exhaustion attack.

## Finding Description

The `TransactionOutputsWithProofRequest` struct contains a `proof_version` field that specifies which ledger version should be used to generate cryptographic proofs for the requested transaction outputs. [1](#0-0) 

When a storage service receives such a request, it validates using the `can_service_transaction_outputs_with_proof` method, which checks whether transaction outputs in the requested range are available and whether a proof can be created at the proof_version. [2](#0-1) 

The critical flaw is in the `can_create_proof` method, which only verifies that the `proof_version` is less than or equal to the synced ledger info version. **The validation does NOT check if the `proof_version` is greater than or equal to the minimum available version after pruning.** [3](#0-2) 

The Aptos blockchain implements a transaction accumulator pruner that deletes old accumulator nodes from the database. [4](#0-3) 

After validation passes, the storage service processes the request by fetching all transaction data BEFORE attempting to generate the proof. The critical sequence in `get_transaction_outputs_with_proof_by_size` creates expensive database iterators, performs extensive I/O to fetch all data items, and only after all I/O is complete, attempts to generate the accumulator range proof. [5](#0-4) [6](#0-5) [7](#0-6) 

When proof generation attempts to read pruned accumulator nodes, the `HashReader::get` implementation returns an error with message "{position} does not exist". [8](#0-7) 

The `get_transaction_accumulator_range_proof` method only checks if the first_version (transaction data) is pruned using `error_if_ledger_pruned`, but it does NOT check if the proof_version (ledger_version parameter) accumulator nodes are pruned. [9](#0-8) 

Because the request passed validation, the peer is NOT marked as unhealthy by the request moderator. The moderator only increments the invalid request count when `storage_server_summary.can_service()` returns false during validation. [10](#0-9) 

Processing errors that occur after validation passes do not increment the invalid request count, as validation occurs before processing. [11](#0-10) 

**Attack Scenario:**
1. Attacker identifies the current synced version (e.g., 10,000) and knows pruning has occurred for versions < 1,000
2. Attacker sends: `TransactionOutputsWithProofRequest { proof_version: 100, start_version: 5000, end_version: 6000 }`
3. Validation passes (100 â‰¤ 10,000 and outputs [5000-6000] are available)
4. Server performs expensive I/O to fetch ~1000 transaction outputs
5. Proof generation fails (accumulator nodes at version 100 are pruned)
6. Error returned to attacker, but peer remains healthy
7. Attacker repeats indefinitely, causing disk thrashing and CPU exhaustion

## Impact Explanation

This vulnerability enables a resource exhaustion Denial of Service attack against validator nodes and full nodes running the storage service. Each malicious request forces the server to create multiple database iterators, perform disk I/O to fetch potentially thousands of transaction records, serialize and deserialize large data structures, and consume CPU cycles for data processing.

Since the malicious peer is never marked as unhealthy (validation passed), they can sustain this attack indefinitely without being rate-limited or blocked. Multiple coordinated attackers could amplify the impact.

This qualifies as **HIGH Severity** per the Aptos Bug Bounty program category "Validator Node Slowdowns" - specifically "DoS through resource exhaustion." The sustained disk I/O and CPU consumption can significantly degrade validator node performance and their ability to participate in consensus. This is a protocol-level bug (missing validation check) that causes resource exhaustion, distinct from network-level DoS attacks which are out of scope.

## Likelihood Explanation

**Likelihood: HIGH**

Attack requirements:
- Knowledge of approximate pruned version range (easily discoverable through trial and error or by querying storage summaries)
- Ability to send network requests to storage service nodes (publicly accessible for state sync to untrusted peers)
- No privileged access required
- No economic cost to the attacker

The attack is trivial to execute and can be automated. Any malicious peer can perform this attack with minimal resources. The transaction accumulator pruner is enabled by default, making pruned accumulator nodes a guaranteed condition on any production node.

## Recommendation

Add a validation check in `can_create_proof` to ensure the `proof_version` is within the available (non-pruned) range:

```rust
fn can_create_proof(&self, proof_version: u64) -> bool {
    self.synced_ledger_info
        .as_ref()
        .map(|li| {
            let synced_version = li.ledger_info().version();
            // Check both upper and lower bounds
            proof_version <= synced_version && proof_version >= self.get_min_readable_version()
        })
        .unwrap_or(false)
}
```

Where `get_min_readable_version()` retrieves the minimum readable version from the LedgerPrunerManager. This ensures validation rejects requests with pruned `proof_version` values before expensive I/O operations occur.

## Proof of Concept

A PoC would involve:
1. Setting up a storage service node with pruning enabled
2. Waiting for pruning to occur (or manually triggering it)
3. Sending repeated `TransactionOutputsWithProofRequest` messages with `proof_version` set to a pruned version
4. Observing the resource consumption (disk I/O, CPU usage) and confirming the peer is not marked as unhealthy
5. Measuring the sustained degradation in storage service performance

The vulnerability can be reproduced by crafting network requests through the state sync protocol with the malicious `proof_version` parameter.

## Notes

This vulnerability demonstrates a missing validation check that allows resource exhaustion attacks. The separation between validation logic (which doesn't check pruning bounds on `proof_version`) and processing logic (which performs expensive I/O before proof generation) creates an exploitable gap. The fact that validation errors and processing errors are treated differently by the request moderator amplifies the impact, as attackers can repeatedly exploit this without being rate-limited.

### Citations

**File:** state-sync/storage-service/types/src/requests.rs (L353-357)
```rust
pub struct TransactionOutputsWithProofRequest {
    pub proof_version: u64, // The version the proof should be relative to
    pub start_version: u64, // The starting version of the transaction output list
    pub end_version: u64,   // The ending version of the transaction output list (inclusive)
}
```

**File:** state-sync/storage-service/types/src/responses.rs (L811-816)
```rust
    fn can_create_proof(&self, proof_version: u64) -> bool {
        self.synced_ledger_info
            .as_ref()
            .map(|li| li.ledger_info().version() >= proof_version)
            .unwrap_or(false)
    }
```

**File:** state-sync/storage-service/types/src/responses.rs (L833-847)
```rust
    fn can_service_transaction_outputs_with_proof(
        &self,
        start_version: u64,
        end_version: u64,
        proof_version: u64,
    ) -> bool {
        let desired_range = match CompleteDataRange::new(start_version, end_version) {
            Ok(desired_range) => desired_range,
            Err(_) => return false,
        };

        let can_service_outputs = self.can_service_transaction_outputs(&desired_range);
        let can_create_proof = self.can_create_proof(proof_version);
        can_service_outputs && can_create_proof
    }
```

**File:** storage/aptosdb/src/ledger_db/transaction_accumulator_db.rs (L149-172)
```rust
    pub(crate) fn prune(begin: Version, end: Version, db_batch: &mut SchemaBatch) -> Result<()> {
        for version_to_delete in begin..end {
            db_batch.delete::<TransactionAccumulatorRootHashSchema>(&version_to_delete)?;
            // The even version will be pruned in the iteration of version + 1.
            if version_to_delete % 2 == 0 {
                continue;
            }

            let first_ancestor_that_is_a_left_child =
                Self::find_first_ancestor_that_is_a_left_child(version_to_delete);

            // This assertion is true because we skip the leaf nodes with address which is a
            // a multiple of 2.
            assert!(!first_ancestor_that_is_a_left_child.is_leaf());

            let mut current = first_ancestor_that_is_a_left_child;
            while !current.is_leaf() {
                db_batch.delete::<TransactionAccumulatorSchema>(&current.left_child())?;
                db_batch.delete::<TransactionAccumulatorSchema>(&current.right_child())?;
                current = current.right_child();
            }
        }
        Ok(())
    }
```

**File:** storage/aptosdb/src/ledger_db/transaction_accumulator_db.rs (L196-200)
```rust
    fn get(&self, position: Position) -> Result<HashValue, anyhow::Error> {
        self.db
            .get::<TransactionAccumulatorSchema>(&position)?
            .ok_or_else(|| anyhow!("{} does not exist.", position))
    }
```

**File:** state-sync/storage-service/server/src/storage.rs (L591-614)
```rust
        // Get the iterators for the transaction, info, write set, events,
        // auxiliary data and persisted auxiliary infos.
        let transaction_iterator = self
            .storage
            .get_transaction_iterator(start_version, num_outputs_to_fetch)?;
        let transaction_info_iterator = self
            .storage
            .get_transaction_info_iterator(start_version, num_outputs_to_fetch)?;
        let transaction_write_set_iterator = self
            .storage
            .get_write_set_iterator(start_version, num_outputs_to_fetch)?;
        let transaction_events_iterator = self
            .storage
            .get_events_iterator(start_version, num_outputs_to_fetch)?;
        let persisted_auxiliary_info_iterator = self
            .storage
            .get_persisted_auxiliary_info_iterator(start_version, num_outputs_to_fetch as usize)?;
        let mut multizip_iterator = itertools::multizip((
            transaction_iterator,
            transaction_info_iterator,
            transaction_write_set_iterator,
            transaction_events_iterator,
            persisted_auxiliary_info_iterator,
        ));
```

**File:** state-sync/storage-service/server/src/storage.rs (L630-696)
```rust
        while !response_progress_tracker.is_response_complete() {
            match multizip_iterator.next() {
                Some((
                    Ok(transaction),
                    Ok(info),
                    Ok(write_set),
                    Ok(events),
                    Ok(persisted_auxiliary_info),
                )) => {
                    // Create the transaction output
                    let output = TransactionOutput::new(
                        write_set,
                        events,
                        info.gas_used(),
                        info.status().clone().into(),
                        TransactionAuxiliaryData::None, // Auxiliary data is no longer supported
                    );

                    // Calculate the number of serialized bytes for the data items
                    let num_transaction_bytes = get_num_serialized_bytes(&transaction)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_info_bytes = get_num_serialized_bytes(&info)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_output_bytes = get_num_serialized_bytes(&output)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_auxiliary_info_bytes =
                        get_num_serialized_bytes(&persisted_auxiliary_info).map_err(|error| {
                            Error::UnexpectedErrorEncountered(error.to_string())
                        })?;

                    // Add the data items to the lists
                    let total_serialized_bytes = num_transaction_bytes
                        + num_info_bytes
                        + num_output_bytes
                        + num_auxiliary_info_bytes;
                    if response_progress_tracker.data_items_fits_in_response(
                        !is_transaction_or_output_request,
                        total_serialized_bytes,
                    ) {
                        transactions_and_outputs.push((transaction, output));
                        transaction_infos.push(info);
                        persisted_auxiliary_infos.push(persisted_auxiliary_info);

                        response_progress_tracker.add_data_item(total_serialized_bytes);
                    } else {
                        break; // Cannot add any more data items
                    }
                },
                Some((Err(error), _, _, _, _))
                | Some((_, Err(error), _, _, _))
                | Some((_, _, Err(error), _, _))
                | Some((_, _, _, Err(error), _))
                | Some((_, _, _, _, Err(error))) => {
                    return Err(Error::StorageErrorEncountered(error.to_string()));
                },
                None => {
                    // Log a warning that the iterators did not contain all the expected data
                    warn!(
                        "The iterators for transactions, transaction infos, write sets, events, \
                        auxiliary data and persisted auxiliary infos are missing data! Start version: {:?}, \
                        end version: {:?}, num outputs to fetch: {:?}, num fetched: {:?}.",
                        start_version, end_version, num_outputs_to_fetch, transactions_and_outputs.len()
                    );
                    break;
                },
            }
        }
```

**File:** state-sync/storage-service/server/src/storage.rs (L703-708)
```rust
            self.storage.get_transaction_accumulator_range_proof(
                start_version,
                num_fetched_outputs as u64,
                proof_version,
            )?
        };
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L548-561)
```rust
    fn get_transaction_accumulator_range_proof(
        &self,
        first_version: Version,
        limit: u64,
        ledger_version: Version,
    ) -> Result<TransactionAccumulatorRangeProof> {
        gauged_api("get_transaction_accumulator_range_proof", || {
            self.error_if_ledger_pruned("Transaction", first_version)?;

            self.ledger_db
                .transaction_accumulator_db()
                .get_transaction_range_proof(Some(first_version), limit, ledger_version)
        })
    }
```

**File:** state-sync/storage-service/server/src/moderator.rs (L155-178)
```rust
            if !storage_server_summary.can_service(
                &self.aptos_data_client_config,
                self.time_service.clone(),
                request,
            ) {
                // Increment the invalid request count for the peer
                let mut unhealthy_peer_state = self
                    .unhealthy_peer_states
                    .entry(*peer_network_id)
                    .or_insert_with(|| {
                        // Create a new unhealthy peer state (this is the first invalid request)
                        let max_invalid_requests =
                            self.storage_service_config.max_invalid_requests_per_peer;
                        let min_time_to_ignore_peers_secs =
                            self.storage_service_config.min_time_to_ignore_peers_secs;
                        let time_service = self.time_service.clone();

                        UnhealthyPeerState::new(
                            max_invalid_requests,
                            min_time_to_ignore_peers_secs,
                            time_service,
                        )
                    });
                unhealthy_peer_state.increment_invalid_request_count(peer_network_id);
```

**File:** state-sync/storage-service/server/src/handler.rs (L206-228)
```rust
    fn validate_and_handle_request(
        &self,
        peer_network_id: &PeerNetworkId,
        request: &StorageServiceRequest,
    ) -> Result<StorageServiceResponse, Error> {
        // Validate the request with the moderator
        self.request_moderator
            .validate_request(peer_network_id, request)?;

        // Process the request
        match &request.data_request {
            DataRequest::GetServerProtocolVersion => {
                let data_response = self.get_server_protocol_version();
                StorageServiceResponse::new(data_response, request.use_compression)
                    .map_err(|error| error.into())
            },
            DataRequest::GetStorageServerSummary => {
                let data_response = self.get_storage_server_summary();
                StorageServiceResponse::new(data_response, request.use_compression)
                    .map_err(|error| error.into())
            },
            _ => self.process_cachable_request(peer_network_id, request),
        }
```
