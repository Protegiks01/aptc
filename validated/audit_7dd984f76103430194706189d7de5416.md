# Audit Report

## Title
Epoch Transition Denial of Service: Stale Batch Loading Causes Validator Node Panic During Restart

## Summary
The quorum store batch management system contains a logic flaw in epoch transition handling. When a validator node restarts after an epoch transition but after new blocks have been committed, the system incorrectly loads batches from the previous epoch without performing epoch-based garbage collection. If these stale batches exhaust storage quotas, the node panics during initialization and cannot recover, creating a denial of service condition.

## Finding Description

The vulnerability exists in how the `BatchStore` determines whether to perform epoch-based garbage collection during initialization.

The `is_new_epoch` flag is determined by checking if the **latest** committed ledger info marks an epoch end: [1](#0-0) 

This creates a critical flaw: after an epoch transition, once any blocks are committed in the new epoch, `ends_epoch()` returns `false` because only the epoch-ending block has this flag set. When `is_new_epoch=false`, the system calls `populate_cache_and_gc_expired_batches_v1/v2()` instead of epoch-aware garbage collection: [2](#0-1) 

The `populate_cache_and_gc_expired_batches_v1()` function retrieves ALL batches from the database without epoch filtering: [3](#0-2) 

The `get_all_batches()` implementation simply iterates over all batches: [4](#0-3) 

The function only filters by expiration timestamp, NOT by epoch: [5](#0-4) 

When quota is exhausted, the `.expect()` at line 278 panics with "Storage limit exceeded upon BatchReader construction". The same issue exists in the v2 variant: [6](#0-5) 

The `QuotaManager::update_quota()` enforces strict limits and returns errors when quotas are exhausted: [7](#0-6) 

In contrast, when `is_new_epoch=true`, the system correctly performs epoch-aware garbage collection that deletes batches from previous epochs: [8](#0-7) 

Note line 199 checks `if epoch < current_epoch` before deleting, which prevents the resource exhaustion issue.

**Attack Scenario:**

1. During epoch N, validators create many batches with 60-second expiration
2. Epoch N ends and epoch N+1 begins (epoch-ending block committed)
3. Several blocks are committed in the new epoch (latest ledger info no longer has `ends_epoch=true`)
4. A validator node restarts for maintenance or upgrade
5. `is_new_epoch = latest_ledger_info.ends_epoch()` returns `false`
6. `populate_cache_and_gc_expired_batches_v1/v2()` loads ALL batches from database
7. Stale batches from epoch N that haven't expired consume quota
8. When quota exhausted, the node panics and cannot start

## Impact Explanation

This vulnerability qualifies as **HIGH severity** under the Aptos bug bounty program's "API Crashes" category.

**Impact:**
- Validator nodes panic during startup and cannot recover without manual intervention (database cleanup or waiting for batch expiration)
- If multiple validators restart simultaneously after epoch transitions (common during coordinated upgrades), network liveness could be degraded
- The panic occurs during initialization, making the validator completely unavailable

This maps to HIGH severity rather than CRITICAL because:
- It does not cause fund loss or theft
- It does not violate consensus safety (no double-spending or chain splits)
- It affects availability rather than safety
- Recovery is possible through manual intervention

However, HIGH severity is justified because:
- Validator unavailability directly impacts network liveness
- The scenario affects standard operational patterns (epoch transitions + restarts)
- Multiple validators could be affected simultaneously
- Recovery requires manual intervention or extended downtime

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability has a high likelihood of occurring in production:

1. **Natural Trigger Conditions:** Epoch transitions occur regularly in Aptos when validator sets change. Validator restarts after epoch transitions are common operational practice for upgrades and maintenance.

2. **No Malicious Action Required:** The vulnerability triggers from normal operations. With default 60-second batch expiration: [9](#0-8) 

Batches created near the end of epoch N remain valid for up to 60 seconds into epoch N+1.

3. **Quota Exhaustion Feasibility:** The default quotas are finite: [10](#0-9) 

With 300MB db_quota and 300k batch_quota, high-traffic conditions with many validators creating batches could realistically exhaust these limits when loading unexpired batches from a previous epoch.

4. **Common Operational Pattern:** The scenario (epoch transition → blocks committed → validator restart) is a standard pattern, not an edge case.

## Recommendation

Fix the `is_new_epoch` determination logic to correctly identify when the node has transitioned to a new epoch since the last `BatchStore` initialization, rather than checking if the latest block marks an epoch end.

**Option 1:** Store the epoch number when `BatchStore` is initialized and compare it to the current epoch:

```rust
fn create_batch_store(&mut self) -> Arc<BatchReaderImpl<NetworkSender>> {
    let latest_ledger_info = self.aptos_db.get_latest_ledger_info()
        .expect("could not get latest ledger info");
    let current_epoch = latest_ledger_info.ledger_info().epoch();
    
    // Check if we've entered a new epoch (compare stored epoch with current)
    let is_new_epoch = self.last_batch_store_epoch.map_or(false, |last| current_epoch > last);
    
    let batch_store = Arc::new(BatchStore::new(
        current_epoch,
        is_new_epoch,
        // ... other parameters
    ));
    
    self.last_batch_store_epoch = Some(current_epoch);
    batch_store
}
```

**Option 2:** Always perform epoch-based garbage collection on startup to ensure old batches are removed regardless of when the restart occurs.

## Proof of Concept

The vulnerability can be demonstrated with the following test scenario:

1. Initialize a `BatchStore` in epoch 10 with `is_new_epoch=false`
2. Populate the database with batches from epoch 9 that have expiration times still in the future
3. Observe that `populate_cache_and_gc_expired_batches_v1()` loads these epoch 9 batches into the cache
4. If enough batches exist to exceed quota limits, the initialization panics

A complete PoC would require setting up a test environment with:
- A quorum store database containing batches from a previous epoch
- Expiration times set such that batches haven't expired
- Sufficient batch count or size to exceed configured quotas
- A restart scenario where `latest_ledger_info.ends_epoch()` returns false

## Notes

The core issue is that `is_new_epoch` uses the wrong semantic check. It answers "Is the latest block an epoch-ending block?" when it should answer "Have we entered a new epoch since this BatchStore was last initialized?" This creates a vulnerability window after every epoch transition where stale batches can be incorrectly loaded, potentially exhausting quotas and causing validator crashes.

While the practical likelihood of quota exhaustion depends on network traffic patterns, the logic bug is undeniable and should be fixed to ensure validator reliability during epoch transitions.

### Citations

**File:** consensus/src/quorum_store/quorum_store_builder.rs (L244-244)
```rust
        let is_new_epoch = latest_ledger_info_with_sigs.ledger_info().ends_epoch();
```

**File:** consensus/src/quorum_store/batch_store.rs (L64-84)
```rust
    pub(crate) fn update_quota(&mut self, num_bytes: usize) -> anyhow::Result<StorageMode> {
        if self.batch_balance == 0 {
            counters::EXCEEDED_BATCH_QUOTA_COUNT.inc();
            bail!("Batch quota exceeded ");
        }

        if self.db_balance >= num_bytes {
            self.batch_balance -= 1;
            self.db_balance -= num_bytes;

            if self.memory_balance >= num_bytes {
                self.memory_balance -= num_bytes;
                Ok(StorageMode::MemoryAndPersisted)
            } else {
                Ok(StorageMode::PersistedOnly)
            }
        } else {
            counters::EXCEEDED_STORAGE_QUOTA_COUNT.inc();
            bail!("Storage quota exceeded ");
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L156-176)
```rust
        if is_new_epoch {
            tokio::task::spawn_blocking(move || {
                Self::gc_previous_epoch_batches_from_db_v1(db_clone.clone(), epoch);
                Self::gc_previous_epoch_batches_from_db_v2(db_clone, epoch);
            });
        } else {
            Self::populate_cache_and_gc_expired_batches_v1(
                db_clone.clone(),
                epoch,
                last_certified_time,
                expiration_buffer_usecs,
                &batch_store,
            );
            Self::populate_cache_and_gc_expired_batches_v2(
                db_clone,
                epoch,
                last_certified_time,
                expiration_buffer_usecs,
                &batch_store,
            );
        }
```

**File:** consensus/src/quorum_store/batch_store.rs (L181-210)
```rust
    fn gc_previous_epoch_batches_from_db_v1(db: Arc<dyn QuorumStoreStorage>, current_epoch: u64) {
        let db_content = db.get_all_batches().expect("failed to read data from db");
        info!(
            epoch = current_epoch,
            "QS: Read batches from storage. Len: {}",
            db_content.len(),
        );

        let mut expired_keys = Vec::new();
        for (digest, value) in db_content {
            let epoch = value.epoch();

            trace!(
                "QS: Batchreader recovery content epoch {:?}, digest {}",
                epoch,
                digest
            );

            if epoch < current_epoch {
                expired_keys.push(digest);
            }
        }

        info!(
            "QS: Batch store bootstrap expired keys len {}",
            expired_keys.len()
        );
        db.delete_batches(expired_keys)
            .expect("Deletion of expired keys should not fail");
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L252-254)
```rust
        let db_content = db
            .get_all_batches()
            .expect("failed to read v1 data from db");
```

**File:** consensus/src/quorum_store/batch_store.rs (L263-279)
```rust
        let gc_timestamp = last_certified_time.saturating_sub(expiration_buffer_usecs);
        for (digest, value) in db_content {
            let expiration = value.expiration();

            trace!(
                "QS: Batchreader recovery content exp {:?}, digest {}",
                expiration,
                digest
            );

            if expiration < gc_timestamp {
                expired_keys.push(digest);
            } else {
                batch_store
                    .insert_to_cache(&value.into())
                    .expect("Storage limit exceeded upon BatchReader construction");
            }
```

**File:** consensus/src/quorum_store/batch_store.rs (L319-325)
```rust
            if expiration < gc_timestamp {
                expired_keys.push(digest);
            } else {
                batch_store
                    .insert_to_cache(&value)
                    .expect("Storage limit exceeded upon BatchReader construction");
            }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L103-108)
```rust
    fn get_all_batches(&self) -> Result<HashMap<HashValue, PersistedValue<BatchInfo>>> {
        let mut iter = self.db.iter::<BatchSchema>()?;
        iter.seek_to_first();
        iter.map(|res| res.map_err(Into::into))
            .collect::<Result<HashMap<HashValue, PersistedValue<BatchInfo>>>>()
    }
```

**File:** config/src/config/quorum_store_config.rs (L131-131)
```rust
            batch_expiry_gap_when_init_usecs: Duration::from_secs(60).as_micros() as u64,
```

**File:** config/src/config/quorum_store_config.rs (L133-135)
```rust
            memory_quota: 120_000_000,
            db_quota: 300_000_000,
            batch_quota: 300_000,
```
