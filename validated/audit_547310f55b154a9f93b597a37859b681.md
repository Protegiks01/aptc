# Audit Report

## Title
State Merkle Shard Pruner Progress Tracking Failure Causes State Inconsistency After Crash

## Summary
The `StateMerkleShardPruner::prune()` function contains a critical bug where `current_progress` is never updated within the pruning loop. When the pruning process is interrupted (crash, panic, or restart) after deleting nodes but before completing all iterations, the per-shard progress remains at its original value despite nodes being permanently deleted. This causes the system to claim data availability for versions that have been pruned, leading to `MissingRootError` on queries and state sync failures.

## Finding Description

The vulnerability exists in the main pruning loop where `current_progress` is passed to `get_stale_node_indices()` on every iteration without being updated: [1](#0-0) 

The critical flaw is that:
1. The function receives `current_progress` as a parameter (line 60)
2. Each loop iteration calls `get_stale_node_indices()` with the same `current_progress` value (line 66-68)
3. Progress is only persisted when `done = true` at the very end (lines 85-89)
4. If the process is interrupted mid-loop after batch commits (line 92), multiple batches of nodes are deleted but progress never advances

**State Inconsistency Scenario:**

1. **Initial State**: Shard progress = 5000, stale nodes exist for versions 5000-6000, batch_size = 100
2. **Iteration 1**: `get_stale_node_indices(5000, 6000, 100)` returns 100 indices for versions 5000-5050. These are deleted and committed at line 92. Progress still = 5000.
3. **Iteration 2**: `get_stale_node_indices(5000, 6000, 100)` still uses 5000! The iterator implementation [2](#0-1)  naturally skips deleted nodes and returns versions 5051-5100. Deleted and committed. Progress still = 5000.
4. **CRASH** occurs before loop completion
5. **On Restart**: Shard progress reads 5000 from database using `get_or_initialize_subpruner_progress`: [3](#0-2) 

During initialization, the shard calls `prune(5000, metadata_progress, usize::MAX)` for catchup. If nodes were partially deleted, this either does nothing (if progress equals metadata_progress) or attempts to prune already-deleted nodes.

6. **Query Failure**: A query for version 5025 passes the min_readable_version check: [4](#0-3) 

But when retrieving the actual merkle node, it fails with `MissingRootError` because the root node at that version was already deleted: [5](#0-4) 

This violates the **State Consistency** invariant: versions >= min_readable_version must be queryable with complete merkle proofs.

## Impact Explanation

This is **MEDIUM Severity** (up to $10,000) under "State inconsistencies requiring manual intervention":

1. **Data Availability Failure**: Nodes cannot serve legitimate queries for versions they claim to support via min_readable_version, breaking the data availability promise.

2. **State Sync Failures**: New validators attempting to sync from affected nodes will fail to retrieve merkle proofs for supposedly available versions, preventing network expansion.

3. **Operational Impact**: Requires manual intervention to identify inconsistent shards, potentially requiring database recovery or re-sync from genesis.

4. **Silent Corruption**: The bug is silent until a query attempts to access the missing range, making detection difficult.

The impact is limited to historical data availability and state sync. This does NOT cause consensus divergence because:
- Pruning occurs after blocks are committed
- Transaction execution uses current state, not pruned historical state
- Validators do not query each other's pruned historical state during consensus

The parallel execution of shard pruners increases impact: [6](#0-5) 

Multiple shards can enter inconsistent states independently.

## Likelihood Explanation

**HIGH Likelihood** - This bug will manifest naturally during normal operations:

1. **Common Trigger**: Process crashes, OOM kills, restarts, and panics occur regularly in distributed systems, especially during high load
2. **Frequent Operation**: Pruning runs continuously as a background task when enabled
3. **Multiple Iterations**: With default batch_size (100) and thousands of versions to prune, multi-iteration loops are common
4. **No Prevention**: No safeguards exist to prevent or detect this inconsistency
5. **Silent Failure**: The bug is silent until a query attempts to access the missing range

## Recommendation

Update `current_progress` after each successful batch commit:

```rust
pub(in crate::pruner) fn prune(
    &self,
    mut current_progress: Version,  // Make mutable
    target_version: Version,
    max_nodes_to_prune: usize,
) -> Result<()> {
    loop {
        let mut batch = SchemaBatch::new();
        let (indices, next_version) = StateMerklePruner::get_stale_node_indices(
            &self.db_shard,
            current_progress,
            target_version,
            max_nodes_to_prune,
        )?;

        indices.into_iter().try_for_each(|index| {
            batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
            batch.delete::<S>(&index)
        })?;

        let mut done = true;
        if let Some(next_version) = next_version {
            if next_version <= target_version {
                done = false;
                // Update progress after each batch
                batch.put::<DbMetadataSchema>(
                    &S::progress_metadata_key(Some(self.shard_id)),
                    &DbMetadataValue::Version(next_version),
                )?;
            }
        }

        if done {
            batch.put::<DbMetadataSchema>(
                &S::progress_metadata_key(Some(self.shard_id)),
                &DbMetadataValue::Version(target_version),
            )?;
        }

        self.db_shard.write_schemas(batch)?;

        if done {
            break;
        }
        
        // Update current_progress for next iteration
        if let Some(next_version) = next_version {
            current_progress = next_version;
        }
    }

    Ok(())
}
```

This ensures progress is persisted after each batch commit, preventing state inconsistency after crashes.

## Proof of Concept

A PoC would require:
1. Starting a validator with pruning enabled
2. Triggering pruning of a large version range
3. Forcibly killing the process mid-pruning (after first batch commit)
4. Restarting the validator
5. Querying a version in the partially-pruned range
6. Observing `MissingRootError` despite min_readable_version indicating availability

## Notes

- The severity is MEDIUM, not HIGH, as this affects data availability and state sync but does not cause consensus divergence or fund theft
- The "consensus divergence risk" claim in the original report is overstated - different validators having different pruned ranges does not affect consensus on new blocks
- This is a genuine data integrity issue requiring manual intervention to resolve

### Citations

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L36-53)
```rust
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &S::progress_metadata_key(Some(shard_id)),
            metadata_progress,
        )?;
        let myself = Self {
            shard_id,
            db_shard,
            _phantom: PhantomData,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up {} shard {shard_id}.",
            S::name(),
        );
        myself.prune(progress, metadata_progress, usize::MAX)?;
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L58-100)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
        max_nodes_to_prune: usize,
    ) -> Result<()> {
        loop {
            let mut batch = SchemaBatch::new();
            let (indices, next_version) = StateMerklePruner::get_stale_node_indices(
                &self.db_shard,
                current_progress,
                target_version,
                max_nodes_to_prune,
            )?;

            indices.into_iter().try_for_each(|index| {
                batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
                batch.delete::<S>(&index)
            })?;

            let mut done = true;
            if let Some(next_version) = next_version {
                if next_version <= target_version {
                    done = false;
                }
            }

            if done {
                batch.put::<DbMetadataSchema>(
                    &S::progress_metadata_key(Some(self.shard_id)),
                    &DbMetadataValue::Version(target_version),
                )?;
            }

            self.db_shard.write_schemas(batch)?;

            if done {
                break;
            }
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L168-189)
```rust
    fn prune_shards(
        &self,
        current_progress: Version,
        target_version: Version,
        batch_size: usize,
    ) -> Result<()> {
        THREAD_MANAGER
            .get_background_pool()
            .install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(current_progress, target_version, batch_size)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state merkle shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })
            .map_err(Into::into)
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L191-217)
```rust
    pub(in crate::pruner::state_merkle_pruner) fn get_stale_node_indices(
        state_merkle_db_shard: &DB,
        start_version: Version,
        target_version: Version,
        limit: usize,
    ) -> Result<(Vec<StaleNodeIndex>, Option<Version>)> {
        let mut indices = Vec::new();
        let mut iter = state_merkle_db_shard.iter::<S>()?;
        iter.seek(&StaleNodeIndex {
            stale_since_version: start_version,
            node_key: NodeKey::new_empty_path(0),
        })?;

        let mut next_version = None;
        while indices.len() < limit {
            if let Some((index, _)) = iter.next().transpose()? {
                next_version = Some(index.stale_since_version);
                if index.stale_since_version <= target_version {
                    indices.push(index);
                    continue;
                }
            }
            break;
        }

        Ok((indices, next_version))
    }
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L273-285)
```rust
    pub(super) fn error_if_state_merkle_pruned(
        &self,
        data_type: &str,
        version: Version,
    ) -> Result<()> {
        let min_readable_version = self
            .state_store
            .state_db
            .state_merkle_pruner
            .get_min_readable_version();
        if version >= min_readable_version {
            return Ok(());
        }
```

**File:** storage/jellyfish-merkle/src/lib.rs (L730-741)
```rust
        // in the tree structure.
        for nibble_depth in 0..=ROOT_NIBBLE_HEIGHT {
            let next_node = self
                .reader
                .get_node_with_tag(&next_node_key, "get_proof")
                .map_err(|err| {
                    if nibble_depth == 0 {
                        AptosDbError::MissingRootError(version)
                    } else {
                        err
                    }
                })?;
```
