# Audit Report

## Title
State Snapshot Restore Bypasses Cryptographic Validation in KvOnly Mode, Allowing Malicious Chunks from Compromised Backups

## Summary
The `StateSnapshotRestoreMode::KvOnly` mode in the state snapshot restore process skips Merkle proof validation of chunk data, allowing an attacker with compromised backup storage to inject malicious state data that gets written to the database without cryptographic verification. This creates an inconsistent trust model where manifest integrity is verified but chunk data integrity is not.

## Finding Description

The state snapshot restore system implements three restoration modes through the `StateSnapshotRestoreMode` enum: `Default`, `KvOnly`, and `TreeOnly`. [1](#0-0) 

When `StateSnapshotRestore::add_chunk()` is called with `KvOnly` mode, only the `kv_fn()` closure executes, which writes key-value pairs directly to storage without any Merkle proof validation. [2](#0-1) 

The `kv_fn()` closure calls `StateValueRestore::add_chunk()`, which simply writes the chunk data to the database through `write_kv_batch()` without any cryptographic verification of the chunk contents. [3](#0-2) 

In contrast, the `tree_fn()` closure (used in `TreeOnly` and `Default` modes) calls `JellyfishMerkleRestore::add_chunk_impl()`, which performs cryptographic validation by calling `self.verify(proof)?` to validate chunks against the expected root hash using `SparseMerkleRangeProof`. [4](#0-3) 

The `verify()` method reconstructs the Merkle root from chunk data and proofs to ensure cryptographic integrity. [5](#0-4) 

**The Critical Vulnerability:**

The restore coordinator implements a two-phase restore strategy that intentionally uses `KvOnly` mode in phase 1 for performance optimization. [6](#0-5) 

While the manifest's root hash is verified against the cryptographically signed ledger info, [7](#0-6)  this only validates the manifest metadata, not the actual chunk blob contents.

During phase 1.b, transaction replay uses `NoVerify` mode and only applies KV updates without re-executing transactions through the VM, [8](#0-7)  meaning corrupted base state is not detected.

In phase 2, the tree restoration with `TreeOnly` mode validates chunks from the backup storage, but these may be from a different snapshot version. There is no cross-validation between the KV data written in phase 1 and the tree structure validated in phase 2.

**Attack Scenario:**
1. Attacker compromises backup storage (cloud misconfiguration, supply chain attack, compromised backup service)
2. Attacker modifies KV snapshot chunk blobs while keeping manifest and tree snapshot intact
3. Manifest signature verification passes (only validates manifest, not chunks)
4. Phase 1: Malicious KV chunks written without validation via `KvOnly` mode
5. Phase 1b: Transaction replay with `NoVerify` mode doesn't detect corruption
6. Phase 2: Tree validation validates different (potentially uncorrupted) snapshot chunks
7. Result: Database contains corrupted KV data with valid tree structure, creating state inconsistency

The restore process completes successfully despite the corruption, which is only detected at runtime when the node processes blocks and computes incorrect state roots.

## Impact Explanation

This vulnerability fits the **Medium Severity** category per Aptos bug bounty criteria: "State inconsistencies requiring manual intervention."

**Concrete Impacts:**
- **State Corruption**: Malicious state data persistently written to database without cryptographic validation
- **Node Inoperability**: When the restored node processes blocks post-restore, it computes incorrect state roots, causing consensus verification failures and inability to participate in the network
- **Manual Intervention Required**: Operators must manually identify the corruption source and restore from a trusted backup, requiring downtime and operational overhead
- **Inconsistent Trust Model**: The system implements manifest signature verification (showing distrust of backup storage) but bypasses chunk data validation in `KvOnly` mode, creating an exploitable security gap

**Why Not Higher Severity:**
- Does NOT cause network-wide consensus failure (only affects single restoring node)
- Does NOT enable direct fund theft or unauthorized minting
- Does NOT create permanent network partition
- Corruption is eventually detectable (though detection is delayed)

The vulnerability enables state corruption requiring operational intervention, which directly matches the Medium severity definition.

## Likelihood Explanation

**Likelihood: Medium**

**Prerequisites for Exploitation:**
1. **Compromised Backup Storage** - Realistic threat scenarios include:
   - Cloud storage misconfigurations (public S3 buckets, weak credentials)
   - Compromised backup service providers
   - Supply chain attacks on backup infrastructure
   - Insider threats from personnel with backup access
   - Third-party infrastructure vulnerabilities (AWS, GCP, Azure)

2. **KvOnly Mode Usage** - Automatically triggered in common scenarios:
   - Restore coordinator uses it by design in phase 1 for incremental restores
   - Operators may manually specify `--restore-mode kv_only` for faster restoration
   - The code shows this is an intentional performance optimization

3. **Delayed Detection** - The vulnerability is stealthy:
   - Restore process completes successfully
   - No warnings that validation is being skipped
   - Corruption only detected at runtime during block processing
   - Provides attacker time before detection

**Factors Increasing Likelihood:**
- Modern blockchain operations commonly use cloud backup storage
- Performance pressure incentivizes use of faster `KvOnly` mode
- No runtime safeguards warn operators about skipped validation
- The two-phase restore design is the default coordinator behavior

**Factors Decreasing Likelihood:**
- Requires attacker to have write access to backup storage
- Manifest signatures provide partial defense (though insufficient)
- Professional operators may have additional backup integrity monitoring

The combination of realistic attack preconditions and common usage patterns justifies Medium likelihood.

## Recommendation

**Immediate Fix:**
Enforce cryptographic validation of all chunk data regardless of restore mode. Modify `StateSnapshotRestore::add_chunk()` to validate chunks even in `KvOnly` mode:

```rust
fn add_chunk(&mut self, chunk: Vec<(K, V)>, proof: SparseMerkleRangeProof) -> Result<()> {
    // Always verify the proof first, regardless of restore mode
    let chunk_hashes: Vec<_> = chunk.iter().map(|(k, v)| (k, v.hash())).collect();
    self.tree_restore
        .lock()
        .as_mut()
        .unwrap()
        .verify_chunk_only(chunk_hashes, proof)?;  // Add verification-only method
    
    // Then proceed with mode-specific operations
    match self.restore_mode {
        StateSnapshotRestoreMode::KvOnly => self.kv_restore.lock().as_mut().unwrap().add_chunk(chunk)?,
        // ... rest of the modes
    }
}
```

**Alternative Approach:**
If verification-only mode is not feasible for performance reasons, implement chunk integrity verification at a different layer:
1. Compute and verify chunk blob hashes against signed manifest
2. Include chunk content hashes in the manifest alongside Merkle proofs
3. Verify these hashes before writing any data in `KvOnly` mode

**Defense in Depth:**
- Add runtime warnings when `KvOnly` mode is used
- Implement optional chunk-level signatures in backup format
- Provide post-restore state integrity verification tools
- Document the trust model and security implications of each restore mode

## Proof of Concept

The vulnerability can be demonstrated by:

1. Creating a legitimate state snapshot backup with valid manifest and chunks
2. Modifying a KV snapshot chunk blob file with malicious state data
3. Running restore with the coordinator (which uses `KvOnly` in phase 1)
4. Observing that restore completes successfully
5. Attempting to process blocks, which fails with state root mismatches

Note: A complete PoC would require backup infrastructure setup. The code evidence provided demonstrates that:
- `KvOnly` mode skips proof validation (lines 247 in state_restore/mod.rs)
- Coordinator uses this mode by default (line 252 in coordinators/restore.rs)
- No chunk integrity verification exists in the KV write path (lines 88-127 in state_restore/mod.rs)
- Transaction replay uses `NoVerify` mode (line 296, 367 in coordinators/restore.rs)

The vulnerability is evident from the code structure where cryptographic validation is conditionally skipped based on restore mode, creating an exploitable path for malicious data injection from compromised backup sources.

## Notes

This vulnerability represents an **inconsistent trust model** where the system demonstrates partial distrust of backup storage (by verifying manifest signatures) but fails to validate the actual chunk data content in `KvOnly` mode. The two-phase restore design, while beneficial for performance, introduces a security gap where unvalidated data can be persisted to the database. This is particularly concerning as backup storage is often hosted on third-party infrastructure and represents a realistic attack surface for adversaries targeting blockchain node operations.

### Citations

**File:** storage/aptosdb/src/state_restore/mod.rs (L49-57)
```rust
#[derive(Clone, Copy, Deserialize, Serialize, PartialEq, Eq)]
pub enum StateSnapshotRestoreMode {
    /// Restore both KV and Tree by default
    Default,
    /// Only restore the state KV
    KvOnly,
    /// Only restore the state tree
    TreeOnly,
}
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L88-127)
```rust
    pub fn add_chunk(&mut self, mut chunk: Vec<(K, V)>) -> Result<()> {
        // load progress
        let progress_opt = self.db.get_progress(self.version)?;

        // skip overlaps
        if let Some(progress) = progress_opt {
            let idx = chunk
                .iter()
                .position(|(k, _v)| CryptoHash::hash(k) > progress.key_hash)
                .unwrap_or(chunk.len());
            chunk = chunk.split_off(idx);
        }

        // quit if all skipped
        if chunk.is_empty() {
            return Ok(());
        }

        // save
        let mut usage = progress_opt.map_or(StateStorageUsage::zero(), |p| p.usage);
        let (last_key, _last_value) = chunk.last().unwrap();
        let last_key_hash = CryptoHash::hash(last_key);

        // In case of TreeOnly Restore, we only restore the usage of KV without actually writing KV into DB
        for (k, v) in chunk.iter() {
            usage.add_item(k.key_size() + v.value_size());
        }

        // prepare the sharded kv batch
        let kv_batch: StateValueBatch<K, Option<V>> = chunk
            .into_iter()
            .map(|(k, v)| ((k, self.version), Some(v)))
            .collect();

        self.db.write_kv_batch(
            self.version,
            &kv_batch,
            StateSnapshotProgress::new(last_key_hash, usage),
        )
    }
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L228-258)
```rust
    fn add_chunk(&mut self, chunk: Vec<(K, V)>, proof: SparseMerkleRangeProof) -> Result<()> {
        let kv_fn = || {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_value_add_chunk"]);
            self.kv_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk(chunk.clone())
        };

        let tree_fn = || {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["jmt_add_chunk"]);
            self.tree_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk_impl(chunk.iter().map(|(k, v)| (k, v.hash())).collect(), proof)
        };
        match self.restore_mode {
            StateSnapshotRestoreMode::KvOnly => kv_fn()?,
            StateSnapshotRestoreMode::TreeOnly => tree_fn()?,
            StateSnapshotRestoreMode::Default => {
                // We run kv_fn with TreeOnly to restore the usage of DB
                let (r1, r2) = IO_POOL.join(kv_fn, tree_fn);
                r1?;
                r2?;
            },
        }

        Ok(())
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L339-413)
```rust
    pub fn add_chunk_impl(
        &mut self,
        mut chunk: Vec<(&K, HashValue)>,
        proof: SparseMerkleRangeProof,
    ) -> Result<()> {
        if self.finished {
            info!("State snapshot restore already finished, ignoring entire chunk.");
            return Ok(());
        }

        if let Some(prev_leaf) = &self.previous_leaf {
            let skip_until = chunk
                .iter()
                .find_position(|(key, _hash)| key.hash() > *prev_leaf.account_key());
            chunk = match skip_until {
                None => {
                    info!("Skipping entire chunk.");
                    return Ok(());
                },
                Some((0, _)) => chunk,
                Some((num_to_skip, next_leaf)) => {
                    info!(
                        num_to_skip = num_to_skip,
                        next_leaf = next_leaf,
                        "Skipping leaves."
                    );
                    chunk.split_off(num_to_skip)
                },
            }
        };
        if chunk.is_empty() {
            return Ok(());
        }

        for (key, value_hash) in chunk {
            let hashed_key = key.hash();
            if let Some(ref prev_leaf) = self.previous_leaf {
                ensure!(
                    &hashed_key > prev_leaf.account_key(),
                    "State keys must come in increasing order.",
                )
            }
            self.previous_leaf.replace(LeafNode::new(
                hashed_key,
                value_hash,
                (key.clone(), self.version),
            ));
            self.add_one(key, value_hash);
            self.num_keys_received += 1;
        }

        // Verify what we have added so far is all correct.
        self.verify(proof)?;

        // Write the frozen nodes to storage.
        if self.async_commit {
            self.wait_for_async_commit()?;
            let (tx, rx) = channel();
            self.async_commit_result = Some(rx);

            let mut frozen_nodes = HashMap::new();
            std::mem::swap(&mut frozen_nodes, &mut self.frozen_nodes);
            let store = self.store.clone();

            IO_POOL.spawn(move || {
                let res = store.write_node_batch(&frozen_nodes);
                tx.send(res).unwrap();
            });
        } else {
            self.store.write_node_batch(&self.frozen_nodes)?;
            self.frozen_nodes.clear();
        }

        Ok(())
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L628-697)
```rust
    fn verify(&self, proof: SparseMerkleRangeProof) -> Result<()> {
        let previous_leaf = self
            .previous_leaf
            .as_ref()
            .expect("The previous leaf must exist.");

        let previous_key = previous_leaf.account_key();
        // If we have all siblings on the path from root to `previous_key`, we should be able to
        // compute the root hash. The siblings on the right are already in the proof. Now we
        // compute the siblings on the left side, which represent all the states that have ever
        // been added.
        let mut left_siblings = vec![];

        // The following process might add some extra placeholder siblings on the left, but it is
        // nontrivial to determine when the loop should stop. So instead we just add these
        // siblings for now and get rid of them in the next step.
        let mut num_visited_right_siblings = 0;
        for (i, bit) in previous_key.iter_bits().enumerate() {
            if bit {
                // This node is a right child and there should be a sibling on the left.
                let sibling = if i >= self.partial_nodes.len() * 4 {
                    *SPARSE_MERKLE_PLACEHOLDER_HASH
                } else {
                    Self::compute_left_sibling(
                        &self.partial_nodes[i / 4],
                        previous_key.get_nibble(i / 4),
                        (3 - i % 4) as u8,
                    )
                };
                left_siblings.push(sibling);
            } else {
                // This node is a left child and there should be a sibling on the right.
                num_visited_right_siblings += 1;
            }
        }
        ensure!(
            num_visited_right_siblings >= proof.right_siblings().len(),
            "Too many right siblings in the proof.",
        );

        // Now we remove any extra placeholder siblings at the bottom. We keep removing the last
        // sibling if 1) it's a placeholder 2) it's a sibling on the left.
        for bit in previous_key.iter_bits().rev() {
            if bit {
                if *left_siblings.last().expect("This sibling must exist.")
                    == *SPARSE_MERKLE_PLACEHOLDER_HASH
                {
                    left_siblings.pop();
                } else {
                    break;
                }
            } else if num_visited_right_siblings > proof.right_siblings().len() {
                num_visited_right_siblings -= 1;
            } else {
                break;
            }
        }

        // Left siblings must use the same ordering as the right siblings in the proof
        left_siblings.reverse();

        // Verify the proof now that we have all the siblings
        proof
            .verify(
                self.expected_root_hash,
                SparseMerkleLeafNode::new(*previous_key, previous_leaf.value_hash()),
                left_siblings,
            )
            .map_err(Into::into)
    }
```

**File:** storage/backup/backup-cli/src/coordinators/restore.rs (L247-260)
```rust
                StateSnapshotRestoreController::new(
                    StateSnapshotRestoreOpt {
                        manifest_handle: kv_snapshot.manifest,
                        version: kv_snapshot.version,
                        validate_modules: false,
                        restore_mode: StateSnapshotRestoreMode::KvOnly,
                    },
                    self.global_opt.clone(),
                    Arc::clone(&self.storage),
                    epoch_history.clone(),
                )
                .run()
                .await?;
            }
```

**File:** storage/backup/backup-cli/src/coordinators/restore.rs (L289-302)
```rust
            TransactionRestoreBatchController::new(
                transaction_restore_opt,
                Arc::clone(&self.storage),
                txn_manifests,
                Some(db_next_version),
                Some((kv_replay_version, true /* only replay KV */)),
                epoch_history.clone(),
                VerifyExecutionMode::NoVerify,
                None,
            )
            .run()
            .await?;
            // update the expected version for the first phase restore
            db_next_version = tree_snapshot.version;
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L125-136)
```rust
        let (txn_info_with_proof, li): (TransactionInfoWithProof, LedgerInfoWithSignatures) =
            self.storage.load_bcs_file(&manifest.proof).await?;
        txn_info_with_proof.verify(li.ledger_info(), manifest.version)?;
        let state_root_hash = txn_info_with_proof
            .transaction_info()
            .ensure_state_checkpoint_hash()?;
        ensure!(
            state_root_hash == manifest.root_hash,
            "Root hash mismatch with that in proof. root hash: {}, expected: {}",
            manifest.root_hash,
            state_root_hash,
        );
```
