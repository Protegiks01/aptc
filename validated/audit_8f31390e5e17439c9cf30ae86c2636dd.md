# Audit Report

## Title
DKG Transcript Verification Resource Exhaustion via Failed Cryptographic Verification Retry Loop

## Summary
A malicious validator can exploit missing deduplication logic for failed DKG transcript verifications to force honest validators into repeatedly performing expensive cryptographic operations. The vulnerability exists because the `TranscriptAggregationState::add()` method only records successfully verified transcripts in the contributors set, allowing the reliable broadcast retry mechanism to re-process the same invalid transcript multiple times, causing significant CPU resource exhaustion during DKG.

## Finding Description

The vulnerability exists in the interaction between transcript aggregation and reliable broadcast retry logic in the DKG (Distributed Key Generation) system. When a DKG transcript fails cryptographic verification, the author is not added to the deduplication set, allowing repeated verification attempts.

**Critical Code Flow:**

During DKG epoch transitions, validators use reliable broadcast to collect transcripts. [1](#0-0) 

The reliable broadcast mechanism processes responses through `TranscriptAggregationState::add()` which performs multiple validation stages. [2](#0-1) 

**The Critical Vulnerability:**

The deduplication check at lines 92-94 returns early only if the author is already in the contributors set. [3](#0-2) 

Cryptographic verification occurs at lines 96-101, which includes expensive PVSS verification operations. [4](#0-3) 

**The author is only added to the contributors set at line 116, AFTER all verification checks pass.** [5](#0-4) 

If verification fails, the function returns an error WITHOUT adding the author to contributors, leaving no record that this transcript was already processed.

**Retry Mechanism Exploitation:**

When the `add()` method returns an error, the reliable broadcast mechanism automatically retries with exponential backoff. [6](#0-5) 

The exponential backoff is configured with parameters that allow multiple retry attempts. [7](#0-6) 

**Expensive Cryptographic Operations:**

Each retry performs the full PVSS verification suite, which includes:

- BLS signature batch verification via `batch_verify_soks` [8](#0-7) 
- Low-degree polynomial test on G2 group elements [9](#0-8) 
- Multiple multi-exponentiation operations on G1 and G2 [10](#0-9) 
- Multi-pairing check on BLS12-381 curve [11](#0-10) 

The full verification is invoked through `verify_transcript()` which coordinates all these checks. [12](#0-11) 

**Attack Scenario:**

1. A malicious validator crafts a DKG transcript with valid metadata (correct epoch, author) but invalid cryptographic proofs
2. Honest validators receive this transcript during reliable broadcast
3. The transcript passes basic checks (epoch, voting power, author match, BCS deserialization)
4. The transcript fails expensive cryptographic verification
5. The author is NOT added to the contributors set
6. Reliable broadcast retries the request to the same validator
7. Steps 3-6 repeat until quorum is reached from other validators
8. Each retry wastes significant CPU cycles on cryptographic verification

## Impact Explanation

**High Severity** - This vulnerability aligns with the "Validator node slowdowns" category in the Aptos bug bounty program (High Severity - up to $50,000).

**Concrete Impact:**
- Each malicious validator forces honest validators to repeatedly perform expensive BLS12-381 cryptographic operations (pairing computations require millions of CPU cycles)
- Multiple malicious validators (up to 1/3 of the validator set under Byzantine fault tolerance) can amplify the attack
- CPU resource exhaustion during DKG delays epoch transitions
- Delayed epoch transitions can cascade into broader consensus performance degradation
- The attack breaks the Resource Limits security invariant that operations must respect computational bounds

**Severity Justification:**
This is not a network-level DoS attack (which would be out of scope), but rather a protocol-level resource exhaustion vulnerability caused by missing deduplication logic in the DKG implementation. The bug bounty program explicitly includes "Validator node slowdowns" as a valid High Severity impact category.

## Likelihood Explanation

**High Likelihood** within the Byzantine fault tolerance threat model.

**Attack Requirements:**
- Attacker must be an active validator in the current epoch (required to participate in DKG)
- No collusion with other validators required - a single malicious validator is sufficient
- Attack is trivial to execute: simply respond with any DKG transcript that has valid BCS encoding but invalid cryptographic proofs

**Feasibility:**
- The Byzantine fault tolerance model explicitly assumes up to 1/3 of validators may be malicious
- DKG runs at every epoch transition, providing repeated attack opportunities
- No sophisticated cryptographic expertise required - attacker just provides invalid signatures or polynomial commitments
- The test suite confirms invalid transcripts are rejected but does not test repeated verification attempts [13](#0-12) 

## Recommendation

Add failed transcript authors to a separate tracking set to prevent re-verification of known-invalid transcripts:

```rust
pub struct TranscriptAggregator<S: DKGTrait> {
    pub contributors: HashSet<AccountAddress>,
    pub failed_contributors: HashSet<AccountAddress>,  // Add this
    pub trx: Option<S::Transcript>,
}
```

Modify the `add()` method to check and record failed verifications:

```rust
fn add(&self, sender: Author, dkg_transcript: DKGTranscript) -> anyhow::Result<Option<Self::Aggregated>> {
    // ... existing validation code ...
    
    let mut trx_aggregator = self.trx_aggregator.lock();
    
    // Check both successful and failed contributors
    if trx_aggregator.contributors.contains(&metadata.author) 
        || trx_aggregator.failed_contributors.contains(&metadata.author) {
        return Ok(None);
    }
    
    // ... verification code ...
    
    // On verification failure, record the author before returning error
    match S::verify_transcript(&self.dkg_pub_params, &transcript) {
        Ok(_) => {
            // Existing success path
            trx_aggregator.contributors.insert(metadata.author);
            // ... rest of aggregation logic ...
        },
        Err(e) => {
            trx_aggregator.failed_contributors.insert(metadata.author);
            return Err(anyhow!("[DKG] adding peer transcript failed with trx verification failure: {e}"));
        }
    }
}
```

## Proof of Concept

A complete PoC would require setting up a validator network and implementing a malicious validator that sends invalid transcripts. The key test case would demonstrate:

1. Configuring a reliable broadcast with aggressive retry settings
2. Having a mock validator respond with consistently invalid transcripts
3. Measuring that the same invalid transcript triggers multiple expensive verification calls
4. Verifying that CPU time is wasted proportional to the number of retries

The existing test suite validates that invalid transcripts are rejected but does not test the retry behavior or deduplication of failed verifications.

## Notes

This vulnerability is distinct from network-level DoS attacks. It exploits missing protocol-level deduplication logic to cause resource exhaustion through valid protocol messages. The attack vector is particularly concerning because:

1. It occurs during critical DKG phases that affect epoch transitions
2. The exponential backoff allows multiple retry attempts before timeout
3. Each verification involves multiple expensive cryptographic operations on BLS12-381 curves
4. The Byzantine fault tolerance model explicitly allows for this attacker capability (single malicious validator)

The vulnerability affects the current codebase and has not been addressed in the test suite, which only validates rejection of invalid transcripts but not the retry/deduplication behavior.

### Citations

**File:** dkg/src/agg_trx_producer.rs (L46-88)
```rust
    fn start_produce(
        &self,
        start_time: Duration,
        my_addr: AccountAddress,
        epoch_state: Arc<EpochState>,
        params: DKG::PublicParams,
        agg_trx_tx: Option<Sender<(), DKG::Transcript>>,
    ) -> AbortHandle {
        let epoch = epoch_state.epoch;
        let rb = self.reliable_broadcast.clone();
        let req = DKGTranscriptRequest::new(epoch_state.epoch);
        let agg_state = Arc::new(TranscriptAggregationState::<DKG>::new(
            start_time,
            my_addr,
            params,
            epoch_state,
        ));
        let task = async move {
            let agg_trx = rb
                .broadcast(req, agg_state)
                .await
                .expect("broadcast cannot fail");
            info!(
                epoch = epoch,
                my_addr = my_addr,
                "[DKG] aggregated transcript locally"
            );
            if let Err(e) = agg_trx_tx
                .expect("[DKG] agg_trx_tx should be available")
                .push((), agg_trx)
            {
                // If the `DKGManager` was dropped, this send will fail by design.
                info!(
                    epoch = epoch,
                    my_addr = my_addr,
                    "[DKG] Failed to send aggregated transcript to DKGManager, maybe DKGManager stopped and channel dropped: {:?}", e
                );
            }
        };
        let (abort_handle, abort_registration) = AbortHandle::new_pair();
        tokio::spawn(Abortable::new(task, abort_registration));
        abort_handle
    }
```

**File:** dkg/src/transcript_aggregation/mod.rs (L65-153)
```rust
    fn add(
        &self,
        sender: Author,
        dkg_transcript: DKGTranscript,
    ) -> anyhow::Result<Option<Self::Aggregated>> {
        let DKGTranscript {
            metadata,
            transcript_bytes,
        } = dkg_transcript;
        ensure!(
            metadata.epoch == self.epoch_state.epoch,
            "[DKG] adding peer transcript failed with invalid node epoch",
        );

        let peer_power = self.epoch_state.verifier.get_voting_power(&sender);
        ensure!(
            peer_power.is_some(),
            "[DKG] adding peer transcript failed with illegal dealer"
        );
        ensure!(
            metadata.author == sender,
            "[DKG] adding peer transcript failed with node author mismatch"
        );
        let transcript = bcs::from_bytes(transcript_bytes.as_slice()).map_err(|e| {
            anyhow!("[DKG] adding peer transcript failed with trx deserialization error: {e}")
        })?;
        let mut trx_aggregator = self.trx_aggregator.lock();
        if trx_aggregator.contributors.contains(&metadata.author) {
            return Ok(None);
        }

        S::verify_transcript_extra(&transcript, &self.epoch_state.verifier, false, Some(sender))
            .context("extra verification failed")?;

        S::verify_transcript(&self.dkg_pub_params, &transcript).map_err(|e| {
            anyhow!("[DKG] adding peer transcript failed with trx verification failure: {e}")
        })?;

        // All checks passed. Aggregating.
        let is_self = self.my_addr == sender;
        if !is_self && !self.valid_peer_transcript_seen {
            let secs_since_dkg_start =
                duration_since_epoch().as_secs_f64() - self.start_time.as_secs_f64();
            DKG_STAGE_SECONDS
                .with_label_values(&[
                    self.my_addr.to_hex().as_str(),
                    "first_valid_peer_transcript",
                ])
                .observe(secs_since_dkg_start);
        }

        trx_aggregator.contributors.insert(metadata.author);
        if let Some(agg_trx) = trx_aggregator.trx.as_mut() {
            S::aggregate_transcripts(&self.dkg_pub_params, agg_trx, transcript);
        } else {
            trx_aggregator.trx = Some(transcript);
        }
        let threshold = self.epoch_state.verifier.quorum_voting_power();
        let power_check_result = self
            .epoch_state
            .verifier
            .check_voting_power(trx_aggregator.contributors.iter(), true);
        let new_total_power = match &power_check_result {
            Ok(x) => Some(*x),
            Err(VerifyError::TooLittleVotingPower { voting_power, .. }) => Some(*voting_power),
            _ => None,
        };
        let maybe_aggregated = power_check_result
            .ok()
            .map(|_| trx_aggregator.trx.clone().unwrap());
        info!(
            epoch = self.epoch_state.epoch,
            peer = sender,
            is_self = is_self,
            peer_power = peer_power,
            new_total_power = new_total_power,
            threshold = threshold,
            threshold_exceeded = maybe_aggregated.is_some(),
            "[DKG] added transcript from validator {}, {} out of {} aggregated.",
            self.epoch_state
                .verifier
                .address_to_validator_index()
                .get(&sender)
                .unwrap(),
            new_total_power.unwrap_or(0),
            threshold
        );
        Ok(maybe_aggregated)
    }
```

**File:** crates/reliable-broadcast/src/lib.rs (L191-200)
```rust
                            Err(e) => {
                                log_rpc_failure(e, receiver);

                                let backoff_strategy = backoff_policies
                                    .get_mut(&receiver)
                                    .expect("should be present");
                                let duration = backoff_strategy.next().expect("should produce value");
                                rpc_futures
                                    .push(send_message(receiver, Some(duration)));
                            },
```

**File:** dkg/src/epoch_manager.rs (L212-216)
```rust
                ExponentialBackoff::from_millis(self.rb_config.backoff_policy_base_ms)
                    .factor(self.rb_config.backoff_policy_factor)
                    .max_delay(Duration::from_millis(
                        self.rb_config.backoff_policy_max_delay_ms,
                    )),
```

**File:** crates/aptos-dkg/src/pvss/das/unweighted_protocol.rs (L256-263)
```rust
        batch_verify_soks::<G2Projective, A>(
            self.soks.as_slice(),
            &g_2,
            &self.V[sc.n],
            spks,
            auxs,
            &extra[0],
        )?;
```

**File:** crates/aptos-dkg/src/pvss/das/unweighted_protocol.rs (L266-273)
```rust
        let ldt = LowDegreeTest::random(
            &mut rng,
            sc.t,
            sc.n + 1,
            true,
            sc.get_batch_evaluation_domain(),
        );
        ldt.low_degree_test_on_g2(&self.V)?;
```

**File:** crates/aptos-dkg/src/pvss/das/unweighted_protocol.rs (L288-296)
```rust
        let v = g2_multi_exp(&self.V[..self.V.len() - 1], taus.as_slice());
        let ek = g1_multi_exp(
            eks.iter()
                .map(|ek| Into::<G1Projective>::into(ek))
                .collect::<Vec<G1Projective>>()
                .as_slice(),
            taus.as_slice(),
        );
        let c = g1_multi_exp(self.C.as_slice(), taus.as_slice());
```

**File:** crates/aptos-dkg/src/pvss/das/unweighted_protocol.rs (L307-310)
```rust
        let res = multi_pairing(lhs.iter(), rhs.iter());
        if res != Gt::identity() {
            bail!("Expected zero, but got {} during multi-pairing check", res);
        }
```

**File:** types/src/dkg/real_dkg/mod.rs (L332-401)
```rust
    fn verify_transcript(
        params: &Self::PublicParams,
        trx: &Self::Transcript,
    ) -> anyhow::Result<()> {
        // Verify dealer indices are valid.
        let dealers = trx
            .main
            .get_dealers()
            .iter()
            .map(|player| player.id)
            .collect::<Vec<usize>>();
        let num_validators = params.session_metadata.dealer_validator_set.len();
        ensure!(
            dealers.iter().all(|id| *id < num_validators),
            "real_dkg::verify_transcript failed with invalid dealer index."
        );

        let all_eks = params.pvss_config.eks.clone();

        let addresses = params.verifier.get_ordered_account_addresses();
        let dealers_addresses = dealers
            .iter()
            .filter_map(|&pos| addresses.get(pos))
            .cloned()
            .collect::<Vec<_>>();

        let spks = dealers_addresses
            .iter()
            .filter_map(|author| params.verifier.get_public_key(author))
            .collect::<Vec<_>>();

        let aux = dealers_addresses
            .iter()
            .map(|address| (params.pvss_config.epoch, address))
            .collect::<Vec<_>>();

        trx.main.verify(
            &params.pvss_config.wconfig,
            &params.pvss_config.pp,
            &spks,
            &all_eks,
            &aux,
        )?;

        // Verify fast path is present if and only if fast_wconfig is present.
        ensure!(
            trx.fast.is_some() == params.pvss_config.fast_wconfig.is_some(),
            "real_dkg::verify_transcript failed with mismatched fast path flag in trx and params."
        );

        if let Some(fast_trx) = trx.fast.as_ref() {
            let fast_dealers = fast_trx
                .get_dealers()
                .iter()
                .map(|player| player.id)
                .collect::<Vec<usize>>();
            ensure!(
                dealers == fast_dealers,
                "real_dkg::verify_transcript failed with inconsistent dealer index."
            );
        }

        if let (Some(fast_trx), Some(fast_wconfig)) =
            (trx.fast.as_ref(), params.pvss_config.fast_wconfig.as_ref())
        {
            fast_trx.verify(fast_wconfig, &params.pvss_config.pp, &spks, &all_eks, &aux)?;
        }

        Ok(())
    }
```

**File:** dkg/src/transcript_aggregation/tests.rs (L108-118)
```rust
    // Node with invalid transcript should be rejected.
    let mut bad_trx_0_bytes = good_trx_0_bytes.clone();
    *bad_trx_0_bytes.last_mut().unwrap() = 0xAB;
    let result = trx_agg_state.add(addrs[0], DKGTranscript {
        metadata: DKGTranscriptMetadata {
            epoch: 999,
            author: addrs[0],
        },
        transcript_bytes: bad_trx_0_bytes,
    });
    assert!(result.is_err());
```
