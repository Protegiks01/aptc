# Audit Report

## Title
Byzantine Validator OptQS Denial-of-Service via Window Manipulation

## Summary
Byzantine validators can systematically disable Optimistic Quorum Store (OptQS) by proposing blocks with unavailable payload batches, causing the failure window to grow exponentially and preventing OptQS from re-enabling, thereby permanently degrading network throughput.

## Finding Description

The `ExponentialWindowFailureTracker` in the consensus layer tracks proposal failures to determine when to enable OptQS. Byzantine validators can exploit this mechanism to keep OptQS permanently disabled through the following attack path:

**Step 1: Window Doubling on Failure**

When a `PayloadUnavailable` timeout occurs, the failure tracker doubles the window size: [1](#0-0) 

The `last_consecutive_success_count` is reset to 0 on any `PayloadUnavailable` failure, and the window doubles until reaching `max_window` (typically 100).

**Step 2: OptQS Enabling Condition**

OptQS is only enabled when consecutive successes meet or exceed the window size: [2](#0-1) 

Once the window reaches 100, the system requires 100 consecutive successful proposals without any `PayloadUnavailable` failures to re-enable OptQS.

**Step 3: Byzantine Validator Creates Unavailable Batches**

When a Byzantine validator becomes proposer, they create an `OptQuorumStorePayload` with opt_batches that exist locally but were never broadcast to the network. The validation only checks that batch authors are valid validators, not that batches are available: [3](#0-2) 

This validation does NOT verify signatures on opt_batches or check batch availability on the network.

**Step 4: Payload Availability Check Fails**

When honest validators receive the proposal, they check if opt_batches exist locally: [4](#0-3) 

Missing batches cause the check to return an error with a BitVec of missing authors.

**Step 5: PayloadUnavailable Timeout Triggered**

This missing payload triggers a `PayloadUnavailable` timeout reason: [5](#0-4) 

The `missing_authors` BitVec is included in the timeout reason.

**Step 6: Attack Persistence**

Byzantine validators (even with <1/3 stake) become proposers proportionally to their stake. They can periodically trigger `PayloadUnavailable` failures (e.g., once every 99 rounds) to prevent the system from ever accumulating 100 consecutive successes, keeping OptQS disabled indefinitely.

The `exclude_authors` mechanism only prevents pulling batches from specific authors but doesn't prevent the window growth or the need for 100 consecutive successes: [6](#0-5) 

## Impact Explanation

This constitutes a **High Severity** vulnerability per the Aptos bug bounty criteria under "Validator node slowdowns" and "Significant protocol violations":

1. **Persistent Performance Degradation**: OptQS is a critical throughput optimization. The substantial infrastructure dedicated to OptQS (including ExponentialWindowFailureTracker, OptQSPullParamsProvider, and related configuration) indicates its importance to network performance.

2. **Network-Wide Impact**: Once OptQS is disabled, ALL validators and users experience reduced transaction throughput until manual intervention occurs.

3. **Indefinite Duration**: Unlike temporary network issues, this attack can persist indefinitely as long as Byzantine validators periodically trigger failures.

4. **Protocol Violation**: This represents a deliberate manipulation of the consensus protocol's optimization mechanism, not merely a performance issue.

While the network continues to function using standard quorum store payloads, the persistent throughput degradation constitutes a significant liveness attack that impacts the entire network's utility.

## Likelihood Explanation

**High likelihood** of exploitation:

1. **Natural Proposer Selection**: Byzantine validators with any stake percentage will become proposers proportionally (e.g., 10% stake = proposer ~10% of rounds). No special conditions or timing required. [7](#0-6) 

2. **Low Attack Cost**: Once the window reaches maximum size (100), maintaining the attack requires only periodic failures (approximately one failure per 100 rounds to prevent 100 consecutive successes).

3. **Stealth**: The attack appears as network issues or legitimate payload unavailability rather than obvious malicious behavior, making detection difficult.

4. **No Effective Mitigation**: The reputation system only penalizes validators whose failure rate exceeds a threshold (default 10%), so Byzantine validators can stay below this threshold while still maintaining the attack. [8](#0-7) 

5. **Coordination Potential**: Multiple Byzantine validators can coordinate to cycle through the `exclude_authors` list, further sustaining the attack.

## Recommendation

Implement a more robust OptQS re-enabling mechanism that:

1. **Distinguish Malicious vs. Legitimate Failures**: Track payload unavailability per-author and require consecutive successes only from non-excluded authors.

2. **Decay Window on Partial Success**: Instead of requiring 100 consecutive successes, implement a decay mechanism where the window shrinks after periods of mostly successful proposals (e.g., 95% success rate over the window).

3. **Author-Specific Penalties**: Instead of disabling OptQS globally, disable it only for proposals from authors with recent payload unavailability, while keeping it enabled for reputable validators.

4. **Cap Consecutive Success Requirement**: Even at max window size, cap the consecutive success requirement at a lower threshold (e.g., 20-30) to make recovery more feasible.

Example mitigation (modify `compute_failure_window` in proposal_status_tracker.rs):

```rust
fn compute_failure_window(&mut self) {
    self.last_consecutive_success_count = self.last_consecutive_statuses_matching(|reason| {
        !matches!(
            reason,
            NewRoundReason::Timeout(RoundTimeoutReason::PayloadUnavailable { .. })
        )
    });
    
    if self.last_consecutive_success_count == 0 {
        self.window *= 2;
        self.window = self.window.min(self.max_window);
    } else if self.last_consecutive_success_count >= self.window.min(30) { // Cap at 30
        // Allow reset with fewer successes when window is large
        self.window = 2;
    }
}
```

## Proof of Concept

The vulnerability can be demonstrated by:

1. Setting up a test network with Byzantine validators
2. Having a Byzantine validator become proposer
3. Creating an `OptQuorumStorePayload` with opt_batches that reference local batches never broadcast to the network
4. Observing honest validators timeout with `PayloadUnavailable`
5. Verifying the window grows to maximum
6. Attempting to re-enable OptQS and observing it requires 100 consecutive successes
7. Periodically triggering failures to prevent re-enabling

A complete PoC would require integration testing infrastructure to simulate the multi-validator environment, which is beyond the scope of this report but can be developed if needed.

## Notes

This vulnerability represents a design flaw in the OptQS failure tracking mechanism rather than an implementation bug. The core issue is that the `ExponentialWindowFailureTracker` treats all `PayloadUnavailable` failures equally, regardless of whether they result from malicious behavior or legitimate network issues. This allows Byzantine validators to exploit the mechanism by strategically timing their failures to prevent OptQS from ever re-enabling once the window reaches maximum size.

The attack is particularly insidious because it requires minimal coordination, can be executed by validators with very small stake percentages, and appears as legitimate network issues rather than obvious attacks. The lack of effective mitigation (reputation system doesn't prevent low-frequency failures, exclude_authors doesn't prevent window growth) makes this a persistent threat to network performance.

### Citations

**File:** consensus/src/liveness/proposal_status_tracker.rs (L65-78)
```rust
    fn compute_failure_window(&mut self) {
        self.last_consecutive_success_count = self.last_consecutive_statuses_matching(|reason| {
            !matches!(
                reason,
                NewRoundReason::Timeout(RoundTimeoutReason::PayloadUnavailable { .. })
            )
        });
        if self.last_consecutive_success_count == 0 {
            self.window *= 2;
            self.window = self.window.min(self.max_window);
        } else if self.last_consecutive_success_count == self.past_round_statuses.len() {
            self.window = 2;
        }
    }
```

**File:** consensus/src/liveness/proposal_status_tracker.rs (L80-98)
```rust
    fn get_exclude_authors(&self) -> HashSet<Author> {
        let mut exclude_authors = HashSet::new();

        let limit = self.window;
        for round_reason in self.past_round_statuses.iter().rev().take(limit) {
            if let NewRoundReason::Timeout(RoundTimeoutReason::PayloadUnavailable {
                missing_authors,
            }) = round_reason
            {
                for author_idx in missing_authors.iter_ones() {
                    if let Some(author) = self.ordered_authors.get(author_idx) {
                        exclude_authors.insert(*author);
                    }
                }
            }
        }

        exclude_authors
    }
```

**File:** consensus/src/liveness/proposal_status_tracker.rs (L127-143)
```rust
impl TOptQSPullParamsProvider for OptQSPullParamsProvider {
    fn get_params(&self) -> Option<OptQSPayloadPullParams> {
        if !self.enable_opt_qs {
            return None;
        }

        let tracker = self.failure_tracker.lock();

        counters::OPTQS_LAST_CONSECUTIVE_SUCCESS_COUNT
            .observe(tracker.last_consecutive_success_count as f64);
        if tracker.last_consecutive_success_count < tracker.window {
            warn!(
                "Skipping OptQS: (last_consecutive_successes) {} < {} (window)",
                tracker.last_consecutive_success_count, tracker.window
            );
            return None;
        }
```

**File:** consensus/consensus-types/src/common.rs (L558-572)
```rust
    pub fn verify_opt_batches<T: TBatchInfo>(
        verifier: &ValidatorVerifier,
        opt_batches: &OptBatches<T>,
    ) -> anyhow::Result<()> {
        let authors = verifier.address_to_validator_index();
        for batch in &opt_batches.batch_summary {
            ensure!(
                authors.contains_key(&batch.author()),
                "Invalid author {} for batch {}",
                batch.author(),
                batch.digest()
            );
        }
        Ok(())
    }
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L409-424)
```rust
            Payload::OptQuorumStore(OptQuorumStorePayload::V1(p)) => {
                let mut missing_authors = BitVec::with_num_bits(self.ordered_authors.len() as u16);
                for batch in p.opt_batches().deref() {
                    if self.batch_reader.exists(batch.digest()).is_none() {
                        let index = *self
                            .address_to_validator_index
                            .get(&batch.author())
                            .expect("Payload author should have been verified");
                        missing_authors.set(index as u16);
                    }
                }
                if missing_authors.all_zeros() {
                    Ok(())
                } else {
                    Err(missing_authors)
                }
```

**File:** consensus/src/round_manager.rs (L968-983)
```rust
    fn compute_timeout_reason(&self, round: Round) -> RoundTimeoutReason {
        if self.round_state().vote_sent().is_some() {
            return RoundTimeoutReason::NoQC;
        }

        match self.block_store.get_block_for_round(round) {
            None => RoundTimeoutReason::ProposalNotReceived,
            Some(block) => {
                if let Err(missing_authors) = self.block_store.check_payload(block.block()) {
                    RoundTimeoutReason::PayloadUnavailable { missing_authors }
                } else {
                    RoundTimeoutReason::Unknown
                }
            },
        }
    }
```

**File:** consensus/src/liveness/leader_reputation.rs (L486-550)
```rust
pub struct ProposerAndVoterHeuristic {
    author: Author,
    active_weight: u64,
    inactive_weight: u64,
    failed_weight: u64,
    failure_threshold_percent: u32,
    aggregation: NewBlockEventAggregation,
}

impl ProposerAndVoterHeuristic {
    pub fn new(
        author: Author,
        active_weight: u64,
        inactive_weight: u64,
        failed_weight: u64,
        failure_threshold_percent: u32,
        voter_window_size: usize,
        proposer_window_size: usize,
        reputation_window_from_stale_end: bool,
    ) -> Self {
        Self {
            author,
            active_weight,
            inactive_weight,
            failed_weight,
            failure_threshold_percent,
            aggregation: NewBlockEventAggregation::new(
                voter_window_size,
                proposer_window_size,
                reputation_window_from_stale_end,
            ),
        }
    }
}

impl ReputationHeuristic for ProposerAndVoterHeuristic {
    fn get_weights(
        &self,
        epoch: u64,
        epoch_to_candidates: &HashMap<u64, Vec<Author>>,
        history: &[NewBlockEvent],
    ) -> Vec<u64> {
        assert!(epoch_to_candidates.contains_key(&epoch));

        let (votes, proposals, failed_proposals) =
            self.aggregation
                .get_aggregated_metrics(epoch_to_candidates, history, &self.author);

        epoch_to_candidates[&epoch]
            .iter()
            .map(|author| {
                let cur_votes = *votes.get(author).unwrap_or(&0);
                let cur_proposals = *proposals.get(author).unwrap_or(&0);
                let cur_failed_proposals = *failed_proposals.get(author).unwrap_or(&0);

                if cur_failed_proposals * 100
                    > (cur_proposals + cur_failed_proposals) * self.failure_threshold_percent
                {
                    self.failed_weight
                } else if cur_proposals > 0 || cur_votes > 0 {
                    self.active_weight
                } else {
                    self.inactive_weight
                }
            })
```
