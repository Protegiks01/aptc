# Audit Report

## Title
Critical Consensus Message Loss via Premature Broadcast Abort in DAG Driver

## Summary
The DAG consensus implementation contains a critical vulnerability where reliable broadcast tasks are prematurely aborted when evicted from a bounded queue, causing certified nodes to be lost without proper delivery to all validators. This violates consensus liveness guarantees and can lead to network-wide availability failures.

## Finding Description

The vulnerability stems from a design flaw in the interaction between the DAG driver's bounded broadcast handle queue and the reliable broadcast mechanism that attempts to deliver messages to ALL validators.

The `rb_handles` field stores broadcast abort handles in a `BoundedVecDeque` with capacity set to `window_size_config`. [1](#0-0) [2](#0-1) 

The default window size is 10 rounds for DAG consensus. [3](#0-2) 

When broadcasting a new node, if the queue is full, `push_back()` evicts the oldest handle and returns it. [4](#0-3) [5](#0-4) 

The evicted `DropGuard` automatically triggers abort when dropped, cancelling the async broadcast task. [6](#0-5) 

**The Critical Issue**: The broadcast mechanism waits for responses from ALL validators, not just quorum. The `SignatureBuilder` explicitly documents this behavior and only returns `Some(())` when votes are received from ALL validators (len == verifier.len()). [7](#0-6) [8](#0-7) 

Similarly, `CertificateAckState` waits for acknowledgments from all validators before completing. [9](#0-8) 

The reliable broadcast loop continues retrying failed requests until aggregation completes for all peers, as indicated by the unreachable assertion "Should aggregate with all responses". [10](#0-9) [11](#0-10) 

**Attack Scenario**:
1. Validator V broadcasts a node at round R
2. Signature collection reaches quorum (2f+1 validators {A,B,C}) and creates certificate
3. Certified node broadcast begins to ALL validators but some are slow/partitioned
4. Rounds R+1 through R+10 complete quickly (10 new broadcasts)
5. At round R+11, the new broadcast evicts round R's handle from the bounded queue
6. Round R's broadcast task is aborted before validators {D,E,F} receive the certified node

**Recovery Mechanism Fails**: When validators encounter this node as a parent dependency, they attempt to fetch it. However, fetch is limited to validators who signed the certificate. [12](#0-11) 

If the specific certificate signers {A,B,C} are currently unreachable (crashed, partitioned, or offline), the fetch fails. The currently available validators {D,E,F} cannot progress consensus for any nodes depending on this parent, despite being 2f+1 validators.

The exponential backoff configuration shows broadcasts can take up to 3 seconds with retries, making it feasible for 10+ rounds to complete during a single slow broadcast. [13](#0-12) 

Developer TODO comments indicate awareness of potential issues with the bounded queue mechanism but the fundamental problem remains unaddressed. [14](#0-13) [15](#0-14) 

## Impact Explanation

This vulnerability meets **Critical Severity** criteria per the Aptos bug bounty program under "Total Loss of Liveness/Network Availability":

- **Consensus Liveness Failure**: Validators missing certified nodes cannot add any nodes that depend on them as parents, effectively halting DAG progression for those validators.

- **Network-Wide Impact**: If multiple validators are affected by different missing nodes, the network cannot achieve quorum for new rounds, causing total network liveness loss.

- **Cascading Failures**: Missing nodes from round R block all dependent nodes in rounds R+1, R+2, etc., creating a cascade that prevents forward progress.

- **Fundamental Liveness Violation**: This violates the core consensus guarantee that the system should make progress when 2f+1 validators are available, because the required 2f+1 validators (historical certificate signers) may differ from the currently available 2f+1 validators. The fetch mechanism's artificial limitation to historical signers breaks this property.

## Likelihood Explanation

This vulnerability has **Medium to High** likelihood:

**Contributing Factors**:
- Broadcasts taking longer than 10 rounds is feasible: with exponential backoff reaching 3 seconds and sub-second round times in optimal conditions, 10 rounds can complete in 5-10 seconds while broadcasts may still be retrying
- Network latency spikes are common in production blockchain environments
- Validator restarts or temporary partitions occur regularly during normal operations  
- No attacker required - this happens naturally during normal network variance
- The window size of 10 is relatively small compared to potential broadcast delays

**Realistic Scenario**: A validator with temporarily poor connectivity collects signatures and creates a certificate, but before the certified node broadcast completes to all validators, the network conditions improve and rounds progress quickly, evicting the slow broadcast. The validator then crashes or remains partitioned, making the certified node unrecoverable by the subset of validators who never received it.

## Recommendation

**Immediate Fix**: Modify the broadcast abort mechanism to ensure certified nodes are delivered to all validators before allowing eviction:

1. **Change queue eviction policy**: Only evict broadcast handles after confirming the certified node broadcast has completed to all validators (received all CertifiedAck responses), not based solely on round progression.

2. **Expand fetch responders**: Allow fetching certified nodes from ANY validator who has them, not just certificate signers. Validators who received the certified node during the broadcast can serve it.

3. **Implement durable storage**: Persist certified nodes to storage immediately after certificate creation, allowing recovery even if all original signers go offline.

4. **Increase window size**: Consider making the window size configurable and larger by default (e.g., 50 rounds) to reduce premature evictions.

**Alternative approach**: Replace the time-based bounded queue with a completion-based one that only evicts broadcasts after they successfully complete, with a much higher capacity limit to prevent memory exhaustion.

## Proof of Concept

The vulnerability can be demonstrated through the following scenario:

```rust
// Scenario setup:
// - Network with 4 validators {A, B, C, D}
// - Quorum = 3 validators
// - Window size = 10

// Round R:
// 1. Validator A broadcasts Node N
// 2. Validators A, B, C respond with Votes (quorum reached)
// 3. Certificate created and CertifiedNode broadcast starts
// 4. Validators A, B, C receive and ack CertifiedNode
// 5. Validator D has network issues, doesn't receive CertifiedNode yet

// Rounds R+1 through R+10:
// - 10 more broadcasts happen quickly
// - Each new broadcast adds a handle to rb_handles queue

// Round R+11:
// - New broadcast triggers push_back() on full queue
// - Round R's DropGuard is evicted and dropped
// - AbortHandle.abort() cancels the broadcast task
// - Validator D never receives the CertifiedNode for round R

// Later:
// - Validators A, B, C go offline
// - Validator D tries to build new node with N as parent
// - D requests fetch from certificate signers {A, B, C}
// - All signers are offline, fetch fails
// - D cannot make progress despite being available

// Result: Liveness failure with only 1/4 validators Byzantine (none)
```

The vulnerability is inherent in the design and can be triggered through normal network conditions without any malicious behavior.

### Citations

**File:** consensus/src/dag/dag_driver.rs (L57-57)
```rust
    rb_handles: Mutex<BoundedVecDeque<(DropGuard, u64)>>,
```

**File:** consensus/src/dag/dag_driver.rs (L103-103)
```rust
            rb_handles: Mutex::new(BoundedVecDeque::new(window_size_config as usize)),
```

**File:** consensus/src/dag/dag_driver.rs (L371-372)
```rust
        // TODO: a bounded vec queue can hold more than window rounds, but we want to limit
        // by number of rounds.
```

**File:** consensus/src/dag/dag_driver.rs (L373-380)
```rust
        if let Some((_handle, prev_round_timestamp)) = self
            .rb_handles
            .lock()
            .push_back((DropGuard::new(abort_handle), timestamp))
        {
            // TODO: this observation is inaccurate.
            observe_round(prev_round_timestamp, RoundStage::Finished);
        }
```

**File:** types/src/on_chain_config/consensus_config.rs (L594-594)
```rust
            dag_ordering_causal_history_window: 10,
```

**File:** crates/aptos-collections/src/bounded_vec_deque.rs (L28-38)
```rust
    pub fn push_back(&mut self, item: T) -> Option<T> {
        let oldest = if self.is_full() {
            self.inner.pop_front()
        } else {
            None
        };

        self.inner.push_back(item);
        assert!(self.inner.len() <= self.capacity);
        oldest
    }
```

**File:** crates/reliable-broadcast/src/lib.rs (L186-189)
```rust
                            Ok(may_be_aggragated) => {
                                if let Some(aggregated) = may_be_aggragated {
                                    return Ok(aggregated);
                                }
```

**File:** crates/reliable-broadcast/src/lib.rs (L203-203)
```rust
                    else => unreachable!("Should aggregate with all responses")
```

**File:** crates/reliable-broadcast/src/lib.rs (L232-236)
```rust
impl Drop for DropGuard {
    fn drop(&mut self) {
        self.abort_handle.abort();
    }
}
```

**File:** consensus/src/dag/types.rs (L562-564)
```rust
    /// Processes the [Vote]s received for a given [Node]. Once a supermajority voting power
    /// is reached, this method sends [NodeCertificate] into a channel. It will only return
    /// successfully when [Vote]s are received from all the peers.
```

**File:** consensus/src/dag/types.rs (L600-604)
```rust
        if partial_signatures.signatures().len() == self.epoch_state.verifier.len() {
            Ok(Some(()))
        } else {
            Ok(None)
        }
```

**File:** consensus/src/dag/types.rs (L656-660)
```rust
        if received.len() == self.num_validators {
            Ok(Some(()))
        } else {
            Ok(None)
        }
```

**File:** consensus/src/dag/dag_fetcher.rs (L108-110)
```rust
            LocalFetchRequest::CertifiedNode(node, _) => {
                node.signatures().get_signers_addresses(validators)
            },
```

**File:** config/src/config/dag_consensus_config.rs (L115-118)
```rust
            // A backoff policy that starts at 100ms and doubles each iteration up to 3secs.
            backoff_policy_base_ms: 2,
            backoff_policy_factor: 50,
            backoff_policy_max_delay_ms: 3000,
```
