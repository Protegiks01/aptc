# Audit Report

## Title
Non-Atomic Dual-Database Pruning in TransactionPruner Violates DBSubPruner Contract and Causes State Inconsistency

## Summary
The `TransactionPruner::prune()` and `EventStorePruner::prune()` implementations violate the DBSubPruner trait contract by performing non-atomic writes across two separate databases (internal indexer DB and main ledger DB) when the internal indexer is enabled. This creates a window for database inconsistency if a failure occurs between the two write operations, leading to query correctness violations and permanent metadata inconsistency.

## Finding Description

The `DBSubPruner` trait defines a contract for sub-pruners that perform pruning operations between a current progress version and a target version. [1](#0-0) 

The `TransactionPruner::prune()` implementation violates this contract when `internal_indexer_db` is enabled with transaction indexing. The method performs TWO SEPARATE ATOMIC WRITES to different databases:

**First Write** (to indexer DB): The indexer database write occurs with its own progress metadata update and transaction-by-account pruning. [2](#0-1) 

**Second Write** (to main DB): The main ledger database write occurs separately with its own progress metadata update. [3](#0-2)  and [4](#0-3) 

If the first write (line 67) succeeds and the second write (line 73) fails due to a system crash, disk I/O error, or power loss, the system enters an inconsistent state where:
- **Indexer DB**: `TransactionPrunerProgress = target_version`, account transaction indices pruned
- **Main DB**: `TransactionPrunerProgress = current_progress`, transaction data NOT pruned

The pruner initialization only reads progress from the main database, not the indexer database. [5](#0-4) 

The same vulnerability exists in `EventStorePruner::prune()`, which performs separate writes to the indexer database (line 78) and main database (line 80). [6](#0-5) 

**Query Impact**: When database sharding is enabled, the REST API routes account transaction queries to the indexer database instead of the main database. [7](#0-6) 

If the indexer has pruned transactions that still exist in the main database, queries will incorrectly return "not found" for transactions that should be accessible according to the main database's state.

The startup consistency check in `InternalIndexerDBService::get_start_version()` only validates consistency between different indexer types (state, transaction, event versions). [8](#0-7) 

This check does NOT verify that indexer pruner progress matches main database pruner progress, allowing the inconsistency to persist undetected across node restarts.

## Impact Explanation

**Medium Severity** - This issue constitutes a limited protocol violation with the following impacts:

1. **State Consistency Violation**: The storage system maintains two databases that should remain synchronized. This vulnerability violates the invariant that state transitions must be atomic and consistent across storage components.

2. **Query Correctness Violation**: In sharded database configurations, API queries return incorrect "not found" responses for transactions that still exist in the canonical main database, causing data availability issues for users and applications querying the REST API.

3. **Metadata Inconsistency**: Two separate progress trackers (`DbMetadataKey::TransactionPrunerProgress` and `IndexerMetadataKey::TransactionPrunerProgress`) can diverge permanently without automatic detection or recovery mechanisms.

4. **DBSubPruner Contract Violation**: The implementation violates implicit requirements for idempotency (system state differs after partial vs. complete success) and atomicity (operation is not atomic across both databases).

This qualifies as **Medium Severity** per Aptos bug bounty criteria as it represents a "limited protocol violation" with "state inconsistencies requiring manual intervention." While it affects storage system correctness and API query availability, it does not cause loss of funds, consensus violations, validator crashes, or permanent network disruption.

## Likelihood Explanation

**Medium-High Likelihood**:

1. **Trigger Conditions**: Any system crash, process termination, disk I/O error, or power failure occurring during the narrow window between the two database writes will trigger the vulnerability. These are realistic operational scenarios in production environments.

2. **Frequency**: Pruning operations occur regularly on active nodes with pruning enabled. The more frequent the pruning operations, the higher the probability of hitting the failure window during a system failure event.

3. **Configuration**: This affects nodes with `internal_indexer_db` enabled and transaction/event indexing enabled, which is common in public API nodes and indexer infrastructure.

4. **Detection Difficulty**: The inconsistency is not detected by startup consistency checks and requires manual investigation to discover, allowing the issue to persist unnoticed until query failures are observed.

5. **Real-World Scenarios**: Storage system failures, process crashes, and power losses are realistic operational scenarios in distributed systems, not theoretical edge cases.

## Recommendation

Implement atomic writes across both databases using one of the following approaches:

**Option 1: Use a two-phase commit protocol** to ensure both database writes succeed or both are rolled back.

**Option 2: Write progress metadata only to the main database** and have both pruning operations update the same progress tracker, ensuring atomicity through a single write operation.

**Option 3: Add consistency validation** that checks indexer pruner progress against main database pruner progress during startup in `InternalIndexerDBService::get_start_version()`, and panic or auto-repair if divergence is detected.

**Option 4: Defer indexer database writes** until after the main database write succeeds, but handle rollback if the indexer write fails (requires transactional semantics across both databases).

The most robust solution would be Option 1 or 2, as they prevent the inconsistency from occurring in the first place rather than detecting it after the fact.

## Proof of Concept

The vulnerability can be demonstrated through the following scenario:

1. Start a node with `internal_indexer_db` enabled and `enable_transaction: true`
2. Allow pruning to begin execution in `TransactionPruner::prune()`
3. After the indexer database write completes (line 67) but before the main database write (line 73), simulate a crash by killing the process or triggering a disk I/O error
4. Restart the node
5. Observe that:
   - The indexer database has `TransactionPrunerProgress = target_version`
   - The main database has `TransactionPrunerProgress = old_version`
   - Queries to the REST API with sharding enabled return "not found" for transactions that exist in the main database but were pruned from the indexer

A full integration test would require triggering disk failures or process crashes at precise timing, which is difficult to reproduce deterministically in automated tests. However, the code paths are clearly visible in the source code as documented above.

## Notes

This vulnerability represents a genuine state consistency issue in the storage layer that violates the atomicity guarantees expected from database pruning operations. While it does not directly cause loss of funds or consensus violations, it does cause incorrect query results and permanent metadata inconsistency that requires manual intervention to resolve. The severity is assessed as Medium rather than High because it does not meet the criteria for High severity impacts (validator slowdowns or API crashes) as defined in the Aptos bug bounty program.

### Citations

**File:** storage/aptosdb/src/pruner/db_sub_pruner.rs (L6-14)
```rust
/// Defines the trait for sub-pruner of a parent DB pruner
pub trait DBSubPruner {
    /// Returns the name of the sub pruner.
    fn name(&self) -> &str;

    /// Performs the actual pruning, a target version is passed, which is the target the pruner
    /// tries to prune.
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()>;
}
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L54-57)
```rust
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::TransactionPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L58-68)
```rust
        if let Some(indexer_db) = self.internal_indexer_db.as_ref() {
            if indexer_db.transaction_enabled() {
                let mut index_batch = SchemaBatch::new();
                self.transaction_store
                    .prune_transaction_by_account(&candidate_transactions, &mut index_batch)?;
                index_batch.put::<InternalIndexerMetadataSchema>(
                    &IndexerMetadataKey::TransactionPrunerProgress,
                    &IndexerMetadataValue::Version(target_version),
                )?;
                indexer_db.get_inner_db_ref().write_schemas(index_batch)?;
            } else {
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L73-73)
```rust
        self.ledger_db.transaction_db().write_schemas(batch)
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L84-88)
```rust
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.transaction_db_raw(),
            &DbMetadataKey::TransactionPrunerProgress,
            metadata_progress,
        )?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L71-81)
```rust
        if let Some(mut indexer_batch) = indexer_batch {
            indexer_batch.put::<InternalIndexerMetadataSchema>(
                &IndexerMetadataKey::EventPrunerProgress,
                &IndexerMetadataValue::Version(target_version),
            )?;
            self.expect_indexer_db()
                .get_inner_db_ref()
                .write_schemas(indexer_batch)?;
        }
        self.ledger_db.event_db().write_schemas(batch)
    }
```

**File:** api/src/context.rs (L900-922)
```rust
        let txns_res = if !db_sharding_enabled(&self.node_config) {
            self.db.get_account_ordered_transactions(
                address,
                start_seq_number,
                limit as u64,
                true,
                ledger_version,
            )
        } else {
            self.indexer_reader
                .as_ref()
                .ok_or_else(|| anyhow!("Indexer reader is None"))
                .map_err(|err| {
                    E::internal_with_code(err, AptosErrorCode::InternalError, ledger_info)
                })?
                .get_account_ordered_transactions(
                    address,
                    start_seq_number,
                    limit as u64,
                    true,
                    ledger_version,
                )
                .map_err(|e| AptosDbError::Other(e.to_string()))
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/internal_indexer_db_service.rs (L88-139)
```rust
    pub async fn get_start_version(&self, node_config: &NodeConfig) -> Result<Version> {
        let fast_sync_enabled = node_config
            .state_sync
            .state_sync_driver
            .bootstrapping_mode
            .is_fast_sync();
        let mut main_db_synced_version = self.db_indexer.main_db_reader.ensure_synced_version()?;

        // Wait till fast sync is done
        while fast_sync_enabled && main_db_synced_version == 0 {
            tokio::time::sleep(std::time::Duration::from_secs(1)).await;
            main_db_synced_version = self.db_indexer.main_db_reader.ensure_synced_version()?;
        }

        let start_version = self
            .db_indexer
            .indexer_db
            .get_persisted_version()?
            .map_or(0, |v| v + 1);

        if node_config.indexer_db_config.enable_statekeys() {
            let state_start_version = self
                .db_indexer
                .indexer_db
                .get_state_version()?
                .map_or(0, |v| v + 1);
            if start_version != state_start_version {
                panic!("Cannot start state indexer because the progress doesn't match.");
            }
        }

        if node_config.indexer_db_config.enable_transaction() {
            let transaction_start_version = self
                .db_indexer
                .indexer_db
                .get_transaction_version()?
                .map_or(0, |v| v + 1);
            if start_version != transaction_start_version {
                panic!("Cannot start transaction indexer because the progress doesn't match.");
            }
        }

        if node_config.indexer_db_config.enable_event() {
            let event_start_version = self
                .db_indexer
                .indexer_db
                .get_event_version()?
                .map_or(0, |v| v + 1);
            if start_version != event_start_version {
                panic!("Cannot start event indexer because the progress doesn't match.");
            }
        }
```
