# Audit Report

## Title
Memory Exhaustion Vulnerability in State Merkle Pruner During Shard Catch-Up

## Summary
The state merkle pruner's shard initialization logic can cause out-of-memory (OOM) crashes when catching up large version ranges. During shard pruner initialization, the code uses an unbounded limit (`usize::MAX`) to load all stale node indices from potentially millions of versions into memory at once, causing validator crashes when significant gaps exist between shard progress and metadata progress.

## Finding Description

The vulnerability exists in the shard pruner initialization path where memory-bounded batch processing is bypassed.

**Critical Vulnerability Path:**

During shard pruner initialization, the catch-up logic calls `prune()` with an unbounded limit: [1](#0-0) 

This propagates through the `prune()` method which invokes `get_stale_node_indices` with the unbounded `max_nodes_to_prune` parameter: [2](#0-1) 

The `get_stale_node_indices` function accumulates ALL stale node indices into a Vec when `limit` equals `usize::MAX`: [3](#0-2) 

**Why This Violates Security Guarantees:**

The codebase defines a `batch_size` configuration (default 1,000) specifically to limit memory consumption during pruning operations: [4](#0-3) 

During normal operation, the `PrunerWorker` respects this batch size: [5](#0-4) [6](#0-5) 

However, during initialization, this safeguard is completely bypassed by using `usize::MAX`.

**Triggering Scenario:**

The gap between shard progress and metadata progress can occur when:
1. Validator crashes after metadata pruner updates but before shard pruner completes
2. Shard pruner encounters failures and stops updating its progress
3. Database restoration from an old backup where shard DBs are behind metadata DB
4. Extended operational issues causing shards to fall behind over time

The `get_or_initialize_subpruner_progress` function retrieves existing shard progress from the database: [7](#0-6) 

If this returns an old progress value while metadata_progress is far ahead, the catch-up call attempts to load the entire gap into memory.

**Memory Impact:**

The `StaleNodeIndex` structure contains a version number and a `NodeKey` with `NibblePath`: [8](#0-7) [9](#0-8) [10](#0-9) 

Based on the configuration comment indicating "300k JMT nodes" for a 10k transaction block, a gap of 10 million versions could generate billions of stale node indices. With each `StaleNodeIndex` consuming approximately 64-80 bytes, memory consumption easily exceeds typical validator RAM (64-128 GB).

## Impact Explanation

**High Severity** per Aptos Bug Bounty criteria:

This vulnerability causes **Validator Node Crashes** which is explicitly listed as HIGH severity in the bug bounty program. The impact includes:

1. **Immediate validator termination**: OOM condition kills the validator process
2. **Loss of consensus participation**: Crashed validators cannot participate in AptosBFT consensus
3. **Repeated failure loop**: Every restart attempt triggers the same OOM crash until manual intervention
4. **Potential network impact**: If multiple validators encounter this simultaneously (e.g., after coordinated restarts or upgrades), network liveness could be degraded

This does NOT reach Critical severity because:
- No permanent network partition occurs
- No fund loss or theft is possible
- Recovery is achievable through manual intervention (reducing gap or applying code fix)
- It requires specific operational conditions rather than being universally triggerable

The severity aligns with the "Validator Node Slowdowns/Crashes (High)" category in the Aptos Bug Bounty program.

## Likelihood Explanation

**Medium Likelihood:**

The vulnerability requires three conditions:
1. **Sharding enabled**: Increasingly common for scalability, as evidenced by the codebase architecture [11](#0-10) 

2. **Significant gap exists**: Between shard progress and metadata progress  
3. **Validator initialization**: System restart or pruner reinitialization

Realistic triggering scenarios include:
- **Post-crash recovery**: Validator crashes mid-pruning, leaving shards behind
- **Pruner failures**: Silent failures in shard pruning that accumulate over time
- **Database restoration**: Recovery from backups with temporal inconsistencies
- **Operational issues**: Extended periods where shard pruners fail while metadata progresses

This is NOT a high likelihood vulnerability because:
- Normal operation keeps metadata and shard progress synchronized
- Gap accumulation requires sustained failures or crashes
- Not externally exploitable by attackers
- Requires validator operator actions or system events

However, it represents a serious operational risk that affects validator availability during legitimate maintenance and recovery procedures.

## Recommendation

Replace the unbounded `usize::MAX` limit during initialization with a bounded batch approach that respects the configured `batch_size` parameter. The initialization should iterate in batches similar to normal operation:

```rust
// In StateMerkleShardPruner::new()
// Instead of:
// myself.prune(progress, metadata_progress, usize::MAX)?;

// Use batched catch-up:
let batch_size = 10_000; // Or use configured batch_size
while progress < metadata_progress {
    myself.prune(progress, metadata_progress, batch_size)?;
    progress = myself.get_current_progress()?;
}
```

This ensures memory consumption remains bounded during initialization while still allowing the shard to catch up to the metadata progress.

## Proof of Concept

The vulnerability can be demonstrated by:

1. Running a validator with sharding enabled
2. Artificially creating a gap between shard progress and metadata progress (e.g., by stopping shard pruning while allowing metadata pruning to continue)
3. Restarting the validator to trigger shard initialization
4. Observing OOM crash when the gap exceeds available memory

The exact PoC would require a test harness that can simulate large version gaps and measure memory consumption during initialization.

## Notes

This is a legitimate operational security vulnerability affecting validator availability. While it requires specific operational conditions to trigger, these conditions are realistic and can occur during normal validator operations, maintenance, or recovery procedures. The fix should maintain backward compatibility while preventing unbounded memory allocation during initialization.

### Citations

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L53-53)
```rust
        myself.prune(progress, metadata_progress, usize::MAX)?;
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L66-71)
```rust
            let (indices, next_version) = StateMerklePruner::get_stale_node_indices(
                &self.db_shard,
                current_progress,
                target_version,
                max_nodes_to_prune,
            )?;
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L136-149)
```rust
        let shard_pruners = if state_merkle_db.sharding_enabled() {
            let num_shards = state_merkle_db.num_shards();
            let mut shard_pruners = Vec::with_capacity(num_shards);
            for shard_id in 0..num_shards {
                shard_pruners.push(StateMerkleShardPruner::new(
                    shard_id,
                    state_merkle_db.db_shard_arc(shard_id),
                    metadata_progress,
                )?);
            }
            shard_pruners
        } else {
            Vec::new()
        };
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L191-217)
```rust
    pub(in crate::pruner::state_merkle_pruner) fn get_stale_node_indices(
        state_merkle_db_shard: &DB,
        start_version: Version,
        target_version: Version,
        limit: usize,
    ) -> Result<(Vec<StaleNodeIndex>, Option<Version>)> {
        let mut indices = Vec::new();
        let mut iter = state_merkle_db_shard.iter::<S>()?;
        iter.seek(&StaleNodeIndex {
            stale_since_version: start_version,
            node_key: NodeKey::new_empty_path(0),
        })?;

        let mut next_version = None;
        while indices.len() < limit {
            if let Some((index, _)) = iter.next().transpose()? {
                next_version = Some(index.stale_since_version);
                if index.stale_since_version <= target_version {
                    indices.push(index);
                    continue;
                }
            }
            break;
        }

        Ok((indices, next_version))
    }
```

**File:** config/src/config/storage_config.rs (L398-412)
```rust
impl Default for StateMerklePrunerConfig {
    fn default() -> Self {
        StateMerklePrunerConfig {
            enable: true,
            // This allows a block / chunk being executed to have access to a non-latest state tree.
            // It needs to be greater than the number of versions the state committing thread is
            // able to commit during the execution of the block / chunk. If the bad case indeed
            // happens due to this being too small, a node restart should recover it.
            // Still, defaulting to 1M to be super safe.
            prune_window: 1_000_000,
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
        }
    }
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L55-55)
```rust
            let pruner_result = self.pruner.prune(self.batch_size);
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_pruner_manager.rs (L152-156)
```rust
        PrunerWorker::new(
            pruner,
            state_merkle_pruner_config.batch_size,
            "state_merkle",
        )
```

**File:** storage/aptosdb/src/pruner/pruner_utils.rs (L44-60)
```rust
pub(crate) fn get_or_initialize_subpruner_progress(
    sub_db: &DB,
    progress_key: &DbMetadataKey,
    metadata_progress: Version,
) -> Result<Version> {
    Ok(
        if let Some(v) = sub_db.get::<DbMetadataSchema>(progress_key)? {
            v.expect_version()
        } else {
            sub_db.put::<DbMetadataSchema>(
                progress_key,
                &DbMetadataValue::Version(metadata_progress),
            )?;
            metadata_progress
        },
    )
}
```

**File:** storage/jellyfish-merkle/src/lib.rs (L193-201)
```rust
#[derive(Clone, Debug, Eq, Hash, Ord, PartialEq, PartialOrd)]
#[cfg_attr(any(test, feature = "fuzzing"), derive(Arbitrary))]
pub struct StaleNodeIndex {
    /// The version since when the node is overwritten and becomes stale.
    pub stale_since_version: Version,
    /// The [`NodeKey`](node_type/struct.NodeKey.html) identifying the node associated with this
    /// record.
    pub node_key: NodeKey,
}
```

**File:** storage/jellyfish-merkle/src/node_type/mod.rs (L46-54)
```rust
/// The unique key of each node.
#[derive(Clone, Debug, Hash, Eq, PartialEq, Ord, PartialOrd)]
#[cfg_attr(any(test, feature = "fuzzing"), derive(Arbitrary))]
pub struct NodeKey {
    // The version at which the node is created.
    version: Version,
    // The nibble path this node represents in the tree.
    nibble_path: NibblePath,
}
```

**File:** types/src/nibble/nibble_path/mod.rs (L20-32)
```rust
/// NibblePath defines a path in Merkle tree in the unit of nibble (4 bits).
#[derive(Clone, Hash, Eq, PartialEq, Ord, PartialOrd, Serialize, Deserialize)]
pub struct NibblePath {
    /// Indicates the total number of nibbles in bytes. Either `bytes.len() * 2 - 1` or
    /// `bytes.len() * 2`.
    // Guarantees intended ordering based on the top-to-bottom declaration order of the struct's
    // members.
    num_nibbles: usize,
    /// The underlying bytes that stores the path, 2 nibbles per byte. If the number of nibbles is
    /// odd, the second half of the last byte must be 0.
    bytes: Vec<u8>,
    // invariant num_nibbles <= ROOT_NIBBLE_HEIGHT
}
```
