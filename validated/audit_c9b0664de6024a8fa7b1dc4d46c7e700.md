# Audit Report

## Title
SecretShareManager BlockQueue Not Reset During State Sync Causing Stale Block Processing

## Summary
The `ExecutionProxyClient::reset()` method fails to reset the `SecretShareManager` during state sync operations, while correctly resetting `rand_manager` and `buffer_manager`. This asymmetric reset behavior causes stale blocks to remain in the `BlockQueue` and be processed after sync completion, violating consensus ordering guarantees.

## Finding Description

The vulnerability exists in the state synchronization reset flow within the consensus pipeline. When a node performs state synchronization to catch up with the network, it must clear all pending consensus state to avoid processing outdated blocks.

The core issue is in the `ExecutionProxyClient::reset()` method, which is called during both `sync_to_target` and `sync_for_duration` operations: [1](#0-0) 

This method only retrieves and resets two components (`reset_tx_to_rand_manager` and `reset_tx_to_buffer_manager`) from the handle, completely omitting `reset_tx_to_secret_share_manager`, which exists in the handle structure: [2](#0-1) 

This contrasts sharply with the `end_epoch()` method, which correctly resets all three components including the secret share manager: [3](#0-2) 

The `SecretShareManager` maintains a `BlockQueue` that stores blocks awaiting secret share aggregation: [4](#0-3) 

When reset is properly triggered via `process_reset()`, the method clears this queue: [5](#0-4) 

However, during state sync operations, this reset never occurs, leaving stale blocks in the queue.

**Exploitation Path:**

1. Node processes blocks at round 100, with blocks 95-100 in `SecretShareManager`'s `BlockQueue` awaiting secret share aggregation
2. Node falls behind and initiates state sync to round 200 via `sync_to_target(target_round_200)`
3. The sync path calls `reset(target)` which resets `buffer_manager` (setting `highest_committed_round = 200`) and `rand_manager`, but NOT `secret_share_manager`: [6](#0-5) 

4. State sync completes, consensus resumes from round 201
5. Old blocks (95-100) eventually complete their secret share aggregation
6. `dequeue_ready_prefix()` dequeues these ready blocks in round order (BTreeMap guarantees ordering): [7](#0-6) 

7. Stale blocks 95-100 are sent to `BufferManager` via the outgoing channel
8. `BufferManager::process_ordered_blocks()` processes these blocks without validating their rounds against `highest_committed_round`: [8](#0-7) 

This violates the fundamental consensus invariant that after syncing to round N, only blocks with round > N should be processed.

## Impact Explanation

**Severity: High - Consensus Ordering Violation**

This vulnerability breaks consensus ordering guarantees by allowing blocks from before the sync target to be processed after sync completion. After a node syncs to round 200, it should only process blocks from round 201 onward. Processing blocks 95-100 after the sync creates critical issues:

1. **Ordering Violation**: The execution order guarantee is fundamentally broken, as blocks from before the sync target (rounds 95-100) are processed after blocks from after the sync target (rounds 201+)

2. **Execution Pipeline Corruption**: Stale blocks are sent to the execution pipeline with rounds less than `highest_committed_round`, potentially causing execution errors or undefined behavior

3. **Potential State Divergence**: Different nodes syncing at different times with different pending blocks may process stale blocks differently, potentially leading to state inconsistencies

4. **Pipeline State Confusion**: The `BufferManager` expects blocks > 200 based on `highest_committed_round`, but receives blocks < 200, violating internal invariants

While the exact execution behavior of stale blocks (whether they fail, succeed, or cause state divergence) requires further investigation, the ordering violation itself represents a consensus safety concern that meets High severity criteria per the Aptos bug bounty program.

## Likelihood Explanation

**Likelihood: High**

This vulnerability triggers in common operational scenarios:

1. **State Sync Operations**: Any node performing `sync_to_target` or `sync_for_duration` will experience this bug: [9](#0-8) 

2. **Network Conditions**: Nodes recovering from network partitions, slow execution, or joining as new validators all perform state sync

3. **No Malicious Input Required**: The vulnerability is triggered by normal consensus operations when blocks are pending secret share aggregation during sync

4. **Common Precondition**: When secret sharing is enabled (standard configuration), blocks frequently wait in `SecretShareManager` for aggregation, making the precondition common

The only requirement is that blocks must be in the `SecretShareManager` queue when sync occurs, which is a normal operational state.

## Recommendation

Fix the asymmetry by including `reset_tx_to_secret_share_manager` in the reset flow:

```rust
async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
    let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager, reset_tx_to_secret_share_manager) = {
        let handle = self.handle.read();
        (
            handle.reset_tx_to_rand_manager.clone(),
            handle.reset_tx_to_buffer_manager.clone(),
            handle.reset_tx_to_secret_share_manager.clone(),
        )
    };

    // Reset rand manager
    if let Some(mut reset_tx) = reset_tx_to_rand_manager {
        // ... existing code ...
    }

    // Reset secret share manager (NEW)
    if let Some(mut reset_tx) = reset_tx_to_secret_share_manager {
        let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::ResetDropped)?;
        ack_rx.await.map_err(|_| Error::ResetDropped)?;
    }

    // Reset buffer manager
    if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
        // ... existing code ...
    }

    Ok(())
}
```

Additionally, consider adding validation in `BufferManager::process_ordered_blocks()` to reject blocks with rounds â‰¤ `highest_committed_round`.

## Proof of Concept

The logic bug is evident from code inspection. A functional PoC would require:

1. Setting up a node with secret sharing enabled
2. Creating blocks pending in `SecretShareManager` 
3. Triggering state sync via `sync_to_target`
4. Observing that stale blocks are later sent to `BufferManager` after completing secret sharing
5. Verifying that `BufferManager` processes these stale blocks without rejection

The asymmetry between `reset()` (lines 674-709) and `end_epoch()` (lines 711-760) in the same file demonstrates the incomplete reset logic.

**Notes**

This vulnerability represents a clear logic bug where the reset implementation is incomplete and inconsistent with the epoch boundary handling. The asymmetric treatment of `SecretShareManager` between `reset()` and `end_epoch()` methods demonstrates an oversight in the state sync reset flow. While the precise execution behavior of stale blocks requires runtime testing to fully characterize, the ordering violation and potential for state inconsistency are clear from the code structure.

### Citations

**File:** consensus/src/pipeline/execution_client.rs (L124-131)
```rust
struct BufferManagerHandle {
    pub execute_tx: Option<UnboundedSender<OrderedBlocks>>,
    pub commit_tx:
        Option<aptos_channel::Sender<AccountAddress, (AccountAddress, IncomingCommitRequest)>>,
    pub reset_tx_to_buffer_manager: Option<UnboundedSender<ResetRequest>>,
    pub reset_tx_to_rand_manager: Option<UnboundedSender<ResetRequest>>,
    pub reset_tx_to_secret_share_manager: Option<UnboundedSender<ResetRequest>>,
}
```

**File:** consensus/src/pipeline/execution_client.rs (L661-672)
```rust
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });

        // Reset the rand and buffer managers to the target round
        self.reset(&target).await?;

        // TODO: handle the state sync error (e.g., re-push the ordered
        // blocks to the buffer manager when it's reset but sync fails).
        self.execution_proxy.sync_to_target(target).await
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L674-709)
```rust
    async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
        let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager) = {
            let handle = self.handle.read();
            (
                handle.reset_tx_to_rand_manager.clone(),
                handle.reset_tx_to_buffer_manager.clone(),
            )
        };

        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        }

        if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
            // reset execution phase and commit phase
            let (tx, rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::ResetDropped)?;
            rx.await.map_err(|_| Error::ResetDropped)?;
        }

        Ok(())
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L711-760)
```rust
    async fn end_epoch(&self) {
        let (
            reset_tx_to_rand_manager,
            reset_tx_to_buffer_manager,
            reset_tx_to_secret_share_manager,
        ) = {
            let mut handle = self.handle.write();
            handle.reset()
        };

        if let Some(mut tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop rand manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop rand manager");
        }

        if let Some(mut tx) = reset_tx_to_secret_share_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop secret share manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop secret share manager");
        }

        if let Some(mut tx) = reset_tx_to_buffer_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop buffer manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop buffer manager");
        }
        self.execution_proxy.end_epoch();
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L48-63)
```rust
pub struct SecretShareManager {
    author: Author,
    epoch_state: Arc<EpochState>,
    stop: bool,
    config: SecretShareConfig,
    reliable_broadcast: Arc<ReliableBroadcast<SecretShareMessage, ExponentialBackoff>>,
    network_sender: Arc<NetworkSender>,

    // local channel received from dec_store
    decision_rx: Receiver<SecretSharedKey>,
    // downstream channels
    outgoing_blocks: Sender<OrderedBlocks>,
    // local state
    secret_share_store: Arc<Mutex<SecretShareStore>>,
    block_queue: BlockQueue,
}
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L172-184)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.secret_share_store
            .lock()
            .update_highest_known_round(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L382-424)
```rust
    async fn process_ordered_blocks(&mut self, ordered_blocks: OrderedBlocks) {
        let OrderedBlocks {
            ordered_blocks,
            ordered_proof,
        } = ordered_blocks;

        info!(
            "Receive {} ordered block ends with [epoch: {}, round: {}, id: {}], the queue size is {}",
            ordered_blocks.len(),
            ordered_proof.commit_info().epoch(),
            ordered_proof.commit_info().round(),
            ordered_proof.commit_info().id(),
            self.buffer.len() + 1,
        );

        let request = self.create_new_request(ExecutionRequest {
            ordered_blocks: ordered_blocks.clone(),
        });
        if let Some(consensus_publisher) = &self.consensus_publisher {
            let message = ConsensusObserverMessage::new_ordered_block_message(
                ordered_blocks.clone(),
                ordered_proof.clone(),
            );
            consensus_publisher.publish_message(message);
        }
        self.execution_schedule_phase_tx
            .send(request)
            .await
            .expect("Failed to send execution schedule request");

        let mut unverified_votes = HashMap::new();
        if let Some(block) = ordered_blocks.last() {
            if let Some(votes) = self.pending_commit_votes.remove(&block.round()) {
                for (_, vote) in votes {
                    if vote.commit_info().id() == block.id() {
                        unverified_votes.insert(vote.author(), vote);
                    }
                }
            }
        }
        let item = BufferItem::new_ordered(ordered_blocks, ordered_proof, unverified_votes);
        self.buffer.push_back(item);
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L579-596)
```rust
    async fn process_reset_request(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        info!("Receive reset");

        match signal {
            ResetSignal::Stop => self.stop = true,
            ResetSignal::TargetRound(round) => {
                self.highest_committed_round = round;
                self.latest_round = round;

                let _ = self.drain_pending_commit_proof_till(round);
            },
        }

        self.reset().await;
        let _ = tx.send(ResetAck::default());
        info!("Reset finishes");
    }
```

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L112-127)
```rust
    pub fn dequeue_ready_prefix(&mut self) -> Vec<OrderedBlocks> {
        let mut ready_prefix = vec![];
        while let Some((_starting_round, item)) = self.queue.first_key_value() {
            if item.is_fully_secret_shared() {
                let (_, item) = self.queue.pop_first().expect("First key must exist");
                for block in item.blocks() {
                    observe_block(block.timestamp_usecs(), BlockStage::SECRET_SHARING_READY);
                }
                let QueueItem { ordered_blocks, .. } = item;
                ready_prefix.push(ordered_blocks);
            } else {
                break;
            }
        }
        ready_prefix
    }
```
