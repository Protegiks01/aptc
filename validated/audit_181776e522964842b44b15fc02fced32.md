# Audit Report

## Title
Consensus Persisting Phase Fails to Validate Commit Success, Causing State Inconsistency and Potential Liveness Failure

## Summary
The `PersistingPhase::process()` function unconditionally returns success even when block commits fail, causing the buffer manager to incorrectly update `highest_committed_round` and creating a dangerous state divergence between consensus layer tracking and actual storage state that can lead to validator liveness failure.

## Finding Description

The persisting phase processes batches of ordered blocks and is responsible for waiting for their commit operations to complete. However, it contains a critical flaw in error handling.

The `wait_for_commit_ledger()` method ignores commit results by discarding the Result with `let _`: [1](#0-0) 

The commit_ledger pipeline stage can fail and return errors. The executor's implementation shows multiple error paths including database writes and failpoint injection: [2](#0-1) 

The database writer's commit_ledger can fail with I/O errors, storage errors, and validation failures: [3](#0-2) 

Despite having dependency chains that propagate errors through `parent_block_commit_fut.await?`: [4](#0-3) 

The persisting phase never checks whether these commits actually succeeded. It unconditionally returns `Ok(last_block.round())` regardless of commit outcomes: [5](#0-4) 

**Exploitation Path:**

1. Buffer manager sends batch of blocks [A, B, C] at rounds [10, 11, 12] to persisting phase
2. Block A commits successfully to storage (version 110)
3. Block B's commit fails (database error, disk full, corruption, etc.) at the executor level
4. Due to pipeline dependencies, Block C's commit also fails when awaiting parent
5. Persisting phase still returns `Ok(12)` - claiming success
6. Buffer manager updates `highest_committed_round = 12` and cleans up `pending_commit_blocks`: [6](#0-5) 

7. System state is now inconsistent:
   - Storage layer: committed up to version 110 (round 10)
   - Consensus layer: believes committed up to round 12

When subsequent blocks arrive, they depend on rounds 11-12 which failed to commit, creating a cascade of failures that halts consensus progress.

## Impact Explanation

**Severity: High**

This violates the **State Consistency** invariant. The consensus layer believes blocks are committed when they are not, breaking the fundamental assumption that committed rounds are durably persisted.

**Impact:**
- **Liveness Failure**: The affected validator cannot make progress. All subsequent commit attempts will fail due to missing parent blocks in storage, but the buffer manager believes those parents are committed and won't retry.
- **State Divergence**: Consensus metadata becomes inconsistent with storage reality, requiring node restart to recover.
- **Network Instability**: If multiple validators experience concurrent commit failures (e.g., during high I/O load, disk issues), the network could lose liveness until manual intervention.

This aligns with **HIGH severity** in the Aptos bug bounty program: state inconsistencies requiring manual intervention and temporary liveness issues affecting individual validators.

The severity is not Critical because it:
- Only affects individual validators (not network-wide unless multiple validators fail simultaneously)
- Is recoverable through node restart
- Does not result in fund loss or permanent network partition

## Likelihood Explanation

**Likelihood: Medium to High**

While this requires commit failures (database errors, disk full, I/O failures, corruption), these are realistic operational scenarios:
- High transaction throughput causing disk I/O saturation
- Storage resource exhaustion
- Database corruption from power failures
- Concurrent write conflicts in edge cases

The bug manifests automatically once a commit failure occursâ€”no attacker action is needed. Given the scale of Aptos validator operations, commit failures will eventually occur.

The failpoint injection at the executor level confirms this is a testable and realistic failure mode: [7](#0-6) 

**Mitigation Factor**: The issue self-corrects on node restart as the buffer manager re-initializes from storage state. However, this requires manual intervention and causes service disruption.

## Recommendation

The fix requires propagating commit errors properly:

1. **Fix `wait_for_commit_ledger()`** to return the Result instead of ignoring it:
```rust
pub async fn wait_for_commit_ledger(&self) -> TaskResult<CommitLedgerResult> {
    if let Some(fut) = self.pipeline_futs() {
        fut.commit_ledger_fut.await
    } else {
        Ok(None)
    }
}
```

2. **Fix `PersistingPhase::process()`** to check commit results and return errors:
```rust
async fn process(&self, req: PersistingRequest) -> PersistingResponse {
    let PersistingRequest {
        blocks,
        commit_ledger_info,
    } = req;

    for b in &blocks {
        if let Some(tx) = b.pipeline_tx().lock().as_mut() {
            tx.commit_proof_tx
                .take()
                .map(|tx| tx.send(commit_ledger_info.clone()));
        }
        b.wait_for_commit_ledger().await?; // Propagate errors
    }

    let response = Ok(blocks.last().expect("Blocks can't be empty").round());
    if commit_ledger_info.ledger_info().ends_epoch() {
        self.commit_msg_tx
            .send_epoch_change(EpochChangeProof::new(vec![commit_ledger_info], false))
            .await;
    }
    response
}
```

3. **Add error handling in BufferManager** to handle `Some(Err(_))` from `persisting_phase_rx` and trigger recovery/reset when commit failures occur.

## Proof of Concept

The vulnerability can be tested using the existing failpoint mechanism:

```rust
#[tokio::test]
async fn test_commit_failure_ignored() {
    // Setup consensus pipeline with failpoint enabled
    fail::cfg("executor::commit_blocks", "return").unwrap();
    
    // Submit blocks for commit
    // Block A commits successfully
    // Block B hits failpoint and fails
    // Block C fails due to parent dependency
    
    // Verify: persisting phase returns Ok(C.round())
    // Verify: buffer_manager.highest_committed_round == C.round()
    // Verify: storage only has Block A committed
    // Verify: subsequent blocks fail to commit due to missing parents
    
    fail::remove("executor::commit_blocks");
}
```

The test would demonstrate that commit failures are silently ignored, creating the state inconsistency described.

### Citations

**File:** consensus/consensus-types/src/pipelined_block.rs (L562-568)
```rust
    pub async fn wait_for_commit_ledger(&self) {
        // may be aborted (e.g. by reset)
        if let Some(fut) = self.pipeline_futs() {
            // this may be cancelled
            let _ = fut.commit_ledger_fut.await;
        }
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L362-395)
```rust
    fn commit_ledger(&self, ledger_info_with_sigs: LedgerInfoWithSignatures) -> ExecutorResult<()> {
        let _timer = OTHER_TIMERS.timer_with(&["commit_ledger"]);

        let block_id = ledger_info_with_sigs.ledger_info().consensus_block_id();
        info!(
            LogSchema::new(LogEntry::BlockExecutor).block_id(block_id),
            "commit_ledger"
        );

        // Check for any potential retries
        // TODO: do we still have such retries?
        let committed_block = self.block_tree.root_block();
        if committed_block.num_persisted_transactions()?
            == ledger_info_with_sigs.ledger_info().version() + 1
        {
            return Ok(());
        }

        // Confirm the block to be committed is tracked in the tree.
        self.block_tree.get_block(block_id)?;

        fail_point!("executor::commit_blocks", |_| {
            Err(anyhow::anyhow!("Injected error in commit_blocks.").into())
        });

        let target_version = ledger_info_with_sigs.ledger_info().version();
        self.db
            .writer
            .commit_ledger(target_version, Some(&ledger_info_with_sigs), None)?;

        self.block_tree.prune(ledger_info_with_sigs.ledger_info())?;

        Ok(())
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L78-112)
```rust
    fn commit_ledger(
        &self,
        version: Version,
        ledger_info_with_sigs: Option<&LedgerInfoWithSignatures>,
        chunk_opt: Option<ChunkToCommit>,
    ) -> Result<()> {
        gauged_api("commit_ledger", || {
            // Pre-committing and committing in concurrency is allowed but not pre-committing at the
            // same time from multiple threads, the same for committing.
            // Consensus and state sync must hand over to each other after all pending execution and
            // committing complete.
            let _lock = self
                .commit_lock
                .try_lock()
                .expect("Concurrent committing detected.");
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["commit_ledger"]);

            let old_committed_ver = self.get_and_check_commit_range(version)?;

            let mut ledger_batch = SchemaBatch::new();
            // Write down LedgerInfo if provided.
            if let Some(li) = ledger_info_with_sigs {
                self.check_and_put_ledger_info(version, li, &mut ledger_batch)?;
            }
            // Write down commit progress
            ledger_batch.put::<DbMetadataSchema>(
                &DbMetadataKey::OverallCommitProgress,
                &DbMetadataValue::Version(version),
            )?;
            self.ledger_db.metadata_db().write_schemas(ledger_batch)?;

            // Notify the pruners, invoke the indexer, and update in-memory ledger info.
            self.post_commit(old_committed_ver, version, ledger_info_with_sigs, chunk_opt)
        })
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L1079-1105)
```rust
    async fn commit_ledger(
        pre_commit_fut: TaskFuture<PreCommitResult>,
        commit_proof_fut: TaskFuture<LedgerInfoWithSignatures>,
        parent_block_commit_fut: TaskFuture<CommitLedgerResult>,
        executor: Arc<dyn BlockExecutorTrait>,
        block: Arc<Block>,
    ) -> TaskResult<CommitLedgerResult> {
        let mut tracker = Tracker::start_waiting("commit_ledger", &block);
        parent_block_commit_fut.await?;
        pre_commit_fut.await?;
        let ledger_info_with_sigs = commit_proof_fut.await?;

        // it's committed as prefix
        if ledger_info_with_sigs.commit_info().id() != block.id() {
            return Ok(None);
        }

        tracker.start_working();
        let ledger_info_with_sigs_clone = ledger_info_with_sigs.clone();
        tokio::task::spawn_blocking(move || {
            executor
                .commit_ledger(ledger_info_with_sigs_clone)
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
        Ok(Some(ledger_info_with_sigs))
```

**File:** consensus/src/pipeline/persisting_phase.rs (L59-81)
```rust
    async fn process(&self, req: PersistingRequest) -> PersistingResponse {
        let PersistingRequest {
            blocks,
            commit_ledger_info,
        } = req;

        for b in &blocks {
            if let Some(tx) = b.pipeline_tx().lock().as_mut() {
                tx.commit_proof_tx
                    .take()
                    .map(|tx| tx.send(commit_ledger_info.clone()));
            }
            b.wait_for_commit_ledger().await;
        }

        let response = Ok(blocks.last().expect("Blocks can't be empty").round());
        if commit_ledger_info.ledger_info().ends_epoch() {
            self.commit_msg_tx
                .send_epoch_change(EpochChangeProof::new(vec![commit_ledger_info], false))
                .await;
        }
        response
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L968-973)
```rust
                Some(Ok(round)) = self.persisting_phase_rx.next() => {
                    // see where `need_backpressure()` is called.
                    self.pending_commit_votes = self.pending_commit_votes.split_off(&(round + 1));
                    self.highest_committed_round = round;
                    self.pending_commit_blocks = self.pending_commit_blocks.split_off(&(round + 1));
                },
```
