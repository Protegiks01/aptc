# Audit Report

## Title
Protocol Upgrade Race Condition Causes Consensus Divergence via Inconsistent validator_txn_enabled() Results

## Summary
During protocol upgrades when a new `OnChainConsensusConfig` version is introduced, validators running different binary versions will deserialize the same on-chain config bytes differently, causing `validator_txn_enabled()` to return inconsistent results across the network. This leads to non-recoverable consensus divergence where some validators enter DKG reconfiguration while others finish immediately, and validators disagree on block validity.

## Finding Description

The vulnerability exists in the native function `validator_txn_enabled()` which uses a silent fallback mechanism when BCS deserialization fails: [1](#0-0) 

When deserialization fails, the code falls back to the `Default` implementation which returns `OnChainConsensusConfig::V4` with validator transactions **disabled**: [2](#0-1) [3](#0-2) 

**Attack Path:**

1. Network runs with `OnChainConsensusConfig::V5` with validator transactions enabled
2. Protocol upgrade introduces `V6` (new variant not known to old binaries)
3. Governance updates on-chain config to use `V6` bytes
4. Network has validators with both old and new binaries during rolling upgrade
5. `aptos_governance::reconfigure()` evaluates the condition to decide reconfiguration path: [4](#0-3) 

6. **Old validators**: BCS fails to deserialize V6 → `unwrap_or_default()` returns V4 with vtxn:V0 → `validator_txn_enabled()` returns `false` → calls `finish()` which immediately increments the epoch counter: [5](#0-4) [6](#0-5) 

7. **New validators**: BCS succeeds → returns actual V6 config (e.g., with vtxn:V1) → `validator_txn_enabled()` returns `true` → calls `try_start()` which begins DKG **without** incrementing the epoch: [7](#0-6) 

8. **Result**: Old validators are now in epoch N+1, new validators remain in epoch N waiting for DKG completion. Validators cannot reach consensus across different epochs.

Additionally, `EpochManager` uses the same fallback mechanism when initializing consensus: [8](#0-7) 

This causes validators to initialize `RoundManager` with different `vtxn_config` values, leading to disagreement on block validity: [9](#0-8) 

Old validators (with vtxn disabled) reject `ProposalExt` blocks, while new validators (with vtxn enabled) accept them, creating a permanent consensus split.

## Impact Explanation

**Critical Severity - Network Partition Requiring Hardfork**

This vulnerability causes:
- **Non-recoverable network partition**: Validators diverge into different epochs and cannot synchronize. The epoch mismatch prevents any consensus messages from being processed across the partition.
- **Total loss of liveness**: The network cannot produce new blocks or process transactions as validators disagree on fundamental consensus parameters.
- **Consensus safety violation**: Different validators commit to different blockchain states based on their epoch and block validity rules.
- **Requires hardfork**: Manual intervention is needed to align all validators to the same epoch and consensus configuration.

The impact qualifies for **Critical severity ($1,000,000 max)** under Aptos Bug Bounty criteria as it causes both "Non-recoverable network partition (requires hardfork)" and "Consensus/Safety violations."

## Likelihood Explanation

**High Likelihood - Occurs During Normal Protocol Operations**

This vulnerability will trigger with near certainty during any protocol upgrade that:
1. Adds a new `OnChainConsensusConfig` enum variant (e.g., V6, V7, etc.)
2. Is deployed via rolling upgrade (standard practice for validator operators to minimize downtime)
3. Updates the on-chain config to use the new variant before all validators have upgraded

No malicious actor is required - this is a **logic vulnerability** in the protocol upgrade mechanism itself. The silent fallback using `.unwrap_or_default()` is a design flaw that causes validators to diverge whenever deserialization fails during normal protocol evolution. The attack complexity is zero as it happens automatically during legitimate operations.

## Recommendation

**Fix the silent fallback mechanism to prevent consensus divergence:**

1. **Reject unknown config versions explicitly** instead of silently falling back to defaults:
```rust
pub fn validator_txn_enabled(
    _context: &mut SafeNativeContext,
    _ty_args: &[Type],
    mut args: VecDeque<Value>,
) -> SafeNativeResult<SmallVec<[Value; 1]>> {
    let config_bytes = safely_pop_arg!(args, Vec<u8>);
    let config = bcs::from_bytes::<OnChainConsensusConfig>(&config_bytes)
        .map_err(|e| SafeNativeError::InvariantViolation(
            format!("Failed to deserialize OnChainConsensusConfig: {}", e)
        ))?;
    Ok(smallvec![Value::bool(config.is_vtxn_enabled())])
}
```

2. **Implement versioned migration** where old validators can understand new config versions:
   - Include a `known_to_version` field in the config
   - Old validators gracefully handle unknown fields but validate core structure
   - Use forward-compatible serialization formats

3. **Enforce synchronized upgrades** for consensus-critical config changes:
   - Require all validators to upgrade binaries before on-chain config updates
   - Add governance checks to verify validator binary versions
   - Implement a two-phase upgrade protocol

4. **Add epoch-based compatibility checks** in `EpochManager`:
```rust
let consensus_config = onchain_consensus_config
    .context("Failed to deserialize consensus config - validator binary may be outdated")?;
```

## Proof of Concept

Due to the complexity of simulating a full Aptos network with mixed validator versions, a complete end-to-end PoC would require:
1. A local testnet with multiple validator nodes
2. Deploying validators with different binary versions (one with V6 support, one without)
3. Governance proposal to update on-chain config to V6
4. Observing epoch divergence in validator logs

**Simplified demonstration** showing the deserialization behavior:

```rust
#[test]
fn test_consensus_config_deserialization_divergence() {
    // Simulate V6 config bytes (unknown to old validators)
    #[derive(Serialize)]
    enum OnChainConsensusConfigV6 {
        V6 { 
            alg: ConsensusAlgorithmConfig,
            vtxn: ValidatorTxnConfig::V1 { /* enabled */ },
            window_size: Option<u64>,
            rand_check_enabled: bool,
            new_field: u64, // New field in V6
        }
    }
    
    let v6_config = OnChainConsensusConfigV6::V6 { /* ... */ };
    let v6_bytes = bcs::to_bytes(&v6_config).unwrap();
    
    // Old validator (without V6 support) attempts deserialization
    let old_result: Result<OnChainConsensusConfig, _> = bcs::from_bytes(&v6_bytes);
    assert!(old_result.is_err()); // Fails to deserialize
    
    let old_config = old_result.unwrap_or_default();
    assert_eq!(old_config.is_vtxn_enabled(), false); // Falls back to V4 with disabled vtxn
    
    // New validator (with V6 support) successfully deserializes
    let new_config: OnChainConsensusConfigV6 = bcs::from_bytes(&v6_bytes).unwrap();
    assert_eq!(new_config.is_vtxn_enabled(), true); // Correctly reads V6 with enabled vtxn
    
    // Divergence: old_config.is_vtxn_enabled() != new_config.is_vtxn_enabled()
    // This causes different reconfiguration paths and epoch divergence
}
```

### Citations

**File:** aptos-move/framework/src/natives/consensus_config.rs (L13-21)
```rust
pub fn validator_txn_enabled(
    _context: &mut SafeNativeContext,
    _ty_args: &[Type],
    mut args: VecDeque<Value>,
) -> SafeNativeResult<SmallVec<[Value; 1]>> {
    let config_bytes = safely_pop_arg!(args, Vec<u8>);
    let config = bcs::from_bytes::<OnChainConsensusConfig>(&config_bytes).unwrap_or_default();
    Ok(smallvec![Value::bool(config.is_vtxn_enabled())])
}
```

**File:** types/src/on_chain_config/consensus_config.rs (L147-149)
```rust
    pub fn default_if_missing() -> Self {
        Self::V0
    }
```

**File:** types/src/on_chain_config/consensus_config.rs (L443-451)
```rust
impl Default for OnChainConsensusConfig {
    fn default() -> Self {
        OnChainConsensusConfig::V4 {
            alg: ConsensusAlgorithmConfig::default_if_missing(),
            vtxn: ValidatorTxnConfig::default_if_missing(),
            window_size: DEFAULT_WINDOW_SIZE,
        }
    }
}
```

**File:** aptos-move/framework/aptos-framework/sources/aptos_governance.move (L685-692)
```text
    public entry fun reconfigure(aptos_framework: &signer) {
        system_addresses::assert_aptos_framework(aptos_framework);
        if (consensus_config::validator_txn_enabled() && randomness_config::enabled()) {
            reconfiguration_with_dkg::try_start();
        } else {
            reconfiguration_with_dkg::finish(aptos_framework);
        }
    }
```

**File:** aptos-move/framework/aptos-framework/sources/reconfiguration_with_dkg.move (L24-40)
```text
    public(friend) fun try_start() {
        let incomplete_dkg_session = dkg::incomplete_session();
        if (option::is_some(&incomplete_dkg_session)) {
            let session = option::borrow(&incomplete_dkg_session);
            if (dkg::session_dealer_epoch(session) == reconfiguration::current_epoch()) {
                return
            }
        };
        reconfiguration_state::on_reconfig_start();
        let cur_epoch = reconfiguration::current_epoch();
        dkg::start(
            cur_epoch,
            randomness_config::current(),
            stake::cur_validator_consensus_infos(),
            stake::next_validator_consensus_infos(),
        );
    }
```

**File:** aptos-move/framework/aptos-framework/sources/reconfiguration_with_dkg.move (L46-61)
```text
    public(friend) fun finish(framework: &signer) {
        system_addresses::assert_aptos_framework(framework);
        dkg::try_clear_incomplete_session(framework);
        consensus_config::on_new_epoch(framework);
        execution_config::on_new_epoch(framework);
        gas_schedule::on_new_epoch(framework);
        std::version::on_new_epoch(framework);
        features::on_new_epoch(framework);
        jwk_consensus_config::on_new_epoch(framework);
        jwks::on_new_epoch(framework);
        keyless_account::on_new_epoch(framework);
        randomness_config_seqnum::on_new_epoch(framework);
        randomness_config::on_new_epoch(framework);
        randomness_api_v0_config::on_new_epoch(framework);
        reconfiguration::reconfigure();
    }
```

**File:** aptos-move/framework/aptos-framework/sources/reconfiguration.move (L106-159)
```text
    public(friend) fun reconfigure() acquires Configuration {
        // Do not do anything if genesis has not finished.
        if (chain_status::is_genesis() || timestamp::now_microseconds() == 0 || !reconfiguration_enabled()) {
            return
        };

        let config_ref = borrow_global_mut<Configuration>(@aptos_framework);
        let current_time = timestamp::now_microseconds();

        // Do not do anything if a reconfiguration event is already emitted within this transaction.
        //
        // This is OK because:
        // - The time changes in every non-empty block
        // - A block automatically ends after a transaction that emits a reconfiguration event, which is guaranteed by
        //   VM spec that all transactions comming after a reconfiguration transaction will be returned as Retry
        //   status.
        // - Each transaction must emit at most one reconfiguration event
        //
        // Thus, this check ensures that a transaction that does multiple "reconfiguration required" actions emits only
        // one reconfiguration event.
        //
        if (current_time == config_ref.last_reconfiguration_time) {
            return
        };

        reconfiguration_state::on_reconfig_start();

        // Call stake to compute the new validator set and distribute rewards and transaction fees.
        stake::on_new_epoch();
        storage_gas::on_reconfig();

        assert!(current_time > config_ref.last_reconfiguration_time, error::invalid_state(EINVALID_BLOCK_TIME));
        config_ref.last_reconfiguration_time = current_time;
        spec {
            assume config_ref.epoch + 1 <= MAX_U64;
        };
        config_ref.epoch = config_ref.epoch + 1;

        if (std::features::module_event_migration_enabled()) {
            event::emit(
                NewEpoch {
                    epoch: config_ref.epoch,
                },
            );
        };
        event::emit_event<NewEpochEvent>(
            &mut config_ref.events,
            NewEpochEvent {
                epoch: config_ref.epoch,
            },
        );

        reconfiguration_state::on_reconfig_finish();
    }
```

**File:** consensus/src/epoch_manager.rs (L1178-1201)
```rust
        let onchain_consensus_config: anyhow::Result<OnChainConsensusConfig> = payload.get();
        let onchain_execution_config: anyhow::Result<OnChainExecutionConfig> = payload.get();
        let onchain_randomness_config_seq_num: anyhow::Result<RandomnessConfigSeqNum> =
            payload.get();
        let randomness_config_move_struct: anyhow::Result<RandomnessConfigMoveStruct> =
            payload.get();
        let onchain_jwk_consensus_config: anyhow::Result<OnChainJWKConsensusConfig> = payload.get();
        let dkg_state = payload.get::<DKGState>();

        if let Err(error) = &onchain_consensus_config {
            warn!("Failed to read on-chain consensus config {}", error);
        }

        if let Err(error) = &onchain_execution_config {
            warn!("Failed to read on-chain execution config {}", error);
        }

        if let Err(error) = &randomness_config_move_struct {
            warn!("Failed to read on-chain randomness config {}", error);
        }

        self.epoch_state = Some(epoch_state.clone());

        let consensus_config = onchain_consensus_config.unwrap_or_default();
```

**File:** consensus/src/round_manager.rs (L1116-1124)
```rust
        if !self.vtxn_config.enabled()
            && matches!(
                proposal.block_data().block_type(),
                BlockType::ProposalExt(_)
            )
        {
            counters::UNEXPECTED_PROPOSAL_EXT_COUNT.inc();
            bail!("ProposalExt unexpected while the vtxn feature is disabled.");
        }
```
