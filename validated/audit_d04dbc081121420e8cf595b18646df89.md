# Audit Report

## Title
Unbounded Memory Growth in State Sync Driver Due to Asymmetric Channel Design

## Summary
The state sync driver exhibits a critical backpressure vulnerability where an unbounded consensus notification channel feeds into bounded downstream channels (mempool and storage service). When the driver blocks waiting to forward notifications to full bounded channels, consensus continues queuing notifications in the unbounded channel, causing uncontrolled memory growth that can lead to node crashes.

## Finding Description

The vulnerability originates from an asymmetric channel architecture in the state sync notification system. The consensus-to-state-sync notification channel is created as unbounded: [1](#0-0) 

This allows consensus to send notifications without blocking. However, downstream notification channels are bounded:

**Mempool notification channel** (configurable, default 100): [2](#0-1) [3](#0-2) 

**Storage service notification channel** (fixed size 1): [4](#0-3) 

The state sync driver processes consensus notifications in a single-threaded event loop: [5](#0-4) 

When handling consensus commit notifications, the driver sequentially forwards them to downstream services: [6](#0-5) 

These `.await` calls block if the bounded downstream channels are full. While blocked, the driver cannot process new messages from its event loop. Meanwhile, consensus continues committing blocks and sending notifications: [7](#0-6) 

Consensus logs errors on timeout but continues processing, allowing notifications to accumulate indefinitely in the unbounded channel until memory exhaustion occurs.

## Impact Explanation

This vulnerability qualifies as **High Severity** under Aptos bug bounty criteria for "Validator Node Slowdowns" - specifically DoS through resource exhaustion. The storage service channel with size 1 is particularly problematic, as it can immediately fill when storage operations slow due to disk I/O contention.

Impact includes:
1. **Progressive Memory Exhaustion**: Unbounded queue growth degrades node performance
2. **Node Crashes**: Out-of-memory crashes requiring manual intervention and restart
3. **Network Availability Impact**: Affects both validators and full nodes, potentially causing cascading failures
4. **Consensus Disruption**: Affected validators cannot participate effectively in consensus

This meets HIGH severity criteria because it causes significant performance degradation affecting consensus participation and enables DoS through resource exhaustion without requiring privileged access.

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability can be triggered under realistic operational conditions:

1. **High Transaction Throughput**: Sustained high transaction volume causes mempool channel (size 100) to fill when processing lags
2. **Disk I/O Bottlenecks**: Storage service channel (size 1) fills immediately when storage operations slow
3. **Cascading Effect**: Once blocked, memory growth accelerates with each new block commitment
4. **Natural Occurrence**: Can happen without malicious intent during normal network operation under load

The single-element storage service channel makes this particularly likely, as any transient storage slowdown immediately triggers backpressure.

## Recommendation

Implement bounded channels throughout with proper backpressure signaling:

1. **Replace unbounded consensus channel** with a bounded channel (e.g., size 1000)
2. **Implement backpressure mechanism** where consensus receives errors when the channel is full and can handle gracefully
3. **Add monitoring** for channel fullness metrics to detect backpressure conditions
4. **Consider priority-based dropping** for older notifications when channels approach capacity
5. **Increase storage service channel size** from 1 to allow buffering during transient slowdowns

Alternative: Implement asynchronous forwarding with a separate worker task that can drop or coalesce notifications when downstream consumers are slow, rather than blocking the main driver event loop.

## Proof of Concept

While a complete PoC would require setting up a full Aptos node environment, the vulnerability can be demonstrated through code analysis:

The channel configuration is established at node initialization: [8](#0-7) 

The vulnerability manifests when:
1. Storage or mempool processing slows (simulated by adding delays)
2. Consensus continues committing blocks at high rate
3. State sync driver event loop blocks on bounded channel sends
4. Unbounded consensus channel accumulates notifications
5. Memory usage grows until OOM

The code structure guarantees this behavior under the specified conditions, as the single-threaded event loop cannot process new consensus notifications while blocked on downstream channel sends.

## Notes

This is a classic backpressure vulnerability where an unbounded producer (consensus) feeds into a system with bounded consumers (mempool, storage service) through an intermediary (state sync driver) that cannot apply backpressure upstream. The fix requires either bounding the input channel or implementing non-blocking forwarding with intelligent dropping policies.

### Citations

**File:** state-sync/inter-component/consensus-notifications/src/lib.rs (L62-62)
```rust
    let (notification_sender, notification_receiver) = mpsc::unbounded();
```

**File:** config/src/config/state_sync_config.rs (L147-147)
```rust
            max_pending_mempool_notifications: 100,
```

**File:** state-sync/inter-component/mempool-notifications/src/lib.rs (L52-53)
```rust
    let (notification_sender, notification_receiver) =
        mpsc::channel(max_pending_mempool_notifications as usize);
```

**File:** state-sync/inter-component/storage-service-notifications/src/lib.rs (L21-21)
```rust
const STORAGE_SERVICE_NOTIFICATION_CHANNEL_SIZE: usize = 1;
```

**File:** state-sync/state-sync-driver/src/driver.rs (L222-238)
```rust
            ::futures::select! {
                notification = self.client_notification_listener.select_next_some() => {
                    self.handle_client_notification(notification).await;
                },
                notification = self.commit_notification_listener.select_next_some() => {
                    self.handle_snapshot_commit_notification(notification).await;
                }
                notification = self.consensus_notification_handler.select_next_some() => {
                    self.handle_consensus_or_observer_notification(notification).await;
                }
                notification = self.error_notification_listener.select_next_some() => {
                    self.handle_error_notification(notification).await;
                }
                _ = progress_check_interval.select_next_some() => {
                    self.drive_progress().await;
                }
            }
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L96-104)
```rust
        // Notify the storage service of the committed transactions
        storage_service_notification_handler
            .notify_storage_service_of_committed_transactions(latest_synced_version)
            .await?;

        // Notify mempool of the committed transactions
        mempool_notification_handler
            .notify_mempool_of_committed_transactions(transactions, blockchain_timestamp_usecs)
            .await?;
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L1167-1174)
```rust
        if let Err(e) = monitor!(
            "notify_state_sync",
            state_sync_notifier
                .notify_new_commit(txns, subscribable_events)
                .await
        ) {
            error!(error = ?e, "Failed to notify state synchronizer");
        }
```

**File:** aptos-node/src/state_sync.rs (L159-170)
```rust
    let (mempool_notifier, mempool_listener) =
        aptos_mempool_notifications::new_mempool_notifier_listener_pair(
            state_sync_config
                .state_sync_driver
                .max_pending_mempool_notifications,
        );
    let (consensus_notifier, consensus_listener) =
        aptos_consensus_notifications::new_consensus_notifier_listener_pair(
            state_sync_config
                .state_sync_driver
                .commit_notification_timeout_ms,
        );
```
