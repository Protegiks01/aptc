# Audit Report

## Title
Fast Sync Storage Wrapper Race Condition Causes Inconsistent Transaction Proof Generation During Status Transitions

## Summary
The `FastSyncStorageWrapper` contains a race condition where transaction iterators and cryptographic proof generation can read from different underlying databases during fast sync status transitions, producing mathematically inconsistent responses that fail verification and disrupt state synchronization.

## Finding Description

The `FastSyncStorageWrapper` maintains two separate `AptosDB` instances during fast sync bootstrapping: [1](#0-0) 

Database selection depends on the fast sync status checked at call time: [2](#0-1) 

All `DbReader` methods are delegated through the `delegate_read!` macro, including iterator creation and proof generation methods: [3](#0-2) [4](#0-3) 

The `FastSyncStorageWrapper` implements `DbReader` solely through delegation: [5](#0-4) 

**The Race Condition**: During transaction request processing, `get_transactions_with_proof_by_size` creates iterators, consumes them over time, then generates proofs as separate operations: [6](#0-5) [7](#0-6) [8](#0-7) 

If `finalize_state_snapshot()` executes during iterator consumption, the status transitions from STARTED to FINISHED: [9](#0-8) 

The storage service operates in an independent runtime with no synchronization to the bootstrapping process: [10](#0-9) [11](#0-10) 

**Result**: Iterator creation selects `temporary_db_with_genesis` (at time T1), iterators are bound to that database instance and read transactions from version 0. After status transition, proof generation selects `db_for_fast_sync` (at time T2) and generates accumulator proofs from version N. The response contains cryptographically inconsistent data that cannot be verified.

## Impact Explanation

**Severity: HIGH**

This vulnerability causes significant protocol violations:

1. **State Synchronization Disruption**: Peer nodes receiving inconsistent responses will fail Merkle accumulator verification because proofs from version N cannot verify transactions from version 0. This aborts state sync operations with verification errors.

2. **Network-Wide Impact**: During network upgrades or testnet resets, multiple nodes bootstrap simultaneously. Nodes completing fast sync serve corrupted data to peers through the storage service, amplifying the disruption across the network.

3. **Protocol Invariant Violation**: This breaks the fundamental guarantee that all components of a cryptographic proof bundle must originate from the same consistent ledger state. Accumulator proofs become mathematically invalid when paired with transactions from a different database version.

4. **Temporary but Disruptive**: While nodes can retry synchronization, the immediate impact includes verification failures, delayed node bootstrapping, and potential cascading effects if multiple nodes encounter this during critical operations.

This aligns with **HIGH severity** under "Validator Node Slowdowns" or "Significant protocol violations" as it degrades state synchronization performance and temporarily disrupts network operations, though it does not cause permanent consensus failures or fund loss.

## Likelihood Explanation

**Likelihood: MEDIUM**

The vulnerability requires specific but realistic timing conditions:

**Triggering Factors**:
- Fast sync is standard during node bootstrapping (common operation)
- Any network peer can trigger storage service requests (no special access)
- Size-aware chunking with iterators is enabled by default
- The race window spans iterator consumption duration (configurable via `max_storage_read_wait_time_ms`)
- Only a single `RwLock` protects status, no transaction-level synchronization

**Exploitation Feasibility**:
- Naturally occurs when nodes bootstrap during network upgrades
- Larger transaction batch requests increase the race window duration
- Multiple simultaneous bootstraps amplify probability
- Window is measurable (potentially seconds) not microseconds

The narrow window during `finalize_state_snapshot()` execution combined with concurrent storage service operations creates medium likelihood for this race condition to manifest in production environments.

## Recommendation

Implement atomic database selection for multi-step read operations:

1. **Option 1 - Transaction-Level Locking**: Acquire a read lock on `fast_sync_status` at the start of `get_transactions_with_proof_by_size` and hold it throughout iterator creation, consumption, and proof generation. Release after completing the entire operation.

2. **Option 2 - Snapshot Database Reference**: Capture the database reference once at the start of the request and pass it explicitly through the operation chain, avoiding repeated `get_read_delegatee()` calls.

3. **Option 3 - Status Transition Coordination**: Make `finalize_state_snapshot()` wait for in-flight storage service requests to complete before transitioning status, using a request counter or barrier mechanism.

Example fix (Option 2):
```rust
// Capture database reference once
let db_ref = self.get_aptos_db_read_ref();

// Use captured reference for all operations
let transaction_iterator = db_ref.get_transaction_iterator(...)?;
// ... use iterators ...
let proof = db_ref.get_transaction_accumulator_range_proof(...)?;
```

## Proof of Concept

The vulnerability can be demonstrated by instrumenting the code to introduce delays during iterator consumption and triggering `finalize_state_snapshot()` concurrently. A test would need to:

1. Start a storage service request for transactions with proof
2. Add a delay in the iterator consumption loop
3. Trigger `finalize_state_snapshot()` during that delay
4. Verify that the returned proof fails verification against the returned transactions
5. Check that transaction version is 0 while proof ledger version is N

This requires test infrastructure that can control timing between the storage service thread and the bootstrapping thread, which is complex to implement but demonstrates the theoretical race condition described above.

### Citations

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L32-34)
```rust
    // Used for storing genesis data during fast sync
    temporary_db_with_genesis: Arc<AptosDB>,
    // Used for restoring fast sync snapshot and all the read/writes afterwards
```

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L126-132)
```rust
    pub(crate) fn get_aptos_db_read_ref(&self) -> &AptosDB {
        if self.is_fast_sync_bootstrap_finished() {
            self.db_for_fast_sync.as_ref()
        } else {
            self.temporary_db_with_genesis.as_ref()
        }
    }
```

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L154-170)
```rust
    fn finalize_state_snapshot(
        &self,
        version: Version,
        output_with_proof: TransactionOutputListWithProofV2,
        ledger_infos: &[LedgerInfoWithSignatures],
    ) -> Result<()> {
        let status = self.get_fast_sync_status();
        assert_eq!(status, FastSyncStatus::STARTED);
        self.get_aptos_db_write_ref().finalize_state_snapshot(
            version,
            output_with_proof,
            ledger_infos,
        )?;
        let mut status = self.fast_sync_status.write();
        *status = FastSyncStatus::FINISHED;
        Ok(())
    }
```

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L188-192)
```rust
impl DbReader for FastSyncStorageWrapper {
    fn get_read_delegatee(&self) -> &dyn DbReader {
        self.get_aptos_db_read_ref()
    }
}
```

**File:** storage/storage-interface/src/lib.rs (L99-111)
```rust
macro_rules! delegate_read {
    ($(
        $(#[$($attr:meta)*])*
        fn $name:ident(&self $(, $arg: ident : $ty: ty)* $(,)?) -> $return_type:ty;
    )+) => {
        $(
            $(#[$($attr)*])*
            fn $name(&self, $($arg: $ty),*) -> $return_type {
                self.get_read_delegatee().$name($($arg),*)
            }
        )+
    };
}
```

**File:** storage/storage-interface/src/lib.rs (L217-246)
```rust
        fn get_transaction_iterator(
            &self,
            start_version: Version,
            limit: u64,
        ) -> Result<Box<dyn Iterator<Item = Result<Transaction>> + '_>>;

        fn get_transaction_info_iterator(
            &self,
            start_version: Version,
            limit: u64,
        ) -> Result<Box<dyn Iterator<Item = Result<TransactionInfo>> + '_>>;

        fn get_events_iterator(
            &self,
            start_version: Version,
            limit: u64,
        ) -> Result<Box<dyn Iterator<Item = Result<Vec<ContractEvent>>> + '_>>;

        fn get_write_set_iterator(
            &self,
            start_version: Version,
            limit: u64,
        ) -> Result<Box<dyn Iterator<Item = Result<WriteSet>> + '_>>;

        fn get_transaction_accumulator_range_proof(
            &self,
            start_version: Version,
            limit: u64,
            ledger_version: Version,
        ) -> Result<TransactionAccumulatorRangeProof>;
```

**File:** state-sync/storage-service/server/src/storage.rs (L374-394)
```rust
        let transaction_iterator = self
            .storage
            .get_transaction_iterator(start_version, num_transactions_to_fetch)?;
        let transaction_info_iterator = self
            .storage
            .get_transaction_info_iterator(start_version, num_transactions_to_fetch)?;
        let transaction_events_iterator = if include_events {
            self.storage
                .get_events_iterator(start_version, num_transactions_to_fetch)?
        } else {
            // If events are not included, create a fake iterator (they will be dropped anyway)
            Box::new(std::iter::repeat_n(
                Ok(vec![]),
                num_transactions_to_fetch as usize,
            ))
        };
        let persisted_auxiliary_info_iterator =
            self.storage.get_persisted_auxiliary_info_iterator(
                start_version,
                num_transactions_to_fetch as usize,
            )?;
```

**File:** state-sync/storage-service/server/src/storage.rs (L417-471)
```rust
        // Fetch as many transactions as possible
        while !response_progress_tracker.is_response_complete() {
            match multizip_iterator.next() {
                Some((Ok(transaction), Ok(info), Ok(events), Ok(persisted_auxiliary_info))) => {
                    // Calculate the number of serialized bytes for the data items
                    let num_transaction_bytes = get_num_serialized_bytes(&transaction)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_info_bytes = get_num_serialized_bytes(&info)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_events_bytes = get_num_serialized_bytes(&events)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_auxiliary_info_bytes =
                        get_num_serialized_bytes(&persisted_auxiliary_info).map_err(|error| {
                            Error::UnexpectedErrorEncountered(error.to_string())
                        })?;

                    // Add the data items to the lists
                    let total_serialized_bytes = num_transaction_bytes
                        + num_info_bytes
                        + num_events_bytes
                        + num_auxiliary_info_bytes;
                    if response_progress_tracker
                        .data_items_fits_in_response(true, total_serialized_bytes)
                    {
                        transactions.push(transaction);
                        transaction_infos.push(info);
                        transaction_events.push(events);
                        persisted_auxiliary_infos.push(persisted_auxiliary_info);

                        response_progress_tracker.add_data_item(total_serialized_bytes);
                    } else {
                        break; // Cannot add any more data items
                    }
                },
                Some((Err(error), _, _, _))
                | Some((_, Err(error), _, _))
                | Some((_, _, Err(error), _))
                | Some((_, _, _, Err(error))) => {
                    return Err(Error::StorageErrorEncountered(error.to_string()));
                },
                None => {
                    // Log a warning that the iterators did not contain all the expected data
                    warn!(
                        "The iterators for transactions, transaction infos, events and \
                        persisted auxiliary infos are missing data! Start version: {:?}, \
                        end version: {:?}, num transactions to fetch: {:?}, num fetched: {:?}.",
                        start_version,
                        end_version,
                        num_transactions_to_fetch,
                        transactions.len()
                    );
                    break;
                },
            }
        }
```

**File:** state-sync/storage-service/server/src/storage.rs (L474-478)
```rust
        let accumulator_range_proof = self.storage.get_transaction_accumulator_range_proof(
            start_version,
            transactions.len() as u64,
            proof_version,
        )?;
```

**File:** aptos-node/src/state_sync.rs (L266-294)
```rust
fn setup_state_sync_storage_service(
    config: StateSyncConfig,
    peers_and_metadata: Arc<PeersAndMetadata>,
    network_service_events: NetworkServiceEvents<StorageServiceMessage>,
    db_rw: &DbReaderWriter,
    storage_service_listener: StorageServiceNotificationListener,
) -> anyhow::Result<Runtime> {
    // Create a new state sync storage service runtime
    let storage_service_runtime = aptos_runtimes::spawn_named_runtime("stor-server".into(), None);

    // Spawn the state sync storage service servers on the runtime
    let storage_reader = StorageReader::new(
        config.storage_service,
        Arc::clone(&db_rw.reader),
        TimeService::real(),
    );
    let service = StorageServiceServer::new(
        config,
        storage_service_runtime.handle().clone(),
        storage_reader,
        TimeService::real(),
        peers_and_metadata,
        StorageServiceNetworkEvents::new(network_service_events),
        storage_service_listener,
    );
    storage_service_runtime.spawn(service.start());

    Ok(storage_service_runtime)
}
```

**File:** state-sync/storage-service/server/src/lib.rs (L384-420)
```rust
    pub async fn start(mut self) {
        // Spawn the continuously running tasks
        self.spawn_continuous_storage_summary_tasks().await;

        // Handle the storage requests as they arrive
        while let Some(network_request) = self.network_requests.next().await {
            // All handler methods are currently CPU-bound and synchronous
            // I/O-bound, so we want to spawn on the blocking thread pool to
            // avoid starving other async tasks on the same runtime.
            let storage = self.storage.clone();
            let config = self.storage_service_config;
            let cached_storage_server_summary = self.cached_storage_server_summary.clone();
            let optimistic_fetches = self.optimistic_fetches.clone();
            let subscriptions = self.subscriptions.clone();
            let lru_response_cache = self.lru_response_cache.clone();
            let request_moderator = self.request_moderator.clone();
            let time_service = self.time_service.clone();
            self.runtime.spawn_blocking(move || {
                Handler::new(
                    cached_storage_server_summary,
                    optimistic_fetches,
                    lru_response_cache,
                    request_moderator,
                    storage,
                    subscriptions,
                    time_service,
                )
                .process_request_and_respond(
                    config,
                    network_request.peer_network_id,
                    network_request.protocol_id,
                    network_request.storage_service_request,
                    network_request.response_sender,
                );
            });
        }
    }
```
