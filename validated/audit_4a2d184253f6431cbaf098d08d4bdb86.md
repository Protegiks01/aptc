# Audit Report

## Title
Indexer Metadata Inconsistency After Fast Sync Causes Silent Data Loss in Historical Queries

## Summary
During fast sync state snapshot restoration, the `StateStore::kv_finish()` method incorrectly sets internal indexer metadata (`TransactionVersion` and `EventVersion`) to `snapshot_version - 1`, falsely claiming that transactions and events have been indexed. However, fast sync intentionally skips historical transaction/event data, leaving these indices empty. This causes API queries to pass metadata validation but return empty results, resulting in silent data loss.

## Finding Description

This vulnerability is an integration bug between the fast sync state restoration mechanism and the internal indexer metadata management system.

**Root Cause:**

During fast sync, when `StateStore::kv_finish()` is called to finalize the state snapshot restoration, it unconditionally sets indexer metadata for all data types: [1](#0-0) 

The method sets `TransactionVersion`, `EventVersion`, and `StateVersion` all to `version - 1` whenever the internal indexer is enabled (lines 1296-1306), regardless of whether any transactions or events were actually indexed during the fast sync process.

**Why This Is Wrong:**

Fast sync only saves ONE checkpoint transaction at the snapshot version, not the historical transactions. This is explicitly enforced: [2](#0-1) 

**State Keys vs Transactions/Events:**

While state keys ARE correctly indexed during fast sync via `write_kv_batch()`: [3](#0-2) 

Transactions and events are indexed separately by `DBIndexer.process_a_batch()` which reads transaction and event data from the main database and builds the indices: [4](#0-3) 

**The Consequence:**

When the internal indexer service starts after fast sync, it reads the persisted version to determine where to begin indexing: [5](#0-4) 

The indexer starts from `persisted_version + 1`, which equals `snapshot_version`, completely skipping all historical data from version 0 to `snapshot_version - 1` that was never actually indexed.

**API Query Failure:**

API queries route through the internal indexer when storage sharding is enabled (which defaults to true): [6](#0-5) 

The default configuration has `enable_storage_sharding` set to true: [7](#0-6) 

The query path calls `ensure_cover_ledger_version()` which validates coverage by checking if the indexer's latest version is >= the requested ledger version: [8](#0-7) 

This check passes because the metadata falsely claims coverage up to `snapshot_version - 1`. Then `lookup_events_by_key()` is called, which queries the index: [9](#0-8) 

The iterator finds no entries (because nothing was indexed) and returns an empty vector (line 225), causing **silent data loss** instead of a proper "data not available" error.

## Impact Explanation

This vulnerability qualifies as **HIGH severity** under the Aptos bug bounty program's "API Crashes" / "Significant protocol violations" category:

1. **API Misbehavior**: API nodes using fast sync serve incorrect (empty) results for all historical transaction and event queries in the range [0, snapshot_version-1]. Applications depending on this data will receive empty arrays instead of the actual events/transactions, causing application logic failures.

2. **Silent Data Loss**: The system returns empty results instead of proper "data not available" errors, making the issue extremely difficult to detect and debug. Users cannot distinguish between "no events occurred" vs "events are not indexed."

3. **Widespread Impact**: Affects all nodes that use fast sync for bootstrapping (standard practice for new validators and API nodes) with internal indexer enabled and storage sharding enabled (both default configurations).

4. **Protocol Violation**: The indexer metadata provides false guarantees about data availability, violating the fundamental contract between the storage layer and the query layer.

5. **Permanent Inconsistency**: Once fast sync completes, there's no automatic recovery mechanism. The metadata inconsistency persists indefinitely unless manually corrected.

## Likelihood Explanation

**Likelihood: HIGH**

This bug triggers deterministically whenever:
- A node uses fast sync mode (standard for new node deployments)
- The internal indexer is enabled (automatically enabled when storage sharding is enabled)
- Storage sharding is enabled (default value is `true` as shown in the configuration)
- Any historical queries are made through the API

No malicious actor is required - this is a systematic integration bug between fast sync and the internal indexer. The conditions for this bug are the **default configuration** for production deployments, making it highly likely to affect new node operators who bootstrap using fast sync.

## Recommendation

The `StateStore::kv_finish()` method should only set indexer metadata for data types that were actually indexed during fast sync. Specifically:

1. During fast sync, only `StateVersion` should be set (since state keys are indexed via `write_kv_batch`)
2. `TransactionVersion` and `EventVersion` should NOT be set during fast sync
3. These metadata entries should remain unset (or explicitly set to indicate "not indexed") until the `DBIndexer` actually processes those historical transactions and events

Alternatively, add a check in `InternalIndexerDBService.get_start_version()` to detect when fast sync has completed and force re-indexing from version 0 if transaction/event metadata doesn't match the expected state.

## Proof of Concept

While a complete runnable PoC would require a full node setup with fast sync, the bug can be demonstrated by examining the execution flow:

1. Node starts with fast sync enabled and reaches version 1,000,000
2. `StateStore::kv_finish(1000000, usage)` is called
3. This sets `TransactionVersion` and `EventVersion` to 999,999 in the indexer DB
4. `DBIndexer` starts and reads `get_persisted_version()` â†’ returns 999,999
5. Indexer begins processing from version 1,000,000, skipping all prior history
6. API query for events at version 500,000 calls `ensure_cover_ledger_version(500000)`
7. Check passes (999,999 >= 500,000), then `lookup_events_by_key()` returns empty array
8. Application receives empty results instead of actual events or proper error

The bug is deterministic and requires only the default node configuration to manifest.

## Notes

This is a legitimate integration bug between two subsystems (fast sync and internal indexer) that both work correctly in isolation but fail when combined. The root cause is that `kv_finish()` makes assumptions about what data has been indexed that don't hold true during fast sync restoration. The vulnerability demonstrates a significant protocol violation where the API layer provides incorrect data guarantees to client applications.

### Citations

**File:** storage/aptosdb/src/state_store/mod.rs (L1259-1271)
```rust
        if self.internal_indexer_db.is_some()
            && self
                .internal_indexer_db
                .as_ref()
                .unwrap()
                .statekeys_enabled()
        {
            let keys = node_batch.keys().map(|key| key.0.clone()).collect();
            self.internal_indexer_db
                .as_ref()
                .unwrap()
                .write_keys_to_indexer_db(&keys, version, progress)?;
        }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1281-1315)
```rust
    fn kv_finish(&self, version: Version, usage: StateStorageUsage) -> Result<()> {
        self.ledger_db.metadata_db().put_usage(version, usage)?;
        if let Some(internal_indexer_db) = self.internal_indexer_db.as_ref() {
            if version > 0 {
                let mut batch = SchemaBatch::new();
                batch.put::<InternalIndexerMetadataSchema>(
                    &MetadataKey::LatestVersion,
                    &MetadataValue::Version(version - 1),
                )?;
                if internal_indexer_db.statekeys_enabled() {
                    batch.put::<InternalIndexerMetadataSchema>(
                        &MetadataKey::StateVersion,
                        &MetadataValue::Version(version - 1),
                    )?;
                }
                if internal_indexer_db.transaction_enabled() {
                    batch.put::<InternalIndexerMetadataSchema>(
                        &MetadataKey::TransactionVersion,
                        &MetadataValue::Version(version - 1),
                    )?;
                }
                if internal_indexer_db.event_enabled() {
                    batch.put::<InternalIndexerMetadataSchema>(
                        &MetadataKey::EventVersion,
                        &MetadataValue::Version(version - 1),
                    )?;
                }
                internal_indexer_db
                    .get_inner_db_ref()
                    .write_schemas(batch)?;
            }
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L133-145)
```rust
            // Ensure the output with proof only contains a single transaction output and info
            let num_transaction_outputs = output_with_proof.get_num_outputs();
            let num_transaction_infos = output_with_proof.proof.transaction_infos.len();
            ensure!(
                num_transaction_outputs == 1,
                "Number of transaction outputs should == 1, but got: {}",
                num_transaction_outputs
            );
            ensure!(
                num_transaction_infos == 1,
                "Number of transaction infos should == 1, but got: {}",
                num_transaction_infos
            );
```

**File:** storage/indexer/src/db_indexer.rs (L163-172)
```rust
    pub fn ensure_cover_ledger_version(&self, ledger_version: Version) -> Result<()> {
        let indexer_latest_version = self.get_persisted_version()?;
        if let Some(indexer_latest_version) = indexer_latest_version {
            if indexer_latest_version >= ledger_version {
                return Ok(());
            }
        }

        bail!("ledger version too new")
    }
```

**File:** storage/indexer/src/db_indexer.rs (L209-245)
```rust
    pub fn lookup_events_by_key(
        &self,
        event_key: &EventKey,
        start_seq_num: u64,
        limit: u64,
        ledger_version: u64,
    ) -> Result<
        Vec<(
            u64,     // sequence number
            Version, // transaction version it belongs to
            u64,     // index among events for the same transaction
        )>,
    > {
        let mut iter = self.db.iter::<EventByKeySchema>()?;
        iter.seek(&(*event_key, start_seq_num))?;

        let mut result = Vec::new();
        let mut cur_seq = start_seq_num;
        for res in iter.take(limit as usize) {
            let ((path, seq), (ver, idx)) = res?;
            if path != *event_key || ver > ledger_version {
                break;
            }
            if seq != cur_seq {
                let msg = if cur_seq == start_seq_num {
                    "First requested event is probably pruned."
                } else {
                    "DB corruption: Sequence number not continuous."
                };
                bail!("{} expected: {}, actual: {}", msg, cur_seq, seq);
            }
            result.push((seq, ver, idx));
            cur_seq += 1;
        }

        Ok(result)
    }
```

**File:** storage/indexer/src/db_indexer.rs (L410-550)
```rust
    pub fn process_a_batch(&self, start_version: Version, end_version: Version) -> Result<Version> {
        let _timer: aptos_metrics_core::HistogramTimer = TIMER.timer_with(&["process_a_batch"]);
        let mut version = start_version;
        let num_transactions = self.get_num_of_transactions(version, end_version)?;
        // This promises num_transactions should be readable from main db
        let mut db_iter = self.get_main_db_iter(version, num_transactions)?;
        let mut batch = SchemaBatch::new();
        let mut event_keys: HashSet<EventKey> = HashSet::new();
        db_iter.try_for_each(|res| {
            let (txn, events, writeset) = res?;
            if let Some(signed_txn) = txn.try_as_signed_user_txn() {
                if self.indexer_db.transaction_enabled() {
                    if let ReplayProtector::SequenceNumber(seq_num) = signed_txn.replay_protector()
                    {
                        batch.put::<OrderedTransactionByAccountSchema>(
                            &(signed_txn.sender(), seq_num),
                            &version,
                        )?;
                    }
                }
            }

            if self.indexer_db.event_enabled() {
                events.iter().enumerate().try_for_each(|(idx, event)| {
                    if let ContractEvent::V1(v1) = event {
                        batch
                            .put::<EventByKeySchema>(
                                &(*v1.key(), v1.sequence_number()),
                                &(version, idx as u64),
                            )
                            .expect("Failed to put events by key to a batch");
                        batch
                            .put::<EventByVersionSchema>(
                                &(*v1.key(), version, v1.sequence_number()),
                                &(idx as u64),
                            )
                            .expect("Failed to put events by version to a batch");
                    }
                    if self.indexer_db.event_v2_translation_enabled() {
                        if let ContractEvent::V2(v2) = event {
                            if let Some(translated_v1_event) =
                                self.translate_event_v2_to_v1(v2).map_err(|e| {
                                    anyhow::anyhow!(
                                        "Failed to translate event: {:?}. Error: {}",
                                        v2,
                                        e
                                    )
                                })?
                            {
                                let key = *translated_v1_event.key();
                                let sequence_number = translated_v1_event.sequence_number();
                                self.event_v2_translation_engine
                                    .cache_sequence_number(&key, sequence_number);
                                event_keys.insert(key);
                                batch
                                    .put::<EventByKeySchema>(
                                        &(key, sequence_number),
                                        &(version, idx as u64),
                                    )
                                    .expect("Failed to put events by key to a batch");
                                batch
                                    .put::<EventByVersionSchema>(
                                        &(key, version, sequence_number),
                                        &(idx as u64),
                                    )
                                    .expect("Failed to put events by version to a batch");
                                batch
                                    .put::<TranslatedV1EventSchema>(
                                        &(version, idx as u64),
                                        &translated_v1_event,
                                    )
                                    .expect("Failed to put translated v1 events to a batch");
                            }
                        }
                    }
                    Ok::<(), AptosDbError>(())
                })?;
            }

            if self.indexer_db.statekeys_enabled() {
                writeset.write_op_iter().for_each(|(state_key, write_op)| {
                    if write_op.is_creation() || write_op.is_modification() {
                        batch
                            .put::<StateKeysSchema>(state_key, &())
                            .expect("Failed to put state keys to a batch");
                    }
                });
            }
            version += 1;
            Ok::<(), AptosDbError>(())
        })?;
        assert!(version > 0, "batch number should be greater than 0");

        assert_eq!(num_transactions, version - start_version);

        if self.indexer_db.event_v2_translation_enabled() {
            batch.put::<InternalIndexerMetadataSchema>(
                &MetadataKey::EventV2TranslationVersion,
                &MetadataValue::Version(version - 1),
            )?;

            for event_key in event_keys {
                batch
                    .put::<EventSequenceNumberSchema>(
                        &event_key,
                        &self
                            .event_v2_translation_engine
                            .get_cached_sequence_number(&event_key)
                            .unwrap_or(0),
                    )
                    .expect("Failed to put events by key to a batch");
            }
        }

        if self.indexer_db.transaction_enabled() {
            batch.put::<InternalIndexerMetadataSchema>(
                &MetadataKey::TransactionVersion,
                &MetadataValue::Version(version - 1),
            )?;
        }
        if self.indexer_db.event_enabled() {
            batch.put::<InternalIndexerMetadataSchema>(
                &MetadataKey::EventVersion,
                &MetadataValue::Version(version - 1),
            )?;
        }
        if self.indexer_db.statekeys_enabled() {
            batch.put::<InternalIndexerMetadataSchema>(
                &MetadataKey::StateVersion,
                &MetadataValue::Version(version - 1),
            )?;
        }
        batch.put::<InternalIndexerMetadataSchema>(
            &MetadataKey::LatestVersion,
            &MetadataValue::Version(version - 1),
        )?;
        self.sender
            .send(Some(batch))
            .map_err(|e| AptosDbError::Other(e.to_string()))?;
        Ok(version)
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/internal_indexer_db_service.rs (L102-106)
```rust
        let start_version = self
            .db_indexer
            .indexer_db
            .get_persisted_version()?
            .map_or(0, |v| v + 1);
```

**File:** api/src/context.rs (L1096-1104)
```rust
        let mut res = if !db_sharding_enabled(&self.node_config) {
            self.db
                .get_events(event_key, start, order, limit as u64, ledger_version)?
        } else {
            self.indexer_reader
                .as_ref()
                .ok_or_else(|| anyhow!("Internal indexer reader doesn't exist"))?
                .get_events(event_key, start, order, limit as u64, ledger_version)?
        };
```

**File:** config/src/config/storage_config.rs (L233-233)
```rust
            enable_storage_sharding: true,
```
