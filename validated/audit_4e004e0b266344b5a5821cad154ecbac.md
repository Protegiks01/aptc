# Audit Report

## Title
Race Condition Between Module Publishing and Layout Cache Invalidation Enables Non-Deterministic Execution

## Summary
A race condition exists in the parallel block executor where layout cache invalidation is not atomic with module overriding. When transactions publish modules, each module is marked as overridden individually before the layout cache is flushed, creating a timing window where concurrent transactions can load stale struct layouts containing type information computed from old module definitions. This causes non-deterministic execution across validators, violating consensus safety.

## Finding Description

The vulnerability manifests in the module publishing sequence during parallel block execution. When a transaction commits and publishes modules, the system performs two critical operations non-atomically:

**Individual Module Marking**: During module publishing, the system loops through each published module and marks it as overridden individually: [1](#0-0) 

Inside the loop, each call to `add_module_write_to_module_cache` marks the module as overridden via an atomic boolean: [2](#0-1) [3](#0-2) 

**Delayed Layout Flush**: The layout cache is only flushed AFTER all modules are processed: [4](#0-3) 

**The Race Window**:

The layout cache uses a concurrent DashMap that allows simultaneous reads during the publishing process: [5](#0-4) 

During the gap between marking modules as overridden and flushing layouts, a concurrent transaction can load a layout from the cache: [6](#0-5) 

The loaded `LayoutCacheEntry` contains pre-computed type information (field offsets, sizes, types) from old module definitions: [7](#0-6) 

Critically, the cache key `StructKey` does not include any version information, so old layouts remain cached after module republishing: [8](#0-7) 

When loading cached layouts, the system re-charges gas for dependent modules but does NOT validate that the cached layout structure matches those modules: [9](#0-8) 

The comment explicitly states the intent to trigger validation through module reads, but this only works if modules themselves fail validation, not if the derived layout structure is stale.

**Why Validation Doesn't Catch This**:

BlockSTM's module validation only checks that module reads are consistent—it verifies modules haven't been overridden or version-changed: [10](#0-9) 

The validation verifies modules were read correctly but does NOT validate that derived data structures (layouts) match those modules. The layout structure contains pre-computed type information that becomes stale when modules change, but this staleness is never detected.

**Consensus Safety Violation**:

The commit phase is non-concurrent, but parallel execution continues: [11](#0-10) 

This allows the race: while one thread publishes modules (non-concurrent commit), other threads execute transactions (parallel) and can load stale layouts. Different validators with different thread timings will experience the race differently, leading to divergent execution outcomes.

## Impact Explanation

This qualifies as **Critical** severity under Aptos bug bounty criteria as a **Consensus/Safety Violation**: "Different validators commit different blocks."

When validators process identical blocks with identical transactions, timing-dependent layout cache behavior causes non-deterministic execution:

- Stale layouts contain incorrect field offsets, types, and sizes computed from old module definitions
- Deserialization using stale layouts reads wrong memory locations
- Type operations (comparisons, arithmetic, field access) produce different results
- State transitions diverge between validators
- Different state roots computed for the same block
- Validators disagree on block state → consensus split requiring hardfork

The layout cache stores critical type safety information (struct field ordering, field types, enum variants, sizes). When this information is stale, Move's type safety guarantees are violated at runtime despite bytecode verification passing, as verification happens once but layouts are used repeatedly during execution.

## Likelihood Explanation

**High Likelihood** - This race occurs naturally without attacker involvement:

1. **Frequent Trigger Events**: Module publishing happens during framework upgrades, smart contract deployments, package updates, and any transaction calling `code::publish_package_txn`

2. **Parallel Execution Maximizes Race Window**: Aptos uses BlockSTM with multiple worker threads executing transactions concurrently, with each worker thread being a potential race participant

3. **Natural Timing Variance**: Different validators have different CPU speeds, system loads, memory latencies, and thread scheduling patterns, ensuring different validators hit the race window differently

4. **No Attacker Coordination Required**: The race emerges from legitimate concurrent execution during normal operations

5. **Multiple Opportunities Per Block**: Each module publishing transaction creates a race window, with multiple concurrent transactions increasing probability

## Recommendation

Make layout cache invalidation atomic with module overriding by flushing the layout cache BEFORE or during the module marking loop, or implement layout versioning to prevent stale cache hits. The flush should occur under the same synchronization that prevents concurrent commits:

```rust
// Option 1: Flush before marking modules
if output_before_guard.module_write_set().values().next().is_some() {
    global_module_cache.flush_layout_cache();
}
for write in output_before_guard.module_write_set().values() {
    // mark modules as overridden
}

// Option 2: Add version to StructKey to prevent stale hits
pub struct StructKey {
    pub idx: StructNameIndex,
    pub ty_args_id: TypeVecId,
    pub module_version: ModuleVersion, // NEW
}
```

Alternatively, implement layout validation that checks the cached layout structure matches the current module definitions, not just that modules were read.

## Proof of Concept

A PoC would require:
1. Deploy module M with struct S(u64, u64)
2. Execute transaction that caches layout for S
3. In parallel: Publish new version of M with struct S(u128) + execute transaction using S
4. Race thread timing to load stale layout (u64, u64) while reading new module (u128)
5. Observe type confusion in deserialization/field access
6. Compare state roots across different timing scenarios

The probabilistic nature requires multiple runs with different thread scheduling to demonstrate non-determinism.

### Citations

**File:** aptos-move/block-executor/src/txn_last_input_output.rs (L559-571)
```rust
        for write in output_before_guard.module_write_set().values() {
            published = true;
            if scheduler.is_v2() {
                module_ids_for_v2.insert(write.module_id().clone());
            }
            add_module_write_to_module_cache::<T>(
                write,
                txn_idx,
                runtime_environment,
                global_module_cache,
                versioned_cache.module_cache(),
            )?;
        }
```

**File:** aptos-move/block-executor/src/txn_last_input_output.rs (L572-576)
```rust
        if published {
            // Record validation requirements after the modules are published.
            global_module_cache.flush_layout_cache();
            scheduler.record_validation_requirements(txn_idx, module_ids_for_v2)?;
        }
```

**File:** aptos-move/block-executor/src/code_cache_global.rs (L56-58)
```rust
    fn mark_overridden(&self) {
        self.overridden.store(true, Ordering::Release)
    }
```

**File:** aptos-move/block-executor/src/code_cache_global.rs (L96-96)
```rust
    struct_layouts: DashMap<StructKey, LayoutCacheEntry>,
```

**File:** aptos-move/block-executor/src/code_cache_global.rs (L171-178)
```rust
    pub(crate) fn get_struct_layout_entry(&self, key: &StructKey) -> Option<LayoutCacheEntry> {
        match self.struct_layouts.get(key) {
            None => {
                GLOBAL_LAYOUT_CACHE_MISSES.inc();
                None
            },
            Some(e) => Some(e.deref().clone()),
        }
```

**File:** aptos-move/block-executor/src/code_cache_global.rs (L317-317)
```rust
    global_module_cache.mark_overridden(write.module_id());
```

**File:** third_party/move/move-vm/runtime/src/storage/layout_cache.rs (L59-77)
```rust
/// An entry into layout cache: layout and a set of modules used to construct it.
#[derive(Debug, Clone)]
pub struct LayoutCacheEntry {
    layout: LayoutWithDelayedFields,
    modules: TriompheArc<DefiningModules>,
}

impl LayoutCacheEntry {
    pub(crate) fn new(layout: LayoutWithDelayedFields, modules: DefiningModules) -> Self {
        Self {
            layout,
            modules: TriompheArc::new(modules),
        }
    }

    pub(crate) fn unpack(self) -> (LayoutWithDelayedFields, TriompheArc<DefiningModules>) {
        (self.layout, self.modules)
    }
}
```

**File:** third_party/move/move-vm/runtime/src/storage/layout_cache.rs (L79-83)
```rust
#[derive(Debug, Copy, Clone, Eq, PartialEq, Hash)]
pub struct StructKey {
    pub idx: StructNameIndex,
    pub ty_args_id: TypeVecId,
}
```

**File:** third_party/move/move-vm/runtime/src/storage/loader/lazy.rs (L203-221)
```rust
    fn load_layout_from_cache(
        &self,
        gas_meter: &mut impl DependencyGasMeter,
        traversal_context: &mut TraversalContext,
        key: &StructKey,
    ) -> Option<PartialVMResult<LayoutWithDelayedFields>> {
        let entry = self.module_storage.get_struct_layout(key)?;
        let (layout, modules) = entry.unpack();
        for module_id in modules.iter() {
            // Re-read all modules for this layout, so that transaction gets invalidated
            // on module publish. Also, we re-read them in exactly the same way as they
            // were traversed during layout construction, so gas charging should be exactly
            // the same as on the cache miss.
            if let Err(err) = self.charge_module(gas_meter, traversal_context, module_id) {
                return Some(Err(err));
            }
        }
        Some(Ok(layout))
    }
```

**File:** aptos-move/block-executor/src/captured_reads.rs (L1050-1089)
```rust
    pub(crate) fn validate_module_reads(
        &self,
        global_module_cache: &GlobalModuleCache<K, DC, VC, S>,
        per_block_module_cache: &SyncModuleCache<K, DC, VC, S, Option<TxnIndex>>,
        maybe_updated_module_keys: Option<&BTreeSet<K>>,
    ) -> bool {
        if self.non_delayed_field_speculative_failure {
            return false;
        }

        let validate = |key: &K, read: &ModuleRead<DC, VC, S>| match read {
            ModuleRead::GlobalCache(_) => global_module_cache.contains_not_overridden(key),
            ModuleRead::PerBlockCache(previous) => {
                let current_version = per_block_module_cache.get_module_version(key);
                let previous_version = previous.as_ref().map(|(_, version)| *version);
                current_version == previous_version
            },
        };

        match maybe_updated_module_keys {
            Some(updated_module_keys) if updated_module_keys.len() <= self.module_reads.len() => {
                // When updated_module_keys is smaller, iterate over it and lookup in module_reads
                updated_module_keys
                    .iter()
                    .filter(|&k| self.module_reads.contains_key(k))
                    .all(|key| validate(key, self.module_reads.get(key).unwrap()))
            },
            Some(updated_module_keys) => {
                // When module_reads is smaller, iterate over it and filter by updated_module_keys
                self.module_reads
                    .iter()
                    .filter(|(k, _)| updated_module_keys.contains(k))
                    .all(|(key, read)| validate(key, read))
            },
            None => self
                .module_reads
                .iter()
                .all(|(key, read)| validate(key, read)),
        }
    }
```

**File:** aptos-move/block-executor/src/executor.rs (L980-987)
```rust
    /// This method may be executed by different threads / workers, but is guaranteed to be executed
    /// non-concurrently by the scheduling in parallel executor. This allows to perform light logic
    /// related to committing a transaction in a simple way and without excessive synchronization
    /// overhead. On the other hand, materialization that happens after commit (& after this method)
    /// is concurrent and deals with logic such as patching delayed fields / resource groups
    /// in outputs, which is heavier (due to serialization / deserialization, copies, etc). Moreover,
    /// since prepare_and_queue_commit_ready_txns takes care of synchronization in the flat-combining
    /// way, the materialization can be almost embarrassingly parallelizable.
```
