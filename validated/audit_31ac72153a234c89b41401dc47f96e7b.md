# Audit Report

## Title
Unprotected Concurrent Database Writes in LedgerDb::write_schemas() Enabling State Corruption

## Summary
The `LedgerDb::write_schemas()` function sequentially writes to eight separate RocksDB instances without mutex protection at the database layer. The `finalize_state_snapshot()` function bypasses the `pre_commit_lock` synchronization mechanism used by normal transaction processing, creating a race condition window where concurrent database writes can interleave across the eight sub-databases, leading to state inconsistency and potential consensus divergence.

## Finding Description

**The Core Issue:**

The `LedgerDb::write_schemas()` function performs sequential writes to eight independent RocksDB instances: [1](#0-0) 

Each write to an individual sub-database is atomic within that database, but there is no atomicity guarantee across all eight databases. The function lacks mutex protection to prevent concurrent invocations.

**Race Condition Path 1: Normal Transaction Processing**

When consensus commits blocks via `pre_commit_ledger()`, it acquires the `pre_commit_lock`: [2](#0-1) 

The lock definition exists in AptosDB: [3](#0-2) 

This path then spawns parallel threads to write to individual databases: [4](#0-3) 

**Race Condition Path 2: State Sync Finalization**

The `finalize_state_snapshot()` function calls `ledger_db.write_schemas()` WITHOUT acquiring the `pre_commit_lock`: [5](#0-4) 

Specifically at line 223, the unprotected write occurs. State sync directly calls the database writer: [6](#0-5) 

**Insufficient Coordination Mechanisms:**

The `PreCommitStatus.pause()` only sets a flag and does not wait for in-progress operations: [7](#0-6) 

The `executor.finish()` only releases memory and does not wait for database operations: [8](#0-7) 

Neither mechanism provides hard synchronization at the database layer.

**The Race Window:**

1. Thread 1 (consensus): Holds `pre_commit_lock`, spawns parallel writes to 8 databases (each to separate RocksDB instances)
2. Consensus detects need to sync, calls `PreCommitStatus.pause()` (doesn't wait)
3. Consensus calls `executor.finish()` (doesn't wait for DB operations)
4. Thread 2 (state sync): Calls `finalize_state_snapshot()` without acquiring `pre_commit_lock`
5. Thread 2 sequentially writes to the same 8 databases
6. **Result**: Interleaved writes - some databases have Thread 1's data, others have Thread 2's data

Since there are 8 separate RocksDB instances (when storage sharding is enabled), writes across them are not atomic, enabling this race condition to cause inconsistent state across the databases.

## Impact Explanation

This vulnerability meets **Critical Severity** criteria per Aptos bug bounty guidelines:

**Consensus/Safety Violations**: Different validator nodes can compute different state roots for the same blockchain version if writes interleave during the race window. Transaction metadata in one database may not match events in another, or accumulator roots may not correspond to actual transactions. This directly violates BFT consensus safety guarantees and can cause chain splits.

**State Corruption**: Partial writes across the eight databases create inconsistent ledger state that breaks the invariant that all sub-databases are at the same version. This corrupts Merkle tree structures and breaks proof verification.

**Non-recoverable Network Partition**: If enough validators experience this race condition simultaneously during state sync, they may end up with divergent state roots, potentially requiring hardfork intervention to resolve.

The impact qualifies as Critical under the categories: "Consensus/Safety violations" and "Non-recoverable network partition (requires hardfork)."

## Likelihood Explanation

**Likelihood: Medium-High**

The race window exists whenever:
1. A validator is actively processing transactions (pre-commits in progress)
2. State sync is triggered (common during catch-up, restarts, or falling behind)
3. The timing aligns such that `finalize_state_snapshot()` executes before in-progress pre-commits complete their database writes

**Factors increasing likelihood:**
- State sync is a common operation for validators
- No defensive synchronization at the database layer
- Multiple code paths can trigger the race (restore, finalization, truncation)
- The coordination relies on "soft" guarantees (flags) rather than "hard" mutual exclusion (mutexes)
- Race can occur naturally without attacker control under high load or network partitions

While the race window is narrow (milliseconds), the lack of defensive checks means any timing edge case or future refactoring could expose this vulnerability.

## Recommendation

Add mutex protection to `finalize_state_snapshot()` to ensure mutual exclusion with `pre_commit_ledger()`:

```rust
fn finalize_state_snapshot(
    &self,
    version: Version,
    output_with_proof: TransactionOutputListWithProofV2,
    ledger_infos: &[LedgerInfoWithSignatures],
) -> Result<()> {
    // Acquire pre_commit_lock to prevent concurrent writes
    let _lock = self.pre_commit_lock.lock()
        .expect("Failed to acquire pre_commit_lock for state snapshot finalization");
    
    // ... rest of the implementation
    self.ledger_db.write_schemas(ledger_db_batch)?;
    // ...
}
```

Alternatively, introduce a dedicated cross-database write lock that both `pre_commit_ledger()` and `finalize_state_snapshot()` must acquire before writing to `ledger_db`.

## Proof of Concept

A PoC would require setting up:
1. A validator node actively processing transactions
2. Triggering state sync during active pre-commits
3. Monitoring database state across the 8 sub-databases to detect inconsistency

The race condition is evident from code analysis: `pre_commit_ledger()` uses `pre_commit_lock` while `finalize_state_snapshot()` does not, and both write to the same 8 RocksDB instances without cross-database atomicity.

**Notes**

The vulnerability stems from the architectural decision to use 8 separate RocksDB instances (storage sharding) combined with insufficient synchronization between the consensus commit path and the state sync finalization path. The `pre_commit_lock` exists to prevent concurrent pre-commits, but `finalize_state_snapshot()` bypasses this protection entirely, creating a critical race condition window where database state can become inconsistent across validators.

### Citations

**File:** storage/aptosdb/src/ledger_db/mod.rs (L531-548)
```rust
    pub fn write_schemas(&self, schemas: LedgerDbSchemaBatches) -> Result<()> {
        self.write_set_db
            .write_schemas(schemas.write_set_db_batches)?;
        self.transaction_info_db
            .write_schemas(schemas.transaction_info_db_batches)?;
        self.transaction_db
            .write_schemas(schemas.transaction_db_batches)?;
        self.persisted_auxiliary_info_db
            .write_schemas(schemas.persisted_auxiliary_info_db_batches)?;
        self.event_db.write_schemas(schemas.event_db_batches)?;
        self.transaction_accumulator_db
            .write_schemas(schemas.transaction_accumulator_db_batches)?;
        self.transaction_auxiliary_data_db
            .write_schemas(schemas.transaction_auxiliary_data_db_batches)?;
        // TODO: remove this after sharding migration
        self.ledger_metadata_db
            .write_schemas(schemas.ledger_metadata_db_batches)
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L44-76)
```rust
    fn pre_commit_ledger(&self, chunk: ChunkToCommit, sync_commit: bool) -> Result<()> {
        gauged_api("pre_commit_ledger", || {
            // Pre-committing and committing in concurrency is allowed but not pre-committing at the
            // same time from multiple threads, the same for committing.
            // Consensus and state sync must hand over to each other after all pending execution and
            // committing complete.
            let _lock = self
                .pre_commit_lock
                .try_lock()
                .expect("Concurrent committing detected.");
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["pre_commit_ledger"]);

            chunk
                .state_summary
                .latest()
                .global_state_summary
                .log_generation("db_save");

            self.pre_commit_validation(&chunk)?;
            let _new_root_hash =
                self.calculate_and_commit_ledger_and_state_kv(&chunk, self.skip_index_and_usage)?;

            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["save_transactions__others"]);

            self.state_store.buffered_state().lock().update(
                chunk.result_ledger_state_with_summary(),
                chunk.estimated_total_state_updates(),
                sync_commit || chunk.is_reconfig,
            )?;

            Ok(())
        })
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L125-241)
```rust
    fn finalize_state_snapshot(
        &self,
        version: Version,
        output_with_proof: TransactionOutputListWithProofV2,
        ledger_infos: &[LedgerInfoWithSignatures],
    ) -> Result<()> {
        let (output_with_proof, persisted_aux_info) = output_with_proof.into_parts();
        gauged_api("finalize_state_snapshot", || {
            // Ensure the output with proof only contains a single transaction output and info
            let num_transaction_outputs = output_with_proof.get_num_outputs();
            let num_transaction_infos = output_with_proof.proof.transaction_infos.len();
            ensure!(
                num_transaction_outputs == 1,
                "Number of transaction outputs should == 1, but got: {}",
                num_transaction_outputs
            );
            ensure!(
                num_transaction_infos == 1,
                "Number of transaction infos should == 1, but got: {}",
                num_transaction_infos
            );

            // TODO(joshlind): include confirm_or_save_frozen_subtrees in the change set
            // bundle below.

            // Update the merkle accumulator using the given proof
            let frozen_subtrees = output_with_proof
                .proof
                .ledger_info_to_transaction_infos_proof
                .left_siblings();
            restore_utils::confirm_or_save_frozen_subtrees(
                self.ledger_db.transaction_accumulator_db_raw(),
                version,
                frozen_subtrees,
                None,
            )?;

            // Create a single change set for all further write operations
            let mut ledger_db_batch = LedgerDbSchemaBatches::new();
            let mut sharded_kv_batch = self.state_kv_db.new_sharded_native_batches();
            let mut state_kv_metadata_batch = SchemaBatch::new();
            // Save the target transactions, outputs, infos and events
            let (transactions, outputs): (Vec<Transaction>, Vec<TransactionOutput>) =
                output_with_proof
                    .transactions_and_outputs
                    .into_iter()
                    .unzip();
            let events = outputs
                .clone()
                .into_iter()
                .map(|output| output.events().to_vec())
                .collect::<Vec<_>>();
            let wsets: Vec<WriteSet> = outputs
                .into_iter()
                .map(|output| output.write_set().clone())
                .collect();
            let transaction_infos = output_with_proof.proof.transaction_infos;
            // We should not save the key value since the value is already recovered for this version
            restore_utils::save_transactions(
                self.state_store.clone(),
                self.ledger_db.clone(),
                version,
                &transactions,
                &persisted_aux_info,
                &transaction_infos,
                &events,
                wsets,
                Some((
                    &mut ledger_db_batch,
                    &mut sharded_kv_batch,
                    &mut state_kv_metadata_batch,
                )),
                false,
            )?;

            // Save the epoch ending ledger infos
            restore_utils::save_ledger_infos(
                self.ledger_db.metadata_db(),
                ledger_infos,
                Some(&mut ledger_db_batch.ledger_metadata_db_batches),
            )?;

            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::LedgerCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;
            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::OverallCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;

            // Apply the change set writes to the database (atomically) and update in-memory state
            //
            // state kv and SMT should use shared way of committing.
            self.ledger_db.write_schemas(ledger_db_batch)?;

            self.ledger_pruner.save_min_readable_version(version)?;
            self.state_store
                .state_merkle_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .epoch_snapshot_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .state_kv_pruner
                .save_min_readable_version(version)?;

            restore_utils::update_latest_ledger_info(self.ledger_db.metadata_db(), ledger_infos)?;
            self.state_store.reset();

            Ok(())
        })
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L263-322)
```rust
    fn calculate_and_commit_ledger_and_state_kv(
        &self,
        chunk: &ChunkToCommit,
        skip_index_and_usage: bool,
    ) -> Result<HashValue> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["save_transactions__work"]);

        let mut new_root_hash = HashValue::zero();
        THREAD_MANAGER.get_non_exe_cpu_pool().scope(|s| {
            // TODO(grao): Write progress for each of the following databases, and handle the
            // inconsistency at the startup time.
            //
            // TODO(grao): Consider propagating the error instead of panic, if necessary.
            s.spawn(|_| {
                self.commit_events(
                    chunk.first_version,
                    chunk.transaction_outputs,
                    skip_index_and_usage,
                )
                .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .write_set_db()
                    .commit_write_sets(chunk.first_version, chunk.transaction_outputs)
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .transaction_db()
                    .commit_transactions(
                        chunk.first_version,
                        chunk.transactions,
                        skip_index_and_usage,
                    )
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .persisted_auxiliary_info_db()
                    .commit_auxiliary_info(chunk.first_version, chunk.persisted_auxiliary_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_state_kv_and_ledger_metadata(chunk, skip_index_and_usage)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_transaction_infos(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                new_root_hash = self
                    .commit_transaction_accumulator(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
        });

        Ok(new_root_hash)
    }
```

**File:** storage/aptosdb/src/db/mod.rs (L35-35)
```rust
    pre_commit_lock: std::sync::Mutex<()>,
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L1129-1136)
```rust
    storage
        .writer
        .finalize_state_snapshot(
            version,
            target_output_with_proof.clone(),
            epoch_change_proofs,
        )
        .map_err(|error| format!("Failed to finalize the state snapshot! Error: {:?}", error))?;
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L98-100)
```rust
    pub fn pause(&mut self) {
        self.paused = true;
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L151-155)
```rust
    fn finish(&self) {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "finish"]);

        *self.inner.write() = None;
    }
```
