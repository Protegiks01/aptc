# Audit Report

## Title
Time-of-Check-Time-of-Use Race Condition in Payload Availability Causes Consensus Execution Failures

## Summary
A TOCTOU race condition exists between batch availability checking and transaction retrieval in the OptQuorumStore payload manager. Concurrent batch expiration during normal block commits can remove batches after the availability check passes, causing infinite retry loops that block the execution pipeline and degrade validator performance.

## Finding Description

The vulnerability occurs in the consensus block processing pipeline where payload availability is checked before block execution begins, but the actual transaction retrieval happens asynchronously much later through multiple pipeline stages.

**Time of Check:**
When a proposal is received, the system checks payload availability immediately after block insertion: [1](#0-0) 

This delegates to the payload manager's availability check: [2](#0-1) 

For OptQuorumStore payloads, the check verifies that all optimistic batches exist locally: [3](#0-2) 

The `exists()` method checks the in-memory batch cache: [4](#0-3) 

**Time Gap:**
After passing the availability check, the block proceeds through multiple asynchronous operations before transactions are actually retrieved:
1. Backpressure checking
2. Vote construction and broadcast
3. Pipeline construction with multiple spawned futures

The materialize phase is spawned as a separate future: [5](#0-4) 

**Time of Use:**
Transaction retrieval happens much later during block materialization: [6](#0-5) 

For OptQuorumStore payloads, this eventually calls batch retrieval: [7](#0-6) 

**Concurrent Modification:**
Between the check and use, OTHER blocks can commit, triggering batch cleanup. When a block commits, `notify_commit()` is called: [8](#0-7) 

This updates the certified timestamp and triggers expiration cleanup: [9](#0-8) 

The cleanup removes expired batches from the cache: [10](#0-9) 

**Failure Path:**
If the batch is no longer in the local cache, `get_batch_from_local()` returns an error: [11](#0-10) 

The system then attempts to fetch from peers. If all peers have also removed the expired batch, the request fails: [12](#0-11) 

**Critical Issue - Infinite Retry Loop:**
When `materialize_block()` fails, it enters an infinite retry loop with no timeout or retry limit: [13](#0-12) 

This infinite loop blocks the execution pipeline, preventing subsequent blocks from being processed and causing validator performance degradation.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty criteria for "Validator node slowdowns":

1. **Validator Performance Degradation**: The infinite retry loop with 100ms delays causes continuous resource consumption without progress. Validators stuck in this state waste CPU cycles and memory retrying to fetch unavailable batches.

2. **Pipeline Blockage**: The execution pipeline becomes blocked on the stuck block. Since child blocks depend on parent block execution completion, this cascades to prevent processing of subsequent blocks, effectively creating a pipeline stall.

3. **Consensus Availability Impact**: While not causing total network liveness loss, if multiple validators are simultaneously affected (which is likely given they all experience similar commit patterns and timing), this significantly degrades consensus performance and increases block confirmation times.

The 60-second expiration buffer exists specifically to help slow nodes catch up, but the race condition undermines this protection mechanism. Once the availability check passes, validators proceed with processing, but the lack of any guarantee that batches remain available through execution completion creates this vulnerability.

## Likelihood Explanation

**Likelihood: Moderate to High**

The vulnerability is highly likely to occur under realistic network conditions:

1. **Timing Windows**: The gap between check and use spans multiple async operations (backpressure checks, vote construction, pipeline spawning), easily extending to hundreds of milliseconds or seconds under load.

2. **Natural Trigger Conditions**:
   - Blocks naturally contain batches approaching expiration (within the 60-second buffer window)
   - High commit rates on active networks trigger frequent `notify_commit()` calls
   - Processing delays from network latency or validator resource constraints are common
   - The expiration buffer mechanism itself ensures batches are regularly cleaned up

3. **No Attacker Required**: This is a race condition that occurs during normal network operations. Any validator processing a block with near-expired batches while other blocks commit can experience this issue.

4. **Cascading Effect**: Once one validator gets stuck, it falls behind, making it more likely to receive blocks with batches near expiration, increasing the probability of repeated occurrences.

## Recommendation

Implement timeout and retry limit mechanisms in the materialize loop:

```rust
let result = {
    let max_retries = 10;
    let mut attempts = 0;
    loop {
        match preparer.materialize_block(&block, qc_rx.clone()).await {
            Ok(input_txns) => break Ok(input_txns),
            Err(e) => {
                attempts += 1;
                if attempts >= max_retries {
                    warn!(
                        "[BlockPreparer] failed to prepare block {} after {} attempts, giving up: {}",
                        block.id(),
                        attempts,
                        e
                    );
                    break Err(e);
                }
                warn!(
                    "[BlockPreparer] failed to prepare block {} (attempt {}/{}), retrying: {}",
                    block.id(),
                    attempts,
                    max_retries,
                    e
                );
                tokio::time::sleep(Duration::from_millis(100)).await;
            },
        }
    }
}?;
```

Additional mitigation: Add expiration checking during the availability check phase to reject blocks with batches too close to expiration, preventing them from entering the pipeline in the first place.

## Proof of Concept

This vulnerability occurs during normal network operations and cannot be easily demonstrated with a standalone PoC. However, the race condition can be triggered by:

1. Proposing a block containing OptQuorumStore batches within 60 seconds of their expiration time
2. Ensuring sufficient processing delay (via network latency or validator load) between `check_payload()` and `materialize_block()`
3. Having another block commit during this window, triggering `notify_commit()` and batch expiration cleanup
4. The affected validator will then enter the infinite retry loop attempting to fetch the now-expired batch

The vulnerability can be observed by monitoring validator logs for repeated `[BlockPreparer] failed to prepare block, retrying` messages with `ExecutorError::CouldNotGetData` errors, occurring at 100ms intervals without termination.

## Notes

The claim that this "prevents validators from voting on valid blocks" is partially accurate but requires clarification: validators can still vote on the specific block experiencing the race (voting occurs before/during pipeline construction), but the infinite retry loop blocks the execution pipeline, which prevents processing of subsequent dependent blocks. This indirect effect can degrade consensus participation over time as the validator's execution state falls behind.

The vulnerability is valid and meets High Severity criteria for validator node slowdowns. The infinite retry loop with no timeout is particularly problematic and represents a clear protocol-level issue that should be addressed.

### Citations

**File:** consensus/src/round_manager.rs (L1262-1263)
```rust
        if block_store.check_payload(&proposal).is_err() {
            debug!("Payload not available locally for block: {}", proposal.id());
```

**File:** consensus/src/block_storage/block_store.rs (L605-607)
```rust
    pub fn check_payload(&self, proposal: &Block) -> Result<(), BitVec> {
        self.payload_manager.check_payload_availability(proposal)
    }
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L168-170)
```rust
    fn notify_commit(&self, block_timestamp: u64, payloads: Vec<Payload>) {
        self.batch_reader
            .update_certified_timestamp(block_timestamp);
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L409-424)
```rust
            Payload::OptQuorumStore(OptQuorumStorePayload::V1(p)) => {
                let mut missing_authors = BitVec::with_num_bits(self.ordered_authors.len() as u16);
                for batch in p.opt_batches().deref() {
                    if self.batch_reader.exists(batch.digest()).is_none() {
                        let index = *self
                            .address_to_validator_index
                            .get(&batch.author())
                            .expect("Payload author should have been verified");
                        missing_authors.set(index as u16);
                    }
                }
                if missing_authors.all_zeros() {
                    Ok(())
                } else {
                    Err(missing_authors)
                }
```

**File:** consensus/src/quorum_store/batch_store.rs (L443-472)
```rust
    pub(crate) fn clear_expired_payload(&self, certified_time: u64) -> Vec<HashValue> {
        // To help slow nodes catch up via execution without going to state sync we keep the blocks for 60 extra seconds
        // after the expiration time. This will help remote peers fetch batches that just expired but are within their
        // execution window.
        let expiration_time = certified_time.saturating_sub(self.expiration_buffer_usecs);
        let expired_digests = self.expirations.lock().expire(expiration_time);
        let mut ret = Vec::new();
        for h in expired_digests {
            let removed_value = match self.db_cache.entry(h) {
                Occupied(entry) => {
                    // We need to check up-to-date expiration again because receiving the same
                    // digest with a higher expiration would update the persisted value and
                    // effectively extend the expiration.
                    if entry.get().expiration() <= expiration_time {
                        self.persist_subscribers.remove(entry.get().digest());
                        Some(entry.remove())
                    } else {
                        None
                    }
                },
                Vacant(_) => unreachable!("Expired entry not in cache"),
            };
            // No longer holding the lock on db_cache entry.
            if let Some(value) = removed_value {
                self.free_quota(value);
                ret.push(h);
            }
        }
        ret
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L530-538)
```rust
    pub fn update_certified_timestamp(&self, certified_time: u64) {
        trace!("QS: batch reader updating time {:?}", certified_time);
        self.last_certified_time
            .fetch_max(certified_time, Ordering::SeqCst);

        let expired_keys = self.clear_expired_payload(certified_time);
        if let Err(e) = self.db.delete_batches(expired_keys) {
            debug!("Error deleting batches: {:?}", e)
        }
```

**File:** consensus/src/quorum_store/batch_store.rs (L571-585)
```rust
    pub(crate) fn get_batch_from_local(
        &self,
        digest: &HashValue,
    ) -> ExecutorResult<PersistedValue<BatchInfoExt>> {
        if let Some(value) = self.db_cache.get(digest) {
            if value.payload_storage_mode() == StorageMode::PersistedOnly {
                self.get_batch_from_db(digest, value.batch_info().is_v2())
            } else {
                // Available in memory.
                Ok(value.clone())
            }
        } else {
            Err(ExecutorError::CouldNotGetData)
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L684-709)
```rust
                let fut = async move {
                    let batch_digest = *batch_info.digest();
                    defer!({
                        inflight_requests_clone.lock().remove(&batch_digest);
                    });
                    // TODO(ibalajiarun): Support V2 batch
                    if let Ok(mut value) = batch_store.get_batch_from_local(&batch_digest) {
                        Ok(value.take_payload().expect("Must have payload"))
                    } else {
                        // Quorum store metrics
                        counters::MISSED_BATCHES_COUNT.inc();
                        let subscriber_rx = batch_store.subscribe(*batch_info.digest());
                        let payload = requester
                            .request_batch(
                                batch_digest,
                                batch_info.expiration(),
                                responders,
                                subscriber_rx,
                            )
                            .await?;
                        batch_store.persist(vec![PersistedValue::new(
                            batch_info.into(),
                            Some(payload.clone()),
                        )]);
                        Ok(payload)
                    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L727-732)
```rust
    fn exists(&self, digest: &HashValue) -> Option<PeerId> {
        self.batch_store
            .get_batch_from_local(digest)
            .map(|v| v.author())
            .ok()
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L457-460)
```rust
        let materialize_fut = spawn_shared_fut(
            Self::materialize(self.block_preparer.clone(), block.clone(), qc_rx),
            Some(&mut abort_handles),
        );
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L634-646)
```rust
        let result = loop {
            match preparer.materialize_block(&block, qc_rx.clone()).await {
                Ok(input_txns) => break input_txns,
                Err(e) => {
                    warn!(
                        "[BlockPreparer] failed to prepare block {}, retrying: {}",
                        block.id(),
                        e
                    );
                    tokio::time::sleep(Duration::from_millis(100)).await;
                },
            }
        };
```

**File:** consensus/src/block_preparer.rs (L54-63)
```rust
        let (txns, max_txns_from_block_to_execute, block_gas_limit) = tokio::select! {
                // Poll the block qc future until a QC is received. Ignore None outcomes.
                Some(qc) = block_qc_fut => {
                    let block_voters = Some(qc.ledger_info().get_voters_bitvec().clone());
                    self.payload_manager.get_transactions(block, block_voters).await
                },
                result = self.payload_manager.get_transactions(block, None) => {
                   result
                }
        }?;
```

**File:** consensus/src/quorum_store/batch_requester.rs (L142-151)
```rust
                            Ok(BatchResponse::NotFound(ledger_info)) => {
                                counters::RECEIVED_BATCH_NOT_FOUND_COUNT.inc();
                                if ledger_info.commit_info().epoch() == epoch
                                    && ledger_info.commit_info().timestamp_usecs() > expiration
                                    && ledger_info.verify_signatures(&validator_verifier).is_ok()
                                {
                                    counters::RECEIVED_BATCH_EXPIRED_COUNT.inc();
                                    debug!("QS: batch request expired, digest:{}", digest);
                                    return Err(ExecutorError::CouldNotGetData);
                                }
```
