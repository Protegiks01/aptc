# Audit Report

## Title
Dependency Merge Order Violation in `remove_v2` Causes Validator Node Crashes

## Summary
The `remove_v2` method in BlockSTMv2's resource group handling contains a critical logic flaw in dependency migration. When a transaction executes multiple times (multiple incarnations) and is removed multiple times, the dependency merge order invariant is violated, causing a `PanicError` that crashes validator nodes when fallback is disabled.

## Finding Description

The vulnerability exists in the dependency migration logic when removing resource group size entries. The `remove_v2` method attempts to migrate read dependencies from a removed entry to the next lower entry when they have matching sizes: [1](#0-0) 

This calls `extend_with_higher_dependencies`, which enforces a strict ordering invariant: [2](#0-1) 

The invariant check verifies that the highest transaction index in the target entry must be strictly less than the lowest transaction index being merged: [3](#0-2) 

**Corrected Attack Scenario:**

1. Entry at index 5 exists with size S1
2. Transaction 10 executes, creates entry at index 10 with size S1
3. Transaction 15 reads size, finds entry 10, registers dependency {15: inc0} in entry 10 [4](#0-3) 

4. Transaction 10 aborts, `remove_v2` is called
5. Dependencies {15: inc0} are successfully merged into entry 5 (check passes: entry 5 has no deps or deps < 15)
6. Transaction 10 re-executes (inc 1), creates new entry at index 10 with size S1
7. Transaction 12 reads size, finds entry 10, registers dependency {12: inc0} in entry 10  
8. Transaction 10 aborts again, `remove_v2` is called
9. **BUG TRIGGERS**: Try to merge {12: inc0} into entry 5 which now has {15: inc0}
10. Check fails: `highest_dep_idx in entry 5 = 15 should be < lowest in {12: inc0} = 12`
11. `check_lowest_dependency_idx` returns `PanicError::CodeInvariantError`

The error propagates through the execution stack via `process_resource_group_output_v2`: [5](#0-4) 

When `allow_fallback` is disabled (typical for production validators), the validator panics: [6](#0-5) 

The configuration flag controlling fallback behavior: [7](#0-6) 

## Impact Explanation

**Severity: CRITICAL**

This vulnerability meets the Critical severity criteria under "Total loss of liveness/network availability":

1. **Validator Node Crashes**: When `allow_fallback` is disabled (standard for production validators for performance reasons), triggering this bug causes a panic that crashes the validator node completely

2. **Consensus Liveness Impact**: Multiple validators processing the same block with specific transaction patterns could crash simultaneously, potentially causing consensus liveness failures if enough validators are affected

3. **Deterministic Execution Violation**: Different validators may crash at different times based on scheduling variations in BlockSTM's parallel execution, violating deterministic execution guarantees

4. **Network Availability**: Repeated crashes force validator restarts, reducing network availability and potentially enabling other attacks during recovery periods

The bug exists in production code used by BlockSTMv2 for parallel transaction execution on mainnet.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

The vulnerability can be triggered when:
1. BlockSTMv2 is enabled (production setting on mainnet)
2. Transactions create specific abort/re-execution patterns involving resource groups
3. Multiple transactions read the same resource group size during different execution attempts
4. Transaction re-executions register dependencies in an order that violates the monotonic assumption

These conditions naturally arise in BlockSTM's normal operation:
- Transaction aborts during speculative execution are common and expected
- Resource group operations are used extensively throughout the Aptos Framework
- Re-execution with incremented incarnations is standard BlockSTM behavior
- The parallel scheduler can cause later transactions to read before earlier ones in subsequent incarnations

An attacker could increase likelihood by:
- Submitting transactions that intentionally conflict on resource groups to force aborts
- Creating read-write patterns that maximize re-execution probability
- Targeting heavily-used resource groups to affect multiple validators simultaneously

## Recommendation

The fix requires modifying the dependency merge logic to handle non-monotonic dependency indices that can occur during re-executions. Options include:

1. **Use general `extend` instead of `extend_with_higher_dependencies`**: Replace the strict ordering check with the general extend method that handles overlapping indices by keeping the highest incarnation.

2. **Filter dependencies before merging**: When migrating dependencies to a lower entry, filter out any that would violate the ordering invariant and mark them as invalidated instead.

3. **Track removal history**: Maintain metadata about previous removals to detect and handle the re-execution case specially.

The recommended fix is option 1, as it's simplest and aligns with how dependencies are already merged in other contexts:

```rust
// Replace line 400 with:
next_lower_entry
    .value
    .dependencies
    .lock()
    .extend(std::mem::take(&mut removed_size_deps));
```

This uses the general `extend` method which properly handles overlapping indices by keeping the latest incarnation, rather than enforcing strict ordering.

## Proof of Concept

The vulnerability can be demonstrated with a Rust test that simulates the scenario described. The test would:

1. Create a resource group with base entry at index 5
2. Write entry at index 10, register read dependency from transaction 15
3. Remove entry 10, verify dependencies migrated to entry 5
4. Write entry at index 10 again (simulating re-execution)
5. Register read dependency from transaction 12 
6. Attempt to remove entry 10 again
7. Verify that the `PanicError` is returned due to ordering violation

The existing test suite in `versioned_group_data.rs` does not cover this specific re-execution scenario with multiple removals of the same index.

## Notes

While the specific transaction indices in the original report's scenario were incorrect (transaction 9 cannot read from entry 10 since 9 < 10), the underlying vulnerability is valid. The corrected scenario with transactions 12 and 15 reading from entry 10 at different incarnations accurately demonstrates the bug. The invariant assumption that dependencies can be merged monotonically breaks down when the same transaction index is created and removed multiple times during BlockSTM's speculative execution.

### Citations

**File:** aptos-move/mvhashmap/src/versioned_group_data.rs (L385-402)
```rust
        // Handle dependencies for the removed size entry.
        let mut removed_size_deps = take_dependencies(&removed_size_entry.value.dependencies);
        if let Some((_, next_lower_entry)) = Self::get_latest_entry(
            &group_sizes.size_entries,
            txn_idx,
            ReadPosition::BeforeCurrentTxn,
        ) {
            // If the entry that will be read after removal contains the same size,
            // then the dependencies on size can be registered there and not invalidated.
            // In this case, removed_size_deps gets drained.
            if next_lower_entry.value.size == removed_size_entry.value.size {
                next_lower_entry
                    .value
                    .dependencies
                    .lock()
                    .extend_with_higher_dependencies(std::mem::take(&mut removed_size_deps))?;
            }
        }
```

**File:** aptos-move/mvhashmap/src/versioned_group_data.rs (L485-502)
```rust
    pub fn get_group_size_and_record_dependency(
        &self,
        group_key: &K,
        txn_idx: TxnIndex,
        incarnation: Incarnation,
    ) -> Result<ResourceGroupSize, MVGroupError> {
        match self.group_sizes.get(group_key) {
            Some(g) => {
                Self::get_latest_entry(&g.size_entries, txn_idx, ReadPosition::BeforeCurrentTxn)
                    .map_or(Err(MVGroupError::Uninitialized), |(_, size)| {
                        // TODO(BlockSTMv2): convert to PanicErrors after MVHashMap refactoring.
                        assert_ok!(size.value.dependencies.lock().insert(txn_idx, incarnation));
                        Ok(size.value.size)
                    })
            },
            None => Err(MVGroupError::Uninitialized),
        }
    }
```

**File:** aptos-move/mvhashmap/src/registered_dependencies.rs (L12-25)
```rust
pub(crate) fn check_lowest_dependency_idx(
    dependencies: &BTreeMap<TxnIndex, Incarnation>,
    txn_idx: TxnIndex,
) -> Result<(), PanicError> {
    if let Some((lowest_dep_idx, _)) = dependencies.first_key_value() {
        if *lowest_dep_idx <= txn_idx {
            return Err(code_invariant_error(format!(
                "Dependency for txn {} recorded at idx {}",
                *lowest_dep_idx, txn_idx
            )));
        }
    }
    Ok(())
}
```

**File:** aptos-move/mvhashmap/src/registered_dependencies.rs (L104-117)
```rust
    pub(crate) fn extend_with_higher_dependencies(
        &mut self,
        other: BTreeMap<TxnIndex, Incarnation>,
    ) -> Result<(), PanicError> {
        let dependencies = &mut self.dependencies;
        if let Some((highest_dep_idx, _)) = dependencies.last_key_value() {
            // Highest dependency in self should be strictly less than other dependencies.
            check_lowest_dependency_idx(&other, *highest_dep_idx)?;
        }

        Self::extend_impl(dependencies, other);

        Ok(())
    }
```

**File:** aptos-move/block-executor/src/executor.rs (L286-292)
```rust
                        abort_manager.invalidate_dependencies(
                            versioned_cache.group_data().remove_v2(
                                group_key_ref,
                                idx_to_execute,
                                prev_tags,
                            )?,
                        )?;
```

**File:** aptos-move/block-executor/src/executor.rs (L2581-2583)
```rust
            if !self.config.local.allow_fallback {
                panic!("Parallel execution failed and fallback is not allowed");
            }
```

**File:** types/src/block_executor/config.rs (L57-62)
```rust
    // If specified, parallel execution fallbacks to sequential, if issue occurs.
    // Otherwise, if there is an error in either of the execution, we will panic.
    pub allow_fallback: bool,
    // If true, we will discard the failed blocks and continue with the next block.
    // (allow_fallback needs to be set)
    pub discard_failed_blocks: bool,
```
