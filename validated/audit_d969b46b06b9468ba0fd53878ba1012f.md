After thorough code analysis of the Aptos Core codebase, I must validate this security claim.

# Audit Report

## Title
State Snapshot Restore Race Condition Allows Unverified Data Persistence During State Sync

## Summary
A Time-of-Check to Time-of-Use (TOCTOU) race condition exists in `StateSnapshotRestore::add_chunk` where key-value data is written to the database in parallel with cryptographic proof verification. When a malicious network peer sends invalid state chunks during state synchronization, invalid data can be persisted to disk before proof verification detects the mismatch, leaving the database in a corrupted state requiring manual recovery.

## Finding Description

The vulnerability exists in the state snapshot restoration process during network state synchronization. The core issue is in the Default restore mode, which uses parallel execution without proper transactional coordination.

**Parallel Execution Without Atomicity:**

`StateSnapshotRestore::add_chunk` executes two operations in parallel using `IO_POOL.join()`: [1](#0-0) 

The `kv_fn` writes raw state values directly to RocksDB via `StateValueRestore::add_chunk`: [2](#0-1) 

This triggers an immediate synchronous commit to RocksDB: [3](#0-2) 

The commit operation is synchronous and completes before the function returns: [4](#0-3) 

Meanwhile, `tree_fn` verifies the Sparse Merkle proof: [5](#0-4) 

Proof verification happens in `verify()`: [6](#0-5) [7](#0-6) 

**Critical Flaw:** The `rayon::join()` executes both functions to completion, then checks results. If `kv_fn` successfully commits but `tree_fn` verification fails, the corrupted data persists in RocksDB with no rollback mechanism.

**Attack Vector:**

During bootstrapping, the `process_state_values_payload` method performs only a field comparison check: [8](#0-7) 

This check validates that the `root_hash` **field** matches the expected value, but does NOT cryptographically verify that the `raw_values` are consistent with the proof. A malicious network peer can send:
- `root_hash`: Correct value (copied from expected hash)
- `raw_values`: Malicious/corrupted state data
- `proof`: Mismatched Sparse Merkle proof

The corrupted `raw_values` are written to disk in `kv_fn` before `tree_fn` detects the proof mismatch.

**Default Mode is Production Configuration:** [9](#0-8) 

## Impact Explanation

**Critical Severity** - This vulnerability causes non-recoverable state database inconsistencies:

1. **State Database Corruption**: Invalid state data persists permanently in the KV database with the progress marker indicating successful processing.

2. **Non-Recoverable Without Manual Intervention**: The progress tracking system records the corrupted chunk as successfully processed. On restart, the system continues from the corrupted state rather than rolling back: [10](#0-9) 

The truncation mechanism exists but requires manual intervention to identify and fix the corruption.

3. **Bootstrapping Denial of Service**: Affected nodes cannot complete state synchronization, preventing new nodes from joining the network and existing nodes from recovering from downtime.

4. **Database Inconsistency**: KV data and Merkle tree data become desynchronized, breaking the fundamental invariant that state values match their cryptographic commitments.

This meets **Critical Severity** criteria per Aptos Bug Bounty under "Non-recoverable state inconsistencies" that affect core protocol operations (state synchronization).

## Likelihood Explanation

**High Likelihood:**

1. **Common Operation**: State synchronization is required for all new nodes joining the network and nodes recovering from downtime.

2. **Untrusted Attack Surface**: Any network peer can send `StateValueChunkWithProof` messages during state sync. No special privileges required.

3. **Parallel Execution Design**: The race condition is inherent in the design where verification and persistence are decoupled by parallel execution.

4. **No Cryptographic Pre-Validation**: The bootstrapper's check at line 1021 is insufficient as it only validates field equality, not cryptographic consistency.

## Recommendation

Implement transactional coordination between KV writes and proof verification:

1. **Sequential Verification-Then-Write**: Verify the proof before writing any data to disk, or use a two-phase commit pattern.

2. **Cryptographic Pre-Validation**: Enhance the bootstrapper check to cryptographically verify that `raw_values` match the `proof` before passing to storage layer.

3. **Atomic Rollback**: Implement a rollback mechanism that can undo partial writes when proof verification fails.

4. **Verification Mode**: Consider using `StateSnapshotRestoreMode::TreeOnly` first to verify proofs, then `KvOnly` to write data after validation.

## Proof of Concept

The vulnerability can be demonstrated by:

1. Setting up a malicious network peer that intercepts state sync requests
2. Sending `StateValueChunkWithProof` with:
   - Correct `root_hash` field (copied from expected value)
   - Modified `raw_values` (e.g., manipulated state keys or values)
   - Invalid `proof` that doesn't match the modified raw_values
3. Observing that the bootstrapper's check passes (line 1021)
4. Observing that `add_chunk` returns an error from proof verification
5. Verifying that the corrupted `raw_values` persist in the database despite the error
6. Confirming that the progress marker indicates successful processing of the corrupted chunk

The race condition can be reliably triggered by ensuring the KV write (disk I/O operation) completes before tree verification (CPU-bound hash computation) detects the mismatch.

## Notes

The vulnerability is valid and exploitable, though the impact is more accurately characterized as causing **state database corruption requiring manual recovery** rather than direct consensus divergence. Affected nodes fail to complete state synchronization rather than producing incorrect consensus outputs. However, this still qualifies as Critical severity under the "Non-recoverable state inconsistencies" category that requires manual intervention to resolve.

### Citations

**File:** storage/aptosdb/src/state_restore/mod.rs (L229-236)
```rust
        let kv_fn = || {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_value_add_chunk"]);
            self.kv_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk(chunk.clone())
        };
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L238-245)
```rust
        let tree_fn = || {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["jmt_add_chunk"]);
            self.tree_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk_impl(chunk.iter().map(|(k, v)| (k, v.hash())).collect(), proof)
        };
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L249-254)
```rust
            StateSnapshotRestoreMode::Default => {
                // We run kv_fn with TreeOnly to restore the usage of DB
                let (r1, r2) = IO_POOL.join(kv_fn, tree_fn);
                r1?;
                r2?;
            },
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1158-1158)
```rust
            StateSnapshotRestoreMode::Default,
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1277-1279)
```rust
        self.state_kv_db
            .commit(version, Some(batch), sharded_schema_batch)
    }
```

**File:** storage/aptosdb/src/state_kv_db.rs (L164-168)
```rust
        if !readonly {
            if let Some(overall_kv_commit_progress) = get_state_kv_commit_progress(&state_kv_db)? {
                truncate_state_kv_db_shards(&state_kv_db, overall_kv_commit_progress)?;
            }
        }
```

**File:** storage/aptosdb/src/state_kv_db.rs (L186-200)
```rust
            THREAD_MANAGER.get_io_pool().scope(|s| {
                let mut batches = sharded_state_kv_batches.into_iter();
                for shard_id in 0..NUM_STATE_SHARDS {
                    let state_kv_batch = batches
                        .next()
                        .expect("Not sufficient number of sharded state kv batches");
                    s.spawn(move |_| {
                        // TODO(grao): Consider propagating the error instead of panic, if necessary.
                        self.commit_single_shard(version, shard_id, state_kv_batch)
                            .unwrap_or_else(|err| {
                                panic!("Failed to commit shard {shard_id}: {err}.")
                            });
                    });
                }
            });
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L391-391)
```rust
        self.verify(proof)?;
```

**File:** types/src/proof/definition.rs (L817-823)
```rust
        ensure!(
            current_hash == expected_root_hash,
            "{}: Root hashes do not match. Actual root hash: {:x}. Expected root hash: {:x}.",
            type_name::<Self>(),
            current_hash,
            expected_root_hash,
        );
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L1021-1031)
```rust
        if state_value_chunk_with_proof.root_hash != expected_root_hash {
            self.reset_active_stream(Some(NotificationAndFeedback::new(
                notification_id,
                NotificationFeedback::InvalidPayloadData,
            )))
            .await?;
            return Err(Error::VerificationError(format!(
                "The states chunk with proof root hash: {:?} didn't match the expected hash: {:?}!",
                state_value_chunk_with_proof.root_hash, expected_root_hash,
            )));
        }
```
