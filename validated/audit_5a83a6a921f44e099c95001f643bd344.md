# Audit Report

## Title
Consensus Divergence via Fast/Slow Path Randomness Race Condition Leading to Validator Crashes

## Summary
The Aptos randomness system's dual-path architecture (fast/slow) uses independent cryptographic keys and lacks consistency verification between paths. Byzantine validators can exploit selective share broadcasting to cause different honest validators to compute different randomness values, leading to mismatched execution state roots. When validators with mismatched state attempt to process commit decisions from the majority, assertion failures cause validator crashes, threatening chain liveness and safety.

## Finding Description

The Aptos randomness generation system implements two cryptographically independent threshold signature schemes - fast path and slow path - that can produce different randomness outputs for the same block round.

**Dual-Path Cryptographic Independence:**

During epoch initialization, validators receive separate cryptographic key material for fast and slow paths. The fast path uses `sk.fast`, `pk.fast`, and public key shares from `transcript.fast`, while the slow path uses `sk.main`, `pk.main`, and shares from `transcript.main`. These are distinct threshold signature schemes with different thresholds. [1](#0-0) 

**Race Condition Without Consistency Verification:**

Both paths independently aggregate shares and send results to the same `decision_tx` channel. The RandManager accepts whichever randomness arrives first through a `set_randomness` function that only checks if randomness has already been set, without verifying that both paths produce identical values. [2](#0-1) [3](#0-2) 

**Decoupled Execution Enables Divergence:**

Aptos uses decoupled execution where validators vote on block ordering with a placeholder hash before execution. This allows blocks to be ordered before randomness is fully determined. Each validator then executes the ordered block using their locally-generated randomness. [4](#0-3) 

**Critical Assertion Failure Mechanism:**

After execution, validators sign commit votes containing their computed state root. When a validator receives a commit decision (certificate with 2f+1 signatures), it attempts to apply it using `try_advance_to_aggregated_with_ledger_info`. This function asserts that the received commit_info exactly matches the local commit_info. If they differ (due to different randomness), the assertion fails, causing a panic. [5](#0-4) 

**Attack Vector:**

1. Byzantine validators broadcast fast shares only to 2f+1 honest validators (Set A)
2. Byzantine validators broadcast slow shares only to f honest validators (Set B)
3. Set A completes fast path first → uses fast randomness → computes state root S1
4. Set B completes slow path first → uses slow randomness → computes state root S2
5. Set A (2f+1 validators) aggregates commit votes for S1 → forms commit certificate C1
6. C1 is broadcast to Set B
7. Set B validators execute `assert_eq!(commit_info, *commit_proof.commit_info())` where S2 ≠ S1
8. Assertion fails → Set B validators panic and crash

## Impact Explanation

**Critical Severity** - This vulnerability enables Byzantine validators to crash honest validators, violating both consensus safety and liveness properties:

1. **Validator Crashes**: The assertion failure causes validators to panic, crashing the consensus process on affected nodes. This represents a denial-of-service attack on individual validators through protocol manipulation rather than resource exhaustion.

2. **Chain Liveness Threat**: If Byzantine validators crash more than f validators, the remaining honest validators may fall below the 2f+1 threshold needed for consensus, halting the chain.

3. **Consensus Safety Violation**: The inability of validators to accept valid commit certificates (due to assertion failures) breaks the fundamental consensus assumption that honest validators can process and accept decisions from the supermajority.

4. **Manual Recovery Required**: Crashed validators require manual intervention to restart and resynchronize, creating operational burden and potential for extended downtime.

This aligns with **Critical Severity** under Aptos bug bounty categories as it combines:
- Total Loss of Liveness/Network Availability (validators crash)
- Consensus/Safety Violations (validators cannot process valid commit decisions)
- Non-recoverable without manual intervention

## Likelihood Explanation

**High Likelihood** when Byzantine validators are present:

**Technical Feasibility:**
- Byzantine validators naturally control their own message broadcasting and can selectively send shares to different validator subsets
- No cryptographic or protocol barriers prevent selective broadcasting
- Fast and slow paths have different thresholds by design (e.g., f+1 vs 2f+1), making it easier to trigger completion of one path at some validators but not others
- The attack requires no special network position or timing precision - just selective message delivery

**Detection Difficulty:**
- No consistency checks exist between fast and slow randomness paths
- Divergence only manifests when commit decisions are processed
- By the time the attack is detected, validators have already crashed

**Persistent Vulnerability:**
- Fast/slow dual-path architecture is a fundamental design feature
- Different thresholds are intentional for performance optimization
- Vulnerability persists across epochs as long as fast randomness is enabled

**Mainnet Applicability:**
The vulnerability affects production configurations where `order_vote_enabled: true` enables decoupled execution with ordering votes. [6](#0-5) 

## Recommendation

Implement consistency verification between fast and slow randomness paths:

1. **Dual-Path Verification**: When both paths complete, verify they produce identical randomness values before accepting either:
   ```rust
   pub fn set_randomness(&mut self, round: Round, rand: Randomness, path: PathType) -> Result<bool, RandomnessError> {
       let offset = self.offset(round);
       let block = &mut self.blocks_mut()[offset];
       
       if let Some(existing) = block.get_randomness() {
           // Verify consistency if setting from different path
           if existing.randomness() != rand.randomness() {
               return Err(RandomnessError::PathInconsistency {
                   existing: existing.randomness().to_vec(),
                   new: rand.randomness().to_vec(),
                   path,
               });
           }
           return Ok(false); // Already set and verified
       }
       
       block.set_randomness(rand);
       self.num_undecided_blocks -= 1;
       Ok(true)
   }
   ```

2. **Graceful Commit Mismatch Handling**: Replace assertions with error handling that triggers re-execution or synchronization:
   ```rust
   pub fn try_advance_to_aggregated_with_ledger_info(
       self,
       commit_proof: LedgerInfoWithSignatures,
   ) -> Result<Self, CommitMismatchError> {
       match self {
           Self::Executed(executed_item) => {
               if executed_item.commit_info != *commit_proof.commit_info() {
                   return Err(CommitMismatchError::ExecutionMismatch {
                       local: executed_item.commit_info,
                       received: commit_proof.commit_info().clone(),
                   });
               }
               // Continue with aggregation...
           }
           // ... handle other cases
       }
   }
   ```

3. **Deterministic Path Selection**: Use deterministic criteria (e.g., block hash) to select which path should be authoritative, preventing race conditions.

## Proof of Concept

Due to the complexity of the decoupled execution pipeline and the need for multiple validator nodes with Byzantine behavior, a full PoC requires a multi-node test environment. However, the vulnerability can be demonstrated through the following test scenario:

```rust
#[tokio::test]
async fn test_randomness_path_divergence() {
    // Setup: Create fast and slow rand configs with different keys
    let (fast_config, slow_config) = setup_dual_path_configs();
    let metadata = create_test_metadata(epoch: 1, round: 10);
    
    // Generate shares using different keys
    let fast_share = Share::generate(&fast_config, metadata.clone());
    let slow_share = Share::generate(&slow_config, metadata.clone());
    
    // Aggregate with different threshold sets
    let fast_randomness = aggregate_shares(&fast_config, &[fast_share /* ... f+1 shares */]);
    let slow_randomness = aggregate_shares(&slow_config, &[slow_share /* ... 2f+1 shares */]);
    
    // Verify different keys produce different randomness
    assert_ne!(fast_randomness.randomness(), slow_randomness.randomness());
    
    // Simulate Set A receiving fast randomness first
    let mut block_a = create_test_block();
    block_a.set_randomness(fast_randomness);
    let state_root_a = execute_block_with_randomness(&block_a);
    
    // Simulate Set B receiving slow randomness first
    let mut block_b = create_test_block();
    block_b.set_randomness(slow_randomness);
    let state_root_b = execute_block_with_randomness(&block_b);
    
    // Verify execution divergence
    assert_ne!(state_root_a, state_root_b);
    
    // Simulate commit decision from Set A reaching Set B
    let commit_decision = create_commit_decision(state_root_a);
    
    // This should trigger assertion failure:
    // assert_eq!(state_root_b, commit_decision.commit_info().executed_state_id())
    let result = buffer_item_b.try_advance_to_aggregated_with_ledger_info(commit_decision);
    
    // In actual code, this panics. In test, verify the mismatch would occur.
    assert!(result.is_err() || would_panic());
}
```

The key elements demonstrating the vulnerability:
1. Different cryptographic keys produce different randomness outputs
2. Race condition allows different validators to lock in different randomness values
3. Execution with different randomness produces different state roots
4. Commit decision mismatch triggers assertion failure

---

**Notes:**

The vulnerability is particularly severe because it combines multiple weakness layers:
- Cryptographic independence of fast/slow paths (different keys → different outputs)
- Lack of consistency verification (no check that paths agree)
- First-wins semantics (race condition)
- Decoupled execution (voting before execution)
- Fatal error handling (assertions instead of graceful recovery)

The attack does not require compromising cryptographic primitives or exceeding the 1/3 Byzantine threshold - it exploits the protocol's design assumption that both randomness paths will produce identical values, which is violated by the use of independent cryptographic keys.

### Citations

**File:** consensus/src/epoch_manager.rs (L1104-1159)
```rust
            let augmented_key_pair = WVUF::augment_key_pair(&vuf_pp, sk.main, pk.main, &mut rng);
            let fast_augmented_key_pair = if fast_randomness_is_enabled {
                if let (Some(sk), Some(pk)) = (sk.fast, pk.fast) {
                    Some(WVUF::augment_key_pair(&vuf_pp, sk, pk, &mut rng))
                } else {
                    None
                }
            } else {
                None
            };
            self.rand_storage
                .save_key_pair_bytes(
                    new_epoch,
                    bcs::to_bytes(&(augmented_key_pair.clone(), fast_augmented_key_pair.clone()))
                        .map_err(NoRandomnessReason::KeyPairSerializationError)?,
                )
                .map_err(NoRandomnessReason::KeyPairPersistError)?;
            (augmented_key_pair, fast_augmented_key_pair)
        };

        let (ask, apk) = augmented_key_pair;

        let keys = RandKeys::new(ask, apk, pk_shares, new_epoch_state.verifier.len());

        let rand_config = RandConfig::new(
            self.author,
            new_epoch,
            new_epoch_state.verifier.clone(),
            vuf_pp.clone(),
            keys,
            dkg_pub_params.pvss_config.wconfig.clone(),
        );

        let fast_rand_config = if let (Some((ask, apk)), Some(trx), Some(wconfig)) = (
            fast_augmented_key_pair,
            transcript.fast.as_ref(),
            dkg_pub_params.pvss_config.fast_wconfig.as_ref(),
        ) {
            let pk_shares = (0..new_epoch_state.verifier.len())
                .map(|id| trx.get_public_key_share(wconfig, &Player { id }))
                .collect::<Vec<_>>();

            let fast_keys = RandKeys::new(ask, apk, pk_shares, new_epoch_state.verifier.len());
            let fast_wconfig = wconfig.clone();

            Some(RandConfig::new(
                self.author,
                new_epoch,
                new_epoch_state.verifier.clone(),
                vuf_pp,
                fast_keys,
                fast_wconfig,
            ))
        } else {
            None
        };
```

**File:** consensus/src/rand/rand_gen/rand_store.rs (L267-277)
```rust
        rand_item.try_aggregate(&self.rand_config, self.decision_tx.clone());
        // fast path
        if let (Some(fast_rand_map), Some(fast_rand_config)) =
            (self.fast_rand_map.as_mut(), self.fast_rand_config.as_ref())
        {
            let fast_rand_item = fast_rand_map
                .entry(rand_metadata.round())
                .or_insert_with(|| RandItem::new(self.author, PathType::Fast));
            fast_rand_item.add_metadata(fast_rand_config, rand_metadata.clone());
            fast_rand_item.try_aggregate(fast_rand_config, self.decision_tx.clone());
        }
```

**File:** consensus/src/rand/rand_gen/block_queue.rs (L69-82)
```rust
    pub fn set_randomness(&mut self, round: Round, rand: Randomness) -> bool {
        let offset = self.offset(round);
        if !self.blocks()[offset].has_randomness() {
            observe_block(
                self.blocks()[offset].timestamp_usecs(),
                BlockStage::RAND_ADD_DECISION,
            );
            self.blocks_mut()[offset].set_randomness(rand);
            self.num_undecided_blocks -= 1;
            true
        } else {
            false
        }
    }
```

**File:** consensus/consensus-types/src/vote_proposal.rs (L59-101)
```rust
    /// This function returns the vote data with a dummy executed_state_id and version
    fn vote_data_ordering_only(&self) -> VoteData {
        VoteData::new(
            self.block().gen_block_info(
                *ACCUMULATOR_PLACEHOLDER_HASH,
                0,
                self.next_epoch_state().cloned(),
            ),
            self.block().quorum_cert().certified_block().clone(),
        )
    }

    /// This function returns the vote data with a extension proof.
    /// Attention: this function itself does not verify the proof.
    fn vote_data_with_extension_proof(
        &self,
        new_tree: &InMemoryTransactionAccumulator,
    ) -> VoteData {
        VoteData::new(
            self.block().gen_block_info(
                new_tree.root_hash(),
                new_tree.version(),
                self.next_epoch_state().cloned(),
            ),
            self.block().quorum_cert().certified_block().clone(),
        )
    }

    /// Generate vote data depends on the config.
    pub fn gen_vote_data(&self) -> anyhow::Result<VoteData> {
        if self.decoupled_execution {
            Ok(self.vote_data_ordering_only())
        } else {
            let proposed_block = self.block();
            let new_tree = self.accumulator_extension_proof().verify(
                proposed_block
                    .quorum_cert()
                    .certified_block()
                    .executed_state_id(),
            )?;
            Ok(self.vote_data_with_extension_proof(&new_tree))
        }
    }
```

**File:** consensus/src/pipeline/buffer_item.rs (L232-270)
```rust
    pub fn try_advance_to_aggregated_with_ledger_info(
        self,
        commit_proof: LedgerInfoWithSignatures,
    ) -> Self {
        match self {
            Self::Signed(signed_item) => {
                let SignedItem {
                    executed_blocks,
                    partial_commit_proof: local_commit_proof,
                    ..
                } = *signed_item;
                assert_eq!(
                    local_commit_proof.data().commit_info(),
                    commit_proof.commit_info()
                );
                debug!(
                    "{} advance to aggregated with commit decision",
                    commit_proof.commit_info()
                );
                Self::Aggregated(Box::new(AggregatedItem {
                    executed_blocks,
                    commit_proof,
                }))
            },
            Self::Executed(executed_item) => {
                let ExecutedItem {
                    executed_blocks,
                    commit_info,
                    ..
                } = *executed_item;
                assert_eq!(commit_info, *commit_proof.commit_info());
                debug!(
                    "{} advance to aggregated with commit decision",
                    commit_proof.commit_info()
                );
                Self::Aggregated(Box::new(AggregatedItem {
                    executed_blocks,
                    commit_proof,
                }))
```

**File:** types/src/on_chain_config/consensus_config.rs (L1-50)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use crate::{block_info::Round, on_chain_config::OnChainConfig};
use anyhow::{format_err, Result};
use move_core_types::account_address::AccountAddress;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

/// Default Window Size for Execution Pool.
/// This describes the number of blocks in the Execution Pool Window
pub const DEFAULT_WINDOW_SIZE: Option<u64> = None;
pub const DEFAULT_ENABLED_WINDOW_SIZE: Option<u64> = Some(1);

#[derive(Clone, Debug, Deserialize, PartialEq, Eq, Serialize)]
pub enum ConsensusAlgorithmConfig {
    Jolteon {
        main: ConsensusConfigV1,
        quorum_store_enabled: bool,
    },
    DAG(DagConsensusConfigV1),
    JolteonV2 {
        main: ConsensusConfigV1,
        quorum_store_enabled: bool,
        order_vote_enabled: bool,
    },
}

impl ConsensusAlgorithmConfig {
    pub fn default_for_genesis() -> Self {
        Self::JolteonV2 {
            main: ConsensusConfigV1::default(),
            quorum_store_enabled: true,
            order_vote_enabled: true,
        }
    }

    pub fn default_with_quorum_store_disabled() -> Self {
        Self::JolteonV2 {
            main: ConsensusConfigV1::default(),
            quorum_store_enabled: false,
            order_vote_enabled: true,
        }
    }

    pub fn default_if_missing() -> Self {
        Self::JolteonV2 {
            main: ConsensusConfigV1::default(),
            quorum_store_enabled: true,
            order_vote_enabled: false,
```
