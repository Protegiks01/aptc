# Audit Report

## Title
Unbounded Memory Consumption During State Merkle Shard Pruner Initialization Leads to OOM Crashes

## Summary
The `StateMerkleShardPruner::new()` initialization function bypasses the configured batch size limit by calling `prune()` with `usize::MAX`, causing unbounded memory allocation when processing large backlogs of stale Jellyfish Merkle tree nodes. This leads to out-of-memory crashes and node unavailability during restart operations.

## Finding Description
During initialization, each `StateMerkleShardPruner` performs a "catch-up" operation to synchronize with the metadata pruner's progress. This breaks resource management best practices by attempting to load all pending stale nodes into memory at once.

**Execution Flow:**

1. **Node initialization**: When `AptosDB` is opened, `StateMerklePrunerManager::new()` creates pruner instances for state merkle and epoch snapshot pruning. [1](#0-0) 

2. **Pruner creation**: The manager initializes `StateMerklePruner::new()`, which creates shard pruners when sharding is enabled. [2](#0-1) 

3. **Unbounded catch-up**: Each `StateMerkleShardPruner::new()` calls `prune()` with `usize::MAX` as the `max_nodes_to_prune` parameter, attempting to process all pending work in a single operation. [3](#0-2) 

4. **Unbounded memory allocation**: The `get_stale_node_indices()` function iterates through stale nodes and collects them into a `Vec` until the limit is reached. With `limit = usize::MAX`, this attempts to load the entire backlog into memory. [4](#0-3) 

5. **Memory structure**: Each `StaleNodeIndex` contains a `stale_since_version` field and a `node_key` field. [5](#0-4)  The `NodeKey` contains a `version` and `nibble_path`. [6](#0-5)  The `NibblePath` includes heap-allocated bytes. [7](#0-6) 

**Scale Analysis**: According to configuration comments, "A 10k transaction block (touching 60k state values) yields 300k JMT nodes". [8](#0-7)  This means:
- 100k transactions ≈ 3M stale nodes ≈ 300MB RAM
- 1M transactions ≈ 30M stale nodes ≈ 3GB RAM
- 10M transactions ≈ 300M stale nodes ≈ 30GB RAM

The configured `batch_size` (default 1,000) exists specifically to prevent this issue during normal operation. [9](#0-8)  However, it is completely bypassed during initialization.

## Impact Explanation
This vulnerability qualifies as **HIGH Severity** under the Aptos Bug Bounty program criteria:

**Validator Node Slowdowns**: The initialization phase consumes excessive memory and CPU resources, significantly degrading node performance and potentially preventing the node from participating in consensus.

**API Crashes**: Nodes experience OOM crashes, becoming completely unavailable and unable to serve API requests or participate in block production.

**Broader Network Impact**:
- Validators experiencing this issue cannot rejoin the network after maintenance restarts
- API nodes become unavailable, disrupting user access to blockchain data
- Rolling upgrades across multiple validators could trigger simultaneous OOM crashes
- Critical node operations are completely blocked until manual intervention (restart with reduced backlog or increased memory)

The impact directly affects network availability and validator participation, meeting HIGH severity criteria.

## Likelihood Explanation
This vulnerability has **HIGH likelihood** of occurring in production environments:

**Triggering Conditions** (any of these):
1. Node downtime/crashes followed by automatic restart
2. Deliberate node restarts for maintenance or upgrades
3. Pruning falling behind transaction rate due to high throughput
4. Resource constraints preventing pruner from keeping pace
5. Temporary pruner misconfiguration

**Real-world Scenarios**:
- High-traffic periods create backlogs that accumulate and persist through restarts
- Validator operators perform routine maintenance requiring restarts
- Emergency node restarts to address operational issues
- Network-wide upgrades requiring coordinated restarts

The vulnerability is **silent during accumulation** - nodes operate normally while building backlog, and the problem only manifests during initialization, making it difficult to detect proactively.

## Recommendation
Implement batch processing during initialization to respect the configured `batch_size` limit:

```rust
pub(in crate::pruner) fn new(
    shard_id: usize,
    db_shard: Arc<DB>,
    metadata_progress: Version,
) -> Result<Self> {
    let progress = get_or_initialize_subpruner_progress(
        &db_shard,
        &S::progress_metadata_key(Some(shard_id)),
        metadata_progress,
    )?;
    let myself = Self {
        shard_id,
        db_shard,
        _phantom: PhantomData,
    };

    info!(
        progress = progress,
        metadata_progress = metadata_progress,
        "Catching up {} shard {shard_id}.",
        S::name(),
    );
    
    // Use configured batch_size instead of usize::MAX
    const INIT_BATCH_SIZE: usize = 10_000; // Or read from config
    myself.prune(progress, metadata_progress, INIT_BATCH_SIZE)?;

    Ok(myself)
}
```

Alternatively, implement a chunked initialization that processes the backlog in multiple smaller batches rather than a single unbounded operation.

## Proof of Concept
A proof of concept would require:

1. Set up an Aptos node with state merkle pruning enabled
2. Allow the node to process transactions while accumulating stale nodes
3. Stop the pruner or throttle it to create a large backlog (e.g., 10M+ stale nodes)
4. Restart the node
5. Monitor memory consumption during `StateMerkleShardPruner::new()` initialization
6. Observe OOM crash when backlog is sufficiently large relative to available memory

The vulnerability is inherent in the code structure and does not require specific transaction patterns beyond normal blockchain operation.

## Notes
- This is a reliability issue that becomes a security vulnerability due to its impact on network availability and validator participation
- The configured `batch_size` safety mechanism exists precisely to prevent this type of unbounded memory consumption
- The bypass occurs only during initialization, making it a "restart vulnerability"
- The issue affects both regular state merkle pruning and epoch snapshot pruning, as both use the same code path

### Citations

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L60-67)
```rust
        let state_merkle_pruner = StateMerklePrunerManager::new(
            Arc::clone(&state_merkle_db),
            pruner_config.state_merkle_pruner_config,
        );
        let epoch_snapshot_pruner = StateMerklePrunerManager::new(
            Arc::clone(&state_merkle_db),
            pruner_config.epoch_snapshot_pruner_config.into(),
        );
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L136-149)
```rust
        let shard_pruners = if state_merkle_db.sharding_enabled() {
            let num_shards = state_merkle_db.num_shards();
            let mut shard_pruners = Vec::with_capacity(num_shards);
            for shard_id in 0..num_shards {
                shard_pruners.push(StateMerkleShardPruner::new(
                    shard_id,
                    state_merkle_db.db_shard_arc(shard_id),
                    metadata_progress,
                )?);
            }
            shard_pruners
        } else {
            Vec::new()
        };
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L191-217)
```rust
    pub(in crate::pruner::state_merkle_pruner) fn get_stale_node_indices(
        state_merkle_db_shard: &DB,
        start_version: Version,
        target_version: Version,
        limit: usize,
    ) -> Result<(Vec<StaleNodeIndex>, Option<Version>)> {
        let mut indices = Vec::new();
        let mut iter = state_merkle_db_shard.iter::<S>()?;
        iter.seek(&StaleNodeIndex {
            stale_since_version: start_version,
            node_key: NodeKey::new_empty_path(0),
        })?;

        let mut next_version = None;
        while indices.len() < limit {
            if let Some((index, _)) = iter.next().transpose()? {
                next_version = Some(index.stale_since_version);
                if index.stale_since_version <= target_version {
                    indices.push(index);
                    continue;
                }
            }
            break;
        }

        Ok((indices, next_version))
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L53-53)
```rust
        myself.prune(progress, metadata_progress, usize::MAX)?;
```

**File:** storage/jellyfish-merkle/src/lib.rs (L195-201)
```rust
pub struct StaleNodeIndex {
    /// The version since when the node is overwritten and becomes stale.
    pub stale_since_version: Version,
    /// The [`NodeKey`](node_type/struct.NodeKey.html) identifying the node associated with this
    /// record.
    pub node_key: NodeKey,
}
```

**File:** storage/jellyfish-merkle/src/node_type/mod.rs (L47-54)
```rust
#[derive(Clone, Debug, Hash, Eq, PartialEq, Ord, PartialOrd)]
#[cfg_attr(any(test, feature = "fuzzing"), derive(Arbitrary))]
pub struct NodeKey {
    // The version at which the node is created.
    version: Version,
    // The nibble path this node represents in the tree.
    nibble_path: NibblePath,
}
```

**File:** types/src/nibble/nibble_path/mod.rs (L21-32)
```rust
#[derive(Clone, Hash, Eq, PartialEq, Ord, PartialOrd, Serialize, Deserialize)]
pub struct NibblePath {
    /// Indicates the total number of nibbles in bytes. Either `bytes.len() * 2 - 1` or
    /// `bytes.len() * 2`.
    // Guarantees intended ordering based on the top-to-bottom declaration order of the struct's
    // members.
    num_nibbles: usize,
    /// The underlying bytes that stores the path, 2 nibbles per byte. If the number of nibbles is
    /// odd, the second half of the last byte must be 0.
    bytes: Vec<u8>,
    // invariant num_nibbles <= ROOT_NIBBLE_HEIGHT
}
```

**File:** config/src/config/storage_config.rs (L398-412)
```rust
impl Default for StateMerklePrunerConfig {
    fn default() -> Self {
        StateMerklePrunerConfig {
            enable: true,
            // This allows a block / chunk being executed to have access to a non-latest state tree.
            // It needs to be greater than the number of versions the state committing thread is
            // able to commit during the execution of the block / chunk. If the bad case indeed
            // happens due to this being too small, a node restart should recover it.
            // Still, defaulting to 1M to be super safe.
            prune_window: 1_000_000,
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
        }
    }
```
