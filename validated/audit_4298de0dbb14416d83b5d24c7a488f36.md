# Audit Report

## Title
State Pruner Catch-Up Deletes Historical Data When Prune Window Increases, Breaking State Sync Serving

## Summary
When `prune_window` configuration is increased, pruner initialization uses stale metadata progress from the database, causing permanent deletion of historical data that should be retained under the new configuration. This creates gaps in available data where the storage service may advertise availability but cannot serve requests, breaking state sync protocol functionality.

## Finding Description

The vulnerability exists in the shard pruner initialization logic used by both `StateKvPruner` and `StateMerklePruner` (including `epoch_snapshot_pruner`). During node restart after a `prune_window` increase, the catch-up mechanism deletes data that should now be retained.

**Vulnerable Code Path:**

1. During `StateKvPruner::new()` initialization, metadata progress is retrieved from persistent storage without validation against the current `prune_window`: [1](#0-0) 

2. This metadata progress, which reflects the OLD prune window, is passed to each shard pruner: [2](#0-1) 

3. Each `StateKvShardPruner::new()` performs catch-up pruning from its shard progress to the stale metadata progress: [3](#0-2) 

4. The metadata progress comes directly from database without recalculation: [4](#0-3) 

**The same vulnerability exists in `StateMerklePruner`:** [5](#0-4) [6](#0-5) 

**Attack Scenario:**

1. Node runs with `prune_window = 100,000`, latest version = 1,000,000
   - Metadata pruner prunes to version 900,000
   - Shard pruner lags at version 850,000

2. Operator increases `prune_window = 200,000` to retain more history

3. Node restarts: catch-up deletes versions 850,001-900,000

4. New `min_readable_version = 800,000` but data gap exists at 850,001-900,000

5. State sync queries fail with "State Value is missing": [7](#0-6) 

**Impact on State Sync:**

When serving state value chunks, the code fetches values that may have been improperly pruned: [8](#0-7) 

The storage service calculates availability based on configured prune window but actual data may be missing: [9](#0-8) 

State value queries check pruner boundaries but the bug creates inconsistencies: [10](#0-9) [11](#0-10) 

## Impact Explanation

This issue represents a **MEDIUM severity** protocol violation under Aptos bug bounty criteria:

**Actual Impact:**
- **State Sync Protocol Failure:** Nodes cannot serve historical state values in the gap range, breaking synchronization
- **Permanent Data Loss:** Deleted values require full re-sync from genesis or complete-history nodes  
- **Validator Synchronization Issues:** Lagging validators cannot sync from affected nodes
- **Multi-Node Amplification:** Coordinated config updates compound the issue

**Severity Justification (MEDIUM, not HIGH):**
- Fits "Limited protocol violations, state inconsistencies requiring manual intervention"
- Does NOT break consensus safety or block production
- Does NOT enable fund theft or minting
- Does NOT cause total network halt
- Requires operator config change (not directly exploitable by malicious actors)
- Workarounds exist (re-sync from unaffected nodes)

The issue affects data availability and state sync serving but does not reach HIGH severity thresholds of validator node crashes or significant consensus impact.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

This will occur whenever:
1. Operators increase `prune_window` (legitimate operational activity)
2. Shard pruner progress lags behind metadata pruner (common during normal operation)
3. Node restarts

Common triggers:
- Increasing history retention for state sync capabilities
- Network-wide configuration updates
- Recovery from pruner performance issues

The configuration change is legitimate, making this a realistic operational scenario. The lack of validation makes the issue deterministic when conditions are met.

## Recommendation

**Fix: Validate metadata progress against current prune window during initialization**

In `StateKvPruner::new()` and `StateMerklePruner::new()`, recalculate the appropriate metadata progress based on current `prune_window` and latest version, rather than blindly using stored progress:

```rust
let metadata_progress = metadata_pruner.progress()?;
let latest_version = get_latest_version()?;
let expected_min_readable = latest_version.saturating_sub(prune_window);

// Use the maximum to avoid over-pruning
let safe_metadata_progress = std::cmp::max(metadata_progress, expected_min_readable);

// Pass safe_metadata_progress to shard pruners
```

This ensures that when `prune_window` increases, catch-up pruning doesn't delete data that should now be retained.

## Proof of Concept

The vulnerability can be demonstrated through the following sequence:

1. Initialize node with `ledger_pruner_config.prune_window = 100_000`
2. Process transactions to version 1_000_000, allowing metadata pruner to reach 900_000
3. Let shard pruner lag to 850_000
4. Stop node
5. Update config: `ledger_pruner_config.prune_window = 200_000`  
6. Restart node
7. Observe catch-up pruning deletes versions 850_001-900_000
8. Query state values at version 875_000 â†’ fails with "State Value is missing"
9. Verify `min_readable_version = 800_000` but gap exists

The PoC demonstrates permanent data loss in versions that should be retained per the new configuration, breaking state sync protocol guarantees.

## Notes

**Important Clarifications:**

1. **Multiple Pruners Affected:** Both `StateKvPruner` (state values) and `StateMerklePruner` (merkle tree nodes, including `epoch_snapshot_pruner`) have this vulnerability.

2. **Configuration Complexity:** 
   - `StateKvPruner` uses `ledger_pruner_config` (default: 90M versions)
   - `epoch_snapshot_pruner` uses `epoch_snapshot_pruner_config` (default: 80M versions)  
   - Storage service advertises availability based on `epoch_snapshot_prune_window`
   - Mismatched configurations can compound the issue

3. **Severity Assessment:** While the report claims HIGH severity, this more accurately fits MEDIUM criteria ("Limited protocol violations, state inconsistencies requiring manual intervention") as it doesn't break consensus safety or cause total network unavailability.

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L117-117)
```rust
        let metadata_progress = metadata_pruner.progress()?;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L128-132)
```rust
                shard_pruners.push(StateKvShardPruner::new(
                    shard_id,
                    state_kv_db.db_shard_arc(shard_id),
                    metadata_progress,
                )?);
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L42-42)
```rust
        myself.prune(progress, metadata_progress)?;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs (L75-81)
```rust
    pub(in crate::pruner) fn progress(&self) -> Result<Version> {
        Ok(get_progress(
            self.state_kv_db.metadata_db(),
            &DbMetadataKey::StateKvPrunerProgress,
        )?
        .unwrap_or(0))
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L128-143)
```rust
        let metadata_progress = metadata_pruner.progress()?;

        info!(
            metadata_progress = metadata_progress,
            "Created {} metadata pruner, start catching up all shards.",
            S::name(),
        );

        let shard_pruners = if state_merkle_db.sharding_enabled() {
            let num_shards = state_merkle_db.num_shards();
            let mut shard_pruners = Vec::with_capacity(num_shards);
            for shard_id in 0..num_shards {
                shard_pruners.push(StateMerkleShardPruner::new(
                    shard_id,
                    state_merkle_db.db_shard_arc(shard_id),
                    metadata_progress,
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L53-53)
```rust
        myself.prune(progress, metadata_progress, usize::MAX)?;
```

**File:** storage/aptosdb/src/state_store/mod.rs (L328-333)
```rust
                    AptosDbError::NotFound(format!(
                        "State Value is missing for key {:?} by version {}",
                        state_key, version
                    ))
                })
            })
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1109-1111)
```rust
            res.and_then(|(_, (key, version))| {
                Ok((key.clone(), store.expect_value_by_version(&key, version)?))
            })
```

**File:** state-sync/storage-service/server/src/storage.rs (L155-170)
```rust
        let pruning_window = self.storage.get_epoch_snapshot_prune_window()?;

        if latest_version > pruning_window as Version {
            // lowest_state_version = latest_version - pruning_window + 1;
            let mut lowest_state_version = latest_version
                .checked_sub(pruning_window as Version)
                .ok_or_else(|| {
                    Error::UnexpectedErrorEncountered("Lowest state version has overflown!".into())
                })?;
            lowest_state_version = lowest_state_version.checked_add(1).ok_or_else(|| {
                Error::UnexpectedErrorEncountered("Lowest state version has overflown!".into())
            })?;

            // Create the state range
            let state_range = CompleteDataRange::new(lowest_state_version, latest_version)
                .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L305-315)
```rust
    pub(super) fn error_if_state_kv_pruned(&self, data_type: &str, version: Version) -> Result<()> {
        let min_readable_version = self.state_store.state_kv_pruner.get_min_readable_version();
        ensure!(
            version >= min_readable_version,
            "{} at version {} is pruned, min available version is {}.",
            data_type,
            version,
            min_readable_version
        );
        Ok(())
    }
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L637-637)
```rust
            self.error_if_state_kv_pruned("StateValue", version)?;
```
