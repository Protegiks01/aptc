# Audit Report

## Title
SecretShareManager Panic During State Sync Due to Missing Pipeline Reset

## Summary
During state sync operations, the `ExecutionProxyClient::reset()` method fails to send reset signals to the `SecretShareManager`, while simultaneously aborting pipeline futures on blocks that are shared between the `BufferManager` and `SecretShareManager`. This creates a race condition where `SecretShareManager` attempts to access pipeline futures that have been removed, causing a panic that terminates this critical consensus component.

## Finding Description

This vulnerability stems from incomplete reset coordination between pipeline managers during state sync operations. The execution flow is as follows:

**1. Shared Block References**: When blocks are ordered in consensus, they are sent to both the `SecretShareManager` and `BufferManager` via the coordinator. The coordinator clones `OrderedBlocks` structures, which contain `Vec<Arc<PipelinedBlock>>`, meaning both managers share references to identical block instances. [1](#0-0) 

**2. Incomplete Reset Logic**: During state sync operations (`sync_to_target` and `sync_for_duration`), the `ExecutionProxyClient::reset()` method is invoked. However, this method only extracts and sends reset signals to `rand_manager` and `buffer_manager`, completely omitting the `secret_share_manager`: [2](#0-1) 

Despite the `BufferManagerHandle` structure containing the `reset_tx_to_secret_share_manager` channel: [3](#0-2) 

**3. Pipeline Abortion**: When `BufferManager` receives the reset signal, it calls `reset()` which invokes `abort_pipeline()` on all blocks in its queue: [4](#0-3) 

The `abort_pipeline()` method removes the `PipelineFutures` by calling `.take()` on the mutex: [5](#0-4) 

**4. Panic Condition**: Meanwhile, `SecretShareManager` continues processing blocks from its incoming queue because it never received a reset signal. When it processes a block and attempts to access pipeline futures, it encounters `None` and panics: [6](#0-5) 

**5. Race Condition Window**: The `SecretShareManager` event loop only drains its incoming blocks queue when it receives a reset signal: [7](#0-6) 

However, this reset signal is never sent during `sync_to_target()` operations.

**Confirmation via end_epoch()**: The developers clearly understood that all three managers need reset coordination, as evidenced by the `end_epoch()` method which properly extracts and resets all three managers including `SecretShareManager`: [8](#0-7) 

## Impact Explanation

**Severity: High** (per Aptos Bug Bounty "Validator node slowdowns" criteria)

This vulnerability causes:

1. **Task Termination**: The `SecretShareManager` task panics and terminates, completely disabling the secret sharing subsystem.

2. **Consensus Disruption**: When secret sharing is enabled for randomness generation (used in leader election and consensus operations), the failure of this component prevents the validator from properly participating in consensus protocols.

3. **Triggering via Normal Operations**: State sync is triggered during routine validator operations:
   - Fast-forward sync when catching up with the network [9](#0-8) 
   - Epoch transitions [10](#0-9) 

4. **No Attacker Required**: This is a protocol-level bug requiring no malicious actor—normal state sync operations trigger the vulnerability.

The impact qualifies as High severity because it causes validator component failures that directly disrupt consensus participation, matching the "Validator node slowdowns" and "Significant protocol violations" criteria in the Aptos bug bounty program.

## Likelihood Explanation

**Likelihood: High**

This vulnerability is highly likely to occur because:

1. **Routine Operations**: State sync is performed regularly by validators when syncing with the network, recovering from downtime, or catching up after falling behind.

2. **No Special Conditions**: No Byzantine behavior, malicious actors, or special network conditions are required—the vulnerability exists in the normal protocol flow.

3. **Realistic Timing Window**: The race condition has a genuine timing window. During state sync, blocks continuously flow through both managers asynchronously. When `BufferManager` starts aborting pipelines while `SecretShareManager` processes blocks, the panic is inevitable.

4. **Active When Configured**: When secret sharing is configured for randomness (a production feature), the vulnerable code path is active.

The likelihood is High (not Certain) because it depends on timing—specifically whether `SecretShareManager` is processing blocks when the reset occurs—but given the asynchronous nature and continuous block flow, this race will manifest regularly in production environments.

## Recommendation

Modify the `ExecutionProxyClient::reset()` method to include the `secret_share_manager` reset channel, making it consistent with the `end_epoch()` implementation:

```rust
async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
    let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager, reset_tx_to_secret_share_manager) = {
        let handle = self.handle.read();
        (
            handle.reset_tx_to_rand_manager.clone(),
            handle.reset_tx_to_buffer_manager.clone(),
            handle.reset_tx_to_secret_share_manager.clone(),  // ADD THIS
        )
    };

    if let Some(mut reset_tx) = reset_tx_to_rand_manager {
        // ... existing rand_manager reset code
    }

    if let Some(mut reset_tx) = reset_tx_to_secret_share_manager {  // ADD THIS BLOCK
        let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::ResetDropped)?;
        ack_rx.await.map_err(|_| Error::ResetDropped)?;
    }

    if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
        // ... existing buffer_manager reset code
    }

    Ok(())
}
```

This ensures that `SecretShareManager` receives reset signals and drains its queue before `BufferManager` aborts pipeline futures, eliminating the race condition.

## Proof of Concept

While a full PoC would require setting up a test validator environment with secret sharing enabled, the vulnerability is directly observable from the code structure:

1. Start a validator with secret sharing configured
2. Trigger state sync via `sync_to_target()` (e.g., during catch-up or epoch transition)
3. Observe that `ExecutionProxyClient::reset()` sends reset only to rand_manager and buffer_manager
4. `BufferManager` aborts pipeline futures on shared blocks
5. `SecretShareManager` continues processing and attempts to access aborted pipeline futures
6. Task panics with "pipeline must exist" error

The vulnerability is deterministic given the timing window where `SecretShareManager` processes blocks during the reset operation.

## Notes

This vulnerability represents a critical oversight in the reset coordination logic. The fact that `end_epoch()` correctly resets all three managers while `reset()` omits `SecretShareManager` suggests this was an unintentional omission rather than a design decision. The fix is straightforward and should be prioritized for validators running with secret sharing enabled.

### Citations

**File:** consensus/src/pipeline/execution_client.rs (L130-130)
```rust
    pub reset_tx_to_secret_share_manager: Option<UnboundedSender<ResetRequest>>,
```

**File:** consensus/src/pipeline/execution_client.rs (L335-336)
```rust
                        let _ = rand_manager_input_tx.send(ordered_blocks.clone()).await;
                        let _ = secret_share_manager_input_tx.send(ordered_blocks.clone()).await;
```

**File:** consensus/src/pipeline/execution_client.rs (L674-708)
```rust
    async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
        let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager) = {
            let handle = self.handle.read();
            (
                handle.reset_tx_to_rand_manager.clone(),
                handle.reset_tx_to_buffer_manager.clone(),
            )
        };

        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        }

        if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
            // reset execution phase and commit phase
            let (tx, rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::ResetDropped)?;
            rx.await.map_err(|_| Error::ResetDropped)?;
        }

        Ok(())
```

**File:** consensus/src/pipeline/execution_client.rs (L711-745)
```rust
    async fn end_epoch(&self) {
        let (
            reset_tx_to_rand_manager,
            reset_tx_to_buffer_manager,
            reset_tx_to_secret_share_manager,
        ) = {
            let mut handle = self.handle.write();
            handle.reset()
        };

        if let Some(mut tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop rand manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop rand manager");
        }

        if let Some(mut tx) = reset_tx_to_secret_share_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop secret share manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop secret share manager");
        }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L552-558)
```rust
        while let Some(item) = self.buffer.pop_front() {
            for b in item.get_blocks() {
                if let Some(futs) = b.abort_pipeline() {
                    futs.wait_until_finishes().await;
                }
            }
        }
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L528-547)
```rust
    pub fn abort_pipeline(&self) -> Option<PipelineFutures> {
        if let Some(abort_handles) = self.pipeline_abort_handle.lock().take() {
            let mut aborted = false;
            for handle in abort_handles {
                if !handle.is_finished() {
                    handle.abort();
                    aborted = true;
                }
            }
            if aborted {
                info!(
                    "[Pipeline] Aborting pipeline for block {} {} {}",
                    self.id(),
                    self.epoch(),
                    self.round()
                );
            }
        }
        self.pipeline_futs.lock().take()
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L132-138)
```rust
    async fn process_incoming_block(&self, block: &PipelinedBlock) -> DropGuard {
        let futures = block.pipeline_futs().expect("pipeline must exist");
        let self_secret_share = futures
            .secret_sharing_derive_self_fut
            .await
            .expect("Decryption share computation is expected to succeed")
            .expect("Must not be None");
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L353-360)
```rust
        while !self.stop {
            tokio::select! {
                Some(blocks) = incoming_blocks.next() => {
                    self.process_incoming_blocks(blocks).await;
                }
                Some(reset) = reset_rx.next() => {
                    while matches!(incoming_blocks.try_next(), Ok(Some(_))) {}
                    self.process_reset(reset);
```

**File:** consensus/src/block_storage/sync_manager.rs (L512-514)
```rust
        execution_client
            .sync_to_target(highest_commit_cert.ledger_info().clone())
            .await?;
```

**File:** consensus/src/epoch_manager.rs (L558-560)
```rust
        self.execution_client
            .sync_to_target(ledger_info.clone())
            .await
```
