# Audit Report

## Title
Non-Deterministic Struct Layout Cache Causes Consensus Divergence in Parallel Block Execution

## Summary
The global struct layout cache in BlockSTM parallel execution lacks proper invalidation when transactions are aborted. Stale layouts cached by aborted transaction incarnations persist and can be used by subsequent transactions with different module versions, causing non-deterministic execution across validators and breaking consensus safety.

## Finding Description

The vulnerability exists in the interaction between the global layout cache and transaction abort handling during parallel block execution.

**The Core Issue:**

The global module cache stores computed struct layouts in a concurrent `DashMap` shared across all parallel execution threads [1](#0-0) . Layout insertion uses a "first-writer-wins" policy via `Vacant` entry check [2](#0-1) .

Critically, layouts are cached with module lists but **without version information**. The `LayoutCacheEntry` structure stores only the layout and a `DefiningModules` set containing `ModuleId` (address+name) [3](#0-2) [4](#0-3) , with no version tracking.

**The Race Condition:**

1. **Transaction A (index 5)** executes speculatively, reads module M@V0, computes layout L0
2. **Transaction B (index 4)** publishes M@V1 and calls `flush_layout_cache()` [5](#0-4) , which clears all cached layouts [6](#0-5) 
3. **Critical Race**: Transaction A caches L0 **after** the flush due to parallel execution timing
4. **Transaction A validation fails** (module changed) and is aborted
5. **Abort handling only marks write-sets as ESTIMATE** [7](#0-6)  - **no layout cache clearing**
6. **Stale L0 remains cached** after abort
7. **Transaction A re-executes**, computes L1 from M@V1, but cache insertion is skipped (L0 already present)
8. **Transaction C (index 6)** gets cache hit (L0), calls `load_layout_from_cache()` [8](#0-7) 
9. **Critical flaw**: `load_layout_from_cache()` re-reads modules only for gas charging [9](#0-8) , which calls `charge_module()` that only gets module size [10](#0-9)  - **it does not verify the layout matches the current module version**
10. Transaction C uses **stale L0 with new M@V1**, causing incorrect deserialization

**Non-Determinism:**

Different validators experience different thread scheduling:
- **Validator 1**: Transaction A caches L0 after flush → L0 survives → uses stale layout → State Root X
- **Validator 2**: Transaction A caches L0 before flush (cleared) → computes fresh L1 → State Root Y

**Result**: X ≠ Y → **Consensus divergence**

## Impact Explanation

**Severity: CRITICAL**

This vulnerability directly breaks the "Deterministic Execution" consensus invariant: all validators must produce identical state roots for identical blocks.

**Direct Impacts:**

1. **Consensus Safety Violation**: Different validators compute different state roots for the same block, causing blockchain fork and consensus failure
2. **Non-Recoverable Network Partition**: Validators cannot reconcile without manual intervention or hardfork
3. **Total Loss of Network Availability**: Consensus cannot progress when validators disagree on state

This meets the Aptos Bug Bounty **Critical Severity** criteria:
- Consensus/Safety violations requiring < 1/3 Byzantine validators ✓
- Non-recoverable network partition (hardfork required) ✓
- Total loss of liveness/network availability ✓

The vulnerability affects all validators running BlockSTM parallel execution (default mode).

## Likelihood Explanation

**Likelihood: HIGH**

**Required Conditions:**
1. Parallel block execution (default in Aptos) ✓
2. Module publishing + struct usage in same block (common in DeFi/upgrades) ✓
3. Race window: cache insertion between flush and abort (natural in parallel execution) ✓

**Attacker Requirements:**
- No privileged access required
- Any user can publish modules and submit transactions
- Standard transaction fees only
- Probabilistic but repeatable with multiple attempts

**Realistic Scenarios:**
- Smart contract upgrades with immediate struct usage
- Protocol upgrades during high transaction volume
- Standard DeFi operations with fresh contract deployments

The race window naturally occurs in BlockSTM's parallel execution [11](#0-10)  when threads execute concurrently.

## Recommendation

**Immediate Fix:**

Clear layout cache entries when transactions are aborted. Modify `update_transaction_on_abort()` to add:

```rust
// In executor_utilities.rs, add after line 345:
// Clear any cached layouts that may have been computed during this incarnation
// to prevent stale layouts from being used after module republishing
global_module_cache.flush_layout_cache();
```

**Alternative Fix:**

Include version information in `LayoutCacheEntry` and validate layout compatibility with current module versions in `load_layout_from_cache()`.

**Long-term Solution:**

Redesign layout cache to be transaction-scoped or properly versioned with module cache consistency guarantees.

## Proof of Concept

The vulnerability is demonstrated through code path analysis:

1. Layout caching occurs synchronously during transaction execution [12](#0-11) 
2. Module publishing triggers cache flush [13](#0-12) 
3. Abort handling does not clear layouts [14](#0-13) 
4. Vacant check prevents overwrites [2](#0-1) 
5. Cached layouts used without version validation [8](#0-7) 

A complete PoC would require:
- Deploy module M with struct S(u64)
- In parallel block: Transaction A uses S, Transaction B publishes M with S(u128), Transaction C uses S
- Observe non-deterministic results across validators with different thread scheduling

## Notes

This is a fundamental race condition in the interaction between BlockSTM's parallel execution model and the global layout cache design. The cache assumes layouts remain valid across transaction incarnations, but module republishing invalidates this assumption. The fix requires either proper cache invalidation on abort or version-aware layout validation.

### Citations

**File:** aptos-move/block-executor/src/code_cache_global.rs (L96-96)
```rust
    struct_layouts: DashMap<StructKey, LayoutCacheEntry>,
```

**File:** aptos-move/block-executor/src/code_cache_global.rs (L163-168)
```rust
    pub fn flush_layout_cache(&self) {
        // TODO(layouts):
        //   Flushing is only needed because of enums. Once we refactor layouts to store a single
        //   variant instead, this can be removed.
        self.struct_layouts.clear();
    }
```

**File:** aptos-move/block-executor/src/code_cache_global.rs (L186-189)
```rust
        if let dashmap::Entry::Vacant(e) = self.struct_layouts.entry(*key) {
            e.insert(entry);
        }
        Ok(())
```

**File:** third_party/move/move-vm/runtime/src/storage/layout_cache.rs (L29-33)
```rust
#[derive(Debug, Default)]
pub struct DefiningModules {
    modules: HashSet<ModuleId>,
    seen_modules: Vec<ModuleId>,
}
```

**File:** third_party/move/move-vm/runtime/src/storage/layout_cache.rs (L60-64)
```rust
#[derive(Debug, Clone)]
pub struct LayoutCacheEntry {
    layout: LayoutWithDelayedFields,
    modules: TriompheArc<DefiningModules>,
}
```

**File:** aptos-move/block-executor/src/txn_last_input_output.rs (L572-576)
```rust
        if published {
            // Record validation requirements after the modules are published.
            global_module_cache.flush_layout_cache();
            scheduler.record_validation_requirements(txn_idx, module_ids_for_v2)?;
        }
```

**File:** aptos-move/block-executor/src/executor_utilities.rs (L308-346)
```rust
pub(crate) fn update_transaction_on_abort<T, E>(
    txn_idx: TxnIndex,
    last_input_output: &TxnLastInputOutput<T, E::Output>,
    versioned_cache: &MVHashMap<T::Key, T::Tag, T::Value, DelayedFieldID>,
) where
    T: Transaction,
    E: ExecutorTask<Txn = T>,
{
    counters::SPECULATIVE_ABORT_COUNT.inc();

    // Any logs from the aborted execution should be cleared and not reported.
    clear_speculative_txn_logs(txn_idx as usize);

    // Not valid and successfully aborted, mark the latest write/delta sets as estimates.
    if let Some(keys) = last_input_output.modified_resource_keys(txn_idx) {
        for (k, _) in keys {
            versioned_cache.data().mark_estimate(&k, txn_idx);
        }
    }

    // Group metadata lives in same versioned cache as data / resources.
    // We are not marking metadata change as estimate, but after a transaction execution
    // changes metadata, suffix validation is guaranteed to be triggered. Estimation affecting
    // execution behavior is left to size, which uses a heuristic approach.
    last_input_output
        .for_each_resource_group_key_and_tags(txn_idx, |key, tags| {
            versioned_cache
                .group_data()
                .mark_estimate(key, txn_idx, tags);
            Ok(())
        })
        .expect("Passed closure always returns Ok");

    if let Some(keys) = last_input_output.delayed_field_keys(txn_idx) {
        for k in keys {
            versioned_cache.delayed_fields().mark_estimate(&k, txn_idx);
        }
    }
}
```

**File:** third_party/move/move-vm/runtime/src/storage/loader/lazy.rs (L55-77)
```rust
    fn charge_module(
        &self,
        gas_meter: &mut impl DependencyGasMeter,
        traversal_context: &mut TraversalContext,
        module_id: &ModuleId,
    ) -> PartialVMResult<()> {
        if traversal_context.visit_if_not_special_module_id(module_id) {
            let addr = module_id.address();
            let name = module_id.name();

            let size = self
                .module_storage
                .unmetered_get_existing_module_size(addr, name)
                .map_err(|err| err.to_partial())?;
            gas_meter.charge_dependency(
                DependencyKind::Existing,
                addr,
                name,
                NumBytes::new(size as u64),
            )?;
        }
        Ok(())
    }
```

**File:** third_party/move/move-vm/runtime/src/storage/loader/lazy.rs (L203-221)
```rust
    fn load_layout_from_cache(
        &self,
        gas_meter: &mut impl DependencyGasMeter,
        traversal_context: &mut TraversalContext,
        key: &StructKey,
    ) -> Option<PartialVMResult<LayoutWithDelayedFields>> {
        let entry = self.module_storage.get_struct_layout(key)?;
        let (layout, modules) = entry.unpack();
        for module_id in modules.iter() {
            // Re-read all modules for this layout, so that transaction gets invalidated
            // on module publish. Also, we re-read them in exactly the same way as they
            // were traversed during layout construction, so gas charging should be exactly
            // the same as on the cache miss.
            if let Err(err) = self.charge_module(gas_meter, traversal_context, module_id) {
                return Some(Err(err));
            }
        }
        Some(Ok(layout))
    }
```

**File:** aptos-move/block-executor/src/lib.rs (L4-136)
```rust
/**
The high level parallel execution logic is implemented in 'executor.rs'. The
input of parallel executor is a block of transactions, containing a sequence
of n transactions tx_1, tx_2, ..., tx_n (this defines the preset serialization
order tx_1< tx_2< ...<tx_n).

Each transaction might be executed several times and we refer to the i-th
execution as incarnation i of a transaction. We say that an incarnation is
aborted when the system decides that a subsequent re-execution with an incremented
incarnation number is needed. A version is a pair of a transaction index and
an incarnation number. To support reads and writes by transactions that may
execute concurrently, parallel execution maintains an in-memory multi-version
data structure that separately stores for each memory location the latest value
written per transaction, along with the associated transaction version.
This data structure is implemented in: '../../mvhashmap/src/lib.rs'.
When transaction tx reads a memory location, it obtains from the multi-version
data-structure the value written to this location by the highest transaction
that appears before tx in the preset serialization order, along with the
associated version. For example, transaction tx_5 can read a value written
by transaction tx_3 even if transaction tx_6 has written to same location.
If no smaller transaction has written to a location, then the read
(e.g. all reads by tx_1) is resolved from storage based on the state before
the block execution.

For each incarnation, parallel execution maintains a write-set and a read-set
in 'txn_last_input_output.rs'. The read-set contains the memory locations that
are read during the incarnation, and the corresponding versions. The write-set
describes the updates made by the incarnation as (memory location, value) pairs.
The write-set of the incarnation is applied to shared memory (the multi-version
data-structure) at the end of execution. After an incarnation executes it needs
to pass validation. The validation re-reads the read-set and compares the
observed versions. Intuitively, a successful validation implies that writes
applied by the incarnation are still up-to-date, while a failed validation implies
that the incarnation has to be aborted. For instance, if the transaction was
speculatively executed and read value x=2, but later validation observes x=3,
the results of the transaction execution are no longer applicable and must
be discarded, while the transaction is marked for re-execution.

When an incarnation is aborted due to a validation failure, the entries in the
multi-version data-structure corresponding to its write-set are replaced with
a special ESTIMATE marker. This signifies that the next incarnation is estimated
to write to the same memory location, and is utilized for detecting potential
dependencies. In particular, an incarnation of transaction tx_j stops and waits
on a condition variable whenever it reads a value marked as an ESTIMATE that was
written by a lower transaction tx_k. When the execution of tx_k finishes, it
signals the condition variable and the execution of tx_j continues. This way,
tx_j does not read a value that is likely to cause an abort in the future due to a
validation failure, which would happen if the next incarnation of tx_k would
indeed write to the same location (the ESTIMATE markers that are not overwritten
are removed by the next incarnation).

The parallel executor relies on a collaborative scheduler in 'scheduler.rs',
which coordinates the validation and execution tasks among threads. Since the
preset serialization order dictates that the transactions must be committed in
order, a successful validation of an incarnation does not guarantee that it can
be committed. This is because an abort and re-execution of an earlier transaction
in the block might invalidate the incarnation read-set and necessitate
re-execution. Thus, when a transaction aborts, all higher transactions are
scheduled for re-validation. The same incarnation may be validated multiple times
and by different threads, potentially in parallel, but parallel execution ensures
that only the first abort per version is successful (the rest are ignored).
Since transactions must be committed in order, the scheduler prioritizes tasks
(validation and execution) associated with lower-indexed transactions.
Abstractly, the collaborative scheduler tracks an ordered set (priority queue w.o.
duplicates) V of pending validation tasks and an ordered set E of pending
execution tasks. Initially, V is empty and E contains execution tasks for
the initial incarnation of all transactions in the block. A transaction tx not
in E is either currently being executed or (its last incarnation) has completed.

Each thread repeats the following (loop in 'executor.rs'):
- Check done: if V and E are empty and no other thread is performing a task,
then return.
- Find next task: Perform the task with the smallest transaction index tx in V
and E:
  1. Execution task: Execute the next incarnation of tx. If a value marked as
     ESTIMATE is read, abort execution and add tx back to E. Otherwise:
     (a) If there is a write to a memory location to which the previous finished
         incarnation of tx has not written, create validation tasks for all
         transactions >= tx that are not currently in E or being executed and
         add them to V.
     (b) Otherwise, create a validation task only for tx and add it to V.
  2. Validation task: Validate the last incarnation of tx. If validation
     succeeds, continue. Otherwise, abort:
     (a) Mark every value (in the multi-versioned data-structure) written by
         the incarnation (that failed validation) as an ESTIMATE.
     (b) Create validation tasks for all transactions > tx that are not
         currently in E or being executed and add them to V.
     (c) Create an execution task for transaction tx with an incremented
         incarnation number, and add it to E.
When a transaction tx_k reads an ESTIMATE marker written by tx_j (with j < k),
we say that tx_k encounters a dependency (we treat tx_k as tx_j's dependency
because its read depends on a value that tx_j is estimated to write).
In the above description a transaction is added back to E immediately upon
encountering a dependency. However, we implement a slightly more involved
mechanism. Transaction tx_k is first recorded separately as a dependency of
tx_j, and only added back to E when the next incarnation of tx_j completes
(i.e. when the dependency is resolved).

In 'scheduler.rs', the ordered sets, V and E, are each implemented via a
single atomic counter coupled with a mechanism to track the status of
transactions, i.e. whether a given transaction is ready for validation or
execution, respectively. To pick a task, threads increment the smaller of these
counters until they find a task that is ready to be performed. To add a
(validation or execution) task for transaction tx, the thread updates the
status and reduces the corresponding counter to tx (if it had a larger value).
As an optimization in cases 1(b) and 2(c), instead of reducing the counter
value, the new task is returned back to the caller.

An incarnation of transaction might write to a memory location that was
previously read by an incarnation of a higher transaction according to the preset
serialization order. This is why in 1(a), when an incarnation finishes, new
validation tasks are created for higher transactions. Importantly, validation
tasks are scheduled optimistically, e.g. it is possible to concurrently validate
the latest incarnations of transactions tx_j, tx_{j+1}, tx_{j+2} and tx_{j+4}.
Suppose transactions tx_j, tx_{j+1} and tx_{j+4} are successfully validated,
while the validation of tx_{j+2} fails. When threads are available, parallel
execution capitalizes by performing these validations in parallel, allowing it
to detect the validation failure of tx_{j+2} faster in the above example
(at the expense of a validation of tx_{j+4} that needs to be redone).
Identifying validation failures and aborting incarnations as soon as possible
is crucial for the system performance, as any incarnation that reads values
written by a incarnation that aborts also needs to be aborted, forming a
cascade of aborts.

When an incarnation writes only to a subset of memory locations written by
the previously completed incarnation of the same transaction, i.e. case 1(b),
parallel execution schedules validation just for the incarnation itself.
This is sufficient because of 2(a), as the whole write-set of the previous
incarnation is marked as estimates during the abort. The abort then leads to
optimistically creating validation tasks for higher transactions in 2(b),
and threads that perform these tasks can already detect validation failures
due to the ESTIMATE markers on memory locations, instead of waiting for a
subsequent incarnation to finish.
```

**File:** third_party/move/move-vm/runtime/src/storage/ty_layout_converter.rs (L81-140)
```rust
    pub(crate) fn type_to_type_layout_with_delayed_fields(
        &self,
        gas_meter: &mut impl DependencyGasMeter,
        traversal_context: &mut TraversalContext,
        ty: &Type,
        check_option_type: bool,
    ) -> PartialVMResult<LayoutWithDelayedFields> {
        let ty_pool = self.runtime_environment().ty_pool();
        if self.vm_config().enable_layout_caches {
            let key = match ty {
                Type::Struct { idx, .. } => {
                    let ty_args_id = ty_pool.intern_ty_args(&[]);
                    Some(StructKey {
                        idx: *idx,
                        ty_args_id,
                    })
                },
                Type::StructInstantiation { idx, ty_args, .. } => {
                    let ty_args_id = ty_pool.intern_ty_args(ty_args);
                    Some(StructKey {
                        idx: *idx,
                        ty_args_id,
                    })
                },
                _ => None,
            };

            if let Some(key) = key {
                if let Some(result) = self.struct_definition_loader.load_layout_from_cache(
                    gas_meter,
                    traversal_context,
                    &key,
                ) {
                    return result;
                }

                // Otherwise a cache miss, compute the result and store it.
                let mut modules = DefiningModules::new();
                let layout = self.type_to_type_layout_with_delayed_fields_impl::<false>(
                    gas_meter,
                    traversal_context,
                    &mut modules,
                    ty,
                    check_option_type,
                )?;
                let cache_entry = LayoutCacheEntry::new(layout.clone(), modules);
                self.struct_definition_loader
                    .store_layout_to_cache(&key, cache_entry)?;
                return Ok(layout);
            }
        }

        self.type_to_type_layout_with_delayed_fields_impl::<false>(
            gas_meter,
            traversal_context,
            &mut DefiningModules::new(),
            ty,
            check_option_type,
        )
    }
```
