# Audit Report

## Title
Missing Duplicate Consensus Prevention in PerIssuerMode JWK Manager Leading to Resource Exhaustion

## Summary
The `IssuerLevelConsensusManager` lacks duplicate consensus prevention checks present in `KeyLevelConsensusManager`, allowing multiple concurrent consensus tasks to spawn for the same issuer with identical observations. This causes redundant network broadcasts, CPU exhaustion, and validator node slowdowns qualifying as High Severity under the Aptos bug bounty program.

## Finding Description

The JWK consensus system has two implementations: `IssuerLevelConsensusManager` (PerIssuerMode) and `KeyLevelConsensusManager` (PerKeyMode). A critical architectural flaw exists where PerIssuerMode lacks duplicate prevention logic.

**Root Cause:**

When `IssuerLevelConsensusManager::process_new_observation()` receives a JWK observation, it only checks if the observation differs from on-chain state: [1](#0-0) 

If different, it unconditionally spawns a new consensus task by calling `start_produce()` and updating the consensus state: [2](#0-1) 

**Missing Protection:**

In contrast, `KeyLevelConsensusManager` properly implements duplicate prevention in its `maybe_start_consensus()` method: [3](#0-2) 

This check prevents starting a new consensus if one is already in progress with the same proposal.

**Attack Mechanism:**

JWK observers poll OIDC endpoints every 10 seconds: [4](#0-3) 

When an OIDC provider returns static JWKs (common in production):
1. Observer sends identical observation every 10 seconds via the local observation channel
2. `process_new_observation()` is triggered each time
3. Since `observed != on_chain` remains true (on-chain hasn't updated yet), a new consensus task spawns
4. Each task calls `UpdateCertifier::start_produce()` which spawns a tokio task executing `ReliableBroadcast::broadcast()` [5](#0-4) 

The reliable broadcast sends RPC requests to all N-1 validators, aggregates signatures, and waits for quorum.

**Race Condition with Abort Mechanism:**

When a new consensus task starts, the old `abort_handle_wrapper` is dropped, triggering abort: [6](#0-5) 

However, abort only cancels at await points. If the previous task has passed the `rb.broadcast().await` line, it continues execution and pushes its result to the channel: [7](#0-6) 

**Network Amplification:**

Each concurrent broadcast sends messages to all N-1 validators. With M issuers returning static keys and K concurrent tasks per issuer:
- Total redundant messages: N × M × K × (N-1) per 10-second interval
- For 100 validators, 10 issuers, 3 concurrent tasks: ~270,000 redundant messages every 10 seconds

## Impact Explanation

This qualifies as **High Severity** under the Aptos bug bounty "Validator node slowdowns" category because:

1. **Network Amplification**: Each redundant consensus task broadcasts to all validators using `ReliableBroadcast`, multiplying network traffic by the number of concurrent tasks.

2. **CPU Exhaustion**: Multiple concurrent operations performing:
   - BLS signature signing of observations
   - Network serialization and RPC handling
   - Signature aggregation in `ObservationAggregationState`
   - Cryptographic verification at receiving validators

3. **Scalability Impact**: Effect scales with:
   - Validator count (N²) due to all-to-all broadcast
   - Number of issuers with static keys (linear)
   - Polling frequency (inversely proportional to interval)

4. **Production Likelihood**: OIDC providers (Google, Microsoft, etc.) maintain stable JWKs for extended periods to avoid breaking client integrations, making this scenario the norm rather than exception.

The channel uses KLAST queue style with capacity 1: [8](#0-7) 

While this prevents result queue overflow, it does not prevent the resource exhaustion from multiple concurrent broadcast operations already in flight.

## Likelihood Explanation

**High Likelihood** - This vulnerability triggers automatically in normal production operation:

1. **Natural Occurrence**: OIDC providers maintain stable JWKs for days/weeks between rotations. During stable periods, every 10-second poll returns identical keys, triggering redundant consensus tasks.

2. **No Attacker Required**: The issue manifests without malicious action. All validators naturally spawn redundant tasks when processing stable JWK observations.

3. **Amplification Vector**: An attacker controlling a registered OIDC provider (registered through governance) can deliberately serve static JWKs to maximize the impact, but this is not required for the vulnerability to manifest.

4. **Affects All Validators**: Every validator in the network independently spawns redundant tasks, multiplying the network-wide impact.

## Recommendation

Add duplicate consensus prevention logic to `IssuerLevelConsensusManager::process_new_observation()` similar to `KeyLevelConsensusManager::maybe_start_consensus()`:

```rust
pub fn process_new_observation(
    &mut self,
    issuer: Issuer,
    jwks: Vec<JWKMoveStruct>,
) -> Result<()> {
    let state = self.states_by_issuer.entry(issuer.clone()).or_default();
    state.observed = Some(jwks.clone());
    
    // Check if consensus already started with same observation
    let consensus_already_started = match &state.consensus_state {
        ConsensusState::InProgress { my_proposal, .. }
        | ConsensusState::Finished { my_proposal, .. } => {
            my_proposal.observed.jwks == jwks
        },
        _ => false,
    };
    
    if consensus_already_started {
        return Ok(());
    }
    
    if state.observed.as_ref() != state.on_chain.as_ref().map(ProviderJWKs::jwks) {
        // ... existing consensus initiation logic ...
    }
    Ok(())
}
```

This prevents spawning redundant consensus tasks when an identical observation has already initiated consensus.

## Proof of Concept

```rust
#[tokio::test]
async fn test_duplicate_observation_prevention() {
    // Setup: Create IssuerLevelConsensusManager with mock components
    let manager = setup_test_manager();
    
    // Initial on-chain state: issuer with JWK version 1
    manager.reset_with_on_chain_state(initial_state()).unwrap();
    
    // Observation 1: New JWKs (version 2)
    let new_jwks = vec![test_jwk()];
    manager.process_new_observation(test_issuer(), new_jwks.clone()).unwrap();
    
    // Verify consensus started
    let state = manager.states_by_issuer.get(&test_issuer()).unwrap();
    assert!(matches!(state.consensus_state, ConsensusState::InProgress { .. }));
    
    // Observation 2: IDENTICAL JWKs before first consensus completes
    // BUG: This spawns a second concurrent consensus task
    manager.process_new_observation(test_issuer(), new_jwks.clone()).unwrap();
    
    // The bug allows multiple concurrent broadcasts for same observation
    // Expected: Second call should be no-op
    // Actual: Second consensus task spawned, redundant network traffic generated
}
```

The test demonstrates that repeated calls with identical observations spawn multiple concurrent consensus tasks, each performing full reliable broadcast to all validators.

## Notes

The vulnerability is specific to `IssuerLevelConsensusManager`. The `KeyLevelConsensusManager` implementation (enabled via `JWK_CONSENSUS_PER_KEY_MODE` feature flag) correctly handles this scenario and does not exhibit the vulnerability.

### Citations

**File:** crates/aptos-jwk-consensus/src/jwk_manager/mod.rs (L72-72)
```rust
        let (qc_update_tx, qc_update_rx) = aptos_channel::new(QueueStyle::KLAST, 1, None);
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager/mod.rs (L122-122)
```rust
                        Duration::from_secs(10),
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager/mod.rs (L196-196)
```rust
        if state.observed.as_ref() != state.on_chain.as_ref().map(ProviderJWKs::jwks) {
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager/mod.rs (L206-223)
```rust
            let abort_handle = self
                .update_certifier
                .start_produce(
                    self.epoch_state.clone(),
                    observed.clone(),
                    self.qc_update_tx.clone(),
                )
                .context(
                    "process_new_observation failed with update_certifier.start_produce failure",
                )?;
            state.consensus_state = ConsensusState::InProgress {
                my_proposal: ObservedUpdate {
                    author: self.my_addr,
                    observed: observed.clone(),
                    signature,
                },
                abort_handle_wrapper: QuorumCertProcessGuard::new(abort_handle),
            };
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L180-194)
```rust
        let consensus_already_started = match self
            .states_by_key
            .get(&(update.issuer.clone(), update.kid.clone()))
            .cloned()
        {
            Some(ConsensusState::InProgress { my_proposal, .. })
            | Some(ConsensusState::Finished { my_proposal, .. }) => {
                my_proposal.observed.to_upsert == update.to_upsert
            },
            _ => false,
        };

        if consensus_already_started {
            return Ok(());
        }
```

**File:** crates/aptos-jwk-consensus/src/update_certifier.rs (L67-82)
```rust
        let task = async move {
            let qc_update = rb.broadcast(req, agg_state).await.expect("cannot fail");
            ConsensusMode::log_certify_done(epoch, &qc_update);
            let session_key = ConsensusMode::session_key_from_qc(&qc_update);
            match session_key {
                Ok(key) => {
                    let _ = qc_update_tx.push(key, qc_update);
                },
                Err(e) => {
                    error!("JWK update QCed but could not identify the session key: {e}");
                },
            }
        };
        let (abort_handle, abort_registration) = AbortHandle::new_pair();
        tokio::spawn(Abortable::new(task, abort_registration));
        Ok(abort_handle)
```

**File:** crates/aptos-jwk-consensus/src/types.rs (L96-101)
```rust
impl Drop for QuorumCertProcessGuard {
    fn drop(&mut self) {
        let QuorumCertProcessGuard { handle } = self;
        handle.abort();
    }
}
```
