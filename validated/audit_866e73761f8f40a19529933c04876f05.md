# Audit Report

## Title
Fast Sync Crash Recovery Leaves Database in Inconsistent State with Invalid Pruner Metadata

## Summary
During fast sync finalization, a crash between committing `OverallCommitProgress` and setting pruner `min_readable_version` metadata leaves the database in an inconsistent state. On restart, the node operates with `min_readable_version = 0` while only having state data at the target version V, violating the invariant that historical data from 0 to V should be accessible.

## Finding Description

The vulnerability exists in the non-atomic finalization sequence during fast sync state snapshot restoration.

**The Critical Sequence:**

1. `FastSyncStorageWrapper::finalize_state_snapshot()` delegates to the underlying `AptosDB::finalize_state_snapshot()` [1](#0-0) 

2. Inside `AptosDB::finalize_state_snapshot()`, operations execute in a non-atomic sequence:
   - Lines 155-160: Write frozen subtrees (separate write operation)
   - Lines 163-218: Build batch containing transaction data, ledger infos, and commit progress metadata
   - Line 223: **Atomically commit the batch including `OverallCommitProgress = version`**
   - Lines 225-234: **Four separate write operations** to set `min_readable_version` for each pruner
   - Line 236: Update latest ledger info (separate operation) [2](#0-1) 

**The Vulnerability:**

If a crash/interruption (SIGKILL, OOM, power loss, panic) occurs after line 223 but before lines 225-234 complete, the database state becomes inconsistent.

**On Restart:**

The `FastSyncStorageWrapper::initialize_dbs()` checks if fast sync wrapper should be created based on whether `get_synced_version()` returns 0 [3](#0-2) 

Since `get_synced_version()` reads `OverallCommitProgress` [4](#0-3)  and this was committed to version V, the condition fails and regular `AptosDB` is used directly instead of the fast sync wrapper.

The pruners initialize their `min_readable_version` from database metadata. The `get_ledger_pruner_progress()` function returns 0 when metadata doesn't exist via `unwrap_or(0)` [5](#0-4)  because `save_min_readable_version()` was never called due to the crash.

The crash recovery mechanism `sync_commit_progress()` only truncates data ahead of `OverallCommitProgress`, not missing metadata [6](#0-5) 

**The Inconsistent State:**
- `OverallCommitProgress = V` (committed)
- `min_readable_version = 0` (never updated, defaults to 0)
- Actual data: Only state snapshot at version V exists
- Missing data: Transactions, events, and historical state for versions 0 to V-1

This violates the state consistency invariant where `min_readable_version` should accurately represent what data is available.

## Impact Explanation

**Medium Severity** - State inconsistencies requiring manual intervention:

1. **Data Availability Contract Violation**: The `min_readable_version = 0` indicates all versions from 0 to V should be readable, but they aren't available. Historical queries pass the pruning check [7](#0-6)  but fail with "Not Found" errors when attempting to read non-existent data.

2. **Incorrect Network Peer Information**: The storage service advertises transaction availability based on `get_first_txn_version()` which directly returns `min_readable_version` [8](#0-7) . The `fetch_transaction_range()` function uses this to advertise data availability to peers [9](#0-8) , causing other nodes to attempt syncing historical data (versions 0 to V-1) that doesn't exist.

3. **API/Query Failures**: Any API endpoint or query attempting to read historical transactions, events, or state between versions 0 and V-1 will fail unexpectedly.

4. **No Automatic Recovery**: The inconsistency persists until manual intervention (database reset or metadata correction).

This qualifies as **"State inconsistencies requiring manual intervention"** under the Medium severity category per the Aptos bug bounty program.

## Likelihood Explanation

**Moderate Likelihood** - This vulnerability can occur in production environments:

1. **Fast Sync is Standard**: New validators and fullnodes commonly use fast sync to bootstrap, creating many opportunities for this race condition.

2. **Narrow but Real Window**: The crash window between the atomic commit (line 223) and pruner updates (lines 225-234) is narrow (microseconds to milliseconds), but real-world interruptions occur:
   - Deployment automation sending SIGTERM/SIGKILL during rolling updates
   - OOM killer terminating processes under memory pressure
   - Hardware failures, power outages, kernel panics
   - Container orchestration (Kubernetes) pod evictions

3. **Silent Failure**: The node restarts successfully and operates normally for most operations, making the inconsistency difficult to detect until historical data is queried.

4. **No Automatic Recovery**: The crash recovery mechanism only truncates data ahead of commit progress, not missing metadata.

## Recommendation

Make the pruner metadata updates atomic with the `OverallCommitProgress` commit by including them in the same batch operation:

```rust
// Include pruner metadata in the atomic batch
ledger_db_batch
    .ledger_metadata_db_batches
    .put::<DbMetadataSchema>(
        &DbMetadataKey::LedgerPrunerProgress,
        &DbMetadataValue::Version(version),
    )?;

// Similar for other pruners before the atomic write_schemas() call
self.ledger_db.write_schemas(ledger_db_batch)?;
```

Alternatively, add a recovery mechanism in `sync_commit_progress()` that detects missing pruner metadata and initializes it to the `OverallCommitProgress` value.

## Proof of Concept

To demonstrate this vulnerability:

1. Set up a node with fast sync enabled
2. Initiate fast sync to a target version V
3. During `finalize_state_snapshot()`, inject a crash (via SIGKILL) after the `write_schemas()` call but before `save_min_readable_version()` completes
4. Restart the node
5. Observe `min_readable_version = 0` via metrics or debugging
6. Query historical data for versions 0 to V-1 and observe failures
7. Check peer advertisements showing incorrect data availability range

The vulnerability can be reproduced by adding crash injection points in the finalization code path for testing purposes.

## Notes

This is a genuine state consistency issue in the fast sync crash recovery path. While it doesn't cause fund loss or consensus violations, it creates operational problems requiring manual intervention to restore database consistency. The narrow crash window makes it less likely than other vulnerabilities, but production environments with automated deployments, memory pressure, and hardware failures create realistic triggering scenarios.

### Citations

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L43-100)
```rust
    pub fn initialize_dbs(
        config: &NodeConfig,
        internal_indexer_db: Option<InternalIndexerDB>,
        update_sender: Option<Sender<(Instant, Version)>>,
    ) -> Result<Either<AptosDB, Self>> {
        let mut db_main = AptosDB::open(
            config.storage.get_dir_paths(),
            /*readonly=*/ false,
            config.storage.storage_pruner_config,
            config.storage.rocksdb_configs,
            config.storage.enable_indexer,
            config.storage.buffered_state_target_items,
            config.storage.max_num_nodes_per_lru_cache_shard,
            internal_indexer_db,
            config.storage.hot_state_config,
        )
        .map_err(|err| anyhow!("fast sync DB failed to open {}", err))?;
        if let Some(sender) = update_sender {
            db_main.add_version_update_subscriber(sender)?;
        }

        let mut db_dir = config.storage.dir();
        // when the db is empty and configured to do fast sync, we will create a second DB
        if config
            .state_sync
            .state_sync_driver
            .bootstrapping_mode
            .is_fast_sync()
            && (db_main
                .ledger_db
                .metadata_db()
                .get_synced_version()?
                .map_or(0, |v| v)
                == 0)
        {
            db_dir.push(SECONDARY_DB_DIR);
            let secondary_db = AptosDB::open(
                StorageDirPaths::from_path(db_dir.as_path()),
                /*readonly=*/ false,
                config.storage.storage_pruner_config,
                config.storage.rocksdb_configs,
                config.storage.enable_indexer,
                config.storage.buffered_state_target_items,
                config.storage.max_num_nodes_per_lru_cache_shard,
                None,
                config.storage.hot_state_config,
            )
            .map_err(|err| anyhow!("Secondary DB failed to open {}", err))?;

            Ok(Either::Right(FastSyncStorageWrapper {
                temporary_db_with_genesis: Arc::new(secondary_db),
                db_for_fast_sync: Arc::new(db_main),
                fast_sync_status: Arc::new(RwLock::new(FastSyncStatus::UNKNOWN)),
            }))
        } else {
            Ok(Either::Left(db_main))
        }
    }
```

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L154-170)
```rust
    fn finalize_state_snapshot(
        &self,
        version: Version,
        output_with_proof: TransactionOutputListWithProofV2,
        ledger_infos: &[LedgerInfoWithSignatures],
    ) -> Result<()> {
        let status = self.get_fast_sync_status();
        assert_eq!(status, FastSyncStatus::STARTED);
        self.get_aptos_db_write_ref().finalize_state_snapshot(
            version,
            output_with_proof,
            ledger_infos,
        )?;
        let mut status = self.fast_sync_status.write();
        *status = FastSyncStatus::FINISHED;
        Ok(())
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L125-241)
```rust
    fn finalize_state_snapshot(
        &self,
        version: Version,
        output_with_proof: TransactionOutputListWithProofV2,
        ledger_infos: &[LedgerInfoWithSignatures],
    ) -> Result<()> {
        let (output_with_proof, persisted_aux_info) = output_with_proof.into_parts();
        gauged_api("finalize_state_snapshot", || {
            // Ensure the output with proof only contains a single transaction output and info
            let num_transaction_outputs = output_with_proof.get_num_outputs();
            let num_transaction_infos = output_with_proof.proof.transaction_infos.len();
            ensure!(
                num_transaction_outputs == 1,
                "Number of transaction outputs should == 1, but got: {}",
                num_transaction_outputs
            );
            ensure!(
                num_transaction_infos == 1,
                "Number of transaction infos should == 1, but got: {}",
                num_transaction_infos
            );

            // TODO(joshlind): include confirm_or_save_frozen_subtrees in the change set
            // bundle below.

            // Update the merkle accumulator using the given proof
            let frozen_subtrees = output_with_proof
                .proof
                .ledger_info_to_transaction_infos_proof
                .left_siblings();
            restore_utils::confirm_or_save_frozen_subtrees(
                self.ledger_db.transaction_accumulator_db_raw(),
                version,
                frozen_subtrees,
                None,
            )?;

            // Create a single change set for all further write operations
            let mut ledger_db_batch = LedgerDbSchemaBatches::new();
            let mut sharded_kv_batch = self.state_kv_db.new_sharded_native_batches();
            let mut state_kv_metadata_batch = SchemaBatch::new();
            // Save the target transactions, outputs, infos and events
            let (transactions, outputs): (Vec<Transaction>, Vec<TransactionOutput>) =
                output_with_proof
                    .transactions_and_outputs
                    .into_iter()
                    .unzip();
            let events = outputs
                .clone()
                .into_iter()
                .map(|output| output.events().to_vec())
                .collect::<Vec<_>>();
            let wsets: Vec<WriteSet> = outputs
                .into_iter()
                .map(|output| output.write_set().clone())
                .collect();
            let transaction_infos = output_with_proof.proof.transaction_infos;
            // We should not save the key value since the value is already recovered for this version
            restore_utils::save_transactions(
                self.state_store.clone(),
                self.ledger_db.clone(),
                version,
                &transactions,
                &persisted_aux_info,
                &transaction_infos,
                &events,
                wsets,
                Some((
                    &mut ledger_db_batch,
                    &mut sharded_kv_batch,
                    &mut state_kv_metadata_batch,
                )),
                false,
            )?;

            // Save the epoch ending ledger infos
            restore_utils::save_ledger_infos(
                self.ledger_db.metadata_db(),
                ledger_infos,
                Some(&mut ledger_db_batch.ledger_metadata_db_batches),
            )?;

            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::LedgerCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;
            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::OverallCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;

            // Apply the change set writes to the database (atomically) and update in-memory state
            //
            // state kv and SMT should use shared way of committing.
            self.ledger_db.write_schemas(ledger_db_batch)?;

            self.ledger_pruner.save_min_readable_version(version)?;
            self.state_store
                .state_merkle_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .epoch_snapshot_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .state_kv_pruner
                .save_min_readable_version(version)?;

            restore_utils::update_latest_ledger_info(self.ledger_db.metadata_db(), ledger_infos)?;
            self.state_store.reset();

            Ok(())
        })
    }
```

**File:** storage/aptosdb/src/ledger_db/ledger_metadata_db.rs (L76-78)
```rust
    pub(crate) fn get_synced_version(&self) -> Result<Option<Version>> {
        get_progress(&self.db, &DbMetadataKey::OverallCommitProgress)
    }
```

**File:** storage/aptosdb/src/pruner/pruner_utils.rs (L19-21)
```rust
pub(crate) fn get_ledger_pruner_progress(ledger_db: &LedgerDb) -> Result<Version> {
    Ok(ledger_db.metadata_db().get_pruner_progress().unwrap_or(0))
}
```

**File:** storage/aptosdb/src/state_store/mod.rs (L410-502)
```rust
    pub fn sync_commit_progress(
        ledger_db: Arc<LedgerDb>,
        state_kv_db: Arc<StateKvDb>,
        state_merkle_db: Arc<StateMerkleDb>,
        crash_if_difference_is_too_large: bool,
    ) {
        let ledger_metadata_db = ledger_db.metadata_db();
        if let Some(overall_commit_progress) = ledger_metadata_db
            .get_synced_version()
            .expect("DB read failed.")
        {
            info!(
                overall_commit_progress = overall_commit_progress,
                "Start syncing databases..."
            );
            let ledger_commit_progress = ledger_metadata_db
                .get_ledger_commit_progress()
                .expect("Failed to read ledger commit progress.");
            assert_ge!(ledger_commit_progress, overall_commit_progress);

            let state_kv_commit_progress = state_kv_db
                .metadata_db()
                .get::<DbMetadataSchema>(&DbMetadataKey::StateKvCommitProgress)
                .expect("Failed to read state K/V commit progress.")
                .expect("State K/V commit progress cannot be None.")
                .expect_version();
            assert_ge!(state_kv_commit_progress, overall_commit_progress);

            // LedgerCommitProgress was not guaranteed to commit after all ledger changes finish,
            // have to attempt truncating every column family.
            info!(
                ledger_commit_progress = ledger_commit_progress,
                "Attempt ledger truncation...",
            );
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");

            // State K/V commit progress isn't (can't be) written atomically with the data,
            // because there are shards, so we have to attempt truncation anyway.
            info!(
                state_kv_commit_progress = state_kv_commit_progress,
                "Start state KV truncation..."
            );
            let difference = state_kv_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_state_kv_db(
                &state_kv_db,
                state_kv_commit_progress,
                overall_commit_progress,
                std::cmp::max(difference as usize, 1), /* batch_size */
            )
            .expect("Failed to truncate state K/V db.");

            let state_merkle_max_version = get_max_version_in_state_merkle_db(&state_merkle_db)
                .expect("Failed to get state merkle max version.")
                .expect("State merkle max version cannot be None.");
            if state_merkle_max_version > overall_commit_progress {
                let difference = state_merkle_max_version - overall_commit_progress;
                if crash_if_difference_is_too_large {
                    assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
                }
            }
            let state_merkle_target_version = find_tree_root_at_or_before(
                ledger_metadata_db,
                &state_merkle_db,
                overall_commit_progress,
            )
            .expect("DB read failed.")
            .unwrap_or_else(|| {
                panic!(
                    "Could not find a valid root before or at version {}, maybe it was pruned?",
                    overall_commit_progress
                )
            });
            if state_merkle_target_version < state_merkle_max_version {
                info!(
                    state_merkle_max_version = state_merkle_max_version,
                    target_version = state_merkle_target_version,
                    "Start state merkle truncation..."
                );
                truncate_state_merkle_db(&state_merkle_db, state_merkle_target_version)
                    .expect("Failed to truncate state merkle db.");
            }
        } else {
            info!("No overall commit progress was found!");
        }
    }
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L261-271)
```rust
    pub(super) fn error_if_ledger_pruned(&self, data_type: &str, version: Version) -> Result<()> {
        let min_readable_version = self.ledger_pruner.get_min_readable_version();
        ensure!(
            version >= min_readable_version,
            "{} at version {} is pruned, min available version is {}.",
            data_type,
            version,
            min_readable_version
        );
        Ok(())
    }
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L329-332)
```rust
    fn get_first_txn_version(&self) -> Result<Option<Version>> {
        gauged_api("get_first_txn_version", || {
            Ok(Some(self.ledger_pruner.get_min_readable_version()))
        })
```

**File:** state-sync/storage-service/server/src/storage.rs (L179-192)
```rust
    fn fetch_transaction_range(
        &self,
        latest_version: Version,
    ) -> aptos_storage_service_types::Result<Option<CompleteDataRange<Version>>, Error> {
        let first_transaction_version = self.storage.get_first_txn_version()?;
        if let Some(first_transaction_version) = first_transaction_version {
            let transaction_range =
                CompleteDataRange::new(first_transaction_version, latest_version)
                    .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
            Ok(Some(transaction_range))
        } else {
            Ok(None)
        }
    }
```
