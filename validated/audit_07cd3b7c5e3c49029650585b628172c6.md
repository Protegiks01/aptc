# Audit Report

## Title
Secret Sharing Manager Not Reset During State Sync Causes Execution Failures with Aborted Pipelines

## Summary
The `ExecutionClient::reset()` method fails to reset the `SecretShareManager` during state sync operations, while resetting the `BufferManager` and `RandManager`. This coordination failure allows blocks with aborted pipelines to be marked as ready and sent downstream, causing execution failures and validator performance degradation.

## Finding Description

During state sync operations, the system exhibits a critical coordination failure. When `sync_to_target()` is called, it invokes `reset()` which only resets the `BufferManager` and `RandManager`: [1](#0-0) 

However, the `SecretShareManager` is NOT included in this reset, despite being reset during epoch transitions: [2](#0-1) 

This creates a race condition during state sync:

1. **Pipelines are aborted**: `abort_pipeline_for_state_sync()` is called before state sync: [3](#0-2) [4](#0-3) 

2. **Pipeline abort drops receivers**: When pipelines are aborted, the decryption task receivers are dropped: [5](#0-4) 

3. **Send failures are silently ignored**: The `set_secret_shared_key()` function discards send errors and unconditionally removes rounds from pending: [6](#0-5) 

4. **Blocks incorrectly marked as ready**: When all pending rounds are removed, blocks are marked as ready: [7](#0-6) [8](#0-7) 

5. **Stale blocks sent to reset buffer manager**: These blocks are sent downstream: [9](#0-8) 

When the `BufferManager` (which WAS reset) receives blocks with aborted pipelines and attempts execution, the pipeline futures fail because their tasks were cancelled, resulting in execution errors.

## Impact Explanation

**Severity: High**

This vulnerability qualifies as **High Severity** per Aptos bug bounty criteria:

1. **Validator Node Slowdowns**: Blocks with aborted pipelines cause repeated execution failures, degrading validator performance and wasting computational resources.

2. **Protocol Violation**: The lack of coordination between state sync and the secret sharing manager violates the expected invariant that all pipeline-related components are properly reset during state sync.

3. **Liveness Impact**: Failed executions may block pipeline progress, preventing subsequent blocks from being processed efficiently and potentially causing temporary degradation in block production.

4. **Resource Waste**: The system processes stale blocks that should have been discarded, consuming network bandwidth and execution resources.

While the report's claim about decryption task panics is overstated (cancelled tasks won't panic, they simply stop executing), the core impact of execution failures and validator slowdowns is valid and significant.

## Likelihood Explanation

**Likelihood: Medium-High**

This vulnerability is triggered during normal state sync operations without requiring attacker intervention:

1. **State Sync is a Common Operation**: Nodes regularly perform state sync when catching up to the network or recovering from downtime.

2. **No Special Preconditions**: The vulnerability occurs whenever:
   - Blocks are in the secret sharing queue
   - State sync is triggered
   - Secret key aggregation is in progress

3. **Timing Window**: The asynchronous nature of secret key aggregation creates a realistic window where keys arrive after pipeline abort but before reset.

4. **Production Relevance**: This can occur in production environments during network disruptions, validator restarts, or catch-up scenarios.

## Recommendation

Modify `ExecutionClient::reset()` to include the `SecretShareManager` reset:

```rust
async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
    let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager, reset_tx_to_secret_share_manager) = {
        let handle = self.handle.read();
        (
            handle.reset_tx_to_rand_manager.clone(),
            handle.reset_tx_to_buffer_manager.clone(),
            handle.reset_tx_to_secret_share_manager.clone(), // Add this
        )
    };

    // Reset rand_manager
    if let Some(mut reset_tx) = reset_tx_to_rand_manager {
        // ... existing code ...
    }

    // Add secret_share_manager reset
    if let Some(mut reset_tx) = reset_tx_to_secret_share_manager {
        let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::SecretShareResetDropped)?;
        ack_rx.await.map_err(|_| Error::SecretShareResetDropped)?;
    }

    // Reset buffer_manager
    if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
        // ... existing code ...
    }

    Ok(())
}
```

This ensures all pipeline-related components are properly coordinated during state sync.

## Proof of Concept

The vulnerability can be demonstrated by:

1. Setting up a validator node with secret sharing enabled
2. Adding blocks to the secret sharing queue
3. Triggering state sync via `sync_to_target()`
4. Observing that the `SecretShareManager` is not reset while `BufferManager` and `RandManager` are
5. Monitoring execution logs for failures when blocks with aborted pipelines are processed

The code evidence provided demonstrates the missing reset coordination in the production codebase.

## Notes

While the report's specific claim about decryption task panics is technically inaccurate (cancelled tasks don't execute to reach the `.expect()` call), the core vulnerability is valid: the failure to reset `SecretShareManager` during state sync creates a coordination bug that causes execution failures, validator slowdowns, and potential liveness issues. This represents a legitimate HIGH severity protocol violation in the Aptos consensus implementation.

### Citations

**File:** consensus/src/pipeline/execution_client.rs (L674-708)
```rust
    async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
        let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager) = {
            let handle = self.handle.read();
            (
                handle.reset_tx_to_rand_manager.clone(),
                handle.reset_tx_to_buffer_manager.clone(),
            )
        };

        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        }

        if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
            // reset execution phase and commit phase
            let (tx, rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::ResetDropped)?;
            rx.await.map_err(|_| Error::ResetDropped)?;
        }

        Ok(())
```

**File:** consensus/src/pipeline/execution_client.rs (L711-760)
```rust
    async fn end_epoch(&self) {
        let (
            reset_tx_to_rand_manager,
            reset_tx_to_buffer_manager,
            reset_tx_to_secret_share_manager,
        ) = {
            let mut handle = self.handle.write();
            handle.reset()
        };

        if let Some(mut tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop rand manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop rand manager");
        }

        if let Some(mut tx) = reset_tx_to_secret_share_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop secret share manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop secret share manager");
        }

        if let Some(mut tx) = reset_tx_to_buffer_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop buffer manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop buffer manager");
        }
        self.execution_proxy.end_epoch();
    }
```

**File:** consensus/src/block_storage/sync_manager.rs (L504-514)
```rust
        // abort any pending executor tasks before entering state sync
        // with zaptos, things can run before hitting buffer manager
        if let Some(block_store) = maybe_block_store {
            monitor!(
                "abort_pipeline_for_state_sync",
                block_store.abort_pipeline_for_state_sync().await
            );
        }
        execution_client
            .sync_to_target(highest_commit_cert.ledger_info().clone())
            .await?;
```

**File:** consensus/src/block_storage/block_store.rs (L617-627)
```rust
    pub async fn abort_pipeline_for_state_sync(&self) {
        let blocks = self.inner.read().get_all_blocks();
        // the blocks are not ordered by round here, so we need to abort all then wait
        let futs: Vec<_> = blocks
            .into_iter()
            .filter_map(|b| b.abort_pipeline())
            .collect();
        for f in futs {
            f.wait_until_finishes().await;
        }
    }
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L528-546)
```rust
    pub fn abort_pipeline(&self) -> Option<PipelineFutures> {
        if let Some(abort_handles) = self.pipeline_abort_handle.lock().take() {
            let mut aborted = false;
            for handle in abort_handles {
                if !handle.is_finished() {
                    handle.abort();
                    aborted = true;
                }
            }
            if aborted {
                info!(
                    "[Pipeline] Aborting pipeline for block {} {} {}",
                    self.id(),
                    self.epoch(),
                    self.round()
                );
            }
        }
        self.pipeline_futs.lock().take()
```

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L60-62)
```rust
    pub fn is_fully_secret_shared(&self) -> bool {
        self.pending_secret_key_rounds.is_empty()
    }
```

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L64-77)
```rust
    pub fn set_secret_shared_key(&mut self, round: Round, key: SecretSharedKey) {
        let offset = self.offset(round);
        if self.pending_secret_key_rounds.contains(&round) {
            observe_block(
                self.blocks()[offset].timestamp_usecs(),
                BlockStage::SECRET_SHARING_ADD_DECISION,
            );
            let block = &self.blocks_mut()[offset];
            if let Some(tx) = block.pipeline_tx().lock().as_mut() {
                tx.secret_shared_key_tx.take().map(|tx| tx.send(Some(key)));
            }
            self.pending_secret_key_rounds.remove(&round);
        }
    }
```

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L112-127)
```rust
    pub fn dequeue_ready_prefix(&mut self) -> Vec<OrderedBlocks> {
        let mut ready_prefix = vec![];
        while let Some((_starting_round, item)) = self.queue.first_key_value() {
            if item.is_fully_secret_shared() {
                let (_, item) = self.queue.pop_first().expect("First key must exist");
                for block in item.blocks() {
                    observe_block(block.timestamp_usecs(), BlockStage::SECRET_SHARING_READY);
                }
                let QueueItem { ordered_blocks, .. } = item;
                ready_prefix.push(ordered_blocks);
            } else {
                break;
            }
        }
        ready_prefix
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L160-170)
```rust
    fn process_ready_blocks(&mut self, ready_blocks: Vec<OrderedBlocks>) {
        let rounds: Vec<u64> = ready_blocks
            .iter()
            .flat_map(|b| b.ordered_blocks.iter().map(|b3| b3.round()))
            .collect();
        info!(rounds = rounds, "Processing secret share ready blocks.");

        for blocks in ready_blocks {
            let _ = self.outgoing_blocks.unbounded_send(blocks);
        }
    }
```
