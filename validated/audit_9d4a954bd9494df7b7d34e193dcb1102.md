# Audit Report

## Title
Integer Underflow in MixedPayloadClient Causes Consensus Node Crash or Filtering Bypass

## Summary
The `MixedPayloadClient::pull_payload()` function contains an unchecked integer subtraction that causes u64 underflow when backpressure mechanisms reduce `max_txns_after_filtering` below the number of validator transactions pulled. This leads to consensus node crashes in debug builds or bypassing of transaction filtering limits in release builds.

## Finding Description

The vulnerability exists in the production consensus payload client code where validator transactions are pulled first, then their count is subtracted from the remaining user transaction limit without bounds checking. [1](#0-0) 

**Vulnerability Trigger Conditions:**

1. **Backpressure mechanisms independently reduce transaction limits**: At extreme pipeline latency (6000ms), the backpressure configuration reduces `max_sending_block_txns_after_filtering_override` to 5. [2](#0-1) 

2. **Floor protection clamps to minimum**: The minimum floor `MIN_BLOCK_TXNS_AFTER_FILTERING` is set to 100 (50 * 2). [3](#0-2) [4](#0-3) 

This floor is applied in `calculate_max_block_sizes()` to prevent overly aggressive backpressure. [5](#0-4) 

3. **Validator transactions constrained separately**: Validator transactions are pulled with limit `min(params.max_txns.count(), validator_txn_config.per_block_limit_txn_count())`. [6](#0-5) 

The `max_txns.count()` is proportionally reduced when bytes are reduced via `compute_with_bytes()`. [7](#0-6) 

When bytes are reduced from 3MB to 1MB (MIN_BLOCK_BYTES_OVERRIDE), the count is proportionally reduced from ~5000 to ~1665, but this can still exceed the clamped `max_txns_after_filtering` of 100.

4. **Unchecked subtraction**: Lines 94-95 use standard u64 subtraction (not `saturating_sub`) which will panic in debug builds or wrap in release builds.

**Attack Scenario:**
- Pipeline latency reaches 6000ms, reducing `max_txns_after_filtering` to 5 (clamped to 100 by the protection mechanism)
- `max_txns.count()` is proportionally reduced from ~5000 to ~1665 (3MBâ†’1MB reduction)
- Governance has configured `per_block_limit_txn_count` to 200 (default is 2)
- Validator pool returns 200 transactions (respecting the min(1665, 200) = 200 limit)
- Line 94: `100 - 200` = **u64 UNDERFLOW**

**Default Configuration Safety**: The default `per_block_limit_txn_count` is 2, making the default configuration safe. [8](#0-7) [9](#0-8) 

However, this value can be changed via on-chain governance to any u64 value without validation constraints. [10](#0-9) 

## Impact Explanation

**HIGH Severity** - This vulnerability affects consensus availability and aligns with the Aptos bug bounty category of "Validator Node Crashes":

**Debug Builds (Development/Testing):**
- u64 subtraction underflow causes panic at runtime
- Immediate consensus node crash when attempting to create a block proposal
- Complete loss of individual validator availability
- Validators running debug builds become unable to propose blocks

**Release Builds (Production):**
- Integer wraps to `u64::MAX - (validator_txns.len() - max_txns_after_filtering - 1)`
- Example: `100 - 200 = 18,446,744,073,709,551,516`
- User payload client receives an artificially inflated transaction limit
- Bypasses backpressure protections, potentially causing:
  - Oversized blocks leading to execution timeouts
  - Different behavior between validators running debug vs release builds
  - Consensus inconsistency if validators diverge on block validity
- Could escalate to **CRITICAL** if it causes consensus safety violations across the network

## Likelihood Explanation

**LOW-MEDIUM Likelihood**:

**Preconditions Required:**

1. **Extreme backpressure activation**: 6000ms pipeline latency or severe chain health degradation. While possible during network stress, the system has multiple backpressure levels (7 levels total) designed to prevent reaching this extreme condition. [11](#0-10) 

2. **Governance misconfiguration**: Requires governance to increase `per_block_limit_txn_count` from default of 2 to >100. The Move governance interface provides no technical validation preventing this configuration change, but it requires deliberate governance action through proposal and voting. [12](#0-11) 

3. **Sufficient validator transactions**: The validator transaction pool must have enough pending transactions to reach the configured limit.

**Mitigating Factors:**
- Default configuration is safe (per_block_limit_txn_count = 2)
- Requires both extreme network conditions AND governance configuration change
- No malicious attacker can directly trigger this without governance participation
- The vulnerability represents a governance misconfiguration issue rather than a direct attack vector

## Recommendation

Use `saturating_sub()` instead of standard subtraction to prevent underflow:

```rust
user_txn_pull_params.max_txns_after_filtering = 
    user_txn_pull_params.max_txns_after_filtering.saturating_sub(validator_txns.len() as u64);
user_txn_pull_params.soft_max_txns_after_filtering = 
    user_txn_pull_params.soft_max_txns_after_filtering.saturating_sub(validator_txns.len() as u64);
```

Additionally, consider adding validation in the governance configuration update path to enforce reasonable limits on `per_block_limit_txn_count` relative to `MIN_BLOCK_TXNS_AFTER_FILTERING`.

## Proof of Concept

The vulnerability can be reproduced by:

1. Configuring `ValidatorTxnConfig` with `per_block_limit_txn_count = 200` via governance
2. Simulating extreme backpressure conditions (6000ms pipeline latency)
3. Ensuring the validator transaction pool has 200+ transactions
4. Triggering block proposal, which will cause the underflow at line 94

A concrete test would require:
- Setting up a test network with the modified `ValidatorTxnConfig`
- Injecting artificial latency to trigger maximum backpressure
- Populating the validator transaction pool
- Observing the panic in debug builds or wrapped integer in release builds

## Notes

This vulnerability demonstrates an interaction bug between multiple subsystems (backpressure, governance configuration, and payload pulling). While the default configuration is safe, the lack of validation constraints in the governance update path allows configurations that violate implicit assumptions in the payload client code. The fix should include both immediate protection (saturating arithmetic) and longer-term validation (configuration constraints).

### Citations

**File:** consensus/src/payload_client/mixed.rs (L65-79)
```rust
        let mut validator_txns = self
            .validator_txn_pool_client
            .pull(
                params.max_poll_time,
                min(
                    params.max_txns.count(),
                    self.validator_txn_config.per_block_limit_txn_count(),
                ),
                min(
                    params.max_txns.size_in_bytes(),
                    self.validator_txn_config.per_block_limit_total_bytes(),
                ),
                validator_txn_filter,
            )
            .await;
```

**File:** consensus/src/payload_client/mixed.rs (L94-95)
```rust
        user_txn_pull_params.max_txns_after_filtering -= validator_txns.len() as u64;
        user_txn_pull_params.soft_max_txns_after_filtering -= validator_txns.len() as u64;
```

**File:** config/src/config/consensus_config.rs (L28-28)
```rust
const MIN_BLOCK_TXNS_AFTER_FILTERING: u64 = DEFEAULT_MAX_BATCH_TXNS as u64 * 2;
```

**File:** config/src/config/consensus_config.rs (L263-319)
```rust
            pipeline_backpressure: vec![
                PipelineBackpressureValues {
                    // pipeline_latency looks how long has the oldest block still in pipeline
                    // been in the pipeline.
                    // Block enters the pipeline after consensus orders it, and leaves the
                    // pipeline once quorum on execution result among validators has been reached
                    // (so-(badly)-called "commit certificate"), meaning 2f+1 validators have finished execution.
                    back_pressure_pipeline_latency_limit_ms: 1200,
                    max_sending_block_txns_after_filtering_override:
                        MAX_SENDING_BLOCK_TXNS_AFTER_FILTERING,
                    max_sending_block_bytes_override: 5 * 1024 * 1024,
                    backpressure_proposal_delay_ms: 50,
                },
                PipelineBackpressureValues {
                    back_pressure_pipeline_latency_limit_ms: 1500,
                    max_sending_block_txns_after_filtering_override:
                        MAX_SENDING_BLOCK_TXNS_AFTER_FILTERING,
                    max_sending_block_bytes_override: 5 * 1024 * 1024,
                    backpressure_proposal_delay_ms: 100,
                },
                PipelineBackpressureValues {
                    back_pressure_pipeline_latency_limit_ms: 1900,
                    max_sending_block_txns_after_filtering_override:
                        MAX_SENDING_BLOCK_TXNS_AFTER_FILTERING,
                    max_sending_block_bytes_override: 5 * 1024 * 1024,
                    backpressure_proposal_delay_ms: 200,
                },
                // with execution backpressure, only later start reducing block size
                PipelineBackpressureValues {
                    back_pressure_pipeline_latency_limit_ms: 2500,
                    max_sending_block_txns_after_filtering_override: 1000,
                    max_sending_block_bytes_override: MIN_BLOCK_BYTES_OVERRIDE,
                    backpressure_proposal_delay_ms: 300,
                },
                PipelineBackpressureValues {
                    back_pressure_pipeline_latency_limit_ms: 3500,
                    max_sending_block_txns_after_filtering_override: 200,
                    max_sending_block_bytes_override: MIN_BLOCK_BYTES_OVERRIDE,
                    backpressure_proposal_delay_ms: 300,
                },
                PipelineBackpressureValues {
                    back_pressure_pipeline_latency_limit_ms: 4500,
                    max_sending_block_txns_after_filtering_override: 30,
                    max_sending_block_bytes_override: MIN_BLOCK_BYTES_OVERRIDE,
                    backpressure_proposal_delay_ms: 300,
                },
                PipelineBackpressureValues {
                    back_pressure_pipeline_latency_limit_ms: 6000,
                    // in practice, latencies and delay make it such that ~2 blocks/s is max,
                    // meaning that most aggressively we limit to ~10 TPS
                    // For transactions that are more expensive than that, we should
                    // instead rely on max gas per block to limit latency.
                    max_sending_block_txns_after_filtering_override: 5,
                    max_sending_block_bytes_override: MIN_BLOCK_BYTES_OVERRIDE,
                    backpressure_proposal_delay_ms: 300,
                },
            ],
```

**File:** config/src/config/quorum_store_config.rs (L13-13)
```rust
pub const DEFEAULT_MAX_BATCH_TXNS: usize = 50;
```

**File:** consensus/src/liveness/proposal_generator.rs (L827-837)
```rust
        let (max_block_txns_after_filtering, max_txns_from_block_to_execute) = if self
            .min_max_txns_in_block_after_filtering_from_backpressure
            > max_block_txns_after_filtering
        {
            (
                self.min_max_txns_in_block_after_filtering_from_backpressure,
                Some(max_block_txns_after_filtering),
            )
        } else {
            (max_block_txns_after_filtering, None)
        };
```

**File:** consensus/consensus-types/src/utils.rs (L94-104)
```rust
    pub fn compute_with_bytes(&self, new_size_in_bytes: u64) -> PayloadTxnsSize {
        let new_count = if self.bytes > 0 {
            let factor = new_size_in_bytes as f64 / self.bytes as f64;
            max((self.count as f64 * factor) as u64, 1u64)
        } else {
            // If bytes is zero, then count is zero. In this case, set the new
            // count to be the same as bytes.
            new_size_in_bytes
        };
        PayloadTxnsSize::new_normalized(new_count, new_size_in_bytes)
    }
```

**File:** types/src/on_chain_config/consensus_config.rs (L125-125)
```rust
const VTXN_CONFIG_PER_BLOCK_LIMIT_TXN_COUNT_DEFAULT: u64 = 2;
```

**File:** types/src/on_chain_config/consensus_config.rs (L133-136)
```rust
    V1 {
        per_block_limit_txn_count: u64,
        per_block_limit_total_bytes: u64,
    },
```

**File:** types/src/on_chain_config/consensus_config.rs (L140-144)
```rust
    pub fn default_for_genesis() -> Self {
        Self::V1 {
            per_block_limit_txn_count: VTXN_CONFIG_PER_BLOCK_LIMIT_TXN_COUNT_DEFAULT,
            per_block_limit_total_bytes: VTXN_CONFIG_PER_BLOCK_LIMIT_TOTAL_BYTES_DEFAULT,
        }
```

**File:** aptos-move/framework/aptos-framework/sources/configs/consensus_config.move (L52-56)
```text
    public fun set_for_next_epoch(account: &signer, config: vector<u8>) {
        system_addresses::assert_aptos_framework(account);
        assert!(vector::length(&config) > 0, error::invalid_argument(EINVALID_CONFIG));
        std::config_buffer::upsert<ConsensusConfig>(ConsensusConfig {config});
    }
```
