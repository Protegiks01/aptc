# Audit Report

## Title
State Divergence via randomness_override_seq_num Causes Network Halt with <1/3 Byzantine Validators

## Summary
The `randomness_override_seq_num` configuration field enables a critical vulnerability where fewer than 1/3 Byzantine validators can cause complete network halt through state root divergence. This violates the fundamental BFT liveness guarantee that Aptos must tolerate up to 1/3 Byzantine validators.

## Finding Description

Each validator independently determines randomness enablement by comparing its local `randomness_override_seq_num` against the on-chain `RandomnessConfigSeqNum`. [1](#0-0) 

When `local_seqnum > onchain_seqnum`, the function returns `Self::default_disabled()`, forcing randomness off for that validator only. [2](#0-1) 

Validators with different configurations take divergent execution paths:

**Path 1 - Randomness Disabled**: When `rand_config` is `None`, blocks bypass the RandManager entirely: [3](#0-2) 

**Path 2 - Randomness Enabled**: Validators participate in distributed randomness generation through RandManager.

During block execution, validators with different configurations create different metadata transactions: [4](#0-3) 

The VM executes `block_prologue_ext` with different randomness parameters: [5](#0-4) 

This updates the `PerBlockRandomness` resource with different seed values: [6](#0-5) 

Different resource states produce different state roots, preventing 2f+1 validators from signing the same LedgerInfo, causing network halt.

**Critical Design Flaw**: The recovery mechanism requires ALL validators to coordinate: [7](#0-6) 

The documentation explicitly states "Every validator restarts": [8](#0-7) 

This assumption of perfect coordination violates BFT principles, which must tolerate up to 1/3 Byzantine validators acting arbitrarily.

## Impact Explanation

This qualifies as **Critical Severity** under Aptos Bug Bounty category "Total Loss of Liveness/Network Availability":

1. **Complete Network Halt**: All honest validators cannot commit blocks due to state root divergence
2. **BFT Liveness Violation**: The network should remain live with <1/3 Byzantine validators, but this attack breaks that guarantee
3. **Difficult Recovery**: Requires out-of-band coordination among all validator operators. If Byzantine validators refuse to cooperate, recovery requires hardfork to exclude them
4. **Repeatable Attack**: Attackers can continuously re-trigger the attack

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

Attack Requirements:
- Control <1/3 validator nodes (within BFT tolerance)
- Modify node configuration (single field change)
- No cryptographic expertise required

The attack is realistic because:
- BFT systems MUST assume up to 1/3 validators can be Byzantine
- Configuration modification is trivial compared to protocol-level attacks
- No on-chain detection or prevention mechanism exists
- The design incorrectly assumes all validators will coordinate during recovery

## Recommendation

Implement consensus-level agreement on randomness configuration:

1. **Short-term**: Add validator signature checking to ensure all validators have compatible randomness configurations before block execution
2. **Long-term**: Move randomness enablement decisions to on-chain governance, making it part of consensus rather than local configuration
3. **Recovery**: Implement automatic detection of configuration mismatches and safe fallback modes that don't cause state divergence

Alternatively, ensure randomness is deterministically computed from consensus-agreed data so different configurations cannot produce different execution results.

## Proof of Concept

The existing test demonstrates the vulnerability premise - it shows all validators must coordinate the same override value for recovery to work: [9](#0-8) 

To demonstrate the vulnerability, modify this test so only 2 out of 4 validators set `randomness_override_seq_num = 1`, while the other 2 keep the default value. The network will halt as validators compute different state roots and cannot reach consensus on LedgerInfo signatures.

---

**Notes**: This vulnerability exists because the emergency recovery mechanism was designed assuming perfect coordination among validator operators, which contradicts BFT assumptions. The system should tolerate arbitrary behavior from up to 1/3 validators, including configuration divergence, without losing liveness.

### Citations

**File:** types/src/on_chain_config/randomness_config.rs (L139-151)
```rust
    pub fn from_configs(
        local_seqnum: u64,
        onchain_seqnum: u64,
        onchain_raw_config: Option<RandomnessConfigMoveStruct>,
    ) -> Self {
        if local_seqnum > onchain_seqnum {
            Self::default_disabled()
        } else {
            onchain_raw_config
                .and_then(|onchain_raw| OnChainRandomnessConfig::try_from(onchain_raw).ok())
                .unwrap_or_else(OnChainRandomnessConfig::default_if_missing)
        }
    }
```

**File:** consensus/src/epoch_manager.rs (L1207-1221)
```rust
        info!(
            epoch = epoch_state.epoch,
            local = self.randomness_override_seq_num,
            onchain = onchain_randomness_config_seq_num.seq_num,
            "Checking randomness config override."
        );
        if self.randomness_override_seq_num > onchain_randomness_config_seq_num.seq_num {
            warn!("Randomness will be force-disabled by local config!");
        }

        let onchain_randomness_config = OnChainRandomnessConfig::from_configs(
            self.randomness_override_seq_num,
            onchain_randomness_config_seq_num.seq_num,
            randomness_config_move_struct.ok(),
        );
```

**File:** consensus/src/pipeline/execution_client.rs (L474-477)
```rust
            (None, None) => {
                let (ordered_block_tx, ordered_block_rx) = unbounded();
                (ordered_block_tx, ordered_block_rx, None, None)
            },
```

**File:** consensus/consensus-types/src/block.rs (L597-616)
```rust
    pub fn new_metadata_with_randomness(
        &self,
        validators: &[AccountAddress],
        randomness: Option<Randomness>,
    ) -> BlockMetadataExt {
        BlockMetadataExt::new_v1(
            self.id(),
            self.epoch(),
            self.round(),
            self.author().unwrap_or(AccountAddress::ZERO),
            self.previous_bitvec().into(),
            // For nil block, we use 0x0 which is convention for nil address in move.
            self.block_data()
                .failed_authors()
                .map_or(vec![], |failed_authors| {
                    Self::failed_authors_to_indices(validators, failed_authors)
                }),
            self.timestamp_usecs(),
            randomness,
        )
```

**File:** aptos-move/aptos-vm/src/aptos_vm.rs (L2519-2522)
```rust
            randomness
                .as_ref()
                .map(Randomness::randomness_cloned)
                .as_move_value(),
```

**File:** aptos-move/framework/aptos-framework/sources/randomness.move (L64-72)
```text
    public(friend) fun on_new_block(vm: &signer, epoch: u64, round: u64, seed_for_new_block: Option<vector<u8>>) acquires PerBlockRandomness {
        system_addresses::assert_vm(vm);
        if (exists<PerBlockRandomness>(@aptos_framework)) {
            let randomness = borrow_global_mut<PerBlockRandomness>(@aptos_framework);
            randomness.epoch = epoch;
            randomness.round = round;
            randomness.seed = seed_for_new_block;
        }
    }
```

**File:** testsuite/smoke-test/src/randomness/randomness_stall_recovery.rs (L1-165)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use crate::{
    genesis::enable_sync_only_mode, smoke_test_environment::SwarmBuilder,
    utils::get_on_chain_resource,
};
use aptos::common::types::GasOptions;
use aptos_config::config::{OverrideNodeConfig, PersistableConfig};
use aptos_forge::{NodeExt, Swarm, SwarmExt};
use aptos_logger::{debug, info};
use aptos_types::{on_chain_config::OnChainRandomnessConfig, randomness::PerBlockRandomness};
use std::{
    ops::Add,
    sync::Arc,
    time::{Duration, Instant},
};

/// Chain recovery using a local config from randomness stall should work.
/// See `randomness_config_seqnum.move` for more details.
#[tokio::test]
async fn randomness_stall_recovery() {
    let epoch_duration_secs = 20;

    let (mut swarm, mut cli, _faucet) = SwarmBuilder::new_local(4)
        .with_num_fullnodes(0) //TODO: revert back to 1 after invalid version bug is fixed
        .with_aptos()
        .with_init_config(Arc::new(|_, conf, _| {
            conf.api.failpoints_enabled = true;
        }))
        .with_init_genesis_config(Arc::new(move |conf| {
            conf.epoch_duration_secs = epoch_duration_secs;

            // Ensure randomness is enabled.
            conf.consensus_config.enable_validator_txns();
            conf.randomness_config_override = Some(OnChainRandomnessConfig::default_enabled());
        }))
        .build_with_cli(0)
        .await;

    let root_addr = swarm.chain_info().root_account().address();
    let root_idx = cli.add_account_with_address_to_cli(swarm.root_key(), root_addr);

    let rest_client = swarm.validators().next().unwrap().rest_client();

    info!("Wait for epoch 2.");
    swarm
        .wait_for_all_nodes_to_catchup_to_epoch(2, Duration::from_secs(epoch_duration_secs * 2))
        .await
        .expect("Epoch 2 taking too long to arrive!");

    info!("Halting the chain by putting every validator into sync_only mode.");
    for validator in swarm.validators_mut() {
        enable_sync_only_mode(4, validator).await;
    }

    info!("Chain should have halted.");
    let liveness_check_result = swarm
        .liveness_check(Instant::now().add(Duration::from_secs(20)))
        .await;
    info!("liveness_check_result={:?}", liveness_check_result);
    assert!(liveness_check_result.is_err());

    info!("Hot-fixing all validators.");
    for (idx, validator) in swarm.validators_mut().enumerate() {
        info!("Stopping validator {}.", idx);
        validator.stop();
        let config_path = validator.config_path();
        let mut validator_override_config =
            OverrideNodeConfig::load_config(config_path.clone()).unwrap();
        validator_override_config
            .override_config_mut()
            .randomness_override_seq_num = 1;
        validator_override_config
            .override_config_mut()
            .consensus
            .sync_only = false;
        info!("Updating validator {} config.", idx);
        validator_override_config.save_config(config_path).unwrap();
        info!("Restarting validator {}.", idx);
        validator.start().unwrap();
        info!("Let validator {} bake for 5 secs.", idx);
        tokio::time::sleep(Duration::from_secs(5)).await;
    }

    info!("Hot-fixing the VFNs.");
    for (idx, vfn) in swarm.fullnodes_mut().enumerate() {
        info!("Stopping VFN {}.", idx);
        vfn.stop();
        let config_path = vfn.config_path();
        let mut vfn_override_config = OverrideNodeConfig::load_config(config_path.clone()).unwrap();
        vfn_override_config
            .override_config_mut()
            .randomness_override_seq_num = 1;
        info!("Updating VFN {} config.", idx);
        vfn_override_config.save_config(config_path).unwrap();
        info!("Restarting VFN {}.", idx);
        vfn.start().unwrap();
        info!("Let VFN {} bake for 5 secs.", idx);
        tokio::time::sleep(Duration::from_secs(5)).await;
    }

    let liveness_check_result = swarm
        .liveness_check(Instant::now().add(Duration::from_secs(30)))
        .await;
    assert!(liveness_check_result.is_ok());

    info!("There should be no randomness at the moment.");
    let block_randomness_seed = get_on_chain_resource::<PerBlockRandomness>(&rest_client).await;
    assert!(block_randomness_seed.seed.is_none());

    info!("Bump on-chain conig seqnum to re-enable randomness.");
    let script = r#"
script {
    use aptos_framework::aptos_governance;
    use aptos_framework::randomness_config_seqnum;

    fun main(core_resources: &signer) {
        let framework_signer = aptos_governance::get_signer_testnet_only(core_resources, @0x1);
        randomness_config_seqnum::set_for_next_epoch(&framework_signer, 2);
        aptos_governance::force_end_epoch(&framework_signer); // reconfigure() won't work at the moment.
    }
}
    "#;
    let gas_options = GasOptions {
        gas_unit_price: Some(1),
        max_gas: Some(2000000),
        expiration_secs: 60,
    };
    let txn_summary = cli
        .run_script_with_gas_options(root_idx, script, Some(gas_options))
        .await
        .expect("Txn execution error.");
    debug!("txn_summary={:?}", txn_summary);

    tokio::time::sleep(Duration::from_secs(10)).await;

    let epoch = rest_client
        .get_ledger_information()
        .await
        .unwrap()
        .into_inner()
        .epoch;
    info!(
        "Current epoch is {}. Wait until epoch {}, and randomness should be back.",
        epoch,
        epoch + 1
    );

    swarm
        .wait_for_all_nodes_to_catchup_to_epoch(
            epoch + 1,
            Duration::from_secs(epoch_duration_secs * 2),
        )
        .await
        .unwrap_or_else(|_| panic!("Epoch {} taking too long to arrive!", epoch + 1));

    let PerBlockRandomness {
        epoch: actual_epoch,
        ..
    } = get_on_chain_resource::<PerBlockRandomness>(&rest_client).await;
    // seed is not necessarily generated because of the rand check optimization.
    // but epoch and round should be updated.
    assert_eq!(epoch + 1, actual_epoch);
}
```

**File:** aptos-move/framework/aptos-framework/sources/configs/randomness_config_seqnum.move (L3-9)
```text
/// When randomness generation is stuck due to a bug, the chain is also stuck. Below is the recovery procedure.
/// 1. Ensure more than 2/3 stakes are stuck at the same version.
/// 1. Every validator restarts with `randomness_override_seq_num` set to `X+1` in the node config file,
///    where `X` is the current `RandomnessConfigSeqNum` on chain.
/// 1. The chain should then be unblocked.
/// 1. Once the bug is fixed and the binary + framework have been patched,
///    a governance proposal is needed to set `RandomnessConfigSeqNum` to be `X+2`.
```
