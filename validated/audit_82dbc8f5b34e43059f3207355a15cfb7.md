# Audit Report

## Title
Secret Share Manager Not Reset During Consensus Sync Operations Leading to Consensus Liveness Failure

## Summary
The `reset()` method in `ExecutionProxyClient` fails to reset the `SecretShareManager` during state synchronization operations (`sync_to_target()` and `sync_for_duration()`), while correctly resetting the `RandManager` and `BufferManager`. This causes the `SecretShareManager` to maintain stale state after a validator syncs, leading to rejection of valid secret shares, coordinator deadlocks, and consensus liveness failures.

## Finding Description

When a validator falls behind and needs to synchronize state, the consensus layer calls either `sync_to_target()` or `sync_for_duration()` to catch up. [1](#0-0)  Both methods invoke the `reset()` method to reset internal consensus components to the new synchronized state.

The `reset()` method retrieves and resets only two of the three consensus managers - it retrieves `reset_tx_to_rand_manager` and `reset_tx_to_buffer_manager` from the handle, but does NOT retrieve or reset `reset_tx_to_secret_share_manager`. [2](#0-1) 

In contrast, the `end_epoch()` method correctly retrieves and resets all three managers including `reset_tx_to_secret_share_manager` by calling `handle.reset()` which returns all three reset channels. [3](#0-2) 

The `BufferManagerHandle` stores all three reset channels and its `reset()` method returns all of them. [4](#0-3) 

The `SecretShareManager` maintains critical state that must be synchronized, including the `highest_known_round` field used for validation. When a reset request is received, the `process_reset()` method properly updates this state by calling `update_highest_known_round()`. [5](#0-4) 

However, since `reset()` never sends this reset request to the `SecretShareManager`, the state remains stale. When secret shares arrive after sync, they are validated against `highest_known_round + FUTURE_ROUNDS_TO_ACCEPT` where `FUTURE_ROUNDS_TO_ACCEPT` is defined as 200 rounds. [6](#0-5)  Both `add_self_share()` and `add_share()` methods reject shares that exceed this threshold with "Share from future round" error. [7](#0-6) 

If a validator syncs forward by more than 200 rounds without resetting the secret share manager, all incoming secret shares will be rejected.

**Attack Scenario:**

1. Secret sharing is enabled when `secret_sharing_config` is `Some` during epoch start [8](#0-7) 
2. Validator is processing blocks at round N with `highest_known_round = N`
3. Validator falls behind and needs to sync to round N+300
4. Consensus calls `sync_to_target()` which invokes `reset()`
5. RandManager and BufferManager are reset to round N+300
6. **SecretShareManager is NOT reset and maintains `highest_known_round = N`**
7. Validator resumes consensus at round N+300
8. New blocks arrive requiring secret sharing for decryption
9. Incoming secret shares for round N+300 are rejected (N+300 > N+200)
10. The coordinator waits indefinitely for secret shares that will never arrive

The coordinator requires both `rand_ready` and `secret_ready` flags before forwarding blocks to execution. [9](#0-8)  Without valid secret shares, the `secret_ready` flag never becomes true, blocks remain stuck in the coordinator indefinitely, causing consensus liveness failure for that validator. The decryption pipeline explicitly waits for the secret shared key via blocking await. [10](#0-9) 

## Impact Explanation

This is a **High Severity** vulnerability per Aptos bug bounty criteria:

**Validator Node Slowdowns/Liveness Failure**: Validators that sync while secret sharing is enabled will be unable to process blocks requiring secret shares, causing them to fall out of consensus. This degrades network performance and can lead to validator set disruption.

**Consensus Liveness Issues**: If multiple validators experience this issue simultaneously (e.g., after a network partition), the network could experience significant liveness degradation until validators are manually restarted or the epoch changes (which triggers `end_epoch()` that correctly resets all managers).

This aligns with the High Severity category (up to $50,000) in the Aptos bug bounty program for validator node slowdowns and consensus liveness issues. It does not constitute Critical severity because:
- It does not directly cause loss of funds
- It does not enable permanent chain splits (recoverable via epoch change or restart)
- It requires specific conditions (sync + secret sharing enabled)

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability will trigger whenever:
1. Secret sharing is enabled in production (encrypted transactions feature)
2. A validator falls behind by more than 200 rounds (approximately 20 seconds at 10 rounds/second)
3. The validator calls `sync_to_target()` or `sync_for_duration()` to catch up

These conditions are realistic in production environments:
- Validators regularly experience temporary network issues
- Validators restart and need to sync
- The 200-round threshold is relatively small

The bug is deterministic and will occur every time these conditions are met. The primary uncertainty is whether secret sharing is currently enabled in production, which depends on whether the encrypted transactions feature is active. Regardless of current deployment status, this is a latent bug that requires fixing before or when the feature becomes active.

## Recommendation

Modify the `reset()` method in `ExecutionProxyClient` to retrieve and reset all three managers, consistent with how `end_epoch()` handles resets:

```rust
async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
    let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager, reset_tx_to_secret_share_manager) = {
        let handle = self.handle.read();
        (
            handle.reset_tx_to_rand_manager.clone(),
            handle.reset_tx_to_buffer_manager.clone(),
            handle.reset_tx_to_secret_share_manager.clone(),  // Add this
        )
    };

    // Reset RandManager
    if let Some(mut reset_tx) = reset_tx_to_rand_manager {
        let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::RandResetDropped)?;
        ack_rx.await.map_err(|_| Error::RandResetDropped)?;
    }

    // Reset SecretShareManager (ADD THIS BLOCK)
    if let Some(mut reset_tx) = reset_tx_to_secret_share_manager {
        let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::SecretShareResetDropped)?;
        ack_rx.await.map_err(|_| Error::SecretShareResetDropped)?;
    }

    // Reset BufferManager
    if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
        let (tx, rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::ResetDropped)?;
        rx.await.map_err(|_| Error::ResetDropped)?;
    }

    Ok(())
}
```

## Proof of Concept

The vulnerability is evident from code inspection. A PoC would require:
1. Setting up a validator with secret sharing enabled
2. Allowing it to process blocks at round N
3. Forcing a sync forward by >200 rounds (e.g., to N+300)
4. Observing that incoming secret shares are rejected with "Share from future round" error
5. Confirming that blocks requiring secret shares remain stuck in the coordinator indefinitely

The deterministic nature of this bug makes it reproducible whenever the specified conditions are met in a production environment with encrypted transactions enabled.

## Notes

This vulnerability represents a critical gap in state synchronization logic. While the `end_epoch()` method correctly handles all three managers, the `reset()` method was likely written before the `SecretShareManager` was added, and was never updated to include it. The comments in the code even explicitly mention only "rand and buffer managers" when describing the reset operation, further confirming this oversight.

### Citations

**File:** consensus/src/pipeline/execution_client.rs (L124-176)
```rust
struct BufferManagerHandle {
    pub execute_tx: Option<UnboundedSender<OrderedBlocks>>,
    pub commit_tx:
        Option<aptos_channel::Sender<AccountAddress, (AccountAddress, IncomingCommitRequest)>>,
    pub reset_tx_to_buffer_manager: Option<UnboundedSender<ResetRequest>>,
    pub reset_tx_to_rand_manager: Option<UnboundedSender<ResetRequest>>,
    pub reset_tx_to_secret_share_manager: Option<UnboundedSender<ResetRequest>>,
}

impl BufferManagerHandle {
    pub fn new() -> Self {
        Self {
            execute_tx: None,
            commit_tx: None,
            reset_tx_to_buffer_manager: None,
            reset_tx_to_rand_manager: None,
            reset_tx_to_secret_share_manager: None,
        }
    }

    pub fn init(
        &mut self,
        execute_tx: UnboundedSender<OrderedBlocks>,
        commit_tx: aptos_channel::Sender<AccountAddress, (AccountAddress, IncomingCommitRequest)>,
        reset_tx_to_buffer_manager: UnboundedSender<ResetRequest>,
        reset_tx_to_rand_manager: Option<UnboundedSender<ResetRequest>>,
        maybe_reset_tx_to_secret_share_manager: Option<UnboundedSender<ResetRequest>>,
    ) {
        self.execute_tx = Some(execute_tx);
        self.commit_tx = Some(commit_tx);
        self.reset_tx_to_buffer_manager = Some(reset_tx_to_buffer_manager);
        self.reset_tx_to_rand_manager = reset_tx_to_rand_manager;
        self.reset_tx_to_secret_share_manager = maybe_reset_tx_to_secret_share_manager;
    }

    pub fn reset(
        &mut self,
    ) -> (
        Option<UnboundedSender<ResetRequest>>,
        Option<UnboundedSender<ResetRequest>>,
        Option<UnboundedSender<ResetRequest>>,
    ) {
        let reset_tx_to_rand_manager = self.reset_tx_to_rand_manager.take();
        let reset_tx_to_buffer_manager = self.reset_tx_to_buffer_manager.take();
        let reset_tx_to_secret_share_manager = self.reset_tx_to_secret_share_manager.take();
        self.execute_tx = None;
        self.commit_tx = None;
        (
            reset_tx_to_rand_manager,
            reset_tx_to_buffer_manager,
            reset_tx_to_secret_share_manager,
        )
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L311-365)
```rust
    fn make_coordinator(
        mut rand_manager_input_tx: UnboundedSender<OrderedBlocks>,
        mut rand_ready_block_rx: UnboundedReceiver<OrderedBlocks>,
        mut secret_share_manager_input_tx: UnboundedSender<OrderedBlocks>,
        mut secret_ready_block_rx: UnboundedReceiver<OrderedBlocks>,
    ) -> (
        UnboundedSender<OrderedBlocks>,
        futures_channel::mpsc::UnboundedReceiver<OrderedBlocks>,
    ) {
        let (ordered_block_tx, mut ordered_block_rx) = unbounded::<OrderedBlocks>();
        let (mut ready_block_tx, ready_block_rx) = unbounded::<OrderedBlocks>();

        tokio::spawn(async move {
            let mut inflight_block_tracker: HashMap<
                HashValue,
                (
                    OrderedBlocks,
                    /* rand_ready */ bool,
                    /* secret ready */ bool,
                ),
            > = HashMap::new();
            loop {
                let entry = select! {
                    Some(ordered_blocks) = ordered_block_rx.next() => {
                        let _ = rand_manager_input_tx.send(ordered_blocks.clone()).await;
                        let _ = secret_share_manager_input_tx.send(ordered_blocks.clone()).await;
                        let first_block_id = ordered_blocks.ordered_blocks.first().expect("Cannot be empty").id();
                        inflight_block_tracker.insert(first_block_id, (ordered_blocks, false, false));
                        inflight_block_tracker.entry(first_block_id)
                    },
                    Some(rand_ready_block) = rand_ready_block_rx.next() => {
                        let first_block_id = rand_ready_block.ordered_blocks.first().expect("Cannot be empty").id();
                        inflight_block_tracker.entry(first_block_id).and_modify(|result| {
                            result.1 = true;
                        })
                    },
                    Some(secret_ready_block) = secret_ready_block_rx.next() => {
                        let first_block_id = secret_ready_block.ordered_blocks.first().expect("Cannot be empty").id();
                        inflight_block_tracker.entry(first_block_id).and_modify(|result| {
                            result.2 = true;
                        })
                    },
                };
                let Entry::Occupied(o) = entry else {
                    unreachable!("Entry must exist");
                };
                if o.get().1 && o.get().2 {
                    let (_, (ordered_blocks, _, _)) = o.remove_entry();
                    let _ = ready_block_tx.send(ordered_blocks).await;
                }
            }
        });

        (ordered_block_tx, ready_block_rx)
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L399-436)
```rust
        ) = match (rand_config, secret_sharing_config) {
            (Some(rand_config), Some(secret_sharing_config)) => {
                let (rand_manager_input_tx, rand_ready_block_rx, reset_tx_to_rand_manager) = self
                    .make_rand_manager(
                        &epoch_state,
                        fast_rand_config,
                        rand_msg_rx,
                        highest_committed_round,
                        &network_sender,
                        rand_config,
                        consensus_sk,
                    );

                let (
                    secret_share_manager_input_tx,
                    secret_ready_block_rx,
                    reset_tx_to_secret_share_manager,
                ) = self.make_secret_sharing_manager(
                    &epoch_state,
                    secret_sharing_config,
                    secret_sharing_msg_rx,
                    highest_committed_round,
                    &network_sender,
                );

                let (ordered_block_tx, ready_block_rx) = Self::make_coordinator(
                    rand_manager_input_tx,
                    rand_ready_block_rx,
                    secret_share_manager_input_tx,
                    secret_ready_block_rx,
                );

                (
                    ordered_block_tx,
                    ready_block_rx,
                    Some(reset_tx_to_rand_manager),
                    Some(reset_tx_to_secret_share_manager),
                )
```

**File:** consensus/src/pipeline/execution_client.rs (L642-672)
```rust
    async fn sync_for_duration(
        &self,
        duration: Duration,
    ) -> Result<LedgerInfoWithSignatures, StateSyncError> {
        fail_point!("consensus::sync_for_duration", |_| {
            Err(anyhow::anyhow!("Injected error in sync_for_duration").into())
        });

        // Sync for the specified duration
        let result = self.execution_proxy.sync_for_duration(duration).await;

        // Reset the rand and buffer managers to the new synced round
        if let Ok(latest_synced_ledger_info) = &result {
            self.reset(latest_synced_ledger_info).await?;
        }

        result
    }

    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });

        // Reset the rand and buffer managers to the target round
        self.reset(&target).await?;

        // TODO: handle the state sync error (e.g., re-push the ordered
        // blocks to the buffer manager when it's reset but sync fails).
        self.execution_proxy.sync_to_target(target).await
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L674-709)
```rust
    async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
        let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager) = {
            let handle = self.handle.read();
            (
                handle.reset_tx_to_rand_manager.clone(),
                handle.reset_tx_to_buffer_manager.clone(),
            )
        };

        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        }

        if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
            // reset execution phase and commit phase
            let (tx, rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::ResetDropped)?;
            rx.await.map_err(|_| Error::ResetDropped)?;
        }

        Ok(())
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L711-760)
```rust
    async fn end_epoch(&self) {
        let (
            reset_tx_to_rand_manager,
            reset_tx_to_buffer_manager,
            reset_tx_to_secret_share_manager,
        ) = {
            let mut handle = self.handle.write();
            handle.reset()
        };

        if let Some(mut tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop rand manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop rand manager");
        }

        if let Some(mut tx) = reset_tx_to_secret_share_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop secret share manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop secret share manager");
        }

        if let Some(mut tx) = reset_tx_to_buffer_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop buffer manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop buffer manager");
        }
        self.execution_proxy.end_epoch();
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L172-184)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.secret_share_store
            .lock()
            .update_highest_known_round(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/rand/rand_gen/types.rs (L26-26)
```rust
pub const FUTURE_ROUNDS_TO_ACCEPT: u64 = 200;
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L237-275)
```rust
    pub fn add_self_share(&mut self, share: SecretShare) -> anyhow::Result<()> {
        assert!(
            self.self_author == share.author,
            "Only self shares can be added with metadata"
        );
        let peer_weights = self.secret_share_config.get_peer_weights();
        let metadata = share.metadata();
        ensure!(metadata.epoch == self.epoch, "Share from different epoch");
        ensure!(
            metadata.round <= self.highest_known_round + FUTURE_ROUNDS_TO_ACCEPT,
            "Share from future round"
        );

        let item = self
            .secret_share_map
            .entry(metadata.round)
            .or_insert_with(|| SecretShareItem::new(self.self_author));
        item.add_share_with_metadata(share, peer_weights)?;
        item.try_aggregate(&self.secret_share_config, self.decision_tx.clone());
        Ok(())
    }

    pub fn add_share(&mut self, share: SecretShare) -> anyhow::Result<bool> {
        let weight = self.secret_share_config.get_peer_weight(share.author());
        let metadata = share.metadata();
        ensure!(metadata.epoch == self.epoch, "Share from different epoch");
        ensure!(
            metadata.round <= self.highest_known_round + FUTURE_ROUNDS_TO_ACCEPT,
            "Share from future round"
        );

        let item = self
            .secret_share_map
            .entry(metadata.round)
            .or_insert_with(|| SecretShareItem::new(self.self_author));
        item.add_share(share, weight)?;
        item.try_aggregate(&self.secret_share_config, self.decision_tx.clone());
        Ok(item.has_decision())
    }
```

**File:** consensus/src/pipeline/decryption_pipeline_builder.rs (L115-119)
```rust
        let maybe_decryption_key = secret_shared_key_rx
            .await
            .expect("decryption key should be available");
        // TODO(ibalajiarun): account for the case where decryption key is not available
        let decryption_key = maybe_decryption_key.expect("decryption key should be available");
```
