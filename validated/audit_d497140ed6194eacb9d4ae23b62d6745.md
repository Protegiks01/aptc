# Audit Report

## Title
Reliable Broadcast Panic on Empty Receiver List in Randomness Consensus

## Summary
The `multicast()` function in the reliable broadcast library panics when called with an empty receivers list, executing the `unreachable!()` macro. The randomness consensus subsystems (rand_gen and secret_sharing) have code paths where filtered validator lists can become empty due to race conditions in normal operation, causing validator node crashes.

## Finding Description

The reliable broadcast library's `multicast()` function contains a vulnerability when invoked with an empty receivers list. [1](#0-0) 

When the receivers vector is empty, the loop at lines 164-166 adds no futures to `rpc_futures`. The subsequent `tokio::select!` block at line 168 has both `rpc_futures` and `aggregate_futures` empty. When both `FuturesUnordered` are empty, both `.next()` calls immediately return `None`, causing the `else` branch at line 203 to execute: `unreachable!("Should aggregate with all responses")`. This macro causes a panic, crashing the validator node process.

**Vulnerable Path 1 - Randomness Generation:**

The randomness generation manager spawns a share request task with a fixed 300ms delay, then filters validators to exclude those who already have shares. [2](#0-1) 

At line 274, the task sleeps for 300ms. At lines 275-283, it retrieves existing shares and filters the validator set to create a `targets` list excluding validators who have already submitted shares. If all validators submitted shares proactively during the 300ms window, the `existing_shares` set contains all validators, making `targets` empty. The code logs `targets.len()` at line 288 but does not check if the list is empty before calling `multicast()` at line 290, causing a panic.

**Vulnerable Path 2 - Secret Sharing:**

The secret sharing manager has identical logic with the same 300ms delay and filtering pattern. [3](#0-2) 

At line 248, there is a 300ms sleep. Lines 249-257 filter validators to exclude those with existing shares. Line 262 logs `targets.len()` but does not prevent calling `multicast()` at line 264 with an empty list, causing the same panic.

**JWK Consensus Protection:**

JWK consensus uses `broadcast()` instead of `multicast()`, which always uses the full validator list as receivers. [4](#0-3)  The `broadcast()` function delegates to `multicast()` but always passes the complete validator set as receivers. [5](#0-4) 

## Impact Explanation

**Severity: Medium (up to $10,000)**

This vulnerability causes temporary liveness issues aligning with the Medium severity category in the Aptos bug bounty program:

- **Validator node crashes**: The panic terminates the validator process, requiring manual restart
- **Randomness consensus disruption**: Affects the randomness generation subsystem critical for validator selection and fairness  
- **Multiple simultaneous crashes**: If network conditions align, multiple validators could crash simultaneously
- **NO consensus safety violations**: Does not enable double-spending or state divergence
- **NO funds loss**: Cannot be exploited to steal or manipulate funds
- **Temporary impact**: Recoverable through node restart

This fits the "Limited Protocol Violations" category under Medium severity: temporary liveness issues requiring manual intervention without affecting consensus safety or funds.

## Likelihood Explanation

**Likelihood: Medium**

This is a race condition triggerable in normal operation without malicious action:

1. When a block is committed, validators immediately broadcast their shares proactively
2. The share request task spawns with a fixed 300ms delay
3. In fast network conditions with responsive validators, all shares can be received and processed within 300ms
4. When the delayed task executes, `existing_shares` contains all validators
5. After filtering, `targets` is empty
6. Calling `multicast()` with empty list causes panic

**Factors increasing likelihood:**
- Small validator sets (4-10 validators = faster aggregation)
- Low network latency (< 100ms round-trip time)
- High validator performance (fast share generation and processing)
- The fixed 300ms delay creates a predictable race window

**No attacker action required** - this occurs naturally under favorable network conditions.

## Recommendation

Add empty list checks before calling `multicast()` in both randomness and secret sharing managers:

In `rand_manager.rs` at line 283, after creating the targets list:
```rust
if targets.is_empty() {
    info!("All shares already received, skipping multicast");
    return Ok(());
}
```

In `secret_share_manager.rs` at line 257, after creating the targets list:
```rust
if targets.is_empty() {
    info!("All shares already received, skipping multicast");
    return;
}
```

Alternatively, modify `multicast()` to handle empty receivers gracefully by returning immediately or completing successfully when the list is empty, rather than panicking.

## Proof of Concept

This can be reproduced in a test environment by:

1. Setting up a small validator set (4-5 nodes)
2. Configuring low network latency (< 50ms)
3. Enabling randomness consensus
4. Observing validator logs for the race condition where all shares arrive within the 300ms window
5. When this occurs, the delayed multicast task will panic with "Should aggregate with all responses"

The vulnerability requires integration testing with actual network conditions rather than a simple unit test, as it depends on timing between proactive share broadcasting and the delayed request task.

**Notes**

The vulnerability is real and exploitable in production environments with favorable network conditions. However, it represents a liveness issue rather than a safety violation. Validators will crash and require restart, temporarily disrupting consensus participation, but no funds are at risk and consensus safety properties remain intact. The severity assessment of Medium is appropriate given the temporary and recoverable nature of the impact.

### Citations

**File:** crates/reliable-broadcast/src/lib.rs (L92-102)
```rust
    pub fn broadcast<S: BroadcastStatus<Req, Res> + 'static>(
        &self,
        message: S::Message,
        aggregating: S,
    ) -> impl Future<Output = anyhow::Result<S::Aggregated>> + 'static + use<S, Req, TBackoff, Res>
    where
        <<S as BroadcastStatus<Req, Res>>::Response as TryFrom<Res>>::Error: Debug,
    {
        let receivers: Vec<_> = self.validators.clone();
        self.multicast(message, aggregating, receivers)
    }
```

**File:** crates/reliable-broadcast/src/lib.rs (L104-207)
```rust
    pub fn multicast<S: BroadcastStatus<Req, Res> + 'static>(
        &self,
        message: S::Message,
        aggregating: S,
        receivers: Vec<Author>,
    ) -> impl Future<Output = anyhow::Result<S::Aggregated>> + 'static + use<S, Req, TBackoff, Res>
    where
        <<S as BroadcastStatus<Req, Res>>::Response as TryFrom<Res>>::Error: Debug,
    {
        let network_sender = self.network_sender.clone();
        let time_service = self.time_service.clone();
        let rpc_timeout_duration = self.rpc_timeout_duration;
        let mut backoff_policies: HashMap<Author, TBackoff> = self
            .validators
            .iter()
            .cloned()
            .map(|author| (author, self.backoff_policy.clone()))
            .collect();
        let executor = self.executor.clone();
        let self_author = self.self_author;
        async move {
            let message: Req = message.into();

            let peers = receivers.clone();
            let sender = network_sender.clone();
            let message_clone = message.clone();
            let protocols = Arc::new(
                tokio::task::spawn_blocking(move || {
                    sender.to_bytes_by_protocol(peers, message_clone)
                })
                .await??,
            );

            let send_message = |receiver, sleep_duration: Option<Duration>| {
                let network_sender = network_sender.clone();
                let time_service = time_service.clone();
                let message = message.clone();
                let protocols = protocols.clone();
                async move {
                    if let Some(duration) = sleep_duration {
                        time_service.sleep(duration).await;
                    }
                    let send_fut = if receiver == self_author {
                        network_sender.send_rb_rpc(receiver, message, rpc_timeout_duration)
                    } else if let Some(raw_message) = protocols.get(&receiver).cloned() {
                        network_sender.send_rb_rpc_raw(receiver, raw_message, rpc_timeout_duration)
                    } else {
                        network_sender.send_rb_rpc(receiver, message, rpc_timeout_duration)
                    };
                    (receiver, send_fut.await)
                }
                .boxed()
            };

            let mut rpc_futures = FuturesUnordered::new();
            let mut aggregate_futures = FuturesUnordered::new();

            let mut receivers = receivers;
            network_sender.sort_peers_by_latency(&mut receivers);

            for receiver in receivers {
                rpc_futures.push(send_message(receiver, None));
            }
            loop {
                tokio::select! {
                    Some((receiver, result)) = rpc_futures.next() => {
                        let aggregating = aggregating.clone();
                        let future = executor.spawn(async move {
                            (
                                    receiver,
                                    result
                                        .and_then(|msg| {
                                            msg.try_into().map_err(|e| anyhow::anyhow!("{:?}", e))
                                        })
                                        .and_then(|ack| aggregating.add(receiver, ack)),
                            )
                        }).await;
                        aggregate_futures.push(future);
                    },
                    Some(result) = aggregate_futures.next() => {
                        let (receiver, result) = result.expect("spawned task must succeed");
                        match result {
                            Ok(may_be_aggragated) => {
                                if let Some(aggregated) = may_be_aggragated {
                                    return Ok(aggregated);
                                }
                            },
                            Err(e) => {
                                log_rpc_failure(e, receiver);

                                let backoff_strategy = backoff_policies
                                    .get_mut(&receiver)
                                    .expect("should be present");
                                let duration = backoff_strategy.next().expect("should produce value");
                                rpc_futures
                                    .push(send_message(receiver, Some(duration)));
                            },
                        }
                    },
                    else => unreachable!("Should aggregate with all responses")
                }
            }
        }
    }
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L263-303)
```rust
    fn spawn_aggregate_shares_task(&self, metadata: RandMetadata) -> DropGuard {
        let rb = self.reliable_broadcast.clone();
        let aggregate_state = Arc::new(ShareAggregateState::new(
            self.rand_store.clone(),
            metadata.clone(),
            self.config.clone(),
        ));
        let epoch_state = self.epoch_state.clone();
        let round = metadata.round;
        let rand_store = self.rand_store.clone();
        let task = async move {
            tokio::time::sleep(Duration::from_millis(300)).await;
            let maybe_existing_shares = rand_store.lock().get_all_shares_authors(round);
            if let Some(existing_shares) = maybe_existing_shares {
                let epoch = epoch_state.epoch;
                let request = RequestShare::new(metadata.clone());
                let targets = epoch_state
                    .verifier
                    .get_ordered_account_addresses_iter()
                    .filter(|author| !existing_shares.contains(author))
                    .collect::<Vec<_>>();
                info!(
                    epoch = epoch,
                    round = round,
                    "[RandManager] Start broadcasting share request for {}",
                    targets.len(),
                );
                rb.multicast(request, aggregate_state, targets)
                    .await
                    .expect("Broadcast cannot fail");
                info!(
                    epoch = epoch,
                    round = round,
                    "[RandManager] Finish broadcasting share request",
                );
            }
        };
        let (abort_handle, abort_registration) = AbortHandle::new_pair();
        tokio::spawn(Abortable::new(task, abort_registration));
        DropGuard::new(abort_handle)
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L237-277)
```rust
    fn spawn_share_requester_task(&self, metadata: SecretShareMetadata) -> DropGuard {
        let rb = self.reliable_broadcast.clone();
        let aggregate_state = Arc::new(SecretShareAggregateState::new(
            self.secret_share_store.clone(),
            metadata.clone(),
            self.config.clone(),
        ));
        let epoch_state = self.epoch_state.clone();
        let secret_share_store = self.secret_share_store.clone();
        let task = async move {
            // TODO(ibalajiarun): Make this configurable
            tokio::time::sleep(Duration::from_millis(300)).await;
            let maybe_existing_shares = secret_share_store.lock().get_all_shares_authors(&metadata);
            if let Some(existing_shares) = maybe_existing_shares {
                let epoch = epoch_state.epoch;
                let request = RequestSecretShare::new(metadata.clone());
                let targets = epoch_state
                    .verifier
                    .get_ordered_account_addresses_iter()
                    .filter(|author| !existing_shares.contains(author))
                    .collect::<Vec<_>>();
                info!(
                    epoch = epoch,
                    round = metadata.round,
                    "[SecretShareManager] Start broadcasting share request for {}",
                    targets.len(),
                );
                rb.multicast(request, aggregate_state, targets)
                    .await
                    .expect("Broadcast cannot fail");
                info!(
                    epoch = epoch,
                    round = metadata.round,
                    "[SecretShareManager] Finish broadcasting share request",
                );
            }
        };
        let (abort_handle, abort_registration) = AbortHandle::new_pair();
        tokio::spawn(Abortable::new(task, abort_registration));
        DropGuard::new(abort_handle)
    }
```

**File:** crates/aptos-jwk-consensus/src/update_certifier.rs (L67-69)
```rust
        let task = async move {
            let qc_update = rb.broadcast(req, agg_state).await.expect("cannot fail");
            ConsensusMode::log_certify_done(epoch, &qc_update);
```
