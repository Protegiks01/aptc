# Audit Report

## Title
Stale Epoch Pending Node Causes Validator Liveness Failure During Epoch Transitions

## Summary
The DAG consensus implementation stores a single pending node in ConsensusDB without epoch information in the database key. During validator restarts across epoch transitions, if a stale pending node's round coincidentally matches the expected round in the new epoch, the validator broadcasts a wrong-epoch node that gets rejected by all peers, causing temporary loss of validator participation until automatic recovery through peer progress.

## Finding Description

The DAG consensus uses `NodeSchema` to persist a pending node for crash recovery. The schema stores the node using an empty tuple `()` as the database key, with no epoch information in the key itself. [1](#0-0) 

When a validator creates a node, it saves it as pending before broadcasting. [2](#0-1) 

During validator initialization, the pending node is retrieved and the recovery logic attempts to resume broadcasting if the round matches. [3](#0-2) 

**The Critical Flaw**: The recovery condition only validates the round number, not the epoch: [4](#0-3) 

The `ConsensusDB` persists across epoch transitions without clearing the pending node. When a new epoch starts, the same ConsensusDB instance is reused: [5](#0-4) 

While votes are cleaned up by epoch during initialization [6](#0-5) , there is no corresponding cleanup for the pending node. The `delete_pending_node()` method exists in the trait [7](#0-6)  but is never invoked anywhere in the codebase.

**Attack Scenario:**
1. Validator crashes in epoch N with a pending node stored
2. Network progresses to epoch N+1 while validator is offline
3. Validator restarts in epoch N+1
4. If the stale node's round equals `highest_strong_links_round + 1` in the new epoch, recovery condition passes
5. Validator broadcasts the stale node with epoch=N metadata
6. All peers reject it because their epoch validation fails: [8](#0-7) 
7. The validator cannot collect votes for a certificate
8. Automatic recovery occurs when the validator receives certified nodes from other validators, triggering round advancement: [9](#0-8) 

The epoch validation in `DagStore` also rejects wrong-epoch nodes: [10](#0-9) 

## Impact Explanation

This qualifies as **Medium Severity** per Aptos bug bounty criteria for the following reasons:

**State Inconsistencies Requiring Intervention**: The affected validator cannot participate in consensus until automatic recovery occurs through receiving certified nodes from other validators. This matches the Medium severity category of "Limited Protocol Violations - State inconsistencies requiring manual intervention, Limited funds loss or manipulation, Temporary liveness issues."

**Not Critical** because:
- No funds are at risk
- No permanent damage occurs
- No consensus safety violation (Byzantine fault tolerance ensures network continues)
- Automatic recovery mechanism exists through peer progress

**Not High** because:
- Only affects individual validators temporarily
- Does not affect the entire network
- Network can tolerate f faulty validators without liveness loss

If multiple validators crash simultaneously during epoch transitions (e.g., coordinated upgrades, infrastructure failures), the compounding effect could temporarily degrade network performance, but the 2f+1 Byzantine fault tolerance ensures consensus continues as long as fewer than 1/3 of validators are affected.

## Likelihood Explanation

**Likelihood: Medium**

Required conditions:
1. **Validator crash with pending node stored**: Common during validator upgrades, restarts, or infrastructure issues
2. **Epoch transition while validator is offline**: Epochs transition regularly (approximately every 2 hours on Aptos mainnet)
3. **Stale round matches `new_epoch_highest_round + 1`**: Low individual probability, but possible since rounds continue across epochs

While the third condition has low probability in any single instance, the cumulative probability over many epoch transitions and validators means this issue will manifest periodically in production. The impact is amplified during network-wide events such as coordinated upgrades or infrastructure failures where multiple validators may restart simultaneously near epoch boundaries.

## Recommendation

**Immediate Fix**: Add epoch validation to the pending node recovery logic:

```rust
// In DagDriver::new(), replace the current filter with:
if let Some(node) = pending_node.filter(|node| 
    node.epoch() == epoch_state.epoch && 
    node.round() == highest_strong_links_round + 1
) {
    // Resume broadcasting
    driver.broadcast_node(node);
} else {
    // Delete stale pending node if epoch mismatch
    storage.delete_pending_node().ok();
    // Start new round
    if !driver.dag.read().is_empty() {
        block_on(driver.enter_new_round(highest_strong_links_round + 1));
    }
}
```

**Complete Fix**: Implement proper cleanup during epoch transitions by calling `delete_pending_node()` when starting a new epoch, similar to how votes are cleaned up.

## Proof of Concept

The vulnerability can be demonstrated through the following scenario:

1. Start a validator with DAG consensus enabled
2. Trigger a pending node to be saved (create and begin broadcasting a node)
3. Crash the validator before the node completes broadcasting
4. Wait for an epoch transition
5. Restart the validator when `highest_strong_links_round + 1` in the new epoch equals the stale node's round
6. Observe that the validator broadcasts a wrong-epoch node
7. Monitor peer rejection logs showing epoch mismatch errors
8. Verify temporary loss of validator participation until recovery

## Notes

This vulnerability affects the DAG consensus implementation specifically. The issue stems from an incomplete epoch transition cleanup mechanism where votes are properly filtered by epoch during initialization, but the pending node storage lacks similar epoch-aware cleanup. The automatic recovery through peer-driven round advancement prevents permanent liveness loss, but validators experience a participation gap that could impact consensus performance during the recovery period.

### Citations

**File:** consensus/src/consensusdb/schema/dag/mod.rs (L22-22)
```rust
define_schema!(NodeSchema, (), Node, NODE_CF_NAME);
```

**File:** consensus/src/dag/dag_driver.rs (L90-128)
```rust
        let pending_node = storage
            .get_pending_node()
            .expect("should be able to read dag storage");
        let highest_strong_links_round =
            dag.read().highest_strong_links_round(&epoch_state.verifier);

        let driver = Self {
            author,
            epoch_state,
            dag,
            payload_client,
            reliable_broadcast,
            time_service,
            rb_handles: Mutex::new(BoundedVecDeque::new(window_size_config as usize)),
            storage,
            order_rule,
            fetch_requester,
            ledger_info_provider,
            round_state,
            window_size_config,
            payload_config,
            health_backoff,
            quorum_store_enabled,
            allow_batches_without_pos_in_proposal,
        };

        // If we were broadcasting the node for the round already, resume it
        if let Some(node) =
            pending_node.filter(|node| node.round() == highest_strong_links_round + 1)
        {
            debug!(
                LogSchema::new(LogEvent::NewRound).round(node.round()),
                "Resume round"
            );
            driver
                .round_state
                .set_current_round(node.round())
                .expect("must succeed");
            driver.broadcast_node(node);
```

**File:** consensus/src/dag/dag_driver.rs (L314-317)
```rust
        self.storage
            .save_pending_node(&new_node)
            .expect("node must be saved");
        self.broadcast_node(new_node);
```

**File:** consensus/src/epoch_manager.rs (L1479-1484)
```rust
        let dag_storage = Arc::new(StorageAdapter::new(
            epoch,
            epoch_to_validators,
            self.storage.consensus_db(),
            self.storage.aptos_db(),
        ));
```

**File:** consensus/src/dag/rb_handler.rs (L113-118)
```rust
        ensure!(
            node.epoch() == self.epoch_state.epoch,
            "different epoch {}, current {}",
            node.epoch(),
            self.epoch_state.epoch
        );
```

**File:** consensus/src/dag/rb_handler.rs (L188-211)
```rust
fn read_votes_from_storage(
    storage: &Arc<dyn DAGStorage>,
    epoch: u64,
) -> BTreeMap<u64, BTreeMap<Author, Vote>> {
    let mut votes_by_round_peer = BTreeMap::new();

    let all_votes = storage.get_votes().unwrap_or_default();
    let mut to_delete = vec![];
    for (node_id, vote) in all_votes {
        if node_id.epoch() == epoch {
            votes_by_round_peer
                .entry(node_id.round())
                .or_insert_with(BTreeMap::new)
                .insert(*node_id.author(), vote);
        } else {
            to_delete.push(node_id);
        }
    }
    if let Err(err) = storage.delete_votes(to_delete) {
        error!("unable to clear old signatures: {}", err);
    }

    votes_by_round_peer
}
```

**File:** consensus/src/dag/adapter.rs (L351-353)
```rust
    fn delete_pending_node(&self) -> anyhow::Result<()> {
        Ok(self.consensus_db.delete::<NodeSchema>(vec![()])?)
    }
```

**File:** consensus/src/dag/round_state.rs (L33-53)
```rust
    pub fn check_for_new_round(
        &self,
        highest_strong_links_round: Round,
        strong_links: Vec<NodeCertificate>,
        minimum_delay: Duration,
    ) {
        let current_round = *self.current_round.lock();
        match current_round.cmp(&highest_strong_links_round) {
            // we're behind, move forward immediately
            Ordering::Less => {
                // the receiver can be dropped if we move to a new epoch
                let _ = self.event_sender.send(highest_strong_links_round + 1);
            },
            Ordering::Equal => self.responsive_check.check_for_new_round(
                highest_strong_links_round,
                strong_links,
                minimum_delay,
            ),
            Ordering::Greater => (),
        }
    }
```

**File:** consensus/src/dag/dag_store.rs (L129-134)
```rust
        ensure!(
            node.epoch() == self.epoch_state.epoch,
            "different epoch {}, current {}",
            node.epoch(),
            self.epoch_state.epoch
        );
```
