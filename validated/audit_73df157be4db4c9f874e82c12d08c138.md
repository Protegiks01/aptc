# Audit Report

## Title
Consensus Safety Violation: On-Disk Persistent Storage Lacks fsync, Allows Vote Forgetting and Equivocation After Crash

## Summary
The `OnDiskStorage` backend used in production validator configurations lacks `fsync` calls during critical safety data persistence, allowing validators to forget their most recent vote after a crash and potentially re-sign conflicting votes for the same round, enabling consensus equivocation.

## Finding Description

The vulnerability exists in the `OnDiskStorage` implementation's `write()` method, which persists consensus safety data without synchronizing file contents to disk. [1](#0-0) 

The write operation creates a temporary file, writes JSON data, and atomically renames it to the target file, but **never calls fsync**, `sync_all()`, or `sync_data()`. This means the file contents may remain in OS buffers and be lost on crash.

This storage backend is used by consensus SafetyRules to persist critical voting state. When a validator votes, it updates `SafetyData` containing `last_voted_round` and `last_vote`: [2](#0-1) 

The `set_safety_data()` method persists this data through the internal storage backend: [3](#0-2) 

The safety check prevents double-voting by verifying the new round is greater than `last_voted_round`: [4](#0-3) 

**Exploitation Path:**
1. Validator votes for round R, calling `set_safety_data()` to persist `{last_voted_round: R, last_vote: Vote_A}`
2. `OnDiskStorage.write()` writes to temp file and renames without fsync
3. System crashes before OS buffer flush (power loss, kill -9, kernel panic)
4. On restart, disk contains old state `{last_voted_round: R-1}`
5. Validator receives new proposal for round R, safety check passes (R > R-1), signs Vote_B
6. **Result: Two different votes signed for round R = equivocation**

**Critical Evidence:** Production validator configurations explicitly use `on_disk_storage`: [5](#0-4) [6](#0-5) 

Despite the README warning that OnDiskStorage "should not be used in production": [7](#0-6) 

The configuration sanitizer only rejects `InMemoryStorage` for mainnet, but **allows `OnDiskStorage`**: [8](#0-7) 

While equivocation detection exists at the vote aggregation layer: [9](#0-8) 

This is defensive - it detects equivocation **after** the validator has already violated the safety invariant by signing two conflicting votes.

## Impact Explanation

**Critical Severity** per Aptos bug bounty criteria - Category: "Consensus/Safety Violations"

- **Direct consensus safety violation**: Validators can sign conflicting votes for the same round, breaking the fundamental BFT safety property that requires < 1/3 Byzantine validators for safety
- **Chain forks possible**: With sufficient simultaneous crashes (datacenter power failure), multiple validators could equivocate, potentially creating conflicting quorum certificates
- **Double-spending risk**: Conflicting blocks with valid signatures enable double-spending scenarios
- **Non-recoverable if widespread**: Multiple validators equivocating simultaneously could require hard fork recovery

The atomic rename provides atomicity but **not durability** - a critical distinction in distributed consensus systems where crash-recovery semantics are essential. The fact that the configuration sanitizer explicitly allows `OnDiskStorage` for mainnet (while rejecting `InMemoryStorage`) indicates it is considered production-ready by the codebase, making this a real deployment risk.

## Likelihood Explanation

**High Likelihood:**

1. **Natural occurrence**: Node crashes from power failures, OOM kills, kernel panics, hardware failures happen regularly in production validator operations
2. **No attacker control needed**: Natural crashes trigger the vulnerability without requiring any malicious action
3. **Standard OS behavior**: Write caching and delayed flush to disk is default behavior across all operating systems
4. **Production deployment**: The vulnerable configuration is used in standard validator deployment templates (Docker Compose and Terraform/Helm)
5. **Window of vulnerability**: Typical OS buffer flush delays (5-30 seconds) create a significant exposure window where votes can be lost
6. **Not prevented by sanitizer**: Unlike `InMemoryStorage`, `OnDiskStorage` is explicitly allowed for mainnet validators

## Recommendation

Add `fsync`/`sync_all()` call to the `OnDiskStorage::write()` method before the rename operation:

```rust
fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
    let contents = serde_json::to_vec(data)?;
    let mut file = File::create(self.temp_path.path())?;
    file.write_all(&contents)?;
    file.sync_all()?;  // Add this line to ensure durability
    fs::rename(&self.temp_path, &self.file_path)?;
    Ok(())
}
```

Additionally, consider:
1. Adding a configuration sanitizer check to reject `OnDiskStorage` for mainnet validators (similar to `InMemoryStorage`)
2. Requiring Vault storage backend for production mainnet validators
3. Adding integration tests that verify crash-recovery semantics of safety data persistence

## Proof of Concept

A complete PoC would require:
1. Starting a validator with `OnDiskStorage` backend
2. Having it vote on round R
3. Sending SIGKILL to the process before OS buffer flush (within 5-30 seconds)
4. Restarting the validator
5. Presenting a different proposal for round R
6. Observing the validator sign a conflicting vote

The vulnerability can be demonstrated by examining the write path and OS behavior, showing that without `fsync`, crash-before-flush scenarios will lose the most recent vote data.

## Notes

This vulnerability represents a fundamental violation of consensus safety properties. While equivocation detection exists as a defensive measure, it only identifies the problem after the validator has already broken the safety invariant by signing two conflicting votes for the same round. The BFT protocol assumes < 1/3 Byzantine validators, but this vulnerability allows honest validators to accidentally violate safety through natural crashes, effectively creating Byzantine behavior without actual malicious intent.

The presence of `OnDiskStorage` in production configuration templates and its explicit acceptance by the mainnet configuration sanitizer indicates this is a real deployment risk that warrants immediate attention.

### Citations

**File:** secure/storage/src/on_disk.rs (L64-70)
```rust
    fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
        let contents = serde_json::to_vec(data)?;
        let mut file = File::create(self.temp_path.path())?;
        file.write_all(&contents)?;
        fs::rename(&self.temp_path, &self.file_path)?;
        Ok(())
    }
```

**File:** consensus/consensus-types/src/safety_data.rs (L10-21)
```rust
pub struct SafetyData {
    pub epoch: u64,
    pub last_voted_round: u64,
    // highest 2-chain round, used for 3-chain
    pub preferred_round: u64,
    // highest 1-chain round, used for 2-chain
    #[serde(default)]
    pub one_chain_round: u64,
    pub last_vote: Option<Vote>,
    #[serde(default)]
    pub highest_timeout_round: u64,
}
```

**File:** consensus/safety-rules/src/persistent_safety_storage.rs (L150-170)
```rust
    pub fn set_safety_data(&mut self, data: SafetyData) -> Result<(), Error> {
        let _timer = counters::start_timer("set", SAFETY_DATA);
        counters::set_state(counters::EPOCH, data.epoch as i64);
        counters::set_state(counters::LAST_VOTED_ROUND, data.last_voted_round as i64);
        counters::set_state(
            counters::HIGHEST_TIMEOUT_ROUND,
            data.highest_timeout_round as i64,
        );
        counters::set_state(counters::PREFERRED_ROUND, data.preferred_round as i64);

        match self.internal_store.set(SAFETY_DATA, data.clone()) {
            Ok(_) => {
                self.cached_safety_data = Some(data);
                Ok(())
            },
            Err(error) => {
                self.cached_safety_data = None;
                Err(Error::SecureStorageUnexpectedError(error.to_string()))
            },
        }
    }
```

**File:** consensus/safety-rules/src/safety_rules.rs (L213-232)
```rust
    pub(crate) fn verify_and_update_last_vote_round(
        &self,
        round: Round,
        safety_data: &mut SafetyData,
    ) -> Result<(), Error> {
        if round <= safety_data.last_voted_round {
            return Err(Error::IncorrectLastVotedRound(
                round,
                safety_data.last_voted_round,
            ));
        }

        safety_data.last_voted_round = round;
        trace!(
            SafetyLogSchema::new(LogEntry::LastVotedRound, LogEvent::Update)
                .last_voted_round(safety_data.last_voted_round)
        );

        Ok(())
    }
```

**File:** docker/compose/aptos-node/validator.yaml (L11-14)
```yaml
    backend:
      type: "on_disk_storage"
      path: secure-data.json
      namespace: ~
```

**File:** terraform/helm/aptos-node/files/configs/validator-base.yaml (L14-17)
```yaml
    backend:
      type: "on_disk_storage"
      path: secure-data.json
      namespace: ~
```

**File:** secure/storage/README.md (L37-42)
```markdown
- `OnDisk`: Similar to InMemory, the OnDisk secure storage implementation provides another
useful testing implementation: an on-disk storage engine, where the storage backend is
implemented using a single file written to local disk. In a similar fashion to the in-memory
storage, on-disk should not be used in production environments as it provides no security
guarantees (e.g., encryption before writing to disk). Moreover, OnDisk storage does not
currently support concurrent data accesses.
```

**File:** config/src/config/safety_rules_config.rs (L85-96)
```rust
        if let Some(chain_id) = chain_id {
            // Verify that the secure backend is appropriate for mainnet validators
            if chain_id.is_mainnet()
                && node_type.is_validator()
                && safety_rules_config.backend.is_in_memory()
            {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "The secure backend should not be set to in memory storage in mainnet!"
                        .to_string(),
                ));
            }
```

**File:** consensus/src/pending_votes.rs (L287-308)
```rust
        if let Some((previously_seen_vote, previous_li_digest)) =
            self.author_to_vote.get(&vote.author())
        {
            // is it the same vote?
            if &li_digest == previous_li_digest {
                // we've already seen an equivalent vote before
                let new_timeout_vote = vote.is_timeout() && !previously_seen_vote.is_timeout();
                if !new_timeout_vote {
                    // it's not a new timeout vote
                    return VoteReceptionResult::DuplicateVote;
                }
            } else {
                // we have seen a different vote for the same round
                error!(
                    SecurityEvent::ConsensusEquivocatingVote,
                    remote_peer = vote.author(),
                    vote = vote,
                    previous_vote = previously_seen_vote
                );

                return VoteReceptionResult::EquivocateVote;
            }
```
