# Audit Report

## Title
State Snapshot Restore Completes Successfully with Incomplete Merkle Tree Due to Missing Root Hash Verification

## Summary
The `finish_impl()` method in `JellyfishMerkleRestore` fails to verify that the final computed root hash matches the expected root hash after state snapshot restoration. This allows incomplete state trees to be written to storage when backup manifests are missing chunks, causing state inconsistency across nodes.

## Finding Description

During state snapshot restoration, the system processes chunks from a backup manifest and reconstructs a Jellyfish Merkle tree. The critical security guarantee is that the final restored tree must match the `expected_root_hash` verified against the ledger info.

**The Vulnerability:**

The `expected_root_hash` field is stored in the `JellyfishMerkleRestore` struct but is only used during per-chunk verification, not during finalization. [1](#0-0) 

The field is used in the `verify()` method to validate each chunk as it's added: [2](#0-1) 

However, when `finish_impl()` is called, it freezes remaining partial nodes and writes them to storage without verifying the final root hash: [3](#0-2) 

The method simply returns `Ok(())` after writing nodes to storage at line 788, with no final root hash verification.

**Attack Scenario:**

1. A malicious backup provider creates a manifest listing only a subset of chunks (e.g., chunks 1-5 of 10).

2. The restore controller processes all chunks listed in the manifest: [4](#0-3) 

3. Each chunk passes verification individually because `SparseMerkleRangeProof` proofs include right siblings representing unprocessed portions: [5](#0-4) 

4. When all manifest chunks complete, `finish()` is called: [6](#0-5) 

5. The incomplete tree (containing only keys from chunks 1-5) is frozen and written to storage. The frozen tree's root hash equals `hash(chunks_1_to_5)`, which does NOT equal `expected_root_hash` (which represents the complete tree including chunks 6-10).

6. No verification catches this discrepancy - there is no subsequent validation in the restore coordinator or in `finalize_state_snapshot()`.

**Defense in Depth Failure:**

The only check occurs on subsequent restart when `JellyfishMerkleRestore::new()` verifies the root hash: [7](#0-6) 

This means the incomplete tree remains in storage until the node restarts, during which time the node could attempt to use the incorrect state.

## Impact Explanation

**MEDIUM Severity** (Note: Report claims HIGH, but this assessment aligns with Aptos bug bounty criteria)

This vulnerability causes state consistency violations requiring manual intervention:

1. **State Inconsistency**: Nodes restoring from incomplete manifests produce different state trees than nodes with complete state. When these nodes attempt consensus participation, they produce different state roots, causing consensus validation failures that force re-synchronization.

2. **Validator Onboarding Issues**: New validators using compromised backup sources would have incorrect state, requiring manual detection and re-synchronization from trusted sources.

3. **Limited Scope**: The impact is limited to nodes using the malicious backup source. Consensus mechanisms would detect state root mismatches, preventing the incorrect state from propagating network-wide. Affected nodes can recover by re-syncing from correct backup sources.

4. **Recovery Complexity**: While recoverable through re-synchronization, the vulnerability could delay network recovery during disaster scenarios when multiple nodes need rapid state restoration.

This qualifies as **MEDIUM severity** per Aptos bug bounty categories ("State inconsistencies requiring manual intervention"), NOT CRITICAL, because:
- It does not cause permanent fund loss
- It does not cause network-wide partitions
- It does not cause non-recoverable state corruption
- Affected nodes can re-sync from correct backups
- Detection occurs on restart (lines 200-205)

## Likelihood Explanation

**HIGH Likelihood** - The vulnerability is easily exploitable:

1. **No Privileged Access Required**: Backup storage providers are external entities not listed as trusted roles in the Aptos threat model. Any entity hosting backup data could provide incomplete manifests.

2. **Simple Attack Vector**: The exploit requires only providing a backup manifest with missing chunks - no complex timing, race conditions, or cryptographic breaks needed.

3. **No Manifest Completeness Validation**: The system processes whatever chunks are listed in the manifest without validating completeness: [8](#0-7) 

4. **Real-World Scenarios**: Beyond malicious actors, legitimate backup corruption during transmission or storage could trigger this vulnerability.

## Recommendation

Add final root hash verification in `finish_impl()` before writing nodes to storage:

```rust
pub fn finish_impl(mut self) -> Result<()> {
    self.wait_for_async_commit()?;
    
    // ... existing special case handling ...
    
    self.freeze(0);
    
    // ADDED: Verify final root hash before writing
    let root_node_key = NodeKey::new_empty_path(self.version);
    if let Some(root_node) = self.frozen_nodes.get(&root_node_key) {
        ensure!(
            root_node.hash() == self.expected_root_hash,
            "Final root hash mismatch. Actual: {:x}, Expected: {:x}",
            root_node.hash(),
            self.expected_root_hash
        );
    } else {
        return Err(anyhow!("Root node not found after freezing"));
    }
    
    self.store.write_node_batch(&self.frozen_nodes)?;
    Ok(())
}
```

This ensures incomplete trees are rejected immediately rather than being written to storage and only detected on restart.

## Proof of Concept

While a full PoC would require setting up backup infrastructure, the vulnerability can be demonstrated by:

1. Creating a `StateSnapshotBackup` manifest with only a subset of chunks
2. Calling `StateSnapshotRestoreController::run()` with this incomplete manifest
3. Observing that `finish_impl()` completes successfully without root hash verification
4. Verifying the stored root node has a different hash than `expected_root_hash`
5. Confirming the mismatch is only detected on node restart when `JellyfishMerkleRestore::new()` is called

The code analysis clearly shows the missing verification step in the finalization path.

## Notes

**Severity Clarification**: While the report claims HIGH severity, this vulnerability aligns more closely with MEDIUM severity per Aptos bug bounty categories. It causes "state inconsistencies requiring manual intervention" rather than critical impacts like fund loss or permanent network partitions. The defense-in-depth check on restart (lines 200-205) provides partial mitigation, though it's not immediate.

**Key Technical Points**:
- The vulnerability exists because `SparseMerkleRangeProof` right siblings are used for verification only, not stored in the tree
- The frozen tree contains only nodes from actually added chunks
- Different incomplete manifests produce different root hashes
- Consensus validation would detect mismatches, preventing propagation

### Citations

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L176-176)
```rust
    expected_root_hash: HashValue,
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L196-206)
```rust
        let (finished, partial_nodes, previous_leaf) = if let Some(root_node) =
            tree_reader.get_node_option(&NodeKey::new_empty_path(version), "restore")?
        {
            info!("Previous restore is complete, checking root hash.");
            ensure!(
                root_node.hash() == expected_root_hash,
                "Previous completed restore has root hash {}, expecting {}",
                root_node.hash(),
                expected_root_hash,
            );
            (true, vec![], None)
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L624-697)
```rust
    /// Verifies that all states that have been added so far (from the leftmost one to
    /// `self.previous_leaf`) are correct, i.e., we are able to construct `self.expected_root_hash`
    /// by combining all existing states and `proof`.
    #[allow(clippy::collapsible_if)]
    fn verify(&self, proof: SparseMerkleRangeProof) -> Result<()> {
        let previous_leaf = self
            .previous_leaf
            .as_ref()
            .expect("The previous leaf must exist.");

        let previous_key = previous_leaf.account_key();
        // If we have all siblings on the path from root to `previous_key`, we should be able to
        // compute the root hash. The siblings on the right are already in the proof. Now we
        // compute the siblings on the left side, which represent all the states that have ever
        // been added.
        let mut left_siblings = vec![];

        // The following process might add some extra placeholder siblings on the left, but it is
        // nontrivial to determine when the loop should stop. So instead we just add these
        // siblings for now and get rid of them in the next step.
        let mut num_visited_right_siblings = 0;
        for (i, bit) in previous_key.iter_bits().enumerate() {
            if bit {
                // This node is a right child and there should be a sibling on the left.
                let sibling = if i >= self.partial_nodes.len() * 4 {
                    *SPARSE_MERKLE_PLACEHOLDER_HASH
                } else {
                    Self::compute_left_sibling(
                        &self.partial_nodes[i / 4],
                        previous_key.get_nibble(i / 4),
                        (3 - i % 4) as u8,
                    )
                };
                left_siblings.push(sibling);
            } else {
                // This node is a left child and there should be a sibling on the right.
                num_visited_right_siblings += 1;
            }
        }
        ensure!(
            num_visited_right_siblings >= proof.right_siblings().len(),
            "Too many right siblings in the proof.",
        );

        // Now we remove any extra placeholder siblings at the bottom. We keep removing the last
        // sibling if 1) it's a placeholder 2) it's a sibling on the left.
        for bit in previous_key.iter_bits().rev() {
            if bit {
                if *left_siblings.last().expect("This sibling must exist.")
                    == *SPARSE_MERKLE_PLACEHOLDER_HASH
                {
                    left_siblings.pop();
                } else {
                    break;
                }
            } else if num_visited_right_siblings > proof.right_siblings().len() {
                num_visited_right_siblings -= 1;
            } else {
                break;
            }
        }

        // Left siblings must use the same ordering as the right siblings in the proof
        left_siblings.reverse();

        // Verify the proof now that we have all the siblings
        proof
            .verify(
                self.expected_root_hash,
                SparseMerkleLeafNode::new(*previous_key, previous_leaf.value_hash()),
                left_siblings,
            )
            .map_err(Into::into)
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L750-788)
```rust
    pub fn finish_impl(mut self) -> Result<()> {
        self.wait_for_async_commit()?;
        // Deal with the special case when the entire tree has a single leaf or null node.
        if self.partial_nodes.len() == 1 {
            let mut num_children = 0;
            let mut leaf = None;
            for i in 0..16 {
                if let Some(ref child_info) = self.partial_nodes[0].children[i] {
                    num_children += 1;
                    if let ChildInfo::Leaf(node) = child_info {
                        leaf = Some(node.clone());
                    }
                }
            }

            match num_children {
                0 => {
                    let node_key = NodeKey::new_empty_path(self.version);
                    assert!(self.frozen_nodes.is_empty());
                    self.frozen_nodes.insert(node_key, Node::Null);
                    self.store.write_node_batch(&self.frozen_nodes)?;
                    return Ok(());
                },
                1 => {
                    if let Some(node) = leaf {
                        let node_key = NodeKey::new_empty_path(self.version);
                        assert!(self.frozen_nodes.is_empty());
                        self.frozen_nodes.insert(node_key, node.into());
                        self.store.write_node_batch(&self.frozen_nodes)?;
                        return Ok(());
                    }
                },
                _ => (),
            }
        }

        self.freeze(0);
        self.store.write_node_batch(&self.frozen_nodes)?;
        Ok(())
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L165-182)
```rust
        let resume_point_opt = receiver.lock().as_mut().unwrap().previous_key_hash()?;
        let chunks = if let Some(resume_point) = resume_point_opt {
            manifest
                .chunks
                .into_iter()
                .skip_while(|chunk| chunk.last_key <= resume_point)
                .collect()
        } else {
            manifest.chunks
        };
        if chunks.len() < total_chunks {
            info!(
                chunks_to_add = chunks.len(),
                total_chunks = total_chunks,
                "Resumed state snapshot restore."
            )
        };
        let chunks_to_add = chunks.len();
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L186-226)
```rust
        let storage = self.storage.clone();
        let futs_iter = chunks.into_iter().enumerate().map(|(chunk_idx, chunk)| {
            let storage = storage.clone();
            async move {
                tokio::spawn(async move {
                    let blobs = Self::read_state_value(&storage, chunk.blobs.clone()).await?;
                    let proof = storage.load_bcs_file(&chunk.proof).await?;
                    Result::<_>::Ok((chunk_idx, chunk, blobs, proof))
                })
                .await?
            }
        });
        let con = self.concurrent_downloads;
        let mut futs_stream = stream::iter(futs_iter).buffered_x(con * 2, con);
        let mut start = None;
        while let Some((chunk_idx, chunk, mut blobs, proof)) = futs_stream.try_next().await? {
            start = start.or_else(|| Some(Instant::now()));
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["add_state_chunk"]);
            let receiver = receiver.clone();
            if self.validate_modules {
                blobs = tokio::task::spawn_blocking(move || {
                    Self::validate_modules(&blobs);
                    blobs
                })
                .await?;
            }
            tokio::task::spawn_blocking(move || {
                receiver.lock().as_mut().unwrap().add_chunk(blobs, proof)
            })
            .await??;
            leaf_idx.set(chunk.last_idx as i64);
            info!(
                chunk = chunk_idx,
                chunks_to_add = chunks_to_add,
                last_idx = chunk.last_idx,
                values_per_second = ((chunk.last_idx + 1 - start_idx) as f64
                    / start.as_ref().unwrap().elapsed().as_secs_f64())
                    as u64,
                "State chunk added.",
            );
        }
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L228-230)
```rust
        tokio::task::spawn_blocking(move || receiver.lock().take().unwrap().finish()).await??;
        self.run_mode.finish();
        Ok(())
```
