# Audit Report

## Title
Atomic Pruning Violation: Parallel Sub-Pruner Failures Cause Foreign Key Integrity Violations in Sharded Storage Mode

## Summary
The LedgerPruner's parallel execution of sub-pruners in sharded storage mode lacks atomic commit coordination. When one sub-pruner fails after others have already committed to their separate RocksDB instances, orphaned data remains that references deleted transactions, violating database referential integrity and breaking state synchronization.

## Finding Description

This is a **logic vulnerability** in the storage layer's pruning coordination mechanism that affects the default production configuration.

**Storage Sharding Architecture:** Storage sharding is enabled by default and explicitly required for production networks. [1](#0-0)  The system panics if sharding is not explicitly enabled for mainnet/testnet deployments. [2](#0-1) 

When storage sharding is enabled, each ledger component is stored in a separate physical RocksDB instance. The system opens distinct database instances at separate paths for transaction_db, write_set_db, event_db, and other components. [3](#0-2)  Notably, there is an unresolved TODO comment acknowledging the need to handle data inconsistency. [4](#0-3) 

**Parallel Execution Without Atomic Coordination:** The LedgerPruner executes all 7 sub-pruners in parallel using rayon's `par_iter().try_for_each()`. [5](#0-4)  When any sub-pruner returns an error, the `?` operator causes immediate return, preventing progress update. [6](#0-5) 

**Independent Commits:** Each sub-pruner independently commits its deletions and progress metadata to its own separate database instance. TransactionPruner commits to transaction_db, [7](#0-6)  WriteSetPruner commits to write_set_db, [8](#0-7)  and EventStorePruner commits to event_db. [9](#0-8) 

**The Critical Flaw:** Each database's `write_schemas()` method commits atomically within a single RocksDB instance, [10](#0-9)  but there is no distributed transaction mechanism across multiple separate RocksDB instances. When one sub-pruner fails, previously successful sub-pruners have already permanently committed their changes to separate databases with no rollback capability.

**State Synchronization Failure:** The `get_transaction_outputs` method queries all databases for each version, requiring transactions, events, write_sets, and transaction_info to all be present. [11](#0-10)  Each database query uses the `?` operator, so if any data is missing due to partial pruning, the query returns `AptosDbError::NotFound`, breaking state synchronization for new nodes.

**Non-Idempotent Retry Behavior:** The TransactionPruner must read transactions before pruning to extract their hashes for index deletion. [12](#0-11)  If transactions are already deleted in a partial pruning scenario, the retry will fail when trying to read them. The pruner worker logs errors and retries indefinitely, [13](#0-12)  but without updated progress, each retry attempts the same already-partially-pruned range, preventing automatic recovery.

## Impact Explanation

This is a **HIGH severity** vulnerability per Aptos bug bounty criteria:

**State Synchronization Failures:** New nodes attempting to synchronize blockchain state will fail when encountering partially pruned versions. The `get_transaction_outputs` method requires all related data to be present for each version. Partial pruning causes `NotFound` errors, preventing nodes from syncing historical data. This qualifies as "Significant protocol violations" meeting HIGH severity criteria.

**API Crashes:** The REST API and state storage service will return errors when querying partially pruned data, meeting the "API crashes" criteria for HIGH severity under the Aptos bug bounty program.

**Database Referential Integrity Violations:** Write sets and events reference transactions by version number. Partial pruning creates orphaned references where write sets or events exist but their corresponding transactions do not, violating core database consistency invariants.

**Network Liveness Impact:** While not total network failure (which would be CRITICAL), this significantly degrades the network's ability to onboard new validator nodes or recover nodes from snapshots, impacting decentralization and resilience.

## Likelihood Explanation

This vulnerability has **MEDIUM likelihood** of occurring in production:

**Triggering Conditions:** Disk I/O errors (common in cloud environments), database corruption (occurs during crashes or power failures), resource exhaustion (disk full, too many open files), or any transient database write failure in one sub-pruner while others succeed.

**Frequency:** Pruning runs continuously in the background on validator nodes. With 7 independent sub-pruners committing to separate databases in parallel, the probability of at least one failing while others succeed is non-negligible over time. However, infrastructure is generally reliable, making this MEDIUM (not HIGH) likelihood.

**Detection Difficulty:** The error would be logged, but the inconsistent state persists. Nodes continue operating normally for recent versions, but historical queries and new node synchronization will fail unpredictably.

## Recommendation

Implement atomic coordination across sub-pruners using one of these approaches:

1. **Two-Phase Commit Protocol**: Have all sub-pruners prepare their changes first (phase 1), then commit only if all succeed (phase 2). If any fails during prepare, roll back all prepared changes.

2. **Progress-Based Recovery**: Before updating overall progress, verify all sub-pruners reported the same target version. On restart, detect inconsistent sub-pruner progress states and repair by re-running the incomplete sub-pruners.

3. **Shared Transaction Log**: Maintain a ledger-level transaction log that records pruning intent before execution. On failure, use the log to determine which sub-pruners need rollback or completion.

4. **Sequential Execution**: Replace parallel execution with sequential execution to ensure all-or-nothing semantics, though this reduces performance.

## Proof of Concept

A full proof of concept would require simulating disk I/O failures during pruning, which is environment-dependent. However, the vulnerability can be demonstrated by:

1. Starting a pruning operation that processes versions 1000-1100
2. Injecting a failure (e.g., disk full error) into WriteSetPruner after TransactionPruner and EventStorePruner have committed
3. Observing that transactions and events for versions 1000-1100 are deleted while write sets remain
4. Attempting to call `get_transaction_outputs(1050, 10, ledger_version)` and observing `AptosDbError::NotFound`
5. Verifying that retry attempts fail because TransactionPruner cannot read the already-deleted transactions

The code paths validated in this report demonstrate the vulnerability exists in the current codebase architecture.

## Notes

The developers are aware of potential data inconsistency issues in sharded storage, as evidenced by the TODO comment in the LedgerDb initialization code. This vulnerability represents a concrete manifestation of that concern, specifically in the pruning subsystem. The issue is not theoreticalâ€”it can be triggered by realistic infrastructure failures that occur in production environments.

### Citations

**File:** config/src/config/storage_config.rs (L233-233)
```rust
            enable_storage_sharding: true,
```

**File:** config/src/config/storage_config.rs (L664-668)
```rust
            if (chain_id.is_testnet() || chain_id.is_mainnet())
                && config_yaml["rocksdb_configs"]["enable_storage_sharding"].as_bool() != Some(true)
            {
                panic!("Storage sharding (AIP-97) is not enabled in node config. Please follow the guide to migration your node, and set storage.rocksdb_configs.enable_storage_sharding to true explicitly in your node config. https://aptoslabs.notion.site/DB-Sharding-Migration-Public-Full-Nodes-1978b846eb7280b29f17ceee7d480730");
            }
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L183-279)
```rust
        THREAD_MANAGER.get_non_exe_cpu_pool().scope(|s| {
            s.spawn(|_| {
                let event_db_raw = Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(EVENT_DB_NAME),
                        EVENT_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                );
                event_db = Some(EventDb::new(
                    event_db_raw.clone(),
                    EventStore::new(event_db_raw),
                ));
            });
            s.spawn(|_| {
                persisted_auxiliary_info_db = Some(PersistedAuxiliaryInfoDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(PERSISTED_AUXILIARY_INFO_DB_NAME),
                        PERSISTED_AUXILIARY_INFO_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_accumulator_db = Some(TransactionAccumulatorDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_ACCUMULATOR_DB_NAME),
                        TRANSACTION_ACCUMULATOR_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_auxiliary_data_db = Some(TransactionAuxiliaryDataDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_AUXILIARY_DATA_DB_NAME),
                        TRANSACTION_AUXILIARY_DATA_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )))
            });
            s.spawn(|_| {
                transaction_db = Some(TransactionDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_DB_NAME),
                        TRANSACTION_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_info_db = Some(TransactionInfoDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_INFO_DB_NAME),
                        TRANSACTION_INFO_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                write_set_db = Some(WriteSetDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(WRITE_SET_DB_NAME),
                        WRITE_SET_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
        });
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L281-281)
```rust
        // TODO(grao): Handle data inconsistency.
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L78-84)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L86-87)
```rust
            progress = current_batch_target_version;
            self.record_progress(progress);
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L73-73)
```rust
        self.ledger_db.transaction_db().write_schemas(batch)
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L106-131)
```rust
    fn get_pruning_candidate_transactions(
        &self,
        start: Version,
        end: Version,
    ) -> Result<Vec<(Version, Transaction)>> {
        ensure!(end >= start, "{} must be >= {}", end, start);

        let mut iter = self
            .ledger_db
            .transaction_db_raw()
            .iter::<TransactionSchema>()?;
        iter.seek(&start)?;

        // The capacity is capped by the max number of txns we prune in a single batch. It's a
        // relatively small number set in the config, so it won't cause high memory usage here.
        let mut txns = Vec::with_capacity((end - start) as usize);
        for item in iter {
            let (version, txn) = item?;
            if version >= end {
                break;
            }
            txns.push((version, txn));
        }

        Ok(txns)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/write_set_pruner.rs (L32-32)
```rust
        self.ledger_db.write_set_db().write_schemas(batch)
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L80-80)
```rust
        self.ledger_db.event_db().write_schemas(batch)
```

**File:** storage/schemadb/src/lib.rs (L307-309)
```rust
    pub fn write_schemas(&self, batch: impl IntoRawBatch) -> DbResult<()> {
        self.write_schemas_inner(batch, &sync_write_option())
    }
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L394-400)
```rust
                    let txn_info = self
                        .ledger_db
                        .transaction_info_db()
                        .get_transaction_info(version)?;
                    let events = self.ledger_db.event_db().get_events_by_version(version)?;
                    let write_set = self.ledger_db.write_set_db().get_write_set(version)?;
                    let txn = self.ledger_db.transaction_db().get_transaction(version)?;
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L56-64)
```rust
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
```
