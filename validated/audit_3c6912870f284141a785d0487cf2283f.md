# Audit Report

## Title
Proof Version Manipulation DoS Attack via Historical Proof Regeneration Bypass

## Summary
A Byzantine peer can repeatedly request transaction outputs with an arbitrarily old `proof_version` that references pruned accumulator nodes, forcing the storage service to perform expensive I/O operations before failing at proof generation. The validation logic fails to check if the `proof_version` is within the available (non-pruned) range, and because validation passes, the malicious peer is never marked as unhealthy, allowing unlimited repetition of this resource exhaustion attack.

## Finding Description

The `TransactionOutputsWithProofRequest` struct contains a `proof_version` field that specifies which ledger version should be used to generate cryptographic proofs for the requested transaction outputs. [1](#0-0) 

When a storage service receives such a request, it validates the request using the `can_service_transaction_outputs_with_proof` method, which only checks two conditions: whether the transaction outputs in the requested range are available, and whether a proof can be created at the proof_version. [2](#0-1) 

The critical flaw is in the `can_create_proof` method, which only verifies that the `proof_version` is less than or equal to the synced ledger info version. [3](#0-2)  **The validation does NOT check if the `proof_version` is greater than or equal to the minimum available version after pruning.**

The Aptos blockchain implements a transaction accumulator pruner that deletes old accumulator nodes from the database. [4](#0-3)  The LedgerPrunerManager tracks the minimum readable version for all ledger data including accumulator nodes. [5](#0-4) 

After validation passes, the storage service processes the request by fetching all transaction data BEFORE attempting to generate the proof. The critical sequence in `get_transaction_outputs_with_proof_by_size` is:
1. Lines 591-614: Create expensive database iterators for transactions, infos, write sets, events, and auxiliary data
2. Lines 630-696: Perform extensive I/O to fetch all data items
3. Lines 703-708: **Only after all I/O is complete**, attempt to generate the accumulator range proof [6](#0-5) 

When proof generation attempts to read pruned accumulator nodes, the `HashReader::get` implementation returns an error with message "{position} does not exist". [7](#0-6) 

The `get_transaction_accumulator_range_proof` method only checks if the first_version (transaction data) is pruned using `error_if_ledger_pruned`, but it does NOT check if the proof_version (ledger_version parameter) accumulator nodes are pruned. [8](#0-7) 

Because the request passed validation, the peer is NOT marked as unhealthy by the request moderator. The moderator only increments the invalid request count when `storage_server_summary.can_service()` returns false during validation. [9](#0-8)  Processing errors that occur after validation passes do not increment the invalid request count.

The validation flow shows that validation occurs before processing. [10](#0-9) 

**Attack Scenario:**
1. Attacker identifies the current synced version (e.g., 10,000) and knows pruning has occurred for versions < 1,000
2. Attacker sends: `TransactionOutputsWithProofRequest { proof_version: 100, start_version: 5000, end_version: 6000 }`
3. Validation passes (100 â‰¤ 10,000 and outputs [5000-6000] are available)
4. Server performs expensive I/O to fetch ~1000 transaction outputs
5. Proof generation fails (accumulator nodes at version 100 are pruned)
6. Error returned to attacker, but peer remains healthy
7. Attacker repeats indefinitely, causing disk thrashing and CPU exhaustion

## Impact Explanation

This vulnerability enables a resource exhaustion Denial of Service attack against validator nodes and full nodes running the storage service. Each malicious request forces the server to:

- Create multiple database iterators (transactions, infos, write sets, events, auxiliary data)
- Perform disk I/O to fetch potentially thousands of transaction records
- Serialize and deserialize large data structures
- Consume CPU cycles for data processing

Since the malicious peer is never marked as unhealthy (validation passed), they can sustain this attack indefinitely without being rate-limited or blocked. Multiple coordinated attackers could amplify the impact.

This qualifies as **HIGH Severity** per the Aptos Bug Bounty program category "Validator Node Slowdowns" - specifically "DoS through resource exhaustion." The sustained disk I/O and CPU consumption can significantly degrade validator node performance and their ability to participate in consensus.

## Likelihood Explanation

**Likelihood: HIGH**

Attack requirements:
- Knowledge of approximate pruned version range (easily discoverable through trial and error or by querying storage summaries)
- Ability to send network requests to storage service nodes (publicly accessible for state sync)
- No privileged access required
- No economic cost to the attacker

The attack is trivial to execute and can be automated. Any malicious peer can perform this attack with minimal resources. The transaction accumulator pruner is enabled by default, making pruned accumulator nodes a guaranteed condition on any production node.

## Recommendation

Add validation to check if the `proof_version` is within the available (non-pruned) accumulator range:

```rust
fn can_create_proof(&self, proof_version: u64) -> bool {
    self.synced_ledger_info
        .as_ref()
        .map(|li| {
            let synced_version = li.ledger_info().version();
            // Check upper bound (existing)
            if proof_version > synced_version {
                return false;
            }
            // Add check for lower bound (new)
            // The proof_version must be >= the minimum available version
            // This requires access to the storage's min_readable_version
            true
        })
        .unwrap_or(false)
}
```

Additionally, the `get_transaction_accumulator_range_proof` method should validate that the `ledger_version` parameter (proof_version) is >= min_readable_version before attempting proof generation:

```rust
fn get_transaction_accumulator_range_proof(
    &self,
    first_version: Version,
    limit: u64,
    ledger_version: Version,
) -> Result<TransactionAccumulatorRangeProof> {
    gauged_api("get_transaction_accumulator_range_proof", || {
        self.error_if_ledger_pruned("Transaction", first_version)?;
        // Add this check:
        self.error_if_ledger_pruned("AccumulatorProof", ledger_version)?;
        
        self.ledger_db
            .transaction_accumulator_db()
            .get_transaction_range_proof(Some(first_version), limit, ledger_version)
    })
}
```

## Proof of Concept

The vulnerability can be demonstrated by:
1. Running a full node with pruning enabled
2. Waiting for pruning to occur (version N becomes pruned)
3. Sending a `TransactionOutputsWithProofRequest` with `proof_version` < N and valid `start_version`/`end_version` > N
4. Observing expensive I/O operations followed by proof generation failure
5. Verifying the peer is not marked as unhealthy
6. Repeating the request indefinitely

The attack succeeds because validation only checks if `proof_version <= synced_version` and if the transaction output range is available, without verifying that accumulator nodes at `proof_version` are not pruned.

### Citations

**File:** state-sync/storage-service/types/src/requests.rs (L353-357)
```rust
pub struct TransactionOutputsWithProofRequest {
    pub proof_version: u64, // The version the proof should be relative to
    pub start_version: u64, // The starting version of the transaction output list
    pub end_version: u64,   // The ending version of the transaction output list (inclusive)
}
```

**File:** state-sync/storage-service/types/src/responses.rs (L810-816)
```rust
    /// Returns true iff the peer can create a proof for the given version
    fn can_create_proof(&self, proof_version: u64) -> bool {
        self.synced_ledger_info
            .as_ref()
            .map(|li| li.ledger_info().version() >= proof_version)
            .unwrap_or(false)
    }
```

**File:** state-sync/storage-service/types/src/responses.rs (L833-847)
```rust
    fn can_service_transaction_outputs_with_proof(
        &self,
        start_version: u64,
        end_version: u64,
        proof_version: u64,
    ) -> bool {
        let desired_range = match CompleteDataRange::new(start_version, end_version) {
            Ok(desired_range) => desired_range,
            Err(_) => return false,
        };

        let can_service_outputs = self.can_service_transaction_outputs(&desired_range);
        let can_create_proof = self.can_create_proof(proof_version);
        can_service_outputs && can_create_proof
    }
```

**File:** storage/aptosdb/src/ledger_db/transaction_accumulator_db.rs (L149-172)
```rust
    pub(crate) fn prune(begin: Version, end: Version, db_batch: &mut SchemaBatch) -> Result<()> {
        for version_to_delete in begin..end {
            db_batch.delete::<TransactionAccumulatorRootHashSchema>(&version_to_delete)?;
            // The even version will be pruned in the iteration of version + 1.
            if version_to_delete % 2 == 0 {
                continue;
            }

            let first_ancestor_that_is_a_left_child =
                Self::find_first_ancestor_that_is_a_left_child(version_to_delete);

            // This assertion is true because we skip the leaf nodes with address which is a
            // a multiple of 2.
            assert!(!first_ancestor_that_is_a_left_child.is_leaf());

            let mut current = first_ancestor_that_is_a_left_child;
            while !current.is_leaf() {
                db_batch.delete::<TransactionAccumulatorSchema>(&current.left_child())?;
                db_batch.delete::<TransactionAccumulatorSchema>(&current.right_child())?;
                current = current.right_child();
            }
        }
        Ok(())
    }
```

**File:** storage/aptosdb/src/ledger_db/transaction_accumulator_db.rs (L195-200)
```rust
impl HashReader for TransactionAccumulatorDb {
    fn get(&self, position: Position) -> Result<HashValue, anyhow::Error> {
        self.db
            .get::<TransactionAccumulatorSchema>(&position)?
            .ok_or_else(|| anyhow!("{} does not exist.", position))
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L33-35)
```rust
    /// The minimal readable version for the ledger data.
    min_readable_version: AtomicVersion,
}
```

**File:** state-sync/storage-service/server/src/storage.rs (L591-708)
```rust
        // Get the iterators for the transaction, info, write set, events,
        // auxiliary data and persisted auxiliary infos.
        let transaction_iterator = self
            .storage
            .get_transaction_iterator(start_version, num_outputs_to_fetch)?;
        let transaction_info_iterator = self
            .storage
            .get_transaction_info_iterator(start_version, num_outputs_to_fetch)?;
        let transaction_write_set_iterator = self
            .storage
            .get_write_set_iterator(start_version, num_outputs_to_fetch)?;
        let transaction_events_iterator = self
            .storage
            .get_events_iterator(start_version, num_outputs_to_fetch)?;
        let persisted_auxiliary_info_iterator = self
            .storage
            .get_persisted_auxiliary_info_iterator(start_version, num_outputs_to_fetch as usize)?;
        let mut multizip_iterator = itertools::multizip((
            transaction_iterator,
            transaction_info_iterator,
            transaction_write_set_iterator,
            transaction_events_iterator,
            persisted_auxiliary_info_iterator,
        ));

        // Initialize the fetched data items
        let mut transactions_and_outputs = vec![];
        let mut transaction_infos = vec![];
        let mut persisted_auxiliary_infos = vec![];

        // Create a response progress tracker
        let mut response_progress_tracker = ResponseDataProgressTracker::new(
            num_outputs_to_fetch,
            max_response_size,
            self.config.max_storage_read_wait_time_ms,
            self.time_service.clone(),
        );

        // Fetch as many transaction outputs as possible
        while !response_progress_tracker.is_response_complete() {
            match multizip_iterator.next() {
                Some((
                    Ok(transaction),
                    Ok(info),
                    Ok(write_set),
                    Ok(events),
                    Ok(persisted_auxiliary_info),
                )) => {
                    // Create the transaction output
                    let output = TransactionOutput::new(
                        write_set,
                        events,
                        info.gas_used(),
                        info.status().clone().into(),
                        TransactionAuxiliaryData::None, // Auxiliary data is no longer supported
                    );

                    // Calculate the number of serialized bytes for the data items
                    let num_transaction_bytes = get_num_serialized_bytes(&transaction)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_info_bytes = get_num_serialized_bytes(&info)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_output_bytes = get_num_serialized_bytes(&output)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_auxiliary_info_bytes =
                        get_num_serialized_bytes(&persisted_auxiliary_info).map_err(|error| {
                            Error::UnexpectedErrorEncountered(error.to_string())
                        })?;

                    // Add the data items to the lists
                    let total_serialized_bytes = num_transaction_bytes
                        + num_info_bytes
                        + num_output_bytes
                        + num_auxiliary_info_bytes;
                    if response_progress_tracker.data_items_fits_in_response(
                        !is_transaction_or_output_request,
                        total_serialized_bytes,
                    ) {
                        transactions_and_outputs.push((transaction, output));
                        transaction_infos.push(info);
                        persisted_auxiliary_infos.push(persisted_auxiliary_info);

                        response_progress_tracker.add_data_item(total_serialized_bytes);
                    } else {
                        break; // Cannot add any more data items
                    }
                },
                Some((Err(error), _, _, _, _))
                | Some((_, Err(error), _, _, _))
                | Some((_, _, Err(error), _, _))
                | Some((_, _, _, Err(error), _))
                | Some((_, _, _, _, Err(error))) => {
                    return Err(Error::StorageErrorEncountered(error.to_string()));
                },
                None => {
                    // Log a warning that the iterators did not contain all the expected data
                    warn!(
                        "The iterators for transactions, transaction infos, write sets, events, \
                        auxiliary data and persisted auxiliary infos are missing data! Start version: {:?}, \
                        end version: {:?}, num outputs to fetch: {:?}, num fetched: {:?}.",
                        start_version, end_version, num_outputs_to_fetch, transactions_and_outputs.len()
                    );
                    break;
                },
            }
        }

        // Create the transaction output list with proof
        let num_fetched_outputs = transactions_and_outputs.len();
        let accumulator_range_proof = if num_fetched_outputs == 0 {
            AccumulatorRangeProof::new_empty() // Return an empty proof if no outputs were fetched
        } else {
            self.storage.get_transaction_accumulator_range_proof(
                start_version,
                num_fetched_outputs as u64,
                proof_version,
            )?
        };
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L548-560)
```rust
    fn get_transaction_accumulator_range_proof(
        &self,
        first_version: Version,
        limit: u64,
        ledger_version: Version,
    ) -> Result<TransactionAccumulatorRangeProof> {
        gauged_api("get_transaction_accumulator_range_proof", || {
            self.error_if_ledger_pruned("Transaction", first_version)?;

            self.ledger_db
                .transaction_accumulator_db()
                .get_transaction_range_proof(Some(first_version), limit, ledger_version)
        })
```

**File:** state-sync/storage-service/server/src/moderator.rs (L154-185)
```rust
            // Verify the request is serviceable using the current storage server summary
            if !storage_server_summary.can_service(
                &self.aptos_data_client_config,
                self.time_service.clone(),
                request,
            ) {
                // Increment the invalid request count for the peer
                let mut unhealthy_peer_state = self
                    .unhealthy_peer_states
                    .entry(*peer_network_id)
                    .or_insert_with(|| {
                        // Create a new unhealthy peer state (this is the first invalid request)
                        let max_invalid_requests =
                            self.storage_service_config.max_invalid_requests_per_peer;
                        let min_time_to_ignore_peers_secs =
                            self.storage_service_config.min_time_to_ignore_peers_secs;
                        let time_service = self.time_service.clone();

                        UnhealthyPeerState::new(
                            max_invalid_requests,
                            min_time_to_ignore_peers_secs,
                            time_service,
                        )
                    });
                unhealthy_peer_state.increment_invalid_request_count(peer_network_id);

                // Return the validation error
                return Err(Error::InvalidRequest(format!(
                    "The given request cannot be satisfied. Request: {:?}, storage summary: {:?}",
                    request, storage_server_summary
                )));
            }
```

**File:** state-sync/storage-service/server/src/handler.rs (L206-229)
```rust
    fn validate_and_handle_request(
        &self,
        peer_network_id: &PeerNetworkId,
        request: &StorageServiceRequest,
    ) -> Result<StorageServiceResponse, Error> {
        // Validate the request with the moderator
        self.request_moderator
            .validate_request(peer_network_id, request)?;

        // Process the request
        match &request.data_request {
            DataRequest::GetServerProtocolVersion => {
                let data_response = self.get_server_protocol_version();
                StorageServiceResponse::new(data_response, request.use_compression)
                    .map_err(|error| error.into())
            },
            DataRequest::GetStorageServerSummary => {
                let data_response = self.get_storage_server_summary();
                StorageServiceResponse::new(data_response, request.use_compression)
                    .map_err(|error| error.into())
            },
            _ => self.process_cachable_request(peer_network_id, request),
        }
    }
```
