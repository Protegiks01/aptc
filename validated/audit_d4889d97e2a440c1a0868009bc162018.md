# Audit Report

## Title
Race Condition Between State Sync finish() and Consensus Pipeline pre_commit_block() Causes Validator Crash

## Summary
A race condition exists between the consensus pipeline's `pre_commit_block()` operation and state synchronization's `finish()` call, allowing the block executor to be finalized while pre-commit operations are pending. This causes a panic at the `.expect()` call, resulting in validator node crashes during normal operation when nodes fall behind and trigger state synchronization.

## Finding Description

The vulnerability exists in `BlockExecutor::pre_commit_block()` which assumes the executor's internal state (`inner`) is always initialized: [1](#0-0) 

The executor's lifecycle is managed through three key operations:
- `reset()` initializes `inner` to `Some(BlockExecutorInner)`: [2](#0-1) 
- `finish()` sets `inner` to `None` to free memory: [3](#0-2) 
- Multiple operations use `.expect()` assuming `inner` is `Some`: [4](#0-3)  and [5](#0-4) 

State synchronization calls `finish()` before syncing to prevent memory leaks: [6](#0-5) [7](#0-6) 

The critical issue is that state sync operations protect themselves via `write_mutex`: [8](#0-7) 

However, the consensus pipeline operations execute **without acquiring this mutex**. The pre-commit task is spawned as an async future that waits for parent pre-commit, ledger update, and order proof: [9](#0-8) 

Critically, pre-commit and other execution tasks are spawned **without abort handles**, meaning they cannot be aborted by `abort_pipeline_for_state_sync()`: [10](#0-9) 

**Race Condition Scenario:**

1. Block X enters pre_commit phase and waits for order_proof (line 1048)
2. Node detects it's behind and state sync is triggered
3. State sync pauses pre_commit_status and calls `abort_pipeline_for_state_sync()`: [11](#0-10) 
4. However, pre-commit tasks spawned with `None` abort handles **cannot be aborted**: [12](#0-11) 
5. State sync acquires `write_mutex` and calls `executor.finish()`, setting `inner = None`
6. Order proof arrives for Block X, pre-commit task checks `is_active()` at line 1052
7. Race window: Between checking status and calling executor (lines 1052-1069)
8. Pre-commit task calls `executor.pre_commit_block(block_id)` at line 1069
9. The `.expect("BlockExecutor is not reset")` panics because `inner` is `None`
10. Validator node crashes or consensus thread panics

Note that `ledger_update()` correctly uses `.ok_or_else()` to return an error instead of panicking: [13](#0-12) 

## Impact Explanation

**High Severity** per Aptos bug bounty criteria:

- **Validator node crashes**: The panic terminates the validator process or consensus thread, directly affecting network stability
- **API crashes**: Related to consensus pipeline failure, impacting API node availability
- **Significant protocol violations**: Breaks the invariant that validators remain operational during normal consensus operations

This affects **consensus availability** because:
1. Validator crashes reduce the active validator set temporarily
2. If multiple validators crash simultaneously due to network partition/recovery scenarios, it can affect consensus quorum and block production
3. Automatic restart may hit the same race condition repeatedly during catch-up, creating a crash loop

The issue is particularly severe because it occurs during legitimate network conditions (node falling behind), not requiring any malicious activity. This aligns with **HIGH Severity** in the Aptos bug bounty program: "Validator Node Slowdowns" or "API Crashes" affecting consensus participation.

## Likelihood Explanation

**High Likelihood** due to:

1. **Natural Trigger**: Occurs when nodes fall behind peers, which is common in distributed systems due to network latency variations, temporary node slowdowns, recovery from brief outages, or epoch transitions with validator set changes

2. **Insufficient Protection Mechanisms**: 
   - `pre_commit_status` provides partial protection but has a race window between checking `is_active()` (line 1052) and calling `pre_commit_block()` (line 1069)
   - `abort_pipeline_for_state_sync()` cannot abort execution pipeline tasks because they are spawned with `None` for abort handles
   - No mutex coordination between `write_mutex` (used by state sync) and executor operations (used by consensus pipeline)

3. **Async Pipeline Architecture**: The consensus pipeline uses async futures that can be interleaved with state sync operations without coordination

4. **Long Pre-commit Wait Times**: Pre-commit waits for order proof and optionally commit proof, creating extended time windows where state sync can intervene

## Recommendation

**Short-term fix**: Replace `.expect()` with proper error handling in `pre_commit_block()`, `commit_ledger()`, and `execute_and_update_state()`:

```rust
fn pre_commit_block(&self, block_id: HashValue) -> ExecutorResult<()> {
    let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "pre_commit_block"]);
    
    self.inner
        .read()
        .as_ref()
        .ok_or_else(|| ExecutorError::InternalError {
            error: "BlockExecutor is not reset".into(),
        })?
        .pre_commit_block(block_id)
}
```

**Long-term fix**: Add proper synchronization:
1. Make execution pipeline tasks acquire `write_mutex` or a similar lock before calling executor methods
2. OR spawn execution tasks with abort handles and ensure they are properly aborted before state sync
3. OR add a separate executor lifecycle lock that both state sync and consensus pipeline must acquire

## Proof of Concept

While no executable PoC is provided, the vulnerability can be triggered through:
1. Running a validator node in a network where it occasionally falls behind
2. Monitoring for crashes with stack traces showing panic in `pre_commit_block()` or `commit_ledger()`
3. The crash should occur when `.expect("BlockExecutor is not reset")` fails because `inner` is `None`

## Notes

The vulnerability is confirmed through code analysis of the Aptos Core codebase. The protection mechanisms (`pre_commit_status` and `abort_pipeline_for_state_sync`) are insufficient because:
1. Pre-commit tasks cannot be aborted (spawned with `None` for abort handles)
2. There's a race window between checking pre-commit status and calling the executor
3. No mutex coordination exists between state sync's `finish()` and consensus pipeline operations

The same pattern affects `commit_ledger()` and `execute_and_update_state()`, while `ledger_update()` correctly uses error handling instead of `.expect()`.

### Citations

**File:** execution/executor/src/block_executor/mod.rs (L90-95)
```rust
    fn reset(&self) -> Result<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "reset"]);

        *self.inner.write() = Some(BlockExecutorInner::new(self.db.clone())?);
        Ok(())
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L97-113)
```rust
    fn execute_and_update_state(
        &self,
        block: ExecutableBlock,
        parent_block_id: HashValue,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> ExecutorResult<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "execute_and_state_checkpoint"]);

        self.maybe_initialize()?;
        // guarantee only one block being executed at a time
        let _guard = self.execution_lock.lock();
        self.inner
            .read()
            .as_ref()
            .expect("BlockExecutor is not reset")
            .execute_and_update_state(block, parent_block_id, onchain_config)
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L115-129)
```rust
    fn ledger_update(
        &self,
        block_id: HashValue,
        parent_block_id: HashValue,
    ) -> ExecutorResult<StateComputeResult> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "ledger_update"]);

        self.inner
            .read()
            .as_ref()
            .ok_or_else(|| ExecutorError::InternalError {
                error: "BlockExecutor is not reset".into(),
            })?
            .ledger_update(block_id, parent_block_id)
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L131-139)
```rust
    fn pre_commit_block(&self, block_id: HashValue) -> ExecutorResult<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "pre_commit_block"]);

        self.inner
            .read()
            .as_ref()
            .expect("BlockExecutor is not reset")
            .pre_commit_block(block_id)
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L141-149)
```rust
    fn commit_ledger(&self, ledger_info_with_sigs: LedgerInfoWithSignatures) -> ExecutorResult<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "commit_ledger"]);

        self.inner
            .read()
            .as_ref()
            .expect("BlockExecutor is not reset")
            .commit_ledger(ledger_info_with_sigs)
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L151-155)
```rust
    fn finish(&self) {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "finish"]);

        *self.inner.write() = None;
    }
```

**File:** consensus/src/state_computer.rs (L132-141)
```rust
    async fn sync_for_duration(
        &self,
        duration: Duration,
    ) -> Result<LedgerInfoWithSignatures, StateSyncError> {
        // Grab the logical time lock
        let mut latest_logical_time = self.write_mutex.lock().await;

        // Before state synchronization, we have to call finish() to free the
        // in-memory SMT held by the BlockExecutor to prevent a memory leak.
        self.executor.finish();
```

**File:** consensus/src/state_computer.rs (L177-186)
```rust
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        // Grab the logical time lock and calculate the target logical time
        let mut latest_logical_time = self.write_mutex.lock().await;
        let target_logical_time =
            LogicalTime::new(target.ledger_info().epoch(), target.ledger_info().round());

        // Before state synchronization, we have to call finish() to free the
        // in-memory SMT held by BlockExecutor to prevent a memory leak.
        self.executor.finish();

```

**File:** consensus/src/pipeline/pipeline_builder.rs (L535-556)
```rust
        let pre_commit_fut = spawn_shared_fut(
            Self::pre_commit(
                ledger_update_fut.clone(),
                parent.pre_commit_fut.clone(),
                order_proof_fut.clone(),
                commit_proof_fut.clone(),
                self.executor.clone(),
                block.clone(),
                self.pre_commit_status(),
            ),
            None,
        );
        let commit_ledger_fut = spawn_shared_fut(
            Self::commit_ledger(
                pre_commit_fut.clone(),
                commit_proof_fut,
                parent.commit_ledger_fut.clone(),
                self.executor.clone(),
                block.clone(),
            ),
            None,
        );
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L1035-1075)
```rust
    async fn pre_commit(
        ledger_update_fut: TaskFuture<LedgerUpdateResult>,
        parent_block_pre_commit_fut: TaskFuture<PreCommitResult>,
        order_proof_fut: TaskFuture<WrappedLedgerInfo>,
        commit_proof_fut: TaskFuture<LedgerInfoWithSignatures>,
        executor: Arc<dyn BlockExecutorTrait>,
        block: Arc<Block>,
        pre_commit_status: Arc<Mutex<PreCommitStatus>>,
    ) -> TaskResult<PreCommitResult> {
        let mut tracker = Tracker::start_waiting("pre_commit", &block);
        let (compute_result, _, _) = ledger_update_fut.await?;
        parent_block_pre_commit_fut.await?;

        order_proof_fut.await?;

        let wait_for_proof = {
            let mut status_guard = pre_commit_status.lock();
            let wait_for_proof = compute_result.has_reconfiguration() || !status_guard.is_active();
            // it's a bit ugly here, but we want to make the check and update atomic in the pre_commit case
            // to avoid race that check returns active, sync manager pauses pre_commit and round gets updated
            if !wait_for_proof {
                status_guard.update_round(block.round());
            }
            wait_for_proof
        };

        if wait_for_proof {
            commit_proof_fut.await?;
            pre_commit_status.lock().update_round(block.round());
        }

        tracker.start_working();
        tokio::task::spawn_blocking(move || {
            executor
                .pre_commit_block(block.id())
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
        Ok(compute_result)
    }
```

**File:** consensus/src/block_storage/sync_manager.rs (L504-514)
```rust
        // abort any pending executor tasks before entering state sync
        // with zaptos, things can run before hitting buffer manager
        if let Some(block_store) = maybe_block_store {
            monitor!(
                "abort_pipeline_for_state_sync",
                block_store.abort_pipeline_for_state_sync().await
            );
        }
        execution_client
            .sync_to_target(highest_commit_cert.ledger_info().clone())
            .await?;
```
