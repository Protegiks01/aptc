# Audit Report

## Title
Consensus Node Panic Due to Dangling Reference to Pruned Highest Certified Block

## Summary
The `BlockTree` maintains a `highest_certified_block_id` pointer that becomes dangling when the referenced block is pruned from memory. When `insert_quorum_cert()` subsequently attempts to access this block for round comparison, it triggers a panic that crashes the validator node during normal consensus operations.

## Finding Description

The vulnerability exists in the consensus block storage layer due to inconsistent state management between block pruning and the `highest_certified_block_id` tracking.

**State Management Issue:**

The `BlockTree` struct maintains `highest_certified_block_id` to track the certified block with the highest round. [1](#0-0) 

This field is initialized to `commit_root_id` during construction [2](#0-1)  and is only updated when a higher-round certified block is inserted [3](#0-2) .

When blocks are pruned via `process_pruned_blocks()`, the `remove_block()` function removes blocks from `id_to_block`, their rounds from `round_to_ids`, and their QCs from `id_to_quorum_cert` [4](#0-3) , but critically, `highest_certified_block_id` is **never updated** during pruning operations.

When pruned blocks exceed the `max_pruned_blocks_in_mem` threshold, they are permanently removed from memory [5](#0-4) .

**Panic Trigger:**

When `insert_quorum_cert()` is called with a new QC [6](#0-5) , it accesses the highest certified block to compare rounds at line 368. The check at line 366 only verifies the **new** block being certified exists, not the **previous** highest certified block being accessed at line 368.

The `highest_certified_block()` method retrieves the block and panics with "Highest cerfified block must exist" if it doesn't exist in the tree [7](#0-6) .

**Vulnerability Scenario:**

1. During network partition, a fork block C receives a valid QC (2f+1 signatures from partitioned subset)
2. Node X receives block C and its QC, calling `insert_quorum_cert()` which sets `highest_certified_block_id = C`
3. Network heals and the honest majority's main chain becomes canonical
4. Node X syncs and commits main chain blocks
5. Fork containing C is identified for pruning [8](#0-7) 
6. C is added to `pruned_block_ids` and eventually removed via `remove_block()` when buffer threshold is exceeded
7. New QC arrives for block D on main chain (with round < C's round)
8. `insert_single_quorum_cert()` verifies block D exists [9](#0-8)  and calls `BlockTree::insert_quorum_cert()`
9. Line 368 evaluates `self.highest_certified_block().round()` to compare with block D's round
10. Attempts to retrieve block C which no longer exists in `id_to_block`
11. Panic occurs â†’ validator node crashes

## Impact Explanation

**Severity: High**

This vulnerability qualifies as **High Severity** per Aptos bug bounty criteria:

- **Validator node crashes** during normal consensus message processing, causing immediate loss of consensus participation
- Requires node restart to recover
- Can affect multiple validators simultaneously if they experience similar network conditions
- Violates the code's own safety invariant assertion documented in the expect message
- Creates **temporary liveness degradation** when multiple nodes crash during network instability

The impact aligns with the bounty program's HIGH severity category for "Validator Node Slowdowns (High): Significant performance degradation affecting consensus" and "API Crashes (High): REST API crashes affecting network participation".

While this doesn't cause permanent state corruption or fund loss, validator node crashes during consensus operations represent a significant availability violation affecting network reliability.

## Likelihood Explanation

**Likelihood: Medium in Adversarial Network Conditions**

The vulnerability can manifest through realistic scenarios:

**Natural Network Partitions:**
- Geographically distributed validators experience network splits (common in production)
- Partitioned groups form conflicting but valid QCs
- Network healing causes fork pruning
- No attacker required - natural distributed system behavior

**Byzantine Validators Within Tolerance:**
- Up to f Byzantine validators vote on fork blocks (within Aptos threat model)
- Combined with honest but partitioned nodes (2f+1 total) forms valid QC
- Main chain prevails when honest majority reconnects
- Fork gets pruned when buffer threshold exceeded (normal operation)

**Required Conditions:**
1. Fork block receives valid QC (2f+1 signatures) - realistic during partition
2. Fork block has higher round than subsequent main chain blocks receiving QCs
3. Pruning buffer exceeds `max_pruned_blocks_in_mem` (10 by default) - normal operation
4. New QC insertion triggers round comparison - normal consensus operation

**Realistic Assessment:**
- Network partitions occur naturally in production distributed systems
- Byzantine validator count < f is within Aptos threat model
- All required operations are normal consensus activities
- More likely in geographically distributed networks with unstable connectivity

## Recommendation

Update `highest_certified_block_id` when blocks are pruned to maintain the invariant that it always points to an existing block. The fix should be implemented in the pruning logic:

**Option 1**: Reset to commit root when highest certified block is pruned
```rust
// In process_pruned_blocks(), after removing blocks
if self.pruned_block_ids.contains(&self.highest_certified_block_id) {
    self.highest_certified_block_id = self.commit_root_id;
    self.highest_quorum_cert = self.get_quorum_cert_for_block(&self.commit_root_id)
        .expect("Commit root must have QC");
}
```

**Option 2**: Check existence before accessing in `insert_quorum_cert()`
```rust
// At line 368, before accessing highest_certified_block().round()
if let Some(highest_block) = self.get_block(&self.highest_certified_block_id) {
    if block.round() > highest_block.round() {
        self.highest_certified_block_id = block.id();
        self.highest_quorum_cert = Arc::clone(&qc);
    }
} else {
    // Highest certified block was pruned, reset to current block
    self.highest_certified_block_id = block.id();
    self.highest_quorum_cert = Arc::clone(&qc);
}
```

**Option 3**: Ensure highest certified block is never pruned by checking before removal

## Proof of Concept

A PoC would require setting up a test scenario with:
1. Multiple validators in a test network
2. Simulated network partition creating fork blocks
3. Fork block receiving QC and updating `highest_certified_block_id`
4. Network healing and main chain committing blocks
5. Pruning the fork and exceeding `max_pruned_blocks_in_mem` threshold
6. Inserting new QC that triggers the panic

The crash would manifest as:
```
thread 'main' panicked at 'Highest cerfified block must exist', consensus/src/block_storage/block_tree.rs:210
```

## Notes

This vulnerability represents a critical gap in the state management invariants. The code assumes `highest_certified_block_id` always points to an existing block (as evidenced by the panic message), but the pruning logic does not maintain this invariant. This is a logic vulnerability where the state management between two components (`highest_certified_block_id` tracking and block pruning) is inconsistent, leading to a safety violation under specific but realistic network conditions.

### Citations

**File:** consensus/src/block_storage/block_tree.rs (L83-83)
```rust
    highest_certified_block_id: HashValue,
```

**File:** consensus/src/block_storage/block_tree.rs (L138-138)
```rust
            highest_certified_block_id: commit_root_id,
```

**File:** consensus/src/block_storage/block_tree.rs (L174-181)
```rust
    fn remove_block(&mut self, block_id: HashValue) {
        // Remove the block from the store
        if let Some(block) = self.id_to_block.remove(&block_id) {
            let round = block.executed_block().round();
            self.round_to_ids.remove(&round);
        };
        self.id_to_quorum_cert.remove(&block_id);
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L208-211)
```rust
    pub(super) fn highest_certified_block(&self) -> Arc<PipelinedBlock> {
        self.get_block(&self.highest_certified_block_id)
            .expect("Highest cerfified block must exist")
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L349-386)
```rust
    pub(super) fn insert_quorum_cert(&mut self, qc: QuorumCert) -> anyhow::Result<()> {
        let block_id = qc.certified_block().id();
        let qc = Arc::new(qc);

        // Safety invariant: For any two quorum certificates qc1, qc2 in the block store,
        // qc1 == qc2 || qc1.round != qc2.round
        // The invariant is quadratic but can be maintained in linear time by the check
        // below.
        precondition!({
            let qc_round = qc.certified_block().round();
            self.id_to_quorum_cert.values().all(|x| {
                (*(*x).ledger_info()).ledger_info().consensus_data_hash()
                    == (*(*qc).ledger_info()).ledger_info().consensus_data_hash()
                    || x.certified_block().round() != qc_round
            })
        });

        match self.get_block(&block_id) {
            Some(block) => {
                if block.round() > self.highest_certified_block().round() {
                    self.highest_certified_block_id = block.id();
                    self.highest_quorum_cert = Arc::clone(&qc);
                }
            },
            None => bail!("Block {} not found", block_id),
        }

        self.id_to_quorum_cert
            .entry(block_id)
            .or_insert_with(|| Arc::clone(&qc));

        if self.highest_ordered_cert.commit_info().round() < qc.commit_info().round() {
            // Question: We are updating highest_ordered_cert but not highest_ordered_root. Is that fine?
            self.highest_ordered_cert = Arc::new(qc.into_wrapped_ledger_info());
        }

        Ok(())
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L405-434)
```rust
    pub(super) fn find_blocks_to_prune(
        &self,
        next_window_root_id: HashValue,
    ) -> VecDeque<HashValue> {
        // Nothing to do if this is the window root
        if next_window_root_id == self.window_root_id {
            return VecDeque::new();
        }

        let mut blocks_pruned = VecDeque::new();
        let mut blocks_to_be_pruned = vec![self.linkable_window_root()];

        while let Some(block_to_remove) = blocks_to_be_pruned.pop() {
            block_to_remove.executed_block().abort_pipeline();
            // Add the children to the blocks to be pruned (if any), but stop when it reaches the
            // new root
            for child_id in block_to_remove.children() {
                if next_window_root_id == *child_id {
                    continue;
                }
                blocks_to_be_pruned.push(
                    self.get_linkable_block(child_id)
                        .expect("Child must exist in the tree"),
                );
            }
            // Track all the block ids removed
            blocks_pruned.push_back(block_to_remove.id());
        }
        blocks_pruned
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L502-509)
```rust
        if self.pruned_block_ids.len() > self.max_pruned_blocks_in_mem {
            let num_blocks_to_remove = self.pruned_block_ids.len() - self.max_pruned_blocks_in_mem;
            for _ in 0..num_blocks_to_remove {
                if let Some(id) = self.pruned_block_ids.pop_front() {
                    self.remove_block(id);
                }
            }
        }
```

**File:** consensus/src/block_storage/block_store.rs (L519-556)
```rust
    pub fn insert_single_quorum_cert(&self, qc: QuorumCert) -> anyhow::Result<()> {
        // If the parent block is not the root block (i.e not None), ensure the executed state
        // of a block is consistent with its QuorumCert, otherwise persist the QuorumCert's
        // state and on restart, a new execution will agree with it.  A new execution will match
        // the QuorumCert's state on the next restart will work if there is a memory
        // corruption, for example.
        match self.get_block(qc.certified_block().id()) {
            Some(pipelined_block) => {
                ensure!(
                    // decoupled execution allows dummy block infos
                    pipelined_block
                        .block_info()
                        .match_ordered_only(qc.certified_block()),
                    "QC for block {} has different {:?} than local {:?}",
                    qc.certified_block().id(),
                    qc.certified_block(),
                    pipelined_block.block_info()
                );
                observe_block(
                    pipelined_block.block().timestamp_usecs(),
                    BlockStage::QC_ADDED,
                );
                if pipelined_block.block().is_opt_block() {
                    observe_block(
                        pipelined_block.block().timestamp_usecs(),
                        BlockStage::QC_ADDED_OPT_BLOCK,
                    );
                }
                pipelined_block.set_qc(Arc::new(qc.clone()));
            },
            None => bail!("Insert {} without having the block in store first", qc),
        };

        self.storage
            .save_tree(vec![], vec![qc.clone()])
            .context("Insert block failed when saving quorum")?;
        self.inner.write().insert_quorum_cert(qc)
    }
```
