# Audit Report

## Title
Vote Durability Failure Enables Consensus Safety Violation Through Equivocation After Machine Crash

## Summary
The Aptos consensus voting mechanism uses non-durable storage writes in both ConsensusDB and SafetyRules OnDiskStorage, allowing votes to be lost during machine crashes. This enables validators to unintentionally equivocate (vote twice for different blocks in the same round) after restart, violating BFT consensus safety guarantees.

## Finding Description

The Aptos consensus system persists votes to prevent equivocation across restarts. However, both storage systems use write operations without durability guarantees, creating a vulnerability window where votes sent over the network can be lost during machine crashes.

**Vote Persistence Architecture:**

When a validator votes on a block, the vote is persisted in two locations:

1. **SafetyRules Storage (OnDiskStorage)**: The `guarded_construct_and_sign_vote_two_chain` function creates a vote, stores it in `safety_data.last_vote`, and persists via `set_safety_data()`. [1](#0-0) 

2. **ConsensusDB**: After SafetyRules returns the vote, `vote_block()` persists it via `storage.save_vote()`. [2](#0-1) 

**Non-Durable Writes:**

ConsensusDB's `commit()` method uses `write_schemas_relaxed()` which explicitly does NOT fsync to disk: [3](#0-2) 

The documentation for `write_schemas_relaxed()` explicitly states: "If this flag is false, and the machine crashes, some recent writes may be lost." [4](#0-3) 

OnDiskStorage's `write()` method creates a temp file, writes data, and renames it, but never calls `sync_all()` or `sync_data()`: [5](#0-4) 

The secure storage README explicitly warns: "on-disk should not be used in production environments" [6](#0-5) 

**Production Configuration:**

Despite this warning, production validator configurations DO use OnDiskStorage: [7](#0-6) [8](#0-7) 

The configuration sanitizer only prevents `InMemoryStorage` on mainnet, allowing `OnDiskStorage`: [9](#0-8) 

**Equivocation Scenario:**

The SafetyData struct contains both `last_voted_round` (the round number) and `last_vote` (the full vote object): [10](#0-9) 

The voting logic checks if a vote already exists for the current round: [11](#0-10) 

If not found, it verifies the round is greater than `last_voted_round`: [12](#0-11) 

**Attack Flow:**

1. Validator votes for block A in round N
2. Writes to OnDiskStorage (no fsync) and ConsensusDB (no fsync)
3. Vote sent over network
4. **Machine crash** (power loss, kernel panic) before OS buffer flush
5. Node restarts, SafetyData shows `last_voted_round = N-1` (write lost)
6. Recovery filters votes by epoch, setting `last_vote = None` if not found: [13](#0-12) 
7. Validator receives proposal for block B in round N
8. Check at line 70-74 doesn't find vote for round N (it was lost)
9. Check at line 218 passes because N > N-1 (last_voted_round wasn't updated)
10. Validator creates and signs new vote for block B
11. **EQUIVOCATION**: Two votes for different blocks in round N

While other validators detect equivocation when receiving both votes [14](#0-13)  the damage may already be done if network partitions allow different validators to count different votes toward conflicting quorum certificates.

## Impact Explanation

**Severity: CRITICAL** (Consensus Safety Violation - up to $1,000,000 per Aptos Bug Bounty)

This vulnerability enables a fundamental consensus safety violation:

1. **Equivocation by Honest Validators**: The system allows honest validators to unintentionally vote for multiple blocks in the same round due to machine crashes, behavior that should only occur with Byzantine (malicious) validators.

2. **BFT Assumption Violation**: BFT safety assumes < 1/3 Byzantine validators. This bug allows honest validators to exhibit Byzantine behavior through crashes, effectively reducing the Byzantine fault tolerance threshold.

3. **Potential Chain Splits**: If different validators receive different votes from the same validator during network partitions or delays, they may form conflicting quorum certificates, potentially leading to chain splits.

4. **Safety Rules Bypass**: The persistent storage mechanism designed to prevent double-voting across restarts is fundamentally broken by the lack of durable writes.

5. **Network-Wide Impact**: Even a single validator experiencing this crash at a critical moment can compromise consensus safety.

This aligns with the Aptos bug bounty Critical severity category for "Consensus/Safety Violations" where different validators could commit different blocks.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

This vulnerability has realistic occurrence conditions:

1. **Common Trigger**: Machine crashes (power failures, kernel panics, hardware failures, OOM kills) are regular occurrences in production infrastructure.

2. **Production Exposure**: Production validator configurations actually use OnDiskStorage despite the explicit warning against production use.

3. **Narrow but Realistic Window**: While the vulnerability window is narrow (milliseconds between write and OS buffer flush), network message propagation is also fast, creating realistic race conditions.

4. **No Prevention**: The configuration sanitizer does not enforce the README's recommendation - it only prevents InMemoryStorage, allowing OnDiskStorage on mainnet validators.

5. **No Recovery Mechanism**: The system has no mechanism to detect that a vote was lost or prevent the subsequent equivocation.

## Recommendation

**Immediate Fixes:**

1. **Add fsync to OnDiskStorage**: Modify the `write()` method to call `file.sync_all()` before rename and sync the parent directory:
```rust
fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
    let contents = serde_json::to_vec(data)?;
    let mut file = File::create(self.temp_path.path())?;
    file.write_all(&contents)?;
    file.sync_all()?;  // ADD THIS
    fs::rename(&self.temp_path, &self.file_path)?;
    // Sync parent directory to ensure rename is durable
    Ok(())
}
```

2. **Use Durable Writes in ConsensusDB**: Replace `write_schemas_relaxed()` with `write_schemas()` for vote persistence:
```rust
fn commit(&self, batch: SchemaBatch) -> Result<(), DbError> {
    self.db.write_schemas(batch)?;  // Use durable write
    Ok(())
}
```

3. **Enforce Vault Usage**: Update the configuration sanitizer to require Vault storage for production mainnet validators, or at minimum, add explicit warnings and require operator acknowledgment.

4. **Add Vote Recovery Check**: Implement a recovery mechanism that detects when a vote might have been sent but not persisted, and broadcasts a recovery message to inform other validators.

## Proof of Concept

The vulnerability exists in the system design and can be demonstrated through the following scenario:

**Setup:**
1. Configure a validator with OnDiskStorage (as in production configs)
2. Instrument the code to simulate a machine crash after vote creation but before OS buffer flush

**Execution:**
1. Validator receives block proposal for round N
2. Vote created and "persisted" (buffered, not fsynced)
3. Vote sent to network
4. Simulate machine crash (kill -9 + clear OS buffers)
5. Restart validator
6. Send different block proposal for round N
7. Observe validator creates second vote for same round

**Expected Result:** Second vote is created and sent, demonstrating equivocation.

**Actual behavior in production:** On real machine crashes (power loss, kernel panic), the same scenario would occur naturally without any simulation needed.

## Notes

This is a fundamental durability issue in the consensus safety mechanisms. The vulnerability is particularly concerning because:

1. Production configurations are vulnerable despite explicit documentation warnings
2. The configuration validation does not enforce the documented best practices
3. Machine crashes are environmental events that occur regularly in production infrastructure
4. The impact directly violates core BFT consensus safety guarantees

The fix requires careful consideration of performance tradeoffs, as adding fsync operations will impact write latency. However, consensus safety is a fundamental requirement that cannot be compromised for performance.

### Citations

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L70-74)
```rust
        if let Some(vote) = safety_data.last_vote.clone() {
            if vote.vote_data().proposed().round() == proposed_block.round() {
                return Ok(vote);
            }
        }
```

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L91-92)
```rust
        safety_data.last_vote = Some(vote.clone());
        self.persistent_storage.set_safety_data(safety_data)?;
```

**File:** consensus/src/round_manager.rs (L1539-1541)
```rust
        self.storage
            .save_vote(&vote)
            .context("[RoundManager] Fail to persist last vote")?;
```

**File:** consensus/src/consensusdb/mod.rs (L156-159)
```rust
    fn commit(&self, batch: SchemaBatch) -> Result<(), DbError> {
        self.db.write_schemas_relaxed(batch)?;
        Ok(())
    }
```

**File:** storage/schemadb/src/lib.rs (L311-318)
```rust
    /// Writes without sync flag in write option.
    /// If this flag is false, and the machine crashes, some recent
    /// writes may be lost.  Note that if it is just the process that
    /// crashes (i.e., the machine does not reboot), no writes will be
    /// lost even if sync==false.
    pub fn write_schemas_relaxed(&self, batch: impl IntoRawBatch) -> DbResult<()> {
        self.write_schemas_inner(batch, &WriteOptions::default())
    }
```

**File:** secure/storage/src/on_disk.rs (L64-70)
```rust
    fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
        let contents = serde_json::to_vec(data)?;
        let mut file = File::create(self.temp_path.path())?;
        file.write_all(&contents)?;
        fs::rename(&self.temp_path, &self.file_path)?;
        Ok(())
    }
```

**File:** secure/storage/README.md (L37-42)
```markdown
- `OnDisk`: Similar to InMemory, the OnDisk secure storage implementation provides another
useful testing implementation: an on-disk storage engine, where the storage backend is
implemented using a single file written to local disk. In a similar fashion to the in-memory
storage, on-disk should not be used in production environments as it provides no security
guarantees (e.g., encryption before writing to disk). Moreover, OnDisk storage does not
currently support concurrent data accesses.
```

**File:** terraform/helm/aptos-node/files/configs/validator-base.yaml (L14-16)
```yaml
    backend:
      type: "on_disk_storage"
      path: secure-data.json
```

**File:** docker/compose/aptos-node/validator.yaml (L11-14)
```yaml
    backend:
      type: "on_disk_storage"
      path: secure-data.json
      namespace: ~
```

**File:** config/src/config/safety_rules_config.rs (L87-95)
```rust
            if chain_id.is_mainnet()
                && node_type.is_validator()
                && safety_rules_config.backend.is_in_memory()
            {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "The secure backend should not be set to in memory storage in mainnet!"
                        .to_string(),
                ));
```

**File:** consensus/consensus-types/src/safety_data.rs (L10-21)
```rust
pub struct SafetyData {
    pub epoch: u64,
    pub last_voted_round: u64,
    // highest 2-chain round, used for 3-chain
    pub preferred_round: u64,
    // highest 1-chain round, used for 2-chain
    #[serde(default)]
    pub one_chain_round: u64,
    pub last_vote: Option<Vote>,
    #[serde(default)]
    pub highest_timeout_round: u64,
}
```

**File:** consensus/safety-rules/src/safety_rules.rs (L218-225)
```rust
        if round <= safety_data.last_voted_round {
            return Err(Error::IncorrectLastVotedRound(
                round,
                safety_data.last_voted_round,
            ));
        }

        safety_data.last_voted_round = round;
```

**File:** consensus/src/persistent_liveness_storage.rs (L405-408)
```rust
            last_vote: match last_vote {
                Some(v) if v.epoch() == epoch => Some(v),
                _ => None,
            },
```

**File:** consensus/src/pending_votes.rs (L300-307)
```rust
                error!(
                    SecurityEvent::ConsensusEquivocatingVote,
                    remote_peer = vote.author(),
                    vote = vote,
                    previous_vote = previously_seen_vote
                );

                return VoteReceptionResult::EquivocateVote;
```
