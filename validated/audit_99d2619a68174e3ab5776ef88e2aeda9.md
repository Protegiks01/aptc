# Audit Report

## Title
SystemTime Clock Adjustment Causes Mempool Task Panic and Broadcast Stalling

## Summary
The mempool broadcast system uses `SystemTime` (wall clock) instead of `Instant` (monotonic clock) for tracking message send times and calculating round-trip times (RTT). When the system clock is adjusted backwards during NTP synchronization or manual adjustment, the code panics when processing broadcast acknowledgments, crashing the mempool coordinator task. This disables transaction processing on the affected validator node.

## Finding Description

The vulnerability exists in the mempool's network broadcast acknowledgment processing:

**Critical Flaw: Panic on ACK Processing**

When a transaction broadcast is sent to a peer, the send time is captured using `SystemTime::now()`: [1](#0-0) 

This timestamp is stored in the broadcast tracking state: [2](#0-1) 

The broadcast tracking structure uses `SystemTime` for timestamps: [3](#0-2) 

When an acknowledgment is received from a peer, the ACK timestamp is captured using `SystemTime::now()`: [4](#0-3) 

The RTT calculation uses `.expect()` which **panics** if the ACK timestamp is earlier than the send timestamp: [5](#0-4) 

**Attack Scenario:**
1. Node sends broadcast at system time T1 (e.g., Unix timestamp 1700000000)
2. System clock is adjusted backwards to T0 < T1 (e.g., via NTP correction to 1699999000)
3. Peer sends ACK, timestamped with current system time T0
4. Code attempts `T0.duration_since(T1)`, which returns `Err` because T0 < T1
5. The `.expect()` panics with message "failed to calculate mempool broadcast RTT"
6. The coordinator task terminates, disabling mempool functionality

**Additional Flaw: Stalled Broadcast Detection**

The system detects expired broadcasts for retry using `SystemTime`: [6](#0-5) 

When the clock is adjusted backwards, `SystemTime::now().duration_since(deadline).is_ok()` returns `false`, preventing expired broadcast detection.

**Root Cause:**

`SystemTime` represents wall clock time that can be adjusted backwards by NTP synchronization, manual adjustments, or leap second corrections. The codebase inconsistently uses `Instant` for broadcast latency measurement: [7](#0-6) 

This shows the developers are aware of `Instant` (monotonic clock) but didn't use it for broadcast tracking, creating the vulnerability.

## Impact Explanation

**Severity: HIGH** (per Aptos Bug Bounty criteria)

This vulnerability qualifies as **HIGH severity** under the "API Crashes" category because:

1. **Component Crash**: The panic occurs in `process_broadcast_ack` which is called directly in the coordinator event loop without error handling. The coordinator task is spawned via: [8](#0-7) 

When the coordinator panics, the task terminates permanently.

2. **Transaction Processing Disabled**: Without the coordinator task, the mempool cannot:
   - Process incoming transaction broadcasts from peers
   - Send outgoing transaction broadcasts
   - Handle client transaction submissions
   - Update peer synchronization states

3. **API Failures**: Transaction submission APIs will fail or timeout when mempool is non-functional, directly impacting node availability for users.

4. **Validator Effectiveness Reduced**: Validators without functioning mempools cannot propose blocks with new user transactions, reducing their contribution to the network.

This meets the HIGH severity criteria: "API Crashes - REST API crashes affecting network participation; Transaction submission failures."

## Likelihood Explanation

**Likelihood: MEDIUM**

This vulnerability has a realistic exploitation path:

**Natural Occurrence:**
- NTP synchronization can make backwards clock adjustments to correct drift
- Virtual machine migrations can cause clock discontinuities
- System maintenance may involve clock adjustments
- The vulnerability window is the duration between broadcast send and ACK receipt (typically milliseconds to seconds)

**Timing Requirements:**
- Backwards clock adjustment must occur between sending a broadcast and receiving its ACK
- While NTP backwards adjustments are less common than forward adjustments, they do occur in production systems
- The frequency of broadcasts combined with the possibility of clock adjustments creates a non-negligible probability

The likelihood is MEDIUM because:
- Natural occurrence is possible without adversarial action
- The timing window is relatively small
- Large backwards adjustments are infrequent in well-maintained systems
- However, when it does occur, the impact is severe (complete mempool shutdown)

## Recommendation

Replace `SystemTime` with `Instant` for all broadcast timing tracking. `Instant` is a monotonic clock that is immune to system time adjustments.

**Changes needed:**

1. Update `BroadcastInfo` to use `Instant`:
```rust
pub struct BroadcastInfo {
    pub sent_messages: BTreeMap<MempoolMessageId, Instant>,  // Changed from SystemTime
    pub retry_messages: BTreeSet<MempoolMessageId>,
    pub backoff_mode: bool,
}
```

2. Update broadcast send time capture:
```rust
let send_time = Instant::now();  // Changed from SystemTime::now()
```

3. Update ACK processing:
```rust
let ack_timestamp = Instant::now();  // Changed from SystemTime::now()
```

4. Update RTT calculation (will never panic with Instant):
```rust
let rtt = ack_timestamp.duration_since(sent_timestamp);  // No .expect() needed
```

5. Update expiry detection:
```rust
let deadline = sent_time + Duration::from_millis(
    self.mempool_config.shared_mempool_ack_timeout_ms,
);
if Instant::now() > deadline {
    expired_message_id = Some(message);
}
```

## Proof of Concept

A proof of concept would require:
1. Setting up a test validator node
2. Sending a broadcast transaction
3. Adjusting the system clock backwards using `clock_settime()` or similar
4. Observing the panic when the ACK is received

Due to the requirement for system-level clock manipulation, this would need to be tested in a controlled environment with appropriate permissions. The vulnerability is evident from code inspection without requiring runtime demonstration.

## Notes

This vulnerability is confirmed through direct code inspection. All cited code paths are verified in the current codebase. The use of `SystemTime` for duration calculations is a well-known anti-pattern in Rust when dealing with intervals, as `SystemTime` is not monotonic. The codebase already uses `Instant` correctly in some places, indicating developer awareness of the distinction, making this an implementation oversight rather than a knowledge gap.

### Citations

**File:** mempool/src/shared_mempool/network.rs (L315-323)
```rust
        if let Some(sent_timestamp) = sync_state.broadcast_info.sent_messages.remove(&message_id) {
            let rtt = timestamp
                .duration_since(sent_timestamp)
                .expect("failed to calculate mempool broadcast RTT");

            let network_id = peer.network_id();
            counters::SHARED_MEMPOOL_BROADCAST_RTT
                .with_label_values(&[network_id.as_str()])
                .observe(rtt.as_secs_f64());
```

**File:** mempool/src/shared_mempool/network.rs (L432-439)
```rust
            let deadline = sent_time.add(Duration::from_millis(
                self.mempool_config.shared_mempool_ack_timeout_ms,
            ));
            if SystemTime::now().duration_since(deadline).is_ok() {
                expired_message_id = Some(message);
            } else {
                pending_broadcasts += 1;
            }
```

**File:** mempool/src/shared_mempool/network.rs (L629-633)
```rust
        state
            .broadcast_info
            .sent_messages
            .insert(message_id, send_time);
        Ok(state.broadcast_info.sent_messages.len())
```

**File:** mempool/src/shared_mempool/network.rs (L643-655)
```rust
        let start_time = Instant::now();
        let (message_id, transactions, metric_label) =
            self.determine_broadcast_batch(peer, scheduled_backoff, smp)?;
        let num_txns = transactions.len();
        let send_time = SystemTime::now();
        self.send_batch_to_peer(peer, message_id.clone(), transactions)
            .await?;
        let num_pending_broadcasts =
            self.update_broadcast_state(peer, message_id.clone(), send_time)?;
        notify_subscribers(SharedMempoolNotification::Broadcast, &smp.subscribers);

        // Log all the metrics
        let latency = start_time.elapsed();
```

**File:** mempool/src/shared_mempool/types.rs (L456-464)
```rust
#[derive(Clone, Debug)]
pub struct BroadcastInfo {
    // Sent broadcasts that have not yet received an ack.
    pub sent_messages: BTreeMap<MempoolMessageId, SystemTime>,
    // Broadcasts that have received a retry ack and are pending a resend.
    pub retry_messages: BTreeSet<MempoolMessageId>,
    // Whether broadcasting to this peer is in backoff mode, e.g. broadcasting at longer intervals.
    pub backoff_mode: bool,
}
```

**File:** mempool/src/shared_mempool/coordinator.rs (L396-403)
```rust
                    let ack_timestamp = SystemTime::now();
                    smp.network_interface.process_broadcast_ack(
                        PeerNetworkId::new(network_id, peer_id),
                        message_id,
                        retry,
                        backoff,
                        ack_timestamp,
                    );
```

**File:** mempool/src/shared_mempool/runtime.rs (L66-76)
```rust
    executor.spawn(coordinator(
        smp,
        executor.clone(),
        network_service_events,
        client_events,
        quorum_store_requests,
        mempool_listener,
        mempool_reconfig_events,
        config.mempool.shared_mempool_peer_update_interval_ms,
        peers_and_metadata,
    ));
```
