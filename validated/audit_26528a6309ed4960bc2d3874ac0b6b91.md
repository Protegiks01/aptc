# Audit Report

## Title
Missing fsync() in OnDiskStorage Enables Consensus Safety Violation Through Vote Equivocation After SIGKILL

## Summary
The `OnDiskStorage` backend used for persisting safety-critical consensus state lacks proper disk synchronization (`fsync`) before returning from write operations. When a validator process is killed with SIGKILL after voting, the SafetyData (including `last_vote`) may not be durably persisted to disk. On restart, the validator can vote again on the same consensus round for a different block, causing equivocation and breaking consensus safety guarantees.

## Finding Description

The vulnerability exists in the interaction between multiple components:

**1. SafetyData Persistence Without fsync**

The `OnDiskStorage::write()` method writes SafetyData to disk but never calls `fsync()` to ensure durability before returning. [1](#0-0) 

The write operation performs: (1) create temporary file, (2) write all contents, (3) atomic rename to target path. However, without `fsync()` on the file handle or parent directory, the data remains in OS buffer cache and is not guaranteed to reach persistent storage before the function returns.

Critically, the `PersistentSafetyStorage` interface explicitly requires: "Any set function is expected to sync to the remote system before returning." [2](#0-1)  The `OnDiskStorage` implementation violates this contract.

**2. Production Deployment Configuration**

Production validator deployment configurations explicitly use OnDiskStorage as their safety rules backend:
- Helm chart configuration [3](#0-2) 
- Docker Compose configuration [4](#0-3) 

The configuration sanitizer only prevents `InMemoryStorage` on mainnet, but explicitly allows `OnDiskStorage`: [5](#0-4) 

**3. Safety Rules Dependency on Persisted Vote Records**

The consensus safety rules rely on checking `SafetyData.last_vote` to prevent equivocation. Before creating a new vote, SafetyRules checks if a vote already exists for the current round and returns it if found: [6](#0-5) 

**4. Vote Persistence Flow**

When a validator votes, it updates SafetyData via the storage layer: [7](#0-6) 

This calls `PersistentSafetyStorage::set_safety_data()` which attempts to persist to storage: [8](#0-7) 

The persistence goes through `OnDiskStorage::set()` which calls the vulnerable `write()` method: [9](#0-8) 

**Attack Scenario:**

1. Validator votes for Block A at consensus round R
2. SafetyRules updates `SafetyData.last_vote` via `set_safety_data()`
3. OnDiskStorage writes to file but doesn't call `fsync()` - data remains in OS buffer cache
4. Process is killed with SIGKILL before OS flushes buffers to disk
5. On restart, SafetyData is read from disk but lacks the vote record for round R (or contains stale data)
6. Validator receives a different proposal (Block B) for round R
7. SafetyRules check fails to find a previous vote for this round
8. **Validator creates a new vote for Block B** - equivocation has occurred

This breaks the fundamental safety guarantee of BFT consensus: validators must never vote for two different blocks at the same round. The consensus protocol assumes this invariant holds, and violations can lead to network partitions or different subsets of validators committing conflicting states.

## Impact Explanation

**Critical Severity** - This meets the Aptos Bug Bounty criteria for Critical impact:

- **Consensus/Safety Violations**: Direct violation of the core AptosBFT safety property (no equivocation). A single validator experiencing this issue after SIGKILL can vote twice at the same round, violating the fundamental BFT assumption.

- **Network Partition Risk**: If multiple validators experience this simultaneously (e.g., during infrastructure issues causing widespread OOM kills or system crashes), the network could split with different validator subsets committing different blocks.

- **Byzantine Fault Amplification**: This converts a crash fault (SIGKILL) into a Byzantine fault (equivocation). BFT consensus protocols are designed to tolerate f Byzantine faults, but this bug causes honest validators to behave Byzantine after crashes, reducing the actual fault tolerance below the theoretical f threshold.

The vulnerability affects the core consensus mechanism that all validators rely on. Unlike typical crash-recovery bugs that only affect liveness, this breaks **safety** - the most critical property of a Byzantine Fault Tolerant consensus protocol. Once safety is violated, the blockchain state can diverge, and recovery requires manual intervention or a hard fork.

## Likelihood Explanation

**HIGH Likelihood**:

- **Triggering Condition**: Only requires SIGKILL (kill -9) to the validator process - no privilege escalation or validator compromise needed.

- **Common Scenarios**: 
  - Out-of-memory (OOM) killer terminating the process (very common in production)
  - System crashes or kernel panics
  - Operator running `kill -9` during emergency shutdowns or debugging
  - Container orchestration systems (Kubernetes) force-killing pods during updates or node drains
  - Hardware failures or power loss

- **Production Configuration**: The default Helm chart and Docker Compose validator configurations use OnDiskStorage, making validators deployed via these methods vulnerable.

- **No Attacker Access Required**: Can happen accidentally during normal operations or infrastructure issues. Does not require any adversarial action.

- **Window of Vulnerability**: Present during every vote operation, between the write() call returning and the OS actually flushing buffers to disk (typically milliseconds to seconds, but can be longer under high I/O load).

The vulnerability is inherent in the storage design and affects normal operations, not requiring any special attack conditions.

## Recommendation

Add `fsync()` calls to the `OnDiskStorage::write()` method to ensure durability before returning:

```rust
fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
    let contents = serde_json::to_vec(data)?;
    let mut file = File::create(self.temp_path.path())?;
    file.write_all(&contents)?;
    file.sync_all()?;  // ADD: Ensure data is flushed to disk
    fs::rename(&self.temp_path, &self.file_path)?;
    
    // ADD: Sync parent directory to ensure rename is durable
    if let Some(parent) = self.file_path.parent() {
        File::open(parent)?.sync_all()?;
    }
    
    Ok(())
}
```

Alternatively, migrate production validators to use Vault storage backend which provides proper durability guarantees, and enforce this at the configuration sanitizer level for mainnet deployments.

## Proof of Concept

The vulnerability can be demonstrated through the following scenario:

1. Deploy a validator using the default Helm chart configuration (OnDiskStorage backend)
2. Trigger a vote on consensus round R for Block A
3. Immediately send SIGKILL to the validator process (simulating OOM killer or crash)
4. Restart the validator
5. Send a vote proposal for a different Block B at the same round R
6. Observe that the validator creates a second vote for round R, violating the no-equivocation property

The technical root cause is confirmed by examining the code flow from vote creation through storage persistence, where no `fsync()` or `sync_all()` call exists in the write path.

## Notes

- The code comment in `on_disk.rs` states "This should not be used in production" but the ConfigSanitizer does not enforce this restriction for mainnet validators
- The missing `fsync()` directly violates the documented interface contract that "Any set function is expected to sync to the remote system before returning"
- This vulnerability affects all validators using OnDiskStorage, which includes the default deployment configurations
- The issue is particularly severe because it converts crash faults (which BFT is designed to tolerate) into Byzantine faults (equivocation), reducing the effective fault tolerance of the network

### Citations

**File:** secure/storage/src/on_disk.rs (L64-70)
```rust
    fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
        let contents = serde_json::to_vec(data)?;
        let mut file = File::create(self.temp_path.path())?;
        file.write_all(&contents)?;
        fs::rename(&self.temp_path, &self.file_path)?;
        Ok(())
    }
```

**File:** secure/storage/src/on_disk.rs (L85-92)
```rust
    fn set<V: Serialize>(&mut self, key: &str, value: V) -> Result<(), Error> {
        let now = self.time_service.now_secs();
        let mut data = self.read()?;
        data.insert(
            key.to_string(),
            serde_json::to_value(GetResponse::new(value, now))?,
        );
        self.write(&data)
```

**File:** consensus/safety-rules/src/persistent_safety_storage.rs (L16-18)
```rust
/// SafetyRules needs an abstract storage interface to act as a common utility for storing
/// persistent data to local disk, cloud, secrets managers, or even memory (for tests)
/// Any set function is expected to sync to the remote system before returning.
```

**File:** consensus/safety-rules/src/persistent_safety_storage.rs (L150-170)
```rust
    pub fn set_safety_data(&mut self, data: SafetyData) -> Result<(), Error> {
        let _timer = counters::start_timer("set", SAFETY_DATA);
        counters::set_state(counters::EPOCH, data.epoch as i64);
        counters::set_state(counters::LAST_VOTED_ROUND, data.last_voted_round as i64);
        counters::set_state(
            counters::HIGHEST_TIMEOUT_ROUND,
            data.highest_timeout_round as i64,
        );
        counters::set_state(counters::PREFERRED_ROUND, data.preferred_round as i64);

        match self.internal_store.set(SAFETY_DATA, data.clone()) {
            Ok(_) => {
                self.cached_safety_data = Some(data);
                Ok(())
            },
            Err(error) => {
                self.cached_safety_data = None;
                Err(Error::SecureStorageUnexpectedError(error.to_string()))
            },
        }
    }
```

**File:** terraform/helm/aptos-node/files/configs/validator-base.yaml (L14-16)
```yaml
    backend:
      type: "on_disk_storage"
      path: secure-data.json
```

**File:** docker/compose/aptos-node/validator.yaml (L11-13)
```yaml
    backend:
      type: "on_disk_storage"
      path: secure-data.json
```

**File:** config/src/config/safety_rules_config.rs (L86-96)
```rust
            // Verify that the secure backend is appropriate for mainnet validators
            if chain_id.is_mainnet()
                && node_type.is_validator()
                && safety_rules_config.backend.is_in_memory()
            {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "The secure backend should not be set to in memory storage in mainnet!"
                        .to_string(),
                ));
            }
```

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L68-74)
```rust
        // if already voted on this round, send back the previous vote
        // note: this needs to happen after verifying the epoch as we just check the round here
        if let Some(vote) = safety_data.last_vote.clone() {
            if vote.vote_data().proposed().round() == proposed_block.round() {
                return Ok(vote);
            }
        }
```

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L91-92)
```rust
        safety_data.last_vote = Some(vote.clone());
        self.persistent_storage.set_safety_data(safety_data)?;
```
