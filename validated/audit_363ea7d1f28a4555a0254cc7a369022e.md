# Audit Report

## Title
Out-of-Order Notification Race Condition Causes Transaction Parking Lot Stalls

## Summary
A race condition between independent notification channels causes valid transactions to become permanently stuck in mempool's parking lot. When reject notifications from consensus arrive before commit notifications from state sync, the sequence number promotion logic fails to process subsequent transactions, requiring manual intervention or timeout expiration.

## Finding Description

The Aptos mempool receives transaction state updates through two independent channels that process concurrently without ordering guarantees:

**1. Reject notifications** arrive via the `quorum_store_requests` mpsc channel and are processed in the main coordinator event loop: [1](#0-0) [2](#0-1) 

**2. Commit notifications** arrive via `MempoolNotificationListener` and are processed in a separate spawned task: [3](#0-2) [4](#0-3) [5](#0-4) 

These channels use `futures::select!` which processes whichever message arrives first, with no synchronization: [6](#0-5) 

The vulnerability occurs in the sequence number promotion logic. When `commit_transaction` is called, it updates the account's sequence number and invokes `process_ready_seq_num_based_transactions`: [7](#0-6) 

The promotion function uses a while loop that processes transactions sequentially but stops immediately when any transaction is missing: [8](#0-7) 

The `process_ready_transaction` function returns false when a transaction doesn't exist, causing the while loop to exit: [9](#0-8) 

**Attack Scenario:**

Consider Alice with transactions T5 (seq=5), T6 (seq=6), T7 (seq=7) in mempool. Block N proposes T5 and T6. T5 succeeds but T6 fails validation.

Consensus sends reject notification via `post_ledger_update` pipeline phase (marked as "off critical path"): [10](#0-9) 

The reject notification is sent through the quorum store requests channel: [11](#0-10) 

This eventually calls `reject_transaction` which removes T6 from all indexes: [12](#0-11) 

State sync sends commit notification after block commit: [13](#0-12) 

**If reject notification arrives first:**
- `reject_transaction` removes T6 from all indexes
- `commit_transaction` sets account sequence to 6 and calls `process_ready_seq_num_based_transactions(Alice, 6)`
- The while loop tries to process T6 (seq=6) but it's already removed
- `process_ready_transaction` returns false because T6 doesn't exist
- The while loop exits immediately
- **T7 remains in parking lot indefinitely**

The mempool promotion logic cannot distinguish between "transaction not yet received" and "transaction was rejected", causing T7 to remain stuck until it expires via system TTL or Alice manually resubmits a valid T6.

## Impact Explanation

This vulnerability qualifies as **Medium Severity** per Aptos bug bounty criteria:

- **State inconsistencies requiring manual intervention**: The mempool's internal state (parking lot) diverges from what should be ready for consensus
- **Transaction liveness degradation**: Valid transactions become stuck and unavailable to consensus despite meeting all requirements
- **No direct fund loss**: Does not enable theft, unauthorized minting, or permanent fund lockup
- **Temporary impact**: Transactions eventually expire via TTL or users can resubmit

This matches the "Limited Protocol Violations" category for Medium severity - state inconsistencies requiring intervention and temporary liveness issues, without consensus safety violations or fund loss.

## Likelihood Explanation

This vulnerability has **medium-to-high likelihood** of occurring in production:

- **No attacker required**: Happens naturally due to system design with two independent asynchronous channels
- **Timing-dependent**: Occurs when reject notifications arrive before corresponding commit notifications due to network latency or processing delays
- **More likely under load**: High transaction volume and block production rate increases the probability of out-of-order delivery
- **Affects any user**: Any account with sequential transactions in mempool can experience this issue
- **Reproducible**: Can be triggered consistently by introducing artificial delays between the notification channels

The separate channel architecture, with `post_ledger_update` explicitly marked as "off critical path" and running asynchronously, makes this an inherent race condition in the system design, not a rare edge case.

## Recommendation

Implement ordering guarantees between reject and commit notifications:

1. **Option 1 - Sequence Numbers**: Add sequence numbers to both notification types and process them in order at the mempool coordinator level, buffering out-of-order notifications.

2. **Option 2 - Unified Channel**: Merge both notification types into a single ordered channel that guarantees commit notifications are processed before corresponding reject notifications.

3. **Option 3 - Deferred Promotion**: Modify `process_ready_seq_num_based_transactions` to mark missing transactions for deferred retry rather than immediately terminating the promotion loop. Add a background task that periodically retries promotion for accounts with parked transactions.

4. **Option 4 - Transaction State Tracking**: Add a "rejected" state to track transactions that were explicitly rejected, distinct from "not yet received". The promotion logic should skip rejected transactions and continue processing subsequent sequence numbers.

## Proof of Concept

A proof of concept would require:

1. Setting up a local Aptos testnet
2. Creating an account Alice with multiple sequential transactions in mempool
3. Proposing a block containing transactions T5 and T6 where T6 will fail validation
4. Introducing artificial delay in the commit notification path (e.g., via time::sleep)
5. Observing that T7 remains in parking lot and is not available to consensus
6. Verifying T7 only becomes available after timeout expiration or manual resubmission

The race condition can be verified by examining mempool logs showing:
- Reject notification processed first (removing T6)
- Commit notification processed second (updating sequence number to 6)
- T7 remaining in parking lot despite having the correct sequence number (7)

## Notes

This is a legitimate liveness vulnerability stemming from the architectural decision to use two independent notification channels without synchronization. While it doesn't cause fund loss or consensus safety violations, it degrades transaction processing reliability and requires manual intervention, qualifying it as Medium severity per the Aptos bug bounty program criteria.

### Citations

**File:** mempool/src/shared_mempool/coordinator.rs (L61-61)
```rust
    mut quorum_store_requests: mpsc::Receiver<QuorumStoreRequest>,
```

**File:** mempool/src/shared_mempool/coordinator.rs (L62-62)
```rust
    mempool_listener: MempoolNotificationListener,
```

**File:** mempool/src/shared_mempool/coordinator.rs (L88-88)
```rust
    spawn_commit_notification_handler(&smp, mempool_listener);
```

**File:** mempool/src/shared_mempool/coordinator.rs (L108-129)
```rust
        ::futures::select! {
            msg = client_events.select_next_some() => {
                handle_client_request(&mut smp, &bounded_executor, msg).await;
            },
            msg = quorum_store_requests.select_next_some() => {
                tasks::process_quorum_store_request(&smp, msg);
            },
            reconfig_notification = mempool_reconfig_events.select_next_some() => {
                handle_mempool_reconfig_event(&mut smp, &bounded_executor, reconfig_notification.on_chain_configs).await;
            },
            (peer, backoff) = scheduled_broadcasts.select_next_some() => {
                tasks::execute_broadcast(peer, backoff, &mut smp, &mut scheduled_broadcasts, executor.clone()).await;
            },
            (network_id, event) = events.select_next_some() => {
                handle_network_event(&bounded_executor, &mut smp, network_id, event).await;
            },
            _ = update_peers_interval.tick().fuse() => {
                handle_update_peers(peers_and_metadata.clone(), &mut smp, &mut scheduled_broadcasts, executor.clone()).await;
            },
            complete => break,
        }
    }
```

**File:** mempool/src/shared_mempool/coordinator.rs (L152-162)
```rust
    tokio::spawn(async move {
        while let Some(commit_notification) = mempool_listener.next().await {
            handle_commit_notification(
                &mempool,
                &mempool_validator,
                &use_case_history,
                commit_notification,
                &num_committed_txns_received_since_peers_updated,
            );
        }
    });
```

**File:** mempool/src/core_mempool/transaction_store.rs (L547-596)
```rust
    fn process_ready_transaction(
        &mut self,
        address: &AccountAddress,
        txn_replay_protector: ReplayProtector,
    ) -> bool {
        if let Some(txns) = self.transactions.get_mut(address) {
            if let Some(txn) = txns.get_mut(&txn_replay_protector) {
                let sender_bucket = sender_bucket(address, self.num_sender_buckets);
                let ready_for_quorum_store = !self.priority_index.contains(txn);

                self.priority_index.insert(txn);

                // If timeline_state is `NonQualified`, then the transaction is never added to the timeline_index,
                // and never broadcasted to the shared mempool.
                let ready_for_mempool_broadcast = txn.timeline_state == TimelineState::NotReady;
                if ready_for_mempool_broadcast {
                    self.timeline_index
                        .get_mut(&sender_bucket)
                        .unwrap()
                        .insert(txn);
                }

                if ready_for_quorum_store {
                    let bucket = self
                        .timeline_index
                        .get(&sender_bucket)
                        .unwrap()
                        .get_bucket(txn.ranking_score);
                    let bucket = format!("{}_{}", sender_bucket, bucket);

                    Self::log_ready_transaction(
                        txn.ranking_score,
                        bucket.as_str(),
                        &mut txn.insertion_info,
                        ready_for_mempool_broadcast,
                        txn.priority_of_sender
                            .clone()
                            .map_or_else(|| "Unknown".to_string(), |priority| priority.to_string())
                            .as_str(),
                    );
                }
                // Remove txn from parking lot after it has been promoted to
                // priority_index / timeline_index, i.e., txn status is ready.
                self.parking_lot_index.remove(txn);

                return true;
            }
        }
        false
    }
```

**File:** mempool/src/core_mempool/transaction_store.rs (L603-633)
```rust
    fn process_ready_seq_num_based_transactions(
        &mut self,
        address: &AccountAddress,
        account_sequence_num: u64,
    ) {
        let mut min_seq = account_sequence_num;
        while self.process_ready_transaction(address, ReplayProtector::SequenceNumber(min_seq)) {
            min_seq += 1;
        }

        if let Some(txns) = self.transactions.get_mut(address) {
            let mut parking_lot_txns = 0;
            for (_, txn) in txns.seq_num_range_mut((Bound::Excluded(min_seq), Bound::Unbounded)) {
                match txn.timeline_state {
                    TimelineState::Ready(_) => {},
                    _ => {
                        self.parking_lot_index.insert(txn);
                        parking_lot_txns += 1;
                    },
                }
            }

            trace!(
                LogSchema::new(LogEntry::ProcessReadyTxns).account(*address),
                first_ready_seq_num = account_sequence_num,
                last_ready_seq_num = min_seq,
                num_parked_txns = parking_lot_txns,
            );
            self.track_indices();
        }
    }
```

**File:** mempool/src/core_mempool/transaction_store.rs (L671-707)
```rust
    pub fn commit_transaction(
        &mut self,
        account: &AccountAddress,
        replay_protector: ReplayProtector,
    ) {
        match replay_protector {
            ReplayProtector::SequenceNumber(txn_sequence_number) => {
                let current_account_seq_number =
                    self.get_account_sequence_number(account).map_or(0, |v| *v);
                let new_account_seq_number =
                    max(current_account_seq_number, txn_sequence_number + 1);
                self.account_sequence_numbers
                    .insert(*account, new_account_seq_number);
                self.clean_committed_transactions_below_account_seq_num(
                    account,
                    new_account_seq_number,
                );
                self.process_ready_seq_num_based_transactions(account, new_account_seq_number);
            },
            ReplayProtector::Nonce(nonce) => {
                if let Some(txns) = self.transactions.get_mut(account) {
                    if let Some(txn) = txns.remove(&ReplayProtector::Nonce(nonce)) {
                        self.index_remove(&txn);
                        trace!(
                            LogSchema::new(LogEntry::CleanCommittedTxn).txns(TxnsLog::new_txn(
                                txn.get_sender(),
                                txn.get_replay_protector()
                            )),
                            "txns cleaned with committing tx {}:{:?}",
                            txn.get_sender(),
                            txn.get_replay_protector()
                        );
                    }
                }
            },
        }
    }
```

**File:** mempool/src/core_mempool/transaction_store.rs (L709-736)
```rust
    pub fn reject_transaction(
        &mut self,
        account: &AccountAddress,
        replay_protector: ReplayProtector,
        hash: &HashValue,
    ) {
        let mut txn_to_remove = None;
        if let Some((indexed_account, indexed_replay_protector)) = self.hash_index.get(hash) {
            if account == indexed_account && replay_protector == *indexed_replay_protector {
                txn_to_remove = self.get_mempool_txn(account, replay_protector).cloned();
            }
        }
        if let Some(txn_to_remove) = txn_to_remove {
            if let Some(txns) = self.transactions.get_mut(account) {
                txns.remove(&replay_protector);
            }
            self.index_remove(&txn_to_remove);

            if aptos_logger::enabled!(Level::Trace) {
                let mut txns_log = TxnsLog::new();
                txns_log.add(
                    txn_to_remove.get_sender(),
                    txn_to_remove.get_replay_protector(),
                );
                trace!(LogSchema::new(LogEntry::CleanRejectedTxn).txns(txns_log));
            }
        }
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L923-974)
```rust
    /// Precondition: ledger update finishes
    /// What it does: For now this is mainly to notify mempool about failed transactions
    /// This is off critical path
    async fn post_ledger_update(
        prepare_fut: TaskFuture<PrepareResult>,
        ledger_update_fut: TaskFuture<LedgerUpdateResult>,
        mempool_notifier: Arc<dyn TxnNotifier>,
        block: Arc<Block>,
    ) -> TaskResult<PostLedgerUpdateResult> {
        let mut tracker = Tracker::start_waiting("post_ledger_update", &block);
        let (user_txns, _) = prepare_fut.await?;
        let (compute_result, _, _) = ledger_update_fut.await?;

        tracker.start_working();
        let compute_status = compute_result.compute_status_for_input_txns();
        // the length of compute_status is user_txns.len() + num_vtxns + 1 due to having blockmetadata
        if user_txns.len() >= compute_status.len() {
            // reconfiguration suffix blocks don't have any transactions
            // otherwise, this is an error
            if !compute_status.is_empty() {
                error!(
                        "Expected compute_status length and actual compute_status length mismatch! user_txns len: {}, compute_status len: {}, has_reconfiguration: {}",
                        user_txns.len(),
                        compute_status.len(),
                        compute_result.has_reconfiguration(),
                    );
            }
        } else {
            let user_txn_status = &compute_status[compute_status.len() - user_txns.len()..];
            // todo: avoid clone
            let txns: Vec<SignedTransaction> = user_txns
                .iter()
                .map(|txn| {
                    txn.borrow_into_inner()
                        .try_as_signed_user_txn()
                        .expect("must be a user txn")
                })
                .cloned()
                .collect();

            // notify mempool about failed transaction
            if let Err(e) = mempool_notifier
                .notify_failed_txn(&txns, user_txn_status)
                .await
            {
                error!(
                    error = ?e, "Failed to notify mempool of rejected txns",
                );
            }
        }
        Ok(())
    }
```

**File:** consensus/src/txn_notifier.rs (L48-99)
```rust
    async fn notify_failed_txn(
        &self,
        user_txns: &[SignedTransaction],
        user_txn_statuses: &[TransactionStatus],
    ) -> Result<(), MempoolError> {
        if user_txns.len() != user_txn_statuses.len() {
            return Err(format_err!(
                "[MempoolNotifier] {} != {}",
                user_txns.len(),
                user_txn_statuses.len()
            )
            .into());
        }

        let mut rejected_txns = vec![];
        for (txn, status) in user_txns.iter().zip_eq(user_txn_statuses) {
            if let TransactionStatus::Discard(reason) = status {
                rejected_txns.push(RejectedTransactionSummary {
                    sender: txn.sender(),
                    replay_protector: txn.replay_protector(),
                    hash: txn.committed_hash(),
                    reason: *reason,
                });
            }
        }

        if rejected_txns.is_empty() {
            return Ok(());
        }

        let (callback, callback_rcv) = oneshot::channel();
        let req = QuorumStoreRequest::RejectNotification(rejected_txns, callback);

        // send to shared mempool
        self.consensus_to_mempool_sender
            .clone()
            .try_send(req)
            .map_err(anyhow::Error::from)?;

        if let Err(e) = monitor!(
            "notify_mempool",
            timeout(
                Duration::from_millis(self.mempool_executed_txn_timeout_ms),
                callback_rcv
            )
            .await
        ) {
            Err(format_err!("[consensus] txn notifier did not receive ACK for commit notification sent to mempool on time: {:?}", e).into())
        } else {
            Ok(())
        }
    }
```

**File:** state-sync/inter-component/mempool-notifications/src/lib.rs (L76-116)
```rust
impl MempoolNotificationSender for MempoolNotifier {
    async fn notify_new_commit(
        &self,
        transactions: Vec<Transaction>,
        block_timestamp_usecs: u64,
    ) -> Result<(), Error> {
        // Get only user transactions from committed transactions
        let user_transactions: Vec<CommittedTransaction> = transactions
            .iter()
            .filter_map(|transaction| match transaction {
                Transaction::UserTransaction(signed_txn) => Some(CommittedTransaction {
                    sender: signed_txn.sender(),
                    replay_protector: signed_txn.replay_protector(),
                    use_case: signed_txn.parse_use_case(),
                }),
                _ => None,
            })
            .collect();

        // Mempool needs to be notified about all transactions (user and non-user transactions).
        // See https://github.com/aptos-labs/aptos-core/issues/1882 for more details.
        let commit_notification = MempoolCommitNotification {
            transactions: user_transactions,
            block_timestamp_usecs,
        };

        // Send the notification to mempool
        if let Err(error) = self
            .notification_sender
            .clone()
            .send(commit_notification)
            .await
        {
            return Err(Error::CommitNotificationError(format!(
                "Failed to notify mempool of committed transactions! Error: {:?}",
                error
            )));
        }

        Ok(())
    }
```
