# Audit Report

## Title
DAG Broadcast Premature Abortion Causes Validator Crash and Potential Consensus Divergence

## Summary
The `rb_handles` bounded queue in `DagDriver` evicts old broadcast abort handles when full, immediately terminating in-flight certified node broadcasts. This creates inconsistent DAG views across validators, leading to validator crashes from assertion failures and potential consensus safety violations.

## Finding Description

The `DagDriver` maintains reliable broadcast abort handles in a bounded queue (`rb_handles`) initialized with capacity equal to `dag_ordering_causal_history_window` (default: 10 rounds). [1](#0-0) [2](#0-1) [3](#0-2) 

When a validator advances through rounds quickly, pushing new entries into the full queue evicts the oldest `DropGuard`: [4](#0-3) 

The evicted `DropGuard` immediately aborts the associated broadcast task when dropped: [5](#0-4) 

Each broadcast task executes two joined phases: (1) broadcasting the uncertified node to collect signatures, and (2) broadcasting the certified node once certified: [6](#0-5) 

**Critical Issue**: When the abort occurs many rounds later during certified node broadcast, the reliable broadcast terminates mid-flight. Since reliable broadcast includes self-send and retries to all validators, aborting mid-broadcast means: [7](#0-6) 

- Some validators successfully receive and add the certified node
- Other validators (including potentially the originating validator via self-send) never receive it
- Different validators maintain inconsistent DAG views

**Validator Crash Mechanism**: When entering a new round, the code computes strong links for the previous round. If insufficient voting power exists (< 2f+1), `get_strong_links_for_round` returns `None`: [8](#0-7) 

The calling code in `enter_new_round` unwraps this with an assertion that only permits empty strong links for round 1: [9](#0-8) 

For any round > 1, this assertion fails, causing a panic that crashes the validator.

**Consensus Divergence**: Different validators computing different strong links for the same round create nodes with different parent sets, leading to:
- Divergent DAG structures across the network
- Different anchor selections in subsequent rounds
- Potential violation of consensus safety (different committed blocks)

## Impact Explanation

**High Severity - Validator Crashes**:
When a validator's own certified node is missing from its DAG due to aborted self-send, and this causes insufficient voting power in that round, attempting to advance to the next round triggers the assertion failure at line 217, panicking the validator. This meets Aptos bug bounty High severity criteria: "Validator node slowdowns" and "Significant protocol violations" as crashed validators cannot participate in consensus.

**Critical Severity - Consensus Divergence**:
Different validators maintaining different subsets of certified nodes in their DAGs compute different strong links and create nodes with different parents. This fundamental inconsistency in DAG structure can lead to different anchor elections and different blocks being committed across the network, potentially violating the consensus safety invariant. This meets Critical severity criteria: "Consensus/Safety violations."

## Likelihood Explanation

**Likelihood: Medium-High**

This vulnerability triggers during normal network operation without requiring an attacker:

1. **Realistic Trigger Conditions**: Validators experiencing temporary network delays followed by rapid catch-up naturally advance through many rounds quickly, filling the queue and triggering evictions.

2. **Low Window Size**: With the default `dag_ordering_causal_history_window` of 10 rounds, advancing just 11 rounds causes eviction. This is easily achievable during normal operation.

3. **Timing Window**: Reliable broadcast continues retrying until receiving sufficient acknowledgments. With network latency and asynchronous operations, there exists a realistic window where broadcasts remain in-flight when the validator advances multiple rounds.

4. **Common Scenarios**: This naturally occurs during validator restarts with catch-up, network partition recovery, or periods of high consensus velocity.

## Recommendation

Implement one or more of the following fixes:

1. **Ensure Broadcast Completion Before Advancing**: Do not allow round advancement until the current round's certified node broadcast has completed successfully, ensuring the validator's own DAG includes its certified node.

2. **Separate Self-Add from Broadcast**: Add the validator's own certified node to its local DAG immediately after certification, independent of the broadcast to other validators.

3. **Unbounded or Larger Queue**: Use an unbounded queue for abort handles, or significantly increase the window size to prevent premature evictions during normal operation.

4. **Graceful Degradation**: Instead of asserting on missing strong links, implement graceful handling that triggers synchronization with peers to recover missing nodes.

## Proof of Concept

```rust
// This demonstrates the bounded queue eviction behavior
// Run in consensus/src/dag/ context

#[cfg(test)]
mod vulnerability_test {
    use super::*;
    use aptos_collections::BoundedVecDeque;
    
    #[test]
    fn test_broadcast_abort_on_eviction() {
        // Simulate window_size_config = 10
        let mut rb_handles = BoundedVecDeque::new(10);
        
        // Fill queue with 10 rounds
        for i in 0..10 {
            let (_abort_handle, abort_registration) = AbortHandle::new_pair();
            let guard = DropGuard::new(abort_handle);
            rb_handles.push_back((guard, i as u64));
        }
        
        // 11th round causes eviction
        let (_abort_handle, abort_registration) = AbortHandle::new_pair();
        let guard = DropGuard::new(abort_handle);
        let evicted = rb_handles.push_back((guard, 10u64));
        
        // evicted.is_some() means the oldest DropGuard was dropped,
        // aborting the broadcast for round 0
        assert!(evicted.is_some());
        
        // The DropGuard for round 0 is now dropped, aborting its broadcast
        // If that broadcast was still in the certified node phase,
        // some validators may not have received it
    }
}
```

**Notes**:
- The vulnerability requires validators to advance through `window_size_config + 1` rounds while a broadcast remains in-flight, which is realistic during network delays and catch-up scenarios
- The default window size of 10 makes this more likely than larger configurations
- The crash specifically occurs when `get_strong_links_for_round` returns `None` for any round > 1, triggering the assertion at line 217
- Consensus divergence severity depends on whether the ordering rules can tolerate partial DAG views, which requires deeper analysis of the anchor selection and ordering mechanisms

### Citations

**File:** consensus/src/dag/dag_driver.rs (L57-57)
```rust
    rb_handles: Mutex<BoundedVecDeque<(DropGuard, u64)>>,
```

**File:** consensus/src/dag/dag_driver.rs (L103-103)
```rust
            rb_handles: Mutex::new(BoundedVecDeque::new(window_size_config as usize)),
```

**File:** consensus/src/dag/dag_driver.rs (L214-219)
```rust
            let strong_links = dag_reader
                .get_strong_links_for_round(new_round - 1, &self.epoch_state.verifier)
                .unwrap_or_else(|| {
                    assert_eq!(new_round, 1, "Only expect empty strong links for round 1");
                    vec![]
                });
```

**File:** consensus/src/dag/dag_driver.rs (L333-370)
```rust
        let node_broadcast = async move {
            debug!(LogSchema::new(LogEvent::BroadcastNode), id = node.id());

            defer!( observe_round(timestamp, RoundStage::NodeBroadcasted); );
            rb.broadcast(node, signature_builder)
                .await
                .expect("Broadcast cannot fail")
        };
        let certified_broadcast = async move {
            let Ok(certificate) = rx.await else {
                error!("channel closed before receiving ceritifcate");
                return;
            };

            debug!(
                LogSchema::new(LogEvent::BroadcastCertifiedNode),
                id = node_clone.id()
            );

            defer!( observe_round(timestamp, RoundStage::CertifiedNodeBroadcasted); );
            let certified_node =
                CertifiedNode::new(node_clone, certificate.signatures().to_owned());
            let certified_node_msg = CertifiedNodeMessage::new(
                certified_node,
                latest_ledger_info.get_latest_ledger_info(),
            );
            rb2.broadcast(certified_node_msg, cert_ack_set)
                .await
                .expect("Broadcast cannot fail until cancelled")
        };
        let core_task = join(node_broadcast, certified_broadcast);
        let author = self.author;
        let task = async move {
            debug!("{} Start reliable broadcast for round {}", author, round);
            core_task.await;
            debug!("Finish reliable broadcast for round {}", round);
        };
        tokio::spawn(Abortable::new(task, abort_registration));
```

**File:** types/src/on_chain_config/consensus_config.rs (L594-594)
```rust
            dag_ordering_causal_history_window: 10,
```

**File:** crates/aptos-collections/src/bounded_vec_deque.rs (L28-38)
```rust
    pub fn push_back(&mut self, item: T) -> Option<T> {
        let oldest = if self.is_full() {
            self.inner.pop_front()
        } else {
            None
        };

        self.inner.push_back(item);
        assert!(self.inner.len() <= self.capacity);
        oldest
    }
```

**File:** crates/reliable-broadcast/src/lib.rs (L146-146)
```rust
                    let send_fut = if receiver == self_author {
```

**File:** crates/reliable-broadcast/src/lib.rs (L232-236)
```rust
impl Drop for DropGuard {
    fn drop(&mut self) {
        self.abort_handle.abort();
    }
}
```

**File:** consensus/src/dag/dag_store.rs (L346-367)
```rust
    pub fn get_strong_links_for_round(
        &self,
        round: Round,
        validator_verifier: &ValidatorVerifier,
    ) -> Option<Vec<NodeCertificate>> {
        if validator_verifier
            .check_voting_power(
                self.get_round_iter(round)?
                    .map(|node_status| node_status.as_node().metadata().author()),
                true,
            )
            .is_ok()
        {
            Some(
                self.get_round_iter(round)?
                    .map(|node_status| node_status.as_node().certificate())
                    .collect(),
            )
        } else {
            None
        }
    }
```
