# Audit Report

## Title
State KV Pruner Catch-Up Deletes Historical Data When Prune Window Increases, Breaking State Sync Serving

## Summary
When the `prune_window` configuration is increased, the `StateKvShardPruner` catch-up pruning mechanism uses stale metadata progress from the database, causing permanent deletion of historical state values that should be retained under the new configuration. This creates a gap in available historical data where the storage service advertises availability but cannot serve requests.

## Finding Description

The vulnerability exists in the initialization flow of `StateKvShardPruner`. During node restart after a `prune_window` configuration increase, the shard pruner performs catch-up pruning using stale metadata progress that reflects the OLD prune window value. [1](#0-0) 

The metadata pruner's progress is retrieved from persistent storage without validation against the current prune window configuration: [2](#0-1) 

Each shard pruner is initialized with this stale metadata_progress and immediately performs catch-up pruning: [3](#0-2) 

The catch-up prune operation deletes all state values between the shard's progress and the metadata_progress: [4](#0-3) 

**Exploitation Scenario:**

1. **Initial State:** Node runs with `prune_window = 100,000`, latest version = 1,000,000
   - Metadata pruner progress in DB = 900,000
   - Shard 0 pruner progress in DB = 850,000

2. **Configuration Change:** Operator increases `prune_window = 200,000` to retain more history

3. **Node Restart:** Catch-up pruning executes
   - Reads metadata_progress = 900,000 from database (OLD value)
   - Reads shard progress = 850,000 from database
   - Executes `prune(850000, 900000)` â€” permanently deletes versions 850,001 to 900,000

4. **Post-Initialization:** New prune window takes effect
   - `min_readable_version = 1,000,000 - 200,000 = 800,000` calculated by the pruner manager: [5](#0-4) 

5. **Gap Created:** Versions 850,001 to 900,000 are advertised as available but deleted
   - Storage service advertises range based on configured prune_window: [6](#0-5) 

   - State value requests check against min_readable_version and fail when data is missing: [7](#0-6) [8](#0-7) 

The root cause is the absence of validation in `get_or_initialize_subpruner_progress()`: [9](#0-8) 

This function retrieves or initializes shard progress using metadata_progress without checking if the metadata_progress is compatible with the current prune_window configuration.

## Impact Explanation

This is a **MEDIUM severity** issue under the Aptos bug bounty program criteria for "Limited protocol violations":

1. **State Inconsistency:** The storage service advertises data availability based on the configured prune_window, but the actual data has a gap due to premature deletion during catch-up pruning

2. **State Sync Protocol Violation:** Nodes cannot serve historical state values to peers requesting sync in the gap range, breaking the data availability contract

3. **Manual Intervention Required:** Recovery requires re-syncing from nodes with complete history or genesis

4. **Permanent Data Loss:** The deleted state values cannot be recovered without external intervention

This does not reach HIGH severity because:
- It does not cause validator slowdowns (validators can sync from other nodes)
- It does not crash APIs
- The network continues operating normally
- Consensus is not affected

This does not reach CRITICAL severity because:
- No loss of funds
- No consensus safety violations
- Network continues operating
- Workarounds exist (sync from unaffected nodes)

## Likelihood Explanation

**Likelihood: MEDIUM**

This issue occurs when:
1. Node operators increase the `prune_window` configuration (legitimate operational activity)
2. The shard pruner progress lags behind metadata pruner progress (common during normal operation)
3. The node restarts (normal maintenance or configuration reload)

Common triggering scenarios:
- Operators increasing history retention for improved state sync serving
- Network-wide configuration updates to improve data availability
- Recovery operations after pruner performance issues

The lack of validation makes the issue deterministic when these conditions are met.

## Recommendation

Add validation during `StateKvShardPruner` initialization to ensure metadata_progress does not exceed the minimum readable version calculated from the current prune_window:

```rust
pub(in crate::pruner) fn new(
    shard_id: usize,
    db_shard: Arc<DB>,
    metadata_progress: Version,
    current_prune_window: Version,
    latest_version: Version,
) -> Result<Self> {
    let progress = get_or_initialize_subpruner_progress(
        &db_shard,
        &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
        metadata_progress,
    )?;
    
    // Calculate min_readable based on current prune_window
    let min_readable_version = latest_version.saturating_sub(current_prune_window);
    
    // Use the minimum of metadata_progress and min_readable_version for catch-up
    let safe_target = std::cmp::min(metadata_progress, min_readable_version);
    
    let myself = Self { shard_id, db_shard };

    info!(
        progress = progress,
        safe_target = safe_target,
        "Catching up state kv shard {shard_id}."
    );
    myself.prune(progress, safe_target)?;

    Ok(myself)
}
```

Alternatively, update the metadata_progress in the database when prune_window increases before initializing shard pruners.

## Proof of Concept

```rust
#[test]
fn test_prune_window_increase_causes_data_gap() {
    // Setup: Create database with prune_window=100k
    let tmp_dir = TempPath::new();
    let db = AptosDB::new_for_test(&tmp_dir);
    
    // Insert state values at versions 800k to 1M
    for version in 800_000..=1_000_000 {
        // Insert state value at version
    }
    
    // Prune with window=100k, setting metadata_progress=900k
    let pruner_config = LedgerPrunerConfig {
        enable: true,
        prune_window: 100_000,
        ..Default::default()
    };
    // Run pruning to metadata_progress=900k
    
    // Simulate shard lagging at 850k
    db.put::<DbMetadataSchema>(
        &DbMetadataKey::StateKvShardPrunerProgress(0),
        &DbMetadataValue::Version(850_000)
    ).unwrap();
    
    // Restart with increased prune_window=200k
    let new_config = LedgerPrunerConfig {
        enable: true,
        prune_window: 200_000,
        ..Default::default()
    };
    
    // Initialize pruner (triggers catch-up from 850k to 900k)
    let pruner_manager = StateKvPrunerManager::new(db.state_kv_db(), new_config);
    
    // Verify: min_readable_version = 800k
    assert_eq!(pruner_manager.get_min_readable_version(), 800_000);
    
    // Verify: versions 850,001 to 900,000 are deleted (GAP)
    for version in 850_001..=900_000 {
        let result = db.get_state_value_by_version(&state_key, version);
        assert!(result.is_err()); // Data is missing!
    }
    
    // Verify: versions 800,001 to 850,000 still exist
    for version in 800_001..=850_000 {
        let result = db.get_state_value_by_version(&state_key, version);
        assert!(result.is_ok()); // Data exists
    }
}
```

## Notes

This vulnerability demonstrates a gap between configuration-based availability advertising and actual data retention. The storage service calculates advertised ranges using the CONFIGURED prune_window, but catch-up pruning uses PERSISTED metadata_progress that may reflect an older, smaller prune_window. This mismatch creates a data availability gap that violates protocol guarantees.

The severity is MEDIUM rather than HIGH because the impact is limited to state sync serving for a specific historical version range, and the network continues operating normally with consensus unaffected. However, it does constitute a protocol violation requiring manual intervention to resolve.

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L112-153)
```rust
    pub fn new(state_kv_db: Arc<StateKvDb>) -> Result<Self> {
        info!(name = STATE_KV_PRUNER_NAME, "Initializing...");

        let metadata_pruner = StateKvMetadataPruner::new(Arc::clone(&state_kv_db));

        let metadata_progress = metadata_pruner.progress()?;

        info!(
            metadata_progress = metadata_progress,
            "Created state kv metadata pruner, start catching up all shards."
        );

        let shard_pruners = if state_kv_db.enabled_sharding() {
            let num_shards = state_kv_db.num_shards();
            let mut shard_pruners = Vec::with_capacity(num_shards);
            for shard_id in 0..num_shards {
                shard_pruners.push(StateKvShardPruner::new(
                    shard_id,
                    state_kv_db.db_shard_arc(shard_id),
                    metadata_progress,
                )?);
            }
            shard_pruners
        } else {
            Vec::new()
        };

        let pruner = StateKvPruner {
            target_version: AtomicVersion::new(metadata_progress),
            progress: AtomicVersion::new(metadata_progress),
            metadata_pruner,
            shard_pruners,
        };

        info!(
            name = pruner.name(),
            progress = metadata_progress,
            "Initialized."
        );

        Ok(pruner)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs (L75-81)
```rust
    pub(in crate::pruner) fn progress(&self) -> Result<Version> {
        Ok(get_progress(
            self.state_kv_db.metadata_db(),
            &DbMetadataKey::StateKvPrunerProgress,
        )?
        .unwrap_or(0))
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L25-45)
```rust
    pub(in crate::pruner) fn new(
        shard_id: usize,
        db_shard: Arc<DB>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
            metadata_progress,
        )?;
        let myself = Self { shard_id, db_shard };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up state kv shard {shard_id}."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L47-72)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();

        let mut iter = self
            .db_shard
            .iter::<StaleStateValueIndexByKeyHashSchema>()?;
        iter.seek(&current_progress)?;
        for item in iter {
            let (index, _) = item?;
            if index.stale_since_version > target_version {
                break;
            }
            batch.delete::<StaleStateValueIndexByKeyHashSchema>(&index)?;
            batch.delete::<StateValueByKeyHashSchema>(&(index.state_key_hash, index.version))?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
            &DbMetadataValue::Version(target_version),
        )?;

        self.db_shard.write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_pruner_manager.rs (L128-142)
```rust
    fn set_pruner_target_db_version(&self, latest_version: Version) {
        assert!(self.pruner_worker.is_some());
        let min_readable_version = latest_version.saturating_sub(self.prune_window);
        self.min_readable_version
            .store(min_readable_version, Ordering::SeqCst);

        PRUNER_VERSIONS
            .with_label_values(&["state_kv_pruner", "min_readable"])
            .set(min_readable_version as i64);

        self.pruner_worker
            .as_ref()
            .unwrap()
            .set_target_db_version(min_readable_version);
    }
```

**File:** state-sync/storage-service/server/src/storage.rs (L146-176)
```rust
    fn fetch_state_values_range(
        &self,
        latest_version: Version,
        transactions_range: &Option<CompleteDataRange<Version>>,
    ) -> aptos_storage_service_types::Result<Option<CompleteDataRange<Version>>, Error> {
        let pruner_enabled = self.storage.is_state_merkle_pruner_enabled()?;
        if !pruner_enabled {
            return Ok(*transactions_range);
        }
        let pruning_window = self.storage.get_epoch_snapshot_prune_window()?;

        if latest_version > pruning_window as Version {
            // lowest_state_version = latest_version - pruning_window + 1;
            let mut lowest_state_version = latest_version
                .checked_sub(pruning_window as Version)
                .ok_or_else(|| {
                    Error::UnexpectedErrorEncountered("Lowest state version has overflown!".into())
                })?;
            lowest_state_version = lowest_state_version.checked_add(1).ok_or_else(|| {
                Error::UnexpectedErrorEncountered("Lowest state version has overflown!".into())
            })?;

            // Create the state range
            let state_range = CompleteDataRange::new(lowest_state_version, latest_version)
                .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
            return Ok(Some(state_range));
        }

        // No pruning has occurred. Return the transactions range.
        Ok(*transactions_range)
    }
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L305-315)
```rust
    pub(super) fn error_if_state_kv_pruned(&self, data_type: &str, version: Version) -> Result<()> {
        let min_readable_version = self.state_store.state_kv_pruner.get_min_readable_version();
        ensure!(
            version >= min_readable_version,
            "{} at version {} is pruned, min available version is {}.",
            data_type,
            version,
            min_readable_version
        );
        Ok(())
    }
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L631-641)
```rust
    fn get_state_value_by_version(
        &self,
        state_store_key: &StateKey,
        version: Version,
    ) -> Result<Option<StateValue>> {
        gauged_api("get_state_value_by_version", || {
            self.error_if_state_kv_pruned("StateValue", version)?;

            self.state_store
                .get_state_value_by_version(state_store_key, version)
        })
```

**File:** storage/aptosdb/src/pruner/pruner_utils.rs (L44-60)
```rust
pub(crate) fn get_or_initialize_subpruner_progress(
    sub_db: &DB,
    progress_key: &DbMetadataKey,
    metadata_progress: Version,
) -> Result<Version> {
    Ok(
        if let Some(v) = sub_db.get::<DbMetadataSchema>(progress_key)? {
            v.expect_version()
        } else {
            sub_db.put::<DbMetadataSchema>(
                progress_key,
                &DbMetadataValue::Version(metadata_progress),
            )?;
            metadata_progress
        },
    )
}
```
