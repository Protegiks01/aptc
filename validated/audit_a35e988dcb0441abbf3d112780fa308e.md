# Audit Report

## Title
State Snapshot Backup Iterator Invalidation via Concurrent Pruning

## Summary
The `state_snapshot` endpoint in the backup service creates a lazy iterator over state merkle tree nodes without database snapshot isolation or min_readable_version validation. If the state merkle pruner deletes nodes during iteration, the iterator fails mid-stream with `AptosDbError::NotFound`, causing backup corruption.

## Finding Description

The backup service endpoint creates an iterator over all state items at a specific version without any protection against concurrent pruning. [1](#0-0) 

This endpoint calls `get_state_item_iter` which creates a lazy iterator: [2](#0-1) 

The iterator implementation delegates to `get_state_key_and_value_iter`, which uses `JellyfishMerkleIterator::new_by_index`: [3](#0-2) 

The JellyfishMerkleIterator reads nodes on-demand during traversal by calling `reader.get_node()`: [4](#0-3) 

Each `get_node` call translates to `get_node_option`, which reads directly from RocksDB without snapshot isolation: [5](#0-4) 

The database read at line 888-890 uses direct RocksDB access: [6](#0-5) 

Line 222 shows `self.inner.get_cf()` - a direct RocksDB read without snapshot isolation.

Meanwhile, the state merkle pruner operates independently and can delete nodes. Pruning is triggered when: [7](#0-6) 

The actual node deletion occurs via: [8](#0-7) 

**Critical Gap**: The backup handler does NOT call `error_if_state_merkle_pruned` before creating the iterator. This function, which validates version availability, is defined as: [9](#0-8) 

When a deleted node is accessed, `get_node_option` returns `None`, which `get_node` converts to an error: [10](#0-9) 

The default prune window is 1,000,000 versions: [11](#0-10) 

**Race Condition Flow**:
1. Backup request arrives for version V (e.g., `GET /state_snapshot/1000000`)
2. Iterator created without version validation
3. Iterator begins traversing tree, reading nodes lazily via `get_node()`
4. Pruner determines V < min_readable_version (e.g., latest=2,100,000, prune_window=1,000,000)
5. Pruner deletes nodes at version V that haven't been read yet
6. Iterator attempts to read deleted node via `get_node_option()`
7. Returns `None`, `get_node()` converts to `AptosDbError::NotFound`
8. Error propagates through iterator chain, backup fails mid-stream with HTTP 500

## Impact Explanation

This is a **High Severity** vulnerability per Aptos bug bounty criteria:

**API Crashes (High)**: The backup service endpoint returns HTTP 500 errors mid-stream when iterator invalidation occurs, causing backup client failures and operational disruption.

**Operational Impact**: 
- Corrupted/incomplete backups undermine disaster recovery capabilities
- State sync nodes consuming backup data may receive partial state snapshots
- Backup monitoring systems receive inconsistent success/failure signals
- Operators cannot reliably backup historical state within the prune window

This does not reach Critical severity because:
- No funds are directly at risk
- Consensus is not affected
- Primary chain operation continues normally
- Only affects backup infrastructure

## Likelihood Explanation

**High Likelihood** of occurrence:

1. **Automatic Triggers**: Pruning runs automatically via background worker when `latest_version >= min_readable_version + prune_window`, requiring no attacker action

2. **Timing Window**: With default prune_window of 1,000,000 versions and typical throughput, backups of historical versions near the prune boundary are routinely at risk

3. **Long-Running Operations**: Full state snapshots can take minutes to hours depending on state size, increasing the exposure window significantly

4. **No Coordination**: No synchronization mechanism exists between backup service and pruner - they operate completely independently

**Real-World Scenario**: 
- Latest version reaches 2,000,000
- Backup operator requests snapshot at version 1,050,000 (within prune window)
- Backup starts and reads 40% of state tree (takes 10 minutes)
- New transactions push latest to 2,100,000
- Pruner triggers: min_readable_version becomes 1,100,000
- Pruner deletes nodes at version 1,050,000 that haven't been read yet
- Backup fails with "Missing node at NodeKey(...)" error

## Recommendation

Implement one of the following mitigations:

1. **Add Version Validation**: Call `error_if_state_merkle_pruned(version)` in `get_state_item_iter()` before creating the iterator to fail fast if the version is already pruned

2. **Use Snapshot Isolation**: Create a RocksDB snapshot at the start of backup and use it for all reads during iteration

3. **Coordination with Pruner**: Implement a reference counting or lease mechanism to prevent pruning of versions currently being backed up

4. **Chunked Validation**: For chunked endpoints (`state_snapshot_chunk`), validate version availability at the start of each chunk request

## Proof of Concept

The vulnerability can be demonstrated by:

1. Starting a backup service with pruning enabled (default configuration)
2. Advancing the chain to version 2,000,000
3. Initiating a backup at version 1,050,000 via `GET /state_snapshot/1050000`
4. During the backup, advance the chain to version 2,100,000 through transaction submission
5. The pruner will trigger and set min_readable_version to 1,100,000
6. The backup iterator will encounter `AptosDbError::NotFound` when accessing nodes that were deleted
7. The HTTP response will terminate mid-stream with an error

A Rust test demonstrating this would require:
- Spinning up a full node with backup service
- Controlling transaction submission to advance versions
- Monitoring backup progress and pruner activity
- Observing the failure when pruner deletes nodes during active backup

The test would confirm that the iterator fails with "Missing node" errors when pruning occurs concurrently with backup operations.

## Notes

This vulnerability represents a real operational risk for Aptos node operators performing backups. While it doesn't affect consensus or chain security directly, reliable backups are critical infrastructure for disaster recovery. The race condition is inherent in the current design where the backup service and pruner operate independently without coordination. The fix requires either adding validation to detect pruned versions early, or implementing proper isolation mechanisms to protect in-flight backup operations from concurrent pruning.

### Citations

**File:** storage/backup/backup-service/src/handlers/mod.rs (L47-56)
```rust
    // GET state_snapshot/<version>
    let bh = backup_handler.clone();
    let state_snapshot = warp::path!(Version)
        .map(move |version| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT, move |bh, sender| {
                bh.get_state_item_iter(version, 0, usize::MAX)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);
```

**File:** storage/aptosdb/src/backup/backup_handler.rs (L145-162)
```rust
    pub fn get_state_item_iter(
        &self,
        version: Version,
        start_idx: usize,
        limit: usize,
    ) -> Result<impl Iterator<Item = Result<(StateKey, StateValue)>> + Send + use<>> {
        let iterator = self
            .state_store
            .get_state_key_and_value_iter(version, start_idx)?
            .take(limit)
            .enumerate()
            .map(move |(idx, res)| {
                BACKUP_STATE_SNAPSHOT_VERSION.set(version as i64);
                BACKUP_STATE_SNAPSHOT_LEAF_IDX.set((start_idx + idx) as i64);
                res
            });
        Ok(Box::new(iterator))
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1064-1081)
```rust
    pub fn get_state_key_and_value_iter(
        self: &Arc<Self>,
        version: Version,
        start_idx: usize,
    ) -> Result<impl Iterator<Item = Result<(StateKey, StateValue)>> + Send + Sync + use<>> {
        let store = Arc::clone(self);
        Ok(JellyfishMerkleIterator::new_by_index(
            Arc::clone(&self.state_merkle_db),
            version,
            start_idx,
        )?
        .map(move |res| match res {
            Ok((_hashed_key, (key, version))) => {
                Ok((key.clone(), store.expect_value_by_version(&key, version)?))
            },
            Err(err) => Err(err),
        }))
    }
```

**File:** storage/jellyfish-merkle/src/iterator/mod.rs (L329-343)
```rust
            match self.reader.get_node(&node_key) {
                Ok(Node::Internal(internal_node)) => {
                    let visit_info = NodeVisitInfo::new(node_key, internal_node);
                    self.parent_stack.push(visit_info);
                },
                Ok(Node::Leaf(leaf_node)) => {
                    let ret = (*leaf_node.account_key(), leaf_node.value_index().clone());
                    Self::cleanup_stack(&mut self.parent_stack);
                    return Some(Ok(ret));
                },
                Ok(Node::Null) => {
                    unreachable!("When tree is empty, done should be already set to true")
                },
                Err(err) => return Some(Err(err)),
            }
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L856-898)
```rust
    fn get_node_option(&self, node_key: &NodeKey, tag: &str) -> Result<Option<Node>> {
        let start_time = Instant::now();
        if !self.cache_enabled() {
            let node_opt = self
                .db_by_key(node_key)
                .get::<JellyfishMerkleNodeSchema>(node_key)?;
            NODE_CACHE_SECONDS
                .observe_with(&[tag, "cache_disabled"], start_time.elapsed().as_secs_f64());
            return Ok(node_opt);
        }
        if let Some(node_cache) = self
            .version_caches
            .get(&node_key.get_shard_id())
            .unwrap()
            .get_version(node_key.version())
        {
            let node = node_cache.get(node_key).cloned();
            NODE_CACHE_SECONDS.observe_with(
                &[tag, "versioned_cache_hit"],
                start_time.elapsed().as_secs_f64(),
            );
            return Ok(node);
        }

        if let Some(lru_cache) = &self.lru_cache {
            if let Some(node) = lru_cache.get(node_key) {
                NODE_CACHE_SECONDS
                    .observe_with(&[tag, "lru_cache_hit"], start_time.elapsed().as_secs_f64());
                return Ok(Some(node));
            }
        }

        let node_opt = self
            .db_by_key(node_key)
            .get::<JellyfishMerkleNodeSchema>(node_key)?;
        if let Some(lru_cache) = &self.lru_cache {
            if let Some(node) = &node_opt {
                lru_cache.put(node_key.clone(), node.clone());
            }
        }
        NODE_CACHE_SECONDS.observe_with(&[tag, "cache_miss"], start_time.elapsed().as_secs_f64());
        Ok(node_opt)
    }
```

**File:** storage/schemadb/src/lib.rs (L216-232)
```rust
    pub fn get<S: Schema>(&self, schema_key: &S::Key) -> DbResult<Option<S::Value>> {
        let _timer = APTOS_SCHEMADB_GET_LATENCY_SECONDS.timer_with(&[S::COLUMN_FAMILY_NAME]);

        let k = <S::Key as KeyCodec<S>>::encode_key(schema_key)?;
        let cf_handle = self.get_cf_handle(S::COLUMN_FAMILY_NAME)?;

        let result = self.inner.get_cf(cf_handle, k).into_db_res()?;
        APTOS_SCHEMADB_GET_BYTES.observe_with(
            &[S::COLUMN_FAMILY_NAME],
            result.as_ref().map_or(0.0, |v| v.len() as f64),
        );

        result
            .map(|raw_value| <S::Value as ValueCodec<S>>::decode_value(&raw_value))
            .transpose()
            .map_err(Into::into)
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_pruner_manager.rs (L67-72)
```rust
    fn maybe_set_pruner_target_db_version(&self, latest_version: Version) {
        let min_readable_version = self.get_min_readable_version();
        if self.is_pruner_enabled() && latest_version >= min_readable_version + self.prune_window {
            self.set_pruner_target_db_version(latest_version);
        }
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L73-76)
```rust
            indices.into_iter().try_for_each(|index| {
                batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
                batch.delete::<S>(&index)
            })?;
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L273-303)
```rust
    pub(super) fn error_if_state_merkle_pruned(
        &self,
        data_type: &str,
        version: Version,
    ) -> Result<()> {
        let min_readable_version = self
            .state_store
            .state_db
            .state_merkle_pruner
            .get_min_readable_version();
        if version >= min_readable_version {
            return Ok(());
        }

        let min_readable_epoch_snapshot_version = self
            .state_store
            .state_db
            .epoch_snapshot_pruner
            .get_min_readable_version();
        if version >= min_readable_epoch_snapshot_version {
            self.ledger_db.metadata_db().ensure_epoch_ending(version)
        } else {
            bail!(
                "{} at version {} is pruned. snapshots are available at >= {}, epoch snapshots are available at >= {}",
                data_type,
                version,
                min_readable_version,
                min_readable_epoch_snapshot_version,
            )
        }
    }
```

**File:** storage/jellyfish-merkle/src/lib.rs (L121-129)
```rust
    fn get_node(&self, node_key: &NodeKey) -> Result<Node<K>> {
        self.get_node_with_tag(node_key, "unknown")
    }

    /// Gets node given a node key. Returns error if the node does not exist.
    fn get_node_with_tag(&self, node_key: &NodeKey, tag: &str) -> Result<Node<K>> {
        self.get_node_option(node_key, tag)?
            .ok_or_else(|| AptosDbError::NotFound(format!("Missing node at {:?}.", node_key)))
    }
```

**File:** config/src/config/storage_config.rs (L400-412)
```rust
        StateMerklePrunerConfig {
            enable: true,
            // This allows a block / chunk being executed to have access to a non-latest state tree.
            // It needs to be greater than the number of versions the state committing thread is
            // able to commit during the execution of the block / chunk. If the bad case indeed
            // happens due to this being too small, a node restart should recover it.
            // Still, defaulting to 1M to be super safe.
            prune_window: 1_000_000,
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
        }
    }
```
