# Audit Report

## Title
Consensus Safety Violation: OnChainConsensusConfig Schema Upgrade Breaks Deterministic Execution via Silent Deserialization Fallback

## Summary
During protocol upgrades that introduce new `OnChainConsensusConfig` schema versions, validators running older binaries fail to deserialize the new format and silently fall back to a default configuration where validator transactions are disabled. This causes different validators to execute governance transactions differently, producing different state roots and triggering consensus failure.

## Finding Description

The vulnerability exists in the native Rust implementation of `validator_txn_enabled_internal()` which uses `.unwrap_or_default()` when deserializing on-chain consensus configuration bytes. [1](#0-0)  When BCS deserialization encounters an unknown enum variant (e.g., V6 on a node that only knows V1-V5), it fails and returns the default configuration.

The default `OnChainConsensusConfig` has validator transactions disabled, [2](#0-1)  specifically returning `ValidatorTxnConfig::V0` via `default_if_missing()`, [3](#0-2)  which has `enabled()` returning false. [4](#0-3) 

The `OnChainConsensusConfig` enum has evolved from V1 to V5, [5](#0-4)  with each version adding new fields. Future schema versions are inevitable as the protocol evolves.

**The Attack Scenario:**

During a protocol upgrade introducing V6, when `aptos_governance::reconfigure()` executes, [6](#0-5)  it calls `consensus_config::validator_txn_enabled()` [7](#0-6)  which invokes the native function to deserialize the config bytes.

**On upgraded validators (know V1-V6)**: Deserialization succeeds, returns actual config value (e.g., `true`), executes `reconfiguration_with_dkg::try_start()` which modifies `ReconfigurationState` [8](#0-7)  and starts a DKG session. [9](#0-8) 

**On old validators (know V1-V5)**: Deserialization fails, returns default (vtxn disabled, returns `false`), executes `reconfiguration_with_dkg::finish()` which clears DKG session [10](#0-9)  and applies epoch configs. [11](#0-10) 

These different code paths produce **different state roots**, breaking the deterministic execution invariant that all validators must produce identical state for identical blocks.

The same vulnerability pattern exists in the consensus `EpochManager`. [12](#0-11) 

## Impact Explanation

This is **Critical Severity** per the Aptos bug bounty "Consensus/Safety Violations" category (up to $1,000,000).

**Broken Invariant**: Deterministic Execution - All validators must produce identical state roots for identical blocks.

**Concrete Impact**:
- **Consensus split**: Validators on different software versions cannot agree on the correct state root for blocks containing governance transactions
- **Network partition**: The blockchain splits into two incompatible chains
- **Requires hard fork**: Recovery requires manual coordination to roll back or fast-forward all validators to a common state
- **Complete loss of liveness**: No new blocks can be committed while validators disagree on state, as fork detection mechanisms identify the divergence [13](#0-12) 

**Affected transactions**: Any transaction calling `consensus_config::validator_txn_enabled()`, most critically `aptos_governance::reconfigure()` which executes for every governance proposal affecting on-chain configuration.

## Likelihood Explanation

**Likelihood: High** - This will occur during protocol upgrades that add new `OnChainConsensusConfig` variants.

**Evidence**:
1. The enum has already evolved from V1 to V5, with V3 adding vtxn config, V4 adding window_size, and V5 adding rand_check_enabled. Future V6, V7, etc. are inevitable.
2. Aptos explicitly supports and tests rolling validator upgrades where validators run different binary versions simultaneously. [14](#0-13) 
3. The `set_for_next_epoch()` function only validates that config bytes are non-empty, [15](#0-14)  with no deserialization validation to ensure all validators can parse the new format.
4. There is always a window where some validators run old binaries and some run new binaries during routine protocol upgrades.

**No attack required**: This occurs naturally during legitimate protocol upgrades performed by governance.

## Recommendation

Implement deserialization validation in `set_for_next_epoch()` to ensure all validators can parse the new configuration format before accepting it:

```move
public fun set_for_next_epoch(account: &signer, config: vector<u8>) {
    system_addresses::assert_aptos_framework(account);
    assert!(vector::length(&config) > 0, error::invalid_argument(EINVALID_CONFIG));
    // Add deserialization check
    assert!(validator_txn_enabled_internal(config), error::invalid_argument(EINVALID_CONFIG));
    std::config_buffer::upsert<ConsensusConfig>(ConsensusConfig {config});
}
```

Alternatively, change the native function to abort on deserialization failure instead of using `.unwrap_or_default()`:

```rust
pub fn validator_txn_enabled(
    _context: &mut SafeNativeContext,
    _ty_args: &[Type],
    mut args: VecDeque<Value>,
) -> SafeNativeResult<SmallVec<[Value; 1]>> {
    let config_bytes = safely_pop_arg!(args, Vec<u8>);
    let config = bcs::from_bytes::<OnChainConsensusConfig>(&config_bytes)
        .map_err(|e| SafeNativeError::InvariantViolation(format!("Failed to deserialize consensus config: {}", e)))?;
    Ok(smallvec![Value::bool(config.is_vtxn_enabled())])
}
```

## Proof of Concept

While a complete PoC would require simulating a multi-validator network during a rolling upgrade, the vulnerability can be demonstrated through code inspection:

1. **Setup**: Deploy V5 config, upgrade some validators to V6-aware binaries
2. **Trigger**: Execute governance transaction that sets V6 config and calls `reconfigure()`
3. **First reconfigure**: All validators agree (checking old V5 config), V6 gets applied
4. **Second reconfigure**: Any subsequent governance action triggers the split
5. **Result**: V6-aware validators execute `try_start()`, V5 validators execute `finish()`, producing different state roots

The vulnerability is verifiable by examining the code paths and confirming that the state modifications in `try_start()` and `finish()` are fundamentally different, making it impossible for validators to reach consensus.

## Notes

This vulnerability is particularly concerning because:
1. It occurs naturally during legitimate protocol upgrades, not requiring any malicious actor
2. The rolling upgrade pattern is explicitly supported and tested in the codebase
3. No validation prevents incompatible configurations from being accepted
4. Recovery requires a hard fork with manual coordination across all validators
5. The issue affects both the Move VM layer (native function) and the consensus layer (EpochManager)

The vulnerability demonstrates a fundamental incompatibility between the protocol's support for rolling upgrades and the use of versioned enum schemas without backward compatibility guarantees.

### Citations

**File:** aptos-move/framework/src/natives/consensus_config.rs (L19-19)
```rust
    let config = bcs::from_bytes::<OnChainConsensusConfig>(&config_bytes).unwrap_or_default();
```

**File:** types/src/on_chain_config/consensus_config.rs (L147-149)
```rust
    pub fn default_if_missing() -> Self {
        Self::V0
    }
```

**File:** types/src/on_chain_config/consensus_config.rs (L162-167)
```rust
    pub fn enabled(&self) -> bool {
        match self {
            ValidatorTxnConfig::V0 => false,
            ValidatorTxnConfig::V1 { .. } => true,
        }
    }
```

**File:** types/src/on_chain_config/consensus_config.rs (L192-213)
```rust
pub enum OnChainConsensusConfig {
    V1(ConsensusConfigV1),
    V2(ConsensusConfigV1),
    V3 {
        alg: ConsensusAlgorithmConfig,
        vtxn: ValidatorTxnConfig,
    },
    V4 {
        alg: ConsensusAlgorithmConfig,
        vtxn: ValidatorTxnConfig,
        // Execution pool block window
        window_size: Option<u64>,
    },
    V5 {
        alg: ConsensusAlgorithmConfig,
        vtxn: ValidatorTxnConfig,
        // Execution pool block window
        window_size: Option<u64>,
        // Whether to check if we can skip generating randomness for blocks
        rand_check_enabled: bool,
    },
}
```

**File:** types/src/on_chain_config/consensus_config.rs (L444-450)
```rust
    fn default() -> Self {
        OnChainConsensusConfig::V4 {
            alg: ConsensusAlgorithmConfig::default_if_missing(),
            vtxn: ValidatorTxnConfig::default_if_missing(),
            window_size: DEFAULT_WINDOW_SIZE,
        }
    }
```

**File:** aptos-move/framework/aptos-framework/sources/aptos_governance.move (L685-692)
```text
    public entry fun reconfigure(aptos_framework: &signer) {
        system_addresses::assert_aptos_framework(aptos_framework);
        if (consensus_config::validator_txn_enabled() && randomness_config::enabled()) {
            reconfiguration_with_dkg::try_start();
        } else {
            reconfiguration_with_dkg::finish(aptos_framework);
        }
    }
```

**File:** aptos-move/framework/aptos-framework/sources/configs/consensus_config.move (L52-56)
```text
    public fun set_for_next_epoch(account: &signer, config: vector<u8>) {
        system_addresses::assert_aptos_framework(account);
        assert!(vector::length(&config) > 0, error::invalid_argument(EINVALID_CONFIG));
        std::config_buffer::upsert<ConsensusConfig>(ConsensusConfig {config});
    }
```

**File:** aptos-move/framework/aptos-framework/sources/configs/consensus_config.move (L71-74)
```text
    public fun validator_txn_enabled(): bool acquires ConsensusConfig {
        let config_bytes = borrow_global<ConsensusConfig>(@aptos_framework).config;
        validator_txn_enabled_internal(config_bytes)
    }
```

**File:** aptos-move/framework/aptos-framework/sources/reconfiguration_state.move (L72-74)
```text
                state.variant = copyable_any::pack(StateActive {
                    start_time_secs: timestamp::now_seconds()
                });
```

**File:** aptos-move/framework/aptos-framework/sources/dkg.move (L75-79)
```text
        dkg_state.in_progress = std::option::some(DKGSessionState {
            metadata: new_session_metadata,
            start_time_us,
            transcript: vector[],
        });
```

**File:** aptos-move/framework/aptos-framework/sources/dkg.move (L100-106)
```text
    public fun try_clear_incomplete_session(fx: &signer) acquires DKGState {
        system_addresses::assert_aptos_framework(fx);
        if (exists<DKGState>(@aptos_framework)) {
            let dkg_state = borrow_global_mut<DKGState>(@aptos_framework);
            dkg_state.in_progress = option::none();
        }
    }
```

**File:** aptos-move/framework/aptos-framework/sources/reconfiguration_with_dkg.move (L46-61)
```text
    public(friend) fun finish(framework: &signer) {
        system_addresses::assert_aptos_framework(framework);
        dkg::try_clear_incomplete_session(framework);
        consensus_config::on_new_epoch(framework);
        execution_config::on_new_epoch(framework);
        gas_schedule::on_new_epoch(framework);
        std::version::on_new_epoch(framework);
        features::on_new_epoch(framework);
        jwk_consensus_config::on_new_epoch(framework);
        jwks::on_new_epoch(framework);
        keyless_account::on_new_epoch(framework);
        randomness_config_seqnum::on_new_epoch(framework);
        randomness_config::on_new_epoch(framework);
        randomness_api_v0_config::on_new_epoch(framework);
        reconfiguration::reconfigure();
    }
```

**File:** consensus/src/epoch_manager.rs (L1201-1201)
```rust
        let consensus_config = onchain_consensus_config.unwrap_or_default();
```

**File:** consensus/src/block_storage/block_store.rs (L192-217)
```rust
        //verify root is correct
        assert!(
            // decoupled execution allows dummy versions
            root_qc.certified_block().version() == 0
                || root_qc.certified_block().version() == root_metadata.version(),
            "root qc version {} doesn't match committed trees {}",
            root_qc.certified_block().version(),
            root_metadata.version(),
        );
        assert!(
            // decoupled execution allows dummy executed_state_id
            root_qc.certified_block().executed_state_id() == *ACCUMULATOR_PLACEHOLDER_HASH
                || root_qc.certified_block().executed_state_id() == root_metadata.accu_hash,
            "root qc state id {} doesn't match committed trees {}",
            root_qc.certified_block().executed_state_id(),
            root_metadata.accu_hash,
        );

        let result = StateComputeResult::new_dummy_with_accumulator(Arc::new(
            InMemoryTransactionAccumulator::new(
                root_metadata.frozen_root_hashes,
                root_metadata.num_leaves,
            )
            .expect("Failed to recover accumulator."),
        ));
        assert_eq!(result.root_hash(), root_metadata.accu_hash);
```

**File:** testsuite/testcases/src/compatibility_test.rs (L1-50)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use crate::{batch_update_gradually, generate_traffic};
use anyhow::bail;
use aptos_forge::{NetworkContextSynchronizer, NetworkTest, Result, SwarmExt, Test};
use async_trait::async_trait;
use log::info;
use std::ops::DerefMut;
use tokio::time::Duration;

pub struct SimpleValidatorUpgrade;

impl SimpleValidatorUpgrade {
    pub const EPOCH_DURATION_SECS: u64 = 30;
}

impl Test for SimpleValidatorUpgrade {
    fn name(&self) -> &'static str {
        "compatibility::simple-validator-upgrade"
    }
}

#[async_trait]
impl NetworkTest for SimpleValidatorUpgrade {
    async fn run<'a>(&self, ctxa: NetworkContextSynchronizer<'a>) -> Result<()> {
        let upgrade_wait_for_healthy = true;
        let upgrade_node_delay = Duration::from_secs(20);
        let upgrade_max_wait = Duration::from_secs(40);

        let epoch_duration = Duration::from_secs(Self::EPOCH_DURATION_SECS);

        // Get the different versions we're testing with
        let (old_version, new_version) = {
            let mut versions = ctxa
                .ctx
                .lock()
                .await
                .swarm
                .read()
                .await
                .versions()
                .collect::<Vec<_>>();
            versions.sort();
            if versions.len() != 2 {
                bail!("exactly two different versions needed to run compat test");
            }

            (versions[0].clone(), versions[1].clone())
        };
```
