# Audit Report

## Title
Consensus Safety Violation: Equivocation via Non-Atomic Vote Signing and State Persistence

## Summary
The `verify_and_update_last_vote_round()` function updates `last_voted_round` in memory without persisting it, delegating persistence responsibility to the caller. In `guarded_construct_and_sign_vote_two_chain()`, the vote is cryptographically signed before the updated safety state is persisted to durable storage. Combined with missing `fsync()` calls in `OnDiskStorage`, this creates a window for equivocation where a validator can vote twice for the same round after a crash or power failure.

## Finding Description

The vulnerability exists in the ordering of operations in the voting flow. The `verify_and_update_last_vote_round()` function updates `last_voted_round` in memory but does NOT persist it: [1](#0-0) 

This function updates `last_voted_round` on line 225 but delegates persistence to the caller. The critical issue occurs in the caller `guarded_construct_and_sign_vote_two_chain()`: [2](#0-1) 

**The problematic sequence:**
1. Lines 77-80: `verify_and_update_last_vote_round()` updates `last_voted_round = R` in memory
2. Line 88: Vote is cryptographically signed (irreversible commitment created)
3. Line 91: Vote stored in memory as `last_vote`
4. Line 92: **Persistence happens HERE** via `set_safety_data()`

**Additional durability issue** - The `OnDiskStorage::write()` function lacks fsync: [3](#0-2) 

The `write()` function performs `write_all()` followed immediately by `fs::rename()` without calling `file.sync_all()`. This violates the atomic write pattern - data may still be in OS page cache when rename occurs.

**Attack Scenario:**
1. Validator receives proposal for block B1 at round R
2. Calls `construct_and_sign_vote_two_chain()` which signs vote for B1
3. **Power failure or crash occurs** after signing (line 88) but before durable persistence (line 92 + fsync)
4. Safety state file is either not updated at all or updated but not fsynced (data lost from OS cache)
5. Validator restarts and loads old `safety_data` where `last_voted_round < R`
6. Validator receives proposal for block B2 ≠ B1 at the same round R
7. Check at line 218 passes because loaded `last_voted_round < R`
8. Validator signs and sends vote for B2
9. **Equivocation achieved** - validator has cryptographically signed two different blocks at round R

This violates the fundamental BFT consensus safety invariant: **a validator must never vote for two different blocks in the same round**.

## Impact Explanation

**Critical Severity** - This is a consensus safety violation that can lead to chain splits and double-spending.

Per Aptos bug bounty criteria, this qualifies as **Critical** under "Consensus/Safety Violations" because:
- **Breaks the one-vote-per-round safety rule** of AptosBFT
- Enables equivocation by honest validators experiencing natural failures (not requiring Byzantine behavior)
- With equivocating validators, can potentially lead to conflicting quorum certificates for the same round
- Could result in chain forks requiring manual intervention

The vulnerability is especially severe because:
1. It affects honest validators experiencing natural operational failures (crashes, power outages, OOM kills)
2. No malicious intent required - normal operational failures trigger it
3. The cryptographic signature from the first vote may have already been gossiped to peers before the crash
4. Detection is difficult since the validator has no record of the first vote after restart

## Likelihood Explanation

**High Likelihood** for validators using `OnDiskStorage` backend:

1. **Common failure scenarios**: Validator crashes, power failures, OOM kills, and kernel panics are routine operational events in distributed systems
2. **Wide vulnerability window**: The window between signing (line 88) and durable persistence (line 92 + OS flush) can be milliseconds to seconds depending on disk I/O load
3. **No fsync() call**: Without explicit sync, data can remain in OS cache for seconds before being flushed to disk
4. **OnDiskStorage in deployment configs**: Production deployment templates use OnDiskStorage: [4](#0-3) [5](#0-4) 

5. **No sanitizer enforcement**: The config sanitizer only blocks `InMemoryStorage` for mainnet validators, not `OnDiskStorage`: [6](#0-5) 

6. **Documented warning ignored**: While the README warns against production use, the actual deployment configs utilize it: [7](#0-6) 

The likelihood is amplified by cloud infrastructure realities: Kubernetes pod evictions, container restarts, and disk I/O saturation during high load.

## Recommendation

**Immediate fixes required:**

1. **Add fsync to OnDiskStorage::write():**
```rust
fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
    let contents = serde_json::to_vec(data)?;
    let mut file = File::create(self.temp_path.path())?;
    file.write_all(&contents)?;
    file.sync_all()?;  // ADD THIS LINE
    fs::rename(&self.temp_path, &self.file_path)?;
    Ok(())
}
```

2. **Persist before signing (architectural fix):**
   - Reorder operations in `guarded_construct_and_sign_vote_two_chain()` to persist safety state BEFORE signing
   - Or implement two-phase commit: persist intent, sign, persist signature

3. **Add config sanitizer check:**
   - Block `OnDiskStorage` for mainnet validators, require `Vault` backend
   - Update production deployment configs to use Vault

4. **Add crash recovery validation:**
   - On restart, check for unsigned persisted state and handle appropriately
   - Implement write-ahead log for critical safety data

## Proof of Concept

The vulnerability can be demonstrated with the following scenario:

1. Configure validator with `OnDiskStorage` backend
2. Instrument the code to crash after line 88 (signature) but before line 92 (persistence) completes
3. Send vote proposal for block B1 at round R
4. Verify validator crashes after signing
5. Restart validator
6. Verify `last_voted_round < R` (old state loaded)
7. Send vote proposal for block B2 ≠ B1 at same round R
8. Verify validator signs second vote (equivocation achieved)
9. Both signed votes are now in circulation with different block hashes at the same round

The PoC requires modifying the code to inject a crash at the vulnerable point, but in production this occurs naturally through system failures.

**Notes:**

This vulnerability exists in the current codebase and affects any validator configured with `OnDiskStorage` backend. While the README documentation warns against production use of `OnDiskStorage`, the actual deployment configurations (terraform/helm and docker-compose templates) use it as the default backend, and the configuration sanitizer does not prevent its use on mainnet. The missing `fsync()` call combined with the sign-before-persist ordering creates a critical window where consensus safety can be violated through natural operational failures rather than Byzantine behavior.

### Citations

**File:** consensus/safety-rules/src/safety_rules.rs (L213-232)
```rust
    pub(crate) fn verify_and_update_last_vote_round(
        &self,
        round: Round,
        safety_data: &mut SafetyData,
    ) -> Result<(), Error> {
        if round <= safety_data.last_voted_round {
            return Err(Error::IncorrectLastVotedRound(
                round,
                safety_data.last_voted_round,
            ));
        }

        safety_data.last_voted_round = round;
        trace!(
            SafetyLogSchema::new(LogEntry::LastVotedRound, LogEvent::Update)
                .last_voted_round(safety_data.last_voted_round)
        );

        Ok(())
    }
```

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L53-95)
```rust
    pub(crate) fn guarded_construct_and_sign_vote_two_chain(
        &mut self,
        vote_proposal: &VoteProposal,
        timeout_cert: Option<&TwoChainTimeoutCertificate>,
    ) -> Result<Vote, Error> {
        // Exit early if we cannot sign
        self.signer()?;

        let vote_data = self.verify_proposal(vote_proposal)?;
        if let Some(tc) = timeout_cert {
            self.verify_tc(tc)?;
        }
        let proposed_block = vote_proposal.block();
        let mut safety_data = self.persistent_storage.safety_data()?;

        // if already voted on this round, send back the previous vote
        // note: this needs to happen after verifying the epoch as we just check the round here
        if let Some(vote) = safety_data.last_vote.clone() {
            if vote.vote_data().proposed().round() == proposed_block.round() {
                return Ok(vote);
            }
        }

        // Two voting rules
        self.verify_and_update_last_vote_round(
            proposed_block.block_data().round(),
            &mut safety_data,
        )?;
        self.safe_to_vote(proposed_block, timeout_cert)?;

        // Record 1-chain data
        self.observe_qc(proposed_block.quorum_cert(), &mut safety_data);
        // Construct and sign vote
        let author = self.signer()?.author();
        let ledger_info = self.construct_ledger_info_2chain(proposed_block, vote_data.hash())?;
        let signature = self.sign(&ledger_info)?;
        let vote = Vote::new_with_signature(vote_data, author, ledger_info, signature);

        safety_data.last_vote = Some(vote.clone());
        self.persistent_storage.set_safety_data(safety_data)?;

        Ok(vote)
    }
```

**File:** secure/storage/src/on_disk.rs (L64-70)
```rust
    fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
        let contents = serde_json::to_vec(data)?;
        let mut file = File::create(self.temp_path.path())?;
        file.write_all(&contents)?;
        fs::rename(&self.temp_path, &self.file_path)?;
        Ok(())
    }
```

**File:** terraform/helm/aptos-node/files/configs/validator-base.yaml (L14-17)
```yaml
    backend:
      type: "on_disk_storage"
      path: secure-data.json
      namespace: ~
```

**File:** docker/compose/aptos-node/validator.yaml (L11-14)
```yaml
    backend:
      type: "on_disk_storage"
      path: secure-data.json
      namespace: ~
```

**File:** config/src/config/safety_rules_config.rs (L85-96)
```rust
        if let Some(chain_id) = chain_id {
            // Verify that the secure backend is appropriate for mainnet validators
            if chain_id.is_mainnet()
                && node_type.is_validator()
                && safety_rules_config.backend.is_in_memory()
            {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "The secure backend should not be set to in memory storage in mainnet!"
                        .to_string(),
                ));
            }
```

**File:** secure/storage/README.md (L37-42)
```markdown
- `OnDisk`: Similar to InMemory, the OnDisk secure storage implementation provides another
useful testing implementation: an on-disk storage engine, where the storage backend is
implemented using a single file written to local disk. In a similar fashion to the in-memory
storage, on-disk should not be used in production environments as it provides no security
guarantees (e.g., encryption before writing to disk). Moreover, OnDisk storage does not
currently support concurrent data accesses.
```
