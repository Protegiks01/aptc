# Audit Report

## Title
State Inconsistency Between Execution and Randomness Manager During sync_for_duration Leads to Consensus Liveness Failures

## Summary
When `RandResetDropped` error occurs during the `sync_for_duration` operation, the execution state is updated but the randomness manager state remains stale. This causes validators to diverge on their internal randomness state, leading to failed share aggregation and consensus liveness issues.

## Finding Description

The vulnerability exists in the ordering of operations within `ExecutionProxyClient::sync_for_duration()`. The execution state sync completes successfully and updates the validator's committed round, but the subsequent randomness manager reset fails with `RandResetDropped`, leaving the rand manager with stale state. [1](#0-0) 

The critical flaw is that `execution_proxy.sync_for_duration()` executes first (line 651) and updates the execution state to the newly synced ledger info within `ExecutionProxy::sync_for_duration()`: [2](#0-1) 

Then `reset()` is called (line 655) to synchronize the rand manager and buffer manager. If the rand manager channel has been dropped or is unresponsive, `RandResetDropped` is returned: [3](#0-2) [4](#0-3) 

When this failure occurs:
1. The validator's execution state has already advanced to round X (the synced state)
2. The randomness manager remains at old round Y (where Y < X)
3. The error is returned but **the execution state update is not rolled back**

This creates a permanent state divergence within the validator. Other validators that successfully completed both sync and reset will have consistent state at round X, while the affected validator has execution at round X but randomness at round Y.

The error propagates up to the RoundManager event loop where it is merely logged: [5](#0-4) 

The validator continues operating with this inconsistent internal state, which breaks randomness generation. The stale rand manager will reject shares for future rounds: [6](#0-5) [7](#0-6) 

Since `highest_known_round` was not updated by the failed reset (which would have occurred via `RandStore::reset()`): [8](#0-7) [9](#0-8) 

Shares for rounds beyond `Y + FUTURE_ROUNDS_TO_ACCEPT` (200 rounds) will be rejected as "Share from future round", preventing the validator from participating in randomness generation for those rounds.

A TODO comment in the code acknowledges synchronization issues between state sync and reset operations: [10](#0-9) 

## Impact Explanation

**Severity: High**

This vulnerability meets the High severity criteria per Aptos bug bounty guidelines as it causes **Validator Node Slowdowns**:

1. **Validator Slowdowns**: Affected validators cannot properly participate in randomness generation, causing them to fall behind in consensus
2. **Significant Protocol Violations**: Breaks the critical invariant that all components of a validator must maintain consistent state
3. **Consensus Liveness Risk**: If multiple validators experience this issue simultaneously (e.g., during network partitions or epoch transitions), the network may fail to aggregate sufficient randomness shares to proceed

The impact is **not** Critical because:
- Individual validator failures don't immediately halt the entire network
- The network can continue with remaining functional validators
- No funds are directly at risk
- The state divergence is internal to the affected validator

However, it qualifies as High because it can cause cascading liveness failures if it affects enough validators during critical operations like state sync after network partitions or consensus observer fallback scenarios.

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability can occur in several realistic scenarios:

1. **Consensus Observer Fallback Mode**: When consensus observers enter fallback mode, they call `sync_for_duration`: [11](#0-10) 

2. **Randomness Manager Task Crash**: If the rand manager task crashes or panics after the execution sync completes but before reset is called

3. **Channel Closure Race Conditions**: During epoch transitions or shutdown sequences, timing windows exist where the rand manager channel could be dropped between sync completion and reset

4. **Network Partitions**: During recovery from network partitions, multiple validators may attempt state sync simultaneously, increasing the probability of timing-related failures

The likelihood is elevated because:
- The vulnerable code path is exercised during normal consensus observer operations
- No attacker action is required - it can occur naturally due to timing or network conditions
- The TODO comment indicates the development team is aware of related synchronization weaknesses

## Recommendation

Implement atomic synchronization between execution state and randomness manager state. The fix should ensure that either both components are updated successfully or both remain at the old state. Consider one of the following approaches:

1. **Pre-reset validation**: Before calling `execution_proxy.sync_for_duration()`, verify that all reset channels are alive and responsive
2. **Rollback mechanism**: If reset fails after sync succeeds, implement a rollback that reverts the execution state to the pre-sync state
3. **Retry mechanism**: Implement automatic retry logic for reset failures, with exponential backoff
4. **Reorder operations**: Call reset before sync in `sync_for_duration`, similar to how `sync_to_target` is structured (though this introduces the opposite problem documented in the TODO)

The most robust solution would be to wrap both operations in a transaction-like mechanism that guarantees atomicity.

## Proof of Concept

While a complete PoC would require setting up a full Aptos validator environment, the vulnerability can be demonstrated by:

1. Starting a consensus observer node
2. Triggering fallback mode by simulating network latency or partition
3. Injecting a fault that causes the rand manager channel to close during the fallback sync
4. Observing that the execution state advances while the rand manager remains stale
5. Verifying that subsequent randomness shares for future rounds are rejected

The vulnerability is evident from the code structure where the execution state update (`latest_logical_time` write at state_computer.rs:162) occurs within a successful sync, but the subsequent reset operation can fail without any rollback mechanism, leaving the system in an inconsistent state.

### Citations

**File:** consensus/src/pipeline/execution_client.rs (L642-659)
```rust
    async fn sync_for_duration(
        &self,
        duration: Duration,
    ) -> Result<LedgerInfoWithSignatures, StateSyncError> {
        fail_point!("consensus::sync_for_duration", |_| {
            Err(anyhow::anyhow!("Injected error in sync_for_duration").into())
        });

        // Sync for the specified duration
        let result = self.execution_proxy.sync_for_duration(duration).await;

        // Reset the rand and buffer managers to the new synced round
        if let Ok(latest_synced_ledger_info) = &result {
            self.reset(latest_synced_ledger_info).await?;
        }

        result
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L669-671)
```rust
        // TODO: handle the state sync error (e.g., re-push the ordered
        // blocks to the buffer manager when it's reset but sync fails).
        self.execution_proxy.sync_to_target(target).await
```

**File:** consensus/src/pipeline/execution_client.rs (L683-692)
```rust
        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
```

**File:** consensus/src/state_computer.rs (L159-163)
```rust
        if let Ok(latest_synced_ledger_info) = &result {
            let ledger_info = latest_synced_ledger_info.ledger_info();
            let synced_logical_time = LogicalTime::new(ledger_info.epoch(), ledger_info.round());
            *latest_logical_time = synced_logical_time;
        }
```

**File:** consensus/src/pipeline/errors.rs (L17-18)
```rust
    #[error("Rand Reset host dropped")]
    RandResetDropped,
```

**File:** consensus/src/round_manager.rs (L2136-2142)
```rust
                        match result {
                            Ok(_) => trace!(RoundStateLogSchema::new(round_state)),
                            Err(e) => {
                                counters::ERROR_COUNT.inc();
                                warn!(kind = error_kind(&e), RoundStateLogSchema::new(round_state), "Error: {:#}", e);
                            }
                        }
```

**File:** consensus/src/rand/rand_gen/rand_store.rs (L253-259)
```rust
    pub fn reset(&mut self, round: u64) {
        self.update_highest_known_round(round);
        // remove future rounds items in case they're already decided
        // otherwise if the block re-enters the queue, it'll be stuck
        let _ = self.rand_map.split_off(&round);
        let _ = self.fast_rand_map.as_mut().map(|map| map.split_off(&round));
    }
```

**File:** consensus/src/rand/rand_gen/rand_store.rs (L285-287)
```rust
        ensure!(
            share.metadata().round <= self.highest_known_round + FUTURE_ROUNDS_TO_ACCEPT,
            "Share from future round"
```

**File:** consensus/src/rand/rand_gen/types.rs (L26-26)
```rust
pub const FUTURE_ROUNDS_TO_ACCEPT: u64 = 200;
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L184-194)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.rand_store.lock().reset(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/consensus_observer/observer/state_sync_manager.rs (L150-160)
```rust
                let latest_synced_ledger_info = match execution_client
                    .clone()
                    .sync_for_duration(fallback_duration)
                    .await
                {
                    Ok(latest_synced_ledger_info) => latest_synced_ledger_info,
                    Err(error) => {
                        error!(LogSchema::new(LogEntry::ConsensusObserver)
                            .message(&format!("Failed to sync for fallback! Error: {:?}", error)));
                        return;
                    },
```
