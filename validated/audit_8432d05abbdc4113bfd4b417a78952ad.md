# Audit Report

## Title
Consensus Split via Local Randomness Override Configuration Divergence in Fast Path Randomness

## Summary
When validators configure different `randomness_override_seq_num` values in their local node configurations, they compute divergent `OnChainRandomnessConfig` values during epoch initialization. This causes validators to decide different randomness values locally during block execution, leading to different state roots and an irrecoverable consensus split.

## Finding Description

The vulnerability stems from the protocol's reliance on manual coordination of local configuration parameters without any validation or enforcement mechanism. Each validator independently computes their effective randomness configuration during epoch initialization, and this divergence propagates through block execution to produce different state roots.

The `randomness_override_seq_num` is defined as a per-validator local configuration parameter intended for emergency recovery: [1](#0-0) 

During epoch initialization, each validator independently computes their `OnChainRandomnessConfig` using this local parameter: [2](#0-1) 

The `from_configs` function returns `Off` when the local sequence number exceeds the on-chain value, disabling randomness for that validator: [3](#0-2) 

This determines whether fast path randomness is enabled, which affects which cryptographic keys and aggregation paths are used: [4](#0-3) 

Validators with fast path enabled generate one type of share, while those without fast path generate different shares using different DKG keys: [5](#0-4) 

The fast and slow paths aggregate shares independently and both send decisions through the same channel: [6](#0-5) 

The critical vulnerability occurs during block execution. Each validator independently waits for their local randomness decision and creates a block metadata transaction with that locally-decided randomness: [7](#0-6) 

This metadata transaction updates the `PerBlockRandomness` global resource with different seed values on different validators: [8](#0-7) 

Different state updates produce different state roots. Validators vote on `BlockInfo` containing the `executed_state_id` field, which represents the state root: [9](#0-8) 

When validators have different state roots, their votes diverge, preventing the 2/3+ quorum required for consensus, causing complete network halt.

## Impact Explanation

This vulnerability meets **Critical Severity** criteria under the Aptos Bug Bounty program for multiple reasons:

**Consensus/Safety Violations**: The fundamental safety property of BFT consensus—that honest validators agree on committed blocks—is violated. Validators with different configurations execute the same block with different randomness values, producing cryptographically different state roots. This is not a >1/3 Byzantine attack; it affects ALL validators based on their local configuration.

**Non-recoverable Network Partition**: Once state roots diverge, validators cannot self-recover. The divergence persists and accumulates across all subsequent blocks. Recovery requires coordinated manual intervention by all validators to align configurations, and potentially requires state rollback or hardfork if blocks have already been committed.

**Total Loss of Liveness**: If validator voting power splits between groups using different configurations, neither group can achieve the required 2/3+ supermajority for quorum certificates. The network completely halts and cannot produce new blocks until manual intervention occurs.

The severity is amplified because the failure mode is silent—only warning logs are emitted at epoch initialization, with no errors preventing the divergence. The split manifests only when blocks requiring randomness are executed, potentially many blocks after the configuration divergence occurred, making diagnosis extremely difficult.

## Likelihood Explanation

**High Likelihood** in production environments:

1. **Operational Reality**: The `randomness_override_seq_num` mechanism exists specifically for emergency recovery from randomness stalls. In production networks with 100+ validators operated by independent entities across different organizations, configuration drift during emergency procedures, rolling updates, or operational testing is practically inevitable.

2. **No Protocol-Level Enforcement**: The recovery documentation describes the intended coordinated procedure: [10](#0-9) 

However, this is purely documentation-based coordination. There is no protocol mechanism to verify or enforce that all validators use the same override value. The only safeguard is a warning log: [11](#0-10) 

3. **Silent Failure Mode**: The existing test validates only the coordinated scenario where all validators use identical values: [12](#0-11) 

There is no test or validation for divergent configurations, and no mechanism prevents validators from starting with different values.

4. **Low Expertise Barrier**: This requires no malicious intent, cryptographic expertise, or protocol knowledge. Any validator operator following outdated runbooks, using different configuration templates during recovery, or performing staged rollouts can trigger this vulnerability.

5. **Realistic Trigger Conditions**: The vulnerability manifests during epoch transitions when any transaction requires randomness. As on-chain randomness adoption increases, the frequency of blocks requiring randomness increases proportionally, raising the probability of triggering the consensus split.

## Recommendation

Implement protocol-level validation to enforce configuration consistency:

1. **Consensus on Override Values**: Include the `randomness_override_seq_num` in the validator's committed state or epoch metadata. During epoch initialization, validators should verify that their local override matches the value committed by >2/3 of validators, or use a governance-controlled on-chain override value instead of per-validator local configuration.

2. **Fail-Safe Validation**: Add explicit validation during block execution that compares the randomness value with peers before committing votes. If randomness values diverge, validators should halt execution and emit critical errors rather than silently producing different state roots.

3. **Randomness Commitment Protocol**: Modify the randomness protocol to commit randomness values before execution, such that the randomness becomes part of the block proposal that all validators agree on before execution, rather than being computed independently by each validator.

4. **Enhanced Testing**: Add explicit test cases for configuration divergence scenarios to validate either that the protocol prevents divergence or handles it gracefully.

## Proof of Concept

A proof of concept would involve:

1. Deploy a local testnet with 4 validators
2. Enable randomness with V2 configuration (fast path enabled)
3. Wait for epoch 2
4. Stop all validators
5. Restart validators 0-1 with `randomness_override_seq_num = 1`
6. Restart validators 2-3 with `randomness_override_seq_num = 0`
7. Submit transactions requiring randomness
8. Observe that validators produce different state roots and cannot achieve quorum
9. Network halts permanently

The smoke test framework already demonstrates the recovery procedure where all validators coordinate on the same value. Modifying it to use different values on different validators would demonstrate the consensus split.

## Notes

This vulnerability represents a fundamental protocol design issue where safety-critical configuration parameters are left to manual coordination without enforcement. While the trust model assumes validator operators are not malicious, it is unrealistic to assume perfect coordination across 100+ independent operators during emergency procedures. The protocol must include safeguards against accidental misconfiguration, not just malicious attacks.

### Citations

**File:** config/src/config/node_config.rs (L78-81)
```rust
    /// In a randomness stall, set this to be on-chain `RandomnessConfigSeqNum` + 1.
    /// Once enough nodes restarted with the new value, the chain should unblock with randomness disabled.
    #[serde(default)]
    pub randomness_override_seq_num: u64,
```

**File:** consensus/src/epoch_manager.rs (L1137-1159)
```rust
        let fast_rand_config = if let (Some((ask, apk)), Some(trx), Some(wconfig)) = (
            fast_augmented_key_pair,
            transcript.fast.as_ref(),
            dkg_pub_params.pvss_config.fast_wconfig.as_ref(),
        ) {
            let pk_shares = (0..new_epoch_state.verifier.len())
                .map(|id| trx.get_public_key_share(wconfig, &Player { id }))
                .collect::<Vec<_>>();

            let fast_keys = RandKeys::new(ask, apk, pk_shares, new_epoch_state.verifier.len());
            let fast_wconfig = wconfig.clone();

            Some(RandConfig::new(
                self.author,
                new_epoch,
                new_epoch_state.verifier.clone(),
                vuf_pp,
                fast_keys,
                fast_wconfig,
            ))
        } else {
            None
        };
```

**File:** consensus/src/epoch_manager.rs (L1213-1215)
```rust
        if self.randomness_override_seq_num > onchain_randomness_config_seq_num.seq_num {
            warn!("Randomness will be force-disabled by local config!");
        }
```

**File:** consensus/src/epoch_manager.rs (L1217-1221)
```rust
        let onchain_randomness_config = OnChainRandomnessConfig::from_configs(
            self.randomness_override_seq_num,
            onchain_randomness_config_seq_num.seq_num,
            randomness_config_move_struct.ok(),
        );
```

**File:** types/src/on_chain_config/randomness_config.rs (L139-151)
```rust
    pub fn from_configs(
        local_seqnum: u64,
        onchain_seqnum: u64,
        onchain_raw_config: Option<RandomnessConfigMoveStruct>,
    ) -> Self {
        if local_seqnum > onchain_seqnum {
            Self::default_disabled()
        } else {
            onchain_raw_config
                .and_then(|onchain_raw| OnChainRandomnessConfig::try_from(onchain_raw).ok())
                .unwrap_or_else(OnChainRandomnessConfig::default_if_missing)
        }
    }
```

**File:** types/src/on_chain_config/randomness_config.rs (L213-219)
```rust
    pub fn fast_randomness_enabled(&self) -> bool {
        match self {
            OnChainRandomnessConfig::Off => false,
            OnChainRandomnessConfig::V1(_) => false,
            OnChainRandomnessConfig::V2(_) => true,
        }
    }
```

**File:** consensus/src/rand/rand_gen/rand_store.rs (L261-277)
```rust
    pub fn add_rand_metadata(&mut self, rand_metadata: FullRandMetadata) {
        let rand_item = self
            .rand_map
            .entry(rand_metadata.round())
            .or_insert_with(|| RandItem::new(self.author, PathType::Slow));
        rand_item.add_metadata(&self.rand_config, rand_metadata.clone());
        rand_item.try_aggregate(&self.rand_config, self.decision_tx.clone());
        // fast path
        if let (Some(fast_rand_map), Some(fast_rand_config)) =
            (self.fast_rand_map.as_mut(), self.fast_rand_config.as_ref())
        {
            let fast_rand_item = fast_rand_map
                .entry(rand_metadata.round())
                .or_insert_with(|| RandItem::new(self.author, PathType::Fast));
            fast_rand_item.add_metadata(fast_rand_config, rand_metadata.clone());
            fast_rand_item.try_aggregate(fast_rand_config, self.decision_tx.clone());
        }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L803-811)
```rust
        let (rand_result, _has_randomness) = rand_check.await?;

        tracker.start_working();
        // if randomness is disabled, the metadata skips DKG and triggers immediate reconfiguration
        let metadata_txn = if let Some(maybe_rand) = rand_result {
            block.new_metadata_with_randomness(&validator, maybe_rand)
        } else {
            block.new_block_metadata(&validator).into()
        };
```

**File:** aptos-move/framework/aptos-framework/sources/randomness.move (L64-72)
```text
    public(friend) fun on_new_block(vm: &signer, epoch: u64, round: u64, seed_for_new_block: Option<vector<u8>>) acquires PerBlockRandomness {
        system_addresses::assert_vm(vm);
        if (exists<PerBlockRandomness>(@aptos_framework)) {
            let randomness = borrow_global_mut<PerBlockRandomness>(@aptos_framework);
            randomness.epoch = epoch;
            randomness.round = round;
            randomness.seed = seed_for_new_block;
        }
    }
```

**File:** types/src/block_info.rs (L36-37)
```rust
    /// The accumulator root hash after executing this block.
    executed_state_id: HashValue,
```

**File:** aptos-move/framework/aptos-framework/sources/configs/randomness_config_seqnum.move (L1-9)
```text
/// Randomness stall recovery utils.
///
/// When randomness generation is stuck due to a bug, the chain is also stuck. Below is the recovery procedure.
/// 1. Ensure more than 2/3 stakes are stuck at the same version.
/// 1. Every validator restarts with `randomness_override_seq_num` set to `X+1` in the node config file,
///    where `X` is the current `RandomnessConfigSeqNum` on chain.
/// 1. The chain should then be unblocked.
/// 1. Once the bug is fixed and the binary + framework have been patched,
///    a governance proposal is needed to set `RandomnessConfigSeqNum` to be `X+2`.
```

**File:** testsuite/smoke-test/src/randomness/randomness_stall_recovery.rs (L65-84)
```rust
    for (idx, validator) in swarm.validators_mut().enumerate() {
        info!("Stopping validator {}.", idx);
        validator.stop();
        let config_path = validator.config_path();
        let mut validator_override_config =
            OverrideNodeConfig::load_config(config_path.clone()).unwrap();
        validator_override_config
            .override_config_mut()
            .randomness_override_seq_num = 1;
        validator_override_config
            .override_config_mut()
            .consensus
            .sync_only = false;
        info!("Updating validator {} config.", idx);
        validator_override_config.save_config(config_path).unwrap();
        info!("Restarting validator {}.", idx);
        validator.start().unwrap();
        info!("Let validator {} bake for 5 secs.", idx);
        tokio::time::sleep(Duration::from_secs(5)).await;
    }
```
