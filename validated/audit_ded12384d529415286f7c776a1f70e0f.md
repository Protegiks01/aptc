# Audit Report

## Title
Epoch Ending Pruner Failure After Rollback Due to Stale Metadata Causing Database Bloat and Node Crashes

## Summary
During database rollback operations (triggered by crash recovery or state sync), the truncation functions fail to update epoch ending pruner progress metadata keys. This causes the epoch snapshot pruner to initialize with stale progress values pointing to non-existent future versions, permanently disabling pruning and leading to unbounded database growth and eventual node crashes due to disk exhaustion.

## Finding Description

The vulnerability exists in the interaction between database truncation during rollbacks and pruner metadata management for the epoch ending state merkle pruner.

**Root Cause:**

During truncation, the `delete_nodes_and_stale_indices_at_or_after_version` function only updates commit progress metadata, not pruner progress metadata. [1](#0-0) 

The function calls `StateMerkleDb::put_progress` which exclusively updates `StateMerkleCommitProgress` and `StateMerkleShardCommitProgress` keys, leaving pruner progress keys (`EpochEndingStateMerklePrunerProgress`, `EpochEndingStateMerkleShardPrunerProgress`) untouched. [2](#0-1) 

**Exploitation Path:**

1. During crash recovery, `StateStore::sync_commit_progress` is called on initialization: [3](#0-2) 

2. This triggers `truncate_state_merkle_db` to rollback to a consistent state: [4](#0-3) 

3. After rollback, pruner progress metadata remains stale (pointing to versions that no longer exist).

4. On restart, the pruner manager initializes by reading the stale pruner progress: [5](#0-4) 

5. The metadata pruner reads this stale progress value: [6](#0-5) 

6. For `StaleNodeIndexCrossEpochSchema` (epoch ending pruner), this uses the `EpochEndingStateMerklePrunerProgress` key: [7](#0-6) 

7. The stale progress value is used to initialize both target and progress: [8](#0-7) 

8. When new data is written, `maybe_set_pruner_target_db_version` checks if `latest_version >= min_readable_version + prune_window`. With stale metadata (e.g., min_readable_version=7000 after rollback to 5000), this check fails: [9](#0-8) 

9. The pruner's prune operation returns early because `progress >= target_version` (both initialized to stale value): [10](#0-9) 

10. New stale nodes accumulate indefinitely, leading to database bloat and eventual disk exhaustion.

## Impact Explanation

**Severity: High**

This vulnerability qualifies as **High** severity under the "Validator Node Slowdowns" category that leads to node crashes:

- **Resource Exhaustion**: Unbounded database growth consumes all available disk space
- **Node Crashes**: Disk full condition causes validator node crashes
- **Availability Impact**: Affected validators cannot participate in consensus
- **Permanent Failure**: Pruner remains broken until manual database rebuild
- **Network Impact**: If multiple validators are affected simultaneously (common during network-wide state sync events), this could escalate to network liveness issues

The vulnerability does NOT:
- Cause consensus violations or state corruption (data correctness is maintained)
- Enable fund theft or unauthorized minting
- Require malicious actors to exploit (triggered by normal operational events)

## Likelihood Explanation

**Likelihood: Medium to High**

This bug is triggered in realistic operational scenarios:

1. **Crash Recovery**: Validators recovering from crashes with uncommitted data (common)
2. **State Sync Operations**: Nodes performing fast sync or state snapshot synchronization
3. **Database Restoration**: Operators restoring from backups during disaster recovery
4. **Network Partitions**: Nodes resyncing after network splits

The truncation mechanism is invoked automatically during node initialization when commit progress is inconsistent, making this a frequently encountered code path. Once triggered, the pruner failure is **permanent** without manual intervention (database rebuild or manual progress metadata correction).

## Recommendation

Update the truncation functions to also reset pruner progress metadata when rolling back. Specifically, modify `delete_nodes_and_stale_indices_at_or_after_version` to update pruner progress keys in addition to commit progress keys:

```rust
fn delete_nodes_and_stale_indices_at_or_after_version(
    db: &DB,
    version: Version,
    shard_id: Option<usize>,
    batch: &mut SchemaBatch,
) -> Result<()> {
    delete_stale_node_index_at_or_after_version::<StaleNodeIndexSchema>(db, version, batch)?;
    delete_stale_node_index_at_or_after_version::<StaleNodeIndexCrossEpochSchema>(
        db, version, batch,
    )?;

    let mut iter = db.iter::<JellyfishMerkleNodeSchema>()?;
    iter.seek(&NodeKey::new_empty_path(version))?;
    for item in iter {
        let (key, _) = item?;
        batch.delete::<JellyfishMerkleNodeSchema>(&key)?;
    }

    // Update commit progress
    StateMerkleDb::put_progress(version.checked_sub(1), shard_id, batch)?;
    
    // ADDED: Also update pruner progress to prevent stale metadata after rollback
    let target_progress = version.checked_sub(1).unwrap_or(0);
    
    // Update both regular and epoch-ending pruner progress
    let pruner_keys = [
        StaleNodeIndexSchema::progress_metadata_key(shard_id),
        StaleNodeIndexCrossEpochSchema::progress_metadata_key(shard_id),
    ];
    
    for key in pruner_keys {
        batch.put::<DbMetadataSchema>(&key, &DbMetadataValue::Version(target_progress))?;
    }
    
    Ok(())
}
```

Additionally, add defensive initialization logic in the pruner manager to detect and correct stale progress values that exceed the current database version.

## Proof of Concept

The vulnerability can be reproduced by:

1. Running a validator node normally until epoch ending pruner has pruned to version V1
2. Triggering a crash or uncommitted state that requires rollback to version V2 < V1
3. Restarting the node to trigger `sync_commit_progress` and truncation
4. Observing that `EpochEndingStateMerklePrunerProgress` still contains V1
5. Attempting to prune new data - the pruner will be stuck and never update its target
6. Monitoring database size growth as stale nodes accumulate without being pruned

**Notes:**
- All file paths reference the `grass-dev-pa/aptos-core-079` repository
- This vulnerability affects the storage layer's pruning mechanism, not consensus correctness
- The bug is deterministically triggered by crash recovery scenarios with rollback
- Manual intervention (database rebuild or metadata correction) is required to recover

### Citations

**File:** storage/aptosdb/src/utils/truncation_helper.rs (L603-622)
```rust
fn delete_nodes_and_stale_indices_at_or_after_version(
    db: &DB,
    version: Version,
    shard_id: Option<usize>,
    batch: &mut SchemaBatch,
) -> Result<()> {
    delete_stale_node_index_at_or_after_version::<StaleNodeIndexSchema>(db, version, batch)?;
    delete_stale_node_index_at_or_after_version::<StaleNodeIndexCrossEpochSchema>(
        db, version, batch,
    )?;

    let mut iter = db.iter::<JellyfishMerkleNodeSchema>()?;
    iter.seek(&NodeKey::new_empty_path(version))?;
    for item in iter {
        let (key, _) = item?;
        batch.delete::<JellyfishMerkleNodeSchema>(&key)?;
    }

    StateMerkleDb::put_progress(version.checked_sub(1), shard_id, batch)
}
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L393-409)
```rust
    pub(crate) fn put_progress(
        version: Option<Version>,
        shard_id: Option<usize>,
        batch: &mut impl WriteBatch,
    ) -> Result<()> {
        let key = if let Some(shard_id) = shard_id {
            DbMetadataKey::StateMerkleShardCommitProgress(shard_id)
        } else {
            DbMetadataKey::StateMerkleCommitProgress
        };

        if let Some(version) = version {
            batch.put::<DbMetadataSchema>(&key, &DbMetadataValue::Version(version))
        } else {
            batch.delete::<DbMetadataSchema>(&key)
        }
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L353-360)
```rust
        if !hack_for_tests && !empty_buffered_state_for_restore {
            Self::sync_commit_progress(
                Arc::clone(&ledger_db),
                Arc::clone(&state_kv_db),
                Arc::clone(&state_merkle_db),
                /*crash_if_difference_is_too_large=*/ true,
            );
        }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L490-498)
```rust
            if state_merkle_target_version < state_merkle_max_version {
                info!(
                    state_merkle_max_version = state_merkle_max_version,
                    target_version = state_merkle_target_version,
                    "Start state merkle truncation..."
                );
                truncate_state_merkle_db(&state_merkle_db, state_merkle_target_version)
                    .expect("Failed to truncate state merkle db.");
            }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_pruner_manager.rs (L67-72)
```rust
    fn maybe_set_pruner_target_db_version(&self, latest_version: Version) {
        let min_readable_version = self.get_min_readable_version();
        if self.is_pruner_enabled() && latest_version >= min_readable_version + self.prune_window {
            self.set_pruner_target_db_version(latest_version);
        }
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_pruner_manager.rs (L119-121)
```rust
        let min_readable_version = pruner_utils::get_state_merkle_pruner_progress(&state_merkle_db)
            .expect("Must succeed.");

```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs (L81-83)
```rust
    pub(in crate::pruner) fn progress(&self) -> Result<Version> {
        Ok(get_progress(&self.metadata_db, &S::progress_metadata_key(None))?.unwrap_or(0))
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs (L33-40)
```rust
impl StaleNodeIndexSchemaTrait for StaleNodeIndexCrossEpochSchema {
    fn progress_metadata_key(shard_id: Option<usize>) -> DbMetadataKey {
        if let Some(shard_id) = shard_id {
            DbMetadataKey::EpochEndingStateMerkleShardPrunerProgress(shard_id)
        } else {
            DbMetadataKey::EpochEndingStateMerklePrunerProgress
        }
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L62-67)
```rust
        let mut progress = self.progress();
        let target_version = self.target_version();

        if progress >= target_version {
            return Ok(progress);
        }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L124-166)
```rust
    pub fn new(state_merkle_db: Arc<StateMerkleDb>) -> Result<Self> {
        info!(name = S::name(), "Initializing...");

        let metadata_pruner = StateMerkleMetadataPruner::new(state_merkle_db.metadata_db_arc());
        let metadata_progress = metadata_pruner.progress()?;

        info!(
            metadata_progress = metadata_progress,
            "Created {} metadata pruner, start catching up all shards.",
            S::name(),
        );

        let shard_pruners = if state_merkle_db.sharding_enabled() {
            let num_shards = state_merkle_db.num_shards();
            let mut shard_pruners = Vec::with_capacity(num_shards);
            for shard_id in 0..num_shards {
                shard_pruners.push(StateMerkleShardPruner::new(
                    shard_id,
                    state_merkle_db.db_shard_arc(shard_id),
                    metadata_progress,
                )?);
            }
            shard_pruners
        } else {
            Vec::new()
        };

        let pruner = StateMerklePruner {
            target_version: AtomicVersion::new(metadata_progress),
            progress: AtomicVersion::new(metadata_progress),
            metadata_pruner,
            shard_pruners,
            _phantom: std::marker::PhantomData,
        };

        info!(
            name = pruner.name(),
            progress = metadata_progress,
            "Initialized."
        );

        Ok(pruner)
    }
```
