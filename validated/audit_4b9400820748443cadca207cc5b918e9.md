# Audit Report

## Title
Unbounded Iterator in StateKvShardPruner Causes Memory Exhaustion During Node Initialization

## Summary
The `StateKvShardPruner::prune()` function lacks batching controls and accumulates all deletion operations in a single unbounded `SchemaBatch` during shard catch-up. When a shard falls behind and the node restarts, the initialization process attempts to process potentially millions of stale entries at once, leading to excessive memory consumption and node crashes.

## Finding Description

The vulnerability exists in the `prune()` function which is called during shard pruner initialization to catch up with the metadata pruner's progress. The function creates a single `SchemaBatch` and iterates through all stale state value indices from `current_progress` to `target_version` without any batching mechanism: [1](#0-0) 

Each stale entry requires two deletions (index and value), and all operations accumulate in memory before being written to the database in a single atomic operation.

During node initialization, `StateKvShardPruner::new()` calls this function to catch up with the metadata pruner's progress: [2](#0-1) 

The parent `StateKvPruner` initializes all shard pruners sequentially during node startup: [3](#0-2) 

The `SchemaBatch` structure has no inherent memory limits - it's simply a HashMap that accumulates operations: [4](#0-3) 

**Critical Design Inconsistency:** In stark contrast, the similar `StateMerkleShardPruner` implements proper internal batching with a loop that processes entries incrementally: [5](#0-4) 

The batching mechanism calls `get_stale_node_indices()` with a `max_nodes_to_prune` limit that restricts the number of entries processed per iteration.

During **normal operation**, the `StateKvPruner::prune()` method implements proper batching: [6](#0-5) 

The normal operation uses a `max_versions` parameter (default 5,000 versions) to batch the pruning work. However, during initialization, the `prune()` function is called with the entire gap between current progress and metadata progress, with no batching applied.

**Attack Scenario:**

1. An attacker sends high-throughput transactions that update state frequently (within gas limits). According to configuration comments, a 10k transaction block can touch 60k state values: [7](#0-6) 

2. Stale entries accumulate across thousands of versions (e.g., 10,000 versions with high state churn)

3. Due to operational issues (crash, disk slowdown, restart), one or more shards fall behind the metadata pruner

4. On node restart, each shard attempts to catch up by processing all accumulated stale entries at once through the unbounded iteration

5. Memory consumption escalates as millions of deletion operations accumulate in the single `SchemaBatch`

6. Node runs out of memory and crashes, creating a persistent DoS condition where the node cannot restart without manual database intervention

## Impact Explanation

**Severity: Medium**

This vulnerability causes **validator node unavailability** requiring manual intervention, which aligns with the Medium severity category: "State inconsistencies requiring intervention." Specifically:

- **Validator node unavailability**: Affected nodes cannot start up, requiring manual database intervention or restoration from backup
- **Liveness impact**: Individual validator nodes become unavailable, though network consensus can continue with remaining validators (no total network liveness loss)
- **DoS amplification vector**: Attackers can amplify normal operational issues into complete node failure by maximizing state churn
- **Recovery complexity**: Requires manual database intervention, partial database restoration, or full node resync

The issue does NOT directly lead to:
- Consensus safety violations (other nodes continue operating correctly)
- Loss of funds or state corruption
- Network-wide partition (only affects individual nodes that restart)
- Critical network availability issues (network continues with remaining validators)

This matches Medium severity rather than High because it affects individual nodes during specific operational conditions, not the broader network consensus or availability.

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability occurs under specific but realistic production conditions:

**Natural occurrence scenarios:**
- Shard disk slowdown causing pruner lag
- Node crash during pruning operations
- Resource exhaustion on specific shard storage
- Node restarts for maintenance or updates

**Attacker-amplified scenarios:**
- Sending high-throughput transactions to maximize state value churn (limited by gas/fees but economically feasible)
- Timing attacks during known maintenance windows when restarts are likely

**Frequency factors:**
- Storage sharding is enabled by default: [8](#0-7) 

- Node restarts are common in production environments (updates, maintenance, crashes)
- The gap only needs to be ~10,000 versions with normal transaction traffic to cause memory issues
- Default ledger prune window is 90 million versions, providing ample opportunity for gaps to accumulate: [9](#0-8) 

- Default batch size for normal operation is 5,000 versions, demonstrating the expected scale of safe batch processing

While an attacker cannot directly force shards to fall behind, they can maximize state churn and wait for natural operational events to trigger the vulnerability.

## Recommendation

Implement batching in `StateKvShardPruner::prune()` similar to `StateMerkleShardPruner::prune()`. The function should:

1. Accept a `max_entries_to_prune` parameter
2. Use a loop to process entries in batches
3. Create a new `SchemaBatch` for each batch iteration
4. Only update the progress metadata when all batches are complete
5. During initialization in `StateKvShardPruner::new()`, use a reasonable batch size (e.g., 10,000 entries) instead of processing the entire gap at once

The implementation should mirror the batching pattern used in `StateMerkleShardPruner` to ensure consistent memory-safe behavior across all pruner components.

## Proof of Concept

The vulnerability is demonstrated through code analysis showing the clear design inconsistency between:
- `StateKvShardPruner::prune()` - no batching during initialization
- `StateMerkleShardPruner::prune()` - proper batching during initialization
- `StateKvPruner::prune()` - proper batching during normal operation

A concrete PoC would require:
1. Setting up a node with storage sharding enabled
2. Running high-throughput transactions to create state churn
3. Artificially stopping pruning on one shard to create a gap of 10,000+ versions
4. Restarting the node to trigger initialization catch-up
5. Monitoring memory consumption during the unbounded iteration

The logic vulnerability is clearly evident from the code structure without requiring runtime exploitation.

## Notes

This is a **logic vulnerability** arising from inconsistent design between initialization and normal operation paths. The same `prune()` function is called in two contexts:

1. **During normal operation**: Called with bounded ranges (5,000 versions by default)
2. **During initialization**: Called with potentially unbounded ranges (entire gap from current to metadata progress)

The vulnerability is particularly concerning because storage sharding is enabled by default in production configurations, making this issue affect the standard deployment configuration rather than an optional feature.

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L25-44)
```rust
    pub(in crate::pruner) fn new(
        shard_id: usize,
        db_shard: Arc<DB>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
            metadata_progress,
        )?;
        let myself = Self { shard_id, db_shard };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up state kv shard {shard_id}."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L47-72)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();

        let mut iter = self
            .db_shard
            .iter::<StaleStateValueIndexByKeyHashSchema>()?;
        iter.seek(&current_progress)?;
        for item in iter {
            let (index, _) = item?;
            if index.stale_since_version > target_version {
                break;
            }
            batch.delete::<StaleStateValueIndexByKeyHashSchema>(&index)?;
            batch.delete::<StateValueByKeyHashSchema>(&(index.state_key_hash, index.version))?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
            &DbMetadataValue::Version(target_version),
        )?;

        self.db_shard.write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L49-86)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_pruner__prune"]);

        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning state kv data."
            );
            self.metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state kv shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning state kv data is done.");
        }

        Ok(target_version)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L124-137)
```rust
        let shard_pruners = if state_kv_db.enabled_sharding() {
            let num_shards = state_kv_db.num_shards();
            let mut shard_pruners = Vec::with_capacity(num_shards);
            for shard_id in 0..num_shards {
                shard_pruners.push(StateKvShardPruner::new(
                    shard_id,
                    state_kv_db.db_shard_arc(shard_id),
                    metadata_progress,
                )?);
            }
            shard_pruners
        } else {
            Vec::new()
        };
```

**File:** storage/schemadb/src/batch.rs (L130-133)
```rust
pub struct SchemaBatch {
    rows: DropHelper<HashMap<ColumnFamilyName, Vec<WriteOp>>>,
    stats: SampledBatchStats,
}
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L58-100)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
        max_nodes_to_prune: usize,
    ) -> Result<()> {
        loop {
            let mut batch = SchemaBatch::new();
            let (indices, next_version) = StateMerklePruner::get_stale_node_indices(
                &self.db_shard,
                current_progress,
                target_version,
                max_nodes_to_prune,
            )?;

            indices.into_iter().try_for_each(|index| {
                batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
                batch.delete::<S>(&index)
            })?;

            let mut done = true;
            if let Some(next_version) = next_version {
                if next_version <= target_version {
                    done = false;
                }
            }

            if done {
                batch.put::<DbMetadataSchema>(
                    &S::progress_metadata_key(Some(self.shard_id)),
                    &DbMetadataValue::Version(target_version),
                )?;
            }

            self.db_shard.write_schemas(batch)?;

            if done {
                break;
            }
        }

        Ok(())
    }
```

**File:** config/src/config/storage_config.rs (L219-238)
```rust
impl Default for RocksdbConfigs {
    fn default() -> Self {
        Self {
            ledger_db_config: RocksdbConfig::default(),
            state_merkle_db_config: RocksdbConfig::default(),
            state_kv_db_config: RocksdbConfig {
                bloom_filter_bits: Some(10.0),
                bloom_before_level: Some(2),
                ..Default::default()
            },
            index_db_config: RocksdbConfig {
                max_open_files: 1000,
                ..Default::default()
            },
            enable_storage_sharding: true,
            high_priority_background_threads: 4,
            low_priority_background_threads: 2,
            shared_block_cache_size: Self::DEFAULT_BLOCK_CACHE_SIZE,
        }
    }
```

**File:** config/src/config/storage_config.rs (L387-395)
```rust
impl Default for LedgerPrunerConfig {
    fn default() -> Self {
        LedgerPrunerConfig {
            enable: true,
            prune_window: 90_000_000,
            batch_size: 5_000,
            user_pruning_window_offset: 200_000,
        }
    }
```

**File:** config/src/config/storage_config.rs (L408-411)
```rust
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
        }
```
