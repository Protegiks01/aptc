# Audit Report

## Title
Critical Epoch-Ending State Snapshot Premature Pruning Vulnerability Preventing Validator Recovery and Network Growth

## Summary
State merkle tree nodes created at epoch-ending versions are incorrectly classified for short-term pruning (1M version window) instead of long-term epoch snapshot pruning (80M version window). This causes fast sync failures when validators attempt to join or recover after the short prune window expires, leading to network partition and consensus unavailability.

## Finding Description

The vulnerability exists in the state merkle tree node pruning classification logic. When a state snapshot is created at epoch-ending version V (marking the end of epoch N), the system retrieves `previous_epoch_ending_version` to determine whether stale JMT nodes should be stored in `StaleNodeIndexCrossEpochSchema` (long-term, 80M versions) or `StaleNodeIndexSchema` (short-term, 1M versions). [1](#0-0) 

The `get_previous_epoch_ending` function explicitly seeks to `version - 1`, returning the PREVIOUS epoch ending (epoch N-1), not the current epoch ending: [2](#0-1) 

This is confirmed by the test which asserts that for the last version, it returns `last_epoch - 1`: [3](#0-2) 

**The Bug:** In the classification logic, stale nodes are checked against this `previous_epoch_ending_version`: [4](#0-3) 

When creating a snapshot at epoch-ending version V (epoch N ending):
- `previous_epoch_ending_version` = epoch N-1's ending version
- Stale nodes at version V fail the check: `V <= previous_epoch_ending_version` (since V > epoch N-1)
- These critical epoch-ending nodes are stored in `StaleNodeIndexSchema` (1M window)
- They should be in `StaleNodeIndexCrossEpochSchema` (80M window)

**The Impact:** When state sync attempts to read epoch-ending state after 1M versions, `error_if_state_merkle_pruned` passes the check because the version is within the epoch snapshot pruner's 80M window AND is marked as epoch ending: [5](#0-4) 

However, when `get_state_value_chunk_with_proof` attempts to generate merkle proofs, it must traverse the JMT nodes: [6](#0-5) [7](#0-6) 

The actual JMT nodes were already pruned by state_merkle_pruner (1M window), causing proof generation to fail with missing node errors.

## Impact Explanation

**Critical Severity** - This vulnerability causes multiple critical availability failures aligning with Aptos bug bounty Critical categories:

1. **Non-recoverable Network Partition**: New validators cannot join the network if they need to fast sync to an epoch boundary older than ~55 hours (at 5000 TPS). This permanently prevents network growth and decentralization, requiring a hardfork to resolve.

2. **Total Loss of Liveness/Network Availability**: If multiple validators experience downtime exceeding the prune window simultaneously, they cannot recover and rejoin consensus. With sufficient validators offline, the network loses BFT consensus quorum (< 2/3 validators available), halting the network entirely.

3. **State Consistency Violation**: The system provides false guarantees - `error_if_state_merkle_pruned` indicates data is available for 80M versions when it's actually pruned after 1M versions, violating state sync availability guarantees.

Default configuration values confirm the prune window discrepancy: [8](#0-7) [9](#0-8) 

## Likelihood Explanation

**High Likelihood** on production networks:

- At mainnet-scale (5000+ TPS): 1M versions â‰ˆ 55 hours
- Epoch reconfigurations occur regularly (every ~2 hours on Aptos mainnet)
- New validators joining after 55+ hours WILL encounter this issue
- Validators experiencing 55+ hours downtime CANNOT recover
- No attacker action required - this is a deterministic bug triggered by normal operations

The vulnerability is guaranteed to manifest on any network operating at moderate-to-high transaction throughput for extended periods.

## Recommendation

The fix should ensure that stale nodes created AT an epoch-ending version are classified as cross-epoch nodes. Modify the classification logic to check if the current version IS an epoch ending, not just if it's before or at the previous epoch ending:

```rust
// In create_jmt_commit_batch_for_shard:
let is_current_epoch_ending = self.ledger_db
    .metadata_db()
    .ensure_epoch_ending(version)
    .is_ok();

stale_node_index_batch.iter().try_for_each(|row| {
    if (previous_epoch_ending_version.is_some() 
        && row.node_key.version() <= previous_epoch_ending_version.unwrap())
        || (is_current_epoch_ending && row.node_key.version() == version)
    {
        batch.put::<StaleNodeIndexCrossEpochSchema>(row, &())
    } else {
        batch.put::<StaleNodeIndexSchema>(row, &())
    }
})?;
```

## Proof of Concept

This vulnerability can be demonstrated by:

1. Starting a validator network and running it through multiple epochs
2. Waiting until 1M+ versions have passed since an epoch boundary
3. Starting a new validator that needs to fast sync to that epoch boundary
4. Observing that `error_if_state_merkle_pruned` passes but `get_state_value_chunk_with_proof` fails with missing JMT nodes

The failure occurs because the nodes were pruned by `state_merkle_pruner` (1M window) even though they represent an epoch-ending snapshot that should be retained for 80M versions by the `epoch_snapshot_pruner`.

### Citations

**File:** storage/aptosdb/src/state_store/state_snapshot_committer.rs (L93-99)
```rust
                    let previous_epoch_ending_version = self
                        .state_db
                        .ledger_db
                        .metadata_db()
                        .get_previous_epoch_ending(version)
                        .unwrap()
                        .map(|(v, _e)| v);
```

**File:** storage/aptosdb/src/ledger_db/ledger_metadata_db.rs (L246-259)
```rust
    pub(crate) fn get_previous_epoch_ending(
        &self,
        version: Version,
    ) -> Result<Option<(u64, Version)>> {
        if version == 0 {
            return Ok(None);
        }
        let prev_version = version - 1;

        let mut iter = self.db.iter::<EpochByVersionSchema>()?;
        // Search for the end of the previous epoch.
        iter.seek_for_prev(&prev_version)?;
        iter.next().transpose()
    }
```

**File:** storage/aptosdb/src/ledger_db/ledger_metadata_db_test.rs (L219-226)
```rust
        let result = ledger_metadata_db.get_previous_epoch_ending(last_version).unwrap();
        if ledger_infos_with_sigs.len() < 2 {
            prop_assert_eq!(result, None);
        } else {
            let (version, epoch) = result.unwrap();
            prop_assert_eq!(epoch, last_epoch - 1);
            prop_assert_eq!(version, ledger_metadata_db.get_latest_ledger_info_in_epoch(epoch).unwrap().commit_info().version());
        }
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L376-386)
```rust
        stale_node_index_batch.iter().try_for_each(|row| {
            ensure!(row.node_key.get_shard_id() == shard_id, "shard_id mismatch");
            if previous_epoch_ending_version.is_some()
                && row.node_key.version() <= previous_epoch_ending_version.unwrap()
            {
                batch.put::<StaleNodeIndexCrossEpochSchema>(row, &())
            } else {
                // These are processed by the state merkle pruner.
                batch.put::<StaleNodeIndexSchema>(row, &())
            }
        })?;
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L273-303)
```rust
    pub(super) fn error_if_state_merkle_pruned(
        &self,
        data_type: &str,
        version: Version,
    ) -> Result<()> {
        let min_readable_version = self
            .state_store
            .state_db
            .state_merkle_pruner
            .get_min_readable_version();
        if version >= min_readable_version {
            return Ok(());
        }

        let min_readable_epoch_snapshot_version = self
            .state_store
            .state_db
            .epoch_snapshot_pruner
            .get_min_readable_version();
        if version >= min_readable_epoch_snapshot_version {
            self.ledger_db.metadata_db().ensure_epoch_ending(version)
        } else {
            bail!(
                "{} at version {} is pruned. snapshots are available at >= {}, epoch snapshots are available at >= {}",
                data_type,
                version,
                min_readable_version,
                min_readable_epoch_snapshot_version,
            )
        }
    }
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L880-891)
```rust
    fn get_state_value_chunk_with_proof(
        &self,
        version: Version,
        first_index: usize,
        chunk_size: usize,
    ) -> Result<StateValueChunkWithProof> {
        gauged_api("get_state_value_chunk_with_proof", || {
            self.error_if_state_merkle_pruned("State merkle", version)?;
            self.state_store
                .get_value_chunk_with_proof(version, first_index, chunk_size)
        })
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L749-755)
```rust
    pub fn get_value_range_proof(
        &self,
        rightmost_key: HashValue,
        version: Version,
    ) -> Result<SparseMerkleRangeProof> {
        self.state_merkle_db.get_range_proof(rightmost_key, version)
    }
```

**File:** config/src/config/storage_config.rs (L398-413)
```rust
impl Default for StateMerklePrunerConfig {
    fn default() -> Self {
        StateMerklePrunerConfig {
            enable: true,
            // This allows a block / chunk being executed to have access to a non-latest state tree.
            // It needs to be greater than the number of versions the state committing thread is
            // able to commit during the execution of the block / chunk. If the bad case indeed
            // happens due to this being too small, a node restart should recover it.
            // Still, defaulting to 1M to be super safe.
            prune_window: 1_000_000,
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
        }
    }
}
```

**File:** config/src/config/storage_config.rs (L415-431)
```rust
impl Default for EpochSnapshotPrunerConfig {
    fn default() -> Self {
        Self {
            enable: true,
            // This is based on ~5K TPS * 2h/epoch * 2 epochs. -- epoch ending snapshots are used
            // by state sync in fast sync mode.
            // The setting is in versions, not epochs, because this makes it behave more like other
            // pruners: a slower network will have longer history in db with the same pruner
            // settings, but the disk space take will be similar.
            // settings.
            prune_window: 80_000_000,
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
        }
    }
}
```
