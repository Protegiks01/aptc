# Audit Report

## Title
Epoch Ending Pruner Failure After Rollback Due to Stale Metadata Causing Database Bloat and Node Crashes

## Summary
During database rollback operations triggered by crash recovery or state sync, the truncation functions fail to update epoch ending pruner progress metadata keys. This causes the epoch snapshot pruner to initialize with stale progress values pointing to non-existent future versions, permanently disabling pruning and leading to unbounded database growth and eventual node crashes due to disk exhaustion.

## Finding Description

The vulnerability exists in the interaction between database truncation during rollbacks and pruner metadata management for the epoch ending state merkle pruner.

**Root Cause:**

During truncation, the `delete_nodes_and_stale_indices_at_or_after_version` function only updates commit progress metadata by calling `StateMerkleDb::put_progress` at the end of the function. [1](#0-0) 

The `StateMerkleDb::put_progress` function exclusively updates `StateMerkleCommitProgress` and `StateMerkleShardCommitProgress` keys, leaving pruner progress keys (`EpochEndingStateMerklePrunerProgress`, `EpochEndingStateMerkleShardPrunerProgress`) untouched. [2](#0-1) 

**Exploitation Path:**

1. During crash recovery, `StateStore::sync_commit_progress` is called on initialization to synchronize database progress across components. [3](#0-2) 

2. This triggers `truncate_state_merkle_db` to rollback the state merkle database to a consistent state. [4](#0-3) 

3. The truncation process calls `delete_nodes_and_stale_indices_at_or_after_version` which deletes stale indices for both `StaleNodeIndexSchema` and `StaleNodeIndexCrossEpochSchema` but only updates commit progress metadata. [5](#0-4) 

4. After rollback, pruner progress metadata remains stale (pointing to versions that no longer exist). The pruner manager initializes by reading this stale pruner progress through `get_state_merkle_pruner_progress`. [6](#0-5) 

5. The `get_state_merkle_pruner_progress` function reads from the schema-specific progress metadata key. [7](#0-6) 

6. For `StaleNodeIndexCrossEpochSchema` (epoch ending pruner), the progress metadata key is `EpochEndingStateMerklePrunerProgress`. [8](#0-7) 

7. The stale progress value is then used to initialize both `target_version` and `progress` in the pruner. [9](#0-8) 

8. When new data is written after recovery, `maybe_set_pruner_target_db_version` checks if `latest_version >= min_readable_version + prune_window`. With stale metadata (e.g., min_readable_version=7000 after rollback to 5000), this check fails indefinitely. [10](#0-9) 

9. The pruner's prune operation returns early because `progress >= target_version` when both are initialized to the same stale value. [11](#0-10) 

10. New stale nodes from `StaleNodeIndexCrossEpochSchema` accumulate indefinitely without pruning, leading to database bloat and eventual disk exhaustion.

## Impact Explanation

**Severity: High**

This vulnerability qualifies as **High** severity under the "Validator Node Slowdowns" category that leads to node crashes through DoS via resource exhaustion:

- **Resource Exhaustion**: Unbounded database growth consumes all available disk space as epoch-ending stale nodes are never pruned
- **Node Crashes**: Disk full condition causes validator node crashes, preventing consensus participation
- **Availability Impact**: Affected validators cannot participate in consensus until manual intervention
- **Permanent Failure**: The pruner remains broken until manual database rebuild or metadata correction
- **Network-Wide Risk**: If multiple validators are affected simultaneously during network-wide state sync events, this could escalate to broader network liveness issues

The vulnerability does NOT:
- Cause consensus violations or state corruption (data correctness is maintained)
- Enable fund theft or unauthorized minting
- Require malicious actors to exploit (triggered by normal operational events)

## Likelihood Explanation

**Likelihood: High**

This bug is triggered in realistic and common operational scenarios:

1. **Crash Recovery**: Validators recovering from crashes with uncommitted data trigger automatic truncation during initialization
2. **State Sync Operations**: Nodes performing fast sync or state snapshot synchronization
3. **Database Restoration**: Operators restoring from backups during disaster recovery
4. **Network Partitions**: Nodes resyncing after network splits

The truncation mechanism is invoked automatically during node initialization when commit progress is inconsistent, as confirmed by the `sync_commit_progress` call in `StateStore::new`. [12](#0-11) 

Once triggered, the pruner failure is **permanent** without manual intervention (database rebuild or manual progress metadata correction), as the pruner will never self-correct since the target version check continuously fails.

## Recommendation

The truncation function should update pruner progress metadata in addition to commit progress metadata. Specifically, `delete_nodes_and_stale_indices_at_or_after_version` should reset both regular and epoch-ending pruner progress keys to align with the truncated version.

**Recommended Fix:**

Modify `delete_nodes_and_stale_indices_at_or_after_version` to also update pruner progress keys:

```rust
fn delete_nodes_and_stale_indices_at_or_after_version(
    db: &DB,
    version: Version,
    shard_id: Option<usize>,
    batch: &mut SchemaBatch,
) -> Result<()> {
    // ... existing deletion logic ...
    
    // Update commit progress
    StateMerkleDb::put_progress(version.checked_sub(1), shard_id, batch)?;
    
    // ADDED: Also update pruner progress keys to prevent stale metadata
    let pruner_progress_version = version.checked_sub(1);
    
    if let Some(progress_version) = pruner_progress_version {
        // Update regular state merkle pruner progress
        let pruner_key = if let Some(shard_id) = shard_id {
            DbMetadataKey::StateMerkleShardPrunerProgress(shard_id)
        } else {
            DbMetadataKey::StateMerklePrunerProgress
        };
        batch.put::<DbMetadataSchema>(&pruner_key, &DbMetadataValue::Version(progress_version))?;
        
        // Update epoch ending state merkle pruner progress
        let epoch_pruner_key = if let Some(shard_id) = shard_id {
            DbMetadataKey::EpochEndingStateMerkleShardPrunerProgress(shard_id)
        } else {
            DbMetadataKey::EpochEndingStateMerklePrunerProgress
        };
        batch.put::<DbMetadataSchema>(&epoch_pruner_key, &DbMetadataValue::Version(progress_version))?;
    } else {
        // If truncating to before genesis, delete progress keys
        let pruner_key = if let Some(shard_id) = shard_id {
            DbMetadataKey::StateMerkleShardPrunerProgress(shard_id)
        } else {
            DbMetadataKey::StateMerklePrunerProgress
        };
        batch.delete::<DbMetadataSchema>(&pruner_key)?;
        
        let epoch_pruner_key = if let Some(shard_id) = shard_id {
            DbMetadataKey::EpochEndingStateMerkleShardPrunerProgress(shard_id)
        } else {
            DbMetadataKey::EpochEndingStateMerklePrunerProgress
        };
        batch.delete::<DbMetadataSchema>(&epoch_pruner_key)?;
    }
    
    Ok(())
}
```

## Proof of Concept

This vulnerability can be demonstrated by:

1. Running a validator node to version 7000
2. Simulating a crash with uncommitted data
3. Restarting the node to trigger truncation back to version 5000
4. Observing that `EpochEndingStateMerklePrunerProgress` remains at 7000
5. Verifying that the epoch snapshot pruner never prunes new stale nodes
6. Monitoring database growth as stale epoch-ending nodes accumulate indefinitely

The relevant database metadata keys can be inspected using the db_debugger tool to confirm the stale metadata issue. [13](#0-12) 

## Notes

This vulnerability specifically affects the epoch ending state merkle pruner (`StaleNodeIndexCrossEpochSchema`) which maintains epoch-ending snapshots with longer retention windows. The regular state merkle pruner (`StaleNodeIndexSchema`) is affected by the same root cause but may have different operational impact due to different pruning windows.

The issue is particularly severe because:
- It affects critical infrastructure (validator nodes)
- It's triggered automatically without operator awareness
- Recovery requires manual intervention or complete database rebuild
- Multiple validators can be affected simultaneously during coordinated operations

### Citations

**File:** storage/aptosdb/src/utils/truncation_helper.rs (L603-622)
```rust
fn delete_nodes_and_stale_indices_at_or_after_version(
    db: &DB,
    version: Version,
    shard_id: Option<usize>,
    batch: &mut SchemaBatch,
) -> Result<()> {
    delete_stale_node_index_at_or_after_version::<StaleNodeIndexSchema>(db, version, batch)?;
    delete_stale_node_index_at_or_after_version::<StaleNodeIndexCrossEpochSchema>(
        db, version, batch,
    )?;

    let mut iter = db.iter::<JellyfishMerkleNodeSchema>()?;
    iter.seek(&NodeKey::new_empty_path(version))?;
    for item in iter {
        let (key, _) = item?;
        batch.delete::<JellyfishMerkleNodeSchema>(&key)?;
    }

    StateMerkleDb::put_progress(version.checked_sub(1), shard_id, batch)
}
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L393-409)
```rust
    pub(crate) fn put_progress(
        version: Option<Version>,
        shard_id: Option<usize>,
        batch: &mut impl WriteBatch,
    ) -> Result<()> {
        let key = if let Some(shard_id) = shard_id {
            DbMetadataKey::StateMerkleShardCommitProgress(shard_id)
        } else {
            DbMetadataKey::StateMerkleCommitProgress
        };

        if let Some(version) = version {
            batch.put::<DbMetadataSchema>(&key, &DbMetadataValue::Version(version))
        } else {
            batch.delete::<DbMetadataSchema>(&key)
        }
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L354-359)
```rust
            Self::sync_commit_progress(
                Arc::clone(&ledger_db),
                Arc::clone(&state_kv_db),
                Arc::clone(&state_merkle_db),
                /*crash_if_difference_is_too_large=*/ true,
            );
```

**File:** storage/aptosdb/src/state_store/mod.rs (L410-502)
```rust
    pub fn sync_commit_progress(
        ledger_db: Arc<LedgerDb>,
        state_kv_db: Arc<StateKvDb>,
        state_merkle_db: Arc<StateMerkleDb>,
        crash_if_difference_is_too_large: bool,
    ) {
        let ledger_metadata_db = ledger_db.metadata_db();
        if let Some(overall_commit_progress) = ledger_metadata_db
            .get_synced_version()
            .expect("DB read failed.")
        {
            info!(
                overall_commit_progress = overall_commit_progress,
                "Start syncing databases..."
            );
            let ledger_commit_progress = ledger_metadata_db
                .get_ledger_commit_progress()
                .expect("Failed to read ledger commit progress.");
            assert_ge!(ledger_commit_progress, overall_commit_progress);

            let state_kv_commit_progress = state_kv_db
                .metadata_db()
                .get::<DbMetadataSchema>(&DbMetadataKey::StateKvCommitProgress)
                .expect("Failed to read state K/V commit progress.")
                .expect("State K/V commit progress cannot be None.")
                .expect_version();
            assert_ge!(state_kv_commit_progress, overall_commit_progress);

            // LedgerCommitProgress was not guaranteed to commit after all ledger changes finish,
            // have to attempt truncating every column family.
            info!(
                ledger_commit_progress = ledger_commit_progress,
                "Attempt ledger truncation...",
            );
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");

            // State K/V commit progress isn't (can't be) written atomically with the data,
            // because there are shards, so we have to attempt truncation anyway.
            info!(
                state_kv_commit_progress = state_kv_commit_progress,
                "Start state KV truncation..."
            );
            let difference = state_kv_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_state_kv_db(
                &state_kv_db,
                state_kv_commit_progress,
                overall_commit_progress,
                std::cmp::max(difference as usize, 1), /* batch_size */
            )
            .expect("Failed to truncate state K/V db.");

            let state_merkle_max_version = get_max_version_in_state_merkle_db(&state_merkle_db)
                .expect("Failed to get state merkle max version.")
                .expect("State merkle max version cannot be None.");
            if state_merkle_max_version > overall_commit_progress {
                let difference = state_merkle_max_version - overall_commit_progress;
                if crash_if_difference_is_too_large {
                    assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
                }
            }
            let state_merkle_target_version = find_tree_root_at_or_before(
                ledger_metadata_db,
                &state_merkle_db,
                overall_commit_progress,
            )
            .expect("DB read failed.")
            .unwrap_or_else(|| {
                panic!(
                    "Could not find a valid root before or at version {}, maybe it was pruned?",
                    overall_commit_progress
                )
            });
            if state_merkle_target_version < state_merkle_max_version {
                info!(
                    state_merkle_max_version = state_merkle_max_version,
                    target_version = state_merkle_target_version,
                    "Start state merkle truncation..."
                );
                truncate_state_merkle_db(&state_merkle_db, state_merkle_target_version)
                    .expect("Failed to truncate state merkle db.");
            }
        } else {
            info!("No overall commit progress was found!");
        }
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_pruner_manager.rs (L67-71)
```rust
    fn maybe_set_pruner_target_db_version(&self, latest_version: Version) {
        let min_readable_version = self.get_min_readable_version();
        if self.is_pruner_enabled() && latest_version >= min_readable_version + self.prune_window {
            self.set_pruner_target_db_version(latest_version);
        }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_pruner_manager.rs (L119-120)
```rust
        let min_readable_version = pruner_utils::get_state_merkle_pruner_progress(&state_merkle_db)
            .expect("Must succeed.");
```

**File:** storage/aptosdb/src/pruner/pruner_utils.rs (L31-42)
```rust
pub(crate) fn get_state_merkle_pruner_progress<S: StaleNodeIndexSchemaTrait>(
    state_merkle_db: &StateMerkleDb,
) -> Result<Version>
where
    StaleNodeIndex: KeyCodec<S>,
{
    Ok(get_progress(
        state_merkle_db.metadata_db(),
        &S::progress_metadata_key(None),
    )?
    .unwrap_or(0))
}
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs (L33-40)
```rust
impl StaleNodeIndexSchemaTrait for StaleNodeIndexCrossEpochSchema {
    fn progress_metadata_key(shard_id: Option<usize>) -> DbMetadataKey {
        if let Some(shard_id) = shard_id {
            DbMetadataKey::EpochEndingStateMerkleShardPrunerProgress(shard_id)
        } else {
            DbMetadataKey::EpochEndingStateMerklePrunerProgress
        }
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L62-67)
```rust
        let mut progress = self.progress();
        let target_version = self.target_version();

        if progress >= target_version {
            return Ok(progress);
        }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L151-153)
```rust
        let pruner = StateMerklePruner {
            target_version: AtomicVersion::new(metadata_progress),
            progress: AtomicVersion::new(metadata_progress),
```

**File:** storage/aptosdb/src/schema/db_metadata/mod.rs (L47-72)
```rust
#[derive(Clone, Debug, Deserialize, Eq, PartialEq, Serialize)]
#[cfg_attr(any(test, feature = "fuzzing"), derive(proptest_derive::Arbitrary))]
pub enum DbMetadataKey {
    LedgerPrunerProgress,
    StateMerklePrunerProgress,
    EpochEndingStateMerklePrunerProgress,
    StateKvPrunerProgress,
    StateSnapshotKvRestoreProgress(Version),
    LedgerCommitProgress,
    StateKvCommitProgress,
    OverallCommitProgress,
    StateKvShardCommitProgress(ShardId),
    StateMerkleCommitProgress,
    StateMerkleShardCommitProgress(ShardId),
    EventPrunerProgress,
    TransactionAccumulatorPrunerProgress,
    TransactionInfoPrunerProgress,
    TransactionPrunerProgress,
    WriteSetPrunerProgress,
    StateMerkleShardPrunerProgress(ShardId),
    EpochEndingStateMerkleShardPrunerProgress(ShardId),
    StateKvShardPrunerProgress(ShardId),
    StateMerkleShardRestoreProgress(ShardId, Version),
    TransactionAuxiliaryDataPrunerProgress,
    PersistedAuxiliaryInfoPrunerProgress,
}
```
