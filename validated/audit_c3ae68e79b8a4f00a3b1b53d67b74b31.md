# Audit Report

## Title
Version-Based Vote Splitting in Per-Key JWK Consensus Causes Liveness Failure

## Summary
Byzantine validators can exploit the shared version counter in per-key JWK consensus mode to split honest validator votes across multiple version numbers for the same key update, preventing any single version from reaching quorum. This occurs because completing one key's update advances the issuer's global version counter, invalidating concurrent consensus processes for other keys of the same issuer through an asynchronous event delivery mechanism.

## Finding Description

The JWK consensus system in per-key mode allows validators to reach consensus independently for each key of an OIDC provider. However, all keys of an issuer share a single version counter that creates a race condition during concurrent key updates.

**Vulnerability Mechanism:**

1. **Version Coupling**: The `ProviderJWKs` struct uses a shared `version` field for all keys of an issuer [1](#0-0) 

2. **Strict Version Matching in Vote Aggregation**: The `ProviderJWKs` struct derives `PartialEq` including the version field [2](#0-1) , and vote aggregation requires exact equality check between local and peer views [3](#0-2) 

3. **Base Version Assignment**: When validators observe a key change, they create a `KeyLevelUpdate` using the current on-chain version as `base_version` [4](#0-3) 

4. **Version Increment on Conversion**: The `KeyLevelUpdate` is converted to `ProviderJWKs` with `version = base_version + 1` [5](#0-4) 

5. **Move Contract Version Validation**: The Move contract validates that the proposed version equals the on-chain version plus one [6](#0-5) 

6. **Asynchronous State Reset**: When a key update commits on-chain, an `ObservedJWKsUpdated` event is emitted [7](#0-6)  and delivered asynchronously to validators through channels [8](#0-7) . Upon receiving this event, validators call `reset_with_on_chain_state` which discards in-progress consensus for any issuer whose version changed [9](#0-8) 

7. **Insufficient Consensus Restart Check**: The `maybe_start_consensus` function only checks if `to_upsert` values match, not whether `base_version` has changed [10](#0-9) 

**Attack Execution:**

1. OIDC provider updates keys K1, K2, K3 simultaneously; on-chain version is 10
2. All validators observe changes through periodic polling [11](#0-10)  and start consensus with `base_version=10` (target `version=11`)
3. Byzantine validators strategically participate heavily in K1 consensus to accelerate it reaching quorum
4. K1's quorum-certified update executes on-chain, advancing version from 10 to 11 [12](#0-11) 
5. The `ObservedJWKsUpdated` event is emitted and delivered asynchronously to validators
6. **Race Condition Window**: Validators receive the event at different times due to network delays and channel processing
7. Validators receiving the event first call `reset_with_on_chain_state`, which removes K2/K3's in-progress consensus states because the issuer's version changed
8. These validators then restart K2/K3 consensus with `base_version=11` (target `version=12`)
9. Validators who haven't processed the event yet continue voting for K2/K3 with `version=11`
10. Vote aggregation fails because validators with `version=11` reject votes for `version=12` and vice versa (exact match required)
11. Neither version reaches quorum threshold
12. Byzantine validators repeat this cascading pattern across multiple keys

## Impact Explanation

**Severity: High** - This constitutes a significant protocol violation under the Aptos bug bounty program.

**Impact:**
- **JWK Update Liveness Failure**: Legitimate JWK updates can be indefinitely prevented from reaching consensus during multi-key rotations, violating liveness guarantees for the JWK consensus subsystem
- **Keyless Account Denial of Service**: Users relying on keyless accounts with affected OIDC providers cannot authenticate if JWK updates are blocked
- **Consensus Resource Exhaustion**: Validators waste computational resources on consensus processes that repeatedly fail due to version mismatches
- **Validator Transaction Pool Pollution**: Failed validator transactions accumulate in the pool

This is classified as **High severity** rather than Critical because:
- It affects JWK consensus liveness, not total network liveness
- No fund loss, consensus safety violation, or double-spending occurs
- No permanent state corruption or chain splits result
- Falls under "Significant protocol violations" (High severity) in the bug bounty criteria

## Likelihood Explanation

**Likelihood: High**

- **Attacker Requirements**: Requires < 1/3 Byzantine validators, which is the standard security assumption for BFT systems and within the threat model
- **Execution Complexity**: Low - Byzantine validators only need to selectively time their vote participation; no special capabilities or privilege escalation required
- **Attack Window**: Persistent - exists whenever multiple keys are being updated for the same issuer, which is common during OIDC provider key rotations
- **Natural Triggering Conditions**: OIDC providers regularly perform key rotations affecting multiple keys simultaneously (e.g., Google, Facebook rotate multiple signing keys)
- **Network Amplification**: Natural network delays between validators already create timing variance; Byzantine validators only need to amplify this existing variance

The vulnerability is highly exploitable because:
1. Per-key consensus mode naturally creates concurrent consensus processes for multiple keys
2. Event delivery through async channels [13](#0-12)  combined with periodic observation polling [14](#0-13)  creates unavoidable race conditions
3. Byzantine validators can predict and exploit timing windows with minimal coordination

## Recommendation

**Immediate Fix**: Include `base_version` in the consensus restart check to prevent validators from continuing with stale version targets:

```rust
// In jwk_manager_per_key.rs, update maybe_start_consensus comparison:
let consensus_already_started = match self
    .states_by_key
    .get(&(update.issuer.clone(), update.kid.clone()))
    .cloned()
{
    Some(ConsensusState::InProgress { my_proposal, .. })
    | Some(ConsensusState::Finished { my_proposal, .. }) => {
        my_proposal.observed.to_upsert == update.to_upsert 
            && my_proposal.observed.base_version == update.base_version  // Add this check
    },
    _ => false,
};
```

**Long-term Fix**: Decouple version counters per-key rather than per-issuer, or implement a synchronization mechanism to ensure all validators receive version updates before processing new observations.

## Proof of Concept

The vulnerability can be demonstrated through integration testing by:

1. Deploy an OIDC provider with multiple keys (K1, K2, K3) at version 10
2. Configure validators to observe this provider
3. Update all three keys simultaneously at the OIDC provider
4. Have Byzantine validators (simulated with <1/3 stake) vote heavily on K1 to accelerate its quorum
5. Monitor the vote distribution for K2 and K3 - votes will split between version=11 and version=12
6. Observe that neither K2 nor K3 reaches quorum despite honest validators agreeing on the key content

The existing test infrastructure in `testsuite/smoke-test/src/jwks/jwk_consensus_per_key.rs` can be extended to reproduce this scenario by introducing controlled delays in event processing between validators.

## Notes

This vulnerability demonstrates a subtle interaction between:
- Synchronous on-chain state updates (Move contract execution)
- Asynchronous event delivery (channel-based notifications)
- Concurrent consensus processes (per-key independent sessions)

The issue is exacerbated in per-key mode specifically because it creates many concurrent consensus sessions that share a single version counter, amplifying the race condition window.

### Citations

**File:** types/src/jwks/mod.rs (L122-128)
```rust
#[derive(Clone, Default, Eq, PartialEq, Serialize, Deserialize, CryptoHasher, BCSCryptoHash)]
pub struct ProviderJWKs {
    #[serde(with = "serde_bytes")]
    pub issuer: Issuer,
    pub version: u64,
    pub jwks: Vec<JWKMoveStruct>,
}
```

**File:** types/src/jwks/mod.rs (L349-357)
```rust
        let version = self
            .base_version
            .checked_add(1)
            .context("KeyLevelUpdate::as_issuer_level_repr failed on version")?;
        Ok(ProviderJWKs {
            issuer: self.issuer.clone(),
            version,
            jwks: vec![JWKMoveStruct::from(jwk_repr)],
        })
```

**File:** crates/aptos-jwk-consensus/src/observation_aggregation/mod.rs (L81-84)
```rust
        ensure!(
            self.local_view == peer_view,
            "adding peer observation failed with mismatched view"
        );
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L138-143)
```rust
                        let update = KeyLevelUpdate {
                            issuer: issuer.clone(),
                            base_version: effectively_onchain.version,
                            kid: kid.clone(),
                            to_upsert: Some(y.clone()),
                        };
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L180-194)
```rust
        let consensus_already_started = match self
            .states_by_key
            .get(&(update.issuer.clone(), update.kid.clone()))
            .cloned()
        {
            Some(ConsensusState::InProgress { my_proposal, .. })
            | Some(ConsensusState::Finished { my_proposal, .. }) => {
                my_proposal.observed.to_upsert == update.to_upsert
            },
            _ => false,
        };

        if consensus_already_started {
            return Ok(());
        }
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L244-254)
```rust
        self.states_by_key.retain(|(issuer, _), _| {
            new_onchain_jwks
                .get(issuer)
                .map(|jwks| jwks.version)
                .unwrap_or_default()
                == self
                    .onchain_jwks
                    .get(issuer)
                    .map(|jwks| jwks.version)
                    .unwrap_or_default()
        });
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L417-420)
```rust
                jwk_updated = jwk_updated_rx.select_next_some() => {
                    let ObservedJWKsUpdated { jwks, .. } = jwk_updated;
                    this.reset_with_on_chain_state(jwks)
                },
```

**File:** aptos-move/framework/aptos-framework/sources/jwks.move (L478-478)
```text
                assert!(cur_issuer_jwks.version + 1 == proposed_provider_jwks.version, error::invalid_argument(EUNEXPECTED_VERSION));
```

**File:** aptos-move/framework/aptos-framework/sources/jwks.move (L493-493)
```text
                cur_issuer_jwks.version = cur_issuer_jwks.version + 1;
```

**File:** aptos-move/framework/aptos-framework/sources/jwks.move (L503-503)
```text
        emit(ObservedJWKsUpdated { epoch, jwks: observed_jwks.jwks });
```

**File:** crates/aptos-jwk-consensus/src/epoch_manager.rs (L113-118)
```rust
            if let Ok(jwk_event) = ObservedJWKsUpdated::try_from(&event) {
                if let Some(tx) = self.jwk_updated_event_txs.as_ref() {
                    let _ = tx.push((), jwk_event);
                }
            }
        }
```

**File:** crates/aptos-jwk-consensus/src/jwk_observer.rs (L59-80)
```rust
        let mut interval = tokio::time::interval(fetch_interval);
        interval.set_missed_tick_behavior(MissedTickBehavior::Delay);
        let mut close_rx = close_rx.into_stream();
        let my_addr = if cfg!(feature = "smoke-test") {
            // Include self validator address in JWK request,
            // so dummy OIDC providers in smoke tests can do things like "key A for validator 1, key B for validator 2".
            Some(my_addr)
        } else {
            None
        };

        loop {
            tokio::select! {
                _ = interval.tick().fuse() => {
                    let timer = Instant::now();
                    let result = fetch_jwks(open_id_config_url.as_str(), my_addr).await;
                    debug!(issuer = issuer, "observe_result={:?}", result);
                    let secs = timer.elapsed().as_secs_f64();
                    if let Ok(mut jwks) = result {
                        OBSERVATION_SECONDS.with_label_values(&[issuer.as_str(), "ok"]).observe(secs);
                        jwks.sort();
                        let _ = observation_tx.push((), (issuer.as_bytes().to_vec(), jwks));
```
