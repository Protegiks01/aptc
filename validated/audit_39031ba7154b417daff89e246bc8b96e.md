# Audit Report

## Title
Commit Lock Leak in BlockSTMv2 Worker Loop Causing Permanent Commit Starvation

## Summary
The `worker_loop_v2()` function in the BlockSTMv2 parallel executor contains a critical lock leak vulnerability where the commit lock acquired via `commit_hooks_try_lock()` is never released when error paths are triggered, permanently blocking all commit processing and forcing parallel execution failure.

## Finding Description

The vulnerability exists in the commit lock acquisition and release pattern within `worker_loop_v2()`. [1](#0-0) 

The fatal flaw occurs because:

1. A worker acquires the commit lock via `commit_hooks_try_lock()` at line 1455
2. While holding the lock, it processes commits in the inner while loop (line 1457)
3. If either `scheduler.start_commit()?` or `prepare_and_queue_commit_ready_txn(...)?` returns an error, the `?` operator causes immediate return from the function
4. The critical `scheduler.commit_hooks_unlock()` call at line 1471 is **never executed**
5. The lock remains permanently held

The `ArmedLock` implementation has no automatic cleanup mechanism: [2](#0-1) 

It is a simple `AtomicU64` without any RAII guard or Drop implementation. Once acquired (value becomes 0), it requires explicit `unlock()` to restore the unlocked state. Since `try_lock()` only succeeds when the value is exactly 3 (unlocked AND armed), a leaked lock permanently prevents all future lock acquisitions.

**Error paths that trigger the leak:**

1. **From start_commit()**: Returns `PanicError` on invariant violations like incorrect commit marker states or next_to_commit_idx inconsistencies. [3](#0-2) 

2. **From prepare_and_queue_commit_ready_txn()**: Can fail during delayed field validation, transaction re-execution, or module publishing. [4](#0-3) 

When validation fails, the function re-executes the transaction while holding the commit lock. If re-execution fails, the error propagates through the `?` operator, leaking the lock.

## Impact Explanation

**HIGH Severity** per Aptos bug bounty criteria: "Validator node slowdowns"

When the lock leak occurs, the worker thread exits with error and the scheduler is halted. [5](#0-4) 

The cascading effects include:
- All workers permanently blocked from processing commits
- Parallel execution fails and returns `Err(())`
- System falls back to sequential execution (if `allow_fallback` enabled) or validator panics (if disabled) [6](#0-5) 

Sequential execution is significantly slower than parallel execution, causing:
- Increased block processing times
- Reduced transaction throughput
- Validator performance degradation
- Potential validator penalties

This satisfies the HIGH severity criterion of "Validator node slowdowns" that significantly affect consensus participation.

## Likelihood Explanation

**Likelihood: Medium**

While the error paths use `code_invariant_error` suggesting these are rare internal invariants, the vulnerability represents a **logic flaw** in error handling - improper resource cleanup on error paths.

The existence of these error paths indicates the developers anticipated these conditions could occur, even if rarely. Any code path leading to `PanicError` during commit processing will trigger the leak, including:
- Delayed field validation failures requiring re-execution that then fails
- Scheduler state inconsistencies
- Module publishing errors

Even if triggering conditions are rare, the lock leak makes any such error unrecoverable, forcing expensive fallback to sequential execution or validator panic.

## Recommendation

Implement proper RAII-style lock guard or ensure unlock is called on all error paths. The recommended fix:

**Option 1**: Wrap the critical section in a closure or scope guard that ensures unlock is called:
```rust
loop {
    while scheduler.commit_hooks_try_lock() {
        let _guard = CommitLockGuard(&scheduler); // Custom RAII guard
        // Perform sequential commit hooks.
        while let Some((txn_idx, incarnation)) = scheduler.start_commit()? {
            self.prepare_and_queue_commit_ready_txn(...)?;
        }
        // Guard automatically unlocks on drop
    }
    // ... rest of loop
}
```

**Option 2**: Use explicit error handling with manual unlock:
```rust
loop {
    while scheduler.commit_hooks_try_lock() {
        let result = (|| {
            while let Some((txn_idx, incarnation)) = scheduler.start_commit()? {
                self.prepare_and_queue_commit_ready_txn(...)?;
            }
            Ok(())
        })();
        
        scheduler.commit_hooks_unlock();
        result?; // Propagate error AFTER unlock
    }
    // ... rest of loop
}
```

## Proof of Concept

While a complete PoC would require triggering the specific error conditions in `start_commit()` or `prepare_and_queue_commit_ready_txn()`, the vulnerability is evident from the code structure:

1. The lock is acquired at line 1455
2. Error-returning functions are called at lines 1457 and 1458-1468 with `?` operator
3. The unlock at line 1471 is only reached if no errors occur
4. Any error causes immediate return, leaking the lock

The logic vulnerability exists independent of the specific trigger conditions, making this a critical error handling bug that requires fixing.

### Citations

**File:** aptos-move/block-executor/src/executor.rs (L990-1067)
```rust
    fn prepare_and_queue_commit_ready_txn(
        &self,
        txn_idx: TxnIndex,
        incarnation: Incarnation,
        num_txns: TxnIndex,
        executor: &E,
        block: &TP,
        num_workers: usize,
        runtime_environment: &RuntimeEnvironment,
        scheduler: SchedulerWrapper,
        shared_sync_params: &SharedSyncParams<T, E, S>,
    ) -> Result<(), PanicOr<ParallelBlockExecutionError>> {
        let versioned_cache = shared_sync_params.versioned_cache;
        let last_input_output = shared_sync_params.last_input_output;
        let global_module_cache = shared_sync_params.global_module_cache;

        let block_limit_processor = &mut shared_sync_params.block_limit_processor.acquire();
        let mut side_effect_at_commit = false;

        if !Self::validate_and_commit_delayed_fields(
            txn_idx,
            versioned_cache,
            last_input_output,
            scheduler.is_v2(),
        )? {
            // Transaction needs to be re-executed, one final time.
            side_effect_at_commit = true;

            scheduler.abort_pre_final_reexecution::<T, E>(
                txn_idx,
                incarnation,
                last_input_output,
                versioned_cache,
            )?;

            Self::execute_txn_after_commit(
                block.get_txn(txn_idx),
                &block.get_auxiliary_info(txn_idx),
                txn_idx,
                incarnation + 1,
                scheduler,
                versioned_cache,
                last_input_output,
                shared_sync_params.start_shared_counter,
                shared_sync_params.delayed_field_id_counter,
                executor,
                shared_sync_params.base_view,
                global_module_cache,
                runtime_environment,
                &self.config.onchain.block_gas_limit_type,
            )?;
        }

        // Publish modules before we decrease validation index (in V1) so that validations observe
        // the new module writes as well.
        if last_input_output.publish_module_write_set(
            txn_idx,
            global_module_cache,
            versioned_cache,
            runtime_environment,
            &scheduler,
        )? {
            side_effect_at_commit = true;
        }

        if side_effect_at_commit {
            scheduler.wake_dependencies_and_decrease_validation_idx(txn_idx)?;
        }

        last_input_output.commit(
            txn_idx,
            num_txns,
            num_workers,
            block_limit_processor,
            shared_sync_params.maybe_block_epilogue_txn_idx,
            &scheduler,
        )
    }
```

**File:** aptos-move/block-executor/src/executor.rs (L1455-1471)
```rust
            while scheduler.commit_hooks_try_lock() {
                // Perform sequential commit hooks.
                while let Some((txn_idx, incarnation)) = scheduler.start_commit()? {
                    self.prepare_and_queue_commit_ready_txn(
                        txn_idx,
                        incarnation,
                        num_txns,
                        executor,
                        block,
                        num_workers as usize,
                        runtime_environment,
                        scheduler_wrapper,
                        shared_sync_params,
                    )?;
                }

                scheduler.commit_hooks_unlock();
```

**File:** aptos-move/block-executor/src/executor.rs (L1778-1799)
```rust
                    if let Err(err) = self.worker_loop_v2(
                        &executor,
                        signature_verified_block,
                        environment,
                        *worker_id,
                        num_workers,
                        &scheduler,
                        &shared_sync_params,
                    ) {
                        // If there are multiple errors, they all get logged: FatalVMError is
                        // logged at construction, below we log CodeInvariantErrors.
                        if let PanicOr::CodeInvariantError(err_msg) = err {
                            alert!(
                                "[BlockSTMv2] worker loop: CodeInvariantError({:?})",
                                err_msg
                            );
                        }
                        shared_maybe_error.store(true, Ordering::SeqCst);

                        // Make sure to halt the scheduler if it hasn't already been halted.
                        scheduler.halt();
                    }
```

**File:** aptos-move/block-executor/src/executor.rs (L2576-2597)
```rust
            // If parallel gave us result, return it
            if let Ok(output) = parallel_result {
                return Ok(output);
            }

            if !self.config.local.allow_fallback {
                panic!("Parallel execution failed and fallback is not allowed");
            }

            // All logs from the parallel execution should be cleared and not reported.
            // Clear by re-initializing the speculative logs.
            init_speculative_logs(signature_verified_block.num_txns() + 1);

            // Flush all caches to re-run from the "clean" state.
            module_cache_manager_guard
                .environment()
                .runtime_environment()
                .flush_all_caches();
            module_cache_manager_guard.module_cache_mut().flush();

            info!("parallel execution requiring fallback");
        }
```

**File:** aptos-move/block-executor/src/scheduler.rs (L24-51)
```rust
pub struct ArmedLock {
    // Last bit:   1 -> unlocked; 0 -> locked
    // Second bit: 1 -> there's work; 0 -> no work
    locked: AtomicU64,
}

impl ArmedLock {
    pub fn new() -> Self {
        Self {
            locked: AtomicU64::new(3),
        }
    }

    // try_lock succeeds when the lock is unlocked and armed (there is work to do).
    pub fn try_lock(&self) -> bool {
        self.locked
            .compare_exchange_weak(3, 0, Ordering::Acquire, Ordering::Relaxed)
            .is_ok()
    }

    pub fn unlock(&self) {
        self.locked.fetch_or(1, Ordering::Release);
    }

    pub fn arm(&self) {
        self.locked.fetch_or(2, Ordering::Release);
    }
}
```

**File:** aptos-move/block-executor/src/scheduler_v2.rs (L606-680)
```rust
    pub(crate) fn start_commit(&self) -> Result<Option<(TxnIndex, Incarnation)>, PanicError> {
        // Relaxed ordering due to armed lock acq-rel.
        let next_to_commit_idx = self.next_to_commit_idx.load(Ordering::Relaxed);
        assert!(next_to_commit_idx <= self.num_txns);

        if self.is_halted() || next_to_commit_idx == self.num_txns {
            // All sequential commit hooks are already dispatched.
            return Ok(None);
        }

        let incarnation = self.txn_statuses.incarnation(next_to_commit_idx);
        if self.txn_statuses.is_executed(next_to_commit_idx) {
            self.commit_marker_invariant_check(next_to_commit_idx)?;

            // All prior transactions are committed and the latest incarnation of the transaction
            // at next_to_commit_idx has finished but has not been aborted. If any of its reads was
            // incorrect, it would have been invalidated by the respective transaction's last
            // (committed) (re-)execution, and led to an abort in the corresponding finish execution
            // (which, inductively, must occur before the transaction is committed). Hence, it
            // must also be safe to commit the current transaction.
            //
            // The only exception is if there are unsatisfied cold validation requirements,
            // blocking the commit. These may not yet be scheduled for validation, or deferred
            // until after the txn finished execution, whereby deferral happens before txn status
            // becomes Executed, while validation and unblocking happens after.
            if self
                .cold_validation_requirements
                .is_commit_blocked(next_to_commit_idx, incarnation)
            {
                // May not commit a txn with an unsatisfied validation requirement. This will be
                // more rare than !is_executed in the common case, hence the order of checks.
                return Ok(None);
            }
            // The check might have passed after the validation requirement has been fulfilled.
            // Yet, if validation failed, the status would be aborted before removing the block,
            // which would increase the incarnation number. It is also important to note that
            // blocking happens during sequential commit hook, while holding the lock (which is
            // also held here), hence before the call of this method.
            if incarnation != self.txn_statuses.incarnation(next_to_commit_idx) {
                return Ok(None);
            }

            if self
                .committed_marker
                .get(next_to_commit_idx as usize)
                .is_some_and(|marker| {
                    marker.swap(CommitMarkerFlag::CommitStarted as u8, Ordering::Relaxed)
                        != CommitMarkerFlag::NotCommitted as u8
                })
            {
                return Err(code_invariant_error(format!(
                    "Marking {} as PENDING_COMMIT_HOOK, but previous marker != NOT_COMMITTED",
                    next_to_commit_idx
                )));
            }

            // TODO(BlockSTMv2): fetch_add as a RMW instruction causes a barrier even with
            // Relaxed ordering. The read is only used to check an invariant, so we can
            // eventually change to just a relaxed write.
            let prev_idx = self.next_to_commit_idx.fetch_add(1, Ordering::Relaxed);
            if prev_idx != next_to_commit_idx {
                return Err(code_invariant_error(format!(
                    "Scheduler committing {}, stored next to commit idx = {}",
                    next_to_commit_idx, prev_idx
                )));
            }

            return Ok(Some((
                next_to_commit_idx,
                self.txn_statuses.incarnation(next_to_commit_idx),
            )));
        }

        Ok(None)
    }
```
