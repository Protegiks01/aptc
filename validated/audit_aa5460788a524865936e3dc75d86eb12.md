# Audit Report

## Title
Unbounded Iterator in StateKvShardPruner Causes Memory Exhaustion During Node Initialization

## Summary
The `StateKvShardPruner::prune()` function lacks batching controls and accumulates all deletion operations in a single unbounded `SchemaBatch` during shard catch-up. When a shard falls behind and the node restarts, the initialization process attempts to process potentially millions of stale entries at once, leading to excessive memory consumption and node crashes.

## Finding Description

The vulnerability exists in the `prune()` function which is called during shard pruner initialization to catch up with the metadata pruner's progress. The function creates a single `SchemaBatch` and iterates through all stale state value indices from `current_progress` to `target_version` without any batching mechanism: [1](#0-0) 

Each stale entry requires two deletions (index and value), and all operations accumulate in memory before being written to the database in a single atomic operation.

During node initialization, `StateKvShardPruner::new()` calls this function to catch up with the metadata pruner's progress: [2](#0-1) 

The parent `StateKvPruner` initializes all shard pruners sequentially during node startup: [3](#0-2) 

The `SchemaBatch` structure has no inherent memory limits - it's simply a HashMap that accumulates operations: [4](#0-3) 

In stark contrast, the similar `StateMerkleShardPruner` implements proper internal batching with a loop that processes entries incrementally: [5](#0-4) 

The batching mechanism calls `get_stale_node_indices()` with a `max_nodes_to_prune` limit that restricts the number of entries processed per iteration: [6](#0-5) 

**Attack Scenario:**

1. An attacker sends high-throughput transactions that update state frequently (within gas limits). According to configuration comments, a 10k transaction block can touch 60k state values: [7](#0-6) 

2. Stale entries accumulate across thousands of versions (e.g., 10,000 versions × 60k entries = 600 million stale entries)

3. Due to operational issues (crash, disk slowdown, restart), one or more shards fall behind the metadata pruner

4. On node restart, each shard attempts to catch up by processing all accumulated stale entries at once through the unbounded iteration

5. Memory consumption: 600M entries × ~48 bytes per key = ~28.8 GB just for deletion keys, plus batch overhead

6. Node runs out of memory and crashes, creating a persistent DoS condition where the node cannot restart without manual database intervention

## Impact Explanation

**Severity: Medium**

This vulnerability causes **validator node unavailability** requiring manual intervention, which aligns with the Medium severity category: "State inconsistencies requiring intervention." Specifically:

- **Validator node unavailability**: Affected nodes cannot start up, requiring manual database intervention or restoration from backup
- **Liveness impact**: Individual validator nodes become unavailable, though network consensus can continue with remaining validators (no total network liveness loss)
- **DoS amplification vector**: Attackers can amplify normal operational issues into complete node failure by maximizing state churn
- **Recovery complexity**: Requires manual database intervention, partial database restoration, or full node resync

The issue does NOT directly lead to:
- Consensus safety violations (other nodes continue operating correctly)
- Loss of funds or state corruption  
- Network-wide partition (only affects individual nodes that restart)
- Critical network availability issues (network continues with remaining validators)

This matches Medium severity rather than High because it affects individual nodes during specific operational conditions, not the broader network consensus or availability.

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability occurs under specific but realistic production conditions:

**Natural occurrence scenarios:**
- Shard disk slowdown causing pruner lag
- Node crash during pruning operations  
- Resource exhaustion on specific shard storage
- Node restarts for maintenance or updates

**Attacker-amplified scenarios:**
- Sending high-throughput transactions to maximize state value churn (limited by gas/fees but economically feasible)
- Timing attacks during known maintenance windows when restarts are likely

**Frequency factors:**
- Storage sharding is enabled by default: [8](#0-7) 

- Node restarts are common in production environments (updates, maintenance, crashes)
- The gap only needs to be ~10,000 versions with normal transaction traffic to cause memory issues
- Default ledger prune window is 90 million versions, providing ample opportunity for gaps to accumulate: [9](#0-8) 

While an attacker cannot directly force shards to fall behind, they can maximize state churn and wait for natural operational events to trigger the vulnerability.

## Recommendation

Implement batching in `StateKvShardPruner::prune()` following the pattern used in `StateMerkleShardPruner`. The fix should:

1. Add a loop that processes entries in batches
2. Create a new `SchemaBatch` for each iteration
3. Track progress incrementally
4. Only update the final progress marker when fully caught up

Example fix pattern (following StateMerkleShardPruner):

```rust
pub(in crate::pruner) fn prune(
    &self,
    current_progress: Version,
    target_version: Version,
    max_entries_per_batch: usize,
) -> Result<()> {
    let mut progress = current_progress;
    
    loop {
        let mut batch = SchemaBatch::new();
        let mut count = 0;
        
        let mut iter = self
            .db_shard
            .iter::<StaleStateValueIndexByKeyHashSchema>()?;
        iter.seek(&progress)?;
        
        let mut done = true;
        for item in iter {
            let (index, _) = item?;
            if index.stale_since_version > target_version {
                break;
            }
            
            batch.delete::<StaleStateValueIndexByKeyHashSchema>(&index)?;
            batch.delete::<StateValueByKeyHashSchema>(&(index.state_key_hash, index.version))?;
            
            progress = index.stale_since_version;
            count += 1;
            
            if count >= max_entries_per_batch {
                done = false;
                break;
            }
        }
        
        if done {
            batch.put::<DbMetadataSchema>(
                &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
                &DbMetadataValue::Version(target_version),
            )?;
        }
        
        self.db_shard.write_schemas(batch)?;
        
        if done {
            break;
        }
    }
    
    Ok(())
}
```

The batch size should be configurable, with a reasonable default (e.g., 10,000 entries per batch, similar to StateMerklePruner's default of 1,000 nodes).

## Proof of Concept

While a full PoC would require setting up a complete node environment with sharded storage, the vulnerability can be demonstrated by examining the code paths:

1. Create a scenario where a shard falls behind by 10,000+ versions with high state churn
2. Restart the node to trigger `StateKvShardPruner::new()`
3. Monitor memory consumption during the catch-up phase
4. Observe unbounded memory growth as all stale entries are loaded into a single `SchemaBatch`

The vulnerability is evident from the code structure comparison - `StateKvShardPruner::prune()` has no loop or batching mechanism, while `StateMerkleShardPruner::prune()` implements proper batching, demonstrating that the Aptos team is aware of the need for batching in similar scenarios but has not applied it consistently.

## Notes

This is a defense-in-depth issue where the vulnerability exists in the code logic (missing batching) but requires specific operational conditions to manifest. The fix is straightforward and follows an established pattern already present in the codebase for similar pruning operations. The impact is limited to individual node availability rather than network-wide consensus or safety, which is why Medium severity is appropriate.

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L25-45)
```rust
    pub(in crate::pruner) fn new(
        shard_id: usize,
        db_shard: Arc<DB>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
            metadata_progress,
        )?;
        let myself = Self { shard_id, db_shard };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up state kv shard {shard_id}."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L47-72)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();

        let mut iter = self
            .db_shard
            .iter::<StaleStateValueIndexByKeyHashSchema>()?;
        iter.seek(&current_progress)?;
        for item in iter {
            let (index, _) = item?;
            if index.stale_since_version > target_version {
                break;
            }
            batch.delete::<StaleStateValueIndexByKeyHashSchema>(&index)?;
            batch.delete::<StateValueByKeyHashSchema>(&(index.state_key_hash, index.version))?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
            &DbMetadataValue::Version(target_version),
        )?;

        self.db_shard.write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L112-153)
```rust
    pub fn new(state_kv_db: Arc<StateKvDb>) -> Result<Self> {
        info!(name = STATE_KV_PRUNER_NAME, "Initializing...");

        let metadata_pruner = StateKvMetadataPruner::new(Arc::clone(&state_kv_db));

        let metadata_progress = metadata_pruner.progress()?;

        info!(
            metadata_progress = metadata_progress,
            "Created state kv metadata pruner, start catching up all shards."
        );

        let shard_pruners = if state_kv_db.enabled_sharding() {
            let num_shards = state_kv_db.num_shards();
            let mut shard_pruners = Vec::with_capacity(num_shards);
            for shard_id in 0..num_shards {
                shard_pruners.push(StateKvShardPruner::new(
                    shard_id,
                    state_kv_db.db_shard_arc(shard_id),
                    metadata_progress,
                )?);
            }
            shard_pruners
        } else {
            Vec::new()
        };

        let pruner = StateKvPruner {
            target_version: AtomicVersion::new(metadata_progress),
            progress: AtomicVersion::new(metadata_progress),
            metadata_pruner,
            shard_pruners,
        };

        info!(
            name = pruner.name(),
            progress = metadata_progress,
            "Initialized."
        );

        Ok(pruner)
    }
```

**File:** storage/schemadb/src/batch.rs (L127-149)
```rust
/// `SchemaBatch` holds a collection of updates that can be applied to a DB atomically. The updates
/// will be applied in the order in which they are added to the `SchemaBatch`.
#[derive(Debug, Default)]
pub struct SchemaBatch {
    rows: DropHelper<HashMap<ColumnFamilyName, Vec<WriteOp>>>,
    stats: SampledBatchStats,
}

impl SchemaBatch {
    /// Creates an empty batch.
    pub fn new() -> Self {
        Self::default()
    }

    /// keep these on the struct itself so that we don't need to update each call site.
    pub fn put<S: Schema>(&mut self, key: &S::Key, value: &S::Value) -> DbResult<()> {
        <Self as WriteBatch>::put::<S>(self, key, value)
    }

    pub fn delete<S: Schema>(&mut self, key: &S::Key) -> DbResult<()> {
        <Self as WriteBatch>::delete::<S>(self, key)
    }
}
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L58-100)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
        max_nodes_to_prune: usize,
    ) -> Result<()> {
        loop {
            let mut batch = SchemaBatch::new();
            let (indices, next_version) = StateMerklePruner::get_stale_node_indices(
                &self.db_shard,
                current_progress,
                target_version,
                max_nodes_to_prune,
            )?;

            indices.into_iter().try_for_each(|index| {
                batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
                batch.delete::<S>(&index)
            })?;

            let mut done = true;
            if let Some(next_version) = next_version {
                if next_version <= target_version {
                    done = false;
                }
            }

            if done {
                batch.put::<DbMetadataSchema>(
                    &S::progress_metadata_key(Some(self.shard_id)),
                    &DbMetadataValue::Version(target_version),
                )?;
            }

            self.db_shard.write_schemas(batch)?;

            if done {
                break;
            }
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L191-217)
```rust
    pub(in crate::pruner::state_merkle_pruner) fn get_stale_node_indices(
        state_merkle_db_shard: &DB,
        start_version: Version,
        target_version: Version,
        limit: usize,
    ) -> Result<(Vec<StaleNodeIndex>, Option<Version>)> {
        let mut indices = Vec::new();
        let mut iter = state_merkle_db_shard.iter::<S>()?;
        iter.seek(&StaleNodeIndex {
            stale_since_version: start_version,
            node_key: NodeKey::new_empty_path(0),
        })?;

        let mut next_version = None;
        while indices.len() < limit {
            if let Some((index, _)) = iter.next().transpose()? {
                next_version = Some(index.stale_since_version);
                if index.stale_since_version <= target_version {
                    indices.push(index);
                    continue;
                }
            }
            break;
        }

        Ok((indices, next_version))
    }
```

**File:** config/src/config/storage_config.rs (L233-233)
```rust
            enable_storage_sharding: true,
```

**File:** config/src/config/storage_config.rs (L387-395)
```rust
impl Default for LedgerPrunerConfig {
    fn default() -> Self {
        LedgerPrunerConfig {
            enable: true,
            prune_window: 90_000_000,
            batch_size: 5_000,
            user_pruning_window_offset: 200_000,
        }
    }
```

**File:** config/src/config/storage_config.rs (L408-410)
```rust
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
```
