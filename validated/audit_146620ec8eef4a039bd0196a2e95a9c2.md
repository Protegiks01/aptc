# Audit Report

## Title
Consensus Observer Execution Pipeline Deadlock via Premature Payload Removal During State Sync

## Summary
The consensus observer can enter a permanent deadlock when commit decisions for future rounds/epochs arrive while blocks are in the execution pipeline. The code removes payloads without aborting in-flight execution tasks, causing them to fail payload lookups and enter an infinite retry loop, permanently blocking the execution pipeline.

## Finding Description
The consensus observer maintains three stores for managing block data: `block_payload_store`, `ordered_block_store`, and `pending_block_store`. When blocks are finalized for execution via `finalize_ordered_block`, they enter an asynchronous execution pipeline that builds pipeline futures and sends blocks to the execution client. [1](#0-0) 

The critical vulnerability occurs in `process_commit_decision_message` when processing commit decisions for future epochs or higher rounds. When the commit is for a future epoch or higher round than the last ordered block, the code calls `update_blocks_for_state_sync_commit` which aggressively removes all payloads for rounds ≤ commit_round. [2](#0-1) 

This payload removal is permanent and implemented via `remove_blocks_for_epoch_round` which uses `split_off` to delete entries from the BTreeMap. [3](#0-2) [4](#0-3) 

**The critical flaw**: The execution pipeline is never aborted when payloads are removed during state sync commit processing. In contrast, when entering fallback mode, the code properly resets the pipeline by calling `clear_pending_block_state()` which invokes `execution_client.reset()`. [5](#0-4) [6](#0-5) 

The `ConsensusObserverPayloadManager` shares the same `Arc<Mutex<BTreeMap>>` with `BlockPayloadStore`, enabling direct access to the payload storage. [7](#0-6) [8](#0-7) 

When blocks in the pipeline reach the materialize phase after payloads are removed, `get_transactions_for_observer` fails with an `InternalError` when the payload is missing from the BTreeMap. [9](#0-8) 

The materialize function in the execution pipeline catches this error and enters an **infinite retry loop** with 100ms sleep intervals. The comment explicitly states "the loop can only be abort by the caller". [10](#0-9) 

The execution chain is: `materialize` → `preparer.materialize_block` → `payload_manager.get_transactions` → `get_transactions_for_observer`. [11](#0-10) 

Since the payload has been permanently removed from the store via `split_off`, retries fail indefinitely, blocking the entire execution pipeline with no recovery mechanism.

## Impact Explanation
**Critical Severity** - This vulnerability causes **total loss of liveness** for consensus observer nodes, meeting the Aptos Bug Bounty Critical severity criteria for "Total Loss of Liveness/Network Availability":

- The execution pipeline enters permanent deadlock with infinite 100ms retry loops attempting to materialize blocks
- No new blocks can be processed while the pipeline is stuck on the failing block
- The node effectively stops participating in consensus observation
- Recovery requires manual intervention or node restart
- If multiple observers are affected simultaneously, network observation capability is severely degraded

This directly aligns with Critical Impact Category #4: "Network halts due to protocol bug / All validators unable to progress". While consensus observers are not validators, they are critical infrastructure components, and their complete liveness failure represents a critical system vulnerability.

## Likelihood Explanation
**HIGH LIKELIHOOD** - This can occur during normal network operations without any malicious intent:

**Triggering Conditions**:
1. Node receives `OrderedBlock` and begins processing it via `finalize_ordered_block`
2. `BlockPayload` messages arrive and are stored in the payload store
3. Block enters the asynchronous execution pipeline, which will eventually call materialize
4. Before the materialize phase completes, a `CommitDecision` for a higher round/epoch arrives from a faster peer
5. `process_commit_decision_message` triggers `update_blocks_for_state_sync_commit`, which removes payloads without aborting the pipeline

**Realistic Scenarios**:
- Network latency causing nodes to receive commits from faster peers while still processing older blocks
- Nodes rejoining the network or temporarily falling behind during normal operations
- Epoch transitions where new epoch commits arrive during old epoch block execution
- Any scenario where the asynchronous pipeline processing is slower than commit decision propagation

**No Special Privileges Required**: Any network peer can send `CommitDecision` messages. The TODO comment explicitly acknowledges that future epoch commit verification is incomplete, making this exploitable during normal network synchronization. [12](#0-11) 

## Recommendation
Implement pipeline abortion when removing payloads during state sync commit processing, similar to the fallback mode implementation:

In `process_commit_decision_message`, before or after calling `update_blocks_for_state_sync_commit`, add:
```rust
// Reset the execution pipeline to prevent in-flight blocks from deadlocking
self.clear_pending_block_state().await;
```

Alternatively, implement selective payload removal that preserves payloads for in-flight blocks, or add timeout/cancellation mechanisms to the materialize retry loop to detect permanently missing payloads.

## Proof of Concept
A complete PoC would require setting up a multi-node consensus observer environment with controlled network delays. The scenario:

1. Start consensus observer node
2. Send `OrderedBlock` message for round N
3. Send `BlockPayload` for round N (stored in payload_store)
4. Block enters execution pipeline (asynchronous)
5. Before materialize completes, send `CommitDecision` for round N+10
6. Observe `update_blocks_for_state_sync_commit` removing payload for round N
7. Monitor logs for infinite "failed to prepare block, retrying" messages
8. Confirm node stops processing new blocks (permanent deadlock)

The vulnerability can be triggered through timing manipulation in network message delivery or by introducing artificial delays in the execution pipeline to create the race condition.

## Notes
This is a race condition vulnerability that becomes more likely under:
- High network latency environments
- Heavy node load causing slow pipeline processing
- Epoch transitions (increased commit decision traffic)
- Nodes catching up after brief disconnections

The root cause is the architectural mismatch between:
1. Synchronous payload removal (immediate `split_off`)
2. Asynchronous pipeline execution (in-flight futures)
3. Lack of coordination between these two systems

The fallback mode implementation demonstrates awareness of the need to reset pipelines when clearing state, but this pattern was not applied to the state sync commit path.

### Citations

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L218-230)
```rust
    async fn clear_pending_block_state(&self) {
        // Clear the observer block data
        let root = self.observer_block_data.lock().clear_block_data();

        // Reset the execution pipeline for the root
        if let Err(error) = self.execution_client.reset(&root).await {
            error!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Failed to reset the execution pipeline for the root! Error: {:?}",
                    error
                ))
            );
        }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L236-246)
```rust
    /// Enters fallback mode for consensus observer by invoking state sync
    async fn enter_fallback_mode(&mut self) {
        // Terminate all active subscriptions (to ensure we don't process any more messages)
        self.subscription_manager.terminate_all_subscriptions();

        // Clear all the pending block state
        self.clear_pending_block_state().await;

        // Start syncing for the fallback
        self.state_sync_manager.sync_for_fallback();
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L249-302)
```rust
    async fn finalize_ordered_block(&mut self, ordered_block: OrderedBlock) {
        info!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "Forwarding ordered blocks to the execution pipeline: {}",
                ordered_block.proof_block_info()
            ))
        );

        let block = ordered_block.first_block();
        let get_parent_pipeline_futs = self
            .observer_block_data
            .lock()
            .get_parent_pipeline_futs(&block, self.pipeline_builder());

        let mut parent_fut = if let Some(futs) = get_parent_pipeline_futs {
            Some(futs)
        } else {
            warn!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Parent block's pipeline futures for ordered block is missing! Ignoring: {:?}",
                    ordered_block.proof_block_info()
                ))
            );
            return;
        };

        for block in ordered_block.blocks() {
            let commit_callback =
                block_data::create_commit_callback(self.observer_block_data.clone());
            self.pipeline_builder().build_for_observer(
                block,
                parent_fut.take().expect("future should be set"),
                commit_callback,
            );
            parent_fut = Some(block.pipeline_futs().expect("pipeline futures just built"));
        }

        // Send the ordered block to the execution pipeline
        if let Err(error) = self
            .execution_client
            .finalize_order(
                ordered_block.blocks().clone(),
                WrappedLedgerInfo::new(VoteData::dummy(), ordered_block.ordered_proof().clone()),
            )
            .await
        {
            error!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Failed to finalize ordered block! Error: {:?}",
                    error
                ))
            );
        }
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L497-498)
```rust
        // TODO: identify the best way to handle an invalid commit decision
        // for a future epoch. In such cases, we currently rely on state sync.
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L518-526)
```rust
            // Otherwise, we should start the state sync process for the commit.
            // Update the block data (to the commit decision).
            self.observer_block_data
                .lock()
                .update_blocks_for_state_sync_commit(&commit_decision);

            // Start state syncing to the commit decision
            self.state_sync_manager
                .sync_to_commit(commit_decision, epoch_changed);
```

**File:** consensus/src/consensus_observer/observer/block_data.rs (L275-291)
```rust
    pub fn update_blocks_for_state_sync_commit(&mut self, commit_decision: &CommitDecision) {
        // Get the commit proof, epoch and round
        let commit_proof = commit_decision.commit_proof();
        let commit_epoch = commit_decision.epoch();
        let commit_round = commit_decision.round();

        // Update the root
        self.update_root(commit_proof.clone());

        // Update the block payload store
        self.block_payload_store
            .remove_blocks_for_epoch_round(commit_epoch, commit_round);

        // Update the ordered block store
        self.ordered_block_store
            .remove_blocks_for_commit(commit_proof);
    }
```

**File:** consensus/src/consensus_observer/observer/payload_store.rs (L33-44)
```rust
    // Block transaction payloads (indexed by epoch and round).
    // This is directly accessed by the payload manager.
    block_payloads: Arc<Mutex<BTreeMap<(u64, Round), BlockPayloadStatus>>>,
}

impl BlockPayloadStore {
    pub fn new(consensus_observer_config: ConsensusObserverConfig) -> Self {
        Self {
            consensus_observer_config,
            block_payloads: Arc::new(Mutex::new(BTreeMap::new())),
        }
    }
```

**File:** consensus/src/consensus_observer/observer/payload_store.rs (L112-119)
```rust
    pub fn remove_blocks_for_epoch_round(&self, epoch: u64, round: Round) {
        // Determine the round to split off
        let split_off_round = round.saturating_add(1);

        // Remove the blocks from the payload store
        let mut block_payloads = self.block_payloads.lock();
        *block_payloads = block_payloads.split_off(&(epoch, split_off_round));
    }
```

**File:** consensus/src/payload_manager/co_payload_manager.rs (L29-76)
```rust
async fn get_transactions_for_observer(
    block: &Block,
    block_payloads: &Arc<Mutex<BTreeMap<(u64, Round), BlockPayloadStatus>>>,
    consensus_publisher: &Option<Arc<ConsensusPublisher>>,
) -> ExecutorResult<(Vec<SignedTransaction>, Option<u64>, Option<u64>)> {
    // The data should already be available (as consensus observer will only ever
    // forward a block to the executor once the data has been received and verified).
    let block_payload = match block_payloads.lock().entry((block.epoch(), block.round())) {
        Entry::Occupied(mut value) => match value.get_mut() {
            BlockPayloadStatus::AvailableAndVerified(block_payload) => block_payload.clone(),
            BlockPayloadStatus::AvailableAndUnverified(_) => {
                // This shouldn't happen (the payload should already be verified)
                let error = format!(
                    "Payload data for block epoch {}, round {} is unverified!",
                    block.epoch(),
                    block.round()
                );
                return Err(InternalError { error });
            },
        },
        Entry::Vacant(_) => {
            // This shouldn't happen (the payload should already be present)
            let error = format!(
                "Missing payload data for block epoch {}, round {}!",
                block.epoch(),
                block.round()
            );
            return Err(InternalError { error });
        },
    };

    // If the payload is valid, publish it to any downstream observers
    let transaction_payload = block_payload.transaction_payload();
    if let Some(consensus_publisher) = consensus_publisher {
        let message = ConsensusObserverMessage::new_block_payload_message(
            block.gen_block_info(HashValue::zero(), 0, None),
            transaction_payload.clone(),
        );
        consensus_publisher.publish_message(message);
    }

    // Return the transactions and the transaction limit
    Ok((
        transaction_payload.transactions(),
        transaction_payload.transaction_limit(),
        transaction_payload.gas_limit(),
    ))
}
```

**File:** consensus/src/payload_manager/co_payload_manager.rs (L78-92)
```rust
pub struct ConsensusObserverPayloadManager {
    txns_pool: Arc<Mutex<BTreeMap<(u64, Round), BlockPayloadStatus>>>,
    consensus_publisher: Option<Arc<ConsensusPublisher>>,
}

impl ConsensusObserverPayloadManager {
    pub fn new(
        txns_pool: Arc<Mutex<BTreeMap<(u64, Round), BlockPayloadStatus>>>,
        consensus_publisher: Option<Arc<ConsensusPublisher>>,
    ) -> Self {
        Self {
            txns_pool,
            consensus_publisher,
        }
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L615-648)
```rust
    async fn materialize(
        preparer: Arc<BlockPreparer>,
        block: Arc<Block>,
        qc_rx: oneshot::Receiver<Arc<QuorumCert>>,
    ) -> TaskResult<MaterializeResult> {
        let mut tracker = Tracker::start_waiting("materialize", &block);
        tracker.start_working();

        let qc_rx = async {
            match qc_rx.await {
                Ok(qc) => Some(qc),
                Err(_) => {
                    warn!("[BlockPreparer] qc tx cancelled for block {}", block.id());
                    None
                },
            }
        }
        .shared();
        // the loop can only be abort by the caller
        let result = loop {
            match preparer.materialize_block(&block, qc_rx.clone()).await {
                Ok(input_txns) => break input_txns,
                Err(e) => {
                    warn!(
                        "[BlockPreparer] failed to prepare block {}, retrying: {}",
                        block.id(),
                        e
                    );
                    tokio::time::sleep(Duration::from_millis(100)).await;
                },
            }
        };
        Ok(result)
    }
```

**File:** consensus/src/block_preparer.rs (L42-69)
```rust
    pub async fn materialize_block(
        &self,
        block: &Block,
        block_qc_fut: Shared<impl Future<Output = Option<Arc<QuorumCert>>>>,
    ) -> ExecutorResult<(Vec<SignedTransaction>, Option<u64>, Option<u64>)> {
        fail_point!("consensus::prepare_block", |_| {
            use aptos_executor_types::ExecutorError;
            use std::{thread, time::Duration};
            thread::sleep(Duration::from_millis(10));
            Err(ExecutorError::CouldNotGetData)
        });
        //TODO(ibalajiarun): measure latency
        let (txns, max_txns_from_block_to_execute, block_gas_limit) = tokio::select! {
                // Poll the block qc future until a QC is received. Ignore None outcomes.
                Some(qc) = block_qc_fut => {
                    let block_voters = Some(qc.ledger_info().get_voters_bitvec().clone());
                    self.payload_manager.get_transactions(block, block_voters).await
                },
                result = self.payload_manager.get_transactions(block, None) => {
                   result
                }
        }?;
        TXNS_IN_BLOCK
            .with_label_values(&["before_filter"])
            .observe(txns.len() as f64);

        Ok((txns, max_txns_from_block_to_execute, block_gas_limit))
    }
```
