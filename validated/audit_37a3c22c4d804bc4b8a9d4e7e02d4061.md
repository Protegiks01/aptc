# Audit Report

## Title
Consensus Observer Pending Block Loss Due to Incorrect Split-Based Removal Logic

## Summary
The `remove_ready_block()` function in the consensus observer's pending block store uses a BTreeMap split operation that incorrectly drops valid pending blocks when payloads arrive out-of-order. The function assumes all blocks with first_round less than a received payload's round are "out-of-date," but this assumption fails when payloads arrive for rounds that don't correspond to any pending block's first round, causing legitimate blocks to be permanently lost from the pending store.

## Finding Description

The consensus observer maintains pending blocks (ordered blocks awaiting payloads) in a BTreeMap keyed by `(epoch, first_block_round)`. [1](#0-0) 

Each OrderedBlock can contain multiple pipelined blocks at different rounds. [2](#0-1) 

When a block payload arrives, the system calls `order_ready_pending_block()` which invokes `remove_ready_pending_block()` to check if any pending blocks are now ready. [3](#0-2)  This delegates to `remove_ready_block()` in the PendingBlockStore. [4](#0-3) 

The critical vulnerability occurs in the split-based removal logic:

1. **Split Operation**: The function calculates `split_round = received_payload_round + 1` and splits the BTreeMap at `(received_payload_epoch, split_round)`. [5](#0-4) 

2. **Last Block Check**: It pops the last (highest key) block from the lower half and checks if all its payloads exist. [6](#0-5) 

3. **Conditional Re-insertion**: If payloads don't exist, it only re-inserts the block if `last_pending_block_round > received_payload_round`. [7](#0-6) 

4. **Mass Deletion**: All remaining blocks in the lower half are unconditionally dropped as "out-of-date". [8](#0-7) 

5. **Store Rebuild**: The store is cleared and rebuilt with only higher round blocks. [9](#0-8) 

**Vulnerability Scenario**:

Assume pending blocks:
- Block A: key `(10, 100)`, contains pipelined blocks at rounds `[100, 101, 102]`
- Block B: key `(10, 110)`, contains pipelined blocks at rounds `[110, 111, 112]`

When a payload for round `105` arrives (from an ordered block not yet received):
1. `remove_ready_block(10, 105)` is called
2. Split at `(10, 106)`: Block A remains in lower half, Block B moves to higher half
3. `pop_last()` retrieves Block A
4. Block A's payloads (100, 101, 102) don't all exist
5. Block A's last round (102) is NOT > 105
6. Block A is NOT re-inserted into higher rounds
7. Block A is permanently dropped
8. When payloads for rounds 100-102 arrive later, Block A is gone

The function incorrectly assumes that if a payload for round R arrives, all pending blocks with first_round < R+1 are outdated. This breaks when:
- Payloads arrive out of order due to network delays
- A payload arrives for a round that doesn't match any pending block
- Ordered blocks have gaps in their first_round values (intentionally created in tests) [10](#0-9) 

The test suite confirms this behavior is intentional but flawed. The `test_remove_ready_block_singular_blocks_missing` test explicitly verifies that when attempting to remove the third block, the first three blocks are all removed. [11](#0-10) 

## Impact Explanation

**Severity: HIGH**

This vulnerability causes significant degradation of consensus observer functionality:

1. **Valid Block Loss**: Legitimate pending blocks are irreversibly removed from the store based on incorrect "out-of-date" assumptions.

2. **Observer Desynchronization**: The observer loses blocks and fails to maintain correct chain state, violating the observer protocol's correctness guarantees.

3. **Liveness Degradation**: Once blocks are lost, the observer cannot progress normally. The system must detect the stall and invoke the fallback mechanism. [12](#0-11) 

4. **Service Interruption**: The fallback to state sync provides recovery but causes temporary service degradation and increased resource usage. This affects all consensus observer nodes, particularly validator fullnodes (VFNs) that run observers by default.

Per Aptos bug bounty criteria, this qualifies as **High Severity** because:
- It causes significant protocol violations (observer nodes failing to maintain consensus state correctly)
- It requires intervention through fallback mechanisms
- It creates state inconsistencies between observers and validators
- It affects production infrastructure (enabled by default on VFNs)

While not directly causing fund loss or affecting core validator consensus, it undermines the reliability of the consensus observer system, which is critical for fullnode synchronization and network monitoring.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

This vulnerability can be triggered under realistic conditions:

1. **Natural Network Reordering**: P2P networks inherently have message reordering. Block payloads are transmitted independently from ordered block messages and can arrive in any order due to:
   - Network latency variations between peers
   - Different routing paths
   - Packet loss and retransmission
   - Peer connection quality differences

2. **No Ordering Validation**: The payload processing path validates signatures and digests but does NOT verify that the payload's round corresponds to any existing pending block. [13](#0-12)  Payloads ahead of the last ordered block are accepted regardless of pending block state. [14](#0-13) 

3. **Multiple Pipelined Blocks**: Since ordered blocks contain multiple pipelined blocks at different rounds, and different ordered blocks have gaps between their first rounds (as shown in test infrastructure), there are frequent opportunities for the split point to fall incorrectly.

4. **Realistic Test Scenarios**: The test suite itself creates ordered blocks with intentional round gaps to simulate production conditions. [15](#0-14) 

The vulnerability does not require malicious behavior - normal network conditions can trigger it. The fallback mechanism provides recovery, but the issue causes repeated service degradation cycles.

## Recommendation

Modify the `remove_ready_block()` logic to preserve pending blocks that are still waiting for payloads, regardless of the received payload's round. The function should only remove blocks that:
1. Have all their payloads available and are ready for processing, OR
2. Are genuinely outdated (e.g., their rounds are less than or equal to the last committed block)

Specifically, change the re-insertion logic to preserve blocks whose payloads haven't arrived yet:

```rust
// Instead of checking if last_block_round > received_payload_round,
// preserve the block if it's still waiting for payloads
if !block_payload_store.all_payloads_exist(pending_block.ordered_block().blocks()) {
    // Block is still waiting for payloads - keep it pending
    blocks_at_higher_rounds.insert(epoch_and_round, pending_block);
}
```

Additionally, consider checking whether the received payload actually corresponds to any pending block before triggering the removal logic.

## Proof of Concept

The existing test suite demonstrates this behavior: [16](#0-15) 

This test shows that when `remove_ready_block` is called with the third block's round, but no payloads exist, the first three blocks are all removed from the pending store. This confirms the vulnerability - valid blocks waiting for payloads are dropped when an unrelated payload round is received.

**Notes**

The vulnerability is a design flaw in the pending block management logic. The code comment explicitly states "Any earlier blocks are considered out-of-date and will be dropped" [17](#0-16) , indicating this is intentional behavior. However, this design assumption is incorrect for asynchronous message delivery in P2P networks where payloads can arrive for future blocks before payloads for earlier pending blocks arrive.

The consensus observer includes a fallback mechanism that eventually recovers through state sync, preventing permanent failure. However, the logic flaw causes repeated cycles of block loss and recovery, degrading observer reliability and performance.

### Citations

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L61-73)
```rust
pub struct PendingBlockStore {
    // The configuration of the consensus observer
    consensus_observer_config: ConsensusObserverConfig,

    // A map of ordered blocks that are without payloads. The key is
    // the (epoch, round) of the first block in the ordered block.
    blocks_without_payloads: BTreeMap<(u64, Round), Arc<PendingBlockWithMetadata>>,

    // A map of ordered blocks that are without payloads. The key is
    // the hash of the first block in the ordered block.
    // Note: this is the same as blocks_without_payloads, but with a different key.
    blocks_without_payloads_by_hash: BTreeMap<HashValue, Arc<PendingBlockWithMetadata>>,
}
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L206-212)
```rust
        // Calculate the round at which to split the blocks
        let split_round = received_payload_round.saturating_add(1);

        // Split the blocks at the epoch and round
        let mut blocks_at_higher_rounds = self
            .blocks_without_payloads
            .split_off(&(received_payload_epoch, split_round));
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L214-215)
```rust
        // Check if the last block is ready (this should be the only ready block).
        // Any earlier blocks are considered out-of-date and will be dropped.
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L217-228)
```rust
        if let Some((epoch_and_round, pending_block)) = self.blocks_without_payloads.pop_last() {
            // If all payloads exist for the block, then the block is ready
            if block_payload_store.all_payloads_exist(pending_block.ordered_block().blocks()) {
                ready_block = Some(pending_block);
            } else {
                // Otherwise, check if we're still waiting for higher payloads for the block
                let last_pending_block_round = pending_block.ordered_block().last_block().round();
                if last_pending_block_round > received_payload_round {
                    blocks_at_higher_rounds.insert(epoch_and_round, pending_block);
                }
            }
        }
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L230-239)
```rust
        // Check if any out-of-date blocks are going to be dropped
        if !self.blocks_without_payloads.is_empty() {
            info!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Dropped {:?} out-of-date pending blocks before epoch and round: {:?}",
                    self.blocks_without_payloads.len(),
                    (received_payload_epoch, received_payload_round)
                ))
            );
        }
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L243-252)
```rust
        // Clear all blocks from the pending block stores
        self.clear_missing_blocks();

        // Update the pending block stores to only include the blocks at higher rounds
        self.blocks_without_payloads = blocks_at_higher_rounds;
        for pending_block in self.blocks_without_payloads.values() {
            let first_block = pending_block.ordered_block().first_block();
            self.blocks_without_payloads_by_hash
                .insert(first_block.id(), pending_block.clone());
        }
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L1025-1079)
```rust
    fn test_remove_ready_block_singular_blocks_missing() {
        // Create a new pending block store
        let max_num_pending_blocks = 100;
        let consensus_observer_config = ConsensusObserverConfig {
            max_num_pending_blocks: max_num_pending_blocks as u64,
            ..ConsensusObserverConfig::default()
        };
        let pending_block_store = Arc::new(Mutex::new(PendingBlockStore::new(
            consensus_observer_config,
        )));

        // Insert the maximum number of blocks into the store
        let current_epoch = 10;
        let starting_round = 100;
        let pending_blocks = create_and_add_pending_blocks(
            pending_block_store.clone(),
            max_num_pending_blocks,
            current_epoch,
            starting_round,
            1,
        );

        // Create an empty block payload store
        let mut block_payload_store = BlockPayloadStore::new(consensus_observer_config);

        // Remove the third block (which is not ready)
        let third_block = pending_blocks[2].clone();
        let third_block_round = third_block.first_block().round();
        let ready_block = pending_block_store.lock().remove_ready_block(
            current_epoch,
            third_block_round,
            &mut block_payload_store,
        );
        assert!(ready_block.is_none());

        // Verify that the first three blocks were removed
        verify_pending_blocks(
            pending_block_store.clone(),
            max_num_pending_blocks - 3,
            &pending_blocks[3..].to_vec(),
        );

        // Remove the last block (which is not ready)
        let last_block = pending_blocks.last().unwrap().clone();
        let last_block_round = last_block.first_block().round();
        let ready_block = pending_block_store.lock().remove_ready_block(
            current_epoch,
            last_block_round,
            &mut block_payload_store,
        );
        assert!(ready_block.is_none());

        // Verify that the store is now empty
        verify_pending_blocks(pending_block_store.clone(), 0, &vec![]);
    }
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L1119-1130)
```rust
    fn create_ordered_block(
        epoch: u64,
        starting_round: Round,
        max_pipelined_blocks: u64,
        block_index: usize,
    ) -> OrderedBlock {
        // Create the pipelined blocks
        let num_pipelined_blocks = rand::thread_rng().gen_range(1, max_pipelined_blocks + 1);
        let mut pipelined_blocks = vec![];
        for j in 0..num_pipelined_blocks {
            // Calculate the block round
            let round = starting_round + ((block_index as Round) * max_pipelined_blocks) + j; // Ensure gaps between blocks
```

**File:** consensus/src/consensus_observer/network/observer_message.rs (L180-205)
```rust
#[derive(Clone, Debug, Deserialize, Eq, PartialEq, Serialize)]
pub struct OrderedBlock {
    blocks: Vec<Arc<PipelinedBlock>>,
    ordered_proof: LedgerInfoWithSignatures,
}

impl OrderedBlock {
    pub fn new(blocks: Vec<Arc<PipelinedBlock>>, ordered_proof: LedgerInfoWithSignatures) -> Self {
        Self {
            blocks,
            ordered_proof,
        }
    }

    /// Returns a reference to the ordered blocks
    pub fn blocks(&self) -> &Vec<Arc<PipelinedBlock>> {
        &self.blocks
    }

    /// Returns a copy of the first ordered block
    pub fn first_block(&self) -> Arc<PipelinedBlock> {
        self.blocks
            .first()
            .cloned()
            .expect("At least one block is expected!")
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L341-353)
```rust
    async fn order_ready_pending_block(&mut self, block_epoch: u64, block_round: Round) {
        // Remove any ready pending block
        let pending_block_with_metadata = self
            .observer_block_data
            .lock()
            .remove_ready_pending_block(block_epoch, block_round);

        // Process the ready ordered block (if it exists)
        if let Some(pending_block_with_metadata) = pending_block_with_metadata {
            self.process_ordered_block(pending_block_with_metadata)
                .await;
        }
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L366-397)
```rust
        // Determine if the payload is behind the last ordered block, or if it already exists
        let last_ordered_block = self.observer_block_data.lock().get_last_ordered_block();
        let payload_out_of_date =
            (block_epoch, block_round) <= (last_ordered_block.epoch(), last_ordered_block.round());
        let payload_exists = self
            .observer_block_data
            .lock()
            .existing_payload_entry(&block_payload);

        // If the payload is out of date or already exists, ignore it
        if payload_out_of_date || payload_exists {
            // Update the metrics for the dropped block payload
            update_metrics_for_dropped_block_payload_message(peer_network_id, &block_payload);
            return;
        }

        // Update the metrics for the received block payload
        update_metrics_for_block_payload_message(peer_network_id, &block_payload);

        // Verify the block payload digests
        if let Err(error) = block_payload.verify_payload_digests() {
            // Log the error and update the invalid message counter
            error!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Failed to verify block payload digests! Ignoring block: {:?}, from peer: {:?}. Error: {:?}",
                    block_payload.block(), peer_network_id,
                    error
                ))
            );
            increment_invalid_message_counter(&peer_network_id, metrics::BLOCK_PAYLOAD_LABEL);
            return;
        }
```

**File:** consensus/src/consensus_observer/observer/block_data.rs (L244-254)
```rust
    pub fn remove_ready_pending_block(
        &mut self,
        received_payload_epoch: u64,
        received_payload_round: Round,
    ) -> Option<Arc<PendingBlockWithMetadata>> {
        self.pending_block_store.remove_ready_block(
            received_payload_epoch,
            received_payload_round,
            &mut self.block_payload_store,
        )
    }
```

**File:** consensus/src/consensus_observer/observer/fallback_manager.rs (L55-85)
```rust
    /// Verifies that the DB is continuing to sync and commit new data, and that
    /// the node has not fallen too far behind the rest of the network.
    /// If not, an error is returned, indicating that we should enter fallback mode.
    pub fn check_syncing_progress(&mut self) -> Result<(), Error> {
        // If we're still within the startup period, we don't need to verify progress
        let time_now = self.time_service.now();
        let startup_period = Duration::from_millis(
            self.consensus_observer_config
                .observer_fallback_startup_period_ms,
        );
        if time_now.duration_since(self.start_time) < startup_period {
            return Ok(()); // We're still in the startup period
        }

        // Fetch the synced ledger info version from storage
        let latest_ledger_info_version =
            self.db_reader
                .get_latest_ledger_info_version()
                .map_err(|error| {
                    Error::UnexpectedError(format!(
                        "Failed to read highest synced version: {:?}",
                        error
                    ))
                })?;

        // Verify that the synced version is increasing appropriately
        self.verify_increasing_sync_versions(latest_ledger_info_version, time_now)?;

        // Verify that the sync lag is within acceptable limits
        self.verify_sync_lag_health(latest_ledger_info_version)
    }
```
