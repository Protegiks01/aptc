# Audit Report

## Title
Vote Persistence Durability Failure Enables Equivocation on Crash

## Summary
The `guarded_construct_and_sign_vote_two_chain()` function in SafetyRules updates `safety_data.last_vote` in memory before durable persistence completes. When using OnDiskStorage backend, the absence of `fsync()` allows votes to be lost on system crash, enabling validators to equivocate and violate consensus safety guarantees.

## Finding Description

The vulnerability exists in the 2-chain consensus voting flow where vote persistence lacks durability guarantees.

In the vote creation flow, the code updates `safety_data.last_vote` in memory and immediately calls `set_safety_data()` to persist it: [1](#0-0) 

The persistent storage contract explicitly documents: "Any set function is expected to sync to the remote system before returning." [2](#0-1) 

However, OnDiskStorage's write implementation calls `write_all()` followed by atomic rename, but never calls `fsync()` or `sync_all()`: [3](#0-2) 

This violates durability guarantees because `write_all()` only writes to the OS page cache. A system crash (power loss, OOM kill, kernel panic) before the OS flushes buffers results in data loss.

**Attack Scenario:**

1. Validator receives Block A proposal at round R
2. Safety checks pass, vote created and signed
3. `safety_data.last_vote` updated in memory for Block A
4. `set_safety_data()` writes to file without fsync - returns success
5. Vote returned to RoundManager and broadcast to network [4](#0-3) 
6. **System crash occurs** before OS flushes write buffers
7. On restart, `safety_data` loaded from disk lacks the vote for Block A
8. Validator receives Block B proposal at same round R
9. Safety check examines `last_vote` for this round - passes because vote is missing [5](#0-4) 
10. New vote created and broadcast for Block B at round R
11. **Equivocation achieved**: Validator voted for two conflicting blocks at the same round

While other validators can detect equivocation when receiving both votes [6](#0-5) , the purpose of SafetyRules is to **prevent** equivocation, not merely detect it. An honest validator exhibiting Byzantine behavior undermines the < 1/3 fault tolerance assumption.

## Impact Explanation

**Critical Severity** - This vulnerability enables consensus safety violations, which aligns with the Aptos bug bounty's highest severity category:

- **Consensus Safety Violation**: Equivocation (double-voting) is the fundamental safety violation in BFT consensus protocols. This directly matches the Critical severity criterion: "Different validators commit different blocks" and "Double-spending achievable with < 1/3 Byzantine validators."

- **Chain Split Potential**: Multiple validators experiencing this issue simultaneously could cause conflicting blocks to be committed, potentially requiring a hardfork to resolve.

- **Byzantine Behavior from Honest Nodes**: Validators become Byzantine through crash-recovery bugs rather than malicious intent, which is particularly insidious as it bypasses trust assumptions about honest validator operators.

This directly breaks the AptosBFT invariant that the protocol must prevent double-spending and chain splits under < 1/3 Byzantine validators.

## Likelihood Explanation

**Medium Likelihood** - The vulnerability has real exploitability with nuanced considerations:

**Factors Increasing Likelihood:**
- Natural system crashes occur regularly in production environments (power failures, OOM kills, kernel panics, hardware faults)
- Timing window exists between `write_all()` return and OS flush (typically milliseconds but non-zero)
- OnDiskStorage is NOT blocked by the config sanitizer for mainnet validators [7](#0-6) 
- Only InMemoryStorage is explicitly prohibited by the sanitizer [8](#0-7) 
- OnDiskStorage appears in production-style validator configuration templates [9](#0-8) 

**Factors Decreasing Likelihood:**
- Documentation explicitly warns that OnDiskStorage "should not be used in production" [10](#0-9) 
- The README states OnDiskStorage "should not be used in production environments as it provides no security guarantees" [11](#0-10) 
- Production mainnet validators should use VaultStorage per recommendations
- Narrow timing window required between write and crash

However, testnets, devnets, and misconfigured validators could all be affected. The vulnerability exists in deployable code with no technical enforcement preventing its use beyond documentation warnings.

## Recommendation

**Immediate Fix**: Add `fsync()` or `sync_all()` to OnDiskStorage's write method:

```rust
fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
    let contents = serde_json::to_vec(data)?;
    let mut file = File::create(self.temp_path.path())?;
    file.write_all(&contents)?;
    file.sync_all()?;  // ADD THIS LINE
    fs::rename(&self.temp_path, &self.file_path)?;
    Ok(())
}
```

**Additional Recommendations**:
1. Add a config sanitizer check that explicitly blocks OnDiskStorage for mainnet validators (not just InMemoryStorage)
2. Consider adding a file-level sync after the rename operation for additional durability
3. Add integration tests that simulate crash scenarios to verify durability guarantees
4. Update documentation to more prominently highlight the durability risks

## Proof of Concept

While a full PoC would require simulating system crashes, the vulnerability can be demonstrated through code analysis:

1. Deploy a validator with OnDiskStorage backend configuration
2. Trigger vote creation through normal consensus flow
3. Observe that `write_all()` returns success without fsync
4. Simulate crash by killing the process before OS buffer flush
5. Restart validator and observe missing vote in loaded safety_data
6. Demonstrate that the validator can vote again on the same round

The core issue is verifiable by code inspection: [3](#0-2)  shows no durability guarantee is provided.

## Notes

This vulnerability represents a subtle but critical gap between documented contracts and implementation. While the persistent storage interface specifies that "Any set function is expected to sync to the remote system before returning," OnDiskStorage violates this contract. The lack of technical enforcement (config sanitizer only blocks InMemoryStorage, not OnDiskStorage) creates a deployment risk where validators following configuration templates could be vulnerable despite documentation warnings.

### Citations

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L70-74)
```rust
        if let Some(vote) = safety_data.last_vote.clone() {
            if vote.vote_data().proposed().round() == proposed_block.round() {
                return Ok(vote);
            }
        }
```

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L91-92)
```rust
        safety_data.last_vote = Some(vote.clone());
        self.persistent_storage.set_safety_data(safety_data)?;
```

**File:** consensus/safety-rules/src/persistent_safety_storage.rs (L18-18)
```rust
/// Any set function is expected to sync to the remote system before returning.
```

**File:** secure/storage/src/on_disk.rs (L16-22)
```rust
/// OnDiskStorage represents a key value store that is persisted to the local filesystem and is
/// intended for single threads (or must be wrapped by a Arc<RwLock<>>). This provides no permission
/// checks and simply offers a proof of concept to unblock building of applications without more
/// complex data stores. Internally, it reads and writes all data to a file, which means that it
/// must make copies of all key material which violates the code base. It violates it because
/// the anticipation is that data stores would securely handle key material. This should not be used
/// in production.
```

**File:** secure/storage/src/on_disk.rs (L64-70)
```rust
    fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
        let contents = serde_json::to_vec(data)?;
        let mut file = File::create(self.temp_path.path())?;
        file.write_all(&contents)?;
        fs::rename(&self.temp_path, &self.file_path)?;
        Ok(())
    }
```

**File:** consensus/src/round_manager.rs (L1520-1527)
```rust
        let vote_result = self.safety_rules.lock().construct_and_sign_vote_two_chain(
            &vote_proposal,
            self.block_store.highest_2chain_timeout_cert().as_deref(),
        );
        let vote = vote_result.context(format!(
            "[RoundManager] SafetyRules Rejected {}",
            block_arc.block()
        ))?;
```

**File:** consensus/src/pending_votes.rs (L287-307)
```rust
        if let Some((previously_seen_vote, previous_li_digest)) =
            self.author_to_vote.get(&vote.author())
        {
            // is it the same vote?
            if &li_digest == previous_li_digest {
                // we've already seen an equivalent vote before
                let new_timeout_vote = vote.is_timeout() && !previously_seen_vote.is_timeout();
                if !new_timeout_vote {
                    // it's not a new timeout vote
                    return VoteReceptionResult::DuplicateVote;
                }
            } else {
                // we have seen a different vote for the same round
                error!(
                    SecurityEvent::ConsensusEquivocatingVote,
                    remote_peer = vote.author(),
                    vote = vote,
                    previous_vote = previously_seen_vote
                );

                return VoteReceptionResult::EquivocateVote;
```

**File:** config/src/config/safety_rules_config.rs (L86-96)
```rust
            // Verify that the secure backend is appropriate for mainnet validators
            if chain_id.is_mainnet()
                && node_type.is_validator()
                && safety_rules_config.backend.is_in_memory()
            {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "The secure backend should not be set to in memory storage in mainnet!"
                        .to_string(),
                ));
            }
```

**File:** config/src/config/secure_backend_config.rs (L45-48)
```rust
    /// Returns true iff the backend is in memory
    pub fn is_in_memory(&self) -> bool {
        matches!(self, SecureBackend::InMemoryStorage)
    }
```

**File:** terraform/helm/aptos-node/files/configs/validator-base.yaml (L14-16)
```yaml
    backend:
      type: "on_disk_storage"
      path: secure-data.json
```

**File:** secure/storage/README.md (L37-42)
```markdown
- `OnDisk`: Similar to InMemory, the OnDisk secure storage implementation provides another
useful testing implementation: an on-disk storage engine, where the storage backend is
implemented using a single file written to local disk. In a similar fashion to the in-memory
storage, on-disk should not be used in production environments as it provides no security
guarantees (e.g., encryption before writing to disk). Moreover, OnDisk storage does not
currently support concurrent data accesses.
```
