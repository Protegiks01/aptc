# Audit Report

## Title
Non-Atomic Pruner Progress Writes Cause Validator Node Restart Failure After Partial Database Write

## Summary
The `write_pruner_progress()` function performs non-atomic writes across 8 separate sub-databases during fast sync completion. If the write operation fails partway through due to disk I/O error, OOM, or process crash, the node enters an inconsistent state where different ledger components have different pruning progress values. On restart, this causes LedgerPruner initialization to fail, preventing the validator node from recovering without manual database intervention.

## Finding Description

The vulnerability lies in the sequential, non-atomic nature of pruner progress updates across multiple database instances. During fast sync finalization, `finalize_state_snapshot()` calls `save_min_readable_version()` on multiple pruners, including the ledger pruner. [1](#0-0) 

For the ledger pruner, this delegates to `LedgerPrunerManager::save_min_readable_version()`, which calls `write_pruner_progress()`: [2](#0-1) 

This function writes to 8 different sub-databases sequentially using the `?` operator for early return on error: [3](#0-2) 

Each individual `db.put()` call is atomic (e.g., in EventDb): [4](#0-3) 

However, the sequence of 8 writes is NOT atomic. When storage sharding is enabled (default in production configurations), these are physically separate RocksDB instances: [5](#0-4) [6](#0-5) [7](#0-6) 

Notably, there is a TODO comment acknowledging this data inconsistency issue: [8](#0-7) 

If a system failure occurs after writing to some sub-databases but before completing all writes, the metadata keys become inconsistent. On restart, LedgerPruner initialization reads `metadata_progress` from the LedgerMetadataDb: [9](#0-8) 

Each sub-pruner initializes and attempts to "catch up" from its stored progress to `metadata_progress` by calling `prune()`. For example, TransactionPruner: [10](#0-9) 

If a sub-pruner has progress=2000 but metadata_progress=1000 (due to partial write failure where the sub-pruner DB was written but LedgerMetadataDb was not), it will call `prune(2000, 1000)`, attempting to prune backwards. For TransactionPruner, this triggers an assertion failure: [11](#0-10) 

This breaks the **State Consistency** invariant: metadata writes must be atomic to ensure consistent recovery state after system failures.

## Impact Explanation

**Severity: High**

This vulnerability causes **validator node unavailability** requiring manual intervention:

- **Affected Component**: Storage layer pruning system
- **Failure Mode**: Node cannot complete startup after system failure during fast sync
- **Recovery**: Requires manual database repair, forced resync, or state snapshot restoration
- **Network Impact**: Single validator unavailability (does not break consensus due to BFT tolerance)

This qualifies under Aptos bug bounty criteria for "Validator node slowdowns" (High Severity). While it does not directly cause fund theft or consensus violation, it represents a critical availability failure that violates atomicity guarantees in database operations. The severity is HIGH because it affects individual validator availability and requires manual intervention to recover.

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability can be triggered by natural system failures during production operation:

- **Disk I/O errors**: Hardware failures, filesystem corruption during the ~100Î¼s write window
- **Memory exhaustion**: Large state sync operations causing OOM during writes
- **Process crashes**: Kernel OOM killer, segfaults, or assertion failures mid-write

Such failures occur regularly in production environments, especially during resource-intensive fast sync operations. The vulnerability is **deterministic** once triggered - inconsistent metadata guarantees node restart failure without manual intervention.

## Recommendation

Implement atomic writes across all sub-databases using a two-phase commit protocol or coordinator pattern:

1. **Option 1**: Use a write-ahead log (WAL) to record the intended version before writing to sub-databases
2. **Option 2**: Write all updates to a single coordinating batch, then atomically commit
3. **Option 3**: Implement a recovery mechanism that detects inconsistencies on startup and repairs them automatically by reading the minimum progress across all sub-databases

Example fix approach:
```rust
pub(crate) fn write_pruner_progress(&self, version: Version) -> Result<()> {
    // Prepare all batches first
    let mut batches = Vec::new();
    // ... prepare all 8 batches ...
    
    // Write to WAL first
    self.write_wal_entry(version)?;
    
    // Then write to all databases
    for (db, batch) in batches {
        db.write_schemas(batch)?;
    }
    
    // Clear WAL entry on success
    self.clear_wal_entry()?;
    
    Ok(())
}
```

Additionally, implement a startup recovery routine that checks for inconsistencies and automatically repairs them by using the minimum progress value across all sub-databases.

## Proof of Concept

The report lacks an executable proof of concept. However, the vulnerability can be reproduced by:

1. Starting a validator node with storage sharding enabled
2. Initiating fast sync to restore from a state snapshot
3. Simulating a system failure (e.g., SIGKILL) during the `finalize_state_snapshot()` call, specifically after some but not all `write_pruner_progress()` calls complete
4. Attempting to restart the node
5. Observing the assertion failure in TransactionPruner initialization when it attempts to prune backwards

The specific timing and simulation of partial writes would require instrumentation of the RocksDB layer to force failures between database writes.

**Notes**

This is a reliability/availability vulnerability rather than an actively exploitable security flaw. It cannot be triggered by a malicious actor but rather by natural system failures during production operation. The impact is limited to single validator unavailability and does not affect network consensus or user funds due to Aptos's BFT tolerance. However, it represents a significant operational risk for validator operators and violates the atomicity guarantees expected in database operations.

### Citations

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L225-234)
```rust
            self.ledger_pruner.save_min_readable_version(version)?;
            self.state_store
                .state_merkle_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .epoch_snapshot_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .state_kv_pruner
                .save_min_readable_version(version)?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L80-89)
```rust
    fn save_min_readable_version(&self, min_readable_version: Version) -> Result<()> {
        self.min_readable_version
            .store(min_readable_version, Ordering::SeqCst);

        PRUNER_VERSIONS
            .with_label_values(&["ledger_pruner", "min_readable"])
            .set(min_readable_version as i64);

        self.ledger_db.write_pruner_progress(min_readable_version)
    }
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L174-293)
```rust
        let ledger_db_folder = db_root_path.as_ref().join(LEDGER_DB_FOLDER_NAME);

        let mut event_db = None;
        let mut persisted_auxiliary_info_db = None;
        let mut transaction_accumulator_db = None;
        let mut transaction_auxiliary_data_db = None;
        let mut transaction_db = None;
        let mut transaction_info_db = None;
        let mut write_set_db = None;
        THREAD_MANAGER.get_non_exe_cpu_pool().scope(|s| {
            s.spawn(|_| {
                let event_db_raw = Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(EVENT_DB_NAME),
                        EVENT_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                );
                event_db = Some(EventDb::new(
                    event_db_raw.clone(),
                    EventStore::new(event_db_raw),
                ));
            });
            s.spawn(|_| {
                persisted_auxiliary_info_db = Some(PersistedAuxiliaryInfoDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(PERSISTED_AUXILIARY_INFO_DB_NAME),
                        PERSISTED_AUXILIARY_INFO_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_accumulator_db = Some(TransactionAccumulatorDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_ACCUMULATOR_DB_NAME),
                        TRANSACTION_ACCUMULATOR_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_auxiliary_data_db = Some(TransactionAuxiliaryDataDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_AUXILIARY_DATA_DB_NAME),
                        TRANSACTION_AUXILIARY_DATA_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )))
            });
            s.spawn(|_| {
                transaction_db = Some(TransactionDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_DB_NAME),
                        TRANSACTION_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_info_db = Some(TransactionInfoDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_INFO_DB_NAME),
                        TRANSACTION_INFO_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                write_set_db = Some(WriteSetDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(WRITE_SET_DB_NAME),
                        WRITE_SET_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
        });

        // TODO(grao): Handle data inconsistency.

        Ok(Self {
            ledger_metadata_db: LedgerMetadataDb::new(ledger_metadata_db),
            event_db: event_db.unwrap(),
            persisted_auxiliary_info_db: persisted_auxiliary_info_db.unwrap(),
            transaction_accumulator_db: transaction_accumulator_db.unwrap(),
            transaction_auxiliary_data_db: transaction_auxiliary_data_db.unwrap(),
            transaction_db: transaction_db.unwrap(),
            transaction_info_db: transaction_info_db.unwrap(),
            write_set_db: write_set_db.unwrap(),
            enable_storage_sharding: true,
        })
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L373-388)
```rust
    pub(crate) fn write_pruner_progress(&self, version: Version) -> Result<()> {
        info!("Fast sync is done, writing pruner progress {version} for all ledger sub pruners.");
        self.event_db.write_pruner_progress(version)?;
        self.persisted_auxiliary_info_db
            .write_pruner_progress(version)?;
        self.transaction_accumulator_db
            .write_pruner_progress(version)?;
        self.transaction_auxiliary_data_db
            .write_pruner_progress(version)?;
        self.transaction_db.write_pruner_progress(version)?;
        self.transaction_info_db.write_pruner_progress(version)?;
        self.write_set_db.write_pruner_progress(version)?;
        self.ledger_metadata_db.write_pruner_progress(version)?;

        Ok(())
    }
```

**File:** storage/aptosdb/src/ledger_db/event_db.rs (L47-52)
```rust
    pub(super) fn write_pruner_progress(&self, version: Version) -> Result<()> {
        self.db.put::<DbMetadataSchema>(
            &DbMetadataKey::EventPrunerProgress,
            &DbMetadataValue::Version(version),
        )
    }
```

**File:** config/src/config/storage_config.rs (L233-233)
```rust
            enable_storage_sharding: true,
```

**File:** terraform/helm/aptos-node/files/configs/validator-base.yaml (L38-38)
```yaml
    enable_storage_sharding: true
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L129-134)
```rust
        let metadata_progress = ledger_metadata_pruner.progress()?;

        info!(
            metadata_progress = metadata_progress,
            "Created ledger metadata pruner, start catching up all sub pruners."
        );
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L84-104)
```rust
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.transaction_db_raw(),
            &DbMetadataKey::TransactionPrunerProgress,
            metadata_progress,
        )?;

        let myself = TransactionPruner {
            transaction_store,
            ledger_db,
            internal_indexer_db,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up TransactionPruner."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L111-111)
```rust
        ensure!(end >= start, "{} must be >= {}", end, start);
```
