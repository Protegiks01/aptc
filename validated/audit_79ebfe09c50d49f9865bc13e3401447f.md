# Audit Report

## Title
Ordered Certificate Rollback During BlockTree Rebuild Violates Finality Invariant

## Summary
The `highest_ordered_cert` field in the consensus layer can decrease its round value when `BlockStore::rebuild()` is invoked during sync operations, violating the critical monotonicity invariant that ordered rounds must never decrease. This occurs because rebuild creates a new `BlockTree` initialized with `root_ordered_cert` derived from the commit certificate, permanently discarding higher previously-observed ordered certificate values.

## Finding Description

The AptosBFT consensus protocol maintains a `highest_ordered_cert` field representing the highest round for which blocks have been ordered for execution. [1](#0-0) 

During normal operation, both `insert_ordered_cert` and `insert_quorum_cert` enforce monotonicity with explicit comparison checks to ensure this value only increases. [2](#0-1) [3](#0-2) 

However, when `BlockStore::rebuild()` is called during sync operations, it completely replaces the in-memory `BlockTree` structure. [4](#0-3)  The replacement occurs through the `build()` method which overwrites the existing tree. [5](#0-4) 

The new tree is initialized via `BlockTree::new()` with a `root_ordered_cert` parameter that directly sets `highest_ordered_cert` without any monotonicity validation. [6](#0-5) 

When `order_vote_enabled = true`, the `root_ordered_cert` is created from the committed ledger info, not the previously-seen highest ordered cert. [7](#0-6)  This means `root_ordered_cert` is based on the **commit round** from storage, which can be significantly lower than the previous `highest_ordered_cert` value due to execution pipelining (ordered_root > commit_root).

**Vulnerability Mechanism:**

1. Node has `highest_ordered_cert` at round 100 (from recently processed order votes/QCs)
2. Node has `ordered_root` at round 90, `commit_root` at round 80 (normal pipelining lag)  
3. Node receives SyncInfo from peer with `highest_commit_cert` at round 95
4. Sync is triggered because the commit cert block doesn't exist locally [8](#0-7) 
5. `fast_forward_sync()` creates recovery data with `root_ordered_cert` based on committed round 95 [9](#0-8) 
6. `rebuild()` replaces the tree, setting `highest_ordered_cert` to round 95
7. **Result:** `highest_ordered_cert` has rolled back from 100 to 95

The rollback is then propagated when the node creates SyncInfo messages, which include the rolled-back `highest_ordered_round`. [10](#0-9) 

Only the `highest_2chain_timeout_cert` is preserved from the old tree during rebuild - `highest_ordered_cert` is not. [11](#0-10) 

## Impact Explanation

**Severity: CRITICAL** - This constitutes a consensus safety violation with multiple severe impacts:

**1. Order Vote Acceptance Window Regression:**
The RoundManager only processes order votes within 100 rounds of the current `highest_ordered_round`. [12](#0-11)  After rollback from round 100 to 95, order votes for rounds 96-100 (previously outside the acceptance window) become acceptable again. This could enable:
- Re-processing of order votes for already-ordered rounds
- Potential ordering of different blocks at the same rounds if vote signatures differ
- Consensus fork if different execution paths are taken

**2. Invariant Violation:**
The fundamental consensus invariant that "ordered rounds are monotonically increasing" is broken. Both `insert_ordered_cert` and `insert_quorum_cert` explicitly enforce this invariant during normal operations, indicating its critical importance to protocol safety. The bypass during rebuild undermines these safety guarantees.

**3. Network-wide State Confusion:**
The rolled-back `highest_ordered_round` is broadcast to peers via SyncInfo messages. [13](#0-12)  This propagates incorrect ordering progress information across the network, potentially causing:
- Cascading rollbacks as other nodes sync to incorrect state
- Inconsistent view of which rounds have been ordered
- Disruption of consensus progress tracking

**4. Round State Management Impact:**
The consensus layer uses `highest_ordered_round` for timeout calculations and round progression. Rollback could cause nodes to regress their round state, affecting participation in consensus.

This meets the **Critical Severity** criteria of "Consensus/Safety violations" as defined in the Aptos bug bounty program, as it violates a core consensus invariant that is explicitly enforced elsewhere in the codebase.

## Likelihood Explanation

**Likelihood: HIGH** - This vulnerability is triggered during normal network operations without requiring attacker control:

**1. Natural Trigger Conditions:**
- Node temporarily falls behind due to network latency (common scenario)
- Node restarts and performs recovery sync
- Network partition temporarily isolates a node
- Any scenario where sync is triggered while execution pipeline has uncommitted ordered blocks

**2. No Byzantine Behavior Required:**
The vulnerability is triggered by legitimate sync operations, not malicious input. The gap between `highest_ordered_cert` and `commit_root` is a natural consequence of execution pipelining, which is the normal mode of operation. [14](#0-13) 

**3. Affects All Nodes:**
When `order_vote_enabled = true` (production configuration for decoupled execution), all nodes are vulnerable during sync operations. The `fast_forward_sync` path is invoked whenever `need_sync_for_ledger_info()` returns true. [15](#0-14) 

**4. Automatic Propagation:**
Once one node experiences rollback, it broadcasts the incorrect `highest_ordered_round` in its SyncInfo messages to all peers, potentially causing network-wide confusion about ordering progress.

The combination of natural occurrence during normal operations, lack of attacker control requirements, and network-wide scope makes this a high-likelihood vulnerability.

## Recommendation

Preserve the `highest_ordered_cert` value across rebuild operations to maintain monotonicity:

```rust
pub async fn rebuild(
    &self,
    root: RootInfo,
    root_metadata: RootMetadata,
    blocks: Vec<Block>,
    quorum_certs: Vec<QuorumCert>,
) {
    let max_pruned_blocks_in_mem = self.inner.read().max_pruned_blocks_in_mem();
    
    // Preserve both highest_2chain_timeout_cert AND highest_ordered_cert from old tree
    let prev_2chain_htc = self.highest_2chain_timeout_cert().map(|tc| tc.as_ref().clone());
    let prev_highest_ordered_cert = self.highest_ordered_cert();
    
    // Ensure root_ordered_cert respects previous highest
    let root_ordered_cert = if root.ordered_cert.commit_info().round() > prev_highest_ordered_cert.commit_info().round() {
        root.ordered_cert
    } else {
        prev_highest_ordered_cert.as_ref().clone()
    };
    
    let root = RootInfo {
        commit_root_block: root.commit_root_block,
        window_root_block: root.window_root_block,
        quorum_cert: root.quorum_cert,
        ordered_cert: root_ordered_cert,
        commit_cert: root.commit_cert,
    };
    
    // Continue with build...
}
```

Alternatively, modify `BlockTree::new()` to accept an optional `previous_highest_ordered_cert` parameter and enforce monotonicity during tree construction.

## Proof of Concept

This vulnerability requires a multi-node consensus environment to demonstrate fully. A conceptual PoC outline:

**Setup:**
- Node A running with order_vote_enabled=true
- Execution pipelining enabled (window_size > 0)
- Create scenario where ordered_root > commit_root

**Trigger:**
1. Process blocks up to round 100, creating `highest_ordered_cert` at round 100
2. Commit only up to round 80, creating `commit_root` at round 80
3. Observe `self.block_store.sync_info().highest_ordered_round() == 100`
4. Trigger sync by simulating SyncInfo from peer with `highest_commit_cert` at round 90
5. This invokes `fast_forward_sync()` â†’ `rebuild()`
6. Observe `self.block_store.sync_info().highest_ordered_round() == 90` (rollback occurred)

**Verification:**
- Log `highest_ordered_cert` before and after rebuild
- Verify order vote acceptance range changes from `(100, 200)` to `(90, 190)`
- Confirm previously rejected votes for rounds 91-100 would now be accepted

The vulnerability is confirmed by code inspection showing that `rebuild()` does not preserve `highest_ordered_cert` and creates a new tree with lower round value based on committed state.

## Notes

This vulnerability violates a fundamental consensus invariant that is explicitly enforced during normal operations. The fact that `insert_ordered_cert` and `insert_quorum_cert` both have explicit monotonicity checks demonstrates that the protocol designers intended this to be a strict invariant. The bypass during rebuild represents a critical gap in maintaining this safety property.

While the full exploitation path to cause consensus divergence between validators is complex and depends on specific network conditions, the invariant violation itself is sufficient to classify this as a critical consensus safety issue. The rollback can occur during normal network operations without requiring byzantine behavior, making it a practical concern for production deployments.

### Citations

**File:** consensus/src/block_storage/block_tree.rs (L90-90)
```rust
    highest_ordered_cert: Arc<WrappedLedgerInfo>,
```

**File:** consensus/src/block_storage/block_tree.rs (L104-148)
```rust
    pub(super) fn new(
        commit_root_id: HashValue,
        window_root: PipelinedBlock,
        root_quorum_cert: QuorumCert,
        root_ordered_cert: WrappedLedgerInfo,
        root_commit_cert: WrappedLedgerInfo,
        max_pruned_blocks_in_mem: usize,
        highest_2chain_timeout_cert: Option<Arc<TwoChainTimeoutCertificate>>,
    ) -> Self {
        assert_eq!(window_root.epoch(), root_ordered_cert.commit_info().epoch());
        assert!(window_root.round() <= root_ordered_cert.commit_info().round());
        let window_root_id = window_root.id();

        // Build the tree from the window root block which is <= the commit root block.
        let mut id_to_block = HashMap::new();
        let mut round_to_ids = BTreeMap::new();
        round_to_ids.insert(window_root.round(), window_root_id);
        id_to_block.insert(window_root_id, LinkableBlock::new(window_root));
        counters::NUM_BLOCKS_IN_TREE.set(1);

        let root_quorum_cert = Arc::new(root_quorum_cert);
        let mut id_to_quorum_cert = HashMap::new();
        id_to_quorum_cert.insert(
            root_quorum_cert.certified_block().id(),
            Arc::clone(&root_quorum_cert),
        );

        let pruned_block_ids = VecDeque::with_capacity(max_pruned_blocks_in_mem);

        BlockTree {
            id_to_block,
            ordered_root_id: commit_root_id,
            commit_root_id, // initially we set commit_root_id = root_id
            window_root_id,
            highest_certified_block_id: commit_root_id,
            highest_quorum_cert: Arc::clone(&root_quorum_cert),
            highest_ordered_cert: Arc::new(root_ordered_cert),
            highest_commit_cert: Arc::new(root_commit_cert),
            id_to_quorum_cert,
            pruned_block_ids,
            max_pruned_blocks_in_mem,
            highest_2chain_timeout_cert,
            round_to_ids,
        }
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L380-383)
```rust
        if self.highest_ordered_cert.commit_info().round() < qc.commit_info().round() {
            // Question: We are updating highest_ordered_cert but not highest_ordered_root. Is that fine?
            self.highest_ordered_cert = Arc::new(qc.into_wrapped_ledger_info());
        }
```

**File:** consensus/src/block_storage/block_tree.rs (L388-392)
```rust
    pub fn insert_ordered_cert(&mut self, ordered_cert: WrappedLedgerInfo) {
        if ordered_cert.commit_info().round() > self.highest_ordered_cert.commit_info().round() {
            self.highest_ordered_cert = Arc::new(ordered_cert);
        }
    }
```

**File:** consensus/src/block_storage/block_store.rs (L259-264)
```rust
        let inner = if let Some(tree_to_replace) = tree_to_replace {
            *tree_to_replace.write() = tree;
            tree_to_replace
        } else {
            Arc::new(RwLock::new(tree))
        };
```

**File:** consensus/src/block_storage/block_store.rs (L352-395)
```rust
    pub async fn rebuild(
        &self,
        root: RootInfo,
        root_metadata: RootMetadata,
        blocks: Vec<Block>,
        quorum_certs: Vec<QuorumCert>,
    ) {
        info!(
            "Rebuilding block tree. root {:?}, blocks {:?}, qcs {:?}",
            root,
            blocks.iter().map(|b| b.id()).collect::<Vec<_>>(),
            quorum_certs
                .iter()
                .map(|qc| qc.certified_block().id())
                .collect::<Vec<_>>()
        );
        let max_pruned_blocks_in_mem = self.inner.read().max_pruned_blocks_in_mem();

        // Rollover the previous highest TC from the old tree to the new one.
        let prev_2chain_htc = self
            .highest_2chain_timeout_cert()
            .map(|tc| tc.as_ref().clone());
        let _ = Self::build(
            root,
            root_metadata,
            blocks,
            quorum_certs,
            prev_2chain_htc,
            self.execution_client.clone(),
            Arc::clone(&self.storage),
            max_pruned_blocks_in_mem,
            Arc::clone(&self.time_service),
            self.vote_back_pressure_limit,
            self.payload_manager.clone(),
            self.order_vote_enabled,
            self.window_size,
            self.pending_blocks.clone(),
            self.pipeline_builder.clone(),
            Some(self.inner.clone()),
        )
        .await;

        self.try_send_for_execution().await;
    }
```

**File:** consensus/src/block_storage/block_store.rs (L680-688)
```rust
    fn sync_info(&self) -> SyncInfo {
        SyncInfo::new_decoupled(
            self.highest_quorum_cert().as_ref().clone(),
            self.highest_ordered_cert().as_ref().clone(),
            self.highest_commit_cert().as_ref().clone(),
            self.highest_2chain_timeout_cert()
                .map(|tc| tc.as_ref().clone()),
        )
    }
```

**File:** consensus/src/persistent_liveness_storage.rs (L145-151)
```rust
        let (root_ordered_cert, root_commit_cert) = if order_vote_enabled {
            // We are setting ordered_root same as commit_root. As every committed block is also ordered, this is fine.
            // As the block store inserts all the fetched blocks and quorum certs and execute the blocks, the block store
            // updates highest_ordered_cert accordingly.
            let root_ordered_cert =
                WrappedLedgerInfo::new(VoteData::dummy(), latest_ledger_info_sig.clone());
            (root_ordered_cert.clone(), root_ordered_cert)
```

**File:** consensus/src/block_storage/sync_manager.rs (L65-93)
```rust
    pub fn need_sync_for_ledger_info(&self, li: &LedgerInfoWithSignatures) -> bool {
        const MAX_PRECOMMIT_GAP: u64 = 200;
        let block_not_exist = self.ordered_root().round() < li.commit_info().round()
            && !self.block_exists(li.commit_info().id());
        // TODO move min gap to fallback (30) to config, and if configurable make sure the value is
        // larger than buffer manager MAX_BACKLOG (20)
        let max_commit_gap = 30.max(2 * self.vote_back_pressure_limit);
        let min_commit_round = li.commit_info().round().saturating_sub(max_commit_gap);
        let current_commit_round = self.commit_root().round();

        if let Some(pre_commit_status) = self.pre_commit_status() {
            let mut status_guard = pre_commit_status.lock();
            if block_not_exist || status_guard.round() < min_commit_round {
                // pause the pre_commit so that pre_commit task doesn't over-commit
                // it can still commit if it receives the LI previously forwarded,
                // but it won't exceed the LI here
                // it'll resume after state sync is done
                status_guard.pause();
                true
            } else {
                if current_commit_round + MAX_PRECOMMIT_GAP < status_guard.round() {
                    status_guard.pause();
                }
                false
            }
        } else {
            block_not_exist || current_commit_round < min_commit_round
        }
    }
```

**File:** consensus/src/block_storage/sync_manager.rs (L279-326)
```rust
    async fn sync_to_highest_quorum_cert(
        &self,
        highest_quorum_cert: QuorumCert,
        highest_commit_cert: WrappedLedgerInfo,
        retriever: &mut BlockRetriever,
    ) -> anyhow::Result<()> {
        if !self.need_sync_for_ledger_info(highest_commit_cert.ledger_info()) {
            return Ok(());
        }

        if let Some(pre_commit_status) = self.pre_commit_status() {
            defer! {
                pre_commit_status.lock().resume();
            }
        }

        let (root, root_metadata, blocks, quorum_certs) = Self::fast_forward_sync(
            &highest_quorum_cert,
            &highest_commit_cert,
            retriever,
            self.storage.clone(),
            self.execution_client.clone(),
            self.payload_manager.clone(),
            self.order_vote_enabled,
            self.window_size,
            Some(self),
        )
        .await?
        .take();
        info!(
            LogSchema::new(LogEvent::CommitViaSync).round(self.ordered_root().round()),
            committed_round = root.commit_root_block.round(),
            block_id = root.commit_root_block.id(),
        );
        self.rebuild(root, root_metadata, blocks, quorum_certs)
            .await;

        if highest_commit_cert.ledger_info().ledger_info().ends_epoch() {
            retriever
                .network
                .send_epoch_change(EpochChangeProof::new(
                    vec![highest_quorum_cert.ledger_info().clone()],
                    /* more = */ false,
                ))
                .await;
        }
        Ok(())
    }
```

**File:** consensus/src/block_storage/sync_manager.rs (L365-525)
```rust
    pub async fn fast_forward_sync<'a>(
        highest_quorum_cert: &'a QuorumCert,
        highest_commit_cert: &'a WrappedLedgerInfo,
        retriever: &'a mut BlockRetriever,
        storage: Arc<dyn PersistentLivenessStorage>,
        execution_client: Arc<dyn TExecutionClient>,
        payload_manager: Arc<dyn TPayloadManager>,
        order_vote_enabled: bool,
        window_size: Option<u64>,
        maybe_block_store: Option<&'a BlockStore>,
    ) -> anyhow::Result<RecoveryData> {
        info!(
            LogSchema::new(LogEvent::StateSync).remote_peer(retriever.preferred_peer),
            "Start state sync to commit cert: {}, quorum cert: {}",
            highest_commit_cert,
            highest_quorum_cert,
        );

        let (target_block_retrieval_payload, num_blocks) =
            Self::generate_target_block_retrieval_payload_and_num_blocks(
                highest_quorum_cert,
                highest_commit_cert,
                window_size,
            );

        // although unlikely, we might wrap num_blocks around on a 32-bit machine
        assert!(num_blocks < usize::MAX as u64);

        BLOCKS_FETCHED_FROM_NETWORK_WHILE_FAST_FORWARD_SYNC.inc_by(num_blocks);
        let mut blocks = retriever
            .retrieve_blocks_in_range(
                highest_quorum_cert.certified_block().id(),
                num_blocks,
                target_block_retrieval_payload,
                highest_quorum_cert
                    .ledger_info()
                    .get_voters(&retriever.validator_addresses()),
            )
            .await?;

        let mut quorum_certs = vec![highest_quorum_cert.clone()];
        quorum_certs.extend(
            blocks
                .iter()
                .take(blocks.len() - 1)
                .map(|block| block.quorum_cert().clone()),
        );

        if !order_vote_enabled {
            // TODO: this is probably still necessary, but need to think harder, it's pretty subtle
            // check if highest_commit_cert comes from a fork
            // if so, we need to fetch it's block as well, to have a proof of commit.
            let highest_commit_certified_block =
                highest_commit_cert.certified_block(order_vote_enabled)?;
            if !blocks
                .iter()
                .any(|block| block.id() == highest_commit_certified_block.id())
            {
                info!(
                    "Found forked QC {}, fetching it as well",
                    highest_commit_cert
                );
                BLOCKS_FETCHED_FROM_NETWORK_WHILE_FAST_FORWARD_SYNC.inc_by(1);

                // Only retrieving one block here, we can simply use TargetBlockRetrieval::TargetBlockId
                let target_block_retrieval_payload =
                    TargetBlockRetrieval::TargetBlockId(highest_commit_certified_block.id());
                let mut additional_blocks = retriever
                    .retrieve_blocks_in_range(
                        highest_commit_certified_block.id(),
                        1,
                        target_block_retrieval_payload,
                        highest_commit_cert
                            .ledger_info()
                            .get_voters(&retriever.validator_addresses()),
                    )
                    .await?;

                assert_eq!(additional_blocks.len(), 1);
                let block = additional_blocks.pop().expect("blocks are empty");
                assert_eq!(
                    block.id(),
                    highest_commit_certified_block.id(),
                    "Expecting in the retrieval response, for commit certificate fork, first block should be {}, but got {}",
                    highest_commit_certified_block.id(),
                    block.id(),
                );
                blocks.push(block);
                quorum_certs.push(
                    highest_commit_cert
                        .clone()
                        .into_quorum_cert(order_vote_enabled)?,
                );
            }
        }

        assert_eq!(blocks.len(), quorum_certs.len());
        info!("[FastForwardSync] Fetched {} blocks. Requested num_blocks {}. Initial block hash {:?}, target block hash {:?}",
            blocks.len(), num_blocks, highest_quorum_cert.certified_block().id(), highest_commit_cert.commit_info().id()
        );
        for (i, block) in blocks.iter().enumerate() {
            assert_eq!(block.id(), quorum_certs[i].certified_block().id());
            if let Some(payload) = block.payload() {
                payload_manager.prefetch_payload_data(
                    payload,
                    block.author().expect("payload block must have author"),
                    block.timestamp_usecs(),
                );
            }
        }

        // Check early that recovery will succeed, and return before corrupting our state in case it will not.
        LedgerRecoveryData::new(highest_commit_cert.ledger_info().clone())
            .find_root(
                &mut blocks.clone(),
                &mut quorum_certs.clone(),
                order_vote_enabled,
                window_size,
            )
            .with_context(|| {
                // for better readability
                quorum_certs.sort_by_key(|qc| qc.certified_block().round());
                format!(
                    "\nRoot: {:?}\nBlocks in db: {}\nQuorum Certs in db: {}\n",
                    highest_commit_cert.commit_info(),
                    blocks
                        .iter()
                        .map(|b| format!("\n\t{}", b))
                        .collect::<Vec<String>>()
                        .concat(),
                    quorum_certs
                        .iter()
                        .map(|qc| format!("\n\t{}", qc))
                        .collect::<Vec<String>>()
                        .concat(),
                )
            })?;

        storage.save_tree(blocks.clone(), quorum_certs.clone())?;
        // abort any pending executor tasks before entering state sync
        // with zaptos, things can run before hitting buffer manager
        if let Some(block_store) = maybe_block_store {
            monitor!(
                "abort_pipeline_for_state_sync",
                block_store.abort_pipeline_for_state_sync().await
            );
        }
        execution_client
            .sync_to_target(highest_commit_cert.ledger_info().clone())
            .await?;

        // we do not need to update block_tree.highest_commit_decision_ledger_info here
        // because the block_tree is going to rebuild itself.

        let recovery_data = match storage.start(order_vote_enabled, window_size) {
            LivenessStorageData::FullRecoveryData(recovery_data) => recovery_data,
            _ => panic!("Failed to construct recovery data after fast forward sync"),
        };

        Ok(recovery_data)
    }
```

**File:** consensus/src/round_manager.rs (L877-907)
```rust
    /// Sync to the sync info sending from peer if it has newer certificates.
    async fn sync_up(&mut self, sync_info: &SyncInfo, author: Author) -> anyhow::Result<()> {
        let local_sync_info = self.block_store.sync_info();
        if sync_info.has_newer_certificates(&local_sync_info) {
            info!(
                self.new_log(LogEvent::ReceiveNewCertificate)
                    .remote_peer(author),
                "Local state {},\n remote state {}", local_sync_info, sync_info
            );
            // Some information in SyncInfo is ahead of what we have locally.
            // First verify the SyncInfo (didn't verify it in the yet).
            sync_info.verify(&self.epoch_state.verifier).map_err(|e| {
                error!(
                    SecurityEvent::InvalidSyncInfoMsg,
                    sync_info = sync_info,
                    remote_peer = author,
                    error = ?e,
                );
                VerifyError::from(e)
            })?;
            SYNC_INFO_RECEIVED_WITH_NEWER_CERT.inc();
            let result = self
                .block_store
                .add_certs(sync_info, self.create_block_retriever(author))
                .await;
            self.process_certificates().await?;
            result
        } else {
            Ok(())
        }
    }
```

**File:** consensus/src/round_manager.rs (L1568-1573)
```rust
            let highest_ordered_round = self.block_store.sync_info().highest_ordered_round();
            let order_vote_round = order_vote_msg.order_vote().ledger_info().round();
            let li_digest = order_vote_msg.order_vote().ledger_info().hash();
            if order_vote_round > highest_ordered_round
                && order_vote_round < highest_ordered_round + 100
            {
```

**File:** consensus/consensus-types/src/sync_info.rs (L125-127)
```rust
    pub fn highest_ordered_round(&self) -> Round {
        self.highest_ordered_cert().commit_info().round()
    }
```
