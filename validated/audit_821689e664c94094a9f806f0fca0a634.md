# Audit Report

## Title
Transaction Accumulator Frozen Subtree Gap in Replay-Verify Snapshot Restoration

## Summary

The `replay_verify.rs` coordinator captures `save_start_version` before updating `next_txn_version` based on snapshot version, causing `TransactionRestoreBatchController` to skip frozen subtree initialization when restoring a snapshot that's newer than existing database transactions. This leads to transaction accumulator failures when the database has partial transaction history.

## Finding Description

The vulnerability exists in the timing of when `save_start_version` is set relative to when `next_txn_version` is updated for snapshot-based restoration.

The problematic sequence occurs when:

1. **Database has partial history**: Database contains transactions 0-500, so `get_next_expected_transaction_version()` returns 501
2. **Snapshot available**: A state snapshot exists at version 1000  
3. **Line 155 captures old value**: `save_start_version = Some(501)` using the original `next_txn_version` [1](#0-0) 
4. **Line 157 updates version**: `next_txn_version` becomes 1001 (max of 501 and snapshot_version+1) [2](#0-1) 
5. **Snapshot is restored**: The snapshot at version 1000 is restored to database state [3](#0-2) 
6. **Frozen subtrees are skipped**: `TransactionRestoreBatchController` receives `first_version = Some(501)` [4](#0-3) 

When `first_version` is `Some(501)`, the critical `confirm_or_save_frozen_subtrees` call is skipped. This function establishes the Merkle accumulator frozen subtrees necessary for the transaction accumulator to function correctly. [5](#0-4) 

The code comment explicitly states that `first_version = Some()` should only be used "when we already finish first phase of db restore", but in replay_verify it's set BEFORE snapshot restoration completes, creating a mismatch. [6](#0-5) 

Later, when attempting to save transactions at version 1000, the `put_transaction_accumulator` operation calls `Accumulator::append` which requires reading frozen subtree nodes up to position 1000. [7](#0-6) 

The `Accumulator::append` function uses the `HashReader` trait to read existing frozen nodes. [8](#0-7) 

The `HashReader` implementation fails when positions don't exist in the database, returning "does not exist" error. [9](#0-8) 

This causes the entire restore operation to fail with "position does not exist" error, leaving the database in an inconsistent state (snapshot restored but transaction accumulator incomplete).

## Impact Explanation

**Medium Severity** per Aptos bug bounty criteria: "State inconsistencies requiring intervention"

The vulnerability causes:
- **Operational Disruption**: Replay-verify operations fail in specific but realistic scenarios
- **State Inconsistency**: Database left with state snapshot at version 1000 but transaction accumulator frozen subtrees only up to version 500
- **Recovery Complexity**: Manual intervention required to either reset database or manually establish frozen subtrees
- **No Data Corruption**: Due to atomic batch commits, no permanent corruption occurs
- **No Consensus Impact**: Failure happens during restore operations before any state commitment, preventing consensus splits

This breaks the **State Consistency** invariant by preventing proper Merkle proof verification when the accumulator structure is incomplete.

## Likelihood Explanation

**Medium Likelihood** - Requires specific conditions but realistic scenario:

**Triggering Conditions:**
1. Database with partial transaction history (e.g., from previous partial restore or pruning)
2. State snapshot available at version newer than database transactions  
3. Operator runs replay-verify spanning the version range

**Realistic Scenarios:**
- Node operator performing incremental restores across different snapshot versions
- Recovery after partial database corruption
- Testing/validation workflows using different backup sets
- Migration scenarios where old transactions are pruned but newer snapshots exist

The issue is deterministic once conditions are met - it will always fail, not intermittent.

## Recommendation

Fix the timing issue by capturing `save_start_version` AFTER the snapshot restoration is complete and AFTER `next_txn_version` is updated:

Move line 155 to after line 188 (after snapshot restoration), ensuring `save_start_version` reflects the post-snapshot state. Alternatively, set `save_start_version = None` when a snapshot is being restored, forcing `TransactionRestoreBatchController` to call `confirm_or_save_frozen_subtrees` to establish the correct frozen subtrees based on the transaction backup's range proof.

## Proof of Concept

The vulnerability can be reproduced by:
1. Setting up a database with transactions 0-500
2. Creating a state snapshot at version 1000
3. Running replay-verify with a transaction backup starting from version 1000
4. Observing the failure when `put_transaction_accumulator` attempts to read non-existent frozen subtree positions

The bug is deterministically triggered by the sequence described in the Finding Description section.

## Notes

This vulnerability is in the storage backup/restore infrastructure (`storage/backup/backup-cli/`), which is critical production infrastructure used by node operators. The bug specifically affects the replay-verify coordinator, which is used for validation and recovery operations. The timing mismatch between when `save_start_version` is captured and when the snapshot is restored creates an incorrect assumption about what frozen subtrees exist in the database, leading to transaction accumulator failures.

### Citations

**File:** storage/backup/backup-cli/src/coordinators/replay_verify.rs (L155-155)
```rust
        let save_start_version = (next_txn_version > 0).then_some(next_txn_version);
```

**File:** storage/backup/backup-cli/src/coordinators/replay_verify.rs (L157-157)
```rust
        next_txn_version = std::cmp::max(next_txn_version, snapshot_version.map_or(0, |v| v + 1));
```

**File:** storage/backup/backup-cli/src/coordinators/replay_verify.rs (L173-188)
```rust
        if !skip_snapshot {
            if let Some(backup) = state_snapshot {
                StateSnapshotRestoreController::new(
                    StateSnapshotRestoreOpt {
                        manifest_handle: backup.manifest,
                        version: backup.version,
                        validate_modules: self.validate_modules,
                        restore_mode: Default::default(),
                    },
                    global_opt.clone(),
                    Arc::clone(&self.storage),
                    None, /* epoch_history */
                )
                .run()
                .await?;
            }
```

**File:** storage/backup/backup-cli/src/coordinators/replay_verify.rs (L198-198)
```rust
            save_start_version,
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L306-312)
```rust
        // If first_version is None, we confirm and save frozen substrees to create a baseline
        // When first version is not None, it only happens when we already finish first phase of db restore and
        // we don't need to confirm and save frozen subtrees again.
        let first_version = self.first_version.unwrap_or(
            self.confirm_or_save_frozen_subtrees(&mut loaded_chunk_stream)
                .await?,
        );
```

**File:** storage/aptosdb/src/ledger_db/transaction_accumulator_db.rs (L108-126)
```rust
    pub fn put_transaction_accumulator(
        &self,
        first_version: Version,
        txn_infos: &[impl Borrow<TransactionInfo>],
        transaction_accumulator_batch: &mut SchemaBatch,
    ) -> Result<HashValue> {
        let txn_hashes: Vec<HashValue> = txn_infos.iter().map(|t| t.borrow().hash()).collect();

        let (root_hash, writes) = Accumulator::append(
            self,
            first_version, /* num_existing_leaves */
            &txn_hashes,
        )?;
        writes.iter().try_for_each(|(pos, hash)| {
            transaction_accumulator_batch.put::<TransactionAccumulatorSchema>(pos, hash)
        })?;

        Ok(root_hash)
    }
```

**File:** storage/aptosdb/src/ledger_db/transaction_accumulator_db.rs (L196-200)
```rust
    fn get(&self, position: Position) -> Result<HashValue, anyhow::Error> {
        self.db
            .get::<TransactionAccumulatorSchema>(&position)?
            .ok_or_else(|| anyhow!("{} does not exist.", position))
    }
```

**File:** storage/accumulator/src/lib.rs (L141-147)
```rust
    pub fn append(
        reader: &R,
        num_existing_leaves: LeafCount,
        new_leaves: &[HashValue],
    ) -> Result<(HashValue, Vec<Node>)> {
        MerkleAccumulatorView::<R, H>::new(reader, num_existing_leaves).append(new_leaves)
    }
```
