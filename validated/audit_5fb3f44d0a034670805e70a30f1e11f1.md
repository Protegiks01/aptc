# Audit Report

## Title
Epoch Snapshot Pruner Can Delete Required State Sync Data Due to Unbounded Progress Metadata

## Summary
The epoch snapshot pruner calculates its target version based on the latest state checkpoint version minus the prune window, without validating that this target doesn't exceed the latest epoch ending version. This architectural flaw allows the pruner to delete epoch ending snapshot data that are still needed for state synchronization, breaking the ability of new or recovering nodes to fast sync to recent epochs.

## Finding Description

The vulnerability exists in how the epoch snapshot pruner determines its target pruning version. The pruner is designed to maintain epoch ending snapshots for state sync, but it lacks bounds checking to ensure it doesn't prune beyond the latest epoch ending.

**Root Cause Flow:**

1. **Pruner Target Calculation Without Epoch Awareness**

When a state snapshot is committed, the epoch snapshot pruner's target is set by calling `maybe_set_pruner_target_db_version()` with the current state checkpoint version. [1](#0-0)  This current version represents the latest state checkpoint, not necessarily an epoch ending version.

2. **Unconstrained Target Version Calculation**

The pruner manager calculates the target as `latest_version.saturating_sub(self.prune_window)` without any validation against epoch boundaries. [2](#0-1)  This uses a simple subtraction without checking if the result would prune needed epoch ending snapshots.

3. **Epoch Ending Nodes Stored Separately**

Nodes from epoch ending snapshots are correctly identified and stored in `StaleNodeIndexCrossEpochSchema` when their version is at or before the previous epoch ending. [3](#0-2)  However, this classification alone doesn't protect them from premature pruning.

4. **Unconditional Deletion Based on Target**

The pruner retrieves all stale node indices where `stale_since_version <= target_version` and deletes them unconditionally. [4](#0-3) [5](#0-4)  No validation prevents this from deleting epoch ending snapshot nodes.

**Test Code Reveals the Bug:**

The test implementation shows the correct approach that production code should implement. [6](#0-5)  This explicitly takes the minimum of the first snapshot and first epoch snapshot to ensure the pruner never prunes beyond the oldest epoch snapshot that needs to be kept. However, this safeguard exists only in test code, not in production.

**Attack Scenario (Natural Occurrence):**

Consider a network where:
- Epochs are time-based (triggered when `timestamp - reconfiguration::last_reconfiguration_time() >= epoch_interval`) [7](#0-6) 
- An epoch runs longer than expected due to low transaction throughput or governance delays
- Epoch 3 ends at version 30M (latest completed epoch)
- Node continues processing transactions to version 120M (regular checkpoints occurring frequently)
- Pruner calculates target: 120M - 80M (prune_window) = 40M [8](#0-7) 
- Pruner deletes all stale nodes with `stale_since_version â‰¤ 40M`
- **This includes the epoch 3 snapshot at version 30M**
- New nodes attempting to fast sync to epoch 3 fail - required merkle tree nodes are missing

This violates the documented invariant that "complete state trees (or 'epoch snapshots') at the end of each recent epochs are available for peers to access, which is important for the health of the chain." [9](#0-8) 

## Impact Explanation

**High Severity** - This vulnerability causes significant protocol violations affecting network availability:

1. **State Sync Protocol Violation**: The configuration explicitly documents that epoch ending snapshots are used by state sync in fast sync mode. [10](#0-9)  Deleting these snapshots breaks the state sync protocol's fast sync capability, forcing nodes to fall back to slower transaction replay methods.

2. **Network Resilience Impact**: New validators cannot efficiently join the network, and existing validators that go offline cannot quickly rejoin if they need to sync from a pruned epoch. This reduces network resilience and increases barriers to participation, directly impacting decentralization.

3. **Operational Cost Increase**: Nodes must either maintain unnecessarily long history to avoid this issue, or risk extended downtime during recovery. This increases operational costs and complexity for validator operators.

The impact is **not** Critical because it does not directly cause consensus failure, funds loss, or permanent network partition. However, it meets **High Severity** criteria as a significant protocol violation that degrades validator operations and network availability.

## Likelihood Explanation

**Highly Likely** - This vulnerability occurs naturally under normal network operation:

1. **Architectural Mismatch**: Epochs are time-based (every `epoch_interval` microseconds, typically 7200 seconds = 2 hours) [7](#0-6) , while the prune window is version-based (80M versions). [8](#0-7)  This creates an inherent mismatch: at variable TPS, the number of versions per epoch varies, but the prune window remains fixed.

2. **Default Configuration Vulnerability**: The default prune window comment states it's designed for "~5K TPS * 2h/epoch * 2 epochs". [10](#0-9)  However, this assumes constant throughput and regular epoch transitions. Any deviation creates the vulnerability conditions.

3. **Checkpoint-Epoch Frequency Gap**: State checkpoints occur very frequently (potentially every block), while epochs occur every ~2 hours. The `current_version` used to set the pruner target naturally advances far beyond the latest epoch ending version, especially during long epochs.

4. **No Production Safeguards**: There are zero validation checks in the production code path [11](#0-10)  to prevent this scenario. The vulnerability triggers automatically when `(latest_checkpoint - prune_window) > some_older_epoch_ending`. The test code proves this safeguard should exist [6](#0-5) , but it's absent in production.

5. **Natural Triggering Conditions**: Networks with variable transaction throughput, governance-delayed reconfigurations, or epoch duration changes will naturally experience this issue. It requires no attacker action and occurs automatically through normal pruning operations.

## Recommendation

Implement the safeguard that exists in test code into the production code path. Specifically, modify the epoch snapshot pruner target calculation to validate against the oldest epoch ending version that needs to be preserved:

In `storage/aptosdb/src/state_store/state_merkle_batch_committer.rs`, replace the unconditional target setting with a minimum check against the latest epoch ending version within the prune window:

```rust
// Get the oldest epoch ending version that should be kept
let oldest_epoch_to_keep = get_oldest_epoch_ending_within_window(current_version, prune_window);

// Set target to the minimum to avoid pruning needed epoch snapshots
let safe_target = std::cmp::min(
    current_version,
    oldest_epoch_to_keep.unwrap_or(Version::MAX)
);

self.state_db
    .epoch_snapshot_pruner
    .maybe_set_pruner_target_db_version(safe_target);
```

This ensures the epoch snapshot pruner never prunes beyond epoch endings that are still within the configured prune window.

## Proof of Concept

The existing test code at `storage/aptosdb/src/db/aptosdb_test.rs` lines 298-301 demonstrates the correct behavior that should be in production. A PoC would involve:

1. Starting a node with default epoch snapshot pruner configuration (80M version window)
2. Running through multiple epochs with variable TPS
3. Allowing state checkpoints to advance significantly beyond the latest epoch ending
4. Observing that the pruner target exceeds an epoch ending version that should be preserved
5. Attempting state sync from that pruned epoch and observing failure

The vulnerability is evident from the code structure itself: production code lacks the validation that test code explicitly implements.

## Notes

This is a fundamental architectural flaw where:
- The pruner operates on version-based windows
- Epochs are time-based boundaries
- State checkpoints occur frequently
- No validation bridges these different coordinate systems

The test suite explicitly demonstrates the correct fix using `std::cmp::min()`, proving the developers were aware of the need for this validation but failed to implement it in the production code path.

### Citations

**File:** storage/aptosdb/src/state_store/state_merkle_batch_committer.rs (L97-98)
```rust
                        .epoch_snapshot_pruner
                        .maybe_set_pruner_target_db_version(current_version);
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_pruner_manager.rs (L67-72)
```rust
    fn maybe_set_pruner_target_db_version(&self, latest_version: Version) {
        let min_readable_version = self.get_min_readable_version();
        if self.is_pruner_enabled() && latest_version >= min_readable_version + self.prune_window {
            self.set_pruner_target_db_version(latest_version);
        }
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_pruner_manager.rs (L162-162)
```rust
        let min_readable_version = latest_version.saturating_sub(self.prune_window);
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L378-381)
```rust
            if previous_epoch_ending_version.is_some()
                && row.node_key.version() <= previous_epoch_ending_version.unwrap()
            {
                batch.put::<StaleNodeIndexCrossEpochSchema>(row, &())
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L208-209)
```rust
                if index.stale_since_version <= target_version {
                    indices.push(index);
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs (L61-64)
```rust
        indices.into_iter().try_for_each(|index| {
            batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
            batch.delete::<S>(&index)
        })?;
```

**File:** storage/aptosdb/src/db/aptosdb_test.rs (L298-301)
```rust
        epoch_snapshot_pruner.set_worker_target_version(std::cmp::min(
            *snapshots.first().unwrap(),
            *epoch_snapshots.first().unwrap_or(&Version::MAX),
        ));
```

**File:** aptos-move/framework/aptos-framework/sources/block.move (L244-246)
```text
        if (timestamp - reconfiguration::last_reconfiguration_time() >= epoch_interval) {
            reconfiguration_with_dkg::try_start();
        };
```

**File:** config/src/config/storage_config.rs (L419-420)
```rust
            // This is based on ~5K TPS * 2h/epoch * 2 epochs. -- epoch ending snapshots are used
            // by state sync in fast sync mode.
```

**File:** config/src/config/storage_config.rs (L425-425)
```rust
            prune_window: 80_000_000,
```

**File:** storage/README.md (L117-119)
```markdown
    # guarantees complete state trees (or "epoch snapshots") at the end of
    # each recent epochs are available for peers to access, which is important
    # for the health of the chain.
```
