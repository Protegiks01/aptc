# Audit Report

## Title
Secret Share Manager Not Reset During State Synchronization Leading to State Inconsistency and Liveness Issues

## Summary
The `ExecutionProxyClient::reset()` method fails to send reset signals to the `SecretShareManager` during state synchronization, while correctly resetting the `RandManager` and `BufferManager`. This causes the secret share manager to retain stale blocks and outdated state after a node syncs to a new ledger info, leading to state inconsistencies between pipeline components and potential liveness failures.

## Finding Description

The vulnerability exists in the coordination between multiple consensus pipeline managers during state synchronization. When a validator node falls behind and must sync to catch up, the `reset()` method is called to clear internal state and prepare for processing blocks from the new synced position. [1](#0-0) 

The `reset()` method retrieves only `reset_tx_to_rand_manager` and `reset_tx_to_buffer_manager` channels to send reset signals, but completely ignores `reset_tx_to_secret_share_manager`, which exists in the `BufferManagerHandle` structure: [2](#0-1) 

The `BufferManagerHandle::reset()` method returns all three reset channels including the secret share manager channel: [3](#0-2) 

However, when the vulnerable `ExecutionProxyClient::reset()` extracts channels from the handle, it only destructures two of the three returned values, ignoring the secret share manager channel entirely.

When both randomness and secret sharing are enabled (standard configuration for on-chain randomness), the system creates a coordinator that requires BOTH the rand manager AND secret share manager to signal readiness before forwarding blocks for execution: [4](#0-3) 

The coordinator tracks readiness flags for both managers and only forwards blocks when both `rand_ready` (index 1) and `secret_ready` (index 2) are true (line 357).

The `SecretShareManager` maintains a `BlockQueue` and `highest_known_round` that must be reset when syncing to a new state. The `process_reset()` method exists for this purpose: [5](#0-4) 

The reset mechanism is functional and processes reset requests in the main event loop: [6](#0-5) 

However, since the `ExecutionProxyClient::reset()` method never sends a reset signal to the secret share manager, this critical state cleanup never occurs during state sync operations.

Both state sync methods call the vulnerable reset: [7](#0-6) 

In contrast, the `end_epoch()` method correctly resets all three managers: [8](#0-7) 

**Attack Scenario:**

1. A validator node is running with both `rand_config` and `secret_sharing_config` enabled (created when both configs are present)
2. The node falls behind consensus (network issues, temporary unavailability, or during initial sync)
3. The node calls `sync_to_target()` or `sync_for_duration()` to catch up
4. These methods call `ExecutionProxyClient::reset()` 
5. The `reset()` method resets the rand manager and buffer manager but NOT the secret share manager
6. The secret share manager retains stale blocks in its queue and outdated `highest_known_round`
7. When new blocks arrive after the sync, the coordinator sends them to both managers
8. The secret share manager's stale queue prevents new blocks from being marked ready
9. The coordinator waits indefinitely for the secret share manager to signal readiness
10. Block execution halts, causing validator liveness failure

This breaks the **State Consistency** invariant: different pipeline components have conflicting views of the current blockchain state.

## Impact Explanation

This vulnerability qualifies as **High Severity** based on Aptos bug bounty criteria:

**Validator Node Slowdowns**: Nodes experiencing this issue will fail to execute blocks until the epoch ends, when `end_epoch()` properly resets all managers. During this period, the affected validator cannot participate in consensus, reducing network capacity.

**Significant Protocol Violations**: The state inconsistency between managers violates the fundamental assumption that all pipeline components maintain synchronized state. This leads to:
- Blocks stuck in the secret share manager queue indefinitely
- The coordinator blocking execution waiting for readiness signals that never arrive
- Reduced validator participation affecting network throughput

**Network-Wide Impact**: If multiple validators sync simultaneously (common during network partitions or coordinated upgrades), this could cause widespread liveness degradation across the validator set.

The issue does not reach Critical Severity because:
- It's recoverable at epoch boundaries
- It doesn't directly cause fund loss or consensus safety violations
- It requires state sync to trigger

## Likelihood Explanation

This vulnerability has **HIGH likelihood** of occurring in production:

**Frequent Trigger Condition**: State synchronization is a common occurrence when:
- New validators join the network
- Validators restart after maintenance
- Validators experience temporary network issues
- Validators fall behind during high network load

**Configuration Requirements**: The vulnerability only manifests when both `rand_config` and `secret_sharing_config` are enabled, which is the standard configuration for networks using on-chain randomness (Aptos mainnet). [9](#0-8) 

**No Special Attacker Capabilities**: The bug triggers automatically during normal state sync operations without requiring any attacker intervention or special network conditions.

**Observable Impact**: Validators experiencing this issue will show clear symptoms (inability to execute blocks, growing queue sizes) making it easy to detect but difficult to recover from without epoch transition.

## Recommendation

Modify the `ExecutionProxyClient::reset()` method to retrieve and send reset signals to all three managers, including the secret share manager:

```rust
async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
    let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager, reset_tx_to_secret_share_manager) = {
        let handle = self.handle.read();
        (
            handle.reset_tx_to_rand_manager.clone(),
            handle.reset_tx_to_buffer_manager.clone(),
            handle.reset_tx_to_secret_share_manager.clone(),
        )
    };

    if let Some(mut reset_tx) = reset_tx_to_rand_manager {
        let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::RandResetDropped)?;
        ack_rx.await.map_err(|_| Error::RandResetDropped)?;
    }

    if let Some(mut reset_tx) = reset_tx_to_secret_share_manager {
        let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::SecretShareResetDropped)?;
        ack_rx.await.map_err(|_| Error::SecretShareResetDropped)?;
    }

    if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
        let (tx, rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::ResetDropped)?;
        rx.await.map_err(|_| Error::ResetDropped)?;
    }

    Ok(())
}
```

## Proof of Concept

The vulnerability can be demonstrated by:

1. Starting a validator with both `rand_config` and `secret_sharing_config` enabled
2. Allowing the validator to process several blocks
3. Triggering state sync by calling `sync_to_target()` with a new ledger info
4. Observing that the secret share manager still has stale blocks in its queue
5. Sending new blocks and observing that the coordinator blocks waiting for `secret_ready` flag
6. Confirming that block execution halts until epoch transition calls `end_epoch()`

The code structure clearly shows the missing reset logic compared to the correct implementation in `end_epoch()`, making this a logic vulnerability with deterministic triggering during state synchronization operations.

### Citations

**File:** consensus/src/pipeline/execution_client.rs (L124-131)
```rust
struct BufferManagerHandle {
    pub execute_tx: Option<UnboundedSender<OrderedBlocks>>,
    pub commit_tx:
        Option<aptos_channel::Sender<AccountAddress, (AccountAddress, IncomingCommitRequest)>>,
    pub reset_tx_to_buffer_manager: Option<UnboundedSender<ResetRequest>>,
    pub reset_tx_to_rand_manager: Option<UnboundedSender<ResetRequest>>,
    pub reset_tx_to_secret_share_manager: Option<UnboundedSender<ResetRequest>>,
}
```

**File:** consensus/src/pipeline/execution_client.rs (L159-177)
```rust
    pub fn reset(
        &mut self,
    ) -> (
        Option<UnboundedSender<ResetRequest>>,
        Option<UnboundedSender<ResetRequest>>,
        Option<UnboundedSender<ResetRequest>>,
    ) {
        let reset_tx_to_rand_manager = self.reset_tx_to_rand_manager.take();
        let reset_tx_to_buffer_manager = self.reset_tx_to_buffer_manager.take();
        let reset_tx_to_secret_share_manager = self.reset_tx_to_secret_share_manager.take();
        self.execute_tx = None;
        self.commit_tx = None;
        (
            reset_tx_to_rand_manager,
            reset_tx_to_buffer_manager,
            reset_tx_to_secret_share_manager,
        )
    }
}
```

**File:** consensus/src/pipeline/execution_client.rs (L311-365)
```rust
    fn make_coordinator(
        mut rand_manager_input_tx: UnboundedSender<OrderedBlocks>,
        mut rand_ready_block_rx: UnboundedReceiver<OrderedBlocks>,
        mut secret_share_manager_input_tx: UnboundedSender<OrderedBlocks>,
        mut secret_ready_block_rx: UnboundedReceiver<OrderedBlocks>,
    ) -> (
        UnboundedSender<OrderedBlocks>,
        futures_channel::mpsc::UnboundedReceiver<OrderedBlocks>,
    ) {
        let (ordered_block_tx, mut ordered_block_rx) = unbounded::<OrderedBlocks>();
        let (mut ready_block_tx, ready_block_rx) = unbounded::<OrderedBlocks>();

        tokio::spawn(async move {
            let mut inflight_block_tracker: HashMap<
                HashValue,
                (
                    OrderedBlocks,
                    /* rand_ready */ bool,
                    /* secret ready */ bool,
                ),
            > = HashMap::new();
            loop {
                let entry = select! {
                    Some(ordered_blocks) = ordered_block_rx.next() => {
                        let _ = rand_manager_input_tx.send(ordered_blocks.clone()).await;
                        let _ = secret_share_manager_input_tx.send(ordered_blocks.clone()).await;
                        let first_block_id = ordered_blocks.ordered_blocks.first().expect("Cannot be empty").id();
                        inflight_block_tracker.insert(first_block_id, (ordered_blocks, false, false));
                        inflight_block_tracker.entry(first_block_id)
                    },
                    Some(rand_ready_block) = rand_ready_block_rx.next() => {
                        let first_block_id = rand_ready_block.ordered_blocks.first().expect("Cannot be empty").id();
                        inflight_block_tracker.entry(first_block_id).and_modify(|result| {
                            result.1 = true;
                        })
                    },
                    Some(secret_ready_block) = secret_ready_block_rx.next() => {
                        let first_block_id = secret_ready_block.ordered_blocks.first().expect("Cannot be empty").id();
                        inflight_block_tracker.entry(first_block_id).and_modify(|result| {
                            result.2 = true;
                        })
                    },
                };
                let Entry::Occupied(o) = entry else {
                    unreachable!("Entry must exist");
                };
                if o.get().1 && o.get().2 {
                    let (_, (ordered_blocks, _, _)) = o.remove_entry();
                    let _ = ready_block_tx.send(ordered_blocks).await;
                }
            }
        });

        (ordered_block_tx, ready_block_rx)
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L400-436)
```rust
            (Some(rand_config), Some(secret_sharing_config)) => {
                let (rand_manager_input_tx, rand_ready_block_rx, reset_tx_to_rand_manager) = self
                    .make_rand_manager(
                        &epoch_state,
                        fast_rand_config,
                        rand_msg_rx,
                        highest_committed_round,
                        &network_sender,
                        rand_config,
                        consensus_sk,
                    );

                let (
                    secret_share_manager_input_tx,
                    secret_ready_block_rx,
                    reset_tx_to_secret_share_manager,
                ) = self.make_secret_sharing_manager(
                    &epoch_state,
                    secret_sharing_config,
                    secret_sharing_msg_rx,
                    highest_committed_round,
                    &network_sender,
                );

                let (ordered_block_tx, ready_block_rx) = Self::make_coordinator(
                    rand_manager_input_tx,
                    rand_ready_block_rx,
                    secret_share_manager_input_tx,
                    secret_ready_block_rx,
                );

                (
                    ordered_block_tx,
                    ready_block_rx,
                    Some(reset_tx_to_rand_manager),
                    Some(reset_tx_to_secret_share_manager),
                )
```

**File:** consensus/src/pipeline/execution_client.rs (L644-672)
```rust
        duration: Duration,
    ) -> Result<LedgerInfoWithSignatures, StateSyncError> {
        fail_point!("consensus::sync_for_duration", |_| {
            Err(anyhow::anyhow!("Injected error in sync_for_duration").into())
        });

        // Sync for the specified duration
        let result = self.execution_proxy.sync_for_duration(duration).await;

        // Reset the rand and buffer managers to the new synced round
        if let Ok(latest_synced_ledger_info) = &result {
            self.reset(latest_synced_ledger_info).await?;
        }

        result
    }

    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });

        // Reset the rand and buffer managers to the target round
        self.reset(&target).await?;

        // TODO: handle the state sync error (e.g., re-push the ordered
        // blocks to the buffer manager when it's reset but sync fails).
        self.execution_proxy.sync_to_target(target).await
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L674-709)
```rust
    async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
        let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager) = {
            let handle = self.handle.read();
            (
                handle.reset_tx_to_rand_manager.clone(),
                handle.reset_tx_to_buffer_manager.clone(),
            )
        };

        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        }

        if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
            // reset execution phase and commit phase
            let (tx, rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::ResetDropped)?;
            rx.await.map_err(|_| Error::ResetDropped)?;
        }

        Ok(())
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L711-760)
```rust
    async fn end_epoch(&self) {
        let (
            reset_tx_to_rand_manager,
            reset_tx_to_buffer_manager,
            reset_tx_to_secret_share_manager,
        ) = {
            let mut handle = self.handle.write();
            handle.reset()
        };

        if let Some(mut tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop rand manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop rand manager");
        }

        if let Some(mut tx) = reset_tx_to_secret_share_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop secret share manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop secret share manager");
        }

        if let Some(mut tx) = reset_tx_to_buffer_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop buffer manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop buffer manager");
        }
        self.execution_proxy.end_epoch();
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L172-184)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.secret_share_store
            .lock()
            .update_highest_known_round(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L324-378)
```rust
    pub async fn start(
        mut self,
        mut incoming_blocks: Receiver<OrderedBlocks>,
        incoming_rpc_request: aptos_channel::Receiver<Author, IncomingSecretShareRequest>,
        mut reset_rx: Receiver<ResetRequest>,
        bounded_executor: BoundedExecutor,
        highest_known_round: Round,
    ) {
        info!("SecretShareManager started");
        let (verified_msg_tx, mut verified_msg_rx) = unbounded();
        let epoch_state = self.epoch_state.clone();
        let dec_config = self.config.clone();
        {
            self.secret_share_store
                .lock()
                .update_highest_known_round(highest_known_round);
        }
        spawn_named!(
            "Secret Share Manager Verification Task",
            Self::verification_task(
                epoch_state,
                incoming_rpc_request,
                verified_msg_tx,
                dec_config,
                bounded_executor,
            )
        );

        let mut interval = tokio::time::interval(Duration::from_millis(5000));
        while !self.stop {
            tokio::select! {
                Some(blocks) = incoming_blocks.next() => {
                    self.process_incoming_blocks(blocks).await;
                }
                Some(reset) = reset_rx.next() => {
                    while matches!(incoming_blocks.try_next(), Ok(Some(_))) {}
                    self.process_reset(reset);
                }
                Some(secret_shared_key) = self.decision_rx.next() => {
                    self.process_aggregated_key(secret_shared_key);
                }
                Some(request) = verified_msg_rx.next() => {
                    self.handle_incoming_msg(request);
                }
                _ = interval.tick().fuse() => {
                    self.observe_queue();
                },
            }
            let maybe_ready_blocks = self.block_queue.dequeue_ready_prefix();
            if !maybe_ready_blocks.is_empty() {
                self.process_ready_blocks(maybe_ready_blocks);
            }
        }
        info!("SecretShareManager stopped");
    }
```
