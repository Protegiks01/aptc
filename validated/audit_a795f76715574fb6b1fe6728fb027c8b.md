# Audit Report

## Title
SystemTime Clock Regression Causes Mempool Coordinator Panic and Node Crash

## Summary
The mempool broadcast acknowledgment system uses non-monotonic `SystemTime` for RTT calculation, which panics when the system clock regresses between sending a broadcast and receiving its ACK. This causes immediate node termination via the crash handler, resulting in complete loss of availability.

## Finding Description
The mempool's broadcast acknowledgment system has a critical design flaw where it uses `SystemTime` (wall-clock time) instead of `Instant` (monotonic clock) for measuring round-trip time (RTT) of broadcast messages.

**Execution Path:**

1. When a broadcast is sent, `SystemTime::now()` is captured and stored as the send time: [1](#0-0) 

2. This timestamp is stored in the `BroadcastInfo.sent_messages` map, which is defined as: [2](#0-1) 

3. When a broadcast ACK is received in the coordinator, the current time is captured using `SystemTime::now()`: [3](#0-2) 

4. The RTT is calculated by calling `duration_since()` with `.expect()`, which panics on error: [4](#0-3) 

5. If the system clock has regressed backward between sending and receiving the ACK, `duration_since()` returns an error (since the ACK timestamp is earlier than the sent timestamp), causing the `.expect()` to panic with the message "failed to calculate mempool broadcast RTT".

6. The panic is caught by the global crash handler which logs the crash and exits the entire process with exit code 12: [5](#0-4) 

The crash handler is installed at node startup: [6](#0-5) 

**Clock Regression Scenarios:**
- NTP synchronization performing backward step adjustments (typically for drifts >128ms)
- Virtual machine or container time synchronization with host
- Manual system clock adjustments by operators
- Leap second adjustments

**Design Inconsistency:**
The codebase correctly uses `Instant` (monotonic clock) for broadcast scheduling and latency tracking: [7](#0-6) 

However, it incorrectly uses `SystemTime` for RTT tracking across message sends and receives.

## Impact Explanation
**High Severity** - This vulnerability causes complete node unavailability, meeting the "API crashes" criteria in the Aptos bug bounty High severity category (up to $50,000):

- **Complete Node Crash**: The panic triggers `process::exit(12)`, terminating the entire node process
- **Loss of All Services**: The node immediately stops participating in consensus, stops processing transactions, and all APIs become unavailable
- **Manual Restart Required**: The node does not recover automatically and requires operator intervention to restart
- **No Data Corruption**: The node state remains intact on disk and can resume after manual restart

While individual nodes can be restarted, if multiple validators experience clock adjustments simultaneously (e.g., during NTP server changes or leap second events), it could cause significant network disruption affecting consensus participation.

## Likelihood Explanation
**Medium Likelihood** - This vulnerability can be triggered naturally without attacker involvement, though the probability of occurrence during the specific broadcast window is moderate:

**Natural Triggers:**
1. **NTP Synchronization**: Modern NTP implementations typically use "slew" mode for gradual adjustments, but use "step" mode for larger clock drifts (>128ms typically), causing backwards jumps
2. **VM/Container Time Sync**: Virtual machines and containers can experience abrupt time synchronization with their hosts, especially after VM migration or resume operations
3. **Manual Clock Changes**: System administrators may manually adjust clocks during maintenance
4. **Leap Seconds**: While rare, leap second events can cause time discontinuities

**Triggering Requirements:**
- Clock regression must occur during the 2-second ACK timeout window while a broadcast is awaiting acknowledgment: [8](#0-7) 
- Any clock regression during this window will trigger the panic

The likelihood is medium because while clock regressions do occur naturally in production environments (especially in virtualized infrastructure), they must coincide with the specific 2-second window when broadcasts are pending acknowledgment.

## Recommendation
Replace `SystemTime` with `Instant` for RTT tracking to ensure monotonic time measurement that is immune to system clock adjustments.

**Recommended Fix:**

1. Change `BroadcastInfo.sent_messages` to use `Instant` instead of `SystemTime`:
```rust
// In mempool/src/shared_mempool/types.rs
pub struct BroadcastInfo {
    pub sent_messages: BTreeMap<MempoolMessageId, Instant>, // Changed from SystemTime
    // ... rest of fields
}
```

2. Update the send time capture to use `Instant`:
```rust
// In mempool/src/shared_mempool/network.rs
let send_time = Instant::now(); // Changed from SystemTime::now()
```

3. Update the ACK timestamp capture to use `Instant`:
```rust
// In mempool/src/shared_mempool/coordinator.rs
let ack_timestamp = Instant::now(); // Changed from SystemTime::now()
```

4. Update the `process_broadcast_ack` function signature:
```rust
pub fn process_broadcast_ack(
    &self,
    peer: PeerNetworkId,
    message_id: MempoolMessageId,
    retry: bool,
    backoff: bool,
    timestamp: Instant, // Changed from SystemTime
)
```

5. The RTT calculation will now use `Instant::duration_since()`, which returns a `Duration` directly (not a `Result`) and cannot fail due to monotonic time guarantees.

## Proof of Concept
To reproduce this vulnerability:

1. Set up an Aptos validator node with the current codebase
2. Ensure the node is actively broadcasting mempool transactions to peers
3. While a broadcast is pending acknowledgment (within the 2-second timeout window), manually adjust the system clock backward by any amount (e.g., `sudo date -s "1 minute ago"`)
4. When the ACK is received, the node will panic with "failed to calculate mempool broadcast RTT" and exit with code 12
5. Observe that the node requires manual restart to resume operations

Alternatively, in a test environment with VMs, trigger a VM time synchronization event while broadcasts are pending to observe the same crash behavior.

## Notes
- This vulnerability affects node availability but does not compromise consensus safety, fund security, or data integrity
- The state remains intact on disk, allowing clean recovery after restart
- The fix is straightforward and follows the pattern already used elsewhere in the codebase for broadcast scheduling
- Modern production deployments should use NTP slew mode to minimize the risk, but this does not eliminate the vulnerability entirely, especially in virtualized environments

### Citations

**File:** mempool/src/shared_mempool/network.rs (L316-318)
```rust
            let rtt = timestamp
                .duration_since(sent_timestamp)
                .expect("failed to calculate mempool broadcast RTT");
```

**File:** mempool/src/shared_mempool/network.rs (L647-647)
```rust
        let send_time = SystemTime::now();
```

**File:** mempool/src/shared_mempool/types.rs (L459-459)
```rust
    pub sent_messages: BTreeMap<MempoolMessageId, SystemTime>,
```

**File:** mempool/src/shared_mempool/coordinator.rs (L396-396)
```rust
                    let ack_timestamp = SystemTime::now();
```

**File:** crates/crash-handler/src/lib.rs (L57-57)
```rust
    process::exit(12);
```

**File:** aptos-node/src/lib.rs (L234-234)
```rust
    aptos_crash_handler::setup_panic_handler();
```

**File:** mempool/src/shared_mempool/tasks.rs (L117-117)
```rust
        Instant::now() + Duration::from_millis(interval_ms),
```

**File:** config/src/config/mempool_config.rs (L115-115)
```rust
            shared_mempool_ack_timeout_ms: 2_000,
```
