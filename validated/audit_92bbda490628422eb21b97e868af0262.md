# Audit Report

## Title
Transaction Backup Chunk Replay Attack: Frankenstein Backups Pass Verification Without Cross-Chunk Consistency Checks

## Summary
Individual `TransactionChunk` objects from different backups can be combined to create "Frankenstein" backups that pass verification but contain inconsistent transaction sequences. The verification process validates each chunk independently against its own `LedgerInfoWithSignatures` without checking that consecutive chunks' transaction accumulators are cryptographically consistent with each other.

## Finding Description

The vulnerability exists in the backup verification flow where chunks are validated independently without cross-chunk accumulator consistency checks.

**Critical Missing Validation:**

The `TransactionBackup::verify()` function only checks structural continuity of version numbers, ensuring no gaps exist between chunk ranges, but does not validate cryptographic consistency between chunks. [1](#0-0) 

**Independent Chunk Verification:**

Each chunk is loaded and verified independently against its own `LedgerInfoWithSignatures`. During chunk loading, the proof verification at line 167 only checks that transactions in the current chunk match their `TransactionInfos` and are proven by this chunk's `LedgerInfo`, without validating that consecutive chunks' transaction accumulators form a valid chain. [2](#0-1) 

**Verify Mode Has No Cross-Chunk Validation:**

In verify mode, the `go_through_verified_chunks()` function simply iterates through loaded chunks without any accumulator consistency checks between them. [3](#0-2) 

**Missing Accumulator Consistency Check:**

Each chunk's `TransactionAccumulatorRangeProof` contains `left_siblings` representing the frozen subtree roots before that chunk. For legitimate consecutive chunks, chunk[i+1]'s `left_siblings` should match the accumulator state after chunk[i]. The codebase has a `verify_extends_ledger()` method that performs this check using left_siblings, but it is never called during backup verification. [4](#0-3) 

**Limited Frozen Subtree Validation:**

The only place frozen subtrees are checked is during actual database restoration via `confirm_or_save_frozen_subtrees()`, which only runs in Restore mode (not Verify mode) and only validates the first chunk. [5](#0-4) 

**Attack Scenario:**

1. Attacker obtains two valid backups from different chain states (e.g., different forks or time periods):
   - Backup A: Chunk with versions 0-99, proven against LedgerInfo_A
   - Backup B: Chunk with versions 100-199, proven against LedgerInfo_B

2. Attacker creates a manifest combining both chunks with continuous version ranges

3. During verification:
   - `manifest.verify()` checks version continuity → **PASS**
   - Chunk A verifies against LedgerInfo_A → **PASS**
   - Chunk B verifies against LedgerInfo_B → **PASS**
   - **Missing**: No check that Chunk B's `left_siblings` matches the accumulator state after Chunk A

4. Result: A backup that passes all verification checks but contains inconsistent transaction history from different chain states

## Impact Explanation

**Severity: MEDIUM to HIGH** (per Aptos Bug Bounty criteria: "Limited protocol violations" to "Significant protocol violations")

This vulnerability undermines the integrity of the backup/restore system:

1. **Backup Verification Bypass**: Operators using verification tools receive false positives, believing corrupted backups are valid
2. **State Inconsistency Risk**: If a Frankenstein backup is restored, it creates an invalid chain state requiring manual intervention
3. **Disaster Recovery Failure**: In critical recovery scenarios, operators might unknowingly restore from compromised backups, corrupting the database
4. **Chain History Manipulation**: Attackers with backup storage access can craft backups combining transactions from different forks or inconsistent states

The vulnerability breaks the state consistency invariant: while individual chunks have valid Merkle proofs against their respective LedgerInfos, the combined backup represents an invalid state transition sequence that was never committed on any valid chain.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

**Attacker Requirements:**
- Access to multiple backup files (common in shared backup storage environments)
- Ability to create/modify manifest files
- No privileged validator access required
- No cryptographic breaking capabilities needed

**Realistic Scenarios:**
1. **Intentional Attack**: Malicious actor with backup storage access mixes chunks to create poisoned backups
2. **Accidental Mixing**: Operator error during backup management could inadvertently combine chunks from different sources (e.g., mixing testnet and mainnet backups, or backups from different time periods)
3. **Supply Chain Attack**: Compromised backup tooling could generate mixed backups
4. **Disaster Recovery**: During emergency recovery, operators might hastily combine backup fragments without realizing they're from inconsistent sources

The attack is straightforward to execute and requires no sophisticated technical capabilities beyond file system access, making it a realistic threat in operational environments.

## Recommendation

Implement cross-chunk accumulator consistency validation by:

1. **In Verify Mode**: After loading each chunk beyond the first, validate that the chunk's `left_siblings` (from `TransactionAccumulatorRangeProof`) matches the expected accumulator state from the previous chunk. This can be done by:
   - Computing the accumulator root after processing chunk[i]
   - Reconstructing chunk[i+1]'s expected frozen subtrees from chunk[i]'s final state
   - Comparing these with chunk[i+1]'s actual `left_siblings`

2. **In Restore Mode**: Extend the `confirm_or_save_frozen_subtrees()` check to all chunks, not just the first one

3. **Leverage Existing Method**: Use the `verify_extends_ledger()` method which already implements accumulator consistency checking via left_siblings validation

```rust
// In TransactionRestoreBatchController::loaded_chunk_stream()
// After line 400, add accumulator consistency check:
let mut prev_accumulator_root: Option<HashValue> = None;

chunk_manifest_stream
    .and_then(move |chunk| {
        let storage = storage.clone();
        let epoch_history = epoch_history.clone();
        let prev_root = prev_accumulator_root;
        async move {
            let loaded_chunk = LoadedChunk::load(chunk, &storage, epoch_history.as_ref()).await?;
            
            // Validate accumulator consistency with previous chunk
            if let Some(expected_root) = prev_root {
                loaded_chunk.validate_accumulator_consistency(expected_root)?;
            }
            
            // Update accumulator root for next chunk validation
            prev_accumulator_root = Some(loaded_chunk.compute_accumulator_root());
            
            Ok(loaded_chunk)
        }
    })
```

## Proof of Concept

The vulnerability can be demonstrated by:

1. Creating two separate transaction backups from different chain states (e.g., fork scenarios or different time periods)
2. Extracting chunks from each backup with consecutive version ranges
3. Creating a new manifest that references chunks from both backups
4. Running the backup verification tool, which will pass all checks despite the chunks being from inconsistent chain states
5. Attempting restoration would write inconsistent accumulator data to the database

A complete PoC would require setting up two backup sources and demonstrating that the combined manifest passes `TransactionRestoreBatchController` verification in `RestoreRunMode::Verify` mode without detecting the accumulator inconsistency.

## Notes

The vulnerability is particularly concerning because:

1. The codebase already has the necessary validation logic in `verify_extends_ledger()` that checks accumulator consistency via `left_siblings`, but this method is not called during backup verification
2. The check for frozen subtrees exists but only applies to the first chunk in Restore mode, not in Verify mode
3. Operators may rely on backup verification tools to validate backup integrity before disaster recovery scenarios
4. The `put_transaction_accumulator()` database method blindly appends transactions without checking that the existing accumulator state matches expectations, so corrupted backups could successfully write to the database [6](#0-5)

### Citations

**File:** storage/backup/backup-cli/src/backup_types/transaction/manifest.rs (L50-88)
```rust
    pub fn verify(&self) -> Result<()> {
        // check number of waypoints
        ensure!(
            self.first_version <= self.last_version,
            "Bad version range: [{}, {}]",
            self.first_version,
            self.last_version,
        );

        // check chunk ranges
        ensure!(!self.chunks.is_empty(), "No chunks.");

        let mut next_version = self.first_version;
        for chunk in &self.chunks {
            ensure!(
                chunk.first_version == next_version,
                "Chunk ranges not continuous. Expected first version: {}, actual: {}.",
                next_version,
                chunk.first_version,
            );
            ensure!(
                chunk.last_version >= chunk.first_version,
                "Chunk range invalid. [{}, {}]",
                chunk.first_version,
                chunk.last_version,
            );
            next_version = chunk.last_version + 1;
        }

        // check last version in chunk matches manifest
        ensure!(
            next_version - 1 == self.last_version, // okay to -1 because chunks is not empty.
            "Last version in chunks: {}, in manifest: {}",
            next_version - 1,
            self.last_version,
        );

        Ok(())
    }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L100-186)
```rust
    async fn load(
        manifest: TransactionChunk,
        storage: &Arc<dyn BackupStorage>,
        epoch_history: Option<&Arc<EpochHistory>>,
    ) -> Result<Self> {
        let mut file = BufReader::new(storage.open_for_read(&manifest.transactions).await?);
        let mut txns = Vec::new();
        let mut persisted_aux_info = Vec::new();
        let mut txn_infos = Vec::new();
        let mut event_vecs = Vec::new();
        let mut write_sets = Vec::new();

        while let Some(record_bytes) = file.read_record_bytes().await? {
            let (txn, aux_info, txn_info, events, write_set): (
                _,
                PersistedAuxiliaryInfo,
                _,
                _,
                WriteSet,
            ) = match manifest.format {
                TransactionChunkFormat::V0 => {
                    let (txn, txn_info, events, write_set) = bcs::from_bytes(&record_bytes)?;
                    (
                        txn,
                        PersistedAuxiliaryInfo::None,
                        txn_info,
                        events,
                        write_set,
                    )
                },
                TransactionChunkFormat::V1 => bcs::from_bytes(&record_bytes)?,
            };
            txns.push(txn);
            persisted_aux_info.push(aux_info);
            txn_infos.push(txn_info);
            event_vecs.push(events);
            write_sets.push(write_set);
        }

        ensure!(
            manifest.first_version + (txns.len() as Version) == manifest.last_version + 1,
            "Number of items in chunks doesn't match that in manifest. first_version: {}, last_version: {}, items in chunk: {}",
            manifest.first_version,
            manifest.last_version,
            txns.len(),
        );

        let (range_proof, ledger_info) = storage
            .load_bcs_file::<(TransactionAccumulatorRangeProof, LedgerInfoWithSignatures)>(
                &manifest.proof,
            )
            .await?;
        if let Some(epoch_history) = epoch_history {
            epoch_history.verify_ledger_info(&ledger_info)?;
        }

        // make a `TransactionListWithProof` to reuse its verification code.
        let txn_list_with_proof =
            TransactionListWithProofV2::new(TransactionListWithAuxiliaryInfos::new(
                TransactionListWithProof::new(
                    txns,
                    Some(event_vecs),
                    Some(manifest.first_version),
                    TransactionInfoListWithProof::new(range_proof, txn_infos),
                ),
                persisted_aux_info,
            ));
        txn_list_with_proof.verify(ledger_info.ledger_info(), Some(manifest.first_version))?;
        // and disassemble it to get things back.
        let (txn_list_with_proof, persisted_aux_info) = txn_list_with_proof.into_parts();
        let txns = txn_list_with_proof.transactions;
        let range_proof = txn_list_with_proof
            .proof
            .ledger_info_to_transaction_infos_proof;
        let txn_infos = txn_list_with_proof.proof.transaction_infos;
        let event_vecs = txn_list_with_proof.events.expect("unknown to be Some.");

        Ok(Self {
            manifest,
            txns,
            persisted_aux_info,
            txn_infos,
            event_vecs,
            range_proof,
            write_sets,
        })
    }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L403-422)
```rust
    async fn confirm_or_save_frozen_subtrees(
        &self,
        loaded_chunk_stream: &mut Peekable<impl Unpin + Stream<Item = Result<LoadedChunk>>>,
    ) -> Result<Version> {
        let first_chunk = Pin::new(loaded_chunk_stream)
            .peek()
            .await
            .ok_or_else(|| anyhow!("LoadedChunk stream is empty."))?
            .as_ref()
            .map_err(|e| anyhow!("Error: {}", e))?;

        if let RestoreRunMode::Restore { restore_handler } = self.global_opt.run_mode.as_ref() {
            restore_handler.confirm_or_save_frozen_subtrees(
                first_chunk.manifest.first_version,
                first_chunk.range_proof.left_siblings(),
            )?;
        }

        Ok(first_chunk.manifest.first_version)
    }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L746-790)
```rust
    async fn go_through_verified_chunks(
        &self,
        loaded_chunk_stream: impl Stream<Item = Result<LoadedChunk>>,
        first_version: Version,
    ) -> Result<()> {
        let analysis = self
            .output_transaction_analysis
            .as_ref()
            .map(|dir| TransactionAnalysis::new(dir))
            .transpose()?;
        let start = Instant::now();
        loaded_chunk_stream
            .try_fold(analysis, |mut analysis, chunk| async move {
                let mut version = chunk.manifest.first_version;
                let last_version = chunk.manifest.last_version;

                for (txn, persisted_aux_info, txn_info, events, write_set) in
                    itertools::multizip(chunk.unpack())
                {
                    if let Some(analysis) = &mut analysis {
                        analysis.add_transaction(
                            version,
                            &txn,
                            &persisted_aux_info,
                            &txn_info,
                            &events,
                            &write_set,
                        )?;
                    }
                    version += 1;
                }

                VERIFY_TRANSACTION_VERSION.set(last_version as i64);
                info!(
                    version = last_version,
                    accumulative_tps = ((last_version - first_version + 1) as f64
                        / start.elapsed().as_secs_f64())
                        as u64,
                    "Transactions verified."
                );
                Ok(analysis)
            })
            .await?;
        Ok(())
    }
```

**File:** types/src/proof/definition.rs (L927-970)
```rust
    pub fn verify_extends_ledger(
        &self,
        num_txns_in_ledger: LeafCount,
        root_hash: HashValue,
        first_transaction_info_version: Option<Version>,
    ) -> Result<usize> {
        if let Some(first_version) = first_transaction_info_version {
            ensure!(
                first_version <= num_txns_in_ledger,
                "Transaction list too new. Expected version: {}. First transaction version: {}.",
                num_txns_in_ledger,
                first_version
            );
            let num_overlap_txns = (num_txns_in_ledger - first_version) as usize;
            if num_overlap_txns > self.transaction_infos.len() {
                // Entire chunk is in the past, hard to verify if there's a fork.
                // A fork will need to be detected later.
                return Ok(self.transaction_infos.len());
            }
            let overlap_txn_infos = &self.transaction_infos[..num_overlap_txns];

            // Left side of the proof happens to be the frozen subtree roots of the accumulator
            // right before the list of txns are applied.
            let frozen_subtree_roots_from_proof = self
                .ledger_info_to_transaction_infos_proof
                .left_siblings()
                .iter()
                .rev()
                .cloned()
                .collect::<Vec<_>>();
            let accu_from_proof = InMemoryTransactionAccumulator::new(
                frozen_subtree_roots_from_proof,
                first_version,
            )?
            .append(
                &overlap_txn_infos
                    .iter()
                    .map(CryptoHash::hash)
                    .collect::<Vec<_>>()[..],
            );
            // The two accumulator root hashes should be identical.
            ensure!(
                accu_from_proof.root_hash() == root_hash,
                "Fork happens because the current synced_trees doesn't match the txn list provided."
```

**File:** storage/aptosdb/src/ledger_db/transaction_accumulator_db.rs (L108-126)
```rust
    pub fn put_transaction_accumulator(
        &self,
        first_version: Version,
        txn_infos: &[impl Borrow<TransactionInfo>],
        transaction_accumulator_batch: &mut SchemaBatch,
    ) -> Result<HashValue> {
        let txn_hashes: Vec<HashValue> = txn_infos.iter().map(|t| t.borrow().hash()).collect();

        let (root_hash, writes) = Accumulator::append(
            self,
            first_version, /* num_existing_leaves */
            &txn_hashes,
        )?;
        writes.iter().try_for_each(|(pos, hash)| {
            transaction_accumulator_batch.put::<TransactionAccumulatorSchema>(pos, hash)
        })?;

        Ok(root_hash)
    }
```
