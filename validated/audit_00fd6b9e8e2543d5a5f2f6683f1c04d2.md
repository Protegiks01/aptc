# Audit Report

## Title
Race Condition in Batch Store Expiration Causes Validator Panic and Consensus Disruption

## Summary
A race condition exists in the Batch Store's expiration tracking mechanism where non-atomic operations between `db_cache` and `expirations` data structures can lead to stale expiration entries. When a validator processes these stale entries, it triggers an `unreachable!()` panic, causing immediate validator crash and consensus disruption.

## Finding Description

The vulnerability exists in the `insert_to_cache()` method of the Batch Store, which performs two operations that are not atomic:

**First operation**: The method acquires a DashMap lock to insert or replace an entry in `db_cache`. [1](#0-0) 

**Second operation**: After releasing the lock, the method adds the expiration time to the `expirations` tracking structure. [2](#0-1) 

The developer's comment at line 411 assumes this doesn't need to be atomic, but this assumption is violated when the same digest is concurrently inserted with different expiration times.

**The Race Condition:**

1. Thread A inserts digest D with expiration 100 into `db_cache`, then releases the lock but gets preempted before adding to `expirations`
2. Thread B finds the existing entry with expiration 100, replaces it with expiration 200 (since 100 < 200), and adds (D, 200) to `expirations` [3](#0-2) [4](#0-3) 
3. Thread C calls `clear_expired_payload(250)`, finds D expired, and removes it from `db_cache` [5](#0-4) 
4. Thread A resumes and adds stale entry (D, 100) to `expirations`
5. Thread D later calls `clear_expired_payload()`, finds the stale (D, 100) entry, attempts to look up D in `db_cache`, finds it Vacant, and triggers the panic [6](#0-5) 

**Root Cause**: The `TimeExpirations` structure uses a `BinaryHeap` that allows duplicate entries for the same digest with different expiration times, with no mechanism to remove stale entries when replacements occur. [7](#0-6) 

**Production Scenario**: This can occur naturally when the same batch content (identified by digest) is proposed at different times with different expiration timestamps, which is possible during network retries, re-proposals, or when multiple validators independently create batches with the same content. [8](#0-7) 

## Impact Explanation

**Severity: CRITICAL**

This vulnerability meets the CRITICAL severity criteria for "Total loss of liveness/network availability" under the Aptos bug bounty program because:

1. **Immediate validator crash**: When the `unreachable!()` macro is triggered, the validator panics and crashes immediately, ceasing all consensus participation
2. **Non-recoverable without manual intervention**: The validator must be manually restarted, and without a fix, the race condition can recur
3. **Consensus disruption**: Under high load conditions where this race is more likely, multiple validators could crash simultaneously, severely degrading or halting consensus progress
4. **Liveness violation**: Crashed validators cannot vote, propose blocks, or participate in consensus, directly impacting the network's ability to make progress

This directly violates the consensus liveness guarantees that are fundamental to blockchain operation.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

This race condition has a realistic probability of occurring in production:

1. **Natural occurrence**: The same digest with different expirations can legitimately occur during normal operations (network retries, batch re-proposals, epoch transitions)
2. **Race window exists**: There is a clear, unprotected window between lines 409 and 414 where thread interleaving can occur
3. **High load amplification**: Under heavy transaction volume, CPU contention, or GC pauses, the race window becomes more exploitable
4. **No synchronization**: There are no memory barriers or atomic operations protecting this critical section
5. **Test gap**: The existing concurrent test uses different digests for each experiment and doesn't cover this specific interleaving pattern [9](#0-8) 

## Recommendation

Make the insertion into `db_cache` and addition to `expirations` atomic by holding both locks simultaneously, or by using a single synchronized data structure that tracks both the cache entry and its expiration atomically. 

Alternatively, modify `clear_expired_payload()` to handle the `Vacant` case gracefully rather than panicking, as stale expiration entries can legitimately exist due to replacements:

```rust
Vacant(_) => {
    // Stale expiration entry - already removed by a later update
    None
}
```

Additionally, consider implementing deduplication in `TimeExpirations` to remove old entries when the same digest is added with a different expiration time.

## Proof of Concept

The vulnerability can be demonstrated by creating a concurrent stress test that:
1. Spawns multiple threads that repeatedly call `insert_to_cache()` with the same digest but incrementing expiration times
2. Concurrently runs `clear_expired_payload()` with advancing timestamps
3. Introduces small delays between the `db_cache` insertion and `expirations` addition to widen the race window

Under sufficient load and thread scheduling randomness, this will trigger the `unreachable!()` panic. The existing test infrastructure in the batch_store_test.rs file provides a framework for building such a test.

### Citations

**File:** consensus/src/quorum_store/batch_store.rs (L366-409)
```rust
        {
            // Acquire dashmap internal lock on the entry corresponding to the digest.
            let cache_entry = self.db_cache.entry(digest);

            if let Occupied(entry) = &cache_entry {
                match entry.get().expiration().cmp(&expiration_time) {
                    std::cmp::Ordering::Equal => return Ok(false),
                    std::cmp::Ordering::Greater => {
                        debug!(
                            "QS: already have the digest with higher expiration {}",
                            digest
                        );
                        return Ok(false);
                    },
                    std::cmp::Ordering::Less => {},
                }
            };
            let value_to_be_stored = if self
                .peer_quota
                .entry(author)
                .or_insert(QuotaManager::new(
                    self.db_quota,
                    self.memory_quota,
                    self.batch_quota,
                ))
                .update_quota(value.num_bytes() as usize)?
                == StorageMode::PersistedOnly
            {
                PersistedValue::new(value.batch_info().clone(), None)
            } else {
                value.clone()
            };

            match cache_entry {
                Occupied(entry) => {
                    let (k, prev_value) = entry.replace_entry(value_to_be_stored);
                    debug_assert!(k == digest);
                    self.free_quota(prev_value);
                },
                Vacant(slot) => {
                    slot.insert(value_to_be_stored);
                },
            }
        }
```

**File:** consensus/src/quorum_store/batch_store.rs (L411-415)
```rust
        // Add expiration for the inserted entry, no need to be atomic w. insertion.
        #[allow(clippy::unwrap_used)]
        {
            self.expirations.lock().add_item(digest, expiration_time);
        }
```

**File:** consensus/src/quorum_store/batch_store.rs (L451-458)
```rust
            let removed_value = match self.db_cache.entry(h) {
                Occupied(entry) => {
                    // We need to check up-to-date expiration again because receiving the same
                    // digest with a higher expiration would update the persisted value and
                    // effectively extend the expiration.
                    if entry.get().expiration() <= expiration_time {
                        self.persist_subscribers.remove(entry.get().digest());
                        Some(entry.remove())
```

**File:** consensus/src/quorum_store/batch_store.rs (L463-463)
```rust
                Vacant(_) => unreachable!("Expired entry not in cache"),
```

**File:** consensus/src/quorum_store/utils.rs (L60-73)
```rust
pub(crate) struct TimeExpirations<I: Ord> {
    expiries: BinaryHeap<(Reverse<u64>, I)>,
}

impl<I: Ord + Hash> TimeExpirations<I> {
    pub(crate) fn new() -> Self {
        Self {
            expiries: BinaryHeap::new(),
        }
    }

    pub(crate) fn add_item(&mut self, item: I, expiry_time: u64) {
        self.expiries.push((Reverse(expiry_time), item));
    }
```

**File:** consensus/consensus-types/src/proof_of_store.rs (L46-81)
```rust
#[derive(
    Clone, Debug, Deserialize, Serialize, CryptoHasher, BCSCryptoHash, PartialEq, Eq, Hash,
)]
pub struct BatchInfo {
    author: PeerId,
    batch_id: BatchId,
    epoch: u64,
    expiration: u64,
    digest: HashValue,
    num_txns: u64,
    num_bytes: u64,
    gas_bucket_start: u64,
}

impl BatchInfo {
    pub fn new(
        author: PeerId,
        batch_id: BatchId,
        epoch: u64,
        expiration: u64,
        digest: HashValue,
        num_txns: u64,
        num_bytes: u64,
        gas_bucket_start: u64,
    ) -> Self {
        Self {
            author,
            batch_id,
            epoch,
            expiration,
            digest,
            num_txns,
            num_bytes,
            gas_bucket_start,
        }
    }
```

**File:** consensus/src/quorum_store/tests/batch_store_test.rs (L91-114)
```rust
#[tokio::test(flavor = "multi_thread")]
async fn test_extend_expiration_vs_save() {
    let num_experiments = 2000;
    let batch_store = batch_store_for_test(2001);

    let batch_store_clone1 = batch_store.clone();
    let batch_store_clone2 = batch_store.clone();

    let digests: Vec<HashValue> = (0..num_experiments).map(|_| HashValue::random()).collect();
    let later_exp_values: Vec<PersistedValue<BatchInfoExt>> = (0..num_experiments)
        .map(|i| {
            // Pre-insert some of them.
            if i % 2 == 0 {
                assert_ok!(batch_store.save(&request_for_test(
                    &digests[i],
                    i as u64 + 30,
                    1,
                    None
                )));
            }

            request_for_test(&digests[i], i as u64 + 40, 1, None)
        })
        .collect();
```
