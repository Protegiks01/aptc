# Audit Report

## Title
Missing Timeout on `wait_for_commit_ledger()` Causes Indefinite Blocking and Consensus Liveness Failure

## Summary
The consensus pipeline's persisting phase lacks timeout protection when waiting for ledger commitment, causing indefinite blocking when RocksDB write operations stall. This design flaw leads to validator unavailability through back pressure accumulation, with no automatic recovery mechanism.

## Finding Description

The vulnerability exists in the consensus pipeline where blocks await ledger commitment without timeout protection. The persisting phase calls `wait_for_commit_ledger()` directly without any timeout wrapper. [1](#0-0) 

The `wait_for_commit_ledger()` method simply awaits the `commit_ledger_fut` future without any timeout mechanism, allowing indefinite blocking. [2](#0-1) 

The `commit_ledger_fut` is spawned as an unabortable future by passing `None` as the abort handles parameter, meaning it cannot be cancelled even during reset operations. [3](#0-2) 

The commit_ledger implementation uses `spawn_blocking` to call the executor's synchronous commit operation. [4](#0-3) 

This chains through the block executor's commit_ledger method. [5](#0-4) 

Which ultimately performs synchronous RocksDB write operations. [6](#0-5) 

The database writes use synchronous mode with `set_sync(true)` to ensure durability, which can block indefinitely during write stalls. [7](#0-6) 

**Attack Scenario:**
1. RocksDB experiences write stalls due to write buffer saturation, compaction backlog, disk I/O degradation, or lock contention
2. The synchronous `executor.commit_ledger()` call blocks indefinitely in the spawn_blocking task
3. The `wait_for_commit_ledger()` await never completes
4. The persisting phase never sends a response, preventing `highest_committed_round` from advancing [8](#0-7) 
5. The buffer manager detects back pressure using MAX_BACKLOG of 20 rounds [9](#0-8) 
6. When back pressure activates, the buffer manager stops accepting new ordered blocks [10](#0-9) 

**Critical Design Flaw:** The reset mechanism cannot recover because it waits for pending blocks to complete commit_ledger operations [11](#0-10)  and waits for all futures including the unabortable `commit_ledger_fut` to finish. [12](#0-11) 

## Impact Explanation

This is a **HIGH severity** vulnerability per the Aptos bug bounty criteria, specifically matching "Validator node slowdowns":

1. **Validator Unavailability**: The affected validator becomes unable to participate in consensus after ~20 rounds (typically 1-2 minutes), requiring manual restart
2. **Network-Wide Impact**: If multiple validators experience simultaneous RocksDB stalls during high transaction load or infrastructure issues, the network could lose liveness
3. **No Automatic Recovery**: There is no timeout or automatic recovery mechanism - even the reset operation waits for the stuck future to complete
4. **Breaks Liveness Invariant**: Violates the fundamental consensus requirement that honest nodes maintain liveness under operational stress

The severity is HIGH rather than CRITICAL because single-node failures don't immediately halt the network (requiring >2/3 validators to fail), and recovery is possible through node restart.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

This vulnerability manifests under realistic operational conditions:

1. **RocksDB Write Stalls are Common**: Production databases regularly experience write stalls due to write buffer saturation, compaction falling behind transaction rate, L0 file count exceeding thresholds, or disk I/O performance degradation
2. **Documented in Codebase**: The existence of RocksDB performance monitoring infrastructure confirms these issues occur in practice
3. **No Attacker Required**: This happens naturally during network-wide transaction surges, storage infrastructure issues, hardware degradation, or resource exhaustion
4. **No Recovery Path**: The design flaw means stuck operations have no escape hatch except manual node restart

## Recommendation

Implement timeout protection for the commit_ledger operation:

1. Add configurable timeout to `wait_for_commit_ledger()` method (e.g., 30-60 seconds)
2. Use `tokio::time::timeout()` wrapper around the commit_ledger await
3. On timeout, trigger validator alert and graceful recovery (abort pipeline, trigger state sync)
4. Consider making `commit_ledger_fut` abortable by passing abort handles
5. Add circuit breaker pattern to prevent repeated timeout failures
6. Implement health monitoring that proactively detects RocksDB write stall conditions

## Proof of Concept

While a full PoC would require simulating RocksDB write stalls in a test environment, the vulnerability can be demonstrated by:

1. Setting up a validator node with limited disk I/O or storage resources
2. Generating high transaction load to saturate write buffers
3. Monitoring the buffer manager metrics to observe back pressure activation
4. Observing that the validator stops participating in consensus after ~20 rounds
5. Confirming that the validator requires manual restart to recover

The technical evidence provided through code citations demonstrates the complete execution path from persisting phase through to synchronous RocksDB writes without any timeout protection.

### Citations

**File:** consensus/src/pipeline/persisting_phase.rs (L71-71)
```rust
            b.wait_for_commit_ledger().await;
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L104-113)
```rust
    pub async fn wait_until_finishes(self) {
        let _ = join5(
            self.execute_fut,
            self.ledger_update_fut,
            self.pre_commit_fut,
            self.commit_ledger_fut,
            self.notify_state_sync_fut,
        )
        .await;
    }
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L562-568)
```rust
    pub async fn wait_for_commit_ledger(&self) {
        // may be aborted (e.g. by reset)
        if let Some(fut) = self.pipeline_futs() {
            // this may be cancelled
            let _ = fut.commit_ledger_fut.await;
        }
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L547-556)
```rust
        let commit_ledger_fut = spawn_shared_fut(
            Self::commit_ledger(
                pre_commit_fut.clone(),
                commit_proof_fut,
                parent.commit_ledger_fut.clone(),
                self.executor.clone(),
                block.clone(),
            ),
            None,
        );
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L1098-1104)
```rust
        tokio::task::spawn_blocking(move || {
            executor
                .commit_ledger(ledger_info_with_sigs_clone)
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
```

**File:** execution/executor/src/block_executor/mod.rs (L388-390)
```rust
        self.db
            .writer
            .commit_ledger(target_version, Some(&ledger_info_with_sigs), None)?;
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L107-107)
```rust
            self.ledger_db.metadata_db().write_schemas(ledger_batch)?;
```

**File:** storage/schemadb/src/lib.rs (L371-378)
```rust
/// For now we always use synchronous writes. This makes sure that once the operation returns
/// `Ok(())` the data is persisted even if the machine crashes. In the future we might consider
/// selectively turning this off for some non-critical writes to improve performance.
fn sync_write_option() -> rocksdb::WriteOptions {
    let mut opts = rocksdb::WriteOptions::default();
    opts.set_sync(true);
    opts
}
```

**File:** consensus/src/pipeline/buffer_manager.rs (L547-550)
```rust
        while let Some((_, block)) = self.pending_commit_blocks.pop_first() {
            // Those blocks don't have any dependencies, should be able to finish commit_ledger.
            // Abort them can cause error on epoch boundary.
            block.wait_for_commit_ledger().await;
```

**File:** consensus/src/pipeline/buffer_manager.rs (L906-910)
```rust
    fn need_back_pressure(&self) -> bool {
        const MAX_BACKLOG: Round = 20;

        self.back_pressure_enabled && self.highest_committed_round + MAX_BACKLOG < self.latest_round
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L938-938)
```rust
                Some(blocks) = self.block_rx.next(), if !self.need_back_pressure() => {
```

**File:** consensus/src/pipeline/buffer_manager.rs (L968-972)
```rust
                Some(Ok(round)) = self.persisting_phase_rx.next() => {
                    // see where `need_backpressure()` is called.
                    self.pending_commit_votes = self.pending_commit_votes.split_off(&(round + 1));
                    self.highest_committed_round = round;
                    self.pending_commit_blocks = self.pending_commit_blocks.split_off(&(round + 1));
```
