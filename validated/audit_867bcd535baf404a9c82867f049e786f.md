# Audit Report

## Title
Consensus Sync Request State Machine Corruption via TOCTOU Race Condition During Concurrent Sync Duration Notifications

## Summary
A Time-of-Check-Time-of-Use (TOCTOU) race condition exists in the state sync driver that causes consensus tasks to hang indefinitely. When concurrent sync duration notifications arrive from consensus and consensus observer, the state machine responds to the wrong request callback, permanently blocking the original consensus task and corrupting sync state.

## Finding Description

The vulnerability exists in the interaction between `check_sync_request_progress()` and `initialize_sync_duration_request()` across an async yield boundary.

**Critical Execution Flow:**

1. **Arc Reference Capture**: The function captures a cloned Arc reference to the current sync request via `get_sync_request()`. [1](#0-0)  This method returns a cloned Arc. [2](#0-1) 

2. **Satisfaction Check**: The function validates that the sync request has completed its duration. [3](#0-2) 

3. **Async Yield Point**: The function enters a while loop waiting for storage synchronizer to drain pending data, calling `yield_now().await` which yields control to the async runtime. [4](#0-3) 

4. **Race Window**: During the yield, the `futures::select!` loop processes new consensus sync duration notifications. [5](#0-4) [6](#0-5) 

5. **Arc Replacement Without Validation**: The handler calls `initialize_sync_duration_request()` which **replaces the entire Arc** instead of updating its contents. [7](#0-6)  This creates a NEW Arc (Arc2) containing SyncRequest2, while the local variable in `check_sync_request_progress()` still points to the OLD Arc (Arc1) containing SyncRequest1.

6. **Validation Using Stale Reference**: When `check_sync_request_progress()` resumes, it continues using the local Arc variable for validation checks. [8](#0-7) 

7. **Wrong Request Handled**: The function calls `handle_satisfied_sync_request()` which accesses `self.consensus_sync_request` directly. [9](#0-8) 

8. **State Corruption**: This method locks `self.consensus_sync_request` (now pointing to Arc2), takes out SyncRequest2, and responds to SyncRequest2's callback. [10](#0-9) [11](#0-10) 

**Result**: SyncRequest1's callback is never invoked, causing the original consensus task to hang forever. SyncRequest2's callback receives a premature response for an incomplete sync operation.

**Root Cause - No Timeout**: Each sync notification contains a `oneshot::Sender` callback that consensus awaits. [12](#0-11) [13](#0-12)  When consensus calls `sync_for_duration()`, it blocks on `callback_receiver.await` **with no timeout**. [14](#0-13)  If this callback is never sent due to the race condition, the consensus task is permanently blocked.

**Concurrency Enabler**: The vulnerability is enabled because consensus and consensus observer use **separate ExecutionProxy instances** but share the **same state_sync_notifier channel**. [15](#0-14) [16](#0-15)  Both components receive clones of the same `consensus_notifier` in the node initialization. [17](#0-16) [18](#0-17) 

Each ExecutionProxy has its own `write_mutex` that only protects concurrent calls **within** each component, not **across** components. [19](#0-18) [20](#0-19)  This allows consensus and consensus observer to send concurrent sync requests that bypass mutex protection.

## Impact Explanation

This vulnerability meets **Critical Severity** criteria (up to $1,000,000) under the Aptos bug bounty program:

**Total Loss of Liveness/Network Availability**: When a consensus task is permanently blocked waiting for a sync response that never arrives, it prevents:
- Block proposals from being generated
- Votes from being cast on proposed blocks  
- Epoch transitions from completing
- The validator from participating in consensus

**Consensus Protocol Violations**: The state sync component violates its contract with consensus by:
- Failing to respond to valid sync requests
- Sending responses for wrong sync operations
- Corrupting the sync state machine that consensus depends on

**Significant Protocol Violations**: The bug causes:
- Orphaned oneshot callback channels that can never be completed
- Mismatched sync request/response pairs
- Unpredictable behavior when multiple sync requests are in flight

The vulnerability is exploitable under normal network conditions when consensus sends multiple sync requests (e.g., during network partitions, fallback mode, or rapid epoch changes). No Byzantine behavior or validator collusion is required.

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability is triggered when:
1. A sync duration request is in the process of completing (has satisfied its duration)
2. The storage synchronizer still has pending data to drain (causing yield_now() loop)
3. A second sync duration notification arrives during this window

This scenario occurs naturally in several situations:
- **Consensus observer fallback mode**: When consensus observer enters fallback synchronization, it calls `sync_for_duration()` [21](#0-20) 
- **Network instability**: Causing consensus to retry sync operations
- **Epoch transitions**: Where multiple sync operations may be initiated
- **Validator catchup**: After downtime, multiple sync requests may be queued

The race window can persist for seconds or longer if the storage synchronizer has significant pending data, making the race window substantial.

## Recommendation

**Fix 1 - Update Arc Contents Instead of Replacement**:
Modify `initialize_sync_duration_request()` to update the contents of the existing Arc<Mutex<Option<ConsensusSyncRequest>>> instead of replacing the entire Arc:

```rust
pub async fn initialize_sync_duration_request(
    &mut self,
    sync_duration_notification: ConsensusSyncDurationNotification,
) -> Result<(), Error> {
    let start_time = self.time_service.now();
    let consensus_sync_request = ConsensusSyncRequest::new_with_duration(start_time, sync_duration_notification);
    
    // Update contents instead of replacing Arc
    *self.consensus_sync_request.lock() = Some(consensus_sync_request);
    
    Ok(())
}
```

**Fix 2 - Add Validation for Active Requests**:
Check if there's already an active sync request before accepting a new one:

```rust
pub async fn initialize_sync_duration_request(
    &mut self,
    sync_duration_notification: ConsensusSyncDurationNotification,
) -> Result<(), Error> {
    // Check for existing active request
    if self.consensus_sync_request.lock().is_some() {
        return Err(Error::ActiveSyncRequestExists);
    }
    
    let start_time = self.time_service.now();
    let consensus_sync_request = ConsensusSyncRequest::new_with_duration(start_time, sync_duration_notification);
    *self.consensus_sync_request.lock() = Some(consensus_sync_request);
    
    Ok(())
}
```

**Fix 3 - Add Timeout to sync_for_duration**:
Add a timeout wrapper to the callback receiver await to prevent indefinite hangs:

```rust
// In ConsensusNotifier::sync_for_duration
let timeout_duration = Duration::from_secs(300); // 5 minutes
match timeout(timeout_duration, callback_receiver).await {
    Ok(Ok(response)) => { /* handle response */ },
    Ok(Err(error)) => Err(Error::UnexpectedErrorEncountered(format!("{:?}", error))),
    Err(_) => Err(Error::TimeoutWaitingForStateSync),
}
```

## Proof of Concept

The race condition can be demonstrated through the following sequence:

1. Start a validator node with consensus observer enabled
2. Trigger consensus to send a sync duration request
3. While the first request is in the `yield_now()` loop waiting for storage synchronizer to drain
4. Have consensus observer enter fallback mode, sending a second sync duration request
5. The second request replaces the Arc, causing the first request's callback to be orphaned
6. Observe that the original consensus task hangs indefinitely on the callback await

This can be reproduced in a test environment by:
- Setting low fallback thresholds to trigger observer fallback quickly
- Adding artificial delays in storage synchronizer to extend the race window
- Monitoring consensus task states to observe the permanent hang

The vulnerability requires no special privileges and can occur during normal network operations, particularly during periods of high sync activity or network instability.

## Notes

This is a critical liveness vulnerability that affects consensus availability. The root cause is the combination of:
1. Arc replacement semantics (creating new Arc instead of updating contents)
2. Lack of validation for concurrent sync requests
3. Missing timeout protection on the callback receiver
4. Shared notification channel between consensus and consensus observer with separate mutexes

The fix should address the Arc replacement pattern and add proper synchronization or validation to prevent concurrent sync request corruption.

### Citations

**File:** state-sync/state-sync-driver/src/driver.rs (L222-238)
```rust
            ::futures::select! {
                notification = self.client_notification_listener.select_next_some() => {
                    self.handle_client_notification(notification).await;
                },
                notification = self.commit_notification_listener.select_next_some() => {
                    self.handle_snapshot_commit_notification(notification).await;
                }
                notification = self.consensus_notification_handler.select_next_some() => {
                    self.handle_consensus_or_observer_notification(notification).await;
                }
                notification = self.error_notification_listener.select_next_some() => {
                    self.handle_error_notification(notification).await;
                }
                _ = progress_check_interval.select_next_some() => {
                    self.drive_progress().await;
                }
            }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L301-304)
```rust
            ConsensusNotification::SyncForDuration(sync_notification) => {
                self.handle_consensus_sync_duration_notification(sync_notification)
                    .await
            },
```

**File:** state-sync/state-sync-driver/src/driver.rs (L538-538)
```rust
        let consensus_sync_request = self.consensus_notification_handler.get_sync_request();
```

**File:** state-sync/state-sync-driver/src/driver.rs (L539-547)
```rust
        match consensus_sync_request.lock().as_ref() {
            Some(consensus_sync_request) => {
                let latest_synced_ledger_info =
                    utils::fetch_latest_synced_ledger_info(self.storage.clone())?;
                if !consensus_sync_request
                    .sync_request_satisfied(&latest_synced_ledger_info, self.time_service.clone())
                {
                    return Ok(()); // The sync request hasn't been satisfied yet
                }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L556-564)
```rust
        while self.storage_synchronizer.pending_storage_data() {
            sample!(
                SampleRate::Duration(Duration::from_secs(PENDING_DATA_LOG_FREQ_SECS)),
                info!("Waiting for the storage synchronizer to handle pending data!")
            );

            // Yield to avoid starving the storage synchronizer threads.
            yield_now().await;
        }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L569-592)
```rust
        if let Some(sync_request) = consensus_sync_request.lock().as_ref() {
            if sync_request.is_sync_duration_request() {
                // Get the latest synced version and ledger info version
                let latest_synced_version =
                    utils::fetch_pre_committed_version(self.storage.clone())?;
                let latest_synced_ledger_info =
                    utils::fetch_latest_synced_ledger_info(self.storage.clone())?;
                let latest_ledger_info_version = latest_synced_ledger_info.ledger_info().version();

                // Check if the latest synced version matches the latest ledger info version
                if latest_synced_version != latest_ledger_info_version {
                    sample!(
                        SampleRate::Duration(Duration::from_secs(DRIVER_INFO_LOG_FREQ_SECS)),
                        info!(
                            "Waiting for state sync to sync to a ledger info! \
                            Latest synced version: {:?}, latest ledger info version: {:?}",
                            latest_synced_version, latest_ledger_info_version
                        )
                    );

                    return Ok(()); // State sync should continue to run
                }
            }
        }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L597-599)
```rust
        self.consensus_notification_handler
            .handle_satisfied_sync_request(latest_synced_ledger_info)
            .await?;
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L241-243)
```rust
    pub fn get_sync_request(&self) -> Arc<Mutex<Option<ConsensusSyncRequest>>> {
        self.consensus_sync_request.clone()
    }
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L254-256)
```rust
        let consensus_sync_request =
            ConsensusSyncRequest::new_with_duration(start_time, sync_duration_notification);
        self.consensus_sync_request = Arc::new(Mutex::new(Some(consensus_sync_request)));
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L327-328)
```rust
        let mut sync_request_lock = self.consensus_sync_request.lock();
        let consensus_sync_request = sync_request_lock.take();
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L332-337)
```rust
            Some(ConsensusSyncRequest::SyncDuration(_, sync_duration_notification)) => {
                self.respond_to_sync_duration_notification(
                    sync_duration_notification,
                    Ok(()),
                    Some(latest_synced_ledger_info),
                )?;
```

**File:** state-sync/inter-component/consensus-notifications/src/lib.rs (L162-178)
```rust
        match callback_receiver.await {
            Ok(response) => match response.get_result() {
                Ok(_) => response.get_latest_synced_ledger_info().ok_or_else(|| {
                    Error::UnexpectedErrorEncountered(
                        "Sync for duration returned an empty latest synced ledger info!".into(),
                    )
                }),
                Err(error) => Err(Error::UnexpectedErrorEncountered(format!(
                    "Sync for duration returned an error: {:?}",
                    error
                ))),
            },
            Err(error) => Err(Error::UnexpectedErrorEncountered(format!(
                "Sync for duration failure: {:?}",
                error
            ))),
        }
```

**File:** state-sync/inter-component/consensus-notifications/src/lib.rs (L362-365)
```rust
pub struct ConsensusSyncDurationNotification {
    duration: Duration,
    callback: oneshot::Sender<ConsensusNotificationResponse>,
}
```

**File:** state-sync/inter-component/consensus-notifications/src/lib.rs (L368-373)
```rust
    pub fn new(duration: Duration) -> (Self, oneshot::Receiver<ConsensusNotificationResponse>) {
        let (callback, callback_receiver) = oneshot::channel();
        let notification = ConsensusSyncDurationNotification { duration, callback };

        (notification, callback_receiver)
    }
```

**File:** consensus/src/consensus_provider.rs (L65-72)
```rust
    let execution_proxy = ExecutionProxy::new(
        Arc::new(BlockExecutor::<AptosVMBlockExecutor>::new(aptos_db)),
        txn_notifier,
        state_sync_notifier,
        node_config.transaction_filters.execution_filter.clone(),
        node_config.consensus.enable_pre_commit,
        None,
    );
```

**File:** consensus/src/consensus_provider.rs (L158-165)
```rust
        let execution_proxy = ExecutionProxy::new(
            Arc::new(BlockExecutor::<AptosVMBlockExecutor>::new(aptos_db.clone())),
            txn_notifier,
            state_sync_notifier,
            node_config.transaction_filters.execution_filter.clone(),
            node_config.consensus.enable_pre_commit,
            None,
        );
```

**File:** aptos-node/src/lib.rs (L834-834)
```rust
            consensus_notifier.clone(),
```

**File:** aptos-node/src/lib.rs (L846-846)
```rust
        consensus_notifier.clone(),
```

**File:** consensus/src/state_computer.rs (L58-58)
```rust
    write_mutex: AsyncMutex<LogicalTime>,
```

**File:** consensus/src/state_computer.rs (L132-174)
```rust
    async fn sync_for_duration(
        &self,
        duration: Duration,
    ) -> Result<LedgerInfoWithSignatures, StateSyncError> {
        // Grab the logical time lock
        let mut latest_logical_time = self.write_mutex.lock().await;

        // Before state synchronization, we have to call finish() to free the
        // in-memory SMT held by the BlockExecutor to prevent a memory leak.
        self.executor.finish();

        // Inject an error for fail point testing
        fail_point!("consensus::sync_for_duration", |_| {
            Err(anyhow::anyhow!("Injected error in sync_for_duration").into())
        });

        // Invoke state sync to synchronize for the specified duration. Here, the
        // ChunkExecutor will process chunks and commit to storage. However, after
        // block execution and commits, the internal state of the ChunkExecutor may
        // not be up to date. So, it is required to reset the cache of the
        // ChunkExecutor in state sync when requested to sync.
        let result = monitor!(
            "sync_for_duration",
            self.state_sync_notifier.sync_for_duration(duration).await
        );

        // Update the latest logical time
        if let Ok(latest_synced_ledger_info) = &result {
            let ledger_info = latest_synced_ledger_info.ledger_info();
            let synced_logical_time = LogicalTime::new(ledger_info.epoch(), ledger_info.round());
            *latest_logical_time = synced_logical_time;
        }

        // Similarly, after state synchronization, we have to reset the cache of
        // the BlockExecutor to guarantee the latest committed state is up to date.
        self.executor.reset()?;

        // Return the result
        result.map_err(|error| {
            let anyhow_error: anyhow::Error = error.into();
            anyhow_error.into()
        })
    }
```
