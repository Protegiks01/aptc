# Audit Report

## Title
Missing Merkle Proof Verification in KvOnly State Snapshot Restore Mode Enables Invalid State Injection

## Summary
When restoring state snapshots in `StateSnapshotRestoreMode::KvOnly`, the system fails to cryptographically verify chunk Merkle proofs against the validated root hash. While the manifest proof and root hash are correctly verified against validator-signed LedgerInfo, individual chunk data bypasses Merkle proof verification, breaking the chain of cryptographic trust and allowing invalid state injection into AptosDB if backup files are compromised.

## Finding Description

The state snapshot restore process implements a two-level verification system:

**Level 1: Manifest-level verification (correctly implemented)**
The manifest's `TransactionInfoWithProof` and `LedgerInfoWithSignatures` are cryptographically verified against validator signatures, and the state root hash is extracted and verified. [1](#0-0) 

**Level 2: Chunk-level verification (missing in KvOnly mode)**
Each chunk should be verified against the validated root hash using its accompanying `SparseMerkleRangeProof`. The backup format includes these proofs for each chunk. [2](#0-1) 

The `load_bcs_file()` function loads the proof but only performs BCS deserialization without cryptographic verification. [3](#0-2) 

During chunk processing, the `StateSnapshotRestore::add_chunk()` implementation routes to different code paths based on restore mode: [4](#0-3) 

In `Default` or `TreeOnly` mode, `tree_fn()` is called, which invokes `add_chunk_impl()` with cryptographic proof verification. [5](#0-4) 

The verification logic in `SparseMerkleRangeProof::verify()` performs real cryptographic verification by reconstructing the Merkle path and comparing against the expected root hash. [6](#0-5) 

**However, in `KvOnly` mode**, only `kv_fn()` is executed, which calls `StateValueRestore::add_chunk()` that accepts no proof parameter and performs no cryptographic verification. [7](#0-6) 

**Critical Production Usage:**
The two-phase restore process used in production explicitly uses `KvOnly` mode in phase 1: [8](#0-7) 

Additionally, transaction replay after KvOnly restore runs with `VerifyExecutionMode::NoVerify`, providing no compensating control. [9](#0-8) 

**Attack Scenario:**
1. Attacker compromises backup storage (S3/GCS buckets) or provides malicious "trusted" public backups
2. Attacker modifies KV snapshot chunk files to contain arbitrary state data
3. Operator performs restore using the two-phase process (standard production path)
4. Phase 1: Invalid KV data is written to AptosDB without proof verification
5. Phase 2: Tree snapshot is restored (potentially from same compromised source)
6. Result: Database contains corrupted state that could cause consensus failures when validators disagree on state roots

## Impact Explanation

**Severity: High**

This vulnerability breaks the cryptographic chain of trust from validator-signed LedgerInfo → manifest root hash → chunk data. Per Aptos bug bounty criteria, this qualifies as **High severity** due to "State inconsistencies requiring manual intervention."

**Concrete Impacts:**

1. **State Injection**: An attacker with access to backup files can inject arbitrary state into nodes restoring from these backups, despite the manifest root hash being cryptographically verified.

2. **Consensus Safety Risk**: Validator nodes with corrupted state could produce different state roots for the same block height, potentially causing consensus disagreements and network issues.

3. **Chain of Trust Violation**: The system correctly verifies the manifest against LedgerInfo (signed by validators) but fails to complete the verification chain by checking that chunk data matches the verified root hash.

4. **Defense-in-Depth Failure**: The code already performs expensive manifest verification and loads proofs during restore, but fails to execute the final verification step in KvOnly mode, representing an incomplete security implementation.

5. **Supply Chain Risk**: Operators downloading backups from compromised cloud storage or third-party sources have no cryptographic guarantee that chunk data matches the verified manifest root hash.

## Likelihood Explanation

**Likelihood: Medium**

**Required Conditions:**
1. Compromised backup files (medium difficulty - cloud storage compromise, supply chain attack, or insider threat)
2. Operator using these backups for restore (high probability in affected infrastructure)
3. Two-phase restore triggering KvOnly mode (automatically happens in production code path)

**Real-World Scenarios:**
- Compromised S3/GCS backup buckets with inadequate access controls
- Supply chain attacks on backup distribution infrastructure
- Operators downloading "verified" public snapshots from compromised sources
- Backup corruption during transfer or storage without integrity checks
- Insider threats with access to backup infrastructure

The vulnerability is realistic because the two-phase restore process is the standard production code path, and backup security is a known operational challenge in distributed systems.

## Recommendation

Modify `StateSnapshotRestoreMode::KvOnly` to verify Merkle proofs before writing KV data:

**Option 1:** Add proof verification to KvOnly mode by calling the verification logic before writing to database.

**Option 2:** Restructure the two-phase restore to use Default mode (which verifies proofs) instead of splitting into KvOnly + TreeOnly phases.

**Option 3:** Add an independent verification step after KvOnly restore that checks the written KV data against the Merkle tree before allowing the node to participate in consensus.

The fix should ensure that all chunk data written to AptosDB has been cryptographically verified against the validated root hash, completing the chain of trust from validator signatures to stored state.

## Proof of Concept

The vulnerability can be demonstrated by:

1. Creating a valid state snapshot backup with manifest and chunks
2. Modifying a chunk file to contain invalid KV data while keeping the proof file
3. Running the two-phase restore process with the modified backup
4. Observing that the invalid KV data is written to AptosDB without verification errors
5. Verifying that subsequent tree restoration completes successfully despite KV corruption

The core issue is architectural: the `StateSnapshotReceiver` trait requires both chunk data and proof as parameters, but the `KvOnly` implementation ignores the proof parameter entirely, as evidenced by `StateValueRestore::add_chunk()` signature and implementation.

## Notes

This vulnerability represents a defense-in-depth failure rather than a complete security breakdown. The manifest-level verification (against validator-signed LedgerInfo) is correctly implemented, providing a baseline level of security. However, the missing chunk-level verification violates the principle of complete cryptographic verification chains and creates unnecessary risk if backup storage is compromised.

The backup format correctly includes Merkle proofs for each chunk, and the verification logic exists in the codebase. The issue is purely in the KvOnly restore mode implementation, which was likely designed for performance optimization but inadvertently bypassed security controls.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L125-136)
```rust
        let (txn_info_with_proof, li): (TransactionInfoWithProof, LedgerInfoWithSignatures) =
            self.storage.load_bcs_file(&manifest.proof).await?;
        txn_info_with_proof.verify(li.ledger_info(), manifest.version)?;
        let state_root_hash = txn_info_with_proof
            .transaction_info()
            .ensure_state_checkpoint_hash()?;
        ensure!(
            state_root_hash == manifest.root_hash,
            "Root hash mismatch with that in proof. root hash: {}, expected: {}",
            manifest.root_hash,
            state_root_hash,
        );
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/manifest.rs (L24-26)
```rust
    /// BCS serialized `SparseMerkleRangeProof` that proves this chunk adds up to the root hash
    /// indicated in the backup (`StateSnapshotBackup::root_hash`).
    pub proof: FileHandle,
```

**File:** storage/backup/backup-cli/src/utils/storage_ext.rs (L31-32)
```rust
    async fn load_bcs_file<T: DeserializeOwned>(&self, file_handle: &FileHandleRef) -> Result<T> {
        Ok(bcs::from_bytes(&self.read_all(file_handle).await?)?)
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L88-127)
```rust
    pub fn add_chunk(&mut self, mut chunk: Vec<(K, V)>) -> Result<()> {
        // load progress
        let progress_opt = self.db.get_progress(self.version)?;

        // skip overlaps
        if let Some(progress) = progress_opt {
            let idx = chunk
                .iter()
                .position(|(k, _v)| CryptoHash::hash(k) > progress.key_hash)
                .unwrap_or(chunk.len());
            chunk = chunk.split_off(idx);
        }

        // quit if all skipped
        if chunk.is_empty() {
            return Ok(());
        }

        // save
        let mut usage = progress_opt.map_or(StateStorageUsage::zero(), |p| p.usage);
        let (last_key, _last_value) = chunk.last().unwrap();
        let last_key_hash = CryptoHash::hash(last_key);

        // In case of TreeOnly Restore, we only restore the usage of KV without actually writing KV into DB
        for (k, v) in chunk.iter() {
            usage.add_item(k.key_size() + v.value_size());
        }

        // prepare the sharded kv batch
        let kv_batch: StateValueBatch<K, Option<V>> = chunk
            .into_iter()
            .map(|(k, v)| ((k, self.version), Some(v)))
            .collect();

        self.db.write_kv_batch(
            self.version,
            &kv_batch,
            StateSnapshotProgress::new(last_key_hash, usage),
        )
    }
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L246-255)
```rust
        match self.restore_mode {
            StateSnapshotRestoreMode::KvOnly => kv_fn()?,
            StateSnapshotRestoreMode::TreeOnly => tree_fn()?,
            StateSnapshotRestoreMode::Default => {
                // We run kv_fn with TreeOnly to restore the usage of DB
                let (r1, r2) = IO_POOL.join(kv_fn, tree_fn);
                r1?;
                r2?;
            },
        }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L391-391)
```rust
        self.verify(proof)?;
```

**File:** types/src/proof/definition.rs (L782-826)
```rust
    pub fn verify(
        &self,
        expected_root_hash: HashValue,
        rightmost_known_leaf: SparseMerkleLeafNode,
        left_siblings: Vec<HashValue>,
    ) -> Result<()> {
        let num_siblings = left_siblings.len() + self.right_siblings.len();
        let mut left_sibling_iter = left_siblings.iter();
        let mut right_sibling_iter = self.right_siblings().iter();

        let mut current_hash = rightmost_known_leaf.hash();
        for bit in rightmost_known_leaf
            .key()
            .iter_bits()
            .rev()
            .skip(HashValue::LENGTH_IN_BITS - num_siblings)
        {
            let (left_hash, right_hash) = if bit {
                (
                    *left_sibling_iter
                        .next()
                        .ok_or_else(|| format_err!("Missing left sibling."))?,
                    current_hash,
                )
            } else {
                (
                    current_hash,
                    *right_sibling_iter
                        .next()
                        .ok_or_else(|| format_err!("Missing right sibling."))?,
                )
            };
            current_hash = SparseMerkleInternalNode::new(left_hash, right_hash).hash();
        }

        ensure!(
            current_hash == expected_root_hash,
            "{}: Root hashes do not match. Actual root hash: {:x}. Expected root hash: {:x}.",
            type_name::<Self>(),
            current_hash,
            expected_root_hash,
        );

        Ok(())
    }
```

**File:** storage/backup/backup-cli/src/coordinators/restore.rs (L247-259)
```rust
                StateSnapshotRestoreController::new(
                    StateSnapshotRestoreOpt {
                        manifest_handle: kv_snapshot.manifest,
                        version: kv_snapshot.version,
                        validate_modules: false,
                        restore_mode: StateSnapshotRestoreMode::KvOnly,
                    },
                    self.global_opt.clone(),
                    Arc::clone(&self.storage),
                    epoch_history.clone(),
                )
                .run()
                .await?;
```

**File:** storage/backup/backup-cli/src/coordinators/restore.rs (L289-300)
```rust
            TransactionRestoreBatchController::new(
                transaction_restore_opt,
                Arc::clone(&self.storage),
                txn_manifests,
                Some(db_next_version),
                Some((kv_replay_version, true /* only replay KV */)),
                epoch_history.clone(),
                VerifyExecutionMode::NoVerify,
                None,
            )
            .run()
            .await?;
```
