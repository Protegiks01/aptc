# Audit Report

## Title
Race Condition in Block Insertion Causes Node Crash via Stale OrderedBlockWindow Weak Pointers

## Summary
A Time-of-Check-Time-of-Use (TOCTOU) race condition exists between block insertion and block pruning in the consensus layer that can cause validator nodes to panic and crash. The vulnerability occurs when weak pointers to parent blocks become invalid after the read lock is released but before they are accessed, leading to an unrecoverable panic.

## Finding Description

The vulnerability exists in the `BlockStore::insert_block()` method where an `OrderedBlockWindow` containing weak pointers to parent blocks is created while holding a read lock, but those pointers are accessed after the lock is released.

**Vulnerable Code Flow:**

When inserting a new block, the code:
1. Acquires a read lock on the block tree
2. Calls `get_ordered_block_window()` to create an `OrderedBlockWindow` with weak pointers to parent blocks
3. **Releases the read lock** (lock scope ends)
4. Calls `block_window.blocks()` to access those weak pointers for payload prefetching [1](#0-0) 

The `OrderedBlockWindow` stores weak pointers (downgraded from `Arc<PipelinedBlock>`) to avoid circular references: [2](#0-1) 

**Race Condition Window:**

Between the lock release (line 424) and the `blocks()` call (line 425), another thread can:
1. Commit blocks via `commit_callback()` or call `prune_tree()`
2. Acquire a write lock
3. Call `process_pruned_blocks()` which removes blocks from the `id_to_block` HashMap
4. Drop the last `Arc` references to those blocks, deallocating them [3](#0-2) [4](#0-3) 

**Unconditional Panic:**

When `block_window.blocks()` attempts to upgrade the weak pointers, if the blocks have been removed from memory, the upgrade fails and the code unconditionally panics: [5](#0-4) 

The same panic occurs in `pipelined_blocks()`: [6](#0-5) 

This breaks the **Availability** invariant by causing deterministic node crashes during normal consensus operations.

## Impact Explanation

**Severity: High** (Node Crashes affecting validator availability)

This vulnerability causes complete node process crashes with the following impacts:

1. **Validator Unavailability**: When a validator node crashes, it cannot participate in consensus, reducing the network's Byzantine fault tolerance margin and potentially affecting consensus progress if multiple validators are affected.

2. **Service Disruption**: Under high throughput conditions where blocks are rapidly committed and pruned, the race window is hit more frequently, potentially causing repeated crashes and persistent unavailability.

3. **No Attacker Required**: This vulnerability triggers naturally during normal consensus operations when block insertion and pruning operations interleave - no malicious actor or special inputs are needed.

4. **Unrecoverable**: The panic is unhandled and crashes the entire node process, requiring manual restart.

This aligns with the Aptos bug bounty **High Severity** category: "API crashes" - though this is more severe as it crashes the entire validator node process, not just the API layer.

## Likelihood Explanation

**Likelihood: Medium**

The race condition occurs naturally during consensus operations:

- **Frequency**: The race window exists on every `insert_block()` call that performs payload prefetching (when `window_size` is set)
- **High Throughput Amplification**: With thousands of blocks per second in production environments, the small race window (between lock release and weak pointer access) has non-negligible probability of being hit
- **Pruning Triggers**: More likely when `max_pruned_blocks_in_mem` threshold is reached, causing aggressive pruning
- **Network Conditions**: Delayed block propagation combined with rapid commits increases the likelihood of parent blocks being pruned while new blocks are being inserted

While the race window is measured in microseconds, the operations are concurrent and frequent enough that the vulnerability can manifest in production under normal load conditions.

## Recommendation

Hold the read lock for the duration of weak pointer access, or use a different synchronization mechanism:

**Option 1: Extend lock scope**
```rust
let blocks = {
    let guard = self.inner.read();
    let block_window = guard.get_ordered_block_window(&block, self.window_size)?;
    block_window.blocks() // Access while still holding lock
};
```

**Option 2: Clone Arc references instead of downgrading**
Modify `OrderedBlockWindow` to store `Arc<PipelinedBlock>` instead of `Weak`, accepting the temporary circular reference during the window's short lifetime.

**Option 3: Add validation**
Check if blocks still exist before accessing:
```rust
pub fn blocks(&self) -> Result<Vec<Block>, anyhow::Error> {
    let mut blocks: Vec<Block> = vec![];
    for (block_id, block) in self.blocks.iter() {
        let upgraded_block = block.upgrade()
            .context(format!("Block {} was pruned during window access", block_id))?;
        blocks.push(upgraded_block.block().clone());
    }
    Ok(blocks)
}
```

## Proof of Concept

While a full concurrent PoC would require complex test infrastructure, the vulnerability is evident from the code structure:

```rust
// Thread A: insert_block()
let block_window = self.inner.read()  // Lock acquired
    .get_ordered_block_window(&block, self.window_size)?;  // Lock released here

// Thread B can now prune blocks
// self.inner.write().process_pruned_blocks() removes blocks from memory

let blocks = block_window.blocks();  // PANIC if blocks were pruned
```

The race is demonstrable by:
1. Inserting a block (triggering `insert_block()`)
2. Immediately committing and pruning its parent blocks in a concurrent thread
3. Observing the panic when `block_window.blocks()` attempts to upgrade the now-invalid weak pointers

The unconditional panic at line 168-171 of `pipelined_block.rs` confirms the crash is deterministic once the race condition is triggered.

## Notes

- This vulnerability affects all nodes that participate in consensus (validators and fullnodes with consensus enabled)
- The issue is not in `ExecutionWaitPhase` directly, but in the block insertion path that feeds blocks into the execution pipeline
- No existing tests cover concurrent `insert_block()` and `prune_tree()` operations
- The weak pointer pattern is intentional for memory management, but the TOCTOU gap makes it unsafe

### Citations

**File:** consensus/src/block_storage/block_store.rs (L421-425)
```rust
        let block_window = self
            .inner
            .read()
            .get_ordered_block_window(&block, self.window_size)?;
        let blocks = block_window.blocks();
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L136-149)
```rust
pub struct OrderedBlockWindow {
    /// `block_id` (HashValue) helps with logging in the unlikely case there are issues upgrading
    /// the `Weak` pointer (we can use `block_id`)
    blocks: Vec<(HashValue, Weak<PipelinedBlock>)>,
}

impl OrderedBlockWindow {
    pub fn new(blocks: Vec<Arc<PipelinedBlock>>) -> Self {
        Self {
            blocks: blocks
                .iter()
                .map(|x| (x.id(), Arc::downgrade(x)))
                .collect::<Vec<(HashValue, Weak<PipelinedBlock>)>>(),
        }
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L161-174)
```rust
    pub fn blocks(&self) -> Vec<Block> {
        let mut blocks: Vec<Block> = vec![];
        for (block_id, block) in self.blocks.iter() {
            let upgraded_block = block.upgrade();
            if let Some(block) = upgraded_block {
                blocks.push(block.block().clone())
            } else {
                panic!(
                    "Block with id: {} not found during upgrade in OrderedBlockWindow::blocks()",
                    block_id
                )
            }
        }
        blocks
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L177-189)
```rust
    pub fn pipelined_blocks(&self) -> Vec<Arc<PipelinedBlock>> {
        let mut blocks: Vec<Arc<PipelinedBlock>> = Vec::new();
        for (block_id, block) in self.blocks.iter() {
            if let Some(block) = block.upgrade() {
                blocks.push(block);
            } else {
                panic!(
                    "Block with id: {} not found during upgrade in OrderedBlockWindow::pipelined_blocks()",
                    block_id
                )
            }
        }
        blocks
```

**File:** consensus/src/block_storage/block_tree.rs (L174-181)
```rust
    fn remove_block(&mut self, block_id: HashValue) {
        // Remove the block from the store
        if let Some(block) = self.id_to_block.remove(&block_id) {
            let round = block.executed_block().round();
            self.round_to_ids.remove(&round);
        };
        self.id_to_quorum_cert.remove(&block_id);
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L496-509)
```rust
    pub(super) fn process_pruned_blocks(&mut self, mut newly_pruned_blocks: VecDeque<HashValue>) {
        counters::NUM_BLOCKS_IN_TREE.sub(newly_pruned_blocks.len() as i64);
        // The newly pruned blocks are pushed back to the deque pruned_block_ids.
        // In case the overall number of the elements is greater than the predefined threshold,
        // the oldest elements (in the front of the deque) are removed from the tree.
        self.pruned_block_ids.append(&mut newly_pruned_blocks);
        if self.pruned_block_ids.len() > self.max_pruned_blocks_in_mem {
            let num_blocks_to_remove = self.pruned_block_ids.len() - self.max_pruned_blocks_in_mem;
            for _ in 0..num_blocks_to_remove {
                if let Some(id) = self.pruned_block_ids.pop_front() {
                    self.remove_block(id);
                }
            }
        }
```
