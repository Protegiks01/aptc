# Audit Report

## Title
Block Partitioner Wraparound Bug: Missing Writes in Gap Range Allows Cross-Shard Conflicts

## Summary
The `has_write_in_range()` function in the V2 block partitioner contains a critical semantic error when checking for pending writes across shard boundaries. When `start_txn_id > end_txn_id`, the wrapped range implementation checks `[start_txn_id, ∞) ∪ [0, end_txn_id)`, which misses pending writes in indices between `end_txn_id` and `start_txn_id`. This allows undetected cross-shard dependencies, enabling conflicting transactions to execute in parallel within the same round, causing non-deterministic execution across validators.

## Finding Description

The block partitioner's V2 implementation uses `has_write_in_range()` to detect pending writes to storage locations before placing transactions into execution rounds. [1](#0-0) 

This function is invoked by `key_owned_by_another_shard()` during the discarding round phase to identify cross-shard conflicts. [2](#0-1) 

The wrapped case implementation checks `[start_txn_id, ∞) ∪ [0, end_txn_id)` via two separate range queries, missing writes in the gap range `[end_txn_id, start_txn_id)`. [3](#0-2) 

The function is called during round processing to determine if transactions should be discarded to avoid cross-shard conflicts. [4](#0-3) 

**Attack Scenario:**

With 4 shards (boundaries at indices 0, 3, 6, 9) and key K with `anchor_shard_id=2`:
1. Transaction at index 4 (shard 1) writes to K, added to `pending_writes[K]={4}` during initialization
2. When processing transaction at index 1 (shard 0) that reads K:
   - `range_start = start_txn_idxs_by_shard[2] = 6`
   - `range_end = start_txn_idxs_by_shard[0] = 0`
   - Calls `has_write_in_range(6, 0)` which checks `[6, ∞) ∪ [0, 0)` = `[6, ∞)`
   - Write at index 4 is NOT in `[6, ∞)`, so returns false
   - Transaction incorrectly accepted into round 0

Both transactions execute in parallel within round 0 in different shards, creating an undetected read-write conflict. The partitioner documentation confirms the expectation of no in-round cross-shard dependencies. [5](#0-4) 

The test suite validates this invariant by asserting that required edges in non-final rounds must come from different rounds. [6](#0-5) 

However, the test only validates that explicitly recorded dependencies are correct, not that all necessary dependencies are detected, allowing this bug to remain undetected.

## Impact Explanation

**Severity: Critical - Consensus/Safety Violations**

This bug directly violates Aptos's fundamental consensus safety guarantee that all validators must produce identical state roots for identical blocks. When cross-shard read-write conflicts go undetected:

1. **Non-deterministic Execution**: Validators may execute conflicting transactions in different relative orders within parallel shard execution, producing different intermediate states and final state roots
2. **Consensus Failure**: Validators with divergent state roots cannot form consensus on block commitment, preventing chain progress
3. **Network Partition Risk**: Sustained consensus failures could require manual intervention or hard fork to resolve

This meets the **Consensus/Safety Violations (Critical)** category in the Aptos Bug Bounty program, covering scenarios where different validators produce different state roots for the same block with less than 1/3 Byzantine validators.

## Likelihood Explanation

**Likelihood: High**

This bug triggers automatically during normal block processing without any attacker manipulation:

1. **Frequent Condition**: When `anchor_shard_id > shard_id` (occurs in approximately 50% of cases due to random anchor assignment via hash) [7](#0-6) 
2. **Common Pattern**: Transactions accessing shared storage locations across different pre-partitioned shards occur frequently in production
3. **No Special Setup**: The bug manifests in the core partitioning logic used for all blocks with sharded execution enabled [8](#0-7) 

Given Aptos's transaction throughput and cross-shard access patterns, the conditions for triggering this bug likely occur in production blocks regularly.

## Recommendation

The `key_owned_by_another_shard` function should check for writes in ALL shards except the current shard, not just shards "between" the anchor and current shard. The fix should modify the range calculation to check the entire pre-partitioned transaction space excluding only the current shard's range:

```rust
pub(crate) fn key_owned_by_another_shard(&self, shard_id: ShardId, key: StorageKeyIdx) -> bool {
    let tracker_ref = self.trackers.get(&key).unwrap();
    let tracker = tracker_ref.read().unwrap();
    let shard_start = self.start_txn_idxs_by_shard[shard_id];
    let shard_end = if shard_id + 1 < self.num_executor_shards {
        self.start_txn_idxs_by_shard[shard_id + 1]
    } else {
        self.ori_idxs_by_pre_partitioned.len()
    };
    // Check for writes OUTSIDE the current shard [shard_start, shard_end)
    tracker.has_write_in_range(0, shard_start) || 
    tracker.has_write_in_range(shard_end, self.ori_idxs_by_pre_partitioned.len())
}
```

## Proof of Concept

A test demonstrating this vulnerability would create 4 shards with a storage key assigned to anchor shard 2, place a write transaction in shard 1 and a read transaction in shard 0, then verify that both transactions incorrectly end up in the same round despite having a read-write conflict. The existing test framework in `test_utils.rs` can be extended to detect such violations by checking not just that recorded dependencies are correct, but that all necessary dependencies are detected.

### Citations

**File:** execution/block-partitioner/src/v2/conflicting_txn_tracker.rs (L70-84)
```rust
    pub fn has_write_in_range(
        &self,
        start_txn_id: PrePartitionedTxnIdx,
        end_txn_id: PrePartitionedTxnIdx,
    ) -> bool {
        if start_txn_id <= end_txn_id {
            self.pending_writes
                .range(start_txn_id..end_txn_id)
                .next()
                .is_some()
        } else {
            self.pending_writes.range(start_txn_id..).next().is_some()
                || self.pending_writes.range(..end_txn_id).next().is_some()
        }
    }
```

**File:** execution/block-partitioner/src/v2/state.rs (L210-217)
```rust
    /// For a key, check if there is any write between the anchor shard and a given shard.
    pub(crate) fn key_owned_by_another_shard(&self, shard_id: ShardId, key: StorageKeyIdx) -> bool {
        let tracker_ref = self.trackers.get(&key).unwrap();
        let tracker = tracker_ref.read().unwrap();
        let range_start = self.start_txn_idxs_by_shard[tracker.anchor_shard_id];
        let range_end = self.start_txn_idxs_by_shard[shard_id];
        tracker.has_write_in_range(range_start, range_end)
    }
```

**File:** execution/block-partitioner/src/v2/partition_to_matrix.rs (L87-89)
```rust
        // Overview of the logic:
        // 1. Key conflicts are analyzed and a txn from `remaining_txns` either goes to `discarded` or `tentatively_accepted`.
        // 2. Relative orders of txns from the same sender are analyzed and a txn from `tentatively_accepted` either goes to `finally_accepted` or `discarded`.
```

**File:** execution/block-partitioner/src/v2/partition_to_matrix.rs (L116-126)
```rust
                    txn_idxs.into_par_iter().for_each(|txn_idx| {
                        let ori_txn_idx = state.ori_idxs_by_pre_partitioned[txn_idx];
                        let mut in_round_conflict_detected = false;
                        let write_set = state.write_sets[ori_txn_idx].read().unwrap();
                        let read_set = state.read_sets[ori_txn_idx].read().unwrap();
                        for &key_idx in write_set.iter().chain(read_set.iter()) {
                            if state.key_owned_by_another_shard(shard_id, key_idx) {
                                in_round_conflict_detected = true;
                                break;
                            }
                        }
```

**File:** execution/block-partitioner/src/test_utils.rs (L222-224)
```rust
                    if round_id != num_rounds - 1 {
                        assert_ne!(src_txn_idx.round_id, round_id);
                    }
```

**File:** execution/block-partitioner/src/lib.rs (L39-43)
```rust
fn get_anchor_shard_id(storage_location: &StorageLocation, num_shards: usize) -> ShardId {
    let mut hasher = DefaultHasher::new();
    storage_location.hash(&mut hasher);
    (hasher.finish() % num_shards as u64) as usize
}
```

**File:** execution/block-partitioner/src/v2/mod.rs (L177-180)
```rust
        // Step 4: remove cross-shard dependencies by move some txns into new rounds.
        // As a result, we get a txn matrix of no more than `self.max_partitioning_rounds` rows and exactly `num_executor_shards` columns.
        // It's guaranteed that inside every round other than the last round, there's no cross-shard dependency. (But cross-round dependencies are always possible.)
        Self::remove_cross_shard_dependencies(&mut state);
```
