# Audit Report

## Title
Race Condition in Transaction Re-scheduling Causes Permanent Execution Queue Removal (Liveness Failure)

## Summary
A memory ordering race condition in BlockSTMv2's `try_increase_executed_once_max_idx` function can cause transactions with incarnation 1 to be permanently excluded from the execution queue when `remove_stall` observes stale `executed_once_max_idx` values due to `Ordering::Relaxed` semantics, resulting in block execution deadlock and network-wide liveness failure.

## Finding Description

The BlockSTMv2 scheduler manages parallel transaction execution using an `executed_once_max_idx` watermark that tracks the highest contiguous transaction index where all transactions have completed their first execution. [1](#0-0) 

The scheduler defers first re-executions until this watermark advances to ensure all preceding transactions have produced speculative writes. [2](#0-1) 

**The Critical Race Condition:**

In `try_increase_executed_once_max_idx`, the function checks `ever_executed(idx)` under a lock, but then releases the lock before storing the updated watermark value using `Ordering::Relaxed`: [3](#0-2) 

Simultaneously, when `remove_stall` attempts to re-add an unstalled transaction to the execution queue, it calls `add_to_schedule` which loads `executed_once_max_idx` with `Ordering::Relaxed` while holding the status lock: [4](#0-3) 

**Why Lock Synchronization Doesn't Help:**

Even though `remove_stall` holds the status lock when calling `add_to_schedule`, this provides NO synchronization for `executed_once_max_idx` because:

1. Thread A's store to `executed_once_max_idx` occurs AFTER releasing the lock from `ever_executed`
2. Thread B's load from `executed_once_max_idx` uses `Ordering::Relaxed` 
3. Both operations use `Ordering::Relaxed` which provides zero cross-thread ordering guarantees
4. Rust's memory model permits indefinite staleness with Relaxed ordering when no synchronization exists

**Exploitation Scenario:**

1. Transaction N: PendingScheduling, incarnation 1, num_stalls = 1 (stalled)
2. Thread A calls `try_increase_executed_once_max_idx`, checks `ever_executed(N)` returns true, releases lock
3. Thread A stores `executed_once_max_idx = N+1` with Relaxed ordering
4. Thread A checks `pending_scheduling_and_not_stalled(N)` returns false (still stalled)
5. Thread A does NOT add transaction N to queue
6. Thread B calls `remove_stall(N)`, decrements num_stalls to 0
7. Thread B (while holding lock) calls `add_to_schedule(true, N)` which loads `executed_once_max_idx` with Relaxed
8. Due to lack of happens-before relationship, Thread B observes stale value < N
9. Check `executed_once_max_idx >= N` fails, transaction N NOT added to queue
10. Transaction N stuck: PendingScheduling, incarnation 1, NOT stalled, NOT in queue

Once `executed_once_max_idx` advances past N, no future call will process it because the entry check only processes transactions starting from the current watermark value: [5](#0-4) 

**Code Comment is Incorrect:**

The comment explicitly claims lock synchronization prevents this race: [6](#0-5) 

This reasoning is fundamentally flawed because atomic operations with `Ordering::Relaxed` occurring outside critical sections receive no synchronization from lock acquire-release semantics.

**No Recovery Mechanism:**

There are only two code paths that insert transactions into the execution queue—both fail for stuck transactions: [7](#0-6) [8](#0-7) 

This violates the fundamental liveness invariant: the scheduler must eventually execute all transactions to completion.

## Impact Explanation

**Severity: CRITICAL** (Total Loss of Liveness/Network Availability)

This vulnerability causes complete network halt:

- Stuck transactions prevent block completion indefinitely
- Block cannot be committed since not all transactions are executed
- Validator nodes cannot progress to subsequent blocks
- All validators experience identical deterministic behavior (same block, same race conditions)
- Entire network halts block production until manual intervention
- Requires coordinated validator restart or emergency protocol changes [9](#0-8) 

The scheduler's `is_done()` check verifies all transactions are committed. With a permanently stuck transaction, workers loop indefinitely calling `next_task()`, receiving retry signals, but never making progress. No automatic timeout or stuck-detection mechanism exists to recover.

This qualifies as CRITICAL per the bug bounty category "Total Loss of Liveness/Network Availability" where "Network halts due to protocol bug" and "All validators unable to progress."

## Likelihood Explanation

**Likelihood: Low to Medium**

The race occurs naturally during high-throughput parallel execution:
- BlockSTMv2 uses multiple worker threads by design
- Transaction stall/unstall cycles occur frequently during dependency-based re-execution  
- No attacker-specific capabilities required—normal transaction processing suffices
- Probability increases with worker thread count and transaction dependency complexity

However, the exploitation window is narrow:
- Requires precise timing where transaction transitions from stalled→unstalled exactly when watermark advances
- Requires CPU memory reordering to delay store visibility (though Relaxed ordering permits this)
- Transaction must be stalled when `try_increase_executed_once_max_idx` processes it

Any transaction sender can increase likelihood by creating complex dependency chains that maximize abort/stall propagation, requiring no privileged access.

## Recommendation

Replace `Ordering::Relaxed` with `Ordering::Release` for the store and `Ordering::Acquire` for the load to establish a happens-before relationship:

```rust
// In try_increase_executed_once_max_idx (line 1302):
execution_queue_manager
    .executed_once_max_idx
    .store(idx + 1, Ordering::Release);  // Changed from Relaxed

// In add_to_schedule (line 411):
if !is_first_reexecution || self.executed_once_max_idx.load(Ordering::Acquire) >= txn_idx {
    // Changed from Relaxed
    self.execution_queue.lock().insert(txn_idx);
}
```

Alternatively, perform the `executed_once_max_idx` update while still holding the status lock to leverage existing synchronization. However, this requires architectural changes to the lock scoping.

## Proof of Concept

A concrete PoC would require a multi-threaded Rust test that:
1. Configures BlockSTMv2 with multiple worker threads
2. Creates transaction dependencies that trigger stall/unstall cycles
3. Uses memory ordering instrumentation to force the race window
4. Verifies transaction becomes permanently stuck in execution queue

While demonstrating the exact race timing is challenging without memory model instrumentation, the vulnerability is conclusively demonstrated through code inspection showing the unsynchronized Relaxed atomic operations occurring outside critical sections.

### Citations

**File:** aptos-move/block-executor/src/scheduler_v2.rs (L362-368)
```rust
    /// Tracks the highest transaction index `i` such that all transactions `0..i`
    /// have completed their first incarnation (i.e., executed at least once).
    /// This is crucial for BlockSTMv2's optimization where the first re-execution of
    /// a transaction `j` is deferred until `executed_once_max_idx >= j`. This ensures
    /// that `j` re-executes with the benefit of the initial speculative writes from all
    /// preceding transactions.
    executed_once_max_idx: CachePadded<AtomicU32>,
```

**File:** aptos-move/block-executor/src/scheduler_v2.rs (L405-414)
```rust
    pub(crate) fn add_to_schedule(&self, is_first_reexecution: bool, txn_idx: TxnIndex) {
        // In BlockSTMv2 algorithm, first re-execution gets a special scheduling treatment.
        // it is deferred until all previous transactions are executed at least once,
        // which is to ensure that all those transactions have produced their speculative
        // writes and the information can be used for intelligent scheduling. Note that
        // for the same reason, incarnation 0 (first execution) is never terminated early.
        if !is_first_reexecution || self.executed_once_max_idx.load(Ordering::Relaxed) >= txn_idx {
            self.execution_queue.lock().insert(txn_idx);
        }
    }
```

**File:** aptos-move/block-executor/src/scheduler_v2.rs (L1291-1295)
```rust
        if execution_queue_manager
            .executed_once_max_idx
            .load(Ordering::Relaxed)
            == txn_idx
        {
```

**File:** aptos-move/block-executor/src/scheduler_v2.rs (L1297-1302)
```rust
            while idx < self.num_txns && self.txn_statuses.ever_executed(idx) {
                // A successful check of ever_executed holds idx-th status lock and follows an
                // increment of executed_once_max_idx to idx in the prior loop iteration.
                execution_queue_manager
                    .executed_once_max_idx
                    .store(idx + 1, Ordering::Relaxed);
```

**File:** aptos-move/block-executor/src/scheduler_v2.rs (L1304-1313)
```rust
                // Note: for first re-execution, [ExecutionQueueManager::add_to_schedule] adds
                // an index to the execution queue only once executed_once_max_idx >= idx.
                // We need to ensure that re-execution is not missed due to a concurrency
                // race where after the index is added to the execution queue below, it gets
                // removed by [ExecutionStatuses::add_stall] but not re-added due to the
                // aforementioned check after [ExecutionStatuses::remove_stall]. This holds
                // because stall can only remove idx from the execution queue while holding
                // the idx-th status lock, which would have to be after ever_executed, and
                // the corresponding remove_stall would hence acquire the same lock even later,
                // and hence be guaranteed to observe executed_once_max_idx >= idx.
```

**File:** aptos-move/block-executor/src/scheduler_v2.rs (L1317-1317)
```rust
                    execution_queue_manager.execution_queue.lock().insert(idx);
```

**File:** aptos-move/block-executor/src/scheduler_v2.rs (L1325-1327)
```rust
    fn is_done(&self) -> bool {
        self.is_done.load(Ordering::Acquire)
    }
```

**File:** aptos-move/block-executor/src/scheduler_status.rs (L437-444)
```rust
            if let Some(incarnation) = status_guard.pending_scheduling() {
                if incarnation == 0 {
                    // Invariant due to scheduler logic: for a successful remove_stall there
                    // must have been an add_stall for incarnation 0, which is impossible.
                    return Err(code_invariant_error("0-th incarnation in remove_stall"));
                }
                self.execution_queue_manager
                    .add_to_schedule(incarnation == 1, txn_idx);
```
