# Audit Report

## Title
Unbounded Module Cache in Consensus Pipeline Enables Memory Exhaustion DoS Attack

## Summary
The consensus pipeline's randomness check feature uses a `CachedModuleView` with an unbounded `UnsyncModuleCache` that accumulates deserialized modules across blocks without size limits or eviction. Attackers can deploy thousands of unique modules and submit transactions invoking them, causing validators to cache unlimited compiled module data in memory, leading to performance degradation and potential OOM crashes.

## Finding Description

The vulnerability exists in the consensus pipeline's randomness checking optimization introduced in `OnChainConsensusConfig::V5`. The `rand_check` function loads modules for every transaction's entry function into a shared, persistent cache with no memory constraints.

**Vulnerable Components:**

1. **Unbounded Cache Structure**: The `CachedModuleView` uses `UnsyncModuleCache<ModuleId, CompiledModule, Module, AptosModuleExtension, usize>` [1](#0-0)  which is implemented as `RefCell<HashMap<K, VersionedModuleCode<DC, VC, E, V>>>` with no capacity limits or eviction policy [2](#0-1) .

2. **Persistent Shared Cache**: The module cache is created once in `PipelineBuilder::new()` and shared across all blocks processed by that builder instance [3](#0-2) [4](#0-3) .

3. **Module Loading in Consensus**: During `rand_check`, the pipeline iterates through user transactions and loads modules via `unmetered_get_deserialized_module()` without any size checks [5](#0-4) .

4. **Limited Cache Reset**: The cache only resets on state view incompatibility (fork scenarios), not based on size. During normal linear chain progression, only `reset_state_view()` is called which preserves the cache contents [6](#0-5) [7](#0-6) .

5. **Feature Enabled by Default**: The `rand_check_enabled` flag defaults to `true` in the V5 consensus configuration [8](#0-7) .

**Critical Distinction**: This vulnerability affects the `CachedModuleView` used in the consensus pipeline, which is distinct from the `GlobalModuleCache` used in block execution. The `GlobalModuleCache` has size limits (1GB default) and is managed by `ModuleCacheManager` [9](#0-8) , while the consensus pipeline's cache has no such protections.

**Attack Flow:**

1. Attacker deploys thousands of unique Move modules at different addresses (paying storage fees)
2. Attacker submits transactions calling entry functions from these unique modules
3. When validators process blocks via `rand_check()`, each transaction's module is loaded and cached
4. Each unique module (unique `ModuleId`) adds a new entry to the HashMap with no eviction
5. Cache grows continuously across blocks in linear chain operation (only cleared on forks)
6. Memory consumption increases with each new unique module invoked
7. Eventually causes memory pressure, GC overhead, performance degradation, or OOM crashes

**Broken Invariants:**
- **Resource Limits**: All consensus operations should respect memory bounds, but module caching is unbounded
- **DoS Protection**: The unmetered loading allows unlimited memory allocation without proper constraints
- **Consensus Availability**: Validator OOM crashes can disrupt block production and finalization

## Impact Explanation

This vulnerability has **HIGH to CRITICAL severity**:

**HIGH Severity ($50,000)**: Directly causes "Validator node slowdowns" as defined in the Aptos bug bounty program. As the cache grows to hundreds of megabytes or gigabytes, validators experience:
- Increased garbage collection pressure
- Memory allocation slowdowns
- Degraded block processing performance
- Higher latency in consensus rounds

**CRITICAL Severity ($1,000,000)**: Can escalate to "Total loss of liveness/network availability" if multiple validators experience OOM crashes simultaneously. If enough validators crash:
- Block production halts
- Consensus cannot reach quorum
- Network becomes unavailable
- Requires validator restarts to recover

The attack is economically feasible: deploying 10,000 small modules costs approximately 400 APT (storage fees), or roughly $3,000-4,000 at current prices. Scaling to 100,000 modules (~$40,000) could exhaust gigabytes of memory on all validators. This is a protocol-level resource exhaustion vulnerability, distinct from network-level DoS attacks which are out of scope.

## Likelihood Explanation

**Likelihood: HIGH**

1. **Low Barrier to Entry**: Any user with sufficient APT can deploy modules and submit transactions - no special privileges or validator access required
2. **Economic Feasibility**: Storage fees are predictable and relatively low compared to the severe impact on network availability
3. **Operational Stealth**: Module deployment and transaction submission are legitimate protocol operations that won't trigger anomaly detection
4. **Network-Wide Impact**: All validators process the same blocks, so all validators accumulate the same modules in their caches
5. **Persistent Accumulation**: Cache grows across blocks in normal operation and is only cleared during fork scenarios, allowing steady accumulation over time
6. **No Built-in Mitigation**: No automatic size-based eviction, no memory monitoring, no circuit breakers in the `UnsyncModuleCache` implementation

The attack is highly practical and can be executed incrementally (e.g., deploy 1,000 modules per day) to avoid detection while steadily exhausting validator memory.

## Recommendation

Implement size limits and eviction policy for the consensus pipeline's module cache:

1. **Add Size Tracking**: Track the total memory consumption of cached modules similar to `GlobalModuleCache`
2. **Implement Eviction Policy**: Use LRU (Least Recently Used) eviction when cache exceeds size threshold
3. **Configure Size Limits**: Set reasonable maximum cache size (e.g., 100MB-500MB) to prevent unbounded growth
4. **Add Monitoring**: Emit metrics when cache size exceeds thresholds to detect potential attacks
5. **Consider Alternative Design**: Reuse the bounded `GlobalModuleCache` infrastructure if architecturally feasible

Example fix approach:
```rust
// In CachedModuleView, add size tracking field
max_cache_size_bytes: usize,
current_cache_size_bytes: AtomicUsize,

// Before inserting module, check size limit
if self.current_cache_size_bytes.load() + module_size > self.max_cache_size_bytes {
    // Implement LRU eviction or clear least recently used entries
    self.evict_lru_modules(module_size);
}
```

## Proof of Concept

The vulnerability can be demonstrated through the following attack scenario:

```rust
// 1. Deploy N unique modules at different addresses
for i in 0..10000 {
    let module_address = generate_unique_address(i);
    deploy_module(module_address, create_small_module(i));
}

// 2. Submit transactions invoking entry functions from each module
for i in 0..10000 {
    let module_address = generate_unique_address(i);
    submit_transaction(entry_function_call(module_address, "dummy_function"));
}

// 3. Validators process blocks containing these transactions
// 4. During rand_check, each unique module is loaded via unmetered_get_deserialized_module()
// 5. Cache grows: 10,000 modules × ~50KB average = ~500MB memory growth
// 6. Repeat with 100,000 modules → ~5GB memory growth → significant validator slowdowns
// 7. Continue until validators experience OOM crashes
```

The attack exploits the fact that the `UnsyncModuleCache` HashMap will grow indefinitely as each unique `ModuleId` is encountered, with no eviction occurring during normal linear chain progression.

### Citations

**File:** aptos-move/aptos-resource-viewer/src/module_view.rs (L101-102)
```rust
    pub module_cache:
        UnsyncModuleCache<ModuleId, CompiledModule, Module, AptosModuleExtension, usize>,
```

**File:** aptos-move/aptos-resource-viewer/src/module_view.rs (L123-125)
```rust
    pub fn reset_state_view(&mut self, state_view: S) {
        self.state_view = state_view;
    }
```

**File:** third_party/move/move-vm/types/src/code/cache/module_cache.rs (L210-226)
```rust
/// Non-[Sync] version of module cache suitable for sequential execution.
pub struct UnsyncModuleCache<K, DC, VC, E, V> {
    module_cache: RefCell<HashMap<K, VersionedModuleCode<DC, VC, E, V>>>,
}

impl<K, DC, VC, E, V> UnsyncModuleCache<K, DC, VC, E, V>
where
    K: Eq + Hash + Clone,
    VC: Deref<Target = Arc<DC>>,
    V: Clone + Default + Ord,
{
    /// Returns an empty module cache.
    pub fn empty() -> Self {
        Self {
            module_cache: RefCell::new(HashMap::new()),
        }
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L139-139)
```rust
    module_cache: Arc<Mutex<Option<CachedModuleView<CachedStateView>>>>,
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L267-267)
```rust
        let module_cache = Arc::new(Mutex::new(None));
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L719-726)
```rust
                if previous_state_view == expected_state_view {
                    cache_mut.reset_state_view(parent_state_view);
                } else {
                    counters::RAND_BLOCK
                        .with_label_values(&["reset_cache"])
                        .inc();
                    cache_mut.reset_all(parent_state_view);
                }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L732-755)
```rust
            for txn in user_txns.iter() {
                if let Some(txn) = txn.borrow_into_inner().try_as_signed_user_txn() {
                    if let Ok(TransactionExecutableRef::EntryFunction(entry_fn)) =
                        txn.executable_ref()
                    {
                        // use the deserialized API to avoid cloning the metadata
                        // should migrate once we move metadata into the extension and avoid cloning
                        if let Ok(Some(module)) = cache_ref.unmetered_get_deserialized_module(
                            entry_fn.module().address(),
                            entry_fn.module().name(),
                        ) {
                            if get_randomness_annotation_for_entry_function(
                                entry_fn,
                                &module.metadata,
                            )
                            .is_some()
                            {
                                has_randomness = true;
                                break;
                            }
                        }
                    }
                }
            }
```

**File:** types/src/on_chain_config/consensus_config.rs (L217-223)
```rust
    pub fn default_for_genesis() -> Self {
        OnChainConsensusConfig::V5 {
            alg: ConsensusAlgorithmConfig::default_for_genesis(),
            vtxn: ValidatorTxnConfig::default_for_genesis(),
            window_size: DEFAULT_WINDOW_SIZE,
            rand_check_enabled: true,
        }
```

**File:** types/src/block_executor/config.rs (L31-49)
```rust
impl Default for BlockExecutorModuleCacheLocalConfig {
    fn default() -> Self {
        Self {
            prefetch_framework_code: true,
            // Use 1Gb for now, should be large enough to cache all mainnet modules (at the time
            // of writing this comment, 13.11.24).
            max_module_cache_size_in_bytes: 1024 * 1024 * 1024,
            max_struct_name_index_map_num_entries: 1_000_000,
            // Each entry is 4 + 2 * 8 = 20 bytes. This allows ~200 Mb of distinct types.
            max_interned_tys: 10 * 1024 * 1024,
            // Use slightly less for vectors of types.
            max_interned_ty_vecs: 4 * 1024 * 1024,
            // Maximum number of cached layouts.
            max_layout_cache_size: 4_000_000,
            // Maximum number of module IDs to intern.
            max_interned_module_ids: 100_000,
        }
    }
}
```
