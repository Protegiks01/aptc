# Audit Report

## Title
Remote Executor Indefinite Blocking Causes Consensus Round Timeouts and Thread Pool Exhaustion

## Summary
The `RemoteExecutorClient` uses blocking channel receives without timeout when waiting for execution results from remote shards. When remote shards are slow, crashed, or experience network issues, block execution hangs indefinitely while consensus continues to timeout rounds. This leads to unnecessary round changes, accumulation of blocked execution tasks, and eventual thread pool exhaustion causing validator slowdowns.

## Finding Description

The vulnerability exists in the remote executor architecture where block execution is distributed across multiple remote shards for horizontal scaling performance. The critical flaw is in the result collection mechanism.

The `get_output_from_shards()` method performs blocking `rx.recv().unwrap()` calls without any timeout mechanism. [1](#0-0)  These channels are unbounded crossbeam channels created by the NetworkController for receiving execution results from remote shards. [2](#0-1) 

Although the NetworkController is initialized with a `timeout_ms` parameter (5000ms), this timeout only applies to the GRPC layer for network communication, not to the channel receive operations. [3](#0-2) [4](#0-3) 

Meanwhile, consensus operates with round timeouts managed by `RoundState`. The consensus timeout mechanism schedules timeouts and triggers `process_local_timeout()` when rounds take too long. [5](#0-4) 

The execution pipeline uses `tokio::task::spawn_blocking` to execute blocks on a dedicated blocking thread pool. [6](#0-5) 

When block execution via remote shards takes longer than the consensus timeout:

1. **Execution blocks indefinitely**: The `rx.recv()` call waits forever for a result from a slow/crashed remote shard
2. **Consensus continues**: The round timeout fires, and consensus moves to the next round
3. **No pipeline abort**: The `process_local_timeout()` method does not abort pending execution pipelines [7](#0-6) 
4. **Resources accumulate**: Each blocked execution task consumes a thread from the limited thread pool of 64 threads [8](#0-7) 

Critically, aborting tokio tasks that wrap `spawn_blocking` cannot interrupt the blocking operation itself - the threads remain stuck on `rx.recv()` until the channel receives data or is closed.

This creates a resource leak where multiple rounds can timeout while execution tasks accumulate in the blocked state, eventually exhausting the thread pool and degrading validator performance.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos Bug Bounty program, specifically under the "Validator Node Slowdowns" criterion.

**High Severity Impact - Validator Node Slowdowns**:
As execution threads accumulate in blocked state, the validator's blocking thread pool becomes exhausted. This causes:
- Severe performance degradation across all validator operations that depend on the blocking thread pool
- Increased latency in block processing and consensus participation
- Potential inability to execute blocks effectively, causing the validator to fall behind
- Degraded consensus participation that may affect validator rewards

**Operational Impact**:
If execution tasks remain permanently blocked, operators must manually restart validators to recover the thread pool, causing:
- Temporary loss of validator participation in consensus
- Need for manual intervention at scale if multiple validators are affected
- Potential missed rewards during downtime and recovery

While this does not cause immediate consensus safety violations or fund loss, it significantly degrades validator liveness and operational reliability.

## Likelihood Explanation

**Likelihood: MEDIUM** in production environments using remote execution.

This vulnerability has medium likelihood because:

1. **Optional Feature**: Remote execution is an optional feature for horizontal scaling, not enabled by default. Only validators who explicitly configure `remote_executor_addresses` are affected.

2. **Natural Trigger Conditions**: For validators using remote execution, the issue can be triggered naturally through:
   - Network latency spikes or packet loss (common in distributed systems)
   - Remote shard process crashes or restarts
   - Resource contention on remote executor machines
   - Any network partition affecting shard communication

3. **No Protection**: There is no timeout, retry logic, or circuit breaker to protect against slow/unresponsive shards.

4. **Amplification**: A single slow shard blocks the entire block execution (results are collected serially), and multiple concurrent blocks can pile up.

## Recommendation

Implement timeout mechanisms at the channel receive level to prevent indefinite blocking:

1. **Add timeout to channel receives**: Replace `rx.recv().unwrap()` with `rx.recv_timeout(Duration::from_millis(timeout_ms))` to match the gRPC timeout duration.

2. **Implement retry logic**: On timeout, retry the execution request or mark the shard as unavailable and fail gracefully.

3. **Add circuit breaker**: Track shard failures and temporarily disable problematic shards.

4. **Pipeline abort on timeout**: Modify `process_local_timeout()` to abort pending execution pipelines when consensus rounds timeout.

5. **Monitor blocked threads**: Add metrics to track the number of blocked execution threads and alert operators before exhaustion.

## Proof of Concept

A PoC would involve:

1. Starting a validator with remote execution enabled
2. Configuring multiple remote executor shards
3. Simulating network partition or crash of one remote shard after it receives an execution command but before sending results
4. Observing that the coordinator blocks indefinitely on `rx.recv()`
5. Triggering multiple consensus rounds while execution remains blocked
6. Monitoring thread pool exhaustion as blocked threads accumulate
7. Observing validator performance degradation and eventual need for manual restart

The vulnerability can be reproduced in test environments by introducing network delays or killing remote shard processes during execution.

## Notes

This is a production code bug affecting validator reliability, not a network DoS attack. The issue stems from missing error handling (timeout mechanism) in the remote executor client implementation. While remote execution is optional, it is a production-ready feature designed for horizontal scaling, making this a valid security concern for validators that choose to use it. The vulnerability directly aligns with the Aptos Bug Bounty "Validator Node Slowdowns" category.

### Citations

**File:** execution/executor-service/src/remote_executor_client.rs (L154-158)
```rust
            NetworkController::new(
                "remote-executor-coordinator".to_string(),
                coordinator_address,
                5000,
            ),
```

**File:** execution/executor-service/src/remote_executor_client.rs (L163-172)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        trace!("RemoteExecutorClient Waiting for results");
        let mut results = vec![];
        for rx in self.result_rxs.iter() {
            let received_bytes = rx.recv().unwrap().to_bytes();
            let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
            results.push(result.inner?);
        }
        Ok(results)
    }
```

**File:** secure/net/src/network_controller/mod.rs (L95-100)
```rust
    pub fn new(service: String, listen_addr: SocketAddr, timeout_ms: u64) -> Self {
        let inbound_handler = Arc::new(Mutex::new(InboundHandler::new(
            service.clone(),
            listen_addr,
            timeout_ms,
        )));
```

**File:** secure/net/src/network_controller/mod.rs (L128-137)
```rust
    pub fn create_inbound_channel(&mut self, message_type: String) -> Receiver<Message> {
        let (inbound_sender, inbound_receiver) = unbounded();

        self.inbound_handler
            .lock()
            .unwrap()
            .register_handler(message_type, inbound_sender);

        inbound_receiver
    }
```

**File:** consensus/src/liveness/round_state.rs (L233-241)
```rust
    pub fn process_local_timeout(&mut self, round: Round) -> bool {
        if round != self.current_round {
            return false;
        }
        warn!(round = round, "Local timeout");
        counters::TIMEOUT_COUNT.inc();
        self.setup_timeout(1);
        true
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L857-867)
```rust
        tokio::task::spawn_blocking(move || {
            executor
                .execute_and_update_state(
                    (block.id(), txns, auxiliary_info).into(),
                    block.parent_id(),
                    onchain_execution_config,
                )
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
```

**File:** consensus/src/round_manager.rs (L993-1090)
```rust
    pub async fn process_local_timeout(&mut self, round: Round) -> anyhow::Result<()> {
        if !self.round_state.process_local_timeout(round) {
            return Ok(());
        }

        if self.sync_only() {
            self.network
                .broadcast_sync_info(self.block_store.sync_info())
                .await;
            bail!("[RoundManager] sync_only flag is set, broadcasting SyncInfo");
        }

        if self.local_config.enable_round_timeout_msg {
            let timeout = if let Some(timeout) = self.round_state.timeout_sent() {
                timeout
            } else {
                let timeout = TwoChainTimeout::new(
                    self.epoch_state.epoch,
                    round,
                    self.block_store.highest_quorum_cert().as_ref().clone(),
                );
                let signature = self
                    .safety_rules
                    .lock()
                    .sign_timeout_with_qc(
                        &timeout,
                        self.block_store.highest_2chain_timeout_cert().as_deref(),
                    )
                    .context("[RoundManager] SafetyRules signs 2-chain timeout")?;

                let timeout_reason = self.compute_timeout_reason(round);

                RoundTimeout::new(
                    timeout,
                    self.proposal_generator.author(),
                    timeout_reason,
                    signature,
                )
            };

            self.round_state.record_round_timeout(timeout.clone());
            let round_timeout_msg = RoundTimeoutMsg::new(timeout, self.block_store.sync_info());
            self.network
                .broadcast_round_timeout(round_timeout_msg)
                .await;
            warn!(
                round = round,
                remote_peer = self.proposer_election.get_valid_proposer(round),
                event = LogEvent::Timeout,
            );
            bail!("Round {} timeout, broadcast to all peers", round);
        } else {
            let (is_nil_vote, mut timeout_vote) = match self.round_state.vote_sent() {
                Some(vote) if vote.vote_data().proposed().round() == round => {
                    (vote.vote_data().is_for_nil(), vote)
                },
                _ => {
                    // Didn't vote in this round yet, generate a backup vote
                    let nil_block = self
                        .proposal_generator
                        .generate_nil_block(round, self.proposer_election.clone())?;
                    info!(
                        self.new_log(LogEvent::VoteNIL),
                        "Planning to vote for a NIL block {}", nil_block
                    );
                    counters::VOTE_NIL_COUNT.inc();
                    let nil_vote = self.vote_block(nil_block).await?;
                    (true, nil_vote)
                },
            };

            if !timeout_vote.is_timeout() {
                let timeout = timeout_vote.generate_2chain_timeout(
                    self.block_store.highest_quorum_cert().as_ref().clone(),
                );
                let signature = self
                    .safety_rules
                    .lock()
                    .sign_timeout_with_qc(
                        &timeout,
                        self.block_store.highest_2chain_timeout_cert().as_deref(),
                    )
                    .context("[RoundManager] SafetyRules signs 2-chain timeout")?;
                timeout_vote.add_2chain_timeout(timeout, signature);
            }

            self.round_state.record_vote(timeout_vote.clone());
            let timeout_vote_msg = VoteMsg::new(timeout_vote, self.block_store.sync_info());
            self.network.broadcast_timeout_vote(timeout_vote_msg).await;
            warn!(
                round = round,
                remote_peer = self.proposer_election.get_valid_proposer(round),
                voted_nil = is_nil_vote,
                event = LogEvent::Timeout,
            );
            bail!("Round {} timeout, broadcast to all peers", round);
        }
    }
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-27)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;
```
