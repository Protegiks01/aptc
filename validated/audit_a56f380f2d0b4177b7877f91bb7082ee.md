# Audit Report

## Title
Consensus Observer Payload Store Desynchronization During Epoch Transition Causes Legitimate Block Payloads to be Dropped

## Summary
During epoch transitions via the commit sync path, the consensus observer fails to clear stale block payloads from previous epochs. These stale payloads remain in the `block_payloads` BTreeMap and count against the `max_num_pending_blocks` limit, causing new legitimate payloads to be silently dropped, which prevents correct block execution and forces repeated state sync fallbacks.

## Finding Description

The consensus observer maintains a shared `block_payloads` BTreeMap indexed by `(epoch, round)` that stores block transaction payloads: [1](#0-0) 

When inserting new payloads, the system enforces a strict limit (default 150 blocks) and **silently drops** new payloads if exceeded: [2](#0-1) 

During epoch transitions, two distinct code paths exist:

**Path 1 (Fallback Sync)**: Correctly clears the payload store by calling `clear_pending_block_state()` → `clear_block_data()` → `clear_all_payloads()`: [3](#0-2) [4](#0-3) 

**Path 2 (Commit Sync - VULNERABLE)**: Does NOT clear the payload store during epoch transition: [5](#0-4) 

In the vulnerable commit sync path, `wait_for_epoch_start()` retrieves the existing `block_payloads` reference (which may contain stale payloads) and passes it to create the new epoch's payload manager: [6](#0-5) [7](#0-6) 

While `verify_payload_signatures()` is subsequently called, it only processes current-epoch payloads and **does not remove** old-epoch payloads: [8](#0-7) 

The critical flaw is at lines 228-231 where the function breaks early when encountering future epochs, and line 234 where it only processes payloads matching the current epoch. Payloads from previous epochs (where `epoch < current_epoch`) are never removed.

**Attack Scenario:**
1. Observer accumulates payloads during normal operation or while state sync is running (e.g., 140 payloads approaching the 150 limit)
2. State sync completes to a block in a new epoch (epoch N+1) via commit decision
3. Epoch transition occurs via `process_commit_sync_notification` path
4. Old payloads remain in the BTreeMap because no clearing mechanism exists in this path
5. New epoch N+1 blocks arrive with payloads
6. After 10 new payloads, the 150 limit is reached
7. All subsequent payloads are **silently dropped** [9](#0-8) 
8. Observer cannot execute blocks due to missing payloads, triggering fallback to state sync

Block payloads continue to be received and processed during state sync without checks: [10](#0-9) 

## Impact Explanation

This is **HIGH severity** per Aptos bug bounty criteria:

**Validator Node Slowdowns**: Observer nodes experience execution failures due to missing payloads, triggering repeated state sync fallbacks that significantly degrade performance and consensus participation. This directly impacts the observer's ability to serve as a backup validator.

**Significant Protocol Violations**: The observer incorrectly reports payload unavailability when payloads were actually broadcast on the network, violating the consensus observer's core guarantee of tracking consensus state accurately.

**State Inconsistencies**: The payload store becomes desynchronized with actual network state, containing stale epoch payloads while operating in a new epoch, breaking the invariant that the payload store should only contain current-epoch relevant data.

This aligns with HIGH severity impact criteria #8: "Validator Node Slowdowns - Significant performance degradation affecting consensus."

## Likelihood Explanation

**High likelihood** - This vulnerability is triggered naturally during normal network operations:

1. **Common scenario**: Observers routinely accumulate payloads during normal block processing
2. **Automatic trigger**: Any epoch transition via the commit sync path (a common state sync completion scenario)
3. **No attacker required**: Natural network conditions such as brief delays or high block production rates cause payload accumulation
4. **Deterministic**: Once payloads accumulate near the limit before an epoch change via commit sync, the issue is guaranteed to occur

The vulnerability can also be deliberately triggered by an attacker controlling network propagation timing, but more importantly, it occurs naturally without malicious intent.

## Recommendation

Add a call to clear stale payloads during epoch transitions in the commit sync path. Modify `process_commit_sync_notification` to clear old-epoch payloads before or after calling `wait_for_epoch_start()`:

```rust
// In process_commit_sync_notification, after lines 1026-1031
if synced_epoch > current_epoch_state.epoch {
    // Wait for the latest epoch to start
    self.execution_client.end_epoch().await;
    
    // Clear stale payloads from previous epochs before starting new epoch
    self.observer_block_data.lock().get_block_payloads().lock().retain(|(epoch, _), _| {
        *epoch >= synced_epoch
    });
    
    self.wait_for_epoch_start().await;
    // ... rest of the code
}
```

Alternatively, modify `verify_payload_signatures()` to actively remove old-epoch payloads instead of just skipping them.

## Proof of Concept

While a full PoC would require setting up a test network with epoch transitions, the vulnerability can be demonstrated through code analysis showing the missing cleanup path:

1. Trace `process_fallback_sync_notification` → observe `clear_all_payloads()` call at line 961
2. Trace `process_commit_sync_notification` → observe NO `clear_all_payloads()` call
3. Verify `verify_payload_signatures()` at lines 228-257 never removes `epoch < current_epoch` entries
4. Verify `insert_block_payload()` at lines 86-94 silently drops when limit exceeded

The code paths clearly show the asymmetry: fallback sync clears payloads, commit sync does not, creating a state desynchronization vulnerability.

## Notes

This vulnerability is particularly concerning because:
- It affects consensus observer nodes which serve as backup validators
- The silent dropping behavior makes the issue difficult to detect and debug
- The issue compounds over multiple epoch transitions
- The performance degradation from repeated state sync fallbacks can be significant

The fix should ensure consistency between both epoch transition paths (fallback sync and commit sync) regarding payload store cleanup.

### Citations

**File:** consensus/src/consensus_observer/observer/payload_store.rs (L33-36)
```rust
    // Block transaction payloads (indexed by epoch and round).
    // This is directly accessed by the payload manager.
    block_payloads: Arc<Mutex<BTreeMap<(u64, Round), BlockPayloadStatus>>>,
}
```

**File:** consensus/src/consensus_observer/observer/payload_store.rs (L84-95)
```rust
        // Verify that the number of payloads doesn't exceed the maximum
        let max_num_pending_blocks = self.consensus_observer_config.max_num_pending_blocks as usize;
        if self.block_payloads.lock().len() >= max_num_pending_blocks {
            warn!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Exceeded the maximum number of payloads: {:?}. Dropping block: {:?}!",
                    max_num_pending_blocks,
                    block_payload.block(),
                ))
            );
            return; // Drop the block if we've exceeded the maximum
        }
```

**File:** consensus/src/consensus_observer/observer/payload_store.rs (L217-257)
```rust
    pub fn verify_payload_signatures(&mut self, epoch_state: &EpochState) -> Vec<Round> {
        // Get the current epoch
        let current_epoch = epoch_state.epoch;

        // Gather the keys for the block payloads
        let payload_epochs_and_rounds: Vec<(u64, Round)> =
            self.block_payloads.lock().keys().cloned().collect();

        // Go through all unverified blocks and attempt to verify the signatures
        let mut verified_payloads_to_update = vec![];
        for (epoch, round) in payload_epochs_and_rounds {
            // Check if we can break early (BtreeMaps are sorted by key)
            if epoch > current_epoch {
                break;
            }

            // Otherwise, attempt to verify the payload signatures
            if epoch == current_epoch {
                if let Entry::Occupied(mut entry) = self.block_payloads.lock().entry((epoch, round))
                {
                    if let BlockPayloadStatus::AvailableAndUnverified(block_payload) =
                        entry.get_mut()
                    {
                        if let Err(error) = block_payload.verify_payload_signatures(epoch_state) {
                            // Log the verification failure
                            error!(
                                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                                    "Failed to verify the block payload signatures for epoch: {:?} and round: {:?}. Error: {:?}",
                                    epoch, round, error
                                ))
                            );

                            // Remove the block payload from the store
                            entry.remove();
                        } else {
                            // Save the block payload for reinsertion
                            verified_payloads_to_update.push(block_payload.clone());
                        }
                    }
                }
            }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L356-439)
```rust
    async fn process_block_payload_message(
        &mut self,
        peer_network_id: PeerNetworkId,
        message_received_time: Instant,
        block_payload: BlockPayload,
    ) {
        // Get the epoch and round for the block
        let block_epoch = block_payload.epoch();
        let block_round = block_payload.round();

        // Determine if the payload is behind the last ordered block, or if it already exists
        let last_ordered_block = self.observer_block_data.lock().get_last_ordered_block();
        let payload_out_of_date =
            (block_epoch, block_round) <= (last_ordered_block.epoch(), last_ordered_block.round());
        let payload_exists = self
            .observer_block_data
            .lock()
            .existing_payload_entry(&block_payload);

        // If the payload is out of date or already exists, ignore it
        if payload_out_of_date || payload_exists {
            // Update the metrics for the dropped block payload
            update_metrics_for_dropped_block_payload_message(peer_network_id, &block_payload);
            return;
        }

        // Update the metrics for the received block payload
        update_metrics_for_block_payload_message(peer_network_id, &block_payload);

        // Verify the block payload digests
        if let Err(error) = block_payload.verify_payload_digests() {
            // Log the error and update the invalid message counter
            error!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Failed to verify block payload digests! Ignoring block: {:?}, from peer: {:?}. Error: {:?}",
                    block_payload.block(), peer_network_id,
                    error
                ))
            );
            increment_invalid_message_counter(&peer_network_id, metrics::BLOCK_PAYLOAD_LABEL);
            return;
        }

        // If the payload is for the current epoch, verify the proof signatures
        let epoch_state = self.get_epoch_state();
        let verified_payload = if block_epoch == epoch_state.epoch {
            // Verify the block proof signatures
            if let Err(error) = block_payload.verify_payload_signatures(&epoch_state) {
                // Log the error and update the invalid message counter
                error!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "Failed to verify block payload signatures! Ignoring block: {:?}, from peer: {:?}. Error: {:?}",
                        block_payload.block(), peer_network_id, error
                    ))
                );
                increment_invalid_message_counter(&peer_network_id, metrics::BLOCK_PAYLOAD_LABEL);
                return;
            }

            true // We have successfully verified the signatures
        } else {
            false // We can't verify the signatures yet
        };

        // Update the latency metrics for block payload processing
        update_message_processing_latency_metrics(
            message_received_time,
            &peer_network_id,
            metrics::BLOCK_PAYLOAD_LABEL,
        );

        // Update the payload store with the payload
        self.observer_block_data
            .lock()
            .insert_block_payload(block_payload, verified_payload);

        // Check if there are blocks that were missing payloads but are
        // now ready because of the new payload. Note: this should only
        // be done if the payload has been verified correctly.
        if verified_payload {
            self.order_ready_pending_block(block_epoch, block_round)
                .await;
        }
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L961-961)
```rust
        self.clear_pending_block_state().await;
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L1026-1045)
```rust
        // If the epoch has changed, end the current epoch and start the latest one.
        let current_epoch_state = self.get_epoch_state();
        if synced_epoch > current_epoch_state.epoch {
            // Wait for the latest epoch to start
            self.execution_client.end_epoch().await;
            self.wait_for_epoch_start().await;

            // Verify the block payloads for the new epoch
            let new_epoch_state = self.get_epoch_state();
            let verified_payload_rounds = self
                .observer_block_data
                .lock()
                .verify_payload_signatures(&new_epoch_state);

            // Order all the pending blocks that are now ready (these were buffered during state sync)
            for payload_round in verified_payload_rounds {
                self.order_ready_pending_block(new_epoch_state.epoch, payload_round)
                    .await;
            }
        };
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L1065-1071)
```rust
    async fn wait_for_epoch_start(&mut self) {
        // Wait for the epoch state to update
        let block_payloads = self.observer_block_data.lock().get_block_payloads();
        let (payload_manager, consensus_config, execution_config, randomness_config) = self
            .observer_epoch_state
            .wait_for_epoch_start(block_payloads)
            .await;
```

**File:** consensus/src/consensus_observer/observer/block_data.rs (L93-95)
```rust
    pub fn clear_block_data(&mut self) -> LedgerInfoWithSignatures {
        // Clear the payload store
        self.block_payload_store.clear_all_payloads();
```

**File:** consensus/src/consensus_observer/observer/epoch_state.rs (L84-115)
```rust
    pub async fn wait_for_epoch_start(
        &mut self,
        block_payloads: Arc<
            Mutex<BTreeMap<(u64, aptos_consensus_types::common::Round), BlockPayloadStatus>>,
        >,
    ) -> (
        Arc<dyn TPayloadManager>,
        OnChainConsensusConfig,
        OnChainExecutionConfig,
        OnChainRandomnessConfig,
    ) {
        // Extract the epoch state and on-chain configs
        let (epoch_state, consensus_config, execution_config, randomness_config) =
            extract_on_chain_configs(&self.node_config, &mut self.reconfig_events).await;

        // Update the local epoch state and quorum store config
        self.epoch_state = Some(epoch_state.clone());
        self.execution_pool_window_size = consensus_config.window_size();
        self.quorum_store_enabled = consensus_config.quorum_store_enabled();
        info!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "New epoch started: {:?}. Execution pool window: {:?}. Quorum store enabled: {:?}",
                epoch_state.epoch, self.execution_pool_window_size, self.quorum_store_enabled,
            ))
        );

        // Create the payload manager
        let payload_manager: Arc<dyn TPayloadManager> = if self.quorum_store_enabled {
            Arc::new(ConsensusObserverPayloadManager::new(
                block_payloads,
                self.consensus_publisher.clone(),
            ))
```
