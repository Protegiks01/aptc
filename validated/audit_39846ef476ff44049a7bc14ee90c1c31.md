# Audit Report

## Title
BCS Deserialization Failure Prevents Validator Restart After Consensus Type Schema Evolution

## Summary
The consensus persistent storage recovery mechanism uses `.expect()` on BCS deserialization without version handling or backward compatibility, causing validators to panic and fail to restart if the `Vote` or `TwoChainTimeoutCertificate` struct schemas change between software versions.

## Finding Description

The vulnerability exists in the validator startup recovery path where consensus state is restored from persistent storage. When a validator restarts, the `start()` function attempts to deserialize previously saved `Vote` and `TwoChainTimeoutCertificate` data from ConsensusDB using BCS deserialization with `.expect()` calls that panic on failure: [1](#0-0) 

Both `Vote` and `TwoChainTimeoutCertificate` structs derive `Serialize` and `Deserialize` without any versioning mechanism: [2](#0-1) [3](#0-2) 

When the schema of these types changes (field addition, removal, reordering, or type changes), validators that restart with new code cannot deserialize data saved by old code. The `.expect()` calls cause immediate panic during the `start()` function, **before** the fallback mechanism can activate.

A fallback mechanism exists to handle recovery errors, but it is unreachable in this scenario: [4](#0-3) 

The panic occurs at line 528 or 531 during deserialization, while the fallback only catches errors from `RecoveryData::new()` at line 591. The panic prevents the fallback from ever executing, breaking the liveness invariant that validators must be able to restart and rejoin consensus.

The codebase demonstrates awareness of this pattern in other contexts. `OnChainConsensusConfig` uses versioned enums (V1-V5) for schema evolution: [5](#0-4) 

Similarly, `WrappedLedgerInfo` explicitly maintains backward compatibility: [6](#0-5) 

However, `Vote` and `TwoChainTimeoutCertificate` lack these protections despite being persisted to storage and loaded during restart.

## Impact Explanation

This qualifies as **HIGH severity** per the Aptos bug bounty program under "Validator node slowdowns" - though more accurately, this completely prevents validator startup rather than merely slowing it down.

**Impact Scope:**
- **Individual Validator Liveness:** Affected validators cannot restart and participate in consensus
- **Network Liveness Risk:** If multiple validators upgrade simultaneously during a coordinated release, a significant portion of the validator set could become unable to restart
- **Recovery Complexity:** Requires manual intervention (deleting consensus DB) to recover, which loses consensus state. This recovery path is demonstrated in tests: [7](#0-6) 

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH** - This will occur whenever:
1. Consensus types are modified as part of protocol evolution (adding features, optimizing structures)
2. Validators perform rolling upgrades with the new code
3. Validators restart (planned maintenance, crashes, deployments)

This is not a theoretical edge case but a **guaranteed failure mode** during legitimate protocol evolution. The Aptos codebase shows evidence of consensus protocol evolution (order votes, DAG consensus, two_chain_timeout field addition), and the lack of versioning in these persistent types indicates this failure mode has not been addressed.

## Recommendation

Implement versioning for `Vote` and `TwoChainTimeoutCertificate` similar to `OnChainConsensusConfig`. Options include:

1. **Enum-based versioning**: Wrap types in versioned enums (V1, V2, etc.)
2. **Serde attributes**: Use `#[serde(default)]` for new fields with backward-compatible defaults
3. **Migration layer**: Add explicit version tags and migration logic in ConsensusDB
4. **Graceful degradation**: Replace `.expect()` with proper error handling that allows fallback to `PartialRecoveryData`

The immediate fix should replace the `.expect()` calls with `?` operators or explicit error handling that allows the fallback mechanism at line 591 to execute.

## Proof of Concept

The vulnerability can be demonstrated by:
1. Running a validator with version V1 that saves a `Vote` to ConsensusDB
2. Modifying the `Vote` struct to add a new field
3. Attempting to restart the validator with the modified code
4. Observing panic: "unable to deserialize last vote"

While a complete PoC requires modifying the struct definitions, the vulnerability is evident from the code structure: any BCS deserialization failure will panic before the fallback mechanism executes.

## Notes

This vulnerability affects validator liveness during legitimate protocol upgrades, not consensus safety. The epoch checking logic in `RecoveryData::new()` (lines 405-417) would properly handle mismatched epochs IF deserialization succeeds, but the panic prevents this protective logic from executing. The uniform application of versioning patterns used elsewhere in the codebase (like `OnChainConsensusConfig`) would prevent this issue.

### Citations

**File:** consensus/src/persistent_liveness_storage.rs (L526-532)
```rust
        let last_vote = raw_data
            .0
            .map(|bytes| bcs::from_bytes(&bytes[..]).expect("unable to deserialize last vote"));

        let highest_2chain_timeout_cert = raw_data.1.map(|b| {
            bcs::from_bytes(&b).expect("unable to deserialize highest 2-chain timeout cert")
        });
```

**File:** consensus/src/persistent_liveness_storage.rs (L559-594)
```rust
        match RecoveryData::new(
            last_vote,
            ledger_recovery_data.clone(),
            blocks,
            accumulator_summary.into(),
            quorum_certs,
            highest_2chain_timeout_cert,
            order_vote_enabled,
            window_size,
        ) {
            Ok(mut initial_data) => {
                (self as &dyn PersistentLivenessStorage)
                    .prune_tree(initial_data.take_blocks_to_prune())
                    .expect("unable to prune dangling blocks during restart");
                if initial_data.last_vote.is_none() {
                    self.db
                        .delete_last_vote_msg()
                        .expect("unable to cleanup last vote");
                }
                if initial_data.highest_2chain_timeout_certificate.is_none() {
                    self.db
                        .delete_highest_2chain_timeout_certificate()
                        .expect("unable to cleanup highest 2-chain timeout cert");
                }
                info!(
                    "Starting up the consensus state machine with recovery data - [last_vote {}], [highest timeout certificate: {}]",
                    initial_data.last_vote.as_ref().map_or_else(|| "None".to_string(), |v| v.to_string()),
                    initial_data.highest_2chain_timeout_certificate().as_ref().map_or_else(|| "None".to_string(), |v| v.to_string()),
                );

                LivenessStorageData::FullRecoveryData(initial_data)
            },
            Err(e) => {
                error!(error = ?e, "Failed to construct recovery data");
                LivenessStorageData::PartialRecoveryData(ledger_recovery_data)
            },
```

**File:** consensus/consensus-types/src/vote.rs (L22-22)
```rust
#[derive(Deserialize, Serialize, Clone, PartialEq, Eq)]
```

**File:** consensus/consensus-types/src/timeout_2chain.rs (L108-108)
```rust
#[derive(Debug, Clone, Serialize, Deserialize, Eq, PartialEq)]
```

**File:** types/src/on_chain_config/consensus_config.rs (L192-213)
```rust
pub enum OnChainConsensusConfig {
    V1(ConsensusConfigV1),
    V2(ConsensusConfigV1),
    V3 {
        alg: ConsensusAlgorithmConfig,
        vtxn: ValidatorTxnConfig,
    },
    V4 {
        alg: ConsensusAlgorithmConfig,
        vtxn: ValidatorTxnConfig,
        // Execution pool block window
        window_size: Option<u64>,
    },
    V5 {
        alg: ConsensusAlgorithmConfig,
        vtxn: ValidatorTxnConfig,
        // Execution pool block window
        window_size: Option<u64>,
        // Whether to check if we can skip generating randomness for blocks
        rand_check_enabled: bool,
    },
}
```

**File:** consensus/consensus-types/src/wrapped_ledger_info.rs (L14-26)
```rust
/// This struct is similar to QuorumCert, except that the verify function doesn't verify vote_data.
/// This struct is introduced to ensure backward compatibility when upgrading the consensus to use
/// order votes to execute blocks faster. When order votes are enabled, then vote_data and
/// consensus_data_hash inside signed_ledger_info are not used anywhere in the code and can be set
/// to dummy values.
#[derive(Deserialize, Serialize, Clone, Debug, Eq, PartialEq)]
pub struct WrappedLedgerInfo {
    /// The VoteData here is placeholder for backwards compatibility purpose and should not be used
    /// when order votes are enabled.
    vote_data: VoteData,
    /// The signed LedgerInfo of a committed block that carries the data about the certified block.
    signed_ledger_info: LedgerInfoWithSignatures,
}
```

**File:** testsuite/smoke-test/src/consensus/consensusdb_recovery.rs (L44-49)
```rust
    let consensus_db_path = node_config.storage.dir().join(CONSENSUS_DB_NAME);
    // Verify that consensus db exists and
    // we are not deleting a non-existent directory
    assert!(consensus_db_path.as_path().exists());
    // Delete the consensus db to simulate consensus db is nuked
    fs::remove_dir_all(consensus_db_path).unwrap();
```
