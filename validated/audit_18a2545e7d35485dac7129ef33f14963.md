# Audit Report

## Title
State Sync Stream Cleanup Failure Causes Persistent Node Degradation

## Summary
When `handle_storage_synchronizer_error()` fails during stream termination in the state sync driver, the error is only logged but critical cleanup operations are skipped. This leaves the node in an inconsistent state with stale stream references that cannot be cleared, preventing the creation of new streams and causing persistent sync failures requiring manual node restart.

## Finding Description

The vulnerability exists in the error-within-error handling path of the state sync driver. When the storage synchronizer encounters an error and sends an `ErrorNotification`, the driver's `handle_error_notification()` method attempts to clean up by terminating active data streams. [1](#0-0) 

The critical flaw is in how both `continuous_syncer.handle_storage_synchronizer_error()` and `bootstrapper.handle_storage_synchronizer_error()` handle failures. Both methods call `reset_active_stream()` with the `?` operator, which means if stream termination fails, the function returns early without executing the fallback logic. [2](#0-1) [3](#0-2) 

The `reset_active_stream()` method has a fatal design flaw where the `?` operator on line 536 (continuous_syncer) and line 1550 (bootstrapper) causes early return if `terminate_stream_with_feedback()` fails, preventing the cleanup code that sets `active_data_stream = None` and `speculative_stream_state = None` from executing. [4](#0-3) [5](#0-4) 

The stream termination can fail when the streaming service's mpsc channel is closed, resulting in a `SendError` that gets converted to `Error::UnexpectedErrorEncountered`. [6](#0-5) [7](#0-6) 

Once the cleanup fails, the `active_data_stream` field remains non-None with a stale reference. On subsequent `drive_progress()` calls, the check `if self.active_data_stream.is_some()` on line 424 (bootstrapper) and line 81 (continuous_syncer) succeeds, preventing initialization of new streams and instead attempting to process notifications from the stale stream. [8](#0-7) [9](#0-8) 

This creates a cascading failure loop: attempts to fetch notifications from the stale stream repeatedly timeout via `get_data_notification()`, and when critical timeouts occur, the code tries to call `reset_active_stream()` again, which also fails for the same reason. [10](#0-9) [11](#0-10) 

## Impact Explanation

This vulnerability falls under **Medium Severity** per the Aptos bug bounty criteria as it causes "State inconsistencies requiring manual intervention." Specifically:

- **Node Liveness Degradation**: The affected node cannot synchronize state, falling behind the network
- **Availability Impact**: The node becomes unable to serve current state queries or participate in validation
- **Manual Intervention Required**: Recovery requires node restart, as there is no automatic recovery mechanism from this state
- **Does NOT Cause**: Direct fund loss, consensus safety violations, or network-wide disruption

The impact is limited to individual nodes experiencing the error condition, not systemic network failure. However, if multiple nodes encounter this simultaneously (e.g., during coordinated network instability), it could temporarily degrade overall network capacity.

## Likelihood Explanation

**Likelihood: Medium**

This vulnerability can be triggered through multiple realistic production scenarios:

1. **Internal Service Crashes**: If the streaming service crashes or is terminated during error recovery, the channel closure will cause `SendError` during cleanup attempts

2. **Network Instability**: Natural network disruptions causing timing issues where the streaming service terminates connections during the error recovery window

3. **Race Conditions**: Timing-sensitive scenarios where the streaming service is restarting or reconfiguring while the driver attempts error recovery

While the vulnerability requires specific timing conditions (error recovery coinciding with service unavailability), these conditions can occur naturally in production environments. The error handling path is exercised regularly when nodes encounter invalid data or synchronization issues, making this a realistic production concern.

**Note**: The streaming service channel is an internal component, not directly controllable by external network peers. The likelihood is medium rather than high because it requires specific internal timing conditions, though these can occur naturally.

## Recommendation

The fix should ensure cleanup operations complete even if stream termination fails. Replace the `?` operator with explicit error handling that logs the termination failure but continues with cleanup:

```rust
pub async fn reset_active_stream(
    &mut self,
    notification_and_feedback: Option<NotificationAndFeedback>,
) -> Result<(), Error> {
    if let Some(active_data_stream) = &self.active_data_stream {
        let data_stream_id = active_data_stream.data_stream_id;
        if let Err(error) = utils::terminate_stream_with_feedback(
            &mut self.streaming_client,
            data_stream_id,
            notification_and_feedback,
        )
        .await
        {
            // Log the error but continue with cleanup
            warn!("Failed to terminate stream {}, but proceeding with cleanup: {:?}", 
                  data_stream_id, error);
        }
    }

    // Always execute cleanup regardless of termination result
    self.active_data_stream = None;
    self.speculative_stream_state = None;
    Ok(())
}
```

This ensures that even if the streaming service is unavailable, the driver clears its internal state and can recover on the next `drive_progress()` call.

## Proof of Concept

A Rust integration test demonstrating this vulnerability would involve:

1. Setting up a state sync driver with an active data stream
2. Simulating a storage synchronizer error that triggers `handle_storage_synchronizer_error()`
3. Closing the streaming service channel before the cleanup completes
4. Verifying that `active_data_stream` remains set and subsequent `drive_progress()` calls fail to initialize new streams

The test would confirm that the node enters an unrecoverable state requiring restart, as the stale stream reference cannot be cleared through normal operation.

### Citations

**File:** state-sync/state-sync-driver/src/driver.rs (L495-533)
```rust
    async fn handle_error_notification(&mut self, error_notification: ErrorNotification) {
        warn!(LogSchema::new(LogEntry::SynchronizerNotification)
            .error_notification(error_notification.clone())
            .message("Received an error notification from the storage synchronizer!"));

        // Terminate the currently active streams
        let notification_id = error_notification.notification_id;
        let notification_feedback = NotificationFeedback::InvalidPayloadData;
        if self.bootstrapper.is_bootstrapped() {
            if let Err(error) = self
                .continuous_syncer
                .handle_storage_synchronizer_error(NotificationAndFeedback::new(
                    notification_id,
                    notification_feedback,
                ))
                .await
            {
                error!(LogSchema::new(LogEntry::SynchronizerNotification)
                    .message(&format!(
                        "Failed to terminate the active stream for the continuous syncer! Error: {:?}",
                        error
                    )));
            }
        } else if let Err(error) = self
            .bootstrapper
            .handle_storage_synchronizer_error(NotificationAndFeedback::new(
                notification_id,
                notification_feedback,
            ))
            .await
        {
            error!(
                LogSchema::new(LogEntry::SynchronizerNotification).message(&format!(
                    "Failed to terminate the active stream for the bootstrapper! Error: {:?}",
                    error
                ))
            );
        };
    }
```

**File:** state-sync/state-sync-driver/src/continuous_syncer.rs (L77-97)
```rust
    pub async fn drive_progress(
        &mut self,
        consensus_sync_request: Arc<Mutex<Option<ConsensusSyncRequest>>>,
    ) -> Result<(), Error> {
        if self.active_data_stream.is_some() {
            // We have an active data stream. Process any notifications!
            self.process_active_stream_notifications(consensus_sync_request)
                .await
        } else if self.storage_synchronizer.pending_storage_data() {
            // Wait for any pending data to be processed
            sample!(
                SampleRate::Duration(Duration::from_secs(PENDING_DATA_LOG_FREQ_SECS)),
                info!("Waiting for the storage synchronizer to handle pending data!")
            );
            Ok(())
        } else {
            // Fetch a new data stream to start streaming data
            self.initialize_active_data_stream(consensus_sync_request)
                .await
        }
    }
```

**File:** state-sync/state-sync-driver/src/continuous_syncer.rs (L183-198)
```rust
    async fn fetch_next_data_notification(&mut self) -> Result<DataNotification, Error> {
        let max_stream_wait_time_ms = self.driver_configuration.config.max_stream_wait_time_ms;
        let max_num_stream_timeouts = self.driver_configuration.config.max_num_stream_timeouts;
        let result = utils::get_data_notification(
            max_stream_wait_time_ms,
            max_num_stream_timeouts,
            self.active_data_stream.as_mut(),
        )
        .await;
        if matches!(result, Err(Error::CriticalDataStreamTimeout(_))) {
            // If the stream has timed out too many times, we need to reset it
            warn!("Resetting the currently active data stream due to too many timeouts!");
            self.reset_active_stream(None).await?;
        }
        result
    }
```

**File:** state-sync/state-sync-driver/src/continuous_syncer.rs (L501-522)
```rust
    pub async fn handle_storage_synchronizer_error(
        &mut self,
        notification_and_feedback: NotificationAndFeedback,
    ) -> Result<(), Error> {
        // Reset the active stream
        self.reset_active_stream(Some(notification_and_feedback))
            .await?;

        // Fallback to output syncing if we need to
        if let ContinuousSyncingMode::ExecuteTransactionsOrApplyOutputs =
            self.get_continuous_syncing_mode()
        {
            self.output_fallback_handler.fallback_to_outputs();
            metrics::set_gauge(
                &metrics::DRIVER_FALLBACK_MODE,
                ExecutingComponent::ContinuousSyncer.get_label(),
                1,
            );
        }

        Ok(())
    }
```

**File:** state-sync/state-sync-driver/src/continuous_syncer.rs (L525-542)
```rust
    pub async fn reset_active_stream(
        &mut self,
        notification_and_feedback: Option<NotificationAndFeedback>,
    ) -> Result<(), Error> {
        if let Some(active_data_stream) = &self.active_data_stream {
            let data_stream_id = active_data_stream.data_stream_id;
            utils::terminate_stream_with_feedback(
                &mut self.streaming_client,
                data_stream_id,
                notification_and_feedback,
            )
            .await?;
        }

        self.active_data_stream = None;
        self.speculative_stream_state = None;
        Ok(())
    }
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L414-441)
```rust
    pub async fn drive_progress(
        &mut self,
        global_data_summary: &GlobalDataSummary,
    ) -> Result<(), Error> {
        if self.is_bootstrapped() {
            return Err(Error::AlreadyBootstrapped(
                "The bootstrapper should not attempt to make progress!".into(),
            ));
        }

        if self.active_data_stream.is_some() {
            // We have an active data stream. Process any notifications!
            self.process_active_stream_notifications().await?;
        } else if self.storage_synchronizer.pending_storage_data() {
            // Wait for any pending data to be processed
            sample!(
                SampleRate::Duration(Duration::from_secs(PENDING_DATA_LOG_FREQ_SECS)),
                info!("Waiting for the storage synchronizer to handle pending data!")
            );
        } else {
            // Fetch a new data stream to start streaming data
            self.initialize_active_data_stream(global_data_summary)
                .await?;
        }

        // Check if we've now bootstrapped
        self.notify_listeners_if_bootstrapped().await
    }
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L1517-1536)
```rust
    pub async fn handle_storage_synchronizer_error(
        &mut self,
        notification_and_feedback: NotificationAndFeedback,
    ) -> Result<(), Error> {
        // Reset the active stream
        self.reset_active_stream(Some(notification_and_feedback))
            .await?;

        // Fallback to output syncing if we need to
        if let BootstrappingMode::ExecuteOrApplyFromGenesis = self.get_bootstrapping_mode() {
            self.output_fallback_handler.fallback_to_outputs();
            metrics::set_gauge(
                &metrics::DRIVER_FALLBACK_MODE,
                ExecutingComponent::Bootstrapper.get_label(),
                1,
            );
        }

        Ok(())
    }
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L1539-1556)
```rust
    pub async fn reset_active_stream(
        &mut self,
        notification_and_feedback: Option<NotificationAndFeedback>,
    ) -> Result<(), Error> {
        if let Some(active_data_stream) = &self.active_data_stream {
            let data_stream_id = active_data_stream.data_stream_id;
            utils::terminate_stream_with_feedback(
                &mut self.streaming_client,
                data_stream_id,
                notification_and_feedback,
            )
            .await?;
        }

        self.active_data_stream = None;
        self.speculative_stream_state = None;
        Ok(())
    }
```

**File:** state-sync/data-streaming-service/src/streaming_client.rs (L460-472)
```rust
    async fn terminate_stream_with_feedback(
        &self,
        data_stream_id: DataStreamId,
        notification_and_feedback: Option<NotificationAndFeedback>,
    ) -> Result<(), Error> {
        let client_request = StreamRequest::TerminateStream(TerminateStreamRequest {
            data_stream_id,
            notification_and_feedback,
        });
        // We can ignore the receiver as no data will be sent.
        let _receiver = self.send_stream_request(client_request).await?;
        Ok(())
    }
```

**File:** state-sync/data-streaming-service/src/error.rs (L47-50)
```rust
impl From<SendError> for Error {
    fn from(error: SendError) -> Self {
        Error::UnexpectedErrorEncountered(error.to_string())
    }
```

**File:** state-sync/state-sync-driver/src/utils.rs (L196-238)
```rust
/// Fetches a data notification from the given data stream listener. Returns an
/// error if the data stream times out after `max_stream_wait_time_ms`. Also,
/// tracks the number of consecutive timeouts to identify when the stream has
/// timed out too many times.
pub async fn get_data_notification(
    max_stream_wait_time_ms: u64,
    max_num_stream_timeouts: u64,
    active_data_stream: Option<&mut DataStreamListener>,
) -> Result<DataNotification, Error> {
    let active_data_stream = active_data_stream
        .ok_or_else(|| Error::UnexpectedError("The active data stream does not exist!".into()))?;

    let timeout_ms = Duration::from_millis(max_stream_wait_time_ms);
    if let Ok(data_notification) = timeout(timeout_ms, active_data_stream.select_next_some()).await
    {
        // Update the metrics for the data notification receive latency
        metrics::observe_duration(
            &metrics::DATA_NOTIFICATION_LATENCIES,
            metrics::NOTIFICATION_CREATE_TO_RECEIVE,
            data_notification.creation_time,
        );

        // Reset the number of consecutive timeouts for the data stream
        active_data_stream.num_consecutive_timeouts = 0;
        Ok(data_notification)
    } else {
        // Increase the number of consecutive timeouts for the data stream
        active_data_stream.num_consecutive_timeouts += 1;

        // Check if we've timed out too many times
        if active_data_stream.num_consecutive_timeouts >= max_num_stream_timeouts {
            Err(Error::CriticalDataStreamTimeout(format!(
                "{:?}",
                max_num_stream_timeouts
            )))
        } else {
            Err(Error::DataStreamNotificationTimeout(format!(
                "{:?}",
                timeout_ms
            )))
        }
    }
}
```
