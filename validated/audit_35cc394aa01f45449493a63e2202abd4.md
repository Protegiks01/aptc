# Audit Report

## Title
ProofOfStore Replay Attack Enables Resource Exhaustion via Cache Eviction Window

## Summary
A timing vulnerability exists in the ProofOfStore verification system where already-committed batches can be replayed after ProofCache eviction (20-second TTL) but before batch expiration (60 seconds), forcing validators to repeatedly perform expensive BLS aggregate signature verification. This creates a 40-second attack window for resource exhaustion.

## Finding Description

The vulnerability stems from a mismatch between two independent caching mechanisms in the consensus quorum store:

**1. ProofCache with 20-second TTL:**
The ProofCache is initialized with a time-to-live of 20 seconds to cache BLS signature verification results. [1](#0-0) 

**2. BatchProofQueue with 60-second batch expiration:**
Batches are configured to expire 60 seconds after creation via the `batch_expiry_gap_when_init_usecs` parameter. [2](#0-1) 

**The Attack Flow:**

When a `ProofOfStoreMsg` arrives at a validator node, verification happens in this order:

**Step 1:** The message is received as an `UnverifiedEvent::ProofOfStoreMsg` and verification is triggered. [3](#0-2) 

**Step 2:** `ProofOfStoreMsg::verify()` iterates through all proofs and calls expensive verification on each. [4](#0-3) 

**Step 3:** For each proof, `ProofOfStore::verify()` checks the ProofCache first, then performs expensive BLS aggregate signature verification if the cache misses. [5](#0-4) 

**Step 4:** ONLY AFTER all expensive verification completes, the proof is passed to `receive_proofs()` which forwards to `insert_proof()` to check for duplicates/committed batches. [6](#0-5) 

**Step 5:** The duplicate/committed detection in `insert_proof()` rejects the proof AFTER expensive verification has already occurred. [7](#0-6) 

**The Vulnerability Window:**

After a batch is committed, it remains in `BatchProofQueue.items` marked as committed. [8](#0-7) 

The committed status is checked via `is_committed()` which returns true when proof, proof_insertion_time, and txn_summaries are all None. [9](#0-8) 

However:
- ProofCache entry expires after **20 seconds**
- Batch is only removed from tracking after **60 seconds** when `handle_updated_block_timestamp()` processes expirations [10](#0-9) 
- During the 40-second gap, attackers can replay the same `ProofOfStore`
- Each replay bypasses the expired cache and forces full cryptographic verification
- The duplicate detection in `insert_proof()` only rejects AFTER expensive verification

**Exploitation:**

1. Monitor consensus for committed batches (public information)
2. Wait 20 seconds for ProofCache eviction
3. Repeatedly broadcast the same `ProofOfStoreMsg` to all validators
4. Each validator re-verifies expensive BLS signatures for up to 40 seconds
5. No per-proof rate limiting exists at the network listener level [11](#0-10) 

This breaks the **Resource Limits invariant**: expensive cryptographic operations should not be repeatedly performable for already-verified, committed proofs.

## Impact Explanation

**High Severity** per Aptos Bug Bounty criteria: "Validator node slowdowns"

BLS aggregate signature verification is computationally expensive (cryptographic pairing operations typically requiring milliseconds per verification). An attacker can:

- Force all validators to repeatedly verify the same committed proofs
- Sustain attack for 40 seconds per batch (or longer with multiple batches)
- Amplify impact by targeting multiple recently-committed batches simultaneously
- Cause measurable CPU exhaustion on validator nodes
- Potentially slow down consensus participation and block production

The attack requires no privileged accessâ€”any network peer can send `ProofOfStoreMsg` messages containing valid signatures (replayed from committed proofs). The computational cost to attackers is minimal (network bandwidth) while defenders incur significant CPU cost per replayed message.

This aligns with the Aptos Bug Bounty High Severity category: "Validator Node Slowdowns - Significant performance degradation affecting consensus - DoS through resource exhaustion."

## Likelihood Explanation

**High Likelihood:**

- **Attack is trivial to execute**: Monitor committed batches, wait 20 seconds, replay messages
- **No authentication barrier**: Any network peer can send consensus messages with valid signatures
- **Attack window is deterministic**: 40-second gap exists for every committed batch
- **No rate limiting**: System does not track or throttle duplicate proof identities after cache expiration
- **Highly observable**: Committed batches are public information in the blockchain
- **Low cost to attacker**: Minimal bandwidth vs. expensive cryptographic verification by all validators
- **Amplification potential**: Can target multiple batches simultaneously during their respective vulnerability windows

The only barrier is network connectivity to validator nodes, which is a fundamental requirement of the P2P consensus network.

## Recommendation

Reorder the verification logic to check for duplicate/committed proofs BEFORE performing expensive cryptographic verification:

**Option 1 - Check at insert_proof level:**
Modify `insert_proof()` to be called before verification, returning early if the proof is already committed or duplicate. This requires restructuring the message flow.

**Option 2 - Extend ProofCache persistence:**
Increase ProofCache TTL to match or exceed batch expiration (60+ seconds), or implement a separate committed-proof tracking mechanism that persists longer than the cache.

**Option 3 - Add per-proof identity deduplication:**
Implement a deduplic ation layer at the `UnverifiedEvent::verify()` level that tracks recently-verified proof identities (BatchInfoExt) with a TTL matching batch expiration, rejecting replays before cryptographic verification.

**Recommended Fix (Option 3):**
Add a long-lived deduplication cache alongside ProofCache:

```rust
// In epoch_manager.rs
proof_dedup_cache: Cache::builder()
    .max_capacity(node_config.consensus.proof_cache_capacity)
    .time_to_live(Duration::from_secs(60))  // Match batch expiration
    .build(),

// In ProofOfStore::verify()
pub fn verify(&self, validator: &ValidatorVerifier, cache: &ProofCache, dedup_cache: &Cache<BatchInfoExt, ()>) -> anyhow::Result<()> {
    let batch_info_ext: BatchInfoExt = self.info.clone().into();
    
    // Check deduplication cache first (cheap)
    if dedup_cache.get(&batch_info_ext).is_some() {
        return Ok(()); // Already verified recently
    }
    
    // Then check verification cache
    if let Some(signature) = cache.get(&batch_info_ext) {
        if signature == self.multi_signature {
            return Ok(());
        }
    }
    
    // Perform expensive verification
    let result = validator.verify_multi_signatures(&self.info, &self.multi_signature)?;
    
    if result.is_ok() {
        cache.insert(batch_info_ext.clone(), self.multi_signature.clone());
        dedup_cache.insert(batch_info_ext, ());
    }
    result
}
```

## Proof of Concept

The vulnerability can be demonstrated by:

1. Setting up a test network with validators
2. Creating and committing a batch with ProofOfStore
3. Waiting 20 seconds for ProofCache to expire
4. Replaying the same ProofOfStoreMsg multiple times
5. Observing that each replay triggers full BLS verification (measureable via timing/CPU metrics)
6. Confirming that the proof is rejected only after verification completes

The attack requires monitoring validator network traffic to capture valid ProofOfStore messages, then replaying them after the 20-second cache TTL while the 60-second batch expiration window is still active.

---

**Notes:**

This is a protocol-level resource exhaustion vulnerability, NOT a simple network DoS. The root cause is an architectural flaw where expensive cryptographic verification occurs before cheap duplicate/committed status checking. The framework explicitly distinguishes between "Network DoS attacks (out of scope)" and "Validator Node Slowdowns through resource exhaustion (High Severity, in scope)." This vulnerability falls under the latter category as it exploits a protocol design flaw rather than simply flooding the network with invalid messages.

### Citations

**File:** consensus/src/epoch_manager.rs (L250-254)
```rust
            proof_cache: Cache::builder()
                .max_capacity(node_config.consensus.proof_cache_capacity)
                .initial_capacity(1_000)
                .time_to_live(Duration::from_secs(20))
                .build(),
```

**File:** config/src/config/quorum_store_config.rs (L131-131)
```rust
            batch_expiry_gap_when_init_usecs: Duration::from_secs(60).as_micros() as u64,
```

**File:** consensus/src/round_manager.rs (L212-219)
```rust
            UnverifiedEvent::ProofOfStoreMsg(p) => {
                if !self_message {
                    p.verify(max_num_batches, validator, proof_cache)?;
                    counters::VERIFY_MSG
                        .with_label_values(&["proof_of_store"])
                        .observe(start_time.elapsed().as_secs_f64());
                }
                VerifiedEvent::ProofOfStoreMsg(Box::new((*p).into()))
```

**File:** consensus/consensus-types/src/proof_of_store.rs (L566-583)
```rust
    pub fn verify(
        &self,
        max_num_proofs: usize,
        validator: &ValidatorVerifier,
        cache: &ProofCache,
    ) -> anyhow::Result<()> {
        ensure!(!self.proofs.is_empty(), "Empty message");
        ensure!(
            self.proofs.len() <= max_num_proofs,
            "Too many proofs: {} > {}",
            self.proofs.len(),
            max_num_proofs
        );
        for proof in &self.proofs {
            proof.verify(validator, cache)?
        }
        Ok(())
    }
```

**File:** consensus/consensus-types/src/proof_of_store.rs (L635-652)
```rust
    pub fn verify(&self, validator: &ValidatorVerifier, cache: &ProofCache) -> anyhow::Result<()> {
        let batch_info_ext: BatchInfoExt = self.info.clone().into();
        if let Some(signature) = cache.get(&batch_info_ext) {
            if signature == self.multi_signature {
                return Ok(());
            }
        }
        let result = validator
            .verify_multi_signatures(&self.info, &self.multi_signature)
            .context(format!(
                "Failed to verify ProofOfStore for batch: {:?}",
                self.info
            ));
        if result.is_ok() {
            cache.insert(batch_info_ext, self.multi_signature.clone());
        }
        result
    }
```

**File:** consensus/src/quorum_store/proof_manager.rs (L65-69)
```rust
    pub(crate) fn receive_proofs(&mut self, proofs: Vec<ProofOfStore<BatchInfoExt>>) {
        for proof in proofs.into_iter() {
            self.batch_proof_queue.insert_proof(proof);
        }
        self.update_remaining_txns_and_proofs();
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L45-47)
```rust
    fn is_committed(&self) -> bool {
        self.proof.is_none() && self.proof_insertion_time.is_none() && self.txn_summaries.is_none()
    }
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L181-188)
```rust
        if self
            .items
            .get(&batch_key)
            .is_some_and(|item| item.proof.is_some() || item.is_committed())
        {
            counters::inc_rejected_pos_count(counters::POS_DUPLICATE_LABEL);
            return;
        }
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L716-769)
```rust
    pub(crate) fn handle_updated_block_timestamp(&mut self, block_timestamp: u64) {
        // tolerate asynchronous notification
        if self.latest_block_timestamp > block_timestamp {
            return;
        }
        let start = Instant::now();
        self.latest_block_timestamp = block_timestamp;
        if let Some(time_lag) = aptos_infallible::duration_since_epoch()
            .checked_sub(Duration::from_micros(block_timestamp))
        {
            counters::TIME_LAG_IN_BATCH_PROOF_QUEUE.observe_duration(time_lag);
        }

        let expired = self.expirations.expire(block_timestamp);
        let mut num_expired_but_not_committed = 0;
        for key in &expired {
            if let Some(mut queue) = self.author_to_batches.remove(&key.author()) {
                if let Some(batch) = queue.remove(key) {
                    let item = self
                        .items
                        .get(&key.batch_key)
                        .expect("Entry for unexpired batch must exist");
                    if item.proof.is_some() {
                        // not committed proof that is expired
                        num_expired_but_not_committed += 1;
                        counters::GAP_BETWEEN_BATCH_EXPIRATION_AND_CURRENT_TIME_WHEN_COMMIT
                            .observe((block_timestamp - batch.expiration()) as f64);
                        if let Some(ref txn_summaries) = item.txn_summaries {
                            for txn_summary in txn_summaries {
                                if let Some(count) =
                                    self.txn_summary_num_occurrences.get_mut(txn_summary)
                                {
                                    *count -= 1;
                                    if *count == 0 {
                                        self.txn_summary_num_occurrences.remove(txn_summary);
                                    }
                                };
                            }
                        }
                        self.dec_remaining_proofs(&batch.author(), batch.num_txns());
                        counters::GARBAGE_COLLECTED_IN_PROOF_QUEUE_COUNTER
                            .with_label_values(&["expired_proof"])
                            .inc();
                    }
                    claims::assert_some!(self.items.remove(&key.batch_key));
                }
                if !queue.is_empty() {
                    self.author_to_batches.insert(key.author(), queue);
                }
            }
        }
        counters::PROOF_QUEUE_UPDATE_TIMESTAMP_DURATION.observe_duration(start.elapsed());
        counters::NUM_PROOFS_EXPIRED_WHEN_COMMIT.inc_by(num_expired_but_not_committed);
    }
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L846-907)
```rust
    pub(crate) fn mark_committed(&mut self, batches: Vec<BatchInfoExt>) {
        let start = Instant::now();
        for batch in batches.into_iter() {
            let batch_key = BatchKey::from_info(&batch);
            if let Some(item) = self.items.get(&batch_key) {
                if let Some(ref proof) = item.proof {
                    let insertion_time = item
                        .proof_insertion_time
                        .expect("Insertion time is updated with proof");
                    counters::pos_to_commit(
                        proof.gas_bucket_start(),
                        insertion_time.elapsed().as_secs_f64(),
                    );
                    self.dec_remaining_proofs(&batch.author(), batch.num_txns());
                    counters::GARBAGE_COLLECTED_IN_PROOF_QUEUE_COUNTER
                        .with_label_values(&["committed_proof"])
                        .inc();
                }
                let item = self
                    .items
                    .get_mut(&batch_key)
                    .expect("must exist due to check");

                if item.proof.is_some() {
                    if let Some(ref txn_summaries) = item.txn_summaries {
                        for txn_summary in txn_summaries {
                            if let Some(count) =
                                self.txn_summary_num_occurrences.get_mut(txn_summary)
                            {
                                *count -= 1;
                                if *count == 0 {
                                    self.txn_summary_num_occurrences.remove(txn_summary);
                                }
                            };
                        }
                    }
                } else if !item.is_committed() {
                    counters::GARBAGE_COLLECTED_IN_PROOF_QUEUE_COUNTER
                        .with_label_values(&["committed_batch_without_proof"])
                        .inc();
                }
                // The item is just marked committed for now.
                // When the batch is expired, then it will be removed from items.
                item.mark_committed();
            } else {
                let batch_sort_key = BatchSortKey::from_info(&batch);
                self.expirations
                    .add_item(batch_sort_key.clone(), batch.expiration());
                self.author_to_batches
                    .entry(batch.author())
                    .or_default()
                    .insert(batch_sort_key, batch.clone());
                self.items.insert(batch_key, QueueItem {
                    info: batch,
                    txn_summaries: None,
                    proof: None,
                    proof_insertion_time: None,
                });
            }
        }
        counters::PROOF_QUEUE_COMMIT_DURATION.observe_duration(start.elapsed());
    }
```

**File:** consensus/src/quorum_store/network_listener.rs (L95-104)
```rust
                    VerifiedEvent::ProofOfStoreMsg(proofs) => {
                        counters::QUORUM_STORE_MSG_COUNT
                            .with_label_values(&["NetworkListener::proofofstore"])
                            .inc();
                        let cmd = ProofManagerCommand::ReceiveProofs(*proofs);
                        self.proof_manager_tx
                            .send(cmd)
                            .await
                            .expect("could not push Proof proof_of_store");
                    },
```
