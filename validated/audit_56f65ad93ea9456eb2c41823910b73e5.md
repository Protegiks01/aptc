# Audit Report

## Title
Stale Quorum Certificate Pointer Causes Validator Crash After Block Pruning

## Summary
The `highest_certified_block_id` field in the consensus BlockTree is never updated during block pruning operations. When a new quorum certificate is inserted after the highest certified block has been pruned, the validator panics attempting to access the non-existent block, causing an immediate node crash. This vulnerability can be triggered during normal fork resolution without requiring Byzantine validators.

## Finding Description

The BlockTree maintains `highest_certified_block_id` and `highest_quorum_cert` fields to track the certified block with the highest round [1](#0-0) . These fields are updated exclusively when new quorum certificates are inserted via `insert_quorum_cert()` [2](#0-1) .

However, during block pruning operations, these fields are never checked or updated. The `remove_block()` function removes blocks from the tree but does not update the highest certified block pointer [3](#0-2) . The pruning orchestration methods `process_pruned_blocks()` and `commit_callback()` also fail to update these tracking fields [4](#0-3) [5](#0-4) .

This creates a critical invariant violation where `highest_certified_block_id` can point to a block that no longer exists in the tree. The `highest_certified_block()` accessor method uses `.expect()` which panics if the block doesn't exist [6](#0-5) .

**The vulnerability is triggered as follows:**

When `insert_quorum_cert()` is called to insert a new QC, it unconditionally evaluates `self.highest_certified_block().round()` to compare rounds [7](#0-6) . In Rust, both sides of the comparison operator are evaluated, so even if the new block's round is lower, the call to `highest_certified_block()` still occurs. If the highest certified block was previously pruned, this triggers the panic.

**Realistic Attack Scenario:**

1. During network delays or temporary partitions, validator V receives and certifies block B on a minority fork (round 100)
2. Block B becomes V's highest certified block via `insert_quorum_cert()`
3. The majority fork progresses and commits blocks through round 110
4. Validator V synchronizes with the majority fork
5. The `commit_callback()` prunes blocks not on the committed chain path, including block B via `find_blocks_to_prune()` [8](#0-7) 
6. Block B is removed from `id_to_block`, but `highest_certified_block_id` still references B
7. A new QC arrives for any block on the main chain (e.g., round 111)
8. `BlockStore::insert_single_quorum_cert()` is called from the sync manager [9](#0-8) 
9. At line 368 of `block_tree.rs`, the code attempts to evaluate `self.highest_certified_block().round()`
10. The method tries to retrieve pruned block B and panics with "Highest cerfified block must exist" [sic]
11. The validator process terminates immediately

This is production code, not test-only - the `insert_quorum_cert()` method is called during normal consensus operations through the BlockStore interface [10](#0-9) .

## Impact Explanation

**HIGH Severity** per Aptos Bug Bounty criteria:

1. **Validator Node Crash**: The panic causes immediate validator process termination, removing the validator from consensus participation. This directly qualifies as "Validator Node Slowdowns (High)" in the Aptos bug bounty program, as a crash is a severe form of unavailability.

2. **Recoverable but Disruptive**: The validator can restart and recover, as the pruned blocks won't be reloaded from storage and `highest_certified_block_id` will be recomputed from available QCs. However, the restart causes temporary loss of consensus participation.

3. **Liveness Impact**: If multiple validators experience this simultaneously during network partition recovery or epoch transitions, consensus liveness can be degraded until they restart.

4. **Protocol-Level Bug**: This is a deterministic bug in the consensus protocol implementation, not requiring any malicious actors or Byzantine behavior to trigger.

The vulnerability enables validator denial-of-service through a reproducible protocol-level flaw affecting the core consensus block management system.

## Likelihood Explanation

**MEDIUM-HIGH Likelihood**. This vulnerability can be triggered during normal network operations:

1. **Fork Resolution**: During normal consensus operation, validators may vote on different forks due to network propagation delays. When the network converges, blocks on losing forks are pruned. If a validator had a certified block on the losing fork with a higher round than blocks on the winning fork at the time of pruning, the stale pointer remains.

2. **Network Partitions**: When validators temporarily partition and rejoin, they may have certified blocks from minority partitions that get pruned during synchronization with the majority partition.

3. **Synchronization After Downtime**: Validators rejoining after temporary downtime may have gaps in their block tree that trigger pruning of previously certified blocks.

The vulnerability requires no Byzantine validators or malicious actors - only the natural occurrence of forks and pruning during consensus protocol operation. The pruning logic explicitly removes all blocks not on the committed chain's ancestry path [8](#0-7) , making this scenario achievable under realistic network conditions.

## Recommendation

Update the `process_pruned_blocks()` method to check if any pruned block ID matches `highest_certified_block_id`, and if so, reset it to a valid block (such as the commit root or window root):

```rust
pub(super) fn process_pruned_blocks(&mut self, mut newly_pruned_blocks: VecDeque<HashValue>) {
    counters::NUM_BLOCKS_IN_TREE.sub(newly_pruned_blocks.len() as i64);
    
    // Check if highest certified block is being pruned
    if newly_pruned_blocks.contains(&self.highest_certified_block_id) {
        // Reset to commit root which is guaranteed to exist
        self.highest_certified_block_id = self.commit_root_id;
        self.highest_quorum_cert = Arc::clone(
            self.id_to_quorum_cert
                .get(&self.commit_root_id)
                .expect("Commit root QC must exist")
        );
    }
    
    // Continue with existing pruning logic...
    self.pruned_block_ids.append(&mut newly_pruned_blocks);
    // ... rest of method
}
```

Alternatively, modify `insert_quorum_cert()` to handle the case where `highest_certified_block_id` points to a non-existent block:

```rust
match self.get_block(&block_id) {
    Some(block) => {
        let current_highest_round = self.get_block(&self.highest_certified_block_id)
            .map(|b| b.round())
            .unwrap_or(0); // Default to 0 if highest certified block doesn't exist
            
        if block.round() > current_highest_round {
            self.highest_certified_block_id = block.id();
            self.highest_quorum_cert = Arc::clone(&qc);
        }
    },
    None => bail!("Block {} not found", block_id),
}
```

## Proof of Concept

The vulnerability can be demonstrated with a Rust test that:
1. Creates a block tree with a fork
2. Certifies a block on the side fork (making it the highest certified block)
3. Prunes the side fork via `commit_callback()`
4. Attempts to insert a new QC, triggering the panic

```rust
#[tokio::test]
#[should_panic(expected = "Highest cerfified block must exist")]
async fn test_pruned_highest_certified_block_panic() {
    let mut inserter = TreeInserter::default();
    let block_store = inserter.block_store();
    let genesis = block_store.ordered_root();
    
    // Create main chain: genesis -> A1 -> A2
    let a1 = inserter.insert_block_with_qc(certificate_for_genesis(), &genesis, 1).await;
    let a2 = inserter.insert_block(&a1, 2, None).await;
    
    // Create side fork with higher round: genesis -> B1 (round 10)
    let b1 = inserter.insert_block_with_qc(certificate_for_genesis(), &genesis, 10).await;
    
    // B1 is now the highest certified block (round 10 > round 1)
    assert_eq!(block_store.highest_certified_block().round(), 10);
    
    // Commit A2, which prunes B1
    block_store.commit_callback(/* commit A2, this prunes B1 */);
    
    // Try to insert a new QC for a block on the main chain
    // This will panic because highest_certified_block_id still points to pruned B1
    let a3 = inserter.insert_block(&a2, 3, None).await;
    // Panic occurs when insert_block internally calls insert_quorum_cert
}
```

**Notes**

- The panic message contains a typo: "Highest cerfified block must exist" instead of "certified"
- The vulnerability is recoverable through validator restart, but causes temporary service disruption
- The root cause is the missing update of `highest_certified_block_id` during pruning operations
- The issue affects production code paths during normal consensus operations, not just test scenarios
- While the `highest_certified_block()` accessor method is marked `#[cfg(test)]` in the BlockStore wrapper [11](#0-10) , the underlying BlockTree method is production code and is called internally during QC insertion

### Citations

**File:** consensus/src/block_storage/block_tree.rs (L83-86)
```rust
    highest_certified_block_id: HashValue,

    /// The quorum certificate of highest_certified_block
    highest_quorum_cert: Arc<QuorumCert>,
```

**File:** consensus/src/block_storage/block_tree.rs (L174-181)
```rust
    fn remove_block(&mut self, block_id: HashValue) {
        // Remove the block from the store
        if let Some(block) = self.id_to_block.remove(&block_id) {
            let round = block.executed_block().round();
            self.round_to_ids.remove(&round);
        };
        self.id_to_quorum_cert.remove(&block_id);
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L208-211)
```rust
    pub(super) fn highest_certified_block(&self) -> Arc<PipelinedBlock> {
        self.get_block(&self.highest_certified_block_id)
            .expect("Highest cerfified block must exist")
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L366-372)
```rust
        match self.get_block(&block_id) {
            Some(block) => {
                if block.round() > self.highest_certified_block().round() {
                    self.highest_certified_block_id = block.id();
                    self.highest_quorum_cert = Arc::clone(&qc);
                }
            },
```

**File:** consensus/src/block_storage/block_tree.rs (L405-434)
```rust
    pub(super) fn find_blocks_to_prune(
        &self,
        next_window_root_id: HashValue,
    ) -> VecDeque<HashValue> {
        // Nothing to do if this is the window root
        if next_window_root_id == self.window_root_id {
            return VecDeque::new();
        }

        let mut blocks_pruned = VecDeque::new();
        let mut blocks_to_be_pruned = vec![self.linkable_window_root()];

        while let Some(block_to_remove) = blocks_to_be_pruned.pop() {
            block_to_remove.executed_block().abort_pipeline();
            // Add the children to the blocks to be pruned (if any), but stop when it reaches the
            // new root
            for child_id in block_to_remove.children() {
                if next_window_root_id == *child_id {
                    continue;
                }
                blocks_to_be_pruned.push(
                    self.get_linkable_block(child_id)
                        .expect("Child must exist in the tree"),
                );
            }
            // Track all the block ids removed
            blocks_pruned.push_back(block_to_remove.id());
        }
        blocks_pruned
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L496-510)
```rust
    pub(super) fn process_pruned_blocks(&mut self, mut newly_pruned_blocks: VecDeque<HashValue>) {
        counters::NUM_BLOCKS_IN_TREE.sub(newly_pruned_blocks.len() as i64);
        // The newly pruned blocks are pushed back to the deque pruned_block_ids.
        // In case the overall number of the elements is greater than the predefined threshold,
        // the oldest elements (in the front of the deque) are removed from the tree.
        self.pruned_block_ids.append(&mut newly_pruned_blocks);
        if self.pruned_block_ids.len() > self.max_pruned_blocks_in_mem {
            let num_blocks_to_remove = self.pruned_block_ids.len() - self.max_pruned_blocks_in_mem;
            for _ in 0..num_blocks_to_remove {
                if let Some(id) = self.pruned_block_ids.pop_front() {
                    self.remove_block(id);
                }
            }
        }
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L567-600)
```rust
    pub fn commit_callback(
        &mut self,
        storage: Arc<dyn PersistentLivenessStorage>,
        block_id: HashValue,
        block_round: Round,
        finality_proof: WrappedLedgerInfo,
        commit_decision: LedgerInfoWithSignatures,
        window_size: Option<u64>,
    ) {
        let current_round = self.commit_root().round();
        let committed_round = block_round;
        let commit_proof = finality_proof
            .create_merged_with_executed_state(commit_decision)
            .expect("Inconsistent commit proof and evaluation decision, cannot commit block");

        debug!(
            LogSchema::new(LogEvent::CommitViaBlock).round(current_round),
            committed_round = committed_round,
            block_id = block_id,
        );

        let window_root_id = self.find_window_root(block_id, window_size);
        let ids_to_remove = self.find_blocks_to_prune(window_root_id);

        if let Err(e) = storage.prune_tree(ids_to_remove.clone().into_iter().collect()) {
            // it's fine to fail here, as long as the commit succeeds, the next restart will clean
            // up dangling blocks, and we need to prune the tree to keep the root consistent with
            // executor.
            warn!(error = ?e, "fail to delete block");
        }
        self.process_pruned_blocks(ids_to_remove);
        self.update_window_root(window_root_id);
        self.update_highest_commit_cert(commit_proof);
    }
```

**File:** consensus/src/block_storage/block_store.rs (L518-556)
```rust
    /// Validates quorum certificates and inserts it into block tree assuming dependencies exist.
    pub fn insert_single_quorum_cert(&self, qc: QuorumCert) -> anyhow::Result<()> {
        // If the parent block is not the root block (i.e not None), ensure the executed state
        // of a block is consistent with its QuorumCert, otherwise persist the QuorumCert's
        // state and on restart, a new execution will agree with it.  A new execution will match
        // the QuorumCert's state on the next restart will work if there is a memory
        // corruption, for example.
        match self.get_block(qc.certified_block().id()) {
            Some(pipelined_block) => {
                ensure!(
                    // decoupled execution allows dummy block infos
                    pipelined_block
                        .block_info()
                        .match_ordered_only(qc.certified_block()),
                    "QC for block {} has different {:?} than local {:?}",
                    qc.certified_block().id(),
                    qc.certified_block(),
                    pipelined_block.block_info()
                );
                observe_block(
                    pipelined_block.block().timestamp_usecs(),
                    BlockStage::QC_ADDED,
                );
                if pipelined_block.block().is_opt_block() {
                    observe_block(
                        pipelined_block.block().timestamp_usecs(),
                        BlockStage::QC_ADDED_OPT_BLOCK,
                    );
                }
                pipelined_block.set_qc(Arc::new(qc.clone()));
            },
            None => bail!("Insert {} without having the block in store first", qc),
        };

        self.storage
            .save_tree(vec![], vec![qc.clone()])
            .context("Insert block failed when saving quorum")?;
        self.inner.write().insert_quorum_cert(qc)
    }
```

**File:** consensus/src/block_storage/block_store.rs (L659-662)
```rust
    #[cfg(test)]
    fn highest_certified_block(&self) -> Arc<PipelinedBlock> {
        self.inner.read().highest_certified_block()
    }
```

**File:** consensus/src/block_storage/sync_manager.rs (L175-201)
```rust
    pub async fn insert_quorum_cert(
        &self,
        qc: &QuorumCert,
        retriever: &mut BlockRetriever,
    ) -> anyhow::Result<()> {
        match self.need_fetch_for_quorum_cert(qc) {
            NeedFetchResult::NeedFetch => self.fetch_quorum_cert(qc.clone(), retriever).await?,
            NeedFetchResult::QCBlockExist => self.insert_single_quorum_cert(qc.clone())?,
            NeedFetchResult::QCAlreadyExist => return Ok(()),
            _ => (),
        }
        if self.ordered_root().round() < qc.commit_info().round() {
            SUCCESSFUL_EXECUTED_WITH_REGULAR_QC.inc();
            self.send_for_execution(qc.into_wrapped_ledger_info())
                .await?;
            if qc.ends_epoch() {
                retriever
                    .network
                    .broadcast_epoch_change(EpochChangeProof::new(
                        vec![qc.ledger_info().clone()],
                        /* more = */ false,
                    ))
                    .await;
            }
        }
        Ok(())
    }
```
