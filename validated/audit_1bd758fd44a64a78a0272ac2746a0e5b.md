# Audit Report

## Title
V2 Batch Storage Leak: Orphaned Records in batch_v2 Column Family Cause Storage Exhaustion

## Summary
The quorum store batch cleanup logic contains two critical bugs that prevent V2 batches from being deleted from the database. When `enable_batch_v2` is enabled, expired V2 batches accumulate indefinitely in the `batch_v2` column family, causing unbounded storage growth and eventual disk exhaustion on validator nodes.

## Finding Description

The Aptos consensus quorum store uses two separate column families to store batches: `BATCH_CF_NAME` ("batch") for V1 batches and `BATCH_V2_CF_NAME` ("batch_v2") for V2 batches. [1](#0-0) 

When batches are persisted, the code correctly routes them to the appropriate column family based on the batch version. [2](#0-1) 

However, there are **two critical bugs** in the cleanup logic:

**Bug #1: Epoch-based cleanup for V2 batches deletes from wrong column family**

The `gc_previous_epoch_batches_from_db_v2()` function reads V2 batches from the database using `get_all_batches_v2()` but then incorrectly calls `delete_batches()` which targets the V1 column family instead of `delete_batches_v2()`. [3](#0-2) 

The `delete_batches()` method exclusively operates on the `BatchSchema` (V1) column family, while `delete_batches_v2()` targets `BatchV2Schema` (V2). [4](#0-3) [5](#0-4) 

**Bug #2: Regular expiration cleanup only targets V1 column family**

The `update_certified_timestamp()` function, which is called regularly during consensus when blocks are committed, only deletes from the V1 column family. [6](#0-5) 

The `clear_expired_payload()` function removes expired entries from the unified in-memory cache (`db_cache: DashMap<HashValue, PersistedValue<BatchInfoExt>>`) which stores both V1 and V2 batches together. [7](#0-6)  However, the delete operation only calls `delete_batches()`, leaving V2 batch records orphaned in the `batch_v2` column family.

**Attack Path:**

1. A validator enables V2 batches by setting `enable_batch_v2 = true` in the quorum store configuration. [8](#0-7) 

2. The batch generator creates V2 batches when this flag is enabled. [9](#0-8) 

3. V2 batches are stored in the `batch_v2` column family using `save_batch_v2()`. [10](#0-9) 

4. During normal consensus operation, `notify_commit()` triggers `update_certified_timestamp()` on every block commit. [6](#0-5) 

5. Expired V2 batches are removed from the in-memory cache but never deleted from disk because `delete_batches()` targets the wrong column family.

6. V2 batches accumulate indefinitely in the `batch_v2` column family until storage is exhausted.

## Impact Explanation

**Severity: Medium**

This vulnerability causes storage exhaustion on validator nodes running with V2 batches enabled. The impact includes:

1. **Unbounded Storage Growth**: Every V2 batch created remains in the database forever, consuming disk space indefinitely
2. **Node Failure**: When disk space is exhausted, the validator node will crash or become unable to write new data
3. **Consensus Degradation**: Failed validators reduce network resilience and could impact consensus if enough nodes are affected
4. **Operational Overhead**: Requires manual intervention to clean up orphaned records or restore from backup

This meets the **Medium severity** criterion ($10,000 in the Aptos bug bounty program) for "State inconsistencies requiring manual intervention." It does not reach High severity because:
- It doesn't cause immediate consensus violation
- It doesn't directly enable fund theft
- Recovery is possible through disk cleanup and node restart
- The feature defaults to disabled

The vulnerability breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits."

## Likelihood Explanation

**Likelihood: High** (when V2 batches are enabled)

The vulnerability will automatically occur on any validator that:
1. Enables `enable_batch_v2 = true` in their configuration (a single config change)
2. Runs for an extended period (days to weeks depending on batch creation rate)

No attacker action is required - the bug triggers naturally during normal consensus operation. The default configuration disables V2 batches, [11](#0-10)  but any validator operator who enables this feature for testing or deployment will be immediately affected.

## Recommendation

Fix both bugs by ensuring V2 batch cleanup calls the correct deletion method:

**Fix for Bug #1**: In `gc_previous_epoch_batches_from_db_v2()`, change line 241 from:
```rust
db.delete_batches(expired_keys)
```
to:
```rust
db.delete_batches_v2(expired_keys)
```

**Fix for Bug #2**: In `update_certified_timestamp()`, after line 536, add:
```rust
if let Err(e) = self.db.delete_batches_v2(expired_keys.clone()) {
    debug!("Error deleting v2 batches: {:?}", e)
}
```

Alternatively, modify `clear_expired_payload()` to return separate vectors for V1 and V2 expired keys, then delete each using the appropriate method. The correct pattern is already implemented in `populate_cache_and_gc_expired_batches_v2()`. [12](#0-11) 

## Proof of Concept

This is a logic bug in the cleanup code that can be verified through code inspection. The vulnerability manifests over time as orphaned records accumulate. A test case would need to:

1. Enable `enable_batch_v2 = true` in the configuration
2. Create and persist multiple V2 batches
3. Trigger expiration via `update_certified_timestamp()` with an advanced timestamp
4. Verify that V2 batches remain in the `batch_v2` column family after cleanup
5. Verify that the in-memory cache is empty but persistent storage still contains the records

The comparison with the correctly implemented `populate_cache_and_gc_expired_batches_v2()` demonstrates the expected behavior where `delete_batches_v2()` is called for V2 batch cleanup.

## Notes

The vulnerability affects production code in the consensus layer but is not currently exploitable on mainnet because `enable_batch_v2` defaults to `false`. However, this is still a valid security vulnerability because:

1. It's a configurable option in production code, not test code
2. Any validator enabling this feature for testing or future deployment will be affected
3. The bugs prevent a feature from working correctly and cause resource exhaustion
4. Manual intervention is required to recover from the resulting storage exhaustion

### Citations

**File:** consensus/src/quorum_store/schema.rs (L14-16)
```rust
pub(crate) const BATCH_CF_NAME: ColumnFamilyName = "batch";
pub(crate) const BATCH_ID_CF_NAME: ColumnFamilyName = "batch_ID";
pub(crate) const BATCH_V2_CF_NAME: ColumnFamilyName = "batch_v2";
```

**File:** consensus/src/quorum_store/batch_store.rs (L212-243)
```rust
    fn gc_previous_epoch_batches_from_db_v2(db: Arc<dyn QuorumStoreStorage>, current_epoch: u64) {
        let db_content = db
            .get_all_batches_v2()
            .expect("failed to read data from db");
        info!(
            epoch = current_epoch,
            "QS: Read batches from storage. Len: {}",
            db_content.len(),
        );

        let mut expired_keys = Vec::new();
        for (digest, value) in db_content {
            let epoch = value.epoch();

            trace!(
                "QS: Batchreader recovery content epoch {:?}, digest {}",
                epoch,
                digest
            );

            if epoch < current_epoch {
                expired_keys.push(digest);
            }
        }

        info!(
            "QS: Batch store bootstrap expired keys len {}",
            expired_keys.len()
        );
        db.delete_batches(expired_keys)
            .expect("Deletion of expired keys should not fail");
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L332-335)
```rust
        tokio::task::spawn_blocking(move || {
            db.delete_batches_v2(expired_keys)
                .expect("Deletion of expired keys should not fail");
        });
```

**File:** consensus/src/quorum_store/batch_store.rs (L443-472)
```rust
    pub(crate) fn clear_expired_payload(&self, certified_time: u64) -> Vec<HashValue> {
        // To help slow nodes catch up via execution without going to state sync we keep the blocks for 60 extra seconds
        // after the expiration time. This will help remote peers fetch batches that just expired but are within their
        // execution window.
        let expiration_time = certified_time.saturating_sub(self.expiration_buffer_usecs);
        let expired_digests = self.expirations.lock().expire(expiration_time);
        let mut ret = Vec::new();
        for h in expired_digests {
            let removed_value = match self.db_cache.entry(h) {
                Occupied(entry) => {
                    // We need to check up-to-date expiration again because receiving the same
                    // digest with a higher expiration would update the persisted value and
                    // effectively extend the expiration.
                    if entry.get().expiration() <= expiration_time {
                        self.persist_subscribers.remove(entry.get().digest());
                        Some(entry.remove())
                    } else {
                        None
                    }
                },
                Vacant(_) => unreachable!("Expired entry not in cache"),
            };
            // No longer holding the lock on db_cache entry.
            if let Some(value) = removed_value {
                self.free_quota(value);
                ret.push(h);
            }
        }
        ret
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L501-513)
```rust
                    if !batch_info.is_v2() {
                        let persist_request =
                            persist_request.try_into().expect("Must be a V1 batch");
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch(persist_request)
                            .expect("Could not write to DB");
                    } else {
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch_v2(persist_request)
                            .expect("Could not write to DB")
                    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L530-539)
```rust
    pub fn update_certified_timestamp(&self, certified_time: u64) {
        trace!("QS: batch reader updating time {:?}", certified_time);
        self.last_certified_time
            .fetch_max(certified_time, Ordering::SeqCst);

        let expired_keys = self.clear_expired_payload(certified_time);
        if let Err(e) = self.db.delete_batches(expired_keys) {
            debug!("Error deleting batches: {:?}", e)
        }
    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L93-101)
```rust
    fn delete_batches(&self, digests: Vec<HashValue>) -> Result<(), DbError> {
        let mut batch = SchemaBatch::new();
        for digest in digests.iter() {
            trace!("QS: db delete digest {}", digest);
            batch.delete::<BatchSchema>(digest)?;
        }
        self.db.write_schemas_relaxed(batch)?;
        Ok(())
    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L123-131)
```rust
    fn delete_batches_v2(&self, digests: Vec<HashValue>) -> Result<(), DbError> {
        let mut batch = SchemaBatch::new();
        for digest in digests.iter() {
            trace!("QS: db delete digest {}", digest);
            batch.delete::<BatchV2Schema>(digest)?;
        }
        self.db.write_schemas_relaxed(batch)?;
        Ok(())
    }
```

**File:** config/src/config/quorum_store_config.rs (L102-102)
```rust
    pub enable_batch_v2: bool,
```

**File:** config/src/config/quorum_store_config.rs (L144-144)
```rust
            enable_batch_v2: false,
```

**File:** consensus/src/quorum_store/batch_generator.rs (L190-211)
```rust
        if self.config.enable_batch_v2 {
            // TODO(ibalajiarun): Specify accurate batch kind
            let batch_kind = BatchKind::Normal;
            Batch::new_v2(
                batch_id,
                txns,
                self.epoch,
                expiry_time,
                self.my_peer_id,
                bucket_start,
                batch_kind,
            )
        } else {
            Batch::new_v1(
                batch_id,
                txns,
                self.epoch,
                expiry_time,
                self.my_peer_id,
                bucket_start,
            )
        }
```
