# Audit Report

## Title
Non-Atomic Pruner Progress Writes Cause Validator Node Restart Failure After Partial Database Write

## Summary
The `write_pruner_progress()` function performs non-atomic writes across 8 separate sub-databases during fast sync completion. If the write operation fails partway through (e.g., due to disk I/O error, OOM, or process crash), the node enters an inconsistent state where different ledger components have different pruning progress values. On restart, this causes LedgerPruner initialization to fail, preventing the validator node from recovering without manual database intervention.

## Finding Description

The vulnerability lies in the sequential, non-atomic nature of pruner progress updates across multiple database instances. During fast sync finalization, `finalize_state_snapshot()` calls `save_min_readable_version()` on multiple pruners, which for the ledger pruner delegates to `write_pruner_progress()`. [1](#0-0) 

This function writes to 8 different sub-databases sequentially using the `?` operator for early return on error. [2](#0-1) [3](#0-2) 

Each individual `db.put()` call is atomic (implemented as a single-operation batch), but the sequence of 8 writes is NOT atomic. [4](#0-3)  When storage sharding is enabled (mandatory for mainnet/testnet), these are physically separate RocksDB instances. [5](#0-4) 

The critical issue is that `ledger_metadata_db.write_pruner_progress()` is called LAST (line 385), after all other sub-databases. If a system failure occurs after writing to some sub-databases but before completing the write to `ledger_metadata_db`, the metadata keys become inconsistent.

On restart, LedgerPruner initialization reads `metadata_progress` from the LedgerMetadataDb. [6](#0-5) 

Each sub-pruner initializes and attempts to "catch up" from its stored progress to `metadata_progress` by calling `prune()`. [7](#0-6) [8](#0-7) 

If a sub-pruner has progress=2000 but metadata_progress=1000 (due to partial write failure), it will call `prune(2000, 1000)`, attempting to prune backwards. For TransactionPruner, this triggers an error check failure. [9](#0-8)  The `ensure!` macro returns an error when the condition fails. [10](#0-9) 

This breaks the **State Consistency** invariant: metadata writes must be atomic to ensure consistent recovery state after system failures.

## Impact Explanation

**Severity: High**

This vulnerability causes **validator node unavailability** requiring manual intervention:

- **Affected Component**: Storage layer pruning system
- **Failure Mode**: Node cannot complete startup after system failure during fast sync
- **Recovery**: Requires manual database repair, forced resync, or state snapshot restoration
- **Network Impact**: Single validator unavailability (does not break consensus due to BFT tolerance)

This qualifies under Aptos bug bounty criteria for "Validator node slowdowns" (High Severity - up to $50,000). While it does not directly cause fund theft or consensus violation, it represents a critical availability failure that violates atomicity guarantees in database operations. The severity is High because it affects individual validator availability requiring manual intervention, though it does not impact network-wide consensus or funds.

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability can be triggered by natural system failures during production operation:

- **Disk I/O errors**: Hardware failures, filesystem corruption during the write window spanning 8 separate database instances
- **Memory exhaustion**: Large state sync operations causing OOM during writes
- **Process crashes**: Kernel OOM killer, segfaults, or unexpected termination mid-write

Such failures occur regularly in production environments, especially during resource-intensive fast sync operations. The vulnerability is **deterministic** once triggered - inconsistent metadata guarantees node restart failure without manual intervention. The vulnerability window exists across 8 sequential writes to physically separate RocksDB instances when sharding is enabled (which is mandatory for mainnet/testnet nodes).

## Recommendation

Implement atomic writes across all sub-databases for pruner progress updates. The recommended solution is to:

1. **Batch all pruner progress writes**: Collect all 8 progress updates in a coordinated batch before committing any of them
2. **Write metadata last with validation**: Only write to `ledger_metadata_db` after confirming all sub-database writes succeeded
3. **Add recovery logic**: On initialization, if any sub-pruner has progress greater than `metadata_progress`, reset that sub-pruner's progress to `metadata_progress` instead of attempting backward pruning
4. **Implement write-ahead logging**: Use a transaction log to ensure atomic updates across separate RocksDB instances

Example fix for the recovery logic in sub-pruner initialization:

```rust
// In pruner_utils.rs get_or_initialize_subpruner_progress
pub(crate) fn get_or_initialize_subpruner_progress(
    sub_db: &DB,
    progress_key: &DbMetadataKey,
    metadata_progress: Version,
) -> Result<Version> {
    let current_progress = if let Some(v) = sub_db.get::<DbMetadataSchema>(progress_key)? {
        v.expect_version()
    } else {
        metadata_progress
    };
    
    // Fix: If sub-pruner progress exceeds metadata progress (inconsistent state),
    // reset to metadata progress instead of attempting backward pruning
    if current_progress > metadata_progress {
        warn!("Sub-pruner progress {} exceeds metadata progress {}, resetting to metadata progress", 
              current_progress, metadata_progress);
        sub_db.put::<DbMetadataSchema>(
            progress_key,
            &DbMetadataValue::Version(metadata_progress),
        )?;
        return Ok(metadata_progress);
    }
    
    Ok(current_progress)
}
```

## Proof of Concept

While a full PoC would require failure injection during fast sync operations, the vulnerability is clearly demonstrable through code inspection:

1. The non-atomic nature of `write_pruner_progress()` is evident from the sequential writes with early-return error handling
2. The separate RocksDB instances are confirmed when sharding is enabled
3. The backward pruning check in `TransactionPruner::get_pruning_candidate_transactions()` will fail if metadata becomes inconsistent
4. The deterministic failure on restart is guaranteed by the catch-up logic in sub-pruner initialization

**Notes**

This vulnerability affects the storage layer's atomicity guarantees and validator availability. The issue is particularly concerning because:

1. Storage sharding is **mandatory** for mainnet/testnet nodes, making all production validators vulnerable
2. The failure is **deterministic** once the inconsistent state is created - the node cannot recover without manual intervention
3. The TODO comment at line 281 in `ledger_db/mod.rs` indicates developers are aware of potential data inconsistency issues but this specific failure mode may not be addressed
4. The vulnerability window spans 8 sequential database writes, increasing the probability of system failure mid-operation

While this does not break network-wide consensus due to BFT tolerance (the network continues with other validators), it represents a significant operational risk for individual validator operators who must perform manual database recovery to restore their node.

### Citations

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L225-234)
```rust
            self.ledger_pruner.save_min_readable_version(version)?;
            self.state_store
                .state_merkle_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .epoch_snapshot_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .state_kv_pruner
                .save_min_readable_version(version)?;
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L122-293)
```rust
    pub(crate) fn new<P: AsRef<Path>>(
        db_root_path: P,
        rocksdb_configs: RocksdbConfigs,
        env: Option<&Env>,
        block_cache: Option<&Cache>,
        readonly: bool,
    ) -> Result<Self> {
        let sharding = rocksdb_configs.enable_storage_sharding;
        let ledger_metadata_db_path = Self::metadata_db_path(db_root_path.as_ref(), sharding);
        let ledger_metadata_db = Arc::new(Self::open_rocksdb(
            ledger_metadata_db_path.clone(),
            if sharding {
                LEDGER_METADATA_DB_NAME
            } else {
                LEDGER_DB_NAME
            },
            &rocksdb_configs.ledger_db_config,
            env,
            block_cache,
            readonly,
        )?);

        info!(
            ledger_metadata_db_path = ledger_metadata_db_path,
            sharding = sharding,
            "Opened ledger metadata db!"
        );

        if !sharding {
            info!("Individual ledger dbs are not enabled!");
            return Ok(Self {
                ledger_metadata_db: LedgerMetadataDb::new(Arc::clone(&ledger_metadata_db)),
                event_db: EventDb::new(
                    Arc::clone(&ledger_metadata_db),
                    EventStore::new(Arc::clone(&ledger_metadata_db)),
                ),
                persisted_auxiliary_info_db: PersistedAuxiliaryInfoDb::new(Arc::clone(
                    &ledger_metadata_db,
                )),
                transaction_accumulator_db: TransactionAccumulatorDb::new(Arc::clone(
                    &ledger_metadata_db,
                )),
                transaction_auxiliary_data_db: TransactionAuxiliaryDataDb::new(Arc::clone(
                    &ledger_metadata_db,
                )),
                transaction_db: TransactionDb::new(Arc::clone(&ledger_metadata_db)),
                transaction_info_db: TransactionInfoDb::new(Arc::clone(&ledger_metadata_db)),
                write_set_db: WriteSetDb::new(Arc::clone(&ledger_metadata_db)),
                enable_storage_sharding: false,
            });
        }

        let ledger_db_folder = db_root_path.as_ref().join(LEDGER_DB_FOLDER_NAME);

        let mut event_db = None;
        let mut persisted_auxiliary_info_db = None;
        let mut transaction_accumulator_db = None;
        let mut transaction_auxiliary_data_db = None;
        let mut transaction_db = None;
        let mut transaction_info_db = None;
        let mut write_set_db = None;
        THREAD_MANAGER.get_non_exe_cpu_pool().scope(|s| {
            s.spawn(|_| {
                let event_db_raw = Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(EVENT_DB_NAME),
                        EVENT_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                );
                event_db = Some(EventDb::new(
                    event_db_raw.clone(),
                    EventStore::new(event_db_raw),
                ));
            });
            s.spawn(|_| {
                persisted_auxiliary_info_db = Some(PersistedAuxiliaryInfoDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(PERSISTED_AUXILIARY_INFO_DB_NAME),
                        PERSISTED_AUXILIARY_INFO_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_accumulator_db = Some(TransactionAccumulatorDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_ACCUMULATOR_DB_NAME),
                        TRANSACTION_ACCUMULATOR_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_auxiliary_data_db = Some(TransactionAuxiliaryDataDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_AUXILIARY_DATA_DB_NAME),
                        TRANSACTION_AUXILIARY_DATA_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )))
            });
            s.spawn(|_| {
                transaction_db = Some(TransactionDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_DB_NAME),
                        TRANSACTION_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_info_db = Some(TransactionInfoDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_INFO_DB_NAME),
                        TRANSACTION_INFO_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                write_set_db = Some(WriteSetDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(WRITE_SET_DB_NAME),
                        WRITE_SET_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
        });

        // TODO(grao): Handle data inconsistency.

        Ok(Self {
            ledger_metadata_db: LedgerMetadataDb::new(ledger_metadata_db),
            event_db: event_db.unwrap(),
            persisted_auxiliary_info_db: persisted_auxiliary_info_db.unwrap(),
            transaction_accumulator_db: transaction_accumulator_db.unwrap(),
            transaction_auxiliary_data_db: transaction_auxiliary_data_db.unwrap(),
            transaction_db: transaction_db.unwrap(),
            transaction_info_db: transaction_info_db.unwrap(),
            write_set_db: write_set_db.unwrap(),
            enable_storage_sharding: true,
        })
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L373-388)
```rust
    pub(crate) fn write_pruner_progress(&self, version: Version) -> Result<()> {
        info!("Fast sync is done, writing pruner progress {version} for all ledger sub pruners.");
        self.event_db.write_pruner_progress(version)?;
        self.persisted_auxiliary_info_db
            .write_pruner_progress(version)?;
        self.transaction_accumulator_db
            .write_pruner_progress(version)?;
        self.transaction_auxiliary_data_db
            .write_pruner_progress(version)?;
        self.transaction_db.write_pruner_progress(version)?;
        self.transaction_info_db.write_pruner_progress(version)?;
        self.write_set_db.write_pruner_progress(version)?;
        self.ledger_metadata_db.write_pruner_progress(version)?;

        Ok(())
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L80-89)
```rust
    fn save_min_readable_version(&self, min_readable_version: Version) -> Result<()> {
        self.min_readable_version
            .store(min_readable_version, Ordering::SeqCst);

        PRUNER_VERSIONS
            .with_label_values(&["ledger_pruner", "min_readable"])
            .set(min_readable_version as i64);

        self.ledger_db.write_pruner_progress(min_readable_version)
    }
```

**File:** storage/schemadb/src/lib.rs (L238-244)
```rust
    /// Writes single record.
    pub fn put<S: Schema>(&self, key: &S::Key, value: &S::Value) -> DbResult<()> {
        // Not necessary to use a batch, but we'd like a central place to bump counters.
        let mut batch = self.new_native_batch();
        batch.put::<S>(key, value)?;
        self.write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L118-194)
```rust
    pub fn new(
        ledger_db: Arc<LedgerDb>,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Result<Self> {
        info!(name = LEDGER_PRUNER_NAME, "Initializing...");

        let ledger_metadata_pruner = Box::new(
            LedgerMetadataPruner::new(ledger_db.metadata_db_arc())
                .expect("Failed to initialize ledger_metadata_pruner."),
        );

        let metadata_progress = ledger_metadata_pruner.progress()?;

        info!(
            metadata_progress = metadata_progress,
            "Created ledger metadata pruner, start catching up all sub pruners."
        );

        let transaction_store = Arc::new(TransactionStore::new(Arc::clone(&ledger_db)));

        let event_store_pruner = Box::new(EventStorePruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
            internal_indexer_db.clone(),
        )?);
        let persisted_auxiliary_info_pruner = Box::new(PersistedAuxiliaryInfoPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);
        let transaction_accumulator_pruner = Box::new(TransactionAccumulatorPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);

        let transaction_auxiliary_data_pruner = Box::new(TransactionAuxiliaryDataPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);

        let transaction_info_pruner = Box::new(TransactionInfoPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);
        let transaction_pruner = Box::new(TransactionPruner::new(
            Arc::clone(&transaction_store),
            Arc::clone(&ledger_db),
            metadata_progress,
            internal_indexer_db,
        )?);
        let write_set_pruner = Box::new(WriteSetPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);

        let pruner = LedgerPruner {
            target_version: AtomicVersion::new(metadata_progress),
            progress: AtomicVersion::new(metadata_progress),
            ledger_metadata_pruner,
            sub_pruners: vec![
                event_store_pruner,
                persisted_auxiliary_info_pruner,
                transaction_accumulator_pruner,
                transaction_auxiliary_data_pruner,
                transaction_info_pruner,
                transaction_pruner,
                write_set_pruner,
            ],
        };

        info!(
            name = pruner.name(),
            progress = metadata_progress,
            "Initialized."
        );

        Ok(pruner)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L85-109)
```rust
    pub(in crate::pruner) fn new(
        ledger_db: Arc<LedgerDb>,
        metadata_progress: Version,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.event_db_raw(),
            &DbMetadataKey::EventPrunerProgress,
            metadata_progress,
        )?;

        let myself = EventStorePruner {
            ledger_db,
            internal_indexer_db,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up EventStorePruner."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/pruner_utils.rs (L44-60)
```rust
pub(crate) fn get_or_initialize_subpruner_progress(
    sub_db: &DB,
    progress_key: &DbMetadataKey,
    metadata_progress: Version,
) -> Result<Version> {
    Ok(
        if let Some(v) = sub_db.get::<DbMetadataSchema>(progress_key)? {
            v.expect_version()
        } else {
            sub_db.put::<DbMetadataSchema>(
                progress_key,
                &DbMetadataValue::Version(metadata_progress),
            )?;
            metadata_progress
        },
    )
}
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L106-131)
```rust
    fn get_pruning_candidate_transactions(
        &self,
        start: Version,
        end: Version,
    ) -> Result<Vec<(Version, Transaction)>> {
        ensure!(end >= start, "{} must be >= {}", end, start);

        let mut iter = self
            .ledger_db
            .transaction_db_raw()
            .iter::<TransactionSchema>()?;
        iter.seek(&start)?;

        // The capacity is capped by the max number of txns we prune in a single batch. It's a
        // relatively small number set in the config, so it won't cause high memory usage here.
        let mut txns = Vec::with_capacity((end - start) as usize);
        for item in iter {
            let (version, txn) = item?;
            if version >= end {
                break;
            }
            txns.push((version, txn));
        }

        Ok(txns)
    }
```

**File:** storage/storage-interface/src/lib.rs (L758-764)
```rust
macro_rules! db_ensure {
    ($cond:expr, $($arg:tt)*) => {
        if !$cond {
            return Err(AptosDbError::Other(format!($($arg)*)));
        }
    };
}
```
