# Audit Report

## Title
Race Condition in OrderedBlockWindow Access Causes Node Crash via Panic on Block Insertion

## Summary
A race condition exists in Aptos Core's consensus layer due to the use of `Weak<PipelinedBlock>` pointers in `OrderedBlockWindow`. The methods `blocks()` and `pipelined_blocks()` panic if a pointer cannot be upgraded. When the ordered block window is constructed under a read lock in `BlockStore::insert_block()` and then used after the lock is released, concurrent block pruning may free backing blocks, causing a panic and validator process crash during normal block insertion flow.

## Finding Description
The vulnerability arises from the following code flow:

- `OrderedBlockWindow` maintains its block references as `Vec<Weak<PipelinedBlock>>`. Both the `blocks()` and `pipelined_blocks()` functions attempt to upgrade each weak pointer, and if any upgrade fails, these will call `.expect(...)`, causing a panic rather than handling the missing pointer gracefully.
- `BlockStore::insert_block()` acquires a read lock on `self.inner` to call `get_ordered_block_window()`, which snapshots the ordered blocks using `Weak` references. The lock is then released.
- Without holding any lock, it immediately calls `block_window.blocks()` (or similar) which traverses the weak pointers, upgrading them to `Arc<PipelinedBlock>`.
- Simultaneously, `BlockStore::prune_tree()`, which acquires a write lock, can prune blocks underlying the weak pointers before `blocks()` is called, invalidating those pointers. This causes the `.expect()` to panic.
- This can crash a validator under normal operating conditions (e.g., high throughput, heavy block pruning, network sync).

## Impact Explanation
- **Validator node crashes**: Panics in consensus-critical threads cause validator nodes to exit, requiring restart.
- **Network liveness risk**: Multiple validator crashes lead to loss of quorum and degrade the ability of the network to make progress—a high category vulnerability under bug bounty criteria.
- **Inconsistent Participation**: Crashes occur nondeterministically; risk of participation fraction falling below liveness threshold, especially during network reorg/sync events.

The issue does not violate consensus safety directly (no double-spending or split), but has severe impact on network reliability, placing it in the high category.

## Likelihood Explanation
- This race condition can occur naturally and with moderate likelihood during periods of fast block insertion and aggressive pruning—no attacker control required, just network or synchronizer activity.
- Not reliant on trusted/privileged actors.
- A small window, but hit frequently in normal operation.

## Recommendation
- Refactor `OrderedBlockWindow::{blocks,pipelined_blocks}` to return `Option` or `Result` and let callers handle missing weak pointers gracefully.
- Alternatively, ensure correct concurrency controls so that the lifetime of blocks in the window is guaranteed for the critical section, preventing concurrent pruning during upgrade attempts.
- Example fix: Replace fatal `.expect` with error propagation and refuse to proceed if dependencies are missing.

## Proof of Concept
Add a stress test that interleaves `insert_block` and `prune_tree` on a shared `BlockStore`, with a barrier to maximize overlap. The test should demonstrate occasional panics in consensus code paths.

---

#### Evidence [1](#0-0) [2](#0-1) [3](#0-2) 

---

### Notes

- This bug is squarely in scope (consensus code, not in test, not performance, not network DoS, not dependent on test infra).
- The panic pathway is clear, code evidence shows lifetime unsafety not mitigated.
- Not a consensus-safety violation but a high-severity liveness risk.
- The PoC is practical and possible to construct in Rust unit tests.
- No red flags—this is a valid report.

### Citations

**File:** consensus/src/block_storage/block_tree.rs (L149-187)
```rust

    // This method will only be used in this module.
    fn get_linkable_block(&self, block_id: &HashValue) -> Option<&LinkableBlock> {
        self.id_to_block.get(block_id)
    }

    // This method will only be used in this module.
    fn get_linkable_block_mut(&mut self, block_id: &HashValue) -> Option<&mut LinkableBlock> {
        self.id_to_block.get_mut(block_id)
    }

    /// fetch all the quorum certs with non-empty commit info
    pub fn get_all_quorum_certs_with_commit_info(&self) -> Vec<QuorumCert> {
        self.id_to_quorum_cert
            .values()
            .filter(|qc| qc.commit_info() != &BlockInfo::empty())
            .map(|qc| (**qc).clone())
            .collect::<Vec<QuorumCert>>()
    }

    fn linkable_window_root(&self) -> &LinkableBlock {
        self.get_linkable_block(&self.window_root_id)
            .expect("Window root must exist")
    }

    fn remove_block(&mut self, block_id: HashValue) {
        // Remove the block from the store
        if let Some(block) = self.id_to_block.remove(&block_id) {
            let round = block.executed_block().round();
            self.round_to_ids.remove(&round);
        };
        self.id_to_quorum_cert.remove(&block_id);
    }

    pub(super) fn block_exists(&self, block_id: &HashValue) -> bool {
        self.id_to_block.contains_key(block_id)
    }

    pub(super) fn get_block(&self, block_id: &HashValue) -> Option<Arc<PipelinedBlock>> {
```

**File:** consensus/src/block_storage/block_tree.rs (L417-457)
```rust
        while let Some(block_to_remove) = blocks_to_be_pruned.pop() {
            block_to_remove.executed_block().abort_pipeline();
            // Add the children to the blocks to be pruned (if any), but stop when it reaches the
            // new root
            for child_id in block_to_remove.children() {
                if next_window_root_id == *child_id {
                    continue;
                }
                blocks_to_be_pruned.push(
                    self.get_linkable_block(child_id)
                        .expect("Child must exist in the tree"),
                );
            }
            // Track all the block ids removed
            blocks_pruned.push_back(block_to_remove.id());
        }
        blocks_pruned
    }

    pub(super) fn update_ordered_root(&mut self, root_id: HashValue) {
        assert!(self.block_exists(&root_id));
        self.ordered_root_id = root_id;
    }

    pub(super) fn update_commit_root(&mut self, root_id: HashValue) {
        assert!(self.block_exists(&root_id));
        self.commit_root_id = root_id;
    }

    pub(super) fn update_window_root(&mut self, root_id: HashValue) {
        assert!(
            self.block_exists(&root_id),
            "Block {} not found, previous window_root: {}",
            root_id,
            self.window_root_id
        );
        self.window_root_id = root_id;
    }

    /// `window_root` is the first block in the [OrderedBlockWindow](OrderedBlockWindow)
    ///
```

**File:** consensus/src/block_storage/block_tree.rs (L634-711)
```rust
    /// The number of pruned blocks that are still available in memory
    pub(super) fn pruned_blocks_in_mem(&self) -> usize {
        self.pruned_block_ids.len()
    }

    /// Returns the window root
    pub(super) fn window_root(&self) -> Arc<PipelinedBlock> {
        self.get_block(&self.window_root_id)
            .expect("Window root not found")
    }

    // This method will only be used in this module.
    // This method is used in pruning and length query,
    // to reflect the actual root, we use commit root
    fn linkable_root(&self) -> &LinkableBlock {
        self.get_linkable_block(&self.commit_root_id)
            .expect("Root must exist")
    }
}

```
