# Audit Report

## Title
Secret Share Manager Not Reset During State Synchronization Leading to State Inconsistency and Liveness Issues

## Summary
The `ExecutionProxyClient::reset()` method fails to send reset signals to the `SecretShareManager` during state synchronization, while correctly resetting the `RandManager` and `BufferManager`. This causes the secret share manager to retain stale blocks and outdated state after a node syncs to a new ledger info, leading to state inconsistencies between pipeline components and potential liveness failures.

## Finding Description

The vulnerability exists in the coordination between multiple consensus pipeline managers during state synchronization. When a validator node falls behind and must sync to catch up, the `reset()` method is called to clear internal state and prepare for processing blocks from the new synced position.

The `BufferManagerHandle` structure contains three reset channels for coordinating resets across pipeline components: [1](#0-0) 

However, the `ExecutionProxyClient::reset()` method only retrieves and uses two of these three channels (`reset_tx_to_rand_manager` and `reset_tx_to_buffer_manager`), completely ignoring `reset_tx_to_secret_share_manager`: [2](#0-1) 

This reset method is called during state synchronization operations in both `sync_for_duration()` and `sync_to_target()`: [3](#0-2) 

When both randomness and secret sharing are enabled (standard configuration for on-chain randomness), the system creates a coordinator that requires BOTH the rand manager AND secret share manager to signal readiness before forwarding blocks for execution: [4](#0-3) 

The coordinator tracks readiness using two boolean flags and only forwards blocks when both are true: [5](#0-4) 

The `SecretShareManager` maintains a `BlockQueue` and `highest_known_round` that must be reset when syncing to a new state. The `process_reset()` method exists for this purpose and properly clears the block queue: [6](#0-5) 

The `BlockQueue` only dequeues ready blocks from the front of the queue, stopping at the first non-ready block: [7](#0-6) 

**Attack Scenario:**

1. A validator node runs with both `rand_config` and `secret_sharing_config` enabled
2. The node falls behind consensus and initiates state sync
3. State sync calls `ExecutionProxyClient::reset()` which only resets rand and buffer managers
4. The secret share manager retains stale blocks in its queue from rounds before the sync
5. New blocks arrive at higher rounds after the sync
6. These new blocks are sent to both managers by the coordinator
7. The rand manager processes and marks new blocks as ready
8. The secret share manager has old blocks at the front of its queue that will never receive their secret shares (the network has moved on)
9. Since `dequeue_ready_prefix()` only processes from the front and stops at non-ready blocks, new blocks cannot be dequeued
10. The coordinator waits indefinitely for the secret share manager to mark blocks as ready
11. Block execution halts until epoch transition

Notably, `end_epoch()` properly resets all three managers including the secret share manager, which is why the issue is recoverable at epoch boundaries: [8](#0-7) 

## Impact Explanation

This vulnerability qualifies as **High Severity** based on Aptos bug bounty criteria:

**Validator Node Slowdowns (High - up to $50,000)**: Nodes experiencing this issue will fail to execute blocks until the epoch ends. During this period, the affected validator cannot participate in consensus, reducing network capacity and validator rewards. This directly matches the "Validator Node Slowdowns" category in the Aptos bug bounty program.

**State Inconsistency**: Different pipeline components have conflicting views of the blockchain state, violating fundamental assumptions about synchronized component state.

**Network-Wide Impact**: If multiple validators sync simultaneously (common during network partitions or coordinated upgrades), this could cause widespread liveness degradation.

The issue does not reach **Critical Severity** because:
- It's recoverable at epoch boundaries
- It doesn't directly cause fund loss
- It doesn't create permanent consensus safety violations
- It requires state sync to trigger (not affecting all validators continuously)

## Likelihood Explanation

This vulnerability has **HIGH likelihood** of occurring:

**Frequent Trigger Condition**: State synchronization occurs regularly when:
- New validators join the network
- Validators restart after maintenance
- Validators experience temporary network issues
- Validators fall behind during high network load

**Configuration Requirements**: The vulnerability manifests when both `rand_config` and `secret_sharing_config` are enabled, which is the standard configuration for networks using on-chain randomness, as shown in the pipeline initialization logic.

**No Attacker Required**: The bug triggers automatically during normal operations without requiring any malicious actor or special network conditions.

**Observable Symptoms**: Affected validators show clear symptoms (inability to execute blocks, growing queue sizes), making the issue detectable but difficult to recover from without epoch transition.

## Recommendation

Modify `ExecutionProxyClient::reset()` to include the secret share manager reset:

```rust
async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
    let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager, reset_tx_to_secret_share_manager) = {
        let handle = self.handle.read();
        (
            handle.reset_tx_to_rand_manager.clone(),
            handle.reset_tx_to_buffer_manager.clone(),
            handle.reset_tx_to_secret_share_manager.clone(),
        )
    };

    if let Some(mut reset_tx) = reset_tx_to_rand_manager {
        let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::RandResetDropped)?;
        ack_rx.await.map_err(|_| Error::RandResetDropped)?;
    }

    if let Some(mut reset_tx) = reset_tx_to_secret_share_manager {
        let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::SecretShareResetDropped)?;
        ack_rx.await.map_err(|_| Error::SecretShareResetDropped)?;
    }

    if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
        let (tx, rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::ResetDropped)?;
        rx.await.map_err(|_| Error::ResetDropped)?;
    }

    Ok(())
}
```

This ensures all three managers (rand, secret share, and buffer) are properly reset during state synchronization, maintaining state consistency across all pipeline components.

## Proof of Concept

While a full executable PoC would require a complex test environment with multiple validators and state sync simulation, the vulnerability can be demonstrated through code analysis:

1. The `reset()` method demonstrably only retrieves two of three reset channels
2. The coordinator demonstrably requires both managers to signal readiness
3. The `BlockQueue` demonstrably only dequeues from the front and stops at non-ready blocks
4. The `process_reset()` method exists and would clear stale state if called
5. The `end_epoch()` method demonstrates the correct pattern by resetting all three managers

A test scenario would involve:
1. Starting a validator with both randomness and secret sharing enabled
2. Processing blocks to populate the secret share manager queue
3. Triggering state sync to a higher round
4. Observing that the secret share manager retains old blocks while new blocks are blocked from execution
5. Confirming recovery only occurs at epoch transition

## Notes

This vulnerability demonstrates a classic coordination bug where the reset logic was partially implemented. The `end_epoch()` method shows the developers understood all three managers need resetting, but this knowledge was not applied to the `reset()` method used during state synchronization. The fix is straightforward and follows the existing pattern established by `end_epoch()`.

### Citations

**File:** consensus/src/pipeline/execution_client.rs (L128-130)
```rust
    pub reset_tx_to_buffer_manager: Option<UnboundedSender<ResetRequest>>,
    pub reset_tx_to_rand_manager: Option<UnboundedSender<ResetRequest>>,
    pub reset_tx_to_secret_share_manager: Option<UnboundedSender<ResetRequest>>,
```

**File:** consensus/src/pipeline/execution_client.rs (L324-360)
```rust
            let mut inflight_block_tracker: HashMap<
                HashValue,
                (
                    OrderedBlocks,
                    /* rand_ready */ bool,
                    /* secret ready */ bool,
                ),
            > = HashMap::new();
            loop {
                let entry = select! {
                    Some(ordered_blocks) = ordered_block_rx.next() => {
                        let _ = rand_manager_input_tx.send(ordered_blocks.clone()).await;
                        let _ = secret_share_manager_input_tx.send(ordered_blocks.clone()).await;
                        let first_block_id = ordered_blocks.ordered_blocks.first().expect("Cannot be empty").id();
                        inflight_block_tracker.insert(first_block_id, (ordered_blocks, false, false));
                        inflight_block_tracker.entry(first_block_id)
                    },
                    Some(rand_ready_block) = rand_ready_block_rx.next() => {
                        let first_block_id = rand_ready_block.ordered_blocks.first().expect("Cannot be empty").id();
                        inflight_block_tracker.entry(first_block_id).and_modify(|result| {
                            result.1 = true;
                        })
                    },
                    Some(secret_ready_block) = secret_ready_block_rx.next() => {
                        let first_block_id = secret_ready_block.ordered_blocks.first().expect("Cannot be empty").id();
                        inflight_block_tracker.entry(first_block_id).and_modify(|result| {
                            result.2 = true;
                        })
                    },
                };
                let Entry::Occupied(o) = entry else {
                    unreachable!("Entry must exist");
                };
                if o.get().1 && o.get().2 {
                    let (_, (ordered_blocks, _, _)) = o.remove_entry();
                    let _ = ready_block_tx.send(ordered_blocks).await;
                }
```

**File:** consensus/src/pipeline/execution_client.rs (L400-436)
```rust
            (Some(rand_config), Some(secret_sharing_config)) => {
                let (rand_manager_input_tx, rand_ready_block_rx, reset_tx_to_rand_manager) = self
                    .make_rand_manager(
                        &epoch_state,
                        fast_rand_config,
                        rand_msg_rx,
                        highest_committed_round,
                        &network_sender,
                        rand_config,
                        consensus_sk,
                    );

                let (
                    secret_share_manager_input_tx,
                    secret_ready_block_rx,
                    reset_tx_to_secret_share_manager,
                ) = self.make_secret_sharing_manager(
                    &epoch_state,
                    secret_sharing_config,
                    secret_sharing_msg_rx,
                    highest_committed_round,
                    &network_sender,
                );

                let (ordered_block_tx, ready_block_rx) = Self::make_coordinator(
                    rand_manager_input_tx,
                    rand_ready_block_rx,
                    secret_share_manager_input_tx,
                    secret_ready_block_rx,
                );

                (
                    ordered_block_tx,
                    ready_block_rx,
                    Some(reset_tx_to_rand_manager),
                    Some(reset_tx_to_secret_share_manager),
                )
```

**File:** consensus/src/pipeline/execution_client.rs (L650-672)
```rust
        // Sync for the specified duration
        let result = self.execution_proxy.sync_for_duration(duration).await;

        // Reset the rand and buffer managers to the new synced round
        if let Ok(latest_synced_ledger_info) = &result {
            self.reset(latest_synced_ledger_info).await?;
        }

        result
    }

    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });

        // Reset the rand and buffer managers to the target round
        self.reset(&target).await?;

        // TODO: handle the state sync error (e.g., re-push the ordered
        // blocks to the buffer manager when it's reset but sync fails).
        self.execution_proxy.sync_to_target(target).await
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L674-709)
```rust
    async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
        let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager) = {
            let handle = self.handle.read();
            (
                handle.reset_tx_to_rand_manager.clone(),
                handle.reset_tx_to_buffer_manager.clone(),
            )
        };

        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        }

        if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
            // reset execution phase and commit phase
            let (tx, rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::ResetDropped)?;
            rx.await.map_err(|_| Error::ResetDropped)?;
        }

        Ok(())
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L711-760)
```rust
    async fn end_epoch(&self) {
        let (
            reset_tx_to_rand_manager,
            reset_tx_to_buffer_manager,
            reset_tx_to_secret_share_manager,
        ) = {
            let mut handle = self.handle.write();
            handle.reset()
        };

        if let Some(mut tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop rand manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop rand manager");
        }

        if let Some(mut tx) = reset_tx_to_secret_share_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop secret share manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop secret share manager");
        }

        if let Some(mut tx) = reset_tx_to_buffer_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop buffer manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop buffer manager");
        }
        self.execution_proxy.end_epoch();
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L172-184)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.secret_share_store
            .lock()
            .update_highest_known_round(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L112-127)
```rust
    pub fn dequeue_ready_prefix(&mut self) -> Vec<OrderedBlocks> {
        let mut ready_prefix = vec![];
        while let Some((_starting_round, item)) = self.queue.first_key_value() {
            if item.is_fully_secret_shared() {
                let (_, item) = self.queue.pop_first().expect("First key must exist");
                for block in item.blocks() {
                    observe_block(block.timestamp_usecs(), BlockStage::SECRET_SHARING_READY);
                }
                let QueueItem { ordered_blocks, .. } = item;
                ready_prefix.push(ordered_blocks);
            } else {
                break;
            }
        }
        ready_prefix
    }
```
