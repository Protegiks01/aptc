# Audit Report

## Title
Out-of-Memory Vulnerability in TransactionPruner Initialization Causing Unrecoverable Validator Node Crash

## Summary
The `TransactionPruner` initialization flow contains a critical resource management flaw where the catch-up mechanism attempts to prune the entire gap between stored progress and current metadata progress without batch size limiting. When a gap of millions of versions exists, this causes unbounded memory allocation for Transaction objects, leading to OOM crashes and unrecoverable crash loops on validator node restart.

## Finding Description

The vulnerability exists in the initialization flow of the TransactionPruner subsystem. During node startup, `TransactionPruner::new()` performs a "catch-up" operation by directly calling `prune()` with the full gap between its stored progress and the metadata pruner's progress, completely bypassing the batch size limits that protect normal pruning operations.

**Critical Code Flow**:

1. During initialization, `TransactionPruner::new()` calls `prune(progress, metadata_progress)` without any batch limiting: [1](#0-0) 

2. The `prune()` implementation calls `get_pruning_candidate_transactions()` with the full version range: [2](#0-1) 

3. `get_pruning_candidate_transactions()` pre-allocates a vector with capacity `(end - start)` and loads ALL transactions in that range into memory: [3](#0-2) 

4. The misleading comment claims safety based on batch size capping, which is FALSE during initialization: [4](#0-3) 

**Contrast with Normal Operation**:

During normal operation, `LedgerPruner::prune()` correctly enforces batch size limits: [5](#0-4) 

However, this protection is completely bypassed during the initialization catch-up phase in `TransactionPruner::new()`.

**How the Gap Forms**:

Each sub-pruner stores its progress independently in the database. The stored progress is retrieved via: [6](#0-5) 

If TransactionPruner's progress metadata becomes stale (e.g., due to database corruption, silent failures, or crash timing issues) while the metadata_progress advances, a gap forms. With the default prune_window of 90 million versions: [7](#0-6) 

The gap can reach millions of versions.

**Memory Impact Calculation**:

The Transaction enum is explicitly marked as large: [8](#0-7) 

With variants containing SignedTransaction, WriteSetPayload, BlockMetadata, etc., each transaction easily exceeds 500 bytes. For a 90 million version gap:
- Memory required: 90M × 500 bytes = **45+ GB**
- Result: OOM crash during initialization
- Consequence: Unrecoverable crash loop on every restart attempt

## Impact Explanation

This vulnerability meets **HIGH Severity** criteria per the Aptos bug bounty program:

**Validator Node Crashes**: The OOM condition causes immediate validator node termination during the initialization phase, preventing the node from starting.

**Unrecoverable Crash Loop**: Since the vulnerability triggers during initialization, the node crashes every time it attempts to restart. Without manual database intervention to reset the pruner progress metadata, the node cannot recover autonomously.

**Network Availability Impact**: If multiple validators experience the same underlying issue (e.g., a widespread bug affecting TransactionPruner progress updates, or correlated database corruption), this could degrade network liveness and consensus participation, potentially affecting block production rates.

This is NOT a "Network DoS attack" (which is out of scope). This is a resource management bug in the codebase itself that causes validator crashes through unbounded memory allocation—a legitimate availability vulnerability similar to other "validator node crashes" explicitly covered by the bug bounty program.

## Likelihood Explanation

**Likelihood: Medium**

While the code flaw is definitively present, triggering it requires specific operational conditions:

**Realistic Triggering Scenarios**:

1. **Database Corruption**: Partial corruption affecting TransactionPruner's progress metadata key while leaving metadata_progress intact
2. **Silent Pruner Failures**: A subtle bug in TransactionPruner causing progress updates to fail while the node continues operating
3. **Crash Timing**: Node crash occurring after some components update but before atomic write completes (though database transactions should prevent this)
4. **Manual Database Operations**: Administrative database modifications that inadvertently create progress inconsistencies

**Mitigating Factors**:

During normal operation, all sub-pruners execute in parallel with error propagation: [9](#0-8) 

The `try_for_each` ensures that if one sub-pruner fails, all fail together, keeping them synchronized during normal operation.

**Why Medium Likelihood**:
- Normal operation maintains synchronization
- Requires specific failure conditions to create the gap
- But these conditions (corruption, bugs, operational issues) are realistic in long-running validator nodes
- Once triggered, the impact is severe and persistent

## Recommendation

Enforce batch size limiting during initialization catch-up to prevent unbounded memory allocation:

```rust
impl TransactionPruner {
    pub(in crate::pruner) fn new(
        transaction_store: Arc<TransactionStore>,
        ledger_db: Arc<LedgerDb>,
        metadata_progress: Version,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.transaction_db_raw(),
            &DbMetadataKey::TransactionPrunerProgress,
            metadata_progress,
        )?;

        let myself = TransactionPruner {
            transaction_store,
            ledger_db,
            internal_indexer_db,
        };

        // Catch up with batch size limiting
        const CATCH_UP_BATCH_SIZE: u64 = 5_000;
        let mut current_progress = progress;
        
        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up TransactionPruner with batch size limiting."
        );
        
        while current_progress < metadata_progress {
            let batch_target = std::cmp::min(
                current_progress + CATCH_UP_BATCH_SIZE,
                metadata_progress
            );
            myself.prune(current_progress, batch_target)?;
            current_progress = batch_target;
        }

        Ok(myself)
    }
}
```

Apply the same fix to all sub-pruners that perform catch-up during initialization (EventStorePruner, TransactionInfoPruner, TransactionAccumulatorPruner, etc.).

## Proof of Concept

While a full PoC would require setting up a validator node with artificially corrupted pruner progress metadata, the vulnerability can be demonstrated through the code flow:

```rust
// Simulated scenario showing the vulnerability
// Assume: stored TransactionPruner progress = 0
// Assume: metadata_progress = 90_000_000 (default prune_window)

// In TransactionPruner::new():
let progress = 0; // Retrieved from corrupted/stale metadata
let metadata_progress = 90_000_000;

// Line 101 calls without batch limiting:
myself.prune(0, 90_000_000)?;

// In prune():
let candidate_transactions = 
    self.get_pruning_candidate_transactions(0, 90_000_000)?;
// This allocates: Vec::with_capacity(90_000_000)
// Then loads 90 million Transaction objects into memory
// Result: OOM crash

// Node crashes during initialization
// On restart: Same code path → Same crash (unrecoverable loop)
```

The vulnerability is deterministic given the preconditions and can be verified by code inspection without requiring a live attack demonstration.

## Notes

This vulnerability represents a systemic issue in the pruner initialization pattern used across multiple sub-pruners. While TransactionPruner is particularly vulnerable due to the large size of Transaction objects, other pruners using the same pattern should also be reviewed for potential resource exhaustion issues during catch-up operations.

The fix should be applied consistently across all pruner implementations to prevent similar issues and establish a safe initialization pattern for the entire ledger pruner subsystem.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L39-40)
```rust
        let candidate_transactions =
            self.get_pruning_candidate_transactions(current_progress, target_version)?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L101-101)
```rust
        myself.prune(progress, metadata_progress)?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L119-120)
```rust
        // The capacity is capped by the max number of txns we prune in a single batch. It's a
        // relatively small number set in the config, so it won't cause high memory usage here.
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L121-128)
```rust
        let mut txns = Vec::with_capacity((end - start) as usize);
        for item in iter {
            let (version, txn) = item?;
            if version >= end {
                break;
            }
            txns.push((version, txn));
        }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L67-68)
```rust
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L78-84)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/pruner_utils.rs (L44-59)
```rust
pub(crate) fn get_or_initialize_subpruner_progress(
    sub_db: &DB,
    progress_key: &DbMetadataKey,
    metadata_progress: Version,
) -> Result<Version> {
    Ok(
        if let Some(v) = sub_db.get::<DbMetadataSchema>(progress_key)? {
            v.expect_version()
        } else {
            sub_db.put::<DbMetadataSchema>(
                progress_key,
                &DbMetadataValue::Version(metadata_progress),
            )?;
            metadata_progress
        },
    )
```

**File:** config/src/config/storage_config.rs (L391-391)
```rust
            prune_window: 90_000_000,
```

**File:** types/src/transaction/mod.rs (L2943-2977)
```rust
#[allow(clippy::large_enum_variant)]
#[cfg_attr(any(test, feature = "fuzzing"), derive(Arbitrary))]
#[derive(Clone, Debug, Eq, PartialEq, Serialize, Deserialize, CryptoHasher, BCSCryptoHash)]
pub enum Transaction {
    /// Transaction submitted by the user. e.g: P2P payment transaction, publishing module
    /// transaction, etc.
    /// TODO: We need to rename SignedTransaction to SignedUserTransaction, as well as all the other
    ///       transaction types we had in our codebase.
    UserTransaction(SignedTransaction),

    /// Transaction that applies a WriteSet to the current storage, it's applied manually via aptos-db-bootstrapper.
    GenesisTransaction(WriteSetPayload),

    /// Transaction to update the block metadata resource at the beginning of a block,
    /// when on-chain randomness is disabled.
    BlockMetadata(BlockMetadata),

    /// Transaction to let the executor update the global state tree and record the root hash
    /// in the TransactionInfo
    /// The hash value inside is unique block id which can generate unique hash of state checkpoint transaction
    StateCheckpoint(HashValue),

    /// Transaction that only proposed by a validator mainly to update on-chain configs.
    ValidatorTransaction(ValidatorTransaction),

    /// Transaction to update the block metadata resource at the beginning of a block,
    /// when on-chain randomness is enabled.
    BlockMetadataExt(BlockMetadataExt),

    /// Transaction to let the executor update the global state tree and record the root hash
    /// in the TransactionInfo
    /// The hash value inside is unique block id which can generate unique hash of state checkpoint transaction
    /// Replaces StateCheckpoint, with optionally having more data.
    BlockEpilogue(BlockEpiloguePayload),
}
```
