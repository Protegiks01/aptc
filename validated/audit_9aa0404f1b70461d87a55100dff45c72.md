# Audit Report

## Title
Race Condition in PartialStateComputeResult OnceCell Initialization Causes Validator Node Panic on Duplicate Block Insertion

## Summary
Multiple validator threads can concurrently execute ledger update for the same block due to inadequate synchronization in the consensus pipeline's duplicate block handling, causing a panic when racing to initialize OnceCell fields in `PartialStateComputeResult`. This results in validator node crashes and network availability degradation.

## Finding Description

The vulnerability exists in a race condition chain spanning the consensus and execution layers:

**1. Duplicate Block Insertion is Expected Behavior**

The consensus layer explicitly allows duplicate block inserts as documented in code comments. [1](#0-0) 

The RoundManager acknowledges that delayed proposal processing tries to add the same block again, which is considered okay because `insert_block` is idempotent. [2](#0-1) 

**2. Pipeline Construction Precedes Duplicate Detection**

In `insert_block`, there's an unsynchronized check for existing blocks. [3](#0-2)  Two threads can both pass this check before either completes insertion, allowing both to proceed to pipeline building.

The pipeline is built in `insert_block_inner` BEFORE the actual duplicate check in the block tree. [4](#0-3) 

The actual duplicate detection only occurs at BlockTree insertion. [5](#0-4) 

The BlockTree's `insert_block` method performs duplicate detection by checking for existing blocks. [6](#0-5) 

**3. Pipeline Futures Spawn Independent Non-Abortable Tasks**

The `spawn_shared_fut` function immediately spawns independent async tasks using `tokio::spawn`. [7](#0-6) 

Critically, `ledger_update_fut` is spawned with `None` for abort_handles, making it non-abortable. [8](#0-7)  Even when the PipelinedBlock is dropped and `abort_pipeline()` is called [9](#0-8) , these futures continue running independently.

**4. Concurrent Ledger Update Execution**

The `BlockExecutor::ledger_update` method takes `&self` (not `&mut self`), permitting concurrent execution. [10](#0-9) 

When both threads execute, they fetch blocks from the executor's BlockTree. [11](#0-10) 

The `fetch_or_add_block` method returns the existing `Arc<Block>` when a duplicate is detected, ensuring both threads operate on the SAME Block instance. [12](#0-11) 

**5. TOCTOU Vulnerability in Completion Check**

The check for existing results is not atomic with subsequent OnceCell initialization. [13](#0-12) 

Both threads can pass this check before either completes initialization, then both proceed to set OnceCell fields. [14](#0-13) 

**6. OnceCell Panic on Duplicate Initialization**

The OnceCell fields panic when set twice. [15](#0-14) [16](#0-15) 

When the second thread attempts to set already-initialized cells, the `.expect()` calls panic with "StateCheckpointOutput already set" or "LedgerUpdateOutput already set".

**7. Global Panic Handler Crashes Validator**

Aptos uses a global panic handler that catches all panics (including those in tokio tasks) and exits the process. [17](#0-16) [18](#0-17) 

**Attack Scenario:**

1. Attacker sends duplicate blocks/certificates to validator (normal network behavior)
2. Two threads concurrently call `insert_block` for same block_id
3. Both pass unsynchronized check at line 413
4. Both build pipelines, spawning non-abortable `ledger_update_fut`
5. One PipelinedBlock succeeds in BlockTree insertion, other is discarded
6. Both ledger_update futures continue running independently
7. Both fetch same `Arc<Block>` from executor's BlockTree
8. Both check `get_complete_result()` and see None (TOCTOU race)
9. First thread sets OnceCell fields successfully
10. Second thread panics when attempting to set already-initialized cells
11. Global panic handler catches panic and calls `process::exit(12)`
12. Validator node crashes

## Impact Explanation

**High Severity** - This vulnerability causes validator node crashes, meeting the "API crashes" criteria from the Aptos bug bounty High Severity category.

**Availability Impact:**
- Validator node crashes immediately upon panic
- Node must be restarted to resume operation
- Reduces network validator capacity during active consensus
- Multiple validators affected simultaneously degrades network liveness

**Why Not Critical:**
- No consensus safety violation (both threads compute identical deterministic results)
- No funds at risk
- No permanent network partition (nodes can restart)
- No state corruption (computed values are correct, just causes crash)

**Affected Invariant:**
Violates network availability guarantee by causing validator node crashes through race condition in normal protocol operation.

## Likelihood Explanation

**Medium-High Likelihood:**

This vulnerability occurs during normal network operation:
- Validators broadcast certificates for the same block
- Network delays cause duplicate block messages  
- Delayed proposal processing (explicitly supported) retries block insertion

The vulnerability requires:
- **No attacker privileges** - Any network peer can send blocks/certificates
- **Normal protocol behavior** - Duplicate block delivery is explicitly expected
- **Timing overlap** - Two inserts must overlap in critical window between check and insertion

The likelihood increases with:
- Network congestion or delays
- High transaction throughput
- Multiple validators proposing blocks
- Byzantine actors deliberately sending duplicates to exploit race window

Code comments explicitly acknowledging duplicate inserts as normal indicates this is a realistic scenario developers anticipated but incompletely protected against.

## Recommendation

**Immediate Fix**: Add synchronization to prevent concurrent ledger_update execution on the same block:

1. **Option A**: Check-and-add atomic operation in `insert_block`:
```rust
// Use a Mutex or RwLock to make check-and-insert atomic
let mut blocks = self.blocks.write();
if let Some(existing) = blocks.get(&block.id()) {
    return Ok(existing.clone());
}
// Build pipeline only after confirming block doesn't exist
```

2. **Option B**: Add early return in `ledger_update` using atomic check-and-set:
```rust
// Before computing, atomically check and mark as in-progress
if let Some(result) = output.get_complete_result() {
    return Ok(result);
}
// Use a separate OnceCell or atomic flag to mark computation in-progress
```

3. **Option C**: Register abort handles for execute_fut and ledger_update_fut:
```rust
// In pipeline_builder.rs, pass Some(&mut abort_handles) instead of None
let ledger_update_fut = spawn_shared_fut(
    Self::ledger_update(...),
    Some(&mut abort_handles), // Enable abortion
);
```

**Long-term Fix**: Redesign pipeline to ensure only one pipeline per block_id can execute concurrently, or use `try_insert` pattern with atomic operations.

## Proof of Concept

This race condition can be demonstrated by:

1. Setting up two threads that simultaneously call `BlockStore::insert_block()` with the same block
2. Using timing controls to ensure both pass the check at line 413 before either inserts
3. Observing both pipelines spawn and execute
4. Monitoring for the panic in `set_state_checkpoint_output` or `set_ledger_update_output`
5. Confirming the global panic handler exits the process

The vulnerability is triggerable in production by sending duplicate block proposals/certificates to validators during high-activity periods, causing intermittent crashes.

## Notes

This is a concurrency bug in production code affecting validator availability. The race condition exists because:
1. Duplicate block handling is expected behavior
2. Pipeline spawning precedes duplicate detection
3. Spawned futures are not abortable
4. No synchronization prevents concurrent OnceCell initialization
5. Global panic handler crashes the process on panic

The fix requires either preventing duplicate pipelines from being spawned, or ensuring ledger_update operations are properly synchronized.

### Citations

**File:** consensus/src/block_storage/block_store.rs (L413-414)
```rust
        if let Some(existing_block) = self.get_block(block.id()) {
            return Ok(existing_block);
```

**File:** consensus/src/block_storage/block_store.rs (L445-447)
```rust
    /// Duplicate inserts will return the previously inserted block (
    /// note that it is considered a valid non-error case, for example, it can happen if a validator
    /// receives a certificate for a block that is currently being added).
```

**File:** consensus/src/block_storage/block_store.rs (L464-496)
```rust
        if let Some(pipeline_builder) = &self.pipeline_builder {
            let parent_block = self
                .get_block(pipelined_block.parent_id())
                .ok_or_else(|| anyhow::anyhow!("Parent block not found"))?;

            // need weak pointer to break the cycle between block tree -> pipeline block -> callback
            let block_tree = Arc::downgrade(&self.inner);
            let storage = self.storage.clone();
            let id = pipelined_block.id();
            let round = pipelined_block.round();
            let window_size = self.window_size;
            let callback = Box::new(
                move |finality_proof: WrappedLedgerInfo,
                      commit_decision: LedgerInfoWithSignatures| {
                    if let Some(tree) = block_tree.upgrade() {
                        tree.write().commit_callback(
                            storage,
                            id,
                            round,
                            finality_proof,
                            commit_decision,
                            window_size,
                        );
                    }
                },
            );
            pipeline_builder.build_for_consensus(
                &pipelined_block,
                parent_block.pipeline_futs().ok_or_else(|| {
                    anyhow::anyhow!("Parent future doesn't exist, potentially epoch ended")
                })?,
                callback,
            );
```

**File:** consensus/src/block_storage/block_store.rs (L515-515)
```rust
        self.inner.write().insert_block(pipelined_block)
```

**File:** consensus/src/round_manager.rs (L1248-1255)
```rust
        // Since processing proposal is delayed due to backpressure or payload availability, we add
        // the block to the block store so that we don't need to fetch it from remote once we
        // are out of the backpressure. Please note that delayed processing of proposal is not
        // guaranteed to add the block to the block store if we don't get out of the backpressure
        // before the timeout, so this is needed to ensure that the proposed block is added to
        // the block store irrespective. Also, it is possible that delayed processing of proposal
        // tries to add the same block again, which is okay as `insert_block` call
        // is idempotent.
```

**File:** consensus/src/block_storage/block_tree.rs (L312-317)
```rust
        if let Some(existing_block) = self.get_block(&block_id) {
            debug!("Already had block {:?} for id {:?} when trying to add another block {:?} for the same id",
                       existing_block,
                       block_id,
                       block);
            Ok(existing_block)
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L151-151)
```rust
    let join_handle = tokio::spawn(f);
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L502-511)
```rust
        let ledger_update_fut = spawn_shared_fut(
            Self::ledger_update(
                rand_check_fut.clone(),
                execute_fut.clone(),
                parent.ledger_update_fut.clone(),
                self.executor.clone(),
                block.clone(),
            ),
            None,
        );
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L361-364)
```rust
impl Drop for PipelinedBlock {
    fn drop(&mut self) {
        let _ = self.abort_pipeline();
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L115-119)
```rust
    fn ledger_update(
        &self,
        block_id: HashValue,
        parent_block_id: HashValue,
    ) -> ExecutorResult<StateComputeResult> {
```

**File:** execution/executor/src/block_executor/mod.rs (L271-285)
```rust
        let mut block_vec = self
            .block_tree
            .get_blocks_opt(&[block_id, parent_block_id])?;
        let parent_block = block_vec
            .pop()
            .expect("Must exist.")
            .ok_or(ExecutorError::BlockNotFound(parent_block_id))?;
        // At this point of time two things must happen
        // 1. The block tree must also have the current block id with or without the ledger update output.
        // 2. We must have the ledger update output of the parent block.
        // Above is not ture if the block is on a forked branch.
        let block = block_vec
            .pop()
            .expect("Must exist")
            .ok_or(ExecutorError::BlockNotFound(parent_block_id))?;
```

**File:** execution/executor/src/block_executor/mod.rs (L290-294)
```rust
        // TODO(aldenhu): remove, assuming no retries.
        if let Some(complete_result) = block.output.get_complete_result() {
            info!(block_id = block_id, "ledger_update already done.");
            return Ok(complete_result);
        }
```

**File:** execution/executor/src/block_executor/mod.rs (L315-328)
```rust
                output.set_state_checkpoint_output(DoStateCheckpoint::run(
                    &output.execution_output,
                    parent_block.output.ensure_result_state_summary()?,
                    &ProvableStateSummary::new_persisted(self.db.reader.as_ref())?,
                    None,
                )?);
                output.set_ledger_update_output(DoLedgerUpdate::run(
                    &output.execution_output,
                    output.ensure_state_checkpoint_output()?,
                    parent_out
                        .ensure_ledger_update_output()?
                        .transaction_accumulator
                        .clone(),
                )?);
```

**File:** execution/executor/src/block_executor/block_tree/mod.rs (L110-115)
```rust
            Entry::Occupied(entry) => {
                let existing = entry
                    .get()
                    .upgrade()
                    .ok_or_else(|| anyhow!("block dropped unexpected."))?;
                Ok((existing, true, parent_block))
```

**File:** execution/executor/src/types/partial_state_compute_result.rs (L76-80)
```rust
    pub fn set_state_checkpoint_output(&self, state_checkpoint_output: StateCheckpointOutput) {
        self.state_checkpoint_output
            .set(state_checkpoint_output)
            .expect("StateCheckpointOutput already set");
    }
```

**File:** execution/executor/src/types/partial_state_compute_result.rs (L88-92)
```rust
    pub fn set_ledger_update_output(&self, ledger_update_output: LedgerUpdateOutput) {
        self.ledger_update_output
            .set(ledger_update_output)
            .expect("LedgerUpdateOutput already set");
    }
```

**File:** crates/crash-handler/src/lib.rs (L21-30)
```rust
/// Invoke to ensure process exits on a thread panic.
///
/// Tokio's default behavior is to catch panics and ignore them.  Invoking this function will
/// ensure that all subsequent thread panics (even Tokio threads) will report the
/// details/backtrace and then exit.
pub fn setup_panic_handler() {
    panic::set_hook(Box::new(move |pi: &PanicHookInfo<'_>| {
        handle_panic(pi);
    }));
}
```

**File:** crates/crash-handler/src/lib.rs (L56-57)
```rust
    // Kill the process
    process::exit(12);
```
