# Audit Report

## Title
Remote Executor Indefinite Blocking Causes Consensus Round Timeouts and Thread Pool Exhaustion

## Summary
The `RemoteExecutorClient` uses blocking channel receives without timeout when waiting for execution results from remote shards. When remote shards are slow, crashed, or experience network issues, block execution hangs indefinitely while consensus continues to timeout rounds. This leads to unnecessary round changes, accumulation of blocked execution tasks, and eventual thread pool exhaustion causing validator slowdowns.

## Finding Description

The vulnerability exists in the remote executor architecture where block execution is distributed across multiple remote shards for horizontal scaling performance. The critical flaw is in the result collection mechanism. [1](#0-0) 

The `get_output_from_shards()` method performs blocking `rx.recv().unwrap()` calls without any timeout mechanism. These channels are unbounded crossbeam channels created by the NetworkController for receiving execution results from remote shards. [2](#0-1) 

Although the NetworkController is initialized with a `timeout_ms` parameter (5000ms), this timeout only applies to the GRPC layer for network communication, not to the channel receive operations. [3](#0-2) 

Meanwhile, consensus operates with round timeouts managed by `RoundState`. The consensus timeout mechanism schedules timeouts and triggers `process_local_timeout()` when rounds take too long. [4](#0-3) 

The execution pipeline uses `tokio::task::spawn_blocking` to execute blocks on a dedicated blocking thread pool. [5](#0-4) 

When block execution via remote shards takes longer than the consensus timeout:

1. **Execution blocks indefinitely**: The `rx.recv()` call waits forever for a result from a slow/crashed remote shard
2. **Consensus continues**: The round timeout fires, and consensus moves to the next round
3. **No pipeline abort**: The `process_local_timeout()` method does not abort pending execution pipelines
4. **Resources accumulate**: Each blocked execution task consumes a thread from the limited thread pool [6](#0-5) 

Critically, aborting tokio tasks that wrap `spawn_blocking` cannot interrupt the blocking operation itself - the threads remain stuck on `rx.recv()` until the channel receives data or is closed. [7](#0-6) 

This creates a resource leak where multiple rounds can timeout while execution tasks accumulate in the blocked state, eventually exhausting the thread pool and degrading validator performance.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos Bug Bounty program, specifically under the "Validator Node Slowdowns" criterion.

**High Severity Impact - Validator Node Slowdowns**:
As execution threads accumulate in blocked state, the validator's blocking thread pool becomes exhausted. This causes:
- Severe performance degradation across all validator operations that depend on the blocking thread pool
- Increased latency in block processing and consensus participation
- Potential inability to execute blocks effectively, causing the validator to fall behind
- Degraded consensus participation that may affect validator rewards

**Operational Impact**:
If execution tasks remain permanently blocked, operators must manually restart validators to recover the thread pool, causing:
- Temporary loss of validator participation in consensus
- Need for manual intervention at scale if multiple validators are affected
- Potential missed rewards during downtime and recovery

While this does not cause immediate consensus safety violations or fund loss, it significantly degrades validator liveness and operational reliability.

## Likelihood Explanation

**Likelihood: MEDIUM** in production environments using remote execution.

This vulnerability has medium likelihood because:

1. **Optional Feature**: Remote execution is an optional feature for horizontal scaling, not enabled by default. Only validators who explicitly configure `remote_executor_addresses` are affected. [8](#0-7) [9](#0-8) 

2. **Natural Trigger Conditions**: For validators using remote execution, the issue can be triggered naturally through:
   - Network latency spikes or packet loss (common in distributed systems)
   - Remote shard process crashes or restarts
   - Resource contention on remote executor machines
   - Any network partition affecting shard communication

3. **No Protection**: There is no timeout, retry logic, or circuit breaker to protect against slow/unresponsive shards.

4. **Amplification**: A single slow shard blocks the entire block execution (results are collected serially), and multiple concurrent blocks can pile up.

5. **Production Deployment**: The feature has production-level implementation with standalone service binaries and CLI configuration. [10](#0-9) 

## Recommendation

Add timeout protection to the channel receive operation in `get_output_from_shards()`:

```rust
fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
    trace!("RemoteExecutorClient Waiting for results");
    let mut results = vec![];
    let timeout = Duration::from_millis(5000); // Match NetworkController timeout
    
    for rx in self.result_rxs.iter() {
        let received_bytes = rx.recv_timeout(timeout)
            .map_err(|e| VMStatus::Error {
                status_code: StatusCode::UNKNOWN_INVARIANT_VIOLATION_ERROR,
                sub_status: None,
                message: Some(format!("Remote execution timeout: {}", e)),
            })?
            .to_bytes();
        let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes)
            .map_err(|e| VMStatus::Error {
                status_code: StatusCode::UNKNOWN_INVARIANT_VIOLATION_ERROR,
                sub_status: None,
                message: Some(format!("Failed to deserialize result: {}", e)),
            })?;
        results.push(result.inner?);
    }
    Ok(results)
}
```

Additionally, implement retry logic and circuit breaker patterns for remote executor communication to handle transient failures gracefully.

## Proof of Concept

A complete PoC would require:
1. Setting up a validator with remote executor configuration
2. Configuring one remote executor shard to delay responses beyond consensus timeout
3. Observing thread pool exhaustion through metrics as multiple rounds timeout

The vulnerability can be triggered by:
- Introducing artificial network delays between coordinator and remote shard
- Crashing a remote shard process during block execution
- Causing resource exhaustion on remote shard machines

## Notes

- This vulnerability only affects validators who opt to use remote execution for horizontal scaling
- The remote executor shards are part of the validator operator's own infrastructure
- The issue can be triggered naturally through network/system failures without malicious actors
- The severity is HIGH (not Medium as initially assessed) because it matches the "Validator Node Slowdowns" criterion
- The likelihood is MEDIUM (not High) because the feature is optional and not all validators use it

### Citations

**File:** execution/executor-service/src/remote_executor_client.rs (L107-119)
```rust
        let (command_txs, result_rxs) = remote_shard_addresses
            .iter()
            .enumerate()
            .map(|(shard_id, address)| {
                let execute_command_type = format!("execute_command_{}", shard_id);
                let execute_result_type = format!("execute_result_{}", shard_id);
                let command_tx = Mutex::new(
                    controller_mut_ref.create_outbound_channel(*address, execute_command_type),
                );
                let result_rx = controller_mut_ref.create_inbound_channel(execute_result_type);
                (command_tx, result_rx)
            })
            .unzip();
```

**File:** execution/executor-service/src/remote_executor_client.rs (L147-161)
```rust
    pub fn create_remote_sharded_block_executor(
        coordinator_address: SocketAddr,
        remote_shard_addresses: Vec<SocketAddr>,
        num_threads: Option<usize>,
    ) -> ShardedBlockExecutor<S, RemoteExecutorClient<S>> {
        ShardedBlockExecutor::new(RemoteExecutorClient::new(
            remote_shard_addresses,
            NetworkController::new(
                "remote-executor-coordinator".to_string(),
                coordinator_address,
                5000,
            ),
            num_threads,
        ))
    }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L163-172)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        trace!("RemoteExecutorClient Waiting for results");
        let mut results = vec![];
        for rx in self.result_rxs.iter() {
            let received_bytes = rx.recv().unwrap().to_bytes();
            let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
            results.push(result.inner?);
        }
        Ok(results)
    }
```

**File:** consensus/src/liveness/round_state.rs (L338-354)
```rust
    /// Setup the timeout task and return the duration of the current timeout
    fn setup_timeout(&mut self, multiplier: u32) -> Duration {
        let timeout_sender = self.timeout_sender.clone();
        let timeout = self.setup_deadline(multiplier);
        trace!(
            "Scheduling timeout of {} ms for round {}",
            timeout.as_millis(),
            self.current_round
        );
        let abort_handle = self
            .time_service
            .run_after(timeout, SendTask::make(timeout_sender, self.current_round));
        if let Some(handle) = self.abort_handle.replace(abort_handle) {
            handle.abort();
        }
        timeout
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L857-868)
```rust
        tokio::task::spawn_blocking(move || {
            executor
                .execute_and_update_state(
                    (block.id(), txns, auxiliary_info).into(),
                    block.parent_id(),
                    onchain_execution_config,
                )
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
        Ok(start.elapsed())
```

**File:** consensus/src/round_manager.rs (L993-1090)
```rust
    pub async fn process_local_timeout(&mut self, round: Round) -> anyhow::Result<()> {
        if !self.round_state.process_local_timeout(round) {
            return Ok(());
        }

        if self.sync_only() {
            self.network
                .broadcast_sync_info(self.block_store.sync_info())
                .await;
            bail!("[RoundManager] sync_only flag is set, broadcasting SyncInfo");
        }

        if self.local_config.enable_round_timeout_msg {
            let timeout = if let Some(timeout) = self.round_state.timeout_sent() {
                timeout
            } else {
                let timeout = TwoChainTimeout::new(
                    self.epoch_state.epoch,
                    round,
                    self.block_store.highest_quorum_cert().as_ref().clone(),
                );
                let signature = self
                    .safety_rules
                    .lock()
                    .sign_timeout_with_qc(
                        &timeout,
                        self.block_store.highest_2chain_timeout_cert().as_deref(),
                    )
                    .context("[RoundManager] SafetyRules signs 2-chain timeout")?;

                let timeout_reason = self.compute_timeout_reason(round);

                RoundTimeout::new(
                    timeout,
                    self.proposal_generator.author(),
                    timeout_reason,
                    signature,
                )
            };

            self.round_state.record_round_timeout(timeout.clone());
            let round_timeout_msg = RoundTimeoutMsg::new(timeout, self.block_store.sync_info());
            self.network
                .broadcast_round_timeout(round_timeout_msg)
                .await;
            warn!(
                round = round,
                remote_peer = self.proposer_election.get_valid_proposer(round),
                event = LogEvent::Timeout,
            );
            bail!("Round {} timeout, broadcast to all peers", round);
        } else {
            let (is_nil_vote, mut timeout_vote) = match self.round_state.vote_sent() {
                Some(vote) if vote.vote_data().proposed().round() == round => {
                    (vote.vote_data().is_for_nil(), vote)
                },
                _ => {
                    // Didn't vote in this round yet, generate a backup vote
                    let nil_block = self
                        .proposal_generator
                        .generate_nil_block(round, self.proposer_election.clone())?;
                    info!(
                        self.new_log(LogEvent::VoteNIL),
                        "Planning to vote for a NIL block {}", nil_block
                    );
                    counters::VOTE_NIL_COUNT.inc();
                    let nil_vote = self.vote_block(nil_block).await?;
                    (true, nil_vote)
                },
            };

            if !timeout_vote.is_timeout() {
                let timeout = timeout_vote.generate_2chain_timeout(
                    self.block_store.highest_quorum_cert().as_ref().clone(),
                );
                let signature = self
                    .safety_rules
                    .lock()
                    .sign_timeout_with_qc(
                        &timeout,
                        self.block_store.highest_2chain_timeout_cert().as_deref(),
                    )
                    .context("[RoundManager] SafetyRules signs 2-chain timeout")?;
                timeout_vote.add_2chain_timeout(timeout, signature);
            }

            self.round_state.record_vote(timeout_vote.clone());
            let timeout_vote_msg = VoteMsg::new(timeout_vote, self.block_store.sync_info());
            self.network.broadcast_timeout_vote(timeout_vote_msg).await;
            warn!(
                round = round,
                remote_peer = self.proposer_election.get_valid_proposer(round),
                voted_nil = is_nil_vote,
                event = LogEvent::Timeout,
            );
            bail!("Round {} timeout, broadcast to all peers", round);
        }
    }
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L528-547)
```rust
    pub fn abort_pipeline(&self) -> Option<PipelineFutures> {
        if let Some(abort_handles) = self.pipeline_abort_handle.lock().take() {
            let mut aborted = false;
            for handle in abort_handles {
                if !handle.is_finished() {
                    handle.abort();
                    aborted = true;
                }
            }
            if aborted {
                info!(
                    "[Pipeline] Aborting pipeline for block {} {} {}",
                    self.id(),
                    self.epoch(),
                    self.round()
                );
            }
        }
        self.pipeline_futs.lock().take()
    }
```

**File:** execution/executor-service/src/main.rs (L20-21)
```rust
    #[clap(long, num_args = 1..)]
    pub remote_executor_addresses: Vec<SocketAddr>,
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L261-267)
```rust
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
```

**File:** execution/executor-benchmark/src/main.rs (L632-638)
```rust
        remote_executor_client::set_remote_addresses(
            opt.pipeline_opt
                .sharding_opt
                .remote_executor_addresses
                .clone()
                .unwrap(),
        );
```
