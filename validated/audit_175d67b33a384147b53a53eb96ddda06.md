# Audit Report

## Title
DAG Consensus Storage Exhaustion via Unbounded Parent Certificate Duplication

## Summary
A malicious validator can craft DAG consensus nodes with arbitrarily many duplicate parent certificates, bypassing the intended 20MB size limit and causing storage exhaustion. The validation logic fails to check the size of the `parents` field, and the voting power verification does not deduplicate authors, allowing nodes serialized to 35+ MB to be stored and processed.

## Finding Description

The DAG consensus implementation contains two critical validation gaps that enable resource exhaustion attacks:

**Gap 1: Incomplete Node Size Validation**

The node validation logic enforces a 20MB limit on transaction payloads but completely ignores the `parents` field size: [1](#0-0) 

This validation only accounts for `txn_bytes` (validator transactions + payload), allowing the unbounded `parents` field to bypass size limits entirely.

**Gap 2: No Duplicate Parent Certificate Prevention**

The Node structure contains an unbounded vector of parent certificates with no deduplication: [2](#0-1) 

The verification logic checks voting power without preventing duplicates: [3](#0-2) 

Critically, the `sum_voting_power` implementation does NOT deduplicate authors - it counts each author's voting power even if they appear multiple times: [4](#0-3) 

**Exploitation Path:**

1. A malicious validator creates a Node with 50,000 duplicate copies of a single valid parent certificate from the previous round
2. Each NodeCertificate is approximately 200-300 bytes (NodeMetadata + AggregateSignature with BitVec and BLS signature)
3. The duplicated parents pass the voting power check because `sum_voting_power` counts the same validator 50,000 times, trivially exceeding the 2f+1 requirement
4. The oversized node passes validation since only `txn_bytes` is checked, not the total serialized size including parents
5. The node is BCS-serialized with ALL fields and persisted to storage: [5](#0-4) [6](#0-5) 

With 50,000 duplicate certificates at ~300 bytes each (15MB) plus a 20MB payload, a single node reaches 35+ MB. Across 100 malicious nodes in multiple rounds, this causes 3.5+ GB of unnecessary database bloat.

## Impact Explanation

This qualifies as **High Severity** under the Aptos bug bounty program's "Validator Node Slowdowns" category for the following reasons:

1. **Resource Limit Bypass**: The vulnerability bypasses the intended 20MB per-node limit, allowing 35+ MB nodes to be created and stored, violating the principle that "all operations must respect storage and computational limits"

2. **Storage Exhaustion**: Persistent storage bloat of 3.5+ GB with just 100 malicious nodes across rounds, degrading validator database performance

3. **CPU Overhead**: Repeated BCS deserialization of 35+ MB nodes during consensus processing causes significant computational overhead

4. **Memory Pressure**: Loading oversized nodes into memory for processing can cause memory pressure on validator nodes

5. **DoS Through Resource Exhaustion**: The attack constitutes a protocol-level DoS through resource exhaustion, explicitly categorized as HIGH severity in the bounty framework

The attack does not require validator collusion (single malicious validator is sufficient) and directly impacts validator node performance and resource utilization.

## Likelihood Explanation

**Likelihood: High**

The attack is trivial to execute for any malicious validator:

- **No Special Privileges**: Only requires being a validator (an untrusted role in the threat model)
- **No Cryptographic Work**: Reuses valid parent certificates from the previous round
- **No Coordination**: Single validator can execute without coordination
- **Zero Prevention**: The validation logic has no checks for duplicate parent certificates or total node size including parents
- **Repeatable**: Can be executed every round to maximize impact
- **Passes All Validation**: The malicious node passes both voting power checks (due to duplicate counting) and size checks (due to incomplete validation)

## Recommendation

Implement two critical fixes:

**Fix 1: Deduplicate Authors in Voting Power Validation**

Modify `sum_voting_power` to use a HashSet to ensure each author is counted only once:

```rust
pub fn sum_voting_power<'a>(
    &self,
    authors: impl Iterator<Item = &'a AccountAddress>,
) -> std::result::Result<u128, VerifyError> {
    let mut aggregated_voting_power = 0;
    let mut seen = HashSet::new();
    for account_address in authors {
        if !seen.insert(account_address) {
            continue; // Skip duplicates
        }
        match self.get_voting_power(account_address) {
            Some(voting_power) => aggregated_voting_power += voting_power as u128,
            None => return Err(VerifyError::UnknownAuthor),
        }
    }
    Ok(aggregated_voting_power)
}
```

**Fix 2: Validate Total Node Size Including Parents**

Add total node size validation in `NodeBroadcastHandler::validate()`:

```rust
// After existing txn_bytes validation
let total_node_size = txn_bytes + 
    node.parents().iter()
        .map(|p| bcs::to_bytes(p).unwrap_or_default().len() as u64)
        .sum::<u64>();
ensure!(
    total_node_size <= self.payload_config.max_receiving_size_per_round_bytes,
    "total node size exceeds limit"
);
```

Additionally, consider enforcing a maximum number of parent certificates (e.g., 2x validator set size) to prevent excessive duplicate prevention overhead.

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    
    #[test]
    fn test_duplicate_parent_certificates_bypass_validation() {
        // Setup: Create a validator set and DAG state
        let validators = create_test_validators(100);
        let verifier = ValidatorVerifier::new(validators);
        
        // Create a valid parent certificate from round 1
        let parent_cert = create_valid_parent_certificate(1, &validators[0]);
        
        // Create a node with 10,000 duplicate copies of the same parent
        let mut duplicate_parents = Vec::new();
        for _ in 0..10_000 {
            duplicate_parents.push(parent_cert.clone());
        }
        
        let malicious_node = Node::new(
            1, // epoch
            2, // round
            validators[1].address,
            current_timestamp(),
            vec![], // validator_txns
            create_20mb_payload(), // Max allowed payload
            duplicate_parents, // 10,000 duplicate parents
            Extensions::empty(),
        );
        
        // Verify: The node passes validation despite being 30+ MB
        assert!(malicious_node.verify(validators[1].address, &verifier).is_ok());
        
        // Verify: The voting power check passes (counting duplicates)
        let voting_power = verifier.sum_voting_power(
            malicious_node.parents().iter().map(|p| p.metadata().author())
        );
        assert!(voting_power.is_ok()); // Should fail but doesn't due to duplicate counting
        
        // Verify: BCS serialization size exceeds 30 MB
        let serialized = bcs::to_bytes(&malicious_node).unwrap();
        assert!(serialized.len() > 30_000_000);
    }
}
```

## Notes

This vulnerability represents a fundamental protocol flaw where two independent validation gaps combine to enable resource exhaustion:

1. The voting power check's lack of deduplication makes it trivial to satisfy the 2f+1 requirement with a single parent duplicated thousands of times
2. The size validation's failure to account for the `parents` field allows bypassing the intended 20MB limit

The distinction from "Network DoS" (out of scope) is critical: this is not network flooding but rather exploitation of specific protocol validation bugs that cause resource exhaustion. The bounty framework explicitly includes "DoS through resource exhaustion" affecting validator performance as HIGH severity, which this vulnerability clearly demonstrates.

### Citations

**File:** consensus/src/dag/rb_handler.rs (L139-142)
```rust
        let num_txns = num_vtxns + node.payload().len() as u64;
        let txn_bytes = vtxn_total_bytes + node.payload().size() as u64;
        ensure!(num_txns <= self.payload_config.max_receiving_txns_per_round);
        ensure!(txn_bytes <= self.payload_config.max_receiving_size_per_round_bytes);
```

**File:** consensus/src/dag/types.rs (L152-159)
```rust
#[derive(Clone, Serialize, Deserialize, CryptoHasher, Debug, PartialEq)]
pub struct Node {
    metadata: NodeMetadata,
    validator_txns: Vec<ValidatorTransaction>,
    payload: Payload,
    parents: Vec<NodeCertificate>,
    extensions: Extensions,
}
```

**File:** consensus/src/dag/types.rs (L330-340)
```rust
        ensure!(
            verifier
                .check_voting_power(
                    self.parents()
                        .iter()
                        .map(|parent| parent.metadata().author()),
                    true,
                )
                .is_ok(),
            "not enough parents to satisfy voting power"
        );
```

**File:** types/src/validator_verifier.rs (L436-448)
```rust
    pub fn sum_voting_power<'a>(
        &self,
        authors: impl Iterator<Item = &'a AccountAddress>,
    ) -> std::result::Result<u128, VerifyError> {
        let mut aggregated_voting_power = 0;
        for account_address in authors {
            match self.get_voting_power(account_address) {
                Some(voting_power) => aggregated_voting_power += voting_power as u128,
                None => return Err(VerifyError::UnknownAuthor),
            }
        }
        Ok(aggregated_voting_power)
    }
```

**File:** consensus/src/consensusdb/schema/dag/mod.rs (L36-42)
```rust
    fn encode_value(&self) -> Result<Vec<u8>> {
        Ok(bcs::to_bytes(&self)?)
    }

    fn decode_value(data: &[u8]) -> Result<Self> {
        Ok(bcs::from_bytes(data)?)
    }
```

**File:** consensus/src/dag/dag_store.rs (L518-526)
```rust
    pub fn add_node(&self, node: CertifiedNode) -> anyhow::Result<()> {
        self.dag.write().validate_new_node(&node)?;

        // Note on concurrency: it is possible that a prune operation kicks in here and
        // moves the window forward making the `node` stale. Any stale node inserted
        // due to this race will be cleaned up with the next prune operation.

        // mutate after all checks pass
        self.storage.save_certified_node(&node)?;
```
