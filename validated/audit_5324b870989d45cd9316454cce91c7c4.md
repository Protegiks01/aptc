# Audit Report

## Title
Sharded Executor Lost Updates on Non-Total-Supply AggregatorV2 Operations Due to Missing Cross-Shard Aggregation

## Summary
The sharded block executor implements cross-shard aggregation exclusively for the `TOTAL_SUPPLY_STATE_KEY`, leaving all other AggregatorV2 instances (token collection supplies, fungible asset supplies, custom user aggregators) vulnerable to lost updates when modified concurrently across shards. This violates deterministic execution guarantees and causes state inconsistencies between validators.

## Finding Description

The vulnerability exists due to three interconnected design gaps in the sharded execution system:

**1. Incomplete Read/Write Hint Implementation**

The transaction analyzer only provides read/write hints for three specific transaction types: `coin::transfer`, `aptos_account::transfer`, and `aptos_account::create_account`. All other entry functions return empty hints. [1](#0-0) 

Specifically, most Move transactions fall through to the default case and return `empty_rw_set()`: [2](#0-1) 

**2. Partitioner Cannot Detect Conflicts Without Hints**

The partitioner initialization only processes transactions with non-empty hints to build conflict trackers: [3](#0-2) 

The connected component pre-partitioner only merges union-find sets based on write sets: [4](#0-3) 

Transactions with empty hints from different senders remain in separate union-find sets and can be assigned to different shards in the same round, even if they actually conflict.

**3. Selective Aggregation Only for Total Supply**

The `AggregatorOverriddenStateView` only provides special handling for `TOTAL_SUPPLY_STATE_KEY`: [5](#0-4) 

After shard execution completes, only total supply aggregation is performed: [6](#0-5) 

The `aggregate_and_update_total_supply` function only handles the `TOTAL_SUPPLY_STATE_KEY`: [7](#0-6) 

**4. Independent Per-Transaction Materialization**

Delayed field changes (including AggregatorV2 deltas) are materialized independently per transaction: [8](#0-7) 

Each transaction's delayed fields are materialized into resource writes: [9](#0-8) 

**Attack Scenario:**

1. Attacker submits two transactions T1 and T2 from different accounts, both minting tokens in the same collection that uses `create_unbounded_aggregator()` for supply tracking
2. Both transactions are entry functions (not coin transfers), resulting in empty read/write hints
3. The partitioner assigns T1 to Shard 0 and T2 to Shard 1 (different senders, no detected conflicts)
4. Both shards execute in parallel, reading the same base aggregator value (e.g., 100)
5. Shard 0: T1 executes `aggregator.add(50)`, materializes to 150
6. Shard 1: T2 executes `aggregator.add(30)`, materializes to 130
7. Both transaction outputs contain writes to the same StateKey with different values
8. Final state depends on write ordering - either 150 or 130 instead of the correct 180

The final aggregation in `ShardedBlockExecutor::execute_block` simply concatenates outputs without conflict detection: [10](#0-9) 

## Impact Explanation

**Critical Severity - Consensus/Safety Violation**

This vulnerability violates the fundamental blockchain invariant of deterministic execution. Different validators executing the same block may produce different state roots depending on:

1. **Thread scheduling variations**: The order in which shard outputs are processed internally
2. **Race conditions in aggregation**: Non-deterministic write ordering when collecting results
3. **Implementation differences**: Validators with different CPU architectures or scheduling policies

This leads to:

- **State root divergence**: Validators compute different Jellyfish Merkle tree roots for identical blocks
- **Consensus failure**: Validators cannot reach agreement on block commitment
- **Loss of funds**: Token supplies, NFT collection counts, and fungible asset balances incorrectly calculated
- **Network partition**: State inconsistencies requiring manual intervention or hard fork to resolve

This meets the **Critical Severity** criteria per Aptos bug bounty: "Consensus/Safety violations" where different validators produce different state for the same block, and potential "Loss of Funds" as aggregators commonly track token supplies and financial state.

## Likelihood Explanation

**High Likelihood**

The vulnerability has high exploitability due to:

1. **Production code path exists**: Sharded execution is implemented and can be enabled
2. **Common usage patterns**: Token collections and fungible assets actively use AggregatorV2 for parallel supply tracking
3. **No validation barriers**: No checks prevent non-total-supply aggregators from being modified across shards
4. **Natural occurrence**: Any popular NFT collection experiencing concurrent mints triggers this scenario
5. **Simple attack**: Requires only submitting two normal transactions from different accounts

The read/write hint implementation explicitly documents its limitation with a `todo!()` comment, indicating this is a known incomplete feature: [11](#0-10) 

## Recommendation

Implement general cross-shard aggregation for all AggregatorV2 instances, not just total supply:

1. **Extend hint system**: Implement read/write hint extraction for all Move transactions, capturing AggregatorV2 resource accesses
2. **Track all aggregators**: Maintain a registry of all StateKeys containing AggregatorV2 instances during execution
3. **Generalize aggregation**: Extend `aggregate_and_update_total_supply` to handle arbitrary aggregators:
   - Override base values for all tracked aggregators in `AggregatorOverriddenStateView`
   - Compute deltas for all aggregators across all shards
   - Update all transaction outputs with correct aggregated values
4. **Add validation**: Detect and fail-fast when multiple shards write to the same non-total-supply aggregator without proper aggregation

Alternative: Disable sharded execution when transactions with empty hints are present, forcing sequential execution to maintain correctness.

## Proof of Concept

```move
// Create token collection with concurrent supply tracking
module attacker::exploit {
    use aptos_framework::aggregator_v2;
    use aptos_token_objects::collection;
    
    struct CollectionResource has key {
        supply: aggregator_v2::Aggregator<u64>
    }
    
    public entry fun mint_token_t1(account: &signer) {
        // Transaction T1: mint token, increment supply by 50
        let collection_supply = borrow_global_mut<CollectionResource>(@collection_addr);
        aggregator_v2::add(&mut collection_supply.supply, 50);
    }
    
    public entry fun mint_token_t2(account: &signer) {
        // Transaction T2: mint token, increment supply by 30
        let collection_supply = borrow_global_mut<CollectionResource>(@collection_addr);
        aggregator_v2::add(&mut collection_supply.supply, 30);
    }
}

// Attack execution:
// 1. Submit mint_token_t1 from account A
// 2. Submit mint_token_t2 from account B  
// 3. Both get empty hints (not coin transfer)
// 4. Partitioner assigns to different shards
// 5. Both execute in parallel, read base value 100
// 6. T1 materializes: 100 + 50 = 150
// 7. T2 materializes: 100 + 30 = 130
// 8. Final state: 150 or 130 (not 180) - lost update
```

## Notes

This vulnerability affects the sharded execution system's fundamental assumption that transactions can be safely partitioned based on read/write hints. The incomplete hint implementation creates a correctness gap where AggregatorV2 operations—specifically designed for parallel execution—become unsafe when executed across shards. The selective aggregation only for `TOTAL_SUPPLY_STATE_KEY` reveals this was a known concern for the APT coin supply, but the solution was not generalized to all aggregator instances.

### Citations

**File:** types/src/transaction/analyzed_transaction.rs (L236-238)
```rust
pub fn empty_rw_set() -> (Vec<StorageLocation>, Vec<StorageLocation>) {
    (vec![], vec![])
}
```

**File:** types/src/transaction/analyzed_transaction.rs (L246-283)
```rust
        let process_entry_function = |func: &EntryFunction,
                                      sender_address: AccountAddress|
         -> (Vec<StorageLocation>, Vec<StorageLocation>) {
            match (
                *func.module().address(),
                func.module().name().as_str(),
                func.function().as_str(),
            ) {
                (AccountAddress::ONE, "coin", "transfer") => {
                    let receiver_address = bcs::from_bytes(&func.args()[0]).unwrap();
                    rw_set_for_coin_transfer(sender_address, receiver_address, true)
                },
                (AccountAddress::ONE, "aptos_account", "transfer") => {
                    let receiver_address = bcs::from_bytes(&func.args()[0]).unwrap();
                    rw_set_for_coin_transfer(sender_address, receiver_address, false)
                },
                (AccountAddress::ONE, "aptos_account", "create_account") => {
                    let receiver_address = bcs::from_bytes(&func.args()[0]).unwrap();
                    rw_set_for_create_account(sender_address, receiver_address)
                },
                _ => todo!(
                    "Only coin transfer and create account transactions are supported for now"
                ),
            }
        };
        match self {
            Transaction::UserTransaction(signed_txn) => match signed_txn.payload().executable_ref()
            {
                Ok(TransactionExecutableRef::EntryFunction(func))
                    if !signed_txn.payload().is_multisig() =>
                {
                    process_entry_function(func, signed_txn.sender())
                },
                _ => todo!("Only entry function transactions are supported for now"),
            },
            _ => empty_rw_set(),
        }
    }
```

**File:** execution/block-partitioner/src/v2/init.rs (L28-55)
```rust
                    let reads = txn.read_hints.iter().map(|loc| (loc, false));
                    let writes = txn.write_hints.iter().map(|loc| (loc, true));
                    reads
                        .chain(writes)
                        .for_each(|(storage_location, is_write)| {
                            let key_idx = state.add_key(storage_location.state_key());
                            if is_write {
                                state.write_sets[ori_txn_idx]
                                    .write()
                                    .unwrap()
                                    .insert(key_idx);
                            } else {
                                state.read_sets[ori_txn_idx]
                                    .write()
                                    .unwrap()
                                    .insert(key_idx);
                            }
                            state.trackers.entry(key_idx).or_insert_with(|| {
                                let anchor_shard_id = get_anchor_shard_id(
                                    storage_location,
                                    state.num_executor_shards,
                                );
                                RwLock::new(ConflictingTxnTracker::new(
                                    storage_location.clone(),
                                    anchor_shard_id,
                                ))
                            });
                        });
```

**File:** execution/block-partitioner/src/pre_partition/connected_component/mod.rs (L49-56)
```rust
        for txn_idx in 0..state.num_txns() {
            let sender_idx = state.sender_idx(txn_idx);
            let write_set = state.write_sets[txn_idx].read().unwrap();
            for &key_idx in write_set.iter() {
                let key_idx_in_uf = num_senders + key_idx;
                uf.union(key_idx_in_uf, sender_idx);
            }
        }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/aggr_overridden_state_view.rs (L42-48)
```rust
        if *state_key == *TOTAL_SUPPLY_STATE_KEY {
            // TODO: Remove this when we have aggregated total supply implementation for remote
            //       sharding. For now we need this because after all the txns are executed, the
            //       proof checker expects the total_supply to read/written to the tree.
            self.base_view.get_state_value(state_key)?;
            return self.total_supply_base_view_override();
        }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L215-220)
```rust
        sharded_aggregator_service::aggregate_and_update_total_supply(
            &mut sharded_output,
            &mut global_output,
            state_view.as_ref(),
            self.global_executor.get_executor_thread_pool(),
        );
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_aggregator_service.rs (L168-257)
```rust
pub fn aggregate_and_update_total_supply<S: StateView>(
    sharded_output: &mut Vec<Vec<Vec<TransactionOutput>>>,
    global_output: &mut [TransactionOutput],
    state_view: &S,
    executor_thread_pool: Arc<rayon::ThreadPool>,
) {
    let num_shards = sharded_output.len();
    let num_rounds = sharded_output[0].len();

    // The first element is 0, which is the delta for shard 0 in round 0. +1 element will contain
    // the delta for the global shard
    let mut aggr_total_supply_delta = vec![DeltaU128::default(); num_shards * num_rounds + 1];

    // No need to parallelize this as the runtime is O(num_shards * num_rounds)
    // TODO: Get this from the individual shards while getting 'sharded_output'
    let mut aggr_ts_idx = 1;
    for round in 0..num_rounds {
        sharded_output.iter().for_each(|shard_output| {
            let mut curr_delta = DeltaU128::default();
            // Though we expect all the txn_outputs to have total_supply, there can be
            // exceptions like 'block meta' (first txn in the block) and 'chkpt info' (last txn
            // in the block) which may not have total supply. Hence we iterate till we find the
            // last txn with total supply.
            for txn in shard_output[round].iter().rev() {
                if let Some(last_txn_total_supply) = txn.write_set().get_total_supply() {
                    curr_delta =
                        DeltaU128::get_delta(last_txn_total_supply, TOTAL_SUPPLY_AGGR_BASE_VAL);
                    break;
                }
            }
            aggr_total_supply_delta[aggr_ts_idx] =
                curr_delta + aggr_total_supply_delta[aggr_ts_idx - 1];
            aggr_ts_idx += 1;
        });
    }

    // The txn_outputs contain 'txn_total_supply' with
    // 'CrossShardStateViewAggrOverride::total_supply_aggr_base_val' as the base value.
    // The actual 'total_supply_base_val' is in the state_view.
    // The 'delta' for the shard/round is in aggr_total_supply_delta[round * num_shards + shard_id + 1]
    // For every txn_output, we have to compute
    //      txn_total_supply = txn_total_supply - CrossShardStateViewAggrOverride::total_supply_aggr_base_val + total_supply_base_val + delta
    // While 'txn_total_supply' is u128, the intermediate computation can be negative. So we use
    // DeltaU128 to handle any intermediate underflow of u128.
    let total_supply_base_val: u128 = get_state_value(&TOTAL_SUPPLY_STATE_KEY, state_view).unwrap();
    let base_val_delta = DeltaU128::get_delta(total_supply_base_val, TOTAL_SUPPLY_AGGR_BASE_VAL);

    let aggr_total_supply_delta_ref = &aggr_total_supply_delta;
    // Runtime is O(num_txns), hence parallelized at the shard level and at the txns level.
    executor_thread_pool.scope(|_| {
        sharded_output
            .par_iter_mut()
            .enumerate()
            .for_each(|(shard_id, shard_output)| {
                for (round, txn_outputs) in shard_output.iter_mut().enumerate() {
                    let delta_for_round =
                        aggr_total_supply_delta_ref[round * num_shards + shard_id] + base_val_delta;
                    let num_txn_outputs = txn_outputs.len();
                    txn_outputs
                        .par_iter_mut()
                        .with_min_len(optimal_min_len(num_txn_outputs, 32))
                        .for_each(|txn_output| {
                            if let Some(txn_total_supply) =
                                txn_output.write_set().get_total_supply()
                            {
                                txn_output.update_total_supply(
                                    delta_for_round.add_delta(txn_total_supply),
                                );
                            }
                        });
                }
            });
    });

    let delta_for_global_shard = aggr_total_supply_delta[num_shards * num_rounds] + base_val_delta;
    let delta_for_global_shard_ref = &delta_for_global_shard;
    executor_thread_pool.scope(|_| {
        let num_txn_outputs = global_output.len();
        global_output
            .par_iter_mut()
            .with_min_len(optimal_min_len(num_txn_outputs, 32))
            .for_each(|txn_output| {
                if let Some(txn_total_supply) = txn_output.write_set().get_total_supply() {
                    txn_output.update_total_supply(
                        delta_for_global_shard_ref.add_delta(txn_total_supply),
                    );
                }
            });
    });
}
```

**File:** aptos-move/aptos-aggregator/src/delayed_change.rs (L69-86)
```rust
    pub fn apply_to_base(
        &self,
        base_value: DelayedFieldValue,
    ) -> Result<DelayedFieldValue, PanicOr<DelayedFieldsSpeculativeError>> {
        use DelayedApplyChange::*;

        Ok(match self {
            AggregatorDelta { delta } => {
                DelayedFieldValue::Aggregator(delta.apply_to(base_value.into_aggregator_value()?)?)
            },
            SnapshotDelta { delta, .. } => {
                DelayedFieldValue::Snapshot(delta.apply_to(base_value.into_aggregator_value()?)?)
            },
            SnapshotDerived { formula, .. } => {
                DelayedFieldValue::Derived(formula.apply_to(base_value.into_snapshot_value()?))
            },
        })
    }
```

**File:** aptos-move/aptos-vm-types/src/output.rs (L248-251)
```rust
        // materialize delayed fields into resource writes
        self.change_set
            .extend_resource_write_set(patched_resource_write_set.into_iter())?;
        let _ = self.change_set.drain_delayed_field_change_set();
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/mod.rs (L95-115)
```rust
        // wait for all remote executors to send the result back and append them in order by shard id
        info!("ShardedBlockExecutor Received all results");
        let _aggregation_timer = SHARDED_EXECUTION_RESULT_AGGREGATION_SECONDS.start_timer();
        let num_rounds = sharded_output[0].len();
        let mut aggregated_results = vec![];
        let mut ordered_results = vec![vec![]; num_executor_shards * num_rounds];
        // Append the output from individual shards in the round order
        for (shard_id, results_from_shard) in sharded_output.into_iter().enumerate() {
            for (round, result) in results_from_shard.into_iter().enumerate() {
                ordered_results[round * num_executor_shards + shard_id] = result;
            }
        }

        for result in ordered_results.into_iter() {
            aggregated_results.extend(result);
        }

        // Lastly append the global output
        aggregated_results.extend(global_output);

        Ok(aggregated_results)
```
