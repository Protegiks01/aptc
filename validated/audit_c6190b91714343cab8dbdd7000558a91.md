# Audit Report

## Title
Consensus Pipeline Liveness Failure Due to Missing Execution Retry Mechanism

## Summary
The consensus buffer manager lacks a retry mechanism for failed block executions, causing indefinite pipeline stalls for affected validators. When execution fails with an `ExecutorError`, the block remains in "Ordered" state permanently with no automatic recovery, requiring manual intervention to restore validator operation.

## Finding Description

The vulnerability exists in the execution error handling flow within the consensus buffer manager. When a block execution fails, the error is logged but no retry mechanism exists to recover.

When ordered blocks arrive, they are sent for execution exactly once in `process_ordered_blocks`: [1](#0-0) 

The execution pipeline processes these blocks through `ExecutionSchedulePhase` and `ExecutionWaitPhase`, which can fail with various `ExecutorError` types: [2](#0-1) 

When execution fails, `process_execution_response` receives the error, logs it, and returns early without advancing the block state: [3](#0-2) 

The block remains in "Ordered" state. The buffer manager then calls `advance_execution_root()` which detects when a retry is needed and explicitly returns `Some(block_id)` to signal this, with a comment stating "Schedule retry": [4](#0-3) 

However, all callers of `advance_execution_root()` in the main event loop completely ignore this return value: [5](#0-4) 

This contrasts sharply with the signing phase, which properly implements retry logic. When signing hasn't progressed, it spawns a retry request with a 100ms delay: [6](#0-5) 

The retry mechanism itself is implemented via `spawn_retry_request`: [7](#0-6) 

**Security Guarantee Violation**: This breaks the **consensus liveness** guarantee by causing indefinite pipeline stalls for affected validators. The validator cannot process subsequent blocks until the failed block is cleared through manual intervention (restart, state sync, or epoch change).

## Impact Explanation

This vulnerability represents **High Severity** per the Aptos bug bounty criteria, specifically matching the "Validator Node Slowdowns" category:

1. **Validator Pipeline Stall**: When execution fails for a block, that validator's entire consensus pipeline becomes stuck. All subsequent blocks remain in "Ordered" state and cannot progress through execution, signing, or persistence phases.

2. **Manual Intervention Required**: The only recovery mechanisms are external: validator restart, state sync trigger, or epoch boundary reset. No automatic recovery exists within the consensus pipeline itself.

3. **Individual Validator Impact**: This primarily affects individual validators that encounter execution errors. The network as a whole maintains liveness as long as â‰¥2/3 validators remain operational.

4. **Not Critical Severity**: While severe, this does not qualify as Critical because:
   - It affects individual validators, not the entire network
   - No consensus safety violation (no split-brain scenario)
   - No direct fund loss or theft
   - Network continues operating if <2/3 validators affected

The impact aligns with the bug bounty's HIGH severity classification for validator node performance degradation affecting consensus participation.

## Likelihood Explanation

**Medium-High likelihood** of occurrence:

1. **Transient Errors in Production**: Execution failures can occur due to legitimate operational issues:
   - Network timeouts causing `CouldNotGetData`
   - Temporary database connection issues causing `InternalError`
   - Resource exhaustion during high load periods

2. **No Defensive Programming**: The execution phase completely lacks retry logic, unlike the signing phase which implements explicit retry with delays. The code comment "Schedule retry" at line 437 indicates developers recognized the need but never implemented it.

3. **Design Inconsistency**: The asymmetry between signing (has retry) and execution (no retry) suggests this was an oversight rather than intentional design, increasing likelihood it will manifest under production conditions.

## Recommendation

Implement execution retry logic similar to the signing phase. Modify the main event loop to use the return value from `advance_execution_root()`:

```rust
async fn process_execution_response(&mut self, response: ExecutionResponse) {
    // ... existing error handling ...
    
    if let Some(block_id_to_retry) = self.advance_execution_root() {
        // Retry execution for the stuck block
        let item = self.buffer.get(&self.execution_root);
        if let Some(ordered_item) = item.as_ordered() {
            let request = self.create_new_request(ExecutionRequest {
                ordered_blocks: ordered_item.ordered_blocks.clone(),
            });
            let sender = self.execution_schedule_phase_tx.clone();
            Self::spawn_retry_request(sender, request, Duration::from_millis(100));
        }
    }
    
    if self.signing_root.is_none() {
        self.advance_signing_root().await;
    }
}
```

## Proof of Concept

A PoC would require:
1. Setting up a test environment where block execution can be forced to fail with `ExecutorError::CouldNotGetData`
2. Observing that the block remains in "Ordered" state
3. Verifying that `advance_execution_root()` returns `Some(block_id)` but no retry is triggered
4. Confirming that subsequent blocks cannot progress through the pipeline

The vulnerability can be triggered by inducing transient database errors or timeouts during block execution in a test validator setup.

## Notes

- The vulnerability requires execution failures to manifest, which can occur naturally from operational issues or potentially be induced through resource exhaustion
- Recovery mechanisms exist (restart, state sync, epoch boundary) but require external intervention
- The design inconsistency between signing (has retry) and execution (no retry) strongly suggests this was an implementation oversight
- This is a logic vulnerability affecting consensus liveness, not a network DoS attack

### Citations

**File:** consensus/src/pipeline/buffer_manager.rs (L293-306)
```rust
    fn spawn_retry_request<T: Send + 'static>(
        mut sender: Sender<T>,
        request: T,
        duration: Duration,
    ) {
        counters::BUFFER_MANAGER_RETRY_COUNT.inc();
        spawn_named!("retry request", async move {
            tokio::time::sleep(duration).await;
            sender
                .send(request)
                .await
                .expect("Failed to send retry request");
        });
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L382-424)
```rust
    async fn process_ordered_blocks(&mut self, ordered_blocks: OrderedBlocks) {
        let OrderedBlocks {
            ordered_blocks,
            ordered_proof,
        } = ordered_blocks;

        info!(
            "Receive {} ordered block ends with [epoch: {}, round: {}, id: {}], the queue size is {}",
            ordered_blocks.len(),
            ordered_proof.commit_info().epoch(),
            ordered_proof.commit_info().round(),
            ordered_proof.commit_info().id(),
            self.buffer.len() + 1,
        );

        let request = self.create_new_request(ExecutionRequest {
            ordered_blocks: ordered_blocks.clone(),
        });
        if let Some(consensus_publisher) = &self.consensus_publisher {
            let message = ConsensusObserverMessage::new_ordered_block_message(
                ordered_blocks.clone(),
                ordered_proof.clone(),
            );
            consensus_publisher.publish_message(message);
        }
        self.execution_schedule_phase_tx
            .send(request)
            .await
            .expect("Failed to send execution schedule request");

        let mut unverified_votes = HashMap::new();
        if let Some(block) = ordered_blocks.last() {
            if let Some(votes) = self.pending_commit_votes.remove(&block.round()) {
                for (_, vote) in votes {
                    if vote.commit_info().id() == block.id() {
                        unverified_votes.insert(vote.author(), vote);
                    }
                }
            }
        }
        let item = BufferItem::new_ordered(ordered_blocks, ordered_proof, unverified_votes);
        self.buffer.push_back(item);
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L426-452)
```rust
    /// Set the execution root to the first not executed item (Ordered) and send execution request
    /// Set to None if not exist
    /// Return Some(block_id) if the block needs to be scheduled for retry
    fn advance_execution_root(&mut self) -> Option<HashValue> {
        let cursor = self.execution_root;
        self.execution_root = self
            .buffer
            .find_elem_from(cursor.or_else(|| *self.buffer.head_cursor()), |item| {
                item.is_ordered()
            });
        if self.execution_root.is_some() && cursor == self.execution_root {
            // Schedule retry.
            self.execution_root
        } else {
            sample!(
                SampleRate::Frequency(2),
                info!(
                    "Advance execution root from {:?} to {:?}",
                    cursor, self.execution_root
                )
            );
            // Otherwise do nothing, because the execution wait phase is driven by the response of
            // the execution schedule phase, which is in turn fed as soon as the ordered blocks
            // come in.
            None
        }
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L454-488)
```rust
    /// Set the signing root to the first not signed item (Executed) and send execution request
    /// Set to None if not exist
    async fn advance_signing_root(&mut self) {
        let cursor = self.signing_root;
        self.signing_root = self
            .buffer
            .find_elem_from(cursor.or_else(|| *self.buffer.head_cursor()), |item| {
                item.is_executed()
            });
        sample!(
            SampleRate::Frequency(2),
            info!(
                "Advance signing root from {:?} to {:?}",
                cursor, self.signing_root
            )
        );
        if self.signing_root.is_some() {
            let item = self.buffer.get(&self.signing_root);
            let executed_item = item.unwrap_executed_ref();
            let request = self.create_new_request(SigningRequest {
                ordered_ledger_info: executed_item.ordered_proof.clone(),
                commit_ledger_info: executed_item.partial_commit_proof.data().clone(),
                blocks: executed_item.executed_blocks.clone(),
            });
            if cursor == self.signing_root {
                let sender = self.signing_phase_tx.clone();
                Self::spawn_retry_request(sender, request, Duration::from_millis(100));
            } else {
                self.signing_phase_tx
                    .send(request)
                    .await
                    .expect("Failed to send signing request");
            }
        }
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L609-627)
```rust
    async fn process_execution_response(&mut self, response: ExecutionResponse) {
        let ExecutionResponse { block_id, inner } = response;
        // find the corresponding item, may not exist if a reset or aggregated happened
        let current_cursor = self.buffer.find_elem_by_key(self.execution_root, block_id);
        if current_cursor.is_none() {
            return;
        }

        let executed_blocks = match inner {
            Ok(result) => result,
            Err(e) => {
                log_executor_error_occurred(
                    e,
                    &counters::BUFFER_MANAGER_RECEIVED_EXECUTOR_ERROR_COUNT,
                    block_id,
                );
                return;
            },
        };
```

**File:** consensus/src/pipeline/buffer_manager.rs (L935-990)
```rust
        while !self.stop {
            // advancing the root will trigger sending requests to the pipeline
            ::tokio::select! {
                Some(blocks) = self.block_rx.next(), if !self.need_back_pressure() => {
                    self.latest_round = blocks.latest_round();
                    monitor!("buffer_manager_process_ordered", {
                    self.process_ordered_blocks(blocks).await;
                    if self.execution_root.is_none() {
                        self.advance_execution_root();
                    }});
                },
                Some(reset_event) = self.reset_rx.next() => {
                    monitor!("buffer_manager_process_reset",
                    self.process_reset_request(reset_event).await);
                },
                Some(response) = self.execution_schedule_phase_rx.next() => {
                    monitor!("buffer_manager_process_execution_schedule_response", {
                    self.process_execution_schedule_response(response).await;
                })},
                Some(response) = self.execution_wait_phase_rx.next() => {
                    monitor!("buffer_manager_process_execution_wait_response", {
                    self.process_execution_response(response).await;
                    self.advance_execution_root();
                    if self.signing_root.is_none() {
                        self.advance_signing_root().await;
                    }});
                },
                Some(response) = self.signing_phase_rx.next() => {
                    monitor!("buffer_manager_process_signing_response", {
                    self.process_signing_response(response).await;
                    self.advance_signing_root().await
                    })
                },
                Some(Ok(round)) = self.persisting_phase_rx.next() => {
                    // see where `need_backpressure()` is called.
                    self.pending_commit_votes = self.pending_commit_votes.split_off(&(round + 1));
                    self.highest_committed_round = round;
                    self.pending_commit_blocks = self.pending_commit_blocks.split_off(&(round + 1));
                },
                Some(rpc_request) = verified_commit_msg_rx.next() => {
                    monitor!("buffer_manager_process_commit_message",
                    if let Some(aggregated_block_id) = self.process_commit_message(rpc_request) {
                        self.advance_head(aggregated_block_id).await;
                        if self.execution_root.is_none() {
                            self.advance_execution_root();
                        }
                        if self.signing_root.is_none() {
                            self.advance_signing_root().await;
                        }
                    });
                }
                _ = interval.tick().fuse() => {
                    monitor!("buffer_manager_process_interval_tick", {
                    self.update_buffer_manager_metrics();
                    self.rebroadcast_commit_votes_if_needed().await
                    });
```

**File:** execution/executor-types/src/error.rs (L11-43)
```rust
#[derive(Debug, Deserialize, Error, PartialEq, Eq, Serialize, Clone)]
/// Different reasons for proposal rejection
pub enum ExecutorError {
    #[error("Cannot find speculation result for block id {0}")]
    BlockNotFound(HashValue),

    #[error("Cannot get data for batch id {0}")]
    DataNotFound(HashValue),

    #[error(
        "Bad num_txns_to_commit. first version {}, num to commit: {}, target version: {}",
        first_version,
        to_commit,
        target_version
    )]
    BadNumTxnsToCommit {
        first_version: Version,
        to_commit: usize,
        target_version: Version,
    },

    #[error("Internal error: {:?}", error)]
    InternalError { error: String },

    #[error("Serialization error: {0}")]
    SerializationError(String),

    #[error("Received Empty Blocks")]
    EmptyBlocks,

    #[error("request timeout")]
    CouldNotGetData,
}
```
