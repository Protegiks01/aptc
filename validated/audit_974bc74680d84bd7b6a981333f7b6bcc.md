# Audit Report

## Title
Epoch Snapshot Pruner Can Delete Required State Sync Data Due to Unbounded Progress Metadata

## Summary
The epoch snapshot pruner calculates its target version based on the latest state checkpoint version minus the prune window, without validating that this target doesn't exceed the latest epoch ending version. This allows the pruner to delete epoch ending snapshot data (stale merkle tree nodes) that are still needed for ongoing state synchronization, breaking the ability of new or recovering nodes to fast sync to recent epochs.

## Finding Description

The vulnerability exists in how the epoch snapshot pruner determines its target pruning version. The pruner is designed to maintain epoch ending snapshots for state sync, but it lacks bounds checking to ensure it doesn't prune beyond the latest epoch ending.

**Root Cause Flow:**

1. **Pruner Target Calculation**: The epoch snapshot pruner's target is set using `get_latest_state_checkpoint_version()`, which returns the most recent committed state checkpoint - not necessarily an epoch ending version. [1](#0-0) 

2. **No Epoch Boundary Validation**: The target version is calculated as `latest_version.saturating_sub(self.prune_window)` without any check that this value doesn't exceed the latest epoch ending version. [2](#0-1) 

3. **Stale Node Classification**: Stale nodes with versions at or before the previous epoch ending are stored in `StaleNodeIndexCrossEpochSchema` - these represent the epoch ending snapshots required for state sync. [3](#0-2) 

4. **Unconditional Pruning**: The pruner deletes all stale nodes with `stale_since_version <= target_version` without verifying these aren't part of a needed epoch ending snapshot. [4](#0-3) [5](#0-4) 

**The Critical Logic Flaw:**

The pruner uses `stale_since_version` (when the node was overwritten) as the criterion for deletion, not the epoch the node belongs to. When nodes from an epoch ending snapshot (e.g., epoch 3 at version 30M) are overwritten shortly after in epoch 4 (versions 30M-40M), they receive `stale_since_version` values in that range. If the pruner target is 40M (calculated as 120M - 80M prune_window), these nodes are deleted even though they belong to an epoch snapshot that should be retained.

**Attack Scenario (Natural Occurrence):**
- Epoch 3 ends at version 30M
- Node continues processing to version 120M
- Pruner calculates target: 120M - 80M = 40M  
- Stale nodes from epoch 3 that were overwritten at versions 30M-40M are pruned
- New node attempting to fast sync to epoch 3 fails - required merkle tree nodes are missing

This violates the design intent documented in the configuration that "epoch ending snapshots are used by state sync in fast sync mode." [6](#0-5) 

**State Sync Dependency on Merkle Nodes:**

When serving state values with proofs, the system must generate merkle range proofs via `get_range_proof` which calls `get_with_proof`. [7](#0-6)  The proof generation traverses the tree using `get_node_with_tag`. [8](#0-7) 

When these nodes are deleted by the pruner, the `get_node_option` call returns `None` [9](#0-8) , causing `get_node_with_tag` to fail with "Missing node" errors. [10](#0-9) 

## Impact Explanation

**Medium Severity** - This vulnerability causes protocol violations affecting network availability:

1. **State Sync Degradation**: Nodes attempting to fast sync to recent epochs will encounter errors when the serving node cannot retrieve required merkle tree nodes for epoch ending snapshots. The validation logic in `error_if_state_merkle_pruned` will detect the missing data and return an error. [11](#0-10) 

2. **Availability Impact**: New validators and full nodes must use slower synchronization methods (transaction replay) or wait for peers with longer history, increasing operational barriers and reducing network resilience.

3. **Operational Costs**: Node operators must maintain unnecessarily long history or risk being unable to serve state sync requests effectively.

The impact is **not** High/Critical because:
- Doesn't directly cause consensus failure or funds loss
- Nodes can still sync using transaction replay from genesis
- Doesn't cause permanent network partition or total liveness failure
- The error is detected and returned gracefully

This represents a **limited protocol violation** requiring manual intervention or workarounds, fitting the Medium severity category per the Aptos bug bounty framework.

## Likelihood Explanation

**Likely** - This occurs naturally under normal network operation:

1. **Default Configuration Creates Vulnerability**: The default prune window is 80M versions (~2.2 epochs at 5K TPS). [12](#0-11) 

2. **Checkpoint-Epoch Timing Gap**: State checkpoints occur frequently (potentially every block), while epochs occur every ~2 hours. The latest checkpoint version naturally advances beyond epoch boundaries.

3. **No Validation Safeguards**: The pruning code path contains no checks to prevent pruning nodes from recent epoch snapshots. [2](#0-1)  The vulnerability triggers automatically when state churn causes nodes from epoch snapshots to be overwritten within the prune window.

4. **State Churn**: As transactions modify state in the epochs following an epoch ending, nodes from the previous epoch snapshot become stale. Their `stale_since_version` values fall within the range that gets pruned.

## Recommendation

Modify the `set_pruner_target_db_version` method in `StateMerklePrunerManager` for the epoch snapshot pruner to validate that the calculated target doesn't exceed the latest epoch ending version. The fix should:

1. Query the latest epoch ending version from the ledger metadata
2. Ensure the pruner target is bounded by `max(calculated_target, latest_epoch_ending_version)`
3. Only prune nodes from epochs older than the required retention window

Alternatively, modify the pruning logic to check the `node_key.version()` (the epoch the node belongs to) rather than solely relying on `stale_since_version` when deciding which cross-epoch nodes to delete.

## Proof of Concept

While a complete PoC would require a full node setup with multiple epochs of execution, the vulnerability can be demonstrated by examining the code flow:

1. Initialize AptosDB with default epoch snapshot pruner config (80M window)
2. Process blocks through epoch boundaries (e.g., epochs 1, 2, 3)
3. Continue processing to version 120M (past epoch 3's ending at ~30M)
4. Observe that `get_latest_state_checkpoint_version()` returns 120M
5. Pruner calculates target: 120M - 80M = 40M
6. Stale nodes from epoch 3 with `stale_since_version` â‰¤ 40M are deleted
7. Attempt to fast sync a new node to epoch 3
8. Observe that `get_range_proof` fails with "Missing node" errors because the required merkle nodes were pruned

The vulnerability is confirmed by code inspection showing the mismatch between node classification (based on original version) and pruning criteria (based on stale_since_version), with no validation to prevent deletion of epoch snapshot nodes.

## Notes

This is a logic vulnerability in the storage pruning system that occurs naturally under default configuration. The mismatch between how nodes are classified as epoch snapshots versus how they are pruned creates a scenario where epoch-ending merkle tree nodes required for state sync can be incorrectly deleted, degrading the network's ability to efficiently onboard new nodes.

### Citations

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L172-180)
```rust
            if let Some(version) = myself.get_latest_state_checkpoint_version()? {
                myself
                    .state_store
                    .state_merkle_pruner
                    .maybe_set_pruner_target_db_version(version);
                myself
                    .state_store
                    .epoch_snapshot_pruner
                    .maybe_set_pruner_target_db_version(version);
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L273-303)
```rust
    pub(super) fn error_if_state_merkle_pruned(
        &self,
        data_type: &str,
        version: Version,
    ) -> Result<()> {
        let min_readable_version = self
            .state_store
            .state_db
            .state_merkle_pruner
            .get_min_readable_version();
        if version >= min_readable_version {
            return Ok(());
        }

        let min_readable_epoch_snapshot_version = self
            .state_store
            .state_db
            .epoch_snapshot_pruner
            .get_min_readable_version();
        if version >= min_readable_epoch_snapshot_version {
            self.ledger_db.metadata_db().ensure_epoch_ending(version)
        } else {
            bail!(
                "{} at version {} is pruned. snapshots are available at >= {}, epoch snapshots are available at >= {}",
                data_type,
                version,
                min_readable_version,
                min_readable_epoch_snapshot_version,
            )
        }
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_pruner_manager.rs (L159-174)
```rust
    fn set_pruner_target_db_version(&self, latest_version: Version) {
        assert!(self.pruner_worker.is_some());

        let min_readable_version = latest_version.saturating_sub(self.prune_window);
        self.min_readable_version
            .store(min_readable_version, Ordering::SeqCst);

        PRUNER_VERSIONS
            .with_label_values(&[S::name(), "min_readable"])
            .set(min_readable_version as i64);

        self.pruner_worker
            .as_ref()
            .unwrap()
            .set_target_db_version(min_readable_version);
    }
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L378-386)
```rust
            if previous_epoch_ending_version.is_some()
                && row.node_key.version() <= previous_epoch_ending_version.unwrap()
            {
                batch.put::<StaleNodeIndexCrossEpochSchema>(row, &())
            } else {
                // These are processed by the state merkle pruner.
                batch.put::<StaleNodeIndexSchema>(row, &())
            }
        })?;
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L856-897)
```rust
    fn get_node_option(&self, node_key: &NodeKey, tag: &str) -> Result<Option<Node>> {
        let start_time = Instant::now();
        if !self.cache_enabled() {
            let node_opt = self
                .db_by_key(node_key)
                .get::<JellyfishMerkleNodeSchema>(node_key)?;
            NODE_CACHE_SECONDS
                .observe_with(&[tag, "cache_disabled"], start_time.elapsed().as_secs_f64());
            return Ok(node_opt);
        }
        if let Some(node_cache) = self
            .version_caches
            .get(&node_key.get_shard_id())
            .unwrap()
            .get_version(node_key.version())
        {
            let node = node_cache.get(node_key).cloned();
            NODE_CACHE_SECONDS.observe_with(
                &[tag, "versioned_cache_hit"],
                start_time.elapsed().as_secs_f64(),
            );
            return Ok(node);
        }

        if let Some(lru_cache) = &self.lru_cache {
            if let Some(node) = lru_cache.get(node_key) {
                NODE_CACHE_SECONDS
                    .observe_with(&[tag, "lru_cache_hit"], start_time.elapsed().as_secs_f64());
                return Ok(Some(node));
            }
        }

        let node_opt = self
            .db_by_key(node_key)
            .get::<JellyfishMerkleNodeSchema>(node_key)?;
        if let Some(lru_cache) = &self.lru_cache {
            if let Some(node) = &node_opt {
                lru_cache.put(node_key.clone(), node.clone());
            }
        }
        NODE_CACHE_SECONDS.observe_with(&[tag, "cache_miss"], start_time.elapsed().as_secs_f64());
        Ok(node_opt)
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L206-214)
```rust
            if let Some((index, _)) = iter.next().transpose()? {
                next_version = Some(index.stale_since_version);
                if index.stale_since_version <= target_version {
                    indices.push(index);
                    continue;
                }
            }
            break;
        }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs (L53-64)
```rust
        let (indices, next_version) = StateMerklePruner::get_stale_node_indices(
            &self.metadata_db,
            current_progress,
            target_version_for_this_round,
            usize::MAX,
        )?;

        let mut batch = SchemaBatch::new();
        indices.into_iter().try_for_each(|index| {
            batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
            batch.delete::<S>(&index)
        })?;
```

**File:** config/src/config/storage_config.rs (L415-430)
```rust
impl Default for EpochSnapshotPrunerConfig {
    fn default() -> Self {
        Self {
            enable: true,
            // This is based on ~5K TPS * 2h/epoch * 2 epochs. -- epoch ending snapshots are used
            // by state sync in fast sync mode.
            // The setting is in versions, not epochs, because this makes it behave more like other
            // pruners: a slower network will have longer history in db with the same pruner
            // settings, but the disk space take will be similar.
            // settings.
            prune_window: 80_000_000,
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
        }
    }
```

**File:** storage/jellyfish-merkle/src/lib.rs (L126-129)
```rust
    fn get_node_with_tag(&self, node_key: &NodeKey, tag: &str) -> Result<Node<K>> {
        self.get_node_option(node_key, tag)?
            .ok_or_else(|| AptosDbError::NotFound(format!("Missing node at {:?}.", node_key)))
    }
```

**File:** storage/jellyfish-merkle/src/lib.rs (L732-741)
```rust
            let next_node = self
                .reader
                .get_node_with_tag(&next_node_key, "get_proof")
                .map_err(|err| {
                    if nibble_depth == 0 {
                        AptosDbError::MissingRootError(version)
                    } else {
                        err
                    }
                })?;
```

**File:** storage/jellyfish-merkle/src/lib.rs (L801-824)
```rust
    pub fn get_range_proof(
        &self,
        rightmost_key_to_prove: HashValue,
        version: Version,
    ) -> Result<SparseMerkleRangeProof> {
        let (account, proof) = self.get_with_proof(rightmost_key_to_prove, version)?;
        ensure!(account.is_some(), "rightmost_key_to_prove must exist.");

        let siblings = proof
            .siblings()
            .iter()
            .zip(rightmost_key_to_prove.iter_bits())
            .filter_map(|(sibling, bit)| {
                // We only need to keep the siblings on the right.
                if !bit {
                    Some(*sibling)
                } else {
                    None
                }
            })
            .rev()
            .collect();
        Ok(SparseMerkleRangeProof::new(siblings))
    }
```
