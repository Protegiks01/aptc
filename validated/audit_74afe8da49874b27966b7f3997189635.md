# Audit Report

## Title
Async Cancellation Causes Connection Counter Leak in wait_by_hash Endpoint Leading to Denial of Service

## Summary
The `wait_transaction_by_hash()` function in the Aptos REST API contains an async cancellation safety vulnerability. When clients disconnect during the long-polling operation, the connection counter is permanently leaked. After 100 such disconnections, the endpoint degrades to non-waiting mode, effectively disabling its long-polling functionality until node restart.

## Finding Description

The vulnerability exists in the connection counting mechanism for the `/transactions/wait_by_hash/:txn_hash` endpoint. The implementation uses a shared atomic counter to limit concurrent long-polling connections.

The vulnerable code path begins when a request arrives. The counter is incremented using `fetch_add(1)` before entering the async operation [1](#0-0) . If the incremented value exceeds the limit, the request immediately falls back to short polling [2](#0-1) .

When the limit is not exceeded, the function enters a long-polling path that calls `wait_transaction_by_hash_inner()` [3](#0-2) . This is an async operation with multiple await points [4](#0-3) .

The critical flaw is that cleanup code only executes after the await completes successfully [5](#0-4) . In Rust's async runtime, when a Future is dropped due to client disconnection during an await, any code after the last unexecuted await point never runs. This means the counter decrement never executes, permanently leaking the counter value.

The counter is defined as `Arc<AtomicUsize>` shared across all API requests [6](#0-5) , initialized to zero [7](#0-6) , with a default maximum of 100 connections [8](#0-7) .

**Attack Execution:**
1. Attacker sends HTTP GET request to `/transactions/wait_by_hash/{hash}`
2. Connection counter increments from N to N+1
3. Attacker immediately closes the TCP connection
4. The async Future is dropped, cleanup never executes
5. Counter remains at N+1 permanently
6. Repeat 100 times until counter reaches limit
7. All subsequent requests degrade to non-waiting mode

The endpoint does not become completely non-functional - it falls back to immediate transaction lookup without waiting. However, this defeats the purpose of the endpoint and constitutes a persistent degradation requiring manual intervention.

## Impact Explanation

This qualifies as **Medium Severity** under Aptos Bug Bounty criteria:

**State Inconsistencies Requiring Manual Intervention:** The leaked counter represents incorrect internal state that can only be reset by restarting the node. Operators must manually intervene to restore full functionality.

**Limited Availability Impact:** Only the `/transactions/wait_by_hash` endpoint is affected. The endpoint continues to function but is degraded to immediate lookup mode. Other API endpoints, consensus operations, transaction processing, and all blockchain state management remain completely unaffected.

**No Critical Infrastructure Impact:** This does not affect validator operations, consensus safety, liveness, fund custody, or blockchain state integrity. It does not cause validator slowdowns, API crashes, or affect the mempool or transaction submission paths.

**Recoverable Without Data Loss:** A simple node restart clears the leaked counters with no permanent damage to blockchain state or loss of data.

This does not qualify as High severity because it does not crash the API or affect network participation - only one endpoint's long-polling feature is degraded.

## Likelihood Explanation

This vulnerability has **HIGH** likelihood of exploitation:

1. **Minimal Complexity:** Any HTTP client can trigger this by connecting and immediately closing the connection. No authentication, authorization, or special privileges are required.

2. **Low Resource Requirements:** Only 100 cancelled requests needed to fully degrade the endpoint using the default configuration.

3. **Public Availability:** The endpoint is publicly documented in the Aptos REST API specification and is accessible on all public nodes.

4. **Persistent Impact:** Once exploited, the degradation persists indefinitely until manual node restart.

5. **Trivial Tooling:** Standard tools like curl, wget, or any HTTP library can trigger the vulnerability by closing connections during the request.

6. **No Detection:** The attack leaves minimal traces and appears as normal client behavior (connections timing out or being cancelled).

## Recommendation

Implement async cancellation safety using an RAII guard pattern. The guard should automatically decrement the counter when dropped, regardless of how the async operation completes:

```rust
struct ConnectionGuard {
    counter: Arc<AtomicUsize>,
}

impl ConnectionGuard {
    fn new(counter: Arc<AtomicUsize>) -> Self {
        counter.fetch_add(1, Ordering::Relaxed);
        Self { counter }
    }
}

impl Drop for ConnectionGuard {
    fn drop(&mut self) {
        self.counter.fetch_sub(1, Ordering::Relaxed);
    }
}
```

Then modify `wait_transaction_by_hash()` to use the guard:

```rust
async fn wait_transaction_by_hash(...) -> BasicResultWith404<Transaction> {
    // Check limit before creating guard
    if self.context.wait_for_hash_active_connections.load(Ordering::Relaxed) 
        >= self.context.node_config.api.wait_by_hash_max_active_connections {
        // Short poll path
        return self.get_transaction_by_hash_inner(...).await;
    }
    
    // Guard automatically decrements on drop (normal return or cancellation)
    let _guard = ConnectionGuard::new(
        self.context.wait_for_hash_active_connections.clone()
    );
    
    let start_time = std::time::Instant::now();
    WAIT_TRANSACTION_GAUGE.inc();
    
    let result = self.wait_transaction_by_hash_inner(...).await;
    
    WAIT_TRANSACTION_GAUGE.dec();
    WAIT_TRANSACTION_POLL_TIME.with_label_values(&["long"])
        .observe(start_time.elapsed().as_secs_f64());
    result
}
```

This ensures the counter is always decremented, even if the Future is dropped due to client disconnection.

## Proof of Concept

```rust
#[tokio::test]
async fn test_connection_counter_leak_on_cancellation() {
    use tokio::time::{sleep, Duration};
    use std::sync::Arc;
    use std::sync::atomic::{AtomicUsize, Ordering};
    
    let counter = Arc::new(AtomicUsize::new(0));
    let max_connections = 3;
    
    // Simulate 3 cancelled requests
    for i in 0..max_connections {
        let counter_clone = counter.clone();
        let handle = tokio::spawn(async move {
            // Simulate the vulnerable pattern
            counter_clone.fetch_add(1, Ordering::Relaxed);
            
            // Simulate long-running operation
            sleep(Duration::from_secs(10)).await;
            
            // This cleanup never executes if task is cancelled
            counter_clone.fetch_sub(1, Ordering::Relaxed);
        });
        
        // Wait a bit then cancel (simulating client disconnect)
        sleep(Duration::from_millis(10)).await;
        handle.abort();
        
        println!("After disconnect {}: counter = {}", i + 1, counter.load(Ordering::Relaxed));
    }
    
    // Verify counter has leaked
    assert_eq!(counter.load(Ordering::Relaxed), max_connections);
    println!("Counter leaked to: {}", counter.load(Ordering::Relaxed));
    
    // This would now fail the limit check
    let current = counter.fetch_add(1, Ordering::Relaxed);
    assert!(current >= max_connections, "Limit check would trigger degradation");
}
```

This test demonstrates that cancelled async tasks leak the counter, causing the endpoint to eventually hit its connection limit and degrade functionality.

## Notes

The technical analysis confirms this is a legitimate async cancellation safety vulnerability in the Aptos REST API layer. While the report slightly overstates the impact (the endpoint degrades rather than becoming completely non-functional), the core vulnerability is valid and exploitable. The leaked counter persists across all requests and requires node restart to clear, qualifying as a Medium severity issue requiring manual intervention.

### Citations

**File:** api/src/transactions.rs (L240-248)
```rust
        if self
            .context
            .wait_for_hash_active_connections
            .fetch_add(1, std::sync::atomic::Ordering::Relaxed)
            >= self
                .context
                .node_config
                .api
                .wait_by_hash_max_active_connections
```

**File:** api/src/transactions.rs (L250-258)
```rust
            self.context
                .wait_for_hash_active_connections
                .fetch_sub(1, std::sync::atomic::Ordering::Relaxed);
            metrics::WAIT_TRANSACTION_POLL_TIME
                .with_label_values(&["short"])
                .observe(0.0);
            return self
                .get_transaction_by_hash_inner(&accept_type, txn_hash.0)
                .await;
```

**File:** api/src/transactions.rs (L264-271)
```rust
        let result = self
            .wait_transaction_by_hash_inner(
                &accept_type,
                txn_hash.0,
                self.context.node_config.api.wait_by_hash_timeout_ms,
                self.context.node_config.api.wait_by_hash_poll_interval_ms,
            )
            .await;
```

**File:** api/src/transactions.rs (L273-276)
```rust
        WAIT_TRANSACTION_GAUGE.dec();
        self.context
            .wait_for_hash_active_connections
            .fetch_sub(1, std::sync::atomic::Ordering::Relaxed);
```

**File:** api/src/transactions.rs (L893-940)
```rust
    async fn wait_transaction_by_hash_inner(
        &self,
        accept_type: &AcceptType,
        hash: HashValue,
        wait_by_hash_timeout_ms: u64,
        wait_by_hash_poll_interval_ms: u64,
    ) -> BasicResultWith404<Transaction> {
        let start_time = std::time::Instant::now();
        loop {
            let context = self.context.clone();
            let accept_type = accept_type.clone();

            let (internal_ledger_info_opt, storage_ledger_info) =
                api_spawn_blocking(move || context.get_latest_internal_and_storage_ledger_info())
                    .await?;
            let storage_version = storage_ledger_info.ledger_version.into();
            let internal_ledger_version = internal_ledger_info_opt
                .as_ref()
                .map(|info| info.ledger_version.into());
            let latest_ledger_info = internal_ledger_info_opt.unwrap_or(storage_ledger_info);
            let txn_data = self
                .get_by_hash(hash.into(), storage_version, internal_ledger_version)
                .await
                .context(format!("Failed to get transaction by hash {}", hash))
                .map_err(|err| {
                    BasicErrorWith404::internal_with_code(
                        err,
                        AptosErrorCode::InternalError,
                        &latest_ledger_info,
                    )
                })?
                .context(format!("Failed to find transaction with hash: {}", hash))
                .map_err(|_| transaction_not_found_by_hash(hash, &latest_ledger_info))?;

            if matches!(txn_data, TransactionData::Pending(_))
                && (start_time.elapsed().as_millis() as u64) < wait_by_hash_timeout_ms
            {
                tokio::time::sleep(Duration::from_millis(wait_by_hash_poll_interval_ms)).await;
                continue;
            }

            let api = self.clone();
            return api_spawn_blocking(move || {
                api.get_transaction_inner(&accept_type, txn_data, &latest_ledger_info)
            })
            .await;
        }
    }
```

**File:** api/src/context.rs (L84-84)
```rust
    pub wait_for_hash_active_connections: Arc<AtomicUsize>,
```

**File:** api/src/context.rs (L136-136)
```rust
            wait_for_hash_active_connections: Arc::new(AtomicUsize::new(0)),
```

**File:** config/src/config/api_config.rs (L144-144)
```rust
            wait_by_hash_max_active_connections: 100,
```
