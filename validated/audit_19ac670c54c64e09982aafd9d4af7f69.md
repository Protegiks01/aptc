# Audit Report

## Title
Shard Pruning Atomicity Violation Causing Node Startup Failure and Inconsistent Pruning State

## Summary
The StateKV pruner in sharded mode commits global pruning progress to the metadata database before all individual shard pruners complete their work. When a shard pruner fails due to database corruption or I/O errors, the global progress is already updated while some shards remain unpruned. On node restart, the catch-up mechanism fails with a panic if shard corruption persists, rendering the node permanently unavailable without manual intervention.

## Finding Description

The vulnerability exists in the pruning orchestration logic where metadata updates and actual shard deletions are not atomic across the two-phase execution flow.

The execution sequence is:

**Phase 1 - Metadata Update**: The `metadata_pruner.prune()` method is called first. When sharding is enabled, this method iterates through shards without performing actual deletions, then commits the global `StateKvPrunerProgress` to the metadata database. [1](#0-0) 

In the metadata pruner implementation, when sharding is enabled, the function only iterates through shard indices and then commits the global progress: [2](#0-1) 

The critical commit happens at the end of this method: [3](#0-2) 

**Phase 2 - Shard Pruning**: After the metadata is committed, shard pruners execute in parallel. Each shard performs actual deletions and updates its individual progress: [4](#0-3) 

**Failure Scenario**: If any shard pruner fails during parallel execution (disk corruption, I/O error, RocksDB error), the `try_for_each` operation aborts via the `?` operator, but the global metadata progress has already been committed in Phase 1.

**State After Partial Failure:**
- Global `StateKvPrunerProgress` in metadata DB = target version (COMMITTED)
- Successful shards' progress = target version (COMMITTED)
- Failed shard's progress = old version (NOT UPDATED)
- In-memory atomic progress = old version (lines 80-81 never reached)

**Node Restart Catastrophe:**

On restart, the initialization attempts to catch up all shards to the metadata progress: [5](#0-4) 

For each shard, `StateKvShardPruner::new()` attempts to catch up by calling `prune()`: [6](#0-5) 

If the corrupted shard still fails during catch-up (line 42), the error propagates through the `?` operator, causing `StateKvShardPruner::new()` to fail, which in turn causes `StateKvPruner::new()` to fail.

The initialization code uses `.expect()` which panics on any error: [7](#0-6) 

**Result**: The node cannot start and requires manual database recovery.

This breaks state consistency invariants as the pruning state across shards is not consistent, and the system cannot recover automatically.

## Impact Explanation

This qualifies as **HIGH Severity** per Aptos bug bounty criteria:

1. **Validator node unavailability**: When a shard has persistent corruption, the node panics during initialization and cannot restart. This directly maps to "Validator node slowdowns" and availability issues in the HIGH severity category (up to $50,000).

2. **State inconsistencies requiring intervention**: The inconsistent pruning state across shards violates data integrity guarantees and requires manual database recovery, potentially database restoration from backup.

3. **Storage bloat**: Failed shards accumulate unpruned stale data indefinitely, potentially leading to resource exhaustion over time.

The impact affects:
- **All validator nodes** running with sharding enabled (which is the production configuration - sharding defaults to enabled and is required for testnet/mainnet) [8](#0-7) 
- Requires operator intervention and potential database restoration from backup
- Can lead to prolonged node downtime during critical network operations

## Likelihood Explanation

**HIGH Likelihood** - This will occur naturally during normal operational failures:

1. **Disk failures**: Hardware failures are common in long-running distributed systems
2. **I/O errors**: Transient or persistent I/O errors during database operations
3. **Database corruption**: Power failures, filesystem issues, or RocksDB internal errors
4. **Storage exhaustion**: When disk fills up during pruning operations

The vulnerability requires **NO attacker action** - it's triggered by normal operational failures that occur in production systems. Given that Aptos validators run continuously and perform regular pruning operations, encountering this scenario is inevitable over time.

The catch-up logic at startup makes recovery impossible without manual intervention, as the node will panic every time it attempts to start until the database corruption is manually resolved.

## Recommendation

Implement atomic batch operations that ensure metadata progress is only committed after all shard pruners successfully complete:

```rust
// In StateKvPruner::prune() method
pub fn prune(&self, max_versions: usize) -> Result<Version> {
    // ... existing setup code ...
    
    while progress < target_version {
        let current_batch_target_version = 
            min(progress + max_versions as Version, target_version);
            
        // FIRST: Prune all shards (without committing individual progress yet)
        THREAD_MANAGER.get_background_pool().install(|| {
            self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                shard_pruner.prune_without_commit(progress, current_batch_target_version)
            })
        })?;
        
        // SECOND: Only if all shards succeeded, commit everything atomically
        // This includes metadata progress AND all shard progress markers
        self.commit_pruning_progress(current_batch_target_version)?;
        
        progress = current_batch_target_version;
        self.record_progress(progress);
    }
    
    Ok(target_version)
}
```

Additionally, implement graceful recovery logic that can detect and handle inconsistent pruning state during initialization, rather than panicking.

## Proof of Concept

The vulnerability can be demonstrated through the following scenario:

1. Start a validator node with sharding enabled (default configuration)
2. During normal pruning operations, simulate a shard database corruption or I/O error (e.g., by making one shard read-only or corrupting its data)
3. Observe that the metadata pruner commits global progress successfully
4. The shard pruner fails during parallel execution
5. The node continues running with inconsistent state
6. Restart the node
7. Observe the node panic during `StateKvPruner::new()` initialization with error propagating from `StateKvShardPruner::new()`

The node will be unable to start until manual database intervention (repair, restore from backup, or manual progress reset) is performed.

**Note**: A complete Rust test PoC would require mocking RocksDB failures, which is complex but the logic flow is directly verifiable in the cited code sections.

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L64-65)
```rust
            self.metadata_pruner
                .prune(progress, current_batch_target_version)?;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L67-78)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state kv shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L124-133)
```rust
        let shard_pruners = if state_kv_db.enabled_sharding() {
            let num_shards = state_kv_db.num_shards();
            let mut shard_pruners = Vec::with_capacity(num_shards);
            for shard_id in 0..num_shards {
                shard_pruners.push(StateKvShardPruner::new(
                    shard_id,
                    state_kv_db.db_shard_arc(shard_id),
                    metadata_progress,
                )?);
            }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs (L35-50)
```rust
        if self.state_kv_db.enabled_sharding() {
            let num_shards = self.state_kv_db.num_shards();
            // NOTE: This can be done in parallel if it becomes the bottleneck.
            for shard_id in 0..num_shards {
                let mut iter = self
                    .state_kv_db
                    .db_shard(shard_id)
                    .iter::<StaleStateValueIndexByKeyHashSchema>()?;
                iter.seek(&current_progress)?;
                for item in iter {
                    let (index, _) = item?;
                    if index.stale_since_version > target_version {
                        break;
                    }
                }
            }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs (L67-72)
```rust
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        self.state_kv_db.metadata_db().write_schemas(batch)
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L25-44)
```rust
    pub(in crate::pruner) fn new(
        shard_id: usize,
        db_shard: Arc<DB>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
            metadata_progress,
        )?;
        let myself = Self { shard_id, db_shard };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up state kv shard {shard_id}."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_pruner_manager.rs (L114-115)
```rust
        let pruner =
            Arc::new(StateKvPruner::new(state_kv_db).expect("Failed to create state kv pruner."));
```

**File:** config/src/config/storage_config.rs (L233-233)
```rust
            enable_storage_sharding: true,
```
