# Audit Report

## Title
Race Condition in PartialStateComputeResult OnceCell Initialization Causes Validator Node Panic on Duplicate Block Insertion

## Summary
Multiple validator threads can concurrently execute ledger update for the same block due to inadequate synchronization in the consensus pipeline's duplicate block handling, causing a panic when racing to initialize OnceCell fields in `PartialStateComputeResult`. This results in validator node crashes and network availability degradation.

## Finding Description

The vulnerability exists in a race condition chain spanning the consensus and execution layers:

**1. Duplicate Block Insertion is Expected Behavior**

The consensus layer explicitly documents that duplicate block insertion is a valid non-error case when a validator receives a certificate for a block currently being added. [1](#0-0) 

The RoundManager explicitly acknowledges delayed proposal processing may retry block insertion, which is acceptable as the operation is intended to be idempotent. [2](#0-1) 

**2. Pipeline Construction Precedes Duplicate Detection**

The initial block existence check uses an unsynchronized read operation. [3](#0-2) 

The pipeline is built in `insert_block_inner` before the actual duplicate detection occurs. [4](#0-3) 

Duplicate detection only happens when inserting into the BlockTree. [5](#0-4) 

The BlockTree's `insert_block` method checks for and returns existing blocks. [6](#0-5) 

**3. Pipeline Futures Spawn Independent Tasks**

The `set_pipeline_futs` method stores pipeline futures without synchronization. [7](#0-6) 

The `spawn_shared_fut` function immediately spawns independent async tasks using `tokio::spawn`, which continue running even if the PipelinedBlock is discarded. [8](#0-7) 

The ledger_update future is spawned as part of the pipeline construction. [9](#0-8) 

**4. Ledger Update Allows Concurrent Access**

The `BlockExecutor::ledger_update` method takes `&self` and does NOT acquire the execution_lock, permitting concurrent execution. [10](#0-9) 

Note that only `execute_and_update_state` acquires the execution_lock. [11](#0-10) 

Both concurrent threads fetch blocks from the BlockTree using `get_blocks_opt`. [12](#0-11) 

The BlockLookup returns `Arc<Block>` instances that are shared between threads. [13](#0-12) 

**5. TOCTOU Vulnerability in Completion Check**

The check for existing results is not atomic with the subsequent OnceCell initialization. [14](#0-13) 

Both threads can pass this check before either completes, then proceed to set the OnceCell fields. [15](#0-14) 

**6. OnceCell Panic on Duplicate Initialization**

The state_checkpoint_output OnceCell panics with `.expect()` when set twice. [16](#0-15) 

The ledger_update_output OnceCell panics with `.expect()` when set twice. [17](#0-16) 

## Impact Explanation

**High Severity** - This vulnerability causes validator node crashes, meeting the "API crashes" criteria from the Aptos bug bounty High Severity category (up to $50,000).

**Availability Impact:**
- Validator node crashes immediately upon panic
- Node must be restarted to resume operation  
- Reduces network validator capacity
- If multiple validators are affected simultaneously, could degrade network liveness

**Why Not Critical:**
- No consensus safety violation (both threads compute identical deterministic results)
- No funds at risk
- No permanent network partition (nodes can restart)
- No state corruption (values computed are correct, only the panic causes the crash)

**Affected Invariant:**
Violates the network availability guarantee by causing validator node crashes through a race condition in normal protocol operation.

## Likelihood Explanation

**Medium-High Likelihood:**

This vulnerability can occur during normal network operation when:
- Validators broadcast certificates for the same block
- Network delays cause duplicate block messages
- High consensus round activity increases concurrent processing
- Delayed proposal processing (explicitly supported) retries block insertion

The vulnerability requires:
- **No attacker privileges** - Any network peer can send blocks/certificates
- **Normal protocol behavior** - Duplicate block delivery is explicitly expected per code comments
- **Timing overlap** - Two inserts must overlap in the critical window between the check at line 413 and the insert at line 515

The likelihood increases with:
- Network congestion or delays
- High transaction throughput
- Multiple validators proposing blocks
- Byzantine actors deliberately sending duplicate messages to exploit the race window

The code comments explicitly acknowledging duplicate inserts as normal indicates this is a realistic scenario that developers anticipated but incompletely protected against.

## Recommendation

Implement one of the following mitigations:

**Option 1: Check-and-Set with OnceCell::get_or_init()**
Replace the `.expect()` calls with `.get_or_init()` or `.get_or_try_init()` to make the operation idempotent:

```rust
output.state_checkpoint_output.get_or_init(|| {
    DoStateCheckpoint::run(...)
});
```

**Option 2: Acquire a Lock Before ledger_update**
Extend the execution_lock to cover ledger_update operations, or introduce a per-block lock to serialize ledger_update calls for the same block_id.

**Option 3: Early Return After Pipeline Build**
Check if the block already exists immediately after building the pipeline, and abort the spawned tasks if a duplicate is detected before the BlockTree insertion.

**Option 4: Use Try-Set Pattern**
Check the OnceCell::set() result and silently ignore the error if already set, since both threads compute the same deterministic result.

## Proof of Concept

The vulnerability can be triggered by:

1. Setting up a test network with multiple validator nodes
2. Sending duplicate block proposals or certificates to validators during high load
3. Introducing artificial network delays to widen the race window
4. Monitoring validator logs for the panic message: "StateCheckpointOutput already set" or "LedgerUpdateOutput already set"
5. Observing validator node crashes and required restarts

A Rust test demonstrating the race condition would require spawning multiple threads that call `BlockStore::insert_block` concurrently for the same block_id during the critical timing window.

### Citations

**File:** consensus/src/block_storage/block_store.rs (L413-415)
```rust
        if let Some(existing_block) = self.get_block(block.id()) {
            return Ok(existing_block);
        }
```

**File:** consensus/src/block_storage/block_store.rs (L445-447)
```rust
    /// Duplicate inserts will return the previously inserted block (
    /// note that it is considered a valid non-error case, for example, it can happen if a validator
    /// receives a certificate for a block that is currently being added).
```

**File:** consensus/src/block_storage/block_store.rs (L464-496)
```rust
        if let Some(pipeline_builder) = &self.pipeline_builder {
            let parent_block = self
                .get_block(pipelined_block.parent_id())
                .ok_or_else(|| anyhow::anyhow!("Parent block not found"))?;

            // need weak pointer to break the cycle between block tree -> pipeline block -> callback
            let block_tree = Arc::downgrade(&self.inner);
            let storage = self.storage.clone();
            let id = pipelined_block.id();
            let round = pipelined_block.round();
            let window_size = self.window_size;
            let callback = Box::new(
                move |finality_proof: WrappedLedgerInfo,
                      commit_decision: LedgerInfoWithSignatures| {
                    if let Some(tree) = block_tree.upgrade() {
                        tree.write().commit_callback(
                            storage,
                            id,
                            round,
                            finality_proof,
                            commit_decision,
                            window_size,
                        );
                    }
                },
            );
            pipeline_builder.build_for_consensus(
                &pipelined_block,
                parent_block.pipeline_futs().ok_or_else(|| {
                    anyhow::anyhow!("Parent future doesn't exist, potentially epoch ended")
                })?,
                callback,
            );
```

**File:** consensus/src/block_storage/block_store.rs (L515-515)
```rust
        self.inner.write().insert_block(pipelined_block)
```

**File:** consensus/src/round_manager.rs (L1248-1260)
```rust
        // Since processing proposal is delayed due to backpressure or payload availability, we add
        // the block to the block store so that we don't need to fetch it from remote once we
        // are out of the backpressure. Please note that delayed processing of proposal is not
        // guaranteed to add the block to the block store if we don't get out of the backpressure
        // before the timeout, so this is needed to ensure that the proposed block is added to
        // the block store irrespective. Also, it is possible that delayed processing of proposal
        // tries to add the same block again, which is okay as `insert_block` call
        // is idempotent.
        self.block_store
            .insert_block(proposal.clone())
            .await
            .context("[RoundManager] Failed to insert the block into BlockStore")?;

```

**File:** consensus/src/block_storage/block_tree.rs (L312-317)
```rust
        if let Some(existing_block) = self.get_block(&block_id) {
            debug!("Already had block {:?} for id {:?} when trying to add another block {:?} for the same id",
                       existing_block,
                       block_id,
                       block);
            Ok(existing_block)
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L512-514)
```rust
    pub fn set_pipeline_futs(&self, pipeline_futures: PipelineFutures) {
        *self.pipeline_futs.lock() = Some(pipeline_futures);
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L151-151)
```rust
    let join_handle = tokio::spawn(f);
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L874-921)
```rust
    async fn ledger_update(
        rand_check: TaskFuture<RandResult>,
        execute_fut: TaskFuture<ExecuteResult>,
        parent_block_ledger_update_fut: TaskFuture<LedgerUpdateResult>,
        executor: Arc<dyn BlockExecutorTrait>,
        block: Arc<Block>,
    ) -> TaskResult<LedgerUpdateResult> {
        let mut tracker = Tracker::start_waiting("ledger_update", &block);
        let (_, _, prev_epoch_end_timestamp) = parent_block_ledger_update_fut.await?;
        let execution_time = execute_fut.await?;

        tracker.start_working();
        let block_clone = block.clone();
        let result = tokio::task::spawn_blocking(move || {
            executor
                .ledger_update(block_clone.id(), block_clone.parent_id())
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
        let timestamp = block.timestamp_usecs();
        observe_block(timestamp, BlockStage::EXECUTED);
        let epoch_end_timestamp =
            if result.has_reconfiguration() && !result.compute_status_for_input_txns().is_empty() {
                Some(timestamp)
            } else {
                prev_epoch_end_timestamp
            };
        // check for randomness consistency
        let (_, has_randomness) = rand_check.await?;
        if !has_randomness {
            let mut label = "consistent";
            for event in result.execution_output.subscribable_events.get(None) {
                if event.type_tag() == RANDOMNESS_GENERATED_EVENT_MOVE_TYPE_TAG.deref() {
                    error!(
                            "[Pipeline] Block {} {} {} generated randomness event without has_randomness being true!",
                            block.id(),
                            block.epoch(),
                            block.round()
                        );
                    label = "inconsistent";
                    break;
                }
            }
            counters::RAND_BLOCK.with_label_values(&[label]).inc();
        }
        Ok((result, execution_time, epoch_end_timestamp))
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L106-107)
```rust
        // guarantee only one block being executed at a time
        let _guard = self.execution_lock.lock();
```

**File:** execution/executor/src/block_executor/mod.rs (L115-129)
```rust
    fn ledger_update(
        &self,
        block_id: HashValue,
        parent_block_id: HashValue,
    ) -> ExecutorResult<StateComputeResult> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "ledger_update"]);

        self.inner
            .read()
            .as_ref()
            .ok_or_else(|| ExecutorError::InternalError {
                error: "BlockExecutor is not reset".into(),
            })?
            .ledger_update(block_id, parent_block_id)
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L271-273)
```rust
        let mut block_vec = self
            .block_tree
            .get_blocks_opt(&[block_id, parent_block_id])?;
```

**File:** execution/executor/src/block_executor/mod.rs (L291-294)
```rust
        if let Some(complete_result) = block.output.get_complete_result() {
            info!(block_id = block_id, "ledger_update already done.");
            return Ok(complete_result);
        }
```

**File:** execution/executor/src/block_executor/mod.rs (L315-328)
```rust
                output.set_state_checkpoint_output(DoStateCheckpoint::run(
                    &output.execution_output,
                    parent_block.output.ensure_result_state_summary()?,
                    &ProvableStateSummary::new_persisted(self.db.reader.as_ref())?,
                    None,
                )?);
                output.set_ledger_update_output(DoLedgerUpdate::run(
                    &output.execution_output,
                    output.ensure_state_checkpoint_output()?,
                    parent_out
                        .ensure_ledger_update_output()?
                        .transaction_accumulator
                        .clone(),
                )?);
```

**File:** execution/executor/src/block_executor/block_tree/mod.rs (L110-115)
```rust
            Entry::Occupied(entry) => {
                let existing = entry
                    .get()
                    .upgrade()
                    .ok_or_else(|| anyhow!("block dropped unexpected."))?;
                Ok((existing, true, parent_block))
```

**File:** execution/executor/src/types/partial_state_compute_result.rs (L76-80)
```rust
    pub fn set_state_checkpoint_output(&self, state_checkpoint_output: StateCheckpointOutput) {
        self.state_checkpoint_output
            .set(state_checkpoint_output)
            .expect("StateCheckpointOutput already set");
    }
```

**File:** execution/executor/src/types/partial_state_compute_result.rs (L88-92)
```rust
    pub fn set_ledger_update_output(&self, ledger_update_output: LedgerUpdateOutput) {
        self.ledger_update_output
            .set(ledger_update_output)
            .expect("LedgerUpdateOutput already set");
    }
```
