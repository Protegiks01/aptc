# Audit Report

## Title
Irrecoverable Node Liveness Failure Due to Partial State KV Pruner Failure and Aggressive Recovery Mechanism

## Summary
When `StateKvPruner::prune()` partially fails (metadata pruner succeeds but shard pruner fails), the system creates an inconsistent state between the metadata database and shard databases. On node restart, the mandatory catch-up mechanism causes permanent node failure through panic if the underlying issue persists, violating the liveness invariant.

## Finding Description

The vulnerability exists in the state KV pruning mechanism when sharding is enabled (which is **mandatory** for all mainnet and testnet validators). [1](#0-0) 

The pruning operation executes in two sequential phases that can result in partial failure:

**Phase 1: Metadata Pruning** - The metadata pruner commits `DbMetadataKey::StateKvPrunerProgress` to the target version atomically to disk. [2](#0-1)  This occurs in `StateKvMetadataPruner::prune()` before any shard pruning begins. [3](#0-2) 

**Phase 2: Shard Pruning** - After metadata commits, shard pruners execute in parallel. [4](#0-3)  If any shard pruner fails, the entire operation returns an error without updating the in-memory progress tracker. [5](#0-4) 

**Runtime Behavior:** During normal operation, the pruner worker catches errors and retries indefinitely using the in-memory progress. [6](#0-5)  This works as long as the node keeps running.

**Critical Failure on Restart:** When the node restarts, `StateKvPruner::new()` loads the metadata progress from disk. [7](#0-6)  For each shard, it creates a `StateKvShardPruner` which must catch up from its persisted progress to the metadata progress. [8](#0-7) 

If this catch-up fails due to persistent underlying issues (disk corruption in specific version range, I/O errors, hardware failures), the error propagates to the manager which panics with `.expect("Failed to create state kv pruner.")`. [9](#0-8) 

This creates an irrecoverable state where:
1. Metadata DB shows `StateKvPrunerProgress = N` (advanced and committed)
2. Failed shard DB shows `StateKvShardPrunerProgress(shard_id) = M` (M < N, not updated)
3. Unpruned data exists in shard from versions [M, N)
4. On restart, catch-up from M to N fails persistently
5. Node cannot start due to panic, requiring manual database intervention

## Impact Explanation

This qualifies as **High Severity** for the following reasons:

**Validator Unavailability:** A validator node experiencing persistent shard pruning failures becomes permanently unable to restart, removing it from consensus participation until manual database intervention occurs. This exceeds typical "Validator Node Slowdowns" as the node is completely non-functional and cannot be recovered through normal restart procedures.

**No Automatic Recovery:** The `.expect()` panic provides zero graceful degradation. Unlike runtime errors that can be retried indefinitely by the pruner worker, the startup panic creates a binary failure state with no automated recovery path.

**Mandatory Sharding:** Storage sharding is enforced for all mainnet and testnet validators - the node panics on startup if not enabled. [1](#0-0)  This means ALL production validators are potentially affected by this vulnerability.

**State Inconsistency:** The partial commitment creates a database state that violates pruning invariants - metadata claims data is pruned while shards retain it, creating an inconsistent view that the system cannot recover from automatically.

## Likelihood Explanation

**Moderate Likelihood** in production validator environments:

**Trigger Conditions:** Any I/O error, disk corruption, permission issue, or hardware failure during the critical window between metadata commit and shard commits creates the inconsistent state. The metadata pruner commits first, then parallel shard pruning occurs.

**Persistence:** If the underlying cause affects a specific version range (e.g., corrupted data at versions M to N on one shard), the issue persists across restarts. The catch-up mechanism will repeatedly fail on the same corrupted data during initialization.

**Production Realities:** 
- Large-scale deployments routinely experience disk failures
- I/O timeouts under high load are common
- Unclean shutdowns can corrupt specific database regions
- Filesystem issues can affect specific shards

**No Graceful Path:** Unlike runtime errors that retry indefinitely, startup failures are fatal. Operators cannot "wait out" transient issues - the node simply won't start.

## Recommendation

Implement graceful error handling for shard pruner initialization:

1. **Atomic Commitment**: Either commit all pruner progress atomically, or reverse the order (commit shards first, then metadata).

2. **Graceful Initialization**: Replace the `.expect()` panic with proper error handling that allows the node to start in a degraded state:
   - Log the initialization failure
   - Mark affected shards for manual recovery
   - Allow the node to start with pruning disabled until manual intervention

3. **Retry with Backoff**: Implement retry logic during initialization with exponential backoff before failing completely.

4. **Progress Validation**: On startup, validate that all shard progress values are consistent with metadata progress, and handle mismatches gracefully.

5. **Manual Recovery Tool**: Provide a database repair utility to reset inconsistent pruner progress states.

## Proof of Concept

While this vulnerability requires specific environmental conditions (I/O errors during pruning), the logic flaw is evident from the code structure. The non-atomic commitment design combined with panic-on-failure initialization creates a clear failure mode that violates fault-tolerance principles.

A PoC would require simulating disk I/O failures during the critical window, which depends on the specific storage infrastructure. However, the code path analysis demonstrates the vulnerability exists in the design itself.

## Notes

This is a **logic vulnerability** in the pruner design:
- Non-atomic operation across metadata and shard databases
- Lack of graceful error handling during initialization  
- Mandatory feature (sharding) for all production validators
- Violates fault-tolerance and atomicity principles

The vulnerability can be triggered by environmental factors (I/O failures) but represents a fundamental design flaw that should be addressed regardless of trigger probability.

### Citations

**File:** config/src/config/storage_config.rs (L664-668)
```rust
            if (chain_id.is_testnet() || chain_id.is_mainnet())
                && config_yaml["rocksdb_configs"]["enable_storage_sharding"].as_bool() != Some(true)
            {
                panic!("Storage sharding (AIP-97) is not enabled in node config. Please follow the guide to migration your node, and set storage.rocksdb_configs.enable_storage_sharding to true explicitly in your node config. https://aptoslabs.notion.site/DB-Sharding-Migration-Public-Full-Nodes-1978b846eb7280b29f17ceee7d480730");
            }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs (L67-72)
```rust
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        self.state_kv_db.metadata_db().write_schemas(batch)
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L64-65)
```rust
            self.metadata_pruner
                .prune(progress, current_batch_target_version)?;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L67-78)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state kv shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L80-81)
```rust
            progress = current_batch_target_version;
            self.record_progress(progress);
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L117-122)
```rust
        let metadata_progress = metadata_pruner.progress()?;

        info!(
            metadata_progress = metadata_progress,
            "Created state kv metadata pruner, start catching up all shards."
        );
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L53-69)
```rust
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L30-42)
```rust
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
            metadata_progress,
        )?;
        let myself = Self { shard_id, db_shard };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up state kv shard {shard_id}."
        );
        myself.prune(progress, metadata_progress)?;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_pruner_manager.rs (L114-116)
```rust
        let pruner =
            Arc::new(StateKvPruner::new(state_kv_db).expect("Failed to create state kv pruner."));

```
