# Audit Report

## Title
Validator Node Crash via Consensus Observer Configuration Bypass Leading to Concurrent Database Commits

## Summary
The `ConsensusObserverConfig::optimize()` function fails to validate that `observer_enabled` is disabled for validator nodes. When a validator is deployed with a partial configuration containing `observer_enabled: true`, the optimization logic enables `publisher_enabled` without disabling `observer_enabled`, resulting in both the consensus runtime and consensus observer runtime executing simultaneously. This causes concurrent database commits that trigger a panic with "Concurrent committing detected," crashing the validator node.

## Finding Description

The vulnerability exists in the configuration optimization logic for consensus observer settings. The `ConsensusObserverConfig` struct uses the `#[serde(default)]` attribute, which causes serde to fill missing fields with default values during deserialization. [1](#0-0) 

The optimization function checks whether fields were manually set by examining if they exist in the original YAML configuration: [2](#0-1) 

For validator nodes, the function only checks if `publisher_enabled` was manually set and enables it if not: [3](#0-2) 

**Critical flaw:** The logic does NOT check or override `observer_enabled` for validators. If an operator provides a config file with only `observer_enabled: true`, the final state becomes:
- `observer_enabled = true` (from user config)
- `publisher_enabled = true` (from optimizer)

During node startup, both runtimes are created. The consensus observer and publisher creation happens first: [4](#0-3) 

Then the consensus runtime is created: [5](#0-4) 

When `observer_enabled` is true, the consensus observer creates its own execution client with a `BlockExecutor`: [6](#0-5) 

The normal consensus runtime also creates an execution client with its own `BlockExecutor`: [7](#0-6) 

Each `BlockExecutor` instance has its own `execution_lock`: [8](#0-7) 

However, both `BlockExecutor` instances share the same underlying `AptosDB` database. When both runtimes attempt to commit blocks simultaneously, they encounter AptosDB's concurrency detection mechanism in `pre_commit_ledger`: [9](#0-8) 

And in `commit_ledger`: [10](#0-9) 

The `try_lock().expect()` pattern is designed to detect concurrent commits and panic immediately rather than allowing state corruption. When the second runtime attempts to acquire the lock, the `try_lock()` returns `None`, and the `expect()` panics with "Concurrent committing detected."

The consensus observer actively processes blocks by calling `execution_client.finalize_order()`: [11](#0-10) 

## Impact Explanation

This is a **HIGH severity** vulnerability per the Aptos bug bounty criteria:

1. **Validator Node Crash**: The panic from "Concurrent committing detected" causes immediate validator node termination, meeting the "Validator Node Slowdowns (High)" criteria (though this is actually a complete crash, not just a slowdown)

2. **Network Liveness Impact**: If multiple validators are misconfigured this way, the network loses consensus participants, potentially falling below the 2/3 threshold needed for progress

3. **Availability Impact**: Crashed validators cannot participate in consensus, reducing network throughput and increasing latency

The vulnerability qualifies for the High severity tier ($50,000) as it causes complete validator failure.

## Likelihood Explanation

**Likelihood: Medium to High**

- **Easy to Trigger**: Requires only a partial configuration file with `observer_enabled: true`
- **Common Pattern**: Operators frequently use partial configs and rely on defaults, especially when following example configurations or migrating configurations
- **No Malicious Intent Required**: This can happen accidentally during deployment or configuration updates
- **Persistent**: Once deployed, the misconfiguration persists until manually corrected
- **Detection Difficulty**: The issue may not manifest immediately if consensus and observer don't attempt concurrent commits initially

The vulnerability is more likely to occur during:
- Initial validator deployments using incomplete configs
- Configuration migrations when enabling consensus observer features
- Copy-paste errors from fullnode configs to validator configs

## Recommendation

Fix the optimization logic to explicitly disable `observer_enabled` for validators:

```rust
NodeType::Validator => {
    if ENABLE_ON_VALIDATORS {
        if !publisher_manually_set {
            // Only enable the publisher for validators
            consensus_observer_config.publisher_enabled = true;
            modified_config = true;
        }
        // Explicitly disable observer for validators to prevent concurrent execution
        if !observer_manually_set {
            consensus_observer_config.observer_enabled = false;
            modified_config = true;
        }
    }
},
```

Alternatively, check both flags before enabling publisher:

```rust
NodeType::Validator => {
    if ENABLE_ON_VALIDATORS && !publisher_manually_set && !observer_manually_set {
        // Only enable the publisher for validators
        consensus_observer_config.publisher_enabled = true;
        modified_config = true;
    }
},
```

## Proof of Concept

To trigger this vulnerability, create a validator configuration file with:

```yaml
consensus_observer:
  observer_enabled: true
```

Upon startup, the optimizer will set `publisher_enabled: true` without disabling `observer_enabled`, resulting in both being true. This causes both consensus runtime and consensus observer runtime to start, creating separate `BlockExecutor` instances that share the same `AptosDB`. When both attempt to commit blocks, the database's concurrency detection will panic with "Concurrent committing detected," crashing the validator node.

## Notes

This vulnerability demonstrates a critical gap in the configuration optimization logic where the mutual exclusivity of validator consensus runtime and consensus observer runtime is not enforced. The AptosDB's panic-on-concurrent-commit design correctly prevents state corruption, but the configuration layer should prevent this scenario from occurring in the first place.

### Citations

**File:** config/src/config/consensus_observer_config.rs (L19-25)
```rust
#[derive(Clone, Copy, Debug, Deserialize, PartialEq, Serialize)]
#[serde(default, deny_unknown_fields)]
pub struct ConsensusObserverConfig {
    /// Whether the consensus observer is enabled
    pub observer_enabled: bool,
    /// Whether the consensus publisher is enabled
    pub publisher_enabled: bool,
```

**File:** config/src/config/consensus_observer_config.rs (L104-107)
```rust
        // Check if the observer configs are manually set in the local config.
        // If they are, we don't want to override them.
        let observer_manually_set = !local_observer_config_yaml["observer_enabled"].is_null();
        let publisher_manually_set = !local_observer_config_yaml["publisher_enabled"].is_null();
```

**File:** config/src/config/consensus_observer_config.rs (L112-118)
```rust
            NodeType::Validator => {
                if ENABLE_ON_VALIDATORS && !publisher_manually_set {
                    // Only enable the publisher for validators
                    consensus_observer_config.publisher_enabled = true;
                    modified_config = true;
                }
            },
```

**File:** aptos-node/src/lib.rs (L830-838)
```rust
    let (consensus_observer_runtime, consensus_publisher_runtime, consensus_publisher) =
        consensus::create_consensus_observer_and_publisher(
            &node_config,
            consensus_observer_network_interfaces,
            consensus_notifier.clone(),
            consensus_to_mempool_sender.clone(),
            db_rw.clone(),
            consensus_observer_reconfig_subscription,
        );
```

**File:** aptos-node/src/lib.rs (L840-851)
```rust
    // Create the consensus runtime (if enabled)
    let consensus_runtime = consensus::create_consensus_runtime(
        &node_config,
        db_rw.clone(),
        consensus_reconfig_subscription,
        consensus_network_interfaces,
        consensus_notifier.clone(),
        consensus_to_mempool_sender.clone(),
        vtxn_pool,
        consensus_publisher.clone(),
        &mut admin_service,
    );
```

**File:** consensus/src/consensus_provider.rs (L65-72)
```rust
    let execution_proxy = ExecutionProxy::new(
        Arc::new(BlockExecutor::<AptosVMBlockExecutor>::new(aptos_db)),
        txn_notifier,
        state_sync_notifier,
        node_config.transaction_filters.execution_filter.clone(),
        node_config.consensus.enable_pre_commit,
        None,
    );
```

**File:** consensus/src/consensus_provider.rs (L152-165)
```rust
    let execution_client = if node_config.consensus_observer.observer_enabled {
        // Create the execution proxy
        let txn_notifier = Arc::new(MempoolNotifier::new(
            consensus_to_mempool_sender.clone(),
            node_config.consensus.mempool_executed_txn_timeout_ms,
        ));
        let execution_proxy = ExecutionProxy::new(
            Arc::new(BlockExecutor::<AptosVMBlockExecutor>::new(aptos_db.clone())),
            txn_notifier,
            state_sync_notifier,
            node_config.transaction_filters.execution_filter.clone(),
            node_config.consensus.enable_pre_commit,
            None,
        );
```

**File:** execution/executor/src/block_executor/mod.rs (L49-64)
```rust
pub struct BlockExecutor<V> {
    pub db: DbReaderWriter,
    inner: RwLock<Option<BlockExecutorInner<V>>>,
    execution_lock: Mutex<()>,
}

impl<V> BlockExecutor<V>
where
    V: VMBlockExecutor,
{
    pub fn new(db: DbReaderWriter) -> Self {
        Self {
            db,
            inner: RwLock::new(None),
            execution_lock: Mutex::new(()),
        }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L50-53)
```rust
            let _lock = self
                .pre_commit_lock
                .try_lock()
                .expect("Concurrent committing detected.");
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L89-92)
```rust
            let _lock = self
                .commit_lock
                .try_lock()
                .expect("Concurrent committing detected.");
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L287-293)
```rust
        if let Err(error) = self
            .execution_client
            .finalize_order(
                ordered_block.blocks().clone(),
                WrappedLedgerInfo::new(VoteData::dummy(), ordered_block.ordered_proof().clone()),
            )
            .await
```
