# Audit Report

## Title
Race Condition in Layout Cache Invalidation During Module Publishing Causes Non-Deterministic Execution and Consensus Failure

## Summary
A critical race condition exists in the layout cache management during parallel block execution. When a transaction publishes an upgraded module, the new module becomes visible to concurrent transactions before the layout cache is flushed, allowing them to use the new module with stale cached layouts from the old module, leading to non-deterministic execution and consensus divergence.

## Finding Description

The vulnerability exists in the interaction between module publishing and layout cache management during parallel block execution in the Aptos block executor.

**Architecture Background:**

The global module cache maintains two separate, independently-managed data structures: [1](#0-0) 

The layout cache uses `StructKey` as the key, which contains only the struct name index and type arguments, with no module version information: [2](#0-1) 

The `StructNameIndex` is based solely on the struct's address, module name, and struct name, remaining identical across module upgrades: [3](#0-2) 

**The Vulnerability:**

When a transaction publishes a module upgrade, the commit sequence in `publish_module_write_set` creates a race window: [4](#0-3) 

The sequence is:
1. Loop (lines 559-571): Adds all upgraded modules to the per-block cache via `add_module_write_to_module_cache`
2. **RACE WINDOW**: New modules are now visible to all concurrent worker threads
3. Line 574: Flushes the layout cache
4. Line 575: Records validation requirements

During the race window between steps 1 and 3, a concurrent transaction executing on another worker thread can:
- Load the NEW module from the per-block cache (old version is marked overridden)
- Query the layout cache and receive the OLD cached layout (not yet flushed)
- Execute with type confusion (new module structure, old layout)

The `add_module_write_to_module_cache` function marks the old module as overridden immediately: [5](#0-4) 

When a concurrent transaction loads a layout from cache, it receives the cached entry and re-reads modules only for gas charging, but returns the cached layout regardless: [6](#0-5) 

The layout cache is simply cleared without version tracking: [7](#0-6) 

**Critical Issue: Validation Does Not Catch This Race**

The module read validation only checks if the module version changed, not whether the layout cache was stale: [8](#0-7) 

For per-block cache reads, validation passes if the transaction index matches (line 1063-1066). This means if transaction T2 reads the new module at version 5 and the version is still 5 at validation time, validation passes—even if T2 used a stale layout that was later flushed.

## Impact Explanation

**Critical Severity: Consensus/Safety Violation**

This vulnerability breaks the fundamental **Deterministic Execution** invariant required for blockchain consensus. 

When different validators execute the same block:
- Validator A's worker thread executes T2 before T1 flushes the cache → T2 uses old layout with new module
- Validator B's worker thread executes T2 after T1 flushes the cache → T2 recomputes new layout from new module
- Both validators read identical blocks with identical transaction ordering
- But produce **different execution results** due to thread scheduling non-determinism
- This leads to **different state roots** for the same block

The timing dependency arises because:
1. Parallel execution uses multiple worker threads with non-deterministic scheduling
2. Layout cache state is not tracked as a transaction dependency
3. No synchronization prevents concurrent reads during the module publish window

Consequences:
- **Consensus Failure**: Validators cannot agree on state root
- **Network Partition**: Chain splits based on which validators saw which timing
- **Requires Hard Fork**: No automatic recovery mechanism exists
- **Fund Safety Compromised**: Transactions execute differently across validators

This meets the **Critical Severity** criteria per the Aptos bug bounty program: "Consensus/Safety Violations" - specifically, different validators producing different state roots for identical blocks.

## Likelihood Explanation

**High Likelihood**

1. **Common Trigger**: Module upgrades with struct layout changes are standard governance operations on Aptos
2. **Natural Occurrence**: No precise timing manipulation required—parallel execution naturally creates the race window
3. **Broad Attack Surface**: Any module upgrade affecting struct layouts triggers this vulnerability
4. **Default Configuration**: Parallel execution with 8+ worker threads is the default, maximizing race probability
5. **No Detection**: The race is silent—no errors, warnings, or validation failures
6. **No Special Permissions**: Any account authorized to publish modules can trigger this

The vulnerability manifests probabilistically whenever:
- A module with modified struct layouts is published (common)
- Other transactions use those structs concurrently (likely in high-throughput blocks)
- Thread scheduling causes the race window to be hit (statistically frequent with multiple workers)

## Recommendation

**Fix: Atomically Publish Modules and Flush Layout Cache**

Move the layout cache flush inside the module publishing loop to eliminate the race window:

```rust
for write in output_before_guard.module_write_set().values() {
    published = true;
    if scheduler.is_v2() {
        module_ids_for_v2.insert(write.module_id().clone());
    }
    add_module_write_to_module_cache::<T>(
        write,
        txn_idx,
        runtime_environment,
        global_module_cache,
        versioned_cache.module_cache(),
    )?;
    // Flush layout cache immediately after each module is published
    // to prevent concurrent transactions from seeing new module with old layout
    global_module_cache.flush_layout_cache();
}
```

Alternatively, implement layout cache versioning to track which module version each layout corresponds to, and invalidate layouts when their source module is overridden.

## Proof of Concept

Due to the timing-dependent nature of this race condition, a deterministic PoC requires controlling thread scheduling. However, the vulnerability is demonstrable through code analysis:

1. Deploy module `0x1::Token` with `struct Coin { value: u64 }`
2. Execute transaction T1 that upgrades to `struct Coin { value: u64, owner: address }`
3. Execute concurrent transaction T2 that deserializes a `Coin` struct
4. With probability P (dependent on thread scheduling), T2 will:
   - Read the new module (2 fields) from per-block cache
   - Get the old layout (1 field) from global cache
   - Experience type confusion during deserialization

The race window exists at lines 564-574 in `txn_last_input_output.rs`, where modules are added to cache before the layout cache is flushed. Different validators with different thread scheduling will execute T2 with different layouts, causing consensus divergence.

### Citations

**File:** aptos-move/block-executor/src/code_cache_global.rs (L89-97)
```rust
pub struct GlobalModuleCache<K, D, V, E> {
    /// Module cache containing the verified code.
    module_cache: HashMap<K, Entry<D, V, E>>,
    /// Sum of serialized sizes (in bytes) of all cached modules.
    size: usize,
    /// Cached layouts of structs or enums. This cache stores roots only and is invalidated when
    /// modules are published.
    struct_layouts: DashMap<StructKey, LayoutCacheEntry>,
}
```

**File:** aptos-move/block-executor/src/code_cache_global.rs (L162-168)
```rust
    /// Flushes only layout caches.
    pub fn flush_layout_cache(&self) {
        // TODO(layouts):
        //   Flushing is only needed because of enums. Once we refactor layouts to store a single
        //   variant instead, this can be removed.
        self.struct_layouts.clear();
    }
```

**File:** aptos-move/block-executor/src/code_cache_global.rs (L272-319)
```rust
pub(crate) fn add_module_write_to_module_cache<T: BlockExecutableTransaction>(
    write: &ModuleWrite<T::Value>,
    txn_idx: TxnIndex,
    runtime_environment: &RuntimeEnvironment,
    global_module_cache: &GlobalModuleCache<ModuleId, CompiledModule, Module, AptosModuleExtension>,
    per_block_module_cache: &impl ModuleCache<
        Key = ModuleId,
        Deserialized = CompiledModule,
        Verified = Module,
        Extension = AptosModuleExtension,
        Version = Option<TxnIndex>,
    >,
) -> Result<(), PanicError> {
    let state_value = write
        .write_op()
        .as_state_value()
        .ok_or_else(|| PanicError::CodeInvariantError("Modules cannot be deleted".to_string()))?;

    // Since we have successfully serialized the module when converting into this transaction
    // write, the deserialization should never fail.
    let compiled_module = runtime_environment
        .deserialize_into_compiled_module(state_value.bytes())
        .map_err(|err| {
            let msg = format!("Failed to construct the module from state value: {:?}", err);
            PanicError::CodeInvariantError(msg)
        })?;
    let extension = Arc::new(AptosModuleExtension::new(state_value));

    per_block_module_cache
        .insert_deserialized_module(
            write.module_id().clone(),
            compiled_module,
            extension,
            Some(txn_idx),
        )
        .map_err(|err| {
            let msg = format!(
                "Failed to insert code for module {}::{} at version {} to module cache: {:?}",
                write.module_address(),
                write.module_name(),
                txn_idx,
                err
            );
            PanicError::CodeInvariantError(msg)
        })?;
    global_module_cache.mark_overridden(write.module_id());
    Ok(())
}
```

**File:** third_party/move/move-vm/runtime/src/storage/layout_cache.rs (L79-83)
```rust
#[derive(Debug, Copy, Clone, Eq, PartialEq, Hash)]
pub struct StructKey {
    pub idx: StructNameIndex,
    pub ty_args_id: TypeVecId,
}
```

**File:** third_party/move/move-vm/types/src/loaded_data/struct_name_indexing.rs (L70-99)
```rust
    pub fn struct_name_to_idx(
        &self,
        struct_name: &StructIdentifier,
    ) -> PartialVMResult<StructNameIndex> {
        {
            let index_map = self.0.read();
            if let Some(idx) = index_map.forward_map.get(struct_name) {
                return Ok(StructNameIndex(*idx));
            }
        }

        // Possibly need to insert, so make the copies outside of the lock.
        let forward_key = struct_name.clone();
        let backward_value = Arc::new(struct_name.clone());

        let idx = {
            let mut index_map = self.0.write();

            if let Some(idx) = index_map.forward_map.get(struct_name) {
                return Ok(StructNameIndex(*idx));
            }

            let idx = index_map.backward_map.len() as u32;
            index_map.backward_map.push(backward_value);
            index_map.forward_map.insert(forward_key, idx);
            idx
        };

        Ok(StructNameIndex(idx))
    }
```

**File:** aptos-move/block-executor/src/txn_last_input_output.rs (L559-576)
```rust
        for write in output_before_guard.module_write_set().values() {
            published = true;
            if scheduler.is_v2() {
                module_ids_for_v2.insert(write.module_id().clone());
            }
            add_module_write_to_module_cache::<T>(
                write,
                txn_idx,
                runtime_environment,
                global_module_cache,
                versioned_cache.module_cache(),
            )?;
        }
        if published {
            // Record validation requirements after the modules are published.
            global_module_cache.flush_layout_cache();
            scheduler.record_validation_requirements(txn_idx, module_ids_for_v2)?;
        }
```

**File:** third_party/move/move-vm/runtime/src/storage/loader/lazy.rs (L203-221)
```rust
    fn load_layout_from_cache(
        &self,
        gas_meter: &mut impl DependencyGasMeter,
        traversal_context: &mut TraversalContext,
        key: &StructKey,
    ) -> Option<PartialVMResult<LayoutWithDelayedFields>> {
        let entry = self.module_storage.get_struct_layout(key)?;
        let (layout, modules) = entry.unpack();
        for module_id in modules.iter() {
            // Re-read all modules for this layout, so that transaction gets invalidated
            // on module publish. Also, we re-read them in exactly the same way as they
            // were traversed during layout construction, so gas charging should be exactly
            // the same as on the cache miss.
            if let Err(err) = self.charge_module(gas_meter, traversal_context, module_id) {
                return Some(Err(err));
            }
        }
        Some(Ok(layout))
    }
```

**File:** aptos-move/block-executor/src/captured_reads.rs (L1050-1089)
```rust
    pub(crate) fn validate_module_reads(
        &self,
        global_module_cache: &GlobalModuleCache<K, DC, VC, S>,
        per_block_module_cache: &SyncModuleCache<K, DC, VC, S, Option<TxnIndex>>,
        maybe_updated_module_keys: Option<&BTreeSet<K>>,
    ) -> bool {
        if self.non_delayed_field_speculative_failure {
            return false;
        }

        let validate = |key: &K, read: &ModuleRead<DC, VC, S>| match read {
            ModuleRead::GlobalCache(_) => global_module_cache.contains_not_overridden(key),
            ModuleRead::PerBlockCache(previous) => {
                let current_version = per_block_module_cache.get_module_version(key);
                let previous_version = previous.as_ref().map(|(_, version)| *version);
                current_version == previous_version
            },
        };

        match maybe_updated_module_keys {
            Some(updated_module_keys) if updated_module_keys.len() <= self.module_reads.len() => {
                // When updated_module_keys is smaller, iterate over it and lookup in module_reads
                updated_module_keys
                    .iter()
                    .filter(|&k| self.module_reads.contains_key(k))
                    .all(|key| validate(key, self.module_reads.get(key).unwrap()))
            },
            Some(updated_module_keys) => {
                // When module_reads is smaller, iterate over it and filter by updated_module_keys
                self.module_reads
                    .iter()
                    .filter(|(k, _)| updated_module_keys.contains(k))
                    .all(|(key, read)| validate(key, read))
            },
            None => self
                .module_reads
                .iter()
                .all(|(key, read)| validate(key, read)),
        }
    }
```
