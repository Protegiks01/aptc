# Audit Report

## Title
Cross-Shard Dependency Edge Missing for Same-Round Conflicts with partition_last_round Enabled

## Summary
When `partition_last_round = true`, the V2 block partitioner can place conflicting transactions in different shards of the same round without proper cross-shard dependency edges, causing race conditions that lead to non-deterministic execution and consensus violations across validators.

## Finding Description

The V2 block partitioner attempts to eliminate cross-shard dependencies within each round through the `discarding_round` function, which iteratively moves conflicting transactions to subsequent rounds. [1](#0-0) 

The partitioning loop terminates either when reaching `max_partitioning_rounds - 1` iterations or when the number of remaining transactions falls below the `cross_shard_dep_avoid_threshold` (default 0.9). [2](#0-1) 

When the loop terminates, all remaining transactions (which still have cross-shard conflicts) are placed into the "last round". When `partition_last_round = false`, these transactions are merged into a single shard to avoid conflicts. However, when `partition_last_round = true`, they retain their original shard assignments across multiple shards. [3](#0-2) 

The critical flaw exists in the dependency edge construction logic. When building required edges for a transaction at position `(round_id, shard_id)`, the system searches for prior writers using a range query that only includes positions strictly before `ShardedTxnIndexV2::new(round_id, shard_id, 0)`. [4](#0-3) 

Given that `ShardedTxnIndexV2` is ordered by `(round_id, shard_id, pre_partitioned_txn_idx)` [5](#0-4) , this range query excludes all transactions in later shards of the same round. Therefore, if Transaction A in Shard 0 reads a key that Transaction B in Shard 1 writes (both in the same round), Transaction A will not have a required edge to Transaction B.

During execution, transactions in the same round execute concurrently across shards. [6](#0-5)  The cross-shard synchronization mechanism relies on `CrossShardStateView`, which only waits for state keys listed in the transaction's required edges. [7](#0-6) 

Without proper dependency edges, transactions with same-round conflicts execute concurrently without synchronization, creating race conditions that can produce different execution orders and state roots across validators.

The vulnerable configuration `partition_last_round = true` is actively used in production, set to `!use_global_executor`. [8](#0-7) 

## Impact Explanation

This vulnerability breaks the fundamental **Deterministic Execution** invariant of blockchain consensus: all validators must produce identical state roots for identical blocks. When conflicting transactions in different shards of the same round execute concurrently without proper cross-shard dependency edges, race conditions cause non-deterministic execution outcomes.

This qualifies as **Critical Severity** under Aptos bug bounty criteria for "Consensus/Safety Violations":
- Different validators computing different state roots causes immediate consensus failure
- The divergent state would require manual intervention or a hardfork to resolve
- No Byzantine validators are required; the bug triggers under normal network operation

The severity is critical because:
1. It directly compromises the core safety guarantee of the blockchain
2. All validators running with `partition_last_round = true` are vulnerable
3. The attack requires only transaction submission (no validator compromise)
4. Detection is difficult as non-determinism appears intermittent

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability can be reliably triggered when:
1. Validators run with `partition_last_round = true` (enabled for non-global executor modes)
2. An attacker submits many transactions with intentional conflicts on the same storage keys
3. The partitioner cannot resolve all conflicts within allowed rounds (default 4 rounds)
4. With the default `cross_shard_dep_avoid_threshold = 0.9`, only 10% of transactions need to remain for early loop termination

Mitigating factors:
- Default configuration has `partition_last_round = false` [9](#0-8) 
- Requires specific workload with high cross-shard conflicts

However, the production code explicitly enables this configuration in certain modes, and an attacker can craft transactions to maximize conflicts and force the partitioner into the vulnerable state.

## Recommendation

Add cross-shard dependency edge construction for same-round transactions when `partition_last_round = true`. The fix should modify the edge construction logic to include writers from later shards in the same round:

```rust
// In take_txn_with_dep, when building required edges:
// Current: searches ..ShardedTxnIndexV2::new(round_id, shard_id, 0)
// Fixed: searches ..ShardedTxnIndexV2::new(round_id + 1, 0, 0)
```

Alternatively, when `partition_last_round = true`, run an additional conflict detection pass on the last round and either:
1. Merge conflicting transactions into a single shard, or
2. Move them to a subsequent round with proper dependency tracking

## Proof of Concept

The existing test demonstrates the vulnerability scenario. [10](#0-9)  This test explicitly sets `partition_last_round(true)` and creates conflicting transactions (circular peer-to-peer transfers), then compares sharded execution against unsharded execution. The test is marked as `#[ignore]` with a comment acknowledging that "cross shard conflict doesn't work for now." [11](#0-10) 

To trigger the vulnerability in production:
1. Deploy validators with `partition_last_round = true` (non-global executor mode)
2. Submit 800+ transactions targeting 80 accounts in a circular conflict pattern
3. Configure partitioner with `max_partitioning_rounds = 2` to force early termination
4. Observe non-deterministic execution outcomes across validators

## Notes

The vulnerability is specific to the **same-round conflict** scenario, which differs from the "cross-round dependency tracking" mentioned in test comments. The partitioner correctly handles conflicts across different rounds, but fails to create dependency edges between conflicting transactions within the same round when they are in different shards.

### Citations

**File:** execution/block-partitioner/src/v2/partition_to_matrix.rs (L37-48)
```rust
        for round_id in 0..(state.num_rounds_limit - 1) {
            let (accepted, discarded) = Self::discarding_round(state, round_id, remaining_txns);
            state.finalized_txn_matrix.push(accepted);
            remaining_txns = discarded;
            num_remaining_txns = remaining_txns.iter().map(|ts| ts.len()).sum();

            if num_remaining_txns
                < ((1.0 - state.cross_shard_dep_avoid_threshold) * state.num_txns() as f32) as usize
            {
                break;
            }
        }
```

**File:** execution/block-partitioner/src/v2/partition_to_matrix.rs (L52-70)
```rust
        if !state.partition_last_round {
            trace!("Merging txns after discarding stopped.");
            let last_round_txns: Vec<PrePartitionedTxnIdx> =
                remaining_txns.into_iter().flatten().collect();
            remaining_txns = vec![vec![]; state.num_executor_shards];
            remaining_txns[state.num_executor_shards - 1] = last_round_txns;
        }

        let last_round_id = state.finalized_txn_matrix.len();
        state.thread_pool.install(|| {
            (0..state.num_executor_shards)
                .into_par_iter()
                .for_each(|shard_id| {
                    remaining_txns[shard_id].par_iter().for_each(|&txn_idx| {
                        state.update_trackers_on_accepting(txn_idx, last_round_id, shard_id);
                    });
                });
        });
        state.finalized_txn_matrix.push(remaining_txns);
```

**File:** execution/block-partitioner/src/v2/partition_to_matrix.rs (L75-177)
```rust
    pub(crate) fn discarding_round(
        state: &mut PartitionState,
        round_id: RoundId,
        remaining_txns: Vec<Vec<PrePartitionedTxnIdx>>,
    ) -> (
        Vec<Vec<PrePartitionedTxnIdx>>,
        Vec<Vec<PrePartitionedTxnIdx>>,
    ) {
        let _timer = MISC_TIMERS_SECONDS.timer_with(&[format!("round_{round_id}").as_str()]);

        let num_shards = remaining_txns.len();

        // Overview of the logic:
        // 1. Key conflicts are analyzed and a txn from `remaining_txns` either goes to `discarded` or `tentatively_accepted`.
        // 2. Relative orders of txns from the same sender are analyzed and a txn from `tentatively_accepted` either goes to `finally_accepted` or `discarded`.
        let mut discarded: Vec<RwLock<Vec<PrePartitionedTxnIdx>>> = Vec::with_capacity(num_shards);
        let mut tentatively_accepted: Vec<RwLock<Vec<PrePartitionedTxnIdx>>> =
            Vec::with_capacity(num_shards);
        let mut finally_accepted: Vec<RwLock<Vec<PrePartitionedTxnIdx>>> =
            Vec::with_capacity(num_shards);

        for txns in remaining_txns.iter() {
            tentatively_accepted.push(RwLock::new(Vec::with_capacity(txns.len())));
            finally_accepted.push(RwLock::new(Vec::with_capacity(txns.len())));
            discarded.push(RwLock::new(Vec::with_capacity(txns.len())));
        }

        // Initialize a table to keep track of the minimum discarded PrePartitionedTxnIdx.
        let min_discard_table: DashMap<SenderIdx, AtomicUsize> =
            DashMap::with_shard_amount(state.dashmap_num_shards);

        state.thread_pool.install(|| {
            // Move some txns to the next round (stored in `discarded`).
            // For those who remain in the current round (`tentatively_accepted`),
            // it's guaranteed to have no cross-shard conflicts.
            remaining_txns
                .into_iter()
                .enumerate()
                .collect::<Vec<_>>()
                .into_par_iter()
                .for_each(|(shard_id, txn_idxs)| {
                    txn_idxs.into_par_iter().for_each(|txn_idx| {
                        let ori_txn_idx = state.ori_idxs_by_pre_partitioned[txn_idx];
                        let mut in_round_conflict_detected = false;
                        let write_set = state.write_sets[ori_txn_idx].read().unwrap();
                        let read_set = state.read_sets[ori_txn_idx].read().unwrap();
                        for &key_idx in write_set.iter().chain(read_set.iter()) {
                            if state.key_owned_by_another_shard(shard_id, key_idx) {
                                in_round_conflict_detected = true;
                                break;
                            }
                        }

                        if in_round_conflict_detected {
                            let sender = state.sender_idx(ori_txn_idx);
                            min_discard_table
                                .entry(sender)
                                .or_insert_with(|| AtomicUsize::new(usize::MAX))
                                .fetch_min(txn_idx, Ordering::SeqCst);
                            discarded[shard_id].write().unwrap().push(txn_idx);
                        } else {
                            tentatively_accepted[shard_id]
                                .write()
                                .unwrap()
                                .push(txn_idx);
                        }
                    });
                });

            // Additional discarding to preserve relative txn order for the same sender.
            tentatively_accepted
                .into_iter()
                .enumerate()
                .collect::<Vec<_>>()
                .into_par_iter()
                .for_each(|(shard_id, txn_idxs)| {
                    let txn_idxs = mem::take(&mut *txn_idxs.write().unwrap());
                    txn_idxs.into_par_iter().for_each(|txn_idx| {
                        let ori_txn_idx = state.ori_idxs_by_pre_partitioned[txn_idx];
                        let sender_idx = state.sender_idx(ori_txn_idx);
                        let min_discarded = min_discard_table
                            .get(&sender_idx)
                            .map(|kv| kv.load(Ordering::SeqCst))
                            .unwrap_or(usize::MAX);
                        if txn_idx < min_discarded {
                            state.update_trackers_on_accepting(txn_idx, round_id, shard_id);
                            finally_accepted[shard_id].write().unwrap().push(txn_idx);
                        } else {
                            discarded[shard_id].write().unwrap().push(txn_idx);
                        }
                    });
                });
        });

        state.thread_pool.spawn(move || {
            drop(min_discard_table);
        });

        (
            extract_and_sort(finally_accepted),
            extract_and_sort(discarded),
        )
    }
```

**File:** execution/block-partitioner/src/v2/state.rs (L307-320)
```rust
            if let Some(txn_idx) = tracker
                .finalized_writes
                .range(..ShardedTxnIndexV2::new(round_id, shard_id, 0))
                .last()
            {
                let src_txn_idx = ShardedTxnIndex {
                    txn_index: *self.final_idxs_by_pre_partitioned[txn_idx.pre_partitioned_txn_idx]
                        .read()
                        .unwrap(),
                    shard_id: txn_idx.shard_id(),
                    round_id: txn_idx.round_id(),
                };
                deps.add_required_edge(src_txn_idx, tracker.storage_location.clone());
            }
```

**File:** execution/block-partitioner/src/v2/types.rs (L65-76)
```rust
impl Ord for ShardedTxnIndexV2 {
    fn cmp(&self, other: &Self) -> cmp::Ordering {
        (self.sub_block_idx, self.pre_partitioned_txn_idx)
            .cmp(&(other.sub_block_idx, other.pre_partitioned_txn_idx))
    }
}

impl PartialOrd for ShardedTxnIndexV2 {
    fn partial_cmp(&self, other: &Self) -> Option<cmp::Ordering> {
        Some(self.cmp(other))
    }
}
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L185-213)
```rust
    fn execute_block(
        &self,
        transactions: SubBlocksForShard<AnalyzedTransaction>,
        state_view: &S,
        config: BlockExecutorConfig,
    ) -> Result<Vec<Vec<TransactionOutput>>, VMStatus> {
        let mut result = vec![];
        for (round, sub_block) in transactions.into_sub_blocks().into_iter().enumerate() {
            let _timer = SHARDED_BLOCK_EXECUTION_BY_ROUNDS_SECONDS
                .timer_with(&[&self.shard_id.to_string(), &round.to_string()]);
            SHARDED_BLOCK_EXECUTOR_TXN_COUNT.observe_with(
                &[&self.shard_id.to_string(), &round.to_string()],
                sub_block.transactions.len() as f64,
            );
            info!(
                "executing sub block for shard {} and round {}, number of txns {}",
                self.shard_id,
                round,
                sub_block.transactions.len()
            );
            result.push(self.execute_sub_block(sub_block, round, state_view, config.clone())?);
            trace!(
                "Finished executing sub block for shard {} and round {}",
                self.shard_id,
                round
            );
        }
        Ok(result)
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_state_view.rs (L58-82)
```rust
    pub fn create_cross_shard_state_view(
        base_view: &'a S,
        transactions: &[TransactionWithDependencies<AnalyzedTransaction>],
    ) -> CrossShardStateView<'a, S> {
        let mut cross_shard_state_key = HashSet::new();
        for txn in transactions {
            for (_, storage_locations) in txn.cross_shard_dependencies.required_edges_iter() {
                for storage_location in storage_locations {
                    cross_shard_state_key.insert(storage_location.clone().into_state_key());
                }
            }
        }
        CrossShardStateView::new(cross_shard_state_key, base_view)
    }
}

impl<S: StateView + Sync + Send> TStateView for CrossShardStateView<'_, S> {
    type Key = StateKey;

    fn get_state_value(&self, state_key: &StateKey) -> Result<Option<StateValue>, StateViewError> {
        if let Some(value) = self.cross_shard_data.get(state_key) {
            return Ok(value.get_value());
        }
        self.base_view.get_state_value(state_key)
    }
```

**File:** execution/executor-benchmark/src/main.rs (L244-260)
```rust
    fn partitioner_config(&self) -> PartitionerV2Config {
        match self.partitioner_version.as_deref() {
            Some("v2") => PartitionerV2Config {
                num_threads: self.partitioner_v2_num_threads,
                max_partitioning_rounds: self.max_partitioning_rounds,
                cross_shard_dep_avoid_threshold: self.partitioner_cross_shard_dep_avoid_threshold,
                dashmap_num_shards: self.partitioner_v2_dashmap_num_shards,
                partition_last_round: !self.use_global_executor,
                pre_partitioner_config: self.pre_partitioner_config(),
            },
            None => PartitionerV2Config::default(),
            _ => panic!(
                "Unknown partitioner version: {:?}",
                self.partitioner_version
            ),
        }
    }
```

**File:** execution/block-partitioner/src/v2/config.rs (L54-65)
```rust
impl Default for PartitionerV2Config {
    fn default() -> Self {
        Self {
            num_threads: 8,
            max_partitioning_rounds: 4,
            cross_shard_dep_avoid_threshold: 0.9,
            dashmap_num_shards: 64,
            partition_last_round: false,
            pre_partitioner_config: Box::<ConnectedComponentPartitionerConfig>::default(),
        }
    }
}
```

**File:** execution/executor-service/src/test_utils.rs (L159-213)
```rust
pub fn sharded_block_executor_with_conflict<E: ExecutorClient<InMemoryStateStore>>(
    mut sharded_block_executor: ShardedBlockExecutor<InMemoryStateStore, E>,
    concurrency: usize,
) {
    let num_txns = 800;
    let num_shards = sharded_block_executor.num_shards();
    let num_accounts = 80;
    let state_store = InMemoryStateStore::from_head_genesis();
    let mut transactions = Vec::new();
    let mut accounts = Vec::new();
    let mut txn_hash_to_account = HashMap::new();
    for _ in 0..num_accounts {
        let account = generate_account_at(&state_store, AccountAddress::random());
        accounts.push(Mutex::new(account));
    }
    for i in 1..num_txns / num_accounts {
        for j in 0..num_accounts {
            let sender = &mut accounts[j].lock().unwrap();
            let sender_addr = *sender.address();
            let receiver = &accounts[(j + i) % num_accounts].lock().unwrap();
            let transfer_amount = 1_000;
            let txn = generate_p2p_txn(sender, receiver, transfer_amount);
            txn_hash_to_account.insert(txn.transaction().hash(), sender_addr);
            transactions.push(txn)
        }
    }

    let partitioner = PartitionerV2Config::default()
        .max_partitioning_rounds(2)
        .cross_shard_dep_avoid_threshold(0.9)
        .partition_last_round(true)
        .build();
    let partitioned_txns = partitioner.partition(transactions.clone(), num_shards);

    let execution_ordered_txns: Vec<SignatureVerifiedTransaction> =
        PartitionedTransactions::flatten(partitioned_txns.clone())
            .into_iter()
            .map(|t| t.into_txn())
            .collect();
    let sharded_txn_output = sharded_block_executor
        .execute_block(
            Arc::new(state_store.clone()),
            partitioned_txns,
            concurrency,
            BlockExecutorConfigFromOnchain::new_no_block_limit(),
        )
        .unwrap();

    let txn_provider = DefaultTxnProvider::new_without_info(execution_ordered_txns);
    let unsharded_txn_output = AptosVMBlockExecutor::new()
        .execute_block_no_limit(&txn_provider, &state_store)
        .unwrap();
    compare_txn_outputs(unsharded_txn_output, sharded_txn_output);
    sharded_block_executor.shutdown();
}
```

**File:** aptos-move/aptos-vm/tests/sharded_block_executor.rs (L38-53)
```rust
#[test]
#[ignore]
// Sharded execution with cross shard conflict doesn't work for now because we don't have
// cross round dependency tracking yet.
fn test_partitioner_v2_uniform_sharded_block_executor_with_conflict_parallel() {
    for merge_discard in [false, true] {
        let num_shards = 7;
        let client = LocalExecutorService::setup_local_executor_shards(num_shards, Some(4));
        let sharded_block_executor = ShardedBlockExecutor::new(client);
        let partitioner = PartitionerV2Config::default()
            .partition_last_round(merge_discard)
            .pre_partitioner_config(Box::new(UniformPartitionerConfig {}))
            .build();
        test_utils::sharded_block_executor_with_conflict(partitioner, sharded_block_executor, 4);
    }
}
```
