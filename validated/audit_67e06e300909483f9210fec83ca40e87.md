# Audit Report

## Title
Missing fsync() in OnDiskStorage Allows Byzantine Validators to Equivocate by Rewinding last_voted_round State

## Summary
The `OnDiskStorage` implementation lacks `fsync()` after writing consensus safety data, allowing Byzantine validators to intentionally crash after voting but before disk persistence, enabling them to rewind `last_voted_round` state and vote multiple times on the same round, violating AptosBFT safety guarantees.

## Finding Description

The AptosBFT consensus protocol enforces the "first voting rule" through the `verify_and_update_last_vote_round` function, which ensures validators never vote for two different blocks in the same round by maintaining a strictly monotonic `last_voted_round` value. [1](#0-0) 

The critical voting flow in `guarded_construct_and_sign_vote_two_chain` updates `last_voted_round` in memory, signs the vote, persists the safety data, then returns the signed vote: [2](#0-1) 

The persistence operation calls `PersistentSafetyStorage::set_safety_data`: [3](#0-2) 

Which ultimately writes to `OnDiskStorage`. The critical vulnerability is in `OnDiskStorage::write()` which performs atomic rename but **never calls fsync()**: [4](#0-3) 

Without `fsync()`, the written data remains in OS buffer cache and may not reach physical disk for 30+ seconds. Production validator configurations use OnDiskStorage for safety rules: [5](#0-4) 

While the README warns against production use, the configuration sanitizer only prohibits `InMemoryStorage` for mainnet validators, explicitly allowing `OnDiskStorage`: [6](#0-5) 

**Attack Execution:**
1. Byzantine validator receives proposal for round R with block B1
2. Validator votes for B1, updating `last_voted_round = R` in memory
3. `set_safety_data` writes to disk without fsync and returns successfully  
4. Vote for B1 is broadcast to network
5. Validator intentionally crashes (kill -9, power disconnect) before OS flushes buffer
6. Upon restart, validator loads old `last_voted_round < R` from disk
7. Validator receives proposal for round R with different block B2
8. Safety check passes since `last_voted_round < R`
9. Validator votes for B2 and broadcasts it
10. Validator has now equivocated with two conflicting votes for round R

While equivocation detection exists in `PendingVotes`, it operates only on in-memory state per round and cannot detect equivocation across validator restarts: [7](#0-6) 

## Impact Explanation

This vulnerability represents a **Critical Severity** consensus safety violation. Byzantine validators can deliberately equivocate by exploiting the lack of durable persistence, enabling:

1. **Consensus Safety Breaks**: With sufficient Byzantine validators (approaching 1/3 threshold), coordinated equivocation can cause different honest validators to commit conflicting blocks, leading to chain forks
2. **Protocol Invariant Violation**: The fundamental assumption that `last_voted_round` is monotonic and tamper-proof is broken
3. **Unintentional Equivocation**: Even honest validators experiencing power failures or crashes could unintentionally equivocate

This aligns with the Aptos bug bounty Critical category: "Consensus/Safety violations" enabling chain splits with < 1/3 Byzantine validators.

## Likelihood Explanation

The likelihood is **High** because:

1. **Large Exploitation Window**: OS buffer cache retention of 30+ seconds provides ample time for intentional crashes
2. **Trivial Execution**: Byzantine validator operators can crash nodes via kill signals, power disconnects, or code modification
3. **No Complex Timing**: Attack requires no sophisticated synchronization or race condition exploitation
4. **Production Deployment**: OnDiskStorage is used in production Helm chart configurations and permitted by mainnet config validation
5. **Deterministic Outcome**: Attack success depends only on crashing before OS buffer flush, which is fully under attacker control

## Recommendation

Implement proper durability guarantees in `OnDiskStorage::write()`:

```rust
fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
    let contents = serde_json::to_vec(data)?;
    let mut file = File::create(self.temp_path.path())?;
    file.write_all(&contents)?;
    file.sync_all()?; // ADD: Ensure data is flushed to disk
    fs::rename(&self.temp_path, &self.file_path)?;
    
    // Also sync the parent directory to ensure rename is durable
    let parent = self.file_path.parent().ok_or_else(|| Error::from(io::Error::new(io::ErrorKind::Other, "No parent directory")))?;
    File::open(parent)?.sync_all()?;
    
    Ok(())
}
```

Alternatively, migrate production validators to use VaultStorage as the README recommends, and enforce this via config sanitizer for mainnet validators.

## Proof of Concept

While a full PoC would require validator infrastructure, the vulnerability can be demonstrated through the following test scenario:

```rust
#[test]
fn test_ondisk_storage_no_fsync_equivocation() {
    // 1. Create OnDiskStorage and write initial safety data
    let mut storage = OnDiskStorage::new(temp_path);
    let safety_data = SafetyData::new(1, 5, 0, 0, None, 0); // last_voted_round = 5
    storage.set(SAFETY_DATA, safety_data).unwrap();
    
    // 2. Update last_voted_round to 10
    let updated_data = SafetyData::new(1, 10, 0, 0, None, 0);
    storage.set(SAFETY_DATA, updated_data).unwrap();
    // At this point, data is in OS buffer cache but may not be on disk
    
    // 3. Simulate crash by immediately dropping storage and recreating
    drop(storage);
    std::process::Command::new("sync").status().unwrap(); // Force flush
    
    // 4. Reload storage - in real crash scenario without sync, old data would be loaded
    let mut reloaded_storage = OnDiskStorage::new(temp_path);
    let loaded_data: SafetyData = reloaded_storage.get(SAFETY_DATA).unwrap();
    
    // Without fsync, this assertion could fail if crash occurred before buffer flush
    assert_eq!(loaded_data.last_voted_round, 10);
}
```

## Notes

The core technical claims are validated:
- OnDiskStorage lacks fsync() calls for durability
- Production Helm configurations use OnDiskStorage for safety rules
- Config sanitizer permits OnDiskStorage on mainnet
- Voting flow persists before broadcasting, but without durability guarantees
- Equivocation detection is in-memory only per round

However, there is documented guidance against using OnDiskStorage in production (README states "should not be used in production"). The vulnerability exists if and when OnDiskStorage is deployed for consensus safety rules in production environments, which the Helm chart configurations suggest is the current deployment pattern.

### Citations

**File:** consensus/safety-rules/src/safety_rules.rs (L213-232)
```rust
    pub(crate) fn verify_and_update_last_vote_round(
        &self,
        round: Round,
        safety_data: &mut SafetyData,
    ) -> Result<(), Error> {
        if round <= safety_data.last_voted_round {
            return Err(Error::IncorrectLastVotedRound(
                round,
                safety_data.last_voted_round,
            ));
        }

        safety_data.last_voted_round = round;
        trace!(
            SafetyLogSchema::new(LogEntry::LastVotedRound, LogEvent::Update)
                .last_voted_round(safety_data.last_voted_round)
        );

        Ok(())
    }
```

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L77-94)
```rust
        self.verify_and_update_last_vote_round(
            proposed_block.block_data().round(),
            &mut safety_data,
        )?;
        self.safe_to_vote(proposed_block, timeout_cert)?;

        // Record 1-chain data
        self.observe_qc(proposed_block.quorum_cert(), &mut safety_data);
        // Construct and sign vote
        let author = self.signer()?.author();
        let ledger_info = self.construct_ledger_info_2chain(proposed_block, vote_data.hash())?;
        let signature = self.sign(&ledger_info)?;
        let vote = Vote::new_with_signature(vote_data, author, ledger_info, signature);

        safety_data.last_vote = Some(vote.clone());
        self.persistent_storage.set_safety_data(safety_data)?;

        Ok(vote)
```

**File:** consensus/safety-rules/src/persistent_safety_storage.rs (L150-170)
```rust
    pub fn set_safety_data(&mut self, data: SafetyData) -> Result<(), Error> {
        let _timer = counters::start_timer("set", SAFETY_DATA);
        counters::set_state(counters::EPOCH, data.epoch as i64);
        counters::set_state(counters::LAST_VOTED_ROUND, data.last_voted_round as i64);
        counters::set_state(
            counters::HIGHEST_TIMEOUT_ROUND,
            data.highest_timeout_round as i64,
        );
        counters::set_state(counters::PREFERRED_ROUND, data.preferred_round as i64);

        match self.internal_store.set(SAFETY_DATA, data.clone()) {
            Ok(_) => {
                self.cached_safety_data = Some(data);
                Ok(())
            },
            Err(error) => {
                self.cached_safety_data = None;
                Err(Error::SecureStorageUnexpectedError(error.to_string()))
            },
        }
    }
```

**File:** secure/storage/src/on_disk.rs (L64-70)
```rust
    fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
        let contents = serde_json::to_vec(data)?;
        let mut file = File::create(self.temp_path.path())?;
        file.write_all(&contents)?;
        fs::rename(&self.temp_path, &self.file_path)?;
        Ok(())
    }
```

**File:** terraform/helm/aptos-node/files/configs/validator-base.yaml (L11-17)
```yaml
  safety_rules:
    service:
      type: "local"
    backend:
      type: "on_disk_storage"
      path: secure-data.json
      namespace: ~
```

**File:** config/src/config/safety_rules_config.rs (L86-96)
```rust
            // Verify that the secure backend is appropriate for mainnet validators
            if chain_id.is_mainnet()
                && node_type.is_validator()
                && safety_rules_config.backend.is_in_memory()
            {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "The secure backend should not be set to in memory storage in mainnet!"
                        .to_string(),
                ));
            }
```

**File:** consensus/src/pending_votes.rs (L287-309)
```rust
        if let Some((previously_seen_vote, previous_li_digest)) =
            self.author_to_vote.get(&vote.author())
        {
            // is it the same vote?
            if &li_digest == previous_li_digest {
                // we've already seen an equivalent vote before
                let new_timeout_vote = vote.is_timeout() && !previously_seen_vote.is_timeout();
                if !new_timeout_vote {
                    // it's not a new timeout vote
                    return VoteReceptionResult::DuplicateVote;
                }
            } else {
                // we have seen a different vote for the same round
                error!(
                    SecurityEvent::ConsensusEquivocatingVote,
                    remote_peer = vote.author(),
                    vote = vote,
                    previous_vote = previously_seen_vote
                );

                return VoteReceptionResult::EquivocateVote;
            }
        }
```
