# Audit Report

## Title
Missing Retry Mechanism for Execution Failures Causes Permanent Liveness Loss

## Summary
When the execution wait phase returns an `ExecutorError::CouldNotGetData` timeout error, the affected block remains permanently stuck in "Ordered" state and is never re-executed. This causes complete liveness failure for the validator as all subsequent blocks are blocked from processing. Unlike the signing phase which implements retry logic, the execution phase ignores the retry signal, creating an unrecoverable deadlock situation.

## Finding Description
The vulnerability exists in the execution response handling logic within the buffer manager's main event loop. When blocks are ordered and sent for execution, if the execution fails with a `CouldNotGetData` error (commonly triggered by quorum store batch request timeouts), the block processing follows this flawed path:

1. The `ExecutionWaitPhase` awaits compute results and returns an `ExecutionResponse` containing the error result. [1](#0-0) 

2. The buffer manager's `process_execution_response` method receives this error response, logs it via `log_executor_error_occurred`, and returns early without advancing the block from "Ordered" to "Executed" state. [2](#0-1) 

3. The `advance_execution_root` method is designed to detect this stuck situation and return `Some(block_id)` to signal that a retry is needed when the execution root hasn't advanced. [3](#0-2) 

4. However, in the main event loop, this return value is completely ignored and no retry is scheduled. [4](#0-3) 

This is in stark contrast to the signing phase, which properly handles retry scenarios by spawning a delayed retry request when the signing root hasn't advanced. [5](#0-4) 

The `CouldNotGetData` error is defined in the executor error types [6](#0-5)  and occurs in several realistic scenarios:
- Batch request timeout after exhausting retry attempts [7](#0-6) 
- Batch expiration based on ledger timestamp [8](#0-7) 

Once a block becomes stuck, execution is only triggered when ordered blocks are first received [9](#0-8)  and there is no mechanism to retry failed executions during normal operation.

## Impact Explanation
This vulnerability qualifies as **CRITICAL Severity** under the Aptos bug bounty program because it causes **Total Loss of Liveness/Network Availability** (Category 4):

**Complete Validator Halt**: When a block gets stuck due to `CouldNotGetData`, the validator completely stops making consensus progress. The buffer manager cannot advance past the stuck block, preventing all subsequent blocks from being executed, signed, or committed. This represents a fundamental protocol violation causing the validator node to become non-functional.

**No Automatic Recovery**: The only recovery mechanisms are epoch boundary reset [10](#0-9)  or manual state synchronization via reset request [11](#0-10) . During normal epoch operation, there is no automatic recovery, meaning validators remain stuck until external intervention.

**Network-Wide Impact**: If multiple validators encounter batch timeout issues simultaneously (e.g., during network partitions or storage slowdowns), this can lead to widespread liveness failures across the validator set, potentially preventing the network from reaching consensus and processing transactions.

## Likelihood Explanation
This vulnerability has **HIGH likelihood** of occurring in production:

**Common Trigger Conditions**:
- Network latency or partitions causing batch request timeouts
- Storage I/O delays preventing timely batch retrieval
- Quorum store synchronization issues between validators
- High load conditions causing request queue backlogs

**No Special Privileges Required**: This can happen to any validator during normal operations without requiring malicious activity. The batch request timeout mechanism has built-in limits that will eventually return `CouldNotGetData` under adverse network or storage conditions.

**Production Environment Realistic**: Network and storage issues are common in distributed systems, making this a realistic failure mode that could affect mainnet validators during periods of stress or infrastructure degradation.

## Recommendation
Implement retry logic for execution failures similar to the signing phase. Modify the main event loop to use the return value from `advance_execution_root()` to spawn delayed retry requests:

```rust
// In the main event loop, after processing execution response:
Some(response) = self.execution_wait_phase_rx.next() => {
    monitor!("buffer_manager_process_execution_wait_response", {
        self.process_execution_response(response).await;
        if let Some(block_id) = self.advance_execution_root() {
            // Schedule retry for stuck execution
            let cursor = self.buffer.find_elem_by_key(self.execution_root, block_id);
            if let Some(cursor) = cursor {
                let item = self.buffer.get(&cursor);
                if item.is_ordered() {
                    let ordered_item = item.unwrap_ordered_ref();
                    let request = self.create_new_request(ExecutionRequest {
                        ordered_blocks: ordered_item.ordered_blocks.clone(),
                    });
                    let sender = self.execution_schedule_phase_tx.clone();
                    Self::spawn_retry_request(sender, request, Duration::from_millis(100));
                }
            }
        }
        if self.signing_root.is_none() {
            self.advance_signing_root().await;
        }
    });
}
```

## Proof of Concept
While a full PoC requires a complete Aptos testnet environment, the vulnerability can be demonstrated through code inspection:

1. Deploy a validator node under network stress conditions
2. Trigger quorum store batch request timeouts by introducing network latency or storage delays
3. Observe that when `CouldNotGetData` error occurs, the affected block remains in "Ordered" state permanently
4. Monitor that no subsequent blocks are processed until epoch boundary or manual reset
5. Verify via metrics that `BUFFER_MANAGER_RECEIVED_EXECUTOR_ERROR_COUNT` with label "CouldNotGetData" increments but no retry occurs

The vulnerability is evident from the code structure where execution retry signaling exists but is unused, unlike the signing phase which properly implements retry logic.

### Citations

**File:** consensus/src/pipeline/execution_wait_phase.rs (L35-56)
```rust
pub struct ExecutionResponse {
    pub block_id: HashValue,
    pub inner: ExecutorResult<Vec<Arc<PipelinedBlock>>>,
}

pub struct ExecutionWaitPhase;

#[async_trait]
impl StatelessPipeline for ExecutionWaitPhase {
    type Request = ExecutionWaitRequest;
    type Response = ExecutionResponse;

    const NAME: &'static str = "execution";

    async fn process(&self, req: ExecutionWaitRequest) -> ExecutionResponse {
        let ExecutionWaitRequest { block_id, fut } = req;

        ExecutionResponse {
            block_id,
            inner: fut.await,
        }
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L407-410)
```rust
        self.execution_schedule_phase_tx
            .send(request)
            .await
            .expect("Failed to send execution schedule request");
```

**File:** consensus/src/pipeline/buffer_manager.rs (L436-438)
```rust
        if self.execution_root.is_some() && cursor == self.execution_root {
            // Schedule retry.
            self.execution_root
```

**File:** consensus/src/pipeline/buffer_manager.rs (L478-480)
```rust
            if cursor == self.signing_root {
                let sender = self.signing_phase_tx.clone();
                Self::spawn_retry_request(sender, request, Duration::from_millis(100));
```

**File:** consensus/src/pipeline/buffer_manager.rs (L530-533)
```rust
                if commit_proof.ledger_info().ends_epoch() {
                    // the epoch ends, reset to avoid executing more blocks, execute after
                    // this persisting request will result in BlockNotFound
                    self.reset().await;
```

**File:** consensus/src/pipeline/buffer_manager.rs (L579-595)
```rust
    async fn process_reset_request(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        info!("Receive reset");

        match signal {
            ResetSignal::Stop => self.stop = true,
            ResetSignal::TargetRound(round) => {
                self.highest_committed_round = round;
                self.latest_round = round;

                let _ = self.drain_pending_commit_proof_till(round);
            },
        }

        self.reset().await;
        let _ = tx.send(ResetAck::default());
        info!("Reset finishes");
```

**File:** consensus/src/pipeline/buffer_manager.rs (L617-626)
```rust
        let executed_blocks = match inner {
            Ok(result) => result,
            Err(e) => {
                log_executor_error_occurred(
                    e,
                    &counters::BUFFER_MANAGER_RECEIVED_EXECUTOR_ERROR_COUNT,
                    block_id,
                );
                return;
            },
```

**File:** consensus/src/pipeline/buffer_manager.rs (L954-960)
```rust
                Some(response) = self.execution_wait_phase_rx.next() => {
                    monitor!("buffer_manager_process_execution_wait_response", {
                    self.process_execution_response(response).await;
                    self.advance_execution_root();
                    if self.signing_root.is_none() {
                        self.advance_signing_root().await;
                    }});
```

**File:** execution/executor-types/src/error.rs (L41-42)
```rust
    #[error("request timeout")]
    CouldNotGetData,
```

**File:** consensus/src/quorum_store/batch_requester.rs (L148-150)
```rust
                                    counters::RECEIVED_BATCH_EXPIRED_COUNT.inc();
                                    debug!("QS: batch request expired, digest:{}", digest);
                                    return Err(ExecutorError::CouldNotGetData);
```

**File:** consensus/src/quorum_store/batch_requester.rs (L176-178)
```rust
            counters::RECEIVED_BATCH_REQUEST_TIMEOUT_COUNT.inc();
            debug!("QS: batch request timed out, digest:{}", digest);
            Err(ExecutorError::CouldNotGetData)
```
