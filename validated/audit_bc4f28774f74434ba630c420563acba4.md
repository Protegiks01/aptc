# Audit Report

## Title
Race Condition in Hot State Commit Causes Validator Node Panics via expect_hot_slot()

## Summary
A race condition exists in the `HotState` asynchronous commit mechanism where the base hot state data and committed state metadata can become temporarily inconsistent. This inconsistency causes `HotStateLRU::expect_hot_slot()` to panic when metadata points to keys that were evicted or converted to cold slots, resulting in validator node crashes during normal operation.

## Finding Description

The vulnerability stems from a race condition in the `HotState` commit process where two updates occur non-atomically in the `Committer::run()` method. [1](#0-0) 

**Line 196** calls `self.commit(&to_commit)` which updates `self.base` (the shared `DashMap` containing actual hot state data) by inserting/removing entries in the commit method. [2](#0-1) 

**Line 197** then updates `self.committed` with the new state metadata.

**The Critical Race Window:** Between these two non-atomic updates, another thread can call `get_committed()`. [3](#0-2) 

This method returns an `Arc` to `self.base` (which is already updated with new data) alongside `self.committed` (which still contains old metadata). Since `self.base` is a shared `Arc<HotStateBase>` [4](#0-3)  pointing to the same underlying `DashMap` instances, any modifications by the Committer thread are immediately visible to all threads holding references to it.

When the inconsistent state is used to create a `HotStateLRU` in the execution pipeline during `State::update()`, [5](#0-4)  the LRU is initialized with metadata (head, tail, num_items) that references keys which either no longer exist in the base (were evicted) or exist but are now cold slots (were converted from hot to cold).

Later, when `HotStateLRU` operations access these keys, `expect_hot_slot()` panics. [6](#0-5) 

**Panic Locations:**

1. When inserting as head and accessing the old head pointer in `insert_as_head()` [7](#0-6) 

2. When deleting entries and accessing prev/next pointers in the `delete()` method [8](#0-7) 

**Execution Path:** The vulnerability is triggered during normal transaction execution when `CachedStateView::new()` calls `reader.get_persisted_state()`. [9](#0-8) 

This flows through to `HotState::get_committed()` via the `StateStore::get_persisted_state()` [10](#0-9)  and `PersistedState::get_state()` chain. [11](#0-10) 

This execution path is used during block execution in validator nodes. [12](#0-11) 

This breaks the **liveness invariant** - validators must remain operational to participate in consensus.

## Impact Explanation

**Severity: High** (up to $50,000 per Aptos Bug Bounty criteria)

This vulnerability causes **validator node crashes** and qualifies as **API crashes** under the Aptos Bug Bounty High severity category. The concrete impact includes:

- Validator nodes panic unexpectedly during normal operation (no malicious input required)
- Loss of liveness for affected validators
- Downtime while nodes restart and resync
- Potential consensus disruption if multiple validators are affected simultaneously

The vulnerability does not reach Critical severity because it does not compromise consensus safety (no double-spending or chain splits), does not cause permanent damage (nodes can restart), and does not lead to fund theft or unauthorized minting. However, it significantly affects network availability and validator participation.

## Likelihood Explanation

**Likelihood: Medium-High**

This issue is likely to occur in production because:

1. **Normal Operation Trigger**: The race condition is triggered during routine hot state commits when processing transaction execution, not by malicious input. The asynchronous Committer thread runs continuously in a separate thread. [13](#0-12) 

2. **Concurrent Access Pattern**: The `get_persisted_state()` function is called frequently during state updates in the execution pipeline through `CachedStateView::new()`, creating many opportunities for the race condition to occur.

3. **Asynchronous Design**: The commit mechanism uses a separate thread with a backlog queue, increasing the probability of timing windows. Commits are enqueued asynchronously. [14](#0-13) 

4. **High Transaction Volume**: During periods of high transaction throughput, hot state updates occur more frequently, expanding the race window between the base update and metadata update.

The issue may have gone unnoticed due to the intermittent nature of race conditions and the hot state feature being relatively new with incomplete implementation (evidenced by TODO comments throughout the codebase).

## Recommendation

Protect both `self.base` and `self.committed` updates with a single mutex or use atomic operations to ensure consistency. One approach:

```rust
pub fn get_committed(&self) -> (Arc<dyn HotStateView>, State) {
    let state = self.committed.lock().clone();
    let base = self.base.clone();
    (base, state)
}
```

Should be changed to ensure atomicity. The Committer's `run()` method should update both the base and metadata atomically:

```rust
fn run(&mut self) {
    info!("HotState committer thread started.");

    while let Some(to_commit) = self.next_to_commit() {
        // Acquire lock before updating base
        let mut committed = self.committed.lock();
        self.commit(&to_commit);
        *committed = to_commit;
        // Lock released here, ensuring atomic visibility
        
        GAUGE.set_with(&["hot_state_items"], self.base.len() as i64);
        GAUGE.set_with(&["hot_state_key_bytes"], self.total_key_bytes as i64);
        GAUGE.set_with(&["hot_state_value_bytes"], self.total_value_bytes as i64);
    }

    info!("HotState committer quitting.");
}
```

This ensures that any thread calling `get_committed()` will either see the old state (both base and metadata) or the new state (both base and metadata), but never an inconsistent mix.

## Proof of Concept

A PoC would require setting up a multi-threaded test environment that:
1. Continuously enqueues hot state commits in the Committer thread
2. Concurrently calls `get_committed()` from multiple threads during block execution
3. Uses the returned state to create `HotStateLRU` instances
4. Inserts new items into the LRU to trigger `expect_hot_slot()` calls
5. Observes panics when metadata references keys that don't exist or are cold in the base

The race window is small but can be triggered with sufficient concurrency and transaction volume typical in production validator environments.

## Notes

This vulnerability is particularly concerning because:
- It affects the critical path of transaction execution in validator nodes
- It can occur without any malicious input during normal network operation  
- The asynchronous design of the hot state commit mechanism creates inherent race windows
- Multiple validators experiencing this issue simultaneously could disrupt consensus participation

The fix requires ensuring atomic visibility of both the base data and metadata to prevent inconsistent state snapshots.

### Citations

**File:** storage/aptosdb/src/state_store/hot_state.rs (L108-112)
```rust
pub struct HotState {
    base: Arc<HotStateBase>,
    committed: Arc<Mutex<State>>,
    commit_tx: SyncSender<State>,
}
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L131-136)
```rust
    pub fn get_committed(&self) -> (Arc<dyn HotStateView>, State) {
        let state = self.committed.lock().clone();
        let base = self.base.clone();

        (base, state)
    }
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L138-144)
```rust
    pub fn enqueue_commit(&self, to_commit: State) {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["hot_state_enqueue_commit"]);

        self.commit_tx
            .send(to_commit)
            .expect("Failed to queue for hot state commit.")
    }
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L173-178)
```rust
    fn spawn(base: Arc<HotStateBase>, committed: Arc<Mutex<State>>) -> SyncSender<State> {
        let (tx, rx) = std::sync::mpsc::sync_channel(MAX_HOT_STATE_COMMIT_BACKLOG);
        std::thread::spawn(move || Self::new(base, committed, rx).run());

        tx
    }
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L196-197)
```rust
            self.commit(&to_commit);
            *self.committed.lock() = to_commit;
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L235-275)
```rust
    fn commit(&mut self, to_commit: &State) {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["hot_state_commit"]);

        let mut n_insert = 0;
        let mut n_update = 0;
        let mut n_evict = 0;

        let delta = to_commit.make_delta(&self.committed.lock());
        for shard_id in 0..NUM_STATE_SHARDS {
            for (key, slot) in delta.shards[shard_id].iter() {
                if slot.is_hot() {
                    let key_size = key.size();
                    self.total_key_bytes += key_size;
                    self.total_value_bytes += slot.size();
                    if let Some(old_slot) = self.base.shards[shard_id].insert(key, slot) {
                        self.total_key_bytes -= key_size;
                        self.total_value_bytes -= old_slot.size();
                        n_update += 1;
                    } else {
                        n_insert += 1;
                    }
                } else if let Some((key, old_slot)) = self.base.shards[shard_id].remove(&key) {
                    self.total_key_bytes -= key.size();
                    self.total_value_bytes -= old_slot.size();
                    n_evict += 1;
                }
            }
            self.heads[shard_id] = to_commit.latest_hot_key(shard_id);
            self.tails[shard_id] = to_commit.oldest_hot_key(shard_id);
            assert_eq!(
                self.base.shards[shard_id].len(),
                to_commit.num_hot_items(shard_id)
            );

            debug_assert!(self.validate_lru(shard_id).is_ok());
        }

        COUNTER.inc_with_by(&["hot_state_insert"], n_insert);
        COUNTER.inc_with_by(&["hot_state_update"], n_update);
        COUNTER.inc_with_by(&["hot_state_evict"], n_evict);
    }
```

**File:** storage/storage-interface/src/state_store/state.rs (L197-204)
```rust
                    let mut lru = HotStateLRU::new(
                        NonZeroUsize::new(self.hot_state_config.max_items_per_shard).unwrap(),
                        Arc::clone(&persisted_hot_state),
                        overlay,
                        hot_metadata.latest.clone(),
                        hot_metadata.oldest.clone(),
                        hot_metadata.num_items,
                    );
```

**File:** storage/storage-interface/src/state_store/hot_state.rs (L60-68)
```rust
    fn insert_as_head(&mut self, key: StateKey, mut slot: StateSlot) {
        match self.head.take() {
            Some(head) => {
                let mut old_head_slot = self.expect_hot_slot(&head);
                old_head_slot.set_prev(Some(key.clone()));
                slot.set_prev(None);
                slot.set_next(Some(head.clone()));
                self.pending.insert(head, old_head_slot);
                self.pending.insert(key.clone(), slot);
```

**File:** storage/storage-interface/src/state_store/hot_state.rs (L118-134)
```rust
        match old_slot.prev() {
            Some(prev_key) => {
                let mut prev_slot = self.expect_hot_slot(prev_key);
                prev_slot.set_next(old_slot.next().cloned());
                self.pending.insert(prev_key.clone(), prev_slot);
            },
            None => {
                // There is no newer entry. The current key was the head.
                self.head = old_slot.next().cloned();
            },
        }

        match old_slot.next() {
            Some(next_key) => {
                let mut next_slot = self.expect_hot_slot(next_key);
                next_slot.set_prev(old_slot.prev().cloned());
                self.pending.insert(next_key.clone(), next_slot);
```

**File:** storage/storage-interface/src/state_store/hot_state.rs (L157-161)
```rust
    fn expect_hot_slot(&self, key: &StateKey) -> StateSlot {
        let slot = self.get_slot(key).expect("Given key is expected to exist.");
        assert!(slot.is_hot(), "Given key is expected to be hot.");
        slot
    }
```

**File:** storage/storage-interface/src/state_store/state_view/cached_state_view.rs (L126-135)
```rust
    pub fn new(id: StateViewId, reader: Arc<dyn DbReader>, state: State) -> StateViewResult<Self> {
        let (hot_state, persisted_state) = reader.get_persisted_state()?;
        Ok(Self::new_impl(
            id,
            reader,
            hot_state,
            persisted_state,
            state,
        ))
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L252-254)
```rust
    fn get_persisted_state(&self) -> Result<(Arc<dyn HotStateView>, State)> {
        Ok(self.persisted_state.get_state())
    }
```

**File:** storage/aptosdb/src/state_store/persisted_state.rs (L46-48)
```rust
    pub fn get_state(&self) -> (Arc<dyn HotStateView>, State) {
        self.hot_state.get_committed()
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L226-233)
```rust
                let state_view = {
                    let _timer = OTHER_TIMERS.timer_with(&["get_state_view"]);
                    CachedStateView::new(
                        StateViewId::BlockExecution { block_id },
                        Arc::clone(&self.db.reader),
                        parent_output.result_state().latest().clone(),
                    )?
                };
```
