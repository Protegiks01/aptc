# Audit Report

## Title
Missing fsync() in OnDiskStorage Enables Consensus Safety Violations After Crash

## Summary
The `OnDiskStorage` backend used for consensus safety rules persistence lacks `fsync()` calls, causing safety-critical data (`last_voted_round`) to be lost on system crashes. This enables validators to double-vote after restart, violating consensus safety guarantees and potentially causing chain splits.

## Finding Description

The Aptos consensus safety-rules system relies on persistent storage to maintain critical safety invariants. The storage abstraction explicitly states: "Any set function is expected to sync to the remote system before returning" [1](#0-0) , establishing a durability contract that `OnDiskStorage` violates.

**Vulnerable Code Flow:**

When a validator votes for a block, `SafetyRules` updates the `last_voted_round` field to prevent double-voting. The execution path is:

1. **Vote Construction**: The `guarded_construct_and_sign_vote_two_chain` function retrieves safety data, verifies voting rules, and updates `last_voted_round` [2](#0-1) 

2. **Last Vote Round Update**: The safety check enforces that new votes must be for higher rounds than previously voted [3](#0-2) 

3. **Persistence Call**: After creating the vote, the function persists the updated safety data via `persistent_storage.set_safety_data(safety_data)` [4](#0-3) 

4. **Storage Backend**: This eventually calls `OnDiskStorage::write()` which writes data to a temporary file, renames it to the final location, but **never calls `fsync()` or `sync_all()`** [5](#0-4) 

**The Critical Bug:**

The `write()` method returns `Ok(())` after `fs::rename()` completes, but at this point the data only exists in OS page cache—not on physical storage. Without `fsync()`, a system crash (power failure, kernel panic, OOM kill) before the OS flushes buffers will cause the write to be lost.

**Production Deployment Evidence:**

Despite the README warning that OnDiskStorage "should not be used in production" [6](#0-5) , production deployment configurations use it:

- Docker Compose validator config [7](#0-6) 
- Terraform Helm base validator config [8](#0-7) 
- Testnet template [9](#0-8) 

**Configuration Validation Gap:**

The `SafetyRulesConfig` sanitizer only blocks `InMemoryStorage` for mainnet validators, but does **not** block `OnDiskStorage` [10](#0-9) . This allows validators to deploy with the unsafe storage backend on mainnet.

**Attack Scenario:**

1. Validator votes for block at round R
2. `verify_and_update_last_vote_round` updates `last_voted_round = R` in memory
3. `set_safety_data()` writes to disk and returns `Ok(())`
4. Data remains in OS buffer cache (not flushed to disk)
5. **System crash** occurs (power failure, kernel panic, OOM)
6. Node restarts and reads safety data from disk
7. `last_voted_round` has old value < R (pre-crash state)
8. Safety rules check passes: `round > safety_data.last_voted_round`
9. **Validator votes again for round R with potentially different vote**
10. **Consensus safety violation: equivocation/double-voting**

While the consensus layer has equivocation detection [11](#0-10) , this detection occurs at the receiving end AFTER the validator has already sent conflicting votes. The safety rules are meant to PREVENT double-voting at the source, not just detect it after the fact.

## Impact Explanation

**Critical Severity** - This vulnerability enables consensus safety violations under the Aptos Bug Bounty "Consensus/Safety violations" category:

1. **Double-Voting/Equivocation**: Honest validators can unintentionally vote twice in the same round with different votes, breaking the fundamental consensus safety property enshrined in the `last_voted_round` tracking [12](#0-11) 

2. **Chain Splits**: If multiple validators crash and double-vote differently after recovery, the network can form conflicting quorum certificates for the same round, potentially causing permanent chain splits

3. **Byzantine Behavior from Honest Nodes**: Validators following the protocol correctly become accidentally Byzantine through natural operational events, undermining the BFT assumption of at most f Byzantine nodes out of 3f+1

4. **Widespread Deployment**: The vulnerable configuration is present in the Helm base template used for production Kubernetes deployments, affecting any validators deployed using standard Aptos deployment tools

This qualifies as **Critical Severity** under the bug bounty program's "Consensus/Safety violations" category—the ability to break consensus safety guarantees through double-voting.

## Likelihood Explanation

**High Likelihood:**

1. **Natural Trigger**: System crashes are common operational events in production environments (power failures, kernel panics, OOM kills, hardware failures)

2. **No Attacker Required**: The vulnerability triggers through normal system failures, not malicious actions

3. **Default Configuration Issue**: Production deployment templates (Helm base config) use the vulnerable storage backend by default

4. **Large Vulnerability Window**: The time between `write_all()` returning and OS buffer flush can be several seconds under load, creating a significant window of exposure

5. **High Vote Frequency**: Validators vote on every consensus round (sub-second frequency), maximizing the probability that unflushed data exists during a crash

6. **Configuration Validation Gap**: The sanitizer doesn't prevent OnDiskStorage on mainnet, allowing validators to deploy with this configuration

The likelihood increases with the number of validators using default configs, system resource pressure, and infrastructure instability.

## Recommendation

Add `fsync()` or `sync_all()` calls to `OnDiskStorage::write()` to ensure data durability:

```rust
fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
    let contents = serde_json::to_vec(data)?;
    let mut file = File::create(self.temp_path.path())?;
    file.write_all(&contents)?;
    file.sync_all()?;  // Add this line to ensure durability
    fs::rename(&self.temp_path, &self.file_path)?;
    Ok(())
}
```

Additionally, update the `SafetyRulesConfig` sanitizer to block `OnDiskStorage` for mainnet validators, similar to how it blocks `InMemoryStorage`, or require VaultStorage for production deployments.

## Proof of Concept

This vulnerability can be demonstrated by:

1. Starting a validator with OnDiskStorage backend
2. Allowing it to vote on several rounds
3. Sending SIGKILL to the process immediately after a vote (simulating crash)
4. Restarting the validator
5. Observing that it can vote again on previously voted rounds

The proof requires infrastructure testing rather than a code snippet, as it depends on actual file system behavior and OS buffering.

**Notes:**

- The vulnerability is technically valid: OnDiskStorage violates the stated durability contract by not calling fsync()
- Production deployment configurations demonstrate widespread use of the vulnerable backend
- The sanitizer validation gap allows this configuration on mainnet
- While equivocation detection exists at the consensus layer, it only detects violations after they occur, not prevents them
- The safety rules system's purpose is to prevent double-voting at the source, which this vulnerability undermines
- System crashes with unflushed data are realistic operational events in production environments

### Citations

**File:** consensus/safety-rules/src/persistent_safety_storage.rs (L18-18)
```rust
/// Any set function is expected to sync to the remote system before returning.
```

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L53-95)
```rust
    pub(crate) fn guarded_construct_and_sign_vote_two_chain(
        &mut self,
        vote_proposal: &VoteProposal,
        timeout_cert: Option<&TwoChainTimeoutCertificate>,
    ) -> Result<Vote, Error> {
        // Exit early if we cannot sign
        self.signer()?;

        let vote_data = self.verify_proposal(vote_proposal)?;
        if let Some(tc) = timeout_cert {
            self.verify_tc(tc)?;
        }
        let proposed_block = vote_proposal.block();
        let mut safety_data = self.persistent_storage.safety_data()?;

        // if already voted on this round, send back the previous vote
        // note: this needs to happen after verifying the epoch as we just check the round here
        if let Some(vote) = safety_data.last_vote.clone() {
            if vote.vote_data().proposed().round() == proposed_block.round() {
                return Ok(vote);
            }
        }

        // Two voting rules
        self.verify_and_update_last_vote_round(
            proposed_block.block_data().round(),
            &mut safety_data,
        )?;
        self.safe_to_vote(proposed_block, timeout_cert)?;

        // Record 1-chain data
        self.observe_qc(proposed_block.quorum_cert(), &mut safety_data);
        // Construct and sign vote
        let author = self.signer()?.author();
        let ledger_info = self.construct_ledger_info_2chain(proposed_block, vote_data.hash())?;
        let signature = self.sign(&ledger_info)?;
        let vote = Vote::new_with_signature(vote_data, author, ledger_info, signature);

        safety_data.last_vote = Some(vote.clone());
        self.persistent_storage.set_safety_data(safety_data)?;

        Ok(vote)
    }
```

**File:** consensus/safety-rules/src/safety_rules.rs (L213-232)
```rust
    pub(crate) fn verify_and_update_last_vote_round(
        &self,
        round: Round,
        safety_data: &mut SafetyData,
    ) -> Result<(), Error> {
        if round <= safety_data.last_voted_round {
            return Err(Error::IncorrectLastVotedRound(
                round,
                safety_data.last_voted_round,
            ));
        }

        safety_data.last_voted_round = round;
        trace!(
            SafetyLogSchema::new(LogEntry::LastVotedRound, LogEvent::Update)
                .last_voted_round(safety_data.last_voted_round)
        );

        Ok(())
    }
```

**File:** secure/storage/src/on_disk.rs (L16-22)
```rust
/// OnDiskStorage represents a key value store that is persisted to the local filesystem and is
/// intended for single threads (or must be wrapped by a Arc<RwLock<>>). This provides no permission
/// checks and simply offers a proof of concept to unblock building of applications without more
/// complex data stores. Internally, it reads and writes all data to a file, which means that it
/// must make copies of all key material which violates the code base. It violates it because
/// the anticipation is that data stores would securely handle key material. This should not be used
/// in production.
```

**File:** secure/storage/src/on_disk.rs (L64-70)
```rust
    fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
        let contents = serde_json::to_vec(data)?;
        let mut file = File::create(self.temp_path.path())?;
        file.write_all(&contents)?;
        fs::rename(&self.temp_path, &self.file_path)?;
        Ok(())
    }
```

**File:** docker/compose/aptos-node/validator.yaml (L11-14)
```yaml
    backend:
      type: "on_disk_storage"
      path: secure-data.json
      namespace: ~
```

**File:** terraform/helm/aptos-node/files/configs/validator-base.yaml (L14-17)
```yaml
    backend:
      type: "on_disk_storage"
      path: secure-data.json
      namespace: ~
```

**File:** testsuite/pangu_lib/template_testnet_files/validator.yaml (L11-14)
```yaml
    backend:
      type: "on_disk_storage"
      path: secure-data.json
      namespace: ~
```

**File:** config/src/config/safety_rules_config.rs (L85-96)
```rust
        if let Some(chain_id) = chain_id {
            // Verify that the secure backend is appropriate for mainnet validators
            if chain_id.is_mainnet()
                && node_type.is_validator()
                && safety_rules_config.backend.is_in_memory()
            {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "The secure backend should not be set to in memory storage in mainnet!"
                        .to_string(),
                ));
            }
```

**File:** consensus/src/pending_votes.rs (L287-308)
```rust
        if let Some((previously_seen_vote, previous_li_digest)) =
            self.author_to_vote.get(&vote.author())
        {
            // is it the same vote?
            if &li_digest == previous_li_digest {
                // we've already seen an equivalent vote before
                let new_timeout_vote = vote.is_timeout() && !previously_seen_vote.is_timeout();
                if !new_timeout_vote {
                    // it's not a new timeout vote
                    return VoteReceptionResult::DuplicateVote;
                }
            } else {
                // we have seen a different vote for the same round
                error!(
                    SecurityEvent::ConsensusEquivocatingVote,
                    remote_peer = vote.author(),
                    vote = vote,
                    previous_vote = previously_seen_vote
                );

                return VoteReceptionResult::EquivocateVote;
            }
```

**File:** consensus/consensus-types/src/safety_data.rs (L8-21)
```rust
/// Data structure for safety rules to ensure consensus safety.
#[derive(Debug, Deserialize, Eq, PartialEq, Serialize, Clone, Default)]
pub struct SafetyData {
    pub epoch: u64,
    pub last_voted_round: u64,
    // highest 2-chain round, used for 3-chain
    pub preferred_round: u64,
    // highest 1-chain round, used for 2-chain
    #[serde(default)]
    pub one_chain_round: u64,
    pub last_vote: Option<Vote>,
    #[serde(default)]
    pub highest_timeout_round: u64,
}
```
