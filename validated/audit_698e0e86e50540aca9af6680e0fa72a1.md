# Audit Report

## Title
DAG Consensus Storage Exhaustion via Unbounded Parent Certificate Duplication

## Summary
A malicious validator can craft DAG consensus nodes with arbitrarily many duplicate parent certificates, bypassing size validation and causing storage exhaustion. The validation logic only checks transaction payload size but not the size of the parents field, allowing nodes serialized to tens of megabytes to be stored, leading to database bloat and validator performance degradation.

## Finding Description

The DAG consensus implementation fails to validate the number or total size of parent certificates in a Node. The size validation only checks `txn_bytes` (validator transactions + payload), completely ignoring the parents field size: [1](#0-0) 

When a Node is serialized for storage, ALL fields including parents are included without any size limit: [2](#0-1) 

The Node structure contains an unbounded Vec of parent certificates: [3](#0-2) 

The verification logic does not prevent duplicate parent certificates. It only checks that parents are from the previous round and have sufficient voting power: [4](#0-3) 

Critically, the `check_voting_power` implementation does NOT deduplicate authors - it simply sums voting power for all provided authors, counting duplicates multiple times: [5](#0-4) 

The oversized Node is BCS-serialized and persisted to RocksDB: [6](#0-5) 

**Attack Execution:**
1. A malicious validator creates a Node with thousands of duplicate copies of the same valid parent certificate
2. Each NodeCertificate is approximately 200-300 bytes (NodeMetadata ~88 bytes + AggregateSignature with 96-byte BLS signature + BitVec overhead)
3. The duplicated parents pass verification because they're valid certificates from the previous round
4. The voting power check passes easily (duplicates counted multiple times)
5. With 50,000 duplicate certificates at ~300 bytes each (15MB) plus a 20MB payload, a single node reaches 35+ MB
6. The network layer allows messages up to 64 MB: [7](#0-6) 

While DAG pruning exists, it only removes nodes after commit + 3×window_size rounds, allowing temporary storage exhaustion: [8](#0-7) 

## Impact Explanation

This qualifies as **High Severity** under the Aptos bug bounty program:

1. **Validator Node Slowdowns (High)**: Deserializing and processing 35+ MB nodes repeatedly causes significant performance degradation affecting consensus operations
2. **Storage Exhaustion**: Each malicious validator can produce one oversized node per round; across 100 rounds this creates 3.5+ GB of bloat in the consensus database
3. **Resource Limits Violation**: Bypasses the intended 20MB size limit through an unvalidated code path
4. **Memory Pressure**: BCS deserialization of massive structures can cause memory pressure on validator nodes

The attack requires only a single malicious validator (no collusion needed) and directly impacts validator health and network performance. This is protocol-level resource exhaustion exploiting a validation gap, not a network-layer DoS attack.

## Likelihood Explanation

**Likelihood: High**

The attack is trivial to execute:
- Requires only validator privileges (assumed threat model for DAG consensus)
- No cryptographic work needed (reuses valid certificates from previous round)
- No coordination with other validators necessary
- Validation logic has zero checks preventing duplicate parent certificates
- Attack is repeatable every round

A malicious validator can simply construct a Node object with duplicate parent certificates using standard Rust APIs, and the consensus protocol will accept and store it.

## Recommendation

Add validation to prevent duplicate parent certificates and enforce total node size limits:

```rust
// In Node::verify() method, add duplicate check:
let unique_parents: HashSet<_> = self.parents()
    .iter()
    .map(|p| p.metadata().digest())
    .collect();
ensure!(
    unique_parents.len() == self.parents().len(),
    "duplicate parent certificates detected"
);

// In rb_handler.rs validate() method, add total node size check:
let parents_bytes: usize = node.parents()
    .iter()
    .map(|p| bcs::to_bytes(p).unwrap().len())
    .sum();
let total_bytes = txn_bytes + parents_bytes as u64;
ensure!(
    total_bytes <= self.payload_config.max_receiving_size_per_round_bytes,
    "total node size exceeds limit"
);
```

Additionally, consider implementing a maximum number of parents based on the validator set size (e.g., 2f+1 with reasonable buffer).

## Proof of Concept

```rust
// Proof of concept demonstrating the vulnerability
#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_duplicate_parents_bypass() {
        // Setup: Create a valid parent certificate from previous round
        let parent_cert = create_valid_certificate(/* round n-1 */);
        
        // Create a node with 10,000 duplicate copies of the same parent
        let mut parents = Vec::new();
        for _ in 0..10_000 {
            parents.push(parent_cert.clone());
        }
        
        // Create oversized node
        let malicious_node = Node::new(
            epoch,
            round,
            author,
            timestamp,
            vec![], // no validator txns
            Payload::empty(true, true), // minimal payload
            parents, // 10,000 duplicates
            Extensions::empty(),
        );
        
        // Verify passes despite duplicates
        assert!(malicious_node.verify(author, &verifier).is_ok());
        
        // Node serializes to multiple MB without rejection
        let serialized = bcs::to_bytes(&malicious_node).unwrap();
        assert!(serialized.len() > 2_000_000); // > 2 MB from duplicates alone
    }
}
```

## Notes

The vulnerability is mitigated long-term by DAG pruning which removes nodes older than 3×window_size rounds from the commit point. However, this does not prevent the temporary storage exhaustion and performance impact during the window when oversized nodes are present. The attack remains viable for causing sustained degradation by producing oversized nodes every round.

### Citations

**File:** consensus/src/dag/rb_handler.rs (L139-142)
```rust
        let num_txns = num_vtxns + node.payload().len() as u64;
        let txn_bytes = vtxn_total_bytes + node.payload().size() as u64;
        ensure!(num_txns <= self.payload_config.max_receiving_txns_per_round);
        ensure!(txn_bytes <= self.payload_config.max_receiving_size_per_round_bytes);
```

**File:** consensus/src/consensusdb/schema/dag/mod.rs (L88-91)
```rust
impl ValueCodec<CertifiedNodeSchema> for CertifiedNode {
    fn encode_value(&self) -> Result<Vec<u8>> {
        Ok(bcs::to_bytes(&self)?)
    }
```

**File:** consensus/src/dag/types.rs (L152-159)
```rust
#[derive(Clone, Serialize, Deserialize, CryptoHasher, Debug, PartialEq)]
pub struct Node {
    metadata: NodeMetadata,
    validator_txns: Vec<ValidatorTransaction>,
    payload: Payload,
    parents: Vec<NodeCertificate>,
    extensions: Extensions,
}
```

**File:** consensus/src/dag/types.rs (L301-345)
```rust
    pub fn verify(&self, sender: Author, verifier: &ValidatorVerifier) -> anyhow::Result<()> {
        ensure!(
            sender == *self.author(),
            "Author {} doesn't match sender {}",
            self.author(),
            sender
        );
        // TODO: move this check to rpc process logic to delay it as much as possible for performance
        ensure!(self.digest() == self.calculate_digest(), "invalid digest");

        let node_round = self.metadata().round();

        ensure!(node_round > 0, "current round cannot be zero");

        if node_round == 1 {
            ensure!(self.parents().is_empty(), "invalid parents for round 1");
            return Ok(());
        }

        let prev_round = node_round - 1;
        // check if the parents' round is the node's round - 1
        ensure!(
            self.parents()
                .iter()
                .all(|parent| parent.metadata().round() == prev_round),
            "invalid parent round"
        );

        // Verification of the certificate is delayed until we need to fetch it
        ensure!(
            verifier
                .check_voting_power(
                    self.parents()
                        .iter()
                        .map(|parent| parent.metadata().author()),
                    true,
                )
                .is_ok(),
            "not enough parents to satisfy voting power"
        );

        // TODO: validate timestamp

        Ok(())
    }
```

**File:** types/src/validator_verifier.rs (L436-448)
```rust
    pub fn sum_voting_power<'a>(
        &self,
        authors: impl Iterator<Item = &'a AccountAddress>,
    ) -> std::result::Result<u128, VerifyError> {
        let mut aggregated_voting_power = 0;
        for account_address in authors {
            match self.get_voting_power(account_address) {
                Some(voting_power) => aggregated_voting_power += voting_power as u128,
                None => return Err(VerifyError::UnknownAuthor),
            }
        }
        Ok(aggregated_voting_power)
    }
```

**File:** consensus/src/dag/adapter.rs (L367-370)
```rust
    fn save_certified_node(&self, node: &CertifiedNode) -> anyhow::Result<()> {
        Ok(self
            .consensus_db
            .put::<CertifiedNodeSchema>(&node.digest(), node)?)
```

**File:** config/src/config/network_config.rs (L50-50)
```rust
pub const MAX_MESSAGE_SIZE: usize = 64 * 1024 * 1024; /* 64 MiB */
```

**File:** consensus/src/dag/dag_store.rs (L419-429)
```rust
    fn commit_callback(
        &mut self,
        commit_round: Round,
    ) -> Option<BTreeMap<u64, Vec<Option<NodeStatus>>>> {
        let new_start_round = commit_round.saturating_sub(3 * self.window_size);
        if new_start_round > self.start_round {
            self.start_round = new_start_round;
            return Some(self.prune());
        }
        None
    }
```
