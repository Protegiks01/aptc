# Audit Report

## Title
State Snapshot Restore Missing Final Root Hash Verification Allows Incomplete State Restoration

## Summary
The state snapshot restoration process verifies each chunk's `SparseMerkleRangeProof` correctly but lacks a final verification after `finish_impl()` to ensure the restored tree's root hash matches the expected root hash. This allows an attacker controlling backup storage to modify the manifest to remove chunks, resulting in an incomplete but seemingly-validated state restoration.

## Finding Description

The vulnerability exists in the interaction between the chunk verification logic and the finalization process in the Jellyfish Merkle Tree restoration system.

**Manifest Loading and Authentication:**
The `StateSnapshotBackup` manifest is loaded as unauthenticated JSON from backup storage. While the `root_hash` field is authenticated via `TransactionInfoWithProof` and `LedgerInfoWithSignatures`, the `chunks` array itself is not cryptographically authenticated. [1](#0-0) 

The restoration controller verifies the root hash from the authenticated proof matches the manifest, but does not authenticate the chunks array structure: [2](#0-1) 

**Chunk-by-Chunk Verification:**
Each chunk is verified in `add_chunk_impl()` which validates that the accumulated state plus the proof's `right_siblings` can reconstruct the expected root hash: [3](#0-2) 

The verification method computes left siblings from all accumulated state and verifies the complete path to the root: [4](#0-3) 

**The Critical Flaw:**
After all chunks are processed, `finish_impl()` freezes remaining nodes and writes them to storage WITHOUT verifying the final root hash matches the expected value: [5](#0-4) 

There is no check comparing the final tree's root hash against `self.expected_root_hash` before completion. The finalization simply freezes nodes and writes them.

**Post-Finalization Flow:**
After `finish()` is called, the storage synchronizer calls `finish_box()` and then `finalize_state_snapshot()`, but still performs no root hash verification: [6](#0-5) 

**Root Hash Check Limitation:**
The only root hash verification occurs when creating a NEW `JellyfishMerkleRestore` instance on an already-completed restore (to detect if previous restore succeeded), not during the initial restoration: [7](#0-6) 

**Attack Vector:**
1. Attacker gains control over backup storage (compromised backup provider, cloud storage breach, or MITM attack)
2. Attacker modifies the manifest's `chunks` array to remove chunks (e.g., keep chunks 0-2, remove chunks 3-9)
3. Each remaining chunk has a valid `SparseMerkleRangeProof` where `right_siblings` cryptographically commit to keys that should be in the removed chunks
4. During restoration, each chunk verifies successfully: `accumulated_keys + proof.right_siblings = expected_root_hash`
5. The proof's `right_siblings` effectively promise "there are more keys to be added later"
6. No later chunks arrive (removed from manifest)
7. `finish_impl()` completes without verifying all promised keys were delivered
8. The final tree is incomplete with a different root hash than expected

## Impact Explanation

**Impact Classification: MEDIUM Severity**

This qualifies as a "Limited Protocol Violation" under the Medium severity category because:

1. **State Consistency Violation**: The restored state does not match the authenticated root hash, breaking the fundamental guarantee that state is cryptographically verifiable via Merkle proofs.

2. **Operational Impact**: A node restoring from a compromised backup will have incomplete state but the restore process reports success. The node will operate with incorrect state until the inconsistency is detected through:
   - State sync failures when querying missing keys
   - Transaction execution errors when accessing missing state
   - Merkle proof verification failures

3. **Manual Intervention Required**: Detection and recovery require manual intervention - operators must identify the corrupted state, remove it, and re-restore from a trusted backup source.

4. **Limited Scope**: While this breaks the backup/restore protocol's security guarantee, it does not directly:
   - Enable fund theft or minting
   - Cause consensus violations (consensus still works correctly)
   - Affect properly synced nodes (only affects nodes restoring from compromised backups)
   - Create permanent network issues (can be fixed by re-restoration)

## Likelihood Explanation

**Likelihood: Medium**

This attack requires:
1. **Attacker control over backup storage**: Compromised backup system, malicious backup provider, cloud storage breach, or man-in-the-middle attack on backup downloads
2. **Victim node performing state snapshot restoration**: Common for new validators joining the network or nodes recovering from failures
3. **No validator stake or network position required**: Attack operates at the backup/restore protocol level

The attack is straightforward once backup storage is compromised:
1. Download legitimate backup manifest
2. Modify the JSON manifest to remove chunk entries (the `chunks` array is unauthenticated)
3. Victim downloads and restores from the modified backup
4. Restoration completes successfully despite incomplete state

Common scenarios where backup storage might be compromised:
- Third-party backup service provider breach
- Cloud storage credential compromise
- Supply chain attacks on backup infrastructure
- Unencrypted backup transfer interception

However, the likelihood is tempered by:
- Many organizations implement additional integrity checks on backups
- Backup storage compromise is a significant security event
- Nodes can detect the issue through subsequent state sync operations

## Recommendation

Add a final root hash verification in the `finish_impl()` method before completing the restoration:

```rust
pub fn finish_impl(mut self) -> Result<()> {
    self.wait_for_async_commit()?;
    
    // ... existing code to freeze nodes ...
    
    self.freeze(0);
    self.store.write_node_batch(&self.frozen_nodes)?;
    
    // ADD: Verify the final root hash matches expected
    let root_node = self.store.get_node_option(&NodeKey::new_empty_path(self.version), "restore")?
        .ok_or_else(|| AptosDbError::Other("Root node not found after restoration".to_string()))?;
    ensure!(
        root_node.hash() == self.expected_root_hash,
        "Root hash mismatch after restoration. Expected: {:x}, Got: {:x}",
        self.expected_root_hash,
        root_node.hash()
    );
    
    Ok(())
}
```

Additionally, consider authenticating the manifest structure (including the chunks array) using a signature or hash chain to prevent modification of the chunk list itself.

## Proof of Concept

This vulnerability can be demonstrated by:

1. Creating a state snapshot backup with multiple chunks
2. Modifying the manifest JSON to remove the last few chunks
3. Restoring from the modified backup
4. Observing that the restoration completes successfully
5. Verifying that the final root hash does not match the expected value
6. Confirming that subsequent state queries for keys in removed chunks fail

A complete PoC would require setting up a backup/restore test environment, which involves substantial test infrastructure. The vulnerability is demonstrated through code analysis showing the missing verification step.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/manifest.rs (L29-51)
```rust
/// State snapshot backup manifest, representing a complete state view at specified version.
#[derive(Deserialize, Serialize)]
pub struct StateSnapshotBackup {
    /// Version at which this state snapshot is taken.
    pub version: Version,
    /// Epoch in which this state snapshot is taken.
    pub epoch: u64,
    /// Hash of the state tree root.
    pub root_hash: HashValue,
    /// All account blobs in chunks.
    pub chunks: Vec<StateSnapshotChunk>,
    /// BCS serialized
    /// `Tuple(TransactionInfoWithProof, LedgerInfoWithSignatures)`.
    ///   - The `TransactionInfoWithProof` is at `Version` above, and carries the same `root_hash`
    /// above; It proves that at specified version the root hash is as specified in a chain
    /// represented by the LedgerInfo below.
    ///   - The signatures on the `LedgerInfoWithSignatures` has a version greater than or equal to
    /// the version of this backup but is within the same epoch, so the signatures on it can be
    /// verified by the validator set in the same epoch, which can be provided by an
    /// `EpochStateBackup` recovered prior to this to the DB; Requiring it to be in the same epoch
    /// limits the requirement on such `EpochStateBackup` to no older than the same epoch.
    pub proof: FileHandle,
}
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L123-136)
```rust
        let manifest: StateSnapshotBackup =
            self.storage.load_json_file(&self.manifest_handle).await?;
        let (txn_info_with_proof, li): (TransactionInfoWithProof, LedgerInfoWithSignatures) =
            self.storage.load_bcs_file(&manifest.proof).await?;
        txn_info_with_proof.verify(li.ledger_info(), manifest.version)?;
        let state_root_hash = txn_info_with_proof
            .transaction_info()
            .ensure_state_checkpoint_hash()?;
        ensure!(
            state_root_hash == manifest.root_hash,
            "Root hash mismatch with that in proof. root hash: {}, expected: {}",
            manifest.root_hash,
            state_root_hash,
        );
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L196-206)
```rust
        let (finished, partial_nodes, previous_leaf) = if let Some(root_node) =
            tree_reader.get_node_option(&NodeKey::new_empty_path(version), "restore")?
        {
            info!("Previous restore is complete, checking root hash.");
            ensure!(
                root_node.hash() == expected_root_hash,
                "Previous completed restore has root hash {}, expecting {}",
                root_node.hash(),
                expected_root_hash,
            );
            (true, vec![], None)
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L339-413)
```rust
    pub fn add_chunk_impl(
        &mut self,
        mut chunk: Vec<(&K, HashValue)>,
        proof: SparseMerkleRangeProof,
    ) -> Result<()> {
        if self.finished {
            info!("State snapshot restore already finished, ignoring entire chunk.");
            return Ok(());
        }

        if let Some(prev_leaf) = &self.previous_leaf {
            let skip_until = chunk
                .iter()
                .find_position(|(key, _hash)| key.hash() > *prev_leaf.account_key());
            chunk = match skip_until {
                None => {
                    info!("Skipping entire chunk.");
                    return Ok(());
                },
                Some((0, _)) => chunk,
                Some((num_to_skip, next_leaf)) => {
                    info!(
                        num_to_skip = num_to_skip,
                        next_leaf = next_leaf,
                        "Skipping leaves."
                    );
                    chunk.split_off(num_to_skip)
                },
            }
        };
        if chunk.is_empty() {
            return Ok(());
        }

        for (key, value_hash) in chunk {
            let hashed_key = key.hash();
            if let Some(ref prev_leaf) = self.previous_leaf {
                ensure!(
                    &hashed_key > prev_leaf.account_key(),
                    "State keys must come in increasing order.",
                )
            }
            self.previous_leaf.replace(LeafNode::new(
                hashed_key,
                value_hash,
                (key.clone(), self.version),
            ));
            self.add_one(key, value_hash);
            self.num_keys_received += 1;
        }

        // Verify what we have added so far is all correct.
        self.verify(proof)?;

        // Write the frozen nodes to storage.
        if self.async_commit {
            self.wait_for_async_commit()?;
            let (tx, rx) = channel();
            self.async_commit_result = Some(rx);

            let mut frozen_nodes = HashMap::new();
            std::mem::swap(&mut frozen_nodes, &mut self.frozen_nodes);
            let store = self.store.clone();

            IO_POOL.spawn(move || {
                let res = store.write_node_batch(&frozen_nodes);
                tx.send(res).unwrap();
            });
        } else {
            self.store.write_node_batch(&self.frozen_nodes)?;
            self.frozen_nodes.clear();
        }

        Ok(())
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L628-697)
```rust
    fn verify(&self, proof: SparseMerkleRangeProof) -> Result<()> {
        let previous_leaf = self
            .previous_leaf
            .as_ref()
            .expect("The previous leaf must exist.");

        let previous_key = previous_leaf.account_key();
        // If we have all siblings on the path from root to `previous_key`, we should be able to
        // compute the root hash. The siblings on the right are already in the proof. Now we
        // compute the siblings on the left side, which represent all the states that have ever
        // been added.
        let mut left_siblings = vec![];

        // The following process might add some extra placeholder siblings on the left, but it is
        // nontrivial to determine when the loop should stop. So instead we just add these
        // siblings for now and get rid of them in the next step.
        let mut num_visited_right_siblings = 0;
        for (i, bit) in previous_key.iter_bits().enumerate() {
            if bit {
                // This node is a right child and there should be a sibling on the left.
                let sibling = if i >= self.partial_nodes.len() * 4 {
                    *SPARSE_MERKLE_PLACEHOLDER_HASH
                } else {
                    Self::compute_left_sibling(
                        &self.partial_nodes[i / 4],
                        previous_key.get_nibble(i / 4),
                        (3 - i % 4) as u8,
                    )
                };
                left_siblings.push(sibling);
            } else {
                // This node is a left child and there should be a sibling on the right.
                num_visited_right_siblings += 1;
            }
        }
        ensure!(
            num_visited_right_siblings >= proof.right_siblings().len(),
            "Too many right siblings in the proof.",
        );

        // Now we remove any extra placeholder siblings at the bottom. We keep removing the last
        // sibling if 1) it's a placeholder 2) it's a sibling on the left.
        for bit in previous_key.iter_bits().rev() {
            if bit {
                if *left_siblings.last().expect("This sibling must exist.")
                    == *SPARSE_MERKLE_PLACEHOLDER_HASH
                {
                    left_siblings.pop();
                } else {
                    break;
                }
            } else if num_visited_right_siblings > proof.right_siblings().len() {
                num_visited_right_siblings -= 1;
            } else {
                break;
            }
        }

        // Left siblings must use the same ordering as the right siblings in the proof
        left_siblings.reverse();

        // Verify the proof now that we have all the siblings
        proof
            .verify(
                self.expected_root_hash,
                SparseMerkleLeafNode::new(*previous_key, previous_leaf.value_hash()),
                left_siblings,
            )
            .map_err(Into::into)
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L750-789)
```rust
    pub fn finish_impl(mut self) -> Result<()> {
        self.wait_for_async_commit()?;
        // Deal with the special case when the entire tree has a single leaf or null node.
        if self.partial_nodes.len() == 1 {
            let mut num_children = 0;
            let mut leaf = None;
            for i in 0..16 {
                if let Some(ref child_info) = self.partial_nodes[0].children[i] {
                    num_children += 1;
                    if let ChildInfo::Leaf(node) = child_info {
                        leaf = Some(node.clone());
                    }
                }
            }

            match num_children {
                0 => {
                    let node_key = NodeKey::new_empty_path(self.version);
                    assert!(self.frozen_nodes.is_empty());
                    self.frozen_nodes.insert(node_key, Node::Null);
                    self.store.write_node_batch(&self.frozen_nodes)?;
                    return Ok(());
                },
                1 => {
                    if let Some(node) = leaf {
                        let node_key = NodeKey::new_empty_path(self.version);
                        assert!(self.frozen_nodes.is_empty());
                        self.frozen_nodes.insert(node_key, node.into());
                        self.store.write_node_batch(&self.frozen_nodes)?;
                        return Ok(());
                    }
                },
                _ => (),
            }
        }

        self.freeze(0);
        self.store.write_node_batch(&self.frozen_nodes)?;
        Ok(())
    }
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L1122-1136)
```rust
    // Finalize the state snapshot
    state_snapshot_receiver.finish_box().map_err(|error| {
        format!(
            "Failed to finish the state value synchronization! Error: {:?}",
            error
        )
    })?;
    storage
        .writer
        .finalize_state_snapshot(
            version,
            target_output_with_proof.clone(),
            epoch_change_proofs,
        )
        .map_err(|error| format!("Failed to finalize the state snapshot! Error: {:?}", error))?;
```
