# Audit Report

## Title
Strategic Nack Responses Cause Unbounded Retry Storms Leading to Resource Exhaustion and Consensus Delays

## Summary
Byzantine validators can strategically send Nack responses to commit vote broadcasts, triggering unbounded retry mechanisms that cause BoundedExecutor saturation and delay verification of legitimate consensus messages, resulting in significant performance degradation.

## Finding Description

The commit reliable broadcast mechanism contains a critical design flaw where Nack responses trigger indefinite retries without any rate limiting, retry count caps, or anomaly detection. This enables Byzantine validators operating within the < 1/3 Byzantine tolerance to cause severe resource exhaustion.

**Attack Flow:**

When a validator broadcasts a commit vote, the RBNetworkSender treats Nack responses as retriable errors: [1](#0-0) 

This error propagates to the ReliableBroadcast retry mechanism, which schedules retries with exponential backoff: [2](#0-1) 

The backoff strategy iterator produces values indefinitely (capped at max delay but with no retry count limit): [3](#0-2) 

Byzantine validators can send Nacks through the normal response mechanism: [4](#0-3) [5](#0-4) [6](#0-5) 

The TODO comments at lines 783 and 810 acknowledge this design flaw: "send_commit_vote() doesn't care about the response and this should be direct send not RPC".

**Critical Resource Exhaustion Vectors:**

1. **BoundedExecutor Saturation**: Each retry spawns an aggregation task in the BoundedExecutor: [7](#0-6) 

The BoundedExecutor has a default capacity of only 16 concurrent tasks: [8](#0-7) 

When saturated, spawn() blocks until slots become available: [9](#0-8) 

2. **Verification Delay**: The same BoundedExecutor is used for verifying incoming commit messages: [10](#0-9) 

When the executor is saturated with retry aggregation tasks, verification of legitimate incoming commit votes is delayed, creating cascading consensus slowdowns.

3. **Unbounded Queue Growth**: The retry futures accumulate in unbounded FuturesUnordered queues: [11](#0-10) 

4. **Broadcast Completion Blocking**: The AckState requires responses from ALL validators, not just a quorum: [12](#0-11) 

This means broadcasts to Byzantine validators that continuously Nack will retry indefinitely while waiting for all validators to respond.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program's "Validator node slowdowns" category.

The resource exhaustion creates concrete performance degradation:

1. **BoundedExecutor Saturation**: With only 16 slots and continuous retry aggregation tasks, legitimate verification tasks are delayed. Since verification and aggregation share the same executor, this creates a direct impact on consensus message processing.

2. **Memory Pressure**: Unbounded FuturesUnordered queues grow with pending retries, potentially leading to OOM conditions under sustained attack.

3. **Network Bandwidth Waste**: Continuous retry traffic to Byzantine validators consumes bandwidth that could serve legitimate consensus operations.

4. **Cascading Delays**: Multiple honest validators experiencing simultaneous slowdowns amplify the attack impact network-wide, potentially delaying quorum formation for commit decisions.

While consensus doesn't completely stall (broadcasts run asynchronously), the executor saturation meaningfully impacts the critical path of commit message verification, which is essential for pipeline progression.

## Likelihood Explanation

**High likelihood** due to:

1. **Low Attacker Requirements**: Only requires controlling < 1/3 of validators, which is the assumed Byzantine tolerance in BFT systems. This is the standard threat model, not an elevated attack scenario.

2. **Trivial Exploitation**: Byzantine validators simply respond with Nacks instead of Acks—no complex logic, timing attacks, or cryptographic breaks required.

3. **No Defense Mechanisms**: The system has:
   - No rate limiting on Nacks from individual validators
   - No anomaly detection for excessive Nack responses
   - No circuit breaker to abort retries after threshold
   - No retry count limits

4. **Natural Amplification**: Each honest validator independently retries to each Byzantine validator, creating N × M concurrent retry streams that persist for extended periods.

5. **Acknowledged Design Flaw**: The TODO comments at lines 783 and 810 indicate developers are aware this shouldn't be RPC-based, confirming the design issue.

## Recommendation

Implement multi-layered protections:

1. **Convert to Fire-and-Forget**: As indicated by TODO comments, commit votes should be direct sends, not RPCs requiring responses. Remove the RPC pattern entirely for commit vote broadcasts.

2. **Add Retry Limits**: If RPC pattern is retained, implement maximum retry count per validator (e.g., 3 retries) before marking that validator as unresponsive for the current broadcast.

3. **Use Quorum-Based Completion**: Modify AckState to complete when 2f+1 validators respond (quorum), rather than waiting for all N validators:

```rust
impl BroadcastStatus<CommitMessage> for Arc<AckState> {
    fn add(&self, peer: Author, ack: Self::Response) -> anyhow::Result<Option<Self::Aggregated>> {
        // ... existing validation ...
        let mut state = self.validators.lock();
        if state.responded.insert(peer) {
            if state.responded.len() >= state.quorum_size {
                return Ok(Some(()));
            }
        }
        Ok(None)
    }
}
```

4. **Add Nack Rate Limiting**: Track Nack frequency per validator and implement exponential backoff or temporary exclusion for validators exhibiting excessive Nack behavior.

5. **Separate Executor Pools**: Use dedicated BoundedExecutor instances for retry aggregation vs. incoming message verification to prevent cross-contamination.

## Proof of Concept

A Byzantine validator can trigger this vulnerability with the following simple response pattern:

```rust
// Byzantine validator's message handler
fn handle_commit_vote(&self, vote: CommitVote) -> CommitMessage {
    // Always respond with Nack instead of validating and Acking
    CommitMessage::Nack
}
```

To observe the impact:
1. Configure a test network with 100 validators, 33 Byzantine
2. Have Byzantine validators respond with Nacks to all commit vote RPCs
3. Monitor BoundedExecutor queue depth and task spawn latency
4. Observe verification delays for legitimate commit messages as executor saturates
5. Measure increased commit vote broadcast completion times

The attack's effectiveness scales with network size and pipeline depth, with production networks experiencing thousands of concurrent retry streams.

## Notes

This vulnerability represents a protocol-level design flaw in the reliable broadcast mechanism, distinct from network-level DoS attacks. The attack operates within the standard BFT threat model (< 1/3 Byzantine validators) and exploits legitimate protocol features (Nack responses) to cause resource exhaustion. The presence of TODO comments acknowledging the design issue suggests this was a known tradeoff or planned improvement that creates an exploitable vulnerability in the current implementation.

### Citations

**File:** consensus/src/pipeline/commit_reliable_broadcast.rs (L99-105)
```rust
        let mut validators = self.validators.lock();
        if validators.remove(&peer) {
            if validators.is_empty() {
                Ok(Some(()))
            } else {
                Ok(None)
            }
```

**File:** consensus/src/pipeline/commit_reliable_broadcast.rs (L126-128)
```rust
            ConsensusMsg::CommitMessage(resp) if matches!(*resp, CommitMessage::Nack) => {
                bail!("Received nack, will retry")
            },
```

**File:** crates/reliable-broadcast/src/lib.rs (L158-159)
```rust
            let mut rpc_futures = FuturesUnordered::new();
            let mut aggregate_futures = FuturesUnordered::new();
```

**File:** crates/reliable-broadcast/src/lib.rs (L171-181)
```rust
                        let future = executor.spawn(async move {
                            (
                                    receiver,
                                    result
                                        .and_then(|msg| {
                                            msg.try_into().map_err(|e| anyhow::anyhow!("{:?}", e))
                                        })
                                        .and_then(|ack| aggregating.add(receiver, ack)),
                            )
                        }).await;
                        aggregate_futures.push(future);
```

**File:** crates/reliable-broadcast/src/lib.rs (L191-200)
```rust
                            Err(e) => {
                                log_rpc_failure(e, receiver);

                                let backoff_strategy = backoff_policies
                                    .get_mut(&receiver)
                                    .expect("should be present");
                                let duration = backoff_strategy.next().expect("should produce value");
                                rpc_futures
                                    .push(send_message(receiver, Some(duration)));
                            },
```

**File:** consensus/src/pipeline/buffer_manager.rs (L208-210)
```rust
        let rb_backoff_policy = ExponentialBackoff::from_millis(2)
            .factor(50)
            .max_delay(Duration::from_secs(5));
```

**File:** consensus/src/pipeline/buffer_manager.rs (L770-770)
```rust
                            reply_nack(protocol, response_sender);
```

**File:** consensus/src/pipeline/buffer_manager.rs (L783-783)
```rust
                    reply_nack(protocol, response_sender); // TODO: send_commit_vote() doesn't care about the response and this should be direct send not RPC
```

**File:** consensus/src/pipeline/buffer_manager.rs (L810-810)
```rust
                    reply_nack(protocol, response_sender); // TODO: send_commit_proof() doesn't care about the response and this should be direct send not RPC
```

**File:** consensus/src/pipeline/buffer_manager.rs (L918-934)
```rust
        let bounded_executor = self.bounded_executor.clone();
        spawn_named!("buffer manager verification", async move {
            while let Some((sender, commit_msg)) = commit_msg_rx.next().await {
                let tx = verified_commit_msg_tx.clone();
                let epoch_state_clone = epoch_state.clone();
                bounded_executor
                    .spawn(async move {
                        match commit_msg.req.verify(sender, &epoch_state_clone.verifier) {
                            Ok(_) => {
                                let _ = tx.unbounded_send(commit_msg);
                            },
                            Err(e) => warn!("Invalid commit message: {}", e),
                        }
                    })
                    .await;
            }
        });
```

**File:** config/src/config/consensus_config.rs (L379-379)
```rust
            num_bounded_executor_tasks: 16,
```

**File:** crates/bounded-executor/src/executor.rs (L41-52)
```rust
    /// Spawn a [`Future`] on the `BoundedExecutor`. This function is async and
    /// will block if the executor is at capacity until one of the other spawned
    /// futures completes. This function returns a [`JoinHandle`] that the caller
    /// can `.await` on for the results of the [`Future`].
    pub async fn spawn<F>(&self, future: F) -> JoinHandle<F::Output>
    where
        F: Future + Send + 'static,
        F::Output: Send + 'static,
    {
        let permit = self.acquire_permit().await;
        self.executor.spawn(future_with_permit(future, permit))
    }
```
