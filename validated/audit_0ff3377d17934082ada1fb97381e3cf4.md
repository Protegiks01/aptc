# Audit Report

## Title
Vote Durability Failure Enables Consensus Safety Violation Through Equivocation After Machine Crash

## Summary
The Aptos consensus voting mechanism uses non-durable storage writes that can be lost during machine crashes, allowing validators to equivocate (vote twice for different blocks in the same round) after restart, violating BFT consensus safety guarantees.

## Finding Description

The Aptos consensus implementation maintains vote persistence in two separate storage systems: ConsensusDB and SafetyRules storage. Both storage systems use write operations without explicit fsync/flush guarantees, creating a critical vulnerability window where votes can be sent over the network but lost from persistent storage during machine crashes.

**The vulnerability occurs in the following sequence:**

1. When a validator receives a proposal, `vote_block()` creates and signs a vote through SafetyRules, then persists it to storage [1](#0-0) 

2. SafetyRules persists the vote by updating `safety_data.last_vote` and calling `set_safety_data()` [2](#0-1) 

3. ConsensusDB persists the vote using its `commit()` method [3](#0-2) 

4. ConsensusDB's `commit()` uses `write_schemas_relaxed()` which explicitly does NOT fsync to disk [4](#0-3) 

5. The `write_schemas_relaxed()` method documentation explicitly states: "If this flag is false, and the machine crashes, some recent writes may be lost" [5](#0-4) 

6. For SafetyRules storage using OnDiskStorage, the `write()` method creates a temp file, writes data, and renames it, but never calls `sync_all()` or `sync_data()` [6](#0-5) 

7. **Critical Finding**: Production validator configurations use OnDiskStorage despite recommendations against it [7](#0-6) [8](#0-7) 

8. If a machine crash occurs (power loss, kernel panic) before OS buffer flush, both storage systems lose the vote data that was only in memory buffers

**After restart, the equivocation scenario unfolds:**

1. Recovery data filters votes by epoch, setting `last_vote = None` if the vote is not found in storage [9](#0-8) 

2. When receiving a different proposal for the same round, SafetyRules checks if `safety_data.last_vote` exists for the current round [10](#0-9) 

3. Since both storages lost the vote, the check passes and SafetyRules creates a new vote for a different block in the same round

4. **The validator sends a second, conflicting vote for the same round - EQUIVOCATION**

While other validators detect equivocation when they receive both votes [11](#0-10) , the damage may already be done if network partitions or timing allow different validators to count different votes toward conflicting quorum certificates.

This violates the fundamental BFT consensus safety property that honest validators never vote for two different blocks in the same round, effectively reducing the Byzantine fault tolerance threshold because crashed validators exhibit Byzantine behavior.

## Impact Explanation

**Severity: CRITICAL** (up to $1,000,000 per Aptos Bug Bounty)

This vulnerability enables **Consensus Safety Violation**, which aligns with the Critical severity category in the Aptos bug bounty program. The impacts include:

1. **Equivocation**: Validators can unintentionally vote for multiple blocks in the same round, behavior that should only be possible for Byzantine (malicious) validators

2. **BFT Assumption Violation**: The BFT safety guarantee assumes < 1/3 Byzantine validators. This bug allows honest validators to behave Byzantine-like due to crashes, effectively reducing the Byzantine fault tolerance threshold

3. **Potential Chain Splits**: If different validators receive different votes from the same validator before/after the crash during network partitions or delays, they may form conflicting quorum certificates

4. **Safety Rules Bypass**: The persistent storage mechanism designed to prevent double-voting across restarts is fundamentally broken by the lack of durable writes

5. **Network-Wide Impact**: Even a single validator experiencing this crash at a critical moment can compromise consensus safety

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

This vulnerability has realistic occurrence conditions:

1. **Machine crashes are common**: Power failures, kernel panics, hardware failures, and OOM kills are regular occurrences in production infrastructure

2. **Production configuration vulnerability**: The production deployment configurations actually use OnDiskStorage despite the secure storage README stating it "should not be used in production environments" [12](#0-11) 

3. **Timing window exists**: The vulnerability window is narrow (milliseconds between write and OS buffer flush) but network message propagation is also fast, creating realistic race conditions

4. **No config enforcement**: While mainnet validators are prevented from using InMemoryStorage, OnDiskStorage is allowed [13](#0-12) 

5. **No recovery mechanism**: The system has no mechanism to detect or recover from lost votes after a crash

## Recommendation

Implement durable writes for consensus-critical data:

1. **For ConsensusDB**: Replace `write_schemas_relaxed()` with `write_schemas()` (which uses sync writes) for vote persistence:
   ```rust
   fn commit(&self, batch: SchemaBatch) -> Result<(), DbError> {
       self.db.write_schemas(batch)?; // Use sync write instead of relaxed
       Ok(())
   }
   ```

2. **For OnDiskStorage**: Add fsync after write operations:
   ```rust
   fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
       let contents = serde_json::to_vec(data)?;
       let mut file = File::create(self.temp_path.path())?;
       file.write_all(&contents)?;
       file.sync_all()?; // Add fsync
       fs::rename(&self.temp_path, &self.file_path)?;
       Ok(())
   }
   ```

3. **Configuration enforcement**: Update config sanitizer to prevent OnDiskStorage for mainnet validators, requiring VaultStorage for production use

4. **Documentation**: Update deployment configurations to use VaultStorage instead of OnDiskStorage

## Proof of Concept

A complete PoC is challenging because it requires simulating machine crashes, but the vulnerability can be demonstrated through:

1. Start a validator with OnDiskStorage configuration
2. Trigger a vote in round R for block A
3. Use SIGKILL immediately after the vote is sent but before OS buffers flush
4. Restart the validator
5. Send a proposal for a different block B in round R
6. Observe that the validator creates and sends a second vote for round R

The key evidence is in the code itself: the lack of fsync calls in both storage paths combined with production configurations using OnDiskStorage creates a genuine durability gap that violates consensus safety guarantees.

## Notes

This vulnerability is particularly concerning because:

1. **Production Impact**: Despite recommendations against it, actual deployment configurations use OnDiskStorage
2. **Dual Failure**: Both ConsensusDB and SafetyRules storage lack fsync, creating redundant failure points
3. **Silent Failure**: The equivocation only becomes apparent after network-wide vote aggregation, not at the individual validator level
4. **Configuration Mismatch**: The README warns against production use of OnDiskStorage, but Terraform/Helm configurations deploy it anyway

The fix requires both code changes (adding fsync) and operational changes (enforcing VaultStorage for production validators).

### Citations

**File:** consensus/src/round_manager.rs (L1520-1541)
```rust
        let vote_result = self.safety_rules.lock().construct_and_sign_vote_two_chain(
            &vote_proposal,
            self.block_store.highest_2chain_timeout_cert().as_deref(),
        );
        let vote = vote_result.context(format!(
            "[RoundManager] SafetyRules Rejected {}",
            block_arc.block()
        ))?;
        if !block_arc.block().is_nil_block() {
            observe_block(block_arc.block().timestamp_usecs(), BlockStage::VOTED);
        }

        if block_arc.block().is_opt_block() {
            observe_block(
                block_arc.block().timestamp_usecs(),
                BlockStage::VOTED_OPT_BLOCK,
            );
        }

        self.storage
            .save_vote(&vote)
            .context("[RoundManager] Fail to persist last vote")?;
```

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L68-74)
```rust
        // if already voted on this round, send back the previous vote
        // note: this needs to happen after verifying the epoch as we just check the round here
        if let Some(vote) = safety_data.last_vote.clone() {
            if vote.vote_data().proposed().round() == proposed_block.round() {
                return Ok(vote);
            }
        }
```

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L91-92)
```rust
        safety_data.last_vote = Some(vote.clone());
        self.persistent_storage.set_safety_data(safety_data)?;
```

**File:** consensus/src/consensusdb/mod.rs (L115-119)
```rust
    pub fn save_vote(&self, last_vote: Vec<u8>) -> Result<(), DbError> {
        let mut batch = SchemaBatch::new();
        batch.put::<SingleEntrySchema>(&SingleEntryKey::LastVote, &last_vote)?;
        self.commit(batch)
    }
```

**File:** consensus/src/consensusdb/mod.rs (L156-159)
```rust
    fn commit(&self, batch: SchemaBatch) -> Result<(), DbError> {
        self.db.write_schemas_relaxed(batch)?;
        Ok(())
    }
```

**File:** storage/schemadb/src/lib.rs (L311-318)
```rust
    /// Writes without sync flag in write option.
    /// If this flag is false, and the machine crashes, some recent
    /// writes may be lost.  Note that if it is just the process that
    /// crashes (i.e., the machine does not reboot), no writes will be
    /// lost even if sync==false.
    pub fn write_schemas_relaxed(&self, batch: impl IntoRawBatch) -> DbResult<()> {
        self.write_schemas_inner(batch, &WriteOptions::default())
    }
```

**File:** secure/storage/src/on_disk.rs (L64-70)
```rust
    fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
        let contents = serde_json::to_vec(data)?;
        let mut file = File::create(self.temp_path.path())?;
        file.write_all(&contents)?;
        fs::rename(&self.temp_path, &self.file_path)?;
        Ok(())
    }
```

**File:** terraform/helm/aptos-node/files/configs/validator-base.yaml (L11-16)
```yaml
  safety_rules:
    service:
      type: "local"
    backend:
      type: "on_disk_storage"
      path: secure-data.json
```

**File:** docker/compose/aptos-node/validator.yaml (L7-14)
```yaml
consensus:
  safety_rules:
    service:
      type: "local"
    backend:
      type: "on_disk_storage"
      path: secure-data.json
      namespace: ~
```

**File:** consensus/src/persistent_liveness_storage.rs (L405-408)
```rust
            last_vote: match last_vote {
                Some(v) if v.epoch() == epoch => Some(v),
                _ => None,
            },
```

**File:** consensus/src/pending_votes.rs (L287-308)
```rust
        if let Some((previously_seen_vote, previous_li_digest)) =
            self.author_to_vote.get(&vote.author())
        {
            // is it the same vote?
            if &li_digest == previous_li_digest {
                // we've already seen an equivalent vote before
                let new_timeout_vote = vote.is_timeout() && !previously_seen_vote.is_timeout();
                if !new_timeout_vote {
                    // it's not a new timeout vote
                    return VoteReceptionResult::DuplicateVote;
                }
            } else {
                // we have seen a different vote for the same round
                error!(
                    SecurityEvent::ConsensusEquivocatingVote,
                    remote_peer = vote.author(),
                    vote = vote,
                    previous_vote = previously_seen_vote
                );

                return VoteReceptionResult::EquivocateVote;
            }
```

**File:** secure/storage/README.md (L37-42)
```markdown
- `OnDisk`: Similar to InMemory, the OnDisk secure storage implementation provides another
useful testing implementation: an on-disk storage engine, where the storage backend is
implemented using a single file written to local disk. In a similar fashion to the in-memory
storage, on-disk should not be used in production environments as it provides no security
guarantees (e.g., encryption before writing to disk). Moreover, OnDisk storage does not
currently support concurrent data accesses.
```

**File:** config/src/config/safety_rules_config.rs (L85-96)
```rust
        if let Some(chain_id) = chain_id {
            // Verify that the secure backend is appropriate for mainnet validators
            if chain_id.is_mainnet()
                && node_type.is_validator()
                && safety_rules_config.backend.is_in_memory()
            {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "The secure backend should not be set to in memory storage in mainnet!"
                        .to_string(),
                ));
            }
```
