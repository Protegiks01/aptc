# Audit Report

## Title
Race Condition in Randomness Generation Allows Byzantine Validators to Crash Consensus Nodes via unreachable!() Panic

## Summary
A race condition exists between consensus reset operations and asynchronous share aggregation tasks in the randomness generation subsystem. Byzantine validators can exploit this by sending randomness shares immediately after a reset event, causing the `unreachable!()` macro in `RandItem::get_all_shares_authors()` to trigger and panic the entire consensus node.

## Finding Description

The vulnerability stems from an incorrect invariant assumption in the randomness generation code. The function `RandItem::get_all_shares_authors()` contains an `unreachable!()` macro that assumes it will never be called on a `PendingMetadata` state: [1](#0-0) 

However, this assumption is **not guaranteed** by the code structure due to a race condition in the async task execution model.

**Normal Flow:**

1. When a block arrives, `process_incoming_metadata()` is called, which adds randomness metadata (transitioning the `RandItem` from `PendingMetadata` to `PendingDecision` state): [2](#0-1) 

2. The function spawns an async task via `spawn_aggregate_shares_task()` that sleeps for 300ms before calling `get_all_shares_authors()`: [3](#0-2) 

3. A `DropGuard` is returned and stored in the block queue: [4](#0-3) 

The `DropGuard` automatically calls `abort()` when dropped: [5](#0-4) 

**Race Condition Attack:**

1. During the 300ms sleep period, a consensus reset occurs via `process_reset()`, which clears the block queue (dropping all `DropGuards`) and removes items from `rand_store`: [6](#0-5) [7](#0-6) 

2. Byzantine validators send randomness shares for round R. These shares are accepted because the round check passes (shares up to 200 rounds ahead are accepted): [8](#0-7) [9](#0-8) 

3. A new `RandItem` is created in **PendingMetadata** state when the share is added to an empty entry: [10](#0-9) 

4. **Critical Issue:** The async task wakes up from sleep and executes synchronous code. The code between line 274 (sleep) and line 290 (next await) runs atomically without checking the abort flag again, even though `abort()` was called when the `DropGuard` was dropped. The task acquires the lock and calls `get_all_shares_authors(round)` on the newly created `PendingMetadata` item, triggering the `unreachable!()` macro and panicking the node.

The root cause is that Rust's `Abortable` futures only check the abort flag at the beginning of each poll. If a poll has already started executing synchronous code after an await completes, it continues to the next await point even if `abort()` is called during that execution. The lock contention creates a timing window where the item can be removed by reset and re-added by incoming shares while the task is blocked waiting for the lock.

## Impact Explanation

**Severity: HIGH** - Validator node crashes leading to network availability degradation

This vulnerability allows Byzantine validators to crash any consensus node participating in randomness generation. The impact includes:

1. **Individual Node Crashes**: Any validator node can be crashed by exploiting this race condition
2. **Network Availability Impact**: If multiple validators are crashed simultaneously, the network's ability to produce randomness and finalize blocks is degraded
3. **Repeated Attacks**: The attack can be repeated after node restart, causing persistent availability issues
4. **No Recovery Beyond Restart**: While nodes can restart, repeated exploitation could cause prolonged network disruption

Per the Aptos bug bounty criteria, this qualifies as **High Severity** due to:
- "Validator node slowdowns" (crashes are worse than slowdowns)
- "API crashes" (consensus node crashes)
- Degradation of network randomness generation capability

This does NOT reach Critical severity because:
- It doesn't cause permanent network partition (nodes can restart)
- It doesn't violate consensus safety (no fork or double-spend)
- It doesn't cause fund loss or permanent freezing
- It doesn't halt the entire network (only affects individual nodes)

## Likelihood Explanation

**Likelihood: Medium-High**

The attack is feasible with the following characteristics:

1. **Triggerable Conditions**: Consensus resets occur naturally during epoch transitions, round synchronization, and recovery from network issues as evidenced by the reset handling code: [11](#0-10) 

2. **Attacker Requirements**: Byzantine validators need to:
   - Monitor for reset events (observable through network activity)
   - Send randomness shares for recently-processed rounds immediately after reset
   - The timing window is narrow (~300ms sleep duration) but exploitable with automated monitoring

3. **Natural Occurrence**: This can also trigger **without malicious intent**:
   - Honest validators may resend shares after observing reset events
   - Network delays could cause shares to arrive in the vulnerable window
   - Makes this a reliability issue even without active attackers

4. **No Special Privileges**: Any validator can send valid randomness shares, no special access required beyond being part of the validator set

5. **Repeatability**: The attack can be repeated indefinitely, making it a serious availability threat

## Recommendation

The fix should ensure that `get_all_shares_authors()` can handle the `PendingMetadata` state gracefully rather than panicking. Options include:

1. **Return None for PendingMetadata**: Change the function to return `None` instead of calling `unreachable!()`, treating it the same as the `Decided` state
2. **Add state checking**: Have the async task verify the item is still in `PendingDecision` state before calling `get_all_shares_authors()`
3. **Strengthen abort checking**: Check the abort flag after acquiring the lock and before proceeding
4. **Prevent share acceptance after reset**: Add additional validation to reject shares for rounds that were recently reset

Recommended fix:

```rust
fn get_all_shares_authors(&self) -> Option<HashSet<Author>> {
    match self {
        RandItem::PendingDecision {
            share_aggregator, ..
        } => Some(share_aggregator.shares.keys().cloned().collect()),
        RandItem::Decided { .. } | RandItem::PendingMetadata(_) => None,
    }
}
```

## Proof of Concept

The vulnerability can be demonstrated by:

1. Starting a consensus node with randomness generation enabled
2. Triggering a block proposal to spawn the share aggregation task
3. During the 300ms sleep window, triggering a consensus reset (e.g., epoch transition)
4. Immediately sending a randomness share for the same round from a Byzantine validator
5. Observing the node panic with `unreachable!("Should only be called after block is added")`

The specific timing can be achieved through network coordination or by monitoring for natural reset events and exploiting them.

### Citations

**File:** consensus/src/rand/rand_gen/rand_store.rs (L110-124)
```rust
enum RandItem<S> {
    PendingMetadata(ShareAggregator<S>),
    PendingDecision {
        metadata: FullRandMetadata,
        share_aggregator: ShareAggregator<S>,
    },
    Decided {
        self_share: RandShare<S>,
    },
}

impl<S: TShare> RandItem<S> {
    fn new(author: Author, path_type: PathType) -> Self {
        Self::PendingMetadata(ShareAggregator::new(author, path_type))
    }
```

**File:** consensus/src/rand/rand_gen/rand_store.rs (L195-205)
```rust
    fn get_all_shares_authors(&self) -> Option<HashSet<Author>> {
        match self {
            RandItem::PendingDecision {
                share_aggregator, ..
            } => Some(share_aggregator.shares.keys().cloned().collect()),
            RandItem::Decided { .. } => None,
            RandItem::PendingMetadata(_) => {
                unreachable!("Should only be called after block is added")
            },
        }
    }
```

**File:** consensus/src/rand/rand_gen/rand_store.rs (L253-259)
```rust
    pub fn reset(&mut self, round: u64) {
        self.update_highest_known_round(round);
        // remove future rounds items in case they're already decided
        // otherwise if the block re-enters the queue, it'll be stuck
        let _ = self.rand_map.split_off(&round);
        let _ = self.fast_rand_map.as_mut().map(|map| map.split_off(&round));
    }
```

**File:** consensus/src/rand/rand_gen/rand_store.rs (L280-313)
```rust
    pub fn add_share(&mut self, share: RandShare<S>, path: PathType) -> anyhow::Result<bool> {
        ensure!(
            share.metadata().epoch == self.epoch,
            "Share from different epoch"
        );
        ensure!(
            share.metadata().round <= self.highest_known_round + FUTURE_ROUNDS_TO_ACCEPT,
            "Share from future round"
        );
        let rand_metadata = share.metadata().clone();

        let (rand_config, rand_item) = if path == PathType::Fast {
            match (self.fast_rand_config.as_ref(), self.fast_rand_map.as_mut()) {
                (Some(fast_rand_config), Some(fast_rand_map)) => (
                    fast_rand_config,
                    fast_rand_map
                        .entry(rand_metadata.round)
                        .or_insert_with(|| RandItem::new(self.author, path)),
                ),
                _ => anyhow::bail!("Fast path not enabled"),
            }
        } else {
            (
                &self.rand_config,
                self.rand_map
                    .entry(rand_metadata.round)
                    .or_insert_with(|| RandItem::new(self.author, PathType::Slow)),
            )
        };

        rand_item.add_share(share, rand_config)?;
        rand_item.try_aggregate(rand_config, self.decision_tx.clone());
        Ok(rand_item.has_decision())
    }
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L145-169)
```rust
    fn process_incoming_metadata(&self, metadata: FullRandMetadata) -> DropGuard {
        let self_share = S::generate(&self.config, metadata.metadata.clone());
        info!(LogSchema::new(LogEvent::BroadcastRandShare)
            .epoch(self.epoch_state.epoch)
            .author(self.author)
            .round(metadata.round()));
        let mut rand_store = self.rand_store.lock();
        rand_store.update_highest_known_round(metadata.round());
        rand_store
            .add_share(self_share.clone(), PathType::Slow)
            .expect("Add self share should succeed");

        if let Some(fast_config) = &self.fast_config {
            let self_fast_share =
                FastShare::new(S::generate(fast_config, metadata.metadata.clone()));
            rand_store
                .add_share(self_fast_share.rand_share(), PathType::Fast)
                .expect("Add self share for fast path should succeed");
        }

        rand_store.add_rand_metadata(metadata.clone());
        self.network_sender
            .broadcast_without_self(RandMessage::<S, D>::Share(self_share).into_network_message());
        self.spawn_aggregate_shares_task(metadata.metadata)
    }
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L184-194)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.rand_store.lock().reset(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L263-303)
```rust
    fn spawn_aggregate_shares_task(&self, metadata: RandMetadata) -> DropGuard {
        let rb = self.reliable_broadcast.clone();
        let aggregate_state = Arc::new(ShareAggregateState::new(
            self.rand_store.clone(),
            metadata.clone(),
            self.config.clone(),
        ));
        let epoch_state = self.epoch_state.clone();
        let round = metadata.round;
        let rand_store = self.rand_store.clone();
        let task = async move {
            tokio::time::sleep(Duration::from_millis(300)).await;
            let maybe_existing_shares = rand_store.lock().get_all_shares_authors(round);
            if let Some(existing_shares) = maybe_existing_shares {
                let epoch = epoch_state.epoch;
                let request = RequestShare::new(metadata.clone());
                let targets = epoch_state
                    .verifier
                    .get_ordered_account_addresses_iter()
                    .filter(|author| !existing_shares.contains(author))
                    .collect::<Vec<_>>();
                info!(
                    epoch = epoch,
                    round = round,
                    "[RandManager] Start broadcasting share request for {}",
                    targets.len(),
                );
                rb.multicast(request, aggregate_state, targets)
                    .await
                    .expect("Broadcast cannot fail");
                info!(
                    epoch = epoch,
                    round = round,
                    "[RandManager] Finish broadcasting share request",
                );
            }
        };
        let (abort_handle, abort_registration) = AbortHandle::new_pair();
        tokio::spawn(Abortable::new(task, abort_registration));
        DropGuard::new(abort_handle)
    }
```

**File:** consensus/src/rand/rand_gen/block_queue.rs (L17-40)
```rust
pub struct QueueItem {
    ordered_blocks: OrderedBlocks,
    offsets_by_round: HashMap<Round, usize>,
    num_undecided_blocks: usize,
    broadcast_handle: Option<Vec<DropGuard>>,
}

impl QueueItem {
    pub fn new(ordered_blocks: OrderedBlocks, broadcast_handle: Option<Vec<DropGuard>>) -> Self {
        let len = ordered_blocks.ordered_blocks.len();
        assert!(len > 0);
        let offsets_by_round: HashMap<Round, usize> = ordered_blocks
            .ordered_blocks
            .iter()
            .enumerate()
            .map(|(idx, b)| (b.round(), idx))
            .collect();
        Self {
            ordered_blocks,
            offsets_by_round,
            num_undecided_blocks: len,
            broadcast_handle,
        }
    }
```

**File:** crates/reliable-broadcast/src/lib.rs (L222-236)
```rust
pub struct DropGuard {
    abort_handle: AbortHandle,
}

impl DropGuard {
    pub fn new(abort_handle: AbortHandle) -> Self {
        Self { abort_handle }
    }
}

impl Drop for DropGuard {
    fn drop(&mut self) {
        self.abort_handle.abort();
    }
}
```

**File:** consensus/src/rand/rand_gen/types.rs (L26-26)
```rust
pub const FUTURE_ROUNDS_TO_ACCEPT: u64 = 200;
```

**File:** consensus/src/pipeline/buffer_manager.rs (L1-50)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use crate::{
    block_storage::tracing::{observe_block, BlockStage},
    consensus_observer::{
        network::observer_message::ConsensusObserverMessage,
        publisher::consensus_publisher::ConsensusPublisher,
    },
    counters::{self, log_executor_error_occurred},
    monitor,
    network::{IncomingCommitRequest, NetworkSender},
    network_interface::ConsensusMsg,
    pipeline::{
        buffer::{Buffer, Cursor},
        buffer_item::BufferItem,
        commit_reliable_broadcast::{AckState, CommitMessage},
        execution_schedule_phase::ExecutionRequest,
        execution_wait_phase::{ExecutionResponse, ExecutionWaitRequest},
        persisting_phase::PersistingRequest,
        pipeline_phase::CountedRequest,
        signing_phase::{SigningRequest, SigningResponse},
    },
};
use aptos_bounded_executor::BoundedExecutor;
use aptos_config::config::ConsensusObserverConfig;
use aptos_consensus_types::{
    common::{Author, Round},
    pipeline::commit_vote::CommitVote,
    pipelined_block::PipelinedBlock,
};
use aptos_crypto::HashValue;
use aptos_executor_types::ExecutorResult;
use aptos_logger::prelude::*;
use aptos_network::protocols::{rpc::error::RpcError, wire::handshake::v1::ProtocolId};
use aptos_reliable_broadcast::{DropGuard, ReliableBroadcast};
use aptos_time_service::TimeService;
use aptos_types::{
    account_address::AccountAddress, epoch_state::EpochState, ledger_info::LedgerInfoWithSignatures,
};
use bytes::Bytes;
use fail::fail_point;
use futures::{
    channel::{
        mpsc::{unbounded, UnboundedReceiver, UnboundedSender},
        oneshot,
    },
    future::{AbortHandle, Abortable},
    FutureExt, SinkExt, StreamExt,
};
```
