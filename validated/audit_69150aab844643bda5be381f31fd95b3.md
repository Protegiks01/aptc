# Audit Report

## Title
Silent Disk Full Error Leads to Consensus/Storage Divergence and Data Loss in Persisting Phase

## Summary
The consensus persisting phase silently discards storage errors when disk is full, causing the node to believe blocks are committed when they are not actually persisted. This creates a dangerous consensus/storage divergence that leads to data loss on node restart and potential network-wide inconsistencies.

## Finding Description

The vulnerability exists in a critical error handling path during block persistence. When consensus reaches agreement on blocks and attempts to persist them to storage, disk full errors are completely silenced through multiple layers of error suppression:

**Layer 1: Error Discarding in `wait_for_commit_ledger()`**

The `wait_for_commit_ledger()` method explicitly discards the result of the commit operation using `let _ = fut.commit_ledger_fut.await;`. [1](#0-0)  This TaskResult contains any errors from the actual disk write, including disk full errors, but they are completely ignored.

**Layer 2: Unconditional Success Response**

The persisting phase ALWAYS returns `Ok(round)`, regardless of whether the commit succeeded or failed. [2](#0-1)  Even though it calls `wait_for_commit_ledger()` at line 71, it never checks if the commit actually succeeded and unconditionally returns success at line 74.

**Layer 3: Error Path from Storage Layer**

The actual disk write occurs in the storage layer where RocksDB write failures propagate as errors. When disk is full, RocksDB's IOError is converted to `AptosDbError::OtherRocksDbError`. [3](#0-2) 

The error propagates through the executor's commit path. [4](#0-3)  Note that there is NO error logging - only an info log at the start. [5](#0-4) 

The commit ledger function in the storage writer propagates these errors via the `?` operator. [6](#0-5) 

The pipeline builder's commit_ledger function converts these to TaskError but also has no error logging. [7](#0-6) 

**Layer 4: Buffer Manager Ignores Errors**

The buffer manager only matches `Some(Ok(round))` pattern. [8](#0-7)  Even if an error was somehow propagated (which it isn't due to Layer 2), it would be silently ignored as there's no `Some(Err(...))` branch.

**Attack Scenario:**

1. A validator node's disk approaches capacity (can occur naturally through state growth or as a resource exhaustion attack)
2. Consensus achieves quorum on new blocks
3. The persisting phase attempts to commit blocks via `wait_for_commit_ledger()`
4. RocksDB write fails with "No space left on device" error (ENOSPC)
5. Error propagates up as `TaskError::InternalError` in the commit_ledger_fut
6. **Error is silently discarded** at `wait_for_commit_ledger()`
7. Persisting phase returns `Ok(round)` indicating success
8. Buffer manager updates `highest_committed_round` believing blocks are persisted
9. Node continues operating, believing it has committed blocks that are NOT on disk
10. If node crashes or restarts, it loses the "committed" blocks
11. Other validators have these blocks persisted, creating state divergence

This breaks the critical invariant: **"State Consistency: State transitions must be atomic and verifiable"**

The decoupled execution pipeline is always active in production. [9](#0-8) 

## Impact Explanation

**Severity: HIGH** (per Aptos Bug Bounty criteria - "State inconsistencies requiring manual intervention")

The impact is severe:

1. **Data Loss**: Blocks that consensus believes are committed are lost on node restart
2. **State Divergence**: The node's in-memory consensus state diverges from its on-disk storage state
3. **Network Inconsistency**: The affected node has different ledger state than other validators
4. **Silent Failure**: No error logging or alerting occurs, making diagnosis extremely difficult
5. **Validator Penalties**: Node may vote on incorrect state, leading to slashing or removal
6. **Recovery Complexity**: Requires state sync to recover, but the node may not even detect it's out of sync

This doesn't reach "Critical" severity because:
- It doesn't directly cause funds loss across the network
- It doesn't break consensus safety for the entire network (only affects the single node with disk full)
- It's recoverable through state sync

However, it's definitely "High" severity because:
- It causes data loss and state inconsistencies
- It requires manual intervention to detect and fix
- It can degrade validator operations significantly
- Multiple validators could be affected simultaneously if disk management is poor

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

This is likely to occur because:

1. **Natural Occurrence**: Disk full is a common operational issue that happens naturally as blockchain state grows
2. **No Prevention**: There's no backpressure mechanism to stop consensus when disk space is low
3. **Monitoring Gaps**: While there are disk space alerts in the monitoring configuration [10](#0-9) , they may not be acted upon quickly enough
4. **Silent Failure**: The lack of error logging means operators won't immediately know there's a problem
5. **Resource Exhaustion Attacks**: An attacker could accelerate disk consumption through state-heavy transactions

The alerts trigger at warning (<200 GB, 1h duration) and critical (<50 GB, 5m duration) levels. However, these alerts may trigger too late, after some blocks have already failed to persist. The 50GB threshold for critical alerts may not provide enough buffer when blocks are large or the commit rate is high.

## Recommendation

The fix requires proper error handling at multiple layers:

1. **Propagate errors in `wait_for_commit_ledger()`**: Change the return type to `Result<(), TaskError>` and propagate the error instead of discarding it.

2. **Handle errors in persisting phase**: Check the result of `wait_for_commit_ledger()` and return the error if commit failed.

3. **Add error handling in buffer manager**: Add a match branch for `Some(Err(e))` to handle persisting phase errors, log them, and potentially trigger node shutdown or state sync recovery.

4. **Add error logging**: Add error-level logging at each layer where errors occur, especially in the executor's `commit_ledger` function.

5. **Add backpressure mechanism**: Implement a mechanism to slow down or stop consensus when disk space is critically low, before writes start failing.

## Proof of Concept

The vulnerability can be demonstrated by:

1. Setting up a validator node with limited disk space
2. Filling the disk to capacity while consensus is running
3. Observing that RocksDB write operations fail with ENOSPC
4. Confirming that no errors are logged in consensus or executor logs
5. Verifying that `highest_committed_round` continues to increase despite failed writes
6. Restarting the node and observing that it has lost the "committed" blocks
7. Confirming that the node requires state sync to recover

A Rust integration test would need to:
- Mock the storage layer to simulate disk full errors
- Inject the error during commit_ledger operations
- Verify that the persisting phase still returns Ok
- Verify that buffer manager updates highest_committed_round
- Confirm no error logging occurs

### Citations

**File:** consensus/consensus-types/src/pipelined_block.rs (L566-566)
```rust
            let _ = fut.commit_ledger_fut.await;
```

**File:** consensus/src/pipeline/persisting_phase.rs (L71-74)
```rust
            b.wait_for_commit_ledger().await;
        }

        let response = Ok(blocks.last().expect("Blocks can't be empty").round());
```

**File:** storage/schemadb/src/lib.rs (L396-396)
```rust
        | ErrorKind::IOError
```

**File:** execution/executor/src/block_executor/mod.rs (L366-369)
```rust
        info!(
            LogSchema::new(LogEntry::BlockExecutor).block_id(block_id),
            "commit_ledger"
        );
```

**File:** execution/executor/src/block_executor/mod.rs (L388-390)
```rust
        self.db
            .writer
            .commit_ledger(target_version, Some(&ledger_info_with_sigs), None)?;
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L107-107)
```rust
            self.ledger_db.metadata_db().write_schemas(ledger_batch)?;
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L1099-1104)
```rust
            executor
                .commit_ledger(ledger_info_with_sigs_clone)
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
```

**File:** consensus/src/pipeline/buffer_manager.rs (L968-972)
```rust
                Some(Ok(round)) = self.persisting_phase_rx.next() => {
                    // see where `need_backpressure()` is called.
                    self.pending_commit_votes = self.pending_commit_votes.split_off(&(round + 1));
                    self.highest_committed_round = round;
                    self.pending_commit_blocks = self.pending_commit_blocks.split_off(&(round + 1));
```

**File:** types/src/on_chain_config/consensus_config.rs (L239-241)
```rust
    pub fn decoupled_execution(&self) -> bool {
        true
    }
```

**File:** terraform/helm/monitoring/files/rules/alerts.yml (L91-118)
```yaml
  - alert: Validator Low Disk Space (warning)
    expr: (kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~".*(validator|fullnode)-e.*"} - kubelet_volume_stats_used_bytes) / 1024 / 1024 / 1024 < 200
    for: 1h
    labels:
      severity: warning
      summary: "Less than 200 GB of free space on Aptos Node."
    annotations:
      description: "(This is a warning, deal with it in working hours.) A validator or fullnode pod has less than 200 GB of disk space. Take these steps:
        1. If only a few nodes have this issue, it might be that they are not typically spec'd or customized differently, \
          it's most likely a expansion of the volume is needed soon. Talk to the PE team. Otherwise, it's a bigger issue.
        2. Pass this issue on to the storage team. If you are the storage team, read on.
        3. Go to the dashboard and look for the stacked up column family sizes. \
          If the total size on that chart can't justify low free disk space, we need to log in to a node to see if something other than the AptosDB is eating up disk. \
          Start from things under /opt/aptos/data.
        3 Otherwise, if the total size on that chart is the majority of the disk consumption, zoom out and look for anomalies -- sudden increases overall or on a few \
          specific Column Families, etc. Also check average size of each type of data. Reason about the anomaly with changes in recent releases in mind.
        4 If everything made sense, it's a bigger issue, somehow our gas schedule didn't stop state explosion before an alert is triggered. Our recommended disk \
          spec and/or default pruning configuration, as well as storage gas schedule need updates. Discuss with the ecosystem team and send out a PR on the docs site, \
          form a plan to inform the node operator community and prepare for a on-chain proposal to update the gas schedule."
  - alert: Validator Very Low Disk Space (critical)
    expr: (kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~".*(validator|fullnode)-e.*"} - kubelet_volume_stats_used_bytes) / 1024 / 1024 / 1024 < 50
    for: 5m
    labels:
      severity: critical
      summary: "Less than 50 GB of free space on Aptos Node."
    annotations:
      description: "A validator or fullnode pod has less than 50 GB of disk space -- that's dangerously low. \
        1. A warning level alert of disk space less than 200GB should've fired a few days ago at least, search on slack and understand why it's not dealt with.
```
