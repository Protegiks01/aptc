# Audit Report

## Title
Consensus Liveness Failure Due to Missing State Cleanup in SecretShareManager After Mid-Epoch Sync

## Summary
The SecretShareManager does not properly clean up its internal state during mid-epoch synchronization operations, causing blocks to become permanently stuck in the processing queue when they re-enter after a `sync_to_target` operation. This results in consensus liveness failure on affected validator nodes.

## Finding Description

The vulnerability exists in the interaction between three components in the Aptos consensus secret sharing pipeline:

**1. Missing Reset Signal During sync_to_target**

During mid-epoch synchronization, the `reset()` method only sends `ResetSignal::TargetRound` to RandManager and BufferManager, but not to SecretShareManager: [1](#0-0) 

Note that only `reset_tx_to_rand_manager` and `reset_tx_to_buffer_manager` are extracted and used. In contrast, during `end_epoch()`, SecretShareManager does receive a reset signal: [2](#0-1) 

**2. Incomplete State Cleanup in process_reset**

When SecretShareManager does receive a reset (only during epoch end), the `process_reset()` method clears the block_queue but only updates the highest_known_round without cleaning up the HashMap: [3](#0-2) 

The `update_highest_known_round()` method only updates a single field: [4](#0-3) 

The `secret_share_map` HashMap is never cleaned up: [5](#0-4) 

**3. Stale "Decided" State Blocks Re-processing**

When a block that was previously processed to "Decided" state re-enters after sync, both `add_share()` and `add_share_with_metadata()` return early without processing: [6](#0-5) [7](#0-6) 

**Liveness Failure Mechanism:**

When blocks are processed, they are added to the block_queue with all rounds marked as pending secret keys: [8](#0-7) 

The block waits for `set_secret_shared_key()` to be called, which removes the round from `pending_secret_key_rounds`. However, if the SecretShareItem is already "Decided", no aggregation occurs and no decision is sent through the channel. The block remains stuck because `is_fully_secret_shared()` never returns true: [9](#0-8) 

The `dequeue_ready_prefix()` method cannot dequeue stuck blocks: [10](#0-9) 

**Comparison with RandManager:**

The RandManager properly handles this scenario with explicit cleanup logic. The comment explains the exact issue this vulnerability causes: [11](#0-10) 

The RandManager uses `BTreeMap` and `split_off()` to remove stale entries, while SecretShareStore uses `HashMap` without any cleanup mechanism.

## Impact Explanation

This is a **Critical Severity** vulnerability causing **Total Loss of Liveness**:

- **Affected nodes cannot process any blocks** after the stuck block, as the block queue requires processing in order
- **Consensus cannot make progress** on affected nodes - they become unable to participate in block production or validation
- **Requires manual intervention** (node restart) or potential protocol-level recovery
- **Can affect multiple validators** if they sync to the same target during network partitions or recovery scenarios
- **Violates the Consensus Liveness property**, a fundamental security guarantee

Per the Aptos Bug Bounty Program severity categories, "Total loss of liveness/network availability" is explicitly listed as **Critical Severity** (up to $1,000,000). This vulnerability directly causes total loss of liveness on affected validator nodes.

## Likelihood Explanation

**High Likelihood** - This vulnerability triggers automatically during routine operations:

**Common Trigger Scenarios:**
- State synchronization when validators fall behind (routine operation)
- Network partitions followed by recovery
- Validator restarts requiring sync_to_target
- Any mid-epoch sync to a specific ledger info

**No Attacker Required:**
The vulnerability triggers automatically when:
1. A block is processed normally before sync (happens constantly)
2. Node performs sync_to_target (routine when falling behind)
3. Same block re-enters during recovery (guaranteed by consensus recovery protocol)

**Supporting Evidence:**
The fact that RandManager explicitly protects against this exact scenario (with detailed comment) confirms that block re-entry after sync is a real and understood phenomenon in the Aptos consensus architecture. The inconsistency in protection between RandManager and SecretShareManager makes this vulnerability highly likely to occur in production.

## Recommendation

**Immediate Fix:**

1. **Add SecretShareManager to reset() during sync_to_target:**

Modify the `reset()` method in `execution_client.rs` to include reset_tx_to_secret_share_manager:

```rust
async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
    let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager, reset_tx_to_secret_share_manager) = {
        let handle = self.handle.read();
        (
            handle.reset_tx_to_rand_manager.clone(),
            handle.reset_tx_to_buffer_manager.clone(),
            handle.reset_tx_to_secret_share_manager.clone(), // ADD THIS
        )
    };
    
    // ... existing rand_manager and buffer_manager reset logic ...
    
    // ADD THIS BLOCK:
    if let Some(mut reset_tx) = reset_tx_to_secret_share_manager {
        let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::ResetDropped)?;
        ack_rx.await.map_err(|_| Error::ResetDropped)?;
    }
    
    Ok(())
}
```

2. **Implement proper cleanup in SecretShareStore:**

Change `secret_share_map` from `HashMap` to `BTreeMap` and add cleanup logic:

```rust
pub struct SecretShareStore {
    epoch: u64,
    self_author: Author,
    secret_share_config: SecretShareConfig,
    secret_share_map: BTreeMap<Round, SecretShareItem>, // Change from HashMap
    highest_known_round: u64,
    decision_tx: Sender<SecretSharedKey>,
}

pub fn reset(&mut self, round: u64) {
    self.update_highest_known_round(round);
    // Remove future rounds items to prevent blocks from getting stuck
    let _ = self.secret_share_map.split_off(&round);
}
```

3. **Update process_reset to call the new reset method:**

```rust
fn process_reset(&mut self, request: ResetRequest) {
    let ResetRequest { tx, signal } = request;
    let target_round = match signal {
        ResetSignal::Stop => 0,
        ResetSignal::TargetRound(round) => round,
    };
    self.block_queue = BlockQueue::new();
    self.secret_share_store.lock().reset(target_round); // Use reset instead of update_highest_known_round
    self.stop = matches!(signal, ResetSignal::Stop);
    let _ = tx.send(ResetAck::default());
}
```

## Proof of Concept

While a full end-to-end PoC would require a complete consensus test environment, the vulnerability can be demonstrated through the following scenario:

```rust
#[test]
fn test_secret_share_manager_stuck_block_after_sync() {
    // Setup: Create a SecretShareStore with a block already in "Decided" state
    let mut store = SecretShareStore::new(1, author, config, decision_tx);
    
    // Step 1: Process block at round R normally
    let share1 = create_test_share(round_r, author);
    store.add_self_share(share1.clone()).unwrap();
    // Add enough shares to reach "Decided" state
    for validator in validators {
        let share = create_test_share(round_r, validator);
        store.add_share(share).unwrap();
    }
    // Block is now in "Decided" state
    
    // Step 2: Simulate sync_to_target - SecretShareStore is NOT reset
    // (only RandManager and BufferManager are reset in current code)
    store.update_highest_known_round(round_r + 100); // Only updates counter
    
    // Step 3: Block R re-enters after sync
    let share2 = create_test_share(round_r, author);
    let result = store.add_self_share(share2); // Returns Ok() but does nothing
    assert!(result.is_ok()); // Succeeds but no processing occurred
    
    // Step 4: Block is added to block_queue with pending_secret_key_rounds
    let mut queue = BlockQueue::new();
    let mut pending = HashSet::new();
    pending.insert(round_r);
    let item = QueueItem::new(blocks, None, pending);
    queue.push_back(item);
    
    // Step 5: Block can never be dequeued - STUCK FOREVER
    let ready = queue.dequeue_ready_prefix();
    assert!(ready.is_empty()); // Block cannot be dequeued - LIVENESS FAILURE
}
```

The vulnerability is proven by:
1. Code analysis showing missing reset signal to SecretShareManager
2. RandManager's explicit protection against this exact scenario
3. Execution flow showing blocks get stuck in queue when "Decided" items are encountered
4. High likelihood based on routine sync operations in production consensus

### Citations

**File:** consensus/src/pipeline/execution_client.rs (L674-709)
```rust
    async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
        let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager) = {
            let handle = self.handle.read();
            (
                handle.reset_tx_to_rand_manager.clone(),
                handle.reset_tx_to_buffer_manager.clone(),
            )
        };

        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        }

        if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
            // reset execution phase and commit phase
            let (tx, rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::ResetDropped)?;
            rx.await.map_err(|_| Error::ResetDropped)?;
        }

        Ok(())
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L734-745)
```rust
        if let Some(mut tx) = reset_tx_to_secret_share_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop secret share manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop secret share manager");
        }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L112-130)
```rust
    async fn process_incoming_blocks(&mut self, blocks: OrderedBlocks) {
        let rounds: Vec<u64> = blocks.ordered_blocks.iter().map(|b| b.round()).collect();
        info!(rounds = rounds, "Processing incoming blocks.");

        let mut share_requester_handles = Vec::new();
        let mut pending_secret_key_rounds = HashSet::new();
        for block in blocks.ordered_blocks.iter() {
            let handle = self.process_incoming_block(block).await;
            share_requester_handles.push(handle);
            pending_secret_key_rounds.insert(block.round());
        }

        let queue_item = QueueItem::new(
            blocks,
            Some(share_requester_handles),
            pending_secret_key_rounds,
        );
        self.block_queue.push_back(queue_item);
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L172-184)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.secret_share_store
            .lock()
            .update_highest_known_round(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L108-128)
```rust
    fn add_share(&mut self, share: SecretShare, share_weight: u64) -> anyhow::Result<()> {
        match self {
            SecretShareItem::PendingMetadata(aggr) => {
                aggr.add_share(share, share_weight);
                Ok(())
            },
            SecretShareItem::PendingDecision {
                metadata,
                share_aggregator,
            } => {
                ensure!(
                    metadata == &share.metadata,
                    "[SecretShareItem] SecretShare metadata from {} mismatch with block metadata!",
                    share.author,
                );
                share_aggregator.add_share(share, share_weight);
                Ok(())
            },
            SecretShareItem::Decided { .. } => Ok(()),
        }
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L156-182)
```rust
    fn add_share_with_metadata(
        &mut self,
        share: SecretShare,
        share_weights: &HashMap<Author, u64>,
    ) -> anyhow::Result<()> {
        let item = std::mem::replace(self, Self::new(Author::ONE));
        let share_weight = *share_weights
            .get(share.author())
            .expect("Author must exist in weights");
        let new_item = match item {
            SecretShareItem::PendingMetadata(mut share_aggregator) => {
                let metadata = share.metadata.clone();
                share_aggregator.retain(share.metadata(), share_weights);
                share_aggregator.add_share(share, share_weight);
                SecretShareItem::PendingDecision {
                    metadata,
                    share_aggregator,
                }
            },
            SecretShareItem::PendingDecision { .. } => {
                bail!("Cannot add self share in PendingDecision state");
            },
            SecretShareItem::Decided { .. } => return Ok(()),
        };
        let _ = std::mem::replace(self, new_item);
        Ok(())
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L207-214)
```rust
pub struct SecretShareStore {
    epoch: u64,
    self_author: Author,
    secret_share_config: SecretShareConfig,
    secret_share_map: HashMap<Round, SecretShareItem>,
    highest_known_round: u64,
    decision_tx: Sender<SecretSharedKey>,
}
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L233-235)
```rust
    pub fn update_highest_known_round(&mut self, round: u64) {
        self.highest_known_round = std::cmp::max(self.highest_known_round, round);
    }
```

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L60-76)
```rust
    pub fn is_fully_secret_shared(&self) -> bool {
        self.pending_secret_key_rounds.is_empty()
    }

    pub fn set_secret_shared_key(&mut self, round: Round, key: SecretSharedKey) {
        let offset = self.offset(round);
        if self.pending_secret_key_rounds.contains(&round) {
            observe_block(
                self.blocks()[offset].timestamp_usecs(),
                BlockStage::SECRET_SHARING_ADD_DECISION,
            );
            let block = &self.blocks_mut()[offset];
            if let Some(tx) = block.pipeline_tx().lock().as_mut() {
                tx.secret_shared_key_tx.take().map(|tx| tx.send(Some(key)));
            }
            self.pending_secret_key_rounds.remove(&round);
        }
```

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L112-127)
```rust
    pub fn dequeue_ready_prefix(&mut self) -> Vec<OrderedBlocks> {
        let mut ready_prefix = vec![];
        while let Some((_starting_round, item)) = self.queue.first_key_value() {
            if item.is_fully_secret_shared() {
                let (_, item) = self.queue.pop_first().expect("First key must exist");
                for block in item.blocks() {
                    observe_block(block.timestamp_usecs(), BlockStage::SECRET_SHARING_READY);
                }
                let QueueItem { ordered_blocks, .. } = item;
                ready_prefix.push(ordered_blocks);
            } else {
                break;
            }
        }
        ready_prefix
    }
```

**File:** consensus/src/rand/rand_gen/rand_store.rs (L253-259)
```rust
    pub fn reset(&mut self, round: u64) {
        self.update_highest_known_round(round);
        // remove future rounds items in case they're already decided
        // otherwise if the block re-enters the queue, it'll be stuck
        let _ = self.rand_map.split_off(&round);
        let _ = self.fast_rand_map.as_mut().map(|map| map.split_off(&round));
    }
```
