# Audit Report

## Title
Critical Race Condition Between save_tree() and prune_tree() Causes Consensus State Corruption and Potential Chain Splits

## Summary
A critical TOCTOU (Time-of-Check-Time-of-Use) race condition exists between block insertion and pruning operations in the consensus persistent liveness storage. When `save_tree()` writes a block to RocksDB without holding the in-memory tree lock, concurrent `prune_tree()` operations can delete the block's parent, creating orphaned blocks on disk. Upon node restart, these orphaned blocks are pruned, causing consensus state corruption and potential validator inconsistency.

## Finding Description

The vulnerability exists in the non-atomic sequence of operations in `BlockStore::insert_block_inner()`. The function performs two critical operations without proper synchronization: [1](#0-0) 

The function first saves the block to persistent storage via `save_tree()` at line 512-514 **without holding any lock** on the in-memory tree (`self.inner`). Then at line 515, it separately acquires a write lock to update the in-memory tree. This creates a critical race window where the block exists on disk but not in the in-memory data structure.

Meanwhile, `BlockTree::commit_callback()` executes block pruning operations while holding a write lock: [2](#0-1) 

The callback invokes `find_blocks_to_prune()` which only examines the in-memory tree structure: [3](#0-2) 

**Race Condition Timeline:**

**T1**: Thread A calls `insert_block_inner(B2)` where B2's parent is B1
- Executes `save_tree([B2])` writing B2 to RocksDB **without holding `self.inner` lock**

**T2**: Thread B's commit callback acquires write lock on BlockTree (line 479: `tree.write().commit_callback()`)

**T3**: Thread B calls `find_blocks_to_prune()` which reads the in-memory tree
- B2 is **not yet in memory**, so the function is unaware that B2 depends on B1
- Identifies B1 (and ancestors) for pruning

**T4**: Thread B calls `storage.prune_tree([B1, ...])` deleting B1 from RocksDB

**T5**: Thread A acquires write lock and inserts B2 into in-memory tree

**Result**: B2 exists in both RocksDB and memory, but its parent B1 only exists in memory (deleted from disk).

Upon node restart, the recovery process loads blocks from persistent storage and detects B2 as an orphan: [4](#0-3) 

The `RecoveryData::find_blocks_to_prune()` function identifies blocks without valid parent chains: [5](#0-4) 

This function walks through blocks starting from the root. B2's parent B1 does not exist in the loaded blocks (it was deleted from disk), so B2 is marked as an orphan and subsequently pruned during startup.

**Invariant Violations:**

1. **Storage-Memory Consistency**: The design comment explicitly states that storage writes should complete before in-memory updates: [6](#0-5) 

However, the lack of atomic synchronization violates this invariant.

2. **Consensus Safety**: Different validator nodes can have inconsistent block histories after restarts, depending on when they restart relative to the race condition occurrence.

3. **Data Durability**: Blocks that were successfully persisted to disk are later lost during recovery, breaking the durability guarantee.

## Impact Explanation

This vulnerability meets **Critical Severity** criteria per the Aptos bug bounty program under the "Consensus/Safety Violations" category:

**Consensus State Inconsistency**: Validators that restart at different times relative to when the race condition occurs will have different sets of blocks. Node A restarting before the race has B2 in storage, while Node B restarting after the race has B2 pruned. This creates divergent views of the consensus history.

**Loss of Consensus-Critical Data**: The lost blocks may contain valid votes or quorum certificates that are necessary for consensus operation. Loss of these blocks can impact the ability to verify consensus decisions or reconstruct the consensus history.

**Potential Chain Divergence**: If a sufficient number of validators have divergent block histories due to restart timing, they could potentially reach different consensus decisions, especially if the lost blocks are part of competing forks that validators were evaluating.

**State Corruption**: The persistent storage state becomes inconsistent with the in-memory state during normal operation, and different nodes end up with different persistent state after restarts. This violates fundamental blockchain consistency requirements.

The vulnerability affects the core consensus layer's persistent storage mechanism, making it a fundamental threat to blockchain safety and determinism.

## Likelihood Explanation

**High Likelihood** - The race condition can occur during normal validator operation:

1. **Frequent Operations**: Block insertion and commit callbacks (which trigger pruning) are frequent concurrent operations during active consensus participation.

2. **No Special Privileges Required**: Any validator participating in normal consensus can trigger this condition through standard block proposal and voting operations.

3. **Wide Race Window**: The race window is widened by an asynchronous timestamp synchronization operation: [7](#0-6) 

The `await` at line 510 increases the probability that another thread can interleave and execute the commit callback before the write lock is acquired at line 515.

4. **Asynchronous Execution**: The `insert_block_inner()` function is async, allowing multiple block insertions and commit operations to run concurrently.

5. **Silent Corruption**: The corruption is not immediately detectable - both operations appear to succeed, but the state divergence only manifests upon node restart.

The vulnerability requires no attacker coordination and can trigger naturally during periods of high consensus activity.

## Recommendation

Implement atomic synchronization between storage writes and in-memory updates. The write lock on `self.inner` should be acquired **before** calling `save_tree()` to ensure that the in-memory tree reflects exactly what's in persistent storage at all times.

**Recommended Fix:**

```rust
pub async fn insert_block_inner(
    &self,
    pipelined_block: PipelinedBlock,
) -> anyhow::Result<Arc<PipelinedBlock>> {
    // ... existing prefetch and pipeline building code ...
    
    // ensure local time past the block time
    let block_time = Duration::from_micros(pipelined_block.timestamp_usecs());
    let current_timestamp = self.time_service.get_current_timestamp();
    if let Some(t) = block_time.checked_sub(current_timestamp) {
        if t > Duration::from_secs(1) {
            warn!("Long wait time {}ms for block {}", t.as_millis(), pipelined_block);
        }
        self.time_service.wait_until(block_time).await;
    }
    
    // CRITICAL FIX: Acquire write lock BEFORE storage write
    let mut tree = self.inner.write();
    
    // Now perform storage write while holding the lock
    self.storage
        .save_tree(vec![pipelined_block.block().clone()], vec![])
        .context("Insert block failed when saving block")?;
    
    // Insert into in-memory tree while still holding the lock
    tree.insert_block(pipelined_block)
}
```

This ensures that when `commit_callback()` acquires the write lock and calls `find_blocks_to_prune()`, it sees the complete in-memory state including any blocks that have been persisted to storage.

## Proof of Concept

While a complete runnable PoC would require a full consensus test environment, the race condition can be demonstrated through code inspection:

1. The critical race window exists between lines 512-515 of `block_store.rs`
2. The commit callback can acquire the write lock during this window via line 479
3. The pruning logic at line 589 only examines in-memory state
4. Recovery logic at lines 451-476 of `persistent_liveness_storage.rs` prunes orphaned blocks

A stress test that rapidly inserts blocks while triggering commit callbacks would likely reproduce this condition, with evidence appearing in node restart logs showing blocks being pruned that were previously inserted.

## Notes

This vulnerability represents a fundamental synchronization flaw in the consensus persistent storage layer. The race condition violates the design principle stated in the code comments that storage writes should precede in-memory updates. The lack of atomic coordination between these operations creates a window where different validators can end up with inconsistent views of the consensus history, which is a critical safety violation in a distributed consensus system.

### Citations

**File:** consensus/src/block_storage/block_store.rs (L88-89)
```rust
    /// The persistent storage backing up the in-memory data structure, every write should go
    /// through this before in-memory tree.
```

**File:** consensus/src/block_storage/block_store.rs (L499-511)
```rust
        // ensure local time past the block time
        let block_time = Duration::from_micros(pipelined_block.timestamp_usecs());
        let current_timestamp = self.time_service.get_current_timestamp();
        if let Some(t) = block_time.checked_sub(current_timestamp) {
            if t > Duration::from_secs(1) {
                warn!(
                    "Long wait time {}ms for block {}",
                    t.as_millis(),
                    pipelined_block
                );
            }
            self.time_service.wait_until(block_time).await;
        }
```

**File:** consensus/src/block_storage/block_store.rs (L512-515)
```rust
        self.storage
            .save_tree(vec![pipelined_block.block().clone()], vec![])
            .context("Insert block failed when saving block")?;
        self.inner.write().insert_block(pipelined_block)
```

**File:** consensus/src/block_storage/block_tree.rs (L405-434)
```rust
    pub(super) fn find_blocks_to_prune(
        &self,
        next_window_root_id: HashValue,
    ) -> VecDeque<HashValue> {
        // Nothing to do if this is the window root
        if next_window_root_id == self.window_root_id {
            return VecDeque::new();
        }

        let mut blocks_pruned = VecDeque::new();
        let mut blocks_to_be_pruned = vec![self.linkable_window_root()];

        while let Some(block_to_remove) = blocks_to_be_pruned.pop() {
            block_to_remove.executed_block().abort_pipeline();
            // Add the children to the blocks to be pruned (if any), but stop when it reaches the
            // new root
            for child_id in block_to_remove.children() {
                if next_window_root_id == *child_id {
                    continue;
                }
                blocks_to_be_pruned.push(
                    self.get_linkable_block(child_id)
                        .expect("Child must exist in the tree"),
                );
            }
            // Track all the block ids removed
            blocks_pruned.push_back(block_to_remove.id());
        }
        blocks_pruned
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L567-600)
```rust
    pub fn commit_callback(
        &mut self,
        storage: Arc<dyn PersistentLivenessStorage>,
        block_id: HashValue,
        block_round: Round,
        finality_proof: WrappedLedgerInfo,
        commit_decision: LedgerInfoWithSignatures,
        window_size: Option<u64>,
    ) {
        let current_round = self.commit_root().round();
        let committed_round = block_round;
        let commit_proof = finality_proof
            .create_merged_with_executed_state(commit_decision)
            .expect("Inconsistent commit proof and evaluation decision, cannot commit block");

        debug!(
            LogSchema::new(LogEvent::CommitViaBlock).round(current_round),
            committed_round = committed_round,
            block_id = block_id,
        );

        let window_root_id = self.find_window_root(block_id, window_size);
        let ids_to_remove = self.find_blocks_to_prune(window_root_id);

        if let Err(e) = storage.prune_tree(ids_to_remove.clone().into_iter().collect()) {
            // it's fine to fail here, as long as the commit succeeds, the next restart will clean
            // up dangling blocks, and we need to prune the tree to keep the root consistent with
            // executor.
            warn!(error = ?e, "fail to delete block");
        }
        self.process_pruned_blocks(ids_to_remove);
        self.update_window_root(window_root_id);
        self.update_highest_commit_cert(commit_proof);
    }
```

**File:** consensus/src/persistent_liveness_storage.rs (L451-476)
```rust
        quorum_certs: &mut Vec<QuorumCert>,
    ) -> Vec<HashValue> {
        // prune all the blocks that don't have root as ancestor
        let mut tree = HashSet::new();
        let mut to_remove = HashSet::new();
        tree.insert(root_id);
        // assume blocks are sorted by round already
        blocks.retain(|block| {
            if tree.contains(&block.parent_id()) {
                tree.insert(block.id());
                true
            } else {
                to_remove.insert(block.id());
                false
            }
        });
        quorum_certs.retain(|qc| {
            if tree.contains(&qc.certified_block().id()) {
                true
            } else {
                to_remove.insert(qc.certified_block().id());
                false
            }
        });
        to_remove.into_iter().collect()
    }
```

**File:** consensus/src/persistent_liveness_storage.rs (L519-572)
```rust
    fn start(&self, order_vote_enabled: bool, window_size: Option<u64>) -> LivenessStorageData {
        info!("Start consensus recovery.");
        let raw_data = self
            .db
            .get_data()
            .expect("unable to recover consensus data");

        let last_vote = raw_data
            .0
            .map(|bytes| bcs::from_bytes(&bytes[..]).expect("unable to deserialize last vote"));

        let highest_2chain_timeout_cert = raw_data.1.map(|b| {
            bcs::from_bytes(&b).expect("unable to deserialize highest 2-chain timeout cert")
        });
        let blocks = raw_data.2;
        let quorum_certs: Vec<_> = raw_data.3;
        let blocks_repr: Vec<String> = blocks.iter().map(|b| format!("\n\t{}", b)).collect();
        info!(
            "The following blocks were restored from ConsensusDB : {}",
            blocks_repr.concat()
        );
        let qc_repr: Vec<String> = quorum_certs
            .iter()
            .map(|qc| format!("\n\t{}", qc))
            .collect();
        info!(
            "The following quorum certs were restored from ConsensusDB: {}",
            qc_repr.concat()
        );
        // find the block corresponding to storage latest ledger info
        let latest_ledger_info = self
            .aptos_db
            .get_latest_ledger_info()
            .expect("Failed to get latest ledger info.");
        let accumulator_summary = self
            .aptos_db
            .get_accumulator_summary(latest_ledger_info.ledger_info().version())
            .expect("Failed to get accumulator summary.");
        let ledger_recovery_data = LedgerRecoveryData::new(latest_ledger_info);

        match RecoveryData::new(
            last_vote,
            ledger_recovery_data.clone(),
            blocks,
            accumulator_summary.into(),
            quorum_certs,
            highest_2chain_timeout_cert,
            order_vote_enabled,
            window_size,
        ) {
            Ok(mut initial_data) => {
                (self as &dyn PersistentLivenessStorage)
                    .prune_tree(initial_data.take_blocks_to_prune())
                    .expect("unable to prune dangling blocks during restart");
```
