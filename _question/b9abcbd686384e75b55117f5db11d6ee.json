[
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Import: SchemaBatch] The SchemaBatch type (line 14, used at line 33) is from aptos_schemadb. If this external dependency changes its atomicity guarantees or batch size limits, can it silently break the pruning logic's correctness assumptions? (Low)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Function: prune()] [Pruning Invariant] The function should maintain the invariant that after prune(p, t) completes, all entries with stale_since_version <= t are deleted. Does the current implementation guarantee this, especially in the sharding path where no deletions occur (lines 35-50)? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Function: prune()] [Progress Invariant] The progress should always be <= ledger_version and progress should only increase monotonically. Are these invariants enforced by the code, or can incorrect caller behavior violate them causing undefined pruning behavior? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Integration] The StateKvMetadataPruner is used alongside StateKvShardPruner in the pruner module. If StateKvMetadataPruner updates progress (lines 67-70) but StateKvShardPruner fails to prune corresponding shard data, can this create inconsistency where metadata claims pruning succeeded but actual data remains? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Integration] The pruner depends on state_store to create StaleStateValueIndex entries when state is updated. If state_store fails to create these index entries correctly, can this cause pruner to miss stale values, leading to unbounded storage growth? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Observability] The file has no logging statements. If pruning fails silently or performs unexpectedly (especially in the buggy sharding path lines 35-50), how would operators detect and diagnose the issue before disk space exhaustion occurs? (Low)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Metrics] There are no metrics emitted for pruning progress, batch sizes, or deletion counts. Can this lack of observability hide performance degradation or logical bugs until they cause critical failures? (Low)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Function: prune()] [Sharding Implementation Mismatch] The sharding path uses StaleStateValueIndexByKeyHashSchema (lines 42), while non-sharding uses StaleStateValueIndexSchema (line 55). Are these schemas mutually exclusive per configuration, or can both exist simultaneously causing pruning to only clean one schema while the other grows unbounded? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Function: prune()] [Shard Coordination] When sharding is enabled (lines 35-50), the function iterates all shards but updates a single metadata progress value (lines 67-70). If different shards contain different stale_since_version ranges, can this single progress value incorrectly represent the pruning state across all shards? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Function: prune()] [Write Failure Recovery] If write_schemas() fails at line 72, what is the recovery mechanism? Does the caller retry with the same current_progress, or can the failure leave the system in a state where pruning can never proceed past that point? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Function: prune()] [Partial Batch Application] RocksDB's WriteBatch is atomic across column families. However, if the batch is split internally by RocksDB due to size limits, can this cause partial application where some deletes succeed and others fail, breaking the atomicity assumption? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Function: prune()] [Version Overflow] If target_version is u64::MAX and entries exist with stale_since_version = u64::MAX, does the comparison 'index.stale_since_version > target_version' (lines 46, 59) correctly evaluate to false, or can overflow in subsequent arithmetic cause incorrect behavior? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Function: prune()] [Zero Version] If current_progress = 0 and target_version = 0 are passed, does the function handle this edge case correctly, or does it cause issues with the seek operation (lines 43, 56) or version comparisons (lines 46, 59)? (Low)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Function: prune()] [Seek Semantics] The iter.seek(&current_progress) operation (lines 43, 56) uses SeekKeyCodec which encodes just the stale_since_version. Does this correctly position the iterator at the first entry with stale_since_version >= current_progress, or can it skip entries if there are multiple entries with the same stale_since_version? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Function: prune()] [Seek Beyond End] If current_progress is greater than all existing stale_since_version values, does iter.seek() correctly position at end-of-iterator causing the for-loop to execute zero times, or does it cause an error or undefined positioning? (Low)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Function: prune()] [Double Delete] If the same StaleStateValueIndex or StateValue entry is added to the batch multiple times due to duplicate index entries or iterator issues, does RocksDB handle the duplicate delete gracefully, or can this cause errors or undefined behavior? (Low)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Function: prune()] [Delete Non-Existent Key] If an index entry exists but the corresponding StateValue was already deleted by another process, does batch.delete::<StateValueSchema>() (line 63) fail or silently succeed? Can this cause the batch write to fail entirely? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Function: prune()] [Key Encoding Consistency] The StateValueSchema deletion uses (index.state_key, index.version) as the key (line 63). If the StateKey encoding or serialization has changed between when the index was created and when pruning occurs, can this cause the deletion to target the wrong entry or fail entirely? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Function: prune()] [Version Encoding] The stale_since_version and version fields in StaleStateValueIndex are encoded as big-endian u64. If the encoding format changes or is inconsistent, can this cause iterator ordering issues or incorrect comparisons breaking pruning correctness? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Function: prune()] [Metadata Overwrite] The batch.put::<DbMetadataSchema>() operation at lines 67-70 overwrites the previous progress value. If multiple pruners run concurrently and both update metadata, can a race condition cause progress to move backwards or be lost? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Function: progress()] [Metadata Read Consistency] The progress() function reads from metadata_db() (lines 76-79) without any transaction or snapshot isolation. Can a concurrent prune() operation cause progress() to read partial or inconsistent state? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Function: prune()] [Shard Iterator Error Handling] In the sharding loop (lines 38-50), if iter creation or seek fails for one shard (lines 39-43), the '?' operator returns immediately. Does this leave other shards in an inconsistent state, or should errors be collected and all shards processed before failing? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Function: prune()] [Shard Iteration Order] The shards are processed sequentially from 0 to num_shards-1 (line 38). Does the iteration order matter for correctness, or can a malicious actor exploit the ordering to cause specific shards to be processed faster/slower affecting system availability? (Low)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Function: new()] [Constructor Safety] The new() function takes Arc<StateKvDb> by value (lines 24-26). If the caller passes a StateKvDb that's already being used by another pruner instance, can concurrent access to the same database cause race conditions or data corruption? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs] [Function: prune()] [Reentrancy] If prune() somehow causes recursive or reentrant calls to itself (e.g., through callback or error handling), can this cause stack overflow, deadlock, or database corruption due to nested transaction attempts? (Low)"
]