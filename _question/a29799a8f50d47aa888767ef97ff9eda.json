[
  "[File: aptos-core/crates/indexer/src/models/token_models/token_claims.rs] [Function: from_write_table_item()] [Duplicate write processing] If the same WriteTableItem is processed twice (e.g., due to indexer restart), does the UPSERT at line 91 correctly update last_transaction_version, or can duplicate processing be detected and rejected? (Low)",
  "[File: aptos-core/crates/indexer/src/models/token_models/token_claims.rs] [Function: from_write_table_item()] [Missing metadata log detail] At lines 97-103, the warning logs transaction_version and table_handle but not the actual table item contents - if debugging why metadata is missing, can the lack of detail prevent root cause analysis? (Low)",
  "[File: aptos-core/crates/indexer/src/models/token_models/token_claims.rs] [Function: from_write_table_item()] [Invalid token value log detail] At lines 105-111, the warning logs value_type and value but not the expected Token struct fields - can this make it impossible to determine if the issue is malformed on-chain data or indexer parsing bug? (Low)",
  "[File: aptos-core/crates/indexer/src/models/token_models/token_claims.rs] [Function: from_delete_table_item()] [Panic message detail] At lines 136-140, the panic message includes Version, table_handle, and all metadata - can this expose sensitive information or internal system state to attackers who trigger the panic? (Low)",
  "[File: aptos-core/crates/indexer/src/models/token_models/token_claims.rs] [Function: from_write_table_item()] [Silent None return] At lines 52, 61, and 103, the function returns None without logging - can these silent failures hide indexing issues where claims are created on-chain but never appear in queries? (Medium)",
  "[File: aptos-core/crates/indexer/src/models/token_models/token_claims.rs] [Function: from_write_table_item()] [Token data consistency] At lines 73-76, token_data_id_hash and token_data_id are stored - are these values consistent with entries in other token indexer tables (like current_token_datas), or can mismatches cause broken foreign key relationships? (Medium)",
  "[File: aptos-core/crates/indexer/src/models/token_models/token_claims.rs] [Function: from_write_table_item()] [Collection data consistency] At lines 71 and 75, collection_data_id_hash and collection_id are stored - if these don't match records in current_collection_datas table, can queries joining on collections fail or return incorrect results? (Medium)",
  "[File: aptos-core/crates/indexer/src/models/token_models/token_claims.rs] [Function: from_write_table_item()] [Owner consistency with token_ownerships] At line 83, from_address is the claim offerer - should this match an entry in current_token_ownerships proving they own the token, and can claims exist for tokens the offerer doesn't actually own? (High)",
  "[File: aptos-core/crates/indexer/src/models/token_models/token_claims.rs] [Function: from_write_table_item()] [Recipient address validation] At line 84, to_address is the claim recipient - is there validation that this address exists in accounts table, or can claims be made to non-existent addresses cluttering the database? (Low)",
  "[File: aptos-core/crates/indexer/src/models/token_models/token_claims.rs] [Function: from_write_table_item()] [Transaction version as i64] At line 40, txn_version is i64 - if transaction versions are actually u64 on-chain, can large version numbers (>2^63) cause casting overflow and become negative, breaking version comparisons? (Medium)",
  "[File: aptos-core/crates/indexer/src/models/token_models/token_claims.rs] [Function: from_write_table_item()] [Timestamp source trust] At line 41, txn_timestamp is provided by caller - if the caller has bugs or malicious behavior, can incorrect timestamps be written, causing temporal queries to return wrong results? (Medium)",
  "[File: aptos-core/crates/indexer/src/models/token_models/token_claims.rs] [Function: from_delete_table_item()] [Version monotonicity] At line 164, last_transaction_version is updated - if deletes are processed before writes from the same block, can version numbers become non-monotonic within a single claim's history? (Medium)",
  "[File: aptos-core/crates/indexer/src/models/token_models/token_claims.rs] [Function: from_delete_table_item()] [Timestamp for deletes] At line 165, last_transaction_timestamp is updated for deletes - does this represent when the claim was deleted or accepted, and can ambiguous semantics confuse analytics queries? (Low)",
  "[File: aptos-core/crates/indexer/src/models/token_models/token_claims.rs] [Function: from_write_table_item()] [Nested optional unwrapping] At line 44, unwrap() is called on table_item.data which is Option type - if there's any code path where data can be None, will this cause immediate panic without any error handling? (High)",
  "[File: aptos-core/crates/indexer/src/models/token_models/token_claims.rs] [Function: from_write_table_item()] [Pattern matching exhaustiveness] At lines 50-53, only TokenOfferId is matched with _ => None fallback - if TokenWriteSet gains new variants in future, will this silent fallback hide parsing errors that should be investigated? (Low)",
  "[File: aptos-core/crates/indexer/src/models/token_models/token_claims.rs] [Function: from_write_table_item()] [Double unwrap pattern] At lines\n\n### Citations\n\n**File:** crates/indexer/src/models/token_models/token_claims.rs (L1-172)\n```rust\n// Copyright Â© Aptos Foundation\n// SPDX-License-Identifier: Apache-2.0\n\n// This is required because a diesel macro makes clippy sad\n#![allow(clippy::extra_unused_lifetimes)]\n#![allow(clippy::unused_unit)]\n\nuse super::{token_utils::TokenWriteSet, tokens::TableHandleToOwner};\nuse crate::{schema::current_token_pending_claims, util::standardize_address};\nuse aptos_api_types::{DeleteTableItem as APIDeleteTableItem, WriteTableItem as APIWriteTableItem};\nuse bigdecimal::{BigDecimal, Zero};\nuse field_count::FieldCount;\nuse serde::{Deserialize, Serialize};\n\n#[derive(Debug, Deserialize, FieldCount, Identifiable, Insertable, Serialize)]\n#[diesel(primary_key(token_data_id_hash, property_version, from_address, to_address))]\n#[diesel(table_name = current_token_pending_claims)]\npub struct CurrentTokenPendingClaim {\n    pub token_data_id_hash: String,\n    pub property_version: BigDecimal,\n    pub from_address: String,\n    pub to_address: String,\n    pub collection_data_id_hash: String,\n    pub creator_address: String,\n    pub collection_name: String,\n    pub name: String,\n    pub amount: BigDecimal,\n    pub table_handle: String,\n    pub last_transaction_version: i64,\n    pub last_transaction_timestamp: chrono::NaiveDateTime,\n    pub token_data_id: String,\n    pub collection_id: String,\n}\n\nimpl CurrentTokenPendingClaim {\n    /// Token claim is stored in a table in the offerer's account. The key is token_offer_id (token_id + to address)\n    /// and value is token (token_id + amount)\n    pub fn from_write_table_item(\n        table_item: &APIWriteTableItem,\n        txn_version: i64,\n        txn_timestamp: chrono::NaiveDateTime,\n        table_handle_to_owner: &TableHandleToOwner,\n    ) -> anyhow::Result<Option<Self>> {\n        let table_item_data = table_item.data.as_ref().unwrap();\n\n        let maybe_offer = match TokenWriteSet::from_table_item_type(\n            table_item_data.key_type.as_str(),\n            &table_item_data.key,\n            txn_version,\n        )? {\n            Some(TokenWriteSet::TokenOfferId(inner)) => Some(inner),\n            _ => None,\n        };\n        if let Some(offer) = maybe_offer {\n            let maybe_token = match TokenWriteSet::from_table_item_type(\n                table_item_data.value_type.as_str(),\n                &table_item_data.value,\n                txn_version,\n            )? {\n                Some(TokenWriteSet::Token(inner)) => Some(inner),\n                _ => None,\n            };\n            if let Some(token) = maybe_token {\n                let table_handle = standardize_address(&table_item.handle.to_string());\n\n                let maybe_table_metadata = table_handle_to_owner.get(&table_handle);\n\n                if let Some(table_metadata) = maybe_table_metadata {\n                    let token_id = offer.token_id;\n                    let token_data_id_struct = token_id.token_data_id;\n                    let collection_data_id_hash =\n                        token_data_id_struct.get_collection_data_id_hash();\n                    let token_data_id_hash = token_data_id_struct.to_hash();\n                    // Basically adding 0x prefix to the previous 2 lines. This is to be consistent with Token V2\n                    let collection_id = token_data_id_struct.get_collection_id();\n                    let token_data_id = token_data_id_struct.to_id();\n                    let collection_name = token_data_id_struct.get_collection_trunc();\n                    let name = token_data_id_struct.get_name_trunc();\n\n                    return Ok(Some(Self {\n                        token_data_id_hash,\n                        property_version: token_id.property_version,\n                        from_address: standardize_address(&table_metadata.owner_address),\n                        to_address: standardize_address(&offer.to_addr),\n                        collection_data_id_hash,\n                        creator_address: standardize_address(&token_data_id_struct.creator),\n                        collection_name,\n                        name,\n                        amount: token.amount,\n                        table_handle,\n                        last_transaction_version: txn_version,\n                        last_transaction_timestamp: txn_timestamp,\n                        token_data_id,\n                        collection_id,\n                    }));\n                } else {\n                    aptos_logger::warn!(\n                        transaction_version = txn_version,\n                        table_handle = table_handle,"
]