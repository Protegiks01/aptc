[
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Trait: StaleNodeIndexSchemaTrait] [Type confusion] Can an attacker exploit the generic trait design to swap StaleNodeIndexSchema and StaleNodeIndexCrossEpochSchema implementations at runtime, causing regular pruning operations to delete epoch snapshot data and corrupt state recovery mechanisms? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Trait: StaleNodeIndexSchemaTrait] [Trait bound violation] Does the KeyCodec<Self> constraint in the trait definition properly prevent misuse where StaleNodeIndex encoding/decoding could be bypassed, allowing malformed stale node indices to be stored and causing state Merkle tree corruption? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Trait: StaleNodeIndexSchemaTrait] [Interface abuse] Can malicious code implement StaleNodeIndexSchemaTrait for unauthorized schema types, bypassing pruner safety checks and allowing arbitrary deletion of Merkle tree nodes leading to irreversible state loss? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Function: progress_metadata_key() StaleNodeIndexSchema] [Integer overflow] In line 21-22, can an attacker provide an extremely large shard_id value (near usize::MAX) that causes integer overflow when used as array index or memory allocation size in downstream pruning operations, leading to out-of-bounds memory access and potential RCE? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Function: progress_metadata_key() StaleNodeIndexSchema] [Shard ID manipulation] In line 21, if shard_id is Some() but exceeds the actual number of configured shards, can this cause the pruner to write progress metadata to non-existent shard locations, corrupting metadata database and causing loss of pruning progress tracking? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Function: progress_metadata_key() StaleNodeIndexSchema] [Negative shard ID] Although shard_id is usize, can type coercion or unsafe transmutation allow negative values to be passed, causing the metadata key DbMetadataKey::StateMerkleShardPrunerProgress to wrap around and overwrite critical system metadata like consensus progress? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Function: progress_metadata_key() StaleNodeIndexCrossEpochSchema] [Cross-epoch shard ID mismatch] In line 35-36, can providing mismatched shard_id values between epoch-ending and regular pruners cause the same shard to have conflicting progress values, leading to double-pruning or incomplete pruning of critical epoch snapshot data? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Function: progress_metadata_key() StaleNodeIndexSchema] [Shard ID collision] In line 22, can multiple concurrent pruner instances with the same shard_id value race to update DbMetadataKey::StateMerkleShardPrunerProgress, causing lost updates where pruning progress is incorrectly rolled back, resulting in re-pruning of already pruned nodes and potential state inconsistency? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Function: progress_metadata_key() StaleNodeIndexSchema] [Metadata key confusion] In lines 21-25, can race conditions during shard enablement/disablement cause the function to return inconsistent metadata keys (sometimes StateMerklePrunerProgress, sometimes StateMerkleShardPrunerProgress), leading to pruner progress being tracked in multiple locations and causing incorrect pruning decisions? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Function: progress_metadata_key() StaleNodeIndexCrossEpochSchema] [Epoch metadata isolation breach] In lines 34-39, can the epoch-ending pruner accidentally read/write to regular pruner metadata keys if shard_id logic is bypassed, causing epoch snapshots to be pruned prematurely and breaking state sync for new validators? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Function: progress_metadata_key() StaleNodeIndexSchema] [Unsharded fallback exploit] In line 24, when shard_id is None and StateMerklePrunerProgress is returned, can an attacker exploit the transition from sharded to unsharded mode to reset pruning progress, causing the pruner to re-scan and delete nodes that belong to recent versions, corrupting current state? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Impl: StaleNodeIndexSchema vs StaleNodeIndexCrossEpochSchema] [Schema mixing attack] Can an attacker craft database writes that store regular stale node indices in the epoch-ending schema column family (or vice versa), causing the wrong pruner to delete critical data and breaking epoch boundary state commitments? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Function: progress_metadata_key()] [Metadata namespace collision] Between lines 20-26 and 34-40, can identical shard_id values in both implementations cause DbMetadataKey::StateMerkleShardPrunerProgress and DbMetadataKey::EpochEndingStateMerkleShardPrunerProgress to point to the same shard, allowing cross-contamination of progress tracking? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Trait: StaleNodeIndexSchemaTrait] [Polymorphic dispatch confusion] Can generic code using <S: StaleNodeIndexSchemaTrait> accidentally apply epoch-ending pruning logic to regular state pruning (or vice versa) due to trait object type erasure, causing mass deletion of nodes across epoch boundaries? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Function: progress_metadata_key() StaleNodeIndexSchema] [TOCTOU race condition] Between line 21's shard_id check and line 22's metadata key creation, can the shard configuration change (e.g., sharding disabled), causing the pruner to write progress to a now-invalid shard-specific key, orphaning progress data and preventing future pruning? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Function: progress_metadata_key()] [Concurrent shard reconfiguration] Can multiple threads call progress_metadata_key() with different shard_id values during live shard rebalancing, causing pruning progress to be split across multiple metadata keys, leading to incomplete pruning and unbounded storage growth? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Impl: Both schemas] [Non-atomic metadata writes] If progress_metadata_key() is called without proper locking, can concurrent pruners for the same shard obtain the same DbMetadataKey and perform non-atomic read-modify-write operations on pruning progress, causing progress values to be clobbered and versions to be skipped or double-pruned? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Function: progress_metadata_key() StaleNodeIndexCrossEpochSchema] [Epoch boundary confusion] In lines 34-39, during epoch transitions, can the pruner incorrectly select between EpochEndingStateMerklePrunerProgress and EpochEndingStateMerkleShardPrunerProgress based on stale shard configuration, causing epoch snapshot nodes to be pruned before state sync completes? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Impl: StaleNodeIndexCrossEpochSchema] [Epoch snapshot deletion race] Can rapid epoch changes cause the epoch-ending pruner to prune snapshot data from epoch N while validators are still syncing from it, breaking the invariant that epoch snapshots must persist until the next epoch is finalized? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Function: name() StaleNodeIndexCrossEpochSchema] [Epoch pruner misidentification] In line 42-44, the hardcoded name 'epoch_snapshot_pruner' is used for logging and metrics, but can misconfiguration cause this to be applied to regular state pruning, making it impossible to diagnose which pruner is corrupting state? (Low)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Function: progress_metadata_key() StaleNodeIndexSchema] [Progress metadata corruption] If line 22 generates DbMetadataKey::StateMerkleShardPrunerProgress for an invalid shard_id, can subsequent reads of this metadata return None or garbage values, causing the pruner to reset to version 0 and delete all historical state including recent transactions? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Function: progress_metadata_key()] [Metadata key deserialization attack] Can an attacker corrupt the database to store malformed DbMetadataKey enum values that don't match StateMerkleShardPrunerProgress or EpochEndingStateMerkleShardPrunerProgress, causing deserialization to panic and crash the pruner, preventing garbage collection and causing disk space exhaustion? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Trait: StaleNodeIndexSchemaTrait] [Schema version mismatch] If the database contains stale node indices encoded with an old version of the Schema trait, can the trait's KeyCodec constraint fail to detect version mismatches, allowing the pruner to misinterpret node keys and delete wrong Merkle tree nodes? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Function: progress_metadata_key() StaleNodeIndexCrossEpochSchema] [State sync breakage] In line 36-38, if epoch-ending pruner progress is incorrectly advanced past the latest epoch snapshot due to shard_id confusion, can new validators fail to bootstrap because required epoch boundary state has been pruned? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs] [Impl: StaleNodeIndexCrossEpochSchema] [Epoch waypoint unavailability] Can aggressive epoch snapshot pruning based on incorrect progress metadata (line 36-38) cause waypoint proofs to become unavailable, preventing light clients from verifying chain state and breaking trustless verification? (High)"
]