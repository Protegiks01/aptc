[
  "[File: aptos-core/third_party/move/move-compiler-v2/src/file_format_generator/peephole_optimizer.rs] [Edge case: Maximum locals] For functions with maximum allowed locals (e.g., 256), can local index comparisons overflow or behave incorrectly? (Medium)",
  "[File: aptos-core/third_party/move/move-compiler-v2/src/file_format_generator/peephole_optimizer.rs] [Edge case: Deep nesting] Can deeply nested control flow (many branch\n\n### Citations\n\n**File:** third_party/move/move-compiler-v2/src/file_format_generator/peephole_optimizer.rs (L1-172)\n```rust\n// Copyright (c) Aptos Foundation\n// SPDX-License-Identifier: Apache-2.0\n\n//! This module contains the peephole optimizer for the Move file format bytecode.\n//! Peephole optimizations assume that the bytecode is valid, and all user-facing\n//! error checks have already been performed.\n\npub mod inefficient_loads;\npub mod optimizers;\npub mod reducible_pairs;\n\nuse inefficient_loads::InefficientLoads;\nuse move_binary_format::{\n    control_flow_graph::{ControlFlowGraph, VMControlFlowGraph},\n    file_format::{Bytecode, CodeOffset},\n};\nuse optimizers::{BasicBlockOptimizer, TransformedCodeChunk, WindowProcessor};\nuse reducible_pairs::ReduciblePairs;\nuse std::collections::BTreeMap;\n\n/// Pre-requisite: `code` should not have spec block associations.\n/// Run peephole optimizers on the given `code`, possibly modifying it.\n/// Returns the optimized code, along with mapping to original offsets\n/// in `code`.\npub fn optimize(code: &[Bytecode]) -> TransformedCodeChunk {\n    BasicBlockOptimizerPipeline::default().optimize(code)\n}\n\n/// A pipeline of basic block optimizers.\n/// Each optimizer is applied to each basic block in the code, in order.\nstruct BasicBlockOptimizerPipeline {\n    optimizers: Vec<Box<dyn BasicBlockOptimizer>>,\n}\n\nimpl BasicBlockOptimizerPipeline {\n    /// Default optimization pipeline of basic block optimizers.\n    pub fn default() -> Self {\n        Self {\n            optimizers: vec![\n                Box::new(WindowProcessor::new(ReduciblePairs)),\n                Box::new(WindowProcessor::new(InefficientLoads)),\n            ],\n        }\n    }\n\n    /// Run the basic block optimization pipeline on the given `code`,\n    /// returning new (possibly optimized) code.\n    pub fn optimize(&self, code: &[Bytecode]) -> TransformedCodeChunk {\n        let mut code_chunk = TransformedCodeChunk::make_from(code);\n        let mut cfg = VMControlFlowGraph::new(&code_chunk.code);\n        loop {\n            let optimized_blocks = self.get_optimized_blocks(&code_chunk, &cfg);\n            let optimized_code_chunk = Self::flatten_blocks(optimized_blocks);\n            let optimized_cfg = VMControlFlowGraph::new(&optimized_code_chunk.code);\n            if optimized_cfg.num_blocks() == cfg.num_blocks() {\n                // Proxy for convergence of basic block optimizations.\n                // This is okay for peephole optimizations that merge basic blocks.\n                // But may need to revisit if we have peephole optimizations that can\n                // split a basic block.\n                return optimized_code_chunk;\n            } else {\n                // Number of basic blocks changed, re-run the basic-block\n                // optimization pipeline again on the new basic blocks.\n                cfg = optimized_cfg;\n                code_chunk = optimized_code_chunk;\n            }\n        }\n    }\n\n    /// Returns a mapping from the original code's basic block start offsets to the optimized\n    /// basic blocks.\n    fn get_optimized_blocks(\n        &self,\n        original_block: &TransformedCodeChunk,\n        cfg: &VMControlFlowGraph,\n    ) -> BTreeMap<CodeOffset, TransformedCodeChunk> {\n        let mut optimized_blocks = BTreeMap::new();\n        for block_id in cfg.blocks() {\n            let start = cfg.block_start(block_id);\n            let end = cfg.block_end(block_id); // `end` is inclusive\n            let mut block = original_block.extract(start, end);\n            for bb_optimizer in self.optimizers.iter() {\n                block = bb_optimizer\n                    .optimize(&block.code)\n                    .remap(block.original_offsets);\n            }\n            optimized_blocks.insert(start, block);\n        }\n        optimized_blocks\n    }\n\n    /// Flatten the individually optimized basic blocks into a single code vector.\n    fn flatten_blocks(\n        optimized_blocks: BTreeMap<CodeOffset, TransformedCodeChunk>,\n    ) -> TransformedCodeChunk {\n        let mut optimized_code = TransformedCodeChunk::empty();\n        let mut block_mapping = BTreeMap::new();\n        for (offset, block) in optimized_blocks {\n            block_mapping.insert(offset, optimized_code.code.len() as CodeOffset);\n            optimized_code.extend(block, 0);\n        }\n        Self::remap_branch_targets(&mut optimized_code.code, &block_mapping);\n        optimized_code\n    }\n\n    /// Use `remap` to update branch targets in the given `code`.\n    fn remap_branch_targets(code: &mut [Bytecode], remap: &BTreeMap<CodeOffset, CodeOffset>) {\n        for bc in code.iter_mut() {\n            match bc {\n                Bytecode::Branch(offset) | Bytecode::BrTrue(offset) | Bytecode::BrFalse(offset) => {\n                    *offset = *remap.get(offset).expect("
]