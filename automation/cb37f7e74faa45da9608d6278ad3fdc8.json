[
  {
    "question": "[File: config/src/config/consensus_config.rs] [Field: rand_rb_config] [Backoff Policy Manipulation] Can the ReliableBroadcastConfig backoff policy parameters (base: 2ms, factor: 100, max: 10000ms) be exploited to cause either insufficient retry delays or excessive delays in randomness broadcast? (Medium)",
    "url": "https://deepwiki.com/search/-you-are-an-elite-aptos-blockc_14c74194-c036-4998-9995-6531faf62d36?mode=deep",
    "timestamp": "2026-01-25 17:47:45.779046",
    "report_generated": false
  },
  {
    "question": "[File: config/src/config/consensus_config.rs] [Field: rand_rb_config] [RPC Timeout Attack] Can the rpc_timeout_ms (10000ms) in rand_rb_config be exploited by delaying randomness RPC responses to cause consensus rounds to timeout? (Medium)",
    "url": "https://deepwiki.com/search/-you-are-an-elite-aptos-blockc_3a0de71f-83b0-444b-b1fc-7151b5119f8d?mode=deep",
    "timestamp": "2026-01-25 17:47:58.771456",
    "report_generated": false
  },
  {
    "question": "[File: config/src/config/consensus_config.rs] [Field: enable_pre_commit] [Pre-commit Divergence] Can enable_pre_commit being enabled on fullnodes (despite optimize() trying to disable it) cause execution divergence from validators that do commit properly? (Critical)",
    "url": "https://deepwiki.com/search/-you-are-an-elite-aptos-blockc_122e7584-0bb9-4b03-bb5f-e8441bca7264?mode=deep",
    "timestamp": "2026-01-25 17:48:12.138350",
    "report_generated": false
  },
  {
    "question": "[File: config/src/config/consensus_config.rs] [Field: optimistic_sig_verification] [Signature Bypass] Can optimistic_sig_verification being enabled allow blocks with invalid signatures to temporarily propagate, causing consensus confusion before verification completes? (High)",
    "url": "https://deepwiki.com/search/-you-are-an-elite-aptos-blockc_9f623b20-bc8e-49ee-95f1-45a59175f201?mode=deep",
    "timestamp": "2026-01-25 17:48:25.739523",
    "report_generated": false
  },
  {
    "question": "[File: config/src/config/consensus_config.rs] [Field: enable_round_timeout_msg] [Timeout Message Flood] Can enable_round_timeout_msg being enabled allow attackers to flood the network with timeout messages, causing bandwidth exhaustion? (Medium)",
    "url": "https://deepwiki.com/search/-you-are-an-elite-aptos-blockc_547ca47f-957a-45b9-a617-b63c6da75d26?mode=deep",
    "timestamp": "2026-01-25 17:48:40.416807",
    "report_generated": false
  },
  {
    "question": "[File: config/src/config/consensus_config.rs] [Field: enable_optimistic_proposal_rx] [Optimistic Proposal Attack] Can enable_optimistic_proposal_rx allow malformed proposals to be processed before validation, causing crashes or state corruption? (High)",
    "url": "https://deepwiki.com/search/-you-are-an-elite-aptos-blockc_2f1690bf-02a1-4cb3-af5f-898ac2a964ae?mode=deep",
    "timestamp": "2026-01-25 17:48:54.855277",
    "report_generated": false
  },
  {
    "question": "[File: config/src/config/consensus_config.rs] [Field: enable_optimistic_proposal_tx] [Proposal Broadcast Race] Can enable_optimistic_proposal_tx cause proposals to be broadcast\n\n### Citations\n\n**File:** config/src/config/consensus_config.rs (L1-887)\n```rust\n// Copyright © Aptos Foundation\n// Parts of the project are originally copyright © Meta Platforms, Inc.\n// SPDX-License-Identifier: Apache-2.0\n\n#![allow(unexpected_cfgs)]\n\nuse super::DEFEAULT_MAX_BATCH_TXNS;\nuse crate::config::{\n    config_optimizer::ConfigOptimizer, config_sanitizer::ConfigSanitizer,\n    node_config_loader::NodeType, Error, NodeConfig, QuorumStoreConfig, ReliableBroadcastConfig,\n    SafetyRulesConfig, BATCH_PADDING_BYTES,\n};\nuse aptos_crypto::_once_cell::sync::Lazy;\nuse aptos_types::chain_id::ChainId;\nuse cfg_if::cfg_if;\nuse serde::{Deserialize, Serialize};\nuse serde_yaml::Value;\nuse std::path::PathBuf;\n\n// NOTE: when changing, make sure to update QuorumStoreBackPressureConfig::backlog_txn_limit_count as well.\nconst MAX_SENDING_BLOCK_TXNS_AFTER_FILTERING: u64 = 1800;\nconst MAX_SENDING_OPT_BLOCK_TXNS_AFTER_FILTERING: u64 = 1000;\nconst MAX_SENDING_BLOCK_TXNS: u64 = 5000;\npub(crate) static MAX_RECEIVING_BLOCK_TXNS: Lazy<u64> =\n    Lazy::new(|| 10000.max(2 * MAX_SENDING_BLOCK_TXNS));\n// stop reducing size at this point, so 1MB transactions can still go through\nconst MIN_BLOCK_BYTES_OVERRIDE: u64 = 1024 * 1024 + BATCH_PADDING_BYTES as u64;\n// We should reduce block size only until two QS batch sizes.\nconst MIN_BLOCK_TXNS_AFTER_FILTERING: u64 = DEFEAULT_MAX_BATCH_TXNS as u64 * 2;\n\n#[derive(Clone, Debug, Deserialize, PartialEq, Serialize)]\n#[serde(default, deny_unknown_fields)]\npub struct ConsensusConfig {\n    // length of inbound queue of messages\n    pub max_network_channel_size: usize,\n    pub max_sending_block_txns: u64,\n    pub max_sending_block_txns_after_filtering: u64,\n    pub max_sending_opt_block_txns_after_filtering: u64,\n    pub max_sending_block_bytes: u64,\n    pub max_sending_inline_txns: u64,\n    pub max_sending_inline_bytes: u64,\n    pub max_receiving_block_txns: u64,\n    pub max_receiving_block_bytes: u64,\n    pub max_pruned_blocks_in_mem: usize,\n    // Timeout for consensus to get an ack from mempool for executed transactions (in milliseconds)\n    pub mempool_executed_txn_timeout_ms: u64,\n    // Timeout for consensus to pull transactions from mempool and get a response (in milliseconds)\n    pub mempool_txn_pull_timeout_ms: u64,\n    pub round_initial_timeout_ms: u64,\n    pub round_timeout_backoff_exponent_base: f64,\n    pub round_timeout_backoff_max_exponent: usize,\n    pub safety_rules: SafetyRulesConfig,\n    // Only sync committed transactions but not vote for any pending blocks. This is useful when\n    // validators coordinate on the latest version to apply a manual transaction.\n    pub sync_only: bool,\n    // The size of the round/recovery manager and proposal buffer channels.\n    pub internal_per_key_channel_size: usize,\n    pub quorum_store_pull_timeout_ms: u64,\n    // Decides how long the leader waits before proposing empty block if there's no txns in mempool\n    pub quorum_store_poll_time_ms: u64,\n    // Whether to create partial blocks when few transactions exist, or empty blocks when there is\n    // pending ordering, or to wait for quorum_store_poll_count * 30ms to collect transactions for a block\n    //\n    // It is more efficient to execute larger blocks, as it creates less overhead. On the other hand\n    // waiting increases latency (unless we are under high load that added waiting latency\n    // is compensated by faster execution time). So we want to balance the two, by waiting only\n    // when we are saturating the execution pipeline:\n    // - if there are more pending blocks then usual in the execution pipeline,\n    //   block is going to wait there anyways, so we can wait to create a bigger/more efificent block\n    // - in case our node is faster than others, and we don't have many pending blocks,\n    //   but we still see very large recent (pending) blocks, we know that there is demand\n    //   and others are creating large blocks, so we can wait as well.\n    pub wait_for_full_blocks_above_pending_blocks: usize,\n    pub wait_for_full_blocks_above_recent_fill_threshold: f32,\n    pub intra_consensus_channel_buffer_size: usize,\n    pub quorum_store: QuorumStoreConfig,\n    pub vote_back_pressure_limit: u64,\n    /// If backpressure target block size is below it, update `max_txns_to_execute` instead.\n    /// Applied to execution, pipeline and chain health backpressure.\n    /// Needed as we cannot subsplit QS batches.\n    pub min_max_txns_in_block_after_filtering_from_backpressure: u64,\n    pub execution_backpressure: Option<ExecutionBackpressureConfig>,\n    pub pipeline_backpressure: Vec<PipelineBackpressureValues>,\n    // Used to decide if backoff is needed.\n    // must match one of the CHAIN_HEALTH_WINDOW_SIZES values.\n    pub window_for_chain_health: usize,\n    pub chain_health_backoff: Vec<ChainHealthBackoffValues>,\n    // Deprecated\n    pub qc_aggregator_type: QcAggregatorType,\n    // Max blocks allowed for block retrieval requests\n    pub max_blocks_per_sending_request: u64,\n    pub max_blocks_per_sending_request_quorum_store_override: u64,\n    pub max_blocks_per_receiving_request: u64,\n    pub max_blocks_per_receiving_request_quorum_store_override: u64,\n    pub broadcast_vote: bool,\n    pub proof_cache_capacity: u64,\n    pub rand_rb_config: ReliableBroadcastConfig,\n    pub num_bounded_executor_tasks: u64,\n    pub enable_pre_commit: bool,\n    pub max_pending_rounds_in_commit_vote_cache: u64,\n    pub optimistic_sig_verification: bool,\n    pub enable_round_timeout_msg: bool,\n    pub enable_optimistic_proposal_rx: bool,\n    pub enable_optimistic_proposal_tx: bool,\n}\n\n/// Deprecated\n#[derive(Clone, Debug, Default, Deserialize, Eq, PartialEq, Serialize)]\npub enum QcAggregatorType {\n    #[default]\n    NoDelay,\n}\n\n#[derive(Clone, Debug, Deserialize, PartialEq, Serialize)]\npub enum ExecutionBackpressureMetric {\n    Mean,\n    Percentile(f64),\n}\n\n#[derive(Clone, Debug, Deserialize, PartialEq, Serialize)]\npub struct ExecutionBackpressureLookbackConfig {\n    /// Look at execution time for this many last blocks\n    pub num_blocks_to_look_at: usize,\n\n    /// Only blocks above this threshold are treated as potentially needed recalibration\n    /// This is needed as small blocks have overheads that are irrelevant to the transactions\n    /// being executed.\n    pub min_block_time_ms_to_activate: usize,\n\n    /// Backpressure has a second check, where it only activates if\n    /// at least `min_blocks_to_activate` are above `min_block_time_ms_to_activate`\n    pub min_blocks_to_activate: usize,\n\n    /// Out of blocks in the window, the metric to use for calibration.\n    /// i.e. Percentile(0.5) means take a median of last `num_blocks_to_look_at` blocks.\n    pub metric: ExecutionBackpressureMetric,\n    /// Recalibrating max block size, to target blocks taking this long.\n    pub target_block_time_ms: usize,\n}\n\n/// Execution backpressure which handles txn/s variance,\n/// and adjusts block sizes to",
    "url": "https://deepwiki.com/search/-you-are-an-elite-aptos-blockc_ea9bea36-cb4f-41c4-8d8e-4ba1cc259bd6?mode=deep",
    "timestamp": "2026-01-25 17:49:12.204139",
    "report_generated": false
  }
]