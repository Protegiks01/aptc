[
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: prune()] [Error propagation] All operations use '?' operator for error propagation - if any operation fails (lines 56, 57, 59, 63, 64, 71), does the function properly roll back partial changes, or can it leave the batch in a half-applied state requiring manual recovery? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: new()] [Initialization failure] At line 42, if prune() fails during initialization, the error propagates but leaves the pruner in an undefined state - can subsequent operations on a partially-initialized pruner cause crashes or data corruption? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: prune()] [Batch commit failure recovery] If write_schemas at line 71 fails, the progress metadata in the batch is already set to target_version (lines 66-69) - on retry, will the pruner re-seek to current_progress correctly, or skip versions due to incorrect state? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: prune()] [Panic safety] If a panic occurs during iteration at line 58-62 (e.g., from unwrap in user code), can this leave locks held or database transactions open, causing deadlocks or corruption? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: new()] [Progress initialization failure] At line 30-34, get_or_initialize_subpruner_progress can fail - if it returns an error after partially writing progress metadata, can this corrupt the progress tracking permanently? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: new()] [Progress key collision] At line 32, DbMetadataKey::StateKvShardPrunerProgress(shard_id) uniquely identifies each shard's progress - if shard_id values collide or are reused incorrectly, can this cause multiple pruners to share progress metadata and skip pruning? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: prune()] [Progress monotonicity violation] At line 68, target_version is written as progress - if target_version < current_progress due to caller error, can this cause the pruner to go backwards and re-delete already processed data, or skip forward leaving gaps? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: new()] [Metadata-shard desync] The progress retrieved at line 30-34 comes from db_shard's metadata - if metadata_progress parameter at line 28 is from a different database or out of sync, can catch-up at line 42 delete incorrect version ranges? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: prune()] [Progress persistence guarantee] After write_schemas at line 71, is the progress update at line 66-69 guaranteed to be durably persisted, or can node crashes cause progress rollback requiring re-pruning of already deleted data (idempotency violation)? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: new()] [Catch-up version range] At line 42, catch-up prunes from progress to metadata_progress - if this range is extremely large (millions of versions), can the single prune() call exceed memory limits, cause OOM, or take so long that initialization timeouts occur? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Struct: StateKvShardPruner] [Shard ID validation] The shard_id field at line 20 is stored but never validated - if shard_id >= NUM_STATE_SHARDS (16), can this cause out-of-bounds access or routing to non-existent shards when used with db_shard? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: new()] [DB shard mismatch] At line 27, db_shard is provided externally - if the caller passes the wrong shard's DB for the given shard_id, can this cause the pruner to delete data from an incorrect shard, corrupting cross-shard consistency? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: prune()] [Cross-shard data leakage] If state_key_hash at line 64 routes to a different shard than self.shard_id, but deletion is attempted on self.db_shard, can this cause silent failures or corruption where stale data persists because it's in the wrong shard? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: shard_id()] [Shard ID immutability] At line 74-76, shard_id() returns the shard ID - if this value changes after construction (though usize is Copy), can callers make incorrect assumptions about which shard is being pruned? (Low)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Struct: StateKvShardPruner] [Shard boundary] If keys near shard boundaries (key_hash % 16 transitions) exist in multiple shards due to incorrect routing, can pruning one shard leave stale copies in adjacent shards, causing state bloat? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: prune()] [Merkle tree desynchronization] If StateValueByKeyHashSchema deletion at line 64 succeeds but the corresponding Jellyfish Merkle tree state isn't updated, can this cause Merkle root mismatches that prevent state proof generation or verification? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: prune()] [Snapshot consistency] If database snapshots are created while prune() executes at lines 58-71, can this capture an inconsistent view where indices are deleted but values remain, causing snapshot restores to fail? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: prune()] [Query inconsistency window] Between deleting StaleStateValueIndex at line 63 and StateValueByKeyHash at line 64, if queries execute, can they observe intermediate state where indices are missing but values exist, returning incorrect results? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: new()] [Historical query failure] After catch-up pruning at line 42 deletes historical versions, if nodes require those versions for consensus (e.g., state sync serving), can this cause protocol failures or inability to serve historical proofs? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: prune()] [Reorg vulnerability] If blockchain reorganization occurs while pruning executes, and versions being pruned become part of the canonical chain again, can deletion of these versions cause permanent state loss requiring node resync? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: prune()] [Iterator memory exhaustion] The for loop at line 58 iterates over potentially millions of stale entries without pagination or memory limits - can an attacker flood the state with updates causing iterator to consume all available memory? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: prune()] [Batch memory limit] SchemaBatch at line 52 accumulates all deletions in memory before committing - if target_version - current_progress is huge, can this cause OOM crashes or memory pressure affecting validator liveness? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: new()] [Initialization DoS] At line 42, catch-up calls prune() synchronously during initialization - if metadata_progress is far ahead, can this block initialization for extended periods causing node startup failures? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: prune()] [Disk I/O exhaustion] The write_schemas call at line 71 performs bulk deletion - if the batch is extremely large, can this saturate disk I/O bandwidth causing other critical operations (consensus, execution) to slow down or timeout? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs] [Function: prune()] [Compaction storm] Deleting large amounts of data at lines 63-64 triggers RocksDB compaction - can this cause cascading compactions that consume CPU/disk resources and degrade node performance below consensus thresholds? (Medium)"
]