[
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Function: push_with_feedback()] [Queue overflow attack] An attacker controlling a key can fill its queue to max_capacity, then push() will start dropping messages (line 102) - can a Byzantine validator flood a specific key (e.g., critical consensus messages) to cause selective message drops and disrupt consensus? (High)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Function: clear()] [Message loss] The clear() operation (lines 152-155) drains all messages without sending Dropped notifications via feedback channels - can this cause consensus message senders to wait indefinitely for acknowledgments that will never arrive? (High)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Function: push()] [Feedback channel absence] The push() function (lines 86-88) calls push_with_feedback with status_ch = None - does this mean callers have no way to know if their message was dropped, potentially causing consensus messages to be silently lost without retry mechanisms? (High)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Function: poll_next()] [Partial pop] If internal_queue.pop() returns Some (line 174) but the status notification send() panics or is interrupted, can the message be lost without acknowledgment, violating exactly-once processing guarantees for consensus messages? (Medium)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Function: Receiver::drop()] [Message abandonment] When the receiver drops (lines 158-163), all messages remaining in internal_queue are abandoned without sending Dropped status notifications - can this cause senders to wait indefinitely for feedback on consensus-critical messages that will never be processed? (High)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Function: Sender::drop()] [Panic in drop] If the waker.wake() call at line 137 panics (e.g., due to a buggy Waker implementation), can this cause the drop to abort, leaving num_senders in an inconsistent state and breaking the sender count invariant for consensus message channels? (High)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Function: Sender::drop()] [Lock poisoning] If a panic occurs while holding the shared_state lock (line 130), can the Mutex become poisoned, causing all subsequent push() and poll_next() operations to fail and halting all consensus message processing? (Critical)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Function: Receiver::drop()] [Debug assert violation] The debug_assert at line 161 checks !receiver_dropped, but what if drop() is somehow called twice (e.g., via unsafe code)? Will this cause a panic in debug builds but silent corruption in release builds used by validators? (Medium)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Function: Sender::drop()] [Double wake] If multiple senders are dropped concurrently and each reaches num_senders == 0 due to a race in decrement (line 133), can multiple threads call waker.wake() at line 137, causing double wake-ups and potential task executor bugs? (Low)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Function: poll_next()] [Poll contract violation] If poll_next() returns Poll::Pending at line 185 but forgets to register a waker, can this violate the Stream contract and cause the receiver to never be polled again, hanging consensus message processing? (Critical)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Function: poll_next()] [Premature termination] The check for num_senders == 0 at line 180 causes immediate termination, but what if the queue is empty due to temporary drainage and senders still exist? Can this incorrectly signal stream end during normal consensus operation? (High)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Function: is_terminated()] [Inconsistent termination] The is_terminated() check (line 192) only reads stream_terminated but doesn't validate that num_senders == 0 and queue is empty - can this return false positives or false negatives, causing consensus stream consumers to make incorrect decisions? (Medium)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Function: poll_next()] [Return after termination] After setting stream_terminated = true at line 181, poll_next() returns Poll::Ready(None) - but what if it's called again? Will it continue returning None or can subsequent calls violate the FusedStream contract and cause consensus bugs? (Low)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Trait: FusedStream] [Fused stream invariant] The FusedStream implementation (lines 190-194) should ensure poll_next() is never called after returning None, but does the implementation correctly maintain this invariant if callers don't respect it, potentially causing panics in validator message processing? (Medium)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Function: new()] [Zero capacity] The NonZeroUsize macro at line 242 panics if max_queue_size_per_key is 0, but what if this panic happens in a critical path during validator startup? Can this cause the entire validator to crash without proper error handling? (Medium)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Function: Config::new()] [Capacity overflow] If max_capacity (line 200) is set to usize::MAX, can this cause integer overflows in the underlying PerKeyQueue implementation when calculating total queue sizes, leading to memory exhaustion on validators? (High)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Function: new()] [Counter reset] The counters.reset() call at line 244 resets metrics globally - if multiple channels share the same counters, can this cause metrics from other channels to be lost, hiding consensus message drops or queue overflows? (Low)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Struct: Config] [QueueStyle mismatch] The Config allows setting queue_style (line 199) but there's no validation that the style is appropriate for the use case - can a LIFO queue be accidentally used for consensus messages where FIFO ordering is critical, causing consensus safety violations? (High)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Function: Config::counters()] [Static lifetime requirement] The counters parameter requires 'static lifetime (line 201) - can this restriction prevent proper cleanup of metrics during validator restarts or reconfigurations, causing metric label leaks? (Low)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Generic: K] [Hash collision] The key type K must implement Hash (line 30), but what if a malicious peer provides keys that deliberately collide in the hash function? Can this cause all messages to be queued under the same key, violating per-key queue guarantees and causing selective message drops in consensus? (High)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Generic: K] [Clone cost] The key type K must be Clone (line 30), and keys are cloned at line 94 for every push - if K is expensive to clone (e.g., large structs), can this cause significant CPU overhead in high-throughput consensus message processing? (Low)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Generic: M] [Message size] There are no bounds on the message type M - can extremely large messages cause memory exhaustion when queued, especially if combined with large max_capacity settings, causing validator OOM crashes? (High)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Generic: K] [Eq implementation] The key type requires Eq (line 30) but what if the Eq implementation is incorrect or inconsistent with Hash? Can this cause messages intended for different keys to be treated as the same key, mixing consensus messages from different validators? (High)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Struct: SharedState] [Lock contention] Every push() (line 98) and poll_next() (line 173) operation acquires the same Mutex - under high consensus message throughput with many senders and one receiver, can lock contention cause significant performance degradation and increased latency in block processing? (Medium)",
  "[File: aptos-core/crates/channel/src/aptos_channel.rs] [Function: push_with_feedback()] [Lock hold time] The lock is held from line 98 through the entire push operation including waker.wake() at line 110 - if wake() triggers expensive synchronous operations, can this cause extended lock hold times and block other senders, creating consensus message processing bottlenecks? (Medium)"
]