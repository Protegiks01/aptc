[
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [Race condition] Can concurrent calls to maybe_prune_single_version() with different target_version values create race conditions where next_version.load(Ordering::SeqCst) at line 45 reads stale values, causing multiple threads to prune the same version range and corrupt the database batch operations? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [Atomicity violation] Between the next_version.load() at line 45 and next_version.store() at line 76, can a Byzantine validator manipulate timing to cause two pruning operations to proceed with overlapping version ranges, leading to double-deletion of JellyfishMerkleNode entries and state corruption? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [Memory ordering] Does the SeqCst ordering at lines 45 and 76 provide sufficient synchronization guarantees when multiple validator nodes run pruning concurrently against a shared database, or can weaker memory orderings on other systems cause visibility issues leading to inconsistent pruning state? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [TOCTOU vulnerability] Between checking target_version_for_this_round > target_version at line 48 and the actual pruning operation starting at line 53, can another thread modify next_version causing time-of-check-time-of-use issues that bypass version boundaries and prune nodes that should be retained? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [Store ordering] If next_version.store() at line 76 uses SeqCst but the DB write at line 71 doesn't complete atomically, can a crash between these operations leave next_version pointing to a version that wasn't actually pruned in metadata, causing state divergence on restart? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [Integer overflow] At line 47, max(next_version, current_progress) performs max operation on Version (u64) values - can an attacker cause either value to be u64::MAX causing the max() to return incorrect results and bypass pruning boundaries? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [Version manipulation] Can a malicious validator feed a corrupted current_progress value near u64::MAX to line 47's max() operation, causing target_version_for_this_round to skip ahead and prune nodes that are still needed for state verification? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [Initialization bypass] When next_version is 0 (uninitialized), line 47 uses max(0, current_progress) - can an attacker exploit this initialization behavior by providing a manipulated current_progress to skip pruning critical early-version Merkle nodes? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [Version boundary violation] If target_version wraps around u64 or is set to u64::MAX-1, can line 48's comparison target_version_for_this_round > target_version produce unexpected results allowing pruning to continue beyond intended boundaries and delete live state? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [Unwrap panic] At line 76, next_version.unwrap_or(target_version) is used - if get_stale_node_indices returns None when indices.len() == limit (line 214 of mod.rs), does unwrap_or correctly handle the case where we've reached end of stale indices but there might be more beyond target_version? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: new()] [Initialization vulnerability] The next_version AtomicVersion is initialized to 0 at line 35 - can this uninitialized state be exploited during the first pruning operation to cause inconsistent behavior before the first call to maybe_prune_single_version() initializes it properly? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [Partial batch failure] If SchemaBatch operations at lines 61-64 succeed but write_schemas() at line 71 fails after partial writes, can this leave the database in an inconsistent state where some JellyfishMerkleNode entries are deleted but stale indices remain, corrupting Merkle tree integrity? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [Batch atomicity] The SchemaBatch at line 60 collects multiple delete operations for JellyfishMerkleNodeSchema and stale indices - if write_schemas() at line 71 is not truly atomic, can a crash mid-write leave some Merkle nodes deleted but their stale indices still present causing validation failures? (Critical)",
  "[File: aptos-labs/aptos-core",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [Schema deletion ordering] Deletes for JellyfishMerkleNodeSchema at line 62 and stale index schema S at line 63 are batched - if there's a dependency between these schemas, can incorrect deletion ordering cause referential integrity violations in the database? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [Database corruption] If metadata_db.write_schemas(batch) at line 71 partially succeeds then crashes, can the database enter a state where Merkle nodes are deleted but the progress metadata wasn't updated, causing the next pruning run to attempt re-deleting already deleted nodes? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [Progress tracking error] The progress metadata key at line 67 uses S::progress_metadata_key(None) with None shard_id - can this conflict with sharded pruners that use Some(shard_id), causing progress tracking to become inconsistent across pruner instances? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [Stale index exhaustion] When get_stale_node_indices returns empty indices with next_version=None at line 53-58, the unwrap_or(target_version) at line 76 stores target_version - can this cause the pruner to incorrectly skip version ranges where stale nodes actually exist? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [Index processing limit] get_stale_node_indices is called with usize::MAX limit at line 57 - can processing an unbounded number of stale indices in a single batch cause memory exhaustion attacks where malicious validators create excessive stale nodes to DoS the pruner? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [Node key collision] When deleting JellyfishMerkleNode entries using index.node_key at line 62, can hash collisions in node_key cause legitimate live nodes to be accidentally deleted if their keys collide with stale node keys? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [Iterator failure] If get_stale_node_indices at line 53 fails mid-iteration after returning some indices, does the error propagation prevent partial batch writes, or can some indices be processed while others are skipped causing incomplete pruning? (Medium)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [Stale index validation] The indices returned from get_stale_node_indices at line 53-58 are not validated before deletion - can corrupted stale indices with invalid node_keys cause deletion of critical Merkle tree nodes still required for state proofs? (Critical)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [Error propagation] If batch.delete operations at lines 62-63 fail for some indices but succeed for others, does try_for_each properly rollback all batch operations, or can partial deletions be committed causing state inconsistency? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [Recovery failure] After write_schemas() fails at line 71, the function returns the error but next_version at line 76 is never updated - on retry, will the pruner re-attempt the same failed batch potentially hitting the same error repeatedly and halting progress? (High)",
  "[File: aptos-core/storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs] [Function: maybe_prune_single_version()] [Transaction rollback] If any operation in the SchemaBatch fails (lines 61-69), does RocksDB's write_schemas guarantee full rollback, or can some deletes be committed while others fail, leaving the database in an inconsistent state? (Critical)"
]